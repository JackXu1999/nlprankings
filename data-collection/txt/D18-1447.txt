



















































Semi-Supervised Learning for Neural Keyphrase Generation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4142–4153
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4142

Semi-Supervised Learning for Neural Keyphrase Generation

Hai Ye∗ and Lu Wang
College of Computer and Information Science

Northeastern University
Boston, MA 02115

hye.me@outlook.com luwang@ccs.neu.edu

Abstract
We study the problem of generating
keyphrases that summarize the key points for a
given document. While sequence-to-sequence
(seq2seq) models have achieved remarkable
performance on this task (Meng et al., 2017),
model training often relies on large amounts
of labeled data, which is only applicable to
resource-rich domains. In this paper, we pro-
pose semi-supervised keyphrase generation
methods by leveraging both labeled data and
large-scale unlabeled samples for learning.
Two strategies are proposed. First, unlabeled
documents are first tagged with synthetic
keyphrases obtained from unsupervised
keyphrase extraction methods or a self-
learning algorithm, and then combined with
labeled samples for training. Furthermore, we
investigate a multi-task learning framework to
jointly learn to generate keyphrases as well as
the titles of the articles. Experimental results
show that our semi-supervised learning-based
methods outperform a state-of-the-art model
trained with labeled data only.

1 Introduction

Keyphrase extraction concerns the task of se-
lecting a set of phrases from a document that
can indicate the main ideas expressed in the in-
put (Turney, 2000; Hasan and Ng, 2014). It is
an essential task for document understanding be-
cause accurate identification of keyphrases can
be beneficial for a wide range of downstream-
ing natural language processing and information
retrieval applications. For instance, keyphrases
can be leveraged to improve text summarization
systems (Zhang et al., 2004; Liu et al., 2009a;
Wang and Cardie, 2013), facilitate sentiment anal-
ysis and opinion mining (Wilson et al., 2005;
Berend, 2011), and help with document cluster-
ing (Hammouda et al., 2005). Though relatively

∗Work was done while visiting Northeastern University.

Document:  
      In this paper, we consider an enthalpy formulation for a two-phase
Stefan problem arising from the solidification of aluminum during               
process. We solve this free boundary problem in a time varying three-
dimensional domain and consider convective heat transfer in the liquid
phase. The resulting equations are discretized using a characteristics
method in time and a                         method in space, and we propose a
numerical algorithm to solve the obtained nonlinear discretized problem.
Finally, numerical results are given which are compared with industrial
experimental measurements.
Keyphrase: 
                    ; thermal; conduction; convection; 

in document not in document

casting

casting

finite element

finite element

Figure 1: Sample document with labeled keyphrases.

easy to implement, extract-based approaches fail
to generate keyphrases that do not appear in
the source document, which are frequently pro-
duced by human annotators as shown in Figure
1. Recently, Meng et al. (2017) propose to use
a sequence-to-sequence model (Sutskever et al.,
2014) with copying mechanism for keyphrase gen-
eration, which is able to produce phrases that are
not in the input documents.

While seq2seq model demonstrates good per-
formance on keyphrase generation (Meng et al.,
2017), it heavily relies on massive amounts of
labeled data for model training, which is of-
ten unavailable for new domains. To overcome
this drawback, in this work, we investigate semi-
supervised learning for keyphrase generation, by
leveraging abundant unlabeled documents along
with limited labeled data. Intuitively, additional
documents, though unlabeled, can provide use-
ful knowledge on universal linguistic features
and discourse structure, such as context infor-
mation for keyphrases and that keyphrases are
likely to be noun phrases or main verbs. Fur-
thermore, learning with unlabeled data can also
mitigate the overfitting problem, which is often
caused by small size of labeled training data, and
thus improve model generalizability and enhance
keyphrase generation performance on unseen data.

Concretely, two major approaches are proposed
for leveraging unlabeled data. For the first method,



4143

unlabeled documents are first tagged with syn-
thetic keyphrases, then mixed with labeled data
for model pre-training. Synthetic keyphrases are
acquired through existing unsupervised keyphrase
extraction methods (e.g., TF-IDF or TextRank
(Mihalcea and Tarau, 2004)) or a self-learning al-
gorithm. The pre-trained model will further be
fine-tuned on the labeled data only. For the second
approach, we propose a multi-task learning (MTL)
framework1 by jointly learning the main task of
keyphrase generation based on labeled samples,
and an auxiliary task of title generation (Rush
et al., 2015) on unlabeled documents. Here one
encoder is shared among the two tasks. Impor-
tantly, we test our proposed methods on a seq2seq
framework, however, we believe they can be easily
adapted to other encoder-decoder-based systems.

Extensive experiments are conducted in sci-
entific paper domain. Results on five differ-
ent datasets show that all of our semi-supervised
learning-based models can uniformly significantly
outperform a state-of-the-art model (Meng et al.,
2017) as well as several competitive unsupervised
and supervised keyphrase extraction algorithms
based on F1 and recall scores. We further carry out
a cross-domain study on generating keyphrases for
news articles, where our models yield better F1
than a model trained on labeled data only. Finally,
we also show that training with unlabeled samples
can further produce performance gain even when
a large amount of labeled data is available.

2 Related Work

Keyphrase Extraction and Generation. Early
work mostly focuses on the keyphrase extrac-
tion task, and a two-step strategy is typically
designed. Specifically, a large pool of candi-
date phrases are first extracted according to pre-
defined syntactic templates (Mihalcea and Tarau,
2004; Wan and Xiao, 2008; Liu et al., 2009b,
2011) or their estimated importance scores (Hulth,
2003). In the second step, re-ranking is applied
to yield the final keyphrases, based on supervised
learning (Frank et al., 1999; Witten et al., 1999;
Hulth, 2003; Lopez and Romary, 2010; Kim and
Kan, 2009), unsupervised graph algorithms (Mi-
halcea and Tarau, 2004; Wan and Xiao, 2008;
Bougouin et al., 2013), or topic modelings (Liu
et al., 2009c, 2010). Keyphrase generation is stud-

1We use “semi-supervised learning” as a broad term to
refer to the two methods proposed in this paper.

ied in more recent work. For instance, Liu et al.
(2011) propose to use statistic machine translation
model to learn word-alignments between docu-
ments and keyphrases, enabling the model to gen-
erate keyphrases which do not appear in the input.
Meng et al. (2017) train seq2seq-based generation
models (Sutskever et al., 2014) on large-scale la-
beled corpora, which may not be applicable to a
new domain with minimal human labels.

Neural Semi-supervised Learning. As men-
tioned above, though significant success has
been achieved by seq2seq model in many NLP
tasks (Luong et al., 2015; See et al., 2017; Dong
and Lapata, 2016; Wang and Ling, 2016; Ye et al.,
2018), they often rely on large amounts of labeled
data, which are expensive to get. In order to mit-
igate the problem, semi-supervised learning has
been investigated to incorporate unlabeled data for
modeling training (Dai and Le, 2015; Ramachan-
dran et al., 2017). For example, neural machine
translation community has studied the usage of
source-side or target-side monolingual data to im-
prove translation quality (Gülçehre et al., 2015),
where generating synthetic data (Sennrich et al.,
2016; Zhang and Zong, 2016), multi-task learn-
ing (Zhang and Zong, 2016), and autoencoder-
based methods (Cheng et al., 2016) are shown
to be effective. Multi-task learning is also ex-
amined for sequence labeling tasks (Rei, 2017;
Liu et al., 2018). In our paper, we study
semi-supervised learning for keyphrase generation
based on seq2seq models, which has not been ex-
plored before. Besides, we focus on leveraging
source-side unlabeled articles to enhance perfor-
mance with synthetic keyphrase construction or
multi-task learning.

3 Neural Keyphrase Generation Model

In this section, we describe the neural keyphrase
generation model built on a sequence-to-sequence
model (Sutskever et al., 2014) as illustrated in Fig-
ure 2. We denote the input source document as
a sequence x = x1 · · ·x|x| and its correspond-
ing keyphrase set as a = {ai}|a|i=1, with ai as one
keyphrase.

Keyphrase Sequence Formulation. Different
from the setup by Meng et al. (2017), where in-
put article is paired with each keyphrase ai to
consist a training sample, we concatenate the
keyphrases in a into a keyphrase sequence y =



4144

Encoder

D
ecoder

key1
<>

keyn1
<>

key2
<>

key3
... 

Y

NoteIn this paper,  ... 
... ...  
 ... ...  ... key1 ... ...  
... ... 
...            ... ...  
... ...  
... ... ...  key3  ... ... 
... 

X
key2

Figure 2: Neural keyphrase generation model built on
sequence-to-sequence framework. Input is the docu-
ment, and output is the keyphrase sequence consisting
of phrases present (keyi) or absent (keyni ) in the input.

a1 ♦ a2 ♦ · · · ♦ a|a|, where ♦ is a segmenter
to separate the keyphrases2. With this setup, the
seq2seq model is capable to generate all possible
keyphrases in one sequence as well as capture the
contextual information between the keyphrases
from the same sequence.

Seq2Seq Attentional Model. With source doc-
ument x and its keyphrase sequence y, an encoder
encodes x into context vectors, from which a de-
coder then generates y. We set the encoder as one-
layer bi-directional LSTM model and the decoder
as another one-layer LSTM model (Hochreiter and
Schmidhuber, 1997). The probability of generat-
ing target sequence p(y|x) is formulated as:

p(y|x) =
|y|∏
t=1

p(yt|y<t,x) (1)

where y<t = y1 · · · yt−1.
Let ht = [

−→
h t;
←−
h t] denote the hidden state vec-

tor in the encoder at time step t, which is the
concatenation of forward hidden vector

−→
h t and

backward hidden vector
←−
h t. Specifically,

−→
h t =

fLSTMe(xt,
−→
h t−1) and

←−
h t = fLSTMe(xt,

←−
h t+1),

where fLSTMe is an LSTM unit in encoder.
Decoder hidden state is calculated as st =

fLSTMd(yt−1, st−1), where fLSTMd is an LSTM unit
in decoder. We apply global attention mechanism
(Luong et al., 2015) to calculate the context vec-
tor:

ct =

|x|∑
i=1

αt,ihi

αt,i =
exp(Watt[st;hi])∑|x|
k=1 exp(Watt[sthk])

(2)

2We concatenate keyphrases following the original
keyphrase order in the corpora, and we set ♦ as “;” in our
implementation. The effect of keyphrase ordering will be
studied in the future work.

Algorithm 1 Keyphrase Ranking
Input: Generated top R keyphrase sequences S =

[y1, · · · ,yR] ranked with generation possibility from high
to low with beam search

Output: Ranked keyphrase set A with importance from
high to low
function KEY-RANK(S)
A← list() . set A as a empty list.
Q ← dict() . to skip keyword that is already in A.
for yi ∈ S do

yi ← yi.split(“♦”); . split yi by “♦”
for a ∈ yi do

. if a has not been merged in A, then keep it.
if notQ.has key(a) then
A.append(a)
Q.update(dict({a : “”}))

where αt,i is the attention weight; Watt contains
learnable parameters. In this paper, we omit the
bias variables to save space.

The probability to predict yt in the decoder at
time step t is factorized as:

pvocab(yt|y<t, ct) = fsoftmax(Wd1 ·
tanh(Wd2 [st; ct]))

(3)

where fsoftmax is the softmax function and Wd1
and Wd2 are learnable parameters.

Pointer-generator Network. Similar to Meng
et al. (2017), we utilize copying mechanism via
pointer-generator network (See et al., 2017) to al-
low the decoder to directly copy words from in-
put document, thus mitigating out-of-vocabulary
(OOV) problem. At time step t, the generation
probability pgen is calculated as:

pgen = fsigmoid(Wcct +Wsst +Wyyt) (4)

where fsigmoid is a sigmoid function; Wc, Ws and
Wy are learnable parameters. pgen plays a role of
switcher to choose to generate a word from a fixed
vocabulary with probability pvocab or directly copy
a word from the source document with the atten-
tion distribution αt. With a combination of a fixed
vocabulary and the extended source document vo-
cabulary, the probability to predict yt is:

p(yt) = pgenpvocab(yt|y<t, ct)+(1−pgen)
∑

i:yi=yt

αt,i

(5)
where if yt does not appear in the fixed vocabulary,
then the first term will be zero; and if yt is outside
source document, the second term will be zero.



4145

Encoder

Decoder

Encoder

Decoder Decoder1
Decoder2

X,   X(1)

Y,   Y(1)

X

Y

X,   X(2)

Y
Y(2)

pre-train fine-tune
main task

auxiliary task

(a) Synthetic Keyphrase Constrction (b) Multi-task Learning

Encoder

Figure 3: Our two semi-supervised learning methods which are based on (a) synthetic keyphrase construction, and
(b) multi-task learning. (X, Y) represents labeled sample. X(1) and X(2) denotes unlabeled documents. Y(1) refers
to synthetic keyphrases of X(1) and Y(2) means the title of X(2). For method of synthetic keyphrase construction,
model will be pre-trained on the mixture of gold-standards and synthetic data, then fine-tuned on labeled data. For
multi-task learning, model parameters of main task and auxiliary task will be jointly updated. Encoder parameters
of the two tasks are shared.

Supervised Learning. With a labeled dataset
Dp = {x(i),y(i)}Ni=1, the loss function of seq2seq
model is as follows:

L(θ) = −
N∑
i=1

log p(y(i)|x(i); θ) (6)

where θ contains all model parameters.

Keyphrase Inference and Ranking Strategy.
Beam search is utilized for decoding, and the top
R keyphrase sequences are leveraged for produc-
ing the final keyphrases. Here we use a beam size
of 50, and R as 50. We propose a ranking strat-
egy to collect the final set of keyphrases. Con-
cretely, in sequence we collect unique keyphrases
from the top ranked beams to lower ranked beams,
and keyphrases in the same sequence are ordered
as in the generation process. Intuitively, higher
ranked sequences are likely of better quality. As
for keyphrases in the same sequence, we find
that more salient keyphrases are usually generated
first. The ranking method is presented in Algo-
rithm 1.

4 Semi-Supervised Learning for
Keyphrase Generation

As illustrated in Figure 3, two methods are pro-
posed to leverage abundant unlabeled data. The
first is to provide synthetic keyphrases using un-
supervised keyphrase extraction methods or self-
learning algorithm, then mixed with labeled data

for model training, which is described in Sec-
tion 4.1. Furthermore, we introduce multi-task
learning that jointly generates keyphrases and
the title of the document (Section 4.2). We
denote the large-scale unlabeled documents as
Du = {x′(i)}Mi=1 and labeled data as Dp =
{x(i),y(i)}Ni=1, where M � N .

4.1 Synthetic Keyphrase Construction

The first proposed technique is to construct syn-
thetic labeled data by assigning keyphrases for
unlabeled documents, and then mix the synthetic
data with human labeled data for modeling train-
ing. Intuitively, adding training samples with syn-
thetic keyphrases has two potentially benefits: (1)
the encoder is exposed to more documents in train-
ing, and (2) the decoder also benefit from addi-
tional information from identifying contextual in-
formation for keyphrases. We propose and com-
pare two methods to extract synthetic keyphrases.

Unsupervised Learning Methods. Unsuper-
vised learning methods on keyphrase extraction
have been long studied in previous work (Mihal-
cea and Tarau, 2004; Wan and Xiao, 2008). Here
we select two effective and widely used meth-
ods to select keyphrases on unlabeled dataset Du,
which are TF-IDF and TextRank (Mihalcea and
Tarau, 2004). We combine the two methods into
a hybrid approach, in which we first adopt the
two methods to separately select top K keyphrases
from the document, we then take the union with
duplicate removal. To construct the keyphrase se-



4146

Algorithm 2 Training Procedures for Synthetic
Keyphrase Construction
Input: Dp, Ds, θ
Output: θ

function PRE-TRAIN(Dp, Ds, θ)
Dp+s ← Dp ∪ Ds
Shuffle Dp+s randomly
Update θ on Dp+s until converge

function FINE-TUNE(Dp, θ)
Set θ as the best parameters from PRE-TRAIN
Update θ on Dp until converge

quence, we concatenate the terms from TF-IDF
and then from TextRank, following the corre-
sponding ranking order. We set K as 5 in our ex-
periments.

Self-learning Algorithm. Inspired by prior
work (Zhang and Zong, 2016; Sennrich et al.,
2016), we adopt self-learning algorithm to boost
training data. Concretely, we first build a base-
line model by training the seq2seq model on the
labeled corpus Dp. Then the trained baseline
model is utilized to generate synthetic keyphrase
sequence y′ given a unlabeled document x′.
We adopt beam search to generate the synthetic
keyphrase sequences and beam size is set as 10.
The top one beam is selected.

Training Procedure. After the synthetic data
Ds = {x′(i),y

′
(i)}

M
i=1 is obtained by either of the

aforementioned methods, we mix labeled data Dp
with Ds to train the seq2seq model. As described
in Algorithm 2, we combineDp withDs intoDp+s
and shuffle Dp+s randomly, then we pre-train the
model on Dp+s, in which no network parameters
are frozen during the training process. The best
performing model is selected based on validation
set, then fine-tuned on Dp until converge.

4.2 Multi-task Learning with Auxiliary Task
The second approach to leverage unlabeled doc-
uments is to employ a multi-task learning frame-
work which combines the main task of keyphrase
generation with an auxiliary task through parame-
ter sharing strategy. Similar to the model structure
in Zhang and Zong (2016), our main task and the
auxiliary task share an encoder network but have
different decoders. Multi-task learning will bene-
fit from the source-side information to improve the
model generality of encoder.

In most domains such as scientific papers or
news articles, a document usually contains a title
that summarizes the core topics or ideas, with a

Dataset TRAIN VALID
Small-scale
Document-Keyphrase 40, 000 5, 000
Document-SyntheticKeyphrase 400, 000 N/A
Document-Title 400, 000 15, 000
Large-scale
Document-Keyphrase 130, 000 5, 000

Avg. #Tokens in Train LABELED SYN. MTL
Small-scale
Document 176.3 175.9 165.5
Keyphrase Sequence 23.3 23.5 N/A
Title N/A N/A 10.4

Table 1: Statistics of datasets used in our experiments.

similar spirit as keyphrases. We thus choose ti-
tle generation as auxiliary task, which has been
studied as a summarization problem (Rush et al.,
2015; Colmenares et al., 2015). Let D′u =
{x′(i),q(i)}Mi=1 denote the dataset which is as-
signed with titles for unlabeled data Du, the loss
function of multi-task learning is factorized as:

L(θe, θd1 , θd2) = −
N∑
i=1

log p(y(i)|x(i); θe, θd1)

−
M∑
i=1

log p(q(i)|x′(i); θe, θd2) (7)

where θe indicates encoder parameters; θd1 and θ
d
2

are the decoder parameters.

Training Procedure. We adopt a simple alter-
nating training strategy to switch training between
the main task and the auxiliary task. Specifically,
we first estimate parameters on auxiliary task with
D′u for one epoch, then train model on the main
task with Dp (labeled dataset) for T epochs. We
follow this training procedure for several times un-
til the model of our main task converges. We set
T as 3.

5 Experiments

5.1 Datasets
Our major experiments are conducted on scientific
articles which have been studied in previous work
(Hulth, 2003; Nguyen and Kan, 2007; Meng et al.,
2017). We use the dataset from Meng et al. (2017)
which is collected from various online digital li-
braries, e.g. ScienceDirect, ACM Digital Library,
Wiley, and other portals.

As indicated in Table 1, we construct a rel-
atively small-scale labeled dataset with 40K
document-keyphrase3 pairs, and a large-scale

3Here keyphrase refers to the keyphrase sequence.



4147

Model INSPEC KRAPIVIN NUS SEMEVAL KP20KF1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10
Comparisons
TF-IDF 0.223 0.304† 0.113 0.143 0.139 0.181 0.120 0.184† 0.105 0.130
TEXTRANK 0.229 0.275 0.173 0.147 0.195 0.190 0.172 0.181 0.181 0.150
SINGLERANK 0.214 0.297 0.096 0.137 0.145 0.169 0.132 0.169 0.099 0.124
EXPANDRANK 0.211 0.295 0.096 0.136 0.137 0.162 0.135 0.163 N/A N/A
MAUI 0.041 0.033 0.243 0.208† 0.249 0.261† 0.045 0.039 0.265 0.227†

KEA 0.109 0.129 0.120 0.131 0.068 0.081 0.027 0.027 0.180 0.163

SEQ2SEQ-COPY 0.269 0.234 0.274 0.207 0.345 0.282 0.278 0.226 0.291 0.215
Semi-supervised
SYN.UNSUPER. 0.326∗ 0.334∗ 0.283 0.239∗ 0.356 0.317∗ 0.306 0.294∗ 0.300∗ 0.245∗
SYN.SELF-LEARN. 0.310∗ 0.301∗ 0.289 0.236∗ 0.339 0.305 0.295 0.282∗ 0.301∗ 0.240∗
MULTI-TASK 0.326∗ 0.309∗ 0.296 0.240∗ 0.354 0.320∗ 0.322 0.289∗ 0.308∗ 0.243∗

Table 2: Results of present keyphrase generation with metrics F1@5 and F1@10. ∗ marks numbers that are
significantly better than SEQ2SEQ-COPY (p < 0.01, F -test). Due to the high time perplexity, no result is reported
by ExpandRank on KP20K, as done in Meng et al. (2017).

dataset of 400K unlabeled documents. Each doc-
ument contains an abstract and a title of the pa-
per. In multi-task learning setting, the auxiliary
task is to generate the title from the abstract.
For the validation set, we collect 5K document-
keyphrase pairs for the process of pre-training and
fine-tuning for methods based on synthetic data
construction. For multi-task learning, we use the
same 5K document-keyphrase pairs for the main
task training, another 15K document-title pairs for
the auxiliary task. We further conduct experiments
on a 130K large-scale labeled dataset, which in-
cludes the small-scale labeled data.

Similar to Meng et al. (2017), we test our
model on five widely used public datasets from the
scientific domain: INSPEC (Hulth, 2003), NUS
(Nguyen and Kan, 2007), KRAPIVIN (Krapivin
et al., 2009), SEMEVAL-2010 (Kim et al., 2010)
and KP20K (Meng et al., 2017).

5.2 Experimental Settings

Data preprocessing is implemented as in (Meng
et al., 2017). The texts are first tokenized by NLTK
(Bird and Loper, 2004) and lowercased, then the
numbers are replaced with<digit>. We set maxi-
mal length of source text as 200, 40 for target text.
Encoder and decoder both have a vocabulary size
of 50K. The word embedding size is set to 128.
Embeddings are randomly initialized and learned
during training. The size of hidden vector is 512.
Dropout rate is set as 0.3. Maximal gradient nor-
malization is 2. Adagrad (Duchi et al., 2011) is
adopted to train the model with learning rate of
0.15 and the initial accumulator rate is 0.1.

For synthetic data construction, we first use
batch size of 64 for model pre-training and then re-

duce to 32 for model fine-tuning. For both training
stages, after 8 epochs, learning rate be decreased
with a rate of 0.5. For multi-task learning, batch
size is set to 32 and learning rate is reduced to half
after 20 training epochs. To build baseline seq2seq
model, we set batch size as 32 and decrease learn-
ing rate after 8 epochs. For self-learning algo-
rithm, beam size is set to 10 to generate target se-
quences for unlabeled data and the top one is re-
tained.

5.3 Comparisons with Baselines

Evaluation Metrics. Following (Liu et al.,
2011; Meng et al., 2017), we adopt top-N macro-
averaged precision, recall and F-measure (F1) for
model evaluation. Precision means how many top-
N extracted keywords are correct and recall means
how many target keyphrases are extracted in top-
N candidates. Porter Stemmer is applied before
comparisons.

Baselines. We train a baseline seq2seq model on
the small-scale labeled dataset. We further com-
pare with four unsupervised learning methods:
TF-IDF, TextRank (Mihalcea and Tarau, 2004),
SingleRank (Wan and Xiao, 2008), ExpandRank
(Wan and Xiao, 2008), and two supervised learn-
ing methods of Maui (Medelyan et al., 2009) and
KEA (Witten et al., 1999). We follow Meng et al.
(2017) for baselines setups.

Results. Here we show results for present and
absent keyphrase generation separately4. From
the results of present keyphrase generation as
shown in Table 2, although trained on small-scale

4Recall that present means the keyphrase appears in the
document, otherwise, it is absent.



4148

Semi-supervised
Dataset SEQ. SYN.UN. SYN.SELF. MULTI.
INSPEC 0.012 0.018 0.013 0.022
KRAPIVIN 0.01 0.008 0.018 0.021
NUS 0.002 0.011 0.003 0.013
SEMEVAL 0.001 0.008 0.003 0.006
KP20K 0.01 0.016 0.013 0.021

Table 3: Results of absent kephrase generation based
on Recall@10.

labeled corpora, our baseline seq2seq model with
copying mechanism still achieves better F1@5
scores, compared to other baselines. This demon-
strates that a baseline seq2seq model has learned
the mapping patterns from source text to target
keyphrases to some degree. However, small-scale
labeled data still hinders the potential of seq2seq
model according to the poor performance of
F1@10. By leveraging large-scale unlabeled data,
our semi-supervised learning methods achieve sig-
inifcant improvement over seq2seq baseline, as
well as exhibit the best performance in both F1@5
and F1@10 on almost all datasets.

We further compare seq2seq based models on
the task of generating keyphrases beyond input
article vocabulary. Illustrated by Table 3, semi-
supervised learning significantly improves the ab-
sent generation performance, compared to the
baseline seq2seq. Among our models, the multi-
task learning method is more effective at gen-
erating absent keyphrases than the two methods
by leveraging synthetic data. The main reason
may lie in that synthetic keyphrases potentially
introduce noisy annotations, while the decoder
in multi-task learning setting focuses on learn-
ing from gold-standard keyphrases. We can also
see that the overall performances by all models
are low, due to the intrinsic difficulty of absent
keyphrase generation (Meng et al., 2017). More-
over, we only employ 40K labeled data for train-
ing, which is rather limited for training. Besides,
we believe better evaluation methods should be
used instead of exact match, e.g., by considering
paraphrases. This will be studied in the future
work.

5.4 Effect of Synthetic Keyphrase Quality

In this section, we conduct experiments to further
study the effect of synthetic keyphrase quality on
model performance. Two sets of experiments are
undertaken, one for evaluating unsupervised learn-
ing and one for self-learning algorithm.

For self-learning algorithm, we further generate

0 2 4 6 8 10 12 14 16
epoch

35

40

45

50

55

60

pp
l. 

on
 v

al
id

at
io

n 
se

t Beam-3
Syn.Self-Learn(Beam-10)
Trained-Model(Beam-10)

0 2 4 6 8 10 12 14 16
epoch

30

40

50

60

70

80
Syn.Unsuper.(top@1)
Syn.Unsuper.(top@5)
Syn.Unsuper.(top@10)

Figure 4: Pre-training curves with perplexity on vali-
dation set with various options for synthetic keyphrases
construction. Left is for options for self-learning algo-
rithm and right is for unsupervised learning methods.

synthetic keyphrases using following options:

• Beam-size-3: Based on our baseline model
trained with labeled data, we use beam search
with a smaller beam size of 3 to generate syn-
thetic data5.

• Trained-model: We adopt the model which
has been trained with self-learning algorithm
on 40K labeled data and 400K unlabeled
data, to generate the top one keyphrase se-
quence with beam size of 10.

For unsupervised learning method, we origi-
nally merge top-K (K = 5) keyphrases from TF-
IDF and TextRank, here we use options where K
is set as 1 or 10 to extract keyphrases:

• Top@1: Using TF-IDF or TextRank, we only
keep top 1 extraction from each, then take the
union of the two.

• Top@10: Similarly, we keep top 10 extracted
terms from TF-IDF or TextRank, then take
the union.

As illustrated in Figure 4, when models are pre-
trained with synthetic keyphrases of better quality,
results by “Trained-model” consistently produce
better performance (i.e., lower perplexity). Sim-
ilar phenomenon can be observed when “top@5”
and “top@10” are applied for extraction in un-
supervised learning setting. Furthermore, after
models are pre-trained and then fine-tuned, the re-
sults in Figure 5 show that the difference among
baselines becomes insignificant—the quality of
synthetic keyphrases have limited effect on final
scores. The reason might be that though synthetic

5We also experiment with greedy search (i.e. beam size
of 1), however, unknown words are frequently generated.



4149

Inspec Krapivin NUS SemEval KP20k

25

30

35
F1

@
N(

%
)

Beam-3
Syn.Self-Learn.(Beam-10)
Trained-Model.(Beam-10)

Inspec Krapivin NUS SemEval KP20k

25

30

35

F1
@

N(
%

)

Syn.Unsuper.(top@1)
Syn.Unsuper.(top@5)
Syn.Unsuper.(top@10)

Figure 5: Effect of synthetic data quality on present
keyphrase generation (models are pre-trained and fine-
tuned) based on F1@5 (left three columns) and F1@10
(right three columns), on five datasets. The upper is for
self-learning algorithm and the bottom is for unsuper-
vised learning method.

20

22

24

26

28

30

32

34

F
1@

10

17

18

19

20

21

22

23

24

24

26

28

30

32

18

20

22

24

26

28

30

18

19

20

21

22

23

24

25

1/8 1/4 1/2 3/4 1

Inspec

20

22

24

26

28

30

F
1@

10

1/8 1/4 1/2 3/4 1

Krapivin

17

18

19

20

21

22

23

24

1/8 1/4 1/2 3/4 1

NUS

22

24

26

28

30

1/8 1/4 1/2 3/4 1

SemEval

18

20

22

24

26

28

1/8 1/4 1/2 3/4 1

KP20k

18

19

20

21

22

23

24

top@10 top@50

Figure 6: Effect of various amounts of unlabeled
data for training on present keyphrase generation with
F1@10. Upper is for synthetic data construction
method with unsupervised learning. Bottom is for
multi-task learning algorithm.

keyphrases potentially introduce noisy informa-
tion for decoder training, the encoder is still well
trained. In addition, after fine-tuning on labeled
data, the decoder acquires additional knowledge,
thus leading to better performance and minimal
difference among the options.

5.5 Effect of Amount of Unlabeled Data

In this section, we further evaluate whether vary-
ing the amount of unlabeled data will affect model
performance. We conduct experiments based on
methods of synthetic data construction with un-
supervised learning and multi-task learning. We
further carry out experiments with randomly se-
lected 50K(1/8), 100K(1/4), 200K(1/2) and
300K(3/4) unlabeled documents from the pool of
400K unlabeled data. After models being trained,
we adopt beam search to generate keyphrase se-
quences with beam size of 50. We keep top N
keyphrase sequences to yield the final keyphrases
using Algorithm 1. F1@10 is adopted to illustrate

0 2 4 6 8 10 12 14
epoch

21

22

23

24

25

26

27

28

29

pp
l. 

on
 v

al
id

at
io

n 
se

t

Syn.Unsuper(50K)
Syn.Unsuper(100K)
Syn.Unsuper(200K)
Syn.Unsuper(300K)
Syn.Unsuper(400K)

5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0
epoch

20

22

24

26

28

30
Multi-task(50K)
Multi-task(100K)
Multi-task(200K)
Multi-task(300K)
Multi-task(400K)

Figure 7: Perplexity on validation set with varying
amounts of unlabeled data for training. Left is for
fine-tuning procedure based on models trained with
synthetic data constructed with unsupervised learning.
Right is for multi-task learning procedure with perfor-
mance on the main task.

Model F1 Model F1
Our Models Unsupervised
SEQ2SEQ 0.056 TF-IDF 0.270
SYN.UNSUPER. 0.083 TEXTRANK 0.097
SYN.SELF-LEARN. 0.065 SINGLERANK 0.256
MULTI-TASK 0.109 EXPANDRANK 0.269

Table 4: Results of keyphrase generation for news from
DUC dataset with F1. Results of unsupervised learning
methods are adopted from Hasan and Ng (2010).

the model performances. N is set as 10 or 50.
The present keyphrase generation results are

shown in Figure 6, from which we can see that
when increasing the amount of unlabeled data,
model performance is further improved. This
is because additional unlabeled data can provide
with more evidence on linguistic or context fea-
tures and thus make the model, especially the en-
coder, have better generalizability. This finding
echoes with the training procedure illustrated in
Figure 7, where more unlabeled data uniformly
leads to better performance. Therefore, we believe
that leveraging more unlabeled data for model
training can boost model performance.

5.6 A Pilot Study for Cross-Domain Test

Up to now, we have demonstrated the effectiveness
of leveraging unlabeled data for in-domain exper-
iments, but is it still effective when being tested
on a different domain? We thus carry out a pi-
lot cross-domain test on news articles. The widely
used DUC dataset (Wan and Xiao, 2008) is uti-
lized, consisting of 308 articles with 2, 048 labeled
keyphrases.

The experimental results are shown in Table 4
which indicate that: 1) though trained on scientific
papers, our models still have the ability to gener-



4150

Model INSPEC KRAPIVIN NUS SEMEVAL KP20KF1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10
SEQ2SEQ-COPY 0.34 0.329 0.308 0.251 0.36 0.327 0.301 0.285 0.318 0.251
Semi-supervised
SYN.UNSUPER. 0.338 0.34 0.316 0.255 0.365 0.335 0.337 0.308 0.322 0.261∗
SYN.SELF-LEARN. 0.33 0.326 0.304 0.255 0.359 0.336 0.304 0.304 0.321 0.263∗
MULTI-TASK 0.328 0.318 0.323 0.254 0.365 0.326 0.319 0.312 0.328∗ 0.264∗

Table 5: Results of present keyphrase generation on large-scale labeled data with F1@5 and F1@10. ∗ indicates
significant better performance than SEQ2SEQ-COPY with p < 0.01 (F -test).

ate keyphrases for news articles, illustrating that
our models have learned some universal features
between the two domains; and 2) semi-supervised
learning by leveraging unlabeled data improves
the generation performances more, indicating that
our proposed method is reasonably effective when
being tested on cross-domain data. Though un-
supervised methods are still superior, for future
work, we can leverage unlabeled out-of-domain
corpora to improve cross-domain keyphrase gen-
eration performance, which could be a promising
direction for domain adaption or transfer learning.

Semi-supervised
Dataset SEQ. SYN.UN. SYN.SELF. MULTI.
INSPEC 0.021 0.024 0.032 0.033
KRAPIVIN 0.02 0.031 0.043 0.047
NUS 0.009 0.026 0.024 0.036
SEMEVAL 0.011 0.014 0.015 0.02
KP20K 0.021 0.034 0.039 0.046

Table 6: Performance on absent keyphrase generation
by Recall@10 with large-scale labeled training data.

5.7 Training on Large-scale Labeled Data

Finally, it would be interesting to study whether
unlabeled data can still improve performance
when the model is trained on a larger scaled la-
beled data. We conduct experiments on a larger
labeled dataset with 130K pairs, along with the
400K unlabeled data. Here the baseline seq2seq
model is built on the 130K dataset.

From the present keyphrase generation results
in Table 5, it can be seen that unlabeled data is
still helpful for model training on a large-scale la-
beled dataset. This implies that we can also lever-
age unlabeled data to enhance generation perfor-
mance even in a resource-rich setting. Referring
to the absent keyphrase generation results shown
in Table 6, semi-supervised learning also boosts
the scores. From Table 6, training on large-scale
labeled data, absent generation is significantly im-
proved, compared to being trained on a small-scale
labeled data (see Table 3).

6 Conclusion and Future Work

In this paper, we presented a semi-supervised
learning framework that leverages unlabeled data
for keyphrase generation built upon seq2seq mod-
els. We introduced synthetic keyphrases con-
struction algorithm and multi-task learning to ef-
fectively leverage abundant unlabeled documents.
Extensive experiments demonstrated the effective-
ness of our methods, even in scenario where large-
scale labeled data is available.

For future work, we will 1) leverage unlabeled
data to study domain adaptation or transfer learn-
ing for keyphrase generation; and 2) investigate
novel models to improve absent keyphrase gener-
ation when limited labeled data is available based
on semi-supervised learning.

Acknowledgements

This research is based upon work supported in part
by National Science Foundation through Grants
IIS-1566382 and IIS-1813341, and by the Office
of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), via contract # FA8650-17-C-9116. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies, ei-
ther expressed or implied, of ODNI, IARPA, or
the U.S. Government. The U.S. Government is
authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copy-
right annotation therein. We thank three anony-
mous reviewers for their insightful suggestions on
various aspects of this work.

References

Gábor Berend. 2011. Opinion expression mining
by exploiting keyphrase extraction. In Fifth In-
ternational Joint Conference on Natural Language
Processing, IJCNLP 2011, Chiang Mai, Thailand,
November 8-13, 2011, pages 1162–1170.



4151

Steven Bird and Edward Loper. 2004. NLTK: the natu-
ral language toolkit. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, July 21-26, 2004 -
Poster and Demonstration.

Adrien Bougouin, Florian Boudin, and Béatrice Daille.
2013. Topicrank: Graph-based topic ranking for
keyphrase extraction. In Sixth International Joint
Conference on Natural Language Processing, IJC-
NLP 2013, Nagoya, Japan, October 14-18, 2013,
pages 543–551.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

Carlos A. Colmenares, Marina Litvak, Amin Mantrach,
and Fabrizio Silvestri. 2015. HEADS: headline
generation as sequence prediction using an abstract
feature-rich space. In NAACL HLT 2015, The 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Denver, Colorado, USA,
May 31 - June 5, 2015, pages 133–142.

Andrew M. Dai and Quoc V. Le. 2015. Semi-
supervised sequence learning. In Advances in Neu-
ral Information Processing Systems 28: Annual
Conference on Neural Information Processing Sys-
tems 2015, December 7-12, 2015, Montreal, Que-
bec, Canada, pages 3079–3087.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers.

John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

Eibe Frank, Gordon W. Paynter, Ian H. Witten,
Carl Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the Sixteenth International Joint Conference
on Artificial Intelligence, IJCAI 99, Stockholm, Swe-
den, July 31 - August 6, 1999. 2 Volumes, 1450
pages, pages 668–673.

Çaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loı̈c Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. CoRR, abs/1503.03535.

Khaled M Hammouda, Diego N Matute, and Mo-
hamed S Kamel. 2005. Corephrase: Keyphrase ex-
traction for document clustering. In International

Workshop on Machine Learning and Data Mining
in Pattern Recognition, pages 265–274. Springer.

Kazi Saidul Hasan and Vincent Ng. 2010. Conundrums
in unsupervised keyphrase extraction: Making sense
of the state-of-the-art. In COLING 2010, 23rd Inter-
national Conference on Computational Linguistics,
Posters Volume, 23-27 August 2010, Beijing, China,
pages 365–373.

Kazi Saidul Hasan and Vincent Ng. 2014. Automatic
keyphrase extraction: A survey of the state of the
art. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, ACL
2014, June 22-27, 2014, Baltimore, MD, USA, Vol-
ume 1: Long Papers, pages 1262–1273.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP 2003, Sap-
poro, Japan, July 11-12, 2003.

Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in sci-
entific articles. In Proceedings of the Work-
shop on Multiword Expressions: Identification,
Interpretation, Disambiguation and Applications,
MWE@IJCNLP 2009, Singapore, August 6, 2009,
pages 9–16.

Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 task 5 : Au-
tomatic keyphrase extraction from scientific articles.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, SemEval@ACL 2010, Upp-
sala University, Uppsala, Sweden, July 15-16, 2010,
pages 21–26.

Mikalai Krapivin, Aliaksandr Autaeu, and Maurizio
Marchese. 2009. Large dataset for keyphrases ex-
traction. Technical report, University of Trento.

Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of human language technologies: The 2009
annual conference of the North American chapter of
the association for computational linguistics, pages
620–628. Association for Computational Linguis-
tics.

Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009b. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Hu-
man Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, Proceedings, May 31 - June
5, 2009, Boulder, Colorado, USA, pages 620–628.



4152

Liyuan Liu, Jingbo Shang, Xiang Ren,
Frank Fangzheng Xu, Huan Gui, Jian Peng,
and Jiawei Han. 2018. Empower sequence labeling
with task-aware neural language model. In Pro-
ceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence, New Orleans, Louisiana,
USA, February 2-7, 2018.

Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning, CoNLL 2011, Portland, Ore-
gon, USA, June 23-24, 2011, pages 135–144.

Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2010, 9-11 Oc-
tober 2010, MIT Stata Center, Massachusetts, USA,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 366–376.

Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009c. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2009, 6-7 August 2009,
Singapore, A meeting of SIGDAT, a Special Interest
Group of the ACL, pages 257–266.

Patrice Lopez and Laurent Romary. 2010. HUMB: au-
tomatic key term extraction from scientific articles in
GROBID. In Proceedings of the 5th International
Workshop on Semantic Evaluation, SemEval@ACL
2010, Uppsala University, Uppsala, Sweden, July
15-16, 2010, pages 248–251.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015, pages 1412–1421.

Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2009, 6-7 August 2009,
Singapore, A meeting of SIGDAT, a Special Interest
Group of the ACL, pages 1318–1327.

Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing
He, Peter Brusilovsky, and Yu Chi. 2017. Deep
keyphrase generation. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers, pages
582–592.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into text. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language

Processing , EMNLP 2004, A meeting of SIGDAT, a
Special Interest Group of the ACL, held in conjunc-
tion with ACL 2004, 25-26 July 2004, Barcelona,
Spain, pages 404–411.

Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
Asian Digital Libraries. Looking Back 10 Years
and Forging New Frontiers, 10th International Con-
ference on Asian Digital Libraries, ICADL 2007,
Hanoi, Vietnam, December 10-13, 2007, Proceed-
ings, pages 317–326.

Prajit Ramachandran, Peter J. Liu, and Quoc V. Le.
2017. Unsupervised pretraining for sequence to se-
quence learning. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark,
September 9-11, 2017, pages 383–391.

Marek Rei. 2017. Semi-supervised multitask learn-
ing for sequence labeling. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers, pages
2121–2130.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015, pages 379–389.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30
- August 4, Volume 1: Long Papers, pages 1073–
1083.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada, pages 3104–
3112.

Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Inf. Retr., 2(4):303–336.

Xiaojun Wan and Jianguo Xiao. 2008. Single doc-
ument keyphrase extraction using neighborhood
knowledge. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, AAAI
2008, Chicago, Illinois, USA, July 13-17, 2008,
pages 855–860.



4153

Lu Wang and Claire Cardie. 2013. Domain-
Independent Abstract Generation for Focused Meet-
ing Summarization. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1395–
1405, Sofia, Bulgaria. Association for Computa-
tional Linguistics.

Lu Wang and Wang Ling. 2016. Neural network-
based abstract generation for opinions and argu-
ments. In NAACL HLT 2016, The 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, San Diego California, USA, June 12-
17, 2016, pages 47–57.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347–354. Association for Computational Linguis-
tics.

Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
practical automatic keyphrase extraction. In Pro-
ceedings of the Fourth ACM conference on Digital
Libraries, August 11-14, 1999, Berkeley, CA, USA,
pages 254–255.

Hai Ye, Xin Jiang, Zhunchen Luo, and Wenhan Chao.
2018. Interpretable charge predictions for criminal
cases: Learning to generate court views from fact
descriptions. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2018, New Or-
leans, Louisiana, USA, June 1-6, 2018, Volume 1
(Long Papers), pages 1854–1864.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2016, Austin, Texas, USA, Novem-
ber 1-4, 2016, pages 1535–1545.

Yongzheng Zhang, A. Nur Zincir-Heywood, and Evan-
gelos E. Milios. 2004. World wide web site sum-
marization. Web Intelligence and Agent Systems,
2(1):39–53.


