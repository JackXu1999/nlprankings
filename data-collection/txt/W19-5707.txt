

















































Efficient Learning of Output Tier-Based Strictly 2-Local Functions

Phillip Burness
University of Ottawa

pburn036@uottawa.ca

Kevin McMullin
University of Ottawa

kevin.mcmullin@uottawa.ca

Abstract

This paper characterizes the Output Tier-based
Strictly k-Local (OTSLk) class of string-to-
string functions, which are relevant for mod-
eling long-distance phonological processes as
input-output maps. After showing that any
OTSLk function can be learned when k and the
tier are given, we present a new algorithm that
induces the tier itself when k = 2 and provably
learns any total OTSL2 function in polynomial
time and data—the first such learner for any
class of tier-based functions.

1 Introduction

In this paper, we investigate the class of Output
Tier-based Strictly k-Local (OTSLk) functions. In
terms of finite state transducers, OTSLk functions
are those for which the output symbol(s) to be
written at each timestep depends on the k−1 most
recent symbols on the output tape that belong to
the relevant ‘tier’ (a subset of the output alphabet;
Heinz et al., 2011), without regard for any non-
tier symbols that might have been written between
them or after them. We show that they are learn-
able when the contents of the tier are provided as
input to the learner, and introduce an algorithm
that provably and efficiently learns any total OTSL
function when k = 2.

Recent research investigating the computational
properties of phonological patterns observed in
natural language has shown that many attested
processes can be characterized as Strictly Local
(SL) functions (Chandlee, 2014; Chandlee et al.,
2014, 2015). That is, the output at any given
timestep is dependent on the previous k − 1 sym-
bols from either the input string (Input Strictly k-
Local; ISLk) or the output string (Output Strictly
k-Local; OSLk). Multiple characterizations of
these classes exist and their properties are well-
understood. One important distinction between

the two is that non-iterative processes are ISL,
whereas processes that apply iteratively to multi-
ple targets are OSL. Moreover, efficient learning
algorithms exist for both the ISL and OSL func-
tions. The OSL Function Inference Algorithm
(OSLFIA; Chandlee et al., 2015) is of particular
importance to this paper, as we will show that
many of their theoretical results can be general-
ized to OTSL functions in a natural way.

Long-distance phonological processes, for
which a potentially unbounded number of seg-
ments may intervene between the trigger and tar-
get without being affected in any way, are nei-
ther ISL nor OSL for any value of k. For exam-
ple, Samala has a long-distance process of sibi-
lant harmony in which an underlying /s/ surfaces
as [S] if another [S] appears anywhere later in the
word. This is seen when, e.g., the perfective suffix
/-waS/ is added to a root containing /s/, as in /ha-
s-xintila-waS/→[haSxintilawaS] ‘his former gentile
name’ (Applegate, 1972). This process can be un-
derstood as applying iteratively to multiple targets,
as in /s-lu-sisin-waS/→[SluSiSinwaS] ‘It is all grown
awry’. Indeed, it seems that the vast majority of
attested long-distance processes are enforced iter-
atively (Kaplan, 2008; Hansson, 2010). As such,
we focus this paper on OTSLk functions in partic-
ular, which generalize the OSLk class in a way that
allows us to model these kinds of long-distance
processes. We note that ITSLk functions can be
characterized in a similar way and that the learning
strategy outlined below could likely be extended to
total ITSL2 functions.

While the notion of a tier has long been incor-
porated into phonological theory (e.g., Clements,
1980; Goldsmith, 1990; Odden, 1994; Heinz et al.,
2011; McMullin, 2016), the range of possible tiers
is typically assumed to be available to the learner
a priori. Each possible tier could, for example,
be defined in terms of feature specifications or



natural classes of segments (e.g., Hayes and Wil-
son, 2008). Though algorithms have been de-
veloped for inducing a relevant tier from a sam-
ple of positive training, their success is limited
to phonotactic co-occurrence restrictions. This
is true both for constraint-based maximum en-
tropy learners (Gouskova and Gallagher, 2019) as
well as for algorithms that learn grammars for
Tier-based Strictly Local formal languages (Jar-
dine and Heinz, 2016; Jardine and McMullin,
2017). To our knowledge, the algorithm pre-
sented below, which we call the Output Tier-based
Strictly 2-Local Function Inference Algorithm
(OTSL2FIA), is the first algorithm which learns
the relevant tier for transformations of underlying
representations (strings of input segments) to sur-
face forms (strings of output segments).

The remainder of this paper is organized as fol-
lows. Notation and relevant concepts are pre-
sented in Section 2. In Section 3, we define the
OTSL functions and characterize them in terms
of finite state transducers. In Section 4, we high-
light several important properties of OTSL2 func-
tions in particular that can be taken advantage of
during learning. All aspects of the learning algo-
rithm, along with the theoretical learning results,
are described in Section 5. Section 6 discusses
how OTSL2 functions can model various phono-
logical processes and identifies several avenues for
future research. Section 7 concludes.

2 Preliminaries

2.1 Strings and sets

Given a set S, we write card(S) to denote its car-
dinality. For a string w made of symbols from
some alphabet Σ, |w| denotes the length of the
string. We write Σ∗ to denote all possible strings
made from the alphabet Σ, while Σn denotes all
possible strings made from that alphabet with a
length of n, and Σ≤n denotes all such strings with
a length up to n. The unique string of length 0 (the
empty string) is written as λ. Given two strings u
and v, we write u ·v to denote their concatenation,
but often shorten this to uv when context permits.
We write fack(w) to denote all the contiguous
substrings of length k (the k-factors) contained in
a string w.

We assume a fixed but arbitrary total order ≺
over the letters of Σ, an order which we ex-
tend to all strings in Σ∗ by defining the length-
lexicographical order (Oncina et al., 1993; Chan-

dlee et al., 2015) as follows. String w1 oc-
curs length-lexicographically before w2 (written
as w1 / w2) when |w1| < |w2| or, if |w1| =
|w2|, when ai ≺ bi where ai is the ith letter in
w1, bi is the ith letter in w2, and i is the first
position on which w1 and w2 differ. For exam-
ple, given Σ = {a, b} where a ≺ b, we have
λ / a / b / aa / ab / ba / bb / aaa and so on.

A prefix of some string w is any string u such
that w = ux and x ∈ Σ∗. Similarly, a suffix of
some stringw is any string u such thatw = xu and
x ∈ Σ∗. Note that any string is a prefix and suffix
of itself, and that λ is a prefix and suffix of every
string. When |w| ≥ n, Prefn(w) and Suffn(w)
denote the unique prefix and suffix of w with a
length of n; when |w| < n, they simply denote w
itself. We write Pref∗(w) to denote the set of all
prefixes of w. Also, Suffn(Suffn(w1)w2)) =
Suffn(w1w2). Given a string w, one of its pre-
fixes p, and one of its prefixes s, we write p−1 · w
to represent the string w without that prefix p and
write w · s−1 to represent the string w without
that suffix s. For example, a−1 · aba = ba and
aba · a−1 = ab. Finally, given a set of strings S,
we write lcp(S) to denote the longest common
prefix of S, which is the string u such that u is a
prefix of every w ∈ S, and there exists no other
string v such that |v| > |u| and v is also a prefix of
every w ∈ S.

2.2 Functions and transducers

This paper deals exclusively with string-to-string
functions, relations that pair every w ∈ Σ∗ with at
most one y ∈ ∆∗, where Σ and ∆ are the input al-
phabet and output alphabet respectively. The input
language and output language of such a function
are pre image(f) = {x | (∃y)[x 7→f y]} and
image(f) = {y | (∃x)[x 7→f y]}, respectively.
An important concept is that of the tails of an in-
put string w with respect to a function f .

Definition 1. (Tails) Given a function f and an
input w ∈ Σ∗, tailsf (w) = {(y, v) | f(wy) =
uv ∧ u = lcp(f(wΣ∗))}.

In words, tailsf (w) pairs every possible
string y ∈ Σ∗ with the portion of f(wy) that is
directly attributable to y. That is, it describes the
effect that w has on the output of any subsequent
string of input symbols. When tailsf (w1) =
tailsf (w2) we say that w1 and w2 are tail-
equivalent with respect to f .

A related concept to tails and tail-equivalency



is the contribution of a symbol a ∈ Σ relative to a
string w ∈ Σ∗ with respect to a function f .
Definition 2. (Contribution) Given a function f ,
some a ∈ Σ, and some w ∈ Σ∗, contf (a,w) =
lcp(f(wΣ∗))−1 · lcp(f(waΣ∗)).
In words, for an input string x that has the prefix
wa, the contribution of the a in wa is the portion
of f(x) that is uniquely and directly attributable to
that instance of a.

The Output Tier-based Strictly Local functions
that will be introduced below are a proper subclass
of the subsequential functions. Oncina and Garcı́a
(1991) show that when a function is subsequen-
tial, tail-equivalency will partition Σ∗ into finitely
many blocks, allowing us to construct a finite-
state transducer that computes f . In this paper we
use delimited subsequential finite state transducers
(DSFSTs; see Jardine et al., 2014), to character-
ize the class of Output Strictly Local (OSL) func-
tions. The following definition is drawn directly
from Chandlee et al. (2015).

Definition 3. A delimited subsequential fi-
nite state trasnducer (DSFST) is a 6-tuple
〈Q, q0, qf ,Σ,∆, δ〉whereQ is a finite set of states,
q0 ∈ Q is the unique initial state, qf ∈ Q is
the unique final state, Σ is the finite input al-
phabet, ∆ is the finite output alphabet, and δ ⊆
Q× (Σ∪{o,n})×∆∗×Q is the transition func-
tion (where o /∈ Σ indicates the start of the input
and n /∈ Σ indicates the end of the input), and the
following hold:

1. if (q, a, u, q′) ∈ δ then q 6= qf and q′ 6= q0

2. if (q, a, u, qf ) ∈ δ then a = n and q 6= q0

3. if (q0, a, u, q′) ∈ δ then a = o and if
(q,o, u, q′) ∈ δ then q = q0

4. if (q, a, u, q′), (q, a, u′, q′′) ∈ δ then q′ = q′′
and u = u′

Each transition (q, a, u, q′) ∈ δ can be seen
as an instruction to append u to the end of the
output tape and to move to state q′ upon read-
ing a while in state q. This transition func-
tion may be partial, and its recursive exten-
sion δ∗ is the smallest set containing δ closed
under the following conditions: (q, λ, λ, q) ∈
δ∗, and (q, w, u, q′), (q′, a, v, q′′) ∈ δ∗ ⇒
(q, wa, uv, q′′) ∈ δ∗. The initial state of a DS-
FST has no incoming transitions and has exactly

one outgoing transition, which will be for the in-
put o and does not land in the final state. Further-
more, the final state of a DSFST has no outgoing
transitions, and every transition into the final state
is for the input n. DSFSTs are also deterministic
on the input, such that each state has at most one
outgoing transition per input symbol.

The size of a DSFST T = 〈Q, q0, qf ,Σ,∆, δ〉
is |T | = card(Q) +card(δ) +

∑
(q,a,u,q′)∈δ |u|,

and the relation defined by a DSFST is R(T ) =
{(x, y) ∈ Σ∗ ×∆∗ | (q0,oxn, y, qf ) ∈ δ∗}.

The DSFSTs we will use below have a special
property known as onwardness, which informally
means that the writing of the output is never de-
layed. The following formal definition of onward-
ness and a related lemma are borrowed from Chan-
dlee et al. (2015).

Definition 4. (Onwardness) A DSFST is onward if
for every w ∈ Σ∗ and u ∈ ∆∗, (q0,ow, u, q) ∈
δ∗⇐⇒u = lcp(f(wΣ∗)).
Lemma 1. Let the outputs of the edges out
of state q be Outputs(q) = {u | (∃a ∈
Σ ∪ {o,n}) (∃q′ ∈ Q) [(q, a, u, q′) ∈ δ]}.
If T is an onward DSFST and recognizes f ,
then ∀q 6= q0, lcp(Outputs(q)) = λ and
lcp(Outputs(q0)) = lcp(f(Σ∗)).

Below we will frequently make reference to the
length-lexicographically earliest input string that
can lead to a state q in a given transducer T , which
we will denote as wq. A formal definition is pro-
vided here for reference.

Definition 5. (Earliest string) Given a transducer
T = 〈Q, q0, qf ,Σ,∆, δ〉, the earliest string that
leads to q ∈ Q is wq = min/{w ∈ Σ∗ |
∃u, (q0,ow, u, q) ∈ δ∗}.

A distinction that will be important throughout
the rest of this paper is that between the writing
that occurs in a DSFST as it is reading letters
from Σ, and the writing that occurs at the very
end (when the DSFST reads n). To make this dis-
tinction, Chandlee et al. (2015) defined the prefix
function fp associated with a subsequential func-
tion f as follows.

Definition 6. (Prefix function) Given a subse-
quential function f , its associated prefix function
fp is such that fp(w) = lcp(f(wΣ∗)).

Remark 1. Given a subsequential function f ,
some a ∈ Σ, and some input string w ∈
Σ∗, contf (a,w) = fp(w)−1 · fp(wa) and
contf (n, w) = fp(w)−1 · f(w).



2.3 Strict locality and tiers

Chandlee (2014) and Chandlee et al. (2014) orig-
inally introduced the Input Strictly Local (ISL)
and Output Strictly Local (OSL) functions, both
of which generalize Strictly Local (SL) stringsets
to functions based on one of the defining proper-
ties of SL languages, the Suffix Substitution Clo-
sure (Rogers and Pullum, 2011). The definitions
of the ISL and OSL functions exploit a corollary
of this defining property, which Chandlee et al.
(2015) call Suffix-defined Residuals. For reasons
of space, we only discuss the OSL functions be-
low.

Theorem 1. (Suffix Substitution Closure) A lan-
guage L is SL if for all strings u1, v1, u2, v2 there
exists a natural number k such that for any string
x of length k − 1, if u1xv1, u2xv2 ∈ L, then
u1xv2 ∈ L.

Corollary 1. (Suffix-defined Residuals) A lan-
guage L is SL if for all w1, w2 ∈ Σ∗, there ex-
ists a natural number k such that if Suffk−1(w1)
= Suffk−1(w2) then {v | w1v ∈ L} = {v |
w2v ∈ L}, that is w1 and w2 have the same resid-
uals (tails) with respect to L.

Definition 7. (Output Strictly Local functions)
A function f is OSLk if for all w1, w2 ∈
Σ∗, Suffk−1(fp(w1)) = Suffk−1(fp(w2)) ⇒
tailsf (w1) = tailsf (w2).

Chandlee (2014) and Chandlee et al. (2014,
2015) show that most iterative phonological pro-
cesses can be modelled with an OSL function,
with an important exception being long-distance
iterative processes like consonant harmony. This
is parallel to the fact that long-distance phonotac-
tics cannot be represented with an SL stringset,
which motivated Heinz et al. (2011) to define
the Tier-based Strictly Local (TSL) languages—
stringsets that are SL after an erasure function has
applied, masking all symbols that are irrelevant
to the restrictions that the language places on its
strings.

Definition 8. (Erasure function) Given an alpha-
bet Σ, a tier Θ ⊆ Σ, and a string w = a1...an,
EraseΘ(w) = b1...bn where for all i ≤ n,
bi = ai if ai ∈ Θ, else bi = λ.

Informally, EraseΘ(w) returns the string w
with all non-tier elements removed. For con-
venience, we will write SuffnΘ(w) to mean
Suffn(EraseΘ(w)) in what follows.

Definition 9. (Tier-based Strictly Local lan-
guages) A language L is Tier-based Strictly k-
Local (TSLk) if there is a tier Θ ⊆ Σ and a subset
S ⊆ fack(oΘ∗n) such that:
L = {w ∈ Σ∗ | fack(oEraseΘ(w)n) ⊆ S}

3 Output Tier-based Strictly Local
functions and transducers

In this section, we define the OTSL functions,
which generalize the TSL stringsets to functions
in the same way that the OSL functions general-
ize SL stringsets to functions (see Chandlee, 2014;
Chandlee et al., 2015).

Definition 10. (Output Tier-based Strictly Lo-
cal functions) A function f is OTSLk if there
is a tier Θ ⊆ ∆ such that for all w1, w2 ∈
Σ∗, Suffk−1Θ (f

p(w1)) = Suff
k−1
Θ (f

p(w2)) ⇒
tailsf (w1) = tailsf (w2).

The OTSL class properly contains the OSL
functions, since every OSLk function can be de-
scribed as an OTSLk function whose tier is equal
to the entire output alphabet. Note that it is pos-
sible for a single OTSL function to be described
with more than one tier. For example, the identity
function (where Σ = ∆ and f(w) = w) can be
described with any subset of ∆ as its tier. We use
the term k-tier to describe a tier Θ for which f is
OTSLk.

Like the OSLk functions, the OTSLk functions
can be characterized in automata-theoretic terms.
First, we define OTSLk finite state transducers as
follows.

Definition 11. An onward DSFST T =
〈Q, q0, qf ,Σ,∆, δ〉 is OTSLk for the tier Θ ⊆ ∆
if:

1. Q = S ∪ {q0, qf} with S ⊆ Θ≤k−1

2. (∀u ∈ ∆∗)
[(q0,o, u, q′) ∈ δ ⇒ q′ = Suffk−1Θ (u)]

3. (∀q ∈ Q− {q0},∀a ∈ Σ, ∀u ∈ ∆∗)
[(q, a, u, q′) ∈ δ ⇒ q′ = Suffk−1Θ (qu)].

Lemmas 2 and 3, together with Theorem 2,
show that the OTSLk functions and the functions
represented by OTSLk transducers exactly corre-
spond.

Lemma 2. Let T = 〈Q, q0, qf ,Σ,∆, δ〉 be an
OTSLk transducer for the tier Θ. The following
holds: (q0,ow, u, q) ∈ δ∗ ⇒ q = Suffk−1Θ (u).



Lemma 3. Any OTSLk transducer corresponds to
an OTSLk function.

Theorem 2. Given an OTSLk function f and
one of its k-tiers Θ, the DSFST T =
〈Q, q0, qf ,Σ,∆, δ〉 defined as follows computes f :

1. Q = S ∪ {q0, qf} with S ⊆ Θ≤k−1

2. (q0,o, u,Suffk−1Θ (u)) ∈ δ⇐⇒u = fp(λ)

3. a ∈ Σ, (q, a, u,Suff1Θ(qu)) ∈ δ⇐⇒
(∃w) [fp(w) = vr ∧ Suffk−1Θ (vr) = q ∧
fp(wa) = vru]
where r = t1x1t2x2...tk−1xk−1,
ti ∈ Θ, xi ∈ (∆−Θ)∗ and v = fp(w) · r−1

4. (q,n, u, qf ) ∈ δ⇐⇒u = fp(wq)−1 · f(wq)

We note that these are trivial extensions of Lem-
mas 3, 4, and Theorem 2 in Chandlee et al.
(2015). Indeed, only two minor changes are
necessary for this generalization to OTSLk func-
tions. First, each instance of Suffk−1 must be
replaced with Suffk−1Θ . Second, in order to ac-
count for the fact that non-tier elements may come
between relevant tier elements, certain references
to a string q = t1t2...tk−1 must be rewritten as
r = t1x1t2x2...tk−1xk−1, where ti ∈ Θ and
xi ∈ (∆−Θ)∗. As the proofs are otherwise iden-
tical in structure to those found in Chandlee et al.
(2015), we do not provide them here.

It is therefore the case that any OTSLk function
can be represented by an OTSLk transducer. Infor-
mally, this will be an onward DSFST in which the
non-initial and non-final states represent the most
recent k−1 tier symbols written thus far, meaning
that this is the only information that will dictate
what the DSFST writes upon reading the next in-
put symbol.

As an example, Figure 1 presents an OTSL2
transducer that models the unbounded sibilant har-
mony in Samala from Section 1. Note that in order
to achieve the regressive directionality of the pro-
cess, we assume that this transducer reads input
strings from right-to-left (following, e.g., Heinz
and Lai, 2013; Chandlee et al., 2015). Direction-
ality will be further discussed in Section 6.

4 Useful properties of OTSL2 functions

The main goal of this paper is to demonstrate how
OTSL functions can be learned from positive data,
even without prior knowledge of the tier itself. We
note that the tier-induction strategy adopted below

q0

λ

Ss

qf

o:λ

?:?
S:Ss:s

S:S

s:S

?:?

s:s

S:s

?:? n:λ
n:λ

n:λ

Figure 1: An OTSL2 transducer that models un-
bounded sibilant harmony, where ? represents any
symbol that is not s or S

relies on certain properties that hold when k = 2,
but not necessarily for greater values of k. These
are outlined below.

First, when an OTSL2 function f can be de-
scribed with more than one 2-tier, the union of any
two or more such 2-tiers is also a 2-tier for f .

Lemma 4. Given an OTSL2 function f , if Θ1 ⊆ ∆
and Θ2 ⊆ ∆ are both 2-tiers for f , then Ω =
Θ1 ∪Θ2 is also a 2-tier for f .

Proof. If Suff1Ω(f
p(w1)) = Suff1Ω(f

p(w2)) =
t, then t ∈ Θ1 or t ∈ Θ2. If t ∈ Θ1,
then Suff1Θ1(f

p(w1)) = Suff1Θ1(f
p(w2)) =

t ⇒ tailsf (w1) = tailsf (w2). If t ∈
Θ2, then Suff1Θ2(f

p(w1)) = Suff1Θ2(f
p(w2))

= t ⇒ tailsf (w1) = tailsf (w2). There-
fore, Suff1Ω(f

p(w1)) = Suff1Ω(f
p(w2)) = t ⇒

tailsf (w1) = tailsf (w2)

It is this property that allows us to identify a
unique target tier for an OTSL2 function, which
the algorithm can find by flagging and removing
elements of ∆ from its hypothesis when evidence
is found that they cannot be on a relevant tier. We
define this canonical 2-tier as follows.

Definition 12. (Canonical 2-tier) Given an OTSL2
function f , Θ is the canonical 2-tier for f iff
there is no other 2-tier Ω ⊆ ∆ for f such that
card(Ω) ≥ card(Θ).
Remark 2. Given an OTSL2 function f , its canon-
ical 2-tier Θ is a superset of any 2-tier for f . (This
follows immediately from Lemma 4.)

There is therefore a unique canonical 2-tier (i.e.,
the largest one) for each OTSL2 function. Inter-
estingly, this can be exploited during the learning



process, since it leads to the following useful prop-
erty of OTSL2 functions.

Lemma 5. Let f : Σ∗ → ∆∗ be an OTSL2 func-
tion, let Θ be its canonical 2-tier, and let Ω be
such that Θ ⊂ Ω ⊆ ∆. We have ∃a ∈ (Ω − Θ),
∃w1, w2 ∈ Σ∗, and ∃x ∈ Σ ∪ {n} such that
Suff1Ω(f

p(w1)) = Suff1Ω(f
p(w2)) = a and

contf (x,w1) 6= contf (x,w2).

Proof. By contradiction. Suppose that the lemma
is false. This means that ∀a ∈ (Ω−Θ), ∀w1, w2 ∈
Σ∗, and ∀x ∈ Σ ∪ {n}, we have Suff1Ω(fp(w1))
= Suff1Ω(f

p(w2)) = a ⇒ contf (x,w1) =
contf (x,w2). Now, since Θ is a 2-tier for f ,
it is also the case that ∀b ∈ Θ, ∀w1, w2 ∈ Σ∗,
and ∀x ∈ Σ ∪ {n}, we have Suff1Ω(fp(w1))
= Suff1Ω(f

p(w2)) = b ⇒ contf (x,w1) =
contf (x,w2). Together these imply that ∀c ∈
Ω, ∀w1, w2 ∈ Σ∗, and ∀x ∈ Σ ∪ {n}, we
have Suff1Ω(f

p(w1)) = Suff1Ω(f
p(w2)) = c⇒

contf (x,w1) = contf (x,w2).
Since [Suff1Ω(f

p(w1)) = Suff1Ω(f
p(w2))

and contf (x,w1) = contf (x,w2)] ⇒
Suff1Ω(f

p(w1x)) = Suff1Ω(f
p(w2x)), we

also have contf (y, w1x) = contf (y, w2x)
for all y ∈ Σ ∪ {n}. This applies recursively,
giving us Suff1Ω(f

p(w1)) = Suff1Ω(f
p(w2))

⇒ tailsf (w1) = tailsf (w2), which means
that Ω is a 2-tier for f . However, card(Ω) >
card(Θ), contradicting the fact that Θ is the
canonical 2-tier for f .

Importantly, it follows from Lemma 5 that for
any set Ω which is a strict superset of Θ (the
canonical 2-tier), we will always be able to find
evidence that some member of Ω could not be a
member of any 2-tier for f . It is this property
of OTSL2 functions that our algorithm makes use
of to determine which output symbols are in Θ.
Once again, when k > 2, this property does not
necessarily hold.1 Accordingly, we restrict our-
selves to k = 2 when discussing the learning of
OTSL functions without prior knowledge of the
tier. While OTSL2 functions seem sufficient for
modelling a wide range of long-distance phono-
logical processes, we discuss certain exceptions in
Section 6.

1For example, if ∆ = {a, b, c}, there could be an OTSL3
function for which Θ1 = {a, b} and Θ2 = {a, c} are both
3-tiers, but Ω = {a, b, c} is not.

5 Learning OTSL functions

5.1 Learning paradigm

We adopt the criterion for successful learning that
requires exact identification in the limit from pos-
itive data (Gold, 1967), with polynomial bounds
on time and data (de la Higuera, 1997). We first
define what it means for a class of functions to be
represented by a class of representations.

Definition 13. A class T of functions is repre-
sented by a class R of representations if every r ∈
R is of finite size and there is a total and surjective
naming function L : R→ T such that L(r) = t if
and only if for all w ∈ pre image(t), r(w) =
t(w), where r(w) is the output produced by r
given the input w.

The notions of a sample and a learning algo-
rithm are defined as follows.

Definition 14. (Sample) A sample S for a function
t ∈ T is a finite set of data consistent with t, that
is to say (w, u) ∈ S iff t(w) = v. The size of a
sample is the sum of the length of the strings it is
composed of: |S| =

∑
(w,u)∈S |w|+ |u|.

Definition 15. (Learning algorithm) A (T,R)-
learning algorithm A is a program that takes as
input a sample for a function t ∈ T and outputs a
representation from R.

The paradigm relies on the notion of a charac-
teristic sample, adapted here for functions as in
Chandlee et al. (2015).

Definition 16. (Characteristic sample) For a
(T,R)-learning algorithm A, a sample CS is a
characteristic sample of a function t ∈ T if for
all samples S ⊇ CS, A returns a representation r
such that L(r) = t.

The learning paradigm can now be defined as
follows.

Definition 17. (Identification in polynomial time
and data) A class T of functions is identifiable in
polynomial time and data if there exists a (T,R)-
learning algorithm A and two polynomial equa-
tions p() and q() such that:

1. For any sample S of size m for t ∈ T, A
returns a hypothesis r ∈ R in O(p(m)) time.

2. For each representation r ∈ R of size n, with
t = L(r), there exists a characteristic sample
of t for A of size at most O(q(n)).



5.2 Learning when the tier is given

Prior to describing the approach we take to induc-
ing the contents of a tier when k = 2, we note that
learning any OTSLk function from positive data
is relatively straightforward if the value of k and
the tier Θ are known beforehand. In particular, al-
though the OSLFIA presented in Chandlee et al.
(2015) was designed only to learn OSL functions,
it turns out that a minor modification allows us to
extend their result to OTSL functions, so long as k
and Θ are known beforehand. We summarize how
this can be done below.

In its original form, the OSLFIA inevitably fails
to learn any OTSL function that is not itself OSL
(i.e., where Θ 6= ∆). Specifically, since the al-
gorithm labels each landing state of a transition
with the k − 1 suffix of its associated output, it
will always incorrectly determine the landing state
of one of more transitions when there is a long-
distance dependency. Moreover, the exact way in
which the resulting OSL transducer differs from
the target OTSL transducer is somewhat unpre-
dictable. As such, there does not seem to be a
general approach for transforming the OSLFIA’s
output into an appropriate OTSLk transducer.

In cases where Θ is known beforehand, how-
ever, we can circumvent this issue by simply spec-
ifying that non-tier elements should be skipped
over when labelling a state. In doing so, the algo-
rithm will be able to find all of the necessary states
as well as the correct landing state for each tran-
sition in the target OTSLk transducer. This mod-
ification of the OSLFIA is incorporated into the
function build fst, which is detailed in Algo-
rithm 1. While this constitutes one important as-
pect of learning OTSLk functions, it is nonetheless
a major challenge to determine the actual contents
of Θ without a priori knowledge. Although induc-
ing a k-tier for any value of k remains as an open
problem, in the following section we describe how
his can be done when k = 2.

5.3 Learning the contents of a 2-tier

Having shown that the OSLFIA can be modified
to learn an OTSLk function f once Θ (the tier) is
known, we now describe our approach to inducing
Θ itself when k = 2. After this is done, Θ can sim-
ply be fed into the build fst function in order
to produce an OTSL2 transducer that represents f .

The first step toward learning the contents of a
2-tier is to gain as much information as possible

Function build fst(S, Θ, k):
C ← {q0, qf} with q0, qf /∈ Θ≤k−1;
s← lcp({y|(x, y) ∈ S});
q ← Suffk−1Θ (s);
Earliest(q)← o;
Out(q)← s;
δ ← {(q0,o, s, q)};
R← {q};
while R 6= ∅ do

q ← First(R);
s← Earliest(q);
for all a ∈ Σ in alphabetical order do

if ∃(w, u) ∈ S, x ∈ Σ∗ s.t.
w = sax then
v ← lcp({y|∃x, (sax, y) ∈
S});
r ← Suffk−1Θ (v);
δ ←
δ ∪ {(q, a,Out(q)−1 · v, r)};

if r /∈ R ∪ C then
R← R ∪ {r};
Earliest(r)← sa;
Out(r)← v;

if ∃u s.t. (s, u) ∈ S then
δ ← δ∪{(q,n,Out(q)−1 ·u, qf )}

R← R− {q};
C ← C ∪ {q};

return 〈C, q0, qf ,Σ,∆, δ〉
Algorithm 1: Building an OTSLk transducer
when given Θ

about the prefix function fp corresponding to f ,
based only on the evidence provided in the training
sample. To do this, the function estimate fp,
shown in Algorithm 2, goes through every string
x that is the prefix of at least one input string in
the training data, and for every a ∈ Σ, it checks
whether xa is also a prefix of some input string.
If this is the case, there is enough information to
determine fp(x). The function estimate fp
will then add the pair (x, z) to the set P , where
z is the longest common prefix of f(w) for all
(w, f(w)) ∈ S such that x is a prefix of w. We
note that this z will be equal to fp(x) provided
that the training data come from a subsequential
function, and so this technique may be useful for
learning other types of functions as well.

We further note, however, that by using
this strategy, estimate fp is only guaran-
teed to produce all the pairs (x, fp(x)) nec-
essary to discover the tier for total functions.



Function estimate fp(S):
P ← ∅;
X ← {x | x ∈ Pref∗(w), where
(w, u) ∈ S};
Y ← {x ∈ X | (∀a ∈ Σ)[xa ∈ X]};
for each y ∈ Y do

z ← lcp({u | (w, u) ∈ S, where
y ∈ Pref∗(w)});
P ← P ∪ {(y, z)}

return P ;
Algorithm 2: Prefix function estimation

This is because, when there is no pair with
the shape (opaxn, f(pax)) in the training data,
estimate fp does not know whether this is ac-
cidental (i.e., due to the finite nature of the training
data) or because the function is undefined for all
inputs of the shape opaxn. While the ability to
accommodate partial functions would have practi-
cal applications for learning from natural language
data, at present we leave the task of extending
estimate fp in this way to future research.

The full learning algorithm, which we call
the OTSL2 Function Inference Algorithm
(OTSL2FIA) is shown in Algorithm 3. We
assume that Σ and ∆ are fixed and not part of the
input to the learning problem (and that k = 2).
Given a finite sample of training data, it first
estimates the relevant prefix function with the set
P , as described above, and begins with the hy-
pothesis that Θ = ∆ (i.e., that all members of the
output alphabet are on the target tier). Then, for
each a ∈ Θ, it looks through P for any evidence
that a needs to be removed from Θ. To do this,
it builds an auxiliary set Match that contains
every (p, fp) ∈ P for which Suff1Θ(fp(p)) = a
under the current hypothesis for Θ. For each
x ∈ Σ∪{n}, it then checks whether contf (x, p)
is the same for all (p, q) ∈ Match. If this is the
case, a is added to the set Keep. However, if there
is more than one value found for the contribution
of some x ∈ Σ ∪ {n}, it will instead remove a
from Θ, since it cannot possibly be a member of
the target 2-tier. If at any point some symbol gets
removed from Θ, the set Keep is immediately
emptied. This portion of the algorithm will run
until every a in the current hypothesis for Θ gets
added to the set Keep, in which case it knows
it has found the canonical 2-tier of the target
function.

Once the OTSL2FIA converges on the canon-

Data: Sample S ⊂ {o}Σ∗{n} ×∆∗
Result: An OTSL2 transducer

T = 〈C, q0, qf ,Σ,∆, δ〉
P ← estimate fp(S);
Θ← ∆;
Keep← ∅;
while Keep 6= Θ do

for each a ∈ Θ do
Match← {(p, q) ∈ P |
Suff1Θ(q) = a};

for each σ ∈ Σ do
Cσ,a ← {q−1 · y |
(p, q) ∈ Match ∧ (pσ, y) ∈ P};

if card(Cσ,a) > 1 then
Θ← Θ− {a};
Keep← ∅;

Cn,a ← {q−1 · y |
(p, q) ∈ Match ∧ (p, y) ∈ S};

if card(Cn,a) > 1 then
Θ← Θ− {a};
Keep← ∅;

if a ∈ Θ then
Keep← Keep ∪ {a}

T ← build fst(S, Θ, 2);
return T

Algorithm 3: OTSL2FIA

ical 2-tier Θ, the final step is simply to feed Θ
and the sample S into the function build fst
shown above in Algorithm 1 (further specifying
that k = 2). Under the assumption that the train-
ing sample contains the appropriate evidence, as
described in the following section, this will pro-
duce an OTSL2 transducer which represents the
target OTSL2 function.

5.4 Theoretical results

Here we establish several theoretical results,
which culminate in the theorem that the
OTSL2FIA identifies the class of total OTSL2
functions in polynomial time and data.

In what follows, we let f be the target OTSL2
function, Θ� be its canonical 2-tier, and T � =
〈Q�, q0, qf ,Σ,∆, δ�〉 be its target transducer as
defined by Theorem 2. We furthermore let Θ be
the OTSL2FIA’s final tier hypothesis, and T =
〈Q, q0, qf ,Σ,∆, δ〉 be the transducer that is con-
structed on the input.

Lemma 6. (Polynomial time) For any input sam-
ple S, the OTSL2FIA produces T in time polyno-
mial in the size of S.



Proof. Let n =
∑

(w,u)∈S |w|, m = max{|u| :
(w, u) ∈ S}, p = max{|w| : (w, u) ∈ S}, and
s = card(S). We note that these are all linear in
the size of the sample.

The OTSL2FIA starts by calling
estimate fp. This function first deter-
mines all of the input prefixes present in S,
which takes n steps. Then estimate fp
checks, for each prefix x and all a ∈ Σ, whether
xa is also an input prefix in S. There are at
most sm prefixes in S, so this takes at most
card(Σ) · (sm)n steps. Finally, for a subset of
the input prefixes, estimate fp determines
lcp({u | (w, u) s.t. x ∈ Pref∗(w)}), which
with an appropriate data structure (for instance a
prefix tree) can be done in nm steps. The overall
computation time of estimate fp is thus
O(n + (sm)n + (sm)(nm)), which is quartic in
the size of the learning sample.

The portion of the OTSL2FIA that determines
the tier is now run. After i elements have been re-
moved from Θ, the combined while/for loop can
run up to |∆| − i times, and can only remove up
to |∆| items, so the loop will be used fewer than
|∆|2 times, which is a constant. This main loop
first gathers all (w, u) ∈ P that meet a certain cri-
terion into the set Match, which can be done in
card(P )m = (sm)m = sm2 steps. Next, the
main loop enters a for loop that is used card(Σ)
times (a constant) and which attempts to calculate
the contribution of σ ∈ Σ using each (w, u) ∈
Match if it can find (wσ, v) ∈ P . We note that
card(Match) will be at most sm, that finding
(wσ, v) ∈ P takes at most smp steps, and that
calculating the contribution takes at most m steps.
The main loop then attempts to calculate the con-
tribution of n using each (w, u) ∈ Match if it can
find (w, v) ∈ S. We note that finding (w, v) ∈ S
takes at most n steps, and that calculating the con-
tribution takes at most m steps. The overall com-
putation time of this portion of the algorithm is
thusO(sm2+sm(smp+m)+sm(n+m)), which
is quintic in the size of the learning sample.

Finally, the OTSL2FIA feeds Θ and S to the
function build fst. As noted above, this fuc-
tion incorporates a simple modification to the
state-labelling process in Chandlee et al.’s (2015)
OSLFIA. While this change allows it to build an
OTSL transducer once the tier is known, it does
not affect computation time. This final step of
the OTSL2FIA therefore runs in time quadratic

in the size of the learning sample (for OSLFIA
time complexity proofs, see Chandlee et al., 2015).
Since each portion of the OTSL2FIA runs in time
polynomial in the size of the sample, with the
highest complexity being quintic, the overall com-
putation time of the algorithm is therefore polyno-
mial in the size of the learning sample.

The remaining lemmas of this section will show
that for each total OTSL2 function f , there is a fi-
nite kernel of data consistent with f that is a char-
acteristic sample for OTSL2FIA, which we call an
OTSL2FIA seed.

Definition 18. (Seed) Given T �, a sample S con-
tains a seed if:

1. For all q ∈ Q�, (owqn, f(wq)) ∈ S.

2. For all (q, a, u, q′) ∈ δ� such that q′ 6= qf
and a ∈ {o} ∪ Σ, and for all pairs b, c ∈ Σ:

(a) (owqan, f(wqa)) ∈ S
(b) (owqabn, f(wqab)) ∈ S
(c) (owqabcxn, f(wqabcx)) ∈ S, where

x ∈ Σ∗

Lemma 7. If a learning sample S contains a seed,
then the OTSL2FIA can determine contf (x,w)
for all w ∈ Σ∗ and all x ∈ Σ ∪ {n}.

Proof. Let us start with some string owbyn such
that b ∈ Σ and w, y ∈ Σ∗. Since T � is
OTSL2, it will be in the state corresponding to
Suff1Θ�(f

p(w)) immediately prior to reading b
when processing owbyn. Let us call this state
q′. The most recent transition that T � will
have traversed is (q, a, u, q′). The target func-
tion is OTSL2, and so it is the case that either
w = wqa or else can be replaced thereby since
Suff1Θ�(f

p(w)) = Suff1Θ�(f
p(wqa)) and there-

fore contf (b, w) = contf (b, wqa).
Now let us start with some string own such

that w ∈ Σ∗. When T � reads own, it will be
in the state corresponding to Suff1Θ�(f

p(w)) im-
mediately prior to reading n. Let us call this
state q′. The most recent transition that T � will
have traversed is (q, a, u, q′). The target func-
tion is OTSL2, and so it is the case that either
w = wqa or else can be replaced thereby since
Suff1Θ�(f

p(w)) = Suff1Θ�(f
p(wqa)) and there-

fore contf (n, w) = contf (n, wqa).
Now recall that fp(w) = lcp({u | u =

f(wx) ∧ x ∈ Σ∗}). We do not actually need the



entirety of this infinite set to determine fp(w), it
is sufficient to use a set containing f(w) and one
f(wax) for each a ∈ Σ where x ∈ Σ∗ because ev-
ery x ∈ Σ∗ is either λ or begins with some a ∈ Σ.
Let us call such a set a support for determining
fp(w). The function estimate fp takes every
prefix p present in S and checks whether a sup-
port for determining fp(p) exists in S. Then, if a
support exists, estimate fp adds (p, q) to the
set P , where q = lcp({u | (w, u) ∈ S ∧ p ∈
Pref∗}), that is q = fp(p).

By the definition of the seed, for every transition
(q, a, u, q′) ∈ δ� such that q′ 6= qf , the learner
will see owqan, owqabn for all b ∈ Σ, and
at least one input string owqabcxn for all pairs
b, c ∈ Σ. We therefore know that for any pair of
input strings own and owbyn in the domain of
f such that b ∈ Σ and x ∈ Σ∗, the seed will con-
tain all the input strings necessary to build sup-
ports for determining fp(wqa) and fp(wqab) such
that tailsf (wqa) = tailsf (w).

By Remark 1, we know that contf (b, w) =
fp(w)−1 · fp(wb) for all b ∈ Σ and w ∈ Σ∗.
It is therefore the case that for every owbyn,
the algorithm can determine contf (b, wqa) =
fp(wqa)

−1 · fp(wqab) = contf (b, w). Also
by Remark 1, we know that contf (n, w) =
fp(w)−1 · f(w) for all w ∈ Σ∗. It is therefore the
case that for every own, the algorithm can deter-
mine contf (n, wqa) = fp(wqa)−1 · f(wqa) =
contf (n, w).

Lemma 8. (Tier convergence) If a learning sam-
ple S contains a seed, then Θ = Θ�.

Proof. The OTSL2FIA starts with Θ = ∆, and so
either Θ = Θ� already, or else Θ ⊃ Θ�.

We know from Lemma 5 that if Θ ⊃ Θ�,
there will exist a pair of input strings w1 and w2
in the domain of f such that Suff1Θ(f

p(w1)) =
Suff1Θ(f

p(w2)) = a for some a ∈ (Θ − Θ�)
and contf (x,w1) 6= contf (x,w2) for some x ∈
Σ ∪ {n}. We know from Lemma 7 that every
own in the domain of f has at least one corre-
sponding (w′, fp(w′)) ∈ P and at least one cor-
responding (w′a, fp(w′a)) ∈ P for each a ∈ Σ,
where Suff1Θ(f

p(w)) = Suff1Θ(f
p(w′)) and so

contf (x,w) = contf (x,w′) for all x ∈ Σ ∪
{n}.The algorithm will thus be able to calculate
and check all the relevant contributions necessary
to flag and remove at least one a ∈ (Θ−Θ�) when
Θ ⊃ Θ�.

Conversely, there will be no pair of input
strings w3 and w4 in the domain of f such that
contf (x,w3) 6= contf (x,w4) for some x ∈ Σ∪
{n} when Suff1Θ(fp(w3)) = Suff1Θ(fp(w3))
= c for some c ∈ Θ�. When Θ = Θ�, then,
the algorithm will add all a ∈ Θ to Keep and
pass Keep = Θ = Θ� to the build fst func-
tion.

Lemma 9. (Transducer convergence) If a learn-
ing sample S contains a seed then (q0,ow, u, r) ∈
δ∗⇐⇒(q0,ow, u, r) ∈ δ∗� .
Lemma 10. (Characteristic Sample) Any learning
sample containing a seed is a characteristic sam-
ple for the OTSL2FIA.

We do not include the proofs of Lemmas 9 and
10 here, as they are a trivial extension of analogous
proofs in Chandlee et al. (2015, Lemmas 7 and 8).
Again, the generalization requires only that each
instance of Suffk−1 be replaced by Suffk−1Θ in
order for the proofs hold for any OTSLk trans-
ducer, provided that the target tier is passed to
build fst. We further note that when the tar-
get transducer is one that computes a total OTSL2
function with its canonical tier, a seed (as defined
in Definition 18 above) is a superset of that re-
quired by Chandlee et al. (2015, Definition 11).

Lemma 11. (Polynomial data) Given an OTSL2
transducer T �, there exists a seed for the
OTSL2FIA that is of size polynomial in the size
of T �.

Proof. Let T � = 〈Q�, q0, qf ,Σ,∆, δ�〉. For item
1 in Definition 18 there are card(Q�) corre-
sponding input-output pairs (wq, f(wq)) in a seed.
For each of these pairs, it is the case that | o
wq n | ≤ card(Q�) and it is the case that
|f(wq)| ≤

∑
(q,a,u,q′)∈δ� |u|. We denote the lat-

ter quantity with x� =
∑

(q,a,u,q′)∈δ� |u| and note
that x� = O(|T �|). The overall length of the in-
puts in the portion of the seed contributed by item
1 is thus in O(card(Q�)2). The overall length of
the outputs in the portion of the seed contributed
by item 1 is thus in O(card(Q�) · x�). We note
that both of these are quadratic in the size of T �.

For items 2a, 2b, and 2c in Definition 18,
there are respectively 1, card(Σ), and card(Σ)2

corresponding input-output pairs per transition
(q, a, u, q′) ∈ δ�. Factoring out the constant
card(Σ) gives us 3 · card(δ�) pairs. For the
pairs contributed by item 2c, we can restrict our-
selves to pairs (owqabcn, f(wqabc)), since f is a



total function. For each pair, we have |o w n | ≤
card(Q�) + 3 and |f(w)| ≤

∑
(q,a,u,q′)∈δ� |u| +

3m, where m = max{|u| : (q, a, u, q′) ∈ δ�}.
With this last quantity denoted y�, we note that
y� = O(|T �|). The overall length of the inputs
in the portion of the seed contributed by item 2 is
therefore in O((3 · card(δ�))(card(Q�) + 3) =
O(card(δ�) · card(Q�) + card(δ�)), and the
overall length of the outputs in the portion of the
seed contributed by item 2 is in O(3 · card(δ�) ·
y�). Both of these are quadratic in the size of T �.

Altogether, then, the size of the seed is quadratic
in the size of the target transducer.

Theorem 3. The OTSL2FIA identifies the OTSL2
functions in polynomial time and data.

Proof. Immediate from Lemmas 6, 8, 9, 10, and
11.

6 Discussion

The OTSL functions introduced in this paper
are capable of modelling many of the attested
long-distance phonological processes. These pro-
cesses can be assimilatory like sibilant harmony
in Samala (see Section 1), but can also be dis-
similatory. For example, Georgian exhibits a pat-
tern of liquid dissimilation, in which /r/ surfaces
as [l] when preceded at any distance by another [r]
(e.g., /aprik’-uri/ → [aprik’uli] ‘African’; Odden,
1994). Interestingly, the dissimilation does not oc-
cur if there is an intervening [l] (e.g., /kartl-uri/
→ [kartluri] ‘Kartvelian’). The OTSL functions
are fully capable of representing such blocking ef-
fects, as shown in Figure 2. To avoid cluttering the
figure, we omit the final state and all of its incom-
ing transitions (which would be labelled n:λ).

It is worth pointing out that the processes in
Samala and Georgian apply in opposite directions.
In Samala, the trigger is the rightmost sibilant,
whereas in Georgian it is the leftmost liquid. This
distinction can be captured by assuming that input
strings are read from left-to-right in the Georgian
case (i.e., the process is progressive), but from
right-to-left in the Samala case (i.e., the process is
regressive). The direction of reading, then, divides
the OTSL functions into two overlapping but dis-
tinct classes which we call L-OTSL (which read
from the left) and R-OTSL (which read from the
right), following Heinz and Lai (2013) and Chan-
dlee et al. (2015) who make the same distinction

q0

λ

lr

o:λ

?:?

l:lr:r l:l

r:r
?:?

r:l
l:l?:?

Figure 2: An OTSL2 transducer that models un-
bounded liquid dissimilation with blocking, where ?
represents any symbol that is not [l] or [r].

for the subsequential and OSL functions, respec-
tively.

As mentioned above, the OTSL2FIA outlined
in Section 5 only succeeds in learning total func-
tions and is designed specifically to learn OTSL2
functions. The algorithm exploits the fact that
the largest possible 2-tier for an OTSL2 function
f is a superset of every other 2-tier for f , and
will accordingly never run the risk of removing
an element that would need to be subsequently re-
added to the tier. However, it is not clear that this
strategy will succeed for higher values k, which
may be needed to model certain types of patterns.
For example, a reviewer raises the complex case
of retroflexion harmony targetting /n/ in Sanskrit
(also known as nati) as one such pattern. A formal
analysis provided by Graf and Mayer (2018) uses
a class of stringsets that they call Input-Output
Tier-based Strictly Local (IO-TSL). IO-TSL for-
mal languages are like TSL languages except that
input symbols are projected onto the tier based on
(i) the surrounding context of input symbols and
(ii) the symbols that precede it on the tier that has
been projected so far. Under this analysis, Sanskrit
n-retroflexion requires k = 3 on the projected tier.

Finally, while the OTSL class can model long-
distance processes, it can only do so when no more
than a single tier is required. That is, a language
that simultaneously exhibits patterns of, e.g., sibi-
lant harmony and liquid dissimilation would not
be OTSL for any value of k. Further exploration of
these issues will allow us to better understand the
computational properties of phonological transfor-
mations and to establish a boundary of complexity
that is both necessary and sufficient for capturing



the full range of possible phonological systems.

7 Conclusion

This paper has provided both a language-theoretic
and an automata-theoretic characterization of the
OTSL class of functions, which is relevant for
modelling long-distance phonological processes
as string-to-string transformations. We further
demonstrated that by generalizing previous re-
search on OSL functions to the OTSL class, any
OTSLk function can be learned once the tier is
known. Finally, we introduced an algorithm for
efficiently learning any total OTSL2 function from
positive data, even when a relevant tier is not given
a priori. To our knowledge, this is the first al-
gorithm to accomplish this for input-output map-
pings rather than phonotactics. In future research,
we aim to extend this result in multiple ways: to
partial functions, to any value of k, and to pro-
cesses requiring multiple tiers.

Acknowledgements

Special thanks to Jane Chandlee, participants of
the Third Subregular Workshop at Stony Brook
University, and three anonymous reviewers. This
research was supported by the Social Sciences and
Humanities Research Council of Canada.

References
Richard B. Applegate. 1972. Ineseño Chumash gram-

mar. Doctoral dissertation, University of California,
Berkeley.

Jane Chandlee. 2014. Strictly Local phonological pro-
cesses. Ph.D. thesis, University of Delaware.

Jane Chandlee, Rémi Eyraud, and Jeffrey Heinz.
2014. Learning strictly local subsequential func-
tions. Transactions of the Association for Compu-
tational Linguistics, 2:491–503.

Jane Chandlee, Rémi Eyraud, and Jeffrey Heinz. 2015.
Output strictly local functions. In Proceedings of the
14th Meeting on the Mathematics of Language.

George N. Clements. 1980. Vowel harmony in nonlin-
ear generative phonology: an autosegmental model.
Indiana University Linguistics Club, Bloomington,
IN.

Colin de la Higuera. 1997. Characteristic sets for poly-
nomial grammatical inference. Machine Learning,
27(2):125–138.

E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 10:447–474.

John A. Goldsmith. 1990. Autosegmental and metrical
phonology. Blackwell, Oxford.

Maria Gouskova and Gillian Gallagher. 2019. Induc-
ing nonlocal constraints from baseline phonotactics.
Natural Language and Linguistic Theory.

Thomas Graf and Connor Mayer. 2018. Sanskrit n-
retroflexion is input-output tier-based strictly local.
In Proceedings of SIGMORPHON 2018, pages 151–
160.

Gunnar Ólafur Hansson. 2010. Consonant harmony:
long-distance interaction in phonology. University
of California Press, Berkeley, CA.

Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379–440.

Jeffrey Heinz and Regine Lai. 2013. Vowel harmony
and subsequentiality. In Proceedings of the 13th
Meeting on the Mathematics of Language, pages 52–
63, Sofia, Bulgaria.

Jeffrey Heinz, Chetan Rawal, and Herbert G. Tan-
ner. 2011. Tier-based strictly local constraints for
phonology. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 58–64, Portland, OR. Association for
Computational Linguistics.

Adam Jardine, Jane Chandlee, Rémi Eyraud, and Jef-
frey Heinz. 2014. Very efficient learning of struc-
tured classes of subsequential functions from pos-
itive data. In Proceedings of the Twelfth Interna-
tional Conference on Grammatical Inference, vol-
ume 34, pages 94–108.

Adam Jardine and Jeffrey Heinz. 2016. Learning tier-
based strictly 2-local languages. Transactions of the
Association for Computational Linguistics, 4:87–98.

Adam Jardine and Kevin McMullin. 2017. Efficient
learning of Tier-based Strictly k-Local languages. In
Proceedings of Language and Automata Theory and
Applications, 11th International Conference, Lec-
ture Notes in Computer Science. Springer.

Aaron Kaplan. 2008. Noniterativity is an emergent
property of grammar. Ph.D. thesis, University of
California Santa Cruz.

Kevin McMullin. 2016. Tier-based locality in long-
distance phonotactics: learnability and typology.
Ph.D. thesis, University of British Columbia.

David Odden. 1994. Adjacency parameters in phonol-
ogy. Language, 70:289–330.

José Oncina and Pedro Garcı́a. 1991. Inductive learn-
ing of subsequential functions. Techical Report
DSIC II-34, University Politećnia de Valencia.



José Oncina, Pedro Garcı́a, and Enrique Vidal.
1993. Learning subsequential transducers for pat-
tern recognition tasks. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 15:448–
458.

James Rogers and Geoffrey K. Pullum. 2011. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation, 20(3):329–342.


