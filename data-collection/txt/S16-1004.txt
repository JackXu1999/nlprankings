



















































SemEval-2016 Task 7: Determining Sentiment Intensity of English and Arabic Phrases


Proceedings of SemEval-2016, pages 42–51,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

SemEval-2016 Task 7: Determining Sentiment Intensity
of English and Arabic Phrases

Svetlana Kiritchenko and Saif M. Mohammad
National Research Council Canada

svetlana.kiritchenko@nrc-cnrc.gc.ca

saif.mohammad@nrc-cnrc.gc.ca

Mohammad Salameh
University of Alberta

msalameh@ualberta.ca

Abstract

We present a shared task on automatically de-
termining sentiment intensity of a word or a
phrase. The words and phrases are taken from
three domains: general English, English Twit-
ter, and Arabic Twitter. The phrases include
those composed of negators, modals, and de-
gree adverbs as well as phrases formed by
words with opposing polarities. For each of
the three domains, we assembled the datasets
that include multi-word phrases and their con-
stituent words, both manually annotated for
real-valued sentiment intensity scores. The
three datasets were presented as the test sets
for three separate tasks (each focusing on a
specific domain). Five teams submitted nine
system outputs for the three tasks. All datasets
created for this shared task are freely available
to the research community.

1 Introduction

Words have prior associations with sentiment. For
example, honest and competent are associated with
positive sentiment, whereas dishonest and dull are
associated with negative sentiment. Further, the de-
gree of positivity (or negativity), also referred to as
intensity, can vary. For example, most people will
agree that succeed is more positive (or less negative)
than improve, and fail is more negative (or less pos-
itive) than setback. We present a shared task where
automatic systems are asked to predict a prior sen-
timent intensity score for a word or a phrase. The
words and phrases are taken from three domains:
general English, English Twitter, and Arabic Twitter.

For each domain, a separate task with its own devel-
opment and test sets was set up. The phrases include
those composed of negators (e.g., nothing wrong),
modals (e.g., might be fun), and degree adverbs (e.g.,
fairly important) as well as phrases formed by words
with opposing polarities (e.g., lazy sundays).

Lists of words and their associated sentiment are
commonly referred to as sentiment lexicons. They
are used in sentiment analysis. For example, a
number of unsupervised classifiers rely primarily on
sentiment lexicons to determine whether a piece of
text is positive or negative. Supervised classifiers
also often use features drawn from sentiment lexi-
cons (Mohammad et al., 2013; Pontiki et al., 2014).
Sentiment lexicons are also beneficial in stance de-
tection (Mohammad et al., 2016a; Mohammad et
al., 2016b), literary analysis (Hartner, 2013; Kleres,
2011; Mohammad, 2012), and for detecting person-
ality traits (Minamikawa and Yokoyama, 2011; Mo-
hammad and Kiritchenko, 2015).

Existing manually created sentiment lexicons
tend to provide only lists of positive and negative
words (Hu and Liu, 2004; Wilson et al., 2005; Mo-
hammad and Turney, 2013). The coarse-grained
distinctions may be less useful in downstream ap-
plications than having access to fine-grained (real-
valued) sentiment association scores. Most of the
existing sentiment resources are available only for
English. Non-English resources are scarce and often
based on automatic translation of the English lexi-
cons (Abdul-Mageed and Diab, 2014; Eskander and
Rambow, 2015). Manually created sentiment lex-
icons usually include only single words. Yet, the
sentiment of a phrase can differ markedly from the

42



sentiment of its constituent words. Sentiment com-
position is the determining of sentiment of a multi-
word linguistic unit, such as a phrase or a sentence,
from its constituents. Lexicons that include senti-
ment associations for phrases and their constituents
are useful in studying sentiment composition. We
refer to them as sentiment composition lexicons.

Automatically created lexicons often have real-
valued sentiment association scores, have a high
coverage, can include longer phrases, and can eas-
ily be collected for a specific domain. However, due
to the lack of manually annotated real-valued senti-
ment lexicons the quality of automatic lexicons are
often assessed only extrinsically through their use in
sentence-level sentiment prediction. In this shared
task, we intrinsically evaluate automatic methods
that estimate sentiment association scores for terms
in English and Arabic. For this, we assembled
three datasets of phrases and their constituent single
words manually annotated for sentiment with real-
valued scores (Kiritchenko and Mohammad, 2016a;
Kiritchenko and Mohammad, 2016c).

We first introduced this task as part of the
SemEval-2015 Task 10 ‘Sentiment Analysis in Twit-
ter’ Subtask E (Rosenthal et al., 2015). The 2015
test set was restricted to English single words and
simple two-word negated expressions commonly
found in tweets. This year (2016), we broadened
the scope of the task and included three different
domains. Furthermore, we shifted the focus from
single words to longer, more complex phrases to ex-
plore sentiment composition.

Five teams submitted nine system outputs for
the three tasks. All submitted outputs correlated
strongly with the gold term rankings (Kendall’s rank
correlation above 0.35). The best results on all tasks
were achieved with supervised methods by exploit-
ing a variety of sentiment resources. The highest
rank correlation was obtained by team ECNU on the
General English test set (τ = 0.7). On the other two
domains, the results were lower (τ of 0.4-0.5).

All datasets created as part of this shared task are
freely available through the task website.1 For ease
of exploration, we also created online interactive vi-
sualizations for the two English datasets.2

1http://alt.qcri.org/semeval2016/task7/
2http://www.saifmohammad.com/WebPages/SCL.html

2 Task Description

The task is formulated as follows: given a list of
terms (single words and multi-word phrases), an au-
tomatic system needs to provide a score between 0
and 1 that is indicative of the term’s strength of as-
sociation with positive sentiment. A score of 1 indi-
cates maximum association with positive sentiment
(or least association with negative sentiment) and a
score of 0 indicates least association with positive
sentiment (or maximum association with negative
sentiment). If a term is more positive than another,
then it should have a higher score than the other.

There are three tasks, one for each of the three
domains:

• General English Sentiment Modifiers Set:
This dataset comprises English single words
and multi-word phrases from the general do-
main. The phrases are formed by combining
a word and a modifier, where a modifier is a
negator, an auxilary verb, a degree adverb, or
a combination of those, for example, would be
very easy, did not harm, and would have been
nice. The single word terms are chosen from
the set of words that are part of the multi-word
phrases, for example, easy, harm, and nice.

• English Twitter Mixed Polarity Set: This
dataset focuses on English phrases made up of
opposing polarity terms, for example, phrases
such as lazy sundays, best winter break, happy
accident and couldn’t stop smiling. The dataset
also includes single word terms (as separate en-
tries). These terms are chosen from the set of
words that are part of the multi-word phrases.
The multi-word phrases and single-word terms
are drawn from a corpus of tweets, and include
a small number of hashtag words (e.g., #wan-
tit) and creatively spelled words (e.g., plssss).
However, a majority of the terms are those that
one would use in everyday English.

• Arabic Twitter Set: This dataset includes sin-
gle words and phrases commonly found in Ara-
bic tweets. The phrases in this set are formed
only by combining a negator and a word.

Teams could participate in any one, two, or all
three tasks; however, only one submission was al-

43



Task Total Development set Test set
words phrases all words phrases all

General English Sentiment Modifiers 2,999 101 99 200 1,330 1,469 2,799
English Twitter Mixed Polarity 1,269 60 140 200 358 711 1,069
Arabic Twitter 1,366 167 33 200 1,001 165 1,166

Table 1: The number of single-word and multi-word terms in the development and test sets.

lowed per task. For each task, the above description
and a development set (200 terms) were provided
to the participants in advance; there were no train-
ing sets. The three test sets, one for each task, were
released at the start of the evaluation period. The
test sets and the development sets have no terms in
common. The participants were allowed to use the
development sets in any way (for example, for tun-
ing or training), and they were allowed to use any
additional manually or automatically generated re-
sources.

In 2015, the task was set up similarly (Rosen-
thal et al., 2015). Single words and multi-word
phrases from English Twitter comprised the devel-
opment and test sets (1,515 terms in total). The
phrases were simple two-word negated expressions
(e.g., cant waitttt). Participants were allowed to use
these datasets for the development of their systems.

3 Datasets of English and Arabic Terms
Annotated for Sentiment Intensity

The three datasets, General English Sentiment Mod-
ifiers Set, English Twitter Mixed Polarity Set, and
Arabic Twitter Set, were created through manual an-
notation using an annotation scheme known as Best–
Worst Scaling (described below in Section 3.1). The
terms for each set (domain) were chosen as de-
scribed in Sections 3.2, 3.3, and 3.4, respectively.
Note that the exact sources of data and the term se-
lection procedures were not known to the partici-
pants. The total number of words and phrases in-
cluded in each of the datasets can be found in Ta-
ble 1. Table 2 shows a few example entries from
each set.

3.1 Best–Worst Scaling Method of Annotation

Best–Worst Scaling (BWS), also sometimes referred
to as Maximum Difference Scaling (MaxDiff), is
an annotation scheme that exploits the comparative
approach to annotation (Louviere and Woodworth,

Dataset Sentiment
Term score

General English Sentiment Modifiers Set
favor 0.826
would be very easy 0.715
did not harm 0.597
increasingly difficult 0.208
severe 0.083

English Twitter Mixed Polarity Set
best winter break 0.922
breaking free 0.586
isn’t long enough 0.406
breaking 0.250
heart breaking moment 0.102

Arabic Twitter Set
Ym.× (glory) 0.931�éJ
k. ð 	QË @ �èXAªË@# (marital happiness) 0.900
	á�
�®K
# (certainty) 0.738
	áºÓ@ B (not possible) 0.300

H. AëP@ (terrorism) 0.056
Table 2: Examples of entries with real-valued sentiment scores
from the three datasets.

1990; Cohen, 2003; Louviere et al., 2015). Annota-
tors are given four items (4-tuple) and asked which
item is the Best (highest in terms of the property of
interest) and which is the Worst (least in terms of
the property of interest). These annotations can then
be easily converted into real-valued scores of asso-
ciation between the items and the property, which
eventually allows for creating a ranked list of items
as per their association with the property of interest.
The Best–Worst Scaling method has been shown to
produce reliable annotations of terms for sentiment
(Kiritchenko and Mohammad, 2016a).

Given n terms to be annotated, the first step is
to randomly sample this set (with replacement) to
obtain sets of four terms each, 4-tuples, that satisfy
the following criteria:

1. no two 4-tuples have the same four terms;
2. no two terms within a 4-tuple are identical;

44



3. each term in the term list appears approxi-
mately in the same number of 4-tuples;

4. each pair of terms appears approximately in the
same number of 4-tuples.

The terms for the three tasks were annotated sepa-
rately. For each task, 2×n 4-tuples were generated,
where n is the total number of terms in the task.

Next, the sets of 4-tuples were annotated through
a crowdsourcing platform, CrowdFlower. The
annotators were presented with four terms at a
time, and asked which term is the most positive
(or least negative) and which is the most neg-
ative (or least positive). Below is an example
annotation question.3 (The Arabic data was anno-
tated through a similar questionnaire in Arabic.)

Focus terms:
1. shameless self promotion 2. happy tears 3. hug
4. major pain

Q1: Identify the term that is associated with the most
amount of positive sentiment (or least amount of nega-
tive sentiment) – the most positive term:

1. shameless self promotion
2. happy tears
3. hug
4. major pain

Q2: Identify the term that is associated with the most
amount of negative sentiment (or least amount of posi-
tive sentiment) – the most negative term:

1. shameless self promotion
2. happy tears
3. hug
4. major pain

Each 4-tuple was annotated by at least eight respon-
dents. Let majority answer refer to the option most
chosen for a question. For all three datasets, at least
80% of the responses matched the majority answer.

The responses were then translated into real-
valued scores and also a ranking of terms by sen-
timent for all the terms through a simple counting
procedure: For each term, its score is calculated as
the percentage of times the term was chosen as the
most positive minus the percentage of times the term
was chosen as the most negative (Orme, 2009; Flynn

3The full sets of instructions for both English and
Arabic datasets are available on the shared task website:
http://alt.qcri.org/semeval2016/task7/

and Marley, 2014). For this competition, we con-
verted the scores into the range from 0 (the least pos-
itive) to 1 (the most positive). The resulting rank-
ings constituted the gold annotations for the three
datasets. Finally, random samples of 200 terms from
each dataset with the corresponding gold annota-
tions were released to the participants as develop-
ment sets for the three tasks. The rest of the terms
were kept as test sets.

3.2 General English Sentiment Modifiers
Dataset

The terms for this dataset were taken from the Sen-
timent Composition Lexicon for Negators, Modals,
and Degree Adverbs (SCL-NMA) (Kiritchenko and
Mohammad, 2016b).4 SCL-NMA includes all 1,621
positive and negative words from Osgood’s semi-
nal study on word meaning (Osgood et al., 1957)
available in General Inquirer (Stone et al., 1966). In
addition, it includes 1,586 high-frequency phrases
formed by the Osgood words in combination with
simple negators such as no, don’t, and never, modals
such as can, might, and should, or degree adverbs
such as very and fairly.5 The eligible adverbs
were chosen manually from adverbs that appeared in
combination with an Osgood word at least ten times
in the British National Corpus (BNC)6. Each phrase
includes at least one modal, one negator, or one ad-
verb; a phrase can include several modifiers (e.g.,
would be very happy). Sixty-four different (single
or multi-word) modifiers were used in the dataset.

For this shared task, we removed terms that
were used in the SemEval-2015 dataset. The fi-
nal SemEval-2016 General English Sentiment Mod-
ifiers dataset contains 2,999 terms.

3.3 English Twitter Mixed Polarity Dataset

The terms for this dataset were taken in part from the
Sentiment Composition Lexicon for Opposing Polar-
ity Phrases (SCL-OPP) (Kiritchenko and Moham-

4www.saifmohammad.com/WebPages/SCL.html#NMA
5The complete lists of negators, modals, and degree ad-

verbs used in this dataset are available on the task website:
http://alt.qcri.org/semeval2016/task7/

6The British National Corpus, version 3 (BNC XML Edi-
tion). 2007. Distributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium.
URL: http://www.natcorp.ox.ac.uk/

45



mad, 2016c).7 SCL-OPP was created as follows. We
polled the Twitter API (from 2013 to 2015) to col-
lect a corpus of tweets that contain emoticons: ‘:)’
or ‘:(’. From this corpus, we selected bigrams and
trigrams that had at least one positive word and at
least one negative word. The polarity labels (pos-
itive or negative) of the words were determined by
simple look-up in existing sentiment lexicons: Hu
and Liu lexicon (Hu and Liu, 2004), NRC Emo-
tion lexicon (Mohammad and Turney, 2013), MPQA
lexicon (Wilson et al., 2005), and NRC’s Twitter-
specific lexicon (Kiritchenko et al., 2014).8 Apart
from the requirement of having at least one positive
and at least one negative word, an n-gram must sat-
isfy the following criteria:

• the n-gram must have a clear meaning on its
own, (for example, the n-gram should not start
or end with ‘or’, ‘and’, etc.);

• the n-gram should not include a named entity;

• the n-gram should not include obscene lan-
guage.

In addition, we ensured that there was a good variety
of phrases—for example, even though there were a
large number of bigrams of the form super w, where
w is a negative adjective, only a small number of
such bigrams were included. Finally, we aimed to
achieve a good spread in terms of degree of senti-
ment association (from very negative terms to very
positive terms, and all the degrees of polarity in be-
tween). For this, we estimated the sentiment score of
each phrase using an automatic PMI-based method
described in (Kiritchenko et al., 2014). Then, the
full range of sentiment values was divided into 5
bins, and approximately the same number of terms
were selected from each bin.9

In total, 851 n-grams (bigrams and trigrams) were
selected. We also chose for annotation all unigrams

7www.saifmohammad.com/WebPages/SCL.html#OPP
8If a word was marked with conflicting polarity in two lexi-

cons, then that word was not considered as positive or negative.
For example, the word defeat is marked as positive in Hu and
Liu lexicon and marked as negative in MPQA; therefore, we did
not select any phrases with this word.

9Fewer terms were selected from the middle bin that con-
tained phrases with very weak association to sentiment (e.g.,
phrases like cancer foundation, fair game, and a long nap).

that appeared in the selected set of bigrams and tri-
grams. There were 810 such unigrams.

When selecting the terms, we used sentiment as-
sociations obtained from both manual and automatic
lexicons. As a result, some unigrams had erroneous
sentiment associations. After manually annotating
the full set of 1,661 terms (that include unigrams,
bigrams, and trigrams), we found that 114 bigrams
and 161 trigrams had all their comprising unigrams
of the same polarity. These 275 n-grams were dis-
carded from SCL-OPP but are included in this task
dataset. Further, for this task we removed terms that
were used in the SemEval-2015 dataset or in the
General English set. The final SemEval-2016 En-
glish Twitter Mixed Polarity dataset contains 1,269
terms.

3.4 Arabic Twitter Dataset
Mohammad et al. (2015) automatically generated
three high-coverage sentiment lexicons from Ara-
bic tweets using hashtags and emoticons: Arabic
Emoticon Lexicon, Arabic Hashtag Lexicon, and
Dialectal Arabic Hashtag Lexicon.10 In addition to
Modern Standard Arabic (MSA), these three lexi-
cons comprise terms in Dialectal Arabic as well as
hashtagged compound words, e.g., �éJ
k. ð 	QË @ �èXAªË@#
(#MaritalHappiness), which do not usually appear
in manually created lexicons. Apart from unigrams,
they also include entries for bigrams. From these
lexicons, we selected single words as well as bi-
grams representing negated expressions in the form
of ‘negator w’, where negator is a negation trigger
from a list of 16 common Arabic negation words.11

Words used in negated expressions, but missing
from the original list were also included. The se-
lected terms satisfied the following criteria:

• the terms must occur frequently in tweets;

• the terms should not be highly ambiguous.

We also wanted the set of terms as a whole to have
these properties:

• the set should have a good spread in terms
of degree of sentiment association (from very

10http://saifmohammad.com/WebPages/ArabicSA.html
11The complete list of Arabic negators is available on the task

website: http://alt.qcri.org/semeval2016/task7/

46



Team ID Affiliation
ECNU (Wang et al., 2016) East China Normal University, China
iLab-Edinburgh (Refaee and Rieser, 2016) Heriot-Watt University, UK
LSIS (Htait et al., 2016) Aix-Marseille University, France
NileTMRG (El-Beltagy, 2016a) Nile University, Egypt
UWB (Lenc et al., 2016) University of West Bohemia, Czech Republic

Table 3: The participated teams and their affiliations.

negative terms to very positive terms, and all
the degrees of polarity in between);

• the set should include both standard and dialec-
tal Arabic, Romanized words, misspellings,
hashtags, and other categories frequently used
in Twitter. (We chose not to include URLs, user
mentions, named entities, and obscene terms.)

The final SemEval-2016 Arabic Twitter dataset con-
tains 1,366 terms.

4 Evaluation

Sentiment association scores are most meaningful
when compared to each other; they indicate which
term is more positive than the other. Therefore, the
automatic systems were evaluated in terms of their
abilities to correctly rank the terms by the degree of
sentiment association.

For each task, the predicted sentiment intensity
scores submitted by the participated systems were
evaluated by first ranking the terms according to the
proposed sentiment scores and then comparing this
ranked list to the gold rankings. We used Kendall’s
rank correlation coefficient (Kendall’s τ ) as the of-
ficial evaluation metric to determine the similarity
between the ranked lists (Kendall, 1938):

τ =
c− d

n(n− 1)/2

where c is the number of concordant pairs, i.e., pairs
of terms wi and wj for which both the gold ranked
list and the predicted ranked list agree (either both
lists rank wi higher than wj or both lists rank wi
lower than wj); d is the number of discordant pairs,
i.e., pairs of terms wi and wj for which the gold
ranked list and the predicted ranked list disagree
(one list ranks wi higher than wj and the other list
ranks wi lower than wj); and n is the total number

of terms. If any list ranks two terms wi and wj the
same, this pair of terms is considered neither con-
cordant nor discordant. The values of Kendall’s τ
range from -1 to 1.

We also calculated scores for Spearman’s rank
correlation (Spearman, 1904), as an additional (un-
official) metric.

5 Participated Systems

There were nine submissions from five teams—three
submissions for each task. The team affiliations are
shown in Table 3.

Tables 4 and 5 summarize the approaches and re-
sources used by the participants in the (two) English
and (one) Arabic tasks, respectively. Most teams
applied supervised approaches and trained regres-
sion classifiers using a variety of features. Team
ECNU treated the task as a rank prediction task in-
stead of regression and trained a pair-wise ranking
model with the Random Forest algorithm.

The development data available for each task was
used as the training data by some teams. How-
ever, these data were limited (200 instances per
task); therefore, other manually labeled resources
were also explored. One commonly used resource
was the LabMT lexicon—a set of over 100,000 fre-
quent single words from 10 languages, including
English and Arabic (about 10,000 words in each lan-
guage), manually annotated for happiness through
Mechanical Turk (Dodds et al., 2011; Dodds et al.,
2015). For the Arabic task, two teams took advan-
tage of the Arabic Twitter corpus collected by Re-
faee and Rieser (2014). The features employed in-
clude sentiment scores obtained from different senti-
ment lexicons, general and sentiment-specific word
embeddings, pointwise mutual information (PMI)
scores between terms (single words and multi-word
phrases) and sentiment classes, as well as lists of
negators, intensifiers, and diminishers.

47



Team Supervision Algorithm Training Sentiment External corpora and
name data lexicons used other resources used
ECNU supervised Random LabMT, Hu and Liu, MPQA 1.6M tweets (with emoticons)

Forest dev. data
LSIS unsupervised PMI - NRC Emoticon, 10K tweets (with manually

SentiWordNet, MPQA annotated sentiment phrases)
UWB supervised Gaussian dev. data AFINN, JRC pre-trained word2vec embeddings,

regression pre-trained sentiment classifier

Table 4: Summary of the approaches for the two English-language tasks.

Team Supervision Algorithm Training data Sentiment External corpora and
name lexicons used other resources used
iLab- supervised linear regress., LabMT, Arabic ArabSenti, MPQA, 9K tweets (manually
Edinburgh manual rules Twitter corpus Dialectal Arabic labeled for sentiment)
LSIS unsupervised PMI - NRC Emotion 12K tweets (manually

labeled for sentiment),
63K book reviews
(5-star ratings)

NileTMRG supervised regression, dev. data, NileULex 250K tweets (unlabeled),
PMI Arabic Twitter pre-trained sentiment

corpus classifier

Table 5: Summary of the approaches for the Arabic-language task.

Only one team, LSIS, employed an unsupervised
approach to all three tasks. To predict a senti-
ment intensity score for a term, they used the fol-
lowing three sources: existing sentiment lexicons,
PMI scores between terms and sentiment classes
computed on sentiment-annotated corpora, and PMI
scores between terms and words poor and excellent
computed on Google search results.

All teams heavily relied on existing sentiment lex-
icons: AFINN (Nielsen, 2011), ArabSenti (Abdul-
Mageed et al., 2011), Hu and Liu (Hu and Liu,
2004), Dialectal Arabic Lexicon (Refaee and Rieser,
2014), JRC (Steinberger et al., 2012), MPQA (Wil-
son et al., 2005), NRC Emoticon (a.k.a. Senti-
ment140) (Kiritchenko et al., 2014), NRC Emo-
tion (Mohammad and Turney, 2013), NileULex (El-
Beltagy, 2016b), and SentiWordNet (Esuli and Se-
bastiani, 2006). (Note that even though the NRC
Emotion Lexicon was created for English terms, its
translations in close to 40 languages, including Ara-
bic, are available.12)

12http://saifmohammad.com/WebPages/NRC-Emotion-
Lexicon.htm

6 Results

The results for the three tasks are presented in Ta-
bles 6, 7, and 8. Team ECNU showed the best
performance in both English-language tasks. In the
Arabic task, the best performing system was devel-
oped by iLab-Edinburgh.

A few observations can be made from the results:

• On all three datasets, the team rankings based
on the two metrics, Kendall’s τ and Spearman’s
ρ, are the same.

• For most of the teams, the results obtained on
the General English Sentiment Modifiers set
are markedly higher than the results obtained
on the other datasets.

• The English Twitter Mixed Polarity set proved
to be a challenging task for all teams. We have
further analyzed regularities present in differ-
ent kinds of mixed polarity phrases and con-
cluded that for most phrases the sentiment of
the phrase cannot be reliably predicted only
from the parts of speech and polarities of their

48



Team Overall Single words Multi-word phrases
Kendall’s τ Spearman’s ρ Kendall’s τ Spearman’s ρ Kendall’s τ Spearman’s ρ

ECNU 0.704 0.863 0.734 0.884 0.686 0.845
UWB 0.659 0.854 0.644 0.846 0.657 0.849
LSIS 0.350 0.508 0.421 0.599 0.324 0.462

Table 6: Results for General English Sentiment Modifiers test set. The systems are ordered by their overall Kendall’s τ score,
which was the official competition metric. The highest score is shown in bold.

Team Overall Single words Multi-word phrases
Kendall’s τ Spearman’s ρ Kendall’s τ Spearman’s ρ Kendall’s τ Spearman’s ρ

ECNU 0.523 0.674 0.601 0.747 0.494 0.646
LSIS 0.422 0.591 0.384 0.543 0.423 0.593
UWB 0.414 0.578 0.564 0.752 0.366 0.524

Table 7: Results for English Twitter Mixed Polarity test set. The systems are ordered by their overall Kendall’s τ score, which
was the official competition metric. The highest score is shown in bold.

Team Overall Single words Multi-word phrases
Kendall’s τ Spearman’s ρ Kendall’s τ Spearman’s ρ Kendall’s τ Spearman’s ρ

iLab-Edinburgh 0.536 0.680 0.592 0.739 -0.046 -0.069
NileTMRG 0.475 0.658 0.510 0.701 0.078 0.118
LSIS 0.424 0.583 0.478 0.646 0.059 0.088

Table 8: Results for Arabic Twitter test set. The systems are ordered by their overall Kendall’s τ score, which was the official
competition metric. The highest score is shown in bold.

constituent words (Kiritchenko and Moham-
mad, 2016d). For example, a positive adjective
and a negative noun can form either a positive
phrase (e.g., happy tears) or a negative phrase
(e.g., great loss).

• The results achieved on the Arabic Twitter
test set are substantially lower than the results
achieved on a similar English Twitter data used
in the 2015 competition.

• For most teams, the results obtained on sin-
gle words are noticeably higher than the corre-
sponding results on multi-word phrases. This is
especially apparent on the Arabic Twitter data.
The possible reason for this outcome is the lack
of sufficient training data for phrases; none of
the existing manually created English or Arabic
real-valued sentiment lexicons provide annota-
tions for multi-word phrases.

Overall, we observe strong correlations between
the predicted and gold term rankings for terms in the
general English domain as well as for single words
in the other two domains. However, for multi-word
phrases in the English Mixed Polarity set and Ara-

bic Twitter set the correlations are markedly weaker,
especially for the Arabic language. We hope that
the availability of these datasets will foster further
research towards automatic methods for sentiment
composition in English and other languages.

7 Conclusions

We have created three sentiment composition lex-
icons that provide real-valued sentiment associa-
tion scores for multi-word phrases and their con-
stituent single words in three domains: the Gen-
eral English Sentiment Modifiers Set, the English
Twitter Mixed Polarity Set, and the Arabic Twit-
ter Set. The terms were annotated manually us-
ing the Best–Worst Scaling method of annotation.
We included phrases composed of negators, modals,
and degree adverbs—categories known to be chal-
lenging for sentiment analysis. Furthermore, we in-
cluded phrases formed by words with opposing po-
larities. As future work, we would like to extend the
task to cover more domains (e.g., biomedical, legal)
and more languages. All datasets are freely available
to the research community.

49



References
Muhammad Abdul-Mageed and Mona Diab. 2014.

SANA: A large scale multi-genre, multi-dialect lexi-
con for Arabic subjectivity and sentiment analysis. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC).

Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and sentiment
analysis of Modern Standard Arabic. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 587–591.

Steven H. Cohen. 2003. Maximum difference scaling:
Improved measures of importance and preference for
segmentation. Sawtooth Software, Inc.

Peter Sheridan Dodds, Kameron Decker Harris, Isabel M.
Kloumann, Catherine A. Bliss, and Christopher M.
Danforth. 2011. Temporal patterns of happiness and
information in a global social network: Hedonomet-
rics and Twitter. PloS One, 6(12):e26752.

Peter Sheridan Dodds, Eric M. Clark, Suma Desu,
Morgan R. Frank, Andrew J. Reagan, Jake Ryland
Williams, Lewis Mitchell, Kameron Decker Harris, Is-
abel M. Kloumann, James P. Bagrow, et al. 2015.
Human language reveals a universal positivity bias.
Proceedings of the National Academy of Sciences,
112(8):2389–2394.

Samhaa R. El-Beltagy. 2016a. NileTMGR at SemEval-
2016 Task 7: Deriving prior polarities for Arabic senti-
ment terms. In Proceedings of the International Work-
shop on Semantic Evaluation (SemEval).

Samhaa R. El-Beltagy. 2016b. NileULex: A phrase and
word level sentiment lexicon for Egyptian and Mod-
ern Standard Arabic. In Proceedings of the Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).

Ramy Eskander and Owen Rambow. 2015. SLSA: A
sentiment lexicon for Standard Arabic. In Proceed-
ings of the 2015 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 2545–
2550, Lisbon, Portugal.

Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation (LREC),
pages 417–422.

T. N. Flynn and A. A. J. Marley. 2014. Best-worst scal-
ing: theory and methods. In Stephane Hess and An-
drew Daly, editors, Handbook of Choice Modelling,
pages 178–201. Edward Elgar Publishing.

Marcus Hartner. 2013. The lingering after-effects in the
reader’s mind – an investigation into the affective di-
mension of literary reading. Journal of Literary The-
ory Online.

Amal Htait, Sebastien Fournier, and Patrice Bellot. 2016.
LSIS at SemEval-2016 Task 7: Using web search en-
gines for English and Arabic unsupervised sentiment
intensity prediction. In Proceedings of the Interna-
tional Workshop on Semantic Evaluation (SemEval).

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD), pages 168–
177, New York, NY, USA.

Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, pages 81–93.

Svetlana Kiritchenko and Saif M. Mohammad. 2016a.
Capturing reliable fine-grained sentiment associations
by crowdsourcing and best–worst scaling. In Pro-
ceedings of The 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL), San Diego, California.

Svetlana Kiritchenko and Saif M. Mohammad. 2016b.
The effect of negators, modals, and degree adverbs on
sentiment composition. In Proceedings of the Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis (WASSA).

Svetlana Kiritchenko and Saif M. Mohammad. 2016c.
Happy accident: A sentiment composition lexicon for
opposing polarity phrases. In Proceedings of the In-
ternational Conference on Language Resources and
Evaluation (LREC).

Svetlana Kiritchenko and Saif M. Mohammad. 2016d.
Sentiment composition of words with opposing polar-
ities. In Proceedings of The 15th Annual Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL), San Diego, California.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. Journal of Artificial Intelligence Research,
50:723–762.

Jochen Kleres. 2011. Emotions and narrative analysis:
A methodological approach. Journal for the Theory of
Social Behaviour, 41(2):182–202.

Ladislav Lenc, Pavel Krl, and Vclav Rajtmajer. 2016.
UWB at SemEval-2016 Task 7 : Novel method for
automatic sentiment intensity determination. In Pro-
ceedings of the International Workshop on Semantic
Evaluation (SemEval).

Jordan J. Louviere and George G. Woodworth. 1990.
Best-worst analysis. Working Paper. Department of
Marketing and Economic Analysis, University of Al-
berta.

Jordan J. Louviere, Terry N. Flynn, and A. A. J. Marley.
2015. Best-Worst Scaling: Theory, Methods and Ap-
plications. Cambridge University Press.

50



Atsunori Minamikawa and Hiroyuki Yokoyama. 2011.
Personality estimation based on weblog text classifi-
cation. In Modern Approaches in Applied Intelligence,
pages 89–97. Springer.

Saif M. Mohammad and Svetlana Kiritchenko. 2015.
Using hashtags to capture fine emotion categories from
tweets. Computational Intelligence, 31(2):301–326.

Saif M. Mohammad and Peter D. Turney. 2013. Crowd-
sourcing a word–emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the International Workshop on Semantic Evaluation
(SemEval), Atlanta, Georgia, USA, June.

Saif M Mohammad, Mohammad Salameh, and Svetlana
Kiritchenko. 2015. How translation alters sentiment.
Journal of Artificial Intelligence Research, 55:95–130.

Saif M. Mohammad, Svetlana Kiritchenko, Parinaz Sob-
hani, Xiaodan Zhu, and Colin Cherry. 2016a. A
dataset for detecting stance in tweets. In Proceed-
ings of 10th edition of the the Language Resources and
Evaluation Conference (LREC), Portorož, Slovenia.

Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kir-
itchenko. 2016b. Stance and sentiment in tweets. Spe-
cial Section of the ACM Transactions on Internet Tech-
nology on Argumentation in Social Media, Submitted.

Saif M Mohammad. 2012. From once upon a time
to happily ever after: Tracking emotions in mail and
books. Decision Support Systems, 53(4):730–741.

Finn Årup Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC-2011 Workshop on ’Making
Sense of Microposts’: Big things come in small pack-
ages, pages 93–98.

Bryan Orme. 2009. Maxdiff analysis: Simple counting,
individual-level logit, and HB. Sawtooth Software,
Inc.

Charles E Osgood, George J Suci, and Percy Tannen-
baum. 1957. The measurement of meaning. Univer-
sity of Illinois Press.

Maria Pontiki, Harris Papageorgiou, Dimitrios Galanis,
Ion Androutsopoulos, John Pavlopoulos, and Suresh
Manandhar. 2014. SemEval-2014 Task 4: Aspect
based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval), Dublin, Ireland.

Eshrag Refaee and Verena Rieser. 2014. An Arabic
Twitter corpus for subjectivity and sentiment analysis.
In Proceedings of the 9th International Conference on
Language Resources and Evaluation (LREC).

Eshrag Refaee and Verena Rieser. 2016. iLab-Edinburgh
at SemEval-2016 Task 7: A hybrid approach for deter-
mining sentiment intensity of Arabic Twitter phrases.

In Proceedings of the International Workshop on Se-
mantic Evaluation (SemEval).

Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif Mohammad, Alan Ritter, and Veselin Stoyanov.
2015. SemEval-2015 Task 10: Sentiment analysis in
Twitter. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval), pages 450–
462, Denver, Colorado.

Charles Spearman. 1904. The proof and measurement of
association between two things. The American Jour-
nal of Psychology, 15(1):72–101.

Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia Vázquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems,
53(4):689–694.

Philip Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and associates. 1966. The General
Inquirer: A Computer Approach to Content Analysis.
The MIT Press.

Feixiang Wang, Zhihua Zhang, and Man Lan. 2016.
ECNU at SemEval-2016 Task 7: An enhanced super-
vised learning method for lexicon sentiment intensity
ranking. In Proceedings of the International Work-
shop on Semantic Evaluation (SemEval).

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347–354,
Stroudsburg, PA, USA.

51


