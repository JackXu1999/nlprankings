Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 590–598,

Beijing, August 2010

590

Automatic generation of inter-passage links based on semantic similarity

Petr Knoth, Jakub Novotny, Zdenek Zdrahal

Knowledge Media Institute

The Open University

p.knoth@open.ac.uk

Abstract

This paper investigates the use and the
prediction potential of semantic similar-
ity measures for automatic generation of
links across different documents and pas-
sages. First, the correlation between the
way people link content and the results
produced by standard semantic similarity
measures is investigated. The relation be-
tween semantic similarity and the length
of the documents is then also analysed.
Based on these ﬁndings a new method for
link generation is formulated and tested.

Introduction

1
Text retrieval methods are typically designed to
ﬁnd documents relevant
to a query based on
some criterion, such as BM25 or cosine similar-
ity (Manning et al., 2008). Similar criteria have
also been used to identify documents relevant to
the given reference document, thus in principle
linking the reference document to the related doc-
uments (Wilkinson and Smeaton, 1999). This pa-
per studies the correspondence between the results
of this approach and the way linking is performed
by people. The study conﬁrms that the length of
documents is an important factor usually causing
the quality of current link generation approaches
to deteriorate. As a result, methods working at
a ﬁner granularity than documents should be in-
vestigated. This will also improve the speed of
access to information. For example, when users
read through a long document, they should be able
to quickly access a passage in another possibly
This work has been partially supported by Eurogene -

Contract no. ECP-2006-EDU-410018)

long document related to the discussed topic. The
automatic detection of document pairs containing
highly related passages is the task addressed in
this paper.

A number of approaches for automatic link
generation have used measures of semantic sim-
ilarity. While these measures were widely used
for the discovery of related documents in prac-
tise, their correspondence to the way people link
content has not been sufﬁciently investigated (see
Section 2). As our contribution to this topic, we
present in this paper an approach which tries to
ﬁrst investigate this correspondence on a large text
corpus. The resulting method is then motivated by
the outcomes of this analysis.

It has been recognised in information retrieval
that when a collection contains long documents,
better performance is often achieved by breaking
each document into subparts or passages and com-
paring these rather than the whole documents to a
query (Manning et al., 2008). A suitable granular-
ity of the breakdown is dependent on a number of
circumstances, such as the type of the document
collection or the information need. In this work,
we have decided to work at the level of documents
and paragraphs. Our task can be formalized as a
two-step process:

1. Given a collection of documents, our goal is
to identify candidate pairs of documents be-
tween which a link may be induced.

2. Given each candidate pair of documents, our
task is to identify pairs of passages, such that
the topics in the passages are related in both
documents.

The method presented in this paper has many

591

potential applications. First, it may be used for the
interlinking of resources that were not originally
created as hypertext documents and for the main-
tenance or the discovery of new links as the collec-
tion grows. Second, the method can be applied to
improve navigation in collections with long texts,
such as books or newspaper articles. A link may
be identiﬁed by the system automatically and the
user can be pointed immediately to the part of the
text which is relevant to the block of text currently
being read. Similar application has been devel-
oped by (Kolak and Schilit, 2008) who provided a
method for mining repeated word sequences (quo-
tations) from very large text collections and inte-
grated it with the Google Books archive. Other
application areas may involve text summarization
and information retrieval.

The paper makes the following contributions:
• It provides a new interpretation and insight
in the use of semantic similarity measures for
the automatic generation of links.

• It develops a novel two-step approach for
the discovery of passage-passage links across
potentially long documents and it identiﬁes
and discusses the selection of the parameters.
The rest of the paper is organized as follows.
Section 2 presents the related work in the ﬁeld.
Section 3 discusses the data selected for our exper-
iment and Section 4 describes how the data were
processed in order to perform our investigation. In
Section 5, the analysis in which we compared the
results produced by semantic similarity measures
with respect to the way people link content is pre-
sented. Section 6 then draws on this analysis and
introduces the method for automatic generation of
links which is ﬁnally evaluated in Section 7.

2 Related Work
In the 1990s, the main application area for link
generation methods were hypertext construction
systems. A survey of these methods is pro-
vided by (Wilkinson and Smeaton, 1999).
In
the last decade, methods for ﬁnding related docu-
ments became the de-facto standard in large digi-
tal repositories, such as PubMed or the ACM Dig-
ital Library. Search engines including Google also
generate links to related pages or research articles.

Generating links pointing to units of a smaller
granularity than a document, which can be con-
sidered as a task of passage or focused retrieval,
has also been addressed recently. In this task, the
system locates the relevant information inside the
document instead of only providing a link to the
document. The Initiative for the Evaluation of
XML retrieval (INEX) started to play an essential
role in link generation by providing tracks for the
evaluation of link generation systems (Huang et
al., 2008; Huang et al., 2009) using the Wikipedia
collection at both the document and the passage
level.

Current approaches can be divided into three
groups: (1) link-based approaches discover new
links by exploiting an existing link graph (Itakura
and Clarke, 2008; Jenkinson et al., 2008; Lu et
al., 2008). (2) semi-structured approaches try to
discover new links using semi-structured informa-
tion, such as the anchor texts or document titles
(Geva, 2007; Dopichaj et al., 2008; Granitzer et
al., 2008).
(3) purely content-based approaches
use as an input plain text only. They typically
discover related resources by calculating seman-
tic similarity based on document vectors (Allan,
1997; Green, 1998; Zeng and Bloniarz, 2004;
Zhang and Kamps, 2008; He, 2008). Some of the
mentioned approaches, such as (Lu et al., 2008),
combine multiple approaches.

Although link generation methods are widely
used in practise, more work is needed to under-
stand which features contribute to the quality of
the generated links. Work in this area includes the
study of (Green, 1999) who investigated how lex-
ical chaining based on ontologies can contribute
to the quality of the generated links, or the exper-
iments of (Zeng and Bloniarz, 2004) who com-
pared the impact of the manually and automati-
cally extracted keywords. There has also been ef-
fort in developing methods that can in addition to
link generation assign a certain semantic type to
the extracted links and thus describe the relation-
ship between documents (Allan, 1997).

The method presented in this paper is purely
content-based and therefore is applicable in any
text collection. Its use in combination with link-
based or semi-structured approaches is also pos-
sible. The rationale for the method comes from

592

the analysis of the prediction potential of semantic
similarity for automatic link generation presented
in Section 5. Related analysis is presented in (He,
2008) which claims that linked articles are more
likely to be semantically similar1, however, the
study does not provide sufﬁcient evidence to con-
ﬁrm and describe this relationship. In link genera-
tion, we are more interested in asking the opposite
question, i.e. whether articles with higher seman-
tic similarity are more likely to be linked. Our
study provides a new insight into this relationship
and indicates that the relationship is in fact more
complex than originally foreseen by He.

3 Data selection
This section introduces the document collection
used for the analysis and the experiments. The
following properties were required for the docu-
ment collection to be selected for the experiments.
First, in order to be able to measure the correla-
tion between the way people link content and the
results produced by semantic similarity measures,
it was necessary to select a document collection
which can be considered as relatively well inter-
linked. Second, it was important for us to work
with a collection containing a diverse set of top-
ics. Third, we required the collection to contain
articles of varied length. We were mostly inter-
ested in long documents, which create conditions
for the testing of passage retrieval methods. We
decided to use the Wikipedia collection, because
it satisﬁes all our requirements and has also been
used in the INEX Link-The-Wiki-Track.

Wikipedia consists of more than four million
pages spread across ﬁve hundred thousands cat-
egories. As it would be for our calculation un-
necessarily expensive to work with the whole en-
cyclopedia, a smaller, but still a sufﬁciently large
subset of Wikipedia, which satisﬁes our require-
ments of topic diversity and document length, was
selected. Our document collection was generated
from articles in categories containing the words
United Kingdom. This includes categories, such
as United Kingdom, Geography of United King-
dom or History of the United Kingdom. There
are about 3,000 such categories and 57,000 dis-
tinct articles associated to them. As longer arti-

1With respect to the cosine similarity measure.

cles provide better test conditions for passage re-
trieval methods, we selected the 5,000 longest ar-
ticles out of these 57,000. This corresponds to a
set where each article has the length of at least
1,280 words.

4 Data preprocessing
Before discussing the analysis performed on the
document collection, let us brieﬂy describe how
the documents were processed and the semantic
similarity calculated.

First,

the N articles/documents D =
{d1, d2, . . . , dN} in our collection were prepro-
cessed to extract plain text by removing the Wiki
markup. The documents were then tokenized and
a dictionary of terms T = {t1, t2, . . . , tM} was
created. Assuming that the order of words can
be neglected (the bag-of-words assumption) the
document collection can be represented using
In this way,
a N × M term-document matrix.
each document is modelled as a vector corre-
sponding to a particular row of the matrix. As it
is inefﬁcient to represent such a sparse vector in
memory (most of the values are zeros), only the
non-zero values were stored. Term frequency -
inverse document frequency (tﬁdf) weighting was
used to calculate the values of the matrix. Term
is a normalized frequency of
frequency tfti,dj
term ti in document dj:

tfti,dj =

f (ti, dj)

Pk f (tk, dj)

Inverse document frequency idfti measures the
general importance of term ti in the collection of
documents D by counting the number of docu-
ments which contain term ti:

|D|

idfti = log

|dj : ti ∈ dj|
tf idfti,dj = tfti,dj .idfti

Similarity is then deﬁned as the function
sim(−→x ,−→y ) of the document vectors −→x and −→y .
There exists a number of similarity measures used
for the calculation of similarity between two vec-
tors (Manning and Schuetze, 1999), such as co-
sine, overlap, dice or Jaccard measures. Some
studies employ algorithms for the reduction of di-
mensions of the vectors prior to the calculation

593

of similarity to improve the results. These ap-
proaches may involve techniques, such as lexical
chaining (Green, 1999), Latent Semantic Indexing
(Deerwester et al., 1990), random indexing (Wid-
dows and Ferraro, 2008) and Latent Dirichlet Al-
location (Blei et al., 2003). In this work we inten-
tionally adopted perhaps the most standard sim-
ilarity measure - cosine similarity calculated on
the tﬁdf vectors and no dimensionality reduction
technique was used. The formula is provided for
completeness:

simcosine(−→x ,−→y ) = −→x .−→y
|x|.|y|

Cosine similarity with tﬁdf vectors has been
previously used in automatic link generation sys-
tems producing state-of-the-art results when com-
pared to other similarity measures (Chen et al.,
2004). This allows us to report on the effective-
ness of the most widely used measure with respect
to the way the task is completed by people. While
more advanced techniques might be in some cases
better predictors for link generation, we did not
experiment with them as we preferred to focus
on the investigation of the correlation between the
most widely used measure and manually created
links. Such study has to our knowledge never been
done before, but it is necessary for the justiﬁcation
of automatic link generation methods.

5 Semantic similarity as a predictor for

link generation

The document collection described in Section 3
has been analysed as follows. First, pair-wise
similarities using the formulas described in Sec-
tion 4 were calculated. Cosine similarity is a
symmetric function and, therefore, the calculation
of all inter-document similarities in the dataset
of 5, 000 documents requires the evaluation of
5,0002
2 − 5, 000 = 12, 495, 000 combinations. Fig-
ure 1 shows the distribution of the document pairs
(on a log10 scale) with respect to their similarity
value. The frequency follows a power law distri-
bution. In our case, 99% of the pairs have similar-
ity lower than 0.1.

To compare the semantic similarity measures
with the links created by Wikipedia authors, all
inter-document intra-collection links, i.e.
links

Figure 1: The histogram shows the number of
document pairs on a log10 scale (y-axis) with re-
spect to their cosine similarity (x-axis).

created by users of Wikipedia commencing from
and pointing to a document within our collection,
were extracted. These links represent the connec-
tions as seen by the users regardless of their direc-
tion. Each of these links can be associated with
a similarity value calculated in the previous step.
Documents with similarity lower than 0.1 were ig-
nored. Out of the 120, 602 document pairs with
inter-document similarity higher than 0.1, 17, 657
pairs were also connected by a user-created link.
For the evaluation, interval with cosine simi-
larity [0.1, 1] was divided evenly into 100 buck-
ets and all 120,602 document pairs were assigned
to the buckets according their similarity values.
From the distribution shown in Figure 1, buckets
corresponding to higher similarity values contain
fewer document pairs than buckets corresponding
to smaller similarity values. Therefore, for each
bucket, the number of user created links within
the bucket was normalized by the number of doc-
ument pairs in the bucket. This number is the like-
lihood of the document pair being linked and will
be called linked-pair likelihood. The relation be-
tween semantic similarity and linked-pair likeli-
hood is shown in Figure 2.

As reported in Section 2, semantic similarity
has been previously used as a predictor for the
automatic generation of links. The typical sce-
nario was that the similarity between pairs of doc-
uments was calculated and the links between the

594

Figure 2: The linked-pair likelihood (y-axis) with
respect to the cosine similarity (x-axis).

most similar documents were generated (Wilkin-
son and Smeaton, 1999). If this approach was cor-
rect, we would expect the curve shown in Figure 2
to be monotonically increasing. However, the re-
lation shown in Figure 2 is in accordance with our
expectations only up to the point 0.55. For higher
values of inter-document similarity the linked-pair
likelihood does not rise or it even decreases.

Spearman’s rank correlation and Pearson corre-
lation were applied to estimate the correlation co-
efﬁcients and to test the statistical signiﬁcance of
our observation. This was performed in two inter-
vals: [0, 0.55] and [0.55, 1]. A very strong positive
correlation 0.986 and 0.987 have been received
in the ﬁrst interval for the Spearman’s and Pear-
son coefﬁcients respectively. A negative correla-
tion −0.640 and −0.509 have been acquired for
the second interval again for the Spearman’s and
Pearson coefﬁcients respectively. All the mea-
sured correlations are signiﬁcant for p-value well
beyond p < 0.001. Very similar results have been
achieved using different collections of documents.
The results indicate that high similarity value
is not necessarily a good predictor for automatic
link generation. A possible explanation for this
phenomenon is that people create links between
related documents that provide new information
and therefore do not link nearly identical content.
However, as content can be in general linked for
various purposes, more research is needed to in-
vestigate if document pairs at different similarity
levels also exhibit different qualitative properties.

Figure 3: The average cosine similarity (y-axis) of
document pairs of various length (x-axis) between
which there exists a link. The x-axis is calculated
as a log10(l1.l2)

More speciﬁcally, can the value of semantic sim-
ilarity be used as a predictor for relationship typ-
ing?

An important property of semantic similarity
as a measure for automatic generation of links is
the robustness with respect to the length of doc-
uments. As mentioned in Section 4, cosine sim-
ilarity is by deﬁnition normalized by the product
of the documents length. Ideally the cosine sim-
ilarity should be independent of the documents
length. To verify this in our dataset, we have taken
pairs of documents between which Wikipedia
users assigned links and divided them into buckets
with respect to the function log10(l1.l2), where l1
and l2 are the lengths of the two documents in the
document pair and the logarithm is used for scal-
ing. The value of each bucket was calculated as an
average similarity of the bucket members. The re-
sults are shown in Figure 3. The graph shows that
the average similarity value is slightly decreasing
with respect to the length of the articles. Val-
ues −0.484 and −0.231 were obtained for Spear-
man’s and Pearson correlation coefﬁcients respec-
tively. Both correlations are statistically signif-
icant for p < 0.001. A much stronger correla-
tion was measured for Spearman’s than for Pear-
son which can be explained by the fact that Spear-
man’s correlation is calculated based on ranks
rather than real values and is thus less sensitive
to outliers.

595

Our experience from repeating the same experi-
ment on another Wikipedia subset generated from
categories containing the word Geography tells us
that the decrease is even more noticeable when
short and long articles are combined. The de-
crease in average similarity suggests that if co-
sine similarity is used for the automatic gener-
ation of links then document pairs with higher
value of l1.l2 have a higher linked-pair likelihood
than pairs with a smaller value of this quantity.
In other words, links created between documents
with small l1.l2 typically exhibit a larger value
of semantic similarity than links created between
documents with high value of l1.l2. Although the
decrease may seem relatively small, we believe
that this knowledge may be used for improving
automatic link generation methods by adaptively
modifying the thresholds with respect to the l1.l2
length.
6 Link generation method
In this section we introduce the method for the au-
tomatic generation of links. The method can be
divided into two parts (1) Identiﬁcation of candi-
date link pairs (i.e. the generation of document-to-
document links) (2) Recognition of passages shar-
ing a topic between the two documents (i.e.
the
generation of passage-to-passage links).
6.1 Document-to-document links
The algorithm for link generation at the granular-
ity of a document is motivated by the ﬁndings re-
ported in Section 5.

Algorithm 1: Generate document links
Input: A set of document vectors D,
min. sim. α, max. sim. β ∈ [0, 1], C = ∅
Output: A set C of candidate links
of form hdi, dj, simi ∈ C where di and dj are
documents and sim ∈ [0, 1] is their similarity
1.for each {hdi, dji|i, j ∈ ℵ0 ∧ i < j < |D|} do
2.

simdi,dj := similarity(di, dj)
if simdi,dj > α ∧ simdi,dj < β then

C := C ∪ hdi, dj, simdi,dji

3.

4.

The algorithm takes as the input a set of doc-
ument vectors and two constants - the minimum

and maximum similarity thresholds - and iterates
over all pairs of document vectors. It outputs all
document vector pairs, such that their similarity is
higher than α and smaller than β. For well chosen
β, the algorithm does not generate links between
nearly duplicate pairs. If we liked to rank the dis-
covered links according to the conﬁdence of the
system, we would suggest to assign each pair a
value using the following function.

rankdi,dj = |simdi,dj − (α +

β − α

2

)|

The ranking function makes use of the fact that
the system is most conﬁdent in the middle of the
similarity region deﬁned by constants α and β, un-
der the assumption that suitable values for these
constants are used. The higher the rank of a docu-
ment pair, the better the system’s conﬁdence.

6.2 Passage-to-passage links
Due to a high number of combinations, it is typ-
ically infeasible even for relatively small collec-
tions to generate passage-to-passage links across
documents directly. However, the complexity of
this task is substantially reduced when passage-to-
passage links are discovered in a two-step process.

Algorithm 2: Generate passage links
Input: Sets Pi, Pj of paragraph document
vectors for each pair in C
min. sim. γ, max. sim. δ ∈ [0, 1] such that
α < γ ∧ β < δ, , L = ∅
Output: A set L of passage links
of form hpki, plj , simi ∈ L where pki and
plj are paragraphs in documents di, dj
and sim ∈ [0, 1] is their similarity
1.for each {hpki, plji|pki ∈ Pi, plj ∈ Pj} do
2.

simpki ,plj
if simpki ,plj

:= similarity(pki, plj )
> γ ∧ simpki ,plj

< δ then

L := L ∪ hpki, plj , simpki ,plji

3.

4.

As Section 5 suggests, the results of Algorithm
1 may be improved by adaptive changing of the
thresholds α and β based on the length of the doc-
ument vectors. More precisely, in the case of co-
sine similarity, this is the quantity lr = l1.l2. The

596

value α should be higher (β lower) for pairs with
low lr than for pairs with high lr and vice versa.
Although the relative quantiﬁcation of this ratio is
left for future work, we believe that we can ex-
ploit these ﬁndings for the generation of passage-
to-passage links.

More speciﬁcally, we know that the length of
passages (paragraphs in our case) is lower than the
length of the whole documents. Hence, the sim-
ilarity of a linked passage-to-passage pair should
be on average higher than the similarity of a linked
document-to-document pair, as revealed by the
results of our analysis. This knowledge is used
within Algorithm 2 to set the parameters γ and
δ. The algorithm shows, how passage-to-passage
links are calculated for a single document pair
previously identiﬁed by Algorithm 1. Applying
the two-step process allows the discovery of doc-
ument pairs, which are likely to contain strongly
linked passages, at lower similarity levels and to
recognize the related passages at higher similarity
levels while still avoiding duplicate content.

7 Results
The experimental evaluation of the methods pre-
sented in Section 6 is divided into two parts:
(1) the evaluation of document-to-document links
(Algorithm 1) and (2) the evaluation of passage-
to-passage links (Algorithm 2).

7.1 Evaluation of document-to-document

links

As identiﬁed in Section 5 (and shown in Figure 2),
the highest linked-pair likelihood does not occur
at high similarity values, but rather somewhere be-
tween similarity 0.5 and 0.7. According to Figure
2, the linked-pair likelihood in this similarity re-
gion ranges from 60% to 70%. This value is in our
view relatively high and we think that it can be ex-
plained by the fact that Wikipedia articles are un-
der constant scrutiny by users who eventually dis-
cover most of the useful connections. However,
how many document pairs that could be linked
in this similarity region have been missed by the
users? That is, how much can our system help in
the discovery of possible connections?

Suppose that our task would be to ﬁnd docu-
ment pairs about linking of which the system is

most certain. In that case we would set the thresh-
olds α and β somewhere around these values de-
pending on how many links we would like to ob-
tain.
In our evaluation, we have extracted pairs
of documents from the region between α = 0.65
and β = 0.70 regardless of whether there origi-
nally was a link assigned by Wikipedia users. An
evaluation tool which allowed a subject to display
the pair of Wiki documents next to each other and
to decide whether there should or should not be a
link between the documents was then developed.
We did not inform the subject about the existence
or non-existence of links between the pages. More
speciﬁcally, the subject was asked to decide yes
(link generated correctly) if and only if they found
it beneﬁcial for a reader of the ﬁrst or the sec-
ond article to link them together regardless of the
link direction. The subject was asked to decide no
(link generated incorrectly) if and only if they felt
that navigating the user from or to the other doc-
ument does not provide additional value. For ex-
ample, in cases where the relatedness of the doc-
uments is based on their lexical rather than their
semantic similarity.

The study revealed that 91% of the generated
links were judged by the subject as correct and
9% as incorrect. Table 1 shows the results of the
experiment with respect to the links originally as-
signed by the users of Wikipedia. It is interest-
ing to notice that in 3% of the cases the subject
decided not to link the articles even though they
were in fact linked on Wikipedia. Overall, the al-
gorithm discovered in 30% of the cases a useful
connection which was missing in Wikipedia. This
is in line with the ﬁndings of (Huang et al., 2008)
who claims that the validity of existing links in
Wikipedia is sometimes questionable and useful
links may be missing.

An interesting situation in the evaluation oc-
curred when the subject discovered a pair of ar-
ticles with titles Battle of Jutland and Night Ac-
tion at the Battle of Jutland. The Wikipedia page
indicated that it is an orphan and asked users of
Wikipedia to link it to other Wikipedia articles.
Our method would suggest the ﬁrst article as a
good choice.

597

Wikipedia link
yes
0.61
0.03

no
0.30
0.06

Subject’s
decision

yes
no

Table 1: Document-to-document links from the
[0.65, 0.7] similarity region. The subject’s deci-
sion in comparison to the Wikipedia links.

Subject’s decision
at page level

yes
no

Wikipedia link
yes
0.16
0.18

no
0.10
0.56

Table 2: Document-to-document candidate links
generation from the [0.2, 0.21] similarity region
and document pairs with high lr (lr ∈ [7.8 − 8]).
7.2 Evaluation of passage-to-passage linking
The previous section provided evidence that the
document-to-document linking algorithm is capa-
ble of achieving high performance when param-
eters α, β are well selected. However, Section
5 indicated that it is more difﬁcult to discover
links across long document pairs. Thereby, we
have evaluated the passage-to-passage linking on
document pairs with quite low value of similarity
[0.2, 0.21]. According to Figure 2, this region has
only 15% linked-pair likelihood.

Clearly, our goal was not to evaluate the ap-
proach in the best possible environment, but rather
to check whether the method is able to discover
valuable passage-to-passage links from very long
articles with low similarity. Articles with this
value of similarity would be typically ranked very
poorly by link generation methods working at the
document level.

Table 2 shows the results after the ﬁrst step of
the approach, described in Section 6, with respect

Subject’s
decision

yes (correct)
no (incorrect)

System’s decision
yes
0.14
0.24

no
0.46
0.16

Table 3: Passage-to-passage links generation for
very long documents. Passages extracted from the
[0.4, 0.8] similarity region.

to the links assigned by Wikipedia users. As in the
previous experiment, the subject was given pairs
of documents and decided whether they should or
should not be linked. Parameters α and β were
set to 0.2, 0.21 respectively. Table 2 indicates
that that the accuracy (16% + 10% = 26%) is
at this similarity region much lower than the one
reported in Table 1, which is exactly in line with
our expectations. It should be noticed that 34%
of the document pairs were linked by Wikipedia
users, even though only 15% would be predicted
by linked-pair likelihood shown in Figure 2. This
conﬁrms that long document pairs exhibit a higher
probability of being linked in the same similarity
region than shorter document pairs.

If our approach for passage-to-passage link
generation (Algorithm 2) is correct, we should be
able to process the documents paragraphs and de-
tect possible passage-to-passage links. The selec-
tion of the parameters γ and δ inﬂuences the will-
ingness of the system to generate links. For this
experiment, we set the parameters γ, δ to 0.4, 0.8
respectively. The subject was asked to decide: (1)
if the connection discovered by the link generation
method at the granularity of passages was useful
(when the system generated a link) (2) whether
the decision not to generate link is correct (when
the system did not generate a link). The results of
this evaluation are reported in Table 3. It can be
seen that the system made in 60% (14% + 46%)
of the cases the correct decision. Most mistakes
were made by generating links that were not sufﬁ-
ciently related (24%). This might be improved by
using a higher value of γ (lower value of δ).

8 Conclusions

This paper provided a new insight into the use of
semantic similarity as a predictor for automatic
link generation by performing an investigation in
the way people link content. This motivated us
in the development of a novel purely content-
based approach for automatic generation of links
at the granularity of both documents and para-
graphs which does not expect semantic similarity
and linked-pair likelihood to be directly propor-
tional.

598

Itakura, Kelly Y. and Charles L. A. Clarke. 2008. Uni-
versity of waterloo at inex 2008: Adhoc, book, and
link-the-wiki tracks.
In Geva et al. (Geva et al.,
2009), pages 132–139.

Jenkinson, Dylan, Kai-Cheung Leung, and Andrew
Trotman. 2008. Wikisearching and wikilinking. In
Geva et al. (Geva et al., 2009), pages 374–388.

Kolak, Okan and Bill N. Schilit. 2008. Generating
links by mining quotations. In HT ’08: Proceedings
of the nineteenth ACM conference on Hypertext and
hypermedia, pages 117–126, New York, NY, USA.
ACM.

Lu, Wei, Dan Liu, and Zhenzhen Fu. 2008. Csir at
inex 2008 link-the-wiki track. In Geva et al. (Geva
et al., 2009), pages 389–394.

Manning, Christopher D. and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, 1 edition, June.

Manning, Ch. D., P. Raghavan, and H. Sch¨utze. 2008.
Introduction to Information Retrieval. Cambridge,
July.

Widdows, Dominic and Kathleen Ferraro.

2008.
Semantic vectors: a scalable open source pack-
age and online technology management applica-
tion.
In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard Joseph Mari-
ani Jan Odjik Stelios Piperidis Daniel Tapias, ed-
itor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC’08), Mar-
rakech, Morocco, may. European Language Re-
sources Association (ELRA).
http://www.lrec-
conf.org/proceedings/lrec2008/.

Wilkinson, Ross and Alan F. Smeaton. 1999. Auto-
matic link generation. ACM Computing Surveys,
31.

Zeng, Jihong and Peter A. Bloniarz. 2004. From key-
words to links: an automatic approach. Information
Technology: Coding and Computing, International
Conference on, 1:283.

Zhang, Junte and Jaap Kamps. 2008. A content-
based link detection approach using the vector space
model. In Geva et al. (Geva et al., 2009), pages 395–
400.

References
Allan, James. 1997. Building hypertext using infor-
Inf. Process. Manage., 33:145–

mation retrieval.
159, March.

Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. JOURNAL OF
MACHINE LEARNING RESEARCH, 3:993–1022.

Chen, Francine, Ayman Farahat, and Thorsten Brants.
2004. Multiple similarity measures and source-pair
information in story link detection.
In In HLT-
NAACL 2004, pages 2–7.

Deerwester, Scott, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41:391–407.

Dopichaj, Philipp, Andre Skusa, and Andreas Heß.
In Geva

2008. Stealing anchors to link the wiki.
et al. (Geva et al., 2009), pages 343–353.

Geva, Shlomo, Jaap Kamps, and Andrew Trotman, ed-
itors. 2009. Advances in Focused Retrieval, 7th In-
ternational Workshop of the Initiative for the Evalu-
ation of XML Retrieval, INEX 2008, Dagstuhl Cas-
tle, Germany, December 15-18, 2008. Revised and
Selected Papers, volume 5631 of Lecture Notes in
Computer Science. Springer.

Geva, Shlomo. 2007. Gpx: Ad-hoc queries and au-
tomated link discovery in the wikipedia.
In Fuhr,
Norbert, Jaap Kamps, Mounia Lalmas, and An-
drew Trotman, editors, INEX, volume 4862 of Lec-
ture Notes in Computer Science, pages 404–416.
Springer.

Granitzer, Michael, Christin Seifert, and Mario Zech-
In

ner. 2008. Context based wikipedia linking.
Geva et al. (Geva et al., 2009), pages 354–365.

Green, Stephen J. 1998. Automated link generation:
can we do better than term repetition? Comput.
Netw. ISDN Syst., 30(1-7):75–84.

Green, Stephen J. 1999. Building hypertext links
by computing semantic similarity. IEEE Trans. on
Knowl. and Data Eng., 11(5):713–730.

He, Jiyin. 2008. Link detection with wikipedia.
Geva et al. (Geva et al., 2009), pages 366–373.

In

Huang, Wei Che, Andrew Trotman, and Shlomo Geva.
2008. Experiments and evaluation of link discovery
in the wikipedia.

Huang, Wei Che, Shlomo Geva, and Andrew Trotman.
2009. Overview of the inex 2009 link the wiki track.

