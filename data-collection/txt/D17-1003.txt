



















































Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 24–34
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Quasi-Second-Order Parsing
for 1-Endpoint-Crossing, Pagenumber-2 Graphs

Junjie Cao, Sheng Huang, Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University

The MOE Key Laboratory of Computational Linguistics, Peking University
{junjie.cao,huangsheng,ws,wanxiaojun}@pku.edu.cn

Abstract

We propose a new Maximum Subgraph
algorithm for first-order parsing to 1-
endpoint-crossing, pagenumber-2 graphs.
Our algorithm has two characteristics: (1)
it separates the construction for noncross-
ing edges and crossing edges; (2) in a
single construction step, whether to cre-
ate a new arc is deterministic. These two
characteristics make our algorithm rela-
tively easy to be extended to incorpo-
riate crossing-sensitive second-order fea-
tures. We then introduce a new algorithm
for quasi-second-order parsing. Experi-
ments demonstrate that second-order fea-
tures are helpful for Maximum Subgraph
parsing.

1 Introduction

Previous work showed that treating semantic de-
pendency parsing as the search for Maximum Sub-
graphs is not only elegant in theory but also ef-
fective in practice (Kuhlmann and Jonsson, 2015;
Cao et al., 2017). In particular, our previous work
showed that 1-endpoint-crossing, pagenumber-2
(1EC/P2) graphs are an appropriate graph class for
modelling semantic dependency structures (Cao
et al., 2017). On the one hand, it is highly expres-
sive to cover a majority of semantic analysis. On
the other hand, the corresponding Maximum Sub-
graph problem with an arc-factored disambigua-
tion model can be solved in low-degree polyno-
mial time.

Defining disambiguation models on wider con-
texts than individual bi-lexical dependencies im-
proves various syntactic parsers in different ar-
chitectures. This paper studies exact algorithms
for second-order parsing for 1EC/P2 graphs. The
existing algorithm, viz. our previous algorithm

(GCHSW, hereafter), has two properties that make
it hard to incorporate higher-order features in a
principled way. First, GCHSW does not explicitly
consider the construction of noncrossing arcs. We
will show that incorporiating higher-order factors
containing crossing arcs without increasing time
and space complexity is extremely hard. An effec-
tive strategy is to only include higher-order factors
containing only noncrossing arcs (Pitler, 2014).
But this crossing-sensitive strategy is incompat-
ible with GCHSW. Second, all existing higher-
order parsing algorithms for projective trees, in-
cluding (McDonald and Pereira, 2006; Carreras,
2007; Koo and Collins, 2010), require that which
arcs are created in a construction step be deter-
ministic. This design is also incompatible with
GCHSW. In summary, it is not convenient to ex-
tend GCHSW to incorporate higher-order features
while keeping the same time complexity.

In this paper, we introduce an alternative Max-
imum Subgraph algorithm for first-order parsing
to 1EC/P2 graphs. while keeping the same time
and space complexity to GCHSW, our new algo-
rithm has two characteristics that make it rela-
tively easy to be extended to incorporate crossing-
sensitive, second-order features: (1) it separates
the construction for noncrossing edges and pos-
sible crossing edges; (2) whether an edge is cre-
ated is deterministic in each construction rule. We
then introduce a new algorithm to perform second-
order parsing. When all second-order scores are
greater than or equal to 0, it exactly solves the cor-
responding optimization problem.

We implement a practical parser with a sta-
tistical disambiguation model and evaluate it on
four data sets: those used in SemEval 2014
Task 8 (Oepen et al., 2014), and the dependency
graphs extracted from CCGbank (Hockenmaier
and Steedman, 2007). On all data sets, we find
that our second-order parsing models are more ac-

24



curate than the first-order baseline. If we do not
use features derived from syntactic trees, we get
an absolute unlabeled F-score improvement of 1.3
on average. When syntactic analysis is used, we
get an improvement of 0.4 on average.

2 Preliminaries

2.1 Maximum Subgraph Parsing

Semantic dependency parsing can be formulated
as the search for Maximum Subgraph for graph
class G: Given a graph G = (V,A), find a subset
A′ ⊆ A with maximum total score such that the
induced subgraph G′ = (V,A′) belongs to G. For-
mally, we have the following optimization prob-
lem:

arg max
G∗∈G(s,G)

∑
p in G∗

spart(s, p)

G(s, G) denotes the set of all graphs that belong to
G and are compatible with s and G. G is usually
a complete digraph. spart(s, p) evaluates the event
that part p (from a candidate graph G∗) is good.
We define the order of p according to the num-
ber of arcs it contains, in analogy with tree parsing
in terminology. Previous work only discussed the
first-order case:

arg max
G∗∈G(G)

∑
d∈ARC(G∗)

sarc(d)

If G is the set of noncrossing or 1EC/P2 graphs,
the above optimization problem can be solved in
cubic-time (Kuhlmann and Jonsson, 2015) and
quintic-time (Cao et al., 2017) respectively. Fur-
thermore, ignoring one linguistically-rare struc-
ture in 1EC/P2 graphs descreases the complexity
to O(n4). This paper is concerned with second-
order parsing, with a special focus on the follow-
ing factorizations:

And the objective function turns to be:∑
d∈ARC(G∗)

sarc(d) +
∑

s∈SIB(G∗)
ssib(s)

Sun et al. (2017) introduced a dynamic program-
ming algorithm for second-order planar parsing.
Their empirical evaluation showed that second-
order features are effective to improve parsing ac-
curacy. It is still unknown how to incorporate such
features for 1EC/P2 parsing.

a b c d e

Figure 1: e(a,c)’s crossing edges e(b,d) and e(b,e)
share an endpoint b.

a b c d e f

Page 1

Page 2

Figure 2: A pagenumber-2 graph. The upper and
the lower figures represent two half-planes respec-
tively.

2.2 1-Endpoint-Crossing, Pagenumber-2
Graphs

The formal description of the 1-endpoint-crossing
property is adopted from (Pitler et al., 2013).

Definition 1. Edges e1 and e2 cross if e1 and e2
have distinct endpoints and exactly one of the end-
points of e1 lies between the endpoints of e2.

Definition 2. A dependency graph is 1-Endpoint-
Crossing if for any edge e, all edges that cross e
share an endpoint p named pencil point.

Given a sentence s = w0w1 · · ·wn−1 of length
n, the vertices, i.e. words, are indexed with inte-
gers, an arc from wi to wj as a(i,j), and the com-
mon endpoint, namely pencil point, of all edges
crossed with a(i,j) or a(j,i) as pt(i, j). We denote
an edge as e(i,j), if we do not consider its direction.
Figure 1 is an example.

Definition 3. A pagenumber-k graph means it
consists at most k half-planes, and arcs on each
half-plane are noncrossing.

These half-planes may be thought of as the
pages of a book, with the vertex line correspond-
ing to the books spine, and the embedding of a
graph into such a structure is known as a book em-
bedding. Figure 2 is an example.

(Pitler et al., 2013) proved that 1-endpoint-
crossing trees are a subclass of graphs whose pa-
genumber is at most 2. In Cao et al. (2017),
we studied graphs that are constrained to be both
1-endpoint-crossing and pagenumber-2. In this
paper, we ignored a complex and linguistic-rare

x i

k

b

Figure 3: C structure has two crossing chains.

25



i j

k

l

Figure 4: A prototype backbone of 1EC/P2 graphs.
To decompose this structure, GCHSW focuses on
e(i,j) and e(l,j), because these two edges can be
optionally created without violation of both 1EC
and P2 restrictions. Our algorithm focuses on the
existence of e(i,k), and makes it the only edge that
is constructed by applying a corresponding rule.

structure and studied a subset of 1EC/P2 graphs.
The complex structure is named as C structures in
our previous paper, and Figure 3 is the prototype
of C structures. In this paper, we present new algo-
rithms for finding optimal 1EC/P2, C-free graphs.

2.3 The GCHSWAlgorithm

Cao et al. (2017) designed a polynomial time
Maximum Subgraph algorithm, viz. GCHSW, for
1EC/P2 graphs by exploring the following prop-
erty: Every subgraph of a 1EC/P2 graph is also a
1EC/P2 graph. GCHSW defines a number of proto-
type backbones for decomposing a 1EC/P2 graph
in a principled way. In each decomposition step,
GCHSW focuses on the edges that can be created
without violating either the 1EC nor P2 restriction.
Sometimes, multiple edges can be created simulta-
neously in one single step. Figure 4 is an example.

There is an important difference between
GCHSW and Eisner-style Maximum Spanning
Tree algorithms (MST; Eisner, 1996; McDonald
and Pereira, 2006; Koo and Collins, 2010). In
each construction step, GCHSW allows multiple
arcs to be constructed, but whether or not such
arcs are added to the target graph depends on their
arc-weights. If all arcs are assigned scores that
are greater than 0, the output of our algorithm in-
cludes the most complicated 1EC/P2 graphs. For
the higher-order MST algorithms, in a single con-
struction step, it is clear whether adding a new arc,
and which one. There is no local search. This de-
terministic strategy is also followed by Kuhlmann
and Jonsson’s Maximum Subgraph algorithm for
noncrossing graphs. Higher-order MST models
associate higher-order score functions with the
construction of individual dependencies. There-
fore the deterministic strategy is a prerequisite to
incorporate higher-order features. The design of
GCHSW is incompatible with this strategy.

x i k jri ljrx

Figure 5: A typical structure of crossing arcs.

2.4 Challenge of Second-Order Decoding
It is very difficult to enumerate all high-order fea-
tures for crossing arcs. Figure 5 illustrates the
idea. There is a pair of corssing arcs, viz. e(x,k)
and e(i,j). The key strategy to develop a dynamic
programming algorithm to generate such crossing
structure is to treat parts of this structures as inter-
vals/spans together with an external vertex (Pitler
et al., 2013; Cao et al., 2017). Without loss of gen-
erality, we assume [i, j] makes up such an interval
and x is the corresponding external vertex. When
we consider e(i,j), its neighboring edges can be
e(i,ri) and e(lj ,j), and therefore we need to con-
sider searching the best positions of both ri and lj .
Because we have already taken into account three
vertices, viz. x, i and j, the two new positions
increase the time complexity to be at least quintic.

Now consider e(x,k). When we decompose the
whole graph into inverval [i, j] plus x and remain-
ing part, we will factor out e(x,k) in a successive
decomposition for resolving [i, j] plus x. We can-
not capture the second features associated to e(x,k)
and e(x,rx), because they are in different intervals,
and when these intervals are combined, we have
already hidden the position information of k. Ex-
plicitly encoding k increases the time complexity
to be at least quintic too.

Pitler (2014) showed that it is still possible to
build accurate tree parsers by considering only
higher-order features of noncrossing arcs. This is
in part because only a tiny fraction of neighbor-
ing arcs involve crossing arcs. However, this strat-
egy is not easy to by applied to GCHSW, because
GCHSW does not explicitly analyze sub-graphs of
noncrossing arcs.

3 A New Maximum Subgraph Algorithm

Based on the discussion of Section 2.3 and 2.4,
we can see that it is not easy to extend the existing
algorithm, viz. GCHSW, to handle second-order
features. In this paper, we propose an alternative
first-order dynamic programming algorithm. Be-
cause ignoring one linguistically-rare structure as-
sociated with the C problem in GCHSW descreases
the complexity, we exclude this structure in our al-
gorithm. Formally, we introduce a new algorithm

26



IntO[i, j]

i j

LR[i, j, x]

x i j

NO[i, j, x]

x i j

LO[i, j, x]

x i j

RO[i, j, x]

x i j

IntC [i, j]

i j

NC [i, j, x]

x i j

LC [i, j, x]

x i j

RC [i, j, x]

x i j

Figure 6: Graphical representations of sub-problems. Gray curves mean the corresponding edge in this
sub-problem, but should be included in the final generated graph.

IntO(i, j)← max

IntO(i + 1, j)
IntC(i, j)
IntC(i, k) + IntO(k, j)
RC(i, k, x) + IntO(k, x) + LO(x, j, k) + sarc(i, k)
LR(i, k, x) + IntO(k, x) + IntO(x, j, k) + sarc(i, k)
IntO[i, x] + LC [x, k, i] + NO[k, j, x] + sarc(i, k)
RO[i, x, k] + IntO[x, k] + LO[k, j, x] + sarc(i, k)

IntC(i, j)← sarc(i, j) + max

IntO(i + 1, j)
IntC(i, k) + IntO(k, j)
RC(i, k, x) + IntO(k, x) + LO(x, j, k) + sarc(i, k)
LR(i, k, x) + IntO(k, x) + IntO(x, j, k) + sarc(i, k)
IntO[i, x] + LC [x, k, i] + NO[k, j, x] + sarc(i, k)
RO[i, x, k] + IntO[x, k] + LO[k, j, x] + sarc(i, k)

NO(i, j, x)← max
IntO(i, j)
NC(i, j, x) + sarc(x, j)
NC(i, k, x) + IntO(k, j) + sarc(x, k)

NC(i, j, x)← max{
IntO(i, j)
NC(i, k, x) + IntO(k, j) + sarc(x, k)

LR(i, j, x)← max{
LO(i, k, x) + RO(k, j, x)

LO(i, j, x)← max
IntO(i, j)
LC(i, j, x) + sarc(x, j)
LC(i, k, x) + NO(k, j) + sarc(x, k)
IntO(i, k, x) + LO(k, j) + sarc(x, k)

LC(i, j, x)← max
IntO(i, j)
LC(i, j, x) + sarc(x, j)
LC(i, k, x) + NO(k, j, i) + sarc(x, k)
IntO(i, k) + LO(k, j, i) + sarc(x, k)

RO(i, j, x)← max
IntO(i, j)
RC(i, j, x) + sarc(x, j)
NC(i, k, j) + RO(k, j, x) + sarc(x, k)
RO(i, k, x) + IntO(k, j) + sarc(x, k)

RC(i, j, x)← max{
NC(i, k, j) + RO(k, j, x) + sarc(x, k)
RO(i, k, x) + IntO(k, j) + sarc(x, k)

Figure 7: A dynamic program to find optimal 1EC/P2, C-free graphs with arc-factored weights.

to solve the following optimization problem:

arg max
G∗∈G(G)

∑
d∈ARC(G∗)

sarc(d)

where G means 1EC/P2, C-free graphs. Our algo-
rithm has the same time and space complexity to
the degenerated version of GCHSW. We represent
our algorithm using undirected graphs.

3.1 Sub-problems
Following GCHSW, we consider five sub-problems
when we construct a maximum dependency graph
on a given interval [i, k]. Though the sub-
problems introduced by GCHSW and us handle
similar structures, their definitions are quite differ-
ent. The sub-problems are explained as follows:

Int Int[i, j] represents an interval from i to j in-
clusively. And there is no edge e(i′,j′) such
that i′ ∈ [i, j] and j′ /∈ [i, j]. We distinguish
two sub-types for Int. IntO[i, j] may or may
not contain e(i,j), while IntC [i, j] contains
e(i,j).

LR LR[i, j, x] represents an interval from i to j
inclusively and an external vertex x. ∀p ∈

[i, j], pt(x, p) = i or j. LR[i, j, x] implies
the existence of e(i,j), but does not contain
e(i,j). When LR[i, j, x] is combined with
other DP sub-structures, e(i,j) is immediately
created. LR[i, j, x] disallows neither e(x,i)
nor e(x,j).

N N [i, j, x] represents an interval from i to j
inclusively and an external vertex x. ∀p ∈
[i, j], pt(x, p) /∈ [i, j]. N [i, j, x] could con-
tain e(i,j) but disallows e(x,i). We distinguish
two sub-types. NO[i, j, x] may or may not
contain e(x,j). NC [i, j, x] implies the exis-
tence of but does not contain e(x,j). When
N [i, j, x] is combined with others, e(x,j) is
immediately created.

L L[i, j, x] represents an interval from i to j
inclusively as well as an external vertex x.
∀p ∈ [i, j], pt(x, p) = i. L[i, j, x] could con-
tain e(i,j) but disallows e(x,i). We distinguish
sub-two types for L. LO[i, j, x] may or may
not contain e(x,j). LC [i, j, x] implies the ex-
istence of but does not contain e(x,j). When
it is combined with others, e(x,j) is immedi-
ately created.

27



R R[i, j, x] represents an interval from i to j
inclusively as well as an external vertex x.
∀p ∈ [i, j], pt(x, p) = j. R[i, j, x] disal-
lows e(x,j) and e(x,i). We distinguish two
sub-types for R. RO[i, j, x] may or may not
contain e(i,j). RC [i, j, x] implies the exis-
tence of but does not contain e(i,j). When it
is combined with others, e(i,j) is immediately
created.

3.2 Decomposing Sub-problems
Figure 7 gives a sketch of our dynamic program-
ming algorithm. We give a detailed illustration for
Int, a rough idea for L and LR, and omit other
sub-problems. More details about the whole algo-
rithm can be found in the supplementary note.

3.2.1 Decomposing an Int Sub-problem
Consider IntO[i, j] and IntC [i, j] sub-problem.
Because the decomposition for IntC [i, j] is very
similar to IntO[i, j] and needs to be modified
by our second-order parsing algorithm, we only
show the decomposition of IntC [i, j]. Assume
that k(k ∈ (i, j)) is the farthest vertex that is ad-
jacent to i, and x = pt(i, k). If there is no such
k (i.e. there no arc from i to some other node in
this interval), then we denote k as ∅. So it is to x.
We illustrate different cases as following and give
a graphical representation in Figure 8.

Case a: k = ∅. We can directly consider interval
[i + 1, j]. Because there is no edge from i to any
node in [i + 1, j], [i + 1, j] is an IntO.

Case b: x = ∅. x = ∅ means that e(i,k) does not
cross other arcs. So [i, k] and [k, j] are Int.

Case c: x ∈ (k, j]. e(i,k) is taken as a possible
crossing edge. k and x divide the interval [i, j] into
three parts: [i, k], [k, x], [x, j]. Because x may be
j, interval [x, j] may only contain j and become
an empty interval. We define x′ as the pencil point
of all edges from (i, k) to x, and distinguish two
sub-problems as follows.

c.1 Assume that there exists an edge from k to
some node r in (x, j], so x′ can only be k and
pencil point of edges from k to (x, j] is x.
Thus interval [i, k, x] is an R. Due to the exis-
tence of e(i,k), its sub-type is RC. The e(i,k) is
created in this construction and thus not con-
tained by RC [i, k, x]. An edge from within
[k, x] to outside violates the 1EC restriction,
so [k, x] is an Int. Since x is endpoint of edge

from k to [x, r], interval [k, j] is an LO with
external vertex k.

c.2 We assume no edge from k to any node in
[x, j], x′ thus can be i or k. As a result, [x, j]
is an Int and [i, k, x] is an LR.

Case d: x ∈ (i, k).

d.1 Assume that there exist edges from i to
(x, k), so the pencil point of edges from x to
(k, j] is i. Therefore [k, j] is an N. Because x
is pencil point of edges from i to (x, k], [x, k]
is an L. Furthmore, it is an LC because we
generate e(i,k) in this step. It is obvious that
[i, x] is an Int.

d.2 Assume that there exists edges from k to
(i, x), and the pencil point of edges from
x to (k, j] is thus k. Similar to the above
analysis, we reach RO[i, x, k]+IntO[x, k]+
LO[k, j, x] + e(i,k) + e(i,j).

For IntO[i, j], because there may be e(i,j), we
add one more rule: IntO[i, j] = IntC [i, j]. And
we do not need to create e(i,j) in all cases.

3.2.2 Decomposing an L Sub-problem
Without loss of generality, we show the decompo-
sition of LO[i, j, x] as follows. For LC [i, j, x], we
ignore Case b but follow the others.

Case a. If there is no more edge from x to (i, j],
then it will degenerate to IntO[i, j].

Case b. If there exists e(x,j), then it will degen-
erate to LC [i, j, x] + e(x,j).

Case c. Assume that there are edges from x to
(i, j) and e(x,k) is the farthest one. It divides [i, j]
into [i, k] and [k, j].

c.1 If there is an edge from x to (i, k), [i, k] and
[k, j] are LC [i, k, x] and NO[k, j, i].

c.2 If there is no edge from x to (i, k), [i, k] and
[k, j] are IntO[i, k] and LO[k, j, i].

Figure 8 is a graphical representation.

3.2.3 Decomposing an LR Sub-problem
LR[i, j, x] means i or j is the pencil point of edges
from x to (i, j). We show the decomposition of
LR[i, j, x] as follows:

28



(a)

i j
=
i + 1 j

(b)

i k j
=

i k
+

k j

Dashed edge exist?

i

k
x j

(c.1)

i

k

x j
=

i

k
x

+
k x

+
k x j

(c.2)

i

k

x j
=

i

k
x

+
k x

+
x j

Dashed edge exist?

i x
k

j

(d.1)

i x

k

j
=

i x
+

i x k
+

x k j

(d.2)

i x

k

j
=

i x k
+

x k
+

x k j

Figure 8: Decomposition for IntC [i, j] in the first-order parsing algorithm. pt(i, k) = x.

(b)

x i j
=

x i j

(c.1)

x i

k

j
=

x i k
+

i k j

(c.2)

x i

k

j
=

i k
+

i k j

Figure 9: Decomposition for LO[i, j, x].

x i b1

a1

b2

a2

j, b3

Figure 10: b3 = j, Not both e(x,b1) and e(x,a2)
exist.

x i b1

a1

b2

a2

b3

j, a3

Figure 11: a3 = j. Both e(x,b1) and e(x,b3) exist.

Case a. If there is a vertex k within (i, j), which
divides [i, j] into [i, k] and [k, j]. And it guaran-
tees no edge from [i, k) to (k, j]. i is the pencil
point of edges from x to (i, k] because no edge
from j to (i, k) can cross these edges. Similarly j
has to be the pencil point of edges from x to (k, j).
Obviously, [i, k] is an LO and [k, j] is an RO with
external x. Thus the problem is decomposed as
LO[i, k, x] + RO[k, j, x].

Case b. If there is no such vertex k, there must
be edges from [i, k′) to (k′, j] for every k′ in (i, j)
without considering e(i,j). For i + 1, we assume
e(i,a1) is the farthest edge that goes from i. For
a1, we assume e(b1,b2) is the farthest edge from
b1 where b1 is in (i, a1) and b2 is in (a1, j). For
b2, we assume e(a1,a3) is the farthest edge from
a1 where a3 is in (b2, j) and a1 is the pencil
point. We then get the series {a1, a2, a3...an} and
{b1, b2...bm}which guarantees bi < ai , ai < bi+1
and max(an, bm) = j.

If bm = j, we will get a graph like Figure 10. If
e(x,b1) exists, this LR subproblem degenerates to
an L subproblem. If e(x,an) exists, this subprob-
lem degenerates to an R subproblem.

If am = j, we will get a graph like Figure 11.
If there exists only e(x,b1) or e(x,bm), we can solve
it like bm = j. If both exist, this is a typical C-

structure like Figure 3 and we cannot get it through
other decompostion.

The above discussion gives the rough idea of the
correctness of the following conclusion.
Theorem 1. Our new algorithm is sound and
complete with respect to 1EC/P2, C-free graphs.

3.3 Spurious Ambiguity
An LR, L, R or N sub-problem allows to build
crossing arcs, but does not necessarily create
crossing arcs. For example, LC [i, j, x] allows
e(i,j) to cross with e(x,y) (y ∈ (i, j)). Be-
cause every subgraph of a 1EC/P2 graph is also
a 1EC/P2 graph, we allow an LC [i, j, x] to be di-
rectly degenerated to IO[i, j]. In this way, we can
make sure that all subgraphs can be constructed
by our algorithm. Figure 12 shows the rough idea.
To generate the same graph, we have different
derivations. The spurious ambiguity in our algo-
rithm does not affect the correctness of first-order
parsing, because scores are assigned to individ-
ual dependencies, rather than derivation processes.
There is no need to distinguish one special deriva-
tion here.

4 Quasi-Second-Order Extension

We propose a second-order extension of our new
algorithm. We focus on factorizations introduced
in Section 2.1. Especially, the two arcs in a fac-
tor should not cross other arcs. Formally, we in-
troduce a new algorithm to solve the optimization
problem with the following objective:∑

d∈ARC(G∗)
sarc(d) +

∑
s∈SIB(G∗)

max(ssib(s), 0)

In the first-order algorithm, all noncrossing edges
can be constructed as the frontier edge of an IntC.

29



a b c d e

Figure 12: Illustration of spurious ambiguity. The
two solid curves represent two arcs in the target
graph, but not the dashed one. Excluding crossing
edges leads to the first derivation: IntC [a, e] ⇒
e(a,e) + IntC [a, c] + IntO[c, e] + e(a,c). As-
suming that a pair of crossing arcs may exist
yields another derivation: IntC [a, e] ⇒ e(a,e) +
LR[a, c, d] + IntO[k, d] + LO[d, e, c] + e(a,c);
Then LR[a, c, d] ⇒ LO[a, b, d] + RO[b, c, d] ⇒
IntO[a, b] + IntO[b, c].

So we can develop an exact decoding algorithm by
modifying the composition for IntC while keeping
intact the decomposition for LR, N, L, R.

4.1 New Decomposition for IntC
In order to capture the second-order features from
noncrossing neighbors, we need to find the right-
most node adjacent to i, denoted as ri, and the
leftmost node adjacent to j, denoted as lj ,while
i < ri ≤ lj < j. To do this, we split IntC [i, j]
into at most three parts to capture the sibling fac-
tors. Denote the score of adjacent edges e(i,j1)
and e(i,j2) as s2(i, j1, j2). When j is the inner
most node adjacent to i, we denote the score as
s2(i, ∅, j). We give a sketch of the decomposition
in Figure 14 and a graphical representation in Fig-
ure 13. The following is a rough illustration.

Case a: ri = ∅. We further distinguish three
sub-problems:

a.1 If lj = ∅ too, both sides are the inner most
second-order factor.

a.2 There is a crossing arc from j. This case is
handled in the same way as the first-order al-
gorithm.

a.3 lj 6= ∅. We introduce a new decomposition
rule.

Case b: There is a crossing arc from i.

b.1 lj = ∅. Similar case to (a.2).
b.2 There is a crossing arc from j. Similar case

to (a.2).

b.3 There is a noncrossing arc from j. We intro-
duce a new rule to calculate SIB(j, lj , i).

Case c: There is a noncrossing arc from i.

c.1 lj = ∅. Similar to (a.3).

c.2 There is a crossing arc from j. Similar to
(b.3).

c.3 There is a noncrossing arc from j too. We
introduce a new rule to calculate SIB(i, ri, j)
and SIB(j, lj , i).

4.2 Complexity

The complexity of both first- and second-order al-
gorithms can be analyzed in the same way. The
sub-problem Int is of size O(n2), with a calculat-
ing time of order O(n2) at most. For sub-problems
L, R, LR, and N, each has O(n3) elements, with
a unit calculating time O(n). Therefore both algo-
rithms run in time of O(n4) with a space require-
ment of O(n3).

4.3 Discussion

A traditional second-order model takes as
the objective function

∑
s∈SIB(G∗) ssib(s).

Our model instead tries to optimize∑
s∈SIB(G∗) max(ssib(s), 0). This model is

somehow inadequate given that the second-order
score function cannot penalize a bad factor. When
a negative score is assigned to a second-order
factor, it will be taken as 0 by our algorithm.

This inadequacy is due to the spurious am-
biguity problem that is illustrated in Section
3.3. Take the two derivations in Figure 12
for example. The derivation that starts from
IntC [a, e]⇒ IntC [a, c]+IntO[c, e] incorporates
the second-order score ssib(a, c, e). This is dif-
ferent when we consider the derivation that starts
from IntC [a, e] ⇒ LR[a, c, d] + IntO[k, d] +
LO[d, e, c]. Because we assume temporarily
that e(a,c) crosses others, we do not consider
ssib(a, c, e). We can see from this example that
second-order scores not only depend on the de-
rived graphs but also sensitive to the derivation
processes.

If a second-order score is greater than 0, our al-
gorithm selects the derivation that takes it into ac-
count since it increases the total score. If a second-
order score is negative, our algorithm avoids in-
cluding it by selecting other paths. In other words,
our algorithm treats this score as 0.

30



(a.1)

i j
=
i + 1 j − 1

(a.2)

i j
=
i + 1 j

(a.3)

i j
=
i + 1 lj

+
lj j

(b.1)

i j
=

i j − 1

(b.3)

i j
=

i rj
+

rj j

(c.1)

i j
=

i ri
+

ri j − 1

(c.2)

i j
=

i ri
+

ri j

(c.3)

i j
=

i ri
+

ri lj
+

lj j

Figure 13: Decomposition for IntC [i, j] in the second-order parsing algorithm.

IntC(i, j)← sarc(i, j) + max

IntO(i + 1, j − 1) + ssib(i, ∅, j) + ssib(j, ∅, i)
IntO(i + 1, j) + ssib(i, ∅, j)
IntO(i + 1, lj) + IntC(lj , j) + ssib(i, ∅, j)+

ssib(j, lj , i)
IntO(i, j − 1) + ssib(j, ∅, i)
IntO(i, lj) + IntC(lj , j) + ssib(j, lj , i)
IntC(i, ri) + IntO[ri, j − 1] + ssib(i, ri, j)+

ssib(j, ∅, i)
IntC(i, ri) + IntO[ri, j] + ssib(i, ri, j)
IntC(i, ri) + IntO[ri, lj ] + IntC(lj , j)+

ssib(i, ri, j) + ssib(j, lj , i)
RC(i, k, x) + IntO(k, x) + LO(x, j, k) + e(i,k)
LR(i, k, x) + IntO(k, x) + IntO(x, j, k) + e(i,k)
IntO[i, x] + LC [x, k, i] + NO[k, j, x] + e(i,k)
RO[i, x, k] + IntO[x, k] + LO[k, j, x] + e(i,k)

Figure 14: Decomposition for IntC [i, j, x].

5 Practical Parsing

5.1 Derivation-Sensitive Training

We extend our quartic-time parsing algorithm into
a practical parser. In the context of data-driven
parsing, this requires an extra disambiguation
model. As with many other parsers, we employ
a global linear model. Following Zhang et al.
(2016)’s experience, we define rich features ex-
tracted from word, POS-tags and pseudo trees. To
estimate parameters, we utilize the averaged per-
ceptron algorithm (Collins, 2002).

Our training proceudre is sensitive to derivation
rather then derived graphs. For each sentence, we
first apply our algorithm to find the optimal pre-
diction derivation. The we collect all first- and
second-order factors from this derivation to update
parameters. To train a first-order model, because
our algorithm includes all factors, viz. depen-
cies, there is no difference between our derivation-
based method and a traditional derived structure-
based method. For the second-order model, our
method increases the second-order scores some-
how.

5.2 Data and Preprocessing
We evaluate first- and second-order models
on four representative data sets: CCGBank
(Hockenmaier and Steedman, 2007), DeepBank
(Flickinger et al., 2012), Enju HPSGBank (Miyao
et al., 2005) and Prague Dependency TreeBank
(Hajic et al., 2012). We use “standard” training,
validation, and test splits to facilitate comparisons.

• Following previous experimental setup for
English CCG parsing, we use section 02-21 as
training data, section 00 as the development
data, and section 23 for testing.

• The DeepBank, Enju HPSGBank and Prague
Dependency TreeBank are from SemEval
2014 Task 8 (Oepen et al., 2014), and the data
splitting policy follows the shared task.

Experiments for CCG-grounded analysis were per-
formed using automatically assigned POS-tags
that are generated by a symbol-refined HMM tag-
ger (Huang et al., 2010). Experiments for the other
three data sets used POS-tags provided by the
shared task. We also use features extracted from
pseudo trees. We utilize the Mate parser (Bohnet,
2010) to generate pseudo trees. All experimental
results consider directed dependencies in a stan-
dard way. We report Unlabeled Precision (UP),
Recall (UR) and F-score (UF), which are calcu-
lated using the official evaluation tool provided by
SDP2014 shared task.

5.3 Accuracy
Table 1 lists the accuracy of our system. The out-
put of our parser was evaluated against each de-
pendency in the corpus. We can see that the first-
order parser obtains a considerably good accuracy,
with rich syntactic features. Furthermore, we can
see that the introduction of higher-order features
improves parsing substantially for all data sets, as
expected. When syntactic trees are utilized, the

31



DeepBank EnjuBank CCGBank PCEDT
Tree UP UR UF UP UR UF UP UR UF UP UR UF
No 1or 89.43 83.03 86.11 90.10 87.10 88.58 91.63 88.07 89.82 88.13 81.53 84.70

2or 89.23 85.98 87.57 90.88 89.90 90.39 91.96 89.54 90.74 88.56 84.57 86.52
Syn 1or 91.24 87.14 89.14 92.72 90.96 91.83 94.28 91.79 93.02 91.53 86.95 89.18

2or 90.93 88.79 89.85 92.73 92.11 92.42 93.99 92.27 93.13 91.02 88.20 89.59

Table 1: Parsing accuracy evaluated on the development sets.

DeepBank EnjuBank CCGBank PCEDT
Tree UP UR UF UP UR UF UP UR UF UP UR UF
No 1or 88.87 82.50 85.57 90.12 86.76 88.41 91.95 88.29 90.08 86.87 80.45 83.54

2or 88.77 85.61 87.16 91.06 89.50 90.27 92.25 89.80 91.01 87.07 83.45 85.22
Syn 1or 90.68 86.57 88.58 92.82 90.62 91.71 94.32 91.88 93.09 90.11 85.83 87.97

2or 90.13 88.21 89.16 92.84 91.50 92.17 94.09 92.27 93.17 89.73 87.13 88.41
SJW (2or) 89.99 87.77 88.87 92.87 92.04 92.46 93.45 92.51 92.98 89.58 87.73 88.65

Table 2: Parsing accuracy evaluated on the test sets. “SJW” denotes the book embedding parser intro-
duced in (Sun et al., 2017).

improvement is smaller but still significant on the
three SemEval data sets.

Table 2 lists the parsing results on the test data
together with the result obtained by Sun et al.
(SJW; 2017)’s system. The building architectures
of both systems are comparable.

1. Both systems have explicit control of the out-
put structures. While Sun et al.’s system con-
strain the output graph to be P2 only, our sys-
tem adds an additional 1EC restriction.

2. Their system’s second-order features also in-
cludes both-side neighboring features.

3. Their system uses beam search and dual
decomposition and therefore approximate,
while ours perform exact decoding.

We can see that while our purely Maximum Sub-
graph parser obtains better results on DeepBank
and CCGBank; while the book embedding parser
is better on the other two data sets.

5.4 Analysis
Our algorithm is sensitive to the derivation pro-
cess and may exclude a couple of negative second-
order scores by selecting misleading derivations.
Neverthess, our algorithm works in an exact way
to include all positive second-order scores. Table
3 shows the coverage of all second-order factors.
On average, 99.67% second-order factors are cal-
culated by our algorithm. This relatively satisfac-
tory coverage suggests that our algorithm is very
effective to include second-order features. Only a
very small portion is dropped.

DeepBank EnjuBank CCGBank PCEDT
No 99.08 99.52 99.67 98.32
Syn 99.77 99.69 99.88 99.33

Table 3: Coverage of second-order factors on the
developmenet data.

6 Conclusion

This paper proposed two exact, graph-based al-
gorithms for 1EC/P2 parsing with first-order and
quasi-second-order scores. The resulting parser
has the same asymptotic run time as Cao et al.
(2017)’s algorithm. An exploration of other factor-
izations that facilitate semantic dependency pars-
ing may be an interesting avenue for future work.
Recent work has investigated faster decoding for
higher-order graph-based projective parsing e.g.
vine pruning (Rush and Petrov, 2012) and cube
pruning (Zhang and McDonald, 2012). It would
be interesting to extend these lines of work to de-
crease the complexity of our quartic algorithm.

Acknowledgments

This work was supported by 863 Program of China
(2015AA015403), NSFC (61331011), and Key
Laboratory of Science, Technology and Standard
in Press Industry (Key Laboratory of Intelligent
Press Media Technology). Weiwei Sun is the cor-
responding author.

32



References
Bernd Bohnet. 2010. Top accuracy and fast depen-

dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010). Coling 2010 Or-
ganizing Committee, Beijing, China, pages 89–97.
http://www.aclweb.org/anthology/C10-1011.

Junjie Cao, Sheng Huang, Weiwei Sun, and Xiao-
jun Wan. 2017. Parsing to 1-endpoint-crossing,
pagenumber-2 graphs. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics.

Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In In Proc.
EMNLP-CoNLL.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics, pages 1–8.
https://doi.org/10.3115/1118693.1118694.

Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: an exploration. In Proceed-
ings of the 16th conference on Computational lin-
guistics - Volume 1. Association for Computational
Linguistics, Stroudsburg, PA, USA, pages 340–345.

Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012.
Deepbank: A dynamically annotated treebank of the
wall street journal. In Proceedings of the Eleventh
International Workshop on Treebanks and Linguistic
Theories. pages 85–96.

Jan Hajic, Eva Hajicová, Jarmila Panevová, Petr
Sgall, Ondej Bojar, Silvie Cinková, Eva Fucı́ková,
Marie Mikulová, Petr Pajas, Jan Popelka, Jirı́ Se-
mecký, Jana Sindlerová, Jan Stepánek, Josef Toman,
Zdenka Uresová, and Zdenek Zabokrtský. 2012.
Announcing prague czech-english dependency tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation.
Istanbul, Turkey.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics 33(3):355–396.

Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent vari-
able grammars. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing. Association for Computa-
tional Linguistics, Cambridge, MA, pages 12–22.
http://www.aclweb.org/anthology/D10-1002.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the

48th Annual Meeting of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Uppsala, Sweden, pages 1–11.
http://www.aclweb.org/anthology/P10-1001.

Marco Kuhlmann and Peter Jonsson. 2015. Parsing to
noncrossing dependency graphs. Transactions of the
Association for Computational Linguistics 3:559–
570.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006)). volume 6, pages
81–88.

Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii.
2005. Corpus-oriented grammar development for
acquiring a head-driven phrase structure grammar
from the penn treebank. In IJCNLP. pages 684–693.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajic, An-
gelina Ivanova, and Yi Zhang. 2014. Semeval 2014
task 8: Broad-coverage semantic dependency pars-
ing. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014). As-
sociation for Computational Linguistics and Dublin
City University, Dublin, Ireland, pages 63–72.
http://www.aclweb.org/anthology/S14-2008.

Emily Pitler. 2014. A crossing-sensitive third-
order factorization for dependency parsing.
TACL 2:41–54. http://www.transacl.org/wp-
content/uploads/2014/02/39.pdf.

Emily Pitler, Sampath Kannan, and Mitchell Mar-
cus. 2013. Finding optimal 1-endpoint-crossing
trees. TACL 1:13–24. http://www.transacl.org/wp-
content/uploads/2013/03/paper13.pdf.

Alexander Rush and Slav Petrov. 2012. Vine
pruning for efficient multi-pass dependency pars-
ing. In Proceedings of the 2012 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, Montréal, Canada, pages 498–507.
http://www.aclweb.org/anthology/N12-1054.

Weiwei Sun, Junjie Cao, and Xiaojun Wan. 2017. Se-
mantic dependency parsing via book embedding. In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.

Hao Zhang and Ryan McDonald. 2012. General-
ized higher-order dependency parsing with cube
pruning. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning. Association for Computational
Linguistics, Jeju Island, Korea, pages 320–331.
http://www.aclweb.org/anthology/D12-1030.

33



Xun Zhang, Yantao Du, Weiwei Sun, and Xiaojun
Wan. 2016. Transition-based parsing for deep de-
pendency structures. Computational Linguistics
42(3):353–389. http://aclweb.org/anthology/J16-
3001.

34


