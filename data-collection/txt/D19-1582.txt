



















































Event Detection with Multi-Order Graph Convolution and Aggregated Attention


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5766–5770,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5766

Event Detection with Multi-Order Graph Convolution and Aggregated
Attention

Haoran Yan†‡, Xiaolong Jin†‡, Xiangbin Meng§, Jiafeng Guo†‡ and Xueqi Cheng†‡
†CAS Key Laboratory of Network Data Science and Technology,

Institute of Computing Technology, Chinese Academy of Sciences (CAS);
‡School of Computer and Control Engineering, The University of CAS

{yanhaoran17s, jinxiaolong, guojiafeng, cxq}@ict.ac.cn
§School of Computer Science, Beijing University of Posts and Telecommunications

mengxb@bupt.edu.cn

Abstract

Syntactic relations are broadly used in many
NLP tasks. For event detection, syntactic rela-
tion representations based on dependency tree
can better capture the interrelations between
candidate trigger words and related entities
than sentence representations. But, existing
studies only use first-order syntactic relations
(i.e., the arcs) in dependency trees to identify
trigger words. For this reason, this paper pro-
poses a new method for event detection, which
uses a dependency tree based graph convolu-
tion network with aggregative attention to ex-
plicitly model and aggregate multi-order syn-
tactic representations in sentences. Experi-
mental comparison with state-of-the-art base-
lines shows the superiority of the proposed
method.

1 Introduction

As an important information extraction task, event
detection aims to find event mentions with spe-
cific event types from given texts. Each event
mention is identified by a word or phrase called
event trigger, which serve as the main word(s) to
the corresponding event. For example, in the sen-
tence presented in Figure 1, an event detection sys-
tem should be able to recognize the trigger word
“fired” corresponding to the event type “Attack”.

Among existing methods for event detection,
sequence based ones (e.g., Chen et al. (2015);
Nguyen et al. (2016)) only use the given sen-
tences, which suffer from the low efficiency prob-
lem in capturing long-range dependency; On the
contrary, dependency tree based methods utilize
the syntactic relations (i.e., arcs) in the depen-
dency tree of a given sentence to more effectively
capture the interrelation between each candidate
trigger word and its related entities or other trig-
gers. For example, Nguyen and Grishman (2018)
and Liu et al. (2018) treat each dependency tree
as a graph and adopt Graph Convolution Network

(GCN) (Kipf and Welling, 2016) to extract event
triggers.

The syntactic relations between trigger words
and related entities may be of first order, embod-
ied as the direct arcs in the dependency tree. They
may also be of high-order (i.e., the paths more than
one hop in the dependency tree). Particularly, ac-
cording to our statistics on the benchmark ACE-
2005 dataset, about 51% (4977/9793) of event re-
lated entities need more than one hop to get to the
corresponding trigger words and related entities in
their dependency trees. For example, in Figure 1,
it needs at least 4 hops (i.e., “fired”–“evidence”–
“blood”–“soldiers”) to figure out that the word
“fired” means “shot” instead of “dismissed”.

However, the above dependency tree based
methods explicitly use only first-order syntac-
tic relations, although they may also implicitly
capture high-order syntactic relations by stack-
ing more GCN layers. However, as the number
of GCN layers increases, the representations of
neighboring words in the dependency tree will get
more and more similar, since they all are calcu-
lated via those of their neighbors in the depen-
dency tree. This is the so-called over-smoothing
problem (Zhou et al., 2018), which damages the
diversity of the representations of neighboring
words.

To overcome this problem, this paper proposes
a Multi-Order Graph Attention Network based
method for Event Detection, called MOGANED.
It utilizes both first-order syntactic graph and high-
order syntactic graphs to explicitly model multi-
order representations of candidate trigger words.
In order to calculate multi-order representations
for each word, we apply Graph Attention Net-
work (GAT) proposed in (Veličković et al., 2017)
to weight the importance of its neighboring words
in the syntactic graphs of different orders. We
then employ an attention aggregation mechanism
to merge its multi-order representations. Finally,



5767

Figure 1: An example sentence with the dependency parsing result.

through experimental comparison with state-of-
the-art baselines we show the superiority of the
proposed method in terms of both precision and
F1-measure. It should be mentioned that, to the
best of our knowledge, this is the first study apply-
ing GAT to event detection.

2 The Proposed Model

Following the existing studies, we regard event de-
tection as a multi-class classification problem. Let
W = w1, w2, ..., wn be a sentence of length n,
where wi is its i-th token. Since event triggers
may contain multiple words, we adopt the “BIO”
schema to make annotation. The number of labels
is thus be 2L + 1, where L is the number of pre-
defined event types.

The proposed model contains three modules: (i)
word encoding, which encodes input sentence to a
sequence of vectors; (ii) multi-order graph atten-
tion network that performs graph attention convo-
lution over multi-order syntactic graphs; (iii) at-
tention aggregation, which aggregates multi-order
representations of each word with different atten-
tion weights to predict its label.

2.1 Word Encoding

The word encoding module first transforms each
input token wi into a comprehensive embed-
ding vector xi by concatenating its word em-
bedding wordi, entity type embedding eti, POS-
tagging embedding posi and position embedding
psi (Nguyen et al., 2016; Nguyen and Grishman,
2018), where wordi is obtained by looking up a
pre-trained word embedding table on a large cor-
pus and others are randomly initialized.

The input sentence W will then be transformed
to a sequence of vectors X = x1, x2, ..., xn. Since
each word can only be updated by its neighbors in
the dependency tree through dependency arcs, fol-
lowing the previous methods (Liu et al., 2018), we
employ a Bidirectional Long-Short Term Mem-
ory network (BiLSTM) (Hochreiter and Schmid-
huber, 1997) to encode X with its context as P =
p1, p2, ..., pn, which will be used as the input of

the multi-order GAT. Here,

pi = [
−−−−→
LSTM(xi)||

←−−−−
LSTM(xi)], (1)

where || is the concatenation operation.

2.2 Multi-Order Graph Attention Network
Each dependency tree can be represented as a
first-order syntactic graph in terms of an adja-
cency matrix. Let A be the adjacency matrix of
the first-order syntactic graph, which is generated
from the dependency tree of the sentence, W .
A contains three sub matrixes Aalong, Arev and
Aloop (Marcheggiani and Titov, 2017) of the same
dimensions n × n. Aalong(i, j) = 1, if there is a
dependency arc from wi to wj in the dependency
tree, otherwise 0. As the reverse graph of Aalong,
Arev = A

T
along. Aloop is an identity matrix.

The adjacency matrix of the k-th-order syntac-
tic graph is Aksubg = (Asubg)

k, where subg ∈
{along, rev, loop}. Aksubg records all k-hop paths
of Asubg. Note that Akloop = Aloop, since it is an
identity matrix. For simplicity, in what follows we
use ak, bk, ck to denote Akalong, A

k
rev and A

k
loop,

respectively.
The multi-order GAT module simply learns a

list of representations over multi-order syntactic
graphs by a few parallel GAT layers (Veličković
et al., 2017), which weights the importance of
neighbors of each word in each syntactic graph
during convolution.

The representations hki of the k-th-order syntac-
tic graph Ak are calculated from the representa-
tions of the subgraphs of Ak:

hki = f(pi, a
k)⊕ f(pi, bk)⊕ f(pi, ck), (2)

where f(·) is the graph attention convolution func-
tion and ⊕ is element-wise addition, and

f(pi, a
k) = σ

n∑
j=1

(uija
k
ij(Wa,kpj + �a,k)), (3)

where σ is exponential linear unit (ELU) (Clevert
et al., 2015); Wa,k and �a,k are the weight matrix
and bias item for ak; uij is the normalized weight
of the neighbor wj when updating wi,



5768

BiLSTM

4

3

1

5

6
7

4

3

1

5

6
7

4

3

1

5

6
7 11

12

14

16
17

15

13
2 2

!"#$%&' !"#$% !"##$%

4

3

1

5

2

6
7

4

3

1

5

2

6
7

4

3

1

5

2

6
7 11

12

14

16
17

15

13!"#$%&' !"#$% !"##$%

4

3

1

5

2

6
7

4

3

1

5

2

6
7

4

3

1

5

2

6
7 11

12

14

16
17

15

13!"#$%& !"#$ !"##$

!"

ℎ"#

ℎ"#

ℎ"#

!"#

ℎ"

!"#

!"#

!"#

!"#$% !"#!"# !"#$
!"

!ℎ#!
!ℎ#$
!"#
!"#$%

!

!"

!"

…

…
Trigger	Label

Multi	Order	Graph	Attention	NetworkWord	Encoding

4

3

1

5

2

6
7

Attention	Aggregation

dependency			parsing

Figure 2: The illustrative architecture of the proposed MOGANED method, where K=3.

uij = softmax(eij) =
exp(eij)∑

j∈Ni exp(eij)
, (4)

where eij = γ(Wcomb[Wattpi||Wattpj ]). Here,
Ni is the neighbor set of wi in the subgraph,
γ is LeakyReLU (with negative input slope α =
0.2) (Maas et al., 2013); Wcomb and Watt are
weight matrices.

After graph attention convolution, each candi-
date trigger word wi gets a list of multi-order rep-
resentations hki , k ∈ [1,K], whereK is the highest
order used in this module.

2.3 Attention Aggregation
To aggregate the list of multi-order representations
hki of each word wi, we employ a weighted atten-
tion aggregation mechanism from Hierarchical At-
tention Networks (Yang et al., 2016):

hi =
K∑
k=1

vki h
k
i , (5)

where vki is the normalized weight of the k-th or-
der graph representation of the word wi, which is
calculated by

vki = softmax(s
k
i ) =

exp((ski )
T
ctx)∑K

j=1 exp((s
j
i )

T
ctx)

,

(6)
where sji = tanh(Wawah

j
i + �awa). Here, Wawa

and �awa are the weight matrix and the bias term,
respectively; ctx is a randomly initialized context
vector, which captures the importance of graph
representations of each order.

Finally, we use the aggregated representation hi
to predict the trigger label of word wi as follows:

yti = softmax(O
t
i) =

exp(Oti)∑2L+1
q=1 exp(O

q
i )
, (7)

where yqi denotes the probability assigning label q
to word wi and Oi = wohi + �o. Here, wo and
�o are the weight matrix and the bias item, respec-
tively.

2.4 Bias Loss Function

Since the number of “O” labels in the data is much
larger than that of event labels, we use a bias loss
function (Chen et al., 2018) to enhance the influ-
ence of event labels during training:

J(θ) = −
Ns∑
i=1

Ni,w∑
j=1

(log p(ytj |si, θ) · I(O)+

λ log p(ytj |si, θ) · (1− I(O))),

(8)

where Ns is the number of sentences; Ni,w is the
number of words in si; I(O) equals 1, if the label
of the word is “O”; otherwise 0; λ is the weight
parameter larger than 1.

3 Experiments

3.1 Experiment Settings

Dataset and Resources We compare our MO-
GANED model with baseline methods on ACE-
2005 with the same data split, where 40 newswire
documents are used as the test set, 30 newswire
documents as the validation set and 529 remained
documents as the training set.

For data preprocessing, we use the Stanford
CoreNLP toolkit for sentence splitting, tokeniz-
ing, POS-tagging and dependency parsing. We
adopt word embeddings trained over the New
York Times corpus with the Skip-gram algo-
rithm (Mikolov et al., 2013; Chen et al., 2018).
Hyper Parameter Setting The hyper parameters
are tuned on the validation set. For all experiments



5769

Method P R F1
Cross Event 68.7 68.9 68.8

DMCNN 75.6 63.6 69.1
JRNN 66.0 73.9 69.3

DEEB-RNN 72.3 75.8 74.0
dbRNN† 74.1 69.8 71.9

GCN-ED† 77.9 68.8 73.1
JMEE† 76.3 71.3 73.7

MOGANED† 79.5 72.3 75.7

Table 1: Overall performance of different methods on
the test set with gold-standard entities. † indicates that
the method uses dependency arcs.

below, we use 100 as the dimension of word em-
beddings and 50 as that of the rest embeddings.
We set the hidden units of the BiLSTM network
to 100. We set the highest order K to 3 and the
dimension of graph representation to 150. We set
batch size to 30 and utilize a fixed maximum sen-
tence length n = 50 by padding short sentences
and cutting longer ones. During training, we use
the AdaDelta update rule (Zeiler, 2012) with a
learning rate of 0.001. We set the dropout rate to
0.3 and the L2-norm to 1e−5. We set the bias loss
parameter λ to 5.

3.2 Overall Performance

We compare our model with the following state-
of-the-art baselines: 1) Cross Event (Liao and Gr-
ishman, 2010), which uses document level infor-
mation for event extraction; 2) DMCNN (Chen
et al., 2015), which builds a dynamic multi-
pooling CNN model; 3) JRNN (Nguyen et al.,
2016), which uses a bidirectional RNN and human
designed features; 4) DEEB-RNN (Zhao et al.,
2018), which uses hierarchical supervised atten-
tion with document level information for event de-
tection; 5) dbRNN (Sha et al., 2018), which adds
dependency arcs over a Bi-LSTM network to im-
prove event extraction; 6) GCN-ED (Nguyen and
Grishman, 2018), which uses an argument pool-
ing mechanism for event detection based on GCN;
7) JMEE (Liu et al., 2018), which uses GCN with
highway network and self-attention.

Table 1 presents the performance comparison
between different methods. We can see that
MOGANED achieves 1.6% and 1.7% improve-
ment on precision and F1-measure, respectively,
compared with the best baselines. MOGANED
reaches a lower recall than two sequence based

Method P R F1
MOGANED-First 71.0 69.9 70.4
MOGANED-GCN 75.7 71.1 73.3
MOGANED-Mean 77.3 70.9 74.0

MOGANED 79.5 72.3 75.7

Table 2: Performance of modified architectures based
on MOGANED.

methods, JRNN and DEEB-RNN. This may be
caused by the propagated error from the depen-
dency parsing tool. There are long sentences
where trigger words and their corresponding en-
tities are parsed into two independent dependency
trees without connection. Therefore, the trigger
words and their related entities are unable to inter-
act in multi-order graph attention network, which
leads to a lower recall. However, MOGANED
still achieves the best performance in terms of pre-
cision, recall, F1-measure among all dependency
based methods, which suggests the effective of
multi-order representations.

3.3 Ablation Study

This study aims to validate the impacts of multi-
order representation, graph attention network and
attention aggregation. For this purpose, we de-
sign three architectures based on MOGANED: 1)
MOGANED-First: it only uses first-order syntac-
tic graph (i.e.,K=1); 2) MOGANED-GCN: it uses
traditional GCN instead of GAT; 3) MOGANED-
Mean: it adopts mean pooling as the attention
aggregation mechanism of multi-order representa-
tions for each word.

The experimental result is shown in Table 2.
All of these three modified models get lower
performance than MOGANED. MOGANED-First
achieves the worst performance, which suggests
that high-order syntactic relations play an impor-
tant role in event detection. MOGANED-GCN
drops more on precision than recall, which illus-
trates that the attention learned from GAT helps
MOGANED predict trigger words more precisely.
The performance drop of MOGANED-Mean is
the smallest among the three modified models. Al-
though the average of multi-order representations
achieves competitive performance for event de-
tection, the proposed attention aggregation mod-
ule still distinguishes the importance of syntactic
representations of different order, which achieves
1.7% improvement on F1-measure.



5770

4 Conclusion

In this paper, we proposed the MOGANED model
for modeling multi-order representations via GAT
and employed an attention aggregation mecha-
nism to better capture dependency contextual in-
formation for event detection. The experimental
results well demonstrate its effectiveness and su-
periority.

Acknowledgments

We are grateful to Bingbing Xu, Saiping Guan,
Shaobo Liu, Zixuan Li, Long Bai and Yunqi Qiu
for valuable discussion. We also appreciate anony-
mous reviewers for constructive review comments.

This work is supported by National Key Re-
search and Development Program of China under
grant 2016YFB1000902, National Natural Sci-
ence Foundation of China under grants 61772501,
61572473, 61572469 and 91646120, and the GF
Innovative Research Program.

References
Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,

and Jun Zhao. 2015. Event extraction via dy-
namic multi-pooling convolutional neural networks.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 167–176.

Yubo Chen, Hang Yang, Kang Liu, Jun Zhao, and Yan-
tao Jia. 2018. Collective event detection via a hier-
archical and bias tagging networks with gated multi-
level attention mechanisms. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 1267–1276.

Djork-Arné Clevert, Thomas Unterthiner, and Sepp
Hochreiter. 2015. Fast and accurate deep network
learning by exponential linear units (elus). arXiv
preprint arXiv:1511.07289.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 789–797. Association for Computational
Linguistics.

Xiao Liu, Zhunchen Luo, and Heyan Huang. 2018.
Jointly multiple events extraction via attention-
based graph information aggregation. arXiv
preprint arXiv:1809.09078.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In Proc. icml, volume 30,
page 3.

Diego Marcheggiani and Ivan Titov. 2017. En-
coding sentences with graph convolutional net-
works for semantic role labeling. arXiv preprint
arXiv:1703.04826.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 300–309.

Thien Huu Nguyen and Ralph Grishman. 2018. Graph
convolutional networks with argument-aware pool-
ing for event detection. In Thirty-Second AAAI Con-
ference on Artificial Intelligence.

Lei Sha, Feng Qian, Baobao Chang, and Zhifang Sui.
2018. Jointly extracting event triggers and argu-
ments by dependency-bridge rnn and tensor-based
argument interaction. In Thirty-Second AAAI Con-
ference on Artificial Intelligence.

Petar Veličković, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Lio, and Yoshua Bengio.
2017. Graph attention networks. arXiv preprint
arXiv:1710.10903.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

Yue Zhao, Xiaolong Jin, Yuanzhuo Wang, and Xueqi
Cheng. 2018. Document embedding enhanced event
detection with hierarchical and supervised attention.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 414–419.

Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang,
Zhiyuan Liu, and Maosong Sun. 2018. Graph neu-
ral networks: A review of methods and applications.
arXiv preprint arXiv:1812.08434.


