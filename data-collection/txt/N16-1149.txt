



















































Incorporating Side Information into Recurrent Neural Network Language Models


Proceedings of NAACL-HLT 2016, pages 1250–1255,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Incorporating Side Information into Recurrent Neural Network Language
Models

Cong Duy Vu Hoang
University of Melbourne

Melbourne, VIC, Australia
vhoang2@student.unimelb.edu.au

Gholamreza Haffari
Monash University

Clayton, VIC, Australia
gholamreza.haffari@monash.edu

Trevor Cohn
University of Melbourne

Melbourne, VIC, Australia
t.cohn@unimelb.edu.au

Abstract

Recurrent neural network language models
(RNNLM) have recently demonstrated vast
potential in modelling long-term dependen-
cies for NLP problems, ranging from speech
recognition to machine translation. In this
work, we propose methods for conditioning
RNNLMs on external side information, e.g.,
metadata such as keywords, description, doc-
ument title or topic headline. Our experiments
show consistent improvements of RNNLMs
using side information over the baselines for
two different datasets and genres in two lan-
guages. Interestingly, we found that side in-
formation in a foreign language can be highly
beneficial in modelling texts in another lan-
guage, serving as a form of cross-lingual lan-
guage modelling.

1 Introduction

Neural network approaches to language modelling
(LM) have made remarkable performance gains over
traditional count-based ngram LMs (Bengio et al.,
2003; Mnih and Hinton, 2007; Mikolov et al., 2011).
They offer several desirable characteristics, includ-
ing the capacity to generalise over large vocabular-
ies through the use of vector space representation,
and – for recurrent models (Mikolov et al., 2011)
– the ability to encode long distance dependencies
that are impossible to include with a limited context
windows used in conventional ngram LMs. These
early papers have spawned a cottage industry in neu-
ral LM based applications, where text generation
is a key component, including conditional language
models for image captioning (Kiros et al., 2014;

Vinyals et al., 2015) and neural machine translation
(Kalchbrenner and Blunsom, 2013; Sutskever et al.,
2014; Bahdanau et al., 2015).

Inspired by these works for conditioning LMs on
complex side information, such as images and for-
eign text, in this paper we investigate the possibility
of improving LMs in a more traditional setting, that
is when applied directly to text documents. Typi-
cally corpora include rich side information, such as
document titles, authorship, time stamp, keywords
and so on, although this information is usually dis-
carded when applying statistical models. However,
this information can be highly informative, for in-
stance, keywords, titles or descriptions, often in-
clude central topics which will be helpful in mod-
elling or understanding the document text. We pro-
pose mechanisms for encoding this side informa-
tion into a vector space representation, and means
of incorporating it into the generating process in a
RNNLM framework. Evaluating on two corpora and
two different languages, we show consistently sig-
nificant perplexity reductions over the state-of-the-
art RNNLM models.

The contributions of this paper are as follows:

1. We propose a framework for encoding struc-
tured and unstructured side information, and its
incorporation into a RNNLM.

2. We introduce a new corpus, the RIE corpus,
based on the Europarl web archive, with rich
annotations of several types of meta-data.

3. We provide empirical analysis showing consis-
tent improvements from using side information
across two datasets in two languages.

1250



2 Problem Formulation & Model

We first review RNNLM architecture (Mikolov et
al., 2011) before describing our extension in §2.2.

2.1 RNNLM Architecture

The standard RNNLM consists of 3 main layers: an
input layer where each input word has its embedding
via one-hot vector coding; a hidden layer consisting
of recurrent units where a state is conditioned recur-
sively on past states; and an output layer where a
target word will be predicted. RNNLM has an ad-
vantage over conventional n-gram language model
in modelling long distance dependencies effectively.

In general, an RNN operates from left-to-right
over the input word sequence; i.e.,

ht = RU (xt, ht−1)

= f
(
W (hh)ht−1 + W (ih)xt + b(h)

)
xt+1 ∼ softmax

(
W (ho)ht + b(o)

)
;

where f(.) is a non-linear function, e.g., tanh, ap-
plied element-wise to its vector input; ht is the cur-
rent RNN hidden state at time-step t; and matrices
W and vectors b are model parameters. The model
is trained using gradient-based methods to optimise
a (regularised) training objective, e.g. the likelihood
function. In principle, a recurrent unit (RU) can be
employed using different variants of recurrent struc-
tures such as: Long Short Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997), Gated Recur-
rent Unit (GRU) (Cho et al., 2014), or recently
deeper structures, e.g. Depth Gated Long Short
Term Memory (DGLSTM) – a stack of LSTMs with
extra connections between memory cells in deep
layers (Yao et al., 2015). It can be regarded as
being a generalisation of LSTM recurrence to both
time and depth. Such deep recurrent structure may
capture long distance patterns at their most general.
Empirically, we found that RNNLM with DGLSTM
structure appears to be best performer across our
datasets, and therefore is used predominantly in our
experiments.

2.2 Incorporating Side Information

Nowadays, many corpora are archived with side in-
formation or contextual meta-data. In this work, we

htht-1

xt

xt+1

e

htht-1

xt

xt+1

e

a) b)

Figure 1: Integration methods for auxiliary information, e: a)
as input to the RNN, or b) as part of the output softmax layer.

argue that such information can be useful for lan-
guage modelling (and presumably other NLP tasks).
By providing this auxiliary information directly to
the RNNLM, we stand to boost language modelling
performance.

The first question in using side information is how
to encode these unstructured inputs, y, into a vector
representation, denoted e. We discuss several meth-
ods for encoding the auxiliary vector:
BOW additive bag of words, e =

∑
t yt, and

average the average embedding vector,

e =
1
T

∑
t yt, both inspired by (Hermann

and Blunsom, 2014a);
bigram convolution with sum-pooling,

e =
∑

t tanh (yt−1 + yt) (Hermann and
Blunsom, 2014b); and

RNN a recurrent neural network over the word se-
quence (Sutskever et al., 2014), using the final
hidden state(s) as e.

From the above methods, we found that BOW
worked consistently well, outperforming the other
approaches, and moreover lead to a simpler model
with faster training. For this reason we report only
results for the BOW encoding. Note that when using
multiple auxiliary inputs, we use a weighted combi-
nation, e =

∑
i W

(ai)e(i).
The next step is the integration of e into the

RNNLM. We consider two integration methods: as
input to the hidden state (denoted input), and con-
nected to the output softmax layer (output), as
shown in Figure 1 a and b, respectively. In both
cases, we compare experimentally the following in-
tegration strategies:
add adding the vectors together, e.g., using xt +

e as the input to the RNN, such that

1251



ht = RU (xt + e, ht−1);
stack concatenating the vectors, e.g., using[

x>t e>
]> for generating the RNN hidden

state, such that ht = RU
([

xt
e

]
, ht−1

)
;

and
mlp feeding both vectors into an extra perceptron

with single hidden layer, using a tanh non-
linearity and projecting the output to the re-
quired dimensionality; i.e.,

h′t = tanh
(
W (hh

′)ht + W (he)e + b(h
′)
)

xt+1 ∼ softmax
(
W (ho)h′t + b

(o)
)

.

Note that add requires the vectors to be the same di-
mensionality, while the other two methods do not.
The stack method can be quite costly, given that it
increases the size of several matrices, either in the
recurrent unit (for input) or the output mapping for
word generation. This is a problem in the latter case:
given the large size of the vocabulary, the matrix
W (ho) is already very large and making it larger
(doubling the size, to become W (h

′o)) has a size-
able effect on training time (and presumably also
propensity to over-fit). The output+stack method
does however have a compelling interpretation as a
jointly trained product model between a RNNLM
and a unigram model conditioned on the side in-
formation, where both models are formulated as
softmax classifiers. Considered as a product model
(Hinton, 2002; Pascanu et al., 2013), the two com-
ponents can concentrate on different aspects of the
problem where the other model is not confident, and
allowed each model the ability to ‘veto’ certain out-
puts, by assigning them a low probability.

3 Experiments

Datasets. We conducted our experiments on two
datasets with different genres in two languages. As
the first dataset, we use the IWSLT2014 MT track
on TED Talks1 due to its self-contained rich auxil-
iary information, including: title, description, key-
words, and author related information. We chose
the English-French pair for our experiments2 . The
statistics of the training set is shown in Table 1. We

1https://wit3.fbk.eu/ (IWSLT’14 MT Track)
2Our method can be also applied to other language pairs.

tokens (M) types (K) docs sents (K)

TED-en 4.0 18.3 1414 179
TED-fr 4.3 22.6 1414 179
RIE-en 13.7 15.0 200 460
RIE-fr 14.9 19.4 200 460

Table 1: Statistics of the training sets, showing in each cell the
number of word tokens, types, documents (talks or plenaries),

and sentences. Note that “types” here refers to word frequency

thresholded at 5 and 15 for TED Talks and RIE datasets, respec-

tively.

used dev2010 (7 talks/817 sentences) for early stop-
ping of training neural network models. For evalu-
ation, we used different testing sets over years, in-
cluding tst2010 (10/1587), tst2011 (7/768), tst2012
(10/1083).

As the second dataset, we crawled the entire Euro-
pean Parliament3 website, focusing on plenary ses-
sions. Such sessions contain useful structural in-
formation, namely multilingual texts divided into
speaker sessions and topics. We believe that those
texts are interesting and challenging for language
modelling tasks. Our dataset contains 724 plenary
sessions over 12.5 years until June 2011 with mul-
tilingual texts in 22 languages4. We refer to this
dataset by RIE5 (Rich Information Europarl). We
randomly select 200/5/30 plenary sessions as the
training/development/test sets, respectively. We be-
lieve that the new data including side information
pose another challenge for language modelling. Fur-
thermore, the sizes of our working datasets are an or-
der of magnitude larger than the standard Penn Tree-
bank set which is often used for evaluating neural
language models.

Set-up and Baselines. We have used cnn6 to im-
plement our models. We use the same configura-
tions for all neural models: 512 input embedding
and hidden layer dimensions, 2 hidden layers, and
vocabulary sizes as given in Table 1. We used
the same vocabulary for the auxiliary and modelled
text. We trained a conventional 5−gram language
model using modified Kneser-Ney smoothing, with
the KenLM toolkit (Heafield, 2011). We used the

3http://www.europarl.europa.eu/
4We ignored the period from June 2011 onwards, as from

this date the EU stopped creating manual human translations.
5This dataset will be released upon publication.
6https://github.com/clab/cnn/

1252



Method test2010 test2011 test2012
5-gram LM 79.9 77.4 89.9
RNNLM 65.8 63.9 73.0
LSTM 54.1 52.2 58.4
DGLSTM 53.1 52.1 58.8
input+add+k 52.9 52.1 57.5
input+mlp+k 53.3 51.5 57.3
input+stack+k 53.7 51.9 58.1
output+mlp+k 51.7 50.6 55.8
output+mlp+t 52.3 53.5 58.3
output+mlp+d 52.0 49.8 56.3
output+mlp+k+t 51.4 51.1 56.8
output+mlp+k+d 51.2 49.7 55.1
output+mlp+t+d 52.6 51.5 57.2
output+mlp+k+t+d 51.1 50.6 56.3

Table 2: Perplexity scores based on the English part of TED
talks dataset in IWSLT14 MT. +k, +t, +d: with keywords, title,

and description as auxiliary side information respectively. bold:
Statistically significant better than the best baseline.

Wilcoxon signed-rank test (Wilcoxon, 1945) to mea-
sure the statistical significance (p < 0.05) on dif-
ferences between sentence-level perplexity scores
of improved models compared to the best base-
line. Throughout our experiments, punctuation, stop
words and sentence markers (〈s〉, 〈/s〉, 〈unk〉) are fil-
tered out in all auxiliary inputs. We observed that
this filtering was required for BOW to work rea-
sonably well. For each model, the best perplexity
score on development set is used for early stopping
of training models, which was obtained after 2-5 and
2-3 epochs on TED Talks and RIE datasets, respec-
tively.

Results & Analysis. The perplexity results on
TED Talks dataset are presented in Table 2 and
3. RNNLM variants consistently achieve substan-
tially better perplexities compared to the conven-
tional 5−gram language model baseline.7 Of the ba-
sic RNNLM models (middle), the DGLSTM works
consistently better than both the standard RNN and
the LSTM. This may be due to better interactions of
memory cells in hidden layers. Since the DGLSTM
outperformed others8, we used it for all subsequent
experiments. For TED Talks dataset, there are three

7For fair comparison, when computing the perplexity with
the 5-gram LM, we exclude all test words marked as 〈unk〉 (i.e.,
with low counts or OOVs) from consideration.

8This concurs with the finding in (Yao et al., 2015), who
showed that DGLSTM produced the state-of-the-art results over
Penn Treebank dataset.

Method test2010 test2011 test2012
5-gram LM 65.1 60.3 64.8
LSTM 45.0 42.5 44.0
DGLSTM 44.0 41.9 43.0
output+mlp+t 42.1 40.6 42.5
output+mlp+d 40.9 38.9 40.3
output+mlp+t+d 41.7 39.8 42.8
output+mlp+k 40.8 38.3 39.7
output+mlp+d+k 40.2 38.3 39.4

Table 3: Perplexity scores based on the French part of TED
talks dataset in IWSLT14 MT. Note that +k means with key-

words in English.

kinds of side information, including keywords, ti-
tle, description. We attempted to inject those into
different RNNLM layers, resulting in model vari-
ants as shown in Table 2. First, we chose “key-
words” (+k) information as an anchor to figure out
which incorporation method works well. Comparing
input+add+k, input+mlp+k and input+stack+k, the
largest decrease is obtained by output+mlp+k con-
sistently across all test sets (and development sets,
not shown here). We further evaluated the addition
of other side information (e.g., “description” (+d),
“title” (+t)), finding that +d has similar effect as +k
whereas +t has a mixed effect, being detrimental for
one test set (test2011). We suspect that it is due to
often-times short sentences of titles in that test, af-
ter our filtering step, leading to a shortage of useful
information fed into neural network learning. Inter-
estingly, the best performance is obtained when in-
corporating both +k and +d, showing that there is
complementary information in the two auxiliary in-
puts. Further, we also achieved the similar results
in the counterpart of English part (in French) using
output+mlp with both +t and +d as shown in Ta-
ble 3. In French data, no “keywords” information is
available. For this reason, we run additional exper-
iments by injecting English keywords as side infor-
mation into neural models of French. Interestingly,
we found that “keywords” side information in En-
glish effectively improves the modelling of French
texts as shown in Table 3, serving as a new form of
cross-lingual language modelling.

We further achieved similar results by incorpo-
rating the topic headline in the RIE dataset. The
consistently-improved results (in Table 4) demon-
strate the robustness of the output+mlp approach.

1253



Method test (en) test (fr)
5-gram LM 55.7 38.5
LSTM 40.3 28.5
DGLSTM 36.4 25.4
output+mlp+h 33.3 24.0

Table 4: Perplexity scores based on the sampled RIE dataset.
+h: topic headline.

4 Conclusion

We have proposed an effective approach to boost the
performance of RNNLM using auxiliary side infor-
mation (e.g. keywords, title, description, topic head-
line) of a textual utterance. We provided an empir-
ical analysis of various ways of injecting such in-
formation into a distributed representation, which
is then incorporated into either the input, hidden,
or output layer of RNNLM architecture. Our ex-
perimental results reveal consistent improvements
are achieved over strong baselines for different
datasets and genres in two languages. Our future
work will investigate the model performance on a
closely-related task, i.e., neural machine translation
(Sutskever et al., 2014; Bahdanau et al., 2015). Fur-
thermore, we will explore learning methods to com-
bine utterances with and without the auxiliary side
information.

Acknowledgements

The authors would like to thank the reviewers for
valuable comments and feedbacks. Cong Duy Vu
Hoang was supported by research scholarships from
the University of Melbourne, Australia. Dr Trevor
Cohn was supported by the ARC (Future Fellow-
ship).

References

D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural
Machine Translation by Jointly Learning to Align and
Translate. In Proceedings of International Conference
on Learning Representations (ICLR 2015), September.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the Proper-
ties of Neural Machine Translation: Encoder–Decoder

Approaches. In Proceedings of SSST-8, Eighth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation, pages 103–111, Doha, Qatar, October.
Association for Computational Linguistics.

Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Machine
Translation, pages 187–197, Edinburgh, Scotland,
United Kingdom, July.

K. M. Hermann and P. Blunsom. 2014a. Multilingual
Distributed Representations without Word Alignment.
In Proceedings of International Conference on Learn-
ing Representations (ICLR 2014), December.

Karl Moritz Hermann and Phil Blunsom. 2014b. Multi-
lingual Models for Compositional Distributed Seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 58–68, Baltimore, Mary-
land, June. Association for Computational Linguistics.

Geoffrey E Hinton. 2002. Training Products of Experts
by Minimizing Contrastive Divergence. Neural com-
putation, 14(8):1771–1800.

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long
Short-Term Memory. Neural Comput., 9(8):1735–
1780, November.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP 2013).

Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel.
2014. Multimodal Neural Language Models. In Pro-
ceedings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 595–603.

T. Mikolov, S. Kombrink, A. Deoras, and J. H. Burget,
L.and Cernocky. 2011. RNNLM - Recurrent Neural
Network Language Modeling Toolkit. In 2011 IEEE
Workshop on Automatic Speech Recognition & Under-
standing (ASRU). IEEE Automatic Speech Recogni-
tion and Understanding Workshop, December.

Andriy Mnih and Geoffrey Hinton. 2007. Three New
Graphical Models for Statistical Language Modelling.
In Proceedings of the 24th International Conference
on Machine Learning, pages 641–648.

R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. 2013.
How to Construct Deep Recurrent Neural Networks.
ArXiv e-prints, December.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Se-
quence to Sequence Learning with Neural Networks.
In Advances in Neural Information Processing Sys-
tems (NIPS 2014), pages 3104–3112.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-
mitru Erhan. 2015. Show and Tell: A Neural Image
Caption Generator. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June.

1254



Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1 (6):80–83,
Dec.

K. Yao, T. Cohn, K. Vylomova, K. Duh, and C. Dyer.
2015. Depth-Gated LSTM. ArXiv e-prints, August.

1255


