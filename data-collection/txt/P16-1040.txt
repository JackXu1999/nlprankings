



















































Transition-Based Neural Word Segmentation


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 421–431,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Transition-Based Neural Word Segmentation

Meishan Zhang1 and Yue Zhang2 and Guohong Fu1
1. School of Computer Science and Technology, Heilongjiang University, Harbin, China

2. Singapore University of Technology and Design
mason.zms@gmail.com,
yue zhang@sutd.edu.sg,

ghfu@hotmail.com

Abstract

Character-based and word-based methods
are two main types of statistical models
for Chinese word segmentation, the for-
mer exploiting sequence labeling models
over characters and the latter typically ex-
ploiting a transition-based model, with the
advantages that word-level features can
be easily utilized. Neural models have
been exploited for character-based Chi-
nese word segmentation, giving high accu-
racies by making use of external character
embeddings, yet requiring less feature en-
gineering. In this paper, we study a neu-
ral model for word-based Chinese word
segmentation, by replacing the manually-
designed discrete features with neural fea-
tures in a word-based segmentation frame-
work. Experimental results demonstrate
that word features lead to comparable per-
formances to the best systems in the litera-
ture, and a further combination of discrete
and neural features gives top accuracies.

1 Introduction

Statistical word segmentation methods can be cat-
egorized character-based (Xue, 2003; Tseng et al.,
2005) and word-based (Andrew, 2006; Zhang and
Clark, 2007) approaches. The former casts word
segmentation as a sequence labeling problem, us-
ing segmentation tags on characters to mark their
relative positions inside words. The latter, in con-
trast, ranks candidate segmented outputs directly,
extracting both character and full-word features.

An influential character-based word segmenta-
tion model (Peng et al., 2004; Tseng et al., 2005)
uses B/I/E/S labels to mark a character as the be-
ginning, internal (neither beginning nor end), end
and only-character (both beginning and end) of a

character-based word-based

discrete
Peng et al. (2004) Andrew (2006)
Tseng et al. (2005) Zhang and Clark (2007)

neural
Zheng et al. (2013)

this work
Chen et al. (2015b)

Figure 1: Word segmentation methods.

word, respectively, employing conditional random
field (CRF) to model the correspondence between
the input character sequence and output label se-
quence. For each character, features are extracted
from a five-character context window and a two-
label history window. Subsequent work explores
different label sets (Zhao et al., 2006), feature sets
(Shi and Wang, 2007) and semi-supervised learn-
ing (Sun and Xu, 2011), reporting state-of-the-art
accuracies.

Recently, neural network models have been in-
vestigated for the character tagging approach. The
main idea is to replace manual discrete features
with automatic real-valued features, which are de-
rived automatically from distributed character rep-
resentations using neural networks. In particular,
convolution neural network1 (Zheng et al., 2013),
tensor neural network (Pei et al., 2014), recur-
sive neural network (Chen et al., 2015a) and long-
short-term-memory (LSTM) (Chen et al., 2015b)
have been used to derive neural feature represen-
tations from input word sequences, which are fed
into a CRF inference layer.

In this paper, we investigate the effectiveness of
word embedding features for neural network seg-
mentation using transition-based models. Since
it is challenging to integrate word features to
the CRF inference framework of the existing

1The term in this paper is used to denote the neural net-
work structure with convolutional layers, which is different
from the typical convolution neural network that has a pool-
ing layer upon convolutional layers (Krizhevsky et al., 2012).

421



step action buffer(· · ·w−1w0) queue(c0c1 · · · )
0 - φ 中 国 · · ·
1 SEP 中 国 外 · · ·
2 APP 中国 外 企 · · ·
3 SEP 中国 外 企 业 · · ·
4 APP 中国 外企 业 务 · · ·
5 SEP 中国 外企 业 务 发 · · ·
6 APP 中国 外企 业务 发 展 · · ·
7 SEP · · · 业务 发 展 迅 速
8 APP · · · 业务 发展 迅 速
9 SEP · · · 发展 迅 速
10 APP · · · 发展 迅速 φ

Figure 2: Segmentation process of “中国 (Chi-
nese) 外企 (foreign company) 业务 (busi-
ness) 发展 (develop) 迅速 (quickly)”.

character-based methods, we take inspiration from
word-based discrete segmentation instead. In par-
ticular, we follow Zhang and Clark (2007), using
the transition-based framework to decode a sen-
tence from left-to-right incrementally, scoring par-
tially segmented results using both character-level
and word-level features. Beam-search is applied to
reduce error propagation and large-margin train-
ing with early-update (Collins and Roark, 2004) is
used for learning from inexact search.

We replace the discrete word and character fea-
tures of Zhang and Clark (2007) with word and
character embeddings, respectively, and change
their linear model into a deep neural network.
Following Zheng et al. (2013) and Chen et al.
(2015b), we use convolution neural networks to
achieve local feature combination and LSTM to
learn global sentence-level features, respectively.
The resulting model is a word-based neural seg-
menter that can leverage rich embedding features.
Its correlation with existing work on Chinese seg-
mentation is shown in Figure 1.

Results on standard benchmark datasets show
the effectiveness of word embedding features for
neural segmentation. Our method achieves state-
of-the-art results without any preprocess based on
external knowledge such as Chinese idioms of
Chen et al. (2015a) and Chen et al. (2015b). We re-
lease our code under GPL for research reference.2

2 Baseline Discrete Model

We exploit the word-based segmentor of Zhang
and Clark (2011) as the baseline system. It in-
crementally segments a sentence using a transition
system, with a state holding a partially-segmented

2https://github.com/SUTDNLP/NNTransitionSegmentor.

sentence in a buffer s and ordering the next incom-
ing characters in a queue q. Given an input Chi-
nese sentence, the buffer is initially empty and the
queue contains all characters of the sentence, a se-
quence of transition actions are used to consume
characters in the queue and build the output sen-
tence in the buffer. The actions include:

• Append (APP), which removes the first
character from the queue, and appends it to
the last word in the buffer;

• Separate (SEP), which moves the first
character of the queue onto the buffer as a
new (sub) word.

Given the input sequence of characters “中国
外企业务发展迅速” (The business of foreign
company in China develops quickly), the correct
output can be derived using action sequence “SEP
APP SEP APP SEP APP SEP APP SEP APP”, as
shown in Figure 2.
Search. Based on the transition system, the de-
coder searches for an optimal action sequence for
a given sentence. Denote an action sequence as
A = a1 · · · an. We define the score of A as the
total score of all actions in the sequence, which is
computed by:

score(A) =
∑
a∈A

score(a) =
∑
a∈A

w · f(s, q, a),

wherew is the model parameters, f is a feature ex-
traction function, s and q are the buffer and queue
of a certain state before the action a is applied.

The feature templates are shown in Table 1,
which are the same as Zhang and Clark (2011).
These base features include three main source of
information. First, characters in the front of the
queue and the end of the buffer are used for scor-
ing both separate and append actions (e.g. c0).
Second, words that are identified are used to guide
separate actions (e.g. w0). Third, relevant infor-
mation of identified words, such as their lengths
and first/last characters are utilized for additional
features (e.g. len(w−1)).

We follow Zhang and Clark (2011) in using
beam-search for decoding, shown in Algorith 1,
where Θ is the set of model parameters. Initially
the beam contains only the initial state. At each
step, each state in the beam is extended by apply-
ing both SEP and APP, resulting in a set of new
states, which are scored and ranked. The top B are

422



Feature templates Action

c−1c0 APP, SEP
w−1, w−1w−2, w−1c0, w−2len(w−1)

SEP
start(w−1)c0, end(w−1)c0
start(w−1)end(w−1), end(w−2)end(w−1)
w−2len(w−1), len(w−2)w−1
w−1, where len(w−1) = 1

Table 1: Feature templates for the baseline model,
where wi denotes the word in the buffer, ci de-
notes the character in the queue, as shown in Fig-
ure 2, start(.), end(.) and len(.) denote the first,
last character and length of a word, respectively.

Algorithm 1 Beam-search decoding.
function DECODE(c1 · · · cn, Θ)

agenda← { (φ, c1 · · · cn, score=0.0) }
for i in 1 · · ·n

beam← { }
for cand in agenda

new← SEP(cand, ci, Θ)
ADDITEM(beam, new)
new← APP(cand, ci, Θ)
ADDITEM(beam, new)

agenda← TOP-B(beam, B)
best← BESTITEM(agenda)
w1 · · ·wm← EXTRACTWORDS(best)

used as the beam states for the next step. The same
process replaces until all input character are pro-
cessed, and the highest-scored state in the beam is
taken for output. Online leaning with max-margin
is used, which is given in section 4.

3 Transition-Based Neural Model

We use a neural network model to replace the
discrete linear model for scoring transition action
sequences. For better comparison between dis-
crete and neural features, the overall segmentation
framework of the baseline is used, which includes
the incremental segmentation process, the beam-
search decoder and the training process integrated
with beam-search (Zhang and Clark, 2011). In ad-
dition, the neural network scorer takes the simi-
lar feature sources as the baseline, which includes
character information over the input, word infor-
mation of the partially constructed output, and the
history sequence of the actions that have been ap-
plied so far.

The overall architecture of the neural scorer
is shown in Figure 3. Given a certain state

score(SEP) score(APP)

· · ·
hsep · · ·

happ

· · ·
rc· · ·

rw · · ·
ra

word sequence character sequence action sequence

RNN RNN RNN

· · ·w−1w0 · · · c−1c0c1 · · · · · · a−1a0

Figure 3: Scorer for the neural transition-based
Chinese word segmentation model. We denote the
last word in the buffer as w0, the next incoming
character as c0 in the queue in consistent with Fig-
ure 2, and the last applied action as a0.

configuration (s, q), we use three separate re-
current neural networks (RNN) to model the
word sequence · · ·w−1w0, the character se-
quence · · · c−1c0c1 · · · , and the action sequence
· · · a−1a0, respectively, resulting in three dense
real-valued vectors {rw, rc and ra}, respectively.
All the three feature vectors are used scoring the
SEP action. For APP, on the other hand, we use
only the character and action features rc and ra
because the last word w0 in the buffer is a partial
word. Formally, given rw, rc, ra, the action scores
are computed by:

score(SEP) = wsephsep
score(APP) = wapphapp

where

hsep = tanh(Wsep[rw, rc, ra] + bsep)
happ = tanh(Wapp[rc, ra] + bapp)

Wsep,Wapp,bsep,bapp,wsep,wapp are model pa-
rameters.

The neural networks take the embedding forms
of words, characters and actions as input, for ex-
tracting rw, rc and ra, respectively. We exploit the
LSTM-RNN structure (Hochreiter and Schmidhu-
ber, 1997), which can better capture non-local syn-
tactic and semantic information from a sequential
input, yet reducing gradient explosion or dimin-
ishing during training.

In general, given a sequence of input vectors
x0 · · ·xn, the LSTM-RNN computes a sequence
of hidden vectors h0 · · ·hn, respectively, with
each hi being determined by the input xi and the
previous hidden vector hi−1. A cell structure ce is

423



...
wi

...
wi−1

......

...
xwi

......

(a) word representation

...
ai

...
ai−1

......

...
xai

......

(b) action representation

... ⊕ ...
ci , ci−1ci

... ⊕ ...
ci−1 , ci−2ci−1

... ⊕ ...
ci+1 , ci+1ci

...... ......

...
xci

...... ......

(c) character representation

Figure 4: Input representations of LSTMS for ra
(actions) rw (words) and rc (characters).

used to carry long-term memory information over
the history h0 · · ·hi for calculating hi, and infor-
mation flow is controlled by an input gate ig, an
output gate og and a forget gate fg. Formally, the
calculation of hi using hi−1 and xi is:

igi = σ(Wigxi + Uighi−1 + Vigcei−1 + big)
fgi = σ(Wfgxi + Ufghi−1 + Vfgcei−1 + bfg)
cei = fgi � cei−1+

igi � tanh(Wcexi + Ucehi−1 + bce)
ogi = σ(Wogxi + Uoghi−1 + Vogcei + bog)
hi = ogi � tanh(cei),

where U, V,W,b are model parameters, and� de-
notes Hadamard product.

When used to calculate rw, rc and ra, the gen-
eral LSTM structure above is given different input
sequences x0 · · ·xn, according to the word, char-
acter and action sequences, respectively.

3.1 Input representation

Words. Given a word w, we use a looking-up ma-
trix Ew to obtain its embedding ew(w). The ma-
trix can be obtained through pre-training on large
size of auto segmented corpus. As shown in Fig-
ure 4(a), we use a convolutional neural layer upon
a two-word window to obtain · · ·xw−1xw0 for the
LSTM for rw, with the following formula:

xwi = tanh
(
Ww[ew(wi−1), ew(wi)] + bw

)
Actions. We represent an action a with an em-
bedding ea(a) from a looking-up table Ea, and
apply the similar convolutional neural network to
obtain · · ·xa−1xa0 for ra, as shown in Figure 4(b).

Given the input action sequence · · · a−1a0, the xai
is computed by:

xai = tanh
(
Wa[ea(ai−1), ea(ai)] + ba

)
Characters. We make embeddings for both char-
acter unigrams and bigrams by looking-up ma-
trixes Ec and Ebc, respectively, the latter being
shown to be useful by Pei et al. (2014). For
each character ci, the unigram embedding ec(ci)
and the bigram embedding ebc(ci−1ci) are con-
catenated, before being given to a CNN with a
convolution size of 5. For the character sequence
· · · c−1c0c1 · · · of a given state (s, q), we compute
its input vectors · · ·xc−1xc0xc1 · · · for the LSTM for
rc by:

xci = tanh
(
Wc[ec(ci−2)⊕ ebc(ci−3ci−2),
· · · , ec(ci)⊕ ebc(ci−1ci), · · · ,
ec(ci+2)⊕ ebc(ci+1ci+2)] + bc

)
For all the above input representations, the

looking-up tables Ew, Ea , Ec, Ebc and the
weights Ww, Wa, Wc, bw, ba, bc are model pa-
rameters. For calculating rw and ra, we apply the
LSTMs directly over the sequences · · ·xw−1xw0 and
· · ·xa−1xa0 for words and actions, and use the out-
puts hw0 and h

a
0 for rw and ra, respectively. For

calculating rc, we further use a bi-directional ex-
tension of the original LSTM structure. In partic-
ular, the base LSTM is applied to the input char-
acter sequence both from left to right and from
right to left, leading to two hidden node sequences
· · ·hcL−1hcL0 hcL1 · · · and · · ·hcR−1hcR0 hcR1 · · · , re-
spectively. For the current character c0, hcL0 and
hcR0 are concatenated to form the final vector rc.
This is feasible because the character sequence is
input and static, and previous work has demon-
strated better capability of bi-directional LSTM
for modeling sequences (Yao and Zweig, 2015).

3.2 Integrating discrete features

Our model can be extended by integrating the
baseline discrete features into the feature layer. In
particular,

score(SEP) = w′sep(hsep ⊕ fsep)
score(APP) = w′app(happ ⊕ fapp),

where fsep and fapp represent the baseline sparse
vector for SEP and APP features, respectively, and
⊕ denotes the vector concatenation operation.

424



Algorithm 2 Max-margin training with early-
update.
function TRAIN(c1 · · · cn, ag1 · · · agn, Θ)

agenda← { (φ, c1 · · · cn, score=0.0) }
for i in 1 · · ·n

beam← { }
for cand in agenda

new← SEP(cand, ci, Θ)
if {agi 6= SEP} new.score += η
ADDITEM(beam, new)
new← APP(cand, ci, Θ)
if {agi 6= APP} new.score += η
ADDITEM(beam, new)

agenda← TOP-B(beam, B)
if {ITEM(ag1 · · · agi ) /∈ agenda}

Θ = Θ− f(BESTITEM(agenda))
Θ = Θ + f

(
ITEM((ag1 · · · agi )

)
return

if {ITEM(ag1 · · · agn) 6= BESTITEM(agenda)}
Θ = Θ− f(BESTITEM(agenda))
Θ = Θ + f

(
ITEM((ag1 · · · agn)

)
4 Training

To train model parameters for both the discrete
and neural models, we exploit online learning with
early-update as shown in Algorithm 2. A max-
margin objective is exploited,3 which is defined as:

L(Θ) =
1
K

K∑
k=1

l(Agk,Θ) +
λ

2
‖ Θ ‖2

l(Agk,Θ) = maxA
(
score(Ak,Θ) + η · δ(Ak, Agk)

)
− score(Agk,Θ),

where Θ is the set of all parameters, {Agk}Kn=1 are
gold action sequences to segment the training cor-
pus,Ak is the model output action sequence, λ is a
regularization parameter and η is used to tune the
loss margins.

For the discrete models, f(·) denotes the fea-
tures extracted according to the feature templates
in Table 1. For the neural models, f(·) denotes
the corresponding hsep and happ. Thus only the
output layer is updated, and we further use back-
propagation to learn the parameters of the other
layers (LeCun et al., 2012). We use online Ada-

3Zhou et al. (2015) find that max-margin training did not
yield reasonable results for neural transition-based parsing,
which is different from our findings. One likely reason is that
when the number of labels is small max-margin is effective.

CTB60 PKU MSR

Training #sent 23k 17k 78k#word 641k 1,010k 2,122k

Development #sent 2.1k 1.9k 8.7k#word 60k 100k 246k

Test #sent 2.8k 1.9k 4.0k#word 82k 104k 106k

Table 2: Statistics of datasets.

Type hyper-parameters
Network d(hsep) = 100, d(happ) = 80
structure d(hai ) = 20, d(x

a
i ) = 20

d(hwi ) = 50, d(x
w
i ) = 50

d(hcLi ) = d(h
cR
i ) = 50, d(x

c
i ) = 50

d(ew(wi)) = 50, d(ea(ai)) = 20
d(ec(ci)) = 50, d(ebc(ci−1ci)) = 50

Training λ = 10−8, α = 0.01, η = 0.2

Table 3: Hyper-parameter values in our model.

Grad (Duchi et al., 2011) to minimize the objec-
tive function for both the discrete and neural mod-
els. All the matrix and vector parameters are ini-
tialized by uniform sampling in (−0.01, 0.01).

5 Experiments

5.1 Experimental Settings

Data. We use three datasets for evaluation,
namely CTB6, PKU and MSR. The CTB6 corpus
is taken from Chinese Treebank 6.0, and the PKU
and MSR corpora can be obtained from Bake-
Off 2005 (Emerson, 2005). We follow Zhang et
al. (2014), splitting the CTB6 corpus into train-
ing, development and testing sections. For the
PKU and MSR corpora, only the training and test
datasets are specified and we randomly split 10%
of the training sections for development. Table 1
shows the overall statistics of the three datasets.
Embeddings. We use word2vec4 to pre-train
word, character and bi-character embeddings on
Chinese Gigaword corpus (LDC2011T13). In or-
der to train full word embeddings, the corpus is
segmented automatically by our baseline model.
Hyper-parameters. The hyper-parameter values
are tuned according to development performances.
We list their final values in Table 3.

5.2 Development Results

To better understand the word-based neural mod-
els, we perform several development experiments.
All the experiments in this section are conducted
on the CTB6 development dataset.

4http://word2vec.googlecode.com/

425



5 10 15 20

86

88

90

92

94

96

(a) discrete

5 10 15 20

b16 b8 b4 b2 b1

(b) neural(-tune)

5 10 15 20

(c) neural(+tune)

Figure 5: Accuracies against the training epoch
using beam sizes 1, 2, 4, 8 and 16, respectively.

5.2.1 Embeddings and beam size

We study the influence of beam size on the base-
line and neural models. Our neural model has two
choices of using pre-trained word embeddings.
We can either fine-tune or fix the embeddings dur-
ing training. In case of fine-tuning, only words in
the training data can be learned, while embeddings
of out-of-vocabulary (OOV) words could not be
used effectively.5 In addition, following Dyer et
al. (2015) we randomly set words with frequency
1 in the training data as the OOV words in order
to learn the OOV embedding, while avoiding over-
fitting. If the pretrained word embeddings are not
fine-tuned, we can utilize all word embeddings.

Figure 5 shows the development results, where
the training curve of the discrete baseline is shown
in Figure 5(a) and the curve of the neural model
without and with fine tuning are shown in 5(b) and
5(c), respectively. The performance increases with
a larger beam size in all settings. When the beam
increases into 16, the gains levels out. The results
of the discrete model and the neural model without
fine-tuning are highly similar, showing the useful-
ness of beam-search.

On the other hand, with fine-tuning, the results
are different. The model with beam size 1 gives
better accuracies compared to the other models
with the same beam size. However, as the beam
size increases, the performance increases very lit-
tle. The results are consistent with Dyer et al.
(2015), who find that beam-search improves the
results only slightly on dependency parsing. When
a beam size of 16 is used, this model performs the

5We perform experiments using random initialized word
embeddings as well when fine-tune is used, which is a fully
supervised model. The performance is slightly lower.

Model P R F
neural 95.21 95.69 95.45
-word 91.81 92.00 91.90

-character unigram 94.89 95.56 95.22
-character bigram 94.93 95.53 95.23

-action 95.00 95.31 95.17
+discrete features 96.38 96.22 96.30(combined)

Table 4: Feature experiments.

0.8 0.84 0.88 0.92 0.96 1
0.8

0.84

0.88

0.92

0.96

1

discrete
ne

ur
al

Figure 6: Sentence accuracy comparisons for the
discrete and the neural models.

worst compared with the discrete model and the
neural model without fine-tuning. This is likely
because the fine-tuning of embeddings leads to
overfitting of in-vocabulary words, and underfit-
ting over OOV words. Based on the observation,
we exploit fixed word embeddings in our final
models.

5.2.2 Feature ablation
We conduct feature ablation experiments to study
the effects of the word, character unigram, charac-
ter bigram and action features to the neural model.
The results are shown in Table 4. Word features
are particularly important to the model, without
which the performance decreases by 4.5%. The
effects of the character unigram, bigram and ac-
tion features are relatively much weaker.6 This
demonstrates that in the word-based incremental
search framework, words are the most crucial in-
formation to the neural model.

5.2.3 Integrating discrete features
Prior work has shown the effectiveness of integrat-
ing discrete and neural features for several NLP
tasks (Turian et al., 2010; Wang and Manning,

6In all our experiments, we fix the character unigram and
bigram embeddings, because fine-tuning of these embeddings
results in little changes.

426



Models P R F
word-based models
discrete 95.29 95.26 95.28
neural 95.34 94.69 95.01
combined 96.11 95.79 95.95
character-based models
discrete 95.38 95.12 95.25
neural 94.59 94.92 94.76
combined 95.63 95.60 95.61
other models
Zhang et al. (2014) N/A N/A 95.71
Wang et al. (2011) 95.83 95.75 95.79
Zhang and Clark (2011) 95.46 94.78 95.13

Table 5: Main results on CTB60 test dataset.

2013; Durrett and Klein, 2015; Zhang and Zhang,
2015). We investigate the usefulness of such inte-
gration to our word-based segmentor on the devel-
opment dataset. We study it by two ways. First,
we compare the error distributions between the
discrete and the neural models. Intuitively, differ-
ent error distributions are necessary for improve-
ments by integration. We draw a scatter graph to
show their differences, with the (x, y) values of
each point denoting the F-measure scores of the
two models with respect to sentences, respectively.
As shown in Figure 6, the points are rather disper-
sive, showing the differences of the two models.

Further, we directly look at the results after in-
tegration of both discrete and neural features. As
shown in Table 4, the integrated model improves
the accuracies from 95.45% to 96.30%, demon-
strating that the automatically-induced neural fea-
tures contain highly complementary information
to the manual discrete features.

5.3 Final Results

Table 6 shows the final results on CTB6 test
dataset. For thorough comparison, we implement
discrete, neural and combined character-based
models as well.7 In particular, the character-based
discrete model is a CRF tagging model using char-
acter unigrams, bigrams, trigrams and tag transi-
tions (Tseng et al., 2005), and the character-based
neural model exploits a bi-directional LSTM layer
to model character sequences8 and a CRF layer for

7The code is released for research reference under GPL at
https://github.com/SUTDNLP/NNSegmentation.

8We use a concatenation of character unigram and bigram
embeddings at each position as the input to LSTM, because
our experiments show that the character bigram embeddings
are useful, without which character-based neural models are
significantly lower than their discrete counterparts.

Models PKU MSR
our word-based models
discrete 95.1 97.3
neural 95.1 97.0
combined 95.7 97.7
character-based models
discrete 94.9 96.8
neural 94.4 97.2
combined 95.4 97.2
other models
Cai and Zhao (2016) 95.5 96.5
Ma and Hinrichs (2015) 95.1 96.6
Pei et al. (2014) 95.2 97.2
Zhang et al. (2013a) 96.1 97.5
Sun et al. (2012) 95.4 97.4
Zhang and Clark (2011) 95.1 97.1
Sun (2010) 95.2 96.9
Sun et al. (2009) 95.2 97.3

Table 6: Main results on PKU and MSR test
datasets.

output (Chen et al., 2015b).9 The combined model
uses the same method for integrating discrete and
neural features as our word-based model.

The word-based models achieve better perfor-
mances than character-based models, since our
model can exploit additional word information
learnt from large auto-segmented corpus. We also
compare the results with other models. Wang et
al. (2011) is a semi-supervised model that exploits
word statistics from auto-segmented raw corpus,
which is similar with our combined model in using
semi-supervised word information. We achieve
slightly better accuracies. Zhang et al. (2014) is a
joint segmentation, POS-tagging and dependency
parsing model, which can exploit syntactic infor-
mation.

To compare our models with other state-of-the-
art models in the literature, we report the perfor-
mance on the PKU and MSR datasets also.10 Our
combined model gives the best result on the MSR
dataset, and the second best on PKU. The method
of Zhang et al. (2013a) gives the best performance
on PKU by co-training on large-scale data.

5.4 Error Analysis
To study the differences between word-based and
character-based neural models, we conduct error
analysis on the test dataset of CTB60. First,

9Bi-directional LSTM is slightly better than a single left-
right LSTM used in Chen et al. (2015b).

10The results of Chen et al. (2015a) and Chen et al. (2015b)
are not listed, because they take a preprocessing step by re-
placing Chinese idioms with a uniform symbol in their test
data.

427



0.8 0.84 0.88 0.92 0.96 1
0.8

0.84

0.88

0.92

0.96

1

word

ch
ar

ac
te

r

Figure 7: Sentence accuracy comparisons for
word- and character-based neural models.

5 10 15 20 25 30 35 40 45 50+
92

94

96

98

F-
m

ea
su

re
s(

%
)

word character

Figure 8: F-measure against character length.

we examine the error distribution on individual
sentences. Figure 7 shows the F-measure val-
ues of each test sentence by word- and character-
based neural models, respectively, where the x-
axis value denotes the F-measure value of the
word-based neural model, and the y-axis value de-
notes its performance of the character-based neu-
ral model. We can see that the majority scat-
ter points are off the diagonal line, demonstrating
strong differences between the two models. This
results from the differences in feature sources.

Second, we study the F-measure distribution
of the two neural models with respect to sen-
tence lengths. We divide the test sentences into
ten bins, with bin i denoting sentence lengths in
[5 ∗ (i− 1), 5 ∗ i]. Figure 8 shows the results. Ac-
cording to the figure, we observe that word-based
neural model is relatively weaker for sentences
with length in [5, 10], while can better tackle long
sentences.

Third, we compare the two neural models by
their capabilities of modeling words with different
lengths. Figure 9 shows the results. The perfor-

1 2 3 4+
75

80

85

90

95

Figure 9: F-measure against word length, where
the boxes with red dots denote the performances of
word-based neural model, and the boxes with blue
slant lines denote character-based neural model.

mances are lower for words with lengths beyond 2,
and the performance drops significantly for words
with lengths over 3. Overall, the word-based neu-
ral model achieves comparable performances with
the character-based model, but gives significantly
better performances for long words, in particular
when the word length is over 3. This demonstrates
the advantage of word-level features.

6 Related Work

Xue (2003) was the first to propose a character-
tagging method to Chinese word segmentation, us-
ing a maximum entropy model to assign B/I/E/S
tags to each character in the input sentence sepa-
rately. Peng et al. (2004) showed that better results
can be achieved by global learning using a CRF
model. This method has been followed by most
subsequent models in the literature (Tseng et al.,
2005; Zhao, 2009; Sun et al., 2012). The most
effective features have been character unigrams,
bigrams and trigrams within a five-character win-
dow, and a bigram tag window. Special characters
such as alphabets, numbers and date/time charac-
ters are also differentiated for extracting features.

Zheng et al. (2013) built a neural network seg-
mentor, which essentially substitutes the manual
discrete features of Peng et al. (2004), with dense
real-valued features induced automatically from
character embeddings, using a deep neural net-
work structure (Collobert et al., 2011). A tag tran-
sition matrix is used for inference, which makes
the model effectively. Most subsequent work on
neural segmentation followed this method, im-
proving the extraction of emission features by us-
ing more complex neural network structures.

Mansur et al. (2013) experimented with embed-
dings of richer features, and in particular charac-

428



ter bigrams. Pei et al. (2014) used a tensor neu-
ral network to achieve extensive feature combi-
nations, capturing the interaction between charac-
ters and tags. Chen et al. (2015a) used a recur-
sive network structure to the same end, extract-
ing more combined features to model complicated
character combinations in a five-character win-
dow. Chen et al. (2015b) used a LSTM model to
capture long-range dependencies between charac-
ters in a sentence. Xu and Sun (2016) proposed a
dependency-based gated recursive neural network
to efficiently integrate local and long-distance fea-
tures. The above methods are all character-based
models, making no use of full word information.
In contrast, we leverage both character embed-
dings and word embeddings for better accuracies.

For word-based segmentation, Andrew (2006)
used a semi-CRF model to integrate word fea-
tures, Zhang and Clark (2007) used a percep-
tron algorithm with inexact search, and Sun et
al. (2009) used a discriminative latent variable
model to make use of word features. Recently,
there have been several neural-based models us-
ing word-level embedding features (Morita et al.,
2015; Liu et al., 2016; Cai and Zhao, 2016), which
are different from our work in the basic frame-
work. For instance, Liu et al. (2016) follow An-
drew (2006) using a semi-CRF for structured in-
ference.

We followed the global learning and beam-
search framework of Zhang and Clark (2011) in
building a word-based neural segmentor. The
main difference between our model and that of
Zhang and Clark (2011) is that we use a neural
network to induce feature combinations directly
from character and word embeddings. In addi-
tion, the use of a bi-directional LSTM allows us to
leverage non-local information from the word se-
quence, and look-ahead information from the in-
coming character sequence. The automatic neu-
ral features are complementary to the manual dis-
crete features of Zhang and Clark (2011). We
show that our model can accommodate the inte-
gration of both types of features. This is similar in
spirit to the work of Sun (2010) and Wang et al.
(2014), who integrated features of character-based
and word-based segmentors.

Transition-based framework with beam search
has been widely exploited in a number of other
NLP tasks, including syntactic parsing (Zhang and
Nivre, 2011; Zhu et al., 2013), information ex-

traction (Li and Ji, 2014) and the work of joint
models (Zhang et al., 2013b; Zhang et al., 2014).
Recently, the effectiveness of neural features has
been studied for this framework. In the natural
language parsing community, it has achieved great
success. Representative work includes Zhou et al.
(2015), Weiss et al. (2015), Watanabe and Sumita
(2015) and Andor et al. (2016). In this work,
we apply the transition-based neural framework to
Chinese segmentation, in order to exploit word-
level neural features such as word embeddings.

7 Conclusion

We proposed a word-based neural model for Chi-
nese segmentation, which exploits not only char-
acter embeddings as previous work does, but also
word embeddings pre-trained from large scale
corpus. The model achieved comparable per-
formances compared with a discrete word-based
baseline, and also the state-of-the-art character-
based neural models in the literature. We fur-
ther demonstrated that the model can utilize dis-
crete features conveniently, resulting in a com-
bined model that achieved top performances com-
pared with previous work. Finally, we conducted
several comparisons to study the differences be-
tween our word-based model with character-based
neural models, showing that they have different er-
ror characteristics.

Acknowledgments

We thank the anonymous reviewers, Yijia Liu and
Hai Zhao for their constructive comments, which
help to improve the final paper. This work is sup-
ported by National Natural Science Foundation
of China (NSFC) under grant 61170148, Natu-
ral Science Foundation of Heilongjiang Province
(China) under grant No.F2016036, the Singapore
Ministry of Education (MOE) AcRF Tier 2 grant
T2MOE201301 and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
Yue Zhang is the corresponding author.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the ACL 2016.

Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.

429



In Proceedings of the 2006 Conference on EMNLP,
pages 465–472, Sydney, Australia, July.

Deng Cai and Hai Zhao. 2016. Neural word segmen-
tation learning for Chinese. In Proceedings of ACL
2016.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
chinese word segmentation. In Proceedings of the
53nd ACL, pages 1744–1753, July.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015b. Long short-term
memory neural networks for chinese word segmen-
tation. In Proceedings of the 2015 EMNLP, pages
1197–1206, September.

Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain, July.

R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Greg Durrett and Dan Klein. 2015. Neural crf pars-
ing. In Proceedings of the 53nd ACL, pages 302–
312, July.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53nd ACL,
pages 334–343, July.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the Second SIGHAN Workshop on Chinese Lan-
guage Processing, pages 123–133.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Yann A LeCun, Léon Bottou, Genevieve B Orr, and
Klaus-Robert Müller. 2012. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9–48.
Springer.

Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proceedings of
the ACL 2014.

Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, and
Ting Liu. 2016. Exploring segment representations
for neural segmentation models. In Proceedings of
IJCAI 2016.

Jianqiang Ma and Erhard Hinrichs. 2015. Accurate
linear-time chinese word segmentation via embed-
ding matching. In Proceedings of the 53nd ACL,
pages 1733–1743, July.

Mairgup Mansur, Wenzhe Pei, and Baobao Chang.
2013. Feature-based neural language model and
chinese word segmentation. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1271–1277, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.

Hajime Morita, Daisuke Kawahara, and Sadao Kuro-
hashi. 2015. Morphological analysis for unseg-
mented languages using recurrent neural network
language model. In Proceedings of the 2015 Con-
ference on EMNLP, pages 2292–2297.

Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd ACL, pages
293–303, Baltimore, Maryland, June.

Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceedings
of Coling 2004, pages 562–568, Geneva, Switzer-
land, Aug 23–Aug 27.

Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In IJCAI, pages 1707–
1712.

Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on EMNLP, pages
970–979, July.

Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of NAACL 2009, pages 56–64, June.

Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word detec-
tion. In Proceedings of the 50th ACL, pages 253–
262, July.

Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and combi-
nation. In Coling 2010: Posters, pages 1211–1219,
August.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop, pages 168–171.

430



Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384–394, July.

Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence
labeling. In Proceedings of the Sixth International
Joint Conference on Natural Language Processing,
pages 1285–1291, Nagoya, Japan, October. Asian
Federation of Natural Language Processing.

Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmenta-
tion and pos tagging with semi-supervised methods
using large auto-analyzed data. In Proceedings of
5th IJCNLP, pages 309–317, Chiang Mai, Thailand,
November.

Mengqiu Wang, Rob Voigt, and Christopher D. Man-
ning. 2014. Two knives cut better than one: Chi-
nese word segmentation with dual decomposition.
In Proceedings of the 52nd ACL, pages 193–198,
Baltimore, Maryland, June.

Taro Watanabe and Eiichiro Sumita. 2015. Transition-
based neural constituent parsing. In Proceedings of
the 53rd ACL, pages 1169–1179, July.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
transition-based parsing. In Proceedings of the 53rd
ACL, pages 323–333, July.

Jingjing Xu and Xu Sun. 2016. Dependency-based
gated recursive neural network for chinese word seg-
mentation. In Proceedings of ACL 2016.

Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, 8(1).

Kaisheng Yao and Geoffrey Zweig. 2015.
Sequence-to-sequence neural net models for
grapheme-to-phoneme conversion. arXiv preprint
arXiv:1506.00196.

Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of the 45th ACL, pages 840–847,
Prague, Czech Republic, June.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th ACL, pages 188–193, June.

Meishan Zhang and Yue Zhang. 2015. Combin-
ing discrete and continuous features for determin-
istic transition-based dependency parsing. In Pro-
ceedings of the 2015 EMNLP, pages 1316–1321,
September.

Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013a. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the EMNLP 2013,
pages 311–321, Seattle, Washington, USA, October.

Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013b. Chinese parsing exploiting characters.
In Proceedings of the 51st ACL, pages 125–134, Au-
gust.

Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Character-level chinese dependency
parsing. In Proceedings of the 52nd ACL, pages
1326–1336, Baltimore, Maryland, June.

Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in chinese word
segmentation via conditional random field model-
ing. In Proceedings of PACLIC, volume 20, pages
87–94. Citeseer.

Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the EACL, pages 879–887, Athens, Greece, March.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmentation
and POS tagging. In Proceedings of the 2013 Con-
ference on EMNLP, pages 647–657, Seattle, Wash-
ington, USA, October.

Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun
Chen. 2015. A neural probabilistic structured-
prediction model for transition-based dependency
parsing. In Proceedings of the 53rd ACL, pages
1213–1222, July.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st ACL, pages 434–443, August.

431


