



















































Interpolated Spectral NGram Language Models


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5926–5930
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5926

Interpolated Spectral NGram Language Models

Ariadna Quattoni and Xavier Carreras
dMetrics

Brooklyn, NY 11211
{ariadna.quattoni,xavier.carreras}@dmetrics.com

Abstract
Spectral models for learning weighted non-
deterministic automata have nice theoretical
and algorithmic properties. Despite this, it has
been challenging to obtain competitive results
in language modeling tasks, for two main rea-
sons. First, in order to capture long-range de-
pendencies of the data, the method must use
statistics from long substrings, which results
in very large matrices that are difficult to de-
compose. The second is that the loss func-
tion behind spectral learning, based on mo-
ment matching, differs from the probabilis-
tic metrics used to evaluate language mod-
els. In this work we employ a technique
for scaling up spectral learning, and use in-
terpolated predictions that are optimized to
maximize perplexity. Our experiments in
character-based language modeling show that
our method matches the performance of state-
of-the-art ngram models, while being very fast
to train.

1 Introduction

In the recent years we have witnessed the develop-
ment of spectral methods based on matrix decom-
positions to learn Probabilistic Non-deterministic
Finite Automata (PNFA) and related models (Hsu
et al., 2009, 2012; Bailly et al., 2009; Balle et al.,
2011; Cohen et al., 2012; Balle et al., 2014). Es-
sentially, PNFA can be regarded as recurrent neu-
ral networks where the function that predicts the
dynamic state representation from previous states
is linear. Despite the expressiveness of PNFA and
the strong theoretical properties of spectral learn-
ing algorithms, it has been challenging to get com-
petitive results on language modeling tasks. We
argue and confirm with our experiments that there
are two main reasons why using spectral meth-
ods for language modeling is challenging. The
first reason is a scalability problem to handle long
range dependencies. The spectral method is based

on computing a Hankel matrix that contains statis-
tics of expectations over substrings generated by
the target language. If we want to incorporate
long-range dependencies we need to consider long
substrings. A consequence of this is that the Han-
kel matrix can become too large to make it prac-
tical to perform algebraic decompositions. To ad-
dress this problem we use the basis selection tech-
nique by Quattoni et al. (2017) to scale spectral
learning and model long range dependencies. Our
experiments confirm that modeling long range de-
pendencies is essential to obtain competitive lan-
guage models.

The second limitation of classical spectral
methods when applied to language modeling is
that the loss function that the learning algorithm
attempts to minimize is not aligned with the loss
function that is used to evaluate model perfor-
mance. Spectral methods minimize the `2 dis-
tance on the prediction of expectations of sub-
strings up to a certain length (see Balle et al.
(2012) for a formulation of spectral learning in
terms of loss minimization), while language mod-
els are usually evaluated using conditional per-
plexity. There have been some proposals on gen-
eralizing the fundamental ideas of spectral learn-
ing to other loss functions (Parikh et al., 2014;
Quattoni et al., 2014). However, while these ap-
proaches are promising they have the downside
that they lead to relatively expensive iterative con-
vex optimizations and it is still a challenge to scale
them to model long-range dependencies.

In this paper we propose a simpler yet effective
alternative to the iterative optimization. We use
the classical spectral method based on low-rank
matrix decomposition to learn a PNFA that com-
putes substring expectations. Then we use these
expectations as features in an interpolated ngram
model and we learn the weights of the interpola-
tion so as to maximize perplexity. This interpo-



5927

lation step is iterative, but it is a simple and very
efficient convex optimization: the weights of the
interpolation can be trained in a few seconds or
minutes at most. The refinement step allows us to
leverage all the moments computed by the learned
PNFA and to align the spectral method with the
perplexity evaluation metric.

Our experiments on character-level language
model show that: (1) modeling long range depen-
dencies is important; and (2) with the simple in-
terpolation step we can obtain competitive results.
Our perplexity results are significantly better than
feed-forward NNs, as good or better than sophisti-
cated interpolation techniques such as Kneser-Ney
estimation, and close to the performance of RNNs
on two datasets.

The main contribution of our work consists
on combining two simple ideas, i.e. incorporat-
ing long-range dependencies via basis selection
of long substring moments (Section 2), and re-
fining the predictions of the PNFA with an iter-
ative interpolation step (Section 3). Our experi-
ments show that these two simple ideas bring us
one step closer to making spectral methods for
PNFA reach state-of-the-art performance on lan-
guage modeling tasks (Section 4). The advantage
of these methods over other popular approaches
to language modeling is their simplicity and the
fact that they rely on efficient convex optimiza-
tions for training the model parameters. Further-
more, PNFA are probabilistic models for which
efficient inference methods can be easily derived
for computing all sorts of expectations. These ex-
pectations could then be used as features to learn
predictive interpolation models. In this paper we
present experiments with one type of expectation
and interpolation model that illustrates the poten-
tial of this approach.

2 Spectral Language Models

2.1 Probabilistic Non-Deterministic Finite
Automata

We start describing the general class of Weighted
Automata over strings. Let x = x1 · · ·xn be a
sequence of length n over some finite alphabet Σ.
We denote as Σ? the set of all finite sequences, and
we use it as a domain of our functions. We use
x · x′ to denote the concatenation of two strings x
and x′.

A Non-Deterministic Weighted Automaton
(WA) with k states is defined as a tuple: A =

〈α0,α∞, {Aσ}σ∈Σ〉 with: α0, α∞ ∈ Rk are the
initial and final weight vectors; and Aσ ∈ Rk×k
are the transition matrices associated to each sym-
bol σ ∈ Σ. The function fA : Σ? → R realized by
an WA A is defined as:

fA(x) = α
>
0 Ax1 · · ·Axnα∞ . (1)

Probabilistic Non-Deterministic Finite Au-
tomata (PNFA) are WA that compute a probabilis-
tic distribution over strings. One can easily trans-
form a PNFA into another automata that computes
substring expectations via simple transformations
of the model parameters, and the reverse is also
true, see Balle et al. (2014) for details. In this pa-
per we will directly learn and use automata that
compute expectations. With these expectations we
will calculate the conditional probabilities of a lan-
guage model1:

Pr[σ | x1:n] =
fA(x1:n · σ)∑

σ′∈Σ fA(x1:n · σ′)
(2)

Here, n is the length of the left context, analogous
to the order of an NGram model, but we compute
the expectations not from counts but from a PNFA.

2.2 The Spectral Method
We now give a brief description of the spectral
method for estimating a PNFA that computes ex-
pectations over substrings. We only provide a
higher-level description of the method; for a com-
plete derivation and the theory justifying the algo-
rithm we refer the reader to the works by Hsu et al.
(2009) and Balle et al. (2014).

Assume a distribution of strings over some dis-
crete alphabet, our target function f(x) is the ex-
pected number of times that x appears as a sub-
string of a string sampled from the distribution.
At training, we are given strings T from the dis-
tribution and we want to estimate f . We denote
as fT(x) the empirical substring expectation of x
in T.2 Using fT , the spectral method estimates a
WA A with k states, where k is a parameter of the
algorithm, such that fA is a good approximation
of f . The method reduces the learning problem
to computing an SVD decomposition of a special
type of matrix called the Hankel matrix, that col-
lects the observed expectations fT . The method is
described by the following steps:

1For language models, we assume that Σ includes a spe-
cial symbol for end of sentence.

2This corresponds to the number of times that x is ob-
served as substring of any string in T, normalized by the num-
ber of strings in T.



5928

(1) Select a set of prefixes P and suffixes S, that
will serve as indices of the Hankel matrix
for rows and columns respectively. A typi-
cal choice is to select all substrings up to a
certain size n, but this quickly grows, and
in practice prior work uses a small n. In-
stead we use the basis selection technique
presented by Quattoni et al. (2017), which
allows to capture long-range dependencies
(analogous to having a large n) but keeping
the number of prefixes and suffixes manage-
able.

(2) Compute Hankel matrices for (P, S).

(a) Compute H ∈ RP×S, with entries
H(p, s) = fT(p · s).

(b) Compute hP ∈ RP with hP(p) = fT(p)
and hS ∈ RS with hS(s) = fT(s).

(c) For each σ ∈ Σ, compute Hσ ∈ RP×S
with entries Hσ(p, s) = fT(p · σ · s).

(3) Compute a k-rank factorization of H. Com-
pute the truncated SVD of H, i.e. H ≈
UΣV> resulting in a matrix F = UΣ ∈
RP×k and a matrix B = V ∈ RS×k. Thus
H ≈ FB> is an k-rank factorization of H.

(4) Recover the WA A of k states. Let M+ de-
note the Moore-Penrose pseudo-inverse of a
matrix M. The elements of A are recovered
as follows. Initial vector: α>0 = h

>
S B. Final

vector: α∞ = F+hP. Transition Matrices:
Aσ = F

+HσB, for σ ∈ Σ.

The computation is dominated by step (3), the
SVD of the Hankel matrix, which is at most cubic
in the size of the matrix. In practice, this method
is scalable and fast to train.

3 Interpolated Predictions

One limitation of the spectral method is that the
loss that it minimizes is not aligned with the prob-
abilistic metrics used in language modeling, such
as perplexity. Instead the spectral method mini-
mized the `2 loss over the observed empirical mo-
ments, i.e. those substrings collected in the Han-
kel matrix. To align the loss function with a per-
plexity measure we propose a simple refinement
step, where we use the expected counts computed
by the learned PNFA as features of a log-linear
model, and learn interpolation weights. In con-
trast to Equation 2, which uses the longest context

x of length n to compute the conditional proba-
bility, the interpolated model leverages the ability
of the PNFA to model substring expectations of all
lengths up to n. This is similar to classic interpola-
tion of language models (Rosenfeld, 1994; Chen,
2009).

Given a function f computing substring expec-
tations, the interpolation is:

g(x1:n, σ) = exp


n−1∑
j=0

wσ,j log f(xn−j:n · σ)


(3)

where x1:n is a context of size n, σ is the out-
put symbol, andwσ,j are the interpolation weights,
with one parameter per output symbol σ and con-
text length j, with 0 ≤ j < n.

As it is standard with interpolation models, we
train the weights by maximizing the conditional
log-likelihood of the development set. We assume
that f is fixed, which results in a convex optimiza-
tion, and we solve with L-BFGS.

4 Experiments

We present experiments in character-based lan-
guage modeling. Our spectral ngram models work
with a fixed context length, and we show results
varying this length up to relatively large values.

Following the standard, the goal is to learn
a language model that predicts the next symbol
given a sentence prefix, including the prediction of
sentence ends. As datasets we use the Penn Tree-
bank (PTB) prepared by Mikolov et al. (2012)3,
and “War and Peace” (WP) dataset prepared by
Karpathy et al. (2016)4. We use two probabilis-
tic evaluation metrics that are standard in language
modeling tasks: Cross Entropy and Bits per Char-
acter (BpC). Depending on the dataset, we use one
or the other such that we can directly compare to
published results.

Tables 1 and 2 present results in terms of the
context size (n) for the PTB and WP tests re-
spectively. The column “UB” shows an upper-
bound on the performance metric using a context
of size n. This is computed directly using the ex-
pected counts on the test set to compute the con-
ditional distribution. If we were able to estimate
these expectations perfectly, we would achieve the

349 characters; 5017k / 393k / 442k characters in the train
/ dev / test portions.

484 symbols; 2658k / 300k / 300k characters in the train /
dev / test portions.



5929

Spectral
n UB KN longest interp. size H
3 2.60 2.63 2.63 2.63 102
4 1.94 2.01 2.02 2.03 750
5 1.51 1.67 1.70 1.68 1,661
6 1.23 1.54 1.62 1.55 6,360
7 0.98 1.49 1.65 1.49 13,992
8 0.78 1.47 1.67 1.47 35,263
9 0.59 1.47 1.68 1.45 69,292

10 0.46 1.47 1.67 1.45 137,370

Table 1: Bits-per-character on the PTB test set.

Spectral
n UB KN FNN ME long. int. size H
3 1.86 1.93 1.93 1.95 1.95 1.95 174
4 1.38 1.52 1.55 1.59 1.57 1.55 1,258
5 1.06 1.31 1.45 1.43 1.41 1.36 3,278
6 0.82 1.23 1.34 1.36 1.39 1.29 11,859
7 0.62 1.20 1.32 1.33 1.42 1.25 26,848
8 0.46 1.19 - 1.30 1.46 1.24 62,628
9 0.32 1.19 - 1.30 1.47 1.24 121,534

10 0.22 1.19 - 1.30 1.47 1.24 224,159

Table 2: Cross-entropy on the WP test set.

reported performance. As the two tables show,
a context of size 10 already gives a high upper-
bound, suggesting that we can achieve good per-
formance using a fixed but large horizon.

The tables show results of the spectral language
model for different context sizes, using expecta-
tions from the “longest” context or “interpolated”
expectations. A clear trend is that the results im-
prove with the context length, achieving a stable
performance for n = 10. It is also clear that the in-
terpolated predictions work much better than sim-
ply using the longest context. Table 2 also com-
pares to a MaxEnt model (labeled “ME”), which
is an interpolation model of Eq.3 but uses em-
pirical expectations fT(x) computed from train-
ing counts instead of those given by the spec-
tral PNFA. Clearly, the expectations given by the
PNFA generalize better and lead to improvements.

The last column of the two tables shows the
number of rows (and columns) of the (square)
Hankel matrix we factorize for each context size.
This gives an idea of the cost of the estimation al-
gorithm, which goes from a few seconds to a few
hours, depending on the matrix size.5 Following

5Note that without the scalability trick, the Hankel matri-
ces would be simply too big (in the order of millions of rows
and columns) to practically run any experiment. It should be
clear, though, that this is the contribution in Quattoni et al.
(2017), not of this paper.

the theory behind Quattoni et al. (2017), this num-
ber is an upper bound on the size of the minimal
PNFA that reproduces exactly the expected counts
of training substrings.

The tables include a column “KN” with the re-
sults of an ngram language model estimated with
Kneser-Ney interpolation (Kneser and Ney, 1995;
Chen and Goodman, 1999). Looking at the re-
sults on the PTB data in Table 1, our interpolated
model performs equally well, and sometimes bet-
ter, than the KN models using the same context
length. Mikolov et al. (2012) reports the perfor-
mance of other models: a feed-forward neural net-
work6 obtains 1.57, which our model improves
with contexts of n = 6 or larger; an RNN works
at 1.41, slightly better than our best result of 1.45.
Their best result is of 1.37 for a MaxEnt model
with context length of n = 14 engineered for scal-
ability.

For the WP test in Table 2, our model and the
KN model perform similarly, with some slight im-
provements by the KN model. The table also in-
cludes the results of a feed-forward neural network
(FNN) for increasing orders, by Karpathy et al.
(2016). We observe that our interpolated model
works better, with our best result at 1.24. They
also report the results of an RNN obtaining 1.24,
and of LSTM and GRU which both obtain 1.08.

5 Conclusions

In this paper we presented experiments using
character-based spectral ngram language models.
We combine two key ideas: a) modeling of long-
range dependencies via the basis selection of long
substring moments by Quattoni et al. (2017); and
b) efficient optimization of arbitrary prediction
losses (e.g. cross-entropy) via a loss refinement
step. With these two ideas, we can improve the
performance of spectral learning for PNFA, and
bring the results of spectral models closer to the
state-of-the-art.

The ability of the spectral method for PNFA to
estimate substring expectations can be exploited
in other contexts. For example, we are inter-
ested in word-level language models that make use
of character-level PNFA to compute expectations,
which is useful to make predictions on words and
substrings which do not appear in training.

It is also interesting to consider a PNFA as a
special case of an RNN which uses linear transi-

6However, they do not report the order of that model.



5930

tions. Given that we obtain similar results than
feed-forward NN and some RNN, this suggests
that some forms of non-linearities can be approx-
imated by linear models, with the advantage that
some computations (mainly, expectations) can be
done exactly.

Acknowledgments

We are grateful to Matthias Gallé for the discus-
sions around this work, as well as to the anony-
mous reviewers for their useful feedback.

References
Raphaël Bailly, François Denis, and Liva Ralaivola.

2009. Grammatical inference as a principal com-
ponent analysis problem. In Proceedings of the
26th Annual International Conference on Machine
Learning, ICML ’09, pages 33–40, New York, NY,
USA. ACM.

Borja Balle, Xavier Carreras, Franco M. Luque, and
Ariadna Quattoni. 2014. Spectral Learning of
Weighted Automata: A Forward-Backward Perspec-
tive. Machine Learning, 96(1):33–63.

Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2011. A spectral learning algorithm for finite
state transducers. In Proceedings of the 2011th
European Conference on Machine Learning and
Knowledge Discovery in Databases - Volume Part
I, ECMLPKDD’11, pages 156–171, Berlin, Heidel-
berg. Springer-Verlag.

Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2012. Local loss optimization in operator models:
A new insight into spectral learning. In Proceedings
of the 29th International Coference on International
Conference on Machine Learning, ICML’12, pages
1819–1826, USA. Omnipress.

Stanley Chen. 2009. Performance prediction for expo-
nential language models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 450–458.
Association for Computational Linguistics.

Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359 – 394.

Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 223–
231, Jeju Island, Korea. Association for Computa-
tional Linguistics.

Daniel Hsu, Sham M Kakade, and Tong Zhang. 2012.
A spectral algorithm for learning hidden markov
models. Journal of Computer and System Sciences,
78(5):1460–1480.

Daniel J. Hsu, Sham M. Kakade, and Tong Zhang.
2009. A spectral algorithm for learning hidden
markov models. In COLT 2009 - The 22nd Con-
ference on Learning Theory, Montreal, Quebec,
Canada, June 18-21, 2009.

Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2016.
Visualizing and understanding recurrent networks.
In ICLR Workshop Track.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume I, pages 181–184.

Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
2012. Subword language modeling with neu-
ral networks. preprint (http://www. fit. vutbr.
cz/imikolov/rnnlm/char. pdf).

Ankur P. Parikh, Avneesh Saluja, Chris Dyer, and Eric
Xing. 2014. Language modeling with power low
rank ensembles. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1487–1498, Doha,
Qatar. Association for Computational Linguistics.

Ariadna Quattoni, Borja Balle, Xavier Carreras, and
Amir Globerson. 2014. Spectral regularization
for max-margin sequence tagging. In Proceedings
of the 31st International Conference on Machine
Learning (ICML-14), pages 1710–1718. JMLR
Workshop and Conference Proceedings.

Ariadna Quattoni, Xavier Carreras, and Matthias Gallé.
2017. A Maximum Matching Algorithm for Basis
Selection in Spectral Learning. In Proceedings of
the 20th International Conference on Artificial In-
telligence and Statistics, volume 54 of Proceedings
of Machine Learning Research, pages 1477–1485,
Fort Lauderdale, FL, USA. PMLR.

Roni Rosenfeld. 1994. Adaptive statistical language
modeling: A maximum entropy approach. Ph.D.
thesis, Carnegie Mellon University.

https://doi.org/10.1145/1553374.1553379
https://doi.org/10.1145/1553374.1553379
https://doi.org/10.1007/s10994-013-5416-x
https://doi.org/10.1007/s10994-013-5416-x
https://doi.org/10.1007/s10994-013-5416-x
https://doi.org/10.1007/978-3-642-23780-5_20
https://doi.org/10.1007/978-3-642-23780-5_20
http://dl.acm.org/citation.cfm?id=3042573.3042805
http://dl.acm.org/citation.cfm?id=3042573.3042805
http://aclweb.org/anthology/N09-1051
http://aclweb.org/anthology/N09-1051
https://doi.org/https://doi.org/10.1006/csla.1999.0128
https://doi.org/https://doi.org/10.1006/csla.1999.0128
https://doi.org/https://doi.org/10.1006/csla.1999.0128
http://www.aclweb.org/anthology/P12-1024
http://www.aclweb.org/anthology/P12-1024
http://www.cs.mcgill.ca/%7Ecolt2009/papers/011.pdf#page=1
http://www.cs.mcgill.ca/%7Ecolt2009/papers/011.pdf#page=1
http://www.aclweb.org/anthology/D14-1158
http://www.aclweb.org/anthology/D14-1158
http://jmlr.org/proceedings/papers/v32/quattoni14.pdf
http://jmlr.org/proceedings/papers/v32/quattoni14.pdf
http://proceedings.mlr.press/v54/quattoni17a.html
http://proceedings.mlr.press/v54/quattoni17a.html

