



















































Semantic graph parsing with recurrent neural network DAG grammars


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2769–2778,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2769

Semantic Graph Parsing with Recurrent Neural Network DAG
Grammars

Federico Fancellus and Sorcha Gilroyd and Adam LopezL and Mirella LapataL
sSamsung AI Centre (SAIC), Toronto, Canada

dPeak AI, Manchester, United Kingdom
LUniversity of Edinburgh, Edinburgh, United Kingdom

Abstract

Semantic parses are directed acyclic graphs
(DAGs), so semantic parsing should be mod-
eled as graph prediction. But predicting
graphs presents difficult technical challenges,
so it is simpler and more common to pre-
dict the linearized graphs found in seman-
tic parsing datasets using well-understood se-
quence models. The cost of this simplicity
is that the predicted strings may not be well-
formed graphs. We present recurrent neural
network DAG grammars, a graph-aware se-
quence model that ensures only well-formed
graphs while sidestepping many difficulties
in graph prediction. We test our model on
the Parallel Meaning Bank—a multilingual se-
mantic graphbank. Our approach yields com-
petitive results in English and establishes the
first results for German, Italian and Dutch.

1 Introduction

Semantic parsing is the task of mapping natural
language to machine interpretable meaning rep-
resentations, which in turn can be expressed in
many different formalisms, including lambda cal-
culus (Montague, 1973), dependency-based com-
positional semantics (Liang et al., 2011), frame se-
mantics (Baker et al., 1998), abstract meaning rep-
resentations (AMR; Banarescu et al. 2013), mini-
mal recursion semantics (MRS; Copestake et al.
2005), and discourse representation theory (DRT;
Kamp 1981).

Explicitly or implicitly, a representation in any
of these formalisms can be expressed as a directed
acyclic graph (DAG). Consider the sentence “Ev-
ery ship in the dock needs a big anchor”. Its
meaning representation, expressed as a Discourse
Representation Structure (DRS, Kamp 1981), is
shown in Figure 1.1 A DRS is drawn as a box with

1For simplicity, our examples do not show time represen-
tations, though these are consistently present in our data.

x2 b4

dock(x2)

b1

x1 b2

ship(x1)
PARTOF(x1, x2)

⇒

e1, s1, x3 b3
need(e1)

PIVOT(e1, x1)
THEME(e1,x3)

anchor(x3)
big(s1)

TOPIC(s1, x3)

Figure 1: The discourse representation structure for
“Every ship in the dock needs a big anchor”. For ease
of reference in later figures, each box includes a vari-
able corresponding to the box itself, at top right in gray.

two parts: the top part lists variables for discourse
referents (e.g. x1, e1) and the bottom part can con-
tain unary predicates expressing the type of a vari-
able (e.g. ship, need), binary predicates specify-
ing relationships between variables (e.g. PARTOF,
TOPIC), logical operators expressing relationships
between nested boxes (e.g. ⇒, ¬), or binary dis-
course relations (e.g., RESULT, CONTRAST). To
express a DRS as a graph, we represent each box
as a node labeled �; each variable as a node la-
beled by its associated unary predicate; and each
binary predicate, logical operator, or discourse re-
lation as an edge from the first argument to the sec-
ond (Figure 2). To fully realize the representation
as a DAG, additional transformations are some-
times necessary: in DRS, when a box represents
a presupposition, as box b4 does, the label of the
node corresponding to the presupposed variable is
marked (e.g. x2/dockP ); and edges can be reversed
(e.g. TOPIC(s1, x3) becomes TOPICOF(s1, x3)).

Since meaning representations are graphs, se-
mantic parsing should be modeled as graph pre-
diction. But how do we predict graphs? A pop-
ular approach is to predict the linearized graph—
that is, the string representation of the graph found
in most semantic graphbanks. Figure 3 illus-



2770

b1/�

b2/�

x1/ship

x2/dockP
PARTOF

DRS

IMP1

b3/�

e1/need

x3/anchor s1/big
TOPICOF

THEME

DRS

IMP2

PIVOT

Figure 2: The DRS of Figure 1 expressed as a DAG.

trates one style of linearization using PENMAN
notation, in which graphs are written as well-
bracketed strings which can also be interpreted as
trees—note the correspondence between the tree-
like structure of Figure 2 and the string in Fig-
ure 3.2 Each subtree is a bracketed string start-
ing with a node variable and its label (e.g. b2/�),
followed by a list of relations corresponding to
the outgoing edges of the node. A relation con-
sists of the edge label prefixed with a colon (:),
followed by either the subtree rooted at the target
node (e.g. :DRS (x1/ship :PARTOF(x2/dockp))), or
a reference to the target node (e.g. :PIVOT x1).
By convention, if a node is the target of multiple
edges, then the leftmost one is written as a sub-
tree, and the remainder are written as references.
Hence, every node is written as a subtree exactly
once.

The advantage of predicting linearized graphs
is twofold. The first advantage is that graph-
bank datasets usually already contain lineariza-
tions, which can be used without additional work.
These linearizations are provided by annotators or
algorithms and are thus likely to be very consistent
in ways that are beneficial to a learning algorithm.
The second advantage is that we can use simple,
well-understood sequence models (Gu et al., 2016;
Jia and Liang, 2016; van Noord et al., 2018) to
model them. But this simplicity comes with a cost:
sequence models can predict strings that don’t cor-
respond to graphs—for example, strings with ill-
formed bracketings or unbound variable names.
While it is often possible to fix these strings with
pre- or post-processing, we would prefer to model
the problem in a way that does not require this.

Models that predict graphs are complex and

2Although PENMAN notation is now closely associated
with AMR, it can represent quite arbitrary graphs as strings.
Our actual implementation does not use PENMAN notation,
but we use it here for expository purposes since it is relatively
familiar; the underlying ideas are unchanged.

(b1/�
:IMP1(b2/�

:DRS(x1/ship
:PARTOF(x2/dockp)))

:IMP2(b3/�
:DRS(e1/ need

:PIVOT x1
:THEME(x3/ anchor

:TOPICOF(s1/ big)))))

Figure 3: The DAG of Figure 2 expressed as a string.

far less well-understood than models that predict
sequences. Fundamentally, this is because pre-
dicting graphs is difficult: every graph has many
possible linearizations, so from a probabilistic
perspective, the linearization is a latent variable
that must be marginalized out (Li et al., 2018).
Groschwitz et al. (2018) model graphs as trees, in-
terpreted as the (latent) derivation trees of a graph
grammar; Lyu and Titov (2018) model graphs
with a conditional variant of the classic Erdös and
Rényi (1959) model, first predicting an alignment
for each node of the output graph, and then pre-
dicting, for each pair of nodes, whether there is an
edge between them. Buys and Blunsom (2017),
Chen et al. (2018), and Damonte et al. (2017) all
model graph generation as a sequence of actions,
each aligned to a word in the conditioning sen-
tence. Each of these models has a latent variable—
a derivation tree or alignment—which must be ac-
counted for via preprocessing or complex infer-
ence techniques.

Can we combine the simplicity of sequence pre-
diction with the fidelity of graph prediction? We
show that this is possible by developing a new
model that predicts sequences through a simple
string rewriting process, in which each rewrite cor-
responds to a well-defined graph fragment. Im-
portantly, any well-formed string produced by our
model has exactly one derivation, and thus no la-
tent variables. We evaluate our model on the Paral-
lel Meaning Bank (PMB, Abzianidze et al. 2017),
a multilingual corpus of sentences paired with
DRS representations. Our model performs com-
petitively on English, and better than sequence
models in German, Italian, and Dutch.

2 Graph-aware string rewriting

We use a grammar to model the process of rewrit-
ing. Formally, our grammar is a graph grammar,
specifically a restricted DAG grammar (Björklund



2771

et al., 2016), a type of context-free graph grammar
designed to model linearized DAGs. Since lin-
earized DAGs are strings, we present it as a string-
rewriting system, which can be described more
compactly than a graph grammar while making the
connection to sequences more explicit. The cor-
respondence between string rewriting and graph
grammars is given in the Appendix.

A grammar in our model is defined by a set
Σ of terminal symbols consisting of all sym-
bols that can appear in the final string—brackets,
variable types, node labels, and edge labels;
a set N of n + 1 nonterminal symbols de-
noted by {L, T0, . . . , Tn}, for some maximum
value n; an unbounded set V of variable ref-
erences {$1, $2, . . . }; and a set of productions,
which are defined below.

We say that T0 is the start symbol, and for each
symbol Ti ∈ N , we say that i is its rank, and
we say that L has a rank of 0. A nonterminal of
rank i can be written as a function of i variable
references—for example, we can write T2($1, $2).
By convention, we write the rank-0 nonterminals
L and T0 without brackets. Productions in our
grammar take the form α→ β, where α is a func-
tion of rank i over $1, . . . , $i; and β is a linearized
graph in PENMAN format, with each of its sub-
trees replaced by either a function or a variable
reference. Optionally, the variable name in β may
replace one of the variable references in α. All
variable references in a production must appear at
least twice. Hence every variable reference in α
must appear at least once in β, and variables that
do not appear in α must appear at least twice in β.

To illustrate, we will use the following gram-
mar, which can generate the string in Figure 3, as-
suming L can also rewrite as any node label.

T0 → (b/� :IMP1 T1($1) :IMP2 T1($1)) (r1)
T0 → (x/L) (r2)
T0 → (s/L) (r3)
T0 → (x/L :TOPICOF T0) (r4)
T1($1)→ (b/� :DRS T1($1)) (r5)
T1($1)→ (e/L :PIVOT $1 :THEME T0) (r6)
T1(x)→ (x/L :PARTOF T0) (r7)

Our grammar derives strings by first rewriting
the start symbol T0, and at each subsequent step
rewriting the leftmost function in the partially de-
rived string, with special handling for variable ref-
erences described below. A derivation is complete
when no functions remain.

We illustrate the rewriting process in Figure 4.
The start symbol T0 at step 1 is rewritten by pro-
duction r1 in step 2, and the new b variable in-
troduced at this step is deterministically renamed
to the unique name b1. In step 3, the leftmost
T1($1) is rewritten by production r5, and the new
b variable is likewise renamed to the unique b2.
All productions apply in this way, simply replac-
ing a left-hand-side function with a right-hand
side expression. These rewrites are coupled with
a mechanism to correctly handle multiple refer-
ences to shared variables, as illustrated in Step 4
when production r7 is applied. In this produc-
tion, the left-hand-side function applies to the x
variable naming the right-hand-side node. When
this production applies, x is renamed to the unique
x1 as in previous steps, but because it appears in
the left-hand-side, the reference $1 is bound to
this new variable name throughout the partially-
derived string. In this way, the reference to x1 is
passed through the subsequent rewrites, becoming
the target of a PIVOT edge at step 10. Derivations
of a DAG grammar are context-free (Figure 5).3

Our model requires an explicit grammar like the
one in r1...r7, which we obtain by converting each
DAG in the training data into a sequence of pro-
ductions. The conversion yields a single, unique
sequence of productions via a simple linear-time
algorithm that recursively decomposes a DAG into
subgraphs (Björklund et al., 2016). Each subgraph
consists of single node and its outgoing edges,
as exemplified by the PENMAN-formatted right-
hand-sides of r1 through r7. Each outgoing edge
points to a nonterminal symbol representing a sub-
graph. If a subgraph does not share any nodes
with its siblings, it is represented by T0. But if
any subgraphs share a node, then a variable ref-
erence must refer for this node in the production
associated with the lowest common ancestor of all
its incoming edges. For example, in Figure 3, the
common ancestor of the two edges targeting x1 is
the node b1, so production r1 must contain two
copies of variable reference $1 to account for this.
A more mathematical account can be found along
with a proof of correctness in Björklund et al.
(2016) Our implementation follows their descrip-
tion unchanged.

3However, the language of derived strings may not be
context-free due to variable references.



2772

Step Action Production Result
1 START start T0
2 GEN-FRAG r1 (b1/� :IMP1 T1($1) :IMP2 T1($1))
3 GEN-FRAG r5 (b1/� :IMP1 (b2/� :DRS T1($1)) :IMP2 T1($1))
4 GEN-FRAG r7 (b1/� :IMP1 (b2/� :DRS (x1/L :PARTOF T0) :IMP2 T1(x1))
5 GEN-LABEL L→ ship (b1/� :IMP1 (b2/� :DRS (x1/ship :PARTOF T0) :IMP2 T1(x1))
6 GEN-FRAG r2 (b1/� :IMP1 (b2/� :DRS (x1/ship :PARTOF (x2/L)) :IMP2 T1(x1))
7 GEN-LABEL L→ dockp (b1/� :IMP1 (b2/� :DRS (x1/ship :PARTOF (x2/dockp)) :IMP2 T1(x1))
8 REDUCE −− =
9 REDUCE −− =
8 GEN-FRAG r5 (b1/� :IMP1 (b2/� :DRS (x1/ship :PARTOF (x2/dockp))

:IMP2 (b3/� :DRS T1(x1)))
9 GEN-FRAG r6 (b1/� :IMP1 (b2/� :DRS (x1/ship :PARTOF (x2/dockp))

:IMP2 (b3/� :DRS (e1/L :PIVOT x1 :THEME T0)))
10 GEN-LABEL L→ need (b1/� :IMP1 (b2/� :DRS (x1/ship :PARTOF (x2/dockp))

:IMP2 (b3/� :DRS (e1/need :PIVOT x1 :THEME T0)))

Figure 4: A partial derivation of the string in Figure 3. The stack operations follow closely each step in the
derivation, where GEN-FRAG and GEN-LABEL are invoked when rewriting a non-terminal T and a terminal L
respectively. In the result of each step, the leftmost function is underlined, and is rewritten in the fragment in blue
in the next step. On the other hand, a REDUCE operation is invoked when a generated fragment does not contain
non-terminals T to expand further (in this partial derivation, this is the case of the result of production r2).

3 Neural Network Realizer

We model graph parsing with an encoder-decoder
architecture that takes as input a sentence w and
outputs a directed acyclic graph G derived using
the rewriting system of Section 2. Specifically,
we model its derivation tree in top-down, left-to-
right order as a sequence of actions a = a1 . . . a|a|,
inspired by Recurrent Neural Network Grammars
(RNNG, Dyer et al., 2016). As in RNNG, we use
a stack to store partial derivations.

We model two types of actions: GEN-FRAG
rewrites Ti nonterminals, while GEN-LABEL
rewrites L nonterminals, always resulting in a leaf
of the derivation tree. A third REDUCE action is
applied whenever a subtree of the derivation tree
is complete, and since the number of subtrees is
known in advance, it is applied deterministically.
For example, when we predict r1, this determines
that we must rewrite an L and then recursively
rewrite two copies of T1($1) and then apply RE-
DUCE. Hence graph generation reduces to pre-
dicting rewrites only.

We define the probability of generating graphG
conditioned of input sentence w as follows:

p(G|w) = p(a|w) =
|a|∏
i=1

p(ai|a<i, w) (1)

r1

r5

r7

L →anchor r2
L →dockp

r5

r6

L →need r4
L →anchor r3

L →big

Figure 5: A derivation tree corresponding to Fig-
ure 4. Solid edges rewrite Tn nonterminals, while dot-
ted rewrite L nonterminals.

Input Encoder We represent the ith word wi of
input sentence w = w1 . . . w|w| using both learned
and pre-trained word embeddings (wi and w

p
i re-

spectively), lemma embedding (li), part-of-speech
embedding (pi), universal semantic tag (Abzian-
idze and Bos, 2017) embedding (ui), and depen-
dency label embedding (di).4 An input xi is com-
puted as the weighted concatenation of these fea-
tures followed by a non-linear projection (with
vectors and matrices in bold):

xi = tanh(W
(1)[wi;w

p
i ; li;pi;ui;di]) (2)

Input xi is then encoded with a bidirectional
LSTM, yielding contextual representation hei .

4Universal semantic tags are language neutral tags in-
tended to characterize lexical semantics.



2773

Graph decoder Since we know in advance
whether the next action is GEN-FRAG or GEN-
LABEL, we use different models for them.

GEN-FRAG. If step t rewrites a T nontermi-
nal, we predict the production yt that rewrites it
using context vector ct and incoming edge embed-
ding et. To obtain ct we use soft attention (Luong
et al., 2015) and weight each input hidden repre-
sentation hei to decoding hidden state h

d
t :

score(hei ,h
d
t ) = h

e
iW

(2)hdt

αti =
exp(score(hei ,h

d
t ))∑

i′ exp(score(h
e
i′ ,h

d
t ))

ct =
n∑

i=1

αtih
e
i

yt = W
(3)ct + W

(4)e

(3)

The contribution of c and e is weighted by matri-
ces W(3) and W(4), respectively.

We then update the stackLSTM representation
using the embedding of the non-terminal frag-
ment yt (denoted as yet ), as follows:

hdt+1 = LSTM(y
e
t ,h

d
t ) (4)

GEN-LABEL. Labels L can be rewritten to
either semantic constants (e.g., ‘speaker’, ‘now’,
‘hearer’) or unary predicates that often cor-
responds to the lemmas of the input words
(e.g., ‘love’) or. We predict the former using a
model identical to the one for GEN-FRAG. For
the latter, we use a selection mechanism to choose
an input lemma to copy to output. We model se-
lection following Liu et al. (2018), assigning each
input lemma a score oji that we then pass through
a softmax layer to obtain a distibution:

oji = h
dT
j W

(5)hei

pcopyi = SOFTMAX(oji)
(5)

where hi is the encoder hidden state for word wi.
We allow the model to learn whether to use soft-

attention or the selection mechanism through a bi-
nary classifier, conditioned on the decoder hidden
state at time t, hdt . Similar to Equation (4), we
update the stackLSTM with the embedding of ter-
minal predicted.5

5In the PMB, each terminal is annotated for sense
(e.g. ‘n.01’, ‘s.01’) and presupposition (e.g. for ‘dockp’ in
Figure 3) as well. We predict both the sense tag and whether
a terminal is presupposed or not independently conditioned
on the current stackLSTM state and the embedding of the
main terminal labels but are not used to update the state of
the stackLSTM.

REDUCE. When a reduce action is applied, we
use an LSTM to compose the fragments on top of
the stack. Using the derivation tree in Figure 5 as
reference, let [c1, ..., cn] denote the embeddings
of one or more sister nodes ri and pu the embed-
ding of their parent node, which we refer to as chil-
dren and parent fragments respectively. A reduce
operation runs an LSTM over the children frag-
ments and the parent fragment in order and then
uses the final state u to update the stack LSTM as
follows:

i = [c1...cn,pu]
−→u = LSTM(it,hct)

hdt+1 = LSTM(u,h
d
t )

The models are trained to minimize a cross-
entropy loss objective J over the sequence of gold
actions ai in the derivation:

J = −
|a|∑
i=1

log p(ai) (6)

4 Experimental Setup

We evaluated our model on the Parallel Meaning
Bank (PMB; Abzianidze et al. 2017), a semantic
bank where sentences in English, Italian, German,
and Dutch have been annotated following Dis-
course Representation Theory (Kamp and Reyle,
2013). Lexical predicates in PMB are in English,
even for non-English languages. Since this is not
compatible with our copy mechanism, we revert
predicates to their orignal language by substitut-
ing them with the lemmas of the tokens they are
aligned to. In our experiments we used both PMB
v.2.1.0 and v.2.2.06; we included the former re-
lease in order to compare against the state-of-the-
art seq2seq system of van Noord et al. (2018).
Statistics on the data and the grammar extracted
from v.2.2.0 are reported in Table 1.

4.1 Converting DRSs to Graphs

In this section we discuss how DRSs are converted
to acyclic, single-rooted, and fully-instantiated
graphs (i.e., how to translate Figure 1 to Figure 2).
In general, box structures are converted to acyclic
graphs by rendering boxes and lexical predicates

6English data is available for both releases at https:
//github.com/RikVN/DRS_parsing; for the other
languages, we used the officially released data available at
http://pmb.let.rug.nl/data.php

https://github.com/RikVN/DRS_parsing
https://github.com/RikVN/DRS_parsing
http://pmb.let.rug.nl/data.php


2774

train dev test
Gold #inst. #frags avg. rank #inst. #inst.
en 4,574 1,105 1.64 682 650
it — — — 387 386
de — — — 736 735
nl — — — 356 355

train
Silver #inst. #frags avg. rank
en 52,120 11,206 1.90
it 2,546 1,051 1.52
de 3,274 1,781 1.54
nl 874 789 1.45

Table 1: Data statistics of PMB v.2.2.0.

as nodes, while conditions, operators, and dis-
course relations become edge labels between these
nodes.

We consider main boxes (see b2, b3, and b4 in
Figure 1 separately from presuppositional boxes
(see b1), which represent instances that are presup-
posed in the wider discourse context (e.g., definite
expressions). Using Figure 2 as an example, b2,
b3 and b4 become nodes in the graph and material
implication (IMP stands for⇒) becomes an edge
label. If an operator or a relation is binary, as in
this case, we number the edge label so as to pre-
serve the order of the operands.

For each node in a main box, we expand the
graph by adding all relations and variables belong-
ing to it. We identify the head of the first relation
or the first referent mentioned as the head vari-
able. These are ‘ship (x1)’ for b2, and ‘need(e1)’
for b3. We attach the head variable as a child of the
box-node and follow the relations recursively to
expand the subgraph. If while expanding a graph
a variable in a condition is part of a presupposi-
tional box, we introduce it as a new node and add
to its label the superscript p. When expanding the
DAG along the edge PartOf, since x2 is also in
the presuppositional box in Figure 1, we attach
the node ‘dockp’. Graphs extracted this way are
mostly acyclic except for adjectival phrases and
relative clauses where state variables can be them-
selves root (e.g., big “a.01” s1). We get rid of these
extra roots by reversing the direction of the edge
involved and adding an ‘-of’ to the edge label to
flag this change (see TOPIC-OF).

4.2 System Comparison
We compared the performance of our graph parser
(seq2graph below) with a sequence-to-sequence
model (enchanced with a copy mechanism) which

decodes to a string linearization of the graph sim-
ilar to the one shown in Figure 3 (seq2seq + copy
below). We also compare against the recently pro-
posed model of van Noord et al. (2018); they in-
troduce a seq2seq model that generates a DRS as
a concatenation of clauses, essentially a flat ver-
sion of the standard box notation. The decoded
string is made aware of the overall graph structure
during preprocessing where variables are replaced
by indices indicating when they were first intro-
duced and their recency. In contrast, we model the
graph structure explicitly. van Noord et al. (2018)
experimented with both word and character-based
models, as well as with an ensemble of both, using
word embedding features. Since all our models
are word-based, we compare our results with their
best word model, using word embedding features
only (trained using 10-fold cross validation).

4.3 Model Configurations

In addition to word embeddings7, we also report
on experiments which make use of additional fea-
tures. Specifically, for each word we add informa-
tion about its universal PoS tag, lemma, universal
semantic tag, and dependency label.8

In Section 2, we mentioned that given a pro-
duction α → β, variable references in α should
appear at least once in β (i.e., they should have
the same rank). In all experiments so far, we did
not model this constraint explicitly to investigate
whether the model is able by default to predict
rank correctly. However, in exploring model con-
figurations we also report on whether adding this
constrant leads to better performance .

4.4 Cross-lingual Experiments

We conducted two sets of experiments: one mono-
lingual (mono below) where we train and test on
the same language and one cross-lingual (cross
below), where we train a model on English and
test it on one of the other three languages. The
goal of our cross-lingual experiments is to exam-
ine whether we need data in a target language at all
since the semantic representation itself is language
agnostic and lexical predicates are dealt with via

7We used word embeddings pre-trained with Glove
and available at https://nlp.stanford.edu/
projects/glove/.

8Universal PoS tags, lemmas and dependency labels for
all languages were obtained using pretrained UDPipe models
available at http://ufal.mff.cuni.cz/udpipe#
download; gold-standard universal semantic tags were ex-
tracted from the PMB release.

https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/
http://ufal.mff.cuni.cz/udpipe#download
http://ufal.mff.cuni.cz/udpipe#download


2775

the copy mechanism. Most of the features men-
tioned above are cross-linguistic and therefore fit
both mono and cross-lingual settings, with the ex-
ception of lemma and word embeddings, where
we exclude the former and replaced the latter with
multilingual word embeddings.9

4.5 System settings

For training, we used the Adam optimizer
(Kingma and Ba, 2014) with an initial learn-
ing rate of 0.001 and a decay rate of 0.1 every
10 epochs. Randomly initialized and pre-trained
word embeddings have a dimensionality of 128
and 100 respectively, and all other features a di-
mensionality of 50. In all cross-lingual experi-
ments, the pre-trained word embeddings have a di-
mensionality of 300. The LSTMs in the encoder
and the decoder have a dimensionality of 150 and
non-terminal and terminal embeddings during de-
coding have a dimensionality of 50. The system is
trained for 30 epochs, with the best system chosen
based on dev set performance.

4.6 Evaluation Metric

We evaluated our system by scoring the similar-
ity between predicted and gold graphs. We used
Counter (van Noord et al., 2018), an adaptation
of Smatch (Cai and Knight, 2013) to Discourse
Representation Structures where graphs are first
transformed into a set of ‘source node – edge la-
bel – target node’ triples and the best mapping
between the variables is found through an itera-
tive hill-climbing strategy. Furthermore, Counter
checks whether DRSs are well-formed in that all
boxes should be connected, acyclic, with fully in-
stantiated variables, and correctly assigned sense
tags.

It is worth mentioning that there can be cases
where our parser generates ill-formed graphs ac-
cording to Counter; this is however not due to the
model itself but to the way the graph is converted
back in a format accepted by Counter.

All results shown are an averages over 5 runs.

9We experimented with embeddings obtained with it-
erative procrustes (available at https://github.com/
facebookresearch/MUSE) and with Guo et al. (2016)’s
‘robust projection’ method where the embedding of non-
English words is computed as the weighted average of En-
glish ones. We found the first method to perform better on
cross-lingual word similarity tasks and used it in our experi-
ments.

P R F1 illformed
van Noord et al. (2018) — — 72.80 20%
seq2seq + copy 75.57 67.27 71.18 4.12%
seq2graph 75.51 71.69 73.55 0.40%

Table 2: Model performance (Precision, Recall, F1) on
PMB data (v.2.1.0, test set); models were trained on
gold standard data.

#sents
conversion

score F1 illformed
van Noord et al. (2018) 73,778 100% 82.7 1.10%
seq2seq+copy 56.694 89.58% 74.67 13.45%
seq2graph 56.694 89.58% 77.1 0.90%

Table 3: Model performance (Precision, Recall, F1) on
PMB data (v.2.1.0, test set); models were trained on
gold and silver standard data combined.

5 Results

System comparison Table 2 summarizes our re-
sults on the PMB gold data (v.2.1.0, test set). We
compare our graph decoder against the system of
van Noord et al. (2018) and our implementation
of a seq2seq model, enhanced with a copy mecha-
nism. Overall, we see that our graph decoder out-
performs both models. Moreover, it reduces the
number of illformed representations without any
specific constraints or post-processing in order to
ensure the well-formedness of the semantics of the
output.

The PMB (v.2.1.0) contains a large number of
silver standard annotations which have been only
partially manually corrected (see Table 1). Fol-
lowing van Noord et al. (2018), we also trained our
parser on both silver and gold standard data com-
bined. As shown in Table 3, increasing the train-
ing data improves performance but the difference
is not as dramatic as in van Noord et al. (2018).
We found that this is because our parser requires
graphs that are fully instantiated – all unary predi-
cates (e.g. ship(x)) need to be present for the graph
to be fully connected, which is often not the case
for silver graphs. Our model is at a disadvantage
since it could exploit less training data; during
grammar extraction we could not process around
20K sentences and in some cases could not recon-
struct the whole graph, as shown by the conversion
score.10

10The conversion score is computed using Counter where
we consider the converted graph representation as a ‘pre-
dicted’ DRS and the original DRS structure as gold data. As
a comparison, converting the gold DRS sturctures yields a
conversion score of 99.8%.

https://github.com/facebookresearch/MUSE
https://github.com/facebookresearch/MUSE


2776

P R F1 illformed
seq2seq + copy 60.29 74.09 66.48 10.60%
seq2graph 70.69 74.46 72.53 0.10%
+restrict 70.50 74.64 72.51 0.70%
+feats 72.51 76.44 74.42 0.60%
-lemmas 71.53 76.28 73.83 0.32%
-pos 72.11 75.99 74.00 0.35%
-semtag 70.21 74.45 72.27 0.53%
-words 70.45 74.59 72.46 0.64%
-dep 72.16 76.40 74.22 0.50%

Table 4: Model performance (Precision, Recall, F1) on
PMB (v2.2.0, development set).

Model configurations Table 4 reports on vari-
ous ablation experiments investigating which fea-
tures and combinations thereof perform best. The
experiments were conducted on the development
set (PMB v2.2.0). We show a basic version of
our seq2graph model with word embeddings, to
which we add information about rank (+restrict).
We also experimented with the full gamut of ad-
ditional features (+feats) as well as with ablations
of individual feature classes. For comparsion, we
also show the performance of a graph-to-string
model (seq2seq+copy).

As can be seen, all linguistic features seem to
improve performance. Restricting fragment selec-
tion by rank does not seem to improve the over-
all result showing that our baseline model is al-
ready able to predict fragments with the correct
rank throughout the derivation. Subsequent exper-
iments report results with this model using all lin-
guistic features, unless otherwise specified.

Cross-lingual experiments Results on DRT
parsing for languages other than English are re-
ported in Table 5. There is no gold standard train-
ing data for non-English languages in the PMB
(v.2.2.0). We therefore trained our parser on sil-
ver standard data but did use the provided gold
standard data for development and testing (see Ta-
ble 1). We present two versions of our parser,
one where we train and test on the same language
(s2g mono-silver) and another one where a model
is trained on English but tested on the other lan-
guages (s2g cross-silver). We also show the results
of a sequence-to-sequence model enhanced with a
copy mechanism.

In the monolingual setting, our graph parser
outperforms the seq2seq baseline by a large mar-
gin; we hypothesize this is due to the large per-
centage of ill-formed semantics, mostly due to
training on silver data. The difference in perfor-

Italian P R F1 ill
s2s + copy mono-silver 61.33 72.42 66.41 14.80
s2g mono-silver 74.50 77.27 75.86 0.10
s2g cross-silver 70.35 71.91 71.12 0.00

German P R F1 ill
s2s + copy mono-silver 56.86 65.40 60.83 10.67
s2g mono-silver 66.44 69.34 67.86 0.80
s2g cross-silver 66.14 65.72 63.50 0.40

Dutch P R F1 ill
s2s + copy mono-silver 54.27 64.17 58.81 10.67
s2g mono-silver 63.50 68.37 65.84 0.86
s2g cross-silver 62.94 67.32 65.06 0.50

Table 5: Model performance across languages (Preci-
sion, Recall, F1). Results are reported on the PMB test
set (v.2.2.0) for each language.

mance between our cross-lingual parser and the
monolingual parser for all languages is small, and
in Dutch the two parsers perform on par, suggest-
ing that English data and language independent
features can be leveraged to build parsers in other
languages when data is scarse or event absent. We
also conducted various ablation studies to exam-
ine the contribution of individual features to cross-
linguistic semantic parsing. Our experiments re-
vealed that universal semantic tags are most use-
ful, while the multilingual word embeddings that
we have tested with are not. We refer the interested
reader to the supplementary material for more de-
tail on these experiments.

Error Analysis We further analyzed the output
of our parser to gain insight as to what parts of
meaning representation are still challenging. Ta-
ble 6 shows a more detailed break-down of sys-
tem output as computed by Counter, where opera-
tors (e.g., negation, implication), roles (i.e., binary
relations, such as ‘Theme’), concepts (i.e., unary
predicates like ‘ship’), and synsets (i.e., sense tags
like ‘n.01’) are scored separately. Synsets are fur-
ther broken down into into ‘Nouns’, ‘Verbs’, ’Ad-
verbs’, and ‘Adjectives’. We compare our best
seq2graph models (+feats) trained on the PMB
v.2.2.0, gold and gold+silver data respectively.

Adding silver data helps with semantic ele-
ments (operators, roles and concepts), but does
not in the case of sense prediction where the only
category that benefits from additional data are is
nouns. We also found that ignoring the predic-
tion of sense tags altogether helps with the per-
formance of both models.



2777

seq2graph(gold) seq2graph(silver)
all clauses 74.42 76.36
DRS operators 88.05 89.36
Roles 72.95 74.03
Concepts 71.13 74.42
Synsets – Nouns 77.13 81.80

– Verbs 55.63 55.49
– Adverbs 44.44 42.11
– Adjectives 61.67 58.48

-sense 76.91 78.93

Table 6: F1-scores of fine-grained evaluation on the
PMB (v.2.2.0) development set; the seq2graph mod-
els trained on gold (left) and with gold and silver data
combined (right) are compared.

6 Conclusions

In this paper we have introduced a novel graph
parser that can leverage the power and flexibility
of sequential neural models while still operating
on graph structures. Heavy preprocessing tailored
to a specific formalism is replaced by a flexible
grammar extraction method that relies solely on
the graph while yielding performance that is on
par or better than string-based approaches. Fu-
ture work should focus on extending our parser
to other formalisms (AMR, MRS, etc.). We also
plan to explore modelling alternatives, such as tak-
ing different graph generation oders into account
(bottom-up vs. top-down) as well as predicting the
components of a fragment (type, number of edges,
edge labels) separately.

Acknowledgments

We thank Yova Kementchedjhieva, Andreas Gri-
vas and the three anonymous reviewers for their
useful comments. This work was done while
Federico Fancellu was a post-doctoral researcher
at the University of Edinburgh. The views ex-
pressed are his own and do not necessarily repre-
sent the views of Samsung Research. We grate-
fully acknowledge the support of the European
Research Council (Fancellu, Lapata; award num-
ber 681760, “Translating Multiple Modalities into
Text”); and the EPSRC Centre for Doctoral Train-
ing in Data Science, funded by the UK Engi-
neering and Physical Sciences Research Council
(Gilroy; grant EP/L016427/1) and the University
of Edinburgh (Gilroy).

References
Abzianidze, L., Bjerva, J., Evang, K., Haagsma, H.,

van Noord, R., Ludmann, P., Nguyen, D.-D., and
Bos, J. (2017). The parallel meaning bank: Towards

a multilingual corpus of translations annotated with
compositional meaning representations. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 242–247, Valencia,
Spain. Association for Computational Linguistics.

Abzianidze, L. and Bos, J. (2017). Towards universal
semantic tagging. In IWCS 201712th International
Conference on Computational SemanticsShort pa-
pers.

Baker, C. F., Fillmore, C. J., and Lowe, J. B. (1998).
The Berkeley FrameNet project. In Proceedings
of the 36th Annual Meeting of the Association for
Computational Linguistics and 17th International
Conference on Computational Linguistics, Volume
1, pages 86–90, Montreal, Quebec, Canada.

Banarescu, L., Bonial, C., Cai, S., Georgescu, M.,
Griffitt, K., Hermjakob, U., Knight, K., Koehn, P.,
Palmer, M., and Schneider, N. (2013). Abstract
meaning representation for sembanking. In Pro-
ceedings of the 7th Linguistic Annotation Workshop
and Interoperability with Discourse, pages 178–186,
Sofia, Bulgaria. Association for Computational Lin-
guistics.

Björklund, H., Drewes, F., and Ericson, P. (2016). Be-
tween a rock and a hard place–uniform parsing for
hyperedge replacement dag grammars. In Interna-
tional Conference on Language and Automata The-
ory and Applications, pages 521–532. Springer.

Buys, J. and Blunsom, P. (2017). Robust incremen-
tal neural semantic graph parsing. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1215–1226.

Cai, S. and Knight, K. (2013). Smatch: an evaluation
metric for semantic feature structures. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), volume 2, pages 748–752.

Chen, B., Sun, L., and Han, X. (2018). Sequence-
to-action: End-to-end semantic graph generation for
semantic parsing. arXiv preprint arXiv:1809.00773.

Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. (2005). Minimal recursion semantics: An in-
troduction. Research on language and computation,
3(2-3):281–332.

Damonte, M., Cohen, S. B., and Satta, G. (2017). An
incremental parser for abstract meaning representa-
tion. In Proceedings of the 15th Conference of the
European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 536–546.

Dyer, C., Kuncoro, A., Ballesteros, M., and Smith,
N. A. (2016). Recurrent neural network grammars.
In Proceedings of NAACL-HLT, pages 199–209.



2778

Erdös, P. and Rényi, A. (1959). On random graphs.
Publicationes Mathematicae, 6:290–297.

Groschwitz, J., Lindemann, M., Fowlie, M., Johnson,
M., and Koller, A. (2018). Amr dependency pars-
ing with a typed semantic algebra. arXiv preprint
arXiv:1805.11465.

Gu, J., Lu, Z., Li, H., and Li, V. O. (2016). Incorpo-
rating copying mechanism in sequence-to-sequence
learning. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1631–
1640.

Guo, J., Che, W., Yarowsky, D., Wang, H., and Liu,
T. (2016). A representation learning framework for
multi-source transfer parsing. In AAAI, pages 2734–
2740.

Jia, R. and Liang, P. (2016). Data recombination for
neural semantic parsing. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany.

Kamp, H. (1981). A theory of truth and semantic repre-
sentation. Formal semantics-the essential readings,
pages 189–222.

Kamp, H. and Reyle, U. (2013). From discourse to
logic: Introduction to modeltheoretic semantics of
natural language, formal logic and discourse rep-
resentation theory, volume 42. Springer Science &
Business Media.

Kingma, D. P. and Ba, J. (2014). Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia,
P. (2018). Learning deep generative models of
graphs. arXiv preprint arXiv:1803.03324.

Liang, P., Jordan, M., and Klein, D. (2011). Learn-
ing dependency-based compositional semantics. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 590–599, Portland, Ore-
gon.

Liu, J., Cohen, S. B., and Lapata, M. (2018). Dis-
course representation structure parsing. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 429–439. Association for Computa-
tional Linguistics.

Luong, T., Pham, H., and Manning, C. D. (2015).
Effective approaches to attention-based neural ma-
chine translation. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1412–1421.

Lyu, C. and Titov, I. (2018). Amr parsing as graph
prediction with latent alignment. arXiv preprint
arXiv:1805.05286.

Montague, R. (1973). The proper treatment of quantifi-
cation in ordinary English. In Hintikka, K., Moravc-
sik, J., and Suppes, P., editors, Approaches to Natu-
ral Language, volume 49 of Synthese Library, pages
221–242. Springer Netherlands.

van Noord, R., Abzianidze, L., Toral, A., and Bos,
J. (2018). Exploring neural methods for parsing
discourse representation structures. arXiv preprint
arXiv:1810.12579.


