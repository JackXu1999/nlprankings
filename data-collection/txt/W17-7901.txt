The Proceedings of the First Workshop on Human-Informed Translation and Interpreting Technology (HiT-IT), pages 1–10,

1

Varna, Bulgaria, Sept 7, 2017

https://doi.org/10.26615/978-954-452-042-7_001

Enhancing Machine Translation of Academic Course Catalogues with

Terminological Resources

Randy Scansani1

University of Bologna

Forl`ı, Italy

Silvia Bernardini1
University of Bologna

Forl`ı, Italy

Adriano Ferraresi1
University of Bologna

Forl`ı, Italy

Federico Gaspari2

Universit`a per Stranieri “Dante Alighieri”

Reggio Calabria, Italy

Marcello Soffritti1
University of Bologna

Forl`ı, Italy

1name.surname@unibo.it, 2gaspari@unistrada.it

Abstract

This paper describes an approach to trans-
lating course unit descriptions from Ital-
ian and German into English, using a
phrase-based machine translation (MT)
system. The genre is very prominent
among those requiring translation by uni-
versities in European countries in which
English is a non-native language. For each
language combination, an in-domain bilin-
gual corpus including course unit and de-
gree program descriptions is used to train
an MT engine, whose output is then com-
pared to a baseline engine trained on the
Europarl corpus. In a subsequent exper-
iment, a bilingual terminology database
is added to the training sets in both en-
gines and its impact on the output qual-
ity is evaluated based on BLEU and post-
editing score. Results suggest that the use
of domain-speciﬁc corpora boosts the en-
gines quality for both language combi-
nations, especially for German-English,
whereas adding terminological resources
does not seem to bring notable beneﬁts.

1
Introduction
1.1 Background
Following the Bologna process, universities have
been urged to increase their degree of interna-
tionalization, with the aim of creating a European
Higher Education Area (EHEA) that encourages
students’ mobility. This process has brought with
it the need of communicating effectively in En-
glish also for institutions based in countries where
this is not an ofﬁcial language. Nevertheless, pre-
vious work has shown that institutional academic
communication has not undergone a substantial

increase of translated content, both from a quali-
tative and from a quantitative point of view. Calla-
han and Herring (2012) claim that the number of
universities whose website contents are translated
into English varies across the European Union,
with Northern and Western countries paying more
attention to their internationalization than South-
ern ones. When quality is in focus, things do not
improve: many of the translated documents fea-
ture terminological inconsistencies (Candel-Mora
and Carri´o-Pastor, 2014).

As one of the aims in the creation of the
EHEA was to foster students’ mobility, availabil-
ity of multilingual course unit descriptions (or
course catalogues) has become especially impor-
tant. These texts start by indicating the faculty the
course belongs to. After this, brief descriptions
of the learning outcomes and of the course con-
tents are given. The following sections outline the
assessment and teaching methods. Lastly, details
are provided regarding the number of ECTS cred-
its for the course unit, useful links and readings
for students, information about the lecturer’s of-
ﬁce hours and the language in which the course is
taught.

Several aspects make these texts interesting for
our purposes. First, they feature terms that are
typical of institutional academic communication,
but also expressions that belong to the discipline
taught (Ferraresi, 2017). Second, they are usually
drafted or translated by teachers and not by pro-
fessional writers/translators (Fernandez Costales,
2012). Therefore, their disciplinary terminology is
likely to be accurate, but they might not comply
with the standards of institutional academic com-
munication. Finally, they tend to be repetitive and
relatively well-structured, and to be produced in
large numbers on a yearly basis, through a mix of
drafting from scratch and partial revisions or up-

2

dates.

These characteristics make course catalogues an
ideal test bed for the development of tools support-
ing translation and terminology harmonization in
the institutional academic domain. Indeed, the de-
velopment of such tools has been on the agenda of
universities across Europe for several years now,
as testiﬁed, e.g., by previous work in this area
funded by the EU Commission in 2011 and involv-
ing ca. 10 European universities and private com-
panies1. Despite its interest, this project does not
seem to have undergone substantial development
after 2013, nor does it seem to have had the de-
sired impact on the community of stakeholders. In
addition to that, it does not include one of our lan-
guage combinations, i.e. Italian-English.

1.2 Objectives of the Study
In practical terms, being able to automatically
translate texts that typically contain expressions
belonging to different domains – the academic one
and the disciplinary one – raises the question of
how to choose the right resources and how to add
them to the system in order to improve the out-
put quality and to simplify post-editing. We aim
at contributing to machine translation (MT) de-
velopment not only understanding if MT results
for translation in this domain are promising, but
also ﬁnding out the most effective setup for MT
engines, i.e. with a generic corpus, with an in-
domain corpus or with one of these corpora and
a bilingual glossary belonging to the educational
or disciplinary domain.

In addition to focusing on developments for
MT and its architecture, we are laying emphasis
on MT contribution to the work of post-editors
and to translation in the institutional academic do-
main. The development of an MT tool able to
support professional and non-professional post-
editors would speed up the translation of texts,
thus favoring the internationalization of universi-
ties. Moreover, the present study is part of a larger
project that aims to test the impact of terminol-
ogy on output quality, post-editing effort and post-
editor satisfaction2. Since terminology inconsis-
tencies can negatively affect both output quality

1http://www.bologna-translation.eu/
2This work is part of a three-year project that will also in-
clude experiments with post-editors, aimed at measuring their
reactions to machine-translated output enhanced with termi-
nological backup information, as well as tests on neural ma-
chine translation (NMT).

and post-editor’s trust in an MT system, we are
also investigating the relationship (if any) between
the use of terminology resources at various stages
of the MT-PE pipeline, and the perception of out-
put quality and post-editing effort by professional
and non-professional post-editors (Gaspari et al.,
2014; Moorkens et al., 2015). To sum up, even if
at these initial stages we are primarily interested
in discovering the most effective architecture for
our MT tool for this peculiar domain, we see these
initial steps as crucially related to the overall ap-
plication in a real-world scenario where human-
machine interaction is of the essence.

For this study, a phrase-based statistical ma-
chine translation system (PBSMT) was used to
translate course unit descriptions from Italian into
English and from German into English. We built
a baseline engine trained on a subset of the Eu-
roparl3 corpus. Then, a small in-domain corpus
including course unit descriptions and degree pro-
grams (see sect. 3.2) belonging to the disciplinary
domain of the exact sciences was used to build our
in-domain engine. We chose to limit our scope and
concentrate on exact sciences since German and
Italian degree programs whose course units belong
to this domain translate their contents into English
more often than other programs (the scarcity of
high-quality human-translated parallel texts is ar-
guably the major challenge for our work). We en-
riched the two training data sets with a bilingual
terminology database belonging to the educational
domain (see sect. 3.3) and we built two new en-
gines: one trained on the Europarl corpus subset
plus the bilingual terminology database, and one
on the in-domain bilingual sentence pairs plus the
bilingual terminology database. Each of the four
engines for each language combination was then
tuned on a subset of the in-domain corpus (more
details about the resources are given in sect. 3). To
evaluate the output quality, we are relying on two
popular metrics: the widely-used BLEU score (Pa-
pineni et al., 2002) and post-editing score. The
latter is based on edit-distance, like other popu-
lar methods such as TER or HTER (Snover et al.,
2006), i.e. on the post-edits required to turn an MT
output segment into its human reference. Even if
our reference text is not a post-edited translation
of the evaluation data set source side, we chose to
also consider the PES results since they tend to be
more clear for translators and post-editors.

3http://www.statmt.org/europarl/

3

2 Previous Work

A number of approaches have already been de-
veloped to use in-domain resources like corpora,
terminology and multi-word expressions (MWEs)
in statistical machine translation (SMT), to tackle
the domain-adaptation challenge for MT. For ex-
ample, the WMT 2007 shared task was focused on
domain adaptation in a scenario in which a small
in-domain corpus is available and has to be in-
tegrated with large generic corpora (Koehn and
Schroeder, 2007; Civera and Juan, 2007). More
recently, the work by ˇStajner et al. (2016) ad-
dressed the same problem and showed that an
English-Portuguese PBSMT system in the IT do-
main achieved best results when trained on a large
generic corpus and in-domain terminology.

Langlais (2002) showed that adding terminol-
ogy to the phrase-table actually improved the
WER score for the French-English combination
in the military domain. For the same language
combination, Bouamor et al. (2012) used pairs
of MWEs extracted from the Europarl corpus as
one of the training resources, but only observed
a gain of 0.3% BLEU points (Papineni et al.,
2002). Ren et al. (2009) extracted domain-speciﬁc
MWEs from the training corpora showing encour-
aging improvements in terms of BLEU score for
translations from English to Chinese in the patent
domain. A sophisticated approach is the one de-
scribed in Pinnis and Skadins (2012), where terms
and named entities are extracted from in-domain
corpora and then used as seeds to crawl the web
and collect a comparable corpus from which more
terms are extracted and then added to the training
data. This method shows an improvement of up to
24.1% BLEU points for the English-Latvian com-
bination in the automotive domain.

Methods to integrate terminology in MT have
been recently developed focusing on how to dy-
namically insert terminology into a PBSMT sys-
tem, i.e. injecting terminology in an MT engine
without having to stop it or re-train it. Such meth-
ods suit the purpose of the present paper, as they
focus (also) on Italian, German and English. Ar-
can et al. (2014a) tested for the ﬁrst time the
cache-based method (Bertoldi et al., 2013) to in-
ject bilingual terms into a SMT system without
having to stop it. This brought to an improve-
ment of up to 15% BLEU score points for English-
Italian in medical and IT domains. For the same
domains and with the same languages (in both di-

rections), Arcan et al. (2014b) developed an ar-
chitecture to identify terminology in a source text
and translate it using Wikipedia. This study re-
sulted in an improvement of up to 13% BLEU
score points. Moving to approaches focusing ex-
clusively on morphologically complex languages,
Pinnis (2015) reported on a new pre-processing
method for the source text in order to extract ter-
minology, translate it and add it to the MT sys-
tem. An evaluation for English-German, English-
Latvian and English-Lithuanian in the automotive
domain showed an improvement of up to 3.41
BLEU points. The manual evaluation pointed out
an increase of up to 52.6% in the number of the
terms translated correctly.

3 Experimental Setup
3.1 Machine Translation System
The system we used to build the engines for
this experiment is the open-source ModernMT
(MMT)4. MMT (Bertoldi et al., 2017) is a project
funded by the European Union which aims to
provide translators and enterprises with a new
and innovative phrase-based tool. The main rea-
sons behind the choice of this software is that
it is able to build custom engines without long
and computationally complex training and tun-
ing phases, providing high-quality translations for
speciﬁc domains. As a matter of fact,
recent
evaluation (Bertoldi et al., 2017) carried out on
texts from 8 different domains and for two lan-
guage combinations (English-French and English-
German), showed that MMT’s training and tuning
are faster than Moses’, while their quality is simi-
lar. Besides, MMT outperforms Google Translate
when translating texts belonging to speciﬁc do-
mains on which the engine was trained.

For this work, we exploited both the tuning
and testing procedures already implemented in
MMT, i.e. a standard Minimum Error Rate Train-
ing (MERT) (Och, 2003) and a testing phase in
which the engine can be evaluated on a speciﬁc
set of data chosen by the user. The metrics used
are the BLEU score (Papineni et al., 2002) and the
post-editing score (PES), which is the inverse of
the TER score (Snover et al., 2006).

3.2 Corpora
As mentioned in sect. 1.2, we enriched a baseline
and an in-domain MT system using in-domain cor-

4http://www.modernmt.eu/

4

pora and terminology. Due to limitations of the
computational resources available,
training and
tuning an engine on the whole Europarl corpus
would not have been possible. We therefore ex-
tracted a subset of 300,000 sentence pairs from
the Europarl corpus both for Italian-English and
for German-English to use them as the training
set of our baseline engine. Then, bilingual corpora
belonging to the academic domain were needed
as in-domain training, development and evalu-
ation data sets for the two language combina-
tions. Relying only on course unit descriptions to
train our engines could have led to over-ﬁtting of
the models. Also, good-quality bilingual versions
of course unit descriptions are often not avail-
able. To overcome these two issues we added a
small number of degree program descriptions to
our in-domain corpora, i.e. texts that are similar
to course unit descriptions, but provide general in-
formation on a degree program, and are thus less
focused on a speciﬁc discipline or course.

To build our

in-domain corpora, we fol-
lowed the method developed within the CODE
project5. An Italian-English corpus of course
catalogues was also available thanks to this
project. The starting point for the search for in-
stitutional academic texts in German and Italian
was the University Ranking provided by Webo-
metrics6. This website ranks Higher Education In-
stitutions from all over the world based on their
web presence and impact. We crawled the top uni-
versity websites in Italy and Germany and for each
of the two countries we identiﬁed the four univer-
sities with the largest quantity of contents trans-
lated into English. From these, we downloaded
texts within the exact sciences domain.

For the German-English combination, we col-
lected two bigger corpora of course unit descrip-
tions, and two smaller ones of degree program
descriptions. For the Italian-English one we col-
lected a corpus of course unit descriptions and
two smaller corpora of degree program descrip-
tions, to which we then added the CODE course
unit descriptions corpus after cleaning of texts not
belonging to the exact science domain. For both
language combinations, each corpus was extracted

5CODE is a project aimed at building corpora and
tools to support
translation of course unit descriptions
into English and drafting of these texts in English as
a lingua franca. http://code.sslmit.unibo.it/
doku.php

6http://www.webometrics.info/en

from a different university website. We ended up
with 35,200 segment pairs for German-English
and 42,000 for Italian-English. The smallest cor-
pus made of course unit descriptions was used as
evaluation data set, i.e. 4,400 sentence pairs out
of 35,200 for German-English and 3,700 out of
42,000 for Italian-English. 3,500 sentence pairs
from the biggest course unit description corpus
were extracted for each of the language combina-
tions to exploit them as development set. The re-
maining sentence pairs – i.e. 27,300 for German-
English and 34,800 for Italian-English – were used
as training set for the in-domain engines.

3.3 Terminology
The terminology database was created merging
three different IATE (InterActive Terminology for
Europe)7 termbases for both language pairs and
adding to them terms and MWEs extracted from
the Eurydice8 glossaries. More speciﬁcally, the
three different IATE termbases were the follow-
ing: Education, Teaching, Organization of teach-
ing. We also extracted the monolingual terms from
the summary tables at the end of the ﬁve Eurydice
volumes and we chose to keep just the terms in
the ﬁfth volume which are already translated into
English and those terms whose translation in the
target language was relatively straightforward (for
example Direttore di dipartimento in Italian and
“Head of department” in English).

The three different IATE ﬁles were in xml
format and their terms were grouped based on
their underlying concepts, so a single entry of-
ten contained more than one source term re-
lated to many target terms. For example one en-
try included the German terms Erziehung und
Unterricht and Unterricht und Erziehung,
that
are translated into English as “educational ser-
vices”, “instruction services”, “teaching” and “tu-
ition”. We extracted each term pair and merged
them into a single plain-text (tab separated) bilin-
gual termbase, where each pair has its own en-
try. We then collected the Eurydice bilingual
terms and merged them with those extracted from
IATE. At the end of this process, we obtained an
Italian-English termbase with 4,143 bilingual en-
tries and a German-English one with 5,465 bilin-
gual entries.

In order to test the relevance of our term collec-

7http://iate.europa.eu/
8http://eacea.ec.europa.eu/education/

eurydice/

5

tions for the experiment, we computed the number
of types and tokens of the evaluation data set on
the source side, and the number of termbase en-
tries. We then compared these ﬁgures to the de-
gree of overlap between the two resources, i.e. to-
kens and types occurring both in the termbase and
in the source side of the evaluation data set for
both language pairs, so as to gauge the relevance
of the termbase. Since our termbase does not con-
tain any inﬂected form, we are computing the de-
gree of overlap only on the canonical form of the
terms. Results are displayed in Tables 1 and 2.

It-En
Corpus tokens
Corpus types
Termbase entries
Tokens overlap
Types overlap

50,248
6,985
4,142
20.44%
13.27%

Table 1: Types and tokens in the evaluation data
sets, termbase entries, and type/token overlap be-
tween the two resources for It-En.

De-En
Corpus tokens
Corpus types
Termbase entries
Tokens overlap
Types overlap

26,956
5,614
5,462
19.98%
9.63%

Table 2: Types and tokens in the evaluation data
sets, termbase entries, and type/token overlap be-
tween the two resources for De-En.

the total number of

The German-English corpus tokens are half
those in the Italian-English corpus, while the
Italian-English corpus includes ca. 1,300 types
more than the German-English one (approx-
imately 20% of
Italian
types). When German is the source language, the
number of termbase entries is ca. one ﬁfth of the
corpus tokens, while the number of the Italian-
English glossary entries is only one twelfth of the
number of corpus tokens. The ratio between num-
ber of overlapping tokens and number of corpus
tokens remains the same across the two language
combinations (ca. 20%), while the ratio related
to types is 13.27% for Italian-English and 9.63%
for German-English. Based on these ﬁgures, we
would expect our Italian-English termbase to have
a slightly stronger inﬂuence on the output than the

German-English one.

It is also interesting to observe the list of the
glossary words occurring in the output ranked by
their frequency for both source languages. For
German the ﬁrst ﬁve are Informatik, Software,
Vorlesung, Fakult¨at, Studium, while for Italian
we have: corso, insegnamento, calcolo, prova,
voto. Considering the low degree of overlap and
the large presence of basic words for the domain
in both languages – and since most of them are un-
likely to have multiple translations – we decided
not to work on a time-demanding task such as
solving the ambiguities in the termbase.

4 Experimental Results

For both language combinations we used MMT to
create four engines:

• One engine trained on the subset of Europarl

(baseline).

• One engine trained on the subset of Eu-
roparl and the terminology database (base-
line+terms).

• One engine trained on the in-domain corpora

(in-domain).

• One engine trained on the in-domain cor-
pora and the terminology database (in-
domain+terms).

Each engine was then tuned on a development
set formed of 3,500 in-domain sentence pairs and
evaluated on ca. 3,700 segment pairs (for Italian-
English) and 4,400 segment couples (for German-
English) (see sect. 3.2 for a description of the
training, tuning and testing data sets).

Italian-English

4.1
For the engine that translates from Italian into En-
glish, results are shown in Table 3. If we compare
the best in-domain engine to the best baseline ac-
cording to the BLEU score, we can see that, af-
ter the tuning phase, the in-domain engine outper-
forms the baseline+terms by 7.85 points. More-
over, each engine has improved its performance
according to both metrics after being tuned on our
development set.

According to the automatic metrics, and con-
trary to our expectations, adding the terminol-
ogy database did not inﬂuence the in-domain en-
gines or the baseline ones in a substantial way,

6

sometimes actually causing a slight decrease in
the engine performance. For example, our two in-
domain engines had similar performance both be-
fore tuning – when their scores differed by 0.22
BLEU points and 0.19 PES points, with the in-
domain outperforming its counterpart with termi-
nology – and after tuning, when in-domain out-
performed in-domain+terms by 0.78 BLEU points
and 0.35 PES points.

To gain a slightly better insight into the engine
performance, we quickly analyzed the outputs of
the four engines. Many sentences contained un-
translated words or word-order issues. The refer-
ence sentence “During the semester, two guided
visits to relevant experimental workshops on the
topics covered in the course will be organized”
contained the words “semester” and “course” that
are basic words of the domain and appear in the
glossary. However, in both baseline engines the
output is “During the half, will be organized two
visits led to experimental laboratories relevant to
the subjects raised during” and in both the in-
domain ones the output is “During the semester,
will be two visits to experimental laboratories
pertinent to topics covered in the course”. Two
things are interesting here. First of all, the out-
put conﬁrms what the automatic metrics already
highlighted:
the output of the engines without
terms is generally very similar to the output with
terms. Moreover, adding the termbase did not sub-
stantially improve the generic output even when
some of its words appeared in the termbase. We
will discuss these results further in sect. 4.3.

It-En Engine
Baseline
Baseline tuned
Baseline+terms
Baseline+terms tuned
In-domain
In-domain tuned
In-domain+terms
In-domain+terms tuned

BLEU PES
35.27
16.90
22.58
40.08
35.04
17.09
40.36
22.75
50.61
26.72
30.60
53.17
50.42
26.50
29.82
52.82

Table 3: Results for the Italian-English combina-
tion. For each engine, BLEU and PES scores are
given both before and after tuning. The best base-
line and in-domain results are shown in bold.

De-En Engine
Baseline
Baseline tuned
Baseline+terms
Baseline+terms tuned
In-domain
In-domain tuned
In-domain+terms
In-domain+terms tuned

BLEU PES
24.03
41.24
47.70
34.98
42.10
25.65
36.89
49.03
49.06
43.21
50.75
46.31
49.23
43.48
47.05
51.20

Table 4: Results for the German-English com-
bination. For each engine, BLEU and PES scores
are given both before and after tuning. The best
baseline and in-domain results are shown in bold.

4.2 German-English
Table 4 shows results for the German-English
language combination. After tuning, the best in-
domain engine outperformed the baseline by 10.16
points according to BLEU and by 2.17 points ac-
cording to PES. The tuning performed on the in-
domain engines causes an improvement of more
than 3% in terms of BLEU score. Regarding the
baseline engines, the tuning enhances the quality
by ca. 10 BLEU points and 7 PES points, thus
narrowing the performance gap between the two
different kinds of engines.

What is important to notice is that, counter-
intuitively and similarly to what we observed for
the Italian-English combination, the collection of
academic terminology does not affect the transla-
tion output quality: the metrics show an improve-
ment of 0.74 BLEU points and 0.45 PES points
when terminology is added to the in-domain en-
gine (results after the tuning phase). The addition
of terminology seems to be slightly more effective
on the baseline engines, improving the automatic
scores by 1.91 BLEU points and 1.33 PES points
after tuning.

A quick analysis of the output shows the same
issues identiﬁed in sect. 4.1. One example is
the reference sentence “Lecture, exercises, pro-
gramming exercises for individual study”, that is
translated as “Lecture, exercises, Programmier-
aufgaben zum private study” in both in-domain
engines and as “Lecture, exercise, Programmier-
aufgaben on Selbststudium” in both baseline en-
gines (the word couple Vorlesung-Lecture was in
the termbase). The same German word was not
translated in some sentences of the two in-domain
engines outputs – e.g. “Vorlesung (presentation of

7

Slides and presentation interactive examples)”, for
the engine with terms and “Vorlesung (presenta-
tion of presentation and interaktiver examples)”
for the engine without terms – while it was in the
baseline ones: “Lecture (presentation of Folien
and idea interactive examples)”. This is another
negative result for our termbase, since neither of
the in-domain engines translated “Vorlesung” as
“lecture”, while the baseline ones did without the
help of the terminological resource. We will fur-
ther discuss these results in sect. 4.3.

4.3 Discussion

In our experimental setup, adding terminology to
a general-purpose engine and to an in-domain en-
gine does not inﬂuence the output quality sub-
stantially. We compared the ﬁgures in Tables 1
and 2 (regarding the degree of overlap between
the evaluation data set and the bilingual glos-
sary) to the automatic scores assigned to our en-
gines (Tables 3 and 4) to investigate the impact
on output quality. The degree of tokens overlap
between the bilingual glossary and the evaluation
data set is similar for the two source languages
(ca. 20%). Despite this, for the German-English
combination the baseline+terms engine outper-
formed the baseline engine by 1.91 BLEU points
and 1.33 PES points, which is the largest gain ob-
tained in this study adding a bilingual glossary to
the training data set. If we look at the baseline and
baseline+terms engines for Italian-English, for ex-
ample, the latter outperformed the former by only
0.17 BLEU and 0.28 PES points. This might sug-
gest that the target terms in the German-English
glossary were consistent with those used in the ref-
erence text, while for Italian-English there were
more discrepancies between the two resources.

Another variable that has to be taken into ac-
count is the way in which terms are extracted and
injected into the MT engine. As reported in sect. 2,
methods in which terminology is extracted from
other resources and then added to training data
sets (Bouamor et al., 2012) are less effective than,
for example, approaches in which terminology is
extracted from the training data set (Ren et al.,
2009; Pinnis and Skadinˇs, 2012) or injected dy-
namically, i.e. at run-time without re-training, into
the MT engine (Arcan et al., 2014a). In our case
the Italian-English term pairs were 4,143 against
the 34,800 sentence pairs of the training data set,
while for German-English we had 5,465 term pairs

as compared to 27,300 sentence pairs. Due to the
difference in the amount of term pairs and segment
pairs, simply adding the glossary to the sentence
pairs might cause it to lose its inﬂuence on the
training process.

If we look at Tables 3 and 4, we can see that
in-domain segments boost the engine quality both
during training – with the in-domain engines out-
performing the baseline – and after tuning, which
brings remarkable improvements. This could sug-
gest that the PBSMT system is able to extract aca-
demic terms and expressions from the in-domain
corpus, without the need of being enhanced with
a termbase belonging to the same domain. Addi-
tional evidence for this are the examples in sec-
tion 4.1, where some of the termbase words were
not translated in the baseline engines output of
both language combinations, while they were cor-
rectly translated in both in-domain engines. As
a matter of fact, the terminology database was
able to increase the score by more than 1% in
terms of BLEU only on one occasion – i.e. the
baseline engine for German-English –, while for
the other three engine pairs (in-domain with and
without terms for German-English, baseline and
in-domain with and without
terms for Italian-
English) the performance increased by few deci-
mals or even decreased. The same happens if we
look at the PES score.

To further discuss the results without using the
BLEU metric, whose ﬁgures are often less intu-
itive than the PES’ ones especially for translators
and post-editors, it is interesting to notice how
the in-domain engines for both language combi-
nations always reach at least 50% in terms of PES
score after tuning. Despite the low quality of the
examples seen in sections 4.1 and 4.2, these PES
scores are an encouraging result if we consider
that we are carrying out the ﬁrst experiment on
this domain and that we are exploiting quite a
small amount of in-domain resources to build our
engine, a condition that is likely to remain con-
stant given the nature of communication in this
domain. It also suggests that, in this domain, MT
is likely to boost the post-editor’s productivity if
compared to a translation from scratch. Moreover,
we expect to obtain further improvements building
an engine combining both generic and in-domain
resources in the training phase, so as to hopefully
observe a further increase of the PES and hence of
the post-editor productivity.

8

5 Conclusion and Further Work

This paper has described an attempt at evaluat-
ing the potential of the use of in-domain resources
(terminology and corpora) with MT in the institu-
tional academic domain, and more precisely for
the translation of course unit descriptions from
German into English and from Italian into En-
glish. Following the results of the present exper-
iment, we are planning to carry out further work
in this ﬁeld.

Since academic terms are only one subset of
the terminology used in course unit descriptions,
which also includes terms related to the subject
matter of each unit, it would be interesting to in-
vestigate the advantages of adding disciplinary ter-
minology alongside the academic one. We there-
fore plan to combine academic and disciplinary
terminology. Following the encouraging results of
the baseline engine tuned on the in-domain re-
sources, we also plan to investigate the perfor-
mance of an engine trained on both generic and
in-domain resources and tuned on an in-domain
development set. As shown in the work by ˇStajner
et al. (2016), PBSMT systems’ performance in-
creases when the training data set
includes a
small quantity of in-domain resources – corpora
or termbase – and a large generic corpus.

As we have seen in section 3.3 the overlap af-
forded by the termbase used for this experiment
was less than optimal and its structure would re-
quire an accurate procedure to extract the most
likely term pair for this domain, since a source
term often has multiple target translations. For
these reasons and basing on the results, IATE
is probably not the best resource for our pur-
poses. As part of future work, we are interested
in extracting terminology from other resources,
e.g. the UCL-K.U.Leuven University Terminol-
ogy Database9, or, ideally, from a resource de-
veloped collaboratively by universities across Eu-
rope, consistent with EU-wide terminological ef-
forts but more readily usable and focusing on
agreed-upon terms and with limited ambiguity. We
will also test methods to make available the most
relevant terms for the texts to be translated, i.e. ex-
tracting terminology from the training data. In
both cases – use of external resources or extrac-
tion from the training data set – we are planning
to add inﬂected forms of the terms. In addition

9https://sites.uclouvain.be/lexique/

lexique.php

to their extraction, we are considering injection
of terms into an MT system in other ways than
simply adding them to the training set, where the
termbase is likely to play a minor role because of
its small size compared to the corpora. In future
work we will compare methods to add terms at
run-time in a post-editing environment, in order
to analyze the impact of these suggestions on the
post-editors’ work. What we are expecting from
this experiment is to ﬁnd a way to increase the
post-editor trust in the suggested terminology, and
hence in the MT engine.

As this was our ﬁrst attempt to build an MT en-
gine in this domain, sometimes we were forced to
concentrate on more technical aspects, e.g. the im-
provements in the BLEU score to analyze the en-
gines’ development. In future work we are plan-
ning to use metrics that better take into account
terminology translation (e.g. precision, recall, f-
measure) and also manual evaluation to collect
more data on the impact of our work on the post-
editing phase.

To conclude, in this paper we have taken a ﬁrst
step toward the development of a tool that com-
bines machine translation, corpora and terminol-
ogy databases, with the aim of streamlining the
provision of course unit descriptions in English
by European universities. Our in-domain engines
showed encouraging results, even if – as expected
– they are not able to boost a post-editor’s pro-
ductivity yet, while the role of terminology (what
kind, how it is injected into the engine) is still to be
further investigated, as is the conﬁdence-building
potential of quality terminology databases on post-
editing work.

Acknowledgments

The authors would like to thank Mauro Cettolo,
Marcello Federico and Luisa Bentivogli of Fon-
dazione Bruno Kessler (FBK) for their advice and
for help with ModernMT, and three anonymous re-
views for insightful comments on the ﬁrst draft of
this paper. The usual disclaimers apply.

References

Mihael Arcan, Claudio Giuliano, Marco Turchi, and
Paul Buitelaar. 2014b.
Identiﬁcation of bilin-
gual terms from monolingual documents for sta-
tistical machine translation.
In Proceedings
of the 4th International Workshop on Computa-

9

tional Terminology. Dublin, Ireland, pages 22–31.
http://www.aclweb.org/anthology/W14-4803.

Mihael Arcan, Marco Turchi, Sara Tonelli, and Paul
Buitelaar. 2014a.
Enhancing statistical machine
translation with bilingual terminology in a CAT
environment.
In Yaser Al-Onaizan and Michel
Simard, editors, Proceedings of AMTA 2014. Van-
couver, BC.

Nicola Bertoldi, Roldano Cattoni, Mauro Cet-
tolo, Amin Farajian, Marcello Federico, Davide
Caroselli, Luca Mastrostefano, Andrea Rossi,
Marco Trombetti, Ulrich Germann, and David
Madl. 2017. MMT: New open source MT for the
translation industry.
In Proceedings of the 20th
Annual Conference of
the European Association
for Machine Translation. Prague, pages 86–91.
https://ufal.mff.cuni.cz/eamt2017/user-project-
product-papers/papers/user/EAMT2017 paper 88.pdf.

Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Andy Way, Khalil Sima’an, Mikel L. For-
cada, Daniel Grasmick, and Heidi Depaetere, edi-
tors, Proceedings of the XIV Machine Translation
Summit. Nice, France, pages 35–42.

for

Joseph Mariani,

Dhouha Bouamor, Nasredine Semmar, and Pierre
Identifying bilingual
Zweigenbaum. 2012.
multi-word expressions
statistical machine
translation. In Nicoletta Calzolari, Khalid Choukri,
Thierry Declerck, Mehmet U˘gur Do˘gan, Bente
Maegaard,
and
Stelios Piperidis,
the
Eighth International Conference on Language
Resources and Evaluation (LREC-2012). Euro-
pean Language Resources Association (ELRA),
ACL An-
Istanbul, Turkey, pages 674–679.
thology Identiﬁer: L12-1527.
http://www.lrec-
conf.org/proceedings/lrec2012/pdf/886 Paper.pdf.

Jan Odijk,
editors, Proceedings of

Ewa Callahan and Susan C. Herring. 2012.

Lan-
guage choice on university websites: Longitudinal
trends. Journal of International Communication 6
(2012):322–355.

Miguel ´Angel Candel-Mora and Mar´ıa Luisa Carri´o-
Pastor. 2014. Terminology standardization strate-
gies towards the consolidation of the European
Higher Education Area. Procedia - Social and Be-
havioral Sciences 116:166 – 171.

in

Jorge Civera and Alfons Juan. 2007.
statistical machine

Domain
translation
adaptation
with mixture modelling.
In Proceedings of
the Second Workshop on Statistical Machine
Translation. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 177–180.
http://www.aclweb.org/anthology/W/W07/W07-
0222.

Alberto Fernandez Costales. 2012. The international-
In Anthony Pym

ization of institutional websites.

and David Orrego-Carmona, editors, Translation
Research Projects. Tarragona: Intercultural Studies
Group, pages 51–60.

Adriano Ferraresi. 2017. Terminology in European
university settings. The case of course unit de-
scriptions.
In Paola Faini, editor, Terminological
Approaches in the European Context. Cambridge
Scholars Publishing, Newcastle upon Tyne, pages
20–40.

Federico Gaspari, Antonio Toral, Sudip Kumar Naskar,
Declan Groves, and Andy Way. 2014. Perception vs
reality: Measuring machine translation post-editing
productivity.
In Sharon O’Brien, Michel Simard,
and Lucia Specia, editors, Proceedings of AMTA
2014. Vancouver, BC, pages 60–72.

Philipp Koehn and Josh Schroeder. 2007.

in domain adaptation for

Ex-
statisti-
periments
cal machine translation.
In Proceedings of
the Second Workshop on Statistical Machine
Translation. Association
Computational
Linguistics, Prague, StatMT ’07, pages 224–227.
http://dl.acm.org/citation.cfm?id=1626355.1626388.

for

Philippe Langlais. 2002.

Improving a general-
purpose statistical translation engine by termino-
logical lexicons.
In COLING-02 on COMPUT-
ERM 2002: Second International Workshop on
Computational Terminology - Volume 14. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, COMPUTERM ’02, pages 1–7.
https://doi.org/10.3115/1118771.1118776.

Joss Moorkens, Sharon O’Brien, Igor A. L. da Silva,
Norma B. de Lima Fonseca, and F´abio Alves. 2015.
Correlations of perceived post-editing effort with
measurements of actual effort. Machine Translation
29(3-4):267–284.

Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation.
In Pro-
ceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics - Vol-
ume 1. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’03, pages 160–167.
https://doi.org/10.3115/1075096.1075117.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for au-
tomatic evaluation of machine translation.
In
Proceedings of
the
Association for Computational Linguistics. Asso-
ciation for Computational Linguistics, Philadel-
phia, Pennsylvania, ACL ’02, pages 311–318.
https://doi.org/10.3115/1073083.1073135.

the 40th Annual Meeting of

M¯arcis Pinnis. 2015. Dynamic terminology integration
methods in statistical machine translation. In Pro-
ceedings of the 18th Annual Conference of the Euro-
pean Association for Machine Translation. Antalya,
Turkey, pages 89–96.

10

M¯arcis Pinnis and Raivis Skadinˇs. 2012. MT adapta-
tion for under-resourced domains - what works and
what not. In Human Language Technologies - The
Baltic Perspective - Proceedings of the Fifth Interna-
tional Conference Baltic HLT 2012. Tartu, Estonia,
pages 176–184.

Zhixiang Ren, Yajuan L¨u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identiﬁcation, Interpretation, Disam-
biguation and Applications. Association for Compu-
tational Linguistics, Suntec, Singapore, MWE ’09,
pages 47–54.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas. Cambridge, Massachussets,
pages 223–231.

Sanja ˇStajner, Andreia Querido, Nuno Rendeiro,
Jo˜ao Ant´onio Rodrigues, and Ant´onio Branco. 2016.
Use of domain-speciﬁc language resources in ma-
chine translation.
In Nicoletta Calzolari, Khalid
Choukri, Thierry Declerck, Sara Goggi, Marko Gro-
belnik, Bente Maegaard, Joseph Mariani, Helene
Mazo, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016). European Language Resources
Association (ELRA), Paris, France, pages 592–598.

