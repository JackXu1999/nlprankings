



















































Enhancing Neural Data-To-Text Generation Models with External Background Knowledge


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3022–3032,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3022

Enhancing Neural Data-To-Text Generation Models
with External Background Knowledge

Shuang Chen1∗, Jinpeng Wang2, Xiaocheng Feng1, Feng Jiang1,3, Bing Qin1, Chin-Yew Lin2
1 Harbin Institute of Technology, Harbin, China

2 Microsoft Research Asia 3 Peng Cheng Laboratory
hitercs@gmail.com, {jinpwa, cyl}@microsoft.com,
{xcfeng, qinb}@ir.hit.edu.cn, fjiang@hit.edu.cn

Abstract
Recent neural models for data-to-text genera-
tion rely on massive parallel pairs of data and
text to learn the writing knowledge. They of-
ten assume that writing knowledge can be ac-
quired from the training data alone. However,
when people are writing, they not only rely
on the data but also consider related knowl-
edge. In this paper, we enhance neural data-to-
text models with external knowledge in a sim-
ple but effective way to improve the fidelity
of generated text. Besides relying on parallel
data and text as in previous work, our model
attends to relevant external knowledge, en-
coded as a temporary memory, and combines
this knowledge with the context representa-
tion of data before generating words. This al-
lows the model to infer relevant facts which
are not explicitly stated in the data table from
an external knowledge source. Experimen-
tal results on twenty-one Wikipedia infobox-
to-text datasets show our model, KBAtt, con-
sistently improves a state-of-the-art model on
most of the datasets. In addition, to quantify
when and why external knowledge is effective,
we design a metric, KBGain, which shows a
strong correlation with the observed perfor-
mance boost. This result demonstrates the rel-
evance of external knowledge and sparseness
of original data are the main factors affecting
system performance.

1 Introduction

Automatic text generation from structured data
(data-to-text) is a classic task in natural language
generation which aims to automatically gener-
ate fluent, truthful and informative texts based
on structured data (Kukich, 1983; Holmes-Higgin,
1994; Reiter and Dale, 1997). Data-to-text is of-
ten formulated into two subproblems: content se-
lection which decides what contents should be in-
cluded in the text and surface realization which

∗Contribution during internship at Microsoft Research.

Subject Relation Object
Guelma

(Q609871)
country 

(P17)
Algeria
(Q262)

defender 
(Q336286)

instance of
(P31)

association football position
(Q4611891)

… … …
MC El Eulma
(Q2742749)

league
(P118)

Algerian Ligue Professionnelle 1 
(Q647746)

External Background Knowledge from Wikidata

Nacer Hammami (born December 28, 1980)
is an Algerian football player who is
currently playing for MC El Eulma in the
Algerian Ligue Professionnelle 1.

Description

Infobox

Entity Linking

Figure 1: An example of generating description from
a Wikipedia infobox. External background knowledge
expanded from the infobox is helpful for generation.

determines how to generate the text based on se-
lected contents. Traditionally, these two sub-
problems have been tackled separately. In recent
years, neural generation models, especially the
encoder-decoder model, solve these two subprob-
lems jointly and have achieved remarkable suc-
cesses in several benchmarks (Mei et al., 2016;
Lebret et al., 2016; Wiseman et al., 2017; Dušek
et al., 2018; Nie et al., 2018).

Such end-to-end data-to-text models rely on
massive parallel pairs of data and text to learn the
writing knowledge. They often assume that all
writing knowledge can be learned from the train-
ing data. However, when people are writing, they
will not only rely on the data contents themselves
but also consider related knowledge, which is ne-
glected by previous methods. For example, as
shown in Fig. 1, an infobox about a person called
Nacer Hammami is paired with its corresponding
biography description from the Wikipedia. How-
ever, the information in the infobox is not enough
to cover all the facts mentioned in the descrip-
tion. To generate this description from the in-



3023

fobox, we need to expand information based on
external background knowledge from its related
entities. For example, in the description: 1) “is
an Algerian football player” indicates the nation-
ality of Nacer Hammami which is not explicitly
stated in the infobox. However, the place of birth,
Guelma, of Nacer Hammami is given, therefore
the nationality can be inferred from the knowledge
that Guelma is a place in Algeria. 2) “playing for
MC El Eulma in the Algerian Ligue Profession-
nelle 1” describes the fact that MC El Eulma is a
club in the Algerian Ligue Professionnelle 1 which
is also not explicitly stated in the infobox which
can be expanded from external knowledge base.

One may argue that neural models can
learn such knowledge when enough parallel co-
occurrence pairs such as (Guelma, Algerian) and
(MC El Eulma, Algerian Ligue Professionnelle 1)
are available. However even in such case, neu-
ral models still tend to make mistakes for sparse
co-occurrence pairs as we will show in the experi-
ments section.

In this paper we enhance neural-network-
based data-to-text generation models with external
knowledge in a simple but effective way. Besides
learning the association between data and text
from parallel data-text pairs as in previous work,
our model attends to relevant external knowledge,
encoded as a temporary memory, and combines
this knowledge with the context representation of
data before generating words. Specifically, both
infobox and background knowledge facts are en-
coded and a dual-attention mechanism is proposed
to guide the decoder to generate text.

To verify the effectiveness of our proposed
model, Knowledge Base enhanced Attention-
based sequence-to-sequence network (KBAtt),
we conduct experiments on multiple Wikipedia
infobox-to-text datasets including WikiBio (Le-
bret et al., 2016) and 20 new datasets1. Our ex-
periment results show that KBAtt consistently im-
proves a state-of-the-art neural data-to-text model
to achieve higher performances on most of the
datasets. To quantify when and why external
knowledge is effective, we design a metric which
shows a strong correlation with the observed per-
formance boost. This result demonstrates the rel-
evance of external knowledge and sparseness of
original data are the main factors affecting system

1Available at https://github.com/hitercs/
WikiInfo2Text

performance.
The contributions of our work can be summa-

rized as follows:

• We demonstrate that external knowledge base
could be used to enhance the performance of
neural data-to-text models.

• We propose a simple yet effective model,
KBAtt, to integrate external knowledge using
a dual-attention mechanism.

• We design a metric, KBGain, to quantify
when and why external knowledge is effec-
tive.

• We contribute twenty infobox-to-text
datasets from a variety of domains.

2 The Proposed Model

Our model takes a data table D (e.g., a Wikipedia
infobox) and a relevant external knowledge base
(KB) containing a set of facts F as input and gen-
erates a natural language text y = y1, ..., yT con-
sisting of T words. To augment the infobox with
external knowledge, we preserve the Wikipedia in-
ternal hyperlink information in the field values of
infobox, and track these hyperlinks to get their
corresponding entities from Wikidata2 where we
retrieve only one-hop facts. The backbone of our
model is an attention based sequence-to-sequence
model (Bahdanau et al., 2014) equipped with copy
mechanism (See et al., 2017). As shown in Fig. 2,
the model consists of four main components: a
table encoder, a KB encoder, the dual attention
mechanism and a decoder. We describe each com-
ponent in the following sections.

2.1 Table Encoder
In Fig. 2, the input data table D consists of sev-
eral field name and field value pairs. We follow
(Sha et al., 2017; Liu et al., 2017) to tokenize the
field values and transform the input table into a
flattened sequence {(ni, vi)}Ni=1, where each ele-
ment is a token vi from a field value paired with
its corresponding field name ni. To encode the
flattened table, we map each (ni, vi) to vector
xi = [e

ni ; evi ], where eni and evi are trainable
2We adopt Wikidata (dumps version: 20150831) as the

external knowledge base. Although we extend the infobox
with external knowledge by using the Wikipedia hyperlink,
we can also apply entity linking to link input data to the
knowledge base in practice.

https://github.com/hitercs/WikiInfo2Text
https://github.com/hitercs/WikiInfo2Text


3024

Field Name Field Value

Full name Nacer Hammami

Place of birth Guelma

Playing position Defender

Current team MC El Eulma

Field Name Content

Full name Nacer

Full name Hammami

… …

Current team Eulma

Field Name Subject Relation Object 

Place of birth Guelma(Q609871)
country 
(P17)

Algeria 
(Q262)

… … … …

Current team MC El Eulma(Q2742749)
league 
(P118)

Algerian Ligue Professionnelle 1
(Q647746)

Entity Linking

GRU
Flatten Table

GRU

MLPTable Attention

KB Attention

softmax

Table Encoder

KB Encoder

Decoder

Figure 2: A diagram of the knowledge base enhanced neural data-to-text generation model. First, we transform
the table into a flattened sequence, extract entities mentioned in the field value of the infobox and link them to
Wikidata where we can retrieve relevant facts. Then, the table contents and external knowledge base facts are
carefully encoded. Finally, a single layer GRU decoder with a dual attention mechanism decides which part of
information should be used for generation.

word embeddings of ni and vi, and [.; .] is the con-
catenation of vectors. Then each xi is encoded
into a hidden vector hi using a bi-directional GRU
(Cho et al., 2014).

2.2 Knowledge Base Encoder
As shown in Fig. 2, we extract entities mentioned
in the field value of infobox and link them to
Wikidata. Then we can retrieve relevant facts
whose subject is the linked entity from Wikidata.
These facts contain important background knowl-
edge related to the infobox which is helpful for
generation. The KB fact set is denoted by F =
{(nj , sj , rj , oj)}|F |j=1, where sj , rj , oj is the sub-
ject, relation and object of the fact respectively,
and field name nj indicates the current fact is
linked by the field value of nj . In order to inte-
grate such KB facts into the neural model, we ap-
ply Multi-Layer Perceptron (MLP) to encode each
fact into its representation:

f j = tanh(Wf [e
nj ; esj ; erj ; eoj ] + bf ) (1)

where Wf and bf are trainable weights and bias,
while enj , esj , erj and eoj is the embedding of
field name nj , subject sj , relation rj and object
oj respectively. To accommodate generation steps
where no information from the external knowl-
edge is needed, such as generating name field
which is already stated in the table, we apply
a simple strategy by padding a none fact in the
knowledge base.

2.3 Dual Attention Mechanism
After encoding the table and background knowl-
edge base facts, we apply a RNN-based decoder

to generate words conditioned on both table infor-
mation and background knowledge fact informa-
tion. In general, given a decoder hidden state dt
at timestep t, we apply dual attention mechanism
including table attention and KB attention to de-
termine which parts that it should pay attention to.
Next we will introduce table attention and KB at-
tention briefly.

2.3.1 Table Attention

The table attention is based on similarity between
decoder hidden state dt and table contents repre-
sentation {hi}Ni=1. Specially we apply attention
mechanism from (Bahdanau et al., 2014) to calcu-
late the table context representation ctablet .

et,i = v>a tanh(Wadt + Uahi) (2)

αt,i =
exp(et,i)∑N
i=1 exp(et,i)

(3)

ctablet =
N∑
i=1

αt,ihi (4)

where Wa, Ua and va are trainable parameters.
αt,i is the table attention weight.

2.3.2 KB Attention

Besides utilizing table information, the words gen-
erated may contain facts which are not directly
mentioned in the table but could be inferred from
the background knowledge F . In order to inte-
grate such knowledge into the decoder, we apply
KB attention over {f j}

|F |
j=1. Similar to the table at-

tention, we can get KB context representation ckbt .



3025

2.4 Decoder
The decoder is a single layer GRU equipped with
copy mechanism. As for the generation mode,
given a decoder hidden state dt, the decoder at-
tends both table and knowledge base using mecha-
nism described above, and get table context repre-
sentation ctablet and KB context representation c

kb
t .

So the context representation at time step t is given
by:

ct = [c
table
t ; c

kb
t ] (5)

Then given table D and knowledge base facts set
F , the probability of word yt generated from a
fixed vocabulary at time step t is defined as fol-
lows:

Pvocab = softmax(f(dt,yt−1, ct)) (6)

where f(·) is a non-linear function applying on the
decoder hidden state dt, previous word embedding
yt−1 and the current context vector ct.

To tackle the rare and unknown words problem,
we adopt the copy mechanism (See et al., 2017).
Specifically, a gate pgen ∈ [0, 1] is introduced to
switch between copy mode and generation mode.
The generation probability pgen is defined as:

pgen = σ(w>c c
table
t + w

>
d dt + w

>
y yt−1 + b) (7)

where σ stands for the sigmoid function while vec-
tors wc, wd, wy and scalar b are trainable parame-
ters. The joint probability for generating yt at time
step t is formulated as follows:

P (yt|y<t, D, F ) = pgenPvocab(yt) (8)

+ (1− pgen)
∑

i:vi=yt

αt,i

where αt,i is the table attention weight defined in
Equation 3.

2.5 Training
Given training dataset {(Dk, F k, yk)}Sk=1 consist-
ing of S samples, our goal is to maximize the prob-
ability of target description y = y1, ...yT given the
input tableD and the background knowledge facts
F . So the objective function is to minimize the
negative log-likelihood:

J(θ) = − 1
S

S∑
k=1

Tk∑
t=1

logP (ykt |yk<t, Dk, F k) (9)

The objective function is fully differentiable, so
the entire model can be trained end-to-end through
backpropagation. Adam (Kingma and Ba, 2014) is
used to optimize our model.

Subject Relation Object
Guelma

(Q609871)
country 
(P17)

Algeria
(Q262)

MC El Eulma
(Q2742749)

league
(P118)

Algerian Ligue Professionnelle 1
(Q647746)

Algerian
Reference

Infobox

Field Name Field Value
Place of birth Guelma
Current team MC El Eulma

KB
(1) String matching with KB(2) Track field in infobox (3) Select token with max PMI

Ligue Professionnelle 1in the

Learnable (KB-Ref ) Unlearnable (KB-Ref )
Learnable (Infobox-Ref ) A: Infobox and KB are helpful B: Infobox is helpful
Unlearnable (Infobox−Ref ) C: KB is helpful D: not enough data

Figure 3: An illustration of KBGain metric.

3 When and Why External Knowledge is
Beneficial

In this section, we introduce KBGain to quan-
tify when and why external knowledge (KB) is
effective, and then show how KBGain corre-
lates with the performance boost of KBAtt over
Seq2Seq+Copy in 21 datasets in Section 4.4. In-
tuitively, incorporating an external KB should im-
prove data-to-text generation performance. How-
ever, to pinpoint the effect of the additional knowl-
edge is not trivial since we know that (a) not all
external knowledge is relevant and (b) neural mod-
els may memorize certain inference patterns when
parallel data is big enough. We assume (1) match-
ing tokens between the external KB and the ref-
erences indicate relevance of the tokens in the ex-
ternal KB; (2) high frequency of co-occurrence of
matching tokens between the infobox and the ref-
erences indicate good potential for neural models
to learn generation patterns which leads to less ef-
fectiveness of an external KB, i.e. no data sparsity
in the original data. To characterize these factors,
we introduce KBGain.

KBGain measures the portion of learnable to-
kens in the references co-occurred with their cor-
responding external KB entries but filter out those
tokens which could also co-occurred with the in-
fobox. We say a token is learnable from one
source (infobox or KB) when the co-occurrence
frequency between them is higher than a thresh-
old γ which is the minimum size of co-occurrence
above which learning will be effective3. The top
table in Fig. 3 summarizes learnable tokens in 4
possible cases. Among them, case C where the
frequency of infobox and reference (Infobox-Ref)
token co-occurrence does not exceed this thresh-
old but its KB and reference (KB-Ref) token co-

3Here, γ is set 25 by tuning on the development datasets,
please see Appendix A.3 for more details.



3026

Dataset WikiBio Album

Avg. # tokens per sentence 26.1 21.0
Avg. # tokens per table 53.0 57.1
Avg. # fields per table 19.7 15.9
Avg. # table tokens per sent. 11.6 9.7
Avg. # entities per table 5.9 5.5
Avg. # fact tuples per entity 19.0 7.2
Dataset size 728, 321 76, 105

Table 1: Dataset statistics for two sample datasets.

occurrence does, then this is where the KB will be
effective. KBGain is defined as the average ratio
between count of the tokens falling into category
C and the length of their corresponding reference
on the test set.

Specifically, for all tokens in the reference ex-
cept stop words and punctuation, we select those
matched4 on string with the object tokens of KB
but not with the infobox. For example, the to-
ken Algerian in the reference is matched with
the object of the last KB tuple. The KB-Ref to-
ken pair is simply acquired by string matching
while Infobox-Ref token pair can be tracked by
firstly finding the corresponding field based on the
matched tuple, and then selecting the token in its
field value with the highest pointwise mutual in-
formation (PMI).

4 Experiments

4.1 Datasets
In the experiments, we adopt the WikiBio (Le-
bret et al., 2016) along with twenty new infobox-
to-text datasets collected from Wikipedia5. The
full statistics of these datasets could be found in
the Appendix A.2. Table 1 shows the statistics
of two sample datasets. Each dataset consists
of infoboxes as input data and the first sentences
of their corresponding Wikipeida articles as ref-
erences. For example, on datasets WikiBio and
Album, we extract 5.9 and 5.5 entities from each
table, and each entity has 19.0 and 7.2 extended

4To make it simple, we adopt strict string matching. This
is acceptable for rough quantification of the intended mea-
surement since precise and semantic-aware matching is still
an active research area. We also try several popular stemmers
to expand tokens e.g., Algeria to Algerian, but no stemmers
have such capability.

5These twenty datasets are similar to WikiBio but from
different domains, e.g. Album, Book etc., which are charac-
terized by infobox template category name. They are created
with the similar procedure and the same Wikipedia dumps as
outlined in Lebret et al. (2016). For more details about data
collection, please refer to the Appendix A.1.

Dataset Seq2Seq+Copy KBAtt ∆

Single 43.60 47.70 4.10
Station 54.30 56.77 2.47
Australian place 43.73 45.73 2.00
Album 41.04 42.77 1.73
NRHP 48.97 50.43 1.46
Airport 45.17 46.61 1.44
Book 36.07 37.49 1.42
Automobile 18.64 19.95 1.31
Building 24.13 25.21 1.08
UK school 33.64 34.72 1.08
School 37.30 38.33 1.03
Football club season 46.05 47.02 0.97
UK place 41.46 42.38 0.92
Military unit 37.74 38.58 0.84
Military conflict 18.58 19.21 0.63
WikiBio 44.28 44.59 0.31
Television episode 73.59 73.87 0.28
NCAA team season 87.37 87.58 0.21
French commune 90.14 90.33 0.19
Settlement 77.59 77.68 0.09
Video game 29.54 29.48 -0.06

Table 2: BLEU-4 score with two generation models on
21 datasets.

facts tuples, on average. WikiBio and the other 20
datasets consist of 728,321 and 668,796 instances
respectively, and they together cover 84.34% in-
foboxes of the English Wikipedia6.

4.2 Experiment Setup

We conduct experiments on the datasets as in-
troduced in Section 4.1. Seq2Seq+Copy is the
main baseline: a sequence-to-sequence (Seq2Seq)
model equipped with copy mechanism from See
et al. (2017) which is one of the state of art meth-
ods. We also compare our results with other pub-
lished results using the WikiBio dataset. The
model structure of our baseline model is most sim-
ilar to Sha et al. (2017) by removing their spe-
cialized design on order planning which is not the
focus of this paper. Since our aim is to verify
the effectiveness of external knowledge for data-
to-text task, we keep our baseline model as gen-
eral as possible without other specialized design.
Our primary model is KBAtt: a model which in-
tegrates the background knowledge into baseline
model through a KB encoder and KB attention
mechanism. We employ BLEU (Papineni et al.,
2002) as the automatic evaluation metric. In ad-
dition to BLEU, we conduct human evaluation to
assess the factual accuracy of generated sentences.

6Although the English Wikipedia has about 5 million en-
tities, we totally parsed 1,656,458 infoboxes and drop some
of them due to data noise which indicates nearly 2/3 entities
have no infobox.



3027

4.3 Training Details

The dimensions of all trainable word embeddings
are set to 512, and the GRU hidden states sizes
are set to 512. To limit the memory of our model,
we set the maximum number of facts per table to
500. We initialize all the model parameters ran-
domly using a uniform distribution between -0.08
and 0.08. For the model training, we use Adam
(Kingma and Ba, 2014) with initial learning rate of
0.001 as the optimization algorithm. The training
batch size is set to 64. We also apply gradient clip-
ping (Pascanu et al., 2013) with range [-1,1] dur-
ing training. We conduct experiments using single
card NVIDIA Tesla V100. The largest 15 datasets
with more than 9500 training instances are trained
for 20 epochs, while the remaining 6 datasets are
trained for 40 epochs. All the models were se-
lected based on BLEU-4 score on the development
set. All the experiments use greedy search as the
decoding algorithm during testing.

4.4 Main Results

4.4.1 Overall Results
To verify the effectiveness of KBAtt, we con-
duct experiments on 21 Wikipedia infobox-to-text
datasets. Table 2 shows the performances of
KBAtt and Seq2Seq+Copy in terms of BLEU-4
score. As we can see, KBAtt consistently out-
performs Seq2Seq+Copy on most of the datasets,
i.e., more than 0.5 BLEU-4 improvements on 15
out of 21 datasets, and comparable on the remain-
ing 6 categories. To get a better understanding of
when and why external knowledge will be effec-
tive or not, we correlate the performance gains in
terms of BLEU-4 with KBGain metric described
in Section 3 across 21 datasets, and plot their val-
ues on the scatter plot in Fig. 4. The Pearson
correlation coefficient (ρ) between BLUE-4 im-
provements and KBGain is 0.716 which indicates
a strong positive correlation between them. This
confirms our analysis in Section 3 that KBAtt is
effective when the external knowledge is relevant
and the original data is sparse.

4.4.2 Results on WikiBio
To compare with state-of-the-art models, Table 3
shows the results on the WikiBio dataset. Among
them, our baseline Seq2Seq+Copy gains a perfor-
mance of 44.28 which is comparable to Sha et al.
(2017) and Liu et al. (2017) and significantly bet-
ter than Lebret et al. (2016) and Bao et al. (2018),

-0.5
0

0.5
1

1.5
2

2.5
3

3.5
4

4.5

0.00 0.50 1.00 1.50 2.00 2.50

BL
EU

-4
 Im

pr
ov

em
en

t

KBGain (%)

Figure 4: Strong correlation (ρ = 0.716) between KB-
Gain (x) and absolute BLEU-4 improvement (y).

and this baseline model is general without specific
design, e.g., order planning from Sha et al. (2017)
or field-gating encoder from Liu et al. (2017).
Our proposed model, namely KBAtt, obtains a
BLEU-4 score of 44.59. Although the absolute
improvement (+0.31 BLEU-4 points) of KBAtt
over Seq2Seq+Copy is relative small, the differ-
ence between them is statistically significant under
the one-tailed paired t-test at the 99% significance
level. The reason why the absolute improvement is
relative small is that the full WikiBio dataset con-
sists of 728,321 parallel data-to-text pairs which
are enough for neural models to memorize certain
inference patterns for high frequency pairs. How-
ever, as will be shown in the Section 4.5, the base-
line fails on low co-occurrence frequency pairs,
but the KBAtt avoids this problem with the con-
tributions from the external knowledge.

4.4.3 Human Evaluation
We conduct human evaluation to assess the fac-
tual accuracy of the generated sentences. Manu-
ally evaluating the generated results of all datasets
is labour intensive, so we choose to evaluate two
sample datasets for case study purposes. Specifi-
cally, we sample 50 instances each from the Wik-
iBio and the Album datasets, and ask two anno-
tators to extract facts tuples (subject, relation, ob-
ject) from the references and the generated sen-
tences7. In table 4, precision P1, recall R1 and
their overall score F1 measure the extent that the
facts extracted from the generated sentences con-
form to those from references. As we can see,
KBAtt achieves 1.21%, 7.42% improvements in
terms of F1 on WikiBio and Album respectively,
which shows that KBAtt can generate more rel-
evant facts with respect to the reference than the

7The generated sentences are shuffled to avoid model
preferences. The Cohen’s kappa between two annotators is
0.776.



3028

Model BLEU-4

Table NLM (Lebret et al., 2016) 34.70
Table2Seq (Bao et al., 2018) 40.26
Order Planning, full model (Sha et al., 2017) 43.91
Field-gating Seq2Seq, full model (Liu et al., 2017) 44.71

Seq2Seq+Copy 44.28
KBAtt 44.59

Table 3: BLEU-4 scores (%) on WikiBio dataset.

Dataset Model P1 R1 F1 P2
WikiBio Seq2Seq+Copy 69.07% 62.45% 65.59% 88.35%
WikiBio KBAtt 71.81% 62.45% 66.80% 91.85%

Album Seq2Seq+Copy 73.33% 71.84% 72.58% 84.38%
Album KBAtt 81.70% 78.37% 80.00% 90.64%

Table 4: Human based evaluation results.

Seq2Seq+Copy baseline. We further ask the an-
notators to judge whether the facts extracted from
the generated texts are correct against informa-
tion from the infobox, the external knowledge (eg.
Wikidata), or even search engines. In table 4,
P2 measures the ratio of correct facts in the gen-
erated results. We observe that KBAtt improves
3.50% and 6.26% over Seq2Seq+Copy in Wik-
iBio and Album respectively. This shows that
KBAtt is more likely to generate accurate facts
than Seq2Seq+Copy.

4.5 Analysis of Few-Shot Learning Ability

To examine the ability of learning writing knowl-
edge from few examples, we design an experiment
to compare the performance under different num-
ber of training samples for the baseline and our
model. In the training set of WikiBio, about 78.5%
tables contain place of birth field but only 19.0%
tables include the nationality field. However, in
the references, nationality of a person is frequently
mentioned. This means that the ability of inferring
nationality based on place of birth is important.
We collect the cases from the test data with the fol-
lowing conditions: (a) only city level information
is given in the field of place of birth; (b) nation-
ality is not specified in the table; (c) the reference
mentions country name or nationality. From these
cases, we get over 400 unique places of birth to
nationality inference pairs and split them into two
intervals [1, 25) and [25,∞) according to their co-
occurrence frequency in the training set8. For each
interval, we randomly sample 50 test cases and

8The interval threshold 25 is set by following that for KB-
Gain.

42.0

76.080.0 82.0

0.0
20.0
40.0
60.0
80.0

100.0

[1, 25) [25, ∞)

A
cc

ur
ac

y 
(%

)

Co-occurrence frequency

Seq2Seq+Copy KBAtt

Figure 5: Accuracy (%) of nationality information in
the generated sentences w.r.t different co-occurrence
frequency intervals.

manually assess the accuracy of nationality infor-
mation mentioned in the generated sentences9.

Fig. 5 shows the accuracy of nationality infor-
mation in the generated text with respect to differ-
ent co-occurrence frequency intervals: Firstly, we
found that the baseline model struggles to learn the
inference from place of birth to nationality when
their co-occurrence frequency in training set is less
than 25, which shows the difficulty of this task;
Secondly, in interval [1, 25), the baseline model
only gets 42.0% accuracy, but our model achieves
80.0% accuracy, a 38.0% absolute gain. This
confirms our motivation that incorporating exter-
nal knowledge into neural models can improve
model performances especially when the original
data is sparse; Finally, the accuracy of both mod-
els increases as the frequency goes from [1, 25) to
[25,∞), and the improvement of our model over
the baseline model gradually narrows, from 38.0%
to 6.0%. This shows that the baseline model could
learn part of inference patterns when enough par-
allel data is available.

4.6 Case Study

Fig. 6 shows three examples from the develop-
ment set of WikiBio dataset which can demon-
strate how KBAtt succeeds or fails. Fig. 6 (a) and
(b) illustrate KBAtt is helpful when original data
is sparse. However when original data is dense,
the baseline model still can learn it. As shown in
Fig. 6 (a), the baseline model struggles to learn di-
rect association between “german”and “gummers-
bach” (birth place in the table), since they only
co-occur 12 times in the training data. But it is
easy for KBAtt to learn since “german” co-occurs
with “germany” in KB 12,437 times in the training
data. As shown in Fig. 6 (b), although the national-

9In this experiment, a generated result is judged correct
only if it generates the same nationality information as the
reference.



3029

Article title dan van husen
Birth date 30 april 1945
Name dan van husen
Birth place gummersbach
Caption dan van husen
Years active 1968 -- present

Reference dan van husen ( born april 30 , 1945 ) is a german actor who has also performed in hollywood films .
Seq2Seq+Copy dan van husen ( born 30 april 1945 ) is a british actor , screenwriter and producer .
KBAtt dan van husen ( born 30 april 1945 in gummersbach ) is a former german actor .

Field Name Subject Relation Object
Birth place gummersbach country germany

Birth place gummersbach instance of district capital

Article title stephanie moorhouse
Birth date 20 january 1987
Birth place melbourne
Weight 47 kg lb
Discipline wag
Level senior international

Field Name Subject Relation Object
Birth place melbourne country australia

Birth place melbourne instance of city

Reference stephanie moorhouse ( born january 20 , 1987 ) is an australian former artistic gymnast .
Seq2Seq+Copy stephanie moorhouse ( born 20 january 1987 ) is an australian artistic gymnast .
KBAtt stephanie moorhouse ( born 20 january 1987 ) is an australian artistic gymnast .

(a)

(b)
Article title henry roberts ( 

architect )

Birth date 1803

Birth place philadelphia

Death date 1876

Death place florence

Occupation architect

Field Name Subject Relation Object
Birth place philadelphia country united states of america

Birth place philadelphia instance of city

Reference henry roberts ( 16 april 1803 -- 9 march 1876 ) was a british architect best known for fishmongers ' hall in london and for his work on model dwellings for workers .

Seq2Seq+Copy henry roberts ( 1803 -- 1876 ) was an english architect who designed many buildings in florence , florence , and florence .

KBAtt henry roberts ( 1803 -- 1876 ) was an american architect and architect .

(c)

Figure 6: Case study. Three generation examples from development set of WikiBio. Each example consists of the
input infobox, parts of external knowledge base, the reference and two generated sentences by Seq2Seq+Copy and
KBAtt. We mark correct fact information as blue and incorrect ones as red.

ity information is not explicitly stated in the table,
both Seq2Seq+Copy and KBAtt generate correct
nationality information (“australian”). The reason
is that “australian” co-occurs with “melbourne” in
the table 3,391 times in the training data. So, it
is easy for the neural model to learn such infer-
ence patterns. Fig. 6 (c) illustrates the pattern of
inferring nationality from birth place is not always
correct. In this case, KBAtt makes a plausible in-
ference on nationality from birth place, so it gen-
erates “american”. However, this inference pattern
doesn’t hold for this case, because he is British.
Hopefully, such cases are rare since the birth place
conforms to the nationality in most of cases, so
our methods can bring improvement as indicated
by the overall better performance across almost all
Wikipedia categories.

5 Related Work

Data-to-text generation is an important task in nat-
ural language generation which has been stud-
ied for decades (Kukich, 1983; Holmes-Higgin,
1994; Reiter and Dale, 1997). This task is broadly
divided into two subproblems: content selection
(Kukich, 1983; Reiter and Dale, 1997; Duboue
and McKeown, 2003; Barzilay and Lapata, 2005)
and surface realization (Goldberg et al., 1994; Re-

iter et al., 2005).
With the advent of neural text generation, the

distinction between content selection and sur-
face realization becomes blurred. For exam-
ple, Mei et al. (2016) proposed an end-to-end
encoder-aligner-decoder model to learn both con-
tent selection and surface realization jointly which
shows good results on WeatherGov and RoboCub
datasets. Wiseman et al. (2017) generate long
descriptive game summaries from a database of
basketball games where they show the current
state-of-the-art neural models are quite good at
generating fluent outputs, but perform poorly in
content selection and capturing long-term struc-
ture. Our work falls into the task of single sen-
tence generation from Wikipedia infoboxes. The
model structure ranges from feed-forward net-
works work (Lebret et al., 2016) to encoder-
decoder models (Sha et al., 2017; Liu et al.,
2017; Bao et al., 2018; Nema et al., 2018). Re-
cently, Perez-Beltrachini and Lapata (2018) gen-
eralize this task to multi-sentence text genera-
tion, where they focus on bootstrapping genera-
tors from loosely aligned data. However, most
of the work mentioned above assume all the writ-
ing knowledge can be learned from massive par-
allel pairs of training data. Different from the
previous work, we exploit incorporating external



3030

knowledge into this task to improve the fidelity of
generated text.

Our work is also relevant to recent works on in-
tegrating external knowledge into neural models
for other NLP tasks. The motivations of incor-
porating external knowledge range from enrich-
ing the context information (Mihaylov and Frank,
2018) in reading comprehension, improving the
inference ability of models (Chen et al., 2018) in
natural language inference, to providing the model
a knowledge source to copy from in language
modelling (Ahn et al., 2016). Our model, KBAtt,
is most relevant to Mihaylov and Frank (2018),
where they focus on similarity calculation but we
focus on generation in this paper. Moreover, in ad-
dition to demonstrating the positive effect of incor-
porating external knowledge as previous work, we
also design a new metric to quantify the potential
gains of external knowledge for a specific dataset
which can explain when and why our model is ef-
fective.

6 Conclusion

In this paper, we propose a neural data-to-text
generation model, KBAtt, that incorporates ex-
ternal background knowledge in a simple but ef-
fective way to improve fidelity of the generated
text. Experiments on 21 Wikipedia infobox-to-text
datasets show KBAtt consistently achieves better
performance in BLEU than a state-of-the-art base-
line on most of datasets. Meanwhile, to quantify
when and why external knowledge is effective, we
design a metric, KBGain, which shows a strong
correlation with observed performance boost. This
result indicates the relevance of external knowl-
edge and sparseness of original data are the main
factors affecting the effectiveness of KBAtt.

In the future, we plan to investigate integrat-
ing multi-hops knowledge graph behind the data
which has potential to further improve the infer-
ence ability of neural models. It will be worth-
while especially when we extend the task to multi-
ple sentence generation. The main challenge in in-
tegrating multi-hops knowledge graph is the large
search space. We plan to employ reinforcement
learning based techniques to allow the model to
search the optimal inference paths by trial and er-
ror. Besides, we are also interested in integrating
external knowledge into other types of datasets be-
yond Wikipedia infobox-to-text datasets.

7 Acknowledgement

We would like to thank the anonymous reviewers
for their helpful comments. The work of this pa-
per is funded by the project of National Key Re-
search and Development Program of China (No.
2018YFC0832105).

References
Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, and

Yoshua Bengio. 2016. A neural knowledge lan-
guage model. arXiv preprint arXiv:1608.00318.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. International Con-
ference on Learning Representations.

Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuan-
hua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-
to-text: Describing table region with natural lan-
guage. AAAI.

Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 331–338. ACL.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, and Diana
Inkpen. 2018. Natural language inference with ex-
ternal knowledge. Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing.

Pablo A Duboue and Kathleen R McKeown. 2003. Sta-
tistical acquisition of content selection rules for nat-
ural language generation. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing, pages 121–128. ACL.

Ondřej Dušek, Jekaterina Novikova, and Verena Rieser.
2018. Findings of the E2E NLG Challenge. In
Proceedings of the 11th International Conference on
Natural Language Generation, Tilburg, The Nether-
lands. ArXiv:1810.01170.

Eli Goldberg, Norbert Driedger, and Richard I Kit-
tredge. 1994. Using natural-language processing to
produce weather forecasts. IEEE Intelligent Sys-
tems, (2):45–53.

Paul Holmes-Higgin. 1994. Text generation: using dis-
course strategies and focus constraints to generate
natural language text. The Knowledge Engineering
Review, 9(4):421–422.

https://arxiv.org/abs/1810.01170


3031

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. International
Conference on Learning Representations.

Karen Kukich. 1983. Design of a knowledge-based re-
port generator. In Proceedings of the 21st annual
meeting on Association for Computational Linguis-
tics, pages 145–150. ACL.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing.

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,
and Zhifang Sui. 2017. Table-to-text generation by
structure-aware seq2seq learning. AAAI.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60, Bal-
timore, Maryland. Association for Computational
Linguistics.

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2016. What to talk about and how? selective gen-
eration using lstms with coarse-to-fine alignment.
NAACL.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: Enhancing cloze-style reading com-
prehension with external commonsense knowledge.
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics.

Preksha Nema, Shreyas Shetty, Parag Jain, Anirban
Laha, Karthik Sankaranarayanan, and Mitesh M
Khapra. 2018. Generating descriptions from struc-
tured data using a bifocal attention mechanism and
gated orthogonalization. NAACL.

Feng Nie, Jinpeng Wang, Jin-Ge Yao, Rong Pan, and
Chin-Yew Lin. 2018. Operations guided neural net-
works for high fidelity data-to-text generation. Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311–318. ACL.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In The 30th International Conference on
Machine Learning, pages 1310–1318.

Laura Perez-Beltrachini and Mirella Lapata. 2018.
Bootstrapping generators from noisy data. NAACL.

Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Natural Lan-
guage Engineering, 3(1):57–87.

Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167(1-2):137–169.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics.

Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian
Li, Baobao Chang, and Zhifang Sui. 2017. Order-
planning neural text generation from structured data.
AAAI.

Sam Wiseman, Stuart M Shieber, and Alexander M
Rush. 2017. Challenges in data-to-document gener-
ation. Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing.

A Supplemental Material

A.1 Dataset Collection
Our dataset is an extension of WikiBio dataset
(Lebret et al., 2016) where we extend previous
single Biography domain to other twenty do-
mains (e.g. Album, Book etc.) and augment
the infoboxes with external knowledge from Wiki-
data10. Our dataset consists of infoboxes as input
data and the first sentence of their corresponding
Wikipedia article as reference. The infoboxes are
parsed from English Wikipedia dump (Sep 2015)
while the reference sentences are tokenized with
Stanford CoreNLP (Manning et al., 2014). Differ-
ent from (Lebret et al., 2016), we keep the cap-
italization information which we believe is im-
portant for real-world application. To augment
the infoboxes with external knowledge, we pre-
serve the Wikipedia internal hyperlink informa-
tion in the field values of infoboxes, and track
these hyperlinks to get their corresponding enti-
ties from Wikidata (dumps version: 20150831).
Finally we retrieve one-hop facts from Wikidata.
The dataset is aviliable at https://github.
com/hitercs/WikiInfo2Text.

A.2 Full Dataset Statistics
Table 5 summarizes the full dataset statistics about
the twenty-one datasets used in this paper. As
you can see, WikiBio consists of 728,321 in-
stances which is the largest infobox-to-text dataset

10https://wikidata.org

https://doi.org/10.3115/v1/P14-5010
https://doi.org/10.3115/v1/P14-5010
https://github.com/hitercs/WikiInfo2Text
https://github.com/hitercs/WikiInfo2Text
https://wikidata.org


3032

Dataset Avg. #
tokens per
sentence

Avg. #
tokens per

table

Avg. # fields
per table

Avg. # table
tokens per

sent.

Avg. #
entities per

table

Avg. # fact
tuples per

entity

Dataset
size

WikiBio 26.1 53.0 19.7 11.6 5.9 19.0 728321
Settlement 22.1 57.3 50.6 10.3 6.6 14.4 327863
Album 21.0 57.1 15.9 9.7 5.5 7.2 76105
Single 24.3 70.5 19.2 12.6 7.3 7.5 36937
NRHP 23.0 54.4 22.7 10.8 1.4 10.9 35945
French commune 14.7 31.1 20.3 3.7 0.8 126.1 29218
Book 24.3 41.7 20.1 11.1 4.2 16.7 23859
School 24.2 62.6 44.3 11.2 3.7 29.8 18751
Station 19.9 51.8 32.6 9.0 3.2 18.0 16625
Video game 25.0 42.8 13.2 9.4 6.3 6.4 16381
UK place 20.5 30.9 19.4 7.0 3.0 5.2 12279
Military unit 25.4 62.5 27.9 11.9 5.2 18.9 11273
Airport 22.9 74.4 32.9 10.7 3.0 15.0 10934
Building 24.1 47.6 35.8 10.4 2.9 33.6 10052
Military conflict 34.7 96.6 18.0 18.6 8.1 17.3 9592
NCAA team season 24.2 45.1 23.4 12.7 2.7 5.5 7766
Television episode 26.4 78.8 15.9 12.4 10.1 8.0 5930
UK school 23.8 51.6 48.0 10.1 4.1 10.0 4974
Australian place 21.3 43.8 29.1 8.2 5.6 6.5 4972
Automobile 23.3 70.9 17.2 7.8 5.7 18.5 4781
Football club season 24.6 82.8 30.0 9.3 8.8 6.9 4559

Table 5: Dataset statistics for all datasets used in this paper.

in Wikipedia. However, the size of remaining
datasets ranges from 4,559 to 327,863 and more
than 60% of them have less than 20,000 instances.
So, the abundance of data such as WikiBio is
not common among the Wikipedia infobox-to-text
datasets. Meanwhile, these datasets vary in sen-
tence length and length of tokens in table. In ad-
dition, the number of entities extracted from each
table ranges from 0.8 to 10.1 and the average num-
ber of fact tuples per entity ranges from 5.2 to
126.1.

A.3 Threshold Tuning for KBGain metric

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0 20 40 60 80 100

Pe
ar

so
n 

Co
rre

la
tio

n 
Co

ef
fic

ie
nt

Co-occurrence frequency threshold

Figure 7: Pearson correlation coefficient w.r.t Co-
occurrence frequency threshold.

The threshold γ in KBGain metric denotes
the minimum size of co-occurrence above which

learning will be effective. We tune this threshold
by finding the maximum pearson correlation coef-
ficient (ρ) between BLEU-4 improvements and the
values of KBGain on the 21 development datasets.
Fig. 7 shows the curve of pearson correlation co-
efficient w.r.t co-occurrence frequency threshold
ranging from 5 to 100. As we can see, ρ reaches
peak value as 0.753 when the threshold is 25. So
γ is set 25 throughout this paper.


