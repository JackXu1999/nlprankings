



















































Exploring Numeracy in Word Embeddings


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374–3380
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3374

Exploring Numeracy in Word Embeddings

Aakanksha Naik∗, Abhilasha Ravichander∗,
Carolyn Rose, Eduard Hovy

Language Technologies Institute, Carnegie Mellon University
{anaik, aravicha, cprose, ehovy}@cs.cmu.edu

Abstract

Word embeddings are now pervasive across
NLP subfields as the de-facto method of form-
ing text representataions. In this work, we
show that existing embedding models are in-
adequate at constructing representations that
capture salient aspects of mathematical mean-
ing for numbers, which is important for lan-
guage understanding. Numbers are ubiquitous
and frequently appear in text. Inspired by cog-
nitive studies on how humans perceive num-
bers, we develop an analysis framework to test
how well word embeddings capture two es-
sential properties of numbers: magnitude (e.g.
3<4) and numeration (e.g. 3=three). Our ex-
periments reveal that most models capture an
approximate notion of magnitude, but are in-
adequate at capturing numeration. We hope
that our observations provide a starting point
for the development of methods which better
capture numeracy in NLP systems.

1 Introduction

Word embeddings operationalize the distributional
hypothesis, where a word is characterized by “the
company it keeps” (Harris, 1954; Firth, 1957), and
have been shown to capture semantic regularities
in vector space (Mikolov et al., 2013c). They have
been a driving force in NLP in recent years, and
enjoy widespread use in a variety of semantic tasks
(Rumelhart et al.; Mikolov et al., 2013a,b; Col-
lobert and Weston, 2008; Glorot et al., 2011; Tur-
ney and Pantel, 2010; Turney, 2013).

However, to what extent do these word repre-
sentations capture numeric properties? Numbers
often need to be dealt with precisely, and under-
standing the meaning of text also requires a care-
ful understanding of the quantities involved. They
have been identified to play an important role in
textual entailment, a benchmark natural language

∗*The first two authors contributed equally to this work.

understanding task. Marneffe et al. (2008) ex-
tract pairs of contradictions that occur naturally
on Wikipedia and Google News, and find that
as many as 29% of contradictions arise due to
numeric discrepancies. They also identify that
on many Recognizing Textual Entailment (RTE)
datasets, 8.8% of contradictory pairs feature nu-
meric contradictions. Naik et al. (2018) find that
model inability to do numerical reasoning causes
4% of errors made by state-of-the-art models in
Natural Language Inference. Spithourakis and
Riedel (2018) emphasize the importance of nu-
meracy in language modeling. Yet, numbers are
often forgotten and even masked in NLP applica-
tions (Mitchell and Lapata, 2009).

In several domains such as economics, finance
and scientific articles numbers can play a crucial
role in text. Take for example a recent news head-
line,

Met Office: Global Warming could exceed
1.5 C within five years

Ideally, the text representation we use should be
able to capture that global warming can exceed 1.5
C, not 100 C. Magnitude is an essential aspect
of a number’s meaning1 (Dehaene et al., 1998;
Whalen et al., 1999; Cantlon and Brannon, 2006;
Gross, 2011; Cutini and Bonato, 2012; Agrillo
et al., 2012; Feigenson et al., 2004) . Systems
should also be able to draw valid inferences ir-
respective of whether the text uses “five” or “5”.
This requires an understanding of symbolic repre-
sentations used to record numbers in text. Such
representation systems are called numeration sys-
tems, and individual symbols within the system

1Prior work has shown that humans, as well as sev-
eral species of animals share analogue systems that rep-
resent “quantities” or “magnitudes” associated with num-
bers(Dehaene et al., 1998)



3375

are called numerals2. Systems must handle nu-
meration, i.e. associations between distinct sym-
bols used for the same number under different sys-
tems (3=three).

In this work, we examine the extent to which
word embeddings are capable of representing nu-
meracy attributes, asking the question - if pre-
trained word embeddings are utilized for repre-
senting text across NLP tasks, what can they rep-
resent about numbers? Our framework formulates
triples of numbers to probe word embeddings on
their ability to represent magnitude, and their ro-
bustness to differences in numeration. We hope
this analysis highlights limitations of current pre-
trained word embeddings at capturing numeracy,
and will motivate future research to develop more
careful treatments of quantities in text.

2 Analysis Framework

We construct an analysis framework to evaluate
embeddings on their ability to capture magnitude
and numeration. Numbers follow a well-defined
ordering, under a mathematical system, which
holds independent of textual context (e.g.: 0 <
1 < 2...). This ordering is established by magni-
tude (Izard and Dehaene, 2008; Russell, 2009) and
is consistent across numeration systems. There-
fore, an embedding representation that captures
magnitude and numeration precisely should main-
tain this ordering across numeration systems in the
embedding space. We evaluate this ability by con-
structing contrastive tests (Zhu et al., 2018).

A contrastive test for a property p is defined as a
triple (x, x+, x−) such that x is closer to x+ than
x− under p. If embeddings capture p, x will be
closer to x+ than x− in the embedding space, indi-
cating that the embedding method passes the test.
We propose three categories of tests, which differ
in the choice of x−3:

1. OVA (One-vs-All): Define x− = {y|y ∈
X − x, y 6= x+}. A model must identify x to
be closer to x+ than all x−.

2. SC(Strict Contrastive): Choose x− to be
the second-closest to x after x+ under p.

3. BC (Broad Contrastive): Choose x− to be
the furthest from x under property p.

2Several cultures have developed numeration systems
(Zhang and Norman, 1995). In this work, we restrict
our scope to Arabic and English numeration systems (e.g.
Arabic-2, English: two).

3x+ is chosen to be the token closest to x under p.

Model #English #Arabic
GloVe-6B-*D 120 (0.03%) 19409 (4.85%)

GloVe-42B-300D 239 (0.01%) 108839 (5.68%)
GloVe-840B-300D 532 (0.02%) 109353 (4.98%)

FastText-Wiki 374 (0.04%) 25549 (2.56%)
FastText-CC 592 (0.03%) 59386 (2.97%)

SkipGram-BoW 114 (0.06%) 2401 (1.31%)
SkipGram-Dep 111 (0.06%) 2416 (1.39%)

GloVe-Num 1117 (0.02%) 318109 (4.4%)
GloVe-All 973 (0.01%) 189598 (2.8%)

FastText-Num 1117 (0.02%) 317627 (4.4%)
FastText-All 973 (0.01%) 189366 (2.8%)

Word2Vec-Num 486 (0.02%) 67908 (2.7%)
Word2Vec-All 434 (0.01%) 37164 (1.2%)

Table 1: Proportion of English and Arabic numerals
containing representations in different models. Though
embeddings are retrained on the same corpus, pre-
processing choices (eg:lowercasing, filtering low fre-
quency words etc.) result in different vocabularies

OVA requires that x+ must be the closest vector
to x in the embedding space. High performance
on this test would indicate that the property is cap-
tured almost precisely. SC relaxes strictness by
only requiring x+ to be closer than the second-
closest token under property p. Finally BC is the
least strict of the three. Models can succeed on BC
if they manage to capture even an approximate no-
tion of p. We use this framework to construct three
categories of contrastive tests for both magnitude
and numeration. Example tests for magnitude are
shown below4:

1. OVA-MAG: (3, 4, x), such that x = {y|y ∈
X − {3}, y 6= 4}

2. SC-MAG: (3, 4, 5)
3. BC-MAG: (3, 4, 1000000)

Similarly for numeration,

1. OVA-NUM: (3, three, x), such that x =
{y|y ∈ Y, y 6= three}

2. SC-NUM: (3, three, four)
3. BC-NUM: (3, three, billion)

3 Representations

We evaluate the following embedding methods:

Skipgram (Mikolov et al., 2013a): Feedfor-
ward network trained to predict words within
a fixed window surrounding the current word,
with hidden weights used as embeddings. We
evaluate with window sizes in {2, 5}, dependency

4Note that we consider 2 and 4 equidistant from 3, so ex-
amples like (3,2,4) are removed.



3376

Model OVA-MAG SC-MAG BC-MAG
Random 0.04 49.82 49.34

GloVe-6B-50D 7.70 55.62 82.48
GloVe-6B-100D 10.27 57.83 82.83
GloVe-6B-200D 15.88 62.21 83.94
GloVe-6B-300D 18.41 62.92 83.98
GloVe-42B-300D 5.18 55.58 91.86
GloVe-840B-300D 11.06 55.40 88.54

FastText-Wiki 13.94 59.96 96.15
FastText-CC 7.83 53.89 85.40
SkipGram-2 7.12 55.49 95.84
SkipGram-5 8.85 55.40 96.42

SkipGram-Dep 3.32 51.99 94.60

Table 2: Performance (% accuracy) of various embed-
ding models on magnitude tests. We also report the
performance of a random embedding baseline.

parse-based context (Levy and Goldberg, 2014)

GloVe (Pennington et al., 2014): Embeddings
generated by training log-bilinear models to
predict global word co-occurrence statistics.
We evaluate variants with #tokens in {6B, 42B,
840B}; dimensionality in {50, 100, 200, 300}

FastText (Bojanowski et al., 2017): Extended
Skipgram model representing words as character
n-grams to incorporate sub-word information. We
evaluate Wikipedia and Common Crawl variants.

3.1 Retrained Word Vectors

We retrain all models on GigaWord5 and English
Wikipedia6, under the setting: window size=5; di-
mensionality=100. To evaluate whether having
more occurrences of numerals in the training data
correlates with better representations, we train two
variants for each model: one on sentences contain-
ing numbers (56M in total; 1.5B tokens) (Num),
and another on 56M sentences (1.5B tokens) sub-
sampled without constraints (All).

4 Experiments

How many numerals have representations? Table
1 shows the proportion of English7 and Arabic
numerals in each. Overall, numerals make up
less than 5% vocabulary in all models. Despite

5We use the fourth edition: https://catalog.ldc.
upenn.edu/LDC2009T13.

6We use the May 1, 2019 dump from https://
dumps.wikimedia.org/backup-index.html.

7To detect English numerals, we use word2number:
https://pypi.org/project/word2number/.

this, all variants contain representations for suffi-
cient numerals to allow us to apply our framework.

For off-the-shelf variants, we construct 2260
OVA-MAG, SC-MAG and BC-MAG tests. For
numeration, we construct separate tests for each
model, as there are few common numerals. Fur-
ther statistics about number of tests for each model
are reported in table 3. For retrained embeddings,
we construct 31860 OVA-MAG, SC-MAG and
BC-MAG tests, 130 OVA-NUM and SC-NUM
tests, and 136 BC-NUM tests8.

4.1 Evaluating Off-The-Shelf Embeddings

Tables 2 and 3 present the performance of off-
the-shelf embeddings on magnitude and numera-
tion tests respectively. We use cosine similarity9

as the distance metric. High performance on BC-
MAG indicates that all models capture an approxi-
mate notion of magnitude, distinguishing between
very large and very small numbers. We specu-
late this might be because numbers from different
magnitude classes often appear in different con-
texts (See §5.1). As tests become stricter, model
performance drops massively. Models perform
nearly 10x worse on OVA-MAG as compared to
BC-MAG. This suggests model are unable to cap-
ture magnitude precisely. Across models, Skip-
Gram variants and FastText-Wiki perform best on
BC-MAG. However, GloVe outperforms all oth-
ers on OVA-MAG and SC-MAG. On numeration
tests, models fare much worse. With the exception
of GloVe models on BC-NUM, no model signifi-
cantly outperforms a random baseline.

4.2 Evaluating Retrained Embeddings

Table 4 presents the performance of retrained em-
beddings and a random embedding baseline on
magnitude and numeration tests. There is no sig-
nificant difference in performance between Num
and All variants, suggesting that seeing more nu-
merals during training does not necessarily result
in better representations. Results follow similar
trends as off-the-shelf embeddings. All models
capture an approximate notion of magnitude (high
performance on BC-MAG), but do not capture nu-
meration. Across models, FastText variants fare

8Since all embeddings are trained on the same corpus and
share the same vocabulary, there are enough common English
numerals to construct a single set of numeration tests.

9We experiment with Euclidean distance, and observe
similar results (Appendix A and B).

https://catalog.ldc.upenn.edu/LDC2009T13
https://catalog.ldc.upenn.edu/LDC2009T13
https://dumps.wikimedia.org/backup-index.html
https://dumps.wikimedia.org/backup-index.html
https://pypi.org/project/word2number/


3377

OVA-NUM SC-NUM BC-NUM
Model #Tests Rand Emb #Tests Rand Emb #Tests Rand Emb

GloVe-6B-50D 117 0.00 0.85 117 49.57 52.99 117 50.43 79.49
GloVe-6B-100D 117 0.00 0.85 117 52.99 47.86 117 57.26 81.20
GloVe-6B-200D 117 1.71 0.85 117 48.72 57.26 117 42.74 78.63
GloVe-6B-300D 117 1.71 0.00 117 50.43 58.97 117 54.70 88.89
GloVe-42B-300D 226 0.44 0.44 226 52.21 51.33 226 53.98 10.18

GloVe-840B-300D 515 0.19 0.19 515 49.90 50.68 515 49.71 81.94
FastText-Wiki 360 0.28 0.28 360 50.00 49.72 360 56.67 41.67
FastText-CC 572 0.00 0.52 572 46.85 51.22 572 41.26 44.76
SkipGram-2 112 0.00 0.00 112 51.79 48.21 112 49.11 49.11
SkipGram-5 112 0.00 0.89 112 52.68 51.79 112 50.89 14.29

SkipGram-Dep 109 0.92 1.83 109 53.21 48.62 109 52.29 31.19

Table 3: Performance (% accuracy) of various embedding models on numeration tests. Since we construct a
separate set of tests per model, we report the performance of a random embedding model for each set (Rand).
Bolded numbers highlight cases where performance is higher than both random embedding and random choice.
Note that random choice performance for OVA-NUM is 1#Tests .

Model Magnitude Numeration
OVA-MAG SC-MAG BC-MAG OVA-NUM SC-NUM BC-NUM

random 0.00 49.62 49.71 2.31 47.69 53.68
GloVe-Num 0.01 49.47 72.76 0.00 50.00 19.85
GloVe-All 0.01 49.08 74.02 0.00 46.15 19.85

FastText-Num 0.09 51.05 96.69 1.54 54.62 58.09
FastText-All 0.09 51.16 97.90 0.00 46.92 61.03

Word2Vec-Num 0.02 50.12 93.55 0.77 44.62 33.82
Word2Vec-All 0.02 49.37 94.20 0.00 54.62 34.56

Table 4: Performance (% accuracy) of various (retrained) embedding models on magnitude and numeration tests.

best.

5 Discussion

5.1 Performance on Magnitude Tests
Tables 2 and 4 show that most models do not
capture magnitude precisely (low performance on
OVA-MAG; SC-MAG), but seem to learn an ap-
proximate notion of magnitude (high performance
on BC-MAG) 10. To examine the difference in
contexts that separates numbers of vastly varying
magnitudes, we sample 1 million sentences con-
taining numbers from English Wikipedia and Gi-
gaWord and compute pointwise mutual informa-
tion (PMI), defined as
PMI (number, class) = log p(number,class)

p(number,·)p(·,class)
10Cognitive studies show that human babies initially start

recognizing numbers by approximation and their ability to
identify numbers precisely improves over their lifespan (Hal-
berda et al., 2012). (Moyer and Landauer, 1967) were the first
to observe that humans took longer to distinguish between
closer numbers (eg: 8 and 9) than numbers which were fur-
ther away in distance (eg: 2 and 9). This finding has since
been replicated several times (Dehaene, 2011). In our frame-
work, models find it harder to distinguish between closer
numbers (SC-MAG) than distant numbers (BC-MAG)- how-
ever the differences here likely arise from different contexts
in which numbers of vastly varying magnitudes are used.

between the contexts of primitive numbers
(numbers 1-10) and large numbers (>500,>1000,
>3000, >10000, >100000) as shown in Table 5.
We consider the word immediately following the
number as context, since it appears in the context
of the number across embedding methods, regard-
less of sliding window size. We apply add-100
smoothing to identify contexts with maximum dis-
criminatory power.

We observe in table 5 that terms separating
primitives from larger numbers fall into categories
such as days in a month, which are less than 31,
or percentages which are <= 100. In compari-
son, contexts of larger numbers include terms like
‘election’, ‘census’ and ‘world’. As we move be-
yond numbers that are likely to be dates (>3000),
we observe terms such as ’ZIP’ occurring with ZIP
codes in text, ‘block’ occurring in contexts such
as ‘house in 9600 block of Washington Boulevard,
‘Refugees’ which appears in contexts such as ‘re-
locate about 125,000 refugees away from the bor-
der’. We observe that different contexts character-
ize classes of numbers, and speculate that this may
allow embeddings to distinguish between numbers
that appear consistently in vastly different contexts



3378

Primitives λ = 500 Primitives λ = 1000 Primitives λ = 3000 Primitives λ = 10000 Primitives λ = 100000

Wiki

% Summer % Summer % BC % Exchange % Elected
July Census million Census million RPM million HD million Ontario
January Film July Film July BCE July Departs May Owner
April World January World January Inhabitants January Delhi July Spinneys
September Election April Election May Hollywood May Raxaul January Thana

GW

percent index percent World percent DOWN percent novos percent novos
p.m GMT million GMT million Composite million ZIP million Tel
a.m World billion Olympic billion block billion University billion NDI
trillion Olympic p.m Olympics p.m LAS points UP points Refugees
billion Olympics years season years UP p.m Old p.m Eritrean

Table 5: Top 5 nouns by PMI(word, class) for primives and large numbers (numbers > λ), in 1 million sentences
drawn from Wikipedia (wiki) and GigaWord (GW) respectively.

leading to good performance on BC-MAG.

5.2 Recovering magnitude information from
nearest neighbours

Model performance on SC-MAG and BC-MAG
indicates whether ordering relationships between
a number,its closest, second-closest, and furthest
numbers are maintained. However, infinite num-
bers exist, making it infeasible to construct con-
trastive tests to check ordering relationships be-
tween all triples. To mitigate this, we experiment
with a paradigm that performs regression with a
number’s nearest neighbors to predict its magni-
tude. If magnitude can be recovered from the
structure of the embedding space, this provides ev-
idence that magnitude ordering relations are main-
tained to some extent. For this experiment, we di-
vide the set of 2260 numbers common across off-
the-shelf variants11 into training (80%) and test
(20%) sets and run a kNN (k-nearest neighbor)
regressor model to predict magnitude. R2 scores
for are shown in table 6. Most models show rea-
sonably high R2 scores, indicating that some or-
dering relationships must be maintained, helping
embeddings capture approximate notions of mag-
nitude. While this property of current embedding
models is interesting, their failure to capture pre-
cise magnitude is an important issue. Word em-
beddings are used for semantic tasks such as natu-
ral language inference or reading comprehension,
wherein models might need to reason more pre-
cisely about numbers.

6 Conclusion

Current NLP systems rely heavily on word em-
beddings. In this work we demonstrate that three

11We do this to compare results across all models. Re-
trained variants contain embeddings for all 2260 numbers.

Model R2 Score
GloVe-6B-50D 0.53
GloVe-6B-100D 0.75
GloVe-6B-200D 0.67
GloVe-6B-300D 0.62

GloVe-42B-300D 0.44
GloVe-840B-300D 0.83

FastText-Wiki 0.71
FastText-CC 0.56
SkipGram-2 0.67
SkipGram-5 0.76

SkipGram-Dep 0.41
GloVe-Num 0.12
GloVe-All 0.30

FastText-Num 0.73
FastText-All 0.47

Word2Vec-Num 0.68
Word2Vec-All 0.65

Table 6: Results of kNN Regression Test for Magnitude

popular embedding models are inadequate at deal-
ing precisely with numbers, in two aspects: mag-
nitude and numeration. We hope this work will
promote a more careful treatment of language, and
serve a cautionary purpose against using word em-
beddings in downstream tasks without recognizing
their limitations. This work also raises important
questions about other categories of word-like to-
kens that need to be treated like special cases. We
hope the community will carefully consider that
distributed word representations cannot be relied
upon in all scenarios.

7 Acknowledgements

This work has partially been supported by the
National Science Foundation under Grant No.
CNS 13-30596. The authors would like to thank
Thomas Manzini, Shruti Rijhwani and Siddharth
Dalmia for helpful discussions and reviews while
drafting this paper.



3379

References
Christian Agrillo, Laura Piffer, Angelo Bisazza, and

Brian Butterworth. 2012. Evidence for two numeri-
cal systems that are similar in humans and guppies.
PloS one, 7(2):e31923.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Jessica F Cantlon and Elizabeth M Brannon. 2006.
Shared system for ordering small and large num-
bers in monkeys and humans. Psychological sci-
ence, 17(5):401–406.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Simone Cutini and Mario Bonato. 2012. Subitizing and
visual short-term memory in human and non-human
species: a common shared system? Number without
language: comparative psychology and the evolu-
tion of numerical cognition, 129.

Stanislas Dehaene. 2011. The number sense: How the
mind creates mathematics. OUP USA.

Stanislas Dehaene, Ghislaine Dehaene-Lambertz, and
Laurent Cohen. 1998. Abstract representations of
numbers in the animal and human brain. Trends in
neurosciences, 21(8):355–361.

Lisa Feigenson, Stanislas Dehaene, and Elizabeth
Spelke. 2004. Core systems of number. Trends in
cognitive sciences, 8(7):307–314.

J. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis. Philological
Society, Oxford. Reprinted in Palmer, F. (ed. 1968)
Selected Papers of J. R. Firth, Longman, Harlow.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11), pages 513–520.

Hans J Gross. 2011. To bee or not to bee, this is the
question the inborn numerical competence of hu-
mans and honeybees: The inborn numerical com-
petence of humans and honeybees. Communicative
& integrative biology, 4(5):594–597.

Justin Halberda, Ryan Ly, Jeremy B Wilmer, Daniel Q
Naiman, and Laura Germine. 2012. Number sense
across the lifespan as revealed by a massive internet-
based sample. Proceedings of the National Academy
of Sciences, 109(28):11116–11120.

Zellig S Harris. 1954. Distributional structure. Word,
10(2-3):146–162.

Véronique Izard and Stanislas Dehaene. 2008. Cal-
ibrating the mental number line. Cognition,
106(3):1221–1247.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 302–308, Baltimore, Maryland. Association
for Computational Linguistics.

Marie-Catherine Marneffe, Anna N Rafferty, and
Christopher D Manning. 2008. Finding contradic-
tions in text. Proceedings of ACL-08: HLT, pages
1039–1047.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia. Association for Computational Linguistics.

Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 1-Volume
1, pages 430–439. Association for Computational
Linguistics.

Robert S Moyer and Thomas K Landauer. 1967. Time
required for judgements of numerical inequality.
Nature, 215(5109):1519.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2340–2353,
Santa Fe, New Mexico, USA. Association for Com-
putational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

David E Rumelhart, Geoffrey E Hinton, Ronald J
Williams, et al. Learning representations by back-
propagating errors. Cognitive modeling, 5(3):1.

http://www.aclweb.org/anthology/P14-2050
http://www.aclweb.org/anthology/P14-2050
http://www.aclweb.org/anthology/N13-1090
http://www.aclweb.org/anthology/N13-1090
http://www.aclweb.org/anthology/C18-1198
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162


3380

Bertrand Russell. 2009. Principles of mathematics.
Routledge.

Georgios P Spithourakis and Sebastian Riedel. 2018.
Numeracy for language models: Evaluating and im-
proving their ability to predict numbers. arXiv
preprint arXiv:1805.08154.

Peter D Turney. 2013. Distributional semantics beyond
words: Supervised learning of analogy and para-
phrase. Transactions of the Association for Com-
putational Linguistics, 1:353–366.

Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37:141–188.

John Whalen, Charles R Gallistel, and Rochel Gelman.
1999. Nonverbal counting in humans: The psy-
chophysics of number representation. Psychologi-
cal Science, 10(2):130–137.

Jiajie Zhang and Donald A Norman. 1995. A represen-
tational analysis of numeration systems. Cognition,
57(3):271–295.

Xunjie Zhu, Tingfeng Li, and Gerard de Melo. 2018.
Exploring semantic properties of sentence embed-
dings. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 632–637, Melbourne,
Australia. Association for Computational Linguis-
tics.

A Magnitude Tests with Euclidean
Distance

Table 7 describes the performance of word embed-
ding models on magnitude tests with Euclidean
distance.

Model OVA-MAG SC-MAG BC-MAG
Random 0.04 49.82 49.34

GloVe-6B-50D 7.7 54.87 79.78
GloVe-6B-100D 10.27 57.12 78.5
GloVe-6B-200D 15.88 58.72 80.09
GloVe-6B-300D 18.41 60.44 79.82

GloVe-42B-300D 5.18 55.27 55.09
GloVe-840B-300D 11.06 55.49 98.23

SkipGram-2 8.85 55.35 96.37
SkipGram-5 7.12 55.44 95.8

SkipGram-Dep 3.32 51.95 94.56
FastText-CC 7.83 54.07 91.28

FastText-Wiki 13.94 59.34 98.19

Table 7: Performance (% accuracy) of embedding
models on magnitude tests with Euclidean distance

B Numeration Tests with Euclidean
Distance

Tables 8 and 9 describe the performance of word
embedding models on numeration tests with Eu-
clidean distance.

SC-NUM
Model #Tests Rand Emb

GloVe-6B-50D 117 49.57 52.14
GloVe-6B-100D 117 52.99 51.28
GloVe-6B-200D 117 48.72 52.65
GloVe-6B-300D 117 50.43 56.89
GloVe-42B-300D 226 52.21 52.65

GloVe-840B-300D 515 49.90 56.89
FastText-Wiki 360 50.00 49.72
FastText-CC 572 46.85 49.72
SkipGram-2 112 51.79 48.21
SkipGram-5 112 52.68 51.79

SkipGram-Dep 109 53.21 48.62

Table 8: Performance (% accuracy) of embedding
models on SC-NUM

BC-NUM
Model #Tests Rand Emb

GloVe-6B-50D 117 50.43 99.15
GloVe-6B-100D 117 57.26 100.0
GloVe-6B-200D 117 42.74 2.21
GloVe-6B-300D 117 54.70 87.57
GloVe-42B-300D 226 53.98 2.21

GloVe-840B-300D 515 49.71 87.57
FastText-Wiki 360 56.67 98.89
FastText-CC 572 41.26 98.89
SkipGram-2 112 49.11 49.11
SkipGram-5 112 50.89 14.29

SkipGram-Dep 109 52.29 31.19

Table 9: Performance (% accuracy) of embedding
models on BC-NUM

https://www.aclweb.org/anthology/P18-2100
https://www.aclweb.org/anthology/P18-2100

