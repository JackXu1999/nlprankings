



















































OCLSP at SemEval-2016 Task 9: Multilayered LSTM as a Neural Semantic Dependency Parser


Proceedings of SemEval-2016, pages 1212–1217,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

OCLSP at SemEval-2016 Task 9: Multilayered LSTM as a Neural
Semantic Dependency Parser

Lifeng Jin Manjuan Duan William Schuler
Department of Linguistics
The Ohio State University

{jin,duan,schuler}@ling.osu.edu

Abstract

Semantic dependency parsing aims at ex-
tracting arcs and semantic role labels for
all words in a sentence. In this paper,
we propose a semantic dependency parser
which is based on Long Short-term Mem-
ory and makes heavy use of embeddings
of words and POS tags. We describe in
detail the implementation of the neural
parser, including preprocessing, postpro-
cessing and various input features, and
show that the neural parser performs close
to the top system in the shared task and
is very good at capturing non-local depen-
dencies. We also discuss some issues re-
lated to the parser and how to improve it.

1 Introduction
Semantic dependency parsing is a form of se-
mantic analysis where semantic roles for all
words in a sentence are analyzed and specific se-
mantic relations are assigned to each word pair
(Che et al., 2012). Chinese semantic depen-
dency parsing is especially of interest as there is
a remarkable difference between syntactic and
semantic dependencies in Chinese (Che et al.,
2012). In the SemEval 2016 Task 9 Chinese
Semantic Dependency Parsing shared task, se-
mantic dependency parsing for two text genres,
TEXT, which includes sentences from conversa-
tions and primary school textbooks, and NEWS,
which contains newswire text, is explored. We
propose a neural parser with LSTM as the basic
units and make heavy use of vectorial represen-
tations of basic linguistic units like words and

POS tags. In this paper, we give a detailed de-
scription of the neural parser and discuss a few
issues related to the current implementation.

2 Long Short-term Memory
Recurrent Neural Networks, or RNNs, are good
for sequential prediction, but the problem of
exploding or vanishing gradients makes learn-
ing long distance dependencies very difficult
for them (Hochreiter, 1998). The LSTM ar-
chitecture is proposed to address this problem
(Hochreiter and Schmidhuber, 1997). In this pa-
per, we follow the version defined in Graves et
al. (2013b).

it = σ(Wxixt + Whiht−1 + Wcict−1 + bi) (1)
ft = σ(Wxfxt + Whfht−1 + Wcfct−1 + bf ) (2)
ct = ftct−1 + it tanh(Wxcxt + Whcht−1 + bc)

(3)
ot = σ(Wxoxt + Whoht−1 + Wcoct−1 + bo) (4)
ht = ot tanh(ct) (5)

where σ is the sigmoid function, and i, f , o and
c are the input gate, forget gate, output gate
and the cell respectively. xt is the input at time
step t, and ht is the hidden state or the out-
put of the LSTM unit at time step t. With
various gates and a cell, the LSTM unit may
learn to store and release information inside the
cell over many time steps. LSTMs have recently
been used for various NLP tasks such as machine
translation (Bahdanau et al., 2014; Sutskever
et al., 2014), syntactic parsing (Vinyals et al.,
2015) and semantic relatedness prediction and

1212



sentiment classification (Tai et al., 2015). In
this paper we apply the LSTM architecture to
semantic dependency parsing and show that to
use LSTM as a semantic dependency parser is
both very powerful and very promising.

3 Overview of the System

3.1 Parsing Typology
For a sentence S = ⟨w1, w2, ...wi, ...wn⟩, the pro-
posed parser traverses the whole parse chart lin-
early through all possible pairwise combinations
of the elements in the sequence to predict the
dependency labels. For example, the parser will
first consider the word pair ⟨w1, w1⟩ for depen-
dency label prediction, treating the first in the
pair to be the dependent and the second the
head in a dependency relation, and then the
word pair ⟨w1, w2⟩, and so on. When it gets to
⟨wi, wn⟩, it proceeds to parse ⟨wi+1, w1⟩, until
the final pair ⟨wn, wn⟩ is parsed. For cases like
⟨wi, wi⟩, because there is no dependency arc that
connects the same word in the same position,
we use this for predicting the root of the sen-
tence. Therefore the actual input to the parser
is quadratic on the length of the input sequence,
making the complexity of the parser O(n2).

3.2 Architecture of the Neural Parser
The neural parser is shown in figure 1. It con-
sists of two main parts: the lower level which
has a bidirectional LSTM as its main compo-
nent and the upper level which has an LSTM
and an output layer as its main component.

Lower Level: The main component of the
lower level system is a bidirectional LSTM
(Graves et al., 2013a). The bidirectional LSTM
takes the same input sequence with n time-steps,
and runs both forward and backward in time to
generate hidden states for each time-step, and
concatenates the two hidden states for a single
time step to form a single hidden state of the
whole system. For example, for a time sequence
X = ⟨x1, x2, ...xt, ...xn⟩, the input to the bidi-
rectional LSTM is xt, and the output from it
is the concatenation of the hidden state from
the forward LSTM ht and the hidden state of
the backward LSTM h′t. This bidirectional fea-

!"#$

!"#$

!"#$

"%&'()*

*
'

+
',-
./0
',-

+1
'2-
./01

'2-

+
'
./0
'

+1
'
./01

'

+
'+1

'

*11
'

+11
',-
./011

',-
+11

'
./011

'

+11
'

3
'

Figure 1: The OCLSP neural parser. x, h, c and y are
the model input, hidden state of a LSTM, cell state of a
LSTM and the model output respectively.

ture, which is also exploited by Bahdanau et al.
(2014) for neural machine translation, provides
information for the current time step with infor-
mation gathered both from the beginning of the
sentence and the end of the sentence in terms of
cell states and the hidden states.

Upper Level: The main component of the
upper level system is an LSTM. The LSTM
takes in the output from the lower level (ht, h′t)
as well as a secondary input x′′t the system and
generates a hidden state h′′t which is then fed
into a softmax layer to transform into a prob-
ability distribution. If xt = f(wi, wj) where
w is a word in a sentence, f is the function
from words to input vectors and i and j are the
indexes of the words, then the final output of
the upper level of the system can be viewed as
p(labeli,j |f(w1, w1), ..., f(wi, wj), ..., f(wn, wn),
θ), where θ is the model parameters.

3.3 Input Features

The input to the parser consists of two parts:
the primary input xt which is the input fed to

1213



the lower level system and the secondary input
x′′t which is fed to the upper level system. In de-
velopment, different kinds of input features are
added to the model incrementally to help fea-
ture selection. The f function mentioned in the
model description translates word pairs to con-
catenation of different feature vectors.

The system makes heavy use of vectorial rep-
resentations trained with word2vec (Mikolov et
al., 2013). There are two kinds of embeddings
used by the parser: word embeddings and Part-
Of-Speech embeddings. The word embeddings
are all trained with the full Chinese Wikipedia,
1 the Chinese Gigaword (Graff and Chen, 2003)
as well as the training and development datasets
from both genres in this task. The POS embed-
dings are trained with the POS tag sequences
from training and development datasets from
both genres in this task. The embeddings are
fixed during training and test.

Basic features: The basic features for
the lower level system are 300-dimension
word embeddings for the current word pair
⟨wi, wj⟩. The basic features for the up-
per level system are 50-dimension word em-
beddings for the context words of the cur-
rent word pair with a window size of 4, i.e.
⟨wi−3, wi−2, wi−1, wi+1, wi+2, wi+3, wj−3, wj−2,
wj−1, wj+1, wj+2, wj+3⟩. The 50 dimensional
word embeddings are used because we want the
word embeddings at the upper level of the same
length as the word embeddings in the lower
level, and also we want to keep the number of
parameters of the upper level inside a practical
range for training. Whenever there is no word
at a certain position of the window, an all-zero
vector is used to indicate that there is no in-
formation from that position for the system to
use.

POS features: The POS features for the
lower level system are 50 dimensional POS em-
beddings for the current word pair, and for the
upper level system, the 50 dimensional POS em-
beddings are also used for the context words of
the current word pair. The all-zero vector is also
used whenever a position inside the window has

1https://zh.wikipedia.org

Model Labeled F Unlabeled F
TEXT
basic 61.26% 75.31%
basic+pos 65.58% 78.98%
basic+pos+gcg 67.68% 81.12%
NEWS
basic+pos 57.94% 74.42%
basic+pos+gcg 58.95% 75.44%

Table 1: Results of models with different feature com-
binations on development dataset. The performance of
the model with only basic features on NEWS is not re-
ported here because it became evident that it never out-
performed the other two models with more features.

no word.
Predicate-argument features: In previ-

ous work, non-local dependencies expressed us-
ing a generalized categorial grammar (Bach,
1981; Nguyen et al., 2012) have been used to
train an off-the-shelf PCFG parser (Petrov and
Klein, 2007) to accurately parse Chinese (Duan
and Schuler, 2015). Predicate-argument de-
pendency features were extracted from parsed
training sentences using this representation and
the trained parser, which gives directed depen-
dencies with 16 different labels, like arg-1 and
Nmod, for some pairs of the words. These depen-
dencies are transformed first into undirected de-
pendencies with the original labels. This trans-
form mitigates some problems with arbitrary arc
direction discrepancies between two dependency
schemes. Then the labels of these dependencies
are converted into one hot vectors for the upper
level system to use. For the word pairs without a
predicate-argument dependency, an all-zero vec-
tor of length 16 is used.

Table 1 shows the labeled and unlabeled F
scores for models trained with different feature
sets on the development dataset. The POS em-
beddings are good indicators of semantic de-
pendencies as it improves the scores on TEXT
by about 4 percent, and the predicate-argument
features further improve the F score by another
2 percent on TEXT, and 1 percent on NEWS.
The submitted systems use all three feature cat-
egories to train and test.

1214



3.4 Preprocessing and postprocessing
There are a few preprocessing and postprocess-
ing steps that are adopted in the system to gen-
erate better results.

Word Segmentation: The datasets pro-
vided in the task come pre-segmented, but the
training data used by word2vec need to be seg-
mented first to produce embeddings. We use
NLPIR 2015 (Zhang et al., 2003) for segmen-
tation with all words in training and develop-
ment datasets put into the user dictionary for
segmentation. This guarantees that there is no
out-of-vocabulary word in development, which
also increases parser performance on develop-
ment data.

OOV Replacement: The OOV words are
processed in the following way. Suppose there
is a large dataset D1 for training word embed-
dings. The vocabulary of D1 is the set V1. Sup-
pose there is a new dataset D2 which has the
vocabulary set V2. The OOV words from D2
are Voov = V2 − V1. We first train an embed-
ding model W1 for D1 and W2 for D2, and then
for each word voov in Voov, we find the word v
in V1 which has the highest similarity score by
cosine to voov in W2, and replace the voov with
v in V2. This procedure essentially replaces all
the OOV words with the most similar words in
the current semantic space of the word embed-
dings. By doing this, the models do not have to
be retrained every time the semantic space of all
word embeddings is shifted from W1 to W2 with
addition of new embeddings. Also the replaced
embeddings have more semantic information of
the OOV words than a general UNK symbol.

Graph Generation: The proposed parser
has no guarantee to produce a dependency
graph with all nodes connected to the root.
In practice, the raw output of the parser is
often not a graph. The parser is generally
over-conservative, so many words do not have
a dependency arc at all. Therefore the Chu-
Liu/Edmond’s algorithm2 is applied to the prob-
ability matrix for a complete parse chart of a
sentence with the reciprocals of the probability
values as costs of edges between nodes, and the

2https://github.com/mlbright/edmonds

minimal spanning directed graph is generated
for the sentence. This guarantees that all final
outputs are directed graphs.

The Chu-Liu/Edmond’s algorithm outputs a
directed graph with no reentrance, but seman-
tic dependency graphs in the task have reentrant
nodes. In order to produce reentrant dependen-
cies, we set a threshold of σ on the probabilities
for the arcs of a head word. When the parser de-
cides there are multiple heads for a dependent,
and the probabilities of the extra arcs are higher
than σ, these arcs will be incorporated into the
dependency graph. Two submitted systems use
this method to generate final results. One sys-
tem uses the direct output from the parser and
generates an arc for any word without a head
by adding an arc with the highest probability
between the word and any candidate head.

3.5 Training
The loss function of the parser is set to be Nega-
tive Log-likelihood function and the training ob-
jective is to maximize the log-likelihood of the
dependency labels given all word pairs. We use
mini-batch update and Adagrad (Duchi et al.,
2011) to optimize the parameter learning. Each
sentence is a mini-batch. The models used for
different datasets are trained separately. We
used the F score on the development dataset as
the metric to measure convergence. For TEXT,
there are in total 10754 sentences in training and
for NEWS there are 8301 sentences in training.
There are in theory 171 possible dependency la-
bels and 1 no-dependency label in training, but
in practice we only observe 158 different seman-
tic dependency labels and 1 no-dependency la-
bel in training and development. The models for
TEXT are trained for 12 epochs, and the models
for NEWS are trained for 8 epochs.

4 Task Evaluation Results
Three systems with slightly different configu-
rations for the Graph Generation process were
submitted for test. The results are shown in
Table 2. The lbpg system does not use the Chu-
Liu/Edmond’s algorithm for graph generation.
Instead it just generates an arc for a word with-
out a head by adding an arc with the highest

1215



Model LF NLF UF NUF
TEXT
lbpg 65.54% 57.51% 79.39% 63.21%
lbpgs 66.21% 47.79% 79.85% 55.51%
lbpg75 66.38% 57.51% 79.91% 63.87%
TOP 68.59% 50.57% 82.41% 64.58%
NEWS
lbpg 57.22% 45.57% 74.93% 58.03%
lbpgs 57.81% 41.56% 75.54% 54.34%
lbpg75 57.78% 48.89% 75.40% 58.28%
TOP 59.06% 40.84% 77.64% 60.20%

Table 2: Results of the submitted models on test
dataset. LF, NLF, UF and NUF are respectively Labeled
F, Non-local Labeled F, Unlabeled F and Non-local Un-
labeled F scores.

probability between the word and any candidate
head. It is the worst performing model among
the three in terms of F scores. The lbpgs and
lbpg75 are both models with the graph genera-
tion algorithm. The difference between them is
that lbpgs uses a σ value of 100, which means
that all words have only one head in its output,
whereas lbpg75 uses a σ value of 75 meaning that
the extra arcs of a word must have a probability
over 0.75.

For the lbpgs system, it is interesting to see
that only allowing one head per word does not
seem to have a huge impact on the F scores. It
actually performs better on the NEWS dataset
in terms of LF and UF. This indicates that the
systems do not perform very well on the ex-
tra arcs, either predicting the wrong labels for
the arcs or predicting the wrong arcs altogether.
However, the non-local scores see a big drop.

For the lbpg75 system, it is clear that by
choosing a high threshold of extra arc proba-
bility, in TEXT at least, we get slightly higher
F scores. In both datasets, the non-local scores
are the highest with this model, meaning that
for the arcs with high confidence, the non-local
dependency arcs within them are relatively ac-
curate.

5 Discussion and Future Work
Negative Training Cases: One of the most
obvious drawbacks of systems like this is that

there is a disproportional amount of negative
training cases in the training data. In training,
the whole parse chart needs to be filled with
labels. However, most of the time the num-
ber of arcs in a sentence does not exceed 2n.
Therefore the number of word pairs with the
no-dependency label, or the number of negative
cases, is n2 − 2n. When n gets big, for exam-
ple for some sentences in NEWS n can be over
200, the number of negative cases gets very large
compared to the number of positive cases. It
is interesting to see that the parser still cap-
tures fairly well the information from positive
cases, which are few compared to the negative
ones. However, but it may be even better if
the number of negative cases can be greatly re-
duced, the information from positive cases can
be even more accessible and the training time
can be shorter.

OOV Words: The parser relies on word em-
beddings and POS embeddings to make depen-
dency predictions. Due to different segmenta-
tion schemes used by the automatic segmenter
and the annotators, about 10% of the vocabu-
lary in test is OOV words even when the word
embeddings are trained with Chinese Wikipedia
and Chinese Gigaword combined. The OOV
strategy adopted in this paper is shown to be
better than just replacing all OOVs with a uni-
form vector like all-ones or all-zeros, in which
case the parser just treats all OOVs as punctu-
ation. However the relation between the OOV
words and the replacement words do not seem to
correspond to any linguistic or world knowledge
for most of the replacement-OOV word pairs.
For example, the most similar word for replace-
ment of the OOV word “马来语” (Malay lan-
guage) is “910129”, and the most similar word
for “税改” (tax reform) is “面谈成绩” (inter-
view performance). OOVs are a challenge to all
parsers, but it is especially important to parsers
like the one in this paper, where its performance
depends crucially on the quality of the embed-
dings.

Parsing Typology: The parsing typology
used in this paper alleviates problems like reen-
trant nodes and crossing arcs, but at a cost of
efficiency and accuracy. Only about 3% of all

1216



the dependency arcs are extra arcs, i.e. they are
second or third dependency heads for a given
dependent. Such dependencies are very hard to
predict, and the typology used here, although
able to generate such dependencies naturally, is
less than ideal in different ways described above.
More research needs to be done for a better pars-
ing typology for neural parsers which addresses
these issues without losing the ability to predict
extra arcs.

6 Conclusion

We present a neural parser with LSTM in this
paper for the task of Chinese semantic depen-
dency parsing. We have shown that such a
parser may perform close to the top performer in
the task with minimal feature engineering. We
also discussed places where potential improve-
ments can be made.

References
Emmon Bach. 1981. Discontinuous constituents in

generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic
Society (NELS), 11:1–12.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. In ICLR,
pages 1–15.

Wanxiang Che, Meishan Zhang, Yanqiu Shao, and
Ting Liu. 2012. Semeval-2012 task 5 : Chinese
semantic dependency parsing. In SemEval, pages
378–384.

Manjuan Duan and William Schuler. 2015. Pars-
ing chinese with a generalized categorial grammar.
In Proceedings of Grammar Engineering Across
Frameworks 2015, pages 25–32.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

David Graff and Ke Chen. 2003. Chinese gigaword
ldc 2003t09.

Alex Graves, Navdeep Jaitly, and Abdel Rahman
Mohamed. 2013a. Hybrid speech recognition with
deep bidirectional lstm. 2013 IEEE Workshop on
Automatic Speech Recognition and Understanding,
ASRU 2013 - Proceedings, pages 273–278.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013b. Speech recognition with deep re-
current neural networks. IEEE International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), (3):6645–6649.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Comput.,
9(8):1735–1780.

Sepp Hochreiter. 1998. The vanishing gradient
problem during learning recurrent neural nets and
problem solutions. Int. J. Uncertain. Fuzziness
Knowl.-Based Syst., 6(2):107–116.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. CoRR, abs/1301.3:1–
12.

Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency
recovery using generalized categorial grammars.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING
’12), pages 2125–2140.

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
{NAACL HLT} 2007, pages 404–411. Association
for Computational Linguistics, 4.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. NIPS, pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term mem-
ory networks. ACL, pages 1556–1566.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav
Petrov, Ilya Sutskever, and Geoffrey Hinton.
2015. Grammar as a foreign language. In NIPS.

Hua-Ping Zhang, Hong-Kui Yu, Xiong De-Yi, and
Qun Liu. 2003. Hhmm-based chinese lexical
analyzer ictclas. In Proceedings of the Second
SIGHAN Workshop on Chinese Language Pro-
cessing, volume 17, pages 184–187. Association for
Computational Linguistics.

1217


