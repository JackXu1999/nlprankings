



















































SSAS: Semantic Similarity for Abstractive Summarization


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 198–203,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

SSAS: Semantic Similarity for Abstractive Summarization

Raghuram Vadapalli Litton J Kurisinkel Manish Gupta∗ Vasudeva Varma
International Institute of Information Technology – Hyderabad

Hyderabad India
{raghuram.vadapalli, litton.jkurisinkel}@research.iiit.ac.in

{manish.gupta, vv}@iiit.ac.in

Abstract

Ideally a metric evaluating an abstract sys-
tem summary should represent the ex-
tent to which the system-generated sum-
mary approximates the semantic inference
conceived by the reader using a human-
written reference summary. Most of the
previous approaches relied upon word or
syntactic sub-sequence overlap to evalu-
ate system-generated summaries. Such
metrics cannot evaluate the summary at
semantic inference level. Through this
work we introduce the metric of Seman-
tic Similarity for Abstractive Summariza-
tion (SSAS)1, which leverages natural lan-
guage inference and paraphrasing tech-
niques to frame a novel approach to eval-
uate system summaries at semantic infer-
ence level. SSAS is based upon a weighted
composition of quantities representing the
level of agreement, contradiction, topi-
cal neutrality, paraphrasing, and option-
ally ROUGE score between a system-
generated and a human-written summary.

1 Introduction

Abstractive summarization techniques try to
mimic human expert’s capabilities of inference
making and producing a summary in her own writ-
ing style. Automated abstractive summarization
techniques are highly desirable since one needs
a lot of effort and language skills for generating
summaries from varying information sources such
as social media, databases, web articles etc. It
is crucial for a constructive evolution of research

∗The author is also a Principal Applied Scientist at Mi-
crosoft (gmanish@microsoft.com).

1Data and code are shared at http://
somewhereonweb.com.

on abstractive summarization, to establish a metric
which can judge the quality of a system-generated
abstractive summary. An ideal metric should be
able to represent the similarity of semantic infer-
ence perceived by a reader from system-generated
summary to that from a human-written reference
summary.

Most of the existing summarization metrics are
well-suited for extractive summaries, and are di-
rectly or indirectly based upon word or syntactic
substructure overlap (Lin, 2004). Evaluation of
abstractive summarization needs a semantic over-
lap based method. Although there are some met-
rics which attempt to evaluate the summary at se-
mantic level (Nenkova and Passonneau, 2004; Pas-
sonneau et al., 2013; Yang et al., 2016), they ei-
ther demand high level of human involvement or
rely on external discrete vocabulary information
(Miller et al., 1990). Also they are less equipped
to conceive the accurate semantic inference from
long sequences of summary text.

For instance, consider the following statements.
A: Mary lived through an era of liberating reform
for women.
B: Mary’s life spanned years of incredible change
for women.
C: Mary lived through an era of suppression of
women.

Considering A as the reference summary ele-
ment, most of the previous metrics give higher
score to C than B even when C is clearly con-
tradicting A. Actual scores for above samples are
shown in Table 1.

Bowman et al. (2015) mention that understand-
ing entailment and contradiction is fundamental
to understanding natural language. The lack of
consideration of semantics when evaluating sum-
marization automatically, motivates us to propose
a new metric focused on semantic matching be-
tween system and human summaries.

198



Method B C
ROUGE-1 0.33 0.66
ROUGE-2 0.0 0.4
ROUGE-L 0.33 0.66
ROUGE-SU4 0.05 0.45
PEAK 0.45 0.33
SSAS 0.65 0.48

Table 1: Scores given to Samples B and C by Var-
ious Metrics

Our main contributions are as follows.

• We propose a novel metric SSAS for seman-
tic assessment of abstractive summaries.

• The method includes computing various se-
mantic and lexical similarity measures be-
tween reference summary and system sum-
mary, and learning a weight vector to com-
bine these measures into a single score such
that the score maximally correlates with hu-
man evaluation of summaries.

• We experimentally show the robustness and
effectiveness of SSAS.

The rest of the paper is organized as follows.
Section 2 describes previous attempts at evaluating
summarization systems. Section 3 describes our
approach in detail. We discuss our experimental
results in Section 4. We conclude with a summary
in Section 5.

2 Related Work

The following are two broad approaches popular
for evaluation of summaries proposed in the past.

2.1 ROUGE Based Approaches
ROUGE-n measures evaluate the system summary
on the basis of n-gram overlap. ROUGE-SU and
ROUGE-L evaluate content overlap in a more so-
phisticated manner. But they still cannot reliably
capture semantic overlap or semantic contradic-
tion. Recently Ng and Abrecht (2015) tried to en-
hance ROUGE using word embeddings, but word
embeddings cannot grasp the semantic inference
induced by a sequence of words.

2.2 Pyramid Based Approaches
In pyramid evaluation (Nenkova and Passonneau,
2004), Summarization Content Units (SCUs) are
extracted from model summaries and they are

given weights which are equal to the number of
reference summaries they occur in. After this,
a generated summary is given a score which is
equal to the normalized sum of the weights of the
overlapping SCUs. Pyramid score does not eval-
uate the semantic overlap in a continuous space,
and also requires manual efforts when performing
evaluation.

Autopyramid (Passonneau et al., 2013) auto-
mates a part of the pyramid based evaluation
which checks whether an SCU is present in the
generated summary. Though they use various
generic dense representations (Guo and Diab,
2012) for estimating semantic similarity between
SCUs, Autopyramid cannot explicitly quantify the
quality of a summary based on its agreement or
contradiction with a reference summary.

The PEAK (Yang et al., 2016) method for eval-
uation automates the extraction part of the SCUs,
and they use the ADW (Align, Disambiguate and
Walk) algorithm (Pilehvar et al., 2013) to compute
semantic similarity. However, their approach fails
to model contradiction, paraphrase identification
and other features like natural language inference.

3 Approach for SSAS Computation

We first extract SCUs using the automatic SCU
extraction scheme introduced by PEAK model I,
which in turn relies on Open Information Extrac-
tion (OpenIE) (Angeli et al., 2015) for the ex-
traction process. Given a reference summary R
and a system summary S, we obtain SCU sets
SCUs(R) and SCUs(S) with cardinality n and
m respectively. Next, we derive a set of natu-
ral language inference and paraphrasing features
from the text pieces. Computation of these fea-
tures is explained in Section 3.1. After that, we
use a ranking model to learn the weights for com-
bining these features to obtain a score. Finally, we
normalize the obtained score. Ranking and Nor-
malization are discussed in detail in Section 3.2.

3.1 Features for SSAS
SSAS uses natural language inference (NLI) fea-
tures, paraphrase features and ROUGE-SU4 as
features. We discuss these in detail below.

3.1.1 NLI Features
In this subsection, we consider features that cap-
ture natural language inference-based similarity
between text pieces. We leverage the neural at-
tention model, proposed by Cheng et al. (2016)

199



for this purpose. Let E(a, b), C(a, b) and N(a, b)
be the entailment, contradiction and topic neutral-
ity probabilities respectively between two SCUs a
and b such that E(a, b) + C(a, b) + N(a, b) =
1. Based on these probability scores, we com-
pute values for the following features from sets
SCUs(R) and SCUs(S).
Combined Entailment Scores: We compute two
features Fe1 and Fe2 which quantify the combined
entailment score between reference summary and
system summary. Fe1 (Eq. 1) finds the SCU
b ∈ SCUs(S) that is best entailed by each SCU
a ∈ SCUs(R), and then computes average entail-
ment score across all a ∈ SCUs(R). Fe2 (Eq. 2)
is defined similarly, but aggregates scores across
all a ∈ SCUs(S) and considers entailment of an
SCU b ∈ SCUs(R) by an SCU a ∈ SCUs(S).

Fe1 =
1
n

∑
a � SCUs(R)

max
b�SCUs(S)

E(a, b) (1)

Fe2 =
1
m

∑
a � SCUs(S)

max
b�SCUs(R)

E(a, b) (2)

Combined Contradiction Scores: Similar to en-
tailment scores, two features Fc1 and Fc2 quan-
tify the combined contradiction score as shown in
Eqs. 3 and 4 respectively.

Fc1 =
1
n

∑
a � SCUs(R)

max
b�SCUs(S)

C(a, b) (3)

Fc2 =
1
m

∑
a � SCUs(S)

max
b�SCUs(R)

C(a, b) (4)

Combined Topic Neutrality Scores: Finally, two
features Fn1 and Fn2 are computed to quantify
the combined topical neutrality score as shown in
Eqs. 5 and 6 respectively.

Fn1 =
1
n

∑
a � SCUs(R)

max
b�SCUs(S)

N(a, b) (5)

Fn2 =
1
m

∑
a � SCUs(S)

max
b�SCUs(R)

N(a, b) (6)

3.1.2 Paraphrase Features
We compute the paraphrasing probability P (a, b)
for two SCUs a and b using the model proposed by
Kiros et al. (2015) which is trained on MSRP cor-
pus (Bouamor et al., 2012). The combined para-
phrase scores Fp1 and Fp2 are given by Eqs. 7
and 8 respectively.

Fp1 =
1
n

∑
a � SCUs(R)

max
b�SCUs(S)

P (a, b) (7)

Fp2 =
1
m

∑
a � SCUs(S)

max
b�SCUs(R)

P (a, b) (8)

3.1.3 ROUGE-SU4 Feature
Along with other dense semantic level features, n-
gram overlap can also be indicative to evaluate the
summary quality. Following this intuition, we in-
clude ROUGE score between the system summary
S and the reference summary R as one of the fea-
tures.

FR = ROUGE-SU4(S, R) (9)

3.2 Computing SSAS
For every pair (R,S), we concatenate the ex-
tracted features to form the feature vector ~f .

~f = [Fe1, Fe2, Fc1, Fc2, Fn1, Fn2, Fp1, Fp2, FR]

We estimate a score for S with respect to R as
shown in Eq. 10.

score(S, R) = ~λ · [~f, 1] (10)
where ~λ is a learned 10 dimensional parameter
vector (9 for features + 1 for bias). The value of
~λ is optimized so that the score as computed using
Eq. 10 matches human assigned score. Finally, we
compute the normalized SSAS score for a system
summary with respect to the reference summary
using min-max normalization as shown in Eq. 11.

SSAS(S, R) =
~λ · ([~f, 1] − [ ~fmin, 1])

~λ · ([ ~fmax, 1] − [ ~fmin, 1])
(11)

where ~fmax is the feature vector of an ideal sum-
mary obtained by setting values for entailment,
paraphrase and ROUGE features to 1, and rest all
to 0. ~fmin is the feature vector of an extremely
bad summary obtained by setting values for con-
tradiction features to 1, and rest all to 0. Overall,
SSAS scores lie between 0 and 1. The higher the
SSAS score, the better is the system summary S.

200



4 Experiments and Results

In this section, we discuss details of our dataset,
comparison of multiple ranking models, and com-
parison of SSAS with other metrics for summa-
rization.

4.1 Dataset

We obtained DUC2 2002, 2003, 2004 datasets
and the TAC3 dataset which contain triplets
(D,HS,LS) where D denotes the docu-
ment/corpus to be summarized, HS denotes
the human-written summary of D, and LS
denotes the list of system summaries with their
corresponding human evaluation scores.

In total, we collected approximately 250
(D,HS,LS) triplets from these datasets. Since
SSAS is framed to evaluate abstractive summary,
we constructed a test set comprising strictly ab-
stractive summaries for 30 documents in DUC
2002 dataset separate from the training and de-
velopment sets. Although we perform experi-
ments by taking single human-written summary,
extension to use multiple reference summaries is
straightforward. Using multiple references also
aligns with the notion that there is no single best
summary.

4.2 Learning to Rank by Optimizing ~λ

The value of ~λ is optimized by applying ‘Learn-
ing To Rank’ (LTR) techniques over training data
comprising of (D,HS,LS) triplets. The opti-
mization is performed such that the ranking or-
der obtained by Eq. 10 maximally correlates with
ranking order obtained by human scores. We exe-
cuted development experiments with the three dif-
ferent LTR techniques (Burges et al., 2005; Cao
et al., 2007): Pairwise, Listwise and Regression.

The results on the development dataset in terms
of Pearson correlation and Spearman rank corre-
lation are shown in Table 2. As the table shows,
Listwise comparison gained better results though
the results are not significantly different from oth-
ers.

4.3 Evaluating SSAS

As mentioned earlier, to create the test dataset
we chose 30 document-reference summary pairs
from DUC 2002 and asked two human annota-
tors to read each of the reference summary and to

2http://duc.nist.gov/data.html
3http://tac.nist.gov/data/past/2011/Summ11.html

Method PearsonCorrelation
Spearman

Rho
Pairwise 0.970 0.975
Listwise 0.978 0.979
Regression 0.964 0.967

Table 2: Pearson Correlation and Spearman Rho
for the Three Ranking Models

Method Accuracy σ
ROUGE-1 0.810 NA
ROUGE-2 0.782 NA
ROUGE-L 0.825 NA
ROUGE-SU4 0.839 NA
PEAK 0.861 NA
SSAS without Fe1, Fe2 0.854 5.44e-6
SSAS without Fc1, Fc2 0.893 4.38e-6
SSAS without Fn1, Fn2 0.862 6.12e-6
SSAS without Fp1, Fp2 0.845 4.1e-6
SSAS without FR 0.882 5.64e-6
SSAS with all features 0.913 6.22e-6
Human 1.000 NA

Table 3: Results on the Custom Dataset

reproduce the content in their own writing style
with full freedom to choose the convenient vocab-
ulary. The human summarizers are post-graduate
students in computational linguistics and we call
the summary written by them as abstract summary
AbSum. For each document in the test set a ran-
dom subset of sentences are chosen to form a ran-
dom summary RandSum of the document. We
use SSAS to score AbSum and RandSum with
respect to the reference summary. The reliability
of the metric is proportional to the number of times
AbSum is scored better than RandSum.

For the metric to be reliable, it is important to
show that it produces consistent results even if the
training and test data are from different types of
data. For this purpose, we trained the model on
the following three different subsets and recorded

Method Execution Time (sec)
ROUGE 4
PEAK 10
SSAS 200

Table 4: Approximate Execution Times of Various
Metrics on Test Data

201



standard deviation of accuracy: a subset of 80
triples from DUC, a subset of 80 triples from TAC,
a subset of 40 triples each from DUC and TAC.

Table 3 shows the mean results obtained by re-
peatedly selecting sentences for random summary
10 times over each of the above training subsets
and the standard deviation obtained on changing
training subset. The insignificance of standard de-
viation shows that the metric produces consistent
results as long as training data is reliable.

4.4 Analysis of Results
From Table 3, we see that excluding contradic-
tion does not have much impact on results. This
is not surprising as we trained the data on DUC
and TAC datasets which have very few contradic-
tory sentences as they are extractive summaries.
Nonetheless, we propose incorporating contradic-
tion as abstractive summaries have a good chance
of producing contradictory sentences. We can ad-
dress this problem by training on human-evaluated
abstractive summaries which we leave as future
work.

We also see that Fp1 and Fp2 (features corre-
sponding to paraphrasing) are important features
since excluding them has a significant impact on
the results.

Table 4 shows the approximate execution times
for various methods. Since SSAS performs simi-
larity assessment using deep semantic analysis, it
does take significantly large amount of execution
time compared to other methods. However, SSAS
computations for multiple summaries can be eas-
ily parallelized.

5 Conclusions

In this work, we proposed a novel metric SSAS
for semantic assessment of abstractive summaries.
Our experiments show that SSAS outperforms
previously proposed metrics. While the metric
shows a very strong correlation with human judg-
ments, it is computationally very intensive be-
cause of the deep semantic models which are used
to compute various features. In the future, we plan
to explore more efficient ways to obtain the feature
vectors for SSAS computation.

References
Gabor Angeli, Melvin Jose Johnson Premkumar, and

Christopher D. Manning. 2015. Leveraging Lin-
guistic Structure For Open Domain Information Ex-

traction. In Proc. of the 53rd Annual Meeting of the
Association for Computational Linguistics (ACL).
pages 344–354.

Houda Bouamor, Aurélien Max, and Anne Vilnat.
2012. Automatic Acquisition and Validation of Sub-
sentential Paraphrases: A Bilingual Study. Journal
of the Association pour le Traitement Automatique
des Langues (TAL) 53(1):11–37.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A Large Anno-
tated Corpus for Learning Natural Language Infer-
ence. In Proc. of the 2015 Conf. on Empirical Meth-
ods in Natural Language Processing (EMNLP).

Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to Rank using Gradient Descent. In
Proc. of the 22nd Intl. Conf. on Machine learning
(ICML). pages 89–96.

Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to Rank: From Pairwise
Approach to Listwise Approach. In Proc. of the 24th
Intl. Conf. on Machine learning (ICML). pages 129–
136.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long Short-Term Memory Networks for Machine
Reading. Proc. of the 2016 Conf. on Empirical
Methods in Natural Language Processing (EMNLP)
pages 551–562.

Weiwei Guo and Mona T. Diab. 2012. Modeling Sen-
tences in the Latent Space. In Proc. of the 50th An-
nual Meeting of the Association for Computational
Linguistics (ACL). pages 864–872.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-Thought Vectors. In
Proc. of the 2015 Conf. on Advances in Neural In-
formation Processing Systems (NIPS). pages 3294–
3302.

Chin-Yew Lin. 2004. ROUGE: A Package for Au-
tomatic Evaluation of Summaries. In Proc. of
the Workshop on Text Summarization Branches Out
(WAS). pages 74–81.

G. A. Miller, C. Fellbaum, D. Gross, and K. J. Miller.
1990. Introduction to WordNet: An On-Line Lex-
ical Database. Intl. Journal of Lexicography (IJL)
3(4):235–244.

Ani Nenkova and Rebecca J. Passonneau. 2004. Eval-
uating Content Selection in Summarization: The
Pyramid Method. In Proc. of the 2004 Human
Language Technology Conf. of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL). pages 145–152.

Jun-Ping Ng and Viktoria Abrecht. 2015. Better Sum-
marization Evaluation with Word Embeddings for

202



ROUGE. In Proc. of the 2015 Conf. on Em-
pirical Methods in Natural Language Processing
(EMNLP). pages 1925–1930.

Rebecca J Passonneau, Emily Chen, Weiwei Guo, and
Dolores Perin. 2013. Automated Pyramid Scoring
of Summaries using Distributional Semantics. In
Proc. of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL). pages 143–
147.

Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: A Unified Approach for Measuring Seman-
tic Similarity. In Proc. of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL). pages 1341–1351.

Qian Yang, Rebecca J Passonneau, and Gerard
de Melo. 2016. PEAK: Pyramid Evaluation via Au-
tomated Knowledge Extraction. In Proc. of the 13th
AAAI Conf. on Artificial Intelligence (AAAI). pages
2673–2680.

203


