Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394–402,

Beijing, August 2010

394

Better Arabic Parsing: Baselines, Evaluations, and Analysis

Spence Green and Christopher D. Manning

Computer Science Department, Stanford University

{spenceg,manning}@stanford.edu

Abstract

In this paper, we offer broad insight
into the underperformance of Arabic con-
stituency parsing by analyzing the inter-
play of linguistic phenomena, annotation
choices, and model design. First, we iden-
tify sources of syntactic ambiguity under-
studied in the existing parsing literature.
Second, we show that although the Penn
Arabic Treebank is similar to other tree-
banks in gross statistical terms, annotation
consistency remains problematic. Third,
we develop a human interpretable gram-
mar that is competitive with a latent vari-
able PCFG. Fourth, we show how to build
better models for three different parsers.
Finally, we show that in application set-
tings, the absence of gold segmentation
lowers parsing performance by 2–5% F1.

1

Introduction

It is well-known that constituency parsing mod-
els designed for English often do not generalize
easily to other languages and treebanks.1 Expla-
nations for this phenomenon have included the
relative informativeness of lexicalization (Dubey
and Keller, 2003; Arun and Keller, 2005), insensi-
tivity to morphology (Cowan and Collins, 2005;
Tsarfaty and Sima’an, 2008), and the effect of
variable word order (Collins et al., 1999). Cer-
tainly these linguistic factors increase the difﬁ-
culty of syntactic disambiguation. Less frequently
studied is the interplay among language, annota-
tion choices, and parsing model design (Levy and
Manning, 2003; K¨ubler, 2005).

1The apparent difﬁculty of adapting constituency mod-
els to non-conﬁgurational languages has been one motivation
for dependency representations (Hajiˇc and Zem´anek, 2004;
Habash and Roth, 2009).

To investigate the inﬂuence of these factors,
we analyze Modern Standard Arabic (henceforth
MSA, or simply “Arabic”) because of the unusual
opportunity it presents for comparison to English
parsing results. The Penn Arabic Treebank (ATB)
syntactic guidelines (Maamouri et al., 2004) were
purposefully borrowed without major modiﬁca-
tion from English (Marcus et al., 1993). Further,
Maamouri and Bies (2004) argued that the English
guidelines generalize well to other languages. But
Arabic contains a variety of linguistic phenom-
ena unseen in English. Crucially, the conventional
orthographic form of MSA text is unvocalized, a
property that results in a deﬁcient graphical rep-
resentation. For humans, this characteristic can
impede the acquisition of literacy. How do addi-
tional ambiguities caused by devocalization affect
statistical learning? How should the absence of
vowels and syntactic markers inﬂuence annotation
choices and grammar development? Motivated by
these questions, we signiﬁcantly raise baselines
for three existing parsing models through better
grammar engineering.

Our analysis begins with a description of syn-
tactic ambiguity in unvocalized MSA text (§2).
Next we show that the ATB is similar to other tree-
banks in gross statistical terms, but that annotation
consistency remains low relative to English (§3).
We then use linguistic and annotation insights to
develop a manually annotated grammar for Arabic
(§4). To facilitate comparison with previous work,
we exhaustively evaluate this grammar and two
other parsing models when gold segmentation is
assumed (§5). Finally, we provide a realistic eval-
uation in which segmentation is performed both
in a pipeline and jointly with parsing (§6). We
quantify error categories in both evaluation set-
tings. To our knowledge, ours is the ﬁrst analysis
of this kind for Arabic parsing.

395

2 Syntactic Ambiguity in Arabic

Arabic is a morphologically rich language with a
root-and-pattern system similar to other Semitic
languages. The basic word order is VSO, but
SVO, VOS, and VO conﬁgurations are also pos-
sible.2 Nouns and verbs are created by selecting
a consonantal root (usually triliteral or quadrilit-
eral), which bears the semantic core, and adding
afﬁxes and diacritics. Particles are uninﬂected.
Diacritics can also be used to specify grammatical
relations such as case and gender. But diacritics
are not present in unvocalized text, which is the
standard form of, e.g., news media documents.3

Let us consider an example of ambiguity caused
by devocalization. Table 1 shows four words
whose unvocalized surface forms (cid:2)(cid:3) an are indis-
tinguishable. Whereas Arabic linguistic theory as-
signs (1) and (2) to the class of pseudo verbs (cid:2)(cid:4)
(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)(cid:9)(cid:10)(cid:11) inna and her sisters since they can be
inﬂected, the ATB conventions treat (2) as a com-
plementizer, which means that it must be the head
of SBAR. Because these two words have identical
complements, syntax rules are typically unhelp-
ful for distinguishing between them. This is es-
pecially true in the case of quotations—which are
common in the ATB—where (1) will follow a verb
like (2) (Figure 1).

Even with vocalization, there are linguistic cat-
egories that are difﬁcult to identify without se-
mantic clues. Two common cases are the attribu-
tive adjective and the process nominal (cid:12)(cid:3)(cid:13)(cid:14)(cid:15)(cid:16)(cid:3)
maSdar, which can have a verbal reading.4 At-
tributive adjectives are hard because they are or-
thographically identical to nominals; they are in-
ﬂected for gender, number, case, and deﬁniteness.
Moreover,
they are used as substantives much

2Unlike machine translation, constituency parsing is not
signiﬁcantly affected by variable word order. However, when
grammatical relations like subject and object are evaluated,
parsing performance drops considerably (Green et al., 2009).
In particular, the decision to represent arguments in verb-
initial clauses as VP internal makes VSO and VOS conﬁgu-
rations difﬁcult to distinguish. Topicalization of NP subjects
in SVO conﬁgurations causes confusion with VO (pro-drop).
3Techniques for automatic vocalization have been studied
(Zitouni et al., 2006; Habash and Rambow, 2007). However,
the data sparsity induced by vocalization makes it difﬁcult to
train statistical models on corpora of the size of the ATB, so
vocalizing and then parsing may well not help performance.
4Traditional Arabic linguistic theory treats both of these

types as subcategories of noun (cid:17)(cid:18)(cid:19)(cid:3).

Word

Head Of Complement

1
2
3
4

(cid:20)(cid:2)(cid:4) inna “Indeed, truly”

(cid:20)(cid:2)(cid:10) anna “That”

(cid:2)(cid:4) in “If”
(cid:2)(cid:10) an “to”

VP

SBAR
SBAR
SBAR

Noun
Noun
Verb
Verb

POS
VBP
IN
IN
IN

Table 1: Diacritized particles and pseudo-verbs that, after
orthographic normalization, have the equivalent surface form
(cid:2)(cid:3) an. The distinctions in the ATB are linguistically justiﬁed,
but complicate parsing. Table 8a shows that the best model
recovers SBAR at only 71.0% F1.

VP

VP

VBD

(cid:21)(cid:22)(cid:5)(cid:23)(cid:3)

she added

S

VP

PUNC

VBP

“

(cid:2)(cid:3)

NP

NN

. . .

Indeed

(cid:24)(cid:3)(cid:13)(cid:25)

Saddam

(a) Reference

VBD

(cid:21)(cid:22)(cid:5)(cid:23)(cid:3)

she added

SBAR

PUNC

“

IN

(cid:2)(cid:3)

NP

NN

. . .

Indeed

(cid:24)(cid:3)(cid:13)(cid:25)

Saddam

(b) Stanford

Figure 1: The Stanford parser (Klein and Manning, 2002)
is unable to recover the verbal reading of the unvocalized
surface form (cid:2)(cid:3) an (Table 1).

more frequently than is done in English.

Process nominals name the action of the tran-
sitive or ditransitive verb from which they derive.
The verbal reading arises when the maSdar has an
NP argument which, in vocalized text, is marked
in the accusative case. When the maSdar lacks
a determiner, the constituent as a whole resem-
bles the ubiquitous annexation construct (cid:26)(cid:22)(cid:5)(cid:23)(cid:27)(cid:3)
iDafa. Gabbard and Kulick (2008) show that
there is signiﬁcant attachment ambiguity associ-
ated with iDafa, which occurs in 84.3% of the
trees in our development set. Figure 4 shows
a constituent headed by a process nominal with
an embedded adjective phrase. All three mod-
els evaluated in this paper incorrectly analyze the
constituent as iDafa; none of the models attach the
attributive adjectives properly.

For parsing, the most challenging form of am-
biguity occurs at the discourse level. A deﬁning
characteristic of MSA is the prevalence of dis-
course markers to connect and subordinate words
and phrases (Ryding, 2005). Instead of offsetting
new topics with punctuation, writers of MSA in-
sert connectives such as (cid:11) wa and (cid:28) fa to link
new elements to both preceding clauses and the
text as a whole. As a result, Arabic sentences are
usually long relative to English, especially after

396

Length English (WSJ) Arabic (ATB)
≤ 20
≤ 40
≤ 63
≤ 70

41.9%
92.4%
99.7%
99.9%

33.7%
73.2%
92.6%
94.9%

Table 2: Frequency distribution for sentence lengths in the
WSJ (sections 2–23) and the ATB (p1–3). English parsing
evaluations usually report results on sentences up to length
40. Arabic sentences of up to length 63 would need to be
evaluated to account for the same fraction of the data. We
propose a limit of 70 words for Arabic parsing evaluations.

(cid:11) wa
“and”

(cid:28) fa

“so, then”

Part of Speech
conjunction
preposition
abbreviation
conjunction

connective particle

abbreviation

response conditioning particle

subordinating conjunction

Tag
CC
IN
NN
CC
RP
NN
RP
IN

Freq.
4256

6
6
160
67
22
11
3

Table 3: Dev set frequencies for the two most signiﬁcant dis-
course markers in Arabic are skewed toward analysis as a
conjunction.

segmentation (Table 2). The ATB gives several
different analyses to these words to indicate dif-
ferent types of coordination. But it conﬂates the
coordinating and discourse separator functions of
wa ((cid:29)(cid:30)(cid:31)(cid:16)(cid:3) (cid:11)(cid:3)(cid:11)) into one analysis: conjunction
(Table 3). A better approach would be to distin-
guish between these cases, possibly by drawing
on the vast linguistic work on Arabic connectives
(Al-Batal, 1990). We show that noun-noun vs.
discourse-level coordination ambiguity in Arabic
is a signiﬁcant source of parsing errors (Table 8c).

3 Treebank Comparison

3.1 Gross Statistics

Linguistic intuitions like those in the previous sec-
tion inform language-speciﬁc annotation choices.
The resulting structural differences between tree-
banks can account for relative differences in pars-
ing performance. We compared the ATB5 to tree-
banks for Chinese (CTB6), German (Negra), and
English (WSJ) (Table 4). The ATB is disadvan-
taged by having fewer trees with longer average

Trees
Word Typess
Tokens
Tags
Phrasal Cats
Test OOV

ATB
23449
40972
738654

32
22

CTB6
28278
45245
782541

34
26

Negra
20602
51272
355096

499
325

WSJ
43948
46348
1046829

45
27

16.8%

22.2%

30.5% 13.2%

Depth (μ / σ2)
Breadth (μ / σ2)
Length (μ / σ2)
Constituents (μ)
μ Const. / μ Length

Per Sentence

3.87 / 0.74
14.6 / 7.31
31.5 / 22.0

5.01 / 1.44
10.2 / 4.44
27.7 / 18.9

32.8
1.04

32.5
1.18

3.58 / 0.89
7.50 / 4.56
17.2 / 10.9

8.29
0.482

4.18 / 0.74
12.1 / 4.65
23.8 / 11.2

19.6
0.820

Table 4: Gross statistics for several different treebanks. Test
set OOV rate is computed using the following splits: ATB
(Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Ne-
gra (Dubey and Keller, 2003); English, sections 2-21 (train)
and section 23 (test).

yields.6 But to its great advantage, it has a high
ratio of non-terminals/terminals (μ Constituents /
μ Length). Evalb, the standard parsing metric, is
biased toward such corpora (Sampson and Babar-
czy, 2003). Also surprising is the low test set OOV
rate given the possibility of morphological varia-
tion in Arabic.
In general, several gross corpus
statistics favor the ATB, so other factors must con-
tribute to parsing underperformance.

3.2

Inter-annotator Agreement

Annotation consistency is important in any super-
vised learning task.
In the initial release of the
ATB, inter-annotator agreement was inferior to
other LDC treebanks (Maamouri et al., 2008). To
improve agreement during the revision process,
a dual-blind evaluation was performed in which
10% of the data was annotated by independent
teams. Maamouri et al. (2008) reported agree-
ment between the teams (measured with Evalb) at
93.8% F1, the level of the CTB. But Rehbein and
van Genabith (2007) showed that Evalb should
not be used as an indication of real difference—
or similarity—between treebanks.

Instead, we extend the variation n-gram
method of Dickinson (2005) to compare annota-
tion error rates in the WSJ and ATB. For a corpus
C, let M be the set of tuples (cid:3)n, l(cid:4), where n is an
n-gram with bracketing label l. If any n appears

5LDC A-E catalog numbers: LDC2008E61 (ATBp1v4),
LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).
We map the ATB morphological analyses to the shortened
“Bies” tags for all experiments.

6Generative parsing performance is known to deteriorate
with sentence length. As a result, Habash et al. (2006) devel-
oped a technique for splitting and chunking long sentences.
In application settings, this may be a proﬁtable strategy.

397

Corpus

Trees Nuclei
25041
43948
23449
20292

Sample
n-grams

746
2100

Error %

Type
n-gram
12.0% 2.10%
37.0% 1.76%

WSJ 2–23
ATB

Table 5: Evaluation of 100 randomly sampled variation nu-
clei types. The samples from each corpus were indepen-
dently evaluated. The ATB has a much higher fraction of
nuclei per tree, and a higher type-level error rate.

in a corpus position without a bracketing label,
then we also add (cid:3)n, NIL(cid:4) to M. We call the set
of unique n-grams with multiple labels in M the
variation nuclei of C.

Bracketing variation can result from either an-
notation errors or linguistic ambiguity. Human
evaluation is one way to distinguish between the
two cases. Following Dickinson (2005), we ran-
domly sampled 100 variation nuclei from each
corpus and evaluated each sample for the presence
of an annotation error. The human evaluators were
a non-native, ﬂuent Arabic speaker (the ﬁrst au-
thor) for the ATB and a native English speaker for
the WSJ.7

Table 5 shows type- and token-level error rates
for each corpus. The 95% conﬁdence intervals for
type-level errors are (5580, 9440) for the ATB and
(1400, 4610) for the WSJ. The results clearly in-
dicate increased variation in the ATB relative to
the WSJ, but care should be taken in assessing the
magnitude of the difference. On the one hand,
the type-level error rate is not calibrated for the
number of n-grams in the sample. At the same
time, the n-gram error rate is sensitive to samples
with extreme n-gram counts. For example, one of
the ATB samples was the determiner  (cid:16)! dhalik
“that.” The sample occurred in 1507 corpus po-
sitions, and we found that the annotations were
consistent.
If we remove this sample from the
evaluation, then the ATB type-level error rises to
only 37.4% while the n-gram error rate increases
to 6.24%. The number of ATB n-grams also falls
below the WSJ sample size as the largest WSJ
sample appeared in only 162 corpus positions.

7Unlike Dickinson (2005), we strip traces and only con-
sider POS tags when pre-terminals are the only intervening
nodes between the nucleus and its bracketing (e.g., unaries,
base NPs). Since our objective is to compare distributions of
bracketing discrepancies, we do not use heuristics to prune
the set of nuclei.

NP

NP

NNP

DTNNP

NN

(cid:26)(cid:15)"

summit

(cid:24)#$

%&’(cid:16)(cid:3)

Al-Sheikh

Sharm
(a)

NN

(cid:26)(cid:15)"

summit

NP

NP

NNP

(cid:24)#$

NP

DTNNP

Sharm

%&’(cid:16)(cid:3)

Al-Sheikh

(b)

Figure 2: An ATB sample from the human evaluation. The
ATB annotation guidelines specify that proper nouns should
be speciﬁed with a ﬂat NP (a). But the city name Sharm Al-
Sheikh is also iDafa, hence the possibility for the incorrect
annotation in (b).

4 Grammar Development

We can use the preceding linguistic and annota-
tion insights to build a manually annotated Ara-
bic grammar in the manner of Klein and Manning
(2003). Manual annotation results in human in-
terpretable grammars that can inform future tree-
bank annotation decisions. A simple lexicalized
PCFG with second order Markovization gives rel-
atively poor performance: 75.95% F1 on the test
set.8 But this ﬁgure is surprisingly competitive
with a recent state-of-the-art baseline (Table 7).

In our grammar, features are realized as annota-
tions to basic category labels. We start with noun
features since written Arabic contains a very high
proportion of NPs. genitiveMark indicates recur-
sive NPs with a indeﬁnite nominal left daughter
and an NP right daughter. This is the form of re-
cursive levels in iDafa constructs. We also add an
annotation for one-level iDafa (oneLevelIdafa)
constructs since they make up more than 75% of
the iDafa NPs in the ATB (Gabbard and Kulick,
2008). For all other recursive NPs, we add a
common annotation to the POS tag of the head
(recursiveNPHead).

Base NPs are the other signiﬁcant category of
nominal phrases. markBaseNP indicates these
non-recursive nominal phrases. This feature in-
cludes named entities, which the ATB marks with
a ﬂat NP node dominating an arbitrary number of
NNP pre-terminal daughters (Figure 2).

For verbs we add two features. First we mark
any node that dominates (at any level) a verb

8We use head-ﬁnding rules speciﬁed by a native speaker
of Arabic. This PCFG is incorporated into the Stanford
Parser, a factored model that chooses a 1-best parse from the
product of constituency and dependency parses.

398

Feature
—
recursiveNPHead
genitiveMark
splitPUNC
markContainsVerb
markBaseNP
markOneLevelIdafa
splitIN
containsSVO
splitCC
markFem

States Tags
3208
33
38
3287
38
3471
47
4221
47
5766
6586
47
47
7202
94
7595
94
9188
124
9492
10049
141

F1

76.86
77.46
77.88
77.98
79.16
79.5
79.83
80.48
80.66
80.87
80.95

Indiv. ΔF1

—

+0.60
+0.42
+0.10
+1.18
+0.34
+0.33
+0.65
+0.18
+0.21
+0.08

Table 6: Incremental dev set results for the manually anno-
tated grammar (sentences of length ≤ 70).

phrase (markContainsVerb). This feature has a
linguistic justiﬁcation. Historically, Arabic gram-
mar has identiﬁed two sentences types: those that
begin with a nominal ((cid:26)&(cid:15)(cid:18)(cid:19)(cid:3) (cid:26)((cid:15))(cid:16)(cid:3)), and those
that begin with a verb ((cid:26)&((cid:31)*(cid:16)(cid:3) (cid:26)((cid:15))(cid:16)(cid:3)). But for-
eign learners are often surprised by the verbless
predications that are frequently used in Arabic.
Although these are technically nominal, they have
become known as “equational” sentences. mark-
ContainsVerb is especially effective for distin-
guishing root S nodes of equational sentences. We
also mark all nodes that dominate an SVO con-
ﬁguration (containsSVO). In MSA, SVO usually
appears in non-matrix clauses.

Lexicalizing several POS tags improves perfor-
mance. splitIN captures the verb/preposition id-
ioms that are widespread in Arabic. Although
this feature helps, we encounter one consequence
of variable word order. Unlike the WSJ corpus
which has a high frequency of rules like VP →
VB PP, Arabic verb phrases usually have lexi-
calized intervening nodes (e.g., NP subjects and
direct objects).
For example, we might have
VP → VB NP PP, where the NP is the subject.
This annotation choice weakens splitIN.
The ATB gives all punctuation a single tag. For
parsing, this is a mistake, especially in the case
of interrogatives. splitPUNC restores the conven-
tion of the WSJ. We also mark all tags that dom-
inate a word with the feminine ending + taa mar-
buuTa (markFeminine).

To differentiate between the coordinating and
discourse separator functions of conjunctions (Ta-
ble 3), we mark each CC with the label of its
right sister (splitCC). The intuition here is that
the role of a discourse marker can usually be de-

termined by the category of the word that follows
it. Because conjunctions are elevated in the parse
trees when they separate recursive constituents,
we choose the right sister instead of the category
of the next word. We create equivalence classes
for verb, noun, and adjective POS categories.

5 Standard Parsing Experiments

We compare the manually annotated grammar,
which we incorporate into the Stanford parser, to
both the Berkeley (Petrov et al., 2006) and Bikel
(Bikel, 2004) parsers. All experiments use ATB
parts 1–3 divided according to the canonical split
suggested by Chiang et al. (2006). Preprocessing
the raw trees improves parsing performance con-
siderably.9 We ﬁrst discard all trees dominated by
X, which indicates errors and non-linguistic text.
At the phrasal level, we remove all function tags
and traces. We also collapse unary chains with
identical basic categories like NP → NP. The pre-
terminal morphological analyses are mapped to
the shortened “Bies” tags provided with the tree-
bank. Finally, we add “DT” to the tags for deﬁnite
nouns and adjectives (Kulick et al., 2006).

The orthographic normalization strategy we use
is simple.10 In addition to removing all diacrit-
ics, we strip instances of taTweel ,-(cid:8)(cid:30)(cid:7), col-
lapse variants of alif (cid:3) to bare alif,11 and map Ara-
bic punctuation characters to their Latin equiva-
lents. We retain segmentation markers—which
are consistent only in the vocalized section of the
treebank—to differentiate between e.g. (cid:17). “they”
and (cid:17).+ “their.” Because we use the vocalized
section, we must remove null pronoun markers.

In Table 7 we give results for several evalua-
tion metrics. Evalb is a Java re-implementation
of the standard labeled precision/recall metric.12

9Both the corpus split and pre-processing code are avail-

able at http://nlp.stanford.edu/projects/arabic.shtml.

10Other orthographic normalization schemes have been
suggested for Arabic (Habash and Sadat, 2006), but we ob-
serve negligible parsing performance differences between
these and the simple scheme used in this evaluation.

11taTweel (/) is an elongation character used in Arabic
script to justify text. It has no syntactic function. Variants
of alif are inconsistently used in Arabic texts. For alif with
hamza, normalization can be seen as another level of devo-
calization.

12For English, our Evalb implementation is identical to the
most recent reference (EVALB20080701). For Arabic we

399

Leaf Ancestor

Model

System
Baseline

Stanford (v1.6.3)

GoldPOS
Baseline (Self-tag)

Bikel (v1.2)

Baseline (Pre-tag)

Berkeley (Sep. 09)

GoldPOS
(Petrov, 2009)
Baseline

GoldPOS

Length Corpus
0.791
0.773
0.802
0.770
0.752
0.771
0.752
0.775

70
all
70
70
all
70
all
70
all
70
all
70

—

0.809
0.796
0.831

Sent
0.825
0.818
0.836
0.801
0.794
0.804
0.796
0.808

—

0.839
0.834
0.859

Exact LP
358
80.37
78.92
358
81.07
452
77.92
278
76.96
278
78.35
295
77.31
295
309
78.83
— 76.40
82.32
335
81.43
336
496
84.37

Evalb
LR
79.36
77.72
80.27
76.00
75.01
76.72
75.64
77.18
75.30
81.63
80.73
84.21

Tag
%

F1

95.58
95.49
99.95
94.64
94.63
95.68
95.68
96.60

79.86
78.32
80.67
76.95
75.97
77.52
76.47
77.99
75.85 —
81.97
81.08
84.29

95.07
95.02
99.87

Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev
set results with gold POS tags for sentences of length ≤ 40. The Bikel GoldPOS conﬁguration only supplies the gold POS
tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.

F1

85

80

75

Berkeley

Stanford

Bikel

with the number of exactly matching guess trees.

5.1 Parsing Models
The Stanford parser includes both the manually
annotated grammar (§4) and an Arabic unknown
word model with the following lexical features:

training trees

5000

10000

15000

Figure 3: Dev set learning curves for sentence lengths ≤ 70.
All three curves remain steep at the maximum training set
size of 18818 trees.

The Leaf Ancestor metric measures the cost of
transforming guess trees to the reference (Samp-
son and Babarczy, 2003). It was developed in re-
sponse to the non-terminal/terminal bias of Evalb,
but Clegg and Shepherd (2005) showed that it is
also a valuable diagnostic tool for trees with com-
plex deep structures such as those found in the
ATB. For each terminal, the Leaf Ancestor metric
extracts the shortest path to the root. It then com-
putes a normalized Levenshtein edit distance be-
tween the extracted chain and the reference. The
range of the score is between 0 and 1 (higher is
better). We report micro-averaged (whole corpus)
and macro-averaged (per sentence) scores along

add a constraint on the removal of punctuation, which has a
single tag (PUNC) in the ATB. Tokens tagged as PUNC are
not discarded unless they consist entirely of punctuation.

1. Presence of the determiner 0(cid:3) Al
2. Contains digits
3. Ends with the feminine afﬁx + p
4. Various verbal (e.g.,

sufﬁxes (e.g., (cid:26)-)

(cid:3)(cid:11), 1) and adjectival

Other notable parameters are second order vertical
Markovization and marking of unary rules.

Modifying the Berkeley parser for Arabic is
straightforward. After adding a ROOT node to
all trees, we train a grammar using six split-and-
merge cycles and no Markovization. We use the
default inference parameters.

Because the Bikel parser has been parameter-
ized for Arabic by the LDC, we do not change the
default model settings. However, when we pre-
tag the input—as is recommended for English—
we notice a 0.57% F1 improvement. We use the
log-linear tagger of Toutanova et al. (2003), which
gives 96.8% accuracy on the test set.

5.2 Discussion
The Berkeley parser gives state-of-the-art perfor-
mance for all metrics. Our baseline for all sen-
tence lengths is 5.23% F1 higher than the best pre-
vious result. The difference is due to more careful

400

VBG

+2(cid:5)(cid:31)3(cid:18)(cid:3)

restoring

S-NOM

VP

NP

NP

ADJP

NN

NP

(cid:12)(cid:11)2

role

PRP

4

its

DTJJ

5(cid:5)67(cid:16)(cid:3)

DTJJ

,8(cid:5)*(cid:16)(cid:3)

constructive

effective

(a) Reference

NP

NP

NP

NN

+2(cid:5)(cid:31)3(cid:18)(cid:3)

ADJP

DTJJ

,8(cid:5)*(cid:16)(cid:3)

NP

NP

NP

ADJP

ADJP

NN

NP

(cid:12)(cid:11)2

PRP

DTJJ

5(cid:5)67(cid:16)(cid:3)

DTJJ

,8(cid:5)*(cid:16)(cid:3)

4

(b) Stanford

NN

+2(cid:5)(cid:31)3(cid:18)(cid:3)

NP

NP

ADJP

NN

NP

(cid:12)(cid:11)2

PRP

DTJJ

5(cid:5)67(cid:16)(cid:3)

4

(c) Berkeley

NN

+2(cid:5)(cid:31)3(cid:18)(cid:3)

NP

NP

NP

ADJP

NN

NP

(cid:12)(cid:11)2

PRP

DTJJ

5(cid:5)67(cid:16)(cid:3)

4

(d) Bikel

ADJP

DTJJ

,8(cid:5)*(cid:16)(cid:3)

Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmen-
tation). The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs,
maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking
determiners and heading iDafa (Fassi Fehri, 1993). In the ATB, +2(cid:5)(cid:31)3(cid:18)(cid:3) asta’adah is tagged 48 times as a noun and 9 times
as verbal noun. Consequently, all three parsers prefer the nominal reading. Table 8b shows that verbal nouns are the hardest
pre-terminal categories to identify. None of the models attach the attributive adjectives correctly.

pre-processing. However, the learning curves in
Figure 3 show that the Berkeley parser does not
exceed our manual grammar by as wide a mar-
gin as has been shown for other languages (Petrov,
2009). Moreover, the Stanford parser achieves the
most exact Leaf Ancestor matches and tagging ac-
curacy that is only 0.1% below the Bikel model,
which uses pre-tagged input.

In Figure 4 we show an example of variation
between the parsing models. We include a list
of per-category results for selected phrasal labels,
POS tags, and dependencies in Table 8. The er-
rors shown are from the Berkeley parser output,
but they are representative of the other two pars-
ing models.

6

Joint Segmentation and Parsing

Although the segmentation requirements for Ara-
bic are not as extreme as those for Chinese, Ara-
bic is written with certain cliticized prepositions,
pronouns, and connectives connected to adjacent
words. Since these are distinct syntactic units,
they are typically segmented. The ATB segmen-
tation scheme is one of many alternatives. Until
now, all evaluations of Arabic parsing—including
the experiments in the previous section—have as-
sumed gold segmentation. But gold segmentation
is not available in application settings, so a seg-
menter and parser are arranged in a pipeline. Seg-
mentation errors cascade into the parsing phase,
placing an artiﬁcial limit on parsing performance.
Lattice parsing (Chappelier et al., 1999) is an

alternative to a pipeline that prevents cascading
errors by placing all segmentation options into
the parse chart. Recently, lattices have been used
successfully in the parsing of Hebrew (Tsarfaty,
2006; Cohen and Smith, 2007), a Semitic lan-
guage with similar properties to Arabic. We ex-
tend the Stanford parser to accept pre-generated
lattices, where each word is represented as a ﬁnite
state automaton. To combat the proliferation of
parsing edges, we prune the lattices according to
a hand-constructed lexicon of 31 clitics listed in
the ATB annotation guidelines (Maamouri et al.,
2009a). Formally, for a lexicon L and segments
I ∈ L, O /∈ L, each word automaton accepts the
language I∗(O + I)I∗. Aside from adding a simple
rule to correct alif deletion caused by the prepo-
sition 0, no other language-speciﬁc processing is
performed.

Our evaluation includes both weighted and un-
weighted lattices. We weight edges using a
unigram language model estimated with Good-
Turing smoothing. Despite their simplicity, uni-
gram weights have been shown as an effective fea-
ture in segmentation models (Dyer, 2009).13 The
joint parser/segmenter is compared to a pipeline
that uses MADA (v3.0), a state-of-the-art Arabic
segmenter, conﬁgured to replicate ATB segmen-
tation (Habash and Rambow, 2005). MADA uses
an ensemble of SVMs to ﬁrst re-rank the output of
a deterministic morphological analyzer. For each

13Of course, this weighting makes the PCFG an improper
distribution. However, in practice, unknown word models
also make the distribution improper.

401

F1

VP
S
PP
NP

Label
ADJP
SBAR
FRAG

# gold
1216
2918
254
5507
6579
7516
34025
1093
787

59.45
69.81
72.87
78.83
78.91
80.93
84.95
90.64
ADVP
WHNP
96.00
(a) Major phrasal
categories

%

JJ

DTNNP

Tag
JJR

Tag
VBG
VN
VBN

DTNNS
DTJJ
NNP
NN

# gold
182
163
352
932
1516
ADJ NUM 277
2139
818
907
78
2580
(b) Major POS categories

48.84
60.37
72.42
83.48
86.09
88.93
89.94 NOUN QUANT
91.23
91.75
92.41
92.42

VBP
RP
NNS
DTJJR
VBD

PRP
CC
IN
DT

DTNN

# gold
134
1069
3361
4152
10336
6736
352
1366
4076
8676
525

%

92.83
94.29
95.07
95.09
95.23
95.78
98.16
98.24
98.92
99.07
99.81

S

NP

TAG

SBAR

ADJP

NP
S
NP
NP
NP
NP
VP
NP
VP
S

NP
S
NP
NP
NP
NP
TAG
NP
TAG
VP

# gold
946
708
803
2907
1035
2713
3230
805
772
961

Parent Head Modifer Dir
R
R
R
R
R
R
R
L
R
L

F1
0.54
0.57
0.64
0.66
0.67
0.67
0.80
0.85
0.86
0.87
(c) Ten lowest scoring (Collins,
2003)-style dependencies occur-
ring more than 700 times

PP
PP
TAG
SBAR

NP

Table 8: Per category performance of the Berkeley parser on sentence lengths ≤ 70 (dev set, gold segmentation). (a) Of
the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in §2 that lexical ambiguity
explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN)
and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007),
but we ﬁnd that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by
e.g., (cid:3)S S S R(cid:4) and (cid:3)NP NP NP R(cid:4). (cid:3)NP NP PP R(cid:4) and (cid:3)NP NP ADJP R(cid:4) are both iDafa attachment.

input token, the segmentation is then performed
deterministically given the 1-best analysis.

Since guess and gold trees may now have dif-
ferent yields, the question of evaluation is com-
plex. Cohen and Smith (2007) chose a metric like
SParseval (Roark et al., 2006) that ﬁrst aligns the
trees and then penalizes segmentation errors with
an edit-distance metric. But we follow the more
direct adaptation of Evalb suggested by Tsarfaty
(2006), who viewed exact segmentation as the ul-
timate goal. Therefore, we only score guess/gold
pairs with identical character yields, a condition
that allows us to measure parsing, tagging, and
segmentation accuracy by ignoring whitespace.

Table 9 shows that MADA produces a high
quality segmentation, and that the effect of cas-
cading segmentation errors on parsing is only
1.92% F1. However, MADA is language-speciﬁc
and relies on manually constructed dictionaries.
Conversely, the lattice parser requires no linguis-
tic resources and produces segmentations of com-
parable quality. Nonetheless, parse quality is
much lower in the joint model because a lattice
is effectively a long sentence. A cell in the bottom
row of the parse chart is required for each poten-
tial whitespace boundary. As we have said, parse
quality decreases with sentence length. Finally,
we note that simple weighting gives nearly a 2%
F1 improvement, whereas Goldberg and Tsarfaty
(2008) found that unweighted lattices were more
effective for Hebrew.

STANFORD (Gold)
MADA
MADA+STANFORD
STANFORDJOINT
STANFORDJOINT+UNI

LP
81.64

—

79.44
76.13
77.09

LR
80.55

—

78.90
72.61
74.97

F1

81.09

—

79.17
74.33
76.01

95.81

Seg F1 Tag F1 Coverage
100.0%
100.0
96.42%
97.67
96.42%
97.67
94.73%
94.12
96.26
95.87%

94.27
90.13
92.23

—

Table 9: Dev set results for sentences of length ≤ 70. Cov-
erage indicates the fraction of hypotheses in which the char-
acter yield exactly matched the reference. Each model was
able to produce hypotheses for all input sentences. In these
experiments, the input lacks segmentation markers, hence the
slightly different dev set baseline than in Table 6.

7 Conclusion

By establishing signiﬁcantly higher parsing base-
lines, we have shown that Arabic parsing perfor-
mance is not as poor as previously thought, but
remains much lower than English. We have de-
scribed grammar state splits that signiﬁcantly im-
prove parsing performance, catalogued parsing er-
rors, and quantiﬁed the effect of segmentation er-
rors. With a human evaluation we also showed
that ATB inter-annotator agreement remains low
relative to the WSJ corpus. Our results suggest
that current parsing models would beneﬁt from
better annotation consistency and enriched anno-
tation in certain syntactic conﬁgurations.

Acknowledgments We thank Steven Bethard, Evan Rosen,
and Karen Shiells for material contributions to this work. We
are also grateful to Markus Dickinson, Ali Farghaly, Nizar
Habash, Seth Kulick, David McCloskey, Claude Reichard,
Ryan Roth, and Reut Tsarfaty for constructive discussions.
The ﬁrst author is supported by a National Defense Science
and Engineering Graduate (NDSEG) fellowship. This paper
is based on work supported in part by DARPA through IBM.
The content does not necessarily reﬂect the views of the U.S.
Government, and no ofﬁcial endorsement should be inferred.

402

References
Al-Batal, M. 1990. Connectives as cohesive elements in a
modern expository Arabic text. In Eid, Mushira and John
McCarthy, editors, Perspectives on Arabic Linguistics II.
John Benjamins.

Arun, A and F Keller. 2005. Lexicalization in crosslinguistic

probabilistic parsing: The case of French. In ACL.

Bikel, D M. 2004.

Intricacies of Collins’ parsing model.

Computational Linguistics, 30:479–511.

Chappelier, J-C, M Rajman, R Arages, and A Rozenknop.

1999. Lattice parsing for speech recognition. In TALN.

Chiang, D, M Diab, N Habash, O Rambow, and S Shareef.

2006. Parsing Arabic dialects. In EACL.

Clegg, A and A Shepherd. 2005. Evaluating and integrating
treebank parsers on a biomedical corpus. In ACL Work-
shop on Software.

Cohen, S and N A Smith. 2007. Joint morphological and

syntactic disambiguation. In EMNLP.

Collins, M, J Hajic, L Ramshaw, and C Tillmann. 1999. A

statistical parser for Czech. In ACL.

Collins, M. 2003. Head-Driven statistical models for natural
language parsing. Computational Linguistics, 29(4):589–
637.

Cowan, B and M Collins. 2005. Morphology and reranking

for the statistical parsing of Spanish. In NAACL.

Diab, M. 2007. Towards an optimal POS tag set for Modern

Standard Arabic processing. In RANLP.

Dickinson, M. 2005. Error Detection and Correction in An-
notated Corpora. Ph.D. thesis, The Ohio State University.
Dubey, A and F Keller. 2003. Probabilistic parsing for Ger-

man using sister-head dependencies. In ACL.

Dyer, C. 2009. Using a maximum entropy model to build

segmentation lattices for MT. In NAACL.

Fassi Fehri, A. 1993. Issues in the structure of Arabic clauses

and words. Kluwer Academic Publishers.

Gabbard, R and S Kulick. 2008. Construct state modiﬁcation

in the Arabic treebank. In ACL.

Goldberg, Y and R Tsarfaty. 2008. A single generative model
for joint morphological segmentation and syntactic pars-
ing. In ACL.

Green, S, C Sathi, and C D Manning. 2009. NP subject
detection in verb-initial Arabic clauses.
In Proc. of the
Third Workshop on Computational Approaches to Arabic
Script-based Languages (CAASL3).

Habash, N and O Rambow. 2005. Arabic tokenization, part-
of-speech tagging and morphological disambiguation in
one fell swoop. In ACL.

Habash, N and O Rambow. 2007. Arabic diacritization

through full morphological tagging. In NAACL.

Habash, N and R Roth. 2009. CATiB: The Columbia Arabic

Treebank. In ACL, Short Papers.

Habash, N and F Sadat. 2006. Arabic preprocessing schemes

for statistical machine translation. In NAACL.

Habash, N, B Dorr, and C Monz. 2006. Challenges in build-
ing an Arabic-English GHMT system with SMT compo-
nents. In EAMT.

Hajiˇc, J and P Zem´anek. 2004. Prague Arabic dependency

treebank: Development in data and tools. In NEMLAR.

Klein, D and C D Manning. 2002. Fast exact inference with

a factored model for natural language parsing. In NIPS.

Klein, D and C D Manning. 2003. Accurate unlexicalized

parsing. In ACL.

Kulick, S, R Gabbard, and M Marcus. 2006. Parsing the

Arabic Treebank: Analysis and improvements. In TLT.

K¨ubler, S. 2005. How do treebank annotation schemes inﬂu-
ence parsing results? Or how not to compare apples and
oranges. In RANLP.

Levy, R and C D Manning. 2003. Is it harder to parse Chi-

nese, or the Chinese treebank? In ACL.

Maamouri, M and A Bies. 2004. Developing an Arabic
Treebank: Methods, guidelines, procedures, and tools. In
Proc. of the Workshop on Computational Approaches to
Arabic Script-based Languages (CAASL1).

Maamouri, M, A Bies, T Buckwalter, and W Mekki. 2004.
The Penn Arabic Treebank: Building a large-scale anno-
tated Arabic corpus. In NEMLAR.

Maamouri, M, A Bies, and S Kulick. 2008. Enhancing the
Arabic Treebank: A collaborative effort toward new an-
notation guidelines. In LREC.

Maamouri, M, A Bies, S Krouna, F Gaddeche, and
B Bouziri. 2009a. Penn Arabic Treebank guidelines
v4.92. Technical report, Linguistic Data Consortium, Uni-
versity of Pennsylvania, August 5.

Maamouri, M, A Bies, and S Kulick. 2009b. Creating a
methodology for large-scale correction of treebank anno-
tation: The case of the Arabic Treebank. In MEDAR.

Marcus, M, M A Marcinkiewicz, and B Santorini. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313–330.

Petrov, S, L Barrett, R Thibaux, and D Klein. 2006. Learning
In

accurate, compact, and interpretable tree annotation.
ACL.

Petrov, S. 2009. Coarse-to-Fine Natural Language Process-

ing. Ph.D. thesis, University of California-Berkeley.

Rehbein, I and J van Genabith. 2007. Treebank annotation
schemes and parser evaluation for German. In EMNLP-
CoNLL.

Roark, B, M Harper, E Charniak, B Dorr, M Johnson, J G
Kahne, Y Liuf, Mari Ostendorf, J Hale, A Krasnyanskaya,
M Lease, I Shafran, M Snover, R Stewart, and L Yung.
2006. SParseval: Evaluation metrics for parsing speech.
In LREC.

Ryding, K. 2005. A Reference Grammar of Modern Standard

Arabic. Cambridge University Press.

Sampson, G and A Babarczy. 2003. A test of the leaf-
ancestor metric for parse accuracy. Natural Language En-
gineering, 9:365–380.

Toutanova, K, D Klein, C D Manning, and Y Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In NAACL.

Tsarfaty, R and K Sima’an. 2008. Relational-realizational

parsing. In COLING.

Tsarfaty, R. 2006. Integrated morphological and syntactic

disambiguation for Modern Hebrew. In ACL.

Zitouni, I, J S Sorensen, and R Sarikaya. 2006. Maximum

entropy based restoration of Arabic diacritics. In ACL.

Huang, Z and M Harper.

2009.

grammars with latent annotations across languages.
EMNLP.

Self-training PCFG
In

