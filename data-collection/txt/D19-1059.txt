











































Parameter-free Sentence Embedding via Orthogonal Basis


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 638–648,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

638

Parameter-free Sentence Embedding via Orthogonal Basis

Ziyi Yang1⇤, Chenguang Zhu2, and Weizhu Chen3

1Department of Mechanical Engineering, Stanford university
2Microsoft Speech and Dialogue Research Group

3Microsoft Dynamics 365 AI
ziyi.yang@stanford.edu, {chezhu, wzchen}@microsoft.com

Abstract

We propose a simple and robust non-
parameterized approach for building sentence
representations. Inspired by the Gram-
Schmidt Process in geometric theory, we build
an orthogonal basis of the subspace spanned
by a word and its surrounding context in a
sentence. We model the semantic meaning of
a word in a sentence based on two aspects.
One is its relatedness to the word vector sub-
space already spanned by its contextual words.
The other is the word’s novel semantic mean-
ing which shall be introduced as a new ba-
sis vector perpendicular to this existing sub-
space. Following this motivation, we develop
an innovative method based on orthogonal ba-
sis to combine pre-trained word embeddings
into sentence representations. This approach
requires zero parameters, along with efficient
inference performance. We evaluate our ap-
proach on 11 downstream NLP tasks. Our
model shows superior performance compared
with non-parameterized alternatives and it is
competitive to other approaches relying on ei-
ther large amounts of labelled data or pro-
longed training time.

1 Introduction

The concept of word embeddings has been preva-
lent in NLP community in recent years, as they can
characterize semantic similarity between any pair
of words, achieving promising results in a large
number of NLP tasks (Mikolov et al., 2013; Pen-
nington et al., 2014; Salle et al., 2016). However,
due to the hierarchical nature of human language,
it is not sufficient to comprehend text solely based
on isolated understanding of each word. This has
prompted a recent rise in search for semantically
robust embeddings for longer pieces of text, such
as sentences and paragraphs.

⇤ Most of the work was done during internship at Mi-
crosoft

Based on learning paradigms, the existing ap-
proaches to sentence embeddings can be catego-
rized into two categories: i) parameterized meth-
ods and ii) non-parameterized methods.

Parameterized sentence embeddings. These
models are parameterized and require training to
optimize their parameters. SkipThought (Kiros
et al., 2015) is an encoder-decoder model that pre-
dicts adjacent sentences. Pagliardini et al. (2018)
proposes an unsupervised model, Sent2Vec, to
learn an n-gram feature in a sentence to predict the
center word from the surrounding context. Quick
thoughts (QT) (Logeswaran and Lee, 2018) re-
places the encoder with a classifier to predict con-
text sentences from candidate sequences. Khodak
et al. (2018) proposes à la carte to learn a linear
mapping to reconstruct the center word from its
context. Conneau et al. (2017) generates the sen-
tence encoder InferSent using Natural Language
Inference (NLI) dataset. Universal Sentence En-
coder (Yang et al., 2018; Cer et al., 2018) uti-
lizes the emerging transformer structure (Vaswani
et al., 2017; Devlin et al., 2018) that has been
proved powerful in various NLP tasks. The model
is first trained on large scale of unsupervised data
from Wikipedia and forums, and then trained on
the Stanford Natural Language Inference (SNLI)
dataset. Wieting and Gimpel (2017b) propose
the gated recurrent averaging network (GRAN),
which is trained on Paraphrase Database (PPDB)
and English Wikipedia. Subramanian et al. (2018)
leverages a multi-task learning framework to gen-
erate sentence embeddings. Wieting et al. (2015a)
learns the paraphrastic sentence representations as
the simple average of updated word embeddings.

Non-parameterized sentence embedding.
Recent work (Arora et al., 2017) shows that,
surprisingly, a weighted sum or transformation
of word representations can outperform many
sophisticated neural network structures in sen-



639

tence embedding tasks. These methods are
parameter-free and require no further training
upon pre-trained word vectors. Arora et al. (2017)
constructs a sentence embedding called SIF as a
sum of pre-trained word embeddings, weighted by
reverse document frequency. Ethayarajh (2018)
builds upon the random walk model proposed in
SIF by setting the probability of word generation
inversely related to the angular distance between
the word and sentence embeddings. Rücklé et al.
(2018) concatenates different power mean word
embeddings as a sentence vector in p-mean. As
these methods do not have a parameterized model,
they can be easily adapted to novel text domains
with both fast inference speed and high-quality
sentence embeddings. In view of this trend, our
work aims to further advance the frontier of this
group and make its new state-of-the-art.

In this paper, we propose a novel sen-
tence embedding algorithm, Geometric Embed-
ding (GEM), based entirely on the geometric
structure of word embedding space. Given a d-
dim word embedding matrix A 2 Rd⇥n for a sen-
tence with n words, any linear combination of the
sentence’s word embeddings lies in the subspace
spanned by the n word vectors. We analyze the
geometric structure of this subspace in Rd. When
we consider the words in a sentence one-by-one
in order, each word may bring in a novel orthog-
onal basis to the existing subspace. This new ba-
sis can be considered as the new semantic mean-
ing brought in by this word, while the length of
projection in this direction can indicate the inten-
sity of this new meaning. It follows that a word
with a strong intensity should have a larger influ-
ence in the sentence’s meaning. Thus, these in-
tensities can be converted into weights to linearly
combine all word embeddings to obtain the sen-
tence embedding. In this paper, we theoretically
frame the above approach in a QR factorization of
the word embedding matrix A. Furthermore, since
the meaning and importance of a word largely de-
pends on its close neighborhood, we propose the
sliding-window QR factorization method to cap-
ture the context of a word and characterize its sig-
nificance within the context.

In the last step, we adapt a similar approach as
Arora et al. (2017) to remove top principal vec-
tors before generating the final sentence embed-
ding. This step is to ensure commonly shared
background components, e.g. stop words, do not

bias sentence similarity comparison. As we build
a new orthogonal basis for each sentence, we pro-
pose to have disparate background components
for each sentence. This motivates us to put for-
ward a sentence-specific principal vector removal
method, leading to better empirical results.

We evaluate our algorithm on 11 NLP tasks.
Our algorithm outperforms all non-parameterized
methods and many parameterized approaches in
10 tasks. Compared to SIF (Arora et al., 2017), the
performance is boosted by 5.5% on STS bench-
mark dataset, and by 2.5% on SST dataset. Plus,
the running time of our model compares favorably
with existing models.

The rest of this paper is organized as following.
In Section 2, we describe our sentence embedding
algorithm GEM. We evaluate our model on vari-
ous tasks in Section 3 and Section 4. Finally, we
summarize our work in Section 5. Our implemen-
tation is available online1.

2 Approach

We introduce three scores to quantify the impor-
tance of a word, as will be explained in this sec-
tion. First, novelty score ↵n measures the por-
tion of the new semantic meaning in a word. Sec-
ond, significance score ↵s describes the align-
ment between the new semantic meaning and
the sentence-level meaning. Finally, uniqueness
score ↵u examines the uniqueness of the new se-
mantic meaning in the corpus level.

2.1 Quantify New Semantic Meaning
Let us consider the idea of word embeddings
(Mikolov et al., 2013), where a word wi is pro-
jected as a vector vwi 2 Rd. Any sequence of
words can be viewed as a subspace in Rd spanned
by its word vectors. Before the appearance of
the ith word, S is a subspace in Rd spanned by
{vw1 ,vw2 , ...,vwi�1}. Its orthonormal basis is
{q1, q2, ..., qi�1}. The embedding vwi of the ith
word wi can be decomposed into

vwi =
i�1X

j=1

rjqj + riqi

rj = q
T
j vwi

ri = kvwi �
i�1X

j=1

rjqjk2

(1)

1https://github.com/ziyi-yang/GEM



640

where
Pi�1

j=1 rjqj is the part in vwi that resides in
subspace S, and qi is orthogonal to S and is to be
added to S. The above algorithm is also known
as Gram-Schmidt Process. In the case of rank
deficiency, i.e., vwi is already a linear combination
of {q1, q2, ...qi�1}, qi is a zero vector and ri = 0.
In matrix form, this process is also known as QR
factorization, defined as follows.
QR factorization. Define an embedding matrix
of n words as A = [A:,1,A:,2, ...,A:,n] 2 Rd⇥n,
where A:,i is the embedding of the ith word wi in a
word sequence (w1, . . . , wi, . . . , wn). A 2 Rd⇥n
can be factorized into A = QR, where the non-
zero columns in Q 2 Rd⇥n are the orthonormal
basis, and R 2 Rn⇥n is an upper triangular ma-
trix.

The process above computes the novel semantic
meaning of a word w.r.t all preceding words. As
the meaning of a word influences and is influenced
by its close neighbors, we now calculate the novel
orthogonal basis vector qi of each word wi in its
neighborhood, rather than only w.r.t the preceding
words.

Definition 1 (Contextual Window Matrix)
Given a word wi, and its m-

neighborhood window inside the sentence
(wi�m, . . . , wi�1, wi, wi+1, . . . , wi+m) , define
the contextual window matrix of word wi as:

Si = [vwi�m ...vwi�1 ,vwi+1 ...vwi+m ,vwi ] (2)

Here we shuffle vwi to the end of Si to compute
its novel semantic information compared with its
context. Now the QR factorization of Si is

Si = QiRi (3)

Note that qi is the last column of Qi, which is also
the new orthogonal basis vector to this contextual
window matrix.

Next, in order to generate the embedding for a
sentence, we will assign a weight to each of its
words. This weight should characterize how much
new and important information a word brings to
the sentence. The previous process yields the or-
thogonal basis vector qi. We propose that qi repre-
sents the novel semantic meaning brought by word
wi. We will now discuss how to quantify i) the
novelty of qi to other meanings in wi, ii) the sig-
nificance of qi to its context, and iii) the corpus-
wise uniqueness of qi w.r.t the whole corpus.

2.2 Novelty
We propose that a word wi is more important to a
sentence if its novel orthogonal basis vector qi is
a large component in vwi , quantified by the pro-
posed novelty score ↵n. Let r denote the last col-
umn of Ri, and r�1 denote the last element of r,
↵n is defined as:

↵n = exp(
kqik2
kvwik2

) = exp(
r�1
krk2

) (4)

Note that kqik2 = r�1 and kvwik2 = krk2. One
can show that ↵n is the exponential of the nor-
malized distance between vwi and the subspace
spanned by its context.

2.3 Significance
The significance of a word is related to how se-
mantically aligned it is to the meaning of its con-
text. To identify principal directions, i.e. mean-
ings, in the contextual window matrix Si, we em-
ploy Singular Value Decomposition.
Singular Value Decomposition. Given a ma-
trix A 2 Rd⇥n, there exists U 2 Rd⇥n
with orthogonal columns, diagonal matrix ⌃ =
diag(�1, ...,�n), �1 � �2 � ... � �n � 0, and
orthogonal matrix V 2 Rn⇥n, such that A =
U⌃V T .

The columns of U , {U:,j}nj=1, are an orthonor-
mal basis of A’s columns subspace and we pro-
pose that they represent a set of semantic mean-
ings from the context. Their corresponding singu-
lar values {�j}nj=1, denoted by �(A), represent
the importance associated with {U:,j}nj=1. The
SVD of wi’s contextual window matrix is Si =
U i⌃iV iT 2 Rd⇥(2m+1). It follows that qTi U i is
the coordinate of qi in the basis of {U i:,j}

2m+1
j=1 .

Intuitively, a word is more important if its novel
semantic meaning has a better alignment with
more principal meanings in its contextual window.
This can be quantified as k�(Si) � (qTi U i)k2,
where � denotes element-wise product. There-
fore, we define the significance of wi in its context
to be:

↵s =
k�(Si)� (qTi U i)k2

2m+ 1
(5)

It turns out ↵s can be rewritten as

↵s =
kqTi U i⌃ik2
2m+ 1

=
kqTi U i⌃iV ik2

2m+ 1

=
kqTi Sik2
2m+ 1

=
qTi vwi
2m+ 1

=
r�1

2m+ 1

(6)



641

and we use the fact that V i is an orthogonal
matrix and qi is orthogonal to all but the last col-
umn of Si, vwi . Therefore, ↵s is essentially the
distance between wi and the context hyper-plane,
normalized by the context size.

Although ↵s and ↵n look alike in mathemat-
ics form, they model distinct quantities in word
wi against its contextual window. ↵n is a func-
tion of kqik2 divided by kwik2, i.e., the portion
of the new semantic meaning in word wi. In con-
trast, eq. (6) shows that ↵s equals kqik2 divided
by a constant, namely ↵s quantifies the absolute
magnitude of the new semantic meaning qi.

2.4 Corpus-wise Uniqueness
Similar to the idea of inverse document frequency
(IDF) (Sparck Jones, 1972), a word that is com-
monly present in the corpus is likely to be a stop
word, thus its corpus-wise uniqueness is small. In
our solution, we compute the principal directions
of the corpus and then measure their alignment
with the novel orthogonal basis vector qi. If there
is a high alignment, wi will be assigned a rela-
tively low corpus-wise uniqueness score, and vice
versa.

2.4.1 Compute Principal Directions of
Corpus

In Arora et al. (2017), given a corpus contain-
ing a set of N sentences, an embedding matrix
X = [x1,x2, . . . ,xN ] 2 Rd⇥N is generated,
where xi is the sentence embedding for the i-th
sentence in the corpus, computed by SIF algo-
rithm. Then principal vectors of X are computed
and projections onto the principal vectors are re-
moved from each sentence embedding xi.

In contrast to Arora et al. (2017), we do not
form the embedding matrix after we obtain the
final sentence representation. Instead, we obtain
an intermediate coarse-grained embedding matrix
Xc = [g1, . . . , gN ] as follows. Suppose the
SVD of the sentence matrix of the ith sentence is
S = [vw1 , . . . ,vwn ] = U⌃V

T . Then the coarse-
grained embedding for the ith sentence is defined
as:

gi =
nX

j=1

f(�j)U:,j (7)

where f(�j) is a monotonically increasing func-
tion. We then compute the top K principal vectors
{d1, ...,dK} of Xc, with singular values �1 �
�2 � ... � �K .

2.4.2 Uniqueness Score
In contrast to Arora et al. (2017), we select differ-
ent principal vectors of Xc for each sentence, as
different sentences may have disparate alignments
with the corpus. For each sentence, {d1, ...,dK}
are re-ranked in descending order of their corre-
lation with sentence matrix S. The correlation is
defined as:

oi = �ikSTdik2, 1  i  K (8)

Next, the top h principal vectors after re-
ranking based on oi are selected: D =
{dt1 , ...,dth}, with ot1 � ot2 � ... � oth and their
singular values in Xc are �d = [�t1 , ...,�th ] 2
Rh.

Finally, a word wi with new semantic mean-
ing vector qi in this sentence will be assigned a
corpus-wise uniqueness score:

↵u = exp (�k�d � (qTi D)k2/h) (9)

This ensures that common stop words will have
their effect diminished since their embeddings are
closely aligned with the corpus’ principal direc-
tions.

2.5 Sentence Vector
A sentence vector cs is computed as a weighted
sum of its word embeddings, where the weights
come from three scores: a novelty score (↵n), a
significance score (↵s) and a corpus-wise unique-
ness score (↵u).

↵i = ↵n + ↵s + ↵u

cs =
X

i

↵ivwi
(10)

We provide a theoretical explanation of Equa-
tion (10) in Appendix.

Sentence-Dependent Removal of Principal
Components. Arora et al. (2017) shows that given
a set of sentence vectors, removing projections
onto the principal components of the spanned sub-
space can significantly enhance the performance
on semantic similarity task. However, as each sen-
tence may have a different semantic meaning, it
could be sub-optimal to remove the same set of
principal components from all sentences.

Therefore, we propose the sentence-dependent
principal component removal (SDR), where we
re-rank top principal vectors based on correlation



642

with each sentence. Using the method from Sec-
tion 2.4.2, we obtain D = {dt1 , ...,dtr} for a sen-
tence s. The final embedding of this sentence is
then computed as:

cs  cs �
rX

j=1

(dTtjcs)dtj (11)

Ablation experiments show that sentence-
dependent principal component removal can
achieve better result. The complete algorithm is
summarized in Algorithm 1 with an illustration in
Figure 1.

2.6 Handling of out-of-vocabulary Words
In many NLP algorithms, the out-of-vocabulary
(OOV) words are projected to a special “UNK”
token. However, in this way, different OOV words
with drastically different meanings will share the
same embedding. To fix this problem, we change
this projection method by mapping OOVs to pre-
trained in-vocabulary words, based on a hash func-
tion SHA-256 of its characters. In this way, two
different OOV words will almost certainly have
different embeddings. In the experiments, we ap-
ply this OOV projection technique in both STS-B
and CQA tasks.

3 Experiments

3.1 Semantic Similarity Tasks: STS
Benchmark

We evaluate our model on the STS Benchmark
(Cer et al., 2017), a sentence-level semantic simi-
larity dataset. The goal for a model is to predict a
similarity score of two sentences given a sentence
pair. The evaluation is by the Pearson’s coefficient
r between human-labeled similarity (0 - 5 points)
and predictions.
Experimental settings. We report two versions
of our model, one only using GloVe word vectors
(GEM + GloVe), and the other using word vec-
tors concatenated from LexVec, fastText and PSL
(Wieting et al., 2015b) (GEM + L.F.P). The final
similarity score is computed as an inner product of
normalized sentence vectors. Since our model is
non-parameterized, it does not utilize any informa-
tion from the dev set when evaluating on the test
set and vice versa. Hyper-parameters are chosen at
m = 7, h = 17, K = 45, and t = 3 by conducing
hyper-parameters search on dev set. Results on the
dev and test set are reported in Table 1. As shown,

Non-parameterized models dev test

GEM + L.F.P (ours) 83.5 78.4
GEM + LexVec (ours) 81.9 76.5
SIF (Arora et al., 2017) 80.1 72.0
uSIF (Ethayarajh, 2018) 84.2 79.5

LexVec 58.78 50.43
L.F.P 62.4 52.0

word2vec skipgram 70.0 56.5
Glove 52.4 40.6
ELMo 64.6 55.9

Parameterized models

PARANMT-50M (Wieting and Gimpel, 2017a) - 79.9
Reddit + SNLI (Yang et al., 2018) 81.4 78.2

GRAN (Wieting and Gimpel, 2017b) 81.8 76.4
InferSent (Conneau et al., 2017) 80.1 75.8

Sent2Vec (Pagliardini et al., 2018) 78.7 75.5
Paragram-Phrase (Wieting et al., 2015a) 73.9 73.2

Table 1: Pearson’s r ⇥ 100 on STSB. Best results are
in bold.

GEM + L.F.P (ours) 49.11
Reddit + SNLI tuned 47.44

KeLP-contrastive1 49.00
SimBow-contrastive2 47.87

SimBow-primary 47.22

Table 2: MAP on CQA subtask B.

on the test set, our model has a 6.4% higher score
compared with another non-parameterized model
SIF, and 26.4% higher than the baseline of averag-
ing L.F.P word vectors. It also outperforms all pa-
rameterized models including GRAN, InferSent,
Sent2Vec and Reddit+SNLI.

3.2 Semantic Similarity Tasks: CQA
We evaluate our model on subtask B of the Se-
mEval Community Question Answering (CQA)
task, another semantic similarity dataset. Given
an original question Qo and a set of the first ten re-
lated questions (Q1, ..., Q10) retrieved by a search
engine, the model is expected to re-rank the related
questions according to their similarity with respect
to the original question. Each retrieved question
Qi is labelled “PerfectMatch”, “Relevant” or “Ir-
relevant”, with respect to Qo. Mean average pre-
cision (MAP) is used as the evaluation measure.

We encode each question text into a unit vec-
tor u. Retrieved questions {Qi}10i=1 are ranked
according to their cosine similarity with Qo. Re-
sults are shown in Table 2. For comparison, we
include results from the best models in 2017 com-
petition: SimBow (Charlet and Damnati, 2017),



643

!"!"#$ !"%$ !&

= 0.1 = 0.2 = 0.3

'& =
||[0.1. /"

0
, 0.2. /"

3
, 0.1. /"

4
, … ]||3

27 + 1

... + 0.1- 0.4= 0.2

'9 = exp(
0.1

||[0.2, −0.4, … , 0.1]||3
)

/" ∈ BC×(3$%0)

E" ∈ BC× 3$%0

(/" = E"F"G"H)

I" ∈ BC

!0

= 0.5 = 0.6 = -0.1

'J = exp(−|| 0.5.C0, 0.6.C3, −0.1.C4, … ||3/ℎ)

Re-rank
Select

Figure 1: An illustration of GEM algorithm. Top middle: The sentence to encode, with words w1 to wn. The
contextual window of word wi is inside the dashed line. Bottom middle: Form contextual window matrix Si for
wi, compute qi and novelty score ↵n (Section 2.1 and Section 2.2). Bottom left: SVD of Si and compute the
significance score ↵s (Section 2.3). Bottom right: Re-rank and select from principal components (orange blocks)
and compute uniqueness score ↵u (Section 2.4).

Algorithm 1 Geometric Embedding (GEM)
Inputs:

A set of sentences S , vocabulary V , word embeddings {vw 2 Rd |w 2 V}
Outputs:

Sentence embeddings {cs 2 Rd | s 2 S}
for ith sentence s in S do

Form matrix S 2 Rd⇥n, S:,j = vwj and wj is the jth word in s
The SVD is S = U⌃V T

The ith column of the coarse-grained sentence embedding matrix Xc:,i is U(�(S))
3

end for
Take first K singular vectors {d1, ...,dK} and singular values �1 � �2 � ... � �K of Xc
for sentence s in S do

Re-rank {d1, ...,dK} in descending order by oi = �ikSTdik2, 1  i  K.
Select top h principal vectors as D = [dt1 , ...,dth ], with singular values �d = [�t1 ....,�th ].
for word wi in s do

Si = [vwi�m , ...,vwi�1 ,vwi+1 , ...,vwi+m ,vwi ] is the contextual window matrix of wi.
Do QR decomposition Si = QiRi, let qi and r denote the last column of Qi and Ri

↵n = exp(r�1/krk2),↵s = r�1/(2m+ 1),↵u = exp (�k�d � (qTi D)k2/h)
↵i = ↵n + ↵s + ↵u

end for
cs =

P
vi2s ↵ivwi

Principal vectors removal: cs  cs �DDTcs
end for

KeLP (Filice et al., 2017), and Reddit + SNLI
tuned. Note that all three benchmark models re-
quire learning on CQA training set, and SimBow
and KeLP leverage optional features including us-
age of comments and user profiles. In comparison,
our model only uses the question text without any
training. Our model clearly outperforms Reddit +

SNLI tuned, SimBow-primary and KeLP model.

3.3 Supervised tasks

We further test our model on nine supervised
tasks, including seven classification tasks: movie
review (MR) (Pang and Lee, 2005), Stanford
Sentiment Treebank (SST) (Socher et al., 2013),



644

question-type classification (TREC) (Voorhees
and Dang, 2003), opinion polarity (MPQA)
(Wiebe et al., 2005), product reviews (CR) (Hu
and Liu, 2004), subjectivity/objectivity classifica-
tion (SUBJ) (Pang and Lee, 2004) and paraphrase
identification (MRPC) (Dolan et al., 2004). We
also evaluate on two entailment and semantic re-
latedness tasks: SICK similarity (SICK-R) and the
SICK entailment (SICK-E) (Marelli et al., 2014).
The sentence embeddings generated are fixed and
only the downstream task-specific neural structure
is learned. For classification tasks, a linear classi-
fier is trained on top, following Kiros et al. (2015),
and classification accuracy are reported. For relat-
edness tasks, we follow Tai et al. (2015) to train a
logistic regression to learn the probability distribu-
tion of relatedness scores, and we report Pearson’s
correlation. The four hyper-parameters are chosen
the same as those in STS benchmark experiment.
For fair comparison, embeddings models are di-
vided into two categories: non-parameterized and
parameterized ones, as described in section 1. Re-
sults are shown in Table 3.

GEM outperforms all other non-parameterized
sentence embedding models, including SIF, p-
mean (Rücklé et al., 2018), and BOW on
GloVe. The consistent superior performance
again demonstrates GEM’s advantage on weight-
ing scheme. It also compares favorably with most
of parameterized models, including à la carte
(Khodak et al., 2018), FastSent (Hill et al., 2016),
InferSent, QT, Sent2Vec, SkipThought-LN (with
layer normalization) (Kiros et al., 2015), SDAE
(Hill et al., 2016), STN (Subramanian et al., 2018)
and USE (Yang et al., 2018). Note that sentence
representations generated by GEM have much
smaller dimension compared to most of bench-
mark models, and the subsequent neural structure
has fewer trainable parameters. This observation
suggests that local multi-word level information
in sentences has already provided revealing infor-
mation for sophisticated downstream tasks. The
fact that GEM does well on several classification
tasks (e.g. TREC and SUBJ) indicates that the
proposed weight scheme is able to recognize im-
portant words sentences. Also, GEM’s competi-
tive performance on sentiment tasks shows that ex-
ploiting the geometric structures of two sentence
subspaces is semantically informative.

4 Discussion

Comparison with Arora et al. (2017). We would
like to point out that although sharing the idea of
modelling the sentence as the weighted sum of
its word vectors, GEM is substantially different
from Arora et al. (2017). First, we adopt well-
established numerical linear algebra to quantify
the semantic meaning and importance of words
in the sentences context. And this new approach
proves to be effective. Second, the weights in
SIF (and uSIF) are calculated from the statistic
of vocabularies on very large corpus (wikipedia).
In contrast, the weights in GEM are directly
computed from the sentences themselves along
with the dataset, independent with prior statisti-
cal knowledge of language or vocabularies. Fur-
thermore, the components in GEM’s weights are
derived from numerical linear algebra eq. (4) to
(9), while SIF directly includes a hyper-parameter
term in its weight scheme, i.e. its smooth term.

Robustness and Effectiveness. Besides exper-
iments mentioned above, we also test the robust-
ness and effectiveness GEM on several simple but
non-trivial examples. These experiments demon-
strate that GEM is quite insensitive against the re-
moval of non-important stop words. Also GEM
can correctly assign higher weights for words with
more significant semantic meanings in the sen-
tence.

We first test the robustness of GEM by remov-
ing one non-important stop word in a sentence and
computed the similarity between the original sen-
tence and the one after removal. For example:

• original sentence: “the first tropical system
to slam the US this year is expected to make
landfall as a hurricane”

• remove 7 stop words: “first tropical system
slam US this year expected make landfall
hurricane”

The cosine similarity between these two sentences
given by GEM is 0.954. Even though aggressively
removing 7 stop words, GEM still assigns pretty
similar embeddings for these two sentences.

We further demonstrate that GEM does assign
higher weights to words with more significant se-
mantic meanings. Consider the following sen-
tence: ”there are two ducks swimming in the
river”. Weights assigned by GEM are (sorted
from high to low): [ducks: 4.93, river:4.72 ,
swimming: 4.70, two: 3.87, are: 3.54, there:



645

Model Dim Trainingtime (h) MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E

Non-parameterized models

GEM + L.F.P (ours) 900 0 79.8 82.5 93.8 89.9 84.7 91.4 75.4/82.9 86.5 86.2
GEM + GloVe (ours) 300 0 78.8 81.1 93.1 89.4 83.6 88.6 73.4/82.3 86.3 85.3

SIF 300 0 77.3 78.6 90.5 87.0 82.2 78.0 - 86.0 84.6
uSIF 300 0 - - - - 80.7 - - 83.8 81.1

p-mean 3600 0 78.4 80.4 93.1 88.9 83.0 90.6 - - -
GloVe BOW 300 0 78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 80.0 78.6

Paraemterized models

InferSent 4096 24 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 88.4 86.3
Sent2Vec 700 6.5 75.8 80.3 91.1 85.9 - 86.4 72.5/80.8 - -

SkipThought-LN 4800 336 79.4 83.1 93.7 89.3 82.9 88.4 - 85.8 79.5
FastSent 300 2 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - -
à la carte 4800 N/A 81.8 84.3 93.8 87.6 86.7 89.0 - - -

SDAE 2400 192 74.6 78.0 90.8 86.9 - 78.4 73.7/80.7 - -
QT 4800 28 82.4 86.0 94.8 90.2 87.6 92.4 76.9/84.0 87.4 -

STN 4096 168 82.5 87.7 94.0 90.9 83.2 93.0 78.6/84.4 88.8 87.8
USE 512 N/A 81.36 86.08 93.66 87.14 86.24 96.60 - - -

Table 3: Results on supervised tasks. Sentence embeddings are fixed for downstream supervised tasks. Best results
for each task are underlined, best results from models in the same category are in bold. SIF results are extracted
from Arora et al. (2017) and Rücklé et al. (2018), and training time is collected from Logeswaran and Lee (2018).

3.23, in:3.04, the:2.93]. GEM successfully assigns
higher weight to informative words like ducks and
river, and downplay stop words like the and there.
More examples can be found in the Appendix.

Ablation Study. As shown in in Table 4, ev-
ery GEM weight (↵n,↵s,↵u) and proposed prin-
cipal components removal methods contribute to
the performance. As listed on the left, adding
GEM weights improves the score by 8.6% on
STS dataset compared with averaging three con-
catenated word vectors. The sentence-dependent
principal component removal (SDR) proposed in
GEM improves 1.7% compared to directly remov-
ing the top h corpus principal components (SIR).
Using GEM weights and SDR together yields an
overall improvement of 21.1%. As shown on the
right in Table 4, every weight contributes to the
performance of our model. For example, three
weights altogether improve the score in SUBJ task
by 0.38% compared with only using ↵n.

Sensitivity Study. We evaluate the effect of
all four hyper-parameters in our model: the win-
dow size m in the contextual window matrix, the
number of candidate principal components K, the
number of principal components to remove h, and
the power of the singular value in coarse sentence
embedding, i.e. the power t in f(�j) = �tj in
Equation (7). We sweep the hyper-parameters and
test on STSB dev set, SUBJ, and MPQA. Unspec-
ified parameters are fixed at m = 7, K = 45,
h = 17 and t = 3. As shown in Figure 2,

Configurations STSB dev SUBJ

Mean of L.F.P 62.4 -
GEM weights 71.0 -

GEM weights + SIR 81.8 -
GEM weights + SDR 83.5 -

↵n + SDR 81.6 93.42
↵n,↵s + SDR 81.9 93.6

↵n,↵s,↵u + SDR 83.5 93.8

Table 4: Comparison of different configurations
demonstrates the effectiveness of our model on STSB
dev set and SUBJ. SDR stands for sentence-dependent
principal component removal in Section 2.4.2. SIR
stands for sentence-independent principal component
removal, i.e. directly removing top h corpus principal
components from the sentence embedding.

our model is quite robust with respect to hyper-
parameters.

Inference speed. We also compare the infer-
ence speed of our algorithm on the STSB test set
with the benchmark models SkipThought and In-
ferSent. SkipThought and InferSent are run on a
NVIDIA Tesla P100 GPU, and our model is run on
a CPU (Intel Xeon CPU E5-2690 v4 @2.60GHz).
For fair comparison, batch size in InferSent and
SkipThought is set to be 1. The results are shown
in Table 5. It shows that without acceleration from
GPU, our model is still faster than InferSent and is
54% faster than SkipThought.



646

● ● ● ● ●

● ● ● ● ●

● ● ● ● ●

80

85

90

5 6 7 8 9
window size

Pe
rfo

rm
an

ce

● ● ● ● ●

● ● ● ● ●

● ● ● ● ●

80

85

90

35 45 55 65 75
K

●
● ● ● ●

● ● ●
● ●

● ● ● ● ●

80

85

90

8 11 14 17 20
h

Pe
rfo

rm
an

ce

● ● ● ● ●

● ● ● ● ●

● ● ● ● ●

80

85

90

1 2 3 4 5
power of σ

●

●

●

MPQA
STSB
SUBJ

Figure 2: Sensitivity tests on four hyper-parameters,
the window size m in contextual window matrix, the
number of candidate principal components K, the
number of principal components to remove h, and the
exponential power of singular value in coarse sentence
embedding.

Average run time (s) Variance

GEM (CPU) 20.08 0.23
InferSent(GPU) 21.24 0.15

SkipThought (GPU) 43.36 0.10

Table 5: Run time of GEM, InferSent and SkipThought
on encoding sentences in STSB test set. GEM is run on
CPU, InferSent and SkipThought is run on GPU. Data
are collected from 5 trials.

5 Conclusions

We proposed a simple non-parameterized method
to generate sentence embeddings, based entirely
on the geometric structure of the subspace spanned
by word embeddings. Our sentence embedding
evolves from the new orthogonal basis vector
brought in by each word, which represents novel
semantic meaning. The evaluation shows that our
method not only sets up the new state-of-the-art of
non-parameterized models but also performs com-
petitively when compared with models requiring
either large amount of training data or prolonged
training time. In future work, we plan to consider
subwords into the model and explore more geo-
metric structures in sentences.

Acknowledgments

We would like to thank Jade Huang for proofread-
ing the paper and helpful writing suggestions. We
also acknowledge the anonymous reviewers for
their valuable feedback.

References
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.

A simple but tough-to-beat baseline for sentence em-
beddings. International Conference on Learning
Representations.

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
ing text with the natural language toolkit. ” O’Reilly
Media, Inc.”.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. arXiv preprint
arXiv:1708.00055.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
et al. 2018. Universal sentence encoder. arXiv
preprint arXiv:1803.11175.

Delphine Charlet and Geraldine Damnati. 2017. Sim-
bow at semeval-2017 task 3: Soft-cosine semantic
similarity between questions for community ques-
tion answering. In Proceedings of the 11th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2017), pages 315–319.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th international conference
on Computational Linguistics, page 350. Associa-
tion for Computational Linguistics.

Kawin Ethayarajh. 2018. Unsupervised random walk
sentence embeddings: A strong but simple baseline.
In Proceedings of The Third Workshop on Represen-
tation Learning for NLP, pages 91–100.

https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070


647

Simone Filice, Giovanni Da San Martino, and Alessan-
dro Moschitti. 2017. Kelp at semeval-2017 task 3:
Learning pairwise patterns in community question
answering. In Proceedings of the 11th International
Workshop on Semantic Evaluation (SemEval-2017),
pages 326–333.

Felix Hill, Kyunghyun Cho, and Anna Korhonen.
2016. Learning distributed representations of
sentences from unlabelled data. arXiv preprint
arXiv:1602.03483.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Mikhail Khodak, Nikunj Saunshi, Yingyu Liang,
Tengyu Ma, Brandon Stewart, and Sanjeev Arora.
2018. A la carte embedding: Cheap but effective in-
duction of semantic feature vectors. arXiv preprint
arXiv:1805.05388.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.

Lajanugen Logeswaran and Honglak Lee. 2018. An
efficient framework for learning sentence represen-
tations. International Conference on Learning Rep-
resentations.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th international
workshop on semantic evaluation (SemEval 2014),
pages 1–8.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
2018. Unsupervised Learning of Sentence Embed-
dings using Compositional n-Gram Features. In
NAACL 2018 - Conference of the North American
Chapter of the Association for Computational Lin-
guistics.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd annual meeting on association for computa-
tional linguistics, pages 115–124. Association for
Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

A. Rücklé, S. Eger, M. Peyrard, and I. Gurevych. 2018.
Concatenated p-mean Word Embeddings as Univer-
sal Cross-Lingual Sentence Representations. ArXiv
e-prints.

Alexandre Salle, Aline Villavicencio, and Marco Idiart.
2016. Matrix factorization using window sampling
and negative sampling for improved word represen-
tations. CoRR, abs/1606.00819.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing,
pages 1631–1642.

Karen Sparck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of documentation, 28(1):11–21.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
large scale multi-task learning. arXiv preprint
arXiv:1804.00079.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Ellen M Voorhees and Hoa Trang Dang. 2003.
Overview of the trec 2003 question answering track.
In TREC, volume 2003, pages 54–68.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165–210.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015a. Towards universal paraphrastic sen-
tence embeddings. In International Conference on
Learning Representation.

http://arxiv.org/abs/1803.01400
http://arxiv.org/abs/1803.01400


648

John Wieting, Mohit Bansal, Kevin Gimpel, Karen
Livescu, and Dan Roth. 2015b. From paraphrase
database to compositional paraphrase model and
back. arXiv preprint arXiv:1506.03487.

John Wieting and Kevin Gimpel. 2017a. Paranmt-50m:
Pushing the limits of paraphrastic sentence embed-
dings with millions of machine translations. arXiv
preprint arXiv:1711.05732.

John Wieting and Kevin Gimpel. 2017b. Revisiting re-
current networks for paraphrastic sentence embed-
dings. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.

Y. Yang, S. Yuan, D. Cer, S.-y. Kong, N. Con-
stant, P. Pilar, H. Ge, Y.-H. Sung, B. Strope, and
R. Kurzweil. 2018. Learning Semantic Textual Sim-
ilarity from Conversations. ArXiv e-prints.

http://arxiv.org/abs/1804.07754
http://arxiv.org/abs/1804.07754

