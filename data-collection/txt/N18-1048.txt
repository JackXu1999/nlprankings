



















































Post-Specialisation: Retrofitting Vectors of Words Unseen in Lexical Resources


Proceedings of NAACL-HLT 2018, pages 516–527
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Post-Specialisation: Retrofitting Vectors of Words
Unseen in Lexical Resources

Ivan Vulić1, Goran Glavaš2, Nikola Mrkšić3, Anna Korhonen1
1 Language Technology Lab, University of Cambridge

2 Data and Web Science Group, University of Mannheim
3 PolyAI

{iv250,alk23}@cam.ac.uk
goran@informatik.uni-mannheim.de nikola@poly-ai.com

Abstract

Word vector specialisation (also known as
retrofitting) is a portable, light-weight ap-
proach to fine-tuning arbitrary distributional
word vector spaces by injecting external
knowledge from rich lexical resources such
as WordNet. By design, these post-processing
methods only update the vectors of words oc-
curring in external lexicons, leaving the repre-
sentations of all unseen words intact. In this
paper, we show that constraint-driven vector
space specialisation can be extended to unseen
words. We propose a novel post-specialisation
method that: a) preserves the useful linguistic
knowledge for seen words; while b) propagat-
ing this external signal to unseen words in or-
der to improve their vector representations as
well. Our post-specialisation approach explic-
its a non-linear specialisation function in the
form of a deep neural network by learning to
predict specialised vectors from their original
distributional counterparts. The learned func-
tion is then used to specialise vectors of unseen
words. This approach, applicable to any post-
processing model, yields considerable gains
over the initial specialisation models both in in-
trinsic word similarity tasks, and in two down-
stream tasks: dialogue state tracking and lexi-
cal text simplification. The positive effects per-
sist across three languages, demonstrating the
importance of specialising the full vocabulary
of distributional word vector spaces.

1 Introduction

Word representation learning is a key research area
in current Natural Language Processing (NLP),
with its usefulness demonstrated across a range
of tasks (Collobert et al., 2011; Chen and Manning,
2014; Melamud et al., 2016b). The standard tech-
niques for inducing distributed word representa-
tions are grounded in the distributional hypothesis
(Harris, 1954): they rely on co-occurrence informa-
tion in large textual corpora (Mikolov et al., 2013b;

Pennington et al., 2014; Levy and Goldberg, 2014;
Levy et al., 2015; Bojanowski et al., 2017). As a
result, these models tend to coalesce the notions of
semantic similarity and (broader) conceptual relat-
edness, and cannot accurately distinguish antonyms
from synonyms (Hill et al., 2015; Schwartz et al.,
2015). Recently, we have witnessed a rise of in-
terest in representation models that move beyond
stand-alone unsupervised learning: they leverage
external knowledge in human- and automatically-
constructed lexical resources to enrich the semantic
content of distributional word vectors, in a process
termed semantic specialisation.

This is often done as a post-processing (some-
times referred to as retrofitting) step: input word
vectors are fine-tuned to satisfy linguistic con-
straints extracted from lexical resources such as
WordNet or BabelNet (Faruqui et al., 2015; Mrkšić
et al., 2017). The use of external curated knowl-
edge yields improved word vectors for the benefit
of downstream applications (Faruqui, 2016). At
the same time, this specialisation of the distribu-
tional space distinguishes between true similarity
and relatedness, and supports language understand-
ing tasks (Kiela et al., 2015; Mrkšić et al., 2017).

While there is consensus regarding their benefits
and ease of use, one property of the post-processing
specialisation methods slips under the radar: most
existing post-processors update word embeddings
only for words which are present (i.e., seen) in the
external constraints, while vectors of all other (i.e.,
unseen) words remain unaffected. In this work, we
propose a new approach that extends the speciali-
sation framework to unseen words, relying on the
transformation of the vector (sub)space of seen
words. Our intuition is that the process of fine-
tuning seen words provides implicit information on
how to leverage the external knowledge to unseen
words. The method should preserve the already in-
jected knowledge for seen words, simultaneously

516



propagating the external signal to unseen words in
order to improve their vectors.

The proposed post-specialisation method can be
seen as a two-step process, illustrated in Fig. 1a:
1) We use a state-of-the-art specialisation model
to transform the subspace of seen words from the
input distributional space into the specialised sub-
space; 2) We learn a mapping function based on
the transformation of the “seen subspace”, and then
apply it to the distributional subspace of unseen
words. We allow the proposed post-specialisation
model to learn from large external linguistic re-
sources by implementing the mapping as a deep
feed-forward neural network with non-linear acti-
vations. This allows the model to learn the general-
isation of the fine-tuning steps taken by the initial
specialisation model, itself based on a very large
number (e.g., hundreds of thousands) of external
linguistic constraints.

As indicated by the results on word similar-
ity and two downstream tasks (dialogue state
tracking and lexical text simplification) our post-
specialisation method consistently outperforms
state-of-the-art methods which specialise seen
words only. We report improvements using three
distinct input vector spaces for English and for
three test languages (English, German, Italian), ver-
ifying the robustness of our approach.

2 Related Work and Motivation

Vector Space Specialisation A standard ap-
proach to incorporating external and background
knowledge into word vector spaces is to pull the
representations of similar words closer together
and to push words in undesirable relations (e.g.,
antonyms) away from each other. Some models
integrate such constraints into the training proce-
dure and jointly optimize distributional and non-
distributional objectives: they modify the prior or
the regularisation (Yu and Dredze, 2014; Xu et al.,
2014; Bian et al., 2014; Kiela et al., 2015), or
use a variant of the SGNS-style objective (Liu
et al., 2015; Ono et al., 2015; Osborne et al., 2016;
Nguyen et al., 2017). In theory, word embeddings
obtained by these joint models could be as good as
representations produced by models which fine-
tune input vector space. However, their perfor-
mance falls behind that of fine-tuning methods (Wi-
eting et al., 2015). Another disadvantage is that
their architecture is tied to a specific underlying
model (typically word2vec models).

In contrast, fine-tuning models inject external
knowledge from available lexical resources (e.g.,
WordNet, PPDB) into pre-trained word vectors as
a post-processing step (Faruqui et al., 2015; Rothe
and Schütze, 2015; Wieting et al., 2015; Nguyen
et al., 2016; Mrkšić et al., 2016; Cotterell et al.,
2016; Mrkšić et al., 2017). Such post-processing
models are popular because they offer a portable,
flexible, and light-weight approach to incorporating
external knowledge into arbitrary vector spaces,
yielding state-of-the-art results on language under-
standing tasks (Faruqui et al., 2015; Mrkšić et al.,
2016; Kim et al., 2016; Vulić et al., 2017b).

Existing post-processing models, however, suf-
fer from a major limitation. Their modus operandi
is to enrich the distributional information with ex-
ternal knowledge only if such knowledge is present
in a lexical resource. This means that they update
and improve only representations of words actually
seen in external resources. Because such words
constitute only a fraction of the whole vocabulary
(see Sect. 4), most words, unseen in the constraints,
retain their original vectors. The main goal of this
work is to address this shortcoming by specialising
all words from the initial distributional space.

3 Methodology: Post-Specialisation

Our starting point is the state-of-the-art specialisa-
tion model ATTRACT-REPEL (AR) (Mrkšić et al.,
2017), outlined in Sect. 3.1. We opt for the
AR model due to its strong performance and
ease of use, but we note that the proposed post-
specialisation approach for specialising unseen
words, described in Sect. 3.2, is applicable to any
post-processor, as empirically validated in Sect. 5.

3.1 Initial Specialisation Model: AR

Let Vs be the vocabulary, A the set of synony-
mous ATTRACT word pairs (e.g., rich and wealthy),
and R the set of antonymous REPEL word pairs
(e.g., increase and decrease). The ATTRACT-REPEL
procedure operates over mini-batches of such
pairs BA and BR. Let each word pair (xl, xr) in
these sets correspond to a vector pair (xl,xr). A
mini-batch of batt attract word pairs is given by
BA = [(x1l ,x1r), . . . , (xk1l ,xk1r )] (analogously for
BR, which consists of brep pairs).

Next, the sets of negative exam-
ples TA = [(t1l , t

1
r), . . . , (t

k1
l , t

k1
r )] and

TR = [(t
1
l , t

1
r), . . . , (t

k2
l , t

k2
r )] are defined as

pairs of negative examples for each A and R

517



pair in mini-batches BA and BR. These negative
examples are chosen from the word vectors present
in BA or BR so that, for each A pair (xl,xr), the
negative example pair (tl, tr) is chosen so that tl
is the vector closest (in terms of cosine distance) to
xl and tr is closest to xr.1 The negatives are used
1) to force A pairs to be closer to each other than
to their respective negative examples; and 2) to
force R pairs to be further away from each other
than from their negative examples. The first term
of the cost function pulls A pairs together:

Att(BA, TA) =
batt∑

i=1

[
τ
(
δatt + x

i
lt

i
l − xilxir

)

+τ
(
δatt + x

i
rt

i
r − xilxir

) ]
(1)

where τ(z) = max(0, z) is the standard rectifier
function (Nair and Hinton, 2010) and δatt is the at-
tract margin: it determines how much closer these
vectors should be to each other than to their respec-
tive negative examples. The second, REPEL term
in the cost function is analogous: it pushes R word
pairs away from each other by the margin δrep.

Finally, in addition to the A and R terms, a regu-
larisation term is used to preserve the semantic con-
tent originally present in the distributional vector
space, as long as this information does not contra-
dict the injected external knowledge. Let V(B) be
the set of all word vectors present in a mini-batch,
the distributional regularisation term is then:

Reg(BA,BR) =
∑

xi∈V (BA∪BR)
λreg ‖x̂i − xi‖2 (2)

where λreg is the L2-regularisation constant and x̂i
denotes the original (distributional) word vector for
word xi. The full ATTRACT-REPEL cost function is
finally constructed as the sum of all three terms.

3.2 Specialisation of Unseen Words
Problem Formulation The goal is to learn a
global transformation function that generalises the
perturbations of the initial vector space made by
ATTRACT-REPEL (or any other specialisation pro-
cedure), as conditioned on the external constraints.
The learned function propagates the signal coded
in the input constraints to all the words unseen dur-
ing the specialisation process. We seek a regression

1Similarly, for each R pair (xl,xr), the negative pair
(tl, tr) is chosen from the in-batch vectors so that tl is the
vector furthest away from xl and tr is furthest from xr . All
vectors are unit length (re)normalised after each epoch.

function f : Rdim → Rdim, where dim is the vec-
tor space dimensionality. It maps word vectors from
the initial vector space X to the specialised target
space X′. Let X̂′ = f(X) refer to the predicted
mapping of the vector space, while the mapping of
a single word vector is denoted x̂′i = f(xi).

An input distributional vector space Xd repre-
sents words from a vocabulary Vd. Vd may be di-
vided into two vocabulary subsets: Vd = Vs ∪ Vu,
Vs ∩ Vu = ∅, with the accompanying vector sub-
spaces Xd = Xs tXu. Vs refers to the vocabulary
of seen words: those that appear in the external
linguistic constraints and have their embeddings
changed in the specialisation process. Vu denotes
the vocabulary of unseen words: those not present
in the constraints and whose embeddings are unaf-
fected by the specialisation procedure.

The AR specialisation process transforms only
the subspace Xs into the specialised subspace X′s.
All words xi ∈ Vs may now be used as training
examples for learning the explicit mapping function
f from Xs into X′s. If N = |Vs|, we in fact rely on
N training pairs: (xi,x′i) = {xi ∈ Xs,x′i ∈ X′s}.
Function f can then be applied to unseen words
x ∈ Vu to yield the specialised subspace X̂′u =
f(Xu). The specialised space containing all words
is then Xf = X′s ∪ X̂′u. The complete high-level
post-specialisation procedure is outlined in Fig. 1a.

Note that another variant of the approach could
obtain Xf as Xf = f(Xd), that is, the entire distri-
butional space is transformed by f . However, this
variant seems counter-intuitive as it forgets the ac-
tual output of the initial specialisation procedure
and replaces word vectors from X′s with their ap-
proximations, i.e., f -mapped vectors.2

Objective Functions As mentioned, the N seen
words xi ∈ Vs in fact serve as our “pseudo-
translation” pairs supporting the learning of a cross-
space mapping function. In practice, in its high-
level formulation, our mapping problem is equiva-
lent to those encountered in the literature on cross-
lingual word embeddings where the goal is to learn
a shared cross-lingual space given monolingual vec-
tor spaces in two languages andN1 translation pairs
(Mikolov et al., 2013a; Lazaridou et al., 2015; Vulić
and Korhonen, 2016b; Artetxe et al., 2016, 2017;
Conneau et al., 2017; Ruder et al., 2017). In our
setup, the standard objective based on L2-penalised

2We have empirically confirmed the intuition that the first
variant is superior to this alternative. We do not report the
actual quantitative comparison for brevity.

518



xi ∈ Xu
f : Deep neural network

(non-linear regression) x̂
′
i ∈ X̂′u

Xd = Xs ∪ Xu
(distributional)

Linguistic Constraints
(x1 ∈ Xs,y1 ∈ Xs)
(x1 ∈ Xs,y2 ∈ Xs)
(x2 ∈ Xs,y3 ∈ Xs)

· · ·

X′s ∪ Xu
(specialised: seen)

Xf = X
′
s ∪ X̂′u

(specialised final: all)

attract-repel mapping

Training Pairs: Seen
(x1 ∈ Xs,x′1 ∈ X′s)
(x2 ∈ Xs,x′2 ∈ X′s)
(x3 ∈ Xs,x′3 ∈ X′s)

· · ·

(a) High-level illustration

x'i,p
     (d=300)

... ...

...

...

...

...

...

...

...

...

...

... ...

...

......
...

...

...

swish swish swish swish

...

xi 
(d=300)

x'i,h1
      (d1=512)

x'i,h2
      (d2=512)

x'i,hH-1
      (dH-1=512)

x'i,hH
      (dH=512)

...

...

......

...

...

Input

Hidden 1 Hidden 2 Hidden H-1 Hidden H

Output

Xu X'u

(b) Low-level implementation: deep feed-forward neural network

Figure 1: (a) High-level illustration of the post-specialisation approach: the subspace Xs of the initial
distributional vector space Xd = Xs ∪Xu is first specialised/fine-tuned by the ATTRACT-REPEL speciali-
sation model (or any other post-processing model) to obtain the transformed subspace X′s. The words
present (i.e., seen) in the input set of linguistic constraints are now assigned different representations in
Xs (the original distributional vector) and X′s (the specialised vector): they are therefore used as training
examples to learn a non-linear cross-space mapping function. This function is then applied to all word
vectors xi ∈ Xu representing words unseen in the constraints to yield a specialised subspace X̂′u. The
final space is Xf = X′s ∪ X̂′u, and it contains transformed representations for all words from the initial
space Xd. (b) The actual implementation of the non-linear regression function which maps from Xu to
X̂′u: a deep feed-forward fully-connected neural net with non-linearities and H hidden layers.

least squares may be formulated as follows:

fMSE = arg min||f(Xs)−X′s||2F (3)

where || · ||2F denotes the squared Frobenius norm.
In the most common form f(Xs) is simply a lin-
ear map/matrix Wf ∈ Rdim×dim (Mikolov et al.,
2013a) as follows: f(X) = WfX.

After learning f based on the Xs → X′s trans-
formation, one can simply apply f to unseen words:
X̂′u = f(Xu). This linear mapping model, termed
LINEAR-MSE, has an analytical solution (Artetxe
et al., 2016), and has been proven to work well with
cross-lingual embeddings. However, given that the
specialisation model injects hundreds of thousands
(or even millions) of linguistic constraints into the
distributional space (see later in Sect. 4), we sus-
pect that the assumption of linearity is too limiting
and does not fully hold in this particular setup.

Using the same L2-penalized least squares objec-
tive, we can thus replace the linear map with a non-
linear function f : Rdim → Rdim. The non-linear
mapping, illustrated by Fig. 1b, is implemented as a
deep feed-forward fully-connected neural network
(DFFN) with H hidden layers and non-linear acti-
vations. This variant is called NONLINEAR-MSE.

Another variant objective is the contrastive
margin-based ranking loss with negative sampling
(MM) similar to the original ATTRACT-REPEL ob-
jective, used in other applications in prior work
(e.g., for cross-modal mapping) (Weston et al.,
2011; Frome et al., 2013; Lazaridou et al., 2015;
Kummerfeld et al., 2015). Let x̂′i = f(xi) denote
the predicted vector for the word xi ∈ Vs, and let
x′i refer to the “true” vector of xi in the specialised
space X′s after the AR specialisation procedure. The
MM loss is then defined as follows:

JMM =

N∑

i=1

k∑

j 6=i
τ
(
δmm − cos

(
x̂′i,x

′
i

)
+ cos

(
x̂′i,x

′
j

))

where cos is the cosine similarity measure, δmm is
the margin, and k is the number of negative sam-
ples. The objective tries to learn the mapping f so
that each predicted vector x̂′i is by the specified
margin δmm closer to the correct target vector x′i
than to any other of k target vectors x′j serving as
negative examples.3 Function f can again be either
a simple linear map (LINEAR-MM), or implemented
as a DFFN (NONLINEAR-MM, see Fig. 1b).

3We have also experimented with a simpler hinge loss
function without negative examples, formulated as J =

519



4 Experimental Setup

Starting Word Embeddings (Xd = Xs ∪ Xu)
To test the robustness of our approach, we exper-
iment with three well-known, publicly available
collections of English word vectors: 1) Skip-Gram
with Negative Sampling (SGNS-BOW2) (Mikolov
et al., 2013b) trained on the Polyglot Wikipedia (Al-
Rfou et al., 2013) by Levy and Goldberg (2014)
using bag-of-words windows of size 2; 2) GLOVE
Common Crawl (Pennington et al., 2014); and 3)
FASTTEXT (Bojanowski et al., 2017), a SGNS vari-
ant which builds word vectors as the sum of their
constituent character n-gram vectors. All word em-
beddings are 300-dimensional.4

AR Specialisation and Constraints (Xs → X′s)
We experiment with linguistic constraints used be-
fore by (Mrkšić et al., 2017; Vulić et al., 2017a):
they extracted monolingual synonymy/ATTRACT
pairs from the Paraphrase Database (PPDB)
(Ganitkevitch et al., 2013; Pavlick et al., 2015)
(640,435 synonymy pairs in total), while their
antonymy/REPEL constraints came from BabelNet
(Navigli and Ponzetto, 2012) (11,939 pairs).5

The coverage of Vd vocabulary words in the
constraints illustrates well the problem of unseen
words with the fine-tuning specialisation models.
For instance, the constraints cover only a small
subset of the entire vocabulary Vd for SGNS-BOW2:
16.6%. They also cover only 15.3% of the top 200K
most frequent Vd words from FASTTEXT.
Network Design and Parameters (Xu → X̂′u)
The non-linear regression function f : Rd → Rd
is a DFFN with H hidden layers, each of dimen-
sionality d1 = d2 = . . . = dH = 512 (see Fig. 1b).
Non-linear activations are used in each layer and
∑N

i=1 τ
(
δmm − cos(x̂′i,x′i)

)
. For instance, with δmm =

1.0 the idea is to learn a mapping f that, for each xi enforces
the predicted vector and the correct target vector to have a
maximum cosine similarity. We do not report the results with
this variant as, although it outscores the MSE-style objective,
it was consistently outperformed by the MM objective.

4For further details regarding the architectures and training
setup of the used vector collections, we refer the reader to
the original papers. Additional experiments with other word
vectors, e.g., with CONTEXT2VEC (Melamud et al., 2016a)
(which uses bidirectional LSTMs (Hochreiter and Schmidhu-
ber, 1997) for context modeling), and with dependency-word
based embeddings (Bansal et al., 2014; Melamud et al., 2016b)
lead to similar results and same conclusions.

5We have experimented with another set of constraints
used in prior work (Zhang et al., 2014; Ono et al., 2015),
reaching similar conclusions: these were extracted from Word-
Net (Fellbaum, 1998) and Roget (Kipfer, 2009), and comprise
1,023,082 synonymy pairs and 380,873 antonymy pairs.

omitted only before the final output layer to enable
full-range predictions (see Fig. 1b again).

The choices of non-linear activation and ini-
tialisation are guided by recent recommendations
from the literature. First, we use swish (Ramachan-
dran et al., 2017; Elfwing et al., 2017) as non-
linearity, defined as swish(x) = x · sigmoid(βx).
We fix β = 1 as suggested by Ramachandran et al.
(2017).6 Second, we use the HE normal initialisa-
tion (He et al., 2015), which is preferred over the
XAVIER initialisation (Glorot and Bengio, 2010)
for deep models (Mishkin and Matas, 2016; Li
et al., 2016), although in our experiments we do
not observe a significant difference in performance
between the two alternatives. We set H = 5 in
all experiments without any fine-tuning; we also
analyse the impact of the network depth in Sect. 5.

Optimisation For the AR specialisation step, we
adopt the original suggested model setup. Hyper-
parameter values are set to: δatt = 0.6, δrep = 0.0,
λreg = 10

−9 (Mrkšić et al., 2017). The models are
trained for 5 epochs with Adagrad (Duchi et al.,
2011), with batch sizes set to batt = brep = 50,
again as in the original work.

For training the non-linear mapping with DFFN
(Fig. 1b), we use the Adam algorithm (Kingma
and Ba, 2015) with default settings. The model is
trained for 100 epochs with early stopping on a
validation set. We reserve 10% of all available seen
data (i.e., the words from Vs represented in Xs and
X′s) for validation, the rest are used for training. For
the MM objective, we set δmm = 0.6 and k = 25
in all experiments without any fine-tuning.

5 Results and Discussion

5.1 Intrinsic Evaluation: Word Similarity

Evaluation Protocol The first set of experiments
evaluates vector spaces with different specialisation
procedures intrinsically on word similarity bench-
marks: we use the SimLex-999 dataset (Hill et al.,
2015), and SimVerb-3500 (Gerz et al., 2016), a
recent verb pair similarity dataset providing simi-
larity ratings for 3,500 verb pairs.7 Spearman’s ρ

6According to Ramachandran et al. (2017), for deep net-
works swish has a slight edge over the family of LU/ReLU-
related activations (Maas et al., 2013; He et al., 2015; Klam-
bauer et al., 2017). We also observe a minor (and insignificant)
difference in performance in favour of swish.

7While other gold standards such as WordSim-353 (Finkel-
stein et al., 2002) or MEN (Bruni et al., 2014) coalesce the no-
tions of true semantic similarity and (more broad) conceptual
relatedness, SimLex and SimVerb provide explicit guidelines

520



Setup: hold-out Setup: all

GLOVE SGNS-BOW2 FASTTEXT GLOVE SGNS-BOW2 FASTTEXT

SL SV SL SV SL SV SL SV SL SV SL SV

Distributional: Xd .408 .286 .414 .275 .383 .255 .408 .286 .414 .275 .383 .255
+AR specialisation: X′s .408 .286 .414 .275 .383 .255 .690 .578 .658 .544 .629 .502
++Mapping unseen: Xf
LINEAR-MSE .504 .384 .447 .309 .405 .285 .690 .578 .656 .551 .628 .502
NONLINEAR-MSE .549 .407 .484 .344 .459 .329 .694 .586 .663 .556 .631 .506
LINEAR-MM .548 .422 .468 .329 .419 .308 .697 .582 .663 .554 .628 .487
NONLINEAR-MM .603 .480 .531 .391 .471 .349 .705 .600 .667 .562 .638 .507

Table 1: Spearman’s ρ correlation scores for three word vector collections on two English word similarity
datasets, SimLex-999 (SL) and SimVerb-3500 (SV), using different mapping variants, evaluation protocols,
and word vector spaces: from the initial distributional space Xd to the fully specialised space Xf . H = 5.

0 1 2 3 4 5 6 7 8
Number of hidden layers H

0.25

0.35

0.45

0.55

0.65

Sp
ea

rm
an

’s
ρ

co
rr

el
at

io
n

SimLex-999 SimVerb-3500

(a) GLOVE

0 1 2 3 4 5 6 7 8
Number of hidden layers H

0.25

0.35

0.45

0.55

0.65 SimLex-999 SimVerb-3500

(b) SGNS-BOW2

0 1 2 3 4 5 6 7 8
Number of hidden layers H

0.25

0.35

0.45

0.55

0.65

Sp
ea

rm
an

’s
ρ

co
rr

el
at

io
n

SimLex-999 SimVerb-3500

(c) FASTTEXT

Figure 2: The results of the hold-out experiments on SimLex-999 and SimVerb-3500 after applying our
non-linear vector space transformation with different depths (hidden layer size H , see Fig. 1b). The
results are presented as averages over 20 runs with the NONLINEAR-MM variant, the shaded regions are
spanned by the maximum and minimum scores obtained. Thick horizontal lines refer to Spearman’s
rank correlations achieved in the initial space Xd. H = 0 denotes the standard linear regression model
(Mikolov et al., 2013a; Lazaridou et al., 2015) (LINEAR-MM shown since it outperforms LINEAR-MSE).

rank correlation is used as the evaluation metric.
We evaluate word vectors in two settings. First,

in a synthetic hold-out setting, we remove all lin-
guistic constraints which contain words from the
SimLex and SimVerb evaluation data, effectively
forcing all SimLex and SimVerb words to be un-
seen by the AR specialisation model. The spe-
cialised vectors for these words are estimated by
the learned non-linear DFFN mapping model. Sec-
ond, the all setting is a standard “real-life” scenario
where some test (SimLex/SimVerb) words do occur
in the constraints, while the mapping is learned for
the remaining words.

Results and Analysis The results with the three
word vector collections are provided in Tab. 1. In
addition, Fig. 2 plots the influence of the network

to discern between the two, so that related but non-similar
words (e.g. tiger and jungle) have a low rating.

depth H on the model’s performance.

The results suggest that the mapping of unseen
words is universally useful, as the highest corre-
lation scores are obtained with the final fully spe-
cialised vector space Xf for all three input spaces.
The results in the hold-out setup are particularly
indicative of the improvement achieved by our post-
specialisation method. For instance, it achieves a
+0.2 correlation gain with GLOVE on both SimLex
and SimVerb by specialising vector representations
for words present in these datasets without seeing
a single external constraint which contains any of
these words. This suggests that the perturbation
of the seen subspace Xs by ATTRACT-REPEL con-
tains implicit knowledge that can be propagated
to Xu, learning better representations for unseen
words. We observe small but consistent improve-
ments across the board in the all setup. The smaller
gains can be explained by the fact that a majority of

521



SimLex and SimVerb words are present in the ex-
ternal constraints (93.7% and 87.2%, respectively).

The scores also indicate that both non-linearity
and the chosen objective function contribute to the
quality of the learned mapping: largest gains are
reported with the NONLINEAR-MM variant which
a) employs non-linear activations and b) replaces
the basic mean-squared-error objective with max-
margin. The usefulness of the latter has been estab-
lished in prior work on cross-space mapping learn-
ing (Lazaridou et al., 2015). The former indicates
that the initial AR transformation is non-linear. It is
guided by a large number of constraints; their effect
cannot be captured by a simple linear map as in
prior work on, e.g., cross-lingual word embeddings
(Mikolov et al., 2013a; Ruder et al., 2017).

Finally, the analysis of the network depth H
indicates that going deeper helps only to a cer-
tain extent. Adding more layers allows for a richer
parametrisation of the network (which is beneficial
given the number of linguistic constraints used by
AR). This makes the model more expressive, but it
seems to saturate with larger H values.

Post-Specialisation with Other Post-Processors
We also verify that our post-specialisation approach
is not tied to the ATTRACT-REPEL method, and is in-
deed applicable on top of any post-processing spe-
cialisation method. We analyse the impact of post-
specialisation in the hold-out setting using the orig-
inal retrofitting (RFit) model (Faruqui et al., 2015)
and counter-fitting (CFit) (Mrkšić et al., 2016) in
lieu of attract-repel. The results on word similarity
with the best-performing NONLINEAR-MM variant
are summarised in Tab. 2.

The scores again indicate the usefulness of post-
specialisation. As expected, the gains are lower

Figure 3: DST labels (user goals given by slot-value
pairs) in a multi-turn dialogue (Mrkšić et al., 2015).

GLOVE SGNS-BOW2 FASTTEXT

SL SV SL SV SL SV

Xd .408 .286 .414 .275 .383 .255
Xf : RFit .493 .365 .412 .285 .413 .279
Xf : CFit .540 .401 .439 .318 .306 .441

Table 2: Post-specialisation applied to two
other post-processing methods. SL: SimLex; SV:
SimVerb. Hold-out setting. NONLINEAR-MM.

than with ATTRACT-REPEL. RFit falls short of CFit
as by design it can leverage only synonymy (i.e.,
ATTRACT) external constraints.

5.2 Downstream Task I: DST
Next, we evaluate the usefulness of post-
specialisation for two downstream tasks – dialogue
state tracking and lexical text simplification – in
which discerning semantic similarity from other
types of semantic relatedness is crucial. We first
evaluate the importance of post-specialisation for
a downstream language understanding task of dia-
logue state tracking (DST) (Henderson et al., 2014;
Williams et al., 2016), adopting the evaluation pro-
tocol and data of Mrkšić et al. (2017).

DST: Model and Evaluation The DST model is
the first component of modern dialogue pipelines
(Young, 2010), which captures the users’ goals at
each dialogue turn and then updates the dialogue
state. Goals are represented as sets of constraints
expressed as slot-value pairs (e.g., food=Chinese).
The set of slots and the set of values for each slot
constitute the ontology of a dialogue domain. The
probability distribution over the possible states is
the system’s estimate of the user’s goals, and it is
used by the dialogue manager module to select the
subsequent system response (Su et al., 2016). An
example in Fig. 3 illustrates the DST pipeline.

For evaluation, we use the Neural Belief Tracker
(NBT), a state-of-the-art DST model which was
the first to reason purely over pre-trained word
vectors (Mrkšić et al., 2017).8 The NBT uses no
hand-crafted semantic lexicons, instead compos-
ing word vectors into intermediate utterance and
context representations.9 For full model details, we
refer the reader to the original paper. The impor-
tance of word vector specialisation for the DST
task (e.g., distinguishing between synonyms and
antonyms by pulling northern and north closer in

8https://github.com/nmrksic/neural-belief-tracker
9The NBT keeps word vectors fixed during training to

enable generalisation for words unseen in DST training data.

522



ENGLISH hold-out all

Distributional: Xd .797 .797
+AR Spec.: X′s ∪Xu .797 .817
++Mapping: Xf = X′s ∪ X̂′u
LINEAR-MM .815 .818
NONLINEAR-MM .827 .835

Table 3: DST results in two evaluation settings
(hold-out and all) with different GLOVE variants.

the vector space while pushing north and south
away) has been established (Mrkšić et al., 2017).

Again, as in prior work the DST evaluation is
based on the Wizard-of-Oz (WOZ) v2.0 dataset
(Wen et al., 2017; Mrkšić et al., 2017), comprising
1,200 dialogues split into training (600 dialogues),
development (200), and test data (400). In all exper-
iments, we report the standard DST performance
measure: joint goal accuracy, and report scores as
averages over 5 NBT training runs.

Results and Analysis We again evaluate word
vectors in two settings: 1) hold-out, where linguis-
tic constraints with words appearing in the WOZ
data are removed, making all WOZ words unseen
by ATTRACT-REPEL; and 2) all. The results for
the English DST task with different GLOVE word
vector variants are summarised in Tab. 3; similar
trends in results are observed with two other word
vector collections. The scores maintain conclusions
established in the word similarity task. First, seman-
tic specialisation with ATTRACT-REPEL is again
beneficial, and discerning between synonyms and
antonyms improves DST performance. However,
specialising unseen words (the final Xu vector
space) yields further improvements in both evalua-
tion settings, supporting our claim that the speciali-
sation signal can be propagated to unseen words.

This downstream evaluation again demonstrates
the importance of non-linearity, as the peak scores
are reported with the NONLINEAR-MM variant.
More substantial gains in the all setup are observed
in the DST task compared to the word similarity
task. This stems from a lower coverage of the WOZ
data in the AR constraints: 36.3% of all WOZ words
are unseen words. Finally, the scores are higher on
average in the all setup, since this setup uses more
external constraints for AR, and consequently uses
more training examples to learn the mapping.

Other Languages We test the portability of our
framework to two other languages for which we
have similar evaluation data: German (DE) and

Italian (IT). SimLex-999 has been translated and
rescored in the two languages by Leviant and
Reichart (2015), and the WOZ data were trans-
lated and adapted by Mrkšić et al. (2017). Exactly
the same setup is used as in our English exper-
iments, without any additional language-specific
fine-tuning. Linguistic constraints were extracted
from the same sources: synonyms from the PPDB
(135,868 in DE, 362,452 in IT), antonyms from
BabelNet (4,124 in DE, and 16,854 in IT). Our
starting distributional vector spaces are taken from
prior work: IT vectors are from (Dinu et al., 2015),
DE vectors are from (Vulić and Korhonen, 2016a).
The results are summarised in Tab. 4.

Our post-specialisation approach yields consis-
tent improvements over the initial distributional
space and the AR specialisation model in both tasks
and for both languages. We do not observe any gain
on IT SimLex in the all setup since IT constraints
have almost complete coverage of all IT SimLex
words (99.3%; the coverage is 64.8% in German).
As expected, the DST scores in the all setup are
higher than in the hold-out setup due to a larger
number of constraints and training examples.

Lower absolute scores for Italian and German
compared to the ones reported for English are
due to multiple factors, as discussed previously by
Mrkšić et al. (2017): 1) the AR model uses less lin-
guistic constraints for DE and IT; 2) distributional
word vectors are induced from smaller corpora; 3)
linguistic phenomena (e.g., cases and compound-
ing in DE) contribute to data sparsity and also make
the DST task more challenging. However, it is im-
portant to stress the consistent gains over the vector
space specialised by the state-of-the-art ATTRACT-
REPEL model across all three test languages. This
indicates that the proposed approach is language-
agnostic and portable to multiple languages.

5.3 Downstream Task II: Lexical
Simplification

In our second downstream task, we examine the
effects of post-specialisation on lexical simplifica-
tion (LS) in English. LS aims to substitute complex
words (i.e., less commonly used words) with their
simpler synonyms in the context. Simplified text
must keep the meaning of the original text, which
is discerning similarity from relatedness is impor-
tant (e.g., in “The automobile was set on fire” the
word “automobile” should be replaced with “car”
or “vehicle” but not with “wheel” or “driver”).

523



GERMAN ITALIAN

SimLex (Similarity) WOZ (DST) SimLex (Similarity) WOZ (DST)
hold-out all hold-out all hold-out all hold-out all

Distributional: Xd .267 .267 .487 .487 .363 .363 .618 .618
+AR Spec.: X′s ∪Xu .267 .422 .487 .535 .363 .616 .618 .634
++Mapping: Xf
LINEAR-MM .354 .449 .485 .533 .401 .616 .627 .633
NONLINEAR-MM .367 .466 .496 .538 .428 .616 .637 .647

Table 4: Results on word similarity (Spearman’s ρ) and DST (joint goal accuracy) for German and Italian.

Vectors Specialisation Acc. Ch.

Distributional: Xd 66.0 94.0
GLOVE +AR Spec.: X′s ∪Xu 67.6 87.0

++Mapping: Xf 72.3 87.6

Distributional: Xd 57.8 84.0
FASTTEXT +AR Spec.: X′s ∪Xu 69.8 89.4

++Mapping: Xf 74.3 88.8

Distributional: Xd 56.0 79.1
SGNS-BOW2 +AR Spec.: X′s ∪Xu 64.4 86.7

++Mapping: Xf 70.9 86.8

Table 5: Lexical simplification performance with
post-specialisation applied on three input spaces.

We employ LIGHT-LS (Glavaš and Štajner,
2015), a lexical simplification algorithm that: 1)
makes substitutions based on word similarities in a
semantic vector space, and 2) can be provided an
arbitrary embedding space as input.10 For a com-
plex word, LIGHT-LS considers the most similar
words from the vector space as simplification can-
didates. Candidates are ranked according to several
features, indicating simplicity and fitness for the
context (semantic relatedness to the context of the
complex word). The substitution is made if the best
candidate is simpler than the original word. By pro-
viding vector spaces post-specialised for semantic
similarity to LIGHT-LS, we expect to more often
replace complex words with their true synonyms.

We evaluate LIGHT-LS performance in the all
setup on the LS benchmark compiled by Horn et al.
(2014), who crowdsourced 50 manual simplifica-
tions for each complex word. As in prior work,
we evaluate performance with the following met-
rics: 1) Accurracy (Acc.) is the number of correct
simplifications made (i.e., the system made the sim-
plification and its substitution is found in the list of
crowdsourced substitutions), divided by the total
number of indicated complex words; 2) Changed
(Ch.) is the percentage of indicated complex words

10https://github.com/codogogo/lightls

that were replaced by the system (whether or not
the replacement was correct).

LS results are summarised in Tab. 5. Post-
specialised vector spaces consistently yield 5-6%
gain in Accuracy compared to respective distribu-
tional vectors and embeddings specialised with the
state-of-the-art ATTRACT-REPEL model. Similar
to DST evaluation, improvements over ATTRACT-
REPEL demonstrate the importance of specialising
the vectors of the entire vocabulary and not only
the vectors of words from the external constraints.

6 Conclusion and Future Work

We have presented a novel post-processing model,
termed post-specialisation, that specialises word
vectors for the full vocabulary of the input vec-
tor space. Previous post-processing specialisation
models fine-tune word vectors only for words oc-
curring in external lexical resources. In this work,
we have demonstrated that the specialisation of the
subspace of seen words can be leveraged to learn
a mapping function which specialises vectors for
all other words, unseen in the external resources.
Our results across word similarity and downstream
language understanding tasks show consistent im-
provements over the state-of-the-art specialisation
method for all three test languages.

In future work, we plan to extend our approach
to specialisation for asymmetric relations such as
hypernymy or meronymy (Glavaš and Ponzetto,
2017; Nickel and Kiela, 2017; Vulić and Mrkšić,
2018). We will also investigate more sophisticated
non-linear functions. The code is available at:
https://github.com/cambridgeltl/
post-specialisation/.

Acknowledgments

We thank the three anonymous reviewers for their
insightful suggestions. This work is supported by
the ERC Consolidator Grant LEXICAL: Lexical
Acquisition Across Languages (no 648909).

524



References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.

2013. Polyglot: Distributed word representations for
multilingual NLP. In Proceedings of CoNLL, pages
183–192.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of EMNLP, pages 2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of ACL, pages
451–462.

Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring continuous word representations for depen-
dency parsing. In Proceedings of ACL, pages 809–
815.

Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Proceedings of ECML-PKDD, pages 132–
148.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the ACL,
5:135–146.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.

Danqi Chen and Christopher D. Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In Proceedings of EMNLP, pages 740–750.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. CoRR,
abs/1710.04087.

Ryan Cotterell, Hinrich Schütze, and Jason Eisner.
2016. Morphological smoothing and extrapolation
of word embeddings. In Proceedings of ACL, pages
1651–1660.

Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2015. Improving zero-shot learning by mitigat-
ing the hubness problem. In Proceedings of ICLR
(Workshop Papers).

John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2017.
Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning.
CoRR, abs/1702.03118.

Manaal Faruqui. 2016. Diverse Context for Learning
Word Representations. Ph.D. thesis, Carnegie Mel-
lon University.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of NAACL-HLT, pages 1606–1615.

Christiane Fellbaum. 1998. WordNet.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The con-
cept revisited. ACM Transactions on Information
Systems, 20(1):116–131.

Andrea Frome, Gregory S. Corrado, Jonathon Shlens,
Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. 2013. DeViSE: A deep visual-
semantic embedding model. In Proceedings of
NIPS, pages 2121–2129.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL-HLT, pages
758–764.

Daniela Gerz, Ivan Vulić, Felix Hill, Roi Reichart, and
Anna Korhonen. 2016. SimVerb-3500: A large-
scale evaluation set of verb similarity. In Proceed-
ings of EMNLP, pages 2173–2182.

Goran Glavaš and Simone Paolo Ponzetto. 2017.
Dual tensor model for detecting asymmetric lexico-
semantic relations. In Proceedings of EMNLP,
pages 1758–1768.

Goran Glavaš and Sanja Štajner. 2015. Simplifying lex-
ical simplification: Do we need simplified corpora?
In Proceedings of ACL, pages 63–68.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of AISTATS, pages 249–
256.

Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpass-
ing human-level performance on ImageNet classifi-
cation. In Proceedings of ICCV, pages 1026–1034.

Matthew Henderson, Blaise Thomson, and Jason D.
Wiliams. 2014. The Second Dialog State Tracking
Challenge. In Proceedings of SIGDIAL, pages 263–
272.

525



Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
SimLex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4):665–695.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Colby Horn, Cathryn Manduca, and David Kauchak.
2014. Learning a lexical simplifier using wikipedia.
In Proceedings of the ACL, pages 458–463.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In Proceedings of EMNLP, pages 2044–
2048.

Joo-Kyung Kim, Gokhan Tur, Asli Celikyilmaz, Bin
Cao, and Ye-Yi Wang. 2016. Intent detection us-
ing semantically enriched word embeddings. In Pro-
ceedings of SLT.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR (Conference Track).

Barbara Ann Kipfer. 2009. Roget’s 21st Century The-
saurus (3rd Edition). Philip Lief Group.

Günter Klambauer, Thomas Unterthiner, Andreas
Mayr, and Sepp Hochreiter. 2017. Self-normalizing
neural networks. CoRR, abs/1706.02515.

Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick,
and Dan Klein. 2015. An empirical analysis of op-
timization for max-margin NLP. In Proceedings of
EMNLP, pages 273–279.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In Pro-
ceedings of ACL, pages 270–280.

Ira Leviant and Roi Reichart. 2015. Separated by
an un-common language: Towards judgment lan-
guage informed vector space modeling. CoRR,
abs/1508.00106.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of ACL,
pages 302–308.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the ACL,
3:211–225.

Sihan Li, Jiantao Jiao, Yanjun Han, and Tsachy
Weissman. 2016. Demystifying ResNet. CoRR,
abs/1611.01186.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
based on ordinal knowledge constraints. In Proceed-
ings of ACL, pages 1501–1511.

Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In Proceedings of ICML.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016a. Context2vec: Learning generic context em-
bedding with bidirectional LSTM. In Proceedings
of CoNLL, pages 51–61.

Oren Melamud, David McClosky, Siddharth Patward-
han, and Mohit Bansal. 2016b. The role of context
types and dimensionality in learning word embed-
dings. In Proceedings of NAACL-HLT, pages 1030–
1040.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a. Exploiting similarities among languages
for machine translation. arXiv preprint, CoRR,
abs/1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Proceedings of NIPS, pages 3111–
3119.

Dmytro Mishkin and Jiri Matas. 2016. All you need
is a good init. In Proceedings of ICLR (Conference
Track).

Nikola Mrkšić, Diarmuid Ó Séaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2017. Neu-
ral belief tracker: Data-driven dialogue state track-
ing. In Proceedings of ACL, pages 1777–1788.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gašić, Pei-Hao Su, David Vandyke,
Tsung-Hsien Wen, and Steve Young. 2015. Multi-
domain dialog state tracking using recurrent neural
networks. In Proceedings of ACL, pages 794–799.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gašić, Lina Maria Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Steve Young. 2016. Counter-fitting word vectors
to linguistic constraints. In Proceedings of NAACL-
HLT.

Nikola Mrkšić, Ivan Vulić, Diarmuid Ó Séaghdha, Ira
Leviant, Roi Reichart, Milica Gašić, Anna Korho-
nen, and Steve Young. 2017. Semantic specialisa-
tion of distributional word vector spaces using mono-
lingual and cross-lingual constraints. Transactions
of the ACL, 5:309–324.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of ICML, pages 807–814.

Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual seman-
tic network. Artificial Intelligence, 193:217–250.

526



Kim Anh Nguyen, Maximilian Köper, Sabine
Schulte im Walde, and Ngoc Thang Vu. 2017.
Hierarchical embeddings for hypernymy detection
and directionality. In Proceedings of EMNLP,
pages 233–243.

Kim Anh Nguyen, Sabine Schulte im Walde, and
Ngoc Thang Vu. 2016. Integrating distributional
lexical contrast into word embeddings for antonym-
synonym distinction. In Proceedings of ACL, pages
454–459.

Maximilian Nickel and Douwe Kiela. 2017. Poincaré
embeddings for learning hierarchical representa-
tions. In Proceedings of NIPS.

Masataka Ono, Makoto Miwa, and Yutaka Sasaki.
2015. Word embedding-based antonym detection
using thesauri and distributional information. In
Proceedings of NAACL-HLT, pages 984–989.

Dominique Osborne, Shashi Narayan, and Shay Cohen.
2016. Encoding prior knowledge with eigenword
embeddings. Transactions of the ACL, 4:417–430.

Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,
Benjamin Van Durme, and Chris Callison-Burch.
2015. PPDB 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. In Proceedings of ACL, pages
425–430.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of EMNLP, pages 1532–
1543.

Prajit Ramachandran, Barret Zoph, and Quoc V. Le.
2017. Searching for activation functions. CoRR,
abs/1710.05941.

Sascha Rothe and Hinrich Schütze. 2015. AutoEx-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of ACL,
pages 1793–1803.

Sebastian Ruder, Ivan Vulić, and Anders Søgaard. 2017.
A survey of cross-lingual embedding models. CoRR,
abs/1706.04902.

Roy Schwartz, Roi Reichart, and Ari Rappoport. 2015.
Symmetric pattern based word embeddings for im-
proved word similarity prediction. In Proceedings
of CoNLL, pages 258–267.

Pei-Hao Su, Milica Gašić, Nikola Mrkšić, Lina Rojas-
Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016. Continuously
learning neural dialogue management. In arXiv
preprint: 1606.02689.

Ivan Vulić and Anna Korhonen. 2016a. Is "universal
syntax" universally useful for learning distributed
word representations? In Proceedings of ACL, pages
518–524.

Ivan Vulić and Anna Korhonen. 2016b. On the role of
seed lexicons in learning bilingual word embeddings.
In Proceedings of ACL, pages 247–257.

Ivan Vulić and Nikola Mrkšić. 2018. Specialising word
vectors for lexical entailment. In Proceedings of
NAACL-HLT.

Ivan Vulić, Nikola Mrkšić, and Anna Korhonen. 2017a.
Cross-lingual induction and transfer of verb classes
based on word vector space specialisation. In Pro-
ceedings of EMNLP, pages 2536–2548.

Ivan Vulić, Nikola Mrkšić, Roi Reichart, Diarmuid
Ó Séaghdha, Steve Young, and Anna Korhonen.
2017b. Morph-fitting: Fine-tuning word vector
spaces with simple language-specific rules. In Pro-
ceedings of ACL, pages 56–68.

Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić,
Milica Gašić, Lina M. Rojas-Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017. A network-
based end-to-end trainable task-oriented dialogue
system. In Proceedings of EACL.

Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. WSABIE: Scaling up to large vocabulary
image annotation. In Proceedings of IJCAI, pages
2764–2770.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to compo-
sitional paraphrase model and back. Transactions of
the ACL, 3:345–358.

Jason D. Williams, Antoine Raux, and Matthew Hen-
derson. 2016. The Dialog State Tracking Challenge
series: A review. Dialogue & Discourse, 7(3):4–33.

Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. RC-
NET: A general framework for incorporating knowl-
edge into word representations. In Proceedings of
CIKM, pages 1219–1228.

Steve Young. 2010. Cognitive User Interfaces. IEEE
Signal Processing Magazine.

Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of ACL, pages 545–550.

Jingwei Zhang, Jeremy Salwen, Michael Glass, and Al-
fio Gliozzo. 2014. Word semantic representations
using bayesian probabilistic tensor factorization. In
Proceedings of EMNLP, pages 1522–1531.

527


