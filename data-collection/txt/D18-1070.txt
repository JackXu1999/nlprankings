



















































Increasing In-Class Similarity by Retrofitting Embeddings with Demographic Information


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 671–677
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

671

Increasing In-Class Similarity
by Retrofitting Embeddings with Demographic Information

Dirk Hovy
Bocconi University

dirk.hovy@unibocconi.it

Tommaso Fornaciari
Bocconi University

fornaciari@unibocconi.it

Abstract

Most text-classification approaches represent
the input based on textual features, either
feature-based or continuous. However, this ig-
nores strong non-linguistic similarities like ho-
mophily: people within a demographic group
use language more similar to each other than to
non-group members. We use homophily cues
to retrofit text-based author representations
with non-linguistic information, and introduce
a trade-off parameter. This approach increases
in-class similarity between authors, and im-
proves classification performance by making
classes more linearly separable. We evalu-
ate the effect of our method on two author-
attribute prediction tasks with various training-
set sizes and parameter settings. We find that
our method can significantly improve classifi-
cation performance, especially when the num-
ber of labels is large and limited labeled data
is available. It is potentially applicable as pre-
processing step to any text-classification task.

1 Introduction
Predicting socio-demographic author characteris-
tics is becoming ever more relevant with the per-
vasive use of user-generated content. Classifying
user attributes such as age and gender is useful for
a number of applications both in the public sector,
where it can support the investigation of crime (in
forensic linguistics) or the determination of social
policies, and in the private sector, where compa-
nies want to profile a potential consumer market,
targeting communication strategies and advertis-
ing to specific communities. Furthermore, recent
work in NLP has shown that incorporating au-
thor attributes in various NLP tasks can also im-
prove performance (Volkova et al., 2013; Hovy,
2015; Hovy and Søgaard, 2015; Lynn et al., 2017;
Preotiuc-Pietro et al., 2016).

In these tasks, authors are typically represented
via their linguistic profiles, i.e., information avail-

able in the text. This includes both word-based
features as well as continuous representations
(embeddings). Generally, linguistic features are
divided into content-related and strictly stylistic
features. While the first can be effectively repre-
sented by (n-grams of) words which capture the
topic and meaning of a text, the second ones fo-
cus on the use of function words, expressions, pro-
nouns, syntactic structures, etc. There is evidence
in the literature that content-related text charac-
teristics are more effective than stylistic features
for gender and age prediction (Fatima et al., 2017;
Rosenthal and McKeown, 2011). This effect is
the consequence of a non-linguistic auto-selection
process known as homophily: people within in a
demographic group tend to be more similar to each
other than to other groups, and subjects belong-
ing to different groups are therefore naturally more
prone to discuss different topics.

Despite the large amount of available social me-
dia data (in April 2018, Facebook had more than
two billion active users, YouTube and WhatsApp
each one-and-a-half billion, and Twitter 330 mil-
lion, see statista.com), we often encounter scenar-
ios with limited availability of ground-truth user
attributes, leading to remarkable performance dif-
ferences to, say, blogs. This difference is due
to the shortness of social media texts and the
wider range of topics (Rangel et al., 2016), which
weaken linguistic profile features. In such cases,
improving author representations beyond the lin-
guistic profiles can be especially useful.

We implement this intuition of leveraging
demographic homophily by using retrofitting
(Faruqui et al., 2015), a method introduced to re-
fine word vectors to reflect semantic similarity
information from lexicons. In our case, we in-
crease the similarity between the (linguistically-
based) continuous authors representations within
each class (here: age or gender). Authors who

https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/


672

share the same gender or age therefore get more
similar vector representations (see section 3.2).
This effectively increases class-separability and
can thereby improve classification performance.

We also experiment with a trade-off param-
eter α, which controls the relative influence of
the retrofitting process vs. the original embedding
vector on the retrofit representation, allowing us to
explore the effect of both factors on the final pre-
diction outcome.

In order to extend the in-class homophily infor-
mation to unlabeled data, we induce a transforma-
tion matrix to translate between the original and
retrofitted embedding space. This matrix can be
applied to unlabeled data to transform the author
representations in the test set.

We use a set of almost 100K authors to pre-
dict age and gender. In order to explore limited-
resource scenarios, we experiment with a range
of training set sizes. Our results indicate that de-
mographic retrofitting of linguistic representations
substantially increases classification performance
for age and gender prediction, especially in low-
resource scenarios.

It is an easy, fast, and efficient preprocessing
step that can substantially improve classification
performance. We show our method for author-
attribute prediction, but believe it can potentially
be applied to any text-classification task.

Contributions In this paper, we introduce de-
mographic retrofitting based on in-class ho-
mophily, and make the following contributions:

1. we present a substantial expansion of the
original retrofitting algorithm (Faruqui et al.,
2015). In contrast to prior work, which re-
lies on external ontologies, our method relies
solely on the information contained within
the training data.

2. We show how to generalize the transforma-
tion from training data to unlabeled data, us-
ing a translation matrix.

3. We publicly release all our
data and models on our GitHub
page, https://github.com/Bocconi-
NLPLab/retrofit attributes.

2 Data
We use data from Hovy et al. (2015), a collec-
tion of reviews of online companies from various

countries, including author information. We se-
lect all reviews written in English from American
and British sources, if they include both age and
gender of the author, and if the review is at least
10 tokens long (shorter reviews tend to be mis-
tokenized URLs or replies). We aggregate the re-
views by users, so that each instance is a collec-
tion of texts’ from a unique user. This leaves us
with 98,608 individual users, and about 8M words
(roughly 80 words per instance). For each user,
we use the age (discretized by decade) and gen-
der (self-stated as binary, and augmented by Hovy
et al. (2015) based on the users’ first name) as tar-
get variables. We minimally preprocess the text
data, collapsing all numbers into 0s, and tokeniz-
ing via spacy (Honnibal and Johnson, 2015).

3 Methodology
In our experiments, we are interested in the ef-
fect of homophily-inducing retrofitting on author-
attribute prediction. In order to evaluate the effect,
we compare the performance of author representa-
tions based on linguistic input to the performance
of the same representation retrofitted to the author
attribute class in question. In this section, we out-
line the details for the different steps.

3.1 Linguistic author representations

We train Doc2Vec, a paragraph2vec (Le and
Mikolov, 2014) implementation, on the corpus, in-
ducing a 98K-by-300 matrix D, where each row
represents an author. We follow the parametriza-
tion suggested in Lau and Baldwin (2016), setting
the window size to 15, minimum word-frequency
to 10, negative samples to 5, downsampling rate to
0.00001, and run for 1000 iterations. We use the
resulting author embeddings as input to the author-
attribute classifier (see 3.4). We induce the author
embeddings over the entire corpus of 98K authors,
without recurrence to age or gender information.

As comparison, we also create a bag-of-words
(BOW) representation with the same dimensional-
ity. We use χ2 as selection criterion to find the top
300 words in the training data, separately for both
age and gender classification.

3.2 Retrofitting

Our goal is to enhance the author representations,
which are based on linguistic similarity, with de-
mographic information about the target variable
(say, age). In order to introduce this information
into the vector space, we rely on retrofitting, by in-

https://github.com/Bocconi-NLPLab/retrofit_attributes
https://github.com/Bocconi-NLPLab/retrofit_attributes
https://github.com/Bocconi-NLPLab/retrofit_attributes
https://spaCy.io/


673

(a) Non-retrofitted

(b) Retrofitted

Figure 1: Schematic representation of 500 authors
colored by age group, without (top) and with (bot-
tom) retrofitting

creasing the similarity of authors within the same
target group (say, people in their 20s). We thereby
separate the target classes in embedding space,
making them easier to differentiate by a classifier.

Faruqui et al. (2015) introduced retrofitting of
word vectors based on external ontologies, such
as WordNet (Miller, 1995) or PPDB (Ganitkevitch
et al., 2013). Instead of these resources, we map
each labeled author to the list of all other authors
with the same label in the training data. Formally,
we create a set Ω containing tuples of authors
(di, dj |yi = yj). We do this separately for each
demographic dimension - age and gender.

During retrofitting, we iteratively update the au-
thor representation in the training data (initially
linguistically-based) to increase the cosine simi-
larity between authors within the same class (as
defined in Ω). This creates a retrofitted ma-
trix D̂train of the original author matrix Dtrain.
The update for an author representation di is a
weighted combination of the original embedding

and the average over all its current neighbors:

d̂i = αdi + β

∑
j:(i,j)∈Ω d̂j

N

where di is the original linguistic representation
vector, N = |{∀j : (i, j) ∈ Ω}| is the set of
all embeddings in the same label group, and α
and β are hyper-parameters that control the trade-
off between the original representation and the
updates from the neighboring embeddings during
retrofitting. In Faruqui et al. (2015), α = β. In
contrast, we define

β = 1− α

By varying α from 0 to 1, we can control the
strength of the retrofitting process. α = 1 sim-
ply reproduces the original matrix, i.e., D̂ = D,
whereas α = 0 only relies on the neighborhood
updates after the initialization. Figure 1 shows a
sample of 500 users in a non-retrofitted (1a) and
retrofitted (1b) 3D embedding space, colored by
class. The color distribution shows how people be-
longing to the same group get drawn closer to each
other in embeddings space when using retrofitting.

3.3 Translation

We can only retrofit the embeddings of authors in
the training set Dtrain, since we need information
about the class label in order to construct Ω. How-
ever, the retrofitting process changes the configu-
ration of the embedding space (into ˆDtrain), so a
separating hyperplane learned on D̂train will not
be applicable to a test set Dtest in the original em-
bedding space.

In order to extend the homophily information to
authors in the test set, we use a translation matrix
T (a 300 × 300 matrix), which approximates the
transformation from the original training data ma-
trix Dtrain into the retrofitted matrix D̂train. We
obtain T by minimizing the least-square difference
in Dtrain · T = D̂train.
T captures the retrofitting operation, and allows

us to modify the test subjects’ representations as if
age and gender were known, despite the absence
of class information. In particular, by applying T
to the matrix of the author embeddings in the unla-
beled test set Dtest, we obtain a retrofitted version

ˆDtest that preserves the transformation learned on
the training data. Since the least-square approxi-
mation is not perfect, we find that in practice fit-
ting a classifier on the approximation Dtrain · T



674

works better than using D̂train, acting as a regu-
larizer.

3.4 Classification

We retrofit the author embeddings in the training
set (see 3.2) and learn a translation matrix to trans-
form the representations of the remaining authors
in the test set. We train three separate Logistic
Regression classifiers: one on author embeddings,
one on the retrofit embeddings, and one on BOW
features. It is technically possible to retrofit BOW
representations as well, but in practice, the classi-
fier does not converge, as word count-based vec-
tors do not represent a continuous space that cap-
tures latent similarities.

We then use the three classifiers to predict the
author attributes of the remaining authors in the
test data set. We evaluate the results via micro-F1
score (averaged over 100 runs), since our tasks in-
clude imbalanced multi-class scenarios: micro-F1
weights the contribution of each class according
to their relative size and is therefore more infor-
mative than accuracy.

Since we are interested in the effect of the train-
ing set size on performance, we vary the number of
available training examples from 1000 to 10,000,
using the remaining authors as test set. For each
training set size, we collect 100 random subsam-
ples and average over them.

4 Results
The learning curves in Figure 2 show that
retrofitting outperforms both the original author
embeddings and BOW representations for age
(top) and gender (bottom) prediction in terms of
F1. The effect is stronger when little training
data is available. We evaluate the statistical sig-
nificance of the difference between results with
retrofitting and original embeddings via a boot-
strap sampling test. We do not test against BOW,
since this is consistently lower than embeddings.
The resulting p-value are given in the respective
figures. For gender classification, there are small,
but not significant improvements with retrofitting.
By contrast, for age classification, small values of
α (0.01, 0.1, and 0.25) result in significantly better
classification than when using any other method.

The performance differences between the meth-
ods are generally more pronounced for age-
prediction, which has 10 possible labels, than for
gender prediction (two labels). The difference in
optimal α value suggests a relation between α and

label space.
In both tasks, the best result is achieved by

choosing a low α, i.e., by giving more weight to
the demographic association of the users than to
their linguistic feature representations. In prac-
tice, this value should be determined via cross-
validation: here we show different levels of α in
order to give some intuition of its on performance.
Note that the curves for the original embeddings
and BOW are unaffected by α and do not change.
We repeat them at each figure for comparison. In-
creasing α eventually converges with the original
embeddings, but we see that even intermediate val-
ues can be close to the original embeddings.

5 Related Work
The first studies to apply statistical NLP tech-
niques to author attribute prediction are Koppel
et al. (2002); Argamon et al. (2003), using the
British National Corpus (BNC). The same authors
also introduced the use of blogs as data source
(Koppel et al., 2006).

In recent years, predicting socio-demographic
variables from text has seen increased interest,
with several corpora for the classification of age
and gender, covering various languages, such as
English (Schler et al., 2006; Rosenthal and McK-
eown, 2011), Spanish, French, German, Dutch
(Company and Wanner, 2015), Greek (Mikros,
2012), Chinese (Zhang et al., 2016), and Viet-
namese (Pham et al., 2009).

A big contribution in this field, however, was
the shared tasks of the PAN workshops (Rangel
et al., 2013, 2014, 2015, 2016).

Research has identified a variety of linguis-
tic features, ranging from “stylistic features with
n-grams models, parts-of-speech, collocations,
LDA, different readability indexes, vocabulary
richness, correctness or verbosity” (Rangel et al.,
2016). However, none of these papers used demo-
graphic information directly in the author repre-
sentations.

Closest to our method are Lopez-Monroy et al.
(2013), who propose the use of second-order rep-
resentations. They created specific profiles for the
target classes, and exploited them for the creation
of the profile of each document. In both cases, the
linguistic representation of the documents passes
through a class-related profile.

The methods applied in the PAN workshops
also reflect the recent research trend towards
word embeddings, which we explore in this pa-

https://pan.webis.de/index.html
https://pan.webis.de/index.html


675

a) α = 0.01 b) α = 0.1 c) α = 0.25 d) α = 0.5 e) α = 0.75

Figure 2: Learning curves (micro-F1) for 3 classifiers on age (top) and gender prediction (bottom) for
different values of α. Retrofitting influence decreases from left to right. All data points averaged over
100 runs. Shaded area is 95%-confidence interval for retrofitting. p-values denote statistical difference
between original and retrofit embeddings according to bootstrap test.

per. Bayot and Gonçalves (2016) first used
word2vec embeddings as input features to a
SVM classifier, followed by the use of convo-
lutional (CNN) and recurrent neural networks
(RNN) by Miura et al. (2017). Markov et al.
(2016) also created document representations
through word2vec, using a Logistic Regression
classifier.

6 Conclusion

We explore retrofitting text-based author embed-
dings with non-linguistic demographic informa-
tion to increase in-class similarity. This method
increases class-separability to improve classifica-
tion performance. We use a corpus of almost 100K
users, and evaluate the effect of our method on
age and gender prediction for various levels of
available training data. We find that aggressive
retrofitting (prioritizing homophily over linguis-
tic embeddings) is beneficial for prediction per-
formance, especially when the available amount

of training data is limited. While the effect di-
minishes with increased training data size, our ap-
proach provides a simple method to incorporate
non-linguistic knowledge into author representa-
tions. For another application (introducing geo-
graphic information into city representations), see
Hovy and Purschke (2018). Our method is fast,
simple, and applicable to any problem represented
in embedding space. It is therefore a viable pre-
processing step to any text-classification task.

Acknowledgements

We would like to thank the anonymous review-
ers, whose constructive feedback improved the pa-
per, as well as Carlo Baldassi for many enlighten-
ing discussions, as well as Barbara Plank, Noah
Smith, and Afshin Rahimi, whose feedback helped
our conceptualization of the problem.



676

References
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and

Anat Rachel Shimoni. 2003. Gender, genre, and
writing style in formal written texts. TextInter-
disciplinary Journal for the Study of Discourse,
23(3):321–346.

Roy Bayot and Teresa Gonçalves. 2016. Author Pro-
filing using SVMs and Word Embedding Averages-
Notebook for PAN at CLEF 2016.

Juan Soler Company and Leo Wanner. 2015. Multiple
Language Gender Identification for Blog Posts. In
CogSci.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1606–1615.

Mehwish Fatima, Komal Hasan, Saba Anwar, and Rao
Muhammad Adeel Nawab. 2017. Multilingual au-
thor profiling on Facebook. Information Processing
& Management, 53(4):886–904.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 758–764.

Matthew Honnibal and Mark Johnson. 2015. An im-
proved non-monotonic transition system for depen-
dency parsing. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1373–1378, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 752–762.

Dirk Hovy, Anders Johannsen, and Anders Søgaard.
2015. User review sites as a resource for large-
scale sociolinguistic studies. In Proceedings of the
24th International Conference on World Wide Web,
pages 452–461. International World Wide Web Con-
ferences Steering Committee.

Dirk Hovy and Christoph Purschke. 2018. Capturing
Regional Variation with Distributed Place Represen-
tations and Geographic Retrofitting. In Proceedings
of EMNLP.

Dirk Hovy and Anders Søgaard. 2015. Tagging perfor-
mance correlates with author age. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International

Joint Conference on Natural Language Processing
(Volume 2: Short Papers), volume 2, pages 483–488.

Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401–412.

Moshe Koppel, Jonathan Schler, Shlomo Argamon,
and James Pennebaker. 2006. Effects of age and
gender on blogging. In AAAI 2006 Spring Sym-
posium on Computational Approaches to Analysing
Weblogs.

Jey Han Lau and Timothy Baldwin. 2016. An empiri-
cal evaluation of doc2vec with practical insights into
document embedding generation. page 78.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1188–1196.

A Pastor Lopez-Monroy, Manuel Montes-Y-Gomez,
Hugo Jair Escalante, Luis Villasenor-Pineda, and
Esau Villatoro-Tello. 2013. INAOEs participation at
PAN13: Author profiling task. In CLEF 2013 eval-
uation labs and workshop.

Veronica Lynn, Youngseo Son, Vivek Kulkarni, Ni-
ranjan Balasubramanian, and H Andrew Schwartz.
2017. Human Centered NLP with User-Factor
Adaptation. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1146–1155.

Ilia Markov, Helena Gómez-Adorno, Juan-Pablo
Posadas-Durán, Grigori Sidorov, and Alexander
Gelbukh. 2016. Author profiling with doc2vec neu-
ral network-based document embeddings. In Mex-
ican International Conference on Artificial Intelli-
gence, pages 117–131. Springer.

George K Mikros. 2012. Authorship attribution and
gender identification in Greek blogs. Methods and
Applications of Quantitative Linguistics, 21:21–32.

George A Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41.

Yasuhide Miura, Tomoki Taniguchi, Motoki Taniguchi,
and Tomoko Ohkuma. 2017. Author profiling with
word+ character neural attention network. Cappel-
lato et al.[13].

Dang Duc Pham, Giang Binh Tran, and Son Bao Pham.
2009. Author profiling for Vietnamese blogs. In
Asian Language Processing, 2009. IALP’09. Inter-
national Conference on, pages 190–194. IEEE.

Daniel Preotiuc-Pietro, Wei Xu, and Lyle H Ungar.
2016. Discovering user attribute stylistic differences
via paraphrasing. In AAAI, pages 3030–3037.



677

Francisco Rangel, Paolo Rosso, Irina Chugur, Martin
Potthast, Martin Trenkmann, Benno Stein, Ben Ver-
hoeven, and Walter Daelemans. 2014. Overview of
the 2nd author profiling task at pan 2014. In CLEF
2014 Evaluation Labs and Workshop Working Notes
Papers, Sheffield, UK, 2014, pages 1–30.

Francisco Rangel, Paolo Rosso, Moshe Koppel, Ef-
stathios Stamatatos, and Giacomo Inches. 2013.
Overview of the author profiling task at PAN 2013.
In CLEF Conference on Multilingual and Multi-
modal Information Access Evaluation, pages 352–
365. CELCT.

Francisco Rangel, Paolo Rosso, Martin Potthast,
Benno Stein, and Walter Daelemans. 2015.
Overview of the 3rd Author Profiling Task at PAN
2015. In CLEF, page 2015. sn.

Francisco Rangel, Paolo Rosso, Ben Verhoeven, Walter
Daelemans, Martin Potthast, and Benno Stein. 2016.
Overview of the 4th author profiling task at PAN
2016: cross-genre evaluations. In Working Notes
Papers of the CLEF 2016 Evaluation Labs. CEUR
Workshop Proceedings/Balog, Krisztian [edit.]; et
al., pages 750–784.

Sara Rosenthal and Kathleen McKeown. 2011. Age
prediction in blogs: A study of style, content, and
online behavior in pre-and post-social media genera-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 763–
772. Association for Computational Linguistics.

Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W Pennebaker. 2006. Effects of age
and gender on blogging. In AAAI spring sympo-
sium: Computational approaches to analyzing we-
blogs, volume 6, pages 199–205.

Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment anal-
ysis in social media. In Proceedings of EMNLP,
pages 1815–1827.

Wanru Zhang, Andrew Caines, Dimitrios Alikaniotis,
and Paula Buttery. 2016. Predicting Author Age
from Weibo Microblog Posts. In LREC.


