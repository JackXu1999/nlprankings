



















































Sometimes Average is Best: The Importance of Averaging for Prediction using MCMC Inference in Topic Modeling


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752–1757,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Sometimes Average is Best: The Importance of Averaging for Prediction
using MCMC Inference in Topic Modeling

Viet-An Nguyen
Computer Science

University of Maryland
College Park, MD

vietan@cs.umd.edu

Jordan Boyd-Graber
Computer Science

University of Colorado
Boulder, CO

jbg@boydgraber.org

Philip Resnik
Linguistics and UMIACS
University of Maryland

College Park, MD
resnik@umd.edu

Abstract
Markov chain Monte Carlo (MCMC) approxi-
mates the posterior distribution of latent vari-
able models by generating many samples and
averaging over them. In practice, however, it
is often more convenient to cut corners, using
only a single sample or following a suboptimal
averaging strategy. We systematically study dif-
ferent strategies for averaging MCMC samples
and show empirically that averaging properly
leads to significant improvements in prediction.

1 Introduction
Probabilistic topic models are powerful methods to un-
cover hidden thematic structures in text by projecting
each document into a low dimensional space spanned
by a set of topics, each of which is a distribution over
words. Topic models such as latent Dirichlet alloca-
tion (Blei et al., 2003, LDA) and its extensions discover
these topics from text, which allows for effective ex-
ploration, analysis, and summarization of the otherwise
unstructured corpora (Blei, 2012; Blei, 2014).

In addition to exploratory data analysis, a typical goal
of topic models is prediction. Given a set of unanno-
tated training data, unsupervised topic models try to
learn good topics that can generalize to unseen text.
Supervised topic models jointly capture both the text
and associated metadata such as a continuous response
variable (Blei and McAuliffe, 2007; Zhu et al., 2009;
Nguyen et al., 2013), single label (Rosen-Zvi et al.,
2004; Lacoste-Julien et al., 2008; Wang et al., 2009)
or multiple labels (Ramage et al., 2009; Ramage et al.,
2011) to predict metadata from text.

Probabilistic topic modeling requires estimating the
posterior distribution. Exact computation of the poste-
rior is often intractable, which motivates approximate
inference techniques (Asuncion et al., 2009). One popu-
lar approach is Markov chain Monte Carlo (MCMC), a
class of inference algorithms to approximate the target
posterior distribution. To make prediction, MCMC al-
gorithms generate samples on training data to estimate
corpus-level latent variables, and use them to generate
samples to estimate document-level latent variables for
test data. The underlying theory requires averaging on
both training and test samples, but in practice it is often
convenient to cut corners: either skip averaging entirely
by using just the values of the last sample or use a single
training sample and average over test samples.

We systematically study non-averaging and averaging
strategies when performing predictions using MCMC in
topic modeling (Section 2). Using popular unsupervised
(LDA in Section 3) and supervised (SLDA in Section 4)
topic models via thorough experimentation, we show
empirically that cutting corners on averaging leads to
consistently poorer prediction.

2 Learning and Predicting with MCMC
While reviewing all of MCMC is beyond the scope of
this paper, we need to briefly review key concepts.1 To
estimate a target density p(x) in a high-dimensional
space X , MCMC generates samples {xt}Tt=1 while ex-
ploring X using the Markov assumption. Under this
assumption, sample xt+1 depends on sample xt only,
forming a Markov chain, which allows the sampler to
spend more time in the most important regions of the
density. Two concepts control sample collection:

Burn-in B: Depending on the initial value of the
Markov chain, MCMC algorithms take time to reach
the target distribution. Thus, in practice, samples before
a burn-in period B are often discarded.

Sample-lag L: Averaging over samples to estimate
the target distribution requires i.i.d. samples. However,
future samples depend on the current samples (i.e., the
Markov assumption). To avoid autocorrelation, we dis-
card all but every L samples.

2.1 MCMC in Topic Modeling
As generative probabilistic models, topic models define
a joint distribution over latent variables and observable
evidence. In our setting, the latent variables consist of
corpus-level global variables g and document-level lo-
cal variables l; while the evidence consists of words w
and additional metadata y—the latter omitted in unsu-
pervised models.

During training, MCMC estimates the posterior
p(g, lTR |wTR,yTR) by generating a training Markov
chain of TTR samples.2 Each training sample i pro-
vides a set of fully realized global latent variables ĝ(i),
which can generate test data. During test time, given a

1For more details please refer to Neal (1993), Andrieu et
al. (2003), Resnik and Hardisty (2010).

2We omit hyperparameters for clarity. We split data into
training (TR) and testing (TE) folds, and denote the training
iteration i and the testing iteration j within the corresponding
Markov chains.

1752



Training burn-in Btr Training lag LTR Training lag Ltr

Training period Ttr

T
es

t
bu

rn
-i
n

B
t
e

T
es

t
pe

ri
od

T
t
e

T
es

t
la

g
L

t
e

1 2

3 4

2

4

2

4

3 43 4

44

4 4

1

2

3

4

Samples used in Single Final (SF)

Samples used in Single Average (SA)

Samples used in Multiple Final (MF)

Samples used in Multiple Average (MA)

Training chain

Single test chains

sample i in
training chain

(learned model)

test chain i

sample j in 
test chain i 

(prediction S(i,j))

Discarded samples during training

Discarded samples during test

Selected samples during training

Selected samples during test

Figure 1: Illustration of training and test chains in MCMC, showing samples used in four prediction strategies studied
in this paper: Single Final (SF), Single Average (SA), Multiple Final (MF), and Multiple Average (MA).

learned model from training sample i, we generate a test
Markov chain of TTE samples to estimate the local latent
variables p(lTE |wTE, ĝ(i)) of test data. Each sample
j of test chain i provides a fully estimated local latent
variables l̂TE(i, j) to make a prediction.

Figure 1 shows an overview. To reduce the ef-
fects of unconverged and autocorrelated samples, dur-
ing training we use a burn-in period of BTR and a
sample-lag of LTR iterations. We use TTR = {i | i ∈
(BTR, TTR] ∧ (i − BTR) mod LTR = 0} to denote the
set of indices of the selected models. Similarly, BTE
and LTE are the test burn-in and sample-lag. The
set of indices of selected samples in test chains is
TTE = {j | j ∈ (BTE, TTE] ∧ (j −BTE) mod LTE = 0}.
2.2 Averaging Strategies
We use S(i, j) to denote the prediction obtained from
sample j of the test chain i. We now discuss different
strategies to obtain the final prediction:

• Single Final (SF) uses the last sample of last test
chain to obtain the predicted value,

SSF = S(TTR, TTE). (1)

• Single Average (SA) averages over multiple sam-
ples in the last test chain

SSA =
1

|TTE|
∑

j∈TTE
S(TTR, j). (2)

This is a common averaging strategy in which we
obtain a point estimate of the global latent variables
at the end of the training chain. Then, a single test
chain is generated on the test data and multiple sam-
ples of this test chain are averaged to obtain the final
prediction (Chang, 2012; Singh et al., 2012; Jiang et
al., 2012; Zhu et al., 2014).

• Multiple Final (MF) averages over the last sam-
ples of multiple test chains from multiple models

SMF =
1

|TTR|
∑

i∈TTR
S(i, TTE). (3)

• Multiple Average (MA) averages over all samples
of multiple test chains for distinct models,

SMA =
1

|TTR|
1

|TTE|
∑

i∈TTR

∑
j∈TTE

S(i, j), (4)

3 Unsupervised Topic Models
We evaluate the predictive performance of the unsu-
pervised topic model LDA using different averaging
strategies in Section 2.

LDA: Proposed by Blei et al. in 2003, LDA posits that
each document d is a multinomial distribution θd over
K topics, each of which is a multinomial distribution
φk over the vocabulary. LDA’s generative process is:

1. For each topic k ∈ [1,K]
(a) Draw word distribution φk ∼ Dir(β)

2. For each document d ∈ [1, D]
(a) Draw topic distribution θd ∼ Dir(α)
(b) For each word n ∈ [1, Nd]

i. Draw topic zd,n ∼ Mult(θd)
ii. Draw word wd,n ∼ Mult(φzd,n)

In LDA, the global latent variables are topics {φk}Kk=1
and the local latent variables for each document d are
topic proportions θd.

Train: During training, we use collapsed Gibbs sam-
pling to assign each token in the training data with a
topic (Steyvers and Griffiths, 2006). The probability of

1753



assigning token n of training document d to topic k is
p(zTRd,n = k | zTR−d,n,wTR−d,n, wTRd,n = v) ∝

N−d,nTR,d,k + α

N−d,nTR,d,· +Kα
· N

−d,n
TR,k,v + β

N−d,nTR,k,· + V β
, (5)

where NTR,d,k is the number of tokens in the training
document d assigned to topic k, and NTR,k,v is the num-
ber of times word type v assigned to topic k. Marginal
counts are denoted by ·, and −d,n denotes the count
excluding the assignment of token n in document d.

At each training iteration i, we estimate the distribu-
tion over words φ̂k(i) of topic k as

φ̂k,v(i) =
NTR,k,v(i) + β
NTR,k,·(i) + V β

(6)

where the counts NTR,k,v(i) and NTR,k,·(i) are taken at
training iteration i.

Test: Because we lack explicit topic annotations for
these data (c.f. Nguyen et al. (2012)), we use perplexity–
a widely-used metric to measure the predictive power
of topic models on held-old documents. To compute
perplexity, we follow the estimating θ method (Wal-
lach et al., 2009, Section 5.1) and evenly split each test
document d into wTE1d and w

TE2
d . We first run Gibbs

sampling on wTE1d to estimate the topic proportion θ̂
TE
d

of test document d. The probability of assigning topic k
to token n inwTE1d is p(z

TE1
d,n = k | zTE1−d,n,wTE1 , φ̂(i)) ∝

N−d,nTE1,d,k + α

N−d,nTE1,d,· +Kα
· φ̂

k,w
TE1
d,n(i)

(7)

whereNTE1,d,k is the number of tokens inw
TE1
d assigned

to topic k. At each iteration j in test chain i, we can
estimate the topic proportion vector θ̂TEd (i, j) for test
document d as

θ̂TEd,k(i, j) =
NTE1,d,k(i, j) + α
NTE1,d,·(i, j) +Kα

(8)

where both the counts NTE1,d,k(i, j) and NTE1,d,·(i, j)
are taken using sample j of test chain i.

Prediction: Given θ̂TEd (i, j) and φ̂(i) at sample j
of test chain i, we compute the predicted likeli-
hood for each unseen token wTE2d,n as S(i, j) ≡
p(wTE2d,n | θ̂TEd (i, j), φ̂(i)) =

∑K
k=1 θ̂

TE
d,k(i, j) · φ̂k,wTE2d,n(i).

Using different strategies described in Section 2,
we obtain the final predicted likelihood for each un-
seen token p(wTE2d,n | θ̂TEd , φ̂) and compute the perplex-
ity as exp

(
−(∑d∑n log(p(wTE2d,n | θ̂TEd , φ̂)))/N TE2)

where N TE2 is the number of tokens in wTE2 .

Setup: We use three Internet review datasets in our
experiment. For all datasets, we preprocess by tokeniz-
ing, removing stopwords, stemming, adding bigrams to

●

●

●
● ● ● ● ● ● ●

●

●

●
●

● ● ● ● ● ●

●

●

●
●

● ● ● ● ● ●

Restaurant Reviews

Movie Reviews

Hotel Reviews

1160

1200

1240

1950

2000

2050

2100

2150

750

775

800

600 700 800 900 1000

600 700 800 900 1000

600 700 800 900 1000
Number of training iterations

P
er

pl
ex

ity

●Multiple−Average Multiple−Final Single−Average Single−Final

Figure 2: Perplexity of LDA using different averaging
strategies with different number of training iterations
TTR. Perplexity generally decreases with additional
training iterations, but the drop is more pronounced
with multiple test chains.

the vocabulary, and we filter using TF-IDF to obtain a
vocabulary of 10,000 words.3 The three datasets are:

• HOTEL: 240,060 reviews of hotels from TripAdvi-
sor (Wang et al., 2010).
• RESTAURANT: 25,459 reviews of restaurants from
Yelp (Jo and Oh, 2011).
• MOVIE: 5,006 reviews of movies from Rotten
Tomatoes (Pang and Lee, 2005)

We report cross-validated average performance over
five folds, and use K = 50 topics for all datasets. To
update the hyperparameters, we use slice sampling (Wal-
lach, 2008, p. 62).4

Results: Figure 2 shows the perplexity of the four
averaging methods, computed with different number
of training iterations TTR. SA outperforms SF, showing
the benefits of averaging over multiple test samples
from a single test chain. However, both multiple chain
methods (MF and MA) significantly outperform these
two methods.

This result is consistent with Asuncion et al. (2009),
who run multiple training chains but a single test chain
for each training chain and average over them. This
is more costly since training chains are usually signif-
icantly longer than test chains. In addition, multiple
training chains are sensitive to their initialization.

3To find bigrams, we begin with bigram candidates that
occur at least 10 times in the corpus and use a χ2 test to filter
out those having a χ2 value less than 5. We then treat selected
bigrams as single word types and add them to the vocabulary.

4MCMC setup: TTR = 1, 000, BTR = 500, LTR = 50,
TTE = 100, BTE = 50 and LTE = 5.

1754



MSE

pR.squared

0.60

0.65

0.70

0.75

0.25

0.30

0.35

0.40

1000 2000 3000 4000 5000

1000 2000 3000 4000 5000
Number of iterations

(a) Restaurant reviews
MSE

pR.squared

9000

10000

11000

12000

13000

30000

31000

32000

33000

34000

1000 2000 3000 4000 5000

1000 2000 3000 4000 5000
Number of iterations

(b) Movie reviews
MSE

pR.squared

0.400

0.425

0.450

0.475

0.500

0.500

0.525

0.550

0.575

0.600

600 700 800 900 1000

600 700 800 900 1000
Number of iterations

(c) Hotel reviews

Multiple Average

Multiple Final

Single Average

Single Final

Figure 3: Performance of SLDA using different averaging strategies computed at each training iteration.

4 Supervised Topic Models
We evaluate the performance of different prediction
methods using supervised latent Dirichlet allocation
(SLDA) (Blei and McAuliffe, 2007) for sentiment anal-
ysis: predicting review ratings given review text. Each
review text is the document wd and the metadata yd is
the associated rating.

SLDA: Going beyond LDA, SLDA captures the rela-
tionship between latent topics and metadata by mod-
eling each document’s continuous response variable
using a normal linear model, whose covariates are
the document’s empirical distribution of topics: yd ∼
N (ηT z̄d, ρ) where η is the regression parameter vec-
tor and z̄d is the empirical distribution over topics of
document d. The generative process of SLDA is:

1. For each topic k ∈ [1,K]
(a) Draw word distribution φk ∼ Dir(β)
(b) Draw parameter ηk ∼ N (µ, σ)

2. For each document d ∈ [1, D]
(a) Draw topic distribution θd ∼ Dir(α)
(b) For each word n ∈ [1, Nd]

i. Draw topic zd,n ∼ Mult(θd)
ii. Draw word wd,n ∼ Mult(φzd,n)

(c) Draw response yd ∼ N (ηT z̄d, ρ) where
z̄d,k = 1Nd

∑Nd
n=1 I [zd,n = k]

where I [x] = 1 if x is true, and 0 otherwise.
In SLDA, in addition to the K multinomials {φk}Kk=1,

the global latent variables also contain the regression
parameter ηk for each topic k. The local latent variables
of SLDA resembles LDA’s: the topic proportion vector
θd for each document d.

Train: For posterior inference during training, follow-
ing Boyd-Graber and Resnik (2010), we use stochastic
EM, which alternates between (1) a Gibbs sampling

step to assign a topic to each token, and (2) optimizing
the regression parameters. The probability of assigning
topic k to token n in the training document d is
p(zTRd,n = k | zTR−d,n,wTR−d,n, wTRd,n = v) ∝

N (yd;µd,n, ρ) ·
N−d,nTR,d,k + α

N−d,nTR,d,· +Kα
· N

−d,n
TR,k,v + β

N−d,nTR,k,· + V β
(9)

where µd,n = (
∑K

k′=1 ηk′N
−d,n
TR,d,k′ + ηk)/NTR,d is the

mean of the Gaussian generating yd if zTRd,n = k. Here,
NTR,d,k is the number of times topic k is assigned to
tokens in the training document d;NTR,k,v is the number
of times word type v is assigned to topic k; · represents
marginal counts and −d,n indicates counts excluding the
assignment of token n in document d.

We optimize the regression parameters η using L-
BFGS (Liu and Nocedal, 1989) via the likelihood

L(η) = − 1
2ρ

D∑
d=1

(yTRd −ηT z̄TRd )2− 1
2σ

K∑
k=1

(ηk−µ)2 (10)

At each iteration i in the training chain, the estimated
global latent variables include the a multinomial φ̂k(i)
and a regression parameter η̂k(i) for each topic k.

Test: Like LDA, at test time we sample the topic as-
signments for all tokens in the test data

p(zTEd,n = k | zTE−d,n,wTE) ∝
N−d,nTE,d,k + α

N−d,nTE,d,· +Kα
· φ̂k,wTEd,n

(11)

Prediction: The predicted value S(i, j) in this case is
the estimated value of the metadata review rating

S(i, j) ≡ ŷTEd (i, j) = η̂(i)T z̄TEd (i, j), (12)
where the empirical topic distribution of test document d
is z̄TEd,k(i, j) ≡ 1NTE,d

∑NTE,d
n=1 I

[
zTEd,n(i, j) = k

]
.

1755



MSE

pR−squared

0.60

0.65

0.70

0.30

0.35

0.40

50 100 150 200

50 100 150 200
Number of Topics

(a) Restaurant reviews
MSE

pR−squared

0.60

0.70

0.80

0.90

0.00

0.10

0.20

0.30

0.40

40 60 80

40 60 80
Number of Topics

(a) Restaurant reviews
MSE

pR−squared

0.40

0.42

0.44

0.46

0.48

0.52

0.54

0.56

0.58

0.60

50 100 150 200

50 100 150 200
Number of Topics

(a) Restaurant reviews

MLR

SLDA−MA

SLDA−MF

SLDA−SA

SLDA−SF

SVR

Figure 4: Performance of SLDA using different averaging strategies computed at the final training iteration TTR,
compared with two baselines MLR and SVR. Methods using multiple test chains (MF and MA) perform as well as or
better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse.

Experimental setup: We use the same data as in Sec-
tion 3. For all datasets, the metadata are the review
rating, ranging from 1 to 5 stars, which is standard-
ized using z-normalization. We use two evaluation
metrics: mean squared error (MSE) and predictive R-
squared (Blei and McAuliffe, 2007).

For comparison, we consider two baselines: (1) multi-
ple linear regression (MLR), which models the metadata
as a linear function of the features, and (2) support vec-
tor regression (Joachims, 1999, SVR). Both baselines
use the normalized frequencies of unigrams and bigrams
as features. As in the unsupervised case, we report av-
erage performance over five cross-validated folds. For
all models, we use a development set to tune their pa-
rameter(s) and use the set of parameters that gives best
results on the development data at test.5

Results: Figure 3 shows SLDA prediction results with
different averaging strategies, computed at different
training iterations.6 Consistent with the unsupervised
results in Section 3, SA outperforms SF, but both are
outperformed significantly by the two methods using
multiple test chains (MF and MA).

We also compare the performance of the four pre-
diction methods obtained at the final iteration TTR of
the training chain with the two baselines. The results in
Figure 4 show that the two baselines (MLR and SVR) out-
perform significantly the SLDA using only a single test

5For MLR we use a Gaussian prior N (0, 1/λ) with λ =
a · 10b where a ∈ [1, 9] and b ∈ [1, 4]; for SVR, we use
SVMlight (Joachims, 1999) and vary C ∈ [1, 50], which
trades off between training error and margin; for SLDA, we fix
σ = 10 and vary ρ ∈ {0.1, 0.5, 1.0, 1.5, 2.0}, which trades
off between the likelihood of words and response variable.

6MCMC setup: TTR = 5, 000 for RESTAURANT and
MOVIE and 1, 000 for HOTEL; for all datasets BTR = 500,
LTR = 50, TTE = 100, BTE = 20 and LTE = 5.

chains (SF and SA). Methods using multiple test chains
(MF and MA), on the other hand, match the baseline 7

(HOTEL) or do better (RESTAURANT and MOVIE).

5 Discussion and Conclusion
MCMC relies on averaging multiple samples to approxi-
mate target densities. When used for prediction, MCMC
needs to generate and average over both training sam-
ples to learn from training data and test samples to make
prediction. We have shown that simple averaging—not
more aggressive, ad hoc approximations like taking the
final sample (either training or test)—is not just a ques-
tion of theoretical aesthetics, but an important factor in
obtaining good prediction performance.

Compared with SVR and MLR baselines, SLDA using
multiple test chains (MF and MA) performs as well as
or better, while SLDA using a single test chain (SF and
SA) falters. This simple experimental setup choice can
determine whether a model improves over reasonable
baselines. In addition, better prediction with shorter
training is possible with multiple test chains. Thus, we
conclude that averaging using multiple chains produces
above-average results.

Acknowledgments
We thank Jonathan Chang, Ke Zhai and Mohit Iyyer for
helpful discussions, and thank the anonymous reviewers
for insightful comments. This research was supported
in part by NSF under grant #1211153 (Resnik) and
#1018625 (Boyd-Graber and Resnik). Any opinions,
findings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.

7This gap is because SLDA has not converged after 1,000
training iterations (Figure 3).

1756



References
Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and

Michael I. Jordan. 2003. An introduction to MCMC for
machine learning. Machine Learning, 50(1-2):5–43.

Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference for
topic models. In UAI.

David M. Blei and Jon D. McAuliffe. 2007. Supervised topic
models. In NIPS.

David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent
Dirichlet allocation. JMLR, 3.

David M. Blei. 2012. Probabilistic topic models. Commun.
ACM, 55(4):77–84, April.

David M. Blei. 2014. Build, compute, critique, repeat: Data
analysis with latent variable models. Annual Review of
Statistics and Its Application, 1(1):203–232.

Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sen-
timent analysis across languages: Multilingual supervised
latent Dirichlet allocation. In EMNLP.

Jonathan Chang. 2012. lda: Collapsed Gibbs sampling meth-
ods for topic models. http://cran.r-project.
org/web/packages/lda/index.html. [Online;
accessed 02-June-2014].

Qixia Jiang, Jun Zhu, Maosong Sun, and Eric P. Xing. 2012.
Monte Carlo methods for maximum margin supervised
topic models. In NIPS.

Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment
unification model for online review analysis. In WSDM.

Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support Vector
Learning, chapter 11. Cambridge, MA.

Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008.
DiscLDA: Discriminative learning for dimensionality re-
duction and classification. In NIPS.

D. Liu and J. Nocedal. 1989. On the limited memory BFGS
method for large scale optimization. Math. Prog.

Radford M. Neal. 1993. Probabilistic inference using Markov
chain Monte Carlo methods. Technical Report CRG-TR-
93-1, University of Toronto.

Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2012. SITS: A hierarchical nonparametric model using
speaker identity for topic segmentation in multiparty con-
versations. In ACL.

Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In Neural
Information Processing Systems.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In ACL.

Daniel Ramage, David Hall, Ramesh Nallapati, and Christo-
pher Manning. 2009. Labeled LDA: A supervised topic
model for credit attribution in multi-labeled corpora. In
EMNLP.

Daniel Ramage, Christopher D. Manning, and Susan Dumais.
2011. Partially labeled topic models for interpretable text
mining. In KDD, pages 457–465.

Philip Resnik and Eric Hardisty. 2010. Gibbs
sampling for the uninitiated. Technical Report
UMIACS-TR-2010-04, University of Maryland.
http://drum.lib.umd.edu//handle/1903/10058.

Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for authors
and documents. In UAI.

Sameer Singh, Michael Wick, and Andrew McCallum. 2012.
Monte Carlo MCMC: Efficient inference by approximate
sampling. In EMNLP, pages 1104–1113.

Mark Steyvers and Tom Griffiths. 2006. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road to
Meaning. Laurence Erlbaum.

Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic models.
In Leon Bottou and Michael Littman, editors, ICML.

Hanna M Wallach. 2008. Structured Topic Models for Lan-
guage. Ph.D. thesis, University of Cambridge.

Chong Wang, David Blei, and Li Fei-Fei. 2009. Simultaneous
image classification and annotation. In CVPR.

Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. La-
tent aspect rating analysis on review text data: A rating
regression approach. In SIGKDD, pages 783–792.

Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA:
maximum margin supervised topic models for regression
and classification. In ICML.

Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang. 2014.
Gibbs max-margin topic models with data augmentation.
Journal of Machine Learning Research, 15:1073–1110.

1757


