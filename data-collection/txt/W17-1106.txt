



















































A Twitter Corpus and Benchmark Resources for German Sentiment Analysis


Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media , pages 45–51,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

A Twitter Corpus and Benchmark Resources for German Sentiment
Analysis

Mark Cieliebak
SpinningBytes

mc@spinningbytes.com

Jan Deriu
Zurich University of Applied Sciences

deri@zhaw.ch

Dominic Egger
Zurich University of Applied Sciences

eggo@zhaw.ch

Fatih Uzdilli
Zurich University of Applied Sciences

uzdi@zhaw.ch

Abstract

In this paper we present SB10k, a new
corpus for sentiment analysis with approx.
10,000 German tweets.
We use this new corpus and two existing
corpora to provide state-of-the-art bench-
marks for sentiment analysis in German:
we implemented a CNN (based on the
winning system of SemEval-2016) and
a feature-based SVM and compare their
performance on all three corpora.
For the CNN, we also created German
word embeddings trained on 300M
tweets. These word embeddings were
then optimized for sentiment analysis
using distant-supervised learning.
The new corpus, the German word
embeddings (plain and optimized), and
source code to re-run the benchmarks are
publicly available.

1 Introduction

With the advance of deep learning in text analyt-
ics, many benchmarks for text analytics tasks have
been significantly improved in the last four years.
For this reason, Zurich University of Applied Sci-
ences (ZHAW) and SpinningBytes AG are collab-
orating in a joint research project to develop state-
of-the-art solutions for text analytics tasks in sev-
eral European languages. The goal is to adapt and
optimize algorithms for tasks like sentiment analy-
sis, named entity recognition (NER), topic extrac-
tion etc. into industry-ready software libraries.

One very challenging task is automatic senti-
ment analysis. The goal of sentiment analysis is
to classify a text into the classes positive, negative,
mixed, or neutral. Interest in automatic sentiment
analysis has recently increased in both academia

and industry due to the huge number of docu-
ments which are publicly available on social me-
dia. In fact, there exist various initiatives in the
scientific community (such as shared tasks at Se-
mEval (Nakov et al., 2016) or TREC (Ounis et
al., 2008)), competitions at Kaggle1, special tracks
at major conferences like EMNLP or LREC, and
several companies have built commercial senti-
ment analysis tools (Cieliebak et al., 2013).

Deep learning for sentiment analysis. Deep
neural networks have become very successful for
sentiment analysis. In fact, the winner and many
top-ranked systems in SemEval-2016 were using
deep neural networks (SemEval is an international
competition that runs every year several tasks for
semantic evaluation, including sentiment analysis)
(Nakov et al., 2016). The winning system uses
a multi-layer convolutional neural network that is
trained in three phases. For English, this system
achieves an F1-score of 62.7% on the test data of
SemEval-2016 (Deriu et al., 2016), and top scores
on test data from previous years. For this reason,
we decided to adapt the system for sentiment anal-
ysis in German. Details are described in Section 4.

A new corpus for German sentiment. In or-
der to train the CNN, millions of unlabeled and
weakly-labeled German tweets are used for creat-
ing the word embeddings. In addition, a sufficient
amount of manually labeled tweets is required to
train and optimize the system. For languages such
as English, Chinese or Arabic, there exist plenty of
labeled training data for sentiment analysis, while
for other European languages, the resources are
often very limited (cf. ”Related Work”). For Ger-
man, in particular, we are only aware of three sen-
timent corpora of significant size: the DAI tweet
data set, which contains 1800 German tweets with
tweet-level sentiments (Narr et al., 2012); the

1https://www.kaggle.com/c/sentiment-analysis-on-
movie-reviews

45



MGS corpus, which contains 109,130 German
tweets (Mozetič et al., 2016); and the PotTS cor-
pus, which contains 7992 German tweets that were
annotated on phrase level (Sidarenka, 2016). Un-
fortunately, the first corpus is too small for train-
ing a sentiment system, the the second corpus has
a very low inter-annotator agreement (α = 0.34),
indicating low-quality annotations, and the third
corpus is not on sentence level.

For this reason, we decided to construct a
large sentiment corpus with German tweets, called
SB10k. This corpus should allow to train high-
quality machine learning classifiers. It contains
9783 German tweets, each labeled by three anno-
tators. Details of corpus construction and proper-
ties are described in Section 3.

Benchmark for German Sentiment. We
evaluate the performance of the CNN on the three
German sentiment corpora CAI, MGS, and SB10k
in Section 5. In addition, we compare the results
to a baseline system, a feature-based Support
Vector Machine (SVM). To our knowledge, this
is the first large-scale benchmark for sentiment
analysis on German tweets.

Main Contributions. Our main contributions
are:

• Benchmarks for sentiment analysis in Ger-
man on three corpora.

• A new corpus SB10k for German sentiment
with approx. 10000 tweets, manually labeled
by three annotators.

• Publicly available word embeddings trained
on 300M million German tweets (using
word2vec), and modified word embeddings
after distant-supervised learning with 40M
million weakly-labeled sentiment tweets.

The new corpus, word embeddings for Ger-
man (plain and fully-trained) and source code
to re-run the benchmarks are available at
www.spinningbytes.com/resources.

2 Related Work

There exists a tremendous amount of literature on
sentiment analysis in general; for a good introduc-
tion and overview, see the recent book by Bing Liu
(Zhao et al., 2016).

Corpora. Several human labeled corpora for
sentiment analysis are available, which differ in:
languages they cover, size, annotation schemes
(number of annotators, sentiment), and document
domains (tweets, news, blogs, product reviews
etc.). For English there exist various corpora,
e.g. for tweets (Narr et al., 2012), product re-
views (Hu and Liu, 2004) or news (Wiebe et al.,
2005), and sentiment corpora exist also for other
European languges such as Italian (Straniscim et
al., 2016), French (Bosco et al., 2016), Spanish
(Martinez-Camara et al., 2016; Martinez-Camara
et al., 2015) or Dutch (Verhoeven and Daelemans,
2014).

Sentiment Analysis in German. German is the
most-spoken native language in Europe2, and sev-
eral research activities and events are focussed on
German sentiment analysis. The Interest Group
on German Sentiment Analysis (IGGSA)is a Eu-
ropean collaboration of researchers working on
German sentiment analysis. Among other things,
they hosted several workshops and shared tasks
on German Sentiment analysis, e.g. GESTALT-
2014 (Ruppenhofer et al., 2014). For an extended
list of publications on sentiment analysis in Ger-
man, we refer the reader to IGGSA3 .

Machine Learning for Sentiment Analy-
sis. Until recently, feature-based systems were
frequently used for sentiment analysis. In fact,
almost all systems participating in SemEval-2014
were feature-based, with SVM, MaxEnt, and
Naive Bayes being the most popular classifiers in
the competition (Rosenthal et al., 2014). However,
neural networks have shown great promise in NLP
over the past few years. Examples are in semantic
analysis (Shen et al., 2014), machine translation
(Gao et al., 2014) and sentiment analysis (Socher
et al., 2013). In particular, shallow convolutional
neural networks (CNNs) have recently improved
the state-of-the-art in text polarity classification
demonstrating a significant increase in terms of
accuracy compared to previous state-of-the-art
techniques (Kim, 2014; Kalchbrenner et al.,
2014; dos Santos and Gatti, 2014; Severyn and
Moschitti, 2015; Johnson and Zhang, 2015; Rothe
et al., 2016; Deriu et al., 2017).

2www.languageknowledge.eu
3https://sites.google.com/site/iggsahome/

46



3 Corpus Construction

3.1 Goals

We constructed a new sentiment corpus with Ger-
man tweets, called SB10k. This corpus should al-
low to train high-quality machine learning clas-
sifiers. Based on our experiences with machine
learning in other languages, we aimed at the fol-
lowing goals:

• The corpus should contain 10000 tweets, to
provide sufficient data for complex system to
be trained

• Selected tweets should cover a wide variety
of unigrams and topics

• Each tweet should be labeled by three expert
annotators

• Sentiment labels should be as balanced as
possible

3.2 Basic Data Set

Our initial data was made up of tweets col-
lected between 01.08.2013 and 31.10.2013. Those
tweets were a random subselection (10%) of all
tweets published during that time span. With the
langid.py tool (Lui and Baldwin, 2012) we se-
lected all German tweets from within our initial
data. To minimize false positives, we only in-
cluded tweets with a German confidence score of
over 0.999. This resulted in 5.280.157 tweets.

3.3 Tweet Selection

Next we selected the tweets to be annotated. In
order to achieve a large variety of topic and uni-
grams that are covered by the corpus, we applied a
k-means clustering with bag of words features and
cosine similarity to create 2500 clusters of tweets.
Our goal was to have - at the end - four tweets per
cluster, one for each sentiment class.

The majority of tweets in Twitter do not contain
any opinion at all. Hence, selecting a random set
of tweets for manual annotation would result in an
unbalanced set, with a strong majority of neutral
tweets. To find tweets with potentially different
sentiments, we used a straight-forward approach:
For each tweet we counted the number of posi-
tive and negative polarity words in per tweet, us-
ing the German polarity clues lexicon (Waltinger,
2010). Using these polarity words as indicators,
we selected tweets that were ”probably” positive,

negative, mixed, or neutral: A tweet was consid-
ered ”probably positive” if it contained at least one
positive polarity words, but no negative polarity
words; ”probably negative” analogously; ”prob-
ably mixed” if both types of polarity words oc-
cured; and ”probably neutral” if no polarity words
occured. In order to reach an as balanced corpus
as possible and increase the number of tweets with
an opinion, we decided to use primarily proba-
bly mixed tweets, since they tended to be anything
but neutral. Obviously, this approach lessened the
number of observed unigrams and topics to some
degree.

3.4 Manual Annotation

We had 34 annotators (students in computer sci-
ence or linguistics). Every tweet was shown to 3
random annotators and labeled with a sentiment
class by each of those. They were given several
examples and instructed to ”categorize the senti-
ment expressed in a tweet, not the sentiment felt
when reading the tweet”. We added a non-German
flag to clean out tweets wich slipped by the lan-
guage identification, and tweets were marked as
”unknown” when annotators could not decide on
its sentiment.

3.5 Corpus Properties

Basic Outline. The corpus SB10k contains 9783
German tweets. Each tweet has sentiment annota-
tions on tweet level by 3 human annotators, us-
ing sentiment classes positive, negative, neutral,
mixed, and unknown. We aggregate the annota-
tors’ individual classes to assign a sentiment to
each tweet, where tweet t has sentiment S if at
least 2 annotators marked the tweet with S; other-
wise, sentiment of t is unknown. The distribution
of aggregated annotations is shown in Table 1.

Pos. Neg. Neutral Mixed Unknown Total
1682 1077 5266 330 1428 9738

Table 1: Number of tweets per sentiment in SB10k

Unigram Diversity. Goal of our clustering ap-
proach was to achieve a high diversity of unigrams
in our corpus. We therefore compare the diver-
sity of the tweets that were selected by our clus-
tering versus randomly sampled tweets. There are
u = 11.592.947 distinct unigrams in all collected
German tweets (approx. 5 million). There are
9452 unigrams in the labeled tweets (picked from

47



the k-means clustering), thus, the corpus covers
0.00081% of all unigrams. To compare this value
to random sampling, we randomly picked 10000
tweets from all available tweets. This was re-
peated 10 times, resulting in an average coverage
of 0.00075% of all unigrams. Thus, our cluster-
ing approach increases the number of encountered
unigrams by 10.7%.

Annotator Agreement. To analyze the inter-
annotator agreement within our corpus, we use
Krippendorffs Alpha-reliability (Krippendorff,
2007). This agreement score fits well with our
annotation scheme, in contrast to other scores like
Kohens Kappa, since Krippendorffs Alpha basi-
cally computes the coincidence matrix between
any two annotators, and calculates a weighed
sum. We had pairs of annotators which shared as
little as 1 tweet and pairs which shared as many
as 1673 tweets. To mitigate this issue, we only
considered pairs of annotators which shared at
least 50 tweets. This results in α = 0.39, with a
standard deviation of 0.12.

4 Benchmark System: Multi-layer CNN
with Three-Phase Training

4.1 Architecture and Implementation

The winning system of SemEval-2016 by team
”SwissCheese” is based on a convolutional neural
network (CNN) which is trained in three phases.
We adapted and optimized the system for German
sentiment analysis. In the following, we briefly
describe the high-level architecture and parame-
ters of this CNN. For more details on the net-
work topology and technical architecture, see cit-
ederiu17www.

The core component of the system is a multi-
layer convolutional neural network (CNN), which
consists in two consecutive pairs of convolutional-
pooling layers, followed by a single fully con-
nected hidden layer and a soft-max output layer.
The system is trained in three phases. Figure
1 shows a complete overview of the phases of
the learning procedure: i) unsupervised phase,
where word embeddings are created on a corpus
of 300M unlabeled tweets; ii) distant supervised
phase, where the network is trained on a weakly-
labeled dataset of 40M tweets containing emoti-
cons; and iii) supervised phase, where the network
is nally trained on manually annotated tweets. For
English, a similar system achieved an F1-score of

Figure 1: Training Phases Overview.

62.7% on the test data of SemEval-2016 (Deriu et
al., 2016).

Training. The word embeddings are learned on
an unsupervised corpus containing 300M German
tweets. We apply a skip-gram model of window-
size 5 and filter words that occur less than 15
times (Severyn and Moschitti, 2015). The di-
mensionality of the vector representation is set to
d = 52. During the distant-supervised phase, we
use emoticons to infer noisy labels on the tweets
in the training set (Read, 2005; Go et al., 2009).
We used 40M tweets (8M negative, 32M posi-
tive). The neural network was trained on these
data for one epoch, before finally training on the
supervised data for about 20 epochs. The word-
embeddings are updated during both the distant-
and the supervised training phases by applying
back-propagation through the entire network.

Computing Time for Training. On a GPU
computer with 3072 cores and 8GB of RAM, it
took approximately 24 hours to create the word
embeddings, 15 hours for the distant-supervised
phase, and 30 minutes for the supervised phase.

5 Benchmark for German Sentiment
Analysis

We now study how the CNN performs when
trained and/or tested on the three German senti-
ment corpora we are aware of: SB10k (from this
paper, 9738 tweets), MGS corpus (109’130 tweets,
(Mozetič et al., 2016)), and DAI corpus (1800
tweets, (Narr et al., 2012)). Corpora SB10k and
MGS were randomly split into training (90%) and

48



Classifier Training Corpus Test Corpus F1pos F1neg F1neutral F1
SVM SB10k SB10k 66.16 47.80 81.32 56.98
CNN SB10k SB10k 71.46 58.72 81.19 65.09
SVM SB10k MGS 49.50 38.62 66.41 44.06
CNN SB10k MGS 50.41 44.19 71.81 47.30
SVM SB10k DAI (full) 62.30 61.40 81.22 61.85
CNN SB10k DAI (full) 62.79 58.43 79.92 60.61
SVM MGS SB10k 67.77 53.23 80.20 60.50
CNN MGS SB10k 63.94 58.21 70.66 61.07
SVM MGS MGS 60.34 56.48 69.31 58.41
CNN MGS MGS 61.49 58.12 68.62 59.80
SVM MGS DAI (full) 59.32 56.03 74.83 57.68
CNN MGS DAI (full) 61.01 55.74 76.88 58.38

Table 2: Benchmarks for sentiment in German. SVM and CNN were trained on fixed split of each
corpus (90%), and then tested on the remaining texts. For DAI, all texts were used for testing. F1 is
macroaveraged from F1pos and F1neg. Bold numbers identify higher F1 score of both classifiers for each
combination of test and training corpus (2 lines).

testing (10%) subsets4. DAI was not split, since it
was only used for testing.

For comparison, we implemented a feature-
based system using a Support Vector Machine
(SVM). Feature selection is based on the system
described in (Uzdilli et al., 2015), which ranked
8th in the Semeval competition of 2015, and in-
clude n-gram, various lexical features, and statis-
tical text properties. We use the macro-averages
F1-score of positive and negative class, i.e. F1 =
(F1pos + F1neg) / 2, since this is also used in Se-
mEval (Rosenthal et al., 2015) as a standard mea-
sure of quality. The results are reported in Table
2.

Results. We observe from Table 2 that CNN
outperforms SVM in all but one case (SB10k-
DAI). Surprisingly, SVM performs better on
SB10k when trained on the foreign corpus MGS
then when trained on SB10k (60.50 instead of
56.98), while in all other cases the classifier bene-
fits when being trained on the same corpus. There
is a high variance in F1-score for the same system
on different test corpora, e.g. between 47.30 and
65.09 for CNN trained on SB10k.

Both SVM and CNN outperform the reference
system from (Mozetič et al., 2016), which reported
an F1-score of 53.6 for the German part of MGS
(note that they used cross-validation instead of a
fixed split of the corpus).

4These splits are available at www.spinningbytes.
com/resources to allow other researchers to compare
their results with the benchmarks

We also computed macroaveraged 3-class
F1-score F13 = (F1pos + F1neg + F1neutral) / 3,
which is on average 4.42 points higher than F1,
due to the higher values of F1neutral.

6 Conclusion

We have evaluated two state-of-the-art systems for
sentiment analysis in German on three Twitter cor-
pora (on of them new). Since all corpora are pub-
licly available, these results can serve as a bench-
mark for other sentiment systems in German.

The results show that the deep learning system
outperforms the feature-based system in all but
one cases. However, F1-score is around 60% in
most cases, even when a system is trained and
tested on the same corpus (with a fixed split of
data). This means that there is still potential for
inprovement.

7 Acknowledgements

This research has been funded by Commission
for Technology and Innovation (CTI) project no.
18832.1 PFES-ES and by SpinningBytes AG,
Switzerland. We would like to thank Leon Der-
czynski for giving us access to his tweet collec-
tion, and Matthias Flour for his support in labeling
the tweets.

49



References
Cristina Bosco, Mirko Lai, Viviana Patti, and Daniela

Virone. 2016. Tweeting and Being Ironic in the De-
bate about a Political Reform: the French Annotated
Corpus TWitter-MariagePourTous. In Proceedings
of LREC-2016, pages 1619–1626, Portoroz, Slove-
nia.

Mark Cieliebak, Oliver Dürr, and Fatih Uzdilli. 2013.
Potential and Limitations of Commercial Sentiment
Detection Tools. In Proceedings of ESSEM@AI*IA,
pages 47–58, Torino, Italy.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. SwissCheese at SemEval-2016 Task 4: Sen-
timent classification using an ensemble of convo-
lutional neural networks with distant supervision.
In Proceedings of SemEval-2016, pages 1124–1128,
San Diego, California, USA.

Jan Deriu, Aurelien Lucchi, Valeria De Luca, Aliak-
sei Severyn, Simon Müller, Mark Cieliebak, Thomas
Hofmann, and Martin Jaggi. 2017. Leveraging
large amounts of weakly supervised data for multi-
language sentiment classification. In Proceedings of
WWW-2017, Peth, Australia.

Cı́cero Nogueira dos Santos and Maira Gatti. 2014.
Deep Convolutional Neural Networks for Senti-
ment Analysis of Short Texts. In Proceedings of
COLING-2014, pages 69–78, Dublin, Ireland.

Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase repre-
sentations for translation modeling. In Proceedings
of ACL-2014, pages 699–709, Baltimore, Maryland,
USA.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter Sentiment Classification using Distant Supervi-
sion. Technical report, The Stanford Natural Lan-
guage Processing Group, Stanford, CA, USA.

Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
ACM SIGKDD, pages 168–177, Seattle, Washing-
ton, USA.

Rie Johnson and Tong Zhang. 2015. Semi-supervised
Convolutional Neural Networks for Text Categoriza-
tion via Region Embedding. In Advances in Neural
Information Processing Systems 28, pages 919–927,
Montreal, Canada.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A Convolutional Neural Network for
Modelling Sentences. In Proceedings of ACL-2014,
pages 655–665, Baltimore, Maryland, USA.

Yoon Kim. 2014. Convolutional Neural Networks for
Sentence Classification. In Proceedings of EMNLP-
2014, pages 1746–1751, Doha, Quatar.

Klaus Krippendorff. 2007. Computing Krippendorff’s
alpha reliability. Departmental papers (ASC),
page 43.

Marco Lui and Timothy Baldwin. 2012. langid. py:
An off-the-shelf language identification tool. In
Proceedings of the ACL 2012 system demonstra-
tions, pages 25–30, Jeju Island, Korea.

Eugenio Martinez-Camara, M. Teresa Martin-Valdivia,
L. Alfonso Urena-Lopez, and Ruslan Mitkov. 2015.
Polarity classification for Spanish tweets using the
COST corpus. Journal of Information Science,
41(3):263–272.

Eugenio Martinez-Camara, Miguel A. Garcia-
Cumbreras, Julio Villena-Roman, and Janine
Garcia-Morera. 2016. TASS 2015 - The Evo-
lution of the Spanish Opinion Mining Systems.
Procesamiento del Lenguaje Natural, 56:33–40.

Igor Mozetič, Miha Grčar, and Jasmina Smailović.
2016. Multilingual Twitter Sentiment Classifica-
tion: The Role of Human Annotators. PloS one,
11(5):e0155036.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. SemEval-
2016 task 4: Sentiment analysis in Twitter. In Pro-
ceedings of SemEval-2016, pages 1–18, San Diego,
USA.

Sascha Narr, Michael Hulfenhaus, and Sahin Al-
bayrak. 2012. Language-independent twitter sen-
timent analysis. Knowledge Discovery and Machine
Learning (KDML), LWA, pages 12–14.

Iadh Ounis, Craig Macdonald, and Ian Soboroff. 2008.
Overview of the TREC-2008 Blog Track. In Pro-
ceedings of TREC-2008.

Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL stu-
dent research workshop, pages 43–48, Ann Arbor,
Michigan.

Sara Rosenthal, Alan Ritter, Preslav Nakov, and
Veselin Stoyanov. 2014. SemEval-2014 task 9:
Sentiment analysis in Twitter. In Proceedings of
SemEval-2014, pages 73 – 80, Dublin, Ireland.

Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment anal-
ysis in twitter. Proceedings of SemEval-2015, pages
451–463.

Sascha Rothe, Sebastian Ebert, and Hinrich Schutze.
2016. Ultradense Word Embeddings by Orthogonal
Transformation. arXiv.

Josef Ruppenhofer, Roman Klinger, Julia Maria Struß,
Jonathan Sonntag, and Michael Wiegand. 2014. IG-
GSA Shared Tasks on German Sentiment Analysis
(GESTALT). In Workshop Proceedings of the 12th

50



Edition of the KONVENS Conference, pages 164 –
173, Hildesheim, Germany.

Aliaksei Severyn and Alessandro Moschitti. 2015.
UNITN: Training Deep Convolutional Neural Net-
work for Twitter Sentiment Classification. In Pro-
ceedings of SemEval-2015, pages 464–469, Denver,
Colorado.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014. A latent semantic
model with convolutional-pooling structure for in-
formation retrieval. In Proceedings of the 23rd ACM
International Conference on Conference on Infor-
mation and Knowledge Management, pages 101–
110, Shanghai, China.

Uladzimir Sidarenka. 2016. PotTS: The Potsdam
Twitter Sentiment Corpus. In Proceedings of LREC-
2016, pages 1133 – 1141, Portoroz, Slovenia.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP-2013, volume
1631, page 1642, Seattle, Washington, USA.

Marco Straniscim, Christina Bosco, Delia Irazu Hern-
nandez Farias, and Viviana Patti. 2016. Annotating
Sentiment and Irony in the Online Italian Political
Debate on labuonascuola. In Proceedings of LREC-
2016, pages 2892–2899, Portoroz, Slovenia.

Fatih Uzdilli, Martin Jaggi, Dominic Egger, Pascal
Julmy, Leon Derczynski, and Mark Cieliebak. 2015.
Swiss-Chocolate: Combining Flipout Regulariza-
tion and Random Forests with Artificially Built Sub-
systems to Boost Text-Classification for Sentiment.
In Proceedings of SemEval-2015, pages 608–612,
Denver, Colorado.

Ben Verhoeven and Walter Daelemans. 2014. CLiPS
Stylometry Investigation (CSI) corpus: A Dutch cor-
pus for the detection of age, gender, personality,
sentiment and deception in text. In Proceedings of
LREC-2014, pages 3081–3085, Reykjavik, Iceland.

Ulli Waltinger. 2010. GermanPolarityClues: A Lex-
ical Resource for German Sentiment Analysis. In
Proceedings of LREC-2010, pages 1638–1642, Val-
letta, Malta.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165–210.

Jun Zhao, Kang Liu, and Liheng Xu. 2016. Sentiment
Analysis: Mining Opinions, Sentiments, and Emo-
tions. MIT Press, Cambridge, MA, USA.

51


