



















































Position-aware Attention and Supervised Data Improve Slot Filling


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35–45
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Position-aware Attention and Supervised Data Improve Slot Filling

Yuhao Zhang, Victor Zhong, Danqi Chen,
Gabor Angeli, Christopher D. Manning

Stanford University
Stanford, CA 94305

{yuhao, vzhong, danqi}@cs.stanford.edu
{angeli, manning}@cs.stanford.edu

Abstract

Organized relational knowledge in the
form of “knowledge graphs” is important
for many applications. However, the abil-
ity to populate knowledge bases with facts
automatically extracted from documents
has improved frustratingly slowly. This
paper simultaneously addresses two issues
that have held back prior work. We first
propose an effective new model, which
combines an LSTM sequence model with
a form of entity position-aware attention
that is better suited to relation extraction.
Then we build TACRED, a large (119,474
examples) supervised relation extraction
dataset, obtained via crowdsourcing and
targeted towards TAC KBP relations. The
combination of better supervised data and
a more appropriate high-capacity model
enables much better relation extraction
performance. When the model trained on
this new dataset replaces the previous rela-
tion extraction component of the best TAC
KBP 2015 slot filling system, its F1 score
increases markedly from 22.2% to 26.7%.

1 Introduction

A basic but highly important challenge in natu-
ral language understanding is being able to pop-
ulate a knowledge base with relational facts con-
tained in a piece of text. For the text shown in Fig-
ure 1, the system should extract triples, or equiv-
alently, knowledge graph edges, such as hPenner,
per:spouse, Lisa Dillmani. Combining such ex-
tractions, a system can produce a knowledge graph
of relational facts between persons, organizations,
and locations in the text. This task involves en-
tity recognition, mention coreference and/or entity
linking, and relation extraction; we focus on the

Penner is survived by his brother, John, a
copy editor at the Times, and his former wife,
Times sportswriter Lisa Dillman.

Subject Relation Object
Mike Penner per:spouse Lisa Dillman
Mike Penner per:siblings John Penner
Lisa Dillman per:title Sportswriter
Lisa Dillman per:employee of Los Angeles Times
John Penner per:title Copy Editor
John Penner per:employee of Los Angeles Times

Figure 1: An example of relation extraction from
the TAC KBP corpus.

most challenging “slot filling” task of filling in the
relations between entities in the text.

Organized relational knowledge in the form
of “knowledge graphs” has become an important
knowledge resource. These graphs are now exten-
sively used by search engine companies, both to
provide information to end-users and internally to
the system, as a way to understand relationships.
However, up until now, automatic knowledge ex-
traction has proven sufficiently difficult that most
of the facts in these knowledge graphs have been
built up by hand. It is therefore a key challenge
to show that NLP technology can effectively con-
tribute to this important problem.

Existing work on relation extraction (e.g., Ze-
lenko et al., 2003; Mintz et al., 2009; Adel et al.,
2016) has been unable to achieve sufficient re-
call or precision for the results to be usable ver-
sus hand-constructed knowledge bases. Super-
vised training data has been scarce and, while
techniques like distant supervision appear to be a
promising way to extend knowledge bases at low
cost, in practice the training data has often been
too noisy for reliable training of relation extrac-
tion systems (Angeli et al., 2015). As a result
most systems fail to make correct extractions even
in apparently straightforward cases like Figure 1,

35



Example Entity Types & Label

Carey will succeed Cathleen P. Black, who held the position for 15 years and will take on a new
role as chairwoman of Hearst Magazines, the company said.

Types: PERSON/TITLE
Relation: per:title

Irene Morgan Kirkaldy, who was born and reared in Baltimore, lived on Long Island and ran a
child-care center in Queens with her second husband, Stanley Kirkaldy.

Types: PERSON/CITY
Relation: per:city of birth

Pandit worked at the brokerage Morgan Stanley for about 11 years until 2005, when he and some
Morgan Stanley colleagues quit and later founded the hedge fund Old Lane Partners.

Types: ORGANIZATION/PERSON
Relation: org:founded by

Baldwin declined further comment, and said JetBlue chief executive Dave Barger was unavailable. Types: PERSON/TITLE
Relation: no relation

Table 1: Sampled examples from the TACRED dataset. Subject entities are highlighted in blue and
object entities are highlighted in red.

where the best system at the NIST TAC Knowl-
edge Base Population (TAC KBP) 2015 evaluation
failed to recognize the relation between Penner
and Dillman.1 Consequently most automatic sys-
tems continue to make heavy use of hand-written
rules or patterns because it has been hard for ma-
chine learning systems to achieve adequate pre-
cision or to generalize as well across text types.
We believe machine learning approaches have suf-
fered from two key problems: (1) the models used
have been insufficiently tailored to relation extrac-
tion, and (2) there has been insufficient annotated
data available to satisfy the training of data-hungry
models, such as deep learning models.

This work addresses both of these problems.
We propose a new, effective neural network se-
quence model for relation classification. Its ar-
chitecture is better customized for the slot fill-
ing task: the word representations are augmented
by extra distributed representations of word posi-
tion relative to the subject and object of the puta-
tive relation. This means that the neural attention
model can effectively exploit the combination of
semantic similarity-based attention and position-
based attention. Secondly, we markedly improve
the availability of supervised training data by us-
ing Mechanical Turk crowd annotation to pro-
duce a large supervised training dataset (Table 1),
suitable for the common relations between peo-
ple, organizations and locations which are used in
the TAC KBP evaluations. We name this dataset
the TAC Relation Extraction Dataset (TACRED),
and will make it available through the Linguistic
Data Consortium (LDC) in order to respect copy-
rights on the underlying text.

Combining these two gives a system with
markedly better slot filling performance. This is

1Note: former spouses count as spouses in the ontology.

shown not only for a relation classification task on
the crowd-annotated data but also for the incorpo-
ration of the resulting classifiers into a complete
cold start knowledge base population system. On
TACRED, our system achieves a relation classi-
fication F1 score that is 7.9% higher than that of
a strong feature-based classifier, and 3.5% higher
than that of the best previous neural architecture
that we re-implemented. When this model is used
in concert with a pattern-based system on the TAC
KBP 2015 Cold Start Slot Filling evaluation data,
the system achieves an F1 score of 26.7%, which
exceeds the previous state-of-the-art by 4.5% ab-
solute. While this performance certainly does not
solve the knowledge base population problem –
achieving sufficient recall remains a formidable
challenge – this is nevertheless notable progress.

2 A Position-aware Neural Sequence
Model Suitable for Relation Extraction

Existing work on neural relation extraction (e.g.,
Zeng et al., 2014; Nguyen and Grishman, 2015;
Zhou et al., 2016) has focused on convolutional
neural networks (CNNs), recurrent neural net-
works (RNNs), or their combination. While these
models generally work well on the datasets they
are tested on, as we will show, they often fail to
generalize to the longer sentences that are com-
mon in real-world text (such as in TAC KBP).

We believe that existing model architectures
suffer from two problems: (1) Although modern
sequence models such as Long Short-Term Mem-
ory (LSTM) networks have gating mechanisms to
control the relative influence of each individual
word to the final sentence representation (Hochre-
iter and Schmidhuber, 1997), these controls are
not explicitly conditioned on the entire sentence
being classified; (2) Most existing work either

36



h1

ps1

po1

h2

ps2

po2

hn

psn

pon

q

z

…

ana2a1

x1 x2 xn

Mike and Lisa

0

-2

1

-1

h3

x3

ps3

po3

2

0

married…

4

2

a3

(subject) (object)

Figure 2: Our proposed position-aware neural se-
quence model. The model is shown with an exam-
ple sentence Mike and Lisa got married.

does not explicitly model the positions of entities
(i.e., subject and object) in the sequence, or mod-
els the positions only within a local region.

Here, we propose a new neural sequence model
with a position-aware attention mechanism over
an LSTM network to tackle these challenges. This
model can (1) evaluate the relative contribution of
each word after seeing the entire sequence, and (2)
base this evaluation not only on the semantic in-
formation of the sequence, but also on the global
positions of the entities within the sequence.

We formalize the relation extraction task as fol-
lows: Let X = [x1, ..., xn] denote a sentence,
where xi is the i-th token. A subject entity s
and an object entity o are identified in the sen-
tence, corresponding to two non-overlapping con-
secutive spans: Xs = [xs1 , xs1+1, . . . , xs2 ] and
Xo = [xo1 , xo1+1, . . . , xo2 ]. Given the sentence
X and the positions of s and o, the goal is to pre-
dict a relation r 2 R (R is the set of relations) that
holds between s and o or no relation otherwise.

Inspired by the position encoding vectors used
in Collobert et al. (2011) and Zeng et al. (2014),
we define a position sequence relative to the sub-
ject entity [ps1, ..., p

s
n], where

psi =

8><>:
i� s1, i < s1
0, s1  i  s2
i� s2, i > s2

(1)

Here s1, s2 are the starting and ending indices of
the subject entity respectively, and psi 2 Z can be
viewed as the relative distance of token xi to the
subject entity. Similarly, we obtain a position se-
quence [po1, ..., p

o
n] relative to the object entities.

Let x = [x1, ...,xn] be word embeddings of the
sentence, obtained using an embedding matrix E.
Similarly, we obtain position embedding vectors
ps = [ps1, ...,p

s
n] and p

o = [po1, ...,p
o
n] using a

shared position embedding matrix P respectively.
Next, as shown in Figure 2, we obtain hidden state
representations of the sentence by feeding x into
an LSTM:

{h1, ...,hn} = LSTM({x1, ...,xn}) (2)

We define a summary vector q = hn (i.e., the out-
put state of the LSTM). This summary vector en-
codes information about the entire sentence. Then
for each hidden state hi, we calculate an attention
weight ai as:

ui = v> tanh(Whhi + Wqq+

Wspsi + Wop
o
i ) (3)

ai =
exp(ui)Pn

j=1 exp(uj)
(4)

Here Wh,Wq 2 Rda⇥d, Ws,Wo 2 Rda⇥dp
and v 2 Rda are learnable parameters of the net-
work, where d is the dimension of hidden states,
dp is the dimension of position embeddings, and
da is the size of attention layer. Additional param-
eters of the network include embedding matrices
E 2 R|V|⇥d and P 2 R(2L�1)⇥dp , where V is the
vocabulary and L is the maximum sentence length.

We regard attention weight ai as the relative
contribution of the specific word to the sentence
representation. The final sentence representation
z is computed as:

z =
Xn

i=1
aihi (5)

z is later fed into a fully-connected layer followed
by a softmax layer for relation classification.

Note that our model significantly differs from
the attention mechanism in Bahdanau et al. (2015)
and Zhou et al. (2016) in our use of the summary
vector and position embeddings, and the way our
attention weights are computed. An intuitive way
to understand the model is to view the attention
calculation as a selection process, where the goal
is to select relevant contexts over irrelevant ones.

37



Dataset # Rel. # Ex. % Neg.

SemEval-2010 Task 8 19 10,717 17.4%
ACE 2003–2004 24 16,771 N/A
TACRED 42 119,474 78.7%

Table 2: A comparison of existing datasets and our
proposed TACRED dataset. % Neg. denotes the
percentage of negative examples (no relation).

Here the summary vector (q) helps the model to
base this selection on the semantic information
of the entire sentence (rather than on each word
only), while the position vectors (psi and p

o
i ) pro-

vides important spatial information between each
word and the entities.

3 The TAC Relation Extraction Dataset

Previous research has shown that slot filling sys-
tems can greatly benefit from supervised data.
For example, Angeli et al. (2014b) showed that
even a small amount of supervised data can boost
the end-to-end F1 score by 3.9% on the TAC
KBP tasks. However, existing relation extrac-
tion datasets such as the SemEval-2010 Task 8
dataset (Hendrickx et al., 2009) and the Automatic
Content Extraction (ACE) (Strassel et al., 2008)
dataset are less useful for this purpose. This is
mainly because: (1) these datasets are relatively
small for effectively training high-capacity mod-
els (see Table 2), and (2) they capture very differ-
ent types of relations. For example, the SemEval
dataset focuses on semantic relations (e.g., Cause-
Effect, Component-Whole) between two nominals.

One can further argue that it is easy to obtain a
large amount of training data using distant super-
vision (Mintz et al., 2009). In practice, however,
due to the large amount of noise in the induced
data, training relation extractors that perform well
becomes very difficult. For example, Riedel et al.
(2010) show that up to 31% of the distantly super-
vised labels are wrong when creating training data
from aligning Freebase to newswire text.

To tackle these challenges, we collect a large
supervised dataset TACRED, targeted towards the
TAC KBP relations.

Data collection. We create TACRED based on
query entities and annotated system responses in
the yearly TAC KBP evaluations. In each year of
the TAC KBP evaluation (2009–2015), 100 enti-
ties (people or organizations) are given as queries,

Data Split # Ex. Years

Train 75,050 2009–2012
Dev 25,764 2013
Test 18,660 2014

Table 3: Statistics on TACRED: number of exam-
ples and the source of each portion.

for which participating systems should find asso-
ciated relations and object entities. We make use
of Mechanical Turk to annotate each sentence in
the source corpus that contains one of these query
entities. For each sentence, we ask crowd workers
to annotate both the subject and object entity spans
and the relation types.

Dataset stratification. In total we collect
119,474 examples. We stratify TACRED across
years in which the TAC KBP challenge was run,
and use examples corresponding to query entities
from 2009 to 2012 as training split, 2013 as
development split, and 2014 as test split. We
reserve the TAC KBP 2015 evaluation data for
running slot filling evaluations, as presented in
Section 4. Detailed statistics are given in Table 3.

Discussion. Table 1 presents sampled examples
from TACRED. Compared to existing datasets,
TACRED has four advantages. First, it contains
an order of magnitude more relation instances (Ta-
ble 2), enabling the training of expressive mod-
els. Second, we reuse the entity and relation types
of the TAC KBP tasks. We believe these relation
types are of more interest to downstream appli-
cations. Third, we fully annotate all negative in-
stances that appear in our data collection process,
to ensure that models trained on TACRED are not
biased towards predicting false positives on real-
world text. Lastly, the average sentence length in
TACRED is 36.2, compared to 19.1 in the Sem-
Eval dataset, reflecting the complexity of contexts
in which relations occur in real-world text.

Due to space constraints, we describe the data
collection and validation process, system inter-
faces, and more statistics and examples of TAC-
RED in the supplementary material. We will
make TACRED publicly available through the
LDC.

4 Experiments

In this section we evaluate the effectiveness of our
proposed model and TACRED on improving slot

38



filling systems. Specifically, we run two sets of ex-
periments: (1) we evaluate model performance on
the relation extraction task using TACRED, and
(2) we evaluate model performance on the TAC
KBP 2015 cold start slot filling task, by training
the models on TACRED.

4.1 Baseline Models

We compare our model against the following base-
line models for relation extraction and slot filling:

TAC KBP 2015 winning system. To judge our
proposed model against a strong baseline, we
compare against Stanford’s top performing system
on the TAC KBP 2015 cold start slot filling task
(Angeli et al., 2015). At the core of this system
are two relation extractors: a pattern-based extrac-
tor and a logistic regression (LR) classifier. The
pattern-based system uses a total of 4,528 surface
patterns and 169 dependency patterns. The logis-
tic regression model was trained on approximately
2 million bootstrapped examples (using a small
annotated dataset and high-precision pattern sys-
tem output) that are carefully tuned for TAC KBP
slot filling evaluation. It uses a comprehensive fea-
ture set similar to the MIML-RE system for re-
lation extraction (Surdeanu et al., 2012), includ-
ing lemmatized n-grams, sequence NER tags and
POS tags, positions of entities, and various fea-
tures over dependency paths, etc.

Convolutional neural networks. We follow the
1-dimensional CNN architecture by Nguyen and
Grishman (2015) for relation extraction. This
model learns a representation of the input sen-
tence, by first running a series of convolutional op-
erations on the sentence with various filters, and
then feeding the output into a max-pooling layer
to reduce the dimension. The resulting represen-
tation is then fed into a fully-connected layer fol-
lowed by a softmax layer for relation classifica-
tion. As an extension, positional embeddings are
also introduced into this model to better capture
the relative position of each word to the subject
and object entities and were shown to achieve im-
proved results. We use “CNN-PE” to represent the
CNN model with positional embeddings.

Dependency-based recurrent neural networks.
In dependency-based neural models, shortest de-
pendency paths between entities are often used as
input to the neural networks. The intuition is to
eliminate tokens that are potentially less relevant

to the classification of the relation. For the ex-
ample in Figure 1, the shortest dependency path
between the two entities is:

[Penner] survived! brother
! wife! [Lisa Dillman]

We follow the SDP-LSTM model proposed by Xu
et al. (2015b). In this model, each shortest depen-
dency path is divided into two separate sub-paths
from the subject entity and the object entity to the
lowest common ancestor node. Each sub-path is
fed into an LSTM network, and the resulting hid-
den units at each word position are passed into a
max-over-time pooling layer to form the output of
this sub-path. Outputs from the two sub-paths are
then concatenated to form the final representation.

In addition to the above models, we also com-
pare our proposed model against an LSTM se-
quence model without attention mechanism.

4.2 Implementation Details

We map words that occur less than 2 times in the
training set to a special <UNK> token. We use
the pre-trained GloVe vectors (Pennington et al.,
2014) to initialize word embeddings. For all the
LSTM layers, we find that 2-layer stacked LSTMs
generally work better than one-layer LSTMs. We
minimize cross-entropy loss over all 42 relations
using AdaGrad (Duchi et al., 2011). We apply
Dropout with p = 0.5 to CNNs and LSTMs. Dur-
ing training we also find a word dropout strategy
to be very effective: we randomly set a token to be
<UNK> with a probability p. We set p to be 0.06
for the SDP-LSTM model and 0.04 for all other
models.

Entity masking. We replace each subject entity
in the original sentence with a special <NER>-
SUBJ token where <NER> is the corresponding
NER signature of the subject as provided in TAC-
RED. We do the same processing for object en-
tities. This processing step helps (1) provide a
model with entity type information, and (2) pre-
vent a model from overfitting its predictions to
specific entities.

Multi-channel augmentation. Instead of using
only word vectors as input to the network, we
augment the input with part-of-speech (POS) and
named entity recognition (NER) embeddings. We
run Stanford CoreNLP (Manning et al., 2014) to
obtain the POS and NER annotations.

39



Model P R F1

Traditional Patterns 85.3 23.4 36.8
LR 72.0 47.8 57.5
LR + Patterns 71.4 50.1 58.9

Neural CNN 72.1 50.3 59.2
CNN-PE 68.2 55.4 61.1
SDP-LSTM 62.0 54.8 58.2
LSTM 61.4 61.7 61.5
Our model 67.7 63.2 65.4

Ensemble 69.4 64.8 67.0

Table 4: Model performance on the test set of
TACRED, micro-averaged over instances. LR =
Logistic Regression.

We describe our model hyperparameters and
training in detail in the supplementary material.

4.3 Evaluation on TACRED

We first evaluate all models on TACRED. We
train each model for 5 separate runs with inde-
pendent random initializations. For each run we
perform early stopping using the dev set. We then
select the run (among 5) that achieves the median
F1 score on the dev set, and report its test set per-
formance.

Table 4 summarizes our results. We observe that
all neural models achieve higher F1 scores than
the logistic regression and patterns systems, which
demonstrates the effectiveness of neural models
for relation extraction. Although positional em-
beddings help increase the F1 by around 2% over
the plain CNN model, a simple (2-layer) LSTM
model performs surprisingly better than CNN and
dependency-based models. Lastly, our proposed
position-aware mechanism is very effective and
achieves an F1 score of 65.4%, with an absolute in-
crease of 3.9% over the best baseline neural model
(LSTM) and 7.9% over the baseline logistic re-
gression system. We also run an ensemble of our
position-aware attention model which takes major-
ity votes from 5 runs with random initializations
and it further pushes the F1 score up by 1.6%.

We find that different neural architectures show
a different balance between precision and recall.
CNN-based models tend to have higher precision;
RNN-based models have better recall. This can
be explained by noting that the filters in CNNs are
essentially a form of “fuzzy n-gram patterns”.

query entity:

hop-0 slot:

hop-1 slot:

Mike Penner

per:spouse

per:title

 Lisa Dillman

 Sportswriter

(query) (fillers)

Figure 3: An example query and corresponding
fillers in the TAC KBP cold start slot filling task.

4.4 Evaluation on TAC KBP Slot Filling
Second, we evaluate the slot filling performance
of all models using the TAC KBP 2015 cold start
slot filling task (Ellis et al., 2015). In this task,
about 50k newswire and Web forum documents
are selected as the evaluation corpus. A slot filling
system is asked to answer a series of queries with
two-hop slots (Figure 3): The first slot asks about
fillers of a relation with the query entity as the sub-
ject (Mike Penner), and we term this a hop-0 slot;
the second slot asks about fillers with the system’s
hop-0 output as the subject, and we term this a
hop-1 slot. System predictions are then evaluated
against gold annotations, and micro-averaged pre-
cision, recall and F1 scores are calculated at the
hop-0 and hop-1 levels. Lastly hop-all scores are
calculated by combining hop-0 and hop-1 scores.2

Evaluating relation extraction systems on slot
filling is particularly challenging in that: (1) End-
to-end cold start slot filling scores conflate the per-
formance of all modules in the system (i.e., en-
tity recognizer, entity linker and relation extrac-
tor). (2) Errors in hop-0 predictions can easily
propagate to hop-1 predictions. To fairly evalu-
ate each relation extraction model on this task, we
use Stanford’s 2015 slot filling system as our basic
pipeline.3 It is a very strong baseline specifically
tuned for TAC KBP evaluation and ranked top in
the 2015 evaluation. We then plug in the corre-
sponding relation extractor trained on TACRED,
keeping all other modules unchanged.

Table 5 presents our results. We find that:
(1) by only training our logistic regression model
on TACRED (in contrast to on the 2 million boot-
strapped examples used in the 2015 Stanford sys-
tem) and combining it with patterns, we obtain a
higher hop-0 F1 score than the 2015 Stanford sys-

2In the TAC KBP cold start slot filling evaluation, a hop-1
slot is transferred to a pseudo-slot which is treated equally as
a hop-0 slot. Hop-all precision, recall and F1 are then calcu-
lated by combining these pseudo-slot predictions and hop-0
predictions.

3This system uses the fine-grained NER system in Stan-
ford CoreNLP (Manning et al., 2014) for entity detection and
the Illinois Wikifier (Ratinov et al., 2011) for entity linking.

40



Hop-0 Hop-1 Hop-all
Model P R F1 P R F1 P R F1

Patterns 63.8 17.7 27.7 49.3 8.6 14.7 58.9 13.3 21.8
LR 36.6 21.9 27.4 15.1 10.1 12.2 25.6 16.3 19.9
+ Patterns (2015 winning system) 37.5 24.5 29.7 16.5 12.8 14.4 26.6 19.0 22.2

LR trained on TACRED 32.7 20.6 25.3 7.9 9.5 8.6 16.8 15.3 16.0
+ Patterns 36.5 26.5 30.7 11.0 15.3 12.8 20.1 21.2 20.6

Our model 39.0 28.9 33.2 17.7 13.9 15.6 28.2 21.5 24.4
+ Patterns 40.2 31.5 35.3 19.4 16.5 17.8 29.7 24.2 26.7

Table 5: Model performance on TAC KBP 2015 slot filling evaluation, micro-averaged over queries.
Hop-0 scores are calculated on the simple single-hop slot filling results; hop-1 scores are calculated
on slot filling results chained on systems’ hop-0 predictions; hop-all scores are calculated based on the
combination of the two. LR = logistic regression.

Model Dev F1

Final Model 66.22
– Position-aware attention 65.12
– Attention 64.71
– Pre-trained embeddings 65.34
– Word dropout 65.69
– All above 63.60

Table 6: An ablation test of our position-aware
attention model, evaluated on TACRED dev set.
Scores are median of 5 models.

tem, and a similar hop-all F1; (2) our proposed
position-aware attention model substantially out-
performs the 2015 Stanford system on all hop-0,
hop-1 and hop-all F1 scores. Combining it with
the patterns, we achieve a hop-all F1 of 26.7%, an
absolute improvement of 4.5% over the previous
state-of-the-art result.

4.5 Analysis

Model ablation. Table 6 presents the results
of an ablation test of our position-aware atten-
tion model on the development set of TACRED.
The entire attention mechanism contributes about
1.5% F1, where the position-aware term in Eq. (3)
alone contributes about 1% F1 score.

Impact of negative examples. Figure 4 shows
how the slot filling evaluation scores change as we
change the amount of negative (i.e., no relation)
training data provided to our proposed model. We
find that: (1) At hop-0 level, precision increases as
we provide more negative examples, while recall
stays almost unchanged. F1 score keeps increas-
ing. (2) At hop-all level, F1 score increases by

20 30 40 50 60 70 80 90 100
5

10

15

20

25

30

35

40

45

Negative Examples Sample Rate (%)

Sc
or

es
on

K
B

P
Sl

ot
Fi

lli
ng

Ta
sk

(%
)

hop-0 prec. hop-0 rec.
hop-0 F1 hop-all F1

Figure 4: Change of slot filling hop-0 and hop-
all scores as number of negative training examples
changes. 100% is with all the negative examples
included in the training set; the left side scores
have positives and negatives roughly balanced.

about 10% as we change the amount of negative
examples from 20% to 100%.

Performance by sentence length. Figure 5
shows performance on varying sentence lengths.
We find that: (1) Performance of all models de-
grades substantially as the sentences get longer.
(2) Compared to the baseline Logistic Regression
model, all neural models handle long sentences
better. (3) Compared to CNN-PE model, RNN-
based models are more robust on long sentences,
and notably SDP-LSTM model is least sensitive to
sentence length. (4) Our proposed model achieves
equal or better results on sentences of all lengths,
except for sentences with more than 60 tokens
where SDP-LSTM model achieves the best result.

41



10 20 30 40 50 60 >60
40

45

50

55

60

65

70

75

80

85

90

95

Sentence Length

F 1
sc

or
es

(%
)

LR
CNN-PE
SDP-LSTM
LSTM
Our model

Figure 5: TACRED development set F1 scores for
sentences of varying lengths.

Improvement by slot types. We calculate the
F1 score for each slot type and compare the
improvement from using our proposed model
across slot types. When compared with the
CNN-PE model, our position-aware attention
model achieves improved F1 scores on 30
out of the 41 slot types, with the top 5 slot
types being org:members, per:country of death,
org:shareholders, per:children and per:religion.
When compared with SDP-LSTM model, our
model achieves improved F1 scores on 26
out of the 41 slot types, with the top 5 slot
types being org:political/religious affiliation,
per:country of death, org:alternate names,
per:religion and per:alternate names. We ob-
serve that slot types with relatively sparse training
examples tend to be improved by using the
position-aware attention model.

Attention visualization. Lastly, Figure 6 shows
the visualization of attention weights assigned by
our model on sampled sentences from the devel-
opment set. We find that the model learns to pay
more attention to words that are informative for
the relation (e.g., “graduated from”, “niece” and
“chairman”), though it still makes mistakes (e.g.,
“refused to name the three”). We also observe that
the model tends to put a lot of weight onto object
entities, as the object NER signatures are very in-
formative to the classification of relations.

5 Related Work

Relation extraction. There are broadly three
main lines of work on relation extraction: first,
fully-supervised approaches (Zelenko et al., 2003;
Bunescu and Mooney, 2005), where a statisti-

cal classifier is trained on an annotated dataset;
second, distant supervision (Mintz et al., 2009;
Surdeanu et al., 2012), where a training set is
formed by projecting the relations in an existing
knowledge base onto textual instances that contain
the entities that the relation connects; and third,
Open IE (Fader et al., 2011; Mausam et al., 2012),
which views its goal as producing subject-relation-
object triples and expressing the relation in text.

Slot filling and knowledge base population.
The most widely-known effort to evaluate slot fill-
ing and KBP systems is the yearly TAC KBP slot
filling tasks, starting from 2009 (McNamee and
Dang, 2009). Participants in slot filling tasks usu-
ally make use of hybrid systems that combine pat-
terns, Open IE, distant supervision and supervised
systems for relation extraction (Kisiel et al., 2015;
Finin et al., 2015; Zhang et al., 2016).

Datasets for relation extraction. Popular
general-domain datasets include the ACE dataset
(Strassel et al., 2008) and the SemEval-2010 task
8 dataset (Hendrickx et al., 2009). In addition,
the BioNLP Shared Tasks (Kim et al., 2009) are
yearly efforts on creating datasets and evaluations
for biomedical information extraction systems.

Deep learning models for relation extraction.
Many deep learning models have been proposed
for relation extraction, with a focus on end-to-end
training using CNNs (Zeng et al., 2014; Nguyen
and Grishman, 2015) and RNNs (Zhang et al.,
2015). Other popular approaches include using
CNN or RNN over dependency paths between en-
tities (Xu et al., 2015a,b), augmenting RNNs with
different components (Xu et al., 2016; Zhou et al.,
2016), and combining RNNs and CNNs (Vu et al.,
2016; Wang et al., 2016). Adel et al. (2016) com-
pares the performance of CNN models against tra-
ditional approaches on slot filling using a portion
of the TAC KBP evaluation data.

6 Conclusion

We introduce a state-of-the-art position-aware
neural sequence model for relation extraction, as
well as TACRED, a large-scale, crowd-sourced
dataset that is orders of magnitude larger than pre-
vious relation extraction datasets. Our proposed
model outperforms a strong feature-based classi-
fier and all baseline neural models. In combination
with the new dataset, it improves the state-of-the-

42



Sampled Sentences Predicted Labels

PER-SUBJ graduated from North Korea ’s elite Kim Il Sung University and

ORG-OBJ ORG-OBJ .

per:schools attended

The cause was a heart attack following a case of pneumonia , said

PER-SUBJ ’s niece , PER-OBJ PER-OBJ .

per:other family

Independent ORG-SUBJ ORG-SUBJ ORG-SUBJ ( ECC ) chairman PER-OBJ

PER-OBJ refused to name the three , saying they would be identified when

the final list of candidates for the august 20 polls is published on Friday .

org:top members/employees

Figure 6: Sampled sentences from the TACRED development set, with words highlighted according to
the attention weights produced by our best model.

art hop-all F1 on the TAC KBP 2015 slot filling
task by 4.5% absolute.

Acknowledgments

We thank the anonymous reviewers for their help-
ful suggestions. We gratefully acknowledge the
support of the Allen Institute for Artificial Intelli-
gence and the support of the Defense Advanced
Research Projects Agency (DARPA) Deep Ex-
ploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL)
contract No. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.

References
Heike Adel, Benjamin Roth, and Hinrich Schütze.

2016. Comparing convolutional neural networks to
traditional models for slot filling. Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology (NAACL-HLT).

Gabor Angeli, Sonal Gupta, Melvin Johnson Premku-
mar, Christopher D. Manning, Christopher Ré, Julie
Tibshirani, Jean Y. Wu, Sen Wu, and Ce Zhang.
2014a. Stanford’s distantly supervised slot filling
systems for KBP 2014. In Text Analysis Conference
(TAC) Proceedings 2014.

Gabor Angeli, Julie Tibshirani, Jean Y. Wu, and
Christopher D. Manning. 2014b. Combining dis-
tant and partial supervision for relation extraction.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2014).

Gabor Angeli, Victor Zhong, Danqi Chen, Arun Cha-
ganty, Jason Bolton, Melvin Johnson Premkumar,
Panupong Pasupat, Sonal Gupta, and Christopher D

Manning. 2015. Bootstrapped self training for
knowledge base population. In Text Analysis Con-
ference (TAC) Proceedings 2015.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Razvan C Bunescu and Raymond J Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing (EMNLP 2005),
pages 724–731.

Kevin Clark and Christopher D. Manning. 2015.
Entity-centric coreference resolution with model
stacking. In Proceedings of the 53th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2015).

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Joe Ellis, Jeremy Getman, Dana Fore, Neil Kuster,
Zhiyi Song, Ann Bies, and Stephanie Strassel. 2015.
Overview of linguistic resources for the TAC KBP
2015 evaluations: Methodologies and results. In
Text Analysis Conference (TAC) Proceedings 2015,
pages 16–17.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011), pages 1535–1545.

Tim Finin, Dawn Lawrie, Paul McNamee, James May-
field, Douglas Oard, Nanyun Peng, Ning Gao, Yiu-
Chang Lin, Joshi MacKin, and Tim Dowd. 2015.

43



HLTCOE participation in TAC KBP 2015: Cold
start and TEDL. In Text Analysis Conference (TAC)
Proceedings 2015.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 94–99.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of BioNLP’09 shared task on event extraction. In
Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing: Shared
Task, pages 1–9.

Bryan Kisiel, Bill McDowell, Matt Gardner, Ndapan-
dula Nakashole, Emmanouil A Platanios, Abulhair
Saparov, Shashank Srivastava, Derry Wijaya, and
Tom Mitchell. 2015. CMUML System for KBP
2015 cold start slot filling. In Text Analysis Con-
ference (TAC) Proceedings 2015.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523–534.

Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Text Analysis Conference (TAC) Proceedings
2009, volume 17, pages 111–113.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011.

Thien Huu Nguyen and Ralph Grishman. 2015. Rela-
tion extraction: Perspective from convolutional neu-
ral networks. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology (NAACL-HLT), pages 39–48.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2014), volume 14, pages 1532–
1543.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL 2011), pages 1375–1384.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. Machine learning and knowledge
discovery in databases, pages 148–163.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Stephanie Strassel, Mark A Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda. 2008. Linguis-
tic resources and evaluation techniques for evalua-
tion of cross-document automatic content extraction.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465.

Ngoc Thang Vu, Heike Adel, Pankaj Gupta, and Hin-
rich Schütze. 2016. Combining recurrent and convo-
lutional neural networks for relation classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
(NAACL-HLT).

Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan
Liu. 2016. Relation classification via multi-level at-
tention CNNs. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2016).

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation classifica-
tion via convolutional neural networks with simple
negative sampling. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2015).

Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen,
Yangyang Lu, and Zhi Jin. 2016. Improved relation
classification by deep recurrent neural networks with

44



data augmentation. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics
(COLING 2016).

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015b. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2015), pages 1785–1794.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of machine learning research,
3:1083–1106.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classification via
convolutional deep neural network. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2014), pages 2335–
2344.

Dongxu Zhang, Dong Wang, and Rong Liu. 2015. Re-
lation classification via recurrent neural network.
Technical report, CSLT 20150024, Tsinghua Uni-
versity.

Yuhao Zhang, Arun Chaganty, Ashwin Paranjape,
Danqi Chen, Jason Bolton, Peng Qi, and Christo-
pher D. Manning. 2016. Stanford at TAC KBP
2016: Sealing pipeline leaks and understanding chi-
nese. In Text Analysis Conference (TAC) Proceed-
ings 2016.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2016), page 207.

45


