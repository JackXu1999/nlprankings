



















































Object Ordering with Bidirectional Matchings for Visual Reasoning


Proceedings of NAACL-HLT 2018, pages 444–451
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Object Ordering with Bidirectional Matchings for Visual Reasoning

Hao Tan and Mohit Bansal
UNC Chapel Hill

{haotan, mbansal}@cs.unc.edu

Abstract
Visual reasoning with compositional natu-
ral language instructions, e.g., based on the
newly-released Cornell Natural Language Vi-
sual Reasoning (NLVR) dataset, is a challeng-
ing task, where the model needs to have the
ability to create an accurate mapping between
the diverse phrases and the several objects
placed in complex arrangements in the image.
Further, this mapping needs to be processed to
answer the question in the statement given the
ordering and relationship of the objects across
three similar images. In this paper, we propose
a novel end-to-end neural model for the NLVR
task, where we first use joint bidirectional at-
tention to build a two-way conditioning be-
tween the visual information and the language
phrases. Next, we use an RL-based pointer
network to sort and process the varying num-
ber of unordered objects (so as to match the
order of the statement phrases) in each of the
three images and then pool over the three de-
cisions. Our model achieves strong improve-
ments (of 4-6% absolute) over the state-of-the-
art on both the structured representation and
raw image versions of the dataset.

1 Introduction

Visual Reasoning (Antol et al., 2015; Andreas
et al., 2016; Bisk et al., 2016; Johnson et al., 2017)
requires a sophisticated understanding of the com-
positional language instruction and its relationship
with the corresponding image. Suhr et al. (2017)
recently proposed a challenging new NLVR task
and dataset in this direction with natural and com-
plex language statements that have to be classified
as true or false given a multi-image set (shown in
Fig. 1). Specifically, each task instance consists of
an image with three sub-images and a statement
which describes the image. The model is asked to
answer the question whether the given statement
is consistent with the image or not.

To solve the task, the designed model needs to
fuse the information from two different domains,

There is at least one tower which has blocks of 
 all three colors 

There is a box with a yellow circle, a yellow  
square and two black items. 

At least one of tower with exactly three blocks  
has a blue block in the middle

Answer: True Answer: True

There is a black block attach to a yellow block  
that is attach to a blue block.Answer: False Answer: True

Figure 1: NLVR task: given an image with 3 sub-
images and a statement, the model needs to predict
whether the statement correctly describes the image or
not. We show 4 such examples which our final BiATT-
Pointer model correctly classifies but the strong base-
line models do not (see Sec. 5).

the visual objects and the language, and learn ac-
curate relationships between the two. Another dif-
ficulty is that the objects in the image do not have a
fixed order and the number of objects also varies.
Moreover, each statement reasons for truth over
three sub-images (instead of the usual single im-
age setup), which also breaks most of the exist-
ing models. In our paper, we introduce a novel
end-to-end model to address these three problems,
leading to strong gains over the previous best
model. Our pointer network based LSTM-RNN
sorts and learns recurrent representations of the
objects in each sub-image, so as to match it bet-
ter with the order of the phrases in the language
statement. For this, it employs an RL-based pol-
icy gradient method with a reward extracted from
the subsequent comprehension model. With these
strong representations of the visual objects and the
statement units, a joint-bidirectional attention flow
model builds consistent, two-way matchings be-
tween the representations in different domains. Fi-
nally, since the scores computed by the bidirec-
tional attention are about the three sub-images,
a pooling combination layer over the three sub-
image representations is required to give the final
score of the whole image.

On the structured-object-representation version
of the dataset, our pointer-based, end-to-end bidi-

444



rectional attention model achieves an accuracy of
73.9%, outperforming the previous (end-to-end)
state-of-the-art method by 6.2% absolute, where
both the pointer network and the bidirectional at-
tention modules contribute significantly. We also
contribute several other strong baselines for this
new NLVR task based on Relation Networks (San-
toro et al., 2017) and BiDAF (Seo et al., 2016).
Furthermore, we also show the result of our joint
bidirectional attention model on the raw-image
version (with pixel-level, spatial-filter CNNs) of
the NLVR dataset, where our model achieves an
accuracy of 69.7% and outperforms the previous
best result by 3.6%. On the unreleased leader-
board test set, our model achieves an accuracy of
71.8% and 66.1% on the structured and raw-image
versions, respectively, leading to 4% absolute im-
provements on both tasks.

2 Related work
Besides the NLVR corpus with a focus on com-
plex and natural compositional language (Suhr
et al., 2017), other useful visual reasoning datasets
have been proposed for navigation and assem-
bly tasks (MacMahon et al., 2006; Bisk et al.,
2016), as well as for visual Q&A tasks which fo-
cus more on complex real-world images (Antol
et al., 2015; Johnson et al., 2017). Specifically
for the NLVR dataset, previous models have in-
corporated property- and count-based features of
the objects and the language (Suhr et al., 2017),
or extra semantic parsing (logical form) annota-
tions (Goldman et al., 2017) – we focus on end-to-
end models for this visual reasoning task.

Attention mechanism (Bahdanau et al., 2014;
Luong et al., 2015; Xu et al., 2015) has been
widely used for conditioned language generation
tasks. It is further used to learn alignments be-
tween different modalities (Lu et al., 2016; Wang
and Jiang, 2016; Seo et al., 2016; Andreas et al.,
2016; Chaplot et al., 2017). In our work, a bidirec-
tional attention mechanism is used to learn a joint
representation of the visual objects and the words
by building matchings between them.

Pointer network (Vinyals et al., 2015) was in-
troduced to learn the conditional probability of an
output sequence. Bello et al. (2016) extended this
to near-optimal combinatorial optimization via re-
inforcement learning. In our work, a policy gra-
dient based pointer network is used to “sort” the
objects conditioned on the statement, such that the
sequence of ordered objects is sent to the subse-

quent comprehension model for a reward.

3 Model

The training datum for this task consists of the
statement s, the structured-representation objects
o in the image I , and the ground truth label y
(which is 1 for true and 0 for false). Our BiATT-
Pointer model (shown in Fig. 2) for the structured-
representation task uses the pointer network to sort
the object sequence (optimized by policy gradi-
ent), and then uses the comprehension model to
calculate the probability P (s, o) of the statement
s being consistent with the image. Our CNN-
BiATT model for the raw-image I dataset version
is similar but learns the structure directly via pixel-
level, spatial-filter CNNs – details in Sec. 5 and
the appendix. In the remainder of this section,
we first describe our BiATT comprehension model
and then the pointer network.

3.1 Comprehension Model with Joint
Bidirectional Attention

We use one bidirectional LSTM-RNN (Hochre-
iter and Schmidhuber, 1997) (denoted by LANG-
LSTM) to read the statement s = w1, w2, . . . , wT,
and output the hidden state representations {hi}.
A word embedding layer is added before the
LSTM to project the words to high-dimension vec-
tors {w̃i}.
h1,h2, . . . , hT = LSTM (w̃1, w̃2, . . . , w̃T) (1)

The raw features of the objects in the j-th sub-
image are {ojk} (since the NLVR dataset has 3 sub-
images per task). A fully-connected (FC) layer
without nonlinearity projects the raw features to
object embeddings {ejk}. We then go through all
the objects in random order (or some learnable or-
der, e.g., via our pointer network, see Sec. 3.2)
by another bidirectional LSTM-RNN (denoted by
OBJ-LSTM), whose output is a sequence of vec-
tors {gjk}which is used as the (left plus right mem-
ory) representation of the objects (the objects in
different sub-images are handled separately):

ejk = W o
j
k + b (2)

gj1, g
j
2, . . . , g

j
Nj = LSTM (e

j
1, e

j
2, . . . , e

j
Nj ) (3)

where Nj is the number of the objects in jth sub-
image. Now, we have two vector sequences for
the representations of the words and the objects,
using which the bidirectional attention then calcu-
lates the score measuring the correspondence be-

445



There is exactly one black triangle  
not touching any edge

Encoder Decoder OBJ 
LSTM

Pointer Network

FC

Attention

LSTM

LSTM

MAX

MAX
Sub-Image 1

MLP

Bidirectional Attention

MAX 

POOL
Probability

�

Attention

FC

LANG 
LSTM

Sub-Image 2

Sub-Image 3

Figure 2: Our BiATT-Pointer model with a pointer network and a joint bidirectional attention module.

tween the statement and the image’s object struc-
ture. To simplify the notation, we will ignore the
sub-image index j. We first merge the LANG-
LSTM hidden outputs {hi} and the object-aware
context vectors {ci} together to get the joint rep-
resentation {ĥi}. The object-aware context vector
ci for a particular word wi is calculated based on
the bilinear attention between the word representa-
tion hi and the representations of the objects {gk}:

αi,k = softmaxk (h
ᵀ
i B1 gk) (4)

ci =
∑

k

αi,k · gk (5)

ĥi = relu (WLANG [hi; ci; hi−ci; hi◦ci]) (6)
where the symbol ◦ denotes element-wise multi-
plication.

Improvement over BiDAF The BiDAF model
of Seo et al. (2016) does not use a full object-
to-words attention mechanism. The query-to-
document attention module in BiDAF added the
attended-context vector to the document represen-
tation instead of the query representation. How-
ever, the inverse attention from the objects to the
words is important in our task because the repre-
sentation of the object depends on its correspond-
ing words. Therefore, different from the BiDAF
model, we create an additional ‘symmetric’ atten-
tion to merge the OBJ-LSTM hidden outputs {gk}
and the statement-aware context vectors {dk} to-
gether to get the joint representation {ĝk}. The
improvement (6.1%) of our BiATT model over the
BiDAF model is shown in Table 1.

βk,i = softmaxi
(
gᵀk B2 hi

)
(7)

dk =
∑

i

βk,i · hi (8)

ĝk = relu (WOBJ [gk; dk; gk−dk; gk◦dk]) (9)

These above vectors {ĥi} and {ĝk} are the rep-
resentations of the words and the objects which

are aware of each other bidirectionally. To make
the final decision, two additional bidirectional
LSTM-RNNs are used to further process the above
attention-based representations via an additional
memory-based layer. Lastly, two max pooling
layers over the hidden output states create two
single-vector outputs for the statement and the
sub-image, respectively:

h̄1, h̄2, . . . , h̄T = LSTM(ĥ1, ĥ2, . . . , ĥT) (10)

ḡ1, ḡ2, . . . , ḡN = LSTM(ĝ1, ĝ2, . . . , ĝN) (11)

h̄ = ele max
i

{
h̄i
}

(12)

ḡ = ele max
k
{ḡk} (13)

where the operator ele max denotes the element-
wise maximum over the vectors. The final scalar
score for the sub-image is given by a 2-layer MLP
over the concatenation of h̄ and ḡ as follows:

score = W2 tanh
(
W1[h̄; ḡ] + b1

)
(14)

Max-Pooling over Sub-Images In order to ad-
dress the 3 sub-images present in each NLVR
task, a max-pooling layer is used to combine the
above-defined scores of the sub-images. Given
that the sub-images do not have any specific order-
ing among them (based on the data collection pro-
cedure (Suhr et al., 2017)), a pooling layer is suit-
able because it is permutation invariant. Moreover,
many of the statements are about the existence of
a special object or relationship in one sub-image
(see Fig. 1) and hence the max-pooling layer ef-
fectively captures the meaning of these statements.
We also tried other combination methods (mean-
pooling, concatenation, LSTM, early pooling on
the features/vectors, etc.); the max pooling (on
scores) approach was the simplest and most effec-
tive method among these (based on the dev set).

The overall probability that the statement cor-
rectly describes the full image (with three sub-
images) is the sigmoid of the final max-pooled

446



score. The loss of the comprehension model is the
negative log probability (i.e., the cross entropy):

P (s, o) =σ

(
max
j

scorej
)

(15)

L(s, o, y) =− y logP (s, o)
− (1− y) log(1− P (s, o)) (16)

where y is the ground truth label.

3.2 Pointer Network
Instead of randomly ordering the objects, humans
look at the objects in an appropriate order w.r.t.
their reading of the given statement and after the
first glance of the image. Following this idea,
we use an additional pointer network (Vinyals
et al., 2015) to find the best object ordering for the
subsequent language comprehension model. The
pointer network contains two RNNs, the encoder
and the decoder. The encoder reads all the objects
in a random order. The decoder then learns a per-
mutation π of the objects’ indices, by recurrently
outputting a distribution over the objects based on
the attention over the encoder hidden outputs. At
each time step, an object is sampled without re-
placement following this distribution. Thus, the
pointer network models a distribution p(π | s, o)
over all the permutations:

p(π | s, o) =
∏

i

p (π(i) | π(< i), s, o) (17)

Furthermore, the appropriate order of the objects
depends on the language statement, and hence the
decoder importantly attends to the hidden outputs
of the LANG-LSTM (see Eqn. 1).

The pointer network is trained via reinforce-
ment learning (RL) based policy gradient opti-
mization. The RL loss LRL(s, o, y) is defined
as the expected comprehension loss (expectation
over the distribution of permutations):

LRL(s, o, y) = Eπ∼p(·|s,o)L(s, o[π], y) (18)

where o[π] denotes the permuted input objects for
permutation π, and L is the loss function defined
in Eqn. 16. Suppose that we sampled a permu-
tation π∗ from the distribution p(π|s, o); then the
above RL loss could be optimized via policy gra-
dient methods (Williams, 1992). The reward R is
the negative loss of the subsequent comprehension
model L(s, o[π∗], y). A baseline b is subtracted
from the reward to reduce the variance (we use the

self-critical baseline of Rennie et al. (2016)). The
gradient of the loss LRL could then be approxi-
mated as:

R =− L(s, o[π∗], y) (19)
∇θLRL(s, o, y) ≈ − (R− b)∇θ log p(π∗ | s, o)

+∇θL(s, o[π∗], y) (20)

This overall BiATT-Pointer model (for the
structured-representation task) is shown in Fig. 2.

4 Experimental Setup

We evaluate our model on the NLVR dataset (Suhr
et al., 2017), for both the structured and raw-image
versions. All model tuning was performed on the
dev set. Given the fact that the dataset is balanced
(the number of true labels and false labels are
roughly the same), the accuracy of the whole cor-
pus is used as the metric. We only use the raw fea-
tures of the statement and the objects with mini-
mal standard preprocessing (e.g., tokenization and
UNK replacement; see appendix for reproducibil-
ity training details).

5 Results and Analysis

Results on Structured Representations Dataset:
Table 1 shows our primary model results. In terms
of previous work, the state-of-the-art result for
end-to-end models is ‘MAXENT’, shown in Suhr
et al. (2017).1 Our proposed BiATT-Pointer model
(Fig. 2) achieves a 6.2% improvement on the pub-
lic test set and a 4.0% improvement on the unre-
leased test set over this SotA model. To show the
individual effectiveness of our BiATT and Pointer
components, we also provide two ablation results:
(1) the bidirectional attention BiATT model with-
out the pointer network; and (2) our BiENC base-
line model without any attention or the pointer
mechanisms. The BiENC model uses the similar-
ity between the last hidden outputs of the LANG-
LSTM and the OBJ-LSTM as the score (Eqn. 14).

Finally, we also reproduce some recent popu-
lar frameworks, i.e., Relationship Network (San-
toro et al., 2017) and BiDAF model (Seo et al.,
2016), which have been proven to be successful in
other machine comprehension and visual reason-
ing tasks. The results of these models are weaker
than our proposed model. Reimplementation de-
tails are shown in the appendix.

1There is also recent work by Goldman et al. (2017), who
use extra, manually-labeled semantic parsing data to achieve
a released/unreleased test accuracy of 80.4%/83.5%, resp.

447



Model Dev Test-P Test-U
STRUCTURED REPRESENTATIONS DATASET

MAXENT (Suhr et al., 2017) 68.0% 67.7% 67.8%
MLP (Suhr et al., 2017) 67.5% 66.3% 65.3%
ImageFeat+RNN (Suhr et al., 2017) 57.7% 57.6% 56.3%
RelationNet (Santoro et al., 2017) 65.1% 62.7% -
BiDAF (Seo et al., 2016) 66.5% 68.4% -
BiENC Model 65.1% 63.4% -
BiATT Model 72.6% 72.3% -
BiATT-Pointer Model 74.6% 73.9% 71.8%

RAW IMAGE DATASET
CNN+RNN (Suhr et al., 2017) 56.6% 58.0% 56.3%
NMN (Suhr et al., 2017) 63.1% 66.1% 62.0%
CNN-BiENC Model 58.7% 58.7% -
CNN-BiATT Model 66.9% 69.7% 66.1%

Table 1: Dev, Test-P (public), and Test-U (unreleased) results of our model on the structured-representation and
raw-image datasets, compared to the previous SotA results and other reimplemented baselines.

The top of the three towers  
are not the same. Correct Answer: True

There are 2 boxes with  
at least 2 blue items. Correct Answer: True

There is a blue object  
touching the base. Correct Answer: False

There are at least three yellow objects  
touching any edge. Correct Answer: True

Negative Examples

Figure 3: Incorrectly-classified examples.

Results on Raw Images Dataset: To further show
the effectiveness of our BiATT model, we apply
this model to the raw image version of the NLVR
dataset, with minimal modification. We simply
replace each object-related LSTM with a visual
feature CNN that directly learns the structure via
pixel-level, spatial filters (instead of a pointer net-
work which addresses an unordered sequence of
structured object representations). As shown in
Table 1, this CNN-BiATT model outperforms the
neural module networks (NMN) (Andreas et al.,
2016) previous-best result by 3.6% on the public
test set and 4.1% on the unreleased test set. More
details and the model figure are in the appendix.
Output Example Analysis: Finally, in Fig. 1,
we show some output examples which were suc-
cessfully solved by our BiATT-Pointer model but
failed in our strong baselines. The left two ex-
amples in Fig. 1 could not be handled by the Bi-
ENC model. The right two examples are incorrect
for the BiATT model without the ordering-based
pointer network. Our model can quite successfully
understand the complex meanings of the attributes
and their relationships with the diverse objects, as
well as count the occurrence of and reason over
objects without any specialized features.

Next, in Fig. 3, we also show some negative ex-
amples on which our model fails to predict the cor-
rect answer. The top two examples involve com-

plex high-level phrases e.g., “touching any edge”
or “touching the base”, which are hard for an end-
to-end model to capture, given that such state-
ments are rare in the training data. Based on the re-
sult of the validation set, the max-pooling layer is
selected as the combination method in our model.
The max-pooling layer will choose the highest
score from the sub-images as the final score. Thus,
the layer could easily handle statements about
single-subimage-existence based reasoning (e.g.,
the 4 positively-classified examples in Fig. 1).
However, the bottom two negatively-classified ex-
amples in Fig. 3 could not be resolved because
of the limitation of the max-pooling layer on sce-
narios that consider multiple-subimage-existence.
We did try multiple other pooling and combination
methods, as mentioned in Sec. 3.1. Among these
methods, the concatenation, early pooling and
LSTM-fusion approaches might have the ability
to solve these particular bottom-two failed state-
ments. In our future work, we are addressing mul-
tiple types of pooling methods jointly.

6 Conclusion
We presented a novel end-to-end model with joint
bidirectional attention and object-ordering pointer
networks for visual reasoning. We evaluate our
model on both the structured-representation and
raw-image versions of the NLVR dataset and
achieve substantial improvements over the previ-
ous end-to-end state-of-the-art results.

Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This work was supported by a
Google Faculty Research Award, a Bloomberg
Data Science Research Grant, an IBM Faculty
Award, and NVidia GPU awards.

448



References
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and

Dan Klein. 2016. Neural module networks. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 39–48.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2425–2433.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Irwan Bello, Hieu Pham, Quoc V Le, Mohammad
Norouzi, and Samy Bengio. 2016. Neural combi-
natorial optimization with reinforcement learning.
arXiv preprint arXiv:1611.09940.

Yonatan Bisk, Deniz Yuret, and Daniel Marcu. 2016.
Natural language communication with robots. In
HLT-NAACL, pages 751–761.

Devendra Singh Chaplot, Kanthashree Mysore
Sathyendra, Rama Kumar Pasumarthi, Dheeraj
Rajagopal, and Ruslan Salakhutdinov. 2017. Gated-
attention architectures for task-oriented language
grounding. arXiv preprint arXiv:1706.07230.

Omer Goldman, Veronica Latcinnik, Udi Naveh, Amir
Globerson, and Jonathan Berant. 2017. Weakly-
supervised semantic parsing with abstract examples.
arXiv preprint arXiv:1711.05240.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Identity mappings in deep residual net-
works. In European Conference on Computer Vi-
sion, pages 630–645. Springer.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. 2017. Clevr: A diagnostic dataset for
compositional language and elementary visual rea-
soning. In 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 1988–
1997. IEEE.

Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi
Parikh. 2016. Hierarchical question-image co-
attention for visual question answering. In Advances
In Neural Information Processing Systems, pages
289–297.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, and action in route instructions. Def,
2(6):4.

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2016. Self-critical
sequence training for image captioning. arXiv
preprint arXiv:1612.00563.

Adam Santoro, David Raposo, David GT Barrett, Ma-
teusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Timothy Lillicrap. 2017. A simple neural
network module for relational reasoning. arXiv
preprint arXiv:1706.01427.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.
2017. A corpus of natural language for visual rea-
soning. In 55th Annual Meeting of the Association
for Computational Linguistics, ACL.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. In Reinforcement Learning, pages
5–32. Springer.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual at-
tention. In International Conference on Machine
Learning, pages 2048–2057.

A Supplementary Material

A.1 CNN-BiATT Model Details
As shown in Fig. 4, we apply our BiATT model
to the raw image dataset with minimal modifica-
tion. The visual input of the model for this task
is changed from the unordered structured repre-
sentation set of objects o to the raw image pix-
els I . Hence, we replace all object-related LSTMs
(e.g., the OBJ-LSTM and the LSTM-RNN in the
bidirectional attention in Fig. 2) with visual fea-
ture convolutional neural networks (CNNs) that
directly learn the structure via pixel-level, spa-
tial filters (instead of a pointer network which ad-
dresses an unordered sequence of structured object
representations).

449



There is exactly one black triangle  
not touching any edge

CNN

FC

Attention

LSTM

CNN

MAX

MAX

MLP

Bidirectional Attention

MAX 

POOL
Probability

�

Attention

FC

LANG 
LSTM

Sub-Image 1

Sub-Image 2

Sub-Image 3

Figure 4: Our CNN-BiATT model for the raw-image dataset version replaces every object-related LSTM-RNN
with a spatial-filter convolutional neural network (CNN). The CNN for the raw image-pixels is a pretrained ResNet-
v2-101. A 3-layers CNN with relu activation is used in the bidirectional attention.

The training datum for the NLVR raw-image
version consists of the statement s, the image I
and the ground truth label y. The image I con-
tains three sub-images x1, x2 and x3. We will
use x to indicate any sub-image. The superscript
which indicates the index of the sub-image is ig-
nored to simplify the notation. The representation
of the statement {hi} is calculated by the LANG-
LSTM as before. For the image representation, we
project the sub-image to a sequence of feature vec-
tors (i.e., the feature map) {al : l = 1, . . . , L} cor-
responding to the different image locations. L =
m × m is the size of the features and m is the
width and height of the feature map. The projec-
tion consists of ResNet-V2-101 (He et al., 2016)
and a following fully-connected (FC) layer. We
only use the blocks in the ResNet before the aver-
age pooling layer and the output of the ResNet is
a feature map of size m×m×2048.

f1, . . . , fL = ResNet(x) (21)

al = relu(Wx fl + bx) (22)

The joint-representation of the statement {ĥi} is
the combination of the LANG-LSTM hidden out-
put states {hi} and the image-aware context vec-
tors {ci}:

αi,l = softmaxl (h
ᵀ
i B1 al) (23)

ci =
∑

l

αi,l · al (24)

ĥi = relu (WLANG [hi; ci; hi−ci; hi◦ci]) (25)

The joint-representation of the image {âl} is cal-

culated in the same way:

βl,i = softmaxi
(
aᵀl B2 hi

)
(26)

dl =
∑

i

βl,i · hi (27)

âl = relu (WIMG [al; dl; al−dl; al◦dl]) (28)

The joint-representation of the statement is further
processed by a LSTM-RNN. Different from our
BiATT model, a 3-layers CNN is used for model-
ing the joint-representation of the image {âl}. The
output of the CNN layer is another feature map
{āl}. Each CNN layer has kernel size 3 × 3 and
uses relu as the activation function, and then we
finally use element-wise max operator similar to
Sec. 3.1:

h̄1, h̄2, . . . , h̄T = LSTM(ĥ1, ĥ2, . . . , ĥT) (29)

ā1, ā2, . . . , āL’ = CNN(â1, â2, . . . , âL) (30)

h̄ = ele max
i

{
h̄i
}

(31)

ā = ele max
l
{āl} (32)

At last, we use the same method as our BiATT
model to calculate the score and the loss function:

score(s, x) =W2 tanh
(
W1[h̄; ā] + b1

)
(33)

P (s, I) =σ

(
max
j

score(s, xj)

)
(34)

L(s, I, y) =− y logP (s, I)
− (1− y) log(1− P (s, I)) (35)

A.2 Reimplementation Details for
Relationship Network and BiDAF
Models

We reimplement a Relationship Network (San-
toro et al., 2017), using a three-layer MLP with

450



256 units per layer in the G-net and a three-layer
MLP consisting of 256, 256 (with 0.3 dropout),
and 1 units with ReLU nonlinearities for F-net.
We also reimplement a BiDAF model (Seo et al.,
2016) using 128-dimensional word embedding,
256-dimensional LSTM-RNN and 0.3 dropout
rate. A max pooling layer on top of the model-
ing layer of BiDAF is used to merge the hidden
outputs to a single vector.

A.3 Experimental Setup and Training Details
for Our BiATT-Pointer, BiENC, and
CNN-BiATT Models

A.3.1 BiATT-Pointer

For preprocessing, we replace the words whose
occurrence is less than 3 with the “UNK” token.
We create a 9 dimension vector as the feature of
each object. This feature contains the location
(x, y) in 2D coordinate, the size of the object and
two 3-dimensional hot vectors for the shape and
the color. The (x, y) coordinates are normalized
to the range [−1, 1].

For the model hyperparameters (all lightly
tuned on dev set), the dimension of the word
embedding is 128, and the number of units in
an LSTM cell is 256. The word embedding is
trained from scratch. The object feature is pro-
jected to a 64-dimensional vector. The dimensions
of joint representation ĥi and ĝk are both 512. The
first fully-connected layer in calculating the sub-
images score has 512 units. All the trainable vari-
ables are initialized with the Xavier initializer. To
regularize the training process, we add a dropout
rate 0.3 to the hidden output of the LSTM-RNNs
and before the last MLP layer which calculates the
score for sub-images. We also clip the gradients by
their norm to avoid gradient exploding. The losses
are optimized by a single Adam optimizer and the
learning rate is fixed at 1e-4.

For the pointer network, we sample the objects
following the distribution of the objects at each de-
coder step during training. In inference, we select
the object with maximum probability. We use the
self-critical baseline (Rennie et al., 2016) to sta-
bilize the RL training, where the final score in in-
ference (choosing object with maximum probabil-
ity) is subtracted from the reward. To reduce the
number of parameters, we share the weight of the
fully-connected layer which projects the raw ob-
ject feature to the high dimensional vector in the
pointer encoder, the pointer decoder, and the OBJ-

LSTM. The pointer decoder attends to the hidden
outputs of the LANG-LSTM using bilinear atten-
tion (Luong et al., 2015).

A.3.2 CNN-BiATT
We initialize our model with weights of the pub-
lic pretrained ResNet-V2-101 (based on the Ima-
geNet dataset) and freeze it during training. The
ResNet projects the sub-image to a feature map of
10× 10 × 2048. The feature map is normalized
to a mean of 0 and a standard deviation of 1 be-
fore feeding into the FC layer. The fully connected
layer after the ResNet has 512 units. Each layer of
the 3-layers CNN in the bidirectional attention has
kernel size 3× 3 with 512 filters and no padding.
A.3.3 BiENC
The BiENC model uses LANG-LSTM and OBJ-
LSTM to read the statement and the objects. A
bilinear form calculates the similarity between the
last hidden outputs of the two LSTM-RNNs. The
similarity is directly used as the score of the sub-
image. The CNN-BiENC model replaces the OBJ-
LSTM with a CNN.

451


