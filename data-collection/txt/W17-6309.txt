



















































Leveraging Newswire Treebanks for Parsing Conversational Data with Argument Scrambling


Proceedings of the 15th International Conference on Parsing Technologies, pages 61–66,
Pisa, Italy; September 20–22, 2017. c©2017 Association for Computational Linguistics

Leveraging Newswire Treebanks for Parsing Conversational Data with
Argument Scrambling

Riyaz Ahmad Bhat
Department of Computer Science
University of Colorado Boulder

riyaz.bhat@colorado.edu

Irshad Ahmad Bhat
LTRC, IIIT-H, Hyderabad

Telangana, India
irshad.bhat@research.iiit.ac.in

Dipti Misra Sharma
LTRC, IIIT-H, Hyderabad

Telangana, India
dipti@iiit.ac.in

Abstract

We investigate the problem of parsing con-
versational data of morphologically-rich
languages such as Hindi where argument
scrambling occurs frequently. We evalu-
ate a state-of-the-art non-linear transition-
based parsing system on a new dataset
containing 506 dependency trees for sen-
tences from Bollywood (Hindi) movie
scripts and Twitter posts of Hindi mono-
lingual speakers. We show that a depen-
dency parser trained on a newswire tree-
bank is strongly biased towards the canon-
ical structures and degrades when applied
to conversational data. Inspired by Trans-
formational Generative Grammar (Chom-
sky, 1965), we mitigate the sampling bias
by generating all theoretically possible al-
ternative word orders of a clause from the
existing (kernel) structures in the treebank.
Training our parser on canonical and trans-
formed structures improves performance
on conversational data by around 9% LAS
over the baseline newswire parser.

1 Introduction

In linguistics, every language in assumed to have a
basic constituent order in a clause (Comrie, 1981).
In some languages, constituent order is fixed to
define the grammatical structure of a clause and
the grammatical relations therein, while in others,
that convey grammatical information through in-
flection or word morphology, constituent order as-
sumes discourse function and defines the informa-
tion structure of a sentence (Kiss, 1995). Despite
word order freedom, most of the morphologically-
rich languages exhibit a preferred word order in
formal registers such as newswire. Word order al-
ternations are more commonplace in informal lan-

guage use such as day-to-day conversations and
social media content. For statistical parsing, word
order alternations (argument scrambling) are a ma-
jor bottleneck. Given appropriate pragmatic con-
ditions a transitive sentence in a morphologically-
rich language allows n factorial (n!) permutations,
where n is the number of verb arguments and/or
adjuncts. Argument scrambling often leads to
structural discontinuities. Moreover, these scram-
blings worsen the data sparsity problem since
data-driven parsers are usually trained on a limited
size treebank where most of the valid structures
may never show up. More importantly, most of
the available treebanks are built on newswire text
which is more formal (Plank, 2016). The chances
of any deviation from the canonical word order are
smaller, thereby creating sampling bias.

A common solution to address the sampling
bias is to alter the distribution of classes in
the training data by using sampling techniques
(Van Hulse et al., 2007). However, simple sam-
pling techniques such as minority oversampling
may not be a feasible solution for parsing argu-
ment scramblings which are almost non-existent
in the newswire treebanks (see Table 1). Newswire
data usually represent only a sample of possi-
ble syntactic structures and, therefore, suffer from
non-representation of certain classes that encode
valid arc directionalities. In the Hindi dependency
treebank (HTB) (Bhat et al., 2015), for exam-
ple, dependency relations such as source, time and
place are never extraposed. Therefore, we instead
generate training examples for varied arc direc-
tionalities by transforming the gold syntactic trees
in the training data. We experiment with the Hindi
dependency treebank and show that such transfor-
mations are indeed helpful when we deal with data
with diverse word-orders such as movie dialogues.
Our work is in conformity with earlier attempts
where modifying source syntactic trees to match

61



target distributions benefited parsing of noisy, con-
versational data (Van der Plas et al., 2009; Foster,
2010).

S.No. Order Percentage
1 S O V 91.83
2 O S V 7.80
3 O V S 0.19
4 S V O 0.19
5 V O S 0.0
6 V S O 0.0

Table 1: The table shows
theoretically possible or-
ders of Subject (S), Ob-
ject (O) and Verb (V) in
transitive sentences in the
HTB training data with
their percentages of occur-
rence.

2 Sampling Argument Scrambling via
Syntactic Transformations

In (Chomsky, 1965), Noam Chomsky famously
described syntactic transformations which abstract
away from divergent surface realizations of related
sentences by manipulating the underlying gram-
matical structure of simple sentences called ker-
nels. For example, a typical transformation is
the operation of subject-auxiliary inversion which
generates yes-no questions from the correspond-
ing declarative sentences by swapping the subject
and auxiliary positions. These transformations are
essentially a tool to explain word-order variations
(Mahajan, 1990; Taylan, 1984; King, 1995).

We apply this idea of transformations to canoni-
cal structures in newswire treebanks for generating
trees that represent all of the theoretically viable
word-orders in a morphologically-rich language.
For example, we create a dependency tree where
an indirect object is extraposed by inverting its po-
sition with the head verb, as shown in Figure 1.

dī

Ram

ne

Gopal

ko

kitāb . =⇒
dī

Ram

ne

kitāb Gopal

ko

.

Figure 1: The figure depicts one possible permutation for
the sentence Ram ne Gopal ko kitāb dī. ‘Ram ERG Gopal
DAT book give.’ (Ram gave Gopal a book.). The indirect
object Gopal ko (red, dashed arcs) is postposed by swap-
ping its position with the ditransitive verb dī ‘give’.

Recently, a related approach was proposed by
Wang and Eisner (2016), who employed the con-
cept of creolization to synthesize artificial tree-
banks from Universal Dependency (UD) tree-
banks. They transform nominal and verbal pro-
jections in each tree of a UD language as per the
word-order parameters of other UD language(s)
by using their supervised word-order models. In
single-source transfer parsing, the authors showed

that a parser trained on a target language chosen
from a large pool of synthetic treebanks can sig-
nificantly outperform the same parser when it is
limited to selecting from a smaller pool of natural
language treebanks.

0
10

00
20

00
30

00
40

00
50

00
60

00
70

00
80

00
90

00
10

00
0

11
00

0
12

00
0

13
00

0
14

00
0

15
00

070

75

80

85

90

95

Figure 2: Learning curves plotted against data
size on the X axis and LAS score on the Y axis.

Unlike Wang and Eisner (2016), we do not
choose one word-order for a verbal projection
based on a target distribution, but instead gener-
ate all of its theoretically possible orders. For each
dependency tree, we alter the linear precedence re-
lations between arguments of a verbal projection
in ‘n!’ ways, while keeping their dominance re-
lations intact. However, simply permuting all the
nodes of verbal projections can lead to an over-
whelming number of trees. For example, a data
set of ‘t’ syntactic trees each containing an aver-
age of 10 nodes would allow around t × 10! i.e.,
3 million possible permutations for our training
data size, making training infeasible. Moreover,
we may only need a subset of the permutations
to have a uniform distribution over the possible
word orders. We therefore apply a number of fil-
ters to restrict the permutations. First, we only per-
mute a subset of the training data which is repre-
sentative of the newswire domain. It is often the
case that domain specific constructions are cov-
ered by a limited number of sentences. This can
be seen from the learning curves in Figure 2; the
learning curves flatten out after 4,000 training in-
stances. Second, for each sentence, we only take
the k permutations with the lowest perplexity as-
signed by a language model where k is set to the
number of nodes permuted for each verbal projec-
tion. The language model is trained on a large and
diverse data set (newswire, entertainment, social
media, stories, etc.) Finally, we make sure that the
distribution of the possible word-orders is roughly
uniform or at least less skewed in the augmented
training data.

3 Evaluation Data

For an intrinsic evaluation of the parsing models
on conversational data, we manually annotated de-

62



pendency trees for sentences that represent natural
conversation with at least one structural variation
from the canonical structures of Hindi.1 We used
Bollywood movie scripts as our primary source
of conversational data. Although, dialogues in a
movie are technically artificial, they mimic an ac-
tual conversation. We also mined Twitter posts
of Hindi monolingual speakers. Tweets can of-
ten be categorized as conversational. The data
set was sampled from old and new Bollywood
movies and a large set of tweets of Indian lan-
guage users that we crawled from Twitter us-
ing Tweepy2. For Twitter data, we used an off-
the-shelf language identification system3 to select
Hindi only tweets. From this data, we only want
those dialogues/tweets that contain a minimum of
one argument scrambling. For this purpose we
trained an off-the-shelf convolutional neural net-
work classifier for identifying sentences with ar-
gument scrambling (Kim, 2014).4 We trained the
model using the canonical and transformed tree-
bank data and achieved around∼97% accuracy on
canonical and transformed versions of HTB test
data.5 After automatic identification, we manu-
ally selected 506 sentences from the true positives
for annotation. For POS tagging and dependency
annotation, we used the AnnCorra guidelines de-
fined for treebanking of Indian languages (Bharati
et al., 2009). The data was annotated by an expert
linguist with expertise in Indian language tree-
banking. The annotations were automatically con-
verted to Universal Dependencies (UD) following
UD v1 guidelines for multilingual experimenta-
tion (De Marneffe et al., 2014). Table 2 shows the
distribution of theoretically possible word orders
in transitive sentences in the evaluation set. Un-
like their distribution in the HTB training data, the
word orders in the evaluation set are relatively less
skewed.

S.No. Order Percentage
1 S O V 33.07
2 O S V 23.62
3 O V S 17.32
4 S V O 14.17
5 V O S 9.45
6 V S O 2.36

Table 2: The table shows
theoretically possible or-
ders of Subject, Object
and Verb in transitive
sentences in the Evalu-
ation set with their per-
centages of occurrence.

1HTB’s conversation section has around ∼16,00 sen-
tences taken from fiction which, however, strictly obey
Hindi’s preferred SOV word-order. Therefore, we needed a
new dataset with word-order variations.

2http://www.tweepy.org/
3https://github.com/irshadbhat/litcm
4https://github.com/yoonkim/CNN sentence
5The system often misclassified noisy sentences from

movie scripts and tweets as scrambled.

Most of the movie scripts available online and
the tweets are written in Roman script instead of
the standard Devanagari script, requiring back-
transliteration of the sentences in the evaluation
set before running experiments. We also need nor-
malization of non-standard word forms prevalent
in tweets. We followed the procedure adapted
by Bhat et al. (2017a) to learn a single back-
transliteration and normalization system. We also
performed sentence-level decoding to resolve ho-
mograph ambiguity in Romanized Hindi vocabu-
lary.

4 Experimental Setup

The parsing experiments reported in this paper
are conducted using a non-linear neural network-
based transition system which is similar to (Kiper-
wasser and Goldberg, 2016). The monolin-
gual models are trained on training files of HTB
which uses the Pāninian Grammar framework
(PG) (Bharati et al., 1995), while the multilingual
models are trained on Universal Dependency Tree-
banks of Hindi and English released under version
1.4 of Universal Dependencies (Nivre et al., 2016).

Parsing Models Our underlying parsing method
is based on the arc-eager transition system (Nivre,
2003). The arc-eager system defines a set of con-
figurations for a sentence w1,...,wn, where each
configuration C = (S, B, A) consists of a stack
S, a buffer B, and a set of dependency arcs A. For
each sentence, the parser starts with an initial con-
figuration where S = [ROOT], B = [w1,...,wn]
and A = ∅ and terminates with a configuration C if
the buffer is empty and the stack contains the ROOT.
The parse trees derived from transition sequences
are given by A. To derive the parse tree, the arc-
eager system defines four types of transitions (t):
Shift, Left-Arc, Right-Arc, and Reduce.

We use a non-linear neural network to pre-
dict the transitions for the parser configurations.
The neural network model is the standard feed-
forward neural network with a single layer of hid-
den units. We use 128 hidden units and the RelU
activation function. The output layer uses a soft-
max function for probabilistic multi-class classifi-
cation. The model is trained by minimizing nega-
tive log-likelihood loss with l2-regularization over

6We also experimented with minority oversampling and
instance weighting, however improvments over newswire
were minimal (see §1 for possible reasons).

63



NewswirePG/UD NewswirePG/UD+Transformed NewswirePG/UD NewswireUD+EnglishUD
Data-set Gold POS Auto POS Gold POS Auto POS Gold POS Auto POS

UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
NewswirePG 96.41 92.08 94.55 89.51 96.07−0.34 91.75−0.33 94.29−0.26 89.28−0.23 - - - -

ConversationPG 74.03 64.30 69.52 58.91 84.68+10.65 73.94+9.64 79.07+9.55 67.41+8.5 - - - -
NewswireUD 95.04 92.65 93.85 90.59 94.59−0.45 92.03−0.62 93.32−0.53 89.98−0.61 94.56−0.48 91.87−0.78 93.22−0.63 89.72−0.87

ConversationUD 73.23 64.77 68.81 59.43 83.97+10.74 74.61+9.84 78.38+9.57 67.98+8.55 77.73+4.5 68.12+3.35 71.29+2.48 62.46+3.03

Table 3: Accuracy of our different parsing models on conversational data as well as newswire evaluation sets.
Improvements in superscript are over the newswire baseline. 6

the entire training data. We use Momentum SGD
for optimization (Duchi et al., 2011) and apply
dropout (Hinton et al., 2012).

From each parser configuration, we extract fea-
tures related to the top three nodes in the stack,
the top node in the buffer and the leftmost and
rightmost children of the top three nodes in the
stack and the leftmost child of the top node in
the buffer. Similarly to Kiperwasser and Goldberg
(2016), we use two stacked Bidirectional LSTMs
with 128 hidden nodes for learning the feature rep-
resentations over conjoined word-tag sequences
for each training sentence. We use an additional
Bidirectional LSTM (64 nodes) for learning sepa-
rate representations of words over their character
sequences for capturing out-of-vocabulary (OOV)
words at testing time. We use word dropout
with a dropout probability of 0.1 which enables
character embeddings to drive the learning pro-
cess around 10% of the time instead of full word
representations. This is important for evaluation
on noisy data where OOV words are quite fre-
quent. The monolingual models are initialized us-
ing pre-trained 64 dimensional word embeddings
of Hindi, while multilingual models use Hindi-
English bilingual embeddings from Bhat et al.
(2017a)7, while POS embeddings are randomly
initialized within a range of -0.25 to +0.25 with
32 dimensions.

Moreover, we use pseudo-projective transfor-
mations of Nivre and Nilsson (2005) to han-
dle a higher percentage of non-projective arcs in
the evaluation data (6% as opposed to 2% in
the training data). We use the most informative
scheme of head+path to store the transforma-
tion information. Inverse transformations based
on breadth-first search are applied to recover the
non-projective arcs in a post-processing step.

5 Experiments and Results

We ran two experiments to evaluate the effective-
ness of the tree transformations on the parsing

7https://bitbucket.org/irshadbhat/indic-word2vec-
embeddings

of conversational data. In the first, we leverage
the monolingual annotations by applying syntac-
tic transformations; in the second we use a cross-
lingual treebank with diverse word-orders. For
each experiment type, we report results using both
predicted and gold POS tags. The POS taggers are
trained using an architecture similar to the parser’s
with a single layer MLP which takes its input from
Bi-LSTM representation of the focus word (see
Appendix for the results). We used the newswire
parsing models as the baseline for evaluating the
impact of tree transformations and multilingual
annotations. The augmented models are trained
on the union of the original newswire training data
and the transformed trees. We generated 9K trees
from 4K representative sentences (Figure 2) which
were projectivized before applying syntactic trans-
formations to preserve non-projective arcs. Our
results are reported in Table 3.

As the table shows, our newswire models suf-
fer heavily when applied to conversational data.
The parser indeed seems biased towards canonical
structures of Hindi. It could not correctly parse
extraposed arguments, and could not even identify
direct objects if they were not adjacent to the verb.
However, in both gold and predicted settings, our
augmented parsing models produce results that
are approximately 9% LAS points better than the
state-of-the-art baseline newswire parsers (Bhat
et al., 2017b). Our augmented models even pro-
vided better results with UD dependencies. Proba-
bly due to the increased structural ambiguity, aug-
menting transformed trees with the original train-
ing data led to a slight decrease in the results on
the original Hindi test sets in both UD and PG de-
pendencies. Interestingly, our cross-lingual model
also captured certain levels of scrambling which
could be because the English treebank would at
least provide training instances for SVO word or-
der.

6 Conclusion

In this paper, we showed that leveraging for-
mal newswire treebanks can effectively handle

64



argument scrambling in informal registers of
morphologically-rich languages such as Hindi.
Inspired by Chomskyan syntactic tradition, we
demonstrated that sampling bias can be mitigated
by using syntactic transformations to generate
non-canonical structures as additional training in-
stances from canonical structures in newswire. We
also showed that multilingual resources can be
helpful in mitigating sampling bias.

The code of the parsing mod-
els is available at the GitHub reposi-
tory https://github.com/riyazbhat/
conversation-parser, while the data
can be found under the Universal Dependen-
cies of Hindi at https://github.com/
UniversalDependencies/UD_Hindi.

References
Akshar Bharati, Vineet Chaitanya, Rajeev Sangal,

and KV Ramakrishnamacharyulu. 1995. Natu-
ral Language Processing: A Paninian Perspective.
Prentice-Hall of India New Delhi.

Akshar Bharati, DM Sharma S Husain, L Bai,
R Begam, and R Sangal. 2009. Anncorra: Tree-
banks for indian languages, guidelines for annotat-
ing hindi treebank (version–2.0).

Irshad Bhat, Riyaz A. Bhat, Manish Shrivastava, and
Dipti Sharma. 2017a. Joining hands: Exploit-
ing monolingual treebanks for parsing of code-
mixing data. In Proceedings of the 15th Con-
ference of the European Chapter of the As-
sociation for Computational Linguistics: Vol-
ume 2, Short Papers. Association for Computa-
tional Linguistics, Valencia, Spain, pages 324–330.
http://www.aclweb.org/anthology/E17-2052.

Riyaz Ahmad Bhat, Irshad Ahmad Bhat, and
Dipti Misra Sharma. 2017b. Improving transition-
based dependency parsing of hindi and urdu by
modeling syntactically relevant phenomena. ACM
Transactions on Asian and Low-Resource Language
Information Processing (TALLIP) 16(3):17.

Riyaz Ahmad Bhat, Rajesh Bhatt, Annahita Farudi,
Prescott Klassen, Bhuvana Narasimhan, Martha
Palmer, Owen Rambow, Dipti Misra Sharma, Ash-
wini Vaidya, Sri Ramagurumurthy Vishnu, et al.
2015. The Hindi/Urdu treebank project. In Hand-
book of Linguistic Annotation, Springer Press.

Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax, volume 11. MIT press.

Bernard Comrie. 1981. Language universals and lan-
guage typology. Syntax and Morphology .

Marie-Catherine De Marneffe, Timothy Dozat, Na-
talia Silveira, Katri Haverinen, Filip Ginter, Joakim

Nivre, and Christopher D Manning. 2014. Universal
stanford dependencies: A cross-linguistic typology.
In Proceedings of the Ninth International Confer-
ence on Language Resources and Evaluation. vol-
ume 14, pages 4585–92.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research 12(Jul).

Jennifer Foster. 2010. cba to check the spelling in-
vestigating parser performance on discussion forum
posts. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Association for Computational Linguistics, pages
381–384.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580 .

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 1746–
1751. http://www.aclweb.org/anthology/D14-1181.

Tracy Holloway King. 1995. Configuring Topic and
Focus in Russian. Ph.D. thesis, Stanford University.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics
4:313–327.

Katalin É Kiss. 1995. Discourse Configurational Lan-
guages. Oxford University Press.

Anoop Kumar Mahajan. 1990. The A/A-bar Distinc-
tion and Movement Theory. Ph.D. thesis, Mas-
sachusetts Institute of Technology.

Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT).

Joakim Nivre, Željko Agić, Lars Ahrenberg, Maria Je-
sus Aranzabe, Masayuki Asahara, Aitziber Atutxa,
Miguel Ballesteros, John Bauer, Kepa Ben-
goetxea, Yevgeni Berzak, Riyaz Ahmad Bhat, Eck-
hard Bick, Carl Börstell, Cristina Bosco, Gosse
Bouma, Sam Bowman, Gülşen Cebirolu Eryiit,
Giuseppe G. A. Celano, Fabricio Chalub, Çar
Çöltekin, Miriam Connor, Elizabeth Davidson,
Marie-Catherine de Marneffe, Arantza Diaz de
Ilarraza, Kaja Dobrovoljc, Timothy Dozat, Kira
Droganova, Puneet Dwivedi, Marhaba Eli, Tomaž
Erjavec, Richárd Farkas, Jennifer Foster, Claudia

65



Freitas, Katarı́na Gajdošová, Daniel Galbraith, Mar-
cos Garcia, Moa Gärdenfors, Sebastian Garza, Filip
Ginter, Iakes Goenaga, Koldo Gojenola, Memduh
Gökrmak, Yoav Goldberg, Xavier Gómez Guino-
vart, Berta Gonzáles Saavedra, Matias Grioni, Nor-
munds Grūzītis, Bruno Guillaume, Jan Hajič, Linh
Hà M, Dag Haug, Barbora Hladká, Radu Ion,
Elena Irimia, Anders Johannsen, Fredrik Jørgensen,
Hüner Kaşkara, Hiroshi Kanayama, Jenna Kanerva,
Boris Katz, Jessica Kenney, Natalia Kotsyba, Si-
mon Krek, Veronika Laippala, Lucia Lam, Phng
Lê Hng, Alessandro Lenci, Nikola Ljubešić, Olga
Lyashevskaya, Teresa Lynn, Aibek Makazhanov,
Christopher Manning, Cătălina Mărănduc, David
Mareček, Héctor Martı́nez Alonso, André Martins,
Jan Mašek, Yuji Matsumoto, Ryan McDonald, Anna
Missilä, Verginica Mititelu, Yusuke Miyao, Simon-
etta Montemagni, Keiko Sophie Mori, Shunsuke
Mori, Bohdan Moskalevskyi, Kadri Muischnek,
Nina Mustafina, Kaili Müürisep, Lng Nguyn Th,
Huyn Nguyn Th Minh, Vitaly Nikolaev, Hanna
Nurmi, Petya Osenova, Robert Östling, Lilja Øvre-
lid, Valeria Paiva, Elena Pascual, Marco Passarotti,
Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen,
Barbara Plank, Martin Popel, Lauma Pretkalnia,
Prokopis Prokopidis, Tiina Puolakainen, Sampo
Pyysalo, Alexandre Rademaker, Loganathan Ra-
masamy, Livy Real, Laura Rituma, Rudolf Rosa,
Shadi Saleh, Baiba Saulīte, Sebastian Schuster,
Wolfgang Seeker, Mojgan Seraji, Lena Shakurova,
Mo Shen, Natalia Silveira, Maria Simi, Radu
Simionescu, Katalin Simkó, Mária Šimková, Kiril
Simov, Aaron Smith, Carolyn Spadine, Alane Suhr,
Umut Sulubacak, Zsolt Szántó, Takaaki Tanaka,
Reut Tsarfaty, Francis Tyers, Sumire Uematsu,
Larraitz Uria, Gertjan van Noord, Viktor Varga,
Veronika Vincze, Lars Wallin, Jing Xian Wang,
Jonathan North Washington, Mats Wirén, Zdeněk
Žabokrtský, Amir Zeldes, Daniel Zeman, and
Hanzhi Zhu. 2016. Universal dependencies 1.4.
LINDAT/CLARIN digital library at the Institute of
Formal and Applied Linguistics, Charles University
in Prague. http://hdl.handle.net/11234/1-1827.

Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics.

Barbara Plank. 2016. What to do about non-standard
(or non-canonical) language in nlp. arXiv preprint
arXiv:1608.07836 .

Eser Erguvanlı Taylan. 1984. The Function of Word
Order in Turkish Grammar, volume 106. Univ of
California Press.

Lonneke Van der Plas, James Henderson, and Paola
Merlo. 2009. Domain adaptation with artificial data
for semantic parsing of speech. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Com-
panion Volume: Short Papers. Association for Com-
putational Linguistics, pages 125–128.

Jason Van Hulse, Taghi M Khoshgoftaar, and Amri
Napolitano. 2007. Experimental perspectives on
learning from imbalanced data. In Proceedings of
the 24th international conference on Machine learn-
ing. ACM.

Dingquan Wang and Jason Eisner. 2016. The galac-
tic dependencies treebanks: Getting more data by
synthesizing new languages. Transactions of the As-
sociation for Computational Linguistics 4:491–505.
https://transacl.org/ojs/index.php/tacl/article/view/917.

A Supplementary Material

(ScoreLeftArc, ScoreRightArc, ScoreShift, ScoreReduce)

Softmax

Dense (Dropout 0.25)

Feature Template
(s2r , s2l , s1r , s1l , s0r ,
s0l , b0l , s2, s1, s0, b0)

Bi-LSTM (Dropout 0.25)

Bi-LSTM (Dropout 0.25)

Concat

Character
Bi-LSTM

(Dropout 0.25)

Word
Embeddings

POS
Embeddings

Character
Embeddings

Input Word Sequence Input POS Sequence

y1, y2, . . . , yn

X128

X2816

X256

X256

X160

X64 X64 X32

X32

Word

Word

POS

Figure 3: Parsing Architecture

S.No. Data-set Recall
1. NewswirePG 96.98
2. ConversationPG 91.33
3. NewswireUD 97.59
4. ConversationUD 89.40

Table 4: POS tagging accuracies on PG
and UD evaluation (newswire and con-
verstaion) data.

66


