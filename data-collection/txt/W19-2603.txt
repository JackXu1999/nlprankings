











































Understanding the Polarity of Events in the Biomedical Literature: Deep Learning vs. Linguistically-informed Methods


Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 21–30
Minneapolis, USA, June 6, 2019. c©2019 Association for Computational Linguistics

21

Understanding the Polarity of Events in the Biomedical Literature:
Deep Learning vs. Linguistically-informed Methods

Enrique Noriega-Atala, Zhengzhong Liang,
John A. Bachman†, Clayton T. Morrison, Mihai Surdeanu

University of Arizona, Tucson, Arizona, USA
†Harvard Medical School, Boston, Massachusetts, USA

{enoriega,zhengzhongliang,claytonm,msurdeanu}@email.arizona.edu
john bachman@hms.harvard.edu

Abstract

An important task in the machine reading
of biochemical events expressed in biomed-
ical texts is correctly reading the polarity,
i.e., attributing whether the biochemical event
is a promotion or an inhibition. Here we
present a novel dataset for studying polar-
ity attribution accuracy. We use this dataset
to train and evaluate several deep learning
models for polarity identification, and com-
pare these to a linguistically-informed model.
The best performing deep learning architec-
ture achieves 0.968 average F1 performance
in a five-fold cross-validation study, a consid-
erable improvement over the linguistically in-
formed model average F1 of 0.862.

1 Introduction

Recent advances in information extraction (IE)
have resulted in high-precision, high-throughput
systems tailored to the reading of biomedical sci-
entific publications (Valenzuela-Escárcega et al.,
2018; Peng et al., 2017; Quirk and Poon, 2016;
Kim et al., 2013; Björne and Salakoski, 2013;
Hakala et al., 2013; Bui et al., 2013, inter alia).
This, in turn, has resulted in the use of machine
reading systems as the foundation of more com-
plex, higher-level inference applications in spe-
cific domains such as cancer research (Valenzuela-
Escárcega et al., 2018).

However, the presence of noise in pipelined sys-
tems that use IE as an initial component may seri-
ously hinder the quality of downstream results. In
particular, biomedical research literature is prone
to noise caused by the mischaracterization of the
polarity (e.g., promotion vs. inhibition) of bio-
chemical interactions. This is the focus of this
work.

The identification of polarity in the biomedical
domain is complicated by the fact that the lan-
guage used is often hedged through multiple nega-

tions to stay closer to the complex biology under-
neath. For example, consider the statement: The
inactivation of Bad is sufficient to antagonize p38
MAPK. Under the (simplified but commonly used)
representation of polarized interactions, a naive IE
system would extract a negative interaction be-
tween the two proteins: Bad inhibits p38
MAPK, due to the presence of the negative pred-
icate antagonize. However, a more careful read-
ing of this text indicates that the better represen-
tation for this extraction is a positive interaction:
Bad promotes p38 MAPK,1 due to the inter-
action of two predicates with negative semantics,
inactivation and antagonize. This situation is ex-
acerbated by the fact that statements in this domain
may contain three and even four inter-related pred-
icates that affect polarity (as observed in Section
8).

This paper analyzes the identification of polar-
ity of biomedical interactions, from the perspec-
tive of multiple possible methods. In particular,
the contributions of this work are:

(1) We introduce a novel dataset that annotates
the polarity of biomedical interactions. The
dataset comes in multiple variants. A first vari-
ant was derived using distant supervision (DS)
(Mintz et al., 2009) by aligning a knowledge base
(KB) of protein interactions (Perfetto et al., 2015)
with the outputs of a machine reader (Valenzuela-
Escárcega et al., 2018). This dataset contains
52,779 promotion and 35,177 inhibition interac-
tions. To account for the noise introduced through
the DS process, we provide a second variant of this
dataset consisting of a sample of the full dataset

1This representation is better but not perfect. The
correct representation should be: (decrease of Bad)
causes (decrease of p38 MAPK). However, the
promotes/inhibits representation is widely used both in IE
datasets and by a domain expert, so we continue to use it in
this work.



22

that was manually curated by domain experts. We
divide this sample into an Easy partition where
the IE system initially agreed with the KB, and
a Challenge partition where the IE system’s ex-
tractions conflicted with the KB. These manually-
curated partitions contain 62 and 67 data points,
respectively.

(2) We compare several approaches for polarity
identification, including a linguistically-informed
method (Valenzuela-Escárcega et al., 2018), and
several deep learning (DL) approaches. The
DL methods incorporate: (a) multiple sequence
models that capture the text before/after argu-
ments/predicate, (b) attention models, and (c)
explicit features from the linguistically-informed
method. Our analysis indicates that: (a) the
simpler DL methods perform better than the
more complicated ones, (b) all DL approaches
outperform the standalone linguistically-informed
method, and (c) the difference between the two
strategies grows larger with the complexity of the
text.

2 Related work

The rate of scientific publishing has grown sub-
stantially each year, reaching a level that exceeds
the human capacity to read and process. For
example, PubMed, a search engine of biomed-
ical publications2 now indexes over 25 million
papers, 17 million of which were published be-
tween 1990 and the present. Domain-agnostic
approaches, such as open information extraction
(OpenIE) (Angeli et al., 2015) can begin to miti-
gate this by extracting information in the form of
relation triples. However the widely varied lan-
guage used by authors means that extractions can
be difficult to aggregate and utilize.

On the other hand, there have been significant
efforts to develop domain-specific information ex-
traction approaches that are tailored to scientific
publications. These approaches range from rule-
based to machine learning-based, and hybrid ap-
proaches (Valenzuela-Escárcega et al., 2018; Peng
et al., 2017; Quirk and Poon, 2016; Kim et al.,
2013; Björne and Salakoski, 2013; Hakala et al.,
2013; Bui et al., 2013).

On top of the extractions produced by these
methods, causal influence crucially relies on the
polarity of the influence interactions, i.e., whether

2http://www.ncbi.nlm.nih.gov/pubmed

one factor promotes or inhibits another factor. Bi-
ological models have been assembled from these
interactions and used for domain-specific applica-
tions (Gyori et al., 2017). Here we propose an ap-
proach for automatically detecting this polarity.

Polarity detection has been explored in several
other natural language processing tasks, perhaps
most notably in sentiment analysis (e.g., Pang
et al., 2008; Liu, 2012; Liu and Zhang, 2012),
where the polarity of a text is measured on a spec-
trum from negative to positive sentiment. Simi-
larly, in Wilson et al. (2005), the authors frame the
problem of extracting opinion polarity explicitly
as a sentiment analysis task. Our work is similar
in spirit, but it focuses on the polarity of scientific
statements. In (Lauscher et al., 2017), the authors
investigate the polarity polarity of citations within
the context of bibliometric analysis. In contrast,
our work addresses the polarity of content, i.e.,
events extracted from the biomedical literature.

To summarize, our approach is inspired by this
previous work, but it differs in two ways: first,
we focus on statements in the biomedical domain,
and, second, we extract polarity for specific, struc-
tured events rather than unstructured texts.

3 Linguistically-informed polarity
identification approach

In preliminary analyses, we observed that the ar-
guments of biomedical events are generally cor-
rectly identified, but the polarity of the interac-
tions is often incorrect due to the complex lan-
guage used (see, for example, the example in Sec-
tion 1). Based on this observation, all the methods
introduced in this paper assume that an unlabeled
event is provided, e.g., Bad interacts with
p38 MAPK, and the methods then label the event
with a polarity type, e.g., Bad promotes p38
MAPK.

The first method analyzed, which extends the
approach in Valenzuela-Escárcega et al. (2018),
relies on linguistic cues. The approach takes the
following steps:

1. First, it extracts the syntactic dependency
path between the participants in the interac-
tion.

2. Then, the path is expanded to include modi-
fiers of the words along the above path.

3. Finally, the method counts the number of
polarity-carrying words and affixes (Bach-



23

Figure 1: Example of the linguistically-informed ap-
proach. From the syntactic dependency tree, the
approach extracts the shortest undirected path be-
tween the participants in the interaction, Bad and p38
MAPK: nsubj:xsubj> dobj>, where the > and
< markers indicate the direction of a dependency
arc. Then, the path is extended with modifiers of
the elements on the path: amod< nsubj:xsubj>
mark< dobj> compound<. (The complete path
is highlighted and the negative words are underlined.)
Lastly, the approach counts the number of polarity-
carrying words along this path. An odd number indi-
cates negative event polarity; otherwise the polarity is
positive. In this example, the polarity is positive be-
cause there is an even number of polarity words: inac-
tivated and antagonize.

man et al., 2018) from a defined lexicon. This
lexicon contains 33 elements, such as “in-
hibition” and “loss”. The event is labeled
with negative polarity (inhibits) if the count
of these words is odd. Otherwise, the polar-
ity of the event is positive (promotes).

Figure 1 shows a walkthrough of this algorithm
for the sentence Inactivated Bad is sufficient to an-
tagonize p38 MAPK, which contains an event con-
necting the two entities Bad and p38 MAPK. Step
2 of this algorithm is crucial, as many polarity-
carrying words, e.g., inactivated, do not appear
along the syntactic dependency path between the
event arguments, but rather modify terms on the
path.

We extended the original algorithm in
Valenzuela-Escárcega et al. (2018) as follows:

• We made the polarity lexicon case-
insensitive.

• We changed the algorithm to match the words
in the polarity lexicon only if they occur as a
full word or as a prefix, instead of any sub-
string of a word. For example, in the text Re-
duction of triglyceride synthesis without af-
fecting ALLN-inhibitable protease, the orig-
inal algorithm generates a false positive by
matching inhibit in ALLN-inhibitable.

• We handle verb particles, which were ignored
in the original algorithm. For example, in
the text The Wip1 gene is overexpressed by

switching off p53, the polarity of the inter-
action cannot be detected from the predicate
alone (switching) without its attached particle
(off).

• We adjusted the polarity lexicon, e.g., we
removed target; and we added the suffix -
KD (Bachman et al., 2018), which stands for
knockdown.

4 Deep learning polarity identification
approaches

We propose several deep learning approaches for
the classification of event polarity. In general,
all proposed approaches use recurrent neural net-
work (RNN) architectures, which incorporate both
lexical and structural information into the learn-
ing process by considering one or more sequences
of words from the source sentence for the given
event.

In each of the RNN model variants we inves-
tigate, the input sentence is represented as a se-
quence of word embeddings. Every word wt trig-
gers a recurrent state that generates a hidden vec-
tor ht, which encodes information about the input
word subsequence 1..t. The output of the RNN is
a sequence {ht} of hidden vectors, one for each of
the input words.

The hidden vector sequence is then aggregated
using one of a couple of different strategies (as
described in the next two sub-sections), and then
passed forward as the input to a multi-layer per-
ceptron (MLP) that performs binary classification
of the event’s polarity: positive or negative.

Because our approach applies to biochemical
events, we use the result of the underlying IE
method to encode the predicate of the event, or
its trigger, as a feature in the MLP. That is, if
the trigger belongs to the lexicon of positive-
polarity terms, such as promotes or activates, the
network uses this as evidence for positive po-
larity. Conversely, if the trigger belongs to the
negative-polarity lexicon, such as inhibits, the trig-
ger is evidence of negative polarity. We use the
same dictionary of polarity-carrying words as the
linguistically-informed method.

We investigated two families of architectures:
(a) passing the entire input sentence to a single
recurrent network, and (b) splitting the sentence
into several semantic segments and passing these
fragments to independent RNNs (see Figure 2).



24

“Under basal conditions”  “so that …”

LSTM LSTM

MLP

Prediction

LSTM LSTM

“TSC2 negatively regulate” “the expression of mTOR”

Rule trigger
(i.e. Inhibits)

Figure 2: Four-segment LSTM architecture for polar-
ity identification. The four segments model: the text
before the left-most event argument, the text between
the left-most argument and the event predicate, the text
between the predicate and the right-most argument; and
the text after the right-most argument. The outputs of
the four LSTMs are integrated through a MLP, which
also uses the polarity of the event trigger as an explicit
feature.

We describe these approaches in the next two sub-
sections.

4.1 Single-segment architecture
For this variant of the architecture, we consider the
input sequence, consisting of the span of text that
belongs to the event as a single unit, and take the
last vector of the hidden sequence as input to the
MLP, discarding the rest of the sequence’s hidden
states. The output of the MLP directly labels the
polarity of the event on top of this hidden state
vector.

4.2 Four-segment architecture
The structure of the biochemical events modeled
here have the following elements: controller (or
cause), trigger (or predicate), and controlled (or
theme). These elements are text-bounded and par-
tition the source sentence into four regions: a
window of text before the controller, up to three
words, the text between the controller and the trig-
ger, the text between the trigger and the controlled
and the window of text after the controlled. If
the trigger appears before or after both, controller
and controlled (i.e. the phosphorylation of ERK by
MEK), then the event text is considered as a single
segment instead of two.

Each of the four sections of the source sentence
is then fed to an independent LSTM using the
same strategy as in Section 4.1. Figure 2 illustrates

how the sentence Under basal conditions, TSC2
negatively regulates the expression of mTOR, so
that ... is split and processed by this approach. The
last vectors of the four hidden sequences are con-
catenated and passed as input to MLP for polarity
classification.

4.3 Additional enhancements

We implemented and tested the following en-
hancements with both the single-segment and
four-segment architectures from Sections 4.1 and
4.2 respectively.

Pre-trained word embeddings
We used Word2Vec (Mikolov et al., 2013) to pre-
initialize the word embeddings. We pre-trained
these embeddings over the open-access subset of
PubMed Central3. We used dimension 100 for
these vectors.

Character-level embeddings
To capture information present in the morpho-
logical structure of a word, we extended our ap-
proaches to use character-level embeddings. Each
word w in an input sentence is enhanced by adding
character-level embeddings to its word embedding
ew.

Given the characters of word w, each is mapped
to an embedding ec. The resulting sequence of
character embeddings {ect} is then passed for-
wards and backwards through a bi-directional
GRU (Goldberg, 2017). Then, the last hidden vec-
tors of the forward and backward GRUs are con-
catenated into the word’s characters embedding
ewc.

The word embedding and the word’s charac-
ters embedding are then concatenated into an en-
hanced word embedding ew

′
= [ew; ewc], which is

passed as input for the current word of our polarity
network architecture.

Attention mechanisms for aggregation
So far, in all proposed approaches the last element
of a sequence has been used as input to the MLP
for classification. By doing this, the remaining se-
quence leading to the selected hidden vector is dis-
carded with respect to classification. To account
for this potential limitation, we implemented at-
tention mechanisms (Bahdanau et al., 2014) to ag-

3https://www.ncbi.nlm.nih.gov/pmc/
tools/openftlist/

https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/


25

gregate all the hidden vectors into the classifica-
tion step of the network.

The attention mechanism functions as a
weighted average of a sequence {ht} of vectors
dictated by

󰁓T
t αtht. The weight parameters {αt}

are learnt jointly with the rest of the network pa-
rameters. The scalar coefficient αt for the vec-
tor ht is computed using the linear combination:
at = Waht + ba, where the parameters Wa and ba
are shared for all the observations passed through
the network. The resulting sequence of coeffi-
cients {at} is normalized with the softmax func-
tion by αt = softmax({at}) to enforce that the
weights sum up to 1.

The single-segment architecture is enhanced
with this mechanism on top of the sequence of
hidden vectors produced by the recurrent network.
For the four-segment architecture, we tested an
attention mechanism for the hidden vector se-
quences of each segment of the sentence (shallow
attention) and an additional approach that also in-
cludes an attention mechanism to aggregate, in-
stead of concatenate the four resulting sequence
vectors before the MLP step (deep attention). This
deep attention approach computes a weighted av-
erage of the four sequences {si}, dictated by󰁓4

i βisi. Similarly to the weights of the hidden
vectors, each individual weight in {βi} is com-
puted by the linear combination bt = Wbhi + b0,
where the parameters Wb and b0 are shared and
later normalized by βi = softmax({bi}).

Bidirectional LSTMs
At any given index t of a source sentence, the
LSTM network considers only the sequence 1..t
of words to compute the hidden state vector of
wt. Clearly, this formulation discards information
from words to the right of t. To address this limi-
tation, we modified our architecture to use a bidi-
rectional LSTM (Graves et al., 2013) as a drop-in
replacement of the vanilla LSTM wherever it is
used. Similarly to the bidirectional GRU, the bidi-
rectional LSTM contains two distinct LSTM net-
works that process the input sentence left-to-right
(forward) and right-to-left (backward). The last
hidden vectors of both are concatenated and used
for the next step in our architectures.

5 Dataset

To analyze the performance of the above ap-
proaches, we assembled a dataset of sentences as-
sociated with protein-protein interaction events,

as well as polarity labels. The dataset was con-
structed through distant supervision (Mintz et al.,
2009), by aligning events extracted from biomed-
ical literature by Reach, a biomedical IE system
(Valenzuela-Escárcega et al., 2018), with polarity
labels from the SIGNOR database (Perfetto et al.,
2015).

SIGNOR contains approximately 20,000 man-
ually curated protein interactions, the majority of
which are annotated with the polarity of the effect
of the interaction on the downstream protein (ac-
tivation or inhibition). These signed interactions
were used to establish the true polarities for each
pair of proteins in the database. A potential is-
sue with this approach is that an interaction among
proteins may have more than one possible polar-
ity depending on the biological context: for ex-
ample, protein A may activate protein B in cell
type X, but inhibit protein B in cell type Y. To
mitigate this, we filtered the relations in SIGNOR
for those annotated with only a single, unambigu-
ous polarity, under the assumption that for the rel-
atively well-characterized interactions prioritized
for curation in a pathway database, the assign-
ment of a single polarity would be a good indicator
of “ground truth” for the majority of texts. Pro-
cessing the SIGNOR database in this way yielded
17,163 protein-protein interactions among with a
single polarity, composed of the following interac-
tion types: 13,302 interactions with positive polar-
ity, and 3,861 interactions with negative polarity.

We extracted protein-protein-interaction events
from text by running the Reach IE system over
all full-text articles in PubMed Central4, the
PubMed Central Author’s Manuscript collection5,
and MEDLINE6 abstracts (for articles not in-
cluded in the full-text datasets). We kept all in-
formation about the events (e.g., triggers, partici-
pants, overall interaction type), but discarded po-
larity information. We assigned polarity labels
by aligning these events with SIGNOR interac-
tions that involved the same two proteins and the
same overall interaction type, irrespective of sign
(e.g., regulation of activity or regulation of phos-
phorylation). From this dataset we removed: (a)
duplicate sentences, and (b) sentences containing
events where at least one of the participating pro-

4
https://www.ncbi.nlm.nih.gov/pmc/tools/

openftlist/
5
https://www.ncbi.nlm.nih.gov/pmc/about/

mscollection/
6
https://www.nlm.nih.gov/bsd/medline.html

https://www.ncbi.nlm.nih.gov/pmc/about/mscollection/
https://www.nlm.nih.gov/bsd/medline.html


26

tein names could not be grounded to an entry in
the UniProt protein database7. This process pro-
duced 68,935 polarity-labeled events (with sup-
porting sentences). For 54,105 of these events, the
original polarity detector in Reach agreed with the
SIGNOR polarity label (a strong indication that
these sentences are easier to classify). For 14,830
events, Reach’s polarity disagreed with SIGNOR
(an indication that these sentences are more chal-
lenging). We call this dataset the DS dataset (from
distant supervision). Table 1 lists the distribution
of labels for the DS dataset on both the Easy and
Challenge partitions and overall.

Positive polarity Negative polarity

Easy 40, 339 13, 766
Challenge 7, 262 7, 568

Total 47, 601 21, 334

Table 1: Label distribution on the DS dataset.

The distant supervision process is potentially
noisy (Yao et al., 2011). To control for this noise,
we also created two smaller hand-curated datasets,
as follows:

1. We randomly sampled 100 sentences from
the sentences where Reach agreed with SIG-
NOR, and 100 from the sentences where
Reach disagreed with SIGNOR. Based on
the intuition mentioned in the previous para-
graph, we call these partitions Easy and Chal-
lenge.

2. Because the focus of this work is on polarity
identification given a correct event, we elimi-
nated the false positive events from both par-
titions, i.e., events extracted by Reach that
were not supported by the corresponding un-
derlying sentence. Further, we removed sen-
tences containing events where at least one of
the participating protein names could not be
grounded to UniProt. This reduced the size
of the dataset to 62 Easy and 67 Challenge
examples.

3. The remaining sentences were manually cu-
rated by a domain expert. The expert cor-
rected 2 polarity labels in the Easy partition,
and 53 labels in the Challenge partition, con-

7https://www.uniprot.org

firming our expectation that the latter parti-
tion is harder than the former.

To facilitate reproducibility, we will release all
these datasets (and the software) upon acceptance.

6 Results

We performed a five-fold cross validation exper-
iment on the DS dataset introduced in Section 5
to assess the performance of the linguistically-
informed baseline (Section 3) and of the various
neural models previously described in Section 4.
Note that this dataset contains all the elements
from both Easy and Challenge partitions. The
data was split randomly and the experiment was
repeated with five different random seeds and the
numbers reported are the corresponding averages
from all the trials. Table 2 reports these average
scores as well as the standard deviations for all the
approaches analyzed.

Tables 3 and 4 contain the results on the
manually-curated Easy and Challenge partitions,
when the corresponding models were trained on
the entire DS dataset.

The code and data used to generate these results
are available at this URL: https://github.
com/clulab/releases/tree/master/
naacl-essp2019-polarity.

7 Discussion

7.1 Discussion of the main results

Table 2 shows that the linguistically-informed ap-
proach performs reasonably well overall, with a
F1 score of 0.862. This is encouraging, but also
somewhat misleading. The DS dataset consists of
mostly Easy examples, where Reach agreed with
SIGNOR labels. As discussed in Section 5, the
distribution of the examples in the DS dataset is
78.4/21.6% Easy/Challenge. Tables 3 and 4 show
that the performance of the linguistically-informed
approach, which is an improved version of the
method in Reach, drops to 0.143 F1 when eval-
uated solely on challenging sentences.

On the other hand, the results summarized
in Tables 2 through 4 demonstrate that overall,
deep learning architectures that incorporate bidi-
rectional recurrence with character-level embed-
dings perform the best. The reasonable explana-
tion for this is that those specific enhancements
are aimed at capturing more global information
from the sentence, instead of just the information

https://www.uniprot.org
https://github.com/clulab/releases/tree/master/naacl-essp2019-polarity


27

Architecture variant F1 (st. dev.) Precision (st. dev.) Recall (st. dev.)

Linguistically-informed approach 0.862 0.859 0.865

Single-segment architecture
– biLSTM, char embed, no pretrained embed, no attention, trigger 0.968(0.001) 0.967(0.001) 0.969(0.000)
– biLSTM, char embed, no pretrained embed, no attention, no trigger 0.968(0.001) 0.967(0.001) 0.968(0.000)
– LSTM, char embed, no pretrained embed, no attention, trigger 0.966(0.001) 0.964(0.001) 0.967(0.001)
– LSTM, no char embed, no pretrained embed, no attention, trigger 0.961(0.000) 0.959(0.001) 0.963(0.001)
– LSTM, char embed, no pretrained embed, attention, trigger 0.954(0.001) 0.954(0.001) 0.955(0.002)
– LSTM, char embed, pretrained embed, no attention, trigger 0.948(0.001) 0.944(0.002) 0.952(0.001)
– LSTM, no char embed, pretrained embed, no attention, trigger 0.943(0.000) 0.938(0.001) 0.948(0.001)
– biLSTM, char embed, no pretrained embed, no attention, trigger, mask 0.874(0.001) 0.852(0.010) 0.897(0.012)

Four-segment architecture
– LSTM, char embed, no pretrained embed, no attention, trigger 0.956(0.000) 0.956(0.001) 0.956(0.000)
– LSTM, char embed, no pretrained embed, attentiondeep 0.948(0.000) 0.949(0.001) 0.947(0.001)
– LSTM, char embed, no pretrained embed, attentionshallow 0.948(0.000) 0.951(0.001) 0.945(0.001)

Table 2: Deep learning scores from a five-fold cross-validation experiment on the larger DS dataset. The “mask”
option indicates that event participants have been masked (please see Section 7.3 for details).

Architecture variant F1 (st. dev.) Precision (st. dev.) Recall (st. dev.)

Linguistically-informed approach 0.989 0.979 1.0

Single-segment architecture
– biLSTM, char embed, no pretrained embed, no attention, no trigger 0.983(0.009) 0.978(0.000) 0.987(0.017)
– biLSTM, char embed, no pretrained embed, no attention, trigger 0.980(0.011) 0.978(0.000) 0.983(0.021)
– LSTM, no char embed, pretrained embed, no attention, trigger 0.974(0.005) 0.987(0.011) 0.961(0.009)
– LSTM, char embed, no pretrained embed, no attention, trigger 0.972(0.006) 0.978(0.000) 0.965(0.011)
– LSTM, no char embed, no pretrained embed, no attention, trigger 0.972(0.006) 0.978(0.000) 0.965(0.011)
– LSTM, char embed, pretrained embed, no attention, trigger 0.971(0.005) 0.987(0.011) 0.957(0.000)
– LSTM, char embed, no pretrained embed, attention, trigger 0.964(0.011) 0.987(0.011) 0.943(0.022)
– biLSTM, char embed, no pretrained embed, no attention, trigger, mask 0.942(0.017) 0.964(0.017) 0.922(0.029)

Four-segment architecture
– LSTM, char embed, no pretrained embed, no attention, trigger 0.974(0.006) 0.978(0.000) 0.970(0.011)
– LSTM, char embed, no pretrained embed, attentionshallow 0.960(0.005) 0.973(0.008) 0.948(0.011)
– LSTM, char embed, no pretrained embed, attentiondeep 0.958(0.017) 0.965(0.010) 0.952(0.035)

Table 3: Performance of all approaches on the Easy partition. The “mask” option indicates that event participants
have been masked (please see Section 7.3 for details).

Architecture variant F1 (st. dev.) Precision (st. dev.) Recall (st. dev.)

Linguistically-informed approach 0.143 0.138 0.148

Single-segment architecture
– LSTM, char embed, no pretrained embed, no attention, trigger 0.757(0.022) 0.659(0.019) 0.889(0.033)
– biLSTM, char embed, no pretrained embed, no attention, no trigger 0.752(0.007) 0.665(0.011) 0.867(0.018)
– biLSTM, char embed, no pretrained embed, no attention, trigger 0.748(0.031) 0.658(0.032) 0.867(0.030)
– LSTM, no char embed, no pretrained embed, no attention, trigger 0.733(0.008) 0.648(0.008) 0.844(0.015)
– LSTM, char embed, no pretrained embed, attention, trigger 0.703(0.010) 0.628(0.008) 0.800(0.030)
– LSTM, char embed, pretrained embed, no attention, trigger 0.690(0.025) 0.607(0.024) 0.800(0.030)
– LSTM, no char embed, pretrained embed, no attention, trigger 0.686(0.013) 0.610(0.014) 0.785(0.028)
– biLSTM, char embed, no pretrained embed, no attention, trigger, mask 0.576(0.009) 0.472(0.008) 0.741(0.033)
Four-segment architecture
– LSTM, char embed, no pretrained embed, no attention, trigger 0.698(0.014) 0.638(0.008) 0.770(0.028)
– LSTM, char embed, no pretrained embed, attentiondeep 0.696(0.017) 0.640(0.019) 0.763(0.018)
– LSTM, char embed, no pretrained embed, attentionshallow 0.690(0.018) 0.640(0.020) 0.748(0.015)

Table 4: Performance of all approaches on the Challenge partition. The “mask” option indicates that event partici-
pants have been masked (please see Section 7.3 for details).

found around the dependency path representing
the event. Taking into account the full, global in-
formation in the sentence as a single segment re-
sults in a simpler neural network with fewer pa-
rameters, which may also explain why the four-
segment architecture, which splits the sentence
into subsequences according to the components

associated with the cause, predicate and theme,
and runs each through distinct recurrent compo-
nents in the architecture does not perform quite as
well as the full, single-segment architecture.

Although the deep learning models generally
outperform the linguistically-informed model, Ta-
bles 3 and 4 uncover an interesting pattern in the



28

Number of negative
words per sentence Sample size

Best DL approach Linguistically-informed approach

Precision Recall F1 Precision Recall F1

0 49,972 0.978 0.980 0.98 0.873 0.937 0.904
1 16,063 0.90 0.902 0.901 0.694 0.38 0.491
2 2,566 0.94 0.896 0.917 0.773 0.691 0.730
3 300 0.884 0.857 0.87 0.675 0.49 0.568
4 30 1.0 0.92 0.958 0.8 0.48 0.6
5 3 1.0 1.0 1.0 0.5 0.5 0.5
6 1 1.0 1.0 1.0 1.0 1.0 1.0

Table 5: Polarity classification results stratified by the number of polarity-carrying words in the corresponding
sentence.

Number of negative
words per sentence Sample size

Best DL approach Linguistically-informed approach

Precision Recall F1 Precision Recall F1

0 49,972 0.882 0.934 0.907 0.873 0.937 0.904
1 16,063 0.643 0.627 0.761 0.694 0.38 0.491
2 2,566 0.789 0.734 0.761 0.773 0.691 0.730
3 300 0.744 0.689 0.716 0.675 0.49 0.568
4 30 0.941 0.64 0.761 0.8 0.48 0.6
5 3 1.0 1.0 1.0 0.5 0.5 0.5
6 1 1.0 1.0 1.0 1.0 1.0 1.0

Table 6: Polarity classification results stratified by the number of polarity-carrying words in the corresponding
sentence with masked participants.

differential performance on the Easy and Chal-
lenge data sets. In particular, on the Easy data set,
the linguistically informed approach performs ex-
ceptionally well, better than the highest perform-
ing deep learning model. The good performance
of the linguistically-informed model is not surpris-
ing here because, as discussed, the instances in the
data set were those for which the linguistically-
informed agreed with SIGNOR. But it is encour-
aging that the best deep learning model manages
to achieve this performance as well.

On the Challenge data set, however, the
linguistically-informed model performance dives
to an F1 of 0.143. Again, this is not a surprise
given that these data were ones that specifically
disagreed with a version of the linguistic model.
However, the performance of the best deep learn-
ing model degrades just to 0.757 F1, demonstrat-
ing the capacity of the model to maintain relatively
good performance in the face of more challeng-
ing data. We find this very encouraging, especially
considering that the neural models were trained on
the DS dataset, which contains distant-supervision
noise. These results demonstrate that the neural
models are able to generalize despite the presence
of noise.

Somewhat surprisingly, no attention-based
model outperformed the simpler bidirectional
LSTM without attention. This highlights that the

simpler LSTM method is sufficient to model po-
larity in this context, and that, possibly, the at-
tention mechanisms are more likely to overfit on
the distant-supervision noise present in this train-
ing data.

7.2 Analysis of complexity by negative terms

To better understand why the Challenge data set
was more difficult, we compared the performance
of the linguistically-informed approach to the best
deep learning model in detail. In this experiment,
we partitioned the data from the DS dataset into
subsets according to how many negative polar-
ity words (from the negative polarity lexicon de-
scribed in Section 3) appeared in a sentence and
evaluated each subset individually. Training for
the DL approach was performed using five-fold
cross-validation and the testing scores were com-
puted only for the instances with a specific num-
ber of negative polarity words. Table 5 summa-
rizes these results. Unsurprisingly, the scores are
negatively correlated with the number of negative
words in the sentence for both approaches. How-
ever, the linguistic approach suffers a much faster
drop in performance as the complexity of the sen-
tence increases. The best deep learning model,
however, still attains good performance even when
there are more than two negative words in the sen-
tence. For example, the linguistically-informed



29

method drops in performance from 0.904 F1 in
sentences with zero negative words to just 0.6 F1
in sentences with four negative words, whereas
the best neural model drops from 0.98 to 0.958
F1 in the same subsets. This is further proof that
the neural methods are able to aggregate multiple
negative-polarity hints from the larger context sur-
rounding the events.

7.3 Masking participants

To mitigate the potential of our method to over-
fit to the entities present in the events analyzed,
we implemented a variant of the previous analysis
in which we replaced the words that belonged to
a participant in a regulation event, both controller
and controlled, with a predefined token that masks
its identity but preserves its role in the event. For
example, in the sentence PTEN Plays a Role in
the Activation of the PI3K Signaling Pathway, the
participants PTEN and PI3K will be replaced by
the terms CONTROLLER and CONTROLLED, re-
spectively.

Table 6 presents the results of this analysis. The
table indicates that the performance of deep learn-
ing models decreases in general. However, the
same pattern observed when not masking the par-
ticipants arises. That is, the deep learning ap-
proach is not affected as much when the number
of negative terms increases compared to the lin-
guistic approach. Please note that this evaluation
is more stringent and could be considered a lower-
bound to what can be expected from a real world
scenario. It also proves that the deep learning
models do capture most their signal from the struc-
ture of the sentence in which the event is extracted,
and have a degree of resilience when facing partic-
ipants that were not observed during training. Ta-
bles 2–4 also show results for a model trained with
masked participants in the corresponding scenario.

8 Conclusions

We have introduced a corpus for the development
and assessment of approaches to assigning correct
polarity to biochemical events. Using this cor-
pus, we trained and evaluated a variety of deep
learning architectures and compared them to a
linguistically-informed model.

The best-performing deep learning architec-
tures incorporate character embeddings with
a bidirectional LSTM across the entire input
sentence, achieving an average F1 of 0.972

in a five-fold cross-validation study. This
model was found to do just as well as the
linguistically-informed model on examples that
the linguistically-informed model does well on,
but maintains much more robust performance in
the face of more difficult cases.

We also explored a deep learning architecture
that splits the input sentence into components that
are generally meaningful for the task, but found
that this did not reach the accuracy of the single-
segment input model, suggesting that there is im-
portant information spread across sentence com-
ponents that should be jointly processed.

Additional work remains. Further work should
be devoted to gain further F1 improvement, and
the place to start is deeper analyses of the kinds
of errors made by the best performing model. An-
other issue is speed efficiency: the linguistically-
informed model processes a sentence much faster
than the deep learning models, so is better-adapted
for high-throughput use cases. An area of fur-
ther exploration is to consider the pattern observed
in Table 5 and assess the tradeoffs of using the
fast linguistically-informed model for simpler sen-
tences (with no negative words) and then use the
slower deep learning model for more complex sen-
tences.

9 Acknowledgments

This work was supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under
the Automated Scientific Discovery Framework
(ASDF) program, grant W911NF018-1-0124. Mi-
hai Surdeanu declares a financial interest in lum.ai.
This interest has been properly disclosed to the
University of Arizona Institutional Review Com-
mittee and is managed in accordance with its con-
flict of interest policies.



30

References
Gabor Angeli, Melvin Jose Johnson Premkumar, and

Christopher D Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 344–354.

John A. Bachman, Benjamin M. Gyori, and Peter K.
Sorger. 2018. Famplex: a resource for entity recog-
nition and relationship resolution of human protein
families and complexes in biomedical text mining.
BMC Bioinformatics, 19(1):248.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Jari Björne and Tapio Salakoski. 2013. Tees 2.1: Au-
tomated annotation scheme learning in the bionlp
2013 shared task. In Proceedings of the BioNLP
shared task 2013 workshop, pages 16–25.

Quoc-Chinh Bui, David Campos, Erik van Mulligen,
and Jan Kors. 2013. A fast rule-based approach
for biomedical event extraction. In proceedings of
the BioNLP shared task 2013 workshop, pages 104–
108.

Yoav Goldberg. 2017. Neural network methods for nat-
ural language processing. Synthesis Lectures on Hu-
man Language Technologies, 10(1):1–309.

Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In 2013 IEEE workshop on auto-
matic speech recognition and understanding, pages
273–278. IEEE.

Benjamin M. Gyori, John A. Bachman, Kartik Subra-
manian, Jeremy L. Muhlich, Lucian Galescu, and
Peter K. Sorger. 2017. From word models to ex-
ecutable models of signaling networks using au-
tomated assembly. Molecular Systems Biology,
13(11):954.

Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,
Yves Van de Peer, and Filip Ginter. 2013. Evex
in st’13: Application of a large-scale text mining
resource to event extraction and network construc-
tion. In Proceedings of the BioNLP Shared Task
2013 Workshop, pages 26–34.

Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori.
2013. The genia event extraction shared task, 2013
edition-overview. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 8–15.

Anne Lauscher, Goran Glavaš, Simone Paolo Ponzetto,
and Kai Eckert. 2017. Investigating convolutional
networks and domain-specific embeddings for se-
mantic classification of citations. In Proceedings of

the 6th International Workshop on Mining Scientific
Publications, pages 24–28. ACM.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis lectures on human language tech-
nologies, 5(1):1–167.

Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining text data,
pages 415–463. Springer.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Bo Pang, Lillian Lee, et al. 2008. Opinion mining and
sentiment analysis. Foundations and Trends R© in In-
formation Retrieval, 2(1–2):1–135.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Scott Yih. 2017. Cross-sentence n-
ary relation extraction with graph lstms.

Livia Perfetto, Leonardo Briganti, Alberto Calderone,
Andrea Cerquone Perpetuini, Marta Iannuc-
celli, Francesca Langone, Luana Licata, Milica
Marinkovic, Anna Mattioni, Theodora Pavlidou,
et al. 2015. Signor: a database of causal relation-
ships between biological entities. Nucleic acids
research, 44(D1):D548–D554.

Chris Quirk and Hoifung Poon. 2016. Distant super-
vision for relation extraction beyond the sentence
boundary.

Marco A. Valenzuela-Escárcega, Özgün Babur, Gus
Hahn-Powell, Dane Bell, Thomas Hicks, Enrique
Noriega-Atala, Xia Wang, Mihai Surdeanu, Emek
Demir, and Clayton T. Morrison. 2018. Large-scale
automated machine reading discovers new cancer-
driving mechanisms. Database, 2018.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing.

Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456–1466. Association
for Computational Linguistics.

https://doi.org/10.1186/s12859-018-2211-5
http://msb.embopress.org/content/13/11/954
http://arxiv.org/abs/1301.3781
https://www.microsoft.com/en-us/research/publication/cross-sentence-n-ary-relation-extraction-graph-lstms/
https://www.microsoft.com/en-us/research/publication/distant-supervision-relation-extraction-beyond-sentence-boundary/
https://doi.org/10.1093/database/bay098

