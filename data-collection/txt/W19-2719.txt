



















































Using Rhetorical Structure Theory to Assess Discourse Coherence for Non-native Spontaneous Speech


Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 153–162
Minneapolis, MN, June 6, 2019. c©2019 Association for Computational Linguistics

153

Using Rhetorical Structure Theory to Assess Discourse Coherence for
Non-native Spontaneous Speech

Xinhao Wang1, Binod Gyawali2, James V. Bruno2, Hillary R. Molloy1,
Keelan Evanini2, Klaus Zechner2

Educational Testing Service
190 New Montgomery St #1500, San Francisco, CA 94105, USA

2660 Rosedale Road, Princeton, NJ 08541, USA
{xwang002, bgyawali, jbruno, hmolloy}@ets.org

{kevanini, kzechner}@ets.org

Abstract

This study aims to model the discourse struc-
ture of spontaneous spoken responses within
the context of an assessment of English
speaking proficiency for non-native speakers.
Rhetorical Structure Theory (RST) has been
commonly used in the analysis of discourse
organization of written texts; however, lim-
ited research has been conducted to date on
RST annotation and parsing of spoken lan-
guage, in particular, non-native spontaneous
speech. Due to the fact that the measurement
of discourse coherence is typically a key met-
ric in human scoring rubrics for assessments
of spoken language, we conducted research
to obtain RST annotations on non-native spo-
ken responses from a standardized assess-
ment of academic English proficiency. Sub-
sequently, automatic parsers were trained on
these annotations to process non-native spon-
taneous speech. Finally, a set of features were
extracted from automatically generated RST
trees to evaluate the discourse structure of non-
native spontaneous speech, which were then
employed to further improve the validity of an
automated speech scoring system.

1 Introduction

The spread of English as the main global lan-
guage for education and commerce is continuing,
and there is a strong interest in developing assess-
ment systems that can automatically score spon-
taneous speech from non-native speakers with the
goals of reducing the burden on human raters, im-
proving reliability, and generating feedback that
can be used by language learners (Zechner et al.,
2009; Higgins et al., 2011). Various features re-
lated to different aspects of speaking proficiency
have been explored, such as features for pronun-
ciation, prosody, and fluency (Cucchiarini et al.,
2002; Chen et al., 2009; Cheng, 2011; Higgins
et al., 2011), as well as features for vocabulary,

grammar, and content (Yoon et al., 2012; Chen and
Zechner, 2011; Yoon and Bhat, 2012; Chen and
Zechner, 2011; Xie et al., 2012; Qian et al., 2016).

Discourse coherence, which refers to how well
a text or speech is organized to convey infor-
mation, is an important aspect of communica-
tive competence, as is reflected in human scor-
ing rubrics for assessments of non-native English
(ETS, 2012). However, discourse-level features
have rarely been investigated in the context of au-
tomated speech scoring. In order to address this
deficiency, this study aims to explore effective
means to automate the analysis of discourse and
the measurement of coherence in non-native spo-
ken responses, thereby improving the validity of
an automated scoring system.

Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) is one of the most influential
approaches for document-level discourse analysis.
It can represent a document’s discourse structure
using a hierarchical tree in which nodes are re-
cursively linked with rhetorical relations and la-
beled with nucleus or satellite tags to depict the
importance of the child nodes in a relation. In our
previous study (Wang et al., 2017a), RST-based
discourse annotations were obtained on a corpus
of 600 spontaneous spoken responses provided
by non-native English speakers in the context of
an English speaking proficiency assessment. In
this paper, we continued this line of research, and
made further contributions as follows:

• A larger annotated corpus consisting of 1440
non-native spontaneous spoken responses
was obtained using an annotation scheme
based on the RST framework. In addition
to the previously annotated 600 responses
(Wang et al., 2017a), annotations on addi-
tional 840 responses were obtained to en-
large the data set that can be used to train



154

an automatic RST parser. When compar-
ing the annotations from two independent hu-
man experts on 120 responses, the resulting
micro-averaged F1 scores on the three differ-
ent levels of span, nuclearity, and relation1

are 86.8%, 72.2%, and 58.2%, respectively.

• Based on all these manual annotations, au-
tomatic RST parsers were trained and eval-
uated. When comparing the automatically
generated trees with double annotations from
each of the two human experts separately,
the F1 scores on the three levels of span,
nuclearity, and relation are 76.1%/77.0%,
57.6%/59.7%, and 42.6%/44.4%, respec-
tively.

• A set of RST-based features were introduced
to measure the discourse structure of non-
native spontaneous speech, where 1) an auto-
matic speech recognizer (ASR) was used to
transcribe the speech into text; 2) the afore-
mentioned automatic parsers were applied to
build RST trees based on the ASR output;
3) a set of features extracted from the au-
tomatic trees were explored, and the results
show that these discourse features can predict
holistic proficiency scores with an accuracy
of 55.9%. Finally, these features were used
in combination with other types of features to
enhance the validity of an automated speech
scoring system.

2 Previous Work

RST is a descriptive framework that has been
widely used in the analysis of the discourse organi-
zation of written texts (Taboada and Mann, 2006b)
and has also been applied to various natural lan-
guage processing tasks, including language gen-
eration, text summarization, and machine transla-
tion (Taboada and Mann, 2006a). In particular, the
availability of the RST Discourse Treebank (Carl-
son et al., 2001), with annotations on a selection
of 385 Wall Street Journal articles from the Penn
Treebank2, has facilitated RST-based discourse
analysis of written texts, since it provides a stan-
dard benchmark for comparing the performance of
different parsers. A wide range of techniques have

1In this paper, all the reported results on the relation level
use the full labels of both nuclearity and relation for evalua-
tion.

2https://catalog.ldc.upenn.edu/
LDC2002T07

been applied to this task, and document-level dis-
course parsers are available (Marcu, 2000a; Sagae,
2009; Hernault et al., 2010; Joty et al., 2013; Feng
and Hirst, 2014; Li et al., 2014; Ji and Eisenstein,
2014; Li et al., 2016; Liu and Lapata, 2017; Braud
et al., 2017; Wang et al., 2017c). Morey et al.
(2017) replicated the same evaluation procedure
on 9 recent parsers, and indicated that the recent
gains in discourse parsing can be attributed to the
distributed representations.

Another important application of RST closely
related to our research is the automated evalua-
tion of discourse in student essays. For exam-
ple, one study used features for each sentence in
an essay to reflect the status of its parent node as
well as its rhetorical relation based on automati-
cally parsed RST trees, with the goal of provid-
ing feedback to students about the discourse struc-
ture in their essay (Burstein et al., 2003). Another
study compared features derived from deep hierar-
chical discourse relations based on RST trees with
features derived from shallow discourse relations
based on Penn Discourse Treebank (PDTB) anno-
tations (Prasad et al., 2008) and demonstrated the
positive impact of using deep discourse structures
to evaluate text coherence (Feng et al., 2014).

Related work has also been conducted to ana-
lyze discourse relations in spoken language, which
is produced and processed differently from written
texts (Rehbein et al., 2016), and often lacks ex-
plicit discourse connectives that are more frequent
in written language. For example, RST has been
used to analyze the semi-structured interviews of
Alzheimer’s patients (Paulino and Sierra, 2017;
Paulino et al., 2018).

However, the annotation scheme with shallow
discourse structure and relations from the PDTB
(Prasad et al., 2008) has been generally used for
spoken language (Demirsahin and Zeyrek, 2014;
Stoyanchev and Bangalore, 2015) instead of the
rooted-tree structure that is employed in RST. For
example, Tonelli et al. (2010) adapted the PDTB
annotation scheme to annotate discourse relations
in spontaneous conversations in Italian, and Re-
hbein et al. (2016) compared two frameworks,
PDTB and Cognitive approach to Coherence Re-
lations (CCR) (Sanders et al., 1992), for the anno-
tation of discourse relations in spoken language.

Regarding the measurement of discourse co-
herence in the automated assessment of spoken
language, our previous work (Wang et al., 2013,

https://catalog.ldc.upenn.edu/LDC2002T07
https://catalog.ldc.upenn.edu/LDC2002T07


155

2017b) obtained an annotated corpus of non-native
spontaneous speech in which each response was
assigned a coherence score on a scale of 1 to 3,
and several surface-based features were used to
count the use of nouns, pronouns, conjunctions,
and discourse connectives. However, that research
did not investigate features that can actually repre-
sent the hierarchical discourse structure of spoken
responses as described in the RST framework.

In contrast to previous studies, this study fo-
cuses on monologic spoken responses produced
by non-native speakers within the context of a lan-
guage proficiency assessment and aims to identify
the discourse structure of spoken responses. The
RST framework was selected due to the fact that
it can effectively demonstrate the deep hierarchi-
cal discourse structure across an entire response,
rather than focusing on the local coherence of ad-
jacent units.

3 Data and Annotation

3.1 Data

This study obtained manual RST annotations on
a corpus of 1440 spoken responses, where 600
of them were obtained in our previous work
(Wang et al., 2017a), and the additional 840 re-
sponses were annotated more recently. All the
responses were drawn from a large-scale, high-
stakes standardized assessment of English for non-
native speakers, the TOEFL R© Internet-based Test
(TOEFL R© iBT), which assesses English commu-
nication skills for academic purposes (ETS, 2012).
The speaking section of the TOEFL iBT assess-
ment contains six tasks, each of which requires
the test taker to provide an unscripted spoken re-
sponse, 45 or 60 seconds in duration. The cor-
pus used in this study includes 240 responses
from each of six different test questions that com-
prise two different speaking tasks: 1) Independent
questions, in which test takers provide an opin-
ion based on personal experience (N = 480 re-
sponses) and 2) Integrated questions, in which test
takers summarize or discuss material provided in
a reading and/or listening passage (N = 960 re-
sponses). The spoken responses were all manually
transcribed using standard punctuation and capi-
talization.

Responses were all provided with holistic En-
glish proficiency scores on a scale of 1 to 4
(weak to good) by expert human raters in the
context of operational, high-stakes scoring for

the spoken language assessment. The scoring
rubrics address the following three main aspects
of speaking proficiency: delivery (pronunciation,
fluency, prosody), language use (grammar and lex-
ical choice), and topic development (content and
coherence). Responses were balanced for pro-
ficiency levels, i.e., 60 responses were included
from each of the 4 score points from each of the
6 test questions.

In addition to the holistic proficiency scores, the
transcription of each spoken response in this cor-
pus was also provided with a global discourse co-
herence score by two expert annotators (not drawn
from the pool of expert human raters who provided
the holistic scores) in our previous study (Wang
et al., 2013). The score scale for these coherence
scores was from 1 to 3, and the three score points
were defined as follows: 3 = highly coherent (con-
tains no instances of confusing arguments or ex-
amples), 2 = somewhat coherent (contains some
awkward points in which the speaker’s line of ar-
gument is unclear), 1 = barely coherent (the en-
tire response was confusing and hard to follow).
A subset of 600 responses were double annotated,
and the inter-annotator agreement for these coher-
ence scores was with a quadratic weighted kappa
of 0.68.

3.2 Annotation Guidelines

This study used the same annotation guidelines as
in our previous work Wang et al. (2017a), which is
a modified version of the tagging reference man-
ual from the RST Discourse Treebank (Carlson
and Marcu, 2001). According to these guide-
lines, annotators segment a transcribed spoken
response into Elementary Discourse Unit (EDU)
spans of text (corresponding to clauses or clause-
like units), and indicate rhetorical relations be-
tween non-overlapping spans which typically con-
sist of a nucleus (the most essential information in
the rhetorical relation) and a satellite (supporting
or background information).

In contrast to well-formed written text, non-
native spontaneous speech frequently contains un-
grammatical sentences, disfluencies, fillers, hesi-
tations, false starts, and unfinished utterances. In
some cases, these spoken responses do not con-
stitute coherent, well-formed discourse. In order
to account for these differences, we created an
addendum to the RST Discourse Treebank man-
ual introducing the following additional relations:



156

disfluency relations (in which the disfluent span
is the satellite and the corresponding fluent span
is the nucleus), awkward relations (correspond-
ing to portions of the response where the speaker’s
discourse structure is infelicitous; awkward rela-
tions are based on pre-existing relations, such as
awkward-Reason, if the intended relation is clear
but is expressed incoherently, or awkward-Other
if there is no clear relation between the awkward
EDU and the surrounding discourse), unfinished
utterance relations (representing EDUs at the end
of a response that are incomplete because the test
taker ran out of time, in which the incomplete span
is the satellite and the root node of the discourse
tree is the nucleus), and discourse particle rela-
tions (such as you know and right, which are satel-
lites of adjacent spans).

The discourse annotation tool used in the RST
Discourse Treebank3 was also adopted for this
study. Using this tool, annotators incrementally
build hierarchical discourse trees, in which the
leaves are the EDUs and the internal nodes cor-
respond to contiguous spans of text. When the an-
notators assign the rhetorical relation for a node of
the tree, they provide the relation’s label (drawn
from the pre-defined set of relations in the anno-
tation guidelines) and also indicate whether the
spans that comprise the relation are nuclei or satel-
lites. Figure 1 shows an example of an annotated
RST tree for a response with a proficiency score
of 1. This response includes three disfluencies
(EDUs 3, 6, and 9), which are satellites of the
corresponding repair nuclei. In addition, the re-
sponse also includes an awkward Comment-Topic
relation between EDU 2 and the node combin-
ing EDUs 3-11, indicated by awkward-Comment-
Topic-2; in this multinuclear relation, the annota-
tor judged that the second branch of the relation
was awkward, which is indicated by the 2 that was
appended to the relation label.

3.3 Human Annotations

Among the 600 annotations obtained in Wang
et al. (2017a), 120 responses from 6 test ques-
tions (5 responses from each score level for each
question) were double annotated. The standard
evaluation method of F1 scores on three levels
(span, nuclearity, and relation) (Marcu, 2000b)
was used to evaluate the human agreement, where

3Downloaded from http://www.isi.edu/
licensed-sw/RSTTool/index.html

the F1 scores were calculated globally by compar-
ing the two annotators’ labels from all samples,
i.e., a micro-averaged F1 score. The human agree-
ment results are 86.8%, 72.2%, and 58.2%, ac-
cording to the span, nuclearity, and relation lev-
els respectively. This level of agreement is similar
to the inter-annotator agreement rates on the RST
Discourse Treebank, i.e., 88.3% on span, 77.3%
on nuclearity, and 64.7% on relation, respectively
(Joty et al., 2015; Morey et al., 2017).

The human agreement results also indicate that
two annotators tend to agree better on responses
from speakers with higher speaking proficiency
levels, which is demonstrated by positive corre-
lations (Pearson correlation coefficients) between
the F1 agreement scores (F1 scores from each of
the double annotated samples) and the human pro-
ficiency ratings, approximately 0.2 on all three
levels. Meanwhile, the correlations between F1
agreement scores and the human coherence scores
are even higher, reaching 0.358 on the fully la-
beled relation level, which means that human
raters agreed better with each other on responses
receiving higher coherence scores, as expected. In
addition, annotators also provided feedback that
this data set posed some unique challenges com-
pared to the data set used to create the RST Dis-
course Treebank. While the Wall Street Journal ar-
ticles are written and edited by professionals, our
data set consisted of human transcriptions of non-
native spontaneous speech, which were at times
unintelligible due to the lack of proficiency and
transcription inaccuracy.

4 Automatic Parsing

4.1 Parser Training

There has been a variety of research on document-
level discourse parsing based on the RST Dis-
course Treebank, and multiple RST parsers are
available as open source tools. In this study, since
the focus of our research is not to investigate ad-
vanced techniques to improve the state-of-art in
parsing, we employed a pre-existing open-source
parser from Heilman and Sagae (2015)4, which
was implemented following the work of Sagae
(2009) and Ji and Eisenstein (2014). It is a fast,
transition-based parser and can process short doc-
uments such as news articles or essays in less

4Downloaded from https://github.
com/EducationalTestingService/
discourse-parsing

http://www.isi.edu/licensed-sw/RSTTool/index.html
http://www.isi.edu/licensed-sw/RSTTool/index.html
https://github.com/EducationalTestingService/discourse-parsing
https://github.com/EducationalTestingService/discourse-parsing
https://github.com/EducationalTestingService/discourse-parsing


157

Figure 1: Example of an annotated RST tree on a response with a proficiency score of 1.

than a second. Since the ultimate goal is to in-
troduce the discourse parser into an automated
speech scoring system consisting of many inter-
dependent downstream components, reducing the
amount of time required for extracting discourse
features is an advantage. We first examined the
performance of this selected parser by re-training
and re-evaluating it on the RST Discourse Tree-
bank with the standard data partition as in Heil-
man and Sagae (2015), i.e., 347 samples as the
training set, 40 of them were used as the devel-
opment set, and 38 samples as the test set. In this
paper, all the parsers we built were evaluated with
the micro F1 score. When using the gold stan-
dard syntax trees and EDU segmentations, the F1
scores on three levels of span, nuclearity, and re-
lation can reach 84.1%, 69.6%, and 56.5% respec-
tively, which are close to state-of-the-art accuracy,
as reported in Morey et al. (2017).

In this work, the annotated data obtained as de-
scribed in Section 3 was used for parser building
and evaluation. Among the 1440 annotated re-
sponses, the data was split into a training set with
1271 single-annotated responses, a development
set with 49 single-annotated responses, and a test
set with 120 double-annotated responses. After-
wards, the 49 responses in the development set
were further double annotated, which allowed us
to tune the parser on annotations from both human
experts. In contrast to the Wall Street Journal ar-
ticles in the RST Discourse Treebank (RST DT),
the responses in the corpus of non-native sponta-
neous speech (RST SS) are much shorter. Table 1
compares the RST DT and the RST SS data sets
in terms of the means and standard deviations of
the number of EDUs and word tokens. It shows
that the RST SS corpus has more samples (1271
vs. 347 in the training set), but the total num-
bers of EDUs and words in RST SS are similar to

Table 1: Average numbers of EDUs and word tokens
(and their standard deviations) appearing in the RST
Discourse Treebank (RST DT) and the annotated cor-
pus of non-native spontaneous speech (RST SS).

# Samples
# EDUs

Mean (std)
# Words

Mean (std)
RST DT

Train 347 56.0 (51.5) 531.3 (464.0)
Test 38 61.7 (63.4) 570.2 (549.0)

RST SS
Train 1271 14.3 (4.7) 122.9 (36.0)
Dev 49 13.0 (4.7) 112.7 (35.7)
Test 120 14.8 (4.4) 127.4 (33.5)

the RST DT corpus (18,171 vs. 19,443 EDUs and
156,254 vs. 184,352 words in the training set).

In addition, Table 2 shows the most common re-
lations that appear in the training sets of RST SS
and lists their percentages, taken according to
their frequency. The percentage of these rela-
tions appearing in the RST DT are also included
for comparison. The top five most common re-
lations overlap, but the other five relations that
frequently appear in RST SS are relatively rare
in RST DT, especially the disfluency-self-correct-
rpt and disfluency-false-start relations, which is
unique to the spoken responses and will not appear
in the written texts. In addition, the proportions of
each relation appearing in RST SS and RST DT
are quite different.

4.2 Parser Evaluation

For comparison, we trained three different parsers
on both RST DT and RST SS: (a) RST SS: us-
ing the training set from the corpus of non-native
spontaneous speech, where 49 double-annotated
responses were used as the development set; (b)
RST DT: using the training set from the RST Dis-



158

Table 2: Top 10 relations appearing in the training
set of the annotated corpus of spontaneous speech
(RST SS). The percentages of each relation appear-
ing in both RST SS and the RST Discourse Treebank
(RST DT) are listed for comparison.

RST SS RST DT
list 18.2% 13.3%
elaboration
-object-attribute-e

7.8% 10.4%

same-unit 7.1% 11.1%
attribution 5.5% 11.3%
elaboration-additional 4.7% 13.2%
reason 3.3% 0.8%
disfluency
-self-correct-rpt

2.8% –

evidence 2.4% 0.7%
disfluency-false-start 2.3% –
conclusion 2.2% 0.02%

Table 3: Discourse parsing performance in terms of F1
scores (%) on three levels of Span, Nuclearity, and Re-
lation. Human agreements are also listed for compari-
son. Within each cell, two micro F1 scores according to
the gold standards from each of two human annotators
are both reported.

Span Nucleus Relation

RST SS
75.5
76.2

56.4
58.6

41.2
43.1

RST DT
73.0
73.8

53.0
54.8

35.0
36.5

RST SS +
RST DT

76.1
77.0

57.6
59.7

42.6
44.4

Human 86.8 72.2 58.2

course Treebank, where 40 samples from the train-
ing set were separated as the development set; and
(c) RST SS + RST DT: using the training sets
from both RST SS and RST DT, where the devel-
opment set is the same one used in (a). These three
parsers were evaluated on the same test set from
RST SS, where the gold standard EDU segmenta-
tions were used. As shown in Table 3, the parser
trained on RST SS outperformed the one trained
on RST DT, especially on the relation level, i.e.,
41.2%/43.1% vs. 35.0%/36.5%. By combining
both data corpora, the F1 scores can further be im-
proved.

Furthermore, besides using gold standard EDU
segmentations, we also applied the automatic
EDU segmenter within the parser to generate seg-

mentations and then build the RST trees upon
them. The evaluation results showed that F1
scores of all three parsers were greatly reduced
through this transition. For example, they were de-
creased to 53.0%/53.6% on span, 40.4%/41.9% on
nuclearity, and 29.3%/31.1% on relation for parser
(a) trained on RST SS. Therefore, the improve-
ment of EDU segmentations is also a research fo-
cus of our future work. In the following section
on discourse modeling for spontaneous speech,
parser (a), which was trained on RST SS and us-
ing automatic EDU segmentations, was employed
for discourse modeling.

5 Discouse Features

The ultimate goal of this line of research is to in-
vestigate which features are effective for automat-
ically assessing discourse structure in non-native
spontaneous speech. We previously used RST
trees for this purpose and proposed several fea-
tures based on the distribution of relations and
the structure of trees (Wang et al., 2017a), in-
cluding the number of EDUs (n edu), the num-
ber of relations (n rel), the number of awkward
relations (n awk rel), the number of rhetorical re-
lations, i.e., relations that were neither classified
as awkward nor as disfluencies (n rhe rel), the
number of different types of rhetorical relations
(n rhe rel types), the percentage of rhetorical rela-
tions (perc rhe rel) out of all relations, the depth of
the RST trees (tree depth), and the ratio between
n edu and tree depth (ratio nedu depth).

In this work, we first examined these eight fea-
tures on the 1271 single-annotated responses, i.e.,
the RST SS training set used to build the auto-
matic parser as described in Section 4.1. Fea-
tures were extracted from the manually annotated
trees, and then the Pearson correlation coefficients
of these features with both the holistic proficiency
scores as well as the discourse coherence scores
are reported in Table 4, which demonstrates the ef-
fectiveness of these features. The n rhe rel feature
achieves the highest correlation with the holistic
proficiency scores at 0.719, and the normalized
feature perc rhe rel achieves the highest correla-
tion with the coherence scores at 0.609. There
are six features that receive higher correlations
with the proficiency scores, whereas the other
two features (n awk rel and perc rhe rel) receive
higher absolute correlations with the coherence
scores. This is consistent with our previous obser-



159

Table 4: Pearson correlation coefficients (r) of dis-
course features with both the holistic proficiency scores
as well as the discourse coherence scores.

Features Proficiency Coherence
n edu 0.612 0.366
n rel 0.624 0.391
n awk rel -0.425 -0.533
n rhe rel 0.719 0.536
n rhe relTypes 0.675 0.547
perc rhe rel 0.586 0.609
tree depth 0.402 0.249
ratio nedu depth 0.536 0.308

vations, where RST-based discourse features gen-
erally have higher correlations with the holistic
speaking proficiency scores than with the more
specific discourse coherence scores (Wang et al.,
2017a). One potential explanation could be the
difference in score range: 1-3 for the discourse
scores vs. 1-4 for the more fine-grained holistic
proficiency scores.

6 Automated Scoring

Besides examining the discourse features based
on the manually annotated trees as above, this
study also conducted an experiment to examine
them on automatically generated trees to mea-
sure the discourse structure of non-native spon-
taneous speech, and then further employ them in
an automated spoken English assessment system,
SpeechRaterTM (Zechner et al., 2007, 2009).

6.1 Experimental Setup

The task is to build effective classification mod-
els, referred to as “scoring models”, which can au-
tomatically predict the holistic proficiency scores
by measuring the different aspects of non-native
speaking proficiency, including pronunciation,
prosody, fluency, vocabulary, grammar, and, in
particular, discourse in spontaneous speech. In
order to obtain credible evaluation results, this
study collected a large data set from the opera-
tional TOEFL iBT assessment to conduct this ex-
periment, which includes 17,194 speakers who re-
sponded to all the six test questions as described in
Section 3.1. The holistic proficiency scores were
provided during the operational test, but more spe-
cific discourse coherence scores were not available
for this large data set. The whole data set was
partitioned into two sets: one containing 12,194

speakers (73,164 responses) as the training set to
build the scoring models, and the other one con-
taining 5,000 speakers (30,000 responses) to test
the model performance.

The baseline scoring model was built with ap-
proximately 130 automatic features extracted from
the SpeechRater system, which can measure the
pronunciation, prosody, fluency, rhythm, vocab-
ulary, and grammar of spontaneous speech. All
SpeechRater features were extracted either di-
rectly from the speech signal or from the output of
a Kaldi-based automatic speech recognizer (Qian
et al., 2016) with a word error rate of 20.9% on an
independent evaluation set with non-native spon-
taneous speech from the TOEFL iBT speaking
test.

Based on the automatic speech recognition out-
put (without punctuations and capitalization) gen-
erated by SpeechRater, the automatic parsers de-
veloped in section 4.1 were applied to extract RST
trees. Afterwards, the RST-based features were
automatically obtained. Therefore, in this process,
no manual transcriptions or manual annotations
were involved. Furthermore, the RST-based dis-
course features can be combined with the baseline
features to extend the ability of SpeechRater to as-
sess the discourse structure of non-native sponta-
neous speech.

6.2 Results and Discussion

The automatically generated discourse features
were first examined on the scoring model train-
ing partition, where Pearson correlation coeffi-
cients between automatic features and proficiency
scores were calculated. There were two sets of
features extracted and examined, based on two dif-
ferent parsers: one was trained with RST SS and
the other one was trained with both RST SS and
RST DT as shown in Section 4.1.

Table 5 presents the Pearson correlation coeffi-
cients of these two sets of features with the pro-
ficiency scores. For the five features n edu, n rel,
n awk rel, n rhe rel, and tree depth, the difference
is limited, i.e., smaller than 0.004. In contrast, the
other three features, n rhe rel types, perc rhe rel,
and ratio nedu depth, achieve better correlations
with features based on the RST SS parser. This
indicates the effectiveness of our annotations in
capturing discourse in spoken language. There-
fore, in the following experiments on scoring mod-
els, the features were obtained using the parser



160

Table 5: Pearson correlation coefficients (r) of dis-
course features with both the holistic proficiency
scores. RST SS indicates using the parser trained with
the annotations on speech data during the feature gen-
eration, and RST SS + RST DT indicates using the
parser trained with both the annotations on speech data
and the RST Discourse Treebank.

Features RST SS
RST SS +
RST DT

n edu 0.424 0.427
n rel 0.401 0.405
n awk rel -0.096 -0.096
n rhe rel 0.418 0.42
n rhe rel types 0.314 0.308
perc rhe rel 0.225 0.211
tree depth 0.329 0.328
ratio nedu depth 0.316 0.289

trained with the RST SS data. Even though all
these features were extracted with the automatic
speech recognition output and with the automatic
parser, they can still achieve moderate correlations
with the proficiency scores in a range of 0.2-0.5,
except for the feature based on count of awkward
relations. The absolute correlation of n awk rel
feature is less than 0.1, which was caused by the
failure of the automatic parser to identify awkward
relations.

Furthermore, scoring models were built with
SpeechRater features and RST-based discourse
features to automatically predict the holistic pro-
ficiency scores using the machine learning tool of
scikit-learn5 (Pedregosa et al., 2011). For this ex-
periment, we used the Random Forest classifica-
tion method to build the scoring models.

Table 6 shows that the baseline system with
131 SpeechRater features can reach an accuracy
of 65.3%. By introducing the eight RST-based
features, there is a very slight improvement on
the accuracy to 65.4% and no improvement in
terms of the Pearson correlation coefficient be-
tween the automatic and human scores. A scoring
system only using eight RST-based features can
achieve an accuracy of 55.9%. These results indi-
cate that the proposed features can be used to mea-
sure the discourse coherence of non-native sponta-
neous spoken responses. Due to the fact that these

5SKLL, a python tool making the running of scikit-learn
experiments simpler, was used. Downloaded from https:
//github.com/EducationalTestingService/
skll.

Table 6: Performance of the automatic scoring mod-
els to predict holistic proficiency scores. The baseline
system was built with 131 SpeechRater features, and
the automatically generated 8 RST-based features were
appended to measure the discourse structure.

Accuracy (%) r
RST 55.9 0.371
SpeechRater 65.3 0.587
SpeechRater + RST 65.4 0.587

131 SpeechRater features are powerful in mea-
suring various aspects of non-native spontaneous
speech, the improvement by introducing discourse
features to predict the holistic proficiency scores
is limited. But on the other hand, by employing
the proposed discourse-level features, the validity
of an automatic system for English language profi-
ciency assessment can be improved, because it en-
ables the measurement of an important aspect of
speech that appears in the human scoring rubrics.

7 Conclusion

The goal of this research effort is to model dis-
course structure in non-native spontaneous speech
to facilitate the automatic assessment of English
language proficiency. In order to achieve this goal,
we first obtained an annotated corpus of 1440 spo-
ken responses produced by non-native speakers
of English in the context of an English speak-
ing proficiency assessment using Rhetorical Struc-
ture Theory and then trained automatic discourse
parsers based on the human annotations. Subse-
quently, discourse features were extracted from
the speech signal using automatic speech recog-
nition output and automatically parsed RST trees;
these features mostly achieved moderate correla-
tions with human holistic proficiency scores rang-
ing between 0.2 and 0.5. Finally, a scoring model
trained using the eight proposed discourse features
can predict the proficiency scores with an accuracy
of 55.9%, and by introducing them into an auto-
matic speech scoring system, the validity of the
system can be improved.

References
Chloé Braud, Maximin Coavoux, and Anders Søgaard.

2017. Cross-lingual RST discourse parsing. In Pro-
ceedings of the EACL conference.

Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE stuff: Automatic identification

https://github.com/EducationalTestingService/skll
https://github.com/EducationalTestingService/skll
https://github.com/EducationalTestingService/skll


161

of discourse structure in student essays. IEEE Intel-
ligent Systems, 18(1):32–39.

Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging reference manual. Technical Report ISI-TR-
545, ISI Technical Report.

Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurows. 2001. Building a discourse-tagged corpus
in the framework of rhetorical structure theory. In
Proceedings of the 2nd SIGDIAL Workshop on Dis-
course and Dialogue, pages 1–10.

Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 442–449.

Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for auto-
mated scoring of spontaneous non-native speech. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 722–731, Portland, Ore-
gon, USA.

Jian Cheng. 2011. Automatic assessment of prosody in
high-stakes English tests. In Proceedings of Inter-
speech, pages 27–31.

Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers’ fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862–2873.

Isin Demirsahin and Deniz Zeyrek. 2014. Annotating
discourse connectives in spoken Turkish. In Pro-
ceedings of LAW VIII - The 8th Linguistic Annota-
tion Workshop, pages 105–109.

ETS. 2012. The official guide to the TOEFL R© test.
Fourth Edition, McGraw-Hill.

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 511–521.

Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst.
2014. The impact of deep hierarchical discourse
structures in the evaluation of text coherence. In
Proceedings of COLING 2014, The 25th Interna-
tional Conference on Computational Linguistics:
Technical Papers, pages 940–949.

Michael Heilman and Kenji Sagae. 2015. Fast rhetor-
ical structure theory discourse parsing. CoRR,
abs/1505.02425.

Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification.
Dialogue & Discourse, 1(3):1–33.

Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech and Language, 25:282–
306.

Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 13–24.

Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 486–496,
Sofia, Bulgaria.

Shafiq Joty, Giuseppe Carenini, and Raymond T Ng.
2015. Codra: A novel discriminative framework
for rhetorical analysis. Computational Linguistics,
41(3):385–435.

Qi Li, Tianshi Li, and Baobao Chang. 2016. Discourse
parsing with attention-based hierarchical neural net-
works. In Proceedings of the EMNLP conference,
pages 362–371.

Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.
2014. Text-level discourse dependency parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 25–35.

Yang Liu and Mirella Lapata. 2017. Learning contex-
tually informed representations for linear-time dis-
course parsing. In Proceedings of the EMNLP con-
ference, pages 1289–1298.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text - Interdisciplinary
Journal for the Study of Discourse (Text), 8(3):243–
281.

Daniel Marcu. 2000a. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26:395–448.

Daniel Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.

Mathieu Morey, Philippe Muller, and Nicholas Asher.
2017. How much progress have we made on RST
discourse parsing? a replication study of recent re-
sults on the RST-DT. In Proceedings of the EMNLP
conference, pages 1319–1324.



162

A Paulino, Gerardo Sierra, Laura Hernandez-
Dominguez, Iria da Cunha, and Gemma Bel-
Enguix. 2018. Rhetorical relations in the speech of
alzheimer’s patients and healthy elderly subjects:
An approach from the rst. Computacion y Sistemas,
22:895–905.

Anayeli Paulino and Gerardo Sierra. 2017. Applying
the rhetorical structure theory in alzheimer patients’
speech. In Proceedings of the 6th Workshop on Re-
cent Advances in RST and Related Formalisms, page
34–38.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of machine
learning research, 12(Oct):2825–2830.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, and Livio Robaldo. 2008. The Penn Dis-
course TreeBank 2.0. In The 6th International
Conference on Language Resources and Evaluation
(LREC), pages 2961–2968.

Yao Qian, Xinhao Wang, Keelan Evanini, and David
Suendermann-Oeft. 2016. Self-adaptive DNN for
improving spoken language proficiency assessment.
In Proceedings of Interspeech 2016, pages 3122–
3126.

Ines Rehbein, Merel Scholman, and Vera Demberg.
2016. Annotating discourse relations in spoken lan-
guage: A comparison of the PDTB and CCR frame-
works. In The Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
pages 1039–1046.

Kenji Sagae. 2009. Analysis of discourse structure
with syntactic dependencies and data-driven shift-
reduce parsing. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
81–84.

Ted J. M. Sanders, Wilbert P. M. Spooren, and Leo
G. M. Noordman. 1992. Toward a taxonomy of co-
herence relations. Discourse Processes, 15(1):1–35.

Svetlana Stoyanchev and Srinivas Bangalore. 2015.
Discourse in customer care dialogues. Poster pre-
sented at the Workshop of Identification and Anno-
tation of Discourse Relations in Spoken Language.
Saarbrücken, Germany.

Maite Taboada and William C. Mann. 2006a. Appli-
cations of Rhetorical Structure Theory. Discourse
Studies, 8(4):567–588.

Maite Taboada and William C. Mann. 2006b. Rhetor-
ical Structure Theory: Looking back and moving
ahead. Discourse Studies, 8(3):423–459.

Sara Tonelli, Giuseppe Riccardi, Rashmi Prasad, and
Aravind Joshi. 2010. Annotation of discourse re-
lations for conversational spoken dialogs. In The

Seventh International Conference on Language Re-
sources and Evaluation (LREC’10), pages 2084–
2090.

Xinhao Wang, James Bruno, Hillary Molloy, Kee-
lan Evanini, and Klaus Zechner. 2017a. Discourse
annotation of non-native spontaneous spoken re-
sponses using the rhetorical structure theory frame-
work. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 263–268.

Xinhao Wang, Keelan Evanini, and Klaus Zechner.
2013. Coherence modeling for the automated as-
sessment of spontaneous spoken responses. In Hu-
man Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, Proceedings, pages 814–819,
Atlanta, Georgia.

Xinhao Wang, Keelan Evanini, Klaus Zechner, and
Matthew Mulholland. 2017b. Modeling discourse
coherence for the automated scoring of spontaneous
spoken responses. In Proceedings of the Seventh
ISCA workshop on Speech and Language Tech-
nology in Education 2017, SLaTE, August 25–26,
Djurö, Stockholm, Sweden.

Yizhong Wang, Sujian Li, and Houfeng Wang. 2017c.
A two-stage parsing method for text-level discourse
analysis. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 184–188.

Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103–111.

Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners’ syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 600–608.

Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
the Innovative Use of NLP for Building Educational
Applications, pages 180–189.

Klaus Zechner, Derrick Higgins, and Xiaoming Xi.
2007. SpeechraterSM: A construct-driven approach
to scoring spontaneous non-native speech. In Pro-
ceedings of the International Speech Communica-
tion Association Special Interest Group on Speech
and Language Technology in Education, pages 128–
131.

Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883–895.


