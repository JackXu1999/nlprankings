



















































Neural Probabilistic Model for Non-projective MST Parsing


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 59–69,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Neural Probabilistic Model for Non-projective MST Parsing

Xuezhe Ma and Eduard Hovy
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213, USA

xuezhem@cs.cmu.edu, hovy@cmu.edu

Abstract

In this paper, we propose a probabilistic
parsing model that defines a proper con-
ditional probability distribution over non-
projective dependency trees for a given
sentence, using neural representations as
inputs. The neural network architec-
ture is based on bi-directional LSTM-
CNNs, which automatically benefits from
both word- and character-level represen-
tations, by using a combination of bidi-
rectional LSTMs and CNNs. On top
of the neural network, we introduce a
probabilistic structured layer, defining a
conditional log-linear model over non-
projective trees. By exploiting Kirchhoff’s
Matrix-Tree Theorem (Tutte, 1984), the
partition functions and marginals can be
computed efficiently, leading to a straight-
forward end-to-end model training pro-
cedure via back-propagation. We eval-
uate our model on 17 different datasets,
across 14 different languages. Our parser
achieves state-of-the-art parsing perfor-
mance on nine datasets.

1 Introduction

Dependency parsing is one of the first stages in
deep language understanding and has gained in-
terest in the natural language processing (NLP)
community, due to its usefulness in a wide range
of applications. Many NLP systems, such as ma-
chine translation (Xie et al., 2011), entity coref-
erence resolution (Ng, 2010; Durrett and Klein,
2013; Ma et al., 2016), low-resource languages
processing (McDonald et al., 2013; Ma and Xia,
2014), and word sense disambiguation (Fauceglia
et al., 2015), are becoming more sophisticated, in
part because of utilizing syntactic knowledge such

as dependency parsing trees.
Dependency trees represent syntactic relation-

ships through labeled directed edges between
heads and their dependents (modifiers). In the
past few years, several dependency parsing algo-
rithms (Nivre and Scholz, 2004; McDonald et al.,
2005b; Koo and Collins, 2010; Ma and Zhao,
2012a,b) have been proposed, whose high perfor-
mance heavily rely on hand-crafted features and
task-specific resources that are costly to develop,
making dependency parsing models difficult to
adapt to new languages or new domains.

Recently, non-linear neural networks, such as
recurrent neural networks (RNNs) with long-short
term memory (LSTM) and convolution neural net-
works (CNNs), with as input distributed word
representations, also known as word embeddings,
have been broadly applied, with great success,
to NLP problems like part-of-speech (POS) tag-
ging (Collobert et al., 2011) and named entity
recognition (NER) (Chiu and Nichols, 2016).
By utilizing distributed representations as inputs,
these systems are capable of learning hidden in-
formation representations directly from data in-
stead of manually designing hand-crafted features,
yielding end-to-end models (Ma and Hovy, 2016).
Previous studies explored the applicability of neu-
ral representations to traditional graph-based pars-
ing models. Some work (Kiperwasser and Gold-
berg, 2016; Wang and Chang, 2016) replaced
the linear scoring function of each arc in tradi-
tional models with neural networks and used a
margin-based objective (McDonald et al., 2005a)
for model training. Other work (Zhang et al.,
2016; Dozat and Manning, 2016) formalized de-
pendency parsing as independently selecting the
head of each word with cross-entropy objective,
without the guarantee of a general non-projective
tree structure output. Moreover, there have yet
been no previous work on deriving a neural prob-

59



abilistic parsing model to define a proper condi-
tional distribution over non-projective trees for a
given sentence.

In this paper, we propose a probabilistic neu-
ral network-based model for non-projective de-
pendency parsing. This parsing model uses
bi-directional LSTM-CNNs (BLSTM-CNNs) as
backbone to learn neural information representa-
tions, on top of which a probabilistic structured
layer is constructed with a conditional log-linear
model, defining a conditional distribution over all
non-projective dependency trees. The architec-
ture of BLSTM-CNNs is similar to the one used
for sequence labeling tasks (Ma and Hovy, 2016),
where CNNs encode character-level information
of a word into its character-level representation
and BLSTM models context information of each
word. Due to the probabilistic structured out-
put layer, we can use negative log-likelihood as
the training objective, where the partition function
and marginals can be computed via Kirchhoff’s
Matrix-Tree Theorem (Tutte, 1984) to process the
optimization efficiently by back-propagation. At
test time, parsing trees can be decoded with the
maximum spanning tree (MST) algorithm (Mc-
Donald et al., 2005b). We evaluate our model
on 17 treebanks across 14 different languages,
achieving state-of-the-art performance on 9 tree-
banks. The contributions of this work are summa-
rized as: (i) proposing a neural probabilistic model
for non-projective dependency parsing. (ii) giving
empirical evaluations of this model on benchmark
data sets over 14 languages. (iii) achieving state-
of-the-art performance with this parser on nine dif-
ferent treebanks.

2 Neural Probabilistic Parsing Model

In this section, we describe the components (lay-
ers) of our neural parsing model. We introduce
the neural layers in our neural network one-by-one
from top to bottom.

2.1 Edge-Factored Parsing Layer

In this paper, we will use the following notation:
x = {x1, . . . , xn} represents a generic input sen-
tence, where xi is the ith word. y represents a
generic (possibly non-projective) dependency tree,
which represents syntactic relationships through
labeled directed edges between heads and their de-
pendents. For example, Figure 1 shows a depen-
dency tree for the sentence, “Economic news had

Economic news had little effect on financial marketsRoot

root

amod subj amod

dobj

amod

prep pobj

Figure 1: An example labeled dependency tree.

little effect on financial markets”, with the sen-
tences root-symbol as its root. T (x) is used to de-
note the set of possible dependency trees for sen-
tence x.

The probabilistic model for dependency pars-
ing defines a family of conditional probability
p(y|x; Θ) over all y given sentence x, with a log-
linear form:

P (y|x; Θ) =
exp

( ∑
(xh,xm)∈y

φ(xh, xm; Θ)

)
Z(x; Θ)

where Θ is the parameter of this model, shm =
φ(xh, xm; Θ) is the score function of edge from
xh to xm, and

Z(x; Θ) =
∑

y∈T (x)
exp

 ∑
(xh,xm)∈y

shm


is the partition function.

Bi-Linear Score Function. In our model, we
adopt a bi-linear form score function:

φ(xh, xm; Θ) = ϕ(xh)TWϕ(xm)
+UTϕ(xh) + VTϕ(xm) + b

where Θ = {W,U,V,b}, ϕ(xi) is the represen-
tation vector of xi, W,U,V denote the weight
matrix of the bi-linear term and the two weight
vectors of the linear terms in φ, and b denotes the
bias vector.

As discussed in Dozat and Manning (2016), the
bi-linear form of score function is related to the bi-
linear attention mechanism (Luong et al., 2015).
The bi-linear score function differs from the tra-
ditional score function proposed in Kiperwasser
and Goldberg (2016) by adding the bi-linear term.
A similar score function is proposed in Dozat and
Manning (2016). The difference between their and
our score function is that they only used the linear
term for head words (UTϕ(xh)) while use them
for both heads and modifiers.

60



Matrix-Tree Theorem. In order to train the
probabilistic parsing model, as discussed in Koo
et al. (2007), we have to compute the partition
function and the marginals, requiring summation
over the set T (x):

Z(x; Θ) =
∑

y∈T (x)

∏
(xh,xm)∈y

ψ(xh, xm; Θ)

µh,m(x; Θ) =
∑

y∈T (x):(xh,xm)∈y
P (y|x; Θ)

where ψ(xh, xm; Θ) is the potential function:

ψ(xh, xm; Θ) = exp (φ(xh, xm; Θ))

and µh,m(x; Θ) is the marginal for edge from hth
word to mth word for x.

Previous studies (Koo et al., 2007; Smith and
Smith, 2007) have presented how a variant of
Kirchhoff’s Matrix-Tree Theorem (Tutte, 1984)
can be used to evaluate the partition function and
marginals efficiently. In this section, we briefly re-
visit this method.

For a sentence x with n words, we denote x =
{x0, x1, . . . , xn}, where x0 is the root-symbol.
We define a complete graph G on n+ 1 nodes (in-
cluding the root-symbol x0), where each node cor-
responds to a word in x and each edge corresponds
to a dependency arc between two words. Then, we
assign non-negative weights to the edges of this
complete graph with n + 1 nodes, yielding the
weighted adjacency matrix A(Θ) ∈ Rn+1×n+1,
for h,m = 0, . . . , n:

Ah,m(Θ) = ψ(xh, xm; Θ)

Based on the adjacency matrix A(Θ), we have the
Laplacian matrix:

L(Θ) = D(Θ)−A(Θ)

where D(Θ) is the weighted degree matrix:

Dh,m(Θ) =


n∑

h′=0
Ah′,m(Θ) if h = m

0 otherwise

Then, according to Theorem 1 in Koo et al. (2007),
the partition function is equal to the minor of L(Θ)
w.r.t row 0 and column 0:

Z(x; Θ) = L(0,0)(Θ)

where for a matrix A, A(h,m) denotes the minor of
A w.r.t row h and column m; i.e., the determinant

of the submatrix formed by deleting the hth row
and mth column.

The marginals can be computed by calculating
the matrix inversion of the matrix corresponding
to L(0,0)(Θ). The time complexity of computing
the partition function and marginals is O(n3).

Labeled Parsing Model. Though it is originally
designed for unlabeled parsing, our probabilistic
parsing model is easily extended to include depen-
dency labels.

In labeled dependency trees, each edge is rep-
resented by a tuple (xh, xm, l), where xh and xm
are the head word and modifier, respectively, and l
is the label of dependency type of this edge. Then
we can extend the original model for labeled de-
pendency parsing by extending the score function
to include dependency labels:

φ(xh, xm, l; Θ) = ϕ(xh)TWlϕ(xm)
+UTl ϕ(xh) + V

T
l ϕ(xm)

+bl

where Wl,Ul,Vl,bl are the weights and bias
corresponding to dependency label l. Suppose that
there are L different dependency labels, it suffices
to define the new adjacency matrix by assigning
the weight of a edge with the sum of weights over
different dependency labels:

A′h,m(Θ) =
L∑

l=1

ψ(xh, xm, l; Θ)

The partition function and marginals over labeled
dependency trees are obtained by operating on the
new adjacency matrix A′(Θ). The time complex-
ity becomes O(n3 + Ln2). In practice, L is prob-
ably large. For English, the number of edge la-
bels in Stanford Basic Dependencies (De Marn-
effe et al., 2006) is 45, and the number in the tree-
bank of CoNLL-2008 shared task (Surdeanu et al.,
2008) is 70. While, the average length of sen-
tences in English Penn Treebank (Marcus et al.,
1993) is around 23. Thus, L is not negligible com-
paring to n.

It should be noticed that in our labeled model,
for different dependency label l we use the same
vector representation ϕ(xi) for each word xi. The
dependency labels are distinguished (only) by the
parameters (weights and bias) corresponding to
each of them. One advantage of this is that it sig-
nificantly reduces the memory requirement com-
paring to the model in Dozat and Manning (2016)
which distinguishes ϕl(xi) for different label l.

61



P l a y i n g PaddingPadding

Char 

Embedding

Convolution

Max Pooling

Char 

Representation

Figure 2: The convolution neural network for ex-
tracting character-level representations of words.
Dashed arrows indicate a dropout layer applied be-
fore character embeddings are input to CNN.

Maximum Spanning Tree Decoding. The de-
coding problem of this parsing model can be for-
mulated as:

y∗ = argmax
y∈T (x)

P (y|x; Θ)
= argmax

y∈T (x)

∑
(xh,xm)∈y

φ(xh, xm; Θ)

which can be solved by using the Maximum Span-
ning Tree (MST) algorithm described in McDon-
ald et al. (2005b).

2.2 Neural Network for Representation
Learning

Now, the remaining question is how to obtain the
vector representation of each word with a neural
network. In the following subsections, we will
describe the architecture of our neural network
model for representation learning.

2.2.1 CNNs
Previous work (Santos and Zadrozny, 2014) have
shown that CNNs are an effective approach to ex-
tract morphological information (like the prefix or
suffix of a word) from characters of words and en-
code it into neural representations, which has been
proven particularly useful on Out-of-Vocabulary
words (OOV). The CNN architecture our model
uses to extract character-level representation of a
given word is the same as the one used in Ma
and Hovy (2016). The CNN architecture is shown
in Figure 2. Following Ma and Hovy (2016), a
dropout layer (Srivastava et al., 2014) is applied
before character embeddings are input to CNN.

2.2.2 Bi-directional LSTM
LSTM Unit. Recurrent neural networks (RNNs)
are a powerful family of connectionist models that
have been widely applied in NLP tasks, such as
language modeling (Mikolov et al., 2010), se-
quence labeling (Ma and Hovy, 2016) and ma-
chine translation (Cho et al., 2014), to capture con-
text information in languages. Though, in theory,
RNNs are able to learn long-distance dependen-
cies, in practice, they fail due to the gradient van-
ishing/exploding problems (Bengio et al., 1994;
Pascanu et al., 2013).

LSTMs (Hochreiter and Schmidhuber, 1997)
are variants of RNNs designed to cope with these
gradient vanishing problems. Basically, a LSTM
unit is composed of three multiplicative gates
which control the proportions of information to
pass and to forget on to the next time step.

BLSTM. Many linguistic structure prediction
tasks can benefit from having access to both
past (left) and future (right) contexts, while the
LSTM’s hidden state ht takes information only
from past, knowing nothing about the future.
An elegant solution whose effectiveness has been
proven by previous work (Dyer et al., 2015;
Ma and Hovy, 2016) is bi-directional LSTM
(BLSTM). The basic idea is to present each se-
quence forwards and backwards to two separate
hidden states to capture past and future informa-
tion, respectively. Then the two hidden states are
concatenated to form the final output. As dis-
cussed in Dozat and Manning (2016), there are
more than one advantages to apply a multilayer
perceptron (MLP) to the output vectors of BLSTM
before the score function, eg. reducing the dimen-
sionality and overfitting of the model. We follow
this work by using a one-layer perceptron with
elu (Clevert et al., 2015) as activation function.

2.3 BLSTM-CNNs

Finally, we construct our neural network model by
feeding the output vectors of BLSTM (after MLP)
into the parsing layer. Figure 3 illustrates the ar-
chitecture of our network in detail.

For each word, the CNN in Figure 2, with char-
acter embeddings as inputs, encodes the character-
level representation. Then the character-level rep-
resentation vector is concatenated with the word
embedding vector to feed into the BLSTM net-
work. To enrich word-level information, we also
use POS embeddings. Finally, the output vec-

62



We

Word

Embedding

are playing soccer

Char

Representation

Forward

LSTM

Backward

LSTM

LSTM LSTM LSTM LSTM

LSTM LSTM LSTM LSTM

We are soccerplayingParsing

Layer

subj

aux dobj

POS

Embedding

Figure 3: The main architecture of our parsing
model. The character representation for each word
is computed by the CNN in Figure 2. Then
the character representation vector is concatenated
with the word and pos embedding before feeding
into the BLSTM network. Dashed arrows indi-
cate dropout layers applied on the input, hidden
and output vectors of BLSTM.

tors of the neural netwok are fed to the parsing
layer to jointly parse the best (labeled) dependency
tree. As shown in Figure 3, dropout layers are ap-
plied on the input, hidden and output vectors of
BLSTM, using the form of recurrent dropout pro-
posed in Gal and Ghahramani (2016).

3 Network Training

In this section, we provide details about imple-
menting and training the neural parsing model, in-
cluding parameter initialization, model optimiza-
tion and hyper parameter selection.

3.1 Parameter Initialization

Word Embeddings. For all the parsing mod-
els on different languages, we initialize word vec-
tors with pretrained word embeddings. For Chi-

Layer Hyper-parameter Value

CNN
window size 3
number of filters 50

LSTM

number of layers 2
state size 256
initial state 0.0
peepholes Hadamard

MLP
number of layers 1
dimension 100

Dropout
embeddings 0.15
LSTM hidden states 0.25
LSTM layers 0.33

Learning

optimizer Adam
initial learning rate 0.002
decay rate 0.5
gradient clipping 5.0

Table 1: Hyper-parameters for all experiments.

nese, Dutch, English, German and Spanish, we use
the structured-skipgram (Ling et al., 2015) embed-
dings, and for other languages we use the Poly-
glot (Al-Rfou et al., 2013) embeddings. The di-
mensions of embeddings are 100 for English, 50
for Chinese and 64 for other languages.

Character Embeddings. Following Ma and
Hovy (2016), character embeddings are initialized

with uniform samples from [−
√

3
dim ,+

√
3

dim ],
where we set dim = 50.

POS Embedding. Our model also includes POS
embeddings. The same as character embeddings,
POS embeddings are also 50-dimensional, initial-

ized uniformly from [−
√

3
dim ,+

√
3

dim ].

Weights Matrices and Bias Vectors. Matrix
parameters are randomly initialized with uniform

samples from [−
√

6
r+c ,+

√
6

r+c ], where r and c
are the number of of rows and columns in the
structure (Glorot and Bengio, 2010). Bias vec-
tors are initialized to zero, except the bias bf for
the forget gate in LSTM , which is initialized to
1.0 (Jozefowicz et al., 2015).

3.2 Optimization Algorithm
Parameter optimization is performed with the
Adam optimizer (Kingma and Ba, 2014) with
β1 = β2 = 0.9. We choose an initial learn-
ing rate of η0 = 0.002. The learning rate η was
adapted using a schedule S = [e1, e2, . . . , es],
in which the learning rate η is annealed by

63



English Chinese German
Dev Test Dev Test Dev Test

Model UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
Basic 94.51 92.23 94.62 92.54 84.33 81.65 84.35 81.63 90.46 87.77 90.69 88.42
+Char 94.74 92.55 94.73 92.75 85.07 82.63 85.24 82.46 92.16 89.82 92.24 90.18
+POS 94.71 92.60 94.83 92.96 88.98 87.55 89.05 87.74 91.94 89.51 92.19 90.05
Full 94.77 92.66 94.88 92.98 88.51 87.16 88.79 87.47 92.37 90.09 92.58 90.54

Table 2: Parsing performance (UAS and LAS) of different versions of our model on both the development
and test sets for three languages.

multiplying a fixed decay rate ρ = 0.5 after
ei ∈ S epochs respectively. We used S =
[10, 30, 50, 70, 100] and trained all networks for
a total of 120 epochs. While the Adam opti-
mizer automatically adjusts the global learning
rate according to past gradient magnitudes, we
find that this additional decay consistently im-
proves model performance across all settings and
languages. To reduce the effects of “gradient ex-
ploding”, we use a gradient clipping of 5.0 (Pas-
canu et al., 2013). We explored other optimiza-
tion algorithms such as stochastic gradient de-
scent (SGD) with momentum, AdaDelta (Zeiler,
2012), or RMSProp (Dauphin et al., 2015), but
none of them meaningfully improve upon Adam
with learning rate annealing in our preliminary ex-
periments.

Dropout Training. To mitigate overfitting, we
apply the dropout method (Srivastava et al., 2014;
Ma et al., 2017) to regularize our model. As shown
in Figure 2 and 3, we apply dropout on character
embeddings before inputting to CNN, and on the
input, hidden and output vectors of BLSTM. We
apply dropout rate of 0.15 to all the embeddings.
For BLSTM, we use the recurrent dropout (Gal
and Ghahramani, 2016) with 0.25 dropout rate
between hidden states and 0.33 between layers.
We found that the model using the new recur-
rent dropout converged much faster than standard
dropout, while achiving similar performance.

3.3 Hyper-Parameter Selection

Table 1 summarizes the chosen hyper-parameters
for all experiments. We tune the hyper-parameters
on the development sets by random search. We
use the same hyper-parameters across the models
on different treebanks and languages, due to time
constrains. Note that we use 2-layer BLSTM fol-
lowed with 1-layer MLP. We set the state size of
LSTM to 256 and the dimension of MLP to 100.
Tuning these two parameters did not significantly
impact the performance of our model.

Dev Test
UAS LAS UAS LAS

cross-entropy 94.10 91.52 93.77 91.57
global-likelihood 94.77 92.66 94.88 92.98

Table 3: Parsing performance on PTB with differ-
ent training objective functions.

4 Experiments

4.1 Setup

We evaluate our neural probabilistic parser on
the same data setup as Kuncoro et al. (2016),
namely the English Penn Treebank (PTB version
3.0) (Marcus et al., 1993), the Penn Chinese Tree-
bank (CTB version 5.1) (Xue et al., 2002), and the
German CoNLL 2009 corpus (Hajič et al., 2009).
Following previous work, all experiments are eval-
uated on the metrics of unlabeled attachment score
(UAS) and Labeled attachment score (LAS).

4.2 Main Results

We first construct experiments to dissect the effec-
tiveness of each input information (embeddings)
of our neural network architecture by ablation
studies. We compare the performance of four ver-
sions of our model with different inputs — Ba-
sic, +POS, +Char and Full — where the Ba-
sic model utilizes only the pretrained word em-
beddings as inputs, while the +POS and +Char
models augments the basic one with POS embed-
ding and character information, respectively. Ac-
cording to the results shown in Table 2, +Char
model obtains better performance than the Basic
model on all the three languages, showing that
character-level representations are important for
dependency parsing. Second, on English and Ger-
man, +Char and +POS achieves comparable per-
formance, while on Chinese +POS significantly
outperforms +Char model. Finally, the Full model
achieves the best accuracy on English and Ger-
man, but on Chinese +POS obtains the best. Thus,
we guess that the POS information is more useful

64



English Chinese German
System UAS LAS UAS LAS UAS LAS
Bohnet and Nivre (2012) – – 87.3 85.9 91.4 89.4
Chen and Manning (2014) 91.8 89.6 83.9 82.4 – –
Ballesteros et al. (2015) 91.6 89.4 85.3 83.7 88.8 86.1
Dyer et al. (2015) 93.1 90.9 87.2 85.7 – –
Kiperwasser and Goldberg (2016): graph 93.1 91.0 86.6 85.1 – –
Ballesteros et al. (2016) 93.6 91.4 87.7 86.2 – –
Wang and Chang (2016) 94.1 91.8 87.6 86.2 – –
Zhang et al. (2016) 94.1 91.9 87.8 86.2 – –
Cheng et al. (2016) 94.1 91.5 88.1 85.7 – –
Andor et al. (2016) 94.6 92.8 – – 90.9 89.2
Kuncoro et al. (2016) 94.3 92.1 88.9 87.3 91.6 89.2
Dozat and Manning (2016) 95.7 94.1 89.3 88.2 93.5 91.4
This work: Basic 94.6 92.5 84.4 81.6 90.7 88.4
This work: +Char 94.7 92.8 85.2 82.5 92.2 90.2
This work: +POS 94.8 93.0 89.1 87.7 92.2 90.1
This work: Full 94.9 93.0 88.8 87.5 92.6 90.5

Table 4: UAS and LAS of four versions of our model on test sets for three languages, together with
top-performance parsing systems.

for Chinese than English and German.
Table 3 gives the performance on PTB of the

parsers trained with two different objective func-
tions — the cross-entropy objective of each word,
and our objective based on likelihood for an en-
tire tree. The parser with global likelihood ob-
jective outperforms the one with simple cross-
entropy objective, demonstrating the effectiveness
of the global structured objective.

4.3 Comparison with Previous Work

Table 4 illustrates the results of the four versions
of our model on the three languages, together
with twelve previous top-performance systems for
comparison. Our Full model significantly outper-
forms the graph-based parser proposed in Kiper-
wasser and Goldberg (2016) which used simi-
lar neural network architecture for representation
learning (detailed discussion in Section 5). More-
over, our model achieves better results than the
parser distillation method (Kuncoro et al., 2016)
on all the three languages. The results of our
parser are slightly worse than the scores reported
in Dozat and Manning (2016). One possible rea-
son is that, as mentioned in Section 2.1, for labeled
dependency parsing Dozat and Manning (2016)
used different vectors for different dependency la-
bels to represent each word, making their model
require much more memory than ours.

4.4 Experiments on CoNLL Treebanks

Datasets. To make a thorough empirical com-
parison with previous studies, we also evaluate our
system on treebanks from CoNLL shared task on
dependency parsing — the English treebank from
CoNLL-2008 shared task (Surdeanu et al., 2008)
and all 13 treebanks from CoNLL-2006 shared
task (Buchholz and Marsi, 2006). For the tree-
banks from CoNLL-2006 shared task, following
Cheng et al. (2016), we randomly select 5% of
the training data as the development set. UAS
and LAS are evaluated using the official scorer1

of CoNLL-2006 shared task.

Baselines. We compare our model with the
third-order Turbo parser (Martins et al., 2013), the
low-rank tensor based model (Tensor) (Lei et al.,
2014), the randomized greedy inference based
(RGB) model (Zhang et al., 2014), the labeled
dependency parser with inner-to-outer greedy de-
coding algorithm (In-Out) (Ma and Hovy, 2015),
and the bi-direction attention based parser (Bi-
Att) (Cheng et al., 2016). We also compare our
parser against the best published results for indi-
vidual languages. This comparison includes four
additional systems: Koo et al. (2010), Martins
et al. (2011), Zhang and McDonald (2014) and
Pitler and McDonald (2015).

1http://ilk.uvt.nl/conll/software.html

65



Turbo Tensor RGB In-Out Bi-Att +POS Full Best Published
UAS UAS UAS UAS [LAS] UAS [LAS] UAS [LAS] UAS [LAS] UAS LAS

ar 79.64 79.95 80.24 79.60 [67.09] 80.34 [68.58] 80.05 [67.80] 80.80 [69.40] 81.12 –
bg 93.10 93.50 93.72 92.68 [87.79] 93.96 [89.55] 93.66 [89.79] 94.28 [90.60] 94.02 –
zh 89.98 92.68 93.04 92.58 [88.51] – 93.44 [90.04] 93.40 [90.10] 93.04 –
cs 90.32 90.50 90.77 88.01 [79.31] 91.16 [85.14] 91.04 [85.82] 91.18 [85.92] 91.16 85.14
da 91.48 91.39 91.86 91.44 [85.55] 91.56 [85.53] 91.52 [86.57] 91.86 [87.07] 92.00 –
nl 86.19 86.41 87.39 84.45 [80.31] 87.15 [82.41] 87.41 [84.17] 87.85 [84.82] 87.39 –
en 93.22 93.02 93.25 92.45 [89.43] – 94.43 [92.31] 94.66 [92.52] 93.25 –
de 92.41 91.97 92.67 90.79 [87.74] 92.71 [89.80] 93.53 [91.55] 93.62 [91.90] 92.71 89.80
ja 93.52 93.71 93.56 93.54 [91.80] 93.44 [90.67] 93.82 [92.34] 94.02 [92.60] 93.80 –
pt 92.69 91.92 92.36 91.54 [87.68] 92.77 [88.44] 92.59 [89.12] 92.71 [88.92] 93.03 –
sl 86.01 86.24 86.72 84.39 [73.74] 86.01 [75.90] 85.73 [76.48] 86.73 [77.56] 87.06 –
es 85.59 88.00 88.75 86.44 [83.29] 88.74 [84.03] 88.58 [85.03] 89.20 [85.77] 88.75 84.03
sv 91.14 91.00 91.08 89.94 [83.09] 90.50 [84.05] 90.89 [86.58] 91.22 [86.92] 91.85 85.26
tr 76.90 76.84 76.68 75.32 [60.39] 78.43 [66.16] 75.88 [61.72] 77.71 [65.81] 78.43 66.16
av 88.73 89.08 89.44 88.08 [81.84] – 89.47 [84.24] 89.95 [84.99] 89.83 –

Table 5: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art
parsers. “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010),
Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald
(2014), Pitler and McDonald (2015), Ma and Hovy (2015), and Cheng et al. (2016).

Results. Table 5 summarizes the results of our
model, along with the state-of-the-art baselines.
On average across 14 languages, our approach sig-
nificantly outperforms all the baseline systems. It
should be noted that the average UAS of our parser
over the 14 languages is better than that of the
“best published”, which are from different systems
that achieved best results for different languages.

For individual languages, our parser achieves
state-of-the-art performance on both UAS and
LAS on 8 languages — Bulgarian, Chinese,
Czech, Dutch, English, German, Japanese and
Spanish. On Arabic, Danish, Portuguese, Slovene
and Swedish, our parser obtains the best LAS. An-
other interesting observation is that the Full model
outperforms the +POS model on 13 languages.
The only exception is Chinese, which matches the
observation in Section 4.2.

5 Related Work

In recent years, several different neural network
based models have been proposed and success-
fully applied to dependency parsing. Among
these neural models, there are three approaches
most similar to our model — the two graph-
based parsers with BLSTM feature representa-
tion (Kiperwasser and Goldberg, 2016; Wang and
Chang, 2016), and the neural bi-affine attention
parser (Dozat and Manning, 2016).

Kiperwasser and Goldberg (2016) proposed
a graph-based dependency parser which uses
BLSTM for word-level representations. Wang and
Chang (2016) used a similar model with a way

to learn sentence segment embedding based on
an extra forward LSTM network. Both of these
two parsers trained the parsing models by opti-
mizing margin-based objectives. There are three
main differences between their models and ours.
First, they only used linear form score function,
instead of using the bi-linear term between the
vectors of heads and modifiers. Second, They
did not employ CNNs to model character-level
information. Third, we proposed a probabilistic
model over non-projective trees on the top of neu-
ral representations, while they trained their models
with a margin-based objective. Dozat and Man-
ning (2016) proposed neural parsing model us-
ing bi-affine score function, which is similar to
the bi-linear form score function in our model.
Our model mainly differ from this model by using
CNN to model character-level information. More-
over, their model formalized dependency parsing
as independently selecting the head of each word
with cross-entropy objective, while our probabilis-
tic parsing model jointly encodes and decodes
parsing trees for given sentences.

6 Conclusion

In this paper, we proposed a neural probabilistic
model for non-projective dependency parsing, us-
ing the BLSTM-CNNs architecture for represen-
tation learning. Experimental results on 17 tree-
banks across 14 languages show that our parser
significantly improves the accuracy of both depen-
dency structures (UAS) and edge labels (LAS),
over several previously state-of-the-art systems.

66



Acknowledgements

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of DARPA.

References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.

2013. Polyglot: Distributed word representations
for multilingual nlp. In Proceedings of CoNLL-
2013. Sofia, Bulgaria, pages 183–192.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 2442–2452.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of EMNLP-2015. Lisbon, Portugal, pages
349–359.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A. Smith. 2016. Training with exploration im-
proves a greedy stack lstm parser. In Proceedings of
EMNLP-2016. Austin, Texas, pages 2005–2010.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on 5(2):157–166.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In Pro-
ceedings of EMNLP-2012. Jeju Island, Korea, pages
1455–1465.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceeding of CoNLL-2006. New York, NY, pages
149–164.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of EMNLP-2014. Doha,
Qatar, pages 740–750.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao,
and Li Deng. 2016. Bi-directional attention with
agreement for dependency parsing. In Proceedings
of EMNLP-2016. Austin, Texas, pages 2204–2214.

Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics 4:357–370.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .

Djork-Arné Clevert, Thomas Unterthiner, and Sepp
Hochreiter. 2015. Fast and accurate deep network
learning by exponential linear units (elus). arXiv
preprint arXiv:1511.07289 .

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research
12:2493–2537.

Yann N Dauphin, Harm de Vries, Junyoung Chung,
and Yoshua Bengio. 2015. Rmsprop and equili-
brated adaptive learning rates for non-convex opti-
mization. arXiv preprint arXiv:1502.04390 .

Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D. Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC-2006. pages 449–
454.

Timothy Dozat and Christopher D. Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. arXiv preprint arXiv:1611.01734 .

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of EMNLP-2013. Seattle, Washington, USA,
pages 1971–1982.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of ACL-2015 (Volume
1: Long Papers). Beijing, China, pages 334–343.

Nicolas R Fauceglia, Yiu-Chang Lin, Xuezhe Ma, and
Eduard Hovy. 2015. Word sense disambiguation via
propstore and ontonotes for event mention detec-
tion. In Proceedings of the The 3rd Workshop on
EVENTS: Definition, Detection, Coreference, and
Representation. Denver, Colorado, pages 11–15.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International conference on artificial
intelligence and statistics. pages 249–256.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of CoNLL-
2009: Shared Task. pages 1–18.

67



Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of the
32nd International Conference on Machine Learn-
ing (ICML-15). pages 2342–2350.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics
4:313–327.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL-
2010. Uppsala, Sweden, pages 1–11.

Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of
EMNLP-2007. Prague, Czech Republic, pages 141–
150.

Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP-2010. Cam-
bridge, MA, pages 1288–1298.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
into one mst parser. In Proceedings of EMNLP-
2016. Austin, Texas, pages 1744–1753.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of ACL-
2014 (Volume 1: Long Papers). Baltimore, Mary-
land, pages 1381–1391.

Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings of
NAACL-2015. Denver, Colorado, pages 1299–1304.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
EMNLP-2015. Lisbon, Portugal, pages 1412–1421.

Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu,
Yuntian Deng, and Eduard Hovy. 2017. Dropout
with expectation-linear regularization. In Proceed-
ings of the 5th International Conference on Learn-
ing Representations (ICLR-2017). Toulon, France.

Xuezhe Ma and Eduard Hovy. 2015. Efficient inner-to-
outer greedy algorithm for higher-order labeled de-
pendency parsing. In Proceedings of EMNLP-2015.
Lisbon, Portugal, pages 1322–1328.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 1064–1074.

Xuezhe Ma, Zhengzhong Liu, and Eduard Hovy. 2016.
Unsupervised ranking model for entity coreference
resolution. In Proceedings of NAACL-2016. San
Diego, California, USA.

Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
allel guidance and entropy regularization. In Pro-
ceedings of ACL-2014. Baltimore, Maryland, pages
1337–1348.

Xuezhe Ma and Hai Zhao. 2012a. Fourth-order depen-
dency parsing. In Proceedings of COLING 2012:
Posters. Mumbai, India, pages 785–796.

Xuezhe Ma and Hai Zhao. 2012b. Probabilistic models
for high-order projective dependency parsing. Tech-
nical Report, arXiv:1502.04174 .

Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics 19(2):313–330.

Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of ACL-
2013 (Volume 2: Short Papers). Sofia, Bulgaria,
pages 617–622.

Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Dual decomposition with
many overlapping components. In Proceedings
of EMNLP-2011. Edinburgh, Scotland, UK., pages
238–249.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005.
Ann Arbor, Michigan, USA, pages 91–98.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar Täckström, Claudia Bedini, Núria
Bertomeu Castelló, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of ACL-2013. Sofia, Bulgaria,
pages 92–97.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP-2005. Vancouver, Canada, pages
523–530.

68



Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech. volume 2, page 3.

Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
ACL-2010. Association for Computational Linguis-
tics, Uppsala, Sweden, pages 1396–1411.

Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004. Geneva, Switzerland, pages 64–
70.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neu-
ral networks. In Proceedings of ICML-2013. pages
1310–1318.

Emily Pitler and Ryan McDonald. 2015. A linear-time
transition system for crossing interval trees. In Pro-
ceedings of NAACL-2015. Denver, Colorado, pages
662–671.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
tagging. In Proceedings of ICML-2014. pages
1818–1826.

David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of EMNLP-2007. Prague, Czech Re-
public, pages 132–140.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research 15(1):1929–1958.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluı́s Màrquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of CoNLL-
2008. pages 159–177.

William Thomas Tutte. 1984. Graph theory, vol-
ume 11. Addison-Wesley Menlo Park.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Pro-
ceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 2306–2315.

Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of EMNLP-2011. Edin-
burgh, Scotland, UK., pages 216–226.

Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of COLING-2002. pages 1–8.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .

Hao Zhang and Ryan McDonald. 2014. Enforcing
structural diversity in cube-pruned dependency pars-
ing. In Proceedings of ACL-2014 (Volume 2: Short
Papers). Baltimore, Maryland, pages 656–661.

Xingxing Zhang, Jianpeng Cheng, and Mirella Lapata.
2016. Dependency parsing as head selection. arXiv
preprint arXiv:1606.01280 .

Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2014. Greed is good if randomized: New
inference for dependency parsing. In Proceedings of
EMNLP-2014. Doha, Qatar, pages 1013–1024.

69


