



















































Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2032–2043
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Reasoning with Heterogeneous Knowledge
for Commonsense Machine Comprehension

Hongyu Lin1,2 Le Sun1 Xianpei Han1
1State Key Laboratory of Computer Science

Institute of Software, Chinese Academy of Sciences, Beijing, China
2University of Chinese Academy of Sciences, Beijing, China
{hongyu2016,sunle,xianpei}@iscas.ac.cn

Abstract

Reasoning with commonsense knowl-
edge is critical for natural language un-
derstanding. Traditional methods for
commonsense machine comprehension
mostly only focus on one specific kind
of knowledge, neglecting the fact that
commonsense reasoning requires simul-
taneously considering different kinds of
commonsense knowledge. In this paper,
we propose a multi-knowledge reasoning
method, which can exploit heterogeneous
knowledge for commonsense machine
comprehension. Specifically, we first mine
different kinds of knowledge (including
event narrative knowledge, entity semantic
knowledge and sentiment coherent knowl-
edge) and encode them as inference rules
with costs. Then we propose a multi-
knowledge reasoning model, which select-
s inference rules for a specific reasoning
context using attention mechanism, and
reasons by summarizing all valid infer-
ence rules. Experiments on RocStories
show that our method outperforms tradi-
tional models significantly.

1 Introduction

Commonsense knowledge is fundamental in ar-
tificial intelligence, and has long been a key
component in natural language understanding and
human-like reasoning. For example, to understand
the relation between sentences “Mary walked to
a restaurant” and “She ordered some foods”, we
need commonsense knowledge such as “Mary is
a girl”, “restaurant sells food”, etc. The task
of understanding natural language with common-
sense knowledge is usually referred as common-
sense machine comprehension, which has been a

hot topic in recent years (Richardson et al., 2013;
Weston et al., 2015; Zhang et al., 2016).

Recently, RocStories (Mostafazadeh et al.,
2016a), a commonsense machine comprehension
task, has attached many researchers’ attention
due to its significant difference from previous
machine comprehension tasks. RocStories focuses
on reasoning with implicit commonsense knowl-
edge, rather than matching with explicit infor-
mation in given contexts. In this task, a sys-
tem requires choosing a sentence, namely hy-
pothesis, to complete a given commonsense sto-
ry, called as premise document. Table 1 shows
two examples. RocStories proposes a challeng-
ing benchmark task for evaluating commonsense-
based language understanding. As investigated
by Mostafazadeh et al.(2016a), this dataset does
not have any boundary cases and thus results in
100% human performance.

Commonsense machine comprehension, how-
ever, is an natural ability for human but could
be very challenging for computers. In gener-
al, any world knowledge whatsoever in the read-
er’s mind can affect the choice of an interpreta-
tion (Dahlgren et al., 1989). That is, a person can
learn any heterogeneous commonsense knowledge
and make inference of given information based on
all knowledge in his mind. For example, to choose
the right hypothesis for the first premise document
in Table 1, we needs the event narrative knowl-
edge that “X does a thorough job” will lead to “c-
ommends X”, rather than “fire X”. Besides, peo-
ple can further confirm their judgement based on
the sentimental coherence between “finish super
early” and “job well done”. Furthermore, in the
second example, even both hypothesises are con-
sistent with the premise document in both event
and sentimental facets, we can still infer the right
answer easily using the commonsense knowledge
that “puppy” is a dog, meanwhile “kitten” is a cat.

2032



Premise Document Right Hypothesis Wrong Hypothesis
Ron started his new job as a landscaper today.
He loves the outdoors and has always enjoyed working in it.
His boss tells him to re-sod the front yard of the mayor’s home.
Ron is ecstatic, but does a thorough job and finishes super early.

His boss commends him
for a job well done.

Ron is immediately fired
for insubordination.

One day, my sister came over to the house to show us her puppy.
She told us that she had just gotten the puppy across the street.
My sons begged me to get them one.
I told them that if they would care for it, they could have it.

My son said they would,
so we got a dog.

We then grabbed a small
kitten.

Table 1: Examples of RocStories Dataset.

In recent years, many methods have been pro-
posed for commonsense machine comprehension.
However, these methods mostly either focus on
matching explicit information in given texts (We-
ston et al., 2014; Wang and Jiang, 2016a,b; Wang
et al., 2016b; Zhao et al., 2017), or paid atten-
tion to one specific kind of commonsense knowl-
edge, such as event temporal relation (Chamber-
s and Jurafsky, 2008; Modi and Titov, 2014; Pi-
chotta and Mooney, 2016b; Hu et al., 2017) and
event causality (Do et al., 2011; Radinsky et al.,
2012; Hashimoto et al., 2015; Gui et al., 2016). As
discussed above, it is obvious that commonsense
machine comprehension problem is far from set-
tled by considering only explicit or a single kind
of commonsense knowledge. To achieve human-
like comprehension and reasoning, there exist two
main challenges:

1) How to mine and represent different
kinds of implicit knowledge that commonsense
machine comprehension needs. For example, to
complete the first example in Table 1, we need a
system equipped with the event narrative knowl-
edge that “commends X” can be inferred from “X
does a thorough job”, as well as the sentiment
coherent knowledge that “insubordination” and
“finish super early” are sentimental incoherent.

2) How to reason with various kinds of
commonsense knowledge. As shown above,
knowledge that reasoning process needs varies
for different contexts. For human-like common-
sense machine comprehension, a system should
take various kinds of knowledge into considera-
tion, decide what knowledge will be utilized in a
specific reasoning contexts, and make the final de-
cision by taking all utilized knowledge into con-
sideration.

To address the above problems, this paper pro-
poses a new commonsense reasoning approach,
which can mine and exploit heterogeneous knowl-
edge for commonsense machine comprehension.
Specifically, we first mine different kinds of
knowledge from raw text and relevant knowl-

edge base, including event narrative knowledge,
entity semantic knowledge and sentiment coherent
knowledge. These heterogeneous knowledge are
encoded into a uniform representation – inference
rules between elements under different kinds of re-
lations, with an inference cost for each rule. Then
we design a rule selection model using attention
mechanism, modeling which inference rules will
be applied in a specific reasoning context. Final-
ly, we propose a multi-knowledge reasoning mod-
el, which measures the reasoning distance from a
premise document to a hypothesis as the expect-
ed cost sum of all inference rules applied in the
reasoning process.

By modeling and exploiting heterogeneous
knowledge during commonsense reasoning, our
method can achieve more accurate and more ro-
bust performance than traditional methods. Fur-
thermore, our method is a general framework,
which can be extended to incorporate new knowl-
edge easily. Experiments show that our method
achieves a 13.7% accuracy improvement on the s-
tandard RocStories dataset, a significant improve-
ment over previous work.

2 Commonsense Knowledge Acquisition
for Machine Comprehension

As described above, various knowledge can be ex-
ploited for machine comprehension. In this sec-
tion, we describe how to mine different knowl-
edge from different sources. Specifically, we
mine three types of commonly used commonsense
knowledge, including: 1)Event narrative knowl-
edge, which captures temporal and causal relation-
s between events; 2)Entity semantic knowledge,
which captures semantic relations between enti-
ties; 3)Sentiment coherent knowledge, which cap-
tures sentimental coherence between elements.

In this paper, we represent commonsense
knowledge as a set of inference rules given in the

form of X
f−→ Y : s, which means that element

Y can be inferred from element X under relation
f , with an inference cost s. An element can stand

2033



Antecedent Consequent Relation Cost
1© Mary she coreference 0.0
2© restaurant order narrative 0.1
3© restaurant food associative 0.1
4© restaurant food narrative 0.3
5© Mary order narrative 0.5
6© walk sleep narrative 0.8
7© walk food narrative 0.9

Table 2: Examples of Inference Rules.

for either event, entity or sentiment, and this pa-
per represents elements using lemmatized nouns,
verbs and adjectives. The lexical element repre-
sentation can also be easily extended to structural
representation, like the one in (Chambers and Ju-
rafsky, 2008), if needed. However, in auxiliary ex-
periments we found that using structural elements
results in severe sparseness and noises which in
turn will hurt the reasoning performance. There-
fore, we think an individual work is needed to
solve it. Table 2 demonstrates several examples
of inference rules. In following, we describe how
to mine different types of inference rules.

2.1 Mining Event Narrative Knowledge

Event narrative knowledge captures structured
temporal and casual knowledge about stereotyp-
ical event sequences, which is fundamental for
commonsense machine comprehension. For ex-
ample, we can infer “X ordered some foods” from
“X walked to a restaurant” using event narrative
knowledge. Previous work (Chambers and Juraf-
sky, 2008; Rudinger et al., 2015) proves that event
narrative knowledge can be mined from raw texts
unsupervisedly. So we propose two models to en-
code this knowledge using inference rules.

The first one is based on ordered PMI, which
is also proposed by Rudinger et al. (2015). Given
two element e1 and e2, this model calculates the
cost of inference rule e1

narrative−−−−−−→ e2 as:
cost(e1 −→ e2) = −log C(e1, e2)

C(e1, ∗), C(∗, e2) (1)

HereC(e1, e2) is the order sensitive count that ele-
ment e1 occurs before element e2 in different sen-
tences of the same document.

The second model is a variant of the skip-gram
model (Mikolov et al., 2013). The goal of this
model is to find element representations which
can accurately predict relevant elements in sen-
tences afterwards. Formally, given n asymmetric
pairs of elements (e11, e

1
2), (e

2
1, e

2
2), ...., (e

n
1 , e

n
2 ) i-

dentified from training data, the objective of our
model is to maximize the average log proba-

bility 1n
∑n

i=1 logP (e
i
2|ei1). And the probability

P (e2|e1) is defined using the softmax function:
P (e2|e1) ∝ exp(v′e2

T
ve1) (2)

where ve and v′e are “antecedent” and “conse-
quent” vector representation of element e, re-
spectively. We use the negative inner prod-
uct −v′e2Tve1 as the cost of inference rule
e1

skip−gram−−−−−−−→ e2.

2.2 Mining Entity Semantic Knowledge

Entities, often serving as event participants or en-
vironment variables, are important components of
commonsense stories. Intuitively, an entity in hy-
pothesis is reasonable if we can identify seman-
tic relations between it and some parts of premise
document. For example, if a premise document
contains “Starbucks”, then “coffeehouse” and “lat-
te” will be reasonable entities in hypothesis since
“Starbucks” is a possible coreference of “coffee-
house” and it is semantically related to “latte”.

Specifically, we identify mainly two kinds of
semantic relations between entities for common-
sense machine comprehension:

1) Coreference relation, which indicates that
two elements refer to the same entity in environ-
ment. In stories, besides to pronouns, an entity is
often referred using its hypernyms, e.g, the second
example in Table 1 uses “dog” to refer to “puppy”.
Motivated by this observation, we mine corefer-
ence knowledge between elements using Word-

net (Kilgarriff and Fellbaum, 2000): X
coref−−−→ Y

is an inference rule with cost 0 if X and Y are
lemmas in the same Wordnet synset, or with hy-
ponymy relation in Wordnet. Otherwise, the cost
of inference rules between this element-pair under
this relation will be 1.

2) Associative relation, which captures the se-
mantic relatedness between two entities, i.e., “s-
tarbucks” → “latte”, “restaurant” → “food”, etc.
This paper mines associative relations between en-
tities from Wikipedia1, using the method proposed
by Milne and Witten(2008). Specifically, given
two entities e1 and e2, we compute the semantic
distance dist(e1, e2) between them as:

dist(e1, e2) =
log(max(|E1|, |E2|)− log(|E1⋂E2|))
log(|W |)− log(min(|E1|, |E2|))

(3)

where E1 and E2 are the sets of all entities that
link to these two entities in Wikipedia respectively,

1https://www.wikipedia.org/

2034



and W is the entire Wikipedia. We set the cost of
inference rule e1

associative−−−−−−−→ e2 as dist(e1, e2).
2.3 Mining Sentiment Coherent Knowledge
Sentiment is one of the central and pervasive as-
pects of human experience (Ortony et al., 1990).
It plays an important role in commonsense stories,
i.e., a reasonable hypothesis should be sentimen-
tal coherent with its premise document. In this pa-
per, we mine sentiment coherence rules using Sen-
tiWordnet (Baccianella et al., 2010), in which each
synset of Wordnet is assigned with three sentiment
scores: positivity, negativity and objectivity.

Concretely, to identify sentimental coherence
rule between two element e1 and e2, we first com-
pute the positivity, negativity and objectivity s-
cores of every element by averaging the scores of
all synsets it’s in, then we identify an element to
be subjective if its objectivity score is smaller than
a threshold, and the distance between its positiv-
ity and negativity score is greater than a thresh-
old. Finally, for an inference rule e1

senti−−−→ e2,
we set its cost to 1 if e1 and e2 are both sub-
jective and have opposite sentimental polarity, to
-1 if they are both subjective and their sentimen-
tal polarity are the same, and to 0 for other cas-
es. For example, we will mine inference rules
“good senti−−−→ happy : −1”, “perfect senti−−−→ sad : 1”
and “young senti−−−→ happy : 0”.
2.4 Metric Learning to Calibrate Cost

Measurement
So far, we have extracted many inference rules
under different relations. However, because we
extract them from different sources and estimate
their costs using different measurements, the cost
metrics of these rules may not be consistent with
each other. To exploit different types of infer-
ence rules in a unified framework, we here propose
a metric learning based method to calibrate their
costs.

Given an input distance function, a metric learn-
ing method constructs a new distance function
which is “better” than the original one with super-
vision regarding an ideal distance (Kulis, 2012).
To calibrate inference rule cost, we add a non-
linear layer to the original cost sr of inference rule
r under relation f :

cr = sigmoid(wfsr + bf ) (4)

Here cr is the metric-unified inference cost of in-
ference rule r, wf and bf are calibration parame-

ters for inference rules of relation f . We use sig-
moid function in order to normalize costs into 0
to 1. Calibration parameters will be trained along
with other parameters in our model. See Section
3.4 for detail.

2.5 Dealing with Negation

One important linguistic phenomenon needs to
specifically consider is negation. Here we discuss
how to solve negation in our model.

We use ¬X to represent an element X mod-
ified by a negation word (the existence of nega-
tion is detected using dependency relations). Un-
der event narrative relation and sentiment coherent
relation, the existence of negation will reverse the
conclusion. So we add three additional negation

related inference rules for rule X
f−→ Y : s un-

der these relations, including ¬X f−→ Y : 1 − s,
X

f−→ ¬Y : 1 − s and ¬X f−→ ¬Y : s. Here s
is the calibrated cost of the original inference rule.
For entity semantic relations, we just ignore the
negation since it will not affect the inference un-
der these relations.

3 Machine Comprehension via
Commonsense Reasoning

This section describes how to leverage acquired
knowledge for commonsense machine compre-
hension. We first define how to infer from a
premise document to a hypothesis using inference
rules. Then we model how to choose inference
rules for a specific reasoning context. Finally, we
describe how to measure the reasoning distance
from a premise document to a hypothesis by sum-
marizing the costs of all possible inferences.

3.1 Inference from Premise Document to
Hypothesis

Given a premise document D = {d1, d2, ..., dm}
containing m elements, a hypothesis H =
{h1, h2, ..., hn} containing n elements, a valid in-
ference R from D to H is a set of inference rules
that all elements in H can be inferred from one el-
ement inD using one and only one rule inR. This
definition means that all elements in H should be
covered by consequents of inference rules in R,
as well as all antecedents of inference rules in R
should come from D. Figure 1 shows some in-
ference examples, where (a), (b) and (d) are valid
inferences, but (c) is not a valid inference because
its rules can not cover all elements in hypothesis.

2035



D:

H:

(a)

1 2 4

D:

H:

(b)

1 75

D:

H:

(c)

1 2

D:

H:

(d)

1 6

Mary

ordered foods

walked to

She

a restaurant Mary

ordered foods

walked to

She

a restaurant

Mary

ordered foods

walked to

She

a restaurant Mary

slept

walked to

She

a restaurant

narrative
associative

coreference

0.0 0.1 0.1 0.0

0.00.0

0.90.5

0.80.1

Figure 1: Examples of inferences. Numbers in cir-
cle indicates the proposed inference rules in Ta-
ble 2, and values in rectangle are their costs.

By the definition, the size of R and the size of
H are equal. So we use ri to denote the inference
rule in R that applied to derive element hi in H ,
i.e., R = {r1, r2, ..., rn}.

Based on the above definition, we can naturally
define the cost of an inference R as the cost sum
of all inference rules in R. In Figure 1, the cost
for inference (a) is 0.0 + 0.1 + 0.1 = 0.2, and for
inference (d) is 0.0 + 0.8 = 0.8.

3.2 Modeling Inference Probability using
Attention Mechanism

Obviously, there exist multiple valid inferences for
a premise document and a hypothesis. For exam-
ple, in Figure 1, both (a) and (b) are valid infer-
ences for the same premise document and hypoth-
esis. To identify whether a hypothesis is reason-
able, we need to consider all possible inferences.
However, in human reasoning process, not all in-
ference rules have the same possibility to be ap-
plied, because the more reasonable inference will
be proposed more likely. In Figure 1, inference
(a) should have a higher probability than inference
(b) because it is more reasonable to infer “food-
s” from “a restaurant” with associative relation,
rather than from “walked to” with narrative rela-
tion. Besides, the possibility of proposing an infer-
ence should not depend on its cost, e.g., inference
(d) should have high possibility to be proposed de-
spite its high cost, because we often infer event
“sleep” from another event using inference rules
under narrative relation. As examples mentioned
above, the “cost” measures the “correctness” of an
inference rule. A rule with low cost is more like-
ly to be “reasonable”, and a rule with high cost is
more likely to be a contradiction with common-
sense. On the other hand, the “possibility” should
measure how likely a rule will be applied in a giv-
en context, which does not depend on the “cost”

but on the nature of the rule and the given con-
text. Motivated by above observations, we endow
each inference a probability P (R|D,H), indicat-
ing the possibility thatR is chosen to infer hypoth-
esis H from premise document D. For simplicity,
we assume that each element in hypothesis is inde-
pendently inferred using individual inference rule,
then P (R|D,H) can be written as:

P (R|D,H) =
n∏

i=1

P (ri|D,H) (5)

=

n∏
i=1

P (ri|D,hi) (6)

=

n∏
i=1

m∑
j=1

P (ri, dj |D,hi) (7)

Equation (7) clearly shows how an inference rule
is selected given the premise document D and the
element hi in hypothesis. It depends on which ele-
ment dj inD will be selected and which relation f
will be used to infer hi from dj . We then refactor
the probability P (ri, dj |D,hi) to be:

P (ri, dj |D,hi) =
{

0 , antecedent(ri) 6= dj
g(hi, dj , f(ri);D) , otherwise

(8)

Here f(r) is the relation type of inference rule r,
and g(h, d, f ;D) is defined as:

g(h, d, f ;D) =
s(h, d)a(h, f)a(d, f)∑

f∈F
∑

d∈D s(h, d)a(h, f)a(d, f)
(9)

Here F denotes all relation types of inference
rules, s(e1, e2) is a matching function between
two elements e1 and e2, measuring by cosine sim-
ilarity based on GoogleNews word2vec (Mikolov
et al., 2013). And a(e, f) is an attention func-
tion measuring how likely an element e will be
involved with rules under relation f :

a(e, f) = vf
T tanh(Wfe + bf ) (10)

where vf ∈ RK , Wf ∈ RK×F and bf ∈ RK are
attention parameters of relation f , and e ∈ RF is
the feature vector of element e. Here K is the size
of attention hidden layer and F is the dimension of
feature vector. We consider three types of features,
as shown in Table 3. Using attention mechanism,
our method models the possibility that an infer-
ence rule is applied during the inference from a
premise document to a hypothesis by considering
the relatedness between elements and knowledge
category, as well as the relatedness between two
elements, which make it able to select the most
reasonable inference rules to derive each part of
the hypothesis.

2036



Feature Description
Syntax Features

is verb whether this element is a verb
is noun whether this element is a noun
is adj whether this element is an adjective

Lexical Features
is event whether this element belongs to

hyponymy of event sysnset in Wordnet
is entity whether this element belongs to

hyponymy of physical entity sysnset
is attr whether this element belongs to

hyponymy of attribute sysnset
named entity the named entity type of this element

Semantic Features
word

embeddings
300 dimension embeddings of
GoogleNews word2vec model

Table 3: Features for element-relation attention.

3.3 Reasoning Distance Between Premise
Document and Hypothesis

Given a premise document, this section shows how
to measure whether a hypothesis is coherent using
above inference model. Given all valid inferences
from D to H and the probability P (R|D,H) of
selecting inference R to infer H from D, we mea-
sure the reasoning distance L(D → H) as the ex-
pected cost sum of all valid inferences:

L(D → H) = EP (R|D,H)[cost(R)] (11)

= EP (R|D,H)[
n∑

i=1

cost(ri)] (12)

Then using Equation (6) and Equation (7), we can
further rewrite the equation into:

L(D → H) =
∑
R

[

n∏
i=1

P (ri|D,hi)] · [
n∑

i=1

cost(ri)] (13)

=

n∑
i=1

P (ri|D,hi) · cost(ri) (14)

=

n∑
i=1

m∑
j=1

P (ri, dj |D,hi) · cost(ri) (15)

Equation (15) shows that in our framework, the
final cost of inferring the element hi in the hy-
pothesis is the expected cost of all valid inference
rules which can derive hi from one element in the
premise document.

3.4 Model Learning

Following Huang et al. (2013), our model mea-
sures the posterior probability of choosing hypoth-
esis H as the answer of premise document D
through a softmax function:

P (H|D) = exp(−γL(D → H))∑
H′∈HD exp(−γL(D → H ′)

(16)

Here HD is all candidate hypothesises for D, and
γ is a positive smoothing factor. We train our mod-
el by maximizing the likelihood of choosing right

hypothesis H+ for D:

L(θ) = −log
∏

(D,H+)

P (H+|D) (17)

where θ is the parameter set of our model, includ-
ing calibration parameters in Section 2.4 and at-
tention parameters in Section 3.2. L(θ) is differ-
entiable so we can estimate θ using any gradient-
based optimization algorithm.

4 Experiments

4.1 Experimental Settings

Data Preparation. We evaluated our approach on
the Test Set Spring 2016 of RocStories, which con-
sists of 1871 commonsense stories, with each sto-
ry has two candidate story endings. Because sto-
ries in the training set of RocStories do not contain
wrong hypothesis, and our model has a compact
size of parameters, we estimated the parameters
of our model using the Validation Set Spring 2016
of RocStories with 1871 commonsense stories.

We mined event narrative knowledge from the
Training Set Spring 2016 of RocStories, which
consists of 45502 commonsense stories. We per-
formed lemmatisation, part of speech annotation,
named entity tagging, and dependency parsing us-
ing Stanford CoreNLP toolkits (Manning et al.,
2014). We used the Jan. 30, 2010 English ver-
sion of Wikipedia and processed it according to
the method described by Hu et al. (2008).

Model Training. We used normalized initial-
ization (Glorot and Bengio, 2010) to initialize at-
tention parameters in our model. For calibration
parameters, we initialized all wf to 1 and bf to
0. The model parameters were trained using mini-
batch stochastic gradient descent algorithm. As
for hyper-parameters, we set the batch size as 32,
the learning rate as 1, the dimension of attention
hidden layer K as 32, and the smoothing factor γ
as 0.5.

Baselines. We compared our approach with fol-
lowing three baselines:

1) Narrative Event Chain (Chambers and Ju-
rafsky, 2008), which scores hypothesis using PMI
scores between events. We used a simplified ver-
sion of the original model by using only verbs as
event, ignoring the dependency relation between
verbs and their participants. We found such a sim-
plified version achieved better performance than
its original one whose performance was reported
in (Mostafazadeh et al., 2016a).

2) Deep Structured Semantic Model (DSS-

2037



M) (Huang et al., 2013), which achieved the
best performance on RocStories as reported
by Mostafazadeh et al.(2016a). This model mea-
sures the reasoning score between a premise doc-
ument D and a hypothesis H by calculating the
cosine similarity between the overall vector repre-
sentations of D and H , and do not consider any
other task-relevant knowledge.

3) Recurrent Neural Network(RNN) Model
proposed by Pichotta and Mooney(2015), which
transforms all events and their arguments into a
sequence and predict next events and arguments
using a Long Short-Term Memory network. We
used the average generating probability of all el-
ements in H as the reasoning score, and choose
the hypothesis with largest reasoning score as the
system answer.

4.2 Overall Performance

System Accuracy
Narrative Event Chain 57.62%
DSSM 58.52%
RNN Model 58.93%
Our Model 67.02%

Table 4: Comparison of accuracy for our mod-
el and three baselines on RocStories Spring 2016
Test Set. The result of DSSM is adapted from
(Mostafazadeh et al., 2016a).

Table 4 shows the results. From this table, we
can see that:

1) Our model outperforms all baselines signif-
icantly. Compared with baselines, the accuracy
improvement on test set is at least 13.7%. This
demonstrates the effectiveness of our model by
mining and exploiting heteregenous knowledge.

2) The event narrative knowledge only is insuf-
ficient for commonsense machine comprehension.
Compared with Narrative Event Chain Model, our
model achieves a 16.3% accuracy improvemen-
t by considering richer commonsense knowledge,
rather than only narrative event knowledge.

3) It is necessary to distinguish different kinds
of commonsense relations for machine compre-
hension and commonsense reasoning. Compared
with DSSM and RNN, which model all relation-
s between two elements using a single semantic
similarity score, our model achieves significant ac-
curacy improvements by modeling, distinguishing
and selecting different types of commonsense re-
lations between different kinds of elements.

4.3 Effects of Different Knowledge

To investigate the effect of different kinds of
knowledge in our model, we conducted two group-
s of experiments.

The first group of experiments was conducted
using only one kind of knowledge at a time in our
model. Table 5 shows the results. We can see that
using a single kind of knowledge is insufficien-
t for commonsense machine comprehension: all
single-knowledge settings cannot achieve compet-
itive performance to the all-knowledge setting.

System Accuray
Event Narrative Knowledge 60.98%
Entity Semantics Knowledge 57.14%
Sentiment Coherent Knowledge 61.30%
Our Model(All Knowledge) 67.02%

Table 5: Comparison of the performance using s-
ingle type of knowledge.

The second group of experiments was conduct-
ed to investigate whether different knowledge can
complement each other. We conducted experi-
ments by removing one kind of knowledge from
our final model at a time, and investigate the
change of accuracy.

System Accuracy
Our Model(All Knowledge) 67.02%
-w/o Event Narrative Knowledge 63.65%
-w/o Entity Semantic Knowledge 64.89%
-w/o Sentiment Coherent Knowledge 62.85%

Table 6: Comparison of the performance by re-
moving one single type of knowledge.

Table 6 shows the results. We can find that
removing any kind of knowledge will reduce the
accuracy. This verified that all kinds of knowl-
edge containing unique complementary informa-
tion, which cannot be covered by other types of
knowledge.

4.4 Effect of Inference Probability

This section investigates the effect of inference
rule selection probability, and whether our atten-
tion mechanism can effectively model the possi-
bility of inference rule selection. We compared
our method with following two heuristic settings:

1) Minimum Cost Mechanism, which mea-
sures the reasoning distance by only selecting the
inference rule with minimum cost for each hypoth-
esis element.

2038



2) Average Cost Mechanism, which measures
the reasoning distance by setting equal probabili-
ties to all inference rules that can infer a hypothe-
sis element from a premise document element.

System Accuracy
Minimum Cost Mechanism 54.84%
Average Cost Mechanism 63.01%
Our Model(Attention Mechanism) 67.02%

Table 7: Comparison of the performance using
different inference rule selection mechanism.

Table 7 show the results. We can see that: 1) the
minimum cost mechanism cannot achieve compet-
itive performance, we believe this is because the
selection of rules should not depend on the cost
of them, and considering all valid inferences is
critical for reasoning; 2) our attention mechanism
can effectively model the inference rule selec-
tion possibility. Compared with the average cost
mechanism, our method achieved a 6.36% accura-
cy improvement. This also verified the necessity
of an effective inference rule probability model.

4.5 Effect of Negation Rules

This section investigates the effect of special han-
dling of negation mentioned in Section 2.5. To in-
vestigate the necessity of negation rules proposed
in our model, we conducted experiments by re-
moving all negation rules from original system,
and investigate the change of accuracy.

System Accuracy
Our Model 67.02%
-w/o Negation Rules 63.12%

Table 8: Comparison of the performance by re-
moving negation rules.

Table 8 show the results. We can see that re-
moving negation rules will significantly drop the
system performance, which confirm the effective-
ness of our proposed negation rules.

5 Related Work
Endowing computers with the ability of under-
standing commonsense story has long a goal of
natural language processing. There exist two
big challenges: 1)Matching explicit information
in the given context; 2)Incorporating implicit
commonsense knowledge into human-like reason-
ing process. Previous machine comprehension
tasks (Richardson et al., 2013; Weston et al.,
2015; Hermann et al., 2015; Rajpurkar et al.,

2016) mainly focus on the first challenge, lead-
ing their solutions focusing on semantic match-
ing between texts (Weston et al., 2014; Kumar
et al., 2015; Narasimhan and Barzilay, 2015;
Smith et al., 2015; Sukhbaatar et al., 2015; Hill
et al., 2015; Wang et al., 2015, 2016a; Cui et al.,
2016; Trischler et al., 2016a,b; Kadlec et al., 2016;
Kobayashi et al., 2016; Wang and Jiang, 2016b),
but ignore the second issues. One notable task is
SNLI (Bowman et al., 2015), which considers en-
tailment between two sentences. This task, how-
ever, only provides shallow context and thus need-
s a few kinds of implicit knowledge (Rocktäschel
et al., 2015; Wang and Jiang, 2016a; Angeli et al.,
2016; Wang et al., 2016b; Parikh et al., 2016; Hen-
derson and Popa, 2016; Zhao et al., 2017).

Realizing that story understanding needs
commonsense knowledge, many researches
have been proposed to learn structural event
knowledge. Chambers and Jurafsky (2008) first
proposed an unsupervised approach to learn
partially ordered sets of events from raw text.
Many expansions have been introduced later,
including unsupervisedly learning narrative
schemas and scripts (Chambers and Jurafsky,
2009; Regneri et al., 2011), event schemas and
frames (Chambers and Jurafsky, 2011; Balasubra-
manian et al., 2013; Sha et al., 2016; Huang et al.,
2016; Mostafazadeh et al., 2016b), and some
generative models to learn latent structures of
event knowledge (Cheung et al., 2013; Chambers,
2013; Bamman et al., 2014; Nguyen et al., 2015).
Another direction for learning event-centred
knowledge is causality identification (Do et al.,
2011; Radinsky et al., 2012; Berant et al., 2014;
Hashimoto et al., 2015; Gui et al., 2016), which
tried to identify the causality relation in text.

For reasoning over these knowledge, Jans et al.
(2012) extend introduced skip-grams for collect-
ing statistics. Further improvements include incor-
porating more information and more complicated
models (Radinsky and Horvitz, 2013; Modi and
Titov, 2014; Ahrendt and Demberg, 2016). Recen-
t researches tried to solve event prediction prob-
lem by transforming it into an language model-
ing paradigm (Pichotta and Mooney, 2014, 2015,
2016a,b; Rudinger et al., 2015; Hu et al., 2017).

The principal difference between previous work
and our method is that we not only take vari-
ous kinds of implicit commonsense knowledge
into consideration, but also provide a highly

2039



extensible framework to exploit these kinds of
knowledge for commonsense machine compre-
hension. We also notice the recent progress in
RocStories (Mostafazadeh et al., 2017). Rather
than inferring a possible ending generated from
document, recent systems solve this task by dis-
criminatively comparing two candidates. This en-
ables very strong stylistic features being added ex-
plicitly (Schwartz et al., 2017; Bugert et al., 2017)
or implicitly (Schenk and Chiarcos, 2017), which
can select hypothesis without any consideration of
given document. Also, some augmentation strate-
gies are introduced to produce more training da-
ta (Roemmele and Gordon, 2017; Mihaylov and
Frank, 2017; Bugert et al., 2017). These methods
are dataset-sensitive and are not the main concen-
tration of our paper.

6 Conclusions and Future Work
This paper proposes a commonsense machine
comprehension method, which performs effec-
tive commonsense reasoning by taking heteroge-
nous knowledge into consideration. Specifically,
we mine commonsense knowledge from heteroge-
neous knowledge sources and simultaneously ex-
ploit them by proposing a highly extensible multi-
knowledge reasoning framework. Experiment re-
sults shown that our method surpasses baselines
by a large margin.

Currently, there are little labeled training in-
stances for commonsense machine comprehen-
sion, for future work we want to address this is-
sue by developing semi-supervised or unsuper-
vised approaches.

Acknowledgments

This work is supported by the National Natu-
ral Science Foundation of China under Grants
no. 61433015 and 61572477, the National High
Technology Development 863 Program of Chi-
na under Grants no. 2015AA015405, and the
Young Elite Scientists Sponsorship Program no.
YESS20160177. Moreover, we sincerely thank
the reviewers for their valuable comments.

References
Simon Ahrendt and Vera Demberg. 2016. Improving

event prediction by representing script participants.
In Proceedings of NAACL-HLT, pages 546–551.

Gabor Angeli, Neha Nayak, and Christopher D Man-
ning. 2016. Combining natural logic and shallow

reasoning for question answering. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics, volume 1, pages 442–452.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200–2204.

Niranjan Balasubramanian, Stephen Soderland,
Oren Etzioni Mausam, and Oren Etzioni. 2013.
Generating coherent event schemas at scale. In
EMNLP, pages 1721–1731.

David Bamman, Brendan O’Connor, and Noah A
Smith. 2014. Learning latent personas of film char-
acters. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (A-
CL), page 352.

Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Ab-
by Vander Linden, Brittany Harding, Brad Huang,
Peter Clark, and Christopher D Manning. 2014.
Modeling biological processes for reading compre-
hension. In EMNLP.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Michael Bugert, Yevgeniy Puzikov, R Andreas, Ju-
dith Eckle-kohler, Teresa Martin, and Eugenio Mart.
2017. LSDSem 2017 : Exploring Data Generation
Methods for the Story Cloze Test. In The 2nd Work-
shop on Linking Models of Lexical, Sentential and
Discourse-level Semantics (ISDSEM 2017), 2016,
pages 56–61, Valencia, Spain.

Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In EMNLP,
volume 13, pages 1797–1807.

Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 602–610. Association for Computational
Linguistics.

Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without
the templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume
1, pages 976–986. Association for Computational
Linguistics.

Nathanael Chambers and Daniel Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In ACL,
volume 94305, pages 789–797. Citeseer.

Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction.
arXiv preprint arXiv:1302.4813.

2040



Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, T-
ing Liu, and Guoping Hu. 2016. Attention-over-
attention neural networks for reading comprehen-
sion. arXiv preprint arXiv:1607.04423.

Kathleen Dahlgren, Joyce McDowell, and Edward P
Stabler. 1989. Knowledge representation for
commonsense reasoning with text. Computational
Linguistics, 15(3):149–170.

Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011. Minimally supervised event causality identifi-
cation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
294–303. Association for Computational Linguistic-
s.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Aistats, volume 9, pages 249–256.

Lin Gui, Dongyin Wu, Ruifeng Xu, Qin Lu, and
Yu Zhou. 2016. Event-driven emotion cause extrac-
tion with corpus construction. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, pages 1639–1649.

Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer,
and Jong-Hoon Oh. 2015. Generating event causal-
ity hypotheses through semantic relations. In AAAI,
pages 2396–2403.

James Henderson and Diana Nicoleta Popa. 2016. A
vector space for distributional semantics for entail-
ment. arXiv preprint arXiv:1607.03780.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301.

Jian Hu, Lujun Fang, Yang Cao, Hua-Jun Zeng, Hua Li,
Qiang Yang, and Zheng Chen. 2008. Enhancing tex-
t clustering by leveraging wikipedia semantics. In
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 179–186. ACM.

Linmei Hu, Juanzi Li, Liqiang Nie, Xiao-Li Li, and
Chao Shao. 2017. What happens next? future
subevent prediction using contextual hierarchical l-
stm. In Proceedings of the 31th AAAI Conference
on Artificial Intelligence.

Lifu Huang, T Cassidy, X Feng, H Ji, CR Voss, J Han,
and A Sil. 2016. Liberal event extraction and event
schema induction. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
16).

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion & knowledge management, pages 2333–2338.
ACM.

Bram Jans, Steven Bethard, Ivan Vulić, and
Marie Francine Moens. 2012. Skip n-grams
and ranking functions for predicting script events.
In Proceedings of the 13th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 336–344. Association for
Computational Linguistics.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. 2016. Text understanding with the at-
tention sum reader network. arXiv preprint arX-
iv:1603.01547.

Adam Kilgarriff and Christiane Fellbaum. 2000.
Wordnet: An electronic lexical database.

Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and
Kentaro Inui. 2016. Dynamic entity representation
with max-pooling improves machine reading. In
Proceedings of NAACL-HLT, pages 850–855.

Brian Kulis. 2012. Metric learning: A survey. Foun-
dations and Trends in Machine Learning, 5(4):287–
364.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. 2015. Ask
me anything: Dynamic memory networks for natu-
ral language processing. CoRR, abs/1506.07285.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Todor Mihaylov and Anette Frank. 2017. Story Cloze
Ending Selection Baselines and Data Examination.
In The 2nd Workshop on Linking Models of Lexical,
Sentential and Discourse-level Semantics (ISDSEM
2017), pages 2–7, Valencia, Spain.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

David Milne and Ian H Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th ACM
conference on Information and knowledge manage-
ment, pages 509–518. ACM.

Ashutosh Modi and Ivan Titov. 2014. Inducing neural
models of script knowledge. In CoNLL, volume 14,
pages 49–57.

2041



Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016a. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of NAACL-
HLT, pages 839–849.

Nasrin Mostafazadeh, Alyson Grealish, Nathanael
Chambers, James Allen, and Lucy Vanderwende.
2016b. Caters: Causal and temporal relation scheme
for semantic annotation of event structures. In Pro-
ceedings of the The 4th Workshop on EVENTS: Def-
inition, Detection, Coreference, and Representation,
San Diego, California, June. Association for Com-
putational Linguistics.

Nasrin Mostafazadeh, Michael Roth, Annie Louis,
Nathanael William Chambers, and James F. Allen.
2017. LSDSem 2017 Shared Task : The Story Cloze
Test. In The 2nd Workshop on Linking Models of
Lexical, Sentential and Discourse-level Semantics
(ISDSEM 2017), 2016, pages 1–5, Valencia, Spain.

Karthik Narasimhan and Regina Barzilay. 2015.
Machine comprehension with discourse relations.
In ACL (1), pages 1253–1262.

Kiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret,
and Romaric Besançon. 2015. Generative even-
t schema induction with entity disambiguation. In
Proceedings of the 53rd annual meeting of the Asso-
ciation for Computational Linguistics (ACL-15).

Andrew Ortony, Gerald L Clore, and Allan Collins.
1990. The cognitive structure of emotions. Cam-
bridge university press.

Ankur P Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933.

Karl Pichotta and Raymond J Mooney. 2014. Statis-
tical script learning with multi-argument events. In
EACL, volume 14, pages 220–229.

Karl Pichotta and Raymond J Mooney. 2015. Learning
statistical scripts with lstm recurrent neural network-
s. In Proceedings of the 30th AAAI Conference on
Artificial Intelligence.

Karl Pichotta and Raymond J Mooney. 2016a. Statis-
tical script learning with recurrent neural networks.
EMNLP 2016, page 11.

Karl Pichotta and Raymond J Mooney. 2016b. Using
sentence-level lstm language models for script infer-
ence. arXiv preprint arXiv:1604.02993.

Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for news
events prediction. In Proceedings of the 21st
international conference on World Wide Web, pages
909–918. ACM.

Kira Radinsky and Eric Horvitz. 2013. Mining the web
to predict future events. In Proceedings of the sixth
ACM international conference on Web search and
data mining, pages 255–264. ACM.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. arXiv preprint arX-
iv:1606.05250.

Michaela Regneri, Alexander Koller, Josef Ruppen-
hofer, and Manfred Pinkal. 2011. Learning script
participants from unlabeled data. In RANLP, pages
463–470.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP, volume 3, page 4.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
arXiv preprint arXiv:1509.06664.

Melissa Roemmele and Andrew M Gordon. 2017. An
RNN-based Binary Classifier for the Story Cloze
Test. In The 2nd Workshop on Linking Models of
Lexical, Sentential and Discourse-level Semantics
(ISDSEM 2017), pages 74–80, Valencia, Spain.

Rachel Rudinger, Pushpendre Rastogi, Francis Ferraro,
and Benjamin Van Durme. 2015. Script induction as
language modeling. In EMNLP, pages 1681–1686.

Niko Schenk and Christian Chiarcos. 2017. Resource-
Lean Modeling of Coherence in Commonsense S-
tories. In The 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(ISDSEM 2017), pages 68–73, Valencia, Spain.

Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, Noah A Smith, and Computer
Science. 2017. Story Cloze Task : UW NLP System.
In The 2nd Workshop on Linking Models of Lexical,
Sentential and Discourse-level Semantics (ISDSEM
2017), pages 52–55, Valencia, Spain.

Lei Sha, Sujian Li, Baobao Chang, and Zhifang Sui.
2016. Joint learning templates and slots for event
schema induction. In Proceedings of NAACL-HLT,
pages 428–434.

Ellery Smith, Nicola Greco, Matko Bosnjak, and An-
dreas Vlachos. 2015. A strong lexical matching
method for the machine comprehension test. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1693–
1698. Association for Computational Linguistics.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

2042



Adam Trischler, Zheng Ye, Xingdi Yuan, Jing He,
Phillip Bachman, and Kaheer Suleman. 2016a.
A parallel-hierarchical model for machine com-
prehension on sparse data. arXiv preprint arX-
iv:1603.08884.

Adam Trischler, Zheng Ye, Xingdi Yuan, and Ka-
heer Suleman. 2016b. Natural language compre-
hension with the epireader. arXiv preprint arX-
iv:1606.02270.

Bingning Wang, Shangmin Guo, Kang Liu, Shizhu
He, and Jun Zhao. 2016a. Employing external rich
knowledge for machine comprehension. In Pro-
ceedings of IJCAI.

Hai Wang, Mohit Bansal, Kevin Gimpel, and David A
McAllester. 2015. Machine comprehension with
syntax, frames, and semantics. In ACL (2), pages
700–706.

Shuohang Wang and Jing Jiang. 2016a. Learning natu-
ral language inference with lstm. In Proceedings of
NAACL-HLT, pages 1442–1451.

Shuohang Wang and Jing Jiang. 2016b. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905.

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016b. Sentence similarity learning by lexical de-
composition and composition. arXiv preprint arX-
iv:1602.07019.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merriënboer, Armand Joulin,
and Tomas Mikolov. 2015. Towards ai-complete
question answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2014. Memory networks. arXiv preprint arX-
iv:1410.3916.

Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben-
jamin Van Durme. 2016. Ordinal common-sense in-
ference. arXiv preprint arXiv:1611.00601.

Kai Zhao, Liang Huang, and Mingbo Ma. 2017. Tex-
tual entailment with structured attentions and com-
position. arXiv preprint arXiv:1701.01126.

2043


