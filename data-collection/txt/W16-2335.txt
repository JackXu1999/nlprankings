



















































English-Portuguese Biomedical Translation Task Using a Genuine Phrase-Based Statistical Machine Translation Approach


Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 456–462,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

English-Portuguese Biomedical Translation Task Using a Genuine
Phrase-Based Statistical Machine Translation Approach

José Aires1,2 Gabriel Pereira Lopes1,2
1 NOVA LINCS, Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa, Portugal

2 ISTRION BOX, Translation and Revision, Lda, Portugal
{jose.aires,gabriel.lopes,luis.gomes}@istrionbox.com

Luı́s Gomes1,2

Abstract

Our approach to produce translations for
the ACL-2016 Biomedical Translation
Task on the English-Portuguese language
pair, in both directions, is described. Own
preliminary tests results and final results,
measured by the shared task organizers,
are also presented.

1 Introduction

This paper shows how we obtained our results
using our patented Machine Translation system
(Lopes et al., 2015) to produce translations for
the English-Portuguese language pair from the
Biomedical Translation Task.

Our approach differs from common Statisti-
cal Machine Translation approaches like Moses
(Koehn et al., 2007) in several aspects:

• phrases are not analyzed at their word level
in any model;

• the language model depends on the target al-
ternatives of given adjacent sources and does
not try to avoid null scores to phrases that do
not occur;

• the translation score is not log-linear, but in-
stead a tuned weighted average between the
translation model and the language model,
and so no smoothing techniques are required;

• several models can be used with different rel-
evances or weights; and

• instead of simply relying on statistics, we
include human validation and correction on
several stages of the system, namely for vali-
dating extracted term translations, to improve
the quality of the source data used in the au-
tomatically produced translations.

As requested, the translation results were pro-
duced using the sentence-aligned training data de-
scribed below (for the English-Portuguese lan-
guage pair, in our case), provided by the shared
task organizers:

• medline-pubmed: parallel corpora from
medline;

• scielo-gma-biological: parallel biological
documents from the Scielo database (Neves
et al., 2016); and

• scielo-gma-health: parallel health docu-
ments from the Scielo database (Neves et al.,
2016).

Table 1 shows the features of the English (en)
and Portuguese (pt) languages of each provided
corpora, namely their number of lines and words.

corpus lines words
medline-pubmed-en 74,645 917,307
medline-pubmed-pt 74,645 1,041,079
scielo-gma-biological-en 120,301 3,338,244
scielo-gma-biological-pt 120,301 3,736,817
scielo-gma-health-en 507,987 13,443,076
scielo-gma-health-pt 507,987 14,901,240

Table 1: Training corpora data after normalization.

The translation task then consisted in translat-
ing one document from English to Portuguese and
another from Portuguese to English, for both the
biological and the health domains, with the num-
ber of lines and words from those test documents
shown in Table 2.

Besides the provided training data, we have also
included our English and European Portuguese
bilingual lexicon (described in §2.3.2), as well as
our named entities database, for additional term
coverage.

456



document lines words
biological pt2en 4,029 119,410
biological en2pt 4,333 111,038
health pt2en 3,826 111,073
health en2pt 3,858 96,240

Table 2: Test documents data after normalization.

The training corpora had to undergo several pro-
cessing stages in order to support the production of
the intended translations, as described in the fol-
lowing section.

2 Data Processing

In order to produce translations, our system (like
any other Statistical Machine Translation system)
requires a translation model and a language model
to support the translation decoding stage. To cal-
culate such models the available data had to go
through several processing steps described in the
following subsections.

Since each of the training corpus has been made
available separately, we also opted to process each
of them separately so that we were then able to
use them with different weights, assigning more
or less weight to models with higher or lower rel-
evance, respectively. See extended explanation in
§4.

2.1 Considerations about the provided data

It should be noted that we have detected a few
flaws in the provided data, namely several sen-
tences incorrectly considered as parallel, as well
as the existence of many spelling errors, not only
in the training data, but also in the testing docu-
ments.

We believe that many of the typos result from
PDF extraction and/or OCR processes, which are
never perfect, having found and corrected a total of
127,198 misspellings. Yet, it should be noted that
some misspelling errors are easy to correct, but er-
rors which still produce correct words require sen-
tence analysis which was not carried out.

Some of the parallel problems are illustrated,
for instance, by having the first Portuguese line
from medline-pubmed “ERRATA.” aligned with
the first English line “Inequalities in self-rated
health: an analysis of the Brazilian and Por-
tuguese populations.”, which should be “ER-
RATA.” instead.

Filtering wrong translation units as the one

above, as well as translation units which the lan-
guage was not Portuguese, reduced this corpora by
almost 2,000 translation units.

Some errors were simply detected by chance,
like first and last entries of medline-pubmed, while
other errors were detected by looking at the un-
translated terms in the initial testing §3 and realiz-
ing that some terms were misspellings, as well as
spelling and vocabulary differences between Eu-
ropean and Brazilian Portuguese.

corpus lines words
medline-pubmed-en 74,645 917,307
medline-pubmed-rev-en 72,651 898,051
medline-pubmed-pt 74,645 1,041,079
medline-pubmed-rev-pt 72,651 1,006,069

Table 3: medline-pubmed revision impact.

Table 3 shows the differences between the orig-
inal version medline-pubmed and its revised ver-
sion medline-pubmed-rev. The reduction in size
towards the revised version is mainly due to the
removal of non-parallel sentences.

However, efforts to correct such situations were
only made over the mentioned medline-pubmed
parallel document set, since the other sets were
significantly larger, as shown in Table 1. Also, no
corrections were applied to the testing documents
because we assumed they were not supposed to be
edited.

Yet, another “noise” element was the already
mentioned difference in spelling and vocabulary
between European Portuguese (which has been
our main focus of attention throughout our re-
search experience) and Brazilian Portuguese (the
version of the provided biomedical data), which
can also impact results negatively.

2.2 Text tokenization and normalization

Text tokenization ensures that words are prop-
erly separated by a single blank space, while nor-
malization ensures that they are represented by a
“standard” version. In English, this means that
cases like “wasn’t” or “isn’t” are going to be re-
placed by “was not” and “is not”, respectively.
In Portuguese, this means that cases like “do” (of
the) or “nas” (in the) are going to be replaced by
“de o” (of the) and “em as” (in the), respectively.
These tokenization and normalization changes are
reverted when presenting the final translation re-
sults.

457



Whipple disease and central nervous system .

D
oe

nç
a 

de
 W

hi
pp

le
 e

 s
is

te
m

a 
ne

rv
os

o 
ce

nt
ra

l .

Figure 1: Example lexicon- and cognate-based
alignment of a short sentence from the medline-
pubmed corpus. Gray-filled rectangles repre-
sent word- and phrasal-matches from the lexicon
while the checkerboard-filled rectangle shows a
cognaticity-based match.

2.3 Phrase alignment

Phrase-level alignment was obtained with a modi-
fied version of the lexicon-based aligner proposed
by Gomes (2009). The aligner matches bilin-
gual phrase pairs provided in an input lexicon (de-
scribed ahead in §2.3.2) and selects a maximal-
coverage1 subset of coherent alignments. While
the original method imposed a monotonicity con-
straint, i.e. it selected a maximal-coverage chain
of phrase alignments without allowing phrase re-
orderings, the new method applied has a more
relaxed coherency criteria: it only requires that
a source-language phrase is not simultaneously
aligned with two distinct target-language phrases.
Therefore, it allows phrase reordering as shown in
the example in Figure 1.

2.3.1 Alignment as an optimization problem
Similar to the ILP (Integer Linear Programming)
solution proposed by (DeNero and Klein, 2008),
we treat the alignment problem as an optimization
problem, but we employ a greedy optimization al-
gorithm which allows us to align longer sentences
with reasonable time and memory. The algorithm

1Maximal-coverage means that the selected phrase align-
ments cover as much text as possible from both sentences

constructs a solution (a set of coherent alignments)
incrementally. It starts by settling alignments of
longer phrases, which tend to be more reliable,
and progresses towards shorter phrases or words,
which are allowed to align only if they are coher-
ent with previously settled alignments.

2.3.2 Input bilingual lexicon
Our EN-PT input lexicon has 931,568 manually
validated translations (words and phrases). This
lexicon has been compiled in a long term effort
started in the context of project ISTRION2. The
translations were extracted automatically from
several corpora, including Europarl (Koehn and
Monz, 2005), JRC-Acquis (Steinberger et al.,
2006), OPUS EMEA (Tiedemann, 2009) and oth-
ers, using a combination of complementary align-
ment and extraction methods: GIZA (Och and
Ney, 2003), Anymalign (Lardilleux and Lepage,
2009), spelling similarity measure SpSim (Gomes
and Lopes, 2011) combined with co-occurrence
Dice measure, and others. The automatically ex-
tracted word and phrasal translations were auto-
matically classified, prior to human validation, us-
ing an SVM classifier trained on previously val-
idated translations as described by Mahesh et al.
(2015). The automatic classification speeds up hu-
man validation because very few translations (less
than 5%) are incorrectly classified, and only those
need to be manually labeled as correct or incorrect.

We did not perform any extraction or validation
of new translations from the corpus provided for
this shared task. We did, however, complement
our lexicon with cognate and homograph align-
ments using the SpSim (Gomes and Lopes, 2011)
spelling similarity measure.

2.3.3 Lexicon coverage
Our lexicon covers 59.5% of the EN corpus tokens
and 55.4% of the PT corpus tokens. There were
143,317 unique phrasal translations matched out
of 931,568 in our lexicon. The cognaticity-based
matching was responsible for aligning 8% of the
EN corpus and 7.2% of the PT corpus3. The re-
mainder 32.5% of the EN corpus and 37.4% of the
PT corpus were left unaligned. These unaligned
tokens are handled as gaps by the phrase table ex-
traction algorithm described in (Aires et al., 2009).

2Project ISTRION was funded by the Portuguese Founda-
tion for Science and Technology under contract PTDC/EIA-
EIA/114521/2009

3cognaticity alignment was applied only to tokens not
covered by the input lexicon

458



2.4 Language model training

The language model used is supported by the in-
dexation of the texts in each language of the pro-
vided corpora. Such indexation will support deter-
mining the likelihood of the occurrence of phrases
in the target language for the several adjacent
translation fragments in decoding, a process based
on the structures presented in (Aires et al., 2008).

2.5 Translation model training

The translation model depends on the alignment
to determine phrase translation equivalents by es-
tablishing phrase relations between source and tar-
get languages, as well as to determine a degree of
likelihood of those same relations, to be used in
decoding to produce new translations, a process
based on the methodology presented in (Aires et
al., 2009).

2.6 Decoding

The decoding stage is the one that will finally pro-
duce the actual translations. First, an original text
is fragmented into smaller pieces of text, which
will then be used to retrieve their correspond-
ing translations. The several combinations of the
translations of those smaller pieces will represent
many possible translations and the purpose of de-
coding is to find the most likely one, according
to the provided scores from the language and the
translation models. As mentioned before, separate
models can be obtained from separate corpora and
be assigned with different relevances or weights,
according to their importance to the translation in
question.

As such, and as explained in Lopes et al. (2015),
decoding is carried out as a best path finding
in a directed acyclic graph, where its edges are
weighed by: the translation model score between
source and target phrases; and the language model
scores between adjacent target phrases. Each com-
plete path will represent a possible translation in
which the final score is a composition of the scores
of the several edges that compose the given path.
An additional penalty is introduced to provide
lower scores to larger paths, which are known to
produce worse results.

3 Initial Testing Preparation

Since no development data was supplied, we took
the initiative to prepare some development sets in
order to have an idea of the most promising set of

parameters to be used in our system over the pro-
vided data to produce the intended translations. As
such, several documents were removed from the
original training data, composed by the medline-
pubmed, biological and health sets, applying the
training methods on the remaining documents and
using the selected ones to translate and compare
the translations against their originals by deter-
mining their BLEU (Papineni et al., 2002) scores.
However, in order to get a clearer picture of the
type of results that could be expected, some addi-
tional tests were carried out including the selected
set of documents in the training data.

Our translation model supports: a conserva-
tive extraction approach, which is more restric-
tive, allowing fewer translation equivalents, hav-
ing a lower recall but a higher precision; and a
flexible extraction approach, which is more per-
missive, allowing a larger number of equivalents
but at the cost of an increase of incorrect ones. We
were interested in evaluating the impact of both
approaches on results.

Table 4 shows the average results on both trans-
lation directions of those preliminary tests, con-
sisting of the average BLEU scores for the con-
servative (cons.) and flexible (flex.) approaches,
as well as the average times taken to translate the
documents on either extraction approaches. Those
results concern the following configurations:

• full: the documents used for testing were
not removed from the training set (medline-
pubmed, biological and health);

• dev: the documents used for testing were re-
moved form the training set;

• dev-europarl: the same as dev, but including
the europarl corpus; and

• dev-europarl-low: the same as dev-europarl,
but assigned a lower relevance to the europarl
corpus.

configuration cons. flex. time
full 83.98 81.97 15.1 s
dev 51.72 55.46 3.5 s
dev-europarl 52.34 55.98 49.9 s
dev-europarl-low 52.54 56.21 46.8 s

Table 4: Initial testing results.

459



These preliminary tests have shown that the
flexible extraction approach produced on average
better translation results when the reference doc-
uments were not included in the test set, which is
the normal testing situation, so we used the flexi-
ble approach.

The Europarl corpus4, which is signifi-
cantly larger (54,543,044 words in English and
60,375,477 words in Portuguese), was tested
as a source of additional term coverage, which
allowed a translation quality improvement lower
than 1 BLEU point. However, given its significant
increase in processing time because of its large
size, a time increase around 14 times larger, we
had to drop it from the submission tests due to
deadline constraints. Additionally, these results
show that assigning a lower relevance to a corpus
from a totally different domain may have some
positive impact on average results.

Once we have decided, from this initial testing
preparation, which would be the most promising
and interesting features to use in the final runs, we
ran the training processes again to include the doc-
uments that have been left out, this way using the
full data provided by the organizers for the runs to
be submitted.

4 Submitted Results

Considering that the test documents to be trans-
lated, provided by the shared task organization,
share their domain with the training data, we de-
cided to propose for submission the three possi-
ble translation runs for each document according
to the criteria described in each of the following
subsections.

4.1 Run 1

This run uses the medline-pubmed, biological and
health training corpora with the same relevance to
translate every translation test document. These
can be considered our simplest set of tests since
the possible model relevance difference is not ex-
plored and no additional sources are included. In
this case we achieved a total of 7228 unique un-
translated terms5.

4.2 Run 2

This run also uses the medline-pubmed, biological
and health training corpora, but assigns a higher

4http://www.statmt.org/europarl/
5Terms can have one or more words

relevance to the biological corpora to translate
the biological test documents and then assigns a
higher relevance to the health corpora to translate
the health test documents. Because the changes
introduced in this set of tests only concerned the
relevance of the models, the total of 7228 unique
untranslated terms did not change.

4.3 Run 3

This last run shares the same features as the pre-
vious run (assigning higher relevances to corre-
sponding corpora) but this time our bilingual lex-
icon and named entities database was included
for term coverage improvement, and an alignment
based on cognates (Gomes and Lopes, 2011) is
used.

About our bilingual lexicon, considering that it
was built mainly from the European legislation, it
was given a lower relevance because past expe-
riences have shown us that, when the domain is
not shared with the texts to be translated, it should
not have the same relevance in order to reduce the
probability of using inadequate terms for the in-
tended translation domain or subject. Again, this
is a situation that has also been confirmed and
noted in Table 4 between dev-europarl and dev-
europarl-low: reducing the relevance of europarl
contributed to a slight score increase compared to
when the relevance is the same.

As a side note, translating the tests took nearly
14 hours for each run6. Had we included europarl,
judging by Table 4, we would have taken nearly
200 hours, which is more than a week, expecting
to simply gain 0.75 BLEU points, on average, so
we had no other option than leaving it out. Such
increase in translation time is due to the substan-
tial increase of translation equivalents available for
decoding from such a large corpus.

The decision to carry out the alignment based
on cognates was taken because after a first run
of tests we realized that many of the untrans-
lated terms referred to medical terms and diseases,
which shared many letters between both languages
and therefore had a high level of cognaticity.

All these changes allowed a significant reduc-
tion of the unique untranslated terms to a total of
4700, and for all the reasons in this subsection, we
have considered this run as being our best.

6On a 3.3GHz CPU with 32GB RAM and 4TB disk

460



5 Conclusions and Future Work

The scores of our submitted translations are shown
in Table 5.

.

run score
Istrionbox run1 biological en2pt 17.55
Istrionbox run2 biological en2pt 16.47
Istrionbox run3 biological en2pt 16.45
Average 16.80
Istrionbox run1 biological pt2en 20.88
Istrionbox run2 biological pt2en 20.17
Istrionbox run3 biological pt2en 20.14
Average 20.40
Istrionbox run1 health en2pt 19.01
Istrionbox run2 health en2pt 18.33
Istrionbox run3 health en2pt 18.37
Average 18.57
Istrionbox run1 health pt2en 21.50
Istrionbox run2 health pt2en 20.17
Istrionbox run3 health pt2en 20.62
Average 20.76

Table 5: Initial testing results.

The results obtained were clearly below what
we had expected. And what is most disturbing is
the negative impact of features we expected to im-
prove results, an expectation backed by our own
tests.

However, there are a few reasons we can think
of for these values, namely the way the BLEU
measure has been calculated (case sensitivity and
synonyms penalty - translating “home” instead of
“house” might be perfectly fine), the differences
between European Portuguese and Brazilian Por-
tuguese, and the presence of several spelling and
alignment errors in the training data.

Nonetheless, we can still take several actions
to improve our system: namely testing both par-
allel corpora, health and biology, with identical
weights: using Europarl and eventually EMEA
corpus; the refinement of our phrase translation
extraction; the extraction of specific bilingual ter-
minology, additionally to the use of cognaticity;
subsentence realignment after the bilingual termi-
nology extraction, and a more efficient implemen-
tation of the patterns (comparable to a hierarchical
translation) application.

Acknowledgments

This work was supported by ISTRION BOX,
Fundação para a Ciência e Tecnologia through
research project ISTRION (contract PTDC/EIA-
EIA/114521/2009), individual PhD grants
SFRH/BD/48839/2008, SFRH/BD/65059/2009,
SFRH/BD/64371/2009, and NOVA LINCS (ref.
UID/CEC/04516/2013). We would also like to
thank Hugo Delgado for his support.

References
J. Aires, G. P. Lopes, and J. F. da Silva. 2008. Efficient

multi-word expressions extractor using suffix arrays
and related structures. pages 1–8. CIKM-ACM.

J. Aires, G. P. Lopes, and L. Gomes. 2009. Phrase
translation extraction from aligned parallel cor-
pora using suffix arrays and related structures. In
Progress in Artificial Intelligence, volume 5816 of
LNAI, pages 587–597. Springer-Verlag Berlin Hei-
delberg.

J. DeNero and D. Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
08: HLT, Short Papers, pages 25–28, Columbus,
Ohio, June. ACL.

L. Gomes and G. P. Lopes. 2011. Measuring spelling
similarity for cognate identification. In Progress in
Artificial Intelligence, volume 7026 of LNAI, pages
624–633, Lisbon, Portugal, October. Springer.

L. Gomes. 2009. Parallel texts alignment. Master’s
thesis, Faculdade de Ciências e Tecnologia, Univer-
sidade Nova de Lisboa, Monte de Caparica, Portu-
gal.

P. Koehn and C. Monz. 2005. Shared task: Sta-
tistical machine translation between european lan-
guages. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 119–124.
ACL.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions, ACL ’07,
pages 177–180, Stroudsburg, PA, USA. ACL.

A.n Lardilleux and Y. Lepage. 2009. Sampling-based
multilingual alignment. In Proceedings of Recent
Advances in Natural Language Processing, pages
214–218, Borovets Bulgaria, 09.

G. P. Lopes, J. Aires, and L. Gomes. 2015. Statistical
machine translation computer system and method.
Submitted at National (Portugal) Level (INPI), 8.
Provisional Patent Request No. 0151000065353.

461



K. Mahesh, L. Gomes, J. Aires, and G. P. Lopes. 2015.
Selecting translation candidates for parallel corpora
alignment. In Progress in Artificial Intelligence,
volume 9273 of LNAI, pages 723–734, Coimbra,
Portugal, September. Springer.

M. Neves, A. J. Yepes, and A. Névéol. 2016. The
scielo corpus: a parallel corpus of scientific publica-
tions for biomedicine. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Sara Goggi, Marko Grobelnik, Bente Maegaard,
Joseph Mariani, Helene Mazo, Asuncion Moreno,
Jan Odijk, and Stelios Piperidis, editors, Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC 2016),
Paris, France, may. European Language Resources
Association (ELRA).

F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.

K. Papineni, S. Roukos, T. Ward, and Wei-Jing Zhu.
2002. Bleu: A method for automatic evaluation
of machine translation. In Proceedings of the 40th
Annual Meeting on ACL, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. ACL.

R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat,
T. Erjavec, D. Tufis, and D. Varga. 2006. The jrc-
acquis: A multilingual aligned parallel corpus with
20+ languages. In Proceedings of LREC’2006 pp.
2142-2147. Genoa, Italy, 24-26 May 2006, Genoa,
Italy, 5. ELRA.

J. Tiedemann. 2009. News from opus-a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent advances in natural language
processing, volume 5, pages 237–248.

462


