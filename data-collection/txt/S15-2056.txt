



















































DFKI: Multi-objective Optimization for the Joint Disambiguation of Entities and Nouns & Deep Verb Sense Disambiguation


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 335–339,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

DFKI: Multi-objective Optimization for the Joint Disambiguation of Entities
and Nouns & Deep Verb Sense Disambiguation

Dirk Weissenborn
LT, DFKI

Alt-Moabit 91c
Berlin, Germany

dirk.weissenborn@dfki.de

Feiyu Xu
LT, DFKI

Alt-Moabit 91c
Berlin, Germany
feiyu@dfki.de

Hans Uszkoreit
LT, DFKI

Alt-Moabit 91c
Berlin, Germany

uszkoreit@dfki.de

Abstract

We introduce an approach to word sense dis-
ambiguation and entity linking that combines
a set of complementary objectives in an exten-
sible multi-objective formalism. During dis-
ambiguation the system performs continuous
optimization to find optimal probability dis-
tributions over candidate senses. Verb senses
are disambiguated using a separate neural net-
work model. Our results on noun and verb
sense disambiguation as well as entity linking
outperform all other submissions on the Se-
mEval 2015 Task 13 for English.

1 Introduction

The task of assigning the correct meaning to a given
word or entity mention in a document is called word
sense disambiguation (WSD) (Navigli, 2009) or en-
tity linking (EL) (Bunescu and Pasca, 2006), respec-
tively. Successful disambiguation requires not only
an understanding of the topic or domain a document
is dealing with (global), but also an analysis of how
an individual word is used within its local context.
E.g., the meanings of the word “newspaper” as the
company or the physical product, often cannot be
distinguished by the topic, but by recognizing which
type of meaning fits best into the local context of its
occurrence. On the other hand, for an ambiguous
entity mention such as “Michael Jordan” it is impor-
tant to recognize the topic of the wider context to
distinguish, e.g., between the basketball player and
the machine learning expert.

The combination of the two most commonly used
reference knowledge bases for WSD and EL, e.g.,

WordNet (Fellbaum, 1998) and Wikipedia, by Ba-
belNet (Navigli and Ponzetto, 2012) has enabled a
new line of research towards the joint disambigua-
tion of words and named entities. Babelfy (Moro
et al., 2014) has shown the potential of combining
these two tasks in a purely knowledge-driven ap-
proach that jointly finds connections between po-
tential word senses in the global context. On the
other hand, typical supervised methods (Zhong and
Ng, 2010) trained on sense-annotated corpora are
usually quite successful in dealing with individual
words in a local context. Hoffart et al. (2011) rec-
ognize the importance of combining both local con-
text and global context for robust disambiguation.
However, their approach is limited to EL, where op-
timization is performed in a discrete setting.

We present a system that combines disambigua-
tion objectives for both global and local contexts
into a single multi-objective function. In contrast
to prior work we model the problem in a continuous
setting based on probability distributions over can-
didate meanings. Our approach exploits lexical and
encyclopedic knowledge, local context information
and statistics of the mapping from text to candidate
meanings. Furthermore, we introduce a deep learn-
ing approach to verb sense disambiguation based on
semantic role labeling.

2 Approach

The SemEval-2015 task 13 (Moro and Navigli,
2015) requires a system to jointly detect and dis-
ambiguate word and entity mentions given a refer-
ence knowledge base. The provided input to the sys-
tem are tokenized, lemmatized and POS-tagged doc-

335



uments; the output are sense-annotated mentions.
Our system employs BabelNet 1.1.1 as reference

knowledge base (KB). BabelNet is a multilingual se-
mantic graph of concepts and named entities that are
represented by synonym sets, called Babel synsets.

2.1 Mention Extraction & Entity Detection

We define a mention to be a sequence of tokens in
a given document for which there exists at least one
candidate meaning in the KB. The system considers
all content words (nouns, verbs, adjectives, adverbs)
as mentions including also multi-token words of up
to 5 tokens that contain at least one noun. In ad-
dition, we apply a pre-trained stacked linear-chain
CRF (Lafferty et al., 2001) using the FACTORIE
toolkit of version 1.1 (McCallum et al., 2009) to
identify named entity (NE) mentions. In our ap-
proach, we distinguish NEs from common nouns
and treat them as two different classes because there
are many common nouns also referring to NEs mak-
ing disambiguation unnecessarily complicated.

2.2 Candidate Search

After potential mentions are extracted the system
tries to identify their candidate meanings, i.e., the
appropriate synsets. Mentions without such can-
didates are discarded. The mapping of candi-
date mentions to synsets is based on similarities
of their surface strings or lemmas. If the surface
string or lemma of a mention matches the lemma
of a synonym in a synset that has the same part
of speech, the synset will be considered a candi-
date meaning. We allow partial matches for Ba-
belNet synonyms derived from Wikipedia titles or
redirections. A partial match allows the surface
string of a mention to differ by up to two tokens
from the Wikipedia title (excluding everything in
parentheses) if the partial string was used at least
once as an anchor for the corresponding Wikipedia
page. For example, for the Wikipedia title Arm-
strong School District (Pennsylvania), the follow-
ing surface strings would be considered matches:
“Armstrong School District (Pennsylvania)”, “Arm-
strong School District”, “Armstrong”, but not
“School”, since “School” was never used as an an-
chor. If there is no match we try the same procedure
applied to the lowercased text or lemma.

Because of the distinction between nouns and

named entities we treat NE as a separate POS tag.
Candidate synsets for NEs are Babel synsets con-
sidered NEs in BabelNet, and additionally Babel
synsets of all Wikipedia senses that are not consid-
ered NEs. Similarly, candidate synsets for nouns are
noun synsets that are not considered NEs in addi-
tion to all synsets of WordNet senses in BabelNet.
We add synsets of Wikipedia senses and WordNet
senses, respectively, because the distinction of NEs
and simple concepts is not always clear in BabelNet.
For example the synset for “UN” (United Nations) is
considered a concept whereas it could also be con-
sidered a NE. Finally, if there is no candidate for a
potential noun mention we try to find NE candidates
for it and vice versa.

2.3 Disambiguation of Nouns and Named
Entities

We formulate the disambiguation problem in a con-
tinuous setting by using probability distributions
over candidates. This has several advantages over
a discrete setting. First, we can exploit well estab-
lished continuous optimization algorithms, such as
conjugate gradient or LBFGS, which guarantee to
converge to a local optimum. Second, by optimiz-
ing upon probability distributions we are optimizing
the actually desired result in contrast to densest sub-
graph algorithms where such probabilities need to
be calculated artificially afterwards, e.g., Moro et al.
(2014). Third, discrete optimization usually works
on a single candidate per iteration whereas in a con-
tinuous setting, probabilities are adjusted for each
candidate, which is computationally advantageous
for highly ambiguous documents.

Given a set of objectives O the overall objective
function O is defined as the sum of all normalized
objectives O ∈ O given a set of mentions M :

O(M) =
∑
O∈O

O(M)
Omax(M)−Omin(M) . (1)

We normalize each objective using the difference
of their maximum and minimum value for the given
document. For disambiguation we optimize the
multi-objective function using Conjugate Gradient
(Hestenes and Stiefel, 1952) with up to 1000 iter-
ations per document.

336



Coherence Jointly disambiguating all mentions
within a document has been shown to have a large
impact on disambiguation quality. We adopt the idea
of semantic signatures and the idea of maximizing
the semantic agreement among selected candidate
senses from Moro et al. (2014). We define the con-
tinuous objective function based on probability dis-
tributions pm(c) over the candidate set Cm of each
mention m ∈M in a document as follows:

Ocoh(M) =
∑
m∈M
c∈Cm

∑
m′∈M
m′ 6=m
c′∈Cm′

s(m, c,m′, c′)

s(m, c,m′, c′) = pm(c) · pm′(c′) · 1((c, c′) ∈ S)

pm(c) =
eλm,c∑

c′∈Cm e
λm,c′

, (2)

where S denotes the semantic interpretation graph,
1 the indicator function and pm(c) is a softmax
function. The only free, optimizable parameters are
the softmax weights λm,c. This objective can be
interpreted as finding the densest subgraph of the
semantic interpretation graph where each node is
weighted by its probability and therefore each edge
is weighted by the product of its adjacent vertex
probabilities.

Type Classification One of the biggest problems
of supervised approaches to WSD is the size and
synset coverage of training corpora such as Sem-
Cor (Miller et al., 1993). One way to circum-
vent this problem is to use a coarser set of seman-
tic classes that groups synsets together. Previous
studies on using semantic classes for disambigua-
tion showed promising results (Izquierdo-Beviá et
al., 2006). WordNet provides a mapping, called lex-
names, of synsets into 45 types based on the syntac-
tic categories of synsets and their logical groupings1.

A multi-class logistic (softmax) regression model
was trained that calculates a probability distribution
qm(t) over lexnames t given a potential WordNet
mention m. The features used as input to the model
are the following: embedding of the mention’s text,
sum of embeddings of all sentence words, embed-
ding of the dependency parse parent, collocations

1http://wordnet.princeton.edu/man/
lexnames.5WN.html

of surrounding words (Zhong and Ng, 2010), sur-
rounding POS tags and possible lexnames. We used
pre-trained embeddings from Mikolov et al. (2013).

Type classification is included in the overall ob-
jective in the following form:

Otyp(M) =
∑
m∈M
c∈Cm

qm(tc) · pm(c) (3)

Priors Another advantage of working with proba-
bility distributions over candidates is the easy inte-
gration of prior information. E.g., the word “Paris”
without further context has a strong prior on its
meaning as a city instead of a person. Our approach
utilizes prior information in form of frequency
statistics over candidate synsets for a mention’s sur-
face string. These priors are derived from annota-
tion frequencies provided by WordNet for Babel-
synsets containing the respective WordNet sense
and from occurrence frequencies in Wikipedia ex-
tracted by DBpedia Spotlight(Daiber et al., 2013) for
synsets containing only Wikipedia senses. Laplace-
smoothing is applied to all prior frequencies. This
prior is used to initialize the probability distribution
over candidate synsets. Note that the priors are used
“naturally”, i.e., as actual priors and not during con-
text based optimization itself.

Furthermore, because candidate priors for NE
mentions can be very high we add an additional
L2-regularization objective for NE mentions with
λ = 0.001, which we found to work best on de-
velopment data. Finally, named entities were fil-
tered out if they were included in another NE, had
no connection in the semantic interpretation graph
with another candidate sense of the input document
or were overlapping with another NE but were con-
nected worse.

2.4 Disambiguation of Verbs
The disambiguation of verbs requires an approach
that focuses more on the local context and especially
the usage of a verb within a sentence. Therefore, we
train a neural network based on semantic role label-
ing (SRL) and sentence words. Figure 1 illustrates
an example network. The input is composed of the
word embeddings (Turian et al., 2010) for each fea-
ture (word itself, its lemma, SRLs and bag of sen-
tence words). All individual input embeddings are

337



won

win

Obama

Prize

BoW

...

W
w

W
l

WA0

WA1

W b
ow

W c
1

Wc2

Wc3

W
c
m

Input
layer

Hidden
layer

Output
layer

Figure 1: Disambiguation neural network for “won”
in the sentence “Obama won the Nobel Prize.”

50-dimensional and connected to a 100-dimensional
hidden layer. The output layer consists of all can-
didate synsets of the verb. The individual output
weights Wc are candidate specific. To ensure bet-
ter generalization and to deal with the sparseness of
training corpora,Wc is defined as the following sum:

Wc = Ws(c) +
∑

sp∈Ps(c)
Wsp +

∑
se∈Es(c)

Wse , (4)

where s(c) is the respective synset of c, Ps is the set
of all hypernyms of s (transitive closure) and Es are
the synsets entailed by s. We used ClearNLP2(Choi,
2012) for extracting SRLs.

3 Results

The results of our system are shown in Table 1.
Our approaches to the disambiguation of English
nouns, named entities and verbs generally outper-
formed all other submissions across different do-
mains as well as the strong baseline provided by
the most-frequent-sense (MFS). This demonstrates
the system’s capability to adapt to different domains.
However, results on the math and computer domain
also reveal that performance strongly depends on
the document topic. The results for this domain are
worse compared to the other domains for almost all
participating systems, which may indicate that exist-
ing resources do not cover this domain as well as the
others. Another potential explanation is that enforc-
ing only pairwise coherence does not take the hidden

2http://clearnlp.wikispaces.com

bio math gen all
MFS 75.3 43.6 69.2 66.7

best other 76.5 51.4 63.7 64.8
DFKI 79.1 44.9 73.4 70.3

(a) Nouns
bio math gen all

MFS 98.9 57.1 77.4 85.7
best other 98.9 74.3 89.7 87.0

DFKI 100.0 57.1 90.3 88.9

(b) Named Entities
bio math gen all

MFS 52.5 55.7 61.4 55.1
best other 53.8 60.6 70.6 57.1

DFKI 58.3 52.3 66.7 57.7

(c) Verb

Table 1: F1 scores of our system, the best other sys-
tem and an MFS baseline on the disambiguation of
English nouns, named entities and verbs for all do-
mains of the SemEval 2015 task 13. bio- biomedi-
cal; math- math & computer; gen- general

topics computer and maths into account that connect
all concepts in the specific document. This might be
an interesting point for further research.

4 Conclusion

We have presented a robust approach for disam-
biguating nouns and named entities as well as a neu-
ral network for verb sense disambiguation that we
used in the SemEval 2015 task 13. Our system
achieved an overall F1 score of 70.3 for nouns, 88.9
for NEs and 57.7 for verbs across different domains,
outperforming all other submissions for these cate-
gories of English. The disambiguation of nouns and
named entities performs especially well compared
to other systems and can still be extended through
the introduction of additional, complementary ob-
jectives. Disambiguating verbs remains a very chal-
lenging task and the promising results of our model
still leave much room for improvement.

Acknowledgment

This research was partially supported by the
German Federal Ministry of Education and Re-
search (BMBF) through the projects Deepen-
dance (01IW11003), ALL SIDES (01IW14002) and

338



BBDC (01IS14013E) and by Google through a Fo-
cused Research Award granted in July 2013.

References
[Bunescu and Pasca2006] Razvan C Bunescu and Mar-

ius Pasca. 2006. Using encyclopedic knowledge for
named entity disambiguation. In EACL, volume 6,
pages 9–16.

[Choi2012] Jinho D Choi. 2012. Optimization of nat-
ural language processing components for robustness
and scalability.

[Daiber et al.2013] Joachim Daiber, Max Jakob, Chris
Hokamp, and Pablo N Mendes. 2013. Improving effi-
ciency and accuracy in multilingual entity extraction.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.
Wiley Online Library.

[Hestenes and Stiefel1952] Magnus Rudolph Hestenes
and Eduard Stiefel. 1952. Methods of conjugate gra-
dients for solving linear systems, volume 49. National
Bureau of Standards Washington, DC.

[Hoffart et al.2011] Johannes Hoffart, Mohamed Amir
Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred
Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater,
and Gerhard Weikum. 2011. Robust disambiguation
of named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782–792. Association for Compu-
tational Linguistics.

[Izquierdo-Beviá et al.2006] Rubén Izquierdo-Beviá,
Lorenza Moreno-Monteagudo, Borja Navarro, and
Armando Suárez. 2006. Spanish all-words semantic
class disambiguation using cast3lb corpus. In MICAI
2006: Advances in Artificial Intelligence, pages
879–888. Springer.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando CN Pereira. 2001. Conditional random
fields: Probabilistic models for segmenting and label-
ing sequence data.

[McCallum et al.2009] Andrew McCallum, Karl Schultz,
and Sameer Singh. 2009. FACTORIE: Probabilistic
programming via imperatively defined factor graphs.
In Neural Information Processing Systems (NIPS).

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and their
compositionality. In Advances in Neural Information
Processing Systems, pages 3111–3119.

[Miller et al.1993] George A Miller, Claudia Leacock,
Randee Tengi, and Ross T Bunker. 1993. A seman-
tic concordance. In Proceedings of the workshop on
Human Language Technology, pages 303–308. Asso-
ciation for Computational Linguistics.

[Moro and Navigli2015] Andrea Moro and Roberto Nav-
igli. 2015. SemEval-2015 Task 13: Multilingual All-
Words Sense Disambiguation and Entity Linking. In
Proc. of SemEval-2015.

[Moro et al.2014] Andrea Moro, Alessandro Raganato,
and Roberto Navigli. 2014. Entity linking meets word
sense disambiguation: A unified approach. Transac-
tions of the Association for Computational Linguistics,
2.

[Navigli and Ponzetto2012] Roberto Navigli and Si-
mone Paolo Ponzetto. 2012. Babelnet: The automatic
construction, evaluation and application of a wide-
coverage multilingual semantic network. Artificial
Intelligence, 193:217–250.

[Navigli2009] Roberto Navigli. 2009. Word sense dis-
ambiguation: A survey. ACM Computing Surveys
(CSUR), 41(2):10.

[Turian et al.2010] Joseph Turian, Lev Ratinov, and
Yoshua Bengio. 2010. Word representations: a simple
and general method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 384–394.
Association for Computational Linguistics.

[Zhong and Ng2010] Zhi Zhong and Hwee Tou Ng.
2010. It makes sense: A wide-coverage word sense
disambiguation system for free text. In Proceedings of
the ACL 2010 System Demonstrations, pages 78–83.
Association for Computational Linguistics.

339


