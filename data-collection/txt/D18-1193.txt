



















































SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1653–1663
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1653

SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain
Text-to-SQL Task

Tao Yu Michihiro Yasunaga Kai Yang Rui Zhang
Dongxu Wang Zifan Li Dragomir R. Radev
Department of Computer Science, Yale University

{tao.yu,michihiro.yasunaga,k.yang,r.zhang,dragomir.radev}@yale.edu

Abstract

Most existing studies in text-to-SQL tasks do
not require generating complex SQL queries
with multiple clauses or sub-queries, and gen-
eralizing to new, unseen databases. In this
paper we propose SyntaxSQLNet, a syntax
tree network to address the complex and cross-
domain text-to-SQL generation task. Syn-
taxSQLNet employs a SQL specific syntax
tree-based decoder with SQL generation path
history and table-aware column attention en-
coders. We evaluate SyntaxSQLNet on a
new large-scale text-to-SQL corpus containing
databases with multiple tables and complex
SQL queries containing multiple SQL clauses
and nested queries. We use a database split set-
ting where databases in the test set are unseen
during training. Experimental results show
that SyntaxSQLNet can handle a significantly
greater number of complex SQL examples
than prior work, outperforming the previous
state-of-the-art model by 9.5% in exact match-
ing accuracy. To our knowledge, we are the
first to study this complex text-to-SQL task.
Our task and models with the latest updates
are available at https://yale-lily.
github.io/seq2sql/spider.

1 Introduction

Text-to-SQL task is one of the most important sub-
task of semantic parsing in natural language pro-
cessing (NLP). It maps natural language sentences
to corresponding SQL queries.

In recent years, some state-of-the-art methods
with Seq2Seq encoder-decoder architectures are
able to obtain more than 80% exact matching ac-
curacy on some complex text-to-SQL benchmarks
such as ATIS and GeoQuery. These models seem
to have already solved most problems in this area.

However, as (Finegan-Dollak et al., 2018) show,
because of the problematic task definition in the
traditional datasets, most of these models just

SQL Review 
150 man-hours 

Question Review 
& Paraphrase 
150 man-hours 

Final Review 
& Processing 

100 hours 

avg

salary dept_name

NONE

SELECT

ROOT

HAVING

GROUP

>

dept_name

ROOT

OP

Action Modules

AGG max, min, avg, sum, count, none: 

OP =, <, >, >=, <=, !=,LIKE, NOT IN, BETWEEN
: 

IUEN

KW

: INTERSECT, UNION, EXCEPTION, NONE

: SELECT, WHERE, GROUP, ORDER, LIMIT

Pop

ROOT IUEN

NONE KW

IUE 2 sub-SQLs

KW COL

"GROUP" + COL HAVING

"ORDER" "SELECT"/ + COL"HAVING"/ AGG

"WHERE" "HAVING"/ + COL OP

OP ROOT/TERMINAL

"WHERE" + COL AND/OR

History

Current Token Module to call

"ORDER" + COL DESC/ASC/LIMIT

"GROUP"

Stack

IUEN IUEN

"SELECT"

"HAVING"/

Push predicted
token

COL a table column: 

Token Instances

What are the name and lowest instructor salary of
the departments with average salary greater than
the overall average?

Complex input

sentence:

name salary age dept_id dept_name 

Database:

instructorTable 1

departmentTable 2

......

Table n

name salary age dept_id dept_name 

Columns

ID name department_name salary ....

name building budget .......
primary key

foreign key

Correct SQL 

translation:

SELECT min(salary), department_name
FROM instructor 
GROUP BY department_name
HAVING avg(T1.salary) > 
     (SELECT avg(salary) FROM instructor)

Our tree-based

SQL generation: 

ROOT

SELECT

salary

avg

min none salary

Figure 1: To address the complex text-to-SQL gener-
ation task, SyntaxSQLNet employs a tree-based SQL
generator. For example, our model can systematically
generate a nested query as illustrated above.

learn to match semantic parsing results, rather than
truly learn to understand the meanings of inputs
and generalize to new programs and databases.
More specifically, most existing complex text-to-
SQL datasets have less than 500 SQL labels. They
are expanded by paraphrasing 4-10 questions for
each SQL query. Under the standard train and
test split (Zettlemoyer and Collins, 2005), most
queries in the test set also appear in the train
set. The WikiSQL dataset recently developed
by (Zhong et al., 2017) is much larger and does
use different databases for training and testing,
but it only contains very simple SQL queries and

https://yale-lily.github.io/seq2sql/spider
https://yale-lily.github.io/seq2sql/spider


1654

database schemas.
To address those issues in the current semantic

parsing datasets, Yu et al. (2018b) have developed
a large-scale human labeled text-to-SQL dataset
consisting of about 6,000 complex SQL queries
and 200 databases with multiple tables. This
dataset defines a new complex and cross-domain
text-to-SQL task that requires models to general-
ize well to both new SQL queries and databases.
The task cannot be solved easily without truly un-
derstanding the semantic meanings of the input
questions.

In this paper, we propose SyntaxSQLNet, a
SQL specific syntax tree network to address the
aforementioned task. Specifically, to generate
complex SQL queries with multiple clauses, selec-
tions and sub-queries, we develop a syntax tree-
based decoder with SQL generation path history.
To make our model learn to generalize to new
databases with new tables and columns, we also
develop a table-aware column encoder. Our con-
tributions are as follows:
• We propose SQL specific syntax tree networks

for the complex and cross-domain text-to-SQL
task, which is even able to solve nested queries
on new, unseen databases. We are the first to
develop a methodology for this challenging se-
mantic parsing task.
• We introduce a SQL specific syntax tree-based

decoder with SQL path history and table-aware
column attention encoders. Even with no hyper-
parameter tuning, our model can significantly
outperform the previous best models, with 4.8%
boost in exact matching accuracy. Error anal-
ysis shows that our model is able to general-
ize, and solve much more complex (e.g., nested)
queries in unseen databases than prior work.
• We also develop a cross-domain data augmen-

tation method to generate more diverse training
examples across databases, which further im-
proves the exact matching accuracy by 7.5%.
As a result, our model achieves 22.0% accuracy,
a 12.3% total improvement compared with the
previous best models.

2 Related Work

Semantic parsing maps natural language to formal
meaning representations. There are a range of rep-
resentations, such as logic forms and executable
programs (Zelle and Mooney, 1996; Zettlemoyer
and Collins, 2005; Wong and Mooney, 2007; Das

et al., 2010; Liang et al., 2011; Banarescu et al.,
2013; Artzi and Zettlemoyer, 2013; Reddy et al.,
2014; Berant and Liang, 2014; Pasupat and Liang,
2015).

As a sub-task of semantic parsing, the text-to-
SQL problem has been studied for decades (War-
ren and Pereira, 1982; Popescu et al., 2003a, 2004;
Li et al., 2006; Giordani and Moschitti, 2012;
Wang et al., 2017b). The methods proposed in
the database community (Li and Jagadish, 2014;
Yaghmazadeh et al., 2017) tend to involve hand
feature engineering and user interactions with the
systems. In this work, we focus on recent neu-
ral network-based approaches (Yin et al., 2016;
Zhong et al., 2017; Xu et al., 2017; Wang et al.,
2017a; Iyer et al., 2017). Dong and Lapata (2016)
introduce a sequence-to-sequence (seq2seq) ap-
proach to converting texts to logical forms. Most
previous work focuses on a specific table schema.
Zhong et al. (2017) publish the WikiSQL dataset
and propose a seq2seq model with reinforcement
learning to generate SQL queries. Xu et al. (2017)
further improve the results on the WikiSQL task
by using a SQL-sketch based approach employ-
ing a sequence-to-set model. Dong and Lap-
ata (2018) propose a coarse-to-fine model which
achieves the new state-of-the-art performances on
several datasets including WikiSQL. Their model
first generate a sketch of the target program. Then
the model fills in missing details in the sketch.

Our syntax tree-based decoder is related to re-
cent work that exploits syntax information for
code generation tasks (Yin and Neubig, 2017; Ra-
binovich et al., 2017). Yin and Neubig (2017)
introduce a neural model that transduces a nat-
ural language statement into an abstract syntax
tree (AST). While they format the generation pro-
cess as a seq2seq decoding of rules and tokens,
our model uses a module for each grammar com-
ponent, and calls them recursively to generate a
SQL syntax tree. Similarly, Rabinovich et al.
(2017) propose abstract syntax networks that use
a collection of recursive modules for decoding.
Our model differs from theirs in the following
points. First, we exploit a SQL specific grammar
instead of AST. AST-based models have to pre-
dict many non-terminal rules before predicting the
terminal tokens, involving more steps. Whereas,
our SQL-specific grammar enables direct predic-
tion of SQL tokens. Second, our model uses dif-
ferent sequence-to-set modules to avoid the “or-



1655

dering issue” (Xu et al., 2017) in many code gen-
eration tasks. Third, different from (Rabinovich
et al., 2017), we pass a pre-order traverse of SQL
decoding history to each module. This provides
each module with important dependence informa-
tion: e.g., if a SQL query has GROUP BY, it is
very likely that the grouped column have appeared
in SELECT too.

In addition to the distinction in model design,
our work differs from theirs in the data and task
definition. They aim to develop general syn-
tax model for code generation via abstract syn-
tax trees. Instead, we are interested in solving the
complex and cross-domain SQL query generation
problem; this motivates us to take advantage of
SQL specific syntax for decoding, which guides
systematic generation of complex SQL queries.

3 Problem Formulation

This work aims to tackle the complex text-to-SQL
task that involves multiple tables, SQL clauses and
nested queries. Further, we use separate databases
for training and testing, aiming to develop models
that generalize to new databases.

Dataset. We use Spider (Yu et al., 2018b) as
the main dataset, which contains 10,181 ques-
tions, 5,693 unique complex SQL queries, and 200
databases with multiple tables.

Task and Challenges.

• The dataset contains a large number of complex
SQL labels, which involve more tables, SQL
clauses, and nested queries than prior datasets
such as WikiSQL. Existing models developed
for the WikiSQL task cannot handle those com-
plex SQL queries in the Spider dataset.

• The dataset contains 200 databases (∼138 do-
mains), and different databases are used for
training and testing. Unlike most previous se-
mantic parsing tasks (e.g., ATIS), this task re-
quires models to generalize to new, unseen
databases.

In sum, we train and test models on different com-
plex SQL queries from different databases in this
task. This aims to ensure that models can make
the correct prediction only when they truly under-
stand the meaning of the questions under the given
database, rather than by mere memorization.

AGG max, min, avg, sum, count, none: 

OP =, <, >, >=, <=, !=,
LIKE, NOT IN, BETWEEN

: 

IUEN

KW

: INTERSECT, UNION, EXCEPT, NONE

: SELECT, WHERE, GROUP, ORDER

Pop

ROOT IUEN

NONE KW

IUE （2 sub-SQLs）

KW COL

"GROUP" + COL HAVING

"ORDER" "SELECT"/ + COL"HAVING"/ AGG

"WHERE" "HAVING"/ + COL OP

OP ROOT/TERMINAL

"WHERE" + COL AND/OR

History

Current Token Module to call

"ORDER" + COL DESC/ASC/LIMIT

"GROUP"

Stack

IUEN IUEN

"SELECT"

"HAVING"/

Push predicted
token

COL a table column: 

Token Instances

Figure 2: Our modules and SQL grammar used in de-
coding process. A round symbol represents a SQL to-
kens, a table column, etc. A square symbol indicates a
module that predicts the next SQL token from its cor-
responding token instances with the same color.

4 Methodology

Similar to (Rabinovich et al., 2017), our model
structures the decoder as a collection of recursive
modules. However, as we discussed in the re-
lated work section, we make use of a SQL specific
grammar to guide the decoding process, which al-
lows us to take advantage of SQL queries’ well-
defined structure. 1

4.1 Module Overview
Our model decomposes the SQL decoding process
into 9 modules to handle the prediction of differ-
ent SQL components such as keywords, operators,
and columns. We provide the overview in this sec-
tion and more details in later sections.

Figure 2 illustrates our modules and SQL gram-
mar used in decoding process. A round symbol
represents a SQL token, such as SELECT, WHERE,
a table column, etc. A square symbol indicates
a module that predicts the next SQL token from
its corresponding token instances with the same
color. Specifically, we have the following mod-
ules.

• IUEN Module, predicting INTERCEPT,
1Please check out our website for the latest updates

on the model at https://yale-lily.github.io/
seq2sql/spider

https://yale-lily.github.io/seq2sql/spider
https://yale-lily.github.io/seq2sql/spider


1656

UNION, EXCEPT, and NONE, which deter-
mines if we need to call itself again to gen-
erate nested queries.

• KW Module, predicting keywords from
WHERE, GROUP BY, and ORDER BY. All
queries in our dataset have SELECT.

• COL Module, predicting table columns.
• OP Module, for =, >, <, >=, <=, ! =,
LIKE, NOT IN, IN, BETWEEN.

• AGG Module, predicting aggregators from
MAX, MIN, SUM, COUNT, AVG, and NONE.

• Root/Terminal Module, predicting the ROOT
of a new subquery or terminal value. It also
enables our model to generate nested queries.

• AND/OR Module, predicting the presence of
AND or OR operator between two conditions.

• DESC/ASC/LIMIT Module, predicting the
keywords associated with ORDER BY. It is
invoked only when ORDER BY is predicted
before.

• HAVING Module, predicting the presence of
HAVING for GROUP BY clause. It is invoked
only when GROUP BY is predicted earlier.

4.2 SQL Grammar
In order to structure our decoder to generate com-
plex queries, we consider a SQL grammar. It
determines which module to be invoked at each
recursive decoding step. Figure 2 illustrates our
SQL grammar. During decoding process, given
the current SQL token and the SQL history (the
tokens we have gone over to reach the current to-
ken), we determine which module to invoke, and
predict the next SQL token to generate.

To invoke some modules such as HAVING and
OP during decoding, we not only check the type
of current token instance but also see whether
the type of the previously decoded SQL token
is GROUP for HAVING module, and WHERE or
HAVING for OP module.

In the grammar, IUEN and Root/Terminal mod-
ules are able to generate ROOT, which can activate
IUEN module again. In this way, our model can
recursively generate nested subqueries, and can
also predict two or more subqueries in queries that
have EXCEPT, INTERSECT, and UNION.

4.3 Input Encoder
Our inputs of each module consist of three types of
information: question, table schema, and current

SQL decoding history path. We encode a question
sentence by a bi-directional LSTM, BiLSTMQ.
We encode table schema and history path in the
manners described below.

4.3.1 Table-Aware Column Representation
In order to generalize to new databases in testing,
it is important to make our model learn to obtain
necessary information from a database schema.
While SQLNet (Xu et al., 2017) only needed
the column names as WikiSQL dataset only con-
tained one table per question-SQL pair, Spider’s
databases contain multiple tables. To address this
setting, we propose to use both table and column
names to construct column embeddings.

Specifically, given a database, we first obtain
embedding for each table by taking the average
embedding of the words constituting the table
name (e.g., for table name student id, we av-
erage the word embeddings for student and
id). Next, for each column, we obtain the ini-
tial column name embedding in the same man-
ner, and then concatenate the corresponding ta-
ble embedding, and the type information of the
column (string, or number, primary/foreign key)
in a way similar to (Yu et al., 2018a) to pro-
duce a column embedding. On top, we apply an-
other level of BiLSTM (BiLSTMCOL) that con-
nects all columns in the database, to obtain high-
level column embeddings. This way, our encoding
scheme can effectively capture both the global (ta-
ble names) and local (column names and types)
information in the database schema to understand
the question in the context of the given database.

Similarly to (Yu et al., 2018a), while the the
order of column names or table names does not
matter in practice, the use of BiLSTM performed
better than the direct use of input column embed-
dings.

4.3.2 SQL Decoding History
In addition to question and column information,
we also pass the SQL query’s current decoding
history as an input to each module. This enables us
to use the information of previous decoding states
to predict the next SQL token. For example, in
Figure 1, the COL module would be more likely
to predict salary in the subquery by consider-
ing the path history which contains salary for
HAVING, and SELECT in the main query.

In contract, each module in SQLNet does
not consider the previous decoded SQL history.



1657

Hence, if directly applied to our recursive SQL de-
coding steps, each module would just predict the
same output every time it is invoked. By passing
the SQL history, each module is able to predict
a different output according to the history every
time it is called during the recursive SQL genera-
tion process. Also, the SQL history can improve
the performance of each module on long and com-
plex queries because the history helps the model
capture the relations between clauses.

Predicted SQL history is used during test decod-
ing. For training, we first traverse each node in the
the gold query tree in pre-order to generate gold
SQL path history for each training example used
in different modules.

4.3.3 Attention for Input Encoding
For each module, like SQLNet, we apply the at-
tention mechanism to encode question representa-
tion. We also employs this technique on SQL path
history encoding. The specific formulas used are
described in the next section.

4.4 Module Details

Similarly to SQLNet (Xu et al., 2017), we em-
ploy a sketch-based approach for each module.
We apply a seq2set prediction framework intro-
duced by (Xu et al., 2017), to avoid the order is-
sue that happens in seq2seq based models for SQL
generation. For example, in Figure 1, SELECT
salary, dept name is the same as SELECT
dept name, salary. The traditional seq2seq
decoder generates each of them one by one in or-
der; hence the model could get penalized even if
the prediction and gold label are the same as sets.
To avoid this problem, SQLNet predicts them to-
gether in one step so that their order does not affect
the model’s training process. For instance, in Fig-
ure 1, our model invokes the COL module to pre-
dict salary and dept name and push to stack
at the same time. However, SQLNet trains each
modules independently, so no information passes
through each component.

We first describe how to compute the condi-
tional embedding H1/2 of an embedding H1 given
another embedding H2:

H1/2 = softmax(H1WH>2 )H1.
Here W is a trainable parameter. Moreover, we
get a probability distribution from a given score
matrix U by

P(U) = softmax (Vtanh(U)) ,

where V is a trainable parameter.
We denote the hidden states of LSTM on ques-

tion embeddings, path history, and columns em-
beddings as HQ, HHS, and HCOL respectively. In
addition, we denote the hidden states of LSTM
on multiple keywords embeddings and keywords
embeddings as HMKW and HKW respectively. Fi-
nally, we use W to denote trainable parameters
that are not shared between modules. The output
of each module is computed as follows:

IUEN Module In the IUEN module, since
only one of the multiple keywords from
{INTERCEPT,UNION,EXCEPT,NONE} will be
used, we compute the probabilities by

PIUEN = P
(
W1H

>
Q/MKW +W2H

>
HS/MKW +W3H

>
MKW

)
KW Module In the KW module, we first
predict the number of keywords in the SQL
query and then predict the keywords from
{SELECT,WHERE,GROUP BY,ORDER BY}.

P numKW = P
(
Wnum1 H

num
Q/KW

> +Wnum2 H
num
HS/KW

>
)

P valKW = P
(
Wval1 H

val
Q/KW

>
+Wval2 H

val
HS/KW

>
+Wval3 HKW

>
)

COL Module Similarly, in the COL module, we
first predict the number of columns in the SQL
query and then predict which ones to use.

P numCOL = P
(
Wnum1 H

num
Q/COL

> +Wnum2 H
num
HS/COL

>
)

P valCOL = P
(
Wval1 H

val
Q/COL

>
+Wval2 H

val
HS/COL

>
+Wval3 HCOL

>
)

OP Module In the OP module, for each pre-
dicted column from the COL module that is in
the WHERE clause, we first predict the num-
ber of operators on it then predict which op-
erators to use from {=, >, <, >=, <=, !=
, LIKE, NOTIN, IN, BETWEEN}. We use HCS
to denote the embedding of one of the predicted
columns from the COL module.

P numOP = P
(
Wnum1 H

num
Q/CS

> +Wnum2 H
num
HS/CS

> +Wnum3 HCS
>
)

P valOP = P
(
Wval1 H

val
Q/CS

>
+Wval2 H

val
HS/CS

>
+Wval3 HCS

>
)

AGG Module In the AGG module, for each pre-
dicted column from the COL module that is in the
SELECT clause, we first predict the number of ag-
gregators on it then predict which aggregators to
use from {MAX,MIN,SUM,COUNT,AVG,NONE}

P numAGG = P
(
Wnum1 H

num
Q/CS

> +Wnum2 H
num
HS/CS

> +Wnum3 HCS
>
)

P valAGG = P
(
Wval1 H

val
Q/CS

>
+Wval2 H

val
HS/CS

>
+Wval3 HCS

>
)

Root/Terminal Module To predict nested sub-
queries, we add a module to predict if there is a
new “ROOT” after an operator, which allows the



1658

model to decode queries recursively. For each pre-
dicted column from the COL module that is in
the WHERE clause, we first call OP module, and
then predict whether the next decoding step is a
“ROOT” node or a value terminal node by

PRT = P
(
W1H

>
Q/CS +W2H

>
HS/CS +W3H

>
CS

)
AND/OR Module For each condition column
predicted from the COL module with number big-
ger than 1, we predict from {AND,OR} by

PAO = P
(
W1H

>
Q +W2H

>
HS

)
DESC/ASC/LIMIT Module In the DAL mod-
ule, for each predicted column from the COL mod-
ule that is in the ORDER BY clause, we predict
from {DESC,ASC, DESC LIMIT,ASC LIMIT} by

PDAL = P
(
W1H

>
Q/CS +W2H

>
HS/CS +W3H

>
CS

)
HAVING Module In the HAVING module, for
each predicted column from the COL module that
is in the GROUP BY clause, we predict whether it
is in the HAVING clause by

PHAVING = P
(
W1H

>
Q/CS +W2H

>
HS/CS +W3H

>
CS

)
4.5 Recursive SQL Generation
The SQL generation process is a process of ac-
tivating different modules recursively. As illus-
trated in Figure 2, we employ a stack to organize
our decoding process. At each decoding step, we
pop one SQL token instance from the stack, and
invoke a module based on the grammar to predict
the next token instance, and then push the pre-
dicted instance into the stack. The decoding pro-
cess continues until the stack is empty.

More specifically, we initialize a stack with only
ROOT at the first decoding step. At the next step,
the stack pops ROOT. As illustrated in Figure 2,
ROOT actives the IUEN module to predict if there
is EXCEPT, INTERSECT or UNION. If so, there
are two subqueries to be generated in the next
step. If the model predicts NONE instead, it will
be pushed into the stack. The stack pops NONE at
next step. For example, in Figure 2, the current
popped token is SELECT, which is a instance of
keyword (KW) type. It calls the COL module to
predict a column name, which will be pushed to
the stack.

4.6 Data Augmentation
Even though Spider already has a significantly
larger number of complex queries than existing

datasets, the number of training examples for
some complex SQL components is still limited. A
widely used way is to conduct data augmentation
to generate more training examples automatically.
Many studies (Berant and Liang, 2014; Iyer et al.,
2017; Su and Yan, 2017) have shown that data
augmentation can bring significant improvement
in performance. In prior work, data augmentation
was typically performed within a single domain
dataset. We propose a cross-domain data augmen-
tation method to expand our training data for com-
plex queries. Cross-domain data augmentation is
more difficult than the in-domain setting because
question-program pairs tend to have domain spe-
cific words and phrases. To tackle this issue, we
first create a list of universal patterns for question-
SQL pairs, based on the human labeled pairs from
all the different training databases in Spider. To do
so, we use a script to remove (and later fill in) all
the table/column names and value tokens in the la-
beled question-SQL pairs, and then group together
the same SQL query patterns. Consequently, each
SQL query pattern has a list of about 5-20 corre-
sponding questions. In our task, we want to gen-
erate more complex training examples. Thus, we
filter out simple SQL query patterns by measur-
ing the length and the number of SQL keywords
used. We obtain about 280 different complex SQL
query patterns from the 4,000+ SQL labels in the
train set of our corpus. We then select the 50 most
frequent complex SQL patterns that contain multi-
ple SQL components and nested subqueries. After
this, we manually edit the selected SQL patterns
and their corresponding list of questions to make
sure that the table/column/value slots in the ques-
tions have one-to-one correspondence to the slots
in the corresponding SQL query. For each slot, we
also add column type or table information. Thus,
for example, columns with string type do not ap-
pear in the column slot with integer type during
data augmentation (i.e., slot refilling) process. In
this way, our question-SQL patterns are generated
based on existing human labeled examples, which
ensures that the generated training examples are
natural.

Once we have the one-to-one slot mapping be-
tween questions and SQL queries, we apply a
script that takes a new database schema with type
information and generates new question-SQL ex-
amples by filling empty slots. Specifically, for
each database in WikiSQL, we first randomly sam-



1659

ple 10 question-SQL patterns. We randomly sam-
ple columns from the database schema based on
its type: for example, if the slot type in the pat-
tern is “number”, and then we only sample from
columns with “real” type in the current database.
We then refill the slots in both the question and
SQL query with the selected column names. Sim-
ilarly, we also refill table/value slots.

By this data augmentation method, we finally
obtain about 98,000 question and SQL pairs using
about 18,000 WikiSQL databases with one single
table. Additionally, we also use the original Wik-
iSQL training dataset in experiments.

5 Experiments

Dataset In our experiments, we use Spider (Yu
et al., 2018b), the new large-scale human an-
notated text-to-SQL dataset with complex SQL
queries and cross-domain databases. In addition to
their originally annotated data, their training split
includes 752 queries and 1659 questions from six
existing datasets: Restaurants (Tang and Mooney,
2001; Popescu et al., 2003b), GeoQuery (Zelle and
Mooney, 1996), Scholar (Iyer et al., 2017), Aca-
demic (Li and Jagadish, 2014), Yelp and IMDB
(Yaghmazadeh et al., 2017). In total, this dataset
consists of 11,840 questions, 6,445 unique com-
plex SQL queries, and 206 databases with multi-
ple tables. We follow (Yu et al., 2018b), and use
130, 36, 40 databases for train, development, test,
respectively (randomly split).

5.1 Evaluation

We use evaluation metrics including SQL Compo-
nent matching and Exact matching. To compute
the component matching scores, we first decom-
pose predicted queries on SQL clauses including
SELECT, WHERE, GROUP BY, ORDER BY, and
KEYWORDS separately. After that, we evaluate
each predicted clause and the ground truth as bags
of several sub-components, and check whether or
not these two sets of components match exactly.
F1 scores are reported for each of the above 5
clauses. Exact matching score is 1 if the model
predicts all clauses correctly for a given example.

To better understand model performance on
different queries, (Yu et al., 2018b) divide SQL
queries into 4 levels: easy, medium, hard, extra
hard. The definition of difficulty is based on the
number of SQL components, selections, and con-
ditions. Queries that contain more SQL keywords

are considered harder.

5.2 Experimental Settings

Our model is implemented in PyTorch (Paszke
et al., 2017). We build each module based on the
TypeSQL (Yu et al., 2018a) implementation. We
use pre-trained GloVe (Pennington et al., 2014)
embeddings for all question, SQL history, and
schema tokens. All word embeddings are fixed.
For each experiment, the dimension and dropout
rate of all hidden layers is set to 120 and 0.3 re-
spectively. We use Adam (Kingma and Ba, 2015)
with the default hyperparameters for optimization,
with a batch size of 64. The same loss functions in
(Xu et al., 2017) are used.

• Training data: Spider (plus examples from
6 existing datasets) + WikiSQL + data aug-
mentation

• Model architecture: history path + table-
aware column encoding

We will conduct ablation studies to analyze the ef-
fect of each of the proposed techniques.

5.3 Baseline Models

To demonstrate the efficacy of our model in ad-
dressing the complex, cross-domain text-to-SQL
task, we compare the performance of our model
with several previous state-of-the-art models in the
text-to-SQL task. As the dataset and task defini-
tion used in this work are fundamentally different
from prior work using datasets such as GeoQuery,
WikiSQL, we adapted these models to our task in
the same way as (Yu et al., 2018b). Specifically:

Seq2Seq with Attention or Copying In order to
make the models aware of the table schema infor-
mation, Yu et al. (2018b) pass the models with a
vocabulary that contains SQL keywords and col-
umn names of the given database.

(Iyer et al., 2017) Iyer et al. (2017) apply an
attention based seq2seq model similar to (Luong
et al., 2015) to text-to-SQL tasks. Yu et al. (2018b)
adapt their model without user interaction to the
task.

SQLNet & TypeSQL Xu et al. (2017) intro-
duce SQLNet, which employs a column attention
mechanism and a sketch-based method to gener-
ates SQL queries as a slot-filling task. Yu et al.
(2018a) improves SQLNet by utilizing word types
extracted from a knowledge graph or table content



1660

Method Easy Medium Hard Extra Hard All
Seq2Seq 17.9% 2.7% 1.3% 0.6% 5.4%
Seq2Seq+Attention (Dong and Lapata, 2016) 17.9% 2.9% 1.8% 1.3% 5.7%
Seq2Seq+Copying 15.1% 3.4% 1.0% 1.3% 5.2%
Iyer et al. (2017) 7.9% 2.1% 1.3% 1.1% 3.1%
SQLNet (Xu et al., 2017) 23.7% 5.9% 2.3% 0.3% 8.3%
TypeSQL (Yu et al., 2018a) 29.6% 6.1% 2.3% 0.3% 9.7%
SyntaxSQLNet 43.1% 19.2% 17.8% 4.8% 22.0%
-augment 36.9% 16.7% 9.5% 1.6% 17.6%
-wikiSQL -augment 34.1% 11.2% 9.0% 2.2% 14.5%
-table -wikiSQL -augment 30.1% 7.3% 4.5% 1.9% 10.9%
-history -table -wikiSQL -augment 18.8% 4.5% 0.0% 0.0% 6.1%

Table 1: Exact match accuracy on SQL queries with different hardness levels.

Method SELECT WHERE GROUP BY ORDER BY KEYWORDS
Seq2Seq 13.7% 3.7% 3.2% 4.9% 8.9%
Seq2Seq+Attention 14.0% 5.0% 3.2% 6.1% 9.4%
Seq2Seq+Copying 12.0% 2.7% 5.2% 6.9% 6.7%
Iyer et al. (2017) 6.3% 1.9% 3.0% 3.6% 3.5%
SQLNet 24.0% 18.0% 11.8% 47.1% 61.9%
TypeSQL 36.2% 14.7% 6.4% 49.5% 59.4%
SyntaxSQLNet 48.2% 31.6% 28.9% 58.4% 68.9%
-augment 45.3% 29.2% 22.0% 47.3% 67.5%
-wikiSQL -augment 39.0% 20.6% 17.1% 50.2% 65.8%
-table -wikiSQL -augment 29.0% 15.0% 13.1% 45.5% 67.0%
-history -table -wikiSQL -augment 18.6% 10.5% 4.1% 30.0% 49.1%

Table 2: F1 scores of Component Matching on all SQL queries.

to help the model better understand entities and
numbers in natural language inputs. As they are
originally designed for WikiSQL, to conduct ex-
periments on Spider, Yu et al. (2018b) extend their
SELECT and WHERE modules to other SQL com-
ponents.

6 Results and Discussion

Table 1 presents our test results on the Spider
dataset with database splitting. Our model with
full history and data augmentation achieves 22.0%
exact matching on all SQL queries, which is a
12.3% absolute increase compared to the previous
best model, TypeSQL.

6.1 Comparison with Baselines

Even though our individual modules are similar to
SQLNet and TypeSQL, our syntax-aware decoder
allows the modules to incorporate the SQL decod-
ing history, which helps to achieve a significant
gain in exact matching for queries of all hardness
levels. Specifically, even without our data aug-
mentation technique, SyntaxSQLNet outperforms
the previous best, TypeSQL, by 5%. This result

suggests that the syntax and history information is
beneficial for this complex text-to-SQL task.

Moreover, the tree-based decoder enables Syn-
taxSQLNet to systematically generate nested
queries, boosting the performance for Hard/Extra
Hard. As Table 1 shows, SyntaxSQLNet achieves
particularly high scores 17.8% and 4.8% for Hard
and Extra Hard, which contain nested queries. The
Seq2Seq models suffer from generating ungram-
matical queries, yielding very low exact matching
accuracy on Hard and Extra Hard SQL queries. In
contrast, our model generates valid SQL queries
by enforcing the syntax.

For the detailed component matching results in
Table 2, our model consistently outperforms other
previous work by significant margins. Specifi-
cally, our model improve F1 score for most of the
SQL components by more than 10%.

6.2 Ablation Study

In order to understand the techniques that are re-
sponsible for the performance of our model, we
perform an ablation study where we remove one of
the proposed techniques from our model at a time.
The exact match scores are shown in the same ta-



1661

bles as other baselines.

6.2.1 Data Augmentation

Our model’s exact matching performance on all
queries drops 4.4% by excluding data augmen-
tation technique. This drop is particularly large
for GROUP BY and ORDER BY components (Ta-
ble 2), for which the original Spider dataset has
a relatively small number of training examples.
Thus, our cross-domain data augmentation tech-
nique significantly benefits the model performance
by extending to a much larger training dataset.

6.2.2 SQL Decoding History

In order to gain more insight into how our SQL
decoding history addresses complex SQL, we re-
port our model’s performance without SQL path
history. As shown in the Table 1, the model’s per-
formance drops about 4.8% on exacting matching
metric without the SQL history input. More im-
portantly, its performance on hard and extra hard
SQL queries decreases to 0%. This indicates that
our model is able to predict nested queries thanks
to the SQL decoding history.

6.2.3 Column Encoding

To see how our table-aware column encoding af-
fects performance of our model, we also report
the model’s result without using table information
for our column encoding. After excluding the ta-
ble embedding from column embeddings, the test
performance goes down by about 4%. This drop
is especially large for Medium/Hard SQL queries
and SELECT component, where the correct col-
umn prediction is a key. This result shows that the
table-aware encoding is very important to predict
the correct columns in unseen, complex dasebases
(with many foreign keys).

7 Conclusion

In this paper, we presented a syntax tree-based
model to address complex and cross-domain text-
to-SQL task. Utilizing a SQL specific syntax de-
coder, as well as SQL path history and table-aware
column attention encoders, our model outperforms
previous work by a significant margin. The ab-
lation study demonstrates that our proposed tech-
niques are able to predict nested, complex SQL
queries correctly even for unseen databases.

Acknowledgement

We thank Graham Neubig, Tianze Shi, and three
anonymous reviewers for their helpful feedback
and discussion on this work.

References

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion forComputational Linguistics.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse.

Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1415–1425, Baltimore, Maryland. Association for
Computational Linguistics.

Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In NAACL.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 731–742. Association for Computa-
tional Linguistics.

Catherine Finegan-Dollak, Jonathan K. Kummer-
feld, Li Zhang, Karthik Ramanathan Dhanalak-
shmi Ramanathan, Sesh Sadasivam, Rui Zhang, and
Dragomir Radev. 2018. Improving text-to-sql eval-
uation methodology. In ACL 2018. Association for
Computational Linguistics.

Alessandra Giordani and Alessandro Moschitti. 2012.
Translating questions to sql queries with genera-
tive parsers discriminatively reranked. In COLING
(Posters), pages 401–410.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
Jayant Krishnamurthy, and Luke Zettlemoyer. 2017.
Learning a neural semantic parser from user feed-
back. CoRR, abs/1704.08760.



1662

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. The 3rd Inter-
national Conference for Learning Representations,
San Diego.

Fei Li and HV Jagadish. 2014. Constructing an in-
teractive natural language interface for relational
databases. VLDB.

Yunyao Li, Huahai Yang, and HV Jagadish. 2006.
Constructing a generic natural language interface for
an xml database. In EDBT, volume 3896, pages
737–754. Springer.

P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590–599.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing.

Panupong Pasupat and Percy Liang. 2015. Compo-
sitional semantic parsing on semi-structured tables.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Nat-
ural Language Processing, ACL 2015, July 26-31,
2015, Beijing, China, Volume 1: Long Papers, pages
1470–1480.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
NIPS 2017 Workshop.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP, pages 1532–1543.
ACL.

Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
natural language interfaces to databases: Compos-
ing statistical parsing with semantic tractability. In
Proceedings of the 20th international conference on
Computational Linguistics, page 141. Association
for Computational Linguistics.

Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003a. Towards a theory of natural language in-
terfaces to databases. In Proceedings of the 8th in-
ternational conference on Intelligent user interfaces,
pages 149–157. ACM.

Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003b. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th Inter-
national Conference on Intelligent User Interfaces.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code generation
and semantic parsing. In ACL (1), pages 1139–1149.
Association for Computational Linguistics.

Siva Reddy, Mirella Lapata, and Mark Steedman. 2014.
Large-scale semantic parsing without question-
answer pairs. Transactions of the Association for
Computational Linguistics, 2:377–392.

Yu Su and Xifeng Yan. 2017. Cross-domain semantic
parsing via paraphrasing. CoRR, abs/1704.05974.

Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In ECML, vol-
ume 1, pages 466–477. Springer.

Chenglong Wang, Marc Brockschmidt, and Rishabh
Singh. 2017a. Pointing out sql queries from text.
Technical Report.

Chenglong Wang, Alvin Cheung, and Rastislav Bodik.
2017b. Synthesizing highly expressive sql queries
from input-output examples. In Proceedings of the
38th ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 452–
466. ACM.

David HD Warren and Fernando CN Pereira. 1982. An
efficient easily adaptable system for interpreting nat-
ural language queries. Computational Linguistics,
8(3-4):110–122.

Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007), Prague, Czech Re-
public.

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet:
Generating structured queries from natural language
without reinforcement learning. arXiv preprint
arXiv:1711.04436.

Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and
Thomas Dillig. 2017. Sqlizer: Query synthesis
from natural language. Proc. ACM Program. Lang.,
1(OOPSLA):63:1–63:26.

Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao.
2016. Neural enquirer: Learning to query tables in
natural language. In Proceedings of the Twenty-Fifth
International Joint Conference on Artificial Intelli-
gence, IJCAI 2016, New York, NY, USA, 9-15 July
2016, pages 2308–2314.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In ACL (1), pages 440–450. Association for Compu-
tational Linguistics.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018a. Typesql: Knowledge-
based type-aware neural text-to-sql generation. In



1663

Proceedings of NAACL. Association for Computa-
tional Linguistics.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018b. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-sql task. In
EMNLP.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, pages 1050–1055,
Portland, OR. AAAI Press/MIT Press.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. UAI.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language using reinforcement learning.
CoRR, abs/1709.00103.


