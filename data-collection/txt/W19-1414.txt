



















































The R2I_LIS Team Proposes Majority Vote for VarDial's MRC Task


Proceedings of VarDial, pages 138–143
Minneapolis, MN, June 7, 2019 c©2019 Association for Computational Linguistics

138

The R2I LIS Team Proposes Majority Vote for VarDial’s MRC Task

Adrian-Gabriel Chifu
Aix-Marseille Université

Université de Toulon
CNRS, LIS, Marseille, France

adrian.chifu@univ-amu.fr

Abstract

This article presents the model that generated
the runs submitted by the R2I LIS team to
the VarDial2019 evaluation campaign, more
particularly, to the binary classification by di-
alect sub-task of the Moldavian vs. Roma-
nian Cross-dialect Topic identification (MRC)
task. The team proposed a majority vote-
based model, between five supervised machine
learning models, trained on forty manually-
crafted features. One of the three submitted
runs was ranked second at the binary classifi-
cation sub-task, with a performance of 0.7963,
in terms of macro-F1 measure. The other
two runs were ranked third and fourth, respec-
tively.

1 Introduction

The term ”dialect” is used to capture two differ-
ent types of linguistic phenomena: a variety of a
language specific to a particular group of the lan-
guage’s speakers (Oxford Living Dictionaries) and
a socially subordinated language with respect to a
regional or national standard language, but not ac-
tually derived from the standard language (Maiden
and Parry, 2006). In the case of the latter usage,
the standard language it is not considered a ”di-
alect”, since it is the dominant language in state or
a region.

The dynamics and the characteristics of the
language variations are interesting for many re-
search disciplines, Computer Science included.
The Workshop on NLP for Similar Languages, Va-
rieties and Dialects (VarDial) represents a series of
workshops focused on studying diatopic language
variations from a computational perspective. The
first workshop was in 2014, co-located with the
COLING conference (Zampieri et al., 2014) and
VarDial2019 is co-located with the NAACL2019
conference (Zampieri et al., 2019).

Since 2017, evaluation campaigns are proposed
for the VarDial workshops. Four or five tasks are
proposed every year. One of the VarDial2019 eval-
uation campaign (Zampieri et al., 2019) tasks is
the Moldavian vs. Romanian Cross-dialect Topic
identification (MRC) closed training shared task.

The proposed approach tackles the first sub-task
of the MRC task (binary classification between di-
alects). We show how 40 simple features can be
effective for a simple supervised machine learning
architecture. The features are fed to five learning
models and a majority vote between the decisions
of the five classifiers is charged with the final de-
cision.

The motivation behind this approach is to prove
that simple features, thus faster to compute, are ef-
fective for the discrimination task. Another point
is that the majority vote helps improving the per-
formance and also stabilizes the model, making
it more robust, with respect to various train data
splits.

The rest of the article is structured as follows.
Section 2 positions the article with respect to Var-
Dial evaluation campaigns and presents the related
work that provided the data set for the evaluation
task. Section 3 describes the method, while Sec-
tion 4 presents the implementation details and the
experimental framework. In Section 5 the results
are presented and discussed. Section 6 concludes
the paper.

2 Related Work

Our research fits in the context of the VarDial eval-
uation campaigns. As for other evaluation cam-
paigns, the tasks evolve from one edition to an-
other. While some tasks are recurrent, others are
not re-conducted, leaving place for new ones.

The 2017 campaign (Zampieri et al., 2017) had
four tasks: Arabic Dialect Identification (ADI),



139

Cross-lingual Dependency Parcing (CLP), Dis-
criminating between Similar Languages (DSL)
and German Dialect Identification (GDI).

In 2018, the evaluation campaign (Zampieri
et al., 2018) had five tasks, the continuation of
the ADI and GDI tasks and the Morphosyntactic
Tagging of Tweets (MTT), the Discriminating be-
tween Dutch and Flemish in Subtitles (DFS) and
the Indo-Aryan Language Identification (ILI).

The latest evaluation campaign, VarDial2019
has also five shared tasks: the continuation of
the German Dialect Identification (GDI) task,
the Cross-lingual Morphological Analysis (CMA)
task, the Discriminating between Mainland and
Taiwan variation of Mandarin Chinese (DMT)
task, the Moldavian vs. Romanian Cross-
dialect Topic identification (MRC) task and the
Cuneiform Language Identification (CLI) task.

We participated at the MRC task, more specifi-
cally at the sub-task that focuses on the discrim-
ination between Romanian and Moldavian news
texts. To the best of our knowledge, there is
no related work for these specific dialects, ex-
cept for the MOROCO data set paper (Butnaru
and Ionescu, 2019), in which the authors describe
the collected corpus and present empirical stud-
ies on several classification tasks. Some exper-
iments using a shallow string kernels-based ap-
proach and a deep approach, based on character-
level CNNs with Squeeze-and-Excitation blocks
are conducted. The authors also present and an-
alyze the impact of the named entities. In the final
data set, the named entities are removed.

3 Method

The proposed method is based on forty manually-
crafted features that are fed to five supervised ma-
chine learning models for binary classification.
The final output represents a majority vote that de-
cides whether a text is written in Romanian or in
Moldavian (”RO/MD?”). The architecture of the
model is presented in Figure 1.

3.1 Features

The features we considered for this method are
handcrafted and meant to be straightforward, sim-
ple to understand, thus easy and fast to com-
pute. The forty features are of five types: to-
ken statistics-based, character-based, punctuation-
based, word-based and named entity-based fea-
tures, respectively. We believe that frequencies of

some characters, or words may be discriminant for
the classification between the two dialects.

3.1.1 Token Statistics Features
In order to compute these features, the text was
pre-processed. Sentences have been extracted,
based on the punctuation. In order to obtain to-
kens, the punctuation was removed and the re-
maining text was split by spaces. All tokens are
transformed to lowercase.

For a given text, we considered three such fea-
tures: the average number of tokens per sentence,
the total number of tokens and the average number
of characters of a token.

3.1.2 Character Features
The character-based features consider the text at
the character level. A character feature represents
the number of occurrences of the character in the
text. We took into account fifteen such features.
The considered characters are: all the vowels in
Romanian (a, e, i, o u, ă, â and ı̂) and some Ro-
manian consonants (b, c, d, m, n and ş and ţ). Ex-
cept for the ”ş” and ”ţ”, which are specific for the
Romanian language, the choice of the other con-
sonants was completely empirical.

Regarding the character ”ı̂”, we have considered
an extra feature that represents the number of oc-
currences of this character inside a token (not at
the beginning). For instance, the character occur-
rence in the word ”dı̂nsa” was counted, while the
occurrence in the word ”ı̂nceput” was not. This is
very specific to the Moldavian dialect, since in the
Romanian dialect, the character ”ı̂” does not gen-
erally appear (there are a few exceptions) in the
interior of words, being replaced by the character
”â”.

3.1.3 Punctuation Features
The punctuation features concern some punctua-
tion signs (space was also considered here). The
number of occurrences of a punctuation sign rep-
resents a the feature value. Five such features were
considered: space, dot, double quotes, exclama-
tion points and question marks.

3.1.4 Word Features
The word features take into account some words
that we considered as potentially discriminant,
such as prepositions, or dialect/regional words.
The number of occurrences of the selected words
represent the feature values. There are fifteen such
features: ”ci”, ”mai”, ”cu”, ”care”, ”la”, ”ı̂n”, ”o”



140

Figure 1: The architecture of the majority vote-based proposed model.

(the single character between two spaces), ”un”,
”de”, ”pe”, ”şi”, ”dı̂nsa”, ”dı̂nsul”, ”dı̂nşii” and
”dı̂nsele”.

3.1.5 Named Entity Feature
In the data set for the MRC task (Butnaru and
Ionescu, 2019), the name entities are identified and
replaced by ”$NE$”. We decided to take this in-
formation into account, thus the number of occur-
rences of ”$NE$” represent the feature value for
our named entity-based feature.

3.2 Models
The features are fed to five supervised machine
learning models. We have chosen the following
models:

• a KNN classifier (called KNN here);

• a Logistic Regression classifier (called LR
here);

• a SVM Classifier (called SVM here);

• a Neural Network classifier (called NN here);

• a Random Forest classifier (called RF here).

The hyperparameters of each model are pre-
sented later, in Section 4.2.

3.3 Majority Vote
The five models output their respective classifica-
tion decisions. The final decision is made by a
simple majority vote between the five aforemen-
tioned decisions. Having an odd number of votes

will not yield any ex aequo final decisions. For
instance, if three of the models decided in favor
of Romanian and two models decided in favor of
Moldavian, the final decision is Romanian.

4 Experiments

In this section we present the data set, the model
parameters, as well as the submitted runs with
their respective particularities.

4.1 Data Set

For the VarDial2019’s (Zampieri et al., 2019)
MRC task, the MOROCO data set (Butnaru and
Ionescu, 2019) is proposed. We focus on the data
provided for the binary classification sub-task, that
is to say the first sub-task, in which a classifica-
tion model is required to discriminate between the
Moldavian and the Romanian dialects.

The data set contains Moldavian (MD) and Ro-
manian (RO) samples of text collected from the
news domain. The training set (called ”train”)
contains 21719, the development set (called
”dev”) contains 11845 samples and the test set
(called ”test”) contains 5923 samples. A summary
of the data set, containing the class distribution is
presented in Table 1.

Since the training type for this task is a closed
one, only subsets of provided train data have been
used, without any external resources.

4.1.1 Environment
The proposed architecture was implemented in
python (version 3.7.2) and the five supervised



141

# samples Total RO MD
train 21719 11751 9968
dev 11845 6410 5435
test 5923 3205 2718

Table 1: MRC task data set summary.

machine learning models were implemented with
the sklearn library (version 0.20.2). The imple-
mented features have been scaled with a model
from sklearn (StandardScaler), based on training
features and then applied to both training and test-
ing feature sets.

4.2 Model Parameters

We describe here the sklearn hyperparameter set-
tings for each of the five supervised machine learn-
ing models used in the proposed architecture. We
mention that the hyperparameter settings were
chosen empirically. Most of the models are stan-
dard models with slim modifications. A more ro-
bust cross-validation is left as a perspective for the
future work.

KNN. Besides the default configuration, the
number of neighbors was set to five.

LR. Besides the default configuration, the ran-
dom state was set to zero, the solver was ”newton-
cg”, the maximum number of iterations was set to
one thousand and the multi-class parameter was
set to ”auto”.

SVM. Besides the default configuration, the
gamma parameter was set to ”scale”.

NN. Besides the default configuration, the
solver was set to ”adam”, the activation function
was set to ”tanh”, the maximum number of itera-
tions was set to one thousand, the alpha was set to
1e− 5, the size of the hidden layer was set to one
hundrend, the random state was set to one and the
warm start was set as ”True”.

RF. Besides the default configuration, the num-
ber of estimators was set to three hundred, the
maximum depth was set to two and the random
state was set to zero.

4.3 Runs

The MRC task allows three runs per sub-task. We
describe below the particularities of the three runs
that we submitted to the first sub-task.

Run1. For this run, the forty features are com-
puted as described in Section 3.1 and the training
data was represented by the ”train” and the ”dev”

texts, concatenated.
Run2. For this run, the forty features are com-

puted as described in Section 3.1 and the training
data was represented only by the ”train” texts.

Run3. For this run, the forty features are com-
puted as described in Section 3.1, with one mod-
ification: the character features, the punctuation
features and the word features were normalized
by dividing them by the total number of charac-
ters in the corresponding text. The training data
was represented by the ”train” and the ”dev” texts,
concatenated.

5 Results and Discussion

We present here the F1-score results and the con-
fusion matrices of the submitted runs. We dis-
cuss the results both with respect with train and
test data. Finally, we present the relative perfor-
mance of our runs that were ranked second, third
and fourth, with respect to the other participants to
the first sub-task of the MRC task.

5.1 F1-scores

The macro-averaged F1-score was the ranking cri-
terion for the MRC task. The values obtained by
the three submitted runs are presented in Table 2.
We present the performance both on train and on
test and with respect to the five models of the ar-
chitecture. The final decision majority vote (called
”Majority” here) performances are displayed on
the last line of the table. One can clearly notice
that the best performing run, Run3, obtains the
best performance in the case of the most models
(except for RF and test data of NN). Run1 only
gets the best performance on test data for the NN
model. Run2 has the best performance for the RF
model. However, overall, Run1 has a slightly bet-
ter performance (Majority on test: 0.7781) than
Run2 (Majority on test: 0.7762).

Run3 has the normalized features. Thus, as
expected, the normalized features are performing
better than the unnormalized features.

Run1 and Run3 are trained on the concatenated
”train” and ”dev” texts. Thus, as expected, the best
performances are achieved when training on the
most data possible.

Since there are not many differences in terms of
performance between train and test, we may hy-
pothesise that overfitting is not present. The only
exception is for the NN models, for the three runs,
where the absolute difference between train and



142

Runs Run1 Run2 Run3
Model/Split train test train test train test
KNN 0.8367 0.7551 0.8318 0.7467 0.8476 0.7732
LR 0.7248 0.7204 0.7272 0.7218 0.7327 0.7315
SVM 0.7884 0.7765 0.7857 0.7718 0.8305 0.8039
NN 0.8502 0.7889 0.8727 0.7646 0.8933 0.7821
RF 0.6896 0.6973 0.7078 0.7151 0.6994 0.7049
Majority 0.8092 0.7781 0.8117 0.7762 0.8379 0.7964

Table 2: The macro-averaged F1 score for the three runs, by split (train/test) and by model. The best values per
line, for train and test, respectively, are displayed in bold.

Model/Split
train test

predicted predicted
MD RO MD RO

KNN true MD 12861 2542 2072 646RO 2537 15624 690 2515

LR true MD 10070 5333 1805 913RO 3467 14694 652 2553

SVM true MD 11717 3686 1991 727RO 1889 16262 413 2792

NN true MD 13113 2290 2016 702RO 1247 16914 575 2630

RF true MD 7738 7665 1383 1335RO 1751 16410 297 2908

Majority true MD 11570 3833 1928 790RO 1484 16677 389 2816

Table 3: Confusion matrices for Run3, by method and by split (train/test).

test performance is of about 0.1.

5.2 Confusion Matrices

To focus on the best submitted run, we present the
confusion matrices for Run3 in Table 3. In this ta-
ble, we present the detailed confusion matrices for
each of the five models, as well as for the Majority,
both for train and test.

One can notice that the most balanced in terms
of false positives is KNN, while the most unbal-
anced seems to be RF. Even though the examples
in the data set are quite balanced with respect to
the number of samples per class, our model has
a tendency to predict much more texts for the la-
bel ”RO”. For instance, the false positives for the
”RO” class are more than twice as many as for the
”MD” class. This occurs for Majority, both for
train and test.

In Figure 2 we display the confusion matrices
for the test data, for the three runs. One can notice
that Run2 has the most false positives for the class
”MD”. On the other hand, Run1 has the most false

positives for the class ”RO”.

5.3 Other Participants

The ranking of all participants at the first sub-task
of MRC are presented in Table 4. One can notice
that the runner-up, our Run3 is at a significant dis-
tance from the winner (about 0.1 in absolute dif-
ference between the macro-averaged F1-scores).

6 Conclusion

We presented here the approach that generated the
three runs we submitted at VarDial’s MRC task,
most specifically at the first sub-task that aims to
discriminate news texts written in Moldavian and
Romanian dialects. The architecture is based on
forty features and a majority vote between five su-
pervised machine learning models. The submitted
runs ranked second, third and fourth, respectively.
We thus showed that a simple architecture, based
on features simple to compute can still be effective
and competitive.



143

Figure 2: The confusion matrices for the three submitted runs.

Rank Team Run Macro-F1 Weighted-F1 Micro-F1(accuracy)
1 DTeam 1 0.8950 0.8960 0.8965

2
R2I LIS
(Run3) 3 0.7964 0.7989 0.8009

3
R2I LIS
(Run1) 1 0.7781 0.7813 0.7849

4
R2I LIS
(Run2) 2 0.7762 0.7792 0.7820

5 tearsofjoy 1 0.7573 0.7592 0.7596
6 lonewolf 2 0.7354 0.7332 0.7381
7 SC-UPB 1 0.7088 0.7114 0.7121
8 lonewolf 1 0.6560 0.6646 0.6877
9 lonewolf 3 0.6077 0.5997 0.6319
10 SC-UPB 2 0.5081 0.5131 0.5156

Table 4: The ranking of all participants at the MRC’s first sub-task. The runs from this paper are shown in bold.

As future work, we plan to set up a more rigor-
ous cross-validation protocol for the hyperparam-
eter setup, in order to obtain more robust models.
Another lead is to apply a feature selection method
in order to identify the most useful features.

References
Andrei Butnaru and Radu Tudor Ionescu. 2019. MO-

ROCO: The Moldavian and Romanian Dialectal
Corpus. arXiv preprint arXiv:1901.06543.

Martin Maiden and Mair Parry. 2006. The dialects of
Italy. Routledge.

Marcos Zampieri, Shervin Malmasi, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, Jörg Tiedemann, Yves
Scherrer, and Noëmi Aepli. 2017. Findings of the
vardial evaluation campaign 2017. In Proceedings
of the Fourth Workshop on NLP for Similar Lan-
guages, Varieties and Dialects (VarDial), pages 1–
15, Valencia, Spain. Association for Computational
Linguistics.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Ahmed Ali, Suwon Shon, James Glass, Yves Scher-

rer, Tanja Samardžić, Nikola Ljubešić, Jörg Tiede-
mann, Chris van der Lee, Stefan Grondelaers,
Nelleke Oostdijk, Antal van den Bosch, Ritesh Ku-
mar, Bornini Lahiri, and Mayank Jain. 2018. Lan-
guage identification and morphosyntactic tagging:
The second VarDial evaluation campaign. In Pro-
ceedings of the Fifth Workshop on NLP for Similar
Languages, Varieties and Dialects (VarDial), Santa
Fe, USA.

Marcos Zampieri, Shervin Malmasi, Yves Scherrer,
Tanja Samardžić, Francis Tyers, Miikka Silfverberg,
Natalia Klyueva, Tung-Le Pan, Chu-Ren Huang,
Radu Tudor Ionescu, Andrei Butnaru, and Tommi
Jauhiainen. 2019. A Report on the Third VarDial
Evaluation Campaign. In Proceedings of the Sixth
Workshop on NLP for Similar Languages, Varieties
and Dialects (VarDial). Association for Computa-
tional Linguistics.

Marcos Zampieri, Liling Tan, Nikola Ljubešić, and
Jörg Tiedemann. 2014. A report on the dsl shared
task 2014. In Proceedings of the First Workshop on
Applying NLP Tools to Similar Languages, Varieties
and Dialects, pages 58–67, Dublin, Ireland. Associ-
ation for Computational Linguistics and Dublin City
University.

https://doi.org/10.3115/v1/W14-5307
https://doi.org/10.3115/v1/W14-5307

