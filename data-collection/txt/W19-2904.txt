



















































Surprisal and Interference Effects of Case Markers in Hindi Word Order


Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 30–42
Minneapolis, USA, June 7, 2019. c©2019 Association for Computational Linguistics

30

Surprisal and Interference Effects of Case Markers in Hindi Word Order

Sidharth Ranjan
IIT Delhi

sidharth.ranjan@cse.iitd.ac.in

Rajakrishnan Rajkumar
IISER Bhopal

rajak@iiserb.ac.in

Sumeet Agarwal
IIT Delhi

sumeet@iitd.ac.in

Abstract

Based on the Production-Distribution-
Comprehension (PDC) account of language
processing, we formulate two distinct hy-
potheses about case marking, word order
choices and processing in Hindi. Our first
hypothesis is that Hindi tends to optimize
for processing efficiency at both lexical and
syntactic levels. We quantify the role of
case markers in this process. For the task of
predicting the reference sentence occurring in
a corpus (amidst meaning-equivalent gram-
matical variants) using a machine learning
model, surprisal estimates from an artificial
version of the language (i.e., Hindi without
any case markers) result in lower prediction
accuracy compared to natural Hindi. Our
second hypothesis is that Hindi tends to
minimize interference due to case markers
while ordering preverbal constituents. We
show that Hindi tends to avoid placing next
to each other constituents whose heads are
marked by identical case inflections. Our
findings adhere to PDC assumptions and
we discuss their implications for language
production, learning and universals.

1 Introduction

Language universals encode distributional regular-
ities across languages of the world. This study
is motivated by the well known correlation be-
tween case marking and increased word order
flexibility (Sapir, 1921; Blake, 2001), often ex-
pressed as an implicational universal1. The ori-
gin of such universals has been the topic of a
long-standing debate in linguistics and cognitive
science (Fedzechkina et al., 2012). As the cited
work expounds, one view is that language uni-
versals emerged due to constraints specific to the

1“Given X in a particular language, we always find Y”
where X and Y are characteristics of the language (Green-
berg, 1963).

system of language and not related to the other
cognitive faculties (Chomsky, 1965; Fodor, 2001).
Another view is that languages evolved over time
as a consequence of cognitive mechanisms and
pressures linked with language use. Thus, cogni-
tive biases related to processing (Hawkins, 2004),
learnability (Christiansen and Chater, 2008) and
communicative efficiency (Jaeger and Tily, 2011)
have been proposed as underlying the systematic
similarities and divergences between natural lan-
guages.

The Production-Distribution-Comprehension
(PDC) account of language processing proposed
by MacDonald (2013) is an integrated theory
of language production and comprehension
that seeks to connect language production with
typology and comprehension. It is broadly in
the spirit of the second view regarding linguistic
universals described above and posits production
difficulty as the sole factor influencing linguistic
form. Hence it is an unconventional approach,
contrasting radically with alternative accounts
of language use in which language forms are
shaped by constraints on language acquisition
processes or considerations of facilitating lan-
guage comprehension for the listeners. Based on
PDC assumptions, we formulated two hypotheses
linking processing efficiency, case marking, and
word order choices at the level of individual
speakers (as opposed to the population level) in
Hindi, a language having predominantly SOV
word order. Hindi has a rich system of case
markers along with a relatively flexible word
order (Agnihotri, 2007; Kachru, 2006) and thus
adheres to the implicational universal stated at the
outset.

The PDC principle Easy First stipulates that
more accessible words are ordered before less ac-
cessible words. Accessibility of a word is influ-
enced by its ease of retrievability from memory.



31

Inspired by the stated PDC principle, our first hy-
pothesis is that Hindi tends to optimize for pro-
cessing efficiency at both lexical and syntactic lev-
els. We investigate the role of case markers in this
process by comparing the processing efficiency of
natural Hindi and an artificial version of Hindi
without case markers. Based on the PDC principle
of Reduce Interference, our second hypothesis is
that Hindi orders constituents such that phonolog-
ical interference caused by case marker repetition
is minimized. Interference is the idea that entities
with similar properties (like form, meaning, ani-
macy, concreteness and so forth) cause process-
ing difficulties when they occur in proximity. A
long line of research attests the role of interfer-
ence in both production (Bock, 1987; Jaeger et al.,
2012) and comprehension (Van Dyke and McEl-
ree, 2006; Van Dyke, 2007).

In order to test the stated hypotheses, we de-
ploy a machine learning model to predict the ref-
erence sentence occurring in the Hindi-Urdu Tree-
Bank (HUTB) corpus (Bhatt et al., 2009) of writ-
ten text2 (Example 1a below), amidst a set of artifi-
cially created grammatical variants expressing the
same proposition (Examples 1b-1c). Case markers
are shown in bold for illustration purposes.

(1) a. isse
this

pehle
before

jila
district

upabhokta
consumer

adalat
court

ne
ERG

28 April 1998
28 April 1998

ko
ON

apne
own

faisle
decision

mẽ
LOC

company
company

ko
DAT

nirdesh
direction

diyaa thaa
gave

ki...
COMPL

Earlier, the District Consumer Court had di-
rected the company in its decision on April 28,
1998 that ...

b. isse pehle jila upabhokta adalat ne apne faisle
mẽ 28 April 1998 ko company ko nirdesh
diyaa thaa ki...

c. jila upabhokta adalat ne isse pehle apne faisle
mẽ 28 April 1998 ko company ko nirdesh
diyaa thaa ki...

The variants above have two adjacent ko-
marked constituents, potentially causing interfer-
ence during production. So the PDC account
would not prefer these sentences on account of
production difficulty and instead prefer the refer-
ence sentence above. The possibility that speak-
ers chose the reference sentence above so that it
would facilitate comprehension for listeners (com-
pared to variant sentences which might be harder
to interpret) is not considered by the PDC account.

2We concede that the use of written data (due to the lack of
a publicly available Hindi speech corpus) is a major limitation
of our study.

We quantified processing efficiency using sur-
prisal, originally proposed as a measure of lan-
guage comprehension difficulty by Surprisal The-
ory (Hale, 2001; Levy, 2008). Consequently, we
introduced surprisal estimated from n-gram and
dependency parsing models into a logistic regres-
sion model for the task of predicting the refer-
ence sentence. Our choice of surprisal is inspired
by Levy and Gibson (2013), who point out that the
desiderata for PDC to become a theory of power-
ful empirical import is that it should make quan-
titative and localized predictions about incremen-
tal processing difficulty at each word. They high-
light the fact that such a theory already exists, viz.
the Surprisal Theory of language comprehension
mentioned above. A perusal of the literature on
information density in language production sug-
gests that surprisal is a reasonable choice to model
production difficulty as well.

Information density and surprisal are mathemat-
ically equivalent and both quantify the contex-
tual predictability of a linguistic unit. But sur-
prisal is based on different theoretical assumptions
about resource allocation in comprehension. Re-
cent research has demonstrated that reduction phe-
nomena at both lexical (Frank and Jaeger, 2008,
verb contraction) and syntactic (Jaeger, 2010, that-
complementizer choice) levels exhibit the drive to
minimize variation in information density across
the linguistic signal. Moreover, instances of the
same word which have greater predictability tend
to be spoken faster and with less emphasis on
acoustic details (Bell et al., 2009; Pluymaekers
et al., 2005). The work cited above uses lex-
ical frequencies or n-gram models over words
to estimate contextual predictability. More re-
cently, Demberg et al. (2012) showed that syntac-
tic surprisal estimated from a top-down incremen-
tal parser is positively correlated with the duration
of words in spontaneous speech, even in the pres-
ence of controls including word frequencies and
trigram lexical surprisal estimates. Crucial to our
study, words which are predictable in context have
been interpreted to be more accessible in recent
research (Arnold, 2011).

The results of our experiments show that ref-
erence sentences tend to minimize both trigram
and dependency parser surprisal in comparison to
their variants. Further, we show that the predic-
tion accuracies of surprisal estimates derived from
an artificially created version of Hindi without



32

any case markers are significantly worse than the
corresponding surprisal estimates based on nat-
ural Hindi. This experiment demonstrates the
crucial contribution of case markers towards the
predictive ability of surprisal and confirms our
first hypothesis. Subsequently, we demonstrate
that Hindi tends to avoid placing together con-
stituents whose heads are marked by the same case
marker. Moreover, incorporating predictors based
on adjacent case marker sequences in a statisti-
cal model significantly improves model prediction
accuracy over an extremely competitive baseline
provided by n-gram and dependency parser sur-
prisal. Phonological interference is a plausible
explanation for this phenomenon and lends cre-
dence to our second hypothesis. The Hindi sen-
tence comprehension literature provides only lim-
ited support for interference involving case marker
sequences (Vasishth, 2003). Hence, it is plausible
that this effect is a factor confined to the produc-
tion system and not related to considerations of
language comprehension. Further research using
spoken corpora and spontaneous production ex-
periments need to be performed in order to val-
idate the psychological reality of our findings.
Given that symbols used in the Hindi orthography
have a direct correspondence with the sounds of
the language (Vaid and Gupta, 2002), we expect
speech to behave similarly.

Our main contribution is that we broaden the
typological base of the PDC account of language
processing, leveraging its connection with the well
established surprisal theory of language compre-
hension. Levy and Gibson (2013) state that sur-
prisal would enable PDC to be implemented com-
putationally, thus facilitating hypothesis testing
on a wide range of linguistic phenomena cross-
linguistically. To this end, we set up a compu-
tational framework consisting of standard tools
and techniques from the field of Natural Lan-
guage Generation (NLG). Methodologically, the
task of referent sentence prediction is a relatively
novel way of studying word order and is inspired
from the surface realization component of NLG.
Recently, using a similar setup, Rajkumar et al.
(2016) showed the impact of dependency length
on English word order choices.

In this paper, Section 2 provides necessary
background and Section 3 provides details of our
data sets and models. Section 4 presents our exper-
iments and their results. Finally, Section 5 summa-

Marker Case (Gloss) Grammatical
Function

φ nominative (NOM) subject/object
ne ergative (ERG) subject
ko accusative (ACC) object

dative (DAT) subject/indirect object
se instrumental (INS) subject/oblique/adjunct

ka/ki/ke genitive (GEN) subject (infinitives)
specifier

mẽ/par/tak locative (LOC) oblique/adjunct

Table 1: Hindi case markers (Butt and King, 1996).

rizes the conclusions of our study and discusses
the implications of our results for language pro-
duction and learning.

2 Background

This section offers a brief background on Hindi
word order and case marking, surprisal and core
assumptions of the PDC account.

2.1 Hindi Word Order and Case Marking

A long line of work (Butt and King, 1996; Kid-
wai, 2000) has shown that scrambling in Hindi is
influenced by factors like discourse considerations
(topic, focus, background, and completive infor-
mation), semantics (definiteness and animacy),
and prosody (Patil et al., 2008). Hindi follows
the head-marking strategy where case markers are
postpositions which attach to noun phrases and en-
code a range of grammatical functions like subject
and object (see Table 1 and case markers in bold
in Examples 1a and 2a).

2.2 Surprisal Theory

The Surprisal Theory of language comprehension
posits that fine-grained probabilistic knowledge
(attained from prior linguistic experience) helps
comprehenders form expectations about interpre-
tations of the previously encountered structure as
well as upcoming material (Hale, 2001; Levy,
2008). The theory defines surprisal as a measure
of comprehension difficulty. In this work, we used
the following definitions of surprisal:

1. n-gram surprisal: Mathematically, n-gram
surprisal of the (i+1)th word, wi+1, based
on a traditional n-gram model is given by
Si+1 = − logP (wi+1|wi−n+2, ..., wi−1, wi),
as defined by Hale (2001). We estimated n-gram sur-
prisal via trigram models (n=3) over words trained on
1 million sentences from the EMILLE corpus (Baker
et al., 2002) using the SRILM toolkit (Stolcke, 2002)
with Good-Turing discounting.

2. Dependency parser surprisal was computed using
the probabilistic incremental dependency parser devel-
oped by Agrawal et al. (2017), based on the parallel-



33

processing variant of the arc-eager parsing strat-
egy (Nivre, 2008) proposed by Boston et al. (2011).
This parser maintains a set of the k most probable
parses at each word as it proceeds through the sentence.
The probability of a parser state is taken to be the prod-
uct of the probabilities of all transitions made to reach
that state. This parser can thus be used to define a mea-
sure of dependency parser surprisal: for the ith word
in a sentence, we first define the prefix probability αi
as the sum of probabilities of the k maintained parser
states at word i:

αi =
∑

top k derivations d leading to word i

Prob(d) (1)

The dependency parser surprisal at word i+1 is then
computed as:

Ssyni+1 = − log(αi+1/αi) (2)

The dependency parser surprisal of the (i+1)th word is
computed as the negative log-ratio of the sum of prob-
abilities of maintained parser states at word i+1 to the
same sum at word i. We estimated it using a corpus of
12,000 HUTB projective trees.

2.3 Production-Distribution-Comprehension
(PDC) Account

The Production component of the PDC account
posits three factors of production ease. 1. Easy
First: Relatively more accessible (ease of memory
retrieval and conceptual salience) or available ele-
ments are produced earlier in the structure. 2. Plan
Reuse: Speakers tend to repeat previously used
or mentioned structures due to syntactic priming.
3. Reduce Interference: Speakers tend to choose
words which do not interfere with other words in
the utterance plan. These factors compete with
each other during the production process to mould
language forms.

The Distribution component states that the dis-
tribution of structures in natural languages reflects
a bias towards having a greater number of struc-
tures which are easier to produce. Thus PDC at-
tributes the greater frequency of subject relative
clauses compared to object relatives across lan-
guages to production ease. Finally, the Com-
prehension part of the PDC approach proposes
that language comprehension reflects the statis-
tics of the input (i.e., production patterns) per-
ceived by language users. Thus, according to
PDC, the greater difficulty involved in compre-
hending object relative clauses compared to sub-
ject relatives (Gibson, 2000) is because of the
lower exposure to object relatives by virtue of their
lower frequency in the linguistic input to compre-
henders. Levy and Gibson (2013) puts forth the

idea that surprisal (estimated from corpora) is nat-
urally very compatible with the PDC assumption
described above. Maryellen MacDonald and col-
leagues validate PDC predictions using a series
of experiments related to relative clause produc-
tion and comprehension in many languages (Gen-
nari and MacDonald, 2008, 2009; Gennari et al.,
2012).

3 Data and Models

Our data set consists of 8736 reference sentences
corresponding to labeled, projective dependency
trees in the Hindi-Urdu TreeBank (HUTB) corpus
of written Hindi (Bhatt et al., 2009). We gener-
ated variants for each reference sentence by ran-
domly permuting the preverbal constituents of the
root node of its dependency tree. We selected
trees whose roots were verbs. For example, in
the tree depicted in Figure 1 (corresponding to Ex-
ample 1a), we reordered the preverbal constituents
immediately dominated by the verb diyaa and ob-
tained the variants shown in Examples 1b and 1c.
In order to eliminate ungrammatical variants, we
excluded variants containing dependency relation
sequences of the root word not present in the cor-
pus of HUTB gold standard trees. Dependency
relation sequences like k7t-k1, k1-k7t, k7t-k7 and
k7-k4 in Figure 1 simulate grammar rules used in
grammar-based surface realization systems. We
obtained 175801 variants after filtering.

In order to mitigate the imbalance between the
number of reference and variant sentences, we
transformed the data set using a technique de-
scribed in Joachims (2002). As per this technique,
a binary classification problem can be converted
into a pairwise ranking problem by training a clas-
sifier on the difference between the feature vectors
of a reference sentence and its syntactic choice
variants:

w · φ(Reference) > w · φ(V ariant) (3)

w · (φ(Reference)− φ(V ariant)) > 0 (4)

In Equation 3 above, the Reference data point
is predicted to outrank the V ariant data point
when the dot product of the feature vector of
the reference with w (learned feature weights) is
greater than the corresponding product of the vari-
ant. The same can be written (Equation 4) as
the dot product of w with the feature vector dif-
ference being positive. We created ordered pairs



34

ROOT

diyaa

 main

isse

 k7t

adaalat

 k1

1998

 k7t

faesle

 k7

company

 k4

nirdesh

 pof

tha

 lwg__vaux

ki

 k2

.

 rsym

pahle

 lwg__psp

jila

 pof__cn

upbhoktaa

 pof__cn

ne

 lwg__psp

28

 pof__cn

april

 pof__cn

ko

 lwg__psp

apne

 r6

me

 lwg__psp

ko

 lwg__psp

...

 ccof

Figure 1: Example HUTB dependency tree (Table 8 in the Appendix provides a glossary of dependency relations)

consisting of the feature vectors of reference and
variant sentences. Every reference sentence in the
data set was paired with each of its variants (Ex-
amples 1a-1b and Examples 1c-1a constitute two
such pairs). Then the feature values of the first
member of the pair were subtracted from the cor-
responding values of the second member. Pairs al-
ternate between reference-variant (coded as “1”)
and variant-reference (coded as “0”), resulting in
a data set consisting of an equal number of clas-
sification labels of each kind (see Appendix for a
detailed illustration).

4 Experiments

In this section, we describe three experiments to
test our hypotheses on the transformed version of
our data set consisting of 175801 data points us-
ing a logistic regression model. The goal is to
predict “1” and “0” labels (as described in the
previous section) using a set of cognitively mo-
tivated features. We calculated lexical and de-
pendency parser surprisal feature values over en-
tire sentences by summing the log probabilities of
the surprisal values of individual words. We car-
ried out 27-fold cross-validation; for each run, a
model trained on 26 folds (consisting of 1 fold for
hyperparameter tuning) was used to generate pre-
dictions about the remaining fold (100 training it-
erations using lbfgs solver in python scikit-learn
toolkit-v0.16.1).

4.1 Processing Efficiency Experiments

Here, we test the hypothesis that word order
choices in language are optimized for process-
ing efficiency by incorporating trigram and depen-
dency parser surprisal as predictors in a logistic re-

43.81
47.03

0

25

50

75

100

125

Reference Variants

N
gr

am
 s

ur
pr

is
al

Figure 2: Mean trigram surprisal per sentence of ref-
erence and variant sentences (95% confidence intervals
indicated)

gression model. A negative regression coefficient
for these predictors would imply that corpus sen-
tences have lower surprisal than variants. For the
entire corpus, Figure 2 indicates this trend, where
the mean trigram surprisal per sentence of the cor-
pus of reference sentences is lower than the corre-
sponding value of all their variants (Figure 4 in the
Appendix depicts the same trend for syntactic sur-
prisal). For the task of predicting HUTB reference
sentences, both our surprisal measures have a neg-
ative regression coefficient, individually as well as
in combination (first three rows of Table 2). This
confirms our hypothesis that word order choices
optimize for processing efficiency. Given our in-
terpretation of low surprisal as denoting ease of
accessibility, our first experiment shows that Hindi



35

Predictor(s) Accuracy% Weight(s)
Hindi
Parser surprisal 62.10 -0.43
Trigram surprisal 89.96 -0.81
Trigram + parser surprisal 90.14 -0.98, -0.43
Caseless Hindi
Caseless parser surprisal 55.03 -0.29
Caseless trigram surprisal 87.73 -0.83
Caseless trigram + parser surprisal 87.81 -0.93, -0.27

Table 2: Classification accuracies of surprisal for natural and
caseless Hindi (175801 data points)

speakers tend to produce sentences by ordering
preverbal constituents such that more accessible
elements are realized first compared to other com-
peting grammatical variants. This is in line with
the PDC Easy First principle. Further, the classi-
fication accuracies indicate that trigram surprisal
estimated from the EMILLE corpus is very effec-
tive in modelling syntactic choice (89.96% accu-
racy). For the same task, Ranjan (2015) reported
that trigram surprisal estimated from the HUTB it-
self (smaller quantity of in-domain data) resulted
in a lower accuracy of around 85%. In this context,
our results show that a bigger n-gram training set
can overcome the limitation of being from a dif-
ferent domain. A qualitative exploration revealed
that n-gram model surprisal was particularly ef-
fective in reference-variant pairs as shown below
(case markers shown in bold):

(2) a. Paakistan ne
Pakistan ERG

brihaspativaar ko
Thursday at

kathit taur par
allegedly

apne yahaan nirmit
indigenous

paramanu
nuclear

hathiyaar
weapons

dhone me
capable LOC

saksam
carrying

krooj
cruise

missile ka
missile GEN

pareekshan kiyaa hai.
tested

Pakistan has allegedly tested an indigenous
cruise missile capable of carrying nuclear
weapons on Thursday.

b. brihaspativaar ko Pakistan ne kathit taur par
apne ...

The reference sentence (Example 2a with tri-
gram surprisal of 45.60 hartleys) has the ergative-
accusative (ne-ko) ordering of case-marked nouns
compared to the variant (Example 2b with higher
trigram surprisal 47.12) having the opposite or-
dering of nouns. Overall, 6% of a total of 175
HUTB sentences having ergative and accusative
case markers exhibit a non-canonical accusative-
ergative order (Agrawal et al., 2017). In both
sentences above, the case markers in questions
are separated by a single word and hence form
part of a single trigram. Thus trigram surprisal
is able to model the dominant order successfully

while dispreferring the opposite order seen in Ex-
ample 2b. Moreover, dependency parser surprisal
has much lower classification accuracy compared
to trigram surprisal and has a very negligible im-
pact on performance on top of trigram surprisal.
Thus, surprisal estimates from an incremental de-
pendency parser are not effective in modelling
constituent order choices. This is slightly unex-
pected as Agrawal et al. (2017) showed that sur-
prisal estimates derived via the dependency parser
deployed in our work accounts for per-word read-
ing times for the Potsdam-Allahabad corpus over
and above bigram frequencies. Using a similar
setup, Rajkumar et al. (2016) showed that for the
task of predicting English syntactic choice alterna-
tions, PCFG surprisal performed significantly bet-
ter than n-gram model surprisal and the impact
of dependency length is over and above both the
aforementioned surprisal predictors. We are in the
process of creating a constituency structure tree-
bank for Hindi and plan to experiment with sur-
prisal derived from a constituent-structure parser
very soon. In recently completed work, Ranjan
et al. (In Preparation) show that for Hindi, de-
pendency length exhibits a weak effect over and
above surprisal for predicting corpus sentences
amidst artificial variants. Finally, we examined
1022 reference-variant pairs in our dataset where
none of our features was able to predict the ref-
erence sentence correctly. We isolated cases in-
volving other factors like given-new orders (30%
cases), focus or topic considerations (marked by
hi or to markers constituting 10% of cases) and
null subjects (7.5%). Such discourse considera-
tions are not encoded in our surprisal estimates
(confined to single sentences) and further research
can incorporate information about sentences from
the preceding context into surprisal estimates.

Note that when considering the relationship
between communicative efficiency and word or-
der choices, there is a potential ‘levels’ prob-
lem (Levy, 2018). At the level of evolutionary
timescales and entire populations, one might ex-
pect the grammar or distributional properties of
the language to be adapted for efficiency. But
at the level of an individual speaker’s produc-
tion choices, certain measures of efficiency will in
turn depend on the extant distribution of linguistic
forms. So there is a potential circularity in trying
to assess the validity of such measures. Here we
seek to model only the lower of these levels, i.e.,



36

individual choices over a human lifetime. Hence,
all the non-corpus variants we consider are gram-
matical. We assume that the grammar of the lan-
guage is held fixed, and within the set of possible
word order variants of a sentence licensed by that
extant grammar, seek to model why speakers may
have a greater propensity to produce some variants
over others.

4.2 Case Markers and Processing Efficiency

In order to quantify the exact contribution of
Hindi case markers towards the predictive accu-
racy of syntactic and trigram surprisal, we per-
formed similar experiments using an artificial ver-
sion of the language (i.e., Hindi without case
markers). The sentence comprehension literature
demonstrates the vital role of case markers in pre-
dicting the final verb in verb-final constructions of
languages like German (Levy and Keller, 2013)
and Japanese (Grissom II et al., 2016). Moreover,
in recent years, deploying artificial languages to
test hypotheses about language processing and
learning has been in vogue in both connection-
ist modelling (Lupyan and Christiansen, 2002;
Everbroeck, 2003) as well as behavioural exper-
iments (Kurumada and Jaeger, 2015; Fedzechkina
et al., 2017). Inspired by the cited works, we cre-
ated a caseless version of Hindi by removing case
markers (those listed in Table 1) from both refer-
ence and variant sentences. The caseless equiva-
lents of Examples 2a and 2b discussed in the pre-
vious section are given below:

(3) a. Pakistan brihaspativaar kathit taur apne ya-
haan nirmit paramanu hathiyaar dhone mein
saksam krooj missile pareekshan kiyaa hai.

b. brihaspativaar Pakistan kathit taur apne ...

Then we estimated surprisal by stripping off case
markers from the EMILLE corpus (trigram sur-
prisal) as well as HUTB trees (dependency parser
surprisal) so that our surprisal estimates mirrored
the patterns in the caseless version of the language
faithfully. Both surprisal measures derived from
the caseless version of Hindi perform significantly
worse than natural Hindi (last three rows of Ta-
ble 2). Caseless trigram surprisal does 2% worse,
while there is a 7% dip in the performance of case-
less dependency parser surprisal (McNemar’s two-
tailed significance p < 0.001 for both measures).
Thus the caseless language model is not able to
predict the reference sentence shown in Exam-
ple 3a as it awards higher trigram surprisal (45.21),

in comparison to the variant sentence in Exam-
ple 3b, which has a lower surprisal value (43.74).
Figure 3 in the Appendix depicts the lexical sur-
prisal profiles for the examples discussed above
(both regular Hindi and caseless equivalents). De-
pendency parser surprisal also exhibited the same
predictions.

Removing any kinds of words (especially func-
tion words) will result in a decrease in prediction
accuracy. So we compared the prediction accu-
racy of caseless surprisal with another baseline
obtained by removing case markers and all other
postpositions (e.g. ke liye, ke dwara) from both
training and test data. Surprisal estimates derived
from the case marker and postposition stripped
version of Hindi resulted in an extra dip of 0.3%
in the accuracy of trigram surprisal and 2.5% for
dependency parser surprisal compared to surprisal
obtained by stripping just the case markers. Thus
even within the set of postpositions, case markers
play a significant role in lexical and syntactic pre-
dictability and hence processing efficiency. Lack
of case markers reduces the overall information
content of a sentence for both speaker and hearer.
Spontaneous production experiments showed that
Japanese speakers tend to omit the optional marker
-o when the meaning of the sentence is probable
in a given context (Kurumada and Jaeger, 2015).
However, in cases where the meaning is not plau-
sible, speakers tend to mention the case marker, in
spite of entailing greater production effort.

The work of Lupyan and Christiansen (2002)
showed that for artificial SOV languages with no
case marking, a sequential learning device (Sim-
ple Recurrent Network) failed to achieve high ac-
curacy for the task of mapping words to grammati-
cal roles. Their simulations suggest that verb-final
languages need a case system for optimal learning
as word order is not a reliable cue for grammati-
cal function assignment. Using the miniature ar-
tificial language learning paradigm, Fedzechkina
et al. (2017) conducted a study where two groups
of adult learners were exposed to artificial lan-
guages with optional case marking (one fixed or-
der and one flexible order). Learners of the flexi-
ble constituent order language produced more case
markers than learners of the fixed order language,
mirroring typological patterns.

4.2.1 Interference Experiments
In the light of the PDC principle of Minimize
Interference, we investigate whether interference



37

Predictor Sequence Distance
name
φ-ne 1 3
ne-ko 1 3
ko-mẽ 1 2
mẽ-ko 1 1
same-seq 0
diff-seq 4

Table 3: Values of case
features extracted from tree
in Figure 1.

Case marker Weight
sequence
φ-φ -0.002
ke-ke -0.025
ko-ko -0.291
mẽ-mẽ -0.061
tak-tak 0.008
par-par 0.231
se-se 0.055
same-seq -0.009
diff-seq 0.009

Table 4: Learned weights
of some case-sequence pre-
dictors.

between NPs whose heads are marked by the
same case marker influence preverbal constituent
ordering choices in Hindi. Since PDC seeks
to link production and comprehension, our ex-
periments are also motivated by prior work on
case marker interference in sentence compre-
hension in SOV languages like Japanese (Lewis
and Nakayama, 2001), Korean (Lee et al., 2005)
and Hindi (Vasishth, 2003). Our work is di-
rectly related to the experiments on identical
case marking described in Chapter 3 of Vasishth
(2003). In the case of Hindi center-embeddings,
this work examined whether NPs having nominal
heads marked by identical case markers induce
similarity-based interference effects at the subse-
quent verb as predicted by the Retrieval Interfer-
ence Theory (Lewis, 1998; Lewis and Nakayama,
2001). The study shows limited support for in-
terference emanating from phonologically similar
case markers.

In order to investigate interference caused by
case markers in syntactic choice, we designed fea-
tures based on case markers and incorporated them
into our logistic regression model. For each de-
pendency tree, we introduced two types of features
associated with preverbal constituents of the root
verb. 1. Case-sequence features: Counts of case
marker sequences associated with the heads of a
pair of adjacent constituents. We also introduced
generic case-sequence features same-seq and diff-
seq to model the overall trend. For each tree, these
features denote the total number of identical and
different case markers sequences associated with
pairs of adjacent constituents. 2. Case-distance
features: Number of intervening words between
heads of the constituents of root verbs. Here, the
feature name is obtained by combining the case
markers associated with the constituent heads in
question. Constituents which do not have case
marked heads are marked as φ in order to model

Predictor(s) Classification Ranking
accuracy% accuracy%

Case distance features 70.79
Case sequence features 74.94
Random Classifier 21.25
Baseline (trigram+parser surprisal) 90.16 55.04
Baseline+Case distance features 90.85*** 55.68***
Baseline+Case sequence features 91.13*** 56.03***
Baseline+Case distance + sequence features 91.60*** 56.16***

Table 5: Pairwise classification and ranking accuracy (***
denotes McNemar’s two-tailed significance p < 0.001 over
the baseline model).

the fact that languages often use adverbial ele-
ments or other non-case marked arguments to sep-
arate case marked constituents. Table 3 illustrates
our case features based on the dependency tree in
Figure 1 corresponding to Example 1a.

In isolation, the case-sequence and case-
distance features exhibit accuracies around 70%
(second column of Table 5). The case sequence
and distance features together induce a significant
accuracy increase of 1.5% (McNemar’s two-tailed
significance p < 0.001) over a baseline model
consisting of lexical and dependency parser sur-
prisal as features. Though this might be a small
increase when considered in isolation, we would
like to note that our baseline model is extremely
competitive (90.16% accuracy). Even dependency
parser surprisal did not confer considerable per-
formance gains over and above trigram surprisal
as discussed earlier. So in this context, the contri-
bution of case features is noteworthy.

Subsequently, we examined the learned weights
of our case sequence features (Table 4) in our
best model containing surprisal and all the case
marker features. A negative weight is associated
with four of the seven identical case marker se-
quences as well as the same-seq feature encoding
the overall pattern across all case markers. These
negative weights lend support to our hypothesis
that Hindi shows a dispreference for placing to-
gether constituents whose heads are marked us-
ing the same case inflection. Interference due to
repetition of phonologically identical case mark-
ers may be a plausible explanation for this phe-
nomenon. However, three other case marker se-
quences have a positive weight and hence indicate
a tendency towards adjacency. These three case
markers are much lower in frequency in the HUTB
compared to the other four and might not represent
the dominant tendency. However, future inquiries
need to explore the role of case-based facilita-



38

tion (Logačev and Vasishth, 2012). Since our fea-
tures are not sensitive to clause boundaries, con-
clusive evidence for phonological interference will
emerge only after controlling for clause bound-
aries.

The best model (baseline + case marker fea-
tures) picked the reference sentence (Example 1a)
while the baseline model erroneously selected
the artificially generated variants (Examples 1b
and 1c). The reference sentence has two ko-
marked constituents separated by intervening con-
stituents. In contrast, the variant sentences have
two adjacent ko-marked constituents, potentially
causing interference. These examples also high-
light the ambiguous nature of the ko-marker in
denoting several functions in Hindi. As noted
by Ahmed (2006), ko marks both accusative and
dative case on objects (company in the cited ex-
amples) as well as dative subjects. In addition, it
also occurs on spatial and temporal adjuncts (as
in 28 April 1998). In these examples, since ko
marks both dative case and temporality, interfer-
ence might be purely phonological in nature and
not related to the actual grammatical function be-
ing marked. Further, we calculated the ranking ac-
curacy of our main models, i.e., the percentage of
times a model ranked the reference sentence com-
pared to all its variants. Table 5 (column 3) in-
dicates that introducing case marker features into
the baseline model induced significant ranking ac-
curacy gains (McNemar’s two-tailed significance
p < 0.001). So our best model ranked Example 1a
as the best sentence among all the other variants.
Our classification and ranking results suggest that
the PDC Reduce Interference principle of produc-
tion ease is a valid constraint in constituent order-
ing.

In Hindi sentence comprehension, Vasishth
(2003) explored the idea of Positional similar-
ity (Lewis and Nakayama, 2001), whereby the po-
sition of otherwise syntactically indiscriminable
NPs in the structure contribute to interference at
the subsequent verb. So he compared reading
times at the innermost verb in the sequences of
constituents with heads marked by ne-se-ko-ko
and ne-ko-se-ko inflections. However, there was
no significant difference in reading times between
these sequences, thus offering no support for po-
sitional similarity during comprehension. This is
the experimental condition which is most closely
linked to our work. Interpreted in conjunction with

our findings, case marker interference in Hindi ap-
pears to be a constraint on production rather than
comprehension.

5 Discussion

Our main findings are broadly in line with two of
the production ease principles of the PDC account.
Our first experiment shows that the Hindi lan-
guage orders words to optimize production ease
(quantified using surprisal) at both lexical and syn-
tactic levels, consistent with the PDC Easy First
principle. Our second experiment shows that case
markers make a significant contribution towards
the predictive accuracy of both syntactic and tri-
gram surprisal in choosing the reference sentence
amongst grammatical variants denoting the same
meaning. The role of surprisal and case mark-
ers in conferring accessibility needs to be inves-
tigated more thoroughly in future work. Finally,
our third experiment shows that Hindi tends to
disprefer constituent sequences with heads case
marked by identical case markers, as predicted by
the PDC principle of Reduce Interference. How-
ever, the lack of case marker interference in Hindi
comprehension necessitates further inquiries into
the PDC account, which conceives the lexico-
syntactic statistics of production data (result of
biases in utterance planning) as guiding compre-
hension processes. Thus, overall, we would like
to conclude that certain aspects of PDC are vali-
dated by our experimental results. Further com-
putational inquiries will be facilitated by formu-
lating an algorithmic sketch of a process model
outlining the causes of mismatches between pro-
duction and comprehension. Finally, the PDC ac-
count conceives word order variation in languages
of the world as emerging from an interplay of the
three PDC production principles. Crucially, PDC
conceives learning biases to be production biases,
i.e., speakers learn forms which are easier to pro-
duce (MacDonald, 2013). Future inquiries can ex-
plore whether learning outcomes are indeed con-
sistent with typological patterns described by lan-
guage universals.

6 Acknowledgements

We are grateful to the anonymous reviewers of this
workshop, NAACL-2019 and Sigmorphon-2018
for their feedback. The second author acknowl-
edges support from IISER Bhopal’s Faculty Initi-
ation Grant (IISERB/R&D/2018-19/77).



39

References
Rama Kant Agnihotri. 2007. Hindi: An Essential

Grammar. Essential Grammars. Routledge.

Arpit Agrawal, Sumeet Agarwal, and Samar Husain.
2017. Role of expectation and working memory
constraints in hindi comprehension: An eyetrack-
ing corpus analysis. Journal of Eye Movement Re-
search, 10(2).

Tafseer Ahmed. 2006. Spatial, temporal and structural
uses of urdu ko. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the 11th International
LFG Conference. CSLI Publications, Stanford.

Jennifer E. Arnold. 2011. Ordering choices in pro-
duction: For the speaker or for the listener? In
Emily M. Bender and Jennifer E. Arnold, editors,
Language From a Cognitive Perspective: Grammar,
Usage, and Processing, pages 199–222. CSLI Pub-
lishers.

Paul Baker, Andrew Hardie, Tony McEnery, Hamish
Cunningham, and Robert Gaizauskas. 2002. Emille:
a 67-million word corpus of indic languages: data
collection, mark-up and harmonization. In Proceed-
ings of LREC 2002, pages 819–827. Lancaster Uni-
versity.

Alan Bell, Jason M Brenier, Michelle Gregory, Cyn-
thia Girand, and Dan Jurafsky. 2009. Predictability
effects on durations of content and function words
in conversational English. Journal of Memory and
Language, 60(1):92–111.

Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for hindi/urdu. In Proceedings of the Third
Linguistic Annotation Workshop, ACL-IJCNLP ’09,
pages 186–189, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Barry J. Blake. 2001. Case, 2 edition. Cam-
bridge Textbooks in Linguistics. Cambridge Univer-
sity Press.

Kathryn Bock. 1987. An effect of the accessibility of
word forms on sentence structures. Journal of Mem-
ory and Language, 26(2):119 – 137.

Marisa Ferrara Boston, John T. Hale, Shravan Vasishth,
and Reinhold Kliegl. 2011. Parallel processing and
sentence comprehension difficulty. Language and
Cognitive Processes, 26(3):301–349.

Miriam Butt and Tracy Holloway King. 1996. Struc-
tural topic and focus without movement. In Miriam
Butt and Tracy Holloway King, editors, Proceed-
ings of the First LFG Conference. CSLI Publica-
tions, Stanford.

Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax, volume 11. The MIT press.

Morten H. Christiansen and Nick Chater. 2008. Lan-
guage as shaped by the brain. Behavioral and Brain
Sciences, 31(5):489509.

Vera Demberg, Asad B. Sayeed, Philip J. Gorinski,
and Nikolaos Engonopoulos. 2012. Syntactic sur-
prisal affects spoken word duration in conversa-
tional contexts. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ’12, pages 356–
367, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Ezra Van Everbroeck. 2003. Language type frequency
and learnability from a connectionist perspective.
Linguistic Typology, 7(1):1–50.

Maryia Fedzechkina, T. Florian Jaeger, and Elissa L.
Newport. 2012. Language learners restructure their
input to facilitate efficient communication. Pro-
ceedings of the National Academy of Sciences,
109(44):17897–17902.

Maryia Fedzechkina, Elissa L. Newport, and T. Flo-
rian Jaeger. 2017. Balancing effort and information
transmission during language acquisition: Evidence
from word order and case marking. Cognitive Sci-
ence, 41(2):416–446.

Janet Dean Fodor. 2001. Setting syntactic parameters.
The Handbook of Contemporary Syntactic Theory,
pages 730–767.

A. Frank and T.F. Jaeger. 2008. Speaking rationally:
Uniform information density as an optimal strategy
for language production. Cogsci. Washington, DC:
CogSci.

Silvia P. Gennari and Maryellen C. MacDonald. 2008.
Semantic indeterminacy in object relative clauses.
Journal of Memory and Language, 58(2):161 – 187.

Silvia P. Gennari and Maryellen C. MacDonald. 2009.
Linking production and comprehension processes:
The case of relative clauses. Cognition, 111(1):1 –
23.

Silvia P. Gennari, Jelena Mirkovi, and Maryellen C.
MacDonald. 2012. Animacy and competition in rel-
ative clause production: A cross-linguistic investi-
gation. Cognitive Psychology, 65(2):141 – 176.

Edward Gibson. 2000. Dependency locality theory:
A distance-based theory of linguistic complexity.
In Alec Marantz, Yasushi Miyashita, and Wayne
O’Neil, editors, Image, Language, brain: Papers
from the First Mind Articulation Project Symposium.
MIT Press, Cambridge, MA.

Joseph H. Greenberg. 1963. Some universals of gram-
mar with particular reference to the order of mean-
ingful elements. In Joseph H. Greenberg, editor,
Universals of Human Language, pages 73–113. MIT
Press, Cambridge, Mass.

https://bop.unibe.ch/index.php/JEMR/article/view/2968
https://bop.unibe.ch/index.php/JEMR/article/view/2968
https://bop.unibe.ch/index.php/JEMR/article/view/2968
http://dl.acm.org/citation.cfm?id=1698381.1698417
http://dl.acm.org/citation.cfm?id=1698381.1698417
https://doi.org/10.1017/CBO9781139164894
https://doi.org/https://doi.org/10.1016/0749-596X(87)90120-3
https://doi.org/https://doi.org/10.1016/0749-596X(87)90120-3
https://doi.org/10.1080/01690965.2010.492228
https://doi.org/10.1080/01690965.2010.492228
https://doi.org/10.1017/S0140525X08004998
https://doi.org/10.1017/S0140525X08004998
http://dl.acm.org/citation.cfm?id=2390948.2390992
http://dl.acm.org/citation.cfm?id=2390948.2390992
http://dl.acm.org/citation.cfm?id=2390948.2390992
https://doi.org/10.1515/lity.2003.011
https://doi.org/10.1515/lity.2003.011
https://doi.org/10.1073/pnas.1215776109
https://doi.org/10.1073/pnas.1215776109
https://doi.org/10.1111/cogs.12346
https://doi.org/10.1111/cogs.12346
https://doi.org/10.1111/cogs.12346
https://doi.org/https://doi.org/10.1016/j.jml.2007.07.004
https://doi.org/https://doi.org/10.1016/j.cognition.2008.12.006
https://doi.org/https://doi.org/10.1016/j.cognition.2008.12.006
https://doi.org/https://doi.org/10.1016/j.cogpsych.2012.03.002
https://doi.org/https://doi.org/10.1016/j.cogpsych.2012.03.002
https://doi.org/https://doi.org/10.1016/j.cogpsych.2012.03.002
http://www.ling.uni-potsdam.de/~vasishth/Papers/Gibson-Cognition2000.pdf
http://www.ling.uni-potsdam.de/~vasishth/Papers/Gibson-Cognition2000.pdf


40

Alvin Grissom II, Naho Orita, and Jordan Boyd-
Graber. 2016. Incremental prediction of sentence-
final verbs: Humans versus machines. In Proceed-
ings of The 20th SIGNLL Conference on Computa-
tional Natural Language Learning, pages 95–104.
Association for Computational Linguistics.

John Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies, NAACL ’01, pages 1–8, Pitts-
burgh, Pennsylvania. Association for Computational
Linguistics.

John A. Hawkins. 2004. Efficiency and Complexity in
Grammars. Oxford University Press.

Florian Jaeger, Katrina Furth, and Caitlin Hilliard.
2012. Incremental phonological encoding during
unscripted sentence production. Frontiers in Psy-
chology, 3:481.

T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage information density. Cognitive
Psychology, 61(1):23–62.

T. Florian Jaeger and Harold Tily. 2011. Language pro-
cessing complexity and communicative efficiency.
WIRE: Cognitive Science, 2(3):323–335.

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’02,
pages 133–142, New York, NY, USA. ACM.

Y. Kachru. 2006. Hindi. London Oriental and African
language library. John Benjamins Publishing Com-
pany.

Ayesha Kidwai. 2000. XP-Adjunction in Universal
Grammar: Scrambling and Binding in Hindi-Urdu:
Scrambling and Binding in Hindi-Urdu. Oxford
studies in comparative syntax. Oxford University
Press.

Chigusa Kurumada and T. Florian Jaeger. 2015. Com-
municative efficiency in language production: Op-
tional case-marking in japanese. Journal of Memory
and Language, 83(Supplement C):152 – 178.

Sun-Hee Lee, Mineharu Nakayama, and Richard L.
Lewis. 2005. Difficulty of processing Japanese and
Korean center-embedding constructions. In M. Mi-
nami, H. Kobayashi, M. Nakayama, and H.Sirai, ed-
itors, Studies in Language Science, volume Volume
4, pages 99–118. Kurosio Publishers, Tokyo, Tokyo.

Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126 – 1177.

Roger Levy and Edward Gibson. 2013. Surprisal, the
pdc, and the primary locus of processing difficulty
in relative clauses. Frontiers in Psychology, 4(229).

Roger P Levy. 2018. Communicative efficiency, uni-
form information density, and the rational speech act
theory.

Roger P. Levy and Frank Keller. 2013. Expectation
and locality effects in german verb-final structures.
Journal of Memory and Language, 68(2):199 – 222.

Richard L. Lewis. 1998. Interference in working mem-
ory: Retroactive and proactive interference in pars-
ing. CUNY sentence processing conference.

Richard L. Lewis and Mineharu Nakayama. 2001.
Syntactic and positional similarity effects in the pro-
cessing of Japanese embeddings. In Sentence Pro-
cessing in East Asian Languages, pages 85–113,
Stanford, CA. CSLI.

Pavel Logačev and Shravan Vasishth. 2012. Case
matching and conflicting bindings interference. In
Monique Lamers and Peter de Swart, editors, Case,
Word Order and Prominence: Interacting Cues in
Language Production and Comprehension, pages
187–216. Springer Netherlands, Dordrecht.

Gary Lupyan and Morten H. Christiansen. 2002. Case,
word order, and language learnability: Insights from
connectionist modeling. In In Proceedings of the
24th Annual Conference of the Cognitive Science
Society, pages 596–601. Erlbaum.

Maryellen C. MacDonald. 2013. How language pro-
duction shapes language form and comprehension.
Frontiers in Psychology, 4(226):1–16. Published
with commentaries in Frontiers.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513–553.

Umesh Patil, Gerrit Kentner, Anja Gollrad, Frank
Kügler, Caroline Féry, and Shravan Vasishth. 2008.
Focus, word order and intonation in Hindi. Journal
of South Asian Linguistics, 1(1):55–72.

Mark Pluymaekers, Mirjam Ernestus, and R Harald
Baayen. 2005. Lexical frequency and acoustic re-
duction in spoken dutch. The Journal of the Acous-
tical Society of America, 118(4):2561–2569.

Rajakrishnan Rajkumar, Marten van Schijndel,
Michael White, and William Schuler. 2016. In-
vestigating locality effects and surprisal in written
English syntactic choice phenomena. Cognition,
155:204–232.

Sidharth Ranjan. 2015. Investigation of locality effects
in hindi language production. Master’s thesis, In-
dian Institute of Technology (IIT) Delhi. Unpub-
lished thesis.

Sidharth Ranjan, Rajakrishnan Rajkumar, and Sumeet
Agarwal. In Preparation. Locality and surprisal ef-
fects in hindi preverbal constituent ordering.

Edward Sapir. 1921. Language: An Introduction to the
Study of Speech. Harcourt, Brace, New York.

https://doi.org/10.18653/v1/K16-1010
https://doi.org/10.18653/v1/K16-1010
https://doi.org/10.3115/1073336.1073357
https://doi.org/10.3115/1073336.1073357
https://doi.org/10.3389/fpsyg.2012.00481
https://doi.org/10.3389/fpsyg.2012.00481
http://dx.doi.org/10.1016/j.cogpsych.2010.02.002
http://dx.doi.org/10.1016/j.cogpsych.2010.02.002
https://doi.org/10.1002/wcs.126
https://doi.org/10.1002/wcs.126
https://doi.org/10.1145/775047.775067
https://doi.org/10.1145/775047.775067
https://books.google.co.in/books?id=vqHWkWJrFjQC
https://books.google.co.in/books?id=vqHWkWJrFjQC
https://books.google.co.in/books?id=vqHWkWJrFjQC
https://doi.org/https://doi.org/10.1016/j.jml.2015.03.003
https://doi.org/https://doi.org/10.1016/j.jml.2015.03.003
https://doi.org/https://doi.org/10.1016/j.jml.2015.03.003
https://doi.org/http://dx.doi.org/10.1016/j.cognition.2007.05.006
https://doi.org/http://dx.doi.org/10.1016/j.cognition.2007.05.006
https://doi.org/10.31234/osf.io/4cgxh
https://doi.org/10.31234/osf.io/4cgxh
https://doi.org/10.31234/osf.io/4cgxh
https://doi.org/https://doi.org/10.1016/j.jml.2012.02.005
https://doi.org/https://doi.org/10.1016/j.jml.2012.02.005
https://doi.org/10.1007/978-94-007-1463-2_9
https://doi.org/10.1007/978-94-007-1463-2_9
http://lcnl.wisc.edu/publications/archive/266.pdf
http://lcnl.wisc.edu/publications/archive/266.pdf
https://doi.org/10.1162/coli.07-056-R1-07-027
https://doi.org/10.1162/coli.07-056-R1-07-027
http://www.ling.uni-potsdam.de/~vasishth/pdfs/Patil-Kentner-Gollrad-Kuegler-Fery-VasishthJSAL2008.pdf
http://www.sciencedirect.com/science/article/pii/S001002771630155X/
http://www.sciencedirect.com/science/article/pii/S001002771630155X/
http://www.sciencedirect.com/science/article/pii/S001002771630155X/


41

Andreas Stolcke. 2002. SRILM — An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.

J Vaid and Anshum Gupta. 2002. Exploring word
recognition in a semi-alphabetic script: The case of
Devanagari. Brain and Language, 81:679–90.

Julie A. Van Dyke. 2007. Interference effects from
grammatically unavailable constituents during sen-
tence processing. Journal of Experimental Psychol-
ogy. Learning, Memory, and Cognition, 33 2:407–
30.

Julie A. Van Dyke and Brian McElree. 2006. Retrieval
interference in sentence comprehension. Journal of
Memory and Language, 55(2):157 – 166.

S. Vasishth. 2003. Working Memory in Sentence Com-
prehension: Processing Hindi Center Embeddings.
Outstanding Dissertations in Linguistics. Taylor &
Francis.

https://doi.org/https://doi.org/10.1016/j.jml.2006.03.007
https://doi.org/https://doi.org/10.1016/j.jml.2006.03.007
https://books.google.co.in/books?id=Ogk5an7oEloC
https://books.google.co.in/books?id=Ogk5an7oEloC


42

0
1

2
3

4
5

Le
xi

ca
l s

ur
pr

is
al

 (i
n 

bi
ts

)

pakistan ne vrihaspativaar ko kathit taur par apne...
vrihaspativaar ko pakistan ne kathit taur par apne...

Referent
Variant

0
1

2
3

4
5

6

Le
xi

ca
l s

ur
pr

is
al

 (i
n 

bi
ts

)

pakistan vrihaspativaar kathit taur apne...
vrihaspativaar pakistan kathit taur apne...

Referent
Variant

Figure 3: Lexical surprisal profiles of normal and the
caseless artificial version of Hindi

Sentence Label Lexical Syntactic
type surprisal surprisal

Reference 1 97.45 156.64
Variant1 0 97.69 160.77
Variant2 0 98.25 156.91
Variant3 0 97.97 159.50
Variant4 0 98.16 161.94

Table 6: Original dataset

A Appendix

A.1 Joachims Transformation

Consider the first example in the following Hindi
sentences as reference corresponding to ‘Jay-
alalitha has written a letter to the prime minister
on this issue’ and remaining as grammatical vari-
ants expressing the same idea. Assuming this as a
toy dataset, Table 6 denotes their lexical and syn-
tactic surprisal feature values whereas Table 7 rep-
resents its Joachims transformation.

Reference [jayalalitha-ne]1 [is mazle par]2 [pradhanmantri-ko]3 [ek patr]4 V ...

Variant1 [is mazle par]2 [jayalalitha-ne]1 [pradhanmantri-ko]3 [ek patr]4 V ...

Variant2 [jayalalitha-ne]1 [pradhanmantri-ko]3 [is mazle par]2 [ek patr]4 V ...

Variant3 [pradhanmantri-ko]3 [is mazle par]2 [jayalalitha-ne]1 [ek patr]4 V ...

Variant4 [is mazle par]2 [pradhanmantri-ko]3 [jayalalitha-ne]1 [ek patr]4 V ...

3.87 3.99

0

5

10

15

Reference Variants

D
ep

en
de

nc
y 

su
rp

ris
al

Figure 4: Mean syntactic surprisal per sentence of ref-
erence and variant sentences (95% confidence intervals
indicated)

New Label δ Lexical δ Syntactic
surprisal surprisal

Variant1-Reference 0 0.25 4.13
Reference-Variant2 1 -0.81 -0.28
Variant3-Reference 0 0.53 2.87
Reference-Variant4 1 -0.72 -5.30

Table 7: Transformed dataset

Here are the steps to transform the data set using
the Joachims transformation technique.

1. Equal number of ordered pairs of type (Reference, Vari-
ant) and (Variant, Reference) were created.

2. Differences between the feature values of the elements
of these ordered pairs were taken (see Table 7).

3. <Reference-Variant> pairs were labelled as 1 and
<Variant-Reference> pairs were labelled as 0. Here,
1 stands for the correct choice and 0 denotes the incor-
rect choice.

Label Dependency
relation

Invariant syntactic relations
k1 subject/agent
k2 object/patient
k4 recipient
k7 location

(elsewhere)
k7t location

(in time)
r6 genitive/possessive
Local word group (lwg)
lwg psp postposition
lwg vaux auxilliary verb
Symbols
rsym symbol relation
Indirect dependency relations
ccof co-ordination and sub-ordination
pof part of units such as conjunct verbs
pof cn part of units such as compound noun

Table 8: Glossary of dependency relations


