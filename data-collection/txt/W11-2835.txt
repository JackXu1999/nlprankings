



















































StuMaBa : From Deep Representation to Surface


Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 232–235,
Nancy, France, September 2011. c©2011 Association for Computational Linguistics

<StuMaBa>: From Deep Representation to Surface

Bernd Bohnet 1, Simon Mille 2, Benoı̂t Favre 3, Leo Wanner2,4
1Institut für maschinelle Sprachverarbeitung (IMS)

Universität Stuttgart, {first-name.last-name}@ims.uni-stuttgart.de
2Departament de Tecnologies de la Informació i les Comunicacions

Universitat Pompeu Fabra, {first-name.last-name}@upf.edu
3Laboratoire d’Informatique de l’Universite du Maine

{first-name.last-name}@lium.univ-lemans.fr
4Institució Catalana de Recerca i Estudis Avançats (ICREA)

1 Setup of the System

We realize the full generation pipeline, from the
deep (= semantic) representation (SemR), over
the shallow (= surface-syntactic) representation
(SSyntR) to the surface. To account systematically
for the non-isomorphic projection between SemR
and SSyntR, we introduce an intermediate represen-
tation: the so-called deep-syntactic representation
(DSyntR), which does not contain yet (all) function
words (as SemR), but which already contains gram-
matical function relation labels (as SSyntR).1

The system thus realizes the following steps:
1. Semantic graph→ Deep-syntactic tree
2. Deep-syntactic tree→ Surface-syntactic tree
3. Surface-syntactic tree→ Linearized structure
4. Linearized structure→ Surface

In addition, two auxiliary steps are carried out.
The first one is part-of-speech tagging; it is carried
out after step 3. The second one is introduction of
commata; it is done after step 4.

Each step is implemented as a decoder that uses
a classifier to select the appropriate operations. For
the realization of the classifiers, we use Bohnet et al.
(2010)’s implementation of MIRA (Margin Infused
Relaxed Algorithm) (Crammer et al., 2006).

2 Sentence Realization

Sentence generation consists in the application of
the previously trained decoders in sequence 1.–4.,
plus the two auxiliary steps.

1The DSyntR is inspired by the DSynt structures in
(Mel’čuk, 1988), only that the latter are still “deeper”.

Semantic Generation Our derivation of the
DSynt-tree from an input Sem-graph is analogous to
graph-based parsing algorithms (Eisner, 1996). It is
defined as search for the highest scoring tree y from
all possible trees given an input graph x:

F (x) = argmax Score(y), where y ∈MAP (x)
(with MAP (x) as the set of all trees spanning over
the nodes of the Sem-graph x).

As in (Bohnet et al., 2011), the search is a beam
search which creates a maximum spanning tree us-
ing “early update” as introduced for parsing by
Collins and Roark (2004): when the correct beam
element drops out of the beam, we stop and update
the model using the best partial solution. The idea
is that when all items in the current beam are incor-
rect, further processing is obsolete since the correct
solution cannot be reached extending any elements
of the beam. When we reach a final state, i.e. a tree
spanning over all words and the correct solution is
in the beam but not ranked first, we perform an up-
date as well, since the correct element should have
ranked first in the beam.

Algorithm 1 displays the algorithm for the gen-
eration of the DSyntR from the SemR. The algo-
rithm performs a greedy search for the highest scor-
ing tree. extend-tree is the central function of the
algorithm. It expands a tree by one edge, selecting
each time the highest scoring edge. The attachment
point for an outgoing edge is any node; for an in-
coming edge, it can only be the top node of the built
tree.

For score calculation, we use structured fea-
tures composed of the following elements: (i) the
lemmata, (ii) the distance between the starting node

232



Algorithm 1: Semantic generation
//(xi, yi) semantic graph and
// gold deep syntactic tree for training case only
// both trees contain an artifical root node
tree← {} // empty tree
// search start edge
best←−231
for all n1 ∈ xi do

for all n2 ∈ xi & n1 6= n2 do
for all l ∈ edge-labels do

s← score({(synt(n1),synt(n2),l)})
if s > best then

tree← {(synt(n1),synt(n2),l)}
best← s ; root← n1

// computed remaining nodes to be added
rest← nodes(xi) − nodes(tree)
while rest 6= ∅ do

// extend tree: extend tree by one edge
best←−231
for all nr ∈ rest do

for all nt ∈ tree do
for all l ∈ edge-labels do

s← score(tree, {(synt(nt),synt(nr),l)})
if s > best then

tree← tree ∪ {(synt(nt),synt(nr),l)}
best← s ; rest← rest - nr
continue with while

// check for new root
s← score(tree, {(synt(nr),synt(root),l)})
if s > best then

tree← tree ∪ {(synt(nr),synt(root),l)}
root← nr ; best← s ; rest← rest - nr
continue with while

return tree
TRAINING: if predicted tree 6= gold tree
then update weight vector in accordance with the trees

s and the target node t, (iii) the direction of the
path (if the path has a direction), (iv) the sorted bag
of in-going edges labels without repetition, (v) the
path of edge labels between source and target node.
The templates of the composed structured features
are listed in Table 1. We obtain about 2.6 Million
features in total. The features have binary values,
meaning that a structure has/has not a specific fea-
ture.

Deep-Syntactic Generation: Since the DSyntR
contains by definition only content words, func-
tion words must be introduced during the DSyntR–

feature templates
label+dist(s, t)+dir
label+dist(s, t)+lemmas+dir
label+dist(s, t)+lemmat+dir
label+dist(s, t)+lemmas+lemmat+dir
label+dist(s, t)+lemmas+lemmat+dir
label+dist(s, t)+lemmas+bagt+dir
label+dist(s, t)+lemmat+bagt+dir
label+dist(s, t)+lemmas+bags+dir
label+dist(s, t)+lemmat+bags+dir
label+dist(s, t)+bagt+dir
label+path(s, t)+dir

Table 1: Selected feature templates for the SemR →
DSyntR mapping (‘s’ = “source node”, ‘t’ = “target
node”)

SSyntR generation passage in order to obtain a fully
spelled out syntactic tree.

Algorithm 2: DSynt Generation
//(xi, y

g
i ) the deep syntactic tree

// and gold surface syntactic tree for training case only
// R set of rules
// traverse the tree depth-first
yi ←clone(xi)
node-queue← root(xi)
while node-queue 6= ∅ do

//depth first traversal
node← remove-first-element(node-queue)
node-queue← children(node, xi)∪ node-queue
// select the rules which insert a leaf node
leaf-insert-rules← select-leaf-rules(next-node,xi,R)
yi ← apply (leaf-insert-rules,yi)
// during training, we update the weight vector
// if the rules are not equal to the gold rules,
// select the rules which insert a node into the tree
// or a new node label
node-insert-rules← select-node-rules(node,xi,R)
// during training, we update here the weight vector
yi ← apply (edge-insert-rules,yi)

For this passage, we use a tree transducer for
which we automatically derive 27 rules by com-
paring a gold standard set of DSynt structures and
SSynt dependency trees. The rules are of the fol-
lowing three types: 1) Introduction of an edge and a
node: X⇒X labels →Y; as ‘X⇒X P→ ‘,’’ ; 2) Introduc-
tion of a new node and edges between two nodes: X
labeld→ Y⇒ X label1s → N label2s → Y, as ‘X OPRD→ Y⇒
X OPRD→ ’to’ IM→ Y’; 3) Introduction of a new node
label: X⇒ N, as ‘ ‘LOCATION’⇒ ‘on’ ’ .

Discriminative classifiers are trained for each of

233



the three rule types such that they either select a spe-
cific rule or NONE (with “NONE” meaning that no
rule is to be applied). Algorithm 2 displays the al-
gorithm for the generation of the SSyntR from the
DSyntR.

Tree-based Part-of-Speech tagging: For lin-
earization, i.e., word order determination, informa-
tion on the part of speech (PoS) of the node labels
of the SSynt tree is needed. For this purpose, we de-
veloped a tree-based PoS-tagger. The tagger works
similarly to a standard PoS-tagger, except that we
do not use (i) features derived from the context of
the word (i.e., of the token to its left and of the token
to its right) for which the PoS tag is being sought;
and (ii) wordforms (since a semantic graph and thus
also the trees derived from it in the course of gener-
ation are annotated only with lemmata and seman-
tic grammemes). However, we use features derived
from the SSynt structure that we obtained in the pre-
vious step and the grammemes provided in the se-
mantic graph from which we start. As classifier, we
use a linear support vector machine with averaging.

Linearization: For linearization and morpholo-
gization, we use a similar technique as Bohnet et al.
(2010). Linearization is a beam search for optimal
linearization according to a local and a global score
functions.

In order to derive the word order, we use a bottom-
up linearization method. We start by ordering the
words of sub-trees in which the children do not have
children themselves. We continue then with sub-
trees in which all sub-trees are already ordered. This
method allows us to use the order of the sub-trees to
derive features. We order each sub-tree that includes
a head and its children: (1) The algorithm creates
sets of nodes for each sub-tree in the syntactic tree
that contain the children of the node and the node
itself. (2) The linearization algorithm orders the list
of nodes in such that the node list of the children
are ordered first. (3) Complete sentences are built
by introducing the list of nodes in which only the
head was included so far. The algorithm builds thus
n-best lists of ordered sentences by adding ordered
parts left-to-right.

Morphologization: Morphologization selects the
edit script based on the minimal string edit distance

(Levenshtein, 1966) in accordance with the highest
score for each lemma of a sentence obtained during
training and applies the scripts to obtain the word-
forms.

System 1
Mapping Value
Semantics→Deep-Syntax (ULA/LAS) 99.0/95.1
Deep-Syntax→Surface-Syntax (correct) 98.6
Tree-based PoS tagging 97.8
Syntax→ Topology (% sent. eq. to reference) 54.2
Topology→Morphology (accuracy) 98.2
All stages from deep representation
BLEU 76.4
NIST 13.45
All stages from shallow representation
BLEU 88.7
NIST 13.89

System 2
Semantics→Deep-Syntax (ULA/LAS) 99.0/95.1
Deep-Syntax→Surface-Syntax (correct) 98.9
Tree-based PoS tagging 98.2
Syntax→ Topology (% sent. eq. to reference) 57.7
Topology→Morphology (accuracy) 98.2
All stages from deep representation
BLEU 79.6
NIST 13.55
All stages from shallow representation
BLEU 89.6
NIST 13.93

Table 2: Performance of our realizer on the development
set.

3 Evaluation

We submitted two systems (“System 1” and “Sys-
tem 2”). System 2 was a late submission. System
1 can be considered as a baseline system. System
2 introduces commata more accurately because of
an improved feature set. In addition, System 2 uses
the word order of the children of a node as context
to derive features for the linearization. Furthermore,
it uses a language model to rerank output sentences.
For the language model, we use a 5-gram model with
Kneser-Ney smoothing derived from 11 million sen-
tences, cf. (Kneser and Ney, 1995). Table 2 displays
the figures obtained for both the realization stages in
isolation and the entire pipeline.2

2After the first submission of our system, we corrected a
bug. As a consequence, the results improved. The bug occurred
during the mapping of the data from HFG-format into CoNLL

234



References

B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Proceedings
of COLING ’10, pages 98–106, Beijing.

B. Bohnet, S. Mille, and L. Wanner. 2011. Statistical
Language Generation from Semantic Structures. In
Proceedings of the International Conference on De-
pendency Linguistics, Barcelona.

M. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ’04, Stroudsburg, PA, USA.

K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Jour-
nal of Machine Learning Research, 7:551–585.

J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340–345,
Copenhagen.

R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics,Speech
and Signal Processing.

V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics,
10:707–710.

I.A. Mel’čuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press, Albany.

2009 format, which was carried out to obtain training data in
the format of our generator. It consisted in accessing a number
of wrong columns and/or subcolumns of the annotation for a
few features (which led to, e.g., the use of wordforms instead of
lemmata).

235


