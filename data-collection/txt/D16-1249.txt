



















































Supervised Attentions for Neural Machine Translation


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2283–2288,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Supervised Attentions for Neural Machine Translation
Haitao Mi Zhiguo Wang Abe Ittycheriah

T.J. Watson Research Center
IBM

1101 Kitchawan Rd, Yorktown Heights, NY 10598
{hmi, zhigwang, abei}@us.ibm.com

Abstract

In this paper, we improve the attention or
alignment accuracy of neural machine trans-
lation by utilizing the alignments of train-
ing sentence pairs. We simply compute
the distance between the machine attentions
and the “true” alignments, and minimize this
cost in the training procedure. Our experi-
ments on large-scale Chinese-to-English task
show that our model improves both transla-
tion and alignment qualities significantly over
the large-vocabulary neural machine transla-
tion system, and even beats a state-of-the-art
traditional syntax-based system.

1 Introduction

Neural machine translation (NMT) has gained pop-
ularity in recent two years (e.g. (Bahdanau et al.,
2014; Jean et al., 2015; Luong et al., 2015; Mi et al.,
2016b; Li et al., 2016), especially for the attention-
based models of Bahdanau et al. (2014).

The attention model plays a crucial role in NMT,
as it shows which source word(s) the model should
focus on in order to predict the next target word.
However, the attention or alignment quality of NMT
is still very low (Mi et al., 2016a; Tu et al., 2016).

In this paper, we alleviate the above issue by uti-
lizing the alignments (human annotated data or ma-
chine alignments) of the training set. Given the
alignments of all the training sentence pairs, we add
an alignment distance cost to the objective func-
tion. Thus, we not only maximize the log translation
probabilities, but also minimize the alignment dis-
tance cost. Large-scale experiments over Chinese-
to-English on various test sets show that our best
method for a single system improves the transla-
tion quality significantly over the large vocabulary
NMT system (Section 5) and beats the state-of-the-
art syntax-based system.

2 Neural Machine Translation

As shown in Figure 1, attention-based NMT (Bah-
danau et al., 2014) is an encoder-decoder network.
the encoder employs a bi-directional recurrent neu-
ral network to encode the source sentence x =
(x1, ..., xl), where l is the sentence length (includ-
ing the end-of-sentence 〈eos〉), into a sequence of
hidden states h = (h1, ..., hl), each hi is a concate-
nation of a left-to-right

−→
hi and a right-to-left

←−
hi .

Given h, the decoder predicts the target transla-
tion by maximizing the conditional log-probability
of the correct translation y∗ = (y∗1, ...y

∗
m), where

m is the sentence length (including the end-of-
sentence). At each time t, the probability of each
word yt from a target vocabulary Vy is:

p(yt|h, y∗t−1..y∗1) = g(st, y∗t−1), (1)

where g is a two layer feed-forward neural network
over the embedding of the previous word y∗t−1, and
the hidden state st. The st is computed as:

st = q(st−1, y∗t−1, Ht) (2)

Ht =

[∑l
i=1 (αt,i ·

←−
h i)∑l

i=1 (αt,i ·
−→
h i)

]
, (3)

where q is a gated recurrent units, Ht is a weighted
sum of h; the weights, α, are computed with a two
layer feed-forward neural network r:

αt,i =
exp{r(st−1, hi, y∗t−1)}∑l
k=1 exp{r(st−1, hk, y∗t−1)}

(4)

We put all αt,i (t = 1...m, i = 1...l) into a matrix
A′, we have a matrix (alignment) like (c) in Figure 2,
where each row (for each target word) is a probabil-
ity distribution over the source sentence x.

The training objective is to maximize the condi-
tional log-probability of the correct translation y∗

2283



�↵t1 ↵tl

st�1 st…
ot

y1…

…

y|Vy|

Atj

… …Ht =

lX

i=1

(↵ti ·
 �
h i)

lX

i=1

(↵ti ·
�!
h i)

x1 xl

 �
h1

 �
hl
�!
hl

�!
h1

…

…

…x2

�!
h2

 �
h2

x1 xl

 �
h1

 �
hl
�!
hl

�!
h1

…

…

…

 �
hj
�!
hj

xj

…

…

…

↵t2

y⇤t�1
y⇤t

et,1 et,j et,l

↵t,j =
exp(et,j)Pl
i=1 exp(et,i)

st

Figure 1: The architecture of attention-based NMT (Bahdanau et al., 2014). The source sentence x = (x1, ..., xl) with length l,
xl is an end-of-sentence token 〈eos〉 on the source side. The reference translation is y∗ = (y∗1 , ..., y∗m) with length m, similarly,
y∗m is the target side 〈eos〉.

←−
hi and

−→
hi are bi-directional encoder states. αt,j is the attention probability at time t, position j. Ht

is the weighted sum of encoding states. st is a hidden state. ot is an output state. Another one layer neural network projects ot to

the target output vocabulary, and conducts softmax to predict the probability distribution over the output vocabulary. The attention

model (the right box) is a two layer feedforward neural network, At,j is an intermediate state, then another layer converts it into a

real number et,j , the final attention probability at position j is αt,j .

given x with respect to the parameters θ

θ∗ = argmax
θ

N∑

n=1

m∑

t=1

log p(y∗nt |xn, y∗nt−1..y∗n1 ),

(5)
where n is the n-th sentence pair (xn,y∗n) in the
training set, N is the total number of pairs.

3 Alignment Component

The attentions, αt,1...αt,l, in each step t play an im-
portant role in NMT. However, the accuracy is still
far behind the traditional MaxEnt alignment model
in terms of alignment F1 score (Mi et al., 2016b; Tu
et al., 2016). Thus, in this section, we explicitly add
an alignment distance to the objective function in
Eq. 5. The “truth” alignments for each sentence pair
can be from human annotated data, unsupervised or
supervised alignments (e.g. GIZA++ (Och and Ney,
2000) or MaxEnt (Ittycheriah and Roukos, 2005)).

Given an alignment matrix A for a sentence pair
(x,y) in Figure 2 (a), where we have an end-of-
source-sentence token 〈eos〉 = xl, and we align all
the unaligned target words (y∗3 in this example) to
〈eos〉, also we force y∗m (end-of-target-sentence) to
be aligned to xl with probability one. Then we con-
duct two transformations to get the probability dis-
tribution matrices ((b) and (c) in Figure 2).

3.1 Simple Transformation

The first transformation simply normalizes each
row. Figure 2 (b) shows the result matrix A∗. The
last column in red dashed lines shows the alignments
of the special end-of-sentence token 〈eos〉.

3.2 Smoothed Transformation

Given the original alignment matrix A, we create a
matrixA∗ with all points initialized with zero. Then,
for each alignment point At,i = 1, we update A∗
by adding a Gaussian distribution, g(µ, σ), with a
window sizew (t-w, ... t ... t+w). Take theA1,1 = 1
for example, we haveA∗1,1 += 1,A∗1,2 += 0.61, and
A∗1,3 += 0.14 with w=2, g(µ, σ)=g(0, 1). Then we
normalize each row and get (c). In our experiments,
we use a shape distribution, where σ = 0.5.

3.3 Objectives

Alignment Objective: Given the “true” alignment
A∗, and the machine attentions A′ produced by
NMT model, we compute the Euclidean distance
bewteen A∗ and A′.

d(A′,A∗) =

√√√√
m∑

t=1

l∑

i=1

(A′t,i −A∗t,i)2. (6)

2284



1 0 0 0 0

1 1 0 0 0

0 0 0 0 0

0 1 0 0 0

0 0 0 0 1

0 0 0 0 0

x1 x2 xl

y⇤1

y⇤2

y⇤m

y⇤3

y⇤4

y⇤5

x3 x4

1 0 0 0 0

0.5 0.5 0 0 0

0 0 0 0 0

0 1 0 0 0

0 0 0 0 1

0 0 0 0 0

x1 x2 x3 x4

0

0

1

0

0

1

0.57 0.35 0.08 0 0

0.39 0.39 0.18 0.04 0

0 0 0 0.08 0.35

0.26 0.42 0.26 0.06 0

0 0 0.06 0.26 0.42

0 0 0 0 0

x1 x2 x3 x4

0

0

0.57

0

0.26

1

(a) (b) (c)

0

0

1

0

0

1

x5 xlx5 xlx5

Figure 2: Alignment transformation. A special token, 〈eos〉, is introduced to the source sentence, we align all the unaligned target
words (y∗3 in this case) to 〈eos〉. (a): the original alignment matrix A from GIZA++ or MaxEnt aligner. (b): simple normalization
by rows (probability distribution over the source sentence x). (c): smoothed transformation followed by normalization by rows,

and typically, we always align end-of-source-sentence xl to end-of-target-sentence ym by probability one.

NMT Objective: We plug Eq. 6 to Eq. 5, we have

θ∗ = argmax
θ

N∑

n=1

{
m∑

t=1

log p(y∗nt |xn, y∗nt−1..y∗n1 )

− d(A′n,A∗n)
}
.

(7)

There are two parts: translation and alignment, so
we can optimize them jointly, or separately (e.g. we
first optimize alignment only, then optimize transla-
tion). Thus, we divide the network in Figure 1 into
alignment A and translation T parts:

• A: all networks before the hidden state st,

• T: the network g(st, y∗t−1).

If we only optimize A, we keep the parameters in
T unchanged. We can also optimize them jointly
J. In our experiments, we test different optimization
strategies.

4 Related Work

In order to improve the attention or alignment ac-
curacy, Cheng et al. (2016) adapted the agreement-
based learning (Liang et al., 2006; Liang et
al., 2008), and introduced a combined objective
that takes into account both translation directions
(source-to-target and target-to-source) and an agree-
ment term between the two alignment directions.

By contrast, our approach directly uses and op-
timizes NMT parameters using the “supervised”
alignments.

5 Experiments

5.1 Data Preparation
We run our experiments on Chinese to English task.
The training corpus consists of approximately 5 mil-
lion sentences available within the DARPA BOLT
Chinese-English task. The corpus includes a mix of
newswire, broadcast news, and webblog. We do not
include HK Law, HK Hansard and UN data. The
Chinese text is segmented with a segmenter trained
on CTB data using conditional random fields (CRF).
Our development set is the concatenation of sev-
eral tuning sets (GALE Dev, P1R6 Dev, and Dev
12) initially released under the DARPA GALE pro-
gram. The development set is 4491 sentences in to-
tal. Our test sets are NIST MT06 (1664 sentences)
, MT08 news (691 sentences), and MT08 web (666
sentences).

For all NMT systems, the full vocabulary size of
the training set is 300k. In the training procedure,
we use AdaDelta (Zeiler, 2012) to update model
parameters with a mini-batch size 80. Following
Mi et al. (2016a), the output vocabulary for each
mini-batch or sentence is a sub-set of the full vo-
cabulary. For each source sentence, the sentence-
level target vocabularies are union of top 2k most
frequent target words and the top 10 candidates of
the word-to-word/phrase translation tables learned

2285



MT06
MT08

avg.
single system News Web

BP BLEU T-B BP BLEU T-B BP BLEU T-B T-B
Tree-to-string 0.95 34.93 9.45 0.94 31.12 12.90 0.90 23.45 17.72 13.36

Cov. LVNMT (Mi et al., 2016b) 0.92 35.59 10.71 0.89 30.18 15.33 0.97 27.48 16.67 14.24

+A
lig

nm
en

t Zh→ En
A→ J 0.95 35.71 10.38 0.93 30.73 14.98 0.96 27.38 16.24 13.87
A→ T 0.95 28.59 16.99 0.92 24.09 20.89 0.97 20.48 23.31 20.40

A→ T→ J 0.95 35.95 10.24 0.92 30.95 14.62 0.97 26.76 17.04 13.97
J 0.96 36.76 9.67 0.94 31.24 14.80 0.96 28.35 15.61 13.36

GDFA J 0.96 36.44 10.16 0.94 30.66 15.01 0.96 26.67 16.72 13.96

MaxEnt J 0.95 36.80 9.49 0.93 31.74 14.02 0.96 27.53 16.21 13.24J + Gau. 0.96 36.95 9.71 0.94 32.43 13.61 0.97 28.63 15.80 13.04
Table 1: Single system results in terms of (TER-BLEU)/2 (T-B, the lower the better) on 5 million Chinese to English training set.
BP denotes the brevity penalty. NMT results are on a large vocabulary (300k) and with UNK replaced. The second column shows

different alignments (Zh→ En (one direction), GDFA (“grow-diag-final-and”), and MaxEnt (Ittycheriah and Roukos, 2005). A,
T, and J mean optimize alignment only, translation only, and jointly. Gau. denotes the smoothed transformation.

from ‘fast align’ (Dyer et al., 2013). The maximum
length of a source phrase is 4. In the training time,
we add the reference in order to make the translation
reachable.

The Cov. LVNMT system is a re-implementation
of the enhanced NMT system of Mi et al. (2016a),
which employs a coverage embedding model and
achieves better performance over large vocabulary
NMT Jean et al. (2015). The coverage embedding
dimension of each source word is 100.

Following Jean et al. (2015), we dump the align-
ments, attentions, for each sentence, and replace
UNKs with the word-to-word translation model or
the aligned source word.

Our SMT system is a hybrid syntax-based tree-to-
string model (Zhao and Al-onaizan, 2008), a simpli-
fied version of the joint decoding (Liu et al., 2009;
Cmejrek et al., 2013). We parse the Chinese side
with Berkeley parser, and align the bilingual sen-
tences with GIZA++ and MaxEnt. and extract Hi-
ero and tree-to-string rules on the training set. Our
two 5-gram language models are trained on the En-
glish side of the parallel corpus, and on monolin-
gual corpora (around 10 billion words from Giga-
word (LDC2011T07), respectively.As suggested by
Zhang (2016), NMT systems can achieve better re-
sults with the help of those monolingual corpora. In
this paper, our NMT systems only use the bilingual
data. We tune our system with PRO (Hopkins and

May, 2011) to minimize (TER- BLEU)/2 1 on the de-
velopment set.

5.2 Translation Results

Table 1 shows the translation results of all sys-
tems. The syntax-based statistical machine trans-
lation model achieves an average (TER-BLEU)/2 of
13.36 on three test sets. The Cov. LVNMT system
achieves an average (TER-BLEU)/2 of 14.24, which
is about 0.9 points worse than Tree-to-string SMT
system. Please note that all systems are single sys-
tems. It is highly possible that ensemble of NMT
systems with different random seeds can lead to bet-
ter results over SMT.

We test three different alignments:

• Zh→ En (one direction of GIZA++),

• GDFA (the “grow-diag-final-and” heuristic
merge of both directions of GIZA++),

• MaxEnt (trained on 67k hand-aligned sen-
tences).

1The metric used for optimization in this work is (TER-
BLEU)/2 to prevent the system from using sentence length alone
to impact BLEU or TER. Typical SMT systems use target word
count as a feature and it has been observed that BLEU can be
optimized by tweaking the weighting of the target word count
with no improvement in human assessments of translation qual-
ity. Conversely, in order to optimize TER shorter sentences can
be produced. Optimizing the combination of metrics alleviates
this effect (Arne Mauser and Ney, 2008).

2286



The alignment quality improves from Zh → En to
MaxEnt. We also test different optimization strate-
gies: J (jointly), A (alignment only), and T (trans-
lation model only). A combination, A→ T, shows
that we optimize A only first, then we fix A and only
update T part. Gau. denotes the smoothed trans-
formation (Section 3.2). Only the last row uses the
smoothed transformation, all others use the simple
transformation.

Experimental results in Table 1 show some in-
teresting results. First, with the same alignment, J
joint optimization works best than other optimiza-
tion strategies (lines 3 to 6). Unfortunately, break-
ing down the network into two separate parts (A and
T) and optimizing them separately do not help (lines
3 to 5). We have to conduct joint optimization J in
order to get a comparable or better result (lines 3, 5
and 6) over the baseline system.

Second, when we change the training alignment
seeds (Zh→En, GDFA, and MaxEnt) NMT model
does not yield significant different results (lines 6 to
8).

Third, the smoothed transformation (J + Gau.)
gives some improvements over the simple transfor-
mation (the last two lines), and achieves the best
result (1.2 better than LVNMT, and 0.3 better than
Tree-to-string). In terms of BLEU scores, we con-
duct the statistical significance tests with the sign-
test of Collins et al. (2005), the results show that the
improvements of our J + Gau. over LVNMT are
significant on three test sets (p < 0.01).

At last, the brevity penalty (BP) consistently gets
better after we add the alignment cost to NMT objec-
tive. Our alignment objective adjusts the translation
length to be more in line with the human references
accordingly.

5.3 Alignment Results
Table 2 shows the alignment F1 scores on the align-
ment test set (447 hand aligned sentences). The
MaxEnt model is trained on 67k hand-aligned sen-
tences, and achieves an F1 score of 75.96. For NMT
systems, we dump the alignment matrixes and con-
vert them into alignments with following steps. For
each target word, we sort the alphas and add the max
probability link if it is higher than 0.2. If we only
tune the alignment component (A in line 3), we im-
prove the alignment F1 score from 45.76 to 47.87.

system pre. rec. F1
MaxEnt 74.86 77.10 75.96

Cov LVNMT (Mi et al., 2016b) 51.11 41.42 45.76

+A
lig

nm
en

t Zh→ En
A 50.88 45.19 47.87

A→ J 53.18 49.37 51.21
A→ T 50.29 44.90 47.44

A→ T→ J 53.71 49.33 51.43
J 54.29 48.02 50.97

GDFA J 53.88 48.25 50.91

MaxEnt J 44.42 55.25 49.25J + Gau. 48.90 55.38 51.94
Table 2: Alignment F1 scores of different models.

And we further boost the score to 50.97 by tuning
alignment and translation jointly (J in line 7). Inter-
estingly, the system using MaxEnt produces more
alignments in the output, and results in a higher re-
call. This suggests that using MaxEnt can lead to a
sharper attention distribution, as we pick the align-
ment links based on the probabilities of attentions,
the sharper the distribution is, more links we can
pick. We believe that a sharp attention distribution
is a great property of NMT.

Again, the best result is J + Gau. in the last row,
which significantly improves the F1 by 5 points over
the baseline Cov. LVNMT system. When we use
MaxEnt alignments, J + Gau. smoothing gives us
about 1.7 points gain over J system. So it looks in-
teresting to run another J + Gau. over GDFA align-
ment.

Together with the results in Table 1, we conclude
that adding the alignment cost to the training ob-
jective helps both translation and alignment signif-
icantly.

6 Conclusion

In this paper, we utilize the “supervised” alignments,
and put the alignment cost to the NMT objective
function. In this way, we directly optimize the at-
tention model in a supervised way. Experiments
show significant improvements in both translation
and alignment tasks over a very strong LVNMT sys-
tem.

Acknowledgment

We thank the anonymous reviewers for their useful
comments.

2287



References
Sasa Hasan Arne Mauser and Hermann Ney. 2008. Au-

tomatic evaluation measures for statistical machine
translation system optimization. In Proceedings of
LREC 2008, Marrakech, Morocco, may.

D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural
Machine Translation by Jointly Learning to Align and
Translate. ArXiv e-prints, September.

Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu,
Maosong Sun, and Yang Liu. 2016. Agreement-based
joint training for bidirectional attention-based neural
machine translation. In Proceedings of IJCAI, New
York, USA, July.

Martin Cmejrek, Haitao Mi, and Bowen Zhou. 2013.
Flexible and efficient hypergraph interactions for joint
hierarchical and forest-to-string decoding. In Proceed-
ings of the 2013 Conference on Empirical Methods in
Natural Language Processing, pages 545–555, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540,
Ann Arbor, Michigan, June.

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of ibm model 2. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 644–648, Atlanta, Georgia, June.
Association for Computational Linguistics.

Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.

Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT ’05: Proceedings of the HLT and
EMNLP, pages 89–96.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and
Yoshua Bengio. 2015. On using very large target vo-
cabulary for neural machine translation. In Proceed-
ings of ACL, pages 1–10, Beijing, China, July.

Xiaoqing Li, Jiajun Zhang, and Chengqing Zong. 2016.
Towards zero unknown word in neural machine trans-
lation. In Proceedings of IJCAI 2016, pages 2852–
2858, New York, NY, USA, July.

P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Association for Com-
putational Linguistics (NAACL), pages 104–111.

P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In Advances in Neural Information
Processing Systems (NIPS).

Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 2 - Volume 2, ACL ’09,
pages 576–584, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Manning.
2015. Effective approaches to attention-based neu-
ral machine translation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1412–1421, Lisbon, Portu-
gal, September. Association for Computational Lin-
guistics.

Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe
Ittycheriah. 2016a. A coverage embedding model for
neural machine translation. ArXiv e-prints.

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016b.
Vocabulary manipulation for neural machine transla-
tion. In Proceedings of ACL, Berlin, Germany, Au-
gust.

Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’00, pages 440–447, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li. 2016. Coverage-
based Neural Machine Translation. ArXiv e-prints,
January.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR.

Jiajun Zhang. 2016. Exploiting source-side monolingual
data in neural machine translation. In Proceedings of
EMNLP 2016, Austin, Texas, USA, November.

Bing Zhao and Yaser Al-onaizan. 2008. Generalizing lo-
cal and non-local word-reordering patterns for syntax-
based machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 572–581, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

2288


