



















































Data Selection With Fewer Words


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 58–65,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

Data Selection With Fewer Words

Amittai Axelrod
University of Maryland
amittai@umd.edu

Xiaodong He
Microsoft Research

xiaohe@microsoft.com

Philip Resnik
University of Maryland
resnik@umd.edu

Mari Ostendorf
University of Washington
ostendo@uw.edu

Abstract

We present a method that improves
data selection by combining a hybrid
word/part-of-speech representation for
corpora, with the idea of distinguishing
between rare and frequent events. We
validate our approach using data selection
for machine translation, and show that it
maintains or improves BLEU and TER
translation scores while substantially im-
proving vocabulary coverage and reducing
data selection model size. Paradoxically,
the coverage improvement is achieved
by abstracting away over 97% of the
total training corpus vocabulary using
simple part-of-speech tags during the data
selection process.

1 Introduction

Data selection uses a small set of domain-relevant
data to select additional training items from a
much larger, out-of-domain dataset. Its goal is to
filter Big Data down to Good Data: finding the
best, most relevant data to use to train a model for
a particular task.

The prevalent data selection method, cross-
entropy difference (Moore and Lewis, 2010), can
produce domain-specific systems that are usually
as good as or better than systems using all avail-
able training data (Axelrod et al., 2011). The size
of these domain-specific systems scales roughly
linearly with the amount of selected data: a system
trained on the most domain-relevant 10% of the
full out-of-domain dataset will be only one tenth
of the size of a system trained using all the avail-
able data. This can be a large win in settings where
training time matters, and also where compactness
of the final system matters, e.g. running speech
recognition or translation on mobile devices.

While data selection thus eliminates the need to
train systems on the entire pool of available data,

the data selection process itself does not scale well
(it still requires a language model built on the en-
tire pool) and, more significantly, it comes at a
cost: training on selected subsets leads to reduc-
tions in vocabulary coverage compared to training
on the full out-of-domain data pool. This coverage
is important, because most NLP systems face the
problem of handling words that were not seen in
training the system, i.e. out-of-vocabulary (OOV)
words. In automatic speech recognition (ASR), for
example, OOV words pose a substantial problem,
since the system will hallucinate a phonetically
similar word in its vocabulary when an OOV word
is encountered. In machine translation (MT), our
focal application in this paper, OOVs can some-
times be transliterated, but often they are ignored
or passed through without translation, and gaps in
vocabulary coverage can have a significant effect
on MT performance (Daumé III and Jagarlamudi,
2011; Irvine and Callison-burch, 2013).

We introduce a method that preserves the data
selection benefit of reducing translation system
size. Our method performs as well or better than
the standard cross-entropy difference method, as
measured by downstream MT results. To this we
add the benefits of substantially improved lexical
coverage, as well as lower memory requirements
for the data selection model itself.

This improvement stems from constructing a
hybrid representation of the text that abstracts
away words that are infrequent in either of the in-
domain and general corpora. They are replaced
with their part-of-speech (POS) tags, permitting
their n-gram statistics to be robustly aggregated:
intuitively, if a domain-relevant sentence includes
a rare word in some non-rare context (e.g. “An
earthquake in Port-au-Prince”), then another sen-
tence with the same context but a different rare
word is probably also just as relevant (e.g. “An
earthquake in Kodari”). While this method re-
quires pre-processing the corpora to POS tag the

58



data, the idea should generalize to automatically-
derived word classes.

We present results using data selection to train
domain-relevant SMT systems, yielding favorable
performance compared against the standard ap-
proaches of Moore and Lewis (2010) and Axel-
rod et al. (2011). Paradoxically, this is achieved
by a selection process in which the specific lexical
items for infrequent words – up to 97% of the total
vocabulary – are replaced with POS tags.

2 Related Work

Data selection is a widely-used variant of domain
adaptation that requires quantifying the relevance
to the domain of the sentences in a pooled cor-
pus of additional data. The pool is sorted by rel-
evance score, the highest ranked portion is kept,
and the rest discarded.This process – also known
as “rank-and-select” in language modeling (Sethy
et al., 2009) – identifies the subset of the data pool
that is most like the in-domain corpus and keeps it
for translation system training, in lieu of using the
entire data pool. The resulting translation systems
are more compact and cheaper to train and run
than the full-corpus system. The catch, of course,
is that any large data pool can be expected to con-
tain sentences that are at best irrelevant to the do-
main, and at worst detrimental: the goals of fi-
delity (matching in-domain data as closely as pos-
sible) and broad coverage are often at odds (Gascó
et al., 2012). As a result, much work has focused
on fidelity. Mirkin and Besacier (2014) survey the
difficulties of increasing coverage while minimiz-
ing impact on model performance.

We build on the standard approach for data se-
lection in language modeling, which uses cross-
entropy difference as the similarity metric (Moore
and Lewis, 2010). The Moore-Lewis procedure
first trains an in-domain language model (LM) on
the in-domain data, and another LM on the full
pool of general data. It assigns to each full-pool
sentence s a cross-entropy difference score,

HLMIN (s)−HLMPOOL(s), (1)

where Hm(s) is the per-word cross entropy of s
according to language model m. Lower scores
for cross-entropy difference indicate more relevant
sentences, i.e. those that are most like the target
domain and unlike the full pool average. In bilin-
gual settings, the bilingual Moore-Lewis criterion,
introduced by Axelrod et al. (2011), combines the

cross-entropy difference scores from each side of
the corpus; i.e. for sentence pair 〈s1, s2〉:

(HLMIN1 (s1)−HLMPOOL1 (s1))
+(HLMIN2 (s2)−HLMPOOL2 (s2)) (2)

After sorting on the relevant criterion, the top-n
sentences (or sentence pairs) are added to the in-
domain data to create the new, combined training
set. Typically a range of values for n is considered,
selecting the n that performs best on held-out in-
domain data.

While shown to be effective, however, word-
based scores may not capture all facets of rele-
vance. The strategy of a hybrid word/POS rep-
resentation was first explored by Bulyko et al.
(2003), who used class-dependent weights for
mixing multi-source language models. The
classes were a combination of the 100 most fre-
quent words and POS tags. Bisazza and Fed-
erico (2012) target in-domain coverage by using
a hybrid word/POS representation to train an ad-
ditional LM for decoding in an MT pipeline. Toral
(2013) uses a hybrid word/class representation for
data selection for language modeling; he replaces
all named entities with their type (e.g. ’person’,
’organization’), and experiments with also lemma-
tizing the remaining words.

3 Our Approach: Abstracting Away
Words in the Long Tail

Our approach is motivated by the observation that
domain mismatches can have a strong register
component, and this comprises both lexical and
syntactic differences. We are inspired, as well,
by work in stylometry, observing that attempts to
quantify differences between text datasets try to
learn too much from the long tail (Koppel et al.,
2003): most words occur very rarely, meaning that
empirical statistics for them are probably overes-
timating their seen contexts and underestimating
unseen ones.

We therefore adopt a hybrid word/POS repre-
sentation strategy, but, crucially, we focus not
on restricting attention to frequent words, but on
avoiding the undue effects of infrequent words.
The proposal can be realized straightforwardly:
after part-of-speech tagging the in-domain and
pool corpora, we identify all words that appear in-
frequently in either one of the two corpora, and re-
place each of their word tokens with its POS tag.

59



Relevance computation, sentence ranking and sub-
set selection then proceed as usual according to the
Moore-Lewis or bilingual Moore-Lewis criterion.

As an example, consider again the phrases “an
earthquake in Port-au-Prince” and “an earth-
quake in Kodari”, and suppose that the words
an, in, and earthquake are above-threshold in fre-
quency. Our hybrid word/POS representation for
both sentences would be the same: “an earth-
quake in NNP”.

Our approach differs from the standard data
selection method most significantly in its han-
dling of rare words in frequent contexts. Consider
a domain-specific n-gram context c that appears
with a rare word w. For example, in a hypothetical
news domain, let c = “an earthquake in”, made
up of common words, and let w = Port-au-Prince.
Suppose that the in-domain corpus contains the
phrase “an earthquake in Port-au-Prince” eight
times. The word w does not appear any other times
in the in-domain corpus, and the word w′ = Ko-
dari never appears at all.

Now suppose the out-of-domain pool corpus
contains a sentence with “an earthquake in Ko-
dari”. The standard Moore-Lewis method consid-
ers Kodari to be an unknown word, and so only
credits that pool sentence with matching the ele-
ments of c. In contrast, our method replaces both
rare words w and w′ with their POS tag, NNP,
so that the pool sentence contains “an earthquake
in NNP”. Our method thus credits c from the
in-domain corpus, like Moore-Lewis, but we also
credit the sentence with matching the 4-gram “an
earthquake in NNP”, which appears eight times
in the in-domain corpus. Despite not appearing
in the pool corpus, the rare word w from the in-
domain corpus now provides us information about
the relevance of pool sentences containing a syn-
tactically similar rare word w′ that shares the same
context c.

4 Experimentation

We evaluate our data selection approach in a real-
istic small-in-domain-corpus setting, in two ways.
First, as an intrinsic evaluation, we look at vo-
cabulary coverage of the selected data relative to
the in-domain training set, i.e. how many words
from the in-domain corpus are out-of-vocabulary
for selected data, since models trained on those
data would not not be able to handle those words.
Second, as an extrinsic evaluation, we use statisti-

cal machine translation as a downstream task.

4.1 Evaluation Framework

We define our in-domain corpus as the TED talk
translations in the WIT3 TED Chinese-English
corpus (Cettolo et al., 2012), a good example of
a subdomain with little available training data. We
used the IWSLT dev2010 and test2010 sets (also
from WIT3) for tuning and evaluation. The larger
pool from which we selected data was constructed
from an aggregation of 47 LDC Chinese-English
parallel datasets.1 Table 1 contains the corpus
statistics for the task and pool bilingual corpora.

Vocab Vocab
Corpus Sentences (En) (Zh)
TED (task) 145,901 49,323 64,616
LDC (pool) 6,025,295 458,570 714,628

Table 1: Chinese-English Parallel Data.

We used the KenLM toolkit (Heafield, 2011)
to build all language models used in this work
(i.e., both for data selection and for the MT sys-
tems used for extrinsic evaluation). In all cases
the models were 4-gram LMs. We used the Stan-
ford part-of-speech tagger (Toutanova et al., 2003)
when constructing our hybrid representations, to
generate the POS tags for each of the English and
Chinese sides of the corpora.2

We consider three ways of applying data se-
lection using the standard (fully lexicalized) cor-
pus representation and our hybrid representa-
tion. The first two use the monolingual Moore-
Lewis method (Equation 1) to respectively com-
pute relevance scores using the English (output)
side and the Chinese (input) side of the parallel
corpora. The third uses bilingual Moore-Lewis
(Equation 2) to compute the bilingual score over
both sides.

Each of these three variants produces a ver-
sion of the full pool in which the sentences are
ranked by relevance score, from lowest score

1Specifically: LDC2000T47 LDC2002T01 LDC2003E07 LDC2003T17
LDC2004E12 LDC2004T07 LDC2005T06 LDC2006T04 LDC2007E101 LDC2007T09

LDC2007T23 LDC2008E40 LDC2008E56 LDC2008T06 LDC2008T08 LDC2008T18

LDC2009E16 LDC2009E95 LDC2009T02 LDC2009T06 LDC2009T15 LDC2010T03

LDC2010T10 LDC2010T11 LDC2010T12 LDC2010T14 LDC2010T17 LDC2010T21

LDC2012T16 LDC2012T20 LDC2012T24 LDC2013E119 LDC2013E125 LDC2013E132

LDC2013E83 LDC2013T03 LDC2013T05 LDC2013T07 LDC2013T11 LDC2013T16

LDC2014E08 LDC2014E111 LDC2014E50 LDC2014E69 LDC2014E99 LDC2014T04

LDC2014T11.
2The Stanford NLP tools use the Penn tagsets, which

comprise 43 tags for English and 35 for Chinese.

60



English Chinese
TED vocab 49,323 64,616
LDC vocab 458,570 714,628
Joint vocab 470,154 729,283
LDC minus singletons 243,882 373,381
Baseline selection vocab 257,744 388,927

Table 2: Chinese and English vocabulary for the
baseline selection process.

(most domain-like) to highest score (least domain-
like). For each of those ranked pools, we con-
sider increasingly larger subsets of the data: the
best n = 50K, the best n = 100K, and so on.
The largest subset we consider consists of the best
n = 4M sentence pairs out of the 6M available.

4.1.1 Cross-Entropy Difference Baseline
In addition to comparing against a system trained
on all the data, we compare against systems
trained on data selected via the standard cross-
entropy difference method. The joint vocabu-
lary for the TED and LDC data is shown in Ta-
ble 2. However, when training the language mod-
els used for the baseline selection process, we first
pruned the singletons from the LDC vocabulary.
This step is not necessary, but provides a slightly
stronger baseline. The rationale is that ignoring
LDC singletons avoids reserving too much prob-
ability mass for rare words outside of the domain
of interest. Unlike the experimental systems be-
low, pruning the lexicon simply ignores the words
in the corpus and does not replace them with any-
thing. This process removed 47% of the LDC
vocabulary in each language. We then merged
the remaining words from LDC with the complete
TED lexicon. This produced a final vocabulary of
257,744 (En) and 388,927 (Zh) words for the base-
line cross-entropy difference selection process, as
shown in Table 2.

4.1.2 Hybrid Representation Systems
As our infrequent-word threshold (selected ahead
of our experimentation), we retained words with a
count of 10 or more in each corpus, and replaced
all other words with their POS tags to create the
hybrid corpus representation. The minimum count
requirement reduced the vocabulary to 10,036 En-
glish words and 11,440 Chinese words, as shown
in Table 3. All other words were replaced, thus
a minimum count of 10 in each corpus eliminates
over 97% of the vocabulary in each language. We

English Chinese
Joint vocab 470,154 729,283
Vocab with count ≥ 10 10,036 11,440
POS tags 42 35
Hybrid vocab 10,078 11,475

Table 3: Chinese and English vocabulary for the
proposed selection process.

previously found that setting the threshold to 10 is
slightly better than a minimum count of 20 (Axel-
rod, 2014), and varying the threshold further is a
topic for future work; see Section 5.

4.2 Results

4.2.1 Intrinsic Evaluation
As noted, each of the bilingual Moore-Lewis
method and our hybrid word/POS variation pro-
duces a version of the additional training pool in
which sentences are ranked by relevance. We then
select increasingly larger slices of the data from
50k to 4M, as described in Section 4.1, and report
results. As shown in Figures 1 and 2, the hybrid-
selected models show consistently improved vo-
cabulary coverage when compared head-to-head
with models trained on data selected via a Moore-
Lewis method, across all subsets. The only excep-
tion is when examining the vocabulary coverage
in one language while selecting data based on the
other one (e.g. selecting data using the English
half but measuring the TED vocabulary cover-
age in Chinese), where our method provides only
negligible improvement. Overall, the in-domain
(TED) vocabulary coverage is up to 10% better
with our proposed method, and the general-data
(LDC) vocabulary coverage is up to 20% better.

Table 4 illustrates what this looks like in more
detail for a single slice containing the top 2M sen-
tence pairs. The table shows how many more vo-
cabulary items are covered by the 2M sentence
slice selected using our hybrid representation (the
Hyb columns) than are covered by the best 2M
sentences selected using the standard lexical rep-
resentation (the Std columns).

Our method shows this improved vocabulary
coverage regardless of whether one compares the
vocabulary coverage of the methods on the En-
glish side (the first three rows) or the Chinese side
(the second three rows) of the corpora. Further-
more, the results also hold regardless of which
of the three ways of performing cross-entropy-

61



TED Coverage LDC Coverage
Lang Method Std Hyb Std Hyb

En
Mono-en 67% 72% 42% 52%
Mono-Zh 70% 71% 48% 54%
Bilingual 68% 72% 42% 52%

Zh
Mono-En 70% 71% 38% 46%
Mono-Zh 69% 73% 43% 62%
Bilingual 69% 73% 37% 54%

Table 4: Vocabulary coverage comparison be-
tween standard and hybrid-based data selection,
for data-selected samples of 2M sentences.

based data selection one uses. The three ways
are: monolingual Moore-Lewis for the English
and Chinese sides of the parallel corpus (Mono-En
and Mono-Zh, respectively), as well as bilingual
Moore-Lewis (Bilingual).

When selecting 2M sentences, Table 4 shows
that the hybrid representation provides up to an ex-
tra 4-5% in-domain vocabulary coverage in either
language. Furthermore, the hybrid-based methods
obtain up to 10% more general-domain vocabu-
lary coverage for English, and up to 19% more
Chinese general-domain vocabulary coverage. All
improvements are absolute percentage increases.

Figure 2 shows that our hybrid method’s pool
vocabulary coverage increases more rapidly than
the baseline. The standard approach shows vo-
cabulary coverage increasing more or less linearly
with the amount of selection data. By contrast,
our proposed method appears to asymptotically
approach full in-domain vocabulary coverage, par-
ticularly for Chinese. Similarly, Figure 1 shows
that our hybrid method also increases more rapidly
to asymptotically approach full in-domain vocab-
ulary coverage as well.

4.2.2 Extrinsic Evaluation
Improved vocabulary coverage is a positive re-
sult, but we are also interested in downstream ap-
plication performance. Accordingly, we trained
SMT systems using cdec (Dyer et al., 2010) on
subsets of selected data. All SMT systems were
tuned using MIRA (Chiang et al., 2008) on the
dev2010 data from IWSLT (Federico et al., 2011),
and then evaluated on the test2010 IWSLT test
set using both BLEU (Papineni et al., 2002) and
TER (Snover et al., 2006). To isolate the impact
of the data selection method, we present results
just using the selected data, without the combining
with the in-domain data into a multi-model sys-

tem. Note that the hybrid word/POS representa-
tions were only used to compute the cross-entropy
difference scores for determining sentences’ rele-
vance; the MT systems themselves are trained us-
ing the sentences containing the original words.

Figure 3 shows our MT results using both
BLEU and TER. The horizontal line is a static
baseline that uses all the available training data
without data selection. The dashed grey line
is from systems trained on data selected via the
standard Moore-Lewis cross-entropy-difference
method, and the black line is from systems trained
on data selected with our hybrid approach. To
account for variability in MT tuning, each of the
curves in Figure 3 is the average of three tun-
ing/decoding runs.

In terms of system accuracy, our results con-
firm prior work on data selection, demonstrating
that in comparison to training using all available
data, comparable or even better MT performance
can be obtained using only a fraction of the out-
of-domain data available.

Table 5 shows SMT results for the same sub-
set size of 2M sentences used for the coverage
results in Table 4. Systems trained on data se-
lected using the hybrid representation are up to
+0.5 BLEU better, regardless of whether the se-
lection process is monolingual or bilingual. In-
deed, at least for BLEU, it appears that our hy-
brid method may tend to converge to comparable
performance more quickly, a possibility worthy of
future experimentation.

The TER results are mixed for this data se-
lection subset size. The MT evaluation scores
are low in absolute terms, due to only using the
general-domain data, yet are still not inconsistent
with prior research done using this dataset (Fed-
erico et al., 2011). Fluctuations in the perfor-
mance curves are also consistent with prior work,
as IWSLT scores are very jittery. We averaged
results over three tuning runs, for stability. De-
spite that, Figure 3 shows how high-variance TER
scores are on this task.

4.2.3 Selection Model Size
The resulting translation system sizes conform
with prior work: selecting smaller subsets yields
smaller downstream MT systems. For example,
an MT system trained on 1M selected sentences
is ∼2.3GB in size, a factor of 5 smaller than the
11.3GB baseline MT system trained on all 6M
sentences. In addition, we observe a healthy re-

62



Figure 1: Percentage of TED vocabulary covered vs. number of selected sentences by method.

Figure 2: Percentage of LDC vocabulary covered vs. number of selected sentences by method.

63



Figure 3: SMT system scores on the TED Zh-En test2010 set vs. number of selected sentences by
method.

Metric Method Std Hyb

BLEU
Mono-en 8.55 8.95
Mono-Zh 7.70 8.22
Bilingual 8.34 8.68

TER
Mono-En 84.44 82.15
Mono-Zh 80.16 84.51
Bilingual 81.27 81.44

Table 5: SMT system score comparison between
standard and hybrid-based data selection, for data-
selected samples of 2M sentences.

duction in the memory requirements for the data
selection process, which requires training a lan-
guage model on the entire data pool. The bina-
rized language model built using the standard data
selection baseline on the full corpus of 6M sen-
tences requires about 2GB, whereas the equiva-
lent all-data LM for our approach is 25% smaller.3

This means that for any given amount of avail-
able memory, the hybrid method can scale up data
selection to a larger out-of-domain sentence pool.
As a rough example, an 8GB desktop machine can
be used to train an LM on 32M sentences using
the hybrid representation rather than 24M using

3Our back-of-the-envelope estimates ignore the in-
domain LM, which is tiny in comparison.

the standard text; for a large-memory 128GB ma-
chine, our method would allow us to increase the
size of the corpus used to train the full-data LM
from a maximum of 384M sentences to more than
half a billion sentences.

5 Conclusions

We have presented a new method for data selection
that retains the existing advantages of the state-
of-the-art approach, while improving vocabulary
coverage and also improving the ability to scale up
to larger out-of-domain datasets. Our motivation
is in the practical application of NLP technology,
which often requires working with constrained re-
sources and in specific domains with limited train-
ing data. The proposal is conceptually simple,
uses widely available tools, and is easily applied.
A drawback of the proposed approach is that it re-
quires an additional pre-processing step of tagging
all of the training data. For languages for which a
POS tagger is not available, we expect that data-
driven word classes would be a good substitute. In
future work we plan to explore hybrid represen-
tations further, e.g. abstracting away from infre-
quent lexical items via distributional clustering or
morphological analysis, rather than using part-of-
speech information.

64



Acknowledgments

We gratefully thank the anonymous reviewers and
Timo Baumann for their detailed feedback.

References

Axelrod, A. (2014). Data Selection for Statistical
Machine Translation. PhD thesis, University of
Washington.

Axelrod, A., He, X., and Gao, J. (2011). Domain
Adaptation Via Pseudo In-Domain Data Selec-
tion. EMNLP (Empirical Methods in Natural
Language Processing).

Bisazza, A. and Federico, M. (2012). Cutting the
Long Tail : Hybrid Language Models for Trans-
lation Style Adaptation. EACL (European As-
sociation for Computational Linguistics), pages
439–448.

Bulyko, I., Ostendorf, M., and Stolcke, A. (2003).
Getting More Mileage From Web Text Sources
For Conversational Speech Language Model-
ing Using Class-Dependent Mixtures. NAACL
(North American Association for Computa-
tional Linguistics).

Cettolo, M., Girardi, C., and Federico, M. (2012).
WITˆ3 : Web Inventory of Transcribed and
Translated Talks. EAMT (European Association
for Machine Translation).

Chiang, D., Marton, Y., and Resnik, P. (2008). On-
line large-margin training of syntactic and struc-
tural translation features. EMNLP (Empirical
Methods in Natural Language Processing).

Daumé III, H. and Jagarlamudi, J. (2011). Domain
Adaptation for Machine Translation by Mining
Unseen Words University of Maryland. ACL
(Association for Computational Linguistics).

Dyer, C., Lopez, A., Ganitkevitch, J., Weese,
J., Ture, F., Blumson, P., Setiawan, H., Eidel-
man, V., and Resnik, P. (2010). cdec: A De-
coder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Mod-
els. ACL (Association for Computational Lin-
guistics) Interactive Poster and Demonstration
Sessions, (July):7–12.

Federico, M., Bentivogli, L., Paul, M., and Stüker,
S. (2011). Overview of the IWSLT 2011 Eval-
uation Campaign. IWSLT (International Work-
shop on Spoken Language Translation).

Gascó, G., Rocha, M.-A., Sanchis-Trilles, G.,
Andrés-Ferrer, J., and Casacuberta, F. (2012).

Does More Data Always Yield Better Transla-
tions? EACL (European Association for Com-
putational Linguistics).

Heafield, K. (2011). KenLM : Faster and Smaller
Language Model Queries. WMT (Workshop on
Statistical Machine Translation).

Irvine, A. and Callison-burch, C. (2013). Combin-
ing Bilingual and Comparable Corpora for Low
Resource Machine Translation. WMT (Work-
shop on Statistical Machine Translation).

Koppel, M., Akiva, N., and Dagan, I. (2003).
A Corpus-Independent Feature Set for Style-
Based Text Categorization. IJCAI Workshop
on Computational Approaches to Style Analysis
and Synthesis.

Mirkin, S. and Besacier, L. (2014). Data Selection
for Compact Adapted SMT Models. AMTA (As-
sociation for Machine Translation in the Amer-
icas).

Moore, R. C. and Lewis, W. D. (2010). Intel-
ligent Selection of Language Model Training
Data. ACL (Association for Computational Lin-
guistics).

Papineni, K., Roukos, S., Ward, T., and Zhu, W.-j.
(2002). BLEU: a method for automatic evalu-
ation of machine translation. ACL (Association
for Computational Linguistics).

Sethy, A., Georgiou, P. G., Ramabhadran, B., and
Narayanan, S. S. (2009). An iterative rela-
tive entropy minimization based data selection
approach for n-gram model adaptation. IEEE
Transactions on Audio, Speech, and Language
Processing, 17(1):13–23.

Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A Study of Transla-
tion Edit Rate with Targeted Human Annota-
tion. AMTA (Association for Machine Trans-
lation in the Americas), (August):223–231.

Toral, A. (2013). Hybrid Selection of Language
Model Training Data Using Linguistic Informa-
tion and Perplexity. Workshop on Hybrid Ap-
proaches to Translation, pages 8–12.

Toutanova, K., Klein, D., Manning, C. D., and
Singer, Y. (2003). Feature-Rich Part-of-Speech
Tagging with a Cyclic Dependency Network.
NAACL (North American Association for Com-
putational Linguistics).

65


