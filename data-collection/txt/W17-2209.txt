



















































Modeling intra-textual variation with entropy and surprisal: topical vs. stylistic patterns


Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature.

,

Proceedings, pages 68–77, Vancouver, BC, August 4, 2017. c©2017 Association for Computational Linguistics

Modeling intra-textual variation with entropy and surprisal:
topical vs. stylistic patterns

Stefania Degaetano-Ortlieb
Saarland University

Campus A2.2
66123 Saarbrücken

s.degaetano@mx.uni-saarland.de

Elke Teich
Saarland University

Campus A2.2
66123 Saarbrücken

e.teich@mx.uni-saarland.de

Abstract

We present a data-driven approach to in-
vestigate intra-textual variation by com-
bining entropy and surprisal. With this
approach we detect linguistic variation
based on phrasal lexico-grammatical pat-
terns across sections of research articles.
Entropy is used to detect patterns typi-
cal of specific sections. Surprisal is used
to differentiate between more and less
informationally-loaded patterns as well as
types of information (topical vs. stylistic).
While we here focus on research articles in
biology/genetics, the methodology is es-
pecially interesting for digital humanities
scholars, as it can be applied to any text
type or domain and combined with addi-
tional variables (e.g. time, author or social
group) to obtain insights on intra-textual
variation.

1 Introduction

While there is an abundance of studies on lin-
guistic variation according to domain, register and
genre, text-internal variation, i.e. variation based
on changing micro-purposes within a text (Biber
and Finegan, 1994), has received much less atten-
tion. As such internal shifts occur in all kinds of
discourse — be it in spoken (such as spontaneous
conversation or speeches) or written mode (such
as literary texts, written editorials, research arti-
cles) — there has been recently a growing interest
in this type of variation. In general, knowledge on
intra-textual variation leads to a more comprehen-
sive understanding of the data underlying compu-
tational modeling, analysis, interpretation, etc.

In the field of NLP, there is a growing need in
the development of applications that consider vari-
ation also at the textual level to improve perfor-

mance. Considering research articles, approaches
within BioNLP, for instance, have moved from fo-
cusing on abstracts as sources of text mining to us-
ing also full-text articles (Cohen et al., 2010), not
least because this data is made available through
repositories such as PubMedCentral (PMC). To
obtain good performance, corpora created from
such resources are highly annotated with linguis-
tic as well as semantic categories characterizing
e.g. gene names. From these, specific features
are selected with a trade-off between ease of ex-
traction and desired type of information. In the
field of DH, intra-textual variation is considered
especially in literary studies, computational stylis-
tics, and authorship attribution. Hoover (2016)
shows, for example, how knowledge about differ-
ences between text parts helps to improve compu-
tational stylistic approaches. In corpus linguistics,
the common approach to intra-textual variation is
to start with a set of pre-defined linguistic fea-
tures (Biber and Finegan, 1994). While the choice
of features is clearly linguistically informed, this
initial step in analysis is manual and needs to be
carried out anew for every new text type or regis-
ter considered. Also, analysis is restricted to fre-
quency (i.e. unconditioned probabilities).

We present a methodology for investigating
intra-textual variation that is data-driven and based
on conditional probabilities which are calculated
using two information-theoretic measures, entropy
and surprisal. Being data-driven, our approach
can be applied to any text type or domain, avoid-
ing extensive annotations and manual selection of
features possibly involved in variation. Based on
probabilities conditioned on ambient and extra-
linguistic context, it allows to capture variation in
a more fine-grained manner than by considering
mere frequencies.

As a testbed for our approach, we use scien-
tific research articles in genetics, as they clearly

68



exhibit the typical IMRaD (Introduction, Meth-
ods, Results and Discussion) structure of scien-
tific articles, with internal shifts in purpose (see
e.g. Swales (1990)).

We use relative entropy (Kullback-Leibler Di-
vergence) to detect features typical of specific
sections. By considering surprisal (i.e. proba-
bilities of features in their ambient context), we
are able to detect the amount and type of infor-
mation these typical features convey, e.g. more
informationally-loaded expressions (e.g. terminol-
ogy) vs. less informationally-loaded expressions
(e.g. linguistic formula, such as These results show
that). Thus, besides possible topical variation
within articles across sections, we are able to de-
tect also variation of stylistic lexico-grammatical
patterns. While our focus is on research articles,
the methodology can be applied to any text type or
domain to detect (intra-textual) variation in a data-
driven way.

2 Related work

Related work in (corpus) linguistics has mainly
focused on variation across domains, registers or
genres (represented by corpora) and less on vari-
ation within text. Among the few approaches to
intra-textual variation is Swales’ work on moves,
discourse-structuring units with specific commu-
nicative purposes (Swales, 1990), which he ap-
plies to the analysis of research articles. A
different approach is taken by Biber and col-
leagues (e.g. Biber et al. (2007)), who use multi-
dimensional analysis considering detailed, pre-
defined linguistic features to observe intra-textual
variation across research article sections. Gray
(2015) applies the same approach to observe fea-
tures of ‘elaborated’ vs. ‘compressed’ grammati-
cal structures (e.g. finite complement clauses such
as that-clauses vs. adjectives as nominal pre-
modifiers) across disciplines and research article
sections. While quite detailed and linguistically
informed, these approaches are clearly biased to-
wards the pre-selection of features to be investi-
gated.

In computational stylistics, there is related work
on style variation of literary works, where it has
been recently shown that knowledge on intra-
textual variation among literary texts possibly
improves computational stylistic tasks (Hoover,
2016). In terms of methods, similar work is
done especially in the field of authorship attribu-

tion. These approaches aim to determine proba-
ble authors of disputed texts, ranging from con-
sidering frequencies of words, keywords and key-
ness to measures such as Burrow’s Delta and
Kullback-Leibler Divergence (see e.g. Burrows
(2002); Hoover (2004); Jannidis et al. (2015);
Pearl et al. (2016); Savoy (2016)). While we also
use Kullback-Leibler Divergence to obtain typical
features (here: of specific sections of research ar-
ticles), in our approach we also account for the
amount and type of information typical features
provide, allowing a more fine-grained differenti-
ation between topical vs. stylistic features.

In computational linguistics, a related prob-
lem is discourse segmentation. For an early
approach see e.g. TEXTTILING (Hearst, 1997),
a cohesion-driven approach for segmentation of
multi-paragraph subtopic structure. More re-
cently, topic modeling (notably LDA) has been ap-
plied to discourse segmentation as well (e.g. Misra
et al. (2011); see also Riedl and Biemann (2012)
for an overview). The dominant interest is on top-
ical shifts in text as indicator of discourse struc-
ture, however topic modeling estimation is compu-
tationally expensive and needs domain-adaptation.

Recently, there is also an increasing interest in
argumentative and rhetorical structure (e.g. Gou
et al. (2011); Séaghdha and Teufel (2014)). While
recent approaches in this field achieved promis-
ing results, they rely on highly annotated data and
have to be adapted for different domains.

Further, there is work on intra-textual variation
within the BioNLP community, motivated by the
need to extract biomedical knowledge not only
from abstracts, but also from full-text articles (Co-
hen et al., 2010). Besides the use of a pre-defined
linguistic feature set, in BioNLP also ontologies
are widely employed. This again involves a bias
towards feature selection, use of highly annotated
data combined with a restricted use to specific do-
mains.

More recently, information-theoretic notions
have been employed to analyze intra-textual vari-
ation. For example, Verspoor and colleagues em-
ploy Information Gain to measure the difference
between conditional probabilities of tokens being
part of a term within an ontology (Groza and Ver-
spoor, 2015). The intuition behind this is to model
the amount of information a token such as activity
provides when being part of a term such as alpha-
1, 6-mannosyltransferase activity. In this exam-

69



ple, activity provides a low amount of informa-
tion, as it is also widely used within other entries
(over 25,000) in the Gene Ontology. Others com-
bine entropy with a Bayesian approach to unsuper-
vised topic segmentation (Eisenstein and Barzilay,
2008).

We propose here to employ entropy and sur-
prisal to model intra-textual variation. First, this
allows us to detect linguistic features typical of
specific sections (rather than using pre-defined
ones), modeling intra-textual variation in a data-
driven way. Second, by considering the amount
of information (i.e. more or less informationally-
loaded) and the type of information these typi-
cal features provide (i.e. topical vs. stylistic), we
obtain a more comprehensive picture of the type
of variation. Moreover, while the majority of
approaches relies on lexical features, we take a
step of abstraction, focusing also on grammatical
patterns, which adds to the genericity of our ap-
proach.

3 Methodology

3.1 Data
As a dataset, we use a subsection of the SCITEX
corpus (Degaetano-Ortlieb et al., 2013) with re-
search articles from genetics, amounting to ap-
prox. 2.5 million tokens (see Table 1), and cov-
ering the years 2004 to 2006. For tokenization,
lemmatization and part-of-speech (POS) tagging,
we use TreeTagger (Schmid, 1994) with an up-
dated list of abbreviations specific to academic
writing. Sentence splitting is based on labels of
punctuations from POS information.

journal tokens texts
Gene 1,972,206 280
Nucleid Acids Research 612,988 71

Table 1: Journals with corpus size and number of
texts

The two selected journals have the advantage
of having a relatively systematic section labeling,
which allows us to automatically detect sections
by trigger words (e.g. Abstract, Introduction). The
automatic annotation is revised manually to ensure
a high quality section labeling. Table 2 shows the
amount of tokens across sections1.

1As the body of an article can be split into a variety of
sections, rather than trying to match these into methods and
result sections, we opted for putting this material into one
MAIN part.

section tokens
Abstract 33,577
Introduction 143,863
Main (Methods/Results) 2,136,679
Conclusion 271,075

Table 2: Section size

3.2 Methods
To observe differences in phrasal lexico-
grammatical patterns across sections of research
articles, we consider part-of-speech (POS) tri-
grams as features2, as they have shown to perform
best in inspecting lexico-grammatical patterns3.
To consider whether a phrasal pattern transports
more or less information, we also consider the
amount of information in bits being transmitted
by the lexical fillers of POS trigrams in a running
text. For this, we use a model of average surprisal
(AvS), i.e. the (negative log) probability of a given
unit (e.g. a word) in context (e.g. its preceding
words) for all its occurrences, measured in bits:4

AvS(w) =
1

|w|
∑

i

− log2 p(wi|wi−1wi−2wi−3) (1)

where wi is a word, wi−1 to wi−3 its three pre-
ceding words with p(wi|wi−1wi−2wi−3) being the
probability of a word given its preceding three
words. To obtain AvS values for POS trigrams,
we take the mean of the AvS of the three lexical
fillers:

AvS(trigrami) =
AvS(w1) + AvS(w2) + AvS(w3)

3
(2)

This allows us to measure the amount of informa-
tion in bits each instance i, i.e. each lexical real-
ization of a POS trigram, conveys. The distribu-
tion of AvS(trigrami) is divided up into three
quantiles, categorizing the data into low, middle
and high AvS ranges, a methodology that already

2We exclude POS trigrams consisting of characters con-
stituting sentence markers (e.g. fullstops, colons), brackets,
and symbols (e.g. equal sign).

3Note that bi-grams proved to be too short to capture
grammatical information (e.g. passives), four- and five-grams
lead to sparse data.

4For a similar approach see Genzel and Charniak (2002).

70



phrase type example trigram (POS.AvS) example
AdjP mod JJ.NN.NN.high paa2 gene cluster
Citation NP.CC.NP.high Indeed, Wolner and Gralla (12) showed that
Compound NP.NN.NP.high TbR-I inhibitor SB-431542
Gerund VVG.MD.VV.high silencing should prove
NP demonstrative DT.NNS.VHP.low these studies have
Passive NNS.VHP.VBN.high In plants, polyamines have been reported to play a crucial role

in morphogenesis
Past participle VVN.IN.DT.low Based on the data presented in Figure 5
PP mod NN.IN.JJ.middle use of alternative
Semi-modal VVP.TO.VB.low more detailed studies need to be done
that-clause IN.PP.MD.low but it was possible that they could be transcribed
to-inf evaluative JJ.TO.RB.middle useful to finally
V coordination NNS.CC.VV.high to functionally characterize the identified mutations and distin-

guish between polymorphisms
Evaluative it-pattern PP.VBZ.JJ.low it is remarkable that
VP existential EX.VBP.JJ.low There are several hypotheses about
VP interactant PP.VVP.IN.low we show that
VP modal MD.VV.DT.middle could explain the
VP reporting NNS.VVP.IN.low data suggest that

Table 3: Typical phrase types with examples of POS trigrams with AvS range and examples

proved to be useful in capturing diachronic vari-
ationDegaetano-Ortlieb and Teich (2016)5. We
then combine for each instance i information
about the POS trigram and the AvS range it be-
longs to. At the same time, this also provides for
each POS trigram the number of i with low, middle
and high AvS, i.e. how many times a POS trigram
occurs with low, middle or high AvS. These POS
trigrams with AvS ranges serve then as features,
providing a set of 19,776 features.

Detection of typical features from this fea-
ture set is based on Kullback-Leibler Divergence
(KLD; or relative entropy), a well-known measure
of (dis)similarity between probability distributions
(Kullback and Leibler, 1951) used in NLP, speech
processing, and information retrieval. Based on
work by Fankhauser et al. (2014a,b), we create
KLD models for each section (ABSTRACT, IN-
TRODUCTION, MAIN, CONCLUSION), calculat-
ing the average amount of additional bits per fea-
ture (here: POS trigrams with AvS ranges) needed
to encode features of a distribution A (e.g. AB-
STRACT) by using an encoding optimized for a
distribution I (e.g. INTRODUCTION). The more
additional bits are needed, the more distinct or dis-
tant A and I are. This is formalized as:

D(A||I) =
∑

i

p(featurei|A)log2 p(featurei|A)
p(featurei|I) (3)

where p(featurei|A) is the probability of a
feature in a section A (e.g. ABSTRACT) and

5We also considered a division into quartiles, but it proved
to be too narrow.

p(featurei|I) is the probability of that fea-
ture in a section I (e.g. INTRODUCTION).
The log2

p(featurei|A)
p(featurei|I) relates to the differ-

ence between the probability distributions
(log2p(featurei|A) − log2p(featurei|I)), giv-
ing the number of additional bits. These are then
weighted with the probability of p(featurei|A)
so that the sum over all featurei gives the
average number of additional bits per feature, i.e.
the relative entropy. This allows us to determine
whether any two sections are distinct or not
and if they are, to what degree and by which
features. For this, we inspect the ranking (based
on KLD values) of features for one section vs. the
other sections. In terms of typicality, the more
additional bits are used to encode a feature, the
more typical that feature is for a given section
vs. another section. For instance, in a compar-
ison between two sections (e.g. ABSTRACT vs.
INTRODUCTION), the higher the KLD value of a
features for a section (e.g. ABSTRACT), the more
typical that feature is for that given section. In
addition, we test for significance of a feature by an
unpaired Welch’s t-test. Thus, features considered
typical are distinctive according to KLD and show
a p-value below a given threshold (e.g. 0.05).

We thus obtain typical features for each sec-
tion, i.e. typical POS trigrams combined with AvS
ranges, allowing us to see whether a typical POS
trigram carries more or less information (i.e. the
amount of information) as defined by AvS.

For analysis, we then categorize typical POS tri-
grams into phrase types. Table 3 shows examples

71



high

middle

low

Abstract Introduction Main Conclusion

Gerund

PP/AdjP mod

PP/AdjP mod

VP interactant

VP reporting

Citation
AdjP mod

Citation
Passive

Passive

NP demonstrative

Compound

Passive
PP mod

Passive

Passive

Past participle

V coordination
Gerund

VP modal

VP modal

Gerund
to-inf evaluative

VP modal

Semi-modal
PP mod VP existential

Evaluative it-pattern
that-clause

N
um

be
r o

f P
O

S 
tr

ig
ra

m
 (p

at
te

rn
)

Number of POS trigram (pattern)

Evaluative it-pattern

Figure 1: Typical phrase types across sections and AvS range

of POS trigrams with AvS range by their phrase
type with examples of lexical realizations.

4 Analysis

In the analysis, we aim to explore intra-textual
variation taking a variationist approach (rather
than a text segmentation approach) and pursue the
following questions:

(a) Typical features: Which phrasal lexico-
grammatical patterns are typical of specific
sections?

(b) Amount of information: How much informa-
tion (by means of AvS) do phrasal lexico-
grammatical patterns convey?

(c) Type of information: What type of informa-
tion do phrasal lexico-grammatical patterns
convey?

4.1 Typical phrase types across sections
For better comparison across sections, Figure 1
shows the number of POS trigrams (patterns) for
a specific phrase type (on the x and y-axis) and

the frequency per million (fpM) of the phrase type
by circle size across sections with respect to high
(red), middle (yellow) and low (blue) AvS values.
For examples of each phrase type consider Table
3.

Considering ABSTRACT and low AvS (lower
left part of Figure 1), it is strongly characterized by
reporting patterns, mainly used with that-clauses
and relatively general nouns (e.g. data suggest
that, analysis showed that), and by interactant pat-
terns (such as we show that and we report here).
Considering the high AvS range (red), gerunds
(see Example 4) as well as adjectival and prepo-
sitional modification are typical (see Examples 5
and 6, respectively).

(4) Considering some severe limitations of viral
systems [...] synthetic nonviral systems are
highly desirable in the above applications.
(ABSTRACT; VVG.DT.JJ)

(5) The T. maritima rpoA gene coding the
subunit does not complement the
thermosensitive rpoA112 mutation of E.
coli. (ABSTRACT; JJ.NN.NN)

72



(6) The minichromosome maintenance (MCM)
proteins are thought to function as the
replicative helicases in eukarya and
archaea. (ABSTRACT; IN.NN.CC)

INTRODUCTION is characterized by passives
(e.g. been used with), especially with low AvS,
followed by citation with middle and high AvS
(e.g. Wolner and Gralla). Also typical is
the evaluative it-pattern (see Example 7) and a
demonstrative pattern (e.g. these studies/proteins
have) both in the context of presenting previous
work/knowledge.

(7) It has become evident in the last decade that
many, if not the majority, of genes are
regulated post-transcriptionally [...].
(INTRODUCTION, low AvS; PP.VHZ.VVN)

MAIN is strongly characterized by passives (e.g.
analysis was performed), especially with low AvS,
but also with middle and high AvS. Also typi-
cal in the low AvS range are past participle pat-
terns (e.g. performed as described, based on the),
gerund (e.g. purified by using), and coordination
(e.g. and visualized with). In addition, compound
patterns are typical in the high AvS range, being
clearly terminological (such as TbR-I inhibitor SB-
431542, SG parallel G-quadruplex, GC12/ GC3
correlation).

In CONCLUSION modal verb patterns are most
typical across all three AvS ranges (e.g. units
might result, could explain the). In addition, with
low AvS that-clauses are typical (e.g. suggests that
it may require), evaluative it-patterns (e.g. it is im-
portant to note, it is possible that) as well as semi-
modals (e.g. seem/appear to be), existentials (e.g.
there are several/other) and prepositional post-
modification (e.g. present/useful in the). Thus,
modality and evaluation are quite typical for CON-
CLUSION sections in genetics.

Comparing typical phrase types across sections,
we see that while for INTRODUCTION and MAIN
passives are quite typical (especially with low
AvS for both), ABSTRACT and CONCLUSION are
marked by relatively unique typical phrase types
(e.g. reporting verb phrases for ABSTRACT vs.
modal verb phrases for CONCLUSION).

While this is in line with observations made by
Biber and Finegan (1994), who have shown e.g.
a preference of passives in the main part of arti-
cles as well as a common use of modal verbs in

conclusions, besides other features (such as evalu-
ative patterns) we also show the amount of infor-
mation these features transmit (by AvS). Typical
phrase types with high AvS values belong mostly
to nominal groups (compounds and nouns mod-
ified by adjectives (AdjP mod) and prepositional
phrases (PP mod)) conveying topical information,
while those with low AvS values mostly to verb
groups (passives and verb phrases with different
functions such as reporting, evaluative, etc.) con-
veying a more stylistic type of information.

4.2 Amount of information and type of
information of typical phrase types

Zooming into the most frequent lexical realiza-
tions of specific patterns, gives a clearer picture
of the type of information conveyed by different
ranges of AvS.

Here, we present two examples: First, we zoom
into typical patterns of ABSTRACT, showing how
the type of information differs from topical to
stylistic based on the AvS range. Second, we look
at CONCLUSION considering its typical modal
verb phrase across AvS ranges.

Figure 2 shows lexical realizations of typical
phrase types within ABSTRACT across AvS ranges
(high: reddish, middle: yellowish, low: blueish)
with the size relating to frequency for each range.

Typical reporting verb patterns (VP reporting)
with low AvS values (blueish) make use of rela-
tively general nouns (data, analysis, results) with
verbs such as suggest, show and indicate. For VP
interactional, the phrase we show that is the domi-
nant lexical realization, followed, for example, by
phrases such as we characterized the/demonstrate
that/report here. The amount of information trans-
mitted by these phrases is relatively low. The pur-
pose of use of these phrases is more style-oriented
rather than topic-oriented.

Comparing this to lexical realizations of high
AvS values (reddish) for ABSTRACT (see again
Figure 2), we see that these are clearly related to
quite compact linguistic forms expressing either
processes with the gerund form (lining the gas-
trovacular) or scientific terms with adjectival (e.g.
multiple gene cassette) and prepositional modifi-
cation (e.g. helicases in eukarya and archaea6).
Clearly, the amount of information these phrases

6Note that for this pattern we have shown more context
for better understanding, as the pattern would only show
Preposition-Noun-Conjunction, which in the example is re-
alized as in eukarya and.

73



Figure 2: Lexical realizations of typical patterns
with high, middle and low AvS in ABSTRACT

transmit is relatively high and the type of informa-
tion is topic-oriented.

Lexical realizations of middle AvS lie in be-
tween, i.e. terms seem to be more generic (e.g. by
adjectival modification such as positive regulatory
factor or prepositional modification such as lack
of regulatory) and reporting verb phrases are used
in a less confined ambient context (e.g. showed
a common instead of showed that). Thus, these
phrases transmit a relatively moderate amount of
information and can be style- or topic-oriented.

In Figure 3 we zoom into CONCLUSION, show-
ing how the same typical phrase type (here: VP
modal; compare also with Figure 1) can differ in
the information type it conveys depending on its
AvS range. Here, lexical realizations of verb pat-
terns with modal verbs are shown for high (red-
dish), middle (yellowish), and low (blueish) AvS
values. With high AvS, the modal verb is used
in combination with specific terms (e.g. tlh genes,
tRNA isodecoders). From Examples 8 and 9,
we can see how within the whole sentences as-
sumptions are put forward about the two terms
tlh genes and tRNA isodecoders. In the middle
range, the modal verb patterns are used with a va-
riety of verbs. Examples 10 and 11 show relatively

generic preceding contexts (the structure of the
substrate in 10 and subtle changes in 11), which
are used with modal expressions of middle AvS.
In the lower range, the modal verbs are used with
a confined set of verbs (suggest, result), in rel-
atively formalized lexical phrases (may/might be
due), and in relational phrases (may be an, might
be the). Example 12 to 14 show a quite vague pre-
ceding context realized by the use of referring ex-
pressions such as this and there for modal verbs
used with low AvS.

Figure 3: Lexical realizations of modal verb pat-
terns (VP modal) with high, middle and low AvS
in CONCLUSION

(8) A possibility suggested by the presence of
transposable elements in the pericentromeric
heterochromatin of higher eukaryotes is that
the tlh genes might be parts of transposons
that preferentially transpose to
heterochromatic regions. (CONCLUSION)

(9) One can envision that tRNA isodecoders
may be more harmful than useful in
translation. (CONCLUSION)

(10) This assay is performed in the absence of
dNTP, so that the structure of the substrate

74



cannot be affected by polymerization .
(CONCLUSION)

(11) PAX 7 gene expression levels are highly
controlled during tissue development and
subtle changes could lead to important
effects. (CONCLUSION)

(12) Our work does not suggest that gene
expression contributes to the asymmetric
evolution of paralogs that we observed but
again this may be due to small sample size.
(CONCLUSION)

(13) This may be due to the short length ( 11 bp)
of the primer [...]. (CONCLUSION)

(14) There may be a few possible reasons for
why hix-AG is not bound by Hin [...].
(CONCLUSION)

Given that this is just one type of phrase, i.e.
modal verb phrase being typical for CONCLUSION
in genetics, by considering AvS we clearly see
how it still differs in the type of information it
transmits, depending on the ambient context it oc-
curs with, being either topical or stylistic.

5 Section classification

While in the analysis we have taken a variation-
ist approach, we also test how well sections can
be distinguished by typical features obtained by
our approach. Our baseline is a classifier using
all POS trigrams without AvS ranges. In Table 5
we report the F-Measure of three classifiers (Naive
Bayes, Support Vector Machine (SVM) and Ran-
domForest (RF)). Adding AvS ranges improves
classification for all classifiers. Using only typical
POS trigrams obtained by our approach improves
the model considerably. A further improvement is
achieved by considering typical POS trigrams with
AvS ranges. The random forest classifier achiev-
ing the best result with 86.0 of F-Measure.

set BL (NaiveBayes) SVM RF
POS 3grams 76.6 78.2 72.9
POS 3grams+AvS 77.0 80.3 74.2
typPOS 80.3 82.5 85.6
typPOS+AvS 81.1 81.0 86.0

Table 4: Classification results with typical POS tri-
grams and AvS ranges.

Considering classification performance of sec-
tions with Random Forest, ABSTRACT and MAIN

can be best predicted with 94.5 and 92.5 of F-
Measure, followed by INTRODUCTION with 82.8.
CONCLUSION is less well distinguishable, but still
achieves a considerable improvement when con-
sidering typical POS trigrams (from 17.4 to 61.2
of F-Measure).

set ABS INTRO MAIN CONC
POS 3grams 84.1 71.2 88.2 17.4
POS 3grams+AvS 84.8 75.5 87.9 20.1
typPOS 93.3 81.0 92.7 61.2
typPOS+AvS 94.5 82.8 92.5 60.5

Table 5: Classification results by F-Measure for
each section (RandomForest)

6 Conclusion

This paper has presented a novel data-driven ap-
proach to intra-textual variation. We have shown
how sections of research articles from genetics dif-
fer with respect to the phrasal lexico-grammatical
patterns used across sections (see Section 4.1).
We used relative entropy to obtain typical lexico-
grammatical patterns for each section. Moreover,
we have modeled the amount and type of informa-
tion these lexico-grammatical patterns convey (see
Section 4.2) by using average surprisal (AvS),
showing that sections vary in topical as well as
stylistic type of information. In future work, we
plan to model different scientific domains to in-
vestigate which of these lexico-grammatical pat-
terns would generalize across domains and which
are domain-specific.

Being data-driven and using part-of-speech in-
formation to generate features (see Section 3.2),
our approach can be applied to any other domain,
text type and even other languages (given a good
quality POS annotation), since it is not biased by
topical variation. While here we have modeled
intra-textual variation, additional variables such as
time, author, social group, production type, lan-
guage etc. can be integrated into the model. For
an application on diachronic data see Degaetano-
Ortlieb et al. (2016) and Degaetano-Ortlieb and
Teich (2016). As long as the variables are known
(e.g. publication dates for time, author names for
author, etc.), our approach allows to investigate
variation at a more abstract linguistic level than
topical variation. Thus, our approach is directly
relevant to studies in sociolinguistics, historical
linguistics and digital humanities in general.

Assessing the amount and type of information

75



of typical lexico-grammatical patterns is relevant
for more sophisticated text analysis. For exam-
ple, historical linguists might be interested in the
whole AvS range, as specific linguistic features
might move across time between high, middle and
low AvS. A linguistic feature might have high AvS
in one time period (e.g. when it enters language
use its ambient context may be expected to vary
a lot), and low AvS in a later time period (where
the feature is well-established in language use and
might be more confined to a specific ambient con-
text). The transition period would be seen in the
use of the feature in the middle AvS range. In
information retrieval, instead, features with high
AvS are more relevant as they convey more infor-
mation and are topic/content-related. AvS ranges
could also be more fine-grained in this scenario to
distinguish relatively established from new terms.
Considering more fine-grained ranges of high AvS
combined with time as a variable might be a pos-
sible way to explore knowledge change.

7 Acknowledgments

This work is funded by Deutsche Forschungs-
gemeinschaft (DFG) under grants SFB 1102:
Information Density and Linguistic En-
coding (www.sfb1102.uni-saarland.de) and
EXC 284: Multimodal Computing and Interac-
tion (www.mmci.uni-saarland.de). We are also
indebted to Stefan Fischer for his contributions
to corpus processing and Peter Fankhauser
(IdS Mannheim) for his support in statistical anal-
ysis. Also, we thank the anonymous reviewers for
their valuable comments.

References
Douglas Biber, Ulla Connor, and Thomas A. Upton.

2007. Discourse on the Move: Using Corpus Anal-
ysis to Describe Discourse Structure. Benjamins,
Amsterdam.

Douglas Biber and Edward Finegan. 1994. Intra-
textual Variation within Medical Research Articles.
In Susan Conrad and Douglas Biber, editors, Vari-
ation in English: Multi-dimensional Studies, Rout-
ledge Taylor & Francis Group, pages 108–123.

John Burrows. 2002. Delta: a Measure of Stylis-
tic Difference and a Guide to Likely Authorship.
Literary and Linguistic Computing 17(3):267–287.
https://doi.org/10.1093/llc/17.3.267.

K. Bretonnel Cohen, Helen L. Johnson, Karin Ver-
spoor, Christophe Roeder, and Lawrence E. Hunter.

2010. The Structural and Content Aspects of Ab-
stracts versus Bodies of Full Text Journal Articles
are Different. BMC Bioinformatics 11(492):1–10.

Stefania Degaetano-Ortlieb, Hannah Kermes, Ashraf
Khamis, and Elke Teich. 2016. An Information-
Theoretic Approach to Modeling Diachronic
Change in Scientific English. In Carla Suhr, Terttu
Nevalainen, and Irma Taavitsainen, editors, Selected
Papers from Varieng - From Data to Evidence (d2e),
Brill, Language and Computers.

Stefania Degaetano-Ortlieb, Hannah Kermes, Ekate-
rina Lapshinova-Koltunski, and Elke Teich. 2013.
SciTex - A Diachronic Corpus for Analyzing the De-
velopment of Scientific Registers. In Paul Bennett,
Martin Durrell, Silke Scheible, and Richard J. Whitt,
editors, New Methods in Historical Corpus Linguis-
tics, Narr, volume 3 of Corpus Linguistics and In-
terdisciplinary Perspectives on Language - CLIP,
pages 93–104.

Stefania Degaetano-Ortlieb and Elke Teich. 2016.
Information-based Modeling of Diachronic Linguis-
tic Change: From Typicality to Productivity. In Nils
Reiter, Beatrice Alex, and Kalliopi A. Zervanou,
editors, Proceedings of the 10th SIGHUM Work-
shop on Language Technology for Cultural Her-
itage, Social Sciences, and Humanities. Association
for Computational Linguistics (ACL).

Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
Unsupervised Topic Segmentation. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Association for Computational Linguistics, Strouds-
burg, PA, USA, EMNLP ’08, pages 334–343.
http://dl.acm.org/citation.cfm?id=1613715.1613760.

Peter Fankhauser, Hannah Kermes, and Elke Teich.
2014a. Combining Macro- and Microanalysis for
Exploring the Construal of Scientific Disciplinarity.
In Digital Humanities. Lausanne, Switzerland.
URL: http://dharchive.org/paper/DH2014/Poster-
126.xml.

Peter Fankhauser, Jörg Knappen, and Elke Teich.
2014b. Exploring and Visualizing Variation
in Language Resources. In Proceedings of
the Ninth International Conference on Lan-
guage Resources and Evaluation (LREC ’14).
European Language Resources Association
(ELRA), Reykjavik, pages 4125–4128. http://nbn-
resolving.de/urn:nbn:de:bsz:mh39-26224.

Dmitriy Genzel and Eugene Charniak. 2002. En-
tropy Rate Constancy in Text. In Proceedings
of the 40th Annual Meeting on Association
for Computational Linguistics (ACL). Associ-
ation for Computational Linguistics, Philade-
phia, Pennsylvania, USA, pages 199–206.
http://dl.acm.org/citation.cfm?id=1073117.

Yufan Gou, Anna Korhonen, and Thierry Poibeau.
2011. A Weakly-supervised Approach to Argu-

76



mentative Zoning of Scientific Documents. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP). Edin-
burgh, UK, pages 273–283.

Bethany Gray. 2015. On the Complexity of Academic
Writing. Disciplinary Variation and Structural Com-
plexity. In Viviana Cortes and Eniko Csomay, ed-
itors, Corpus-based Research in Applied Linguis-
tics. Studies in Honor of Doug Biber, John Ben-
jamins Publishing Company, Amsterdam / Philadel-
phia, volume 66 of Studies in Corpus Linguistics
(SCL), pages 49–77.

Tudor Groza and Karin Verspoor. 2015. Assessing the
Impact of Case Sensitivity and Term Information
Gain on Biomedical Concept Recognition. PLoS
One 10(3):1–22.

Marti A. Hearst. 1997. TextTiling: Segmenting Text
into Multi-paragraph Subtopic Passages. Computa-
tional Linguistics 23(1):33–64.

David L. Hoover. 2004. Testing Burrows’s Delta.
Literary and Linguistic Computing 19(4):453–475.
https://doi.org/10.1093/llc/19.4.453.

David L. Hoover. 2016. Textual Variation, Text-
Randomization, and Microanalysis. In Proceedings
of Digital Humanities Conference (DH). Kraków,
Poland, pages 223–225.

Fotis Jannidis, Steffen Pielström, Christof Schöch, and
Thorsten Vitt. 2015. Improving Burrows’ Delta - An
Empirical Evaluation of Text Distance Measures. In
Digital Humanities Conference (DH). Sydney, Aus-
tralia.

Solomon Kullback and Richard A. Leibler. 1951. On
Information and Sufficiency. The Annals of Mathe-
matical Statistics 22(1):79–86.

Hemant Misra, François Yvon, Olivier Cappé,
and Joemon Jose. 2011. Text Segmentation:
A Topic Modeling Perspective. Information
Processing & Management 47(4):528 – 544.
https://doi.org/http://dx.doi.org/10.1016/j.ipm.2010.11.008.

Lisa Pearl, Kristine Lu, and Anousheh Haghighi.
2016. The Character in the Letter: Epis-
tolary Attribution in Samuel Richardsons
Clarissa. Digital Scholarship in the Humanities
https://doi.org/https://doi.org/10.1093/llc/fqw007.

Martin Riedl and Chris Biemann. 2012. Text Segmen-
tation with Topic Models. Journal for Language
Technology and Computational Linguistics (JLCL)
27(1):47–70.

Jacques Savoy. 2016. Estimating the Proba-
bility of an Authorship Attribution. Jour-
nal of the Association for Information Sci-
ence and Technology 67(6):1462–1472.
https://doi.org/10.1002/asi.23455.

Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing. Manchester, UK, pages 44–49.

Diarmuid Ó Séaghdha and Simone Teufel. 2014. Un-
supervised Learning of Rhetorical Structure with
Un-topic Models. In Proceedings of the 25th Inter-
national Conference on Computational Linguistics
(COLING). Dublin, Ireland, pages 2–13.

John M. Swales. 1990. Genre Analysis: English
in Academic and Research Settings. Cambridge
Applied Linguistics. Cambridge University Press,
Cambridge.

77


