



















































Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 475–480,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance
Classification with Branch-LSTM

Elena Kochkina12, Maria Liakata12, Isabelle Augenstein3
1 University of Warwick, Coventry, United Kingdom

2 Alan Turing Institute, London, United Kingdom
3 University College London, London, United Kingdom

{E.Kochkina, M.Liakata}@warwick.ac.uk, I.Augenstein@ucl.ac.uk

Abstract

This paper describes team Turing’s sub-
mission to SemEval 2017 RumourEval:
Determining rumour veracity and support
for rumours (SemEval 2017 Task 8, Sub-
task A). Subtask A addresses the challenge
of rumour stance classification, which in-
volves identifying the attitude of Twitter
users towards the truthfulness of the ru-
mour they are discussing. Stance classi-
fication is considered to be an important
step towards rumour verification, therefore
performing well in this task is expected
to be useful in debunking false rumours.
In this work we classify a set of Twit-
ter posts discussing rumours into either
supporting, denying, questioning or com-
menting on the underlying rumours. We
propose a LSTM-based sequential model
that, through modelling the conversational
structure of tweets, which achieves an ac-
curacy of 0.784 on the RumourEval test set
outperforming all other systems in Sub-
task A.

1 Introduction

In stance classification one is concerned with de-
termining the attitude of the author of a text to-
wards a target (Mohammad et al., 2016). Targets
can range from abstract ideas, to concrete entities
and events. Stance classification is an active re-
search area that has been studied in different do-
mains (Ranade et al., 2013; Chuang and Hsieh,
2015). Here we focus on stance classification of
tweets towards the truthfulness of rumours circu-
lating in Twitter conversations in the context of
breaking news. Each conversation is defined by
a tweet that initiates the conversation and a set of
nested replies to it that form a conversation thread.
The goal is to classify each of the tweets in the

conversation thread as either supporting, denying,
querying or commenting (SDQC) on the rumour
initiated by the source tweet. Being able to detect
stance automatically is very useful in the context
of events provoking public resonance and associ-
ated rumours, as a first step towards verification of
early reports (Zhao et al., 2015). For instance, it
has been shown that rumours that are later proven
to be false tend to spark significantly larger num-
bers of denying tweets than rumours that are later
confirmed to be true (Mendoza et al., 2010; Proc-
ter et al., 2013; Derczynski et al., 2014; Zubiaga
et al., 2016b).

Here we focus on exploiting the conversational
structure of social media threads for stance clas-
sification and introduce a novel LSTM-based ap-
proach to harness conversations.

2 Related Work

Single Tweet Stance Classification Stance
classification for rumours was pioneered by
Qazvinian et al. (2011) as a binary classification
task (support/denial). Zeng et al. (2016) perform
stance classification for rumours emerging during
crises. Both works use tweets related to the same
rumour during training and testing.

A model based on bidirectional LSTM encod-
ing of tweets conditioned on targets has been
shown to achieve state-of-the-art on the SemEval-
2016 task 6 dataset (Augenstein et al., 2016).
However the RumourEval task is different as it ad-
dresses conversation threads.

Sequential Stance Classification Lukasik et al.
(2016) and Zubiaga et al. (2016a) consider the
sequential nature of tweet threads in their works.
Lukasik et al. (2016) employ Hawkes processes
to classify temporal sequences of tweets. They
show the importance of using both the textual con-
tent and temporal information about the tweets,
disregarding the discourse structure. Zubiaga et

475



@user0 LOL. 5 million Muslims 
in France, what a disgrace. the 
french worm president and 
politicians killed them. tine for 
croissants now

user 3 comment

user 4

@user3 They who? Stupid and 
partial opinions like this one only 
add noise to any debate.

deny

user 3

@user4 Socialists, Antisemites, 
anti zionists - usual suspects

comment

user 0

France: 10 people dead after 
shooting at HQ of satirical 
weekly newspaper 
#CharlieHebdo, according to 
witnesses http://t.co/
FkYxGmuS58

user 1

MT @user0 France: 10 dead 
after shooting at HQ of satirical 
weekly #CharlieHebdo. If 
Zionists/Jews did this they'd be 
nuking Israel

source
support

comment

branch 1

response 1

user 2
@user0 @user5 A French 
crime of passion or another 
heathen moslem atrocity?

query

branch 2

response 3

response 2

Figure 1: Example of a conversation thread from
the dataset with three branches, two of which are
highlighted. The conversation has a tree structure,
which can be split into individual branches by tak-
ing each leaf node with all its direct parents.

al. (2016a) model the conversational structure of
source tweets and subsequent replies: as a linear
chain and as a tree. They use linear- and tree- ver-
sions of a CRF classifier, outperforming the ap-
proach by Lukasik et al. (2016).

3 Dataset

The dataset provided for this task contains Twit-
ter conversation threads associated with rumours
around ten different events in breaking news, in-
cluding the Paris shootings in Charlie Hebdo, the
Ferguson unrest, the crash of a Germanwings
plane. These events include 325 conversation
threads consisting of 5568 underlying tweets an-
notated for stance at the tweet level (breakdown
between training, testing and development sets is
shown in Table 1) (Derczynski et al., 2017).

# threads # branches # tweets
Development 25 215 281
Testing 28 772 1049
Training 272 3030 4238
Total 325 4017 5568

Table 1: Number of threads, branches and tweets
in the training, development and testing sets.

S D Q C
Development 69 11 28 173
Testing 94 71 106 778
Training 841 333 330 2734
Total 1004 415 464 3685

Table 2: Per-class distribution of tweets in the
training, development and testing sets.

Each thread includes a source tweet that initi-
ates a conversation and nested tweets responding
to either the source tweet or earlier replies. The
thread can be split into linear branches of tweets,
where a branch is defined as a chain of tweets that
starts with a leaf tweet including its direct parent
tweets, all the way up to the source tweet. Fig-
ure 1 shows an example of a conversation along
with its annotations represented as a tree struc-
ture with highlighted branches. The depth of a
tweet is the number of its parents starting from
the root node. Branches 1 and 2 in Figure 1 have
depth one whereas branch 3 has depth three. There
is a clear class imbalance in favour of comment-
ing tweets (66%) and supporting tweets (18%),
whereas the denying (8%) and querying classes
(8%) are under-represented (see Table 2). While
this imbalance poses a challenge, it is also indica-
tive of the realistic scenario where only a few users
question the veracity of a statement.

4 System Description

4.1 Features

Prior to generating features for the tweets, we per-
form a pre-processing step where we remove non-
alphabetic characters, convert all words to lower
case and tokenise texts.1 Once tweet texts are pre-
processed, we extract the following features:
• Word vectors: we use a word2vec (Mikolov

et al., 2013) model pre-trained on the Google
News dataset (300d) 2 using the gensim pack-
age (Řehůřek and Sojka, 2010).
• Tweet lexicon: (1) count of negation words3

and (2) count of swear words.4

1For implementation of all pre-processing routines we use
Python 2.7 with the NLTK package.

2We have also tried using Glove word embeddings trained
on Twitter dataset, but it lead to a decrease in performance on
both development and testing sets comparing to the Google
News word vectors

3A presence of any of the following words would be con-
sidered as a presence of negation: not, no, nobody, nothing,
none, never, neither, nor, nowhere, hardly, scarcely, barely,
don’t, isn’t, wasn’t, shouldn’t, wouldn’t, couldn’t, doesn’t

4A list of 458 bad words was taken from
http://urbanoalvarez.es/blog/2008/04/04/bad-words-list/

476



[As querying] @username Weren’t you the
one who abused her?

[As supporting] ”Go online & amp; put down
’Hillary Clinton illness,’” Rudy says. Yes
– but look up the truth – not health smears
https://t.co/EprqiZhAxM

[As supporting] @username I demand you
retract the lie that people in #Ferguson were
shouting ”kill the police”, local reporting has
refuted your ugly racism

[As commenting] @FoxNews six years ago...
real good evidence. Not!

Figure 2: Examples of misclassified denying
tweets.

• Punctuation: (1) presence of a period, (2)
presence of an exclamation mark, (3) pres-
ence of a question mark, (4) ratio of capital
letters.
• Attachments: (1) presence of a URL and (2)

presence of images.
• Relation to other tweets (1) Word2Vec

cosine similarity wrt source tweet, (2)
Word2Vec cosine similarity wrt preceding
tweet, and (3) Word2Vec cosine similarity
wrt thread
• Content length: (1) word count and (2) char-

acter count.
• Tweet role: whether the tweet is a source

tweet of a conversation.
Tweet representations are obtained by averaging
word vectors in a tweet and then concatenating
with the additional features into a single vector,
at the preprocessing step. This set of features have
shown to be the best comparing to using word2vec
features on their own or any of the reduced com-
binations of these features.

4.2 Branch - LSTM Model

To tackle the task of rumour stance classificaiton,
we propose branch-LSTM, a neural network archi-
tecture that uses layers of LSTM units (Hochre-
iter and Schmidhuber, 1997) to process the whole
branch of tweets, thus incorporating structural in-
formation of the conversation (see the illustration
of the branch-LSTM on the Figure 3). The input
at each time step i of the LSTM layer is the rep-
resentation of the tweet as a vector. We record the

supporting commenting denying commenting

t1 t2 t3 t4

source 
tweet response 1 response 3response 2twe

y

c1 c4c3c2

france
people

witnesses
… +

# words
————

||
LOL

million

now
… +

# words
————

||
they
who

debate
… +

# words
————

||
socialists

antisemites

suspects
… +

# words
————

||

Figure 3: Illustration of the input/output structure
of the branch-nestedLSTM model.

output of each time step so as to attach a label to
each tweet in a branch5. This output is fed through
several dense ReLU layers, a 50% dropout layer,
and then through a softmax layer to obtain class
probabilities. We use zero-padding and masks to
account for the varying lengths of tweet branches.
The model is trained using the categorical cross
entropy loss function. Since there is overlap be-
tween branches originating from the same source
tweet, we exclude the repeating tweets from the
loss function using a mask at the training stage.
The model uses tweet representation as the mean
average of word vectors concatenated with ex-
tra features described above. Due to the short
length of tweets, using more complex models for
learning tweet representations, such as an LSTM
that takes each word as input at each time step
and returns the representation at the final time
step, does not lead to a noticeable difference in
the performance based on cross-validation experi-
ments on the training and development sets, while
taking significantly longer to train. We experi-
mented with replacing the unidirectional LSTMs
with bidirectional LSTMs but could observe no
improvements in accuracy (using cross-validation
results on the training and development set).

5 Experimental Setup

The dataset is split into training, development and
test sets by the task organisers. We determined
the optimal set of hyperparameters via testing the
performance of our model on the development set
for different parameter combinations. We used the

5For implementation of all models we used Python li-
braries Theano (Bastien et al., 2012) and Lasagne (Dieleman
et al., 2015).

477



Accuracy Macro F S D Q C
Development 0.782 0.561 0.621 0.000 0.762 0.860
Testing 0.784 0.434 0.403 0.000 0.462 0.873

Table 3: Results on the development and testing sets. Accuracy and F1 scores: macro-averaged and per
class (S: supporting, D: denying, Q: querying, C: commenting).

Depth # tweets # S # D # Q # C Accuracy MacroF S D Q C
0 28 26 2 0 0 0.929 0.481 0.963 0.000 0.000 0.000
1 704 61 60 81 502 0.739 0.348 0.000 0.000 0.550 0.842
2 128 3 6 7 112 0.875 0.233 0.000 0.000 0.000 0.933
3 60 2 1 5 52 0.867 0.232 0.000 0.000 0.000 0.929
4 41 0 0 3 38 0.927 0.481 0.000 0.000 0.000 0.962
5 27 1 0 1 25 0.926 0.321 0.000 0.000 0.000 0.961
6+ 61 1 2 9 49 0.803 0.223 0.000 0.000 0.000 0.891

Table 4: Number of tweets per depth and performance at each of the depths.

Tree of Parzen Estimators (TPE) algorithm 6 to
search the parameter space, which is defined as
follows: the number of dense ReLU layers varies
from one to four; the number of LSTM layers is
one or two; the mini-batch size is either 32 or 64;
the number of units in the ReLU layer is one of
{100, 200, 300, 400, 500}, and in the LSTM layer
one of {100, 200, 300}; the strength of the L2 reg-
ularisation is one of {0.0, 1e-4, 3e-4, 1e-3} and
the number of epochs is selected from {30, 50, 70,
100}. We performed 100 trials of different param-
eter combinations optimising for accuracy on the
development set in order to choose the best com-
bination. We fixed hyperparameters to train the
model on combined training and development sets
and evaluated on the held out test set.

6 Results

The performance of our model on the testing and
development set is shown in Table 3. Together
with the accuracy we show macro-averaged F-
score and per-class macro-averaged F-scores as
these metrics account for the class imbalance. The
difference in accuracy between testing and devel-
opment set is minimal, however we see significant
difference in Macro-F score due to different class
balance in these sets. Macro-F score could be im-
proved if we used it as a metric for optimising
hyper-parameters. The branch-LSTM model pre-
dicts commenting, the majority class well, how-
ever it is unable to pick out any denying, the most-
challenging under-represented class. Most deny-

6We used the implementation of the TPE algorithm in the
hyperopt package (Bergstra et al., 2013)

Label
Prediction C D Q S

Commenting 760 0 12 6
Denying 68 0 1 2
Querying 69 0 36 1
Supporting 67 0 1 26

Table 5: Confusion matrix for testing set predic-
tions

ing instances get misclassified as commenting (see
Table 5), with only one tweet misclassified as
querying and two as supporting (Figure 2). An
increased amount of labelled data would be help-
ful to improve performance of this model. As we
were considering conversation branches, it is in-
teresting to analyse the performance distribution
across different tweet depths (see Table 4). Maxi-
mum depth/branch length in the testing set is 13
with most tweets concentrated at depths from 0
to 3. Source tweets (depth zero) are usually sup-
porting and the model predicts these very well, but
performance of supporting tweets at other depths
decreases. The model does not show a notice-
able difference in performance on tweets of vary-
ing lengths.

7 Conclusions

This paper describes the Turing system entered in
the SemEval-2017 Task 8 Subtask A. Our method
decomposes the tree structure of conversations
into linear sequences and achieves accuracy 0.784
on the testing set and sets the state-of-the-art for
rumour stance classification. In future work we
plan to explore different methods for modelling
tree-structured conversations.

478



Acknowledgments

This work was supported by The Alan Turing
Institute under the EPSRC grant EP/N510129/1.
Cloud computing resource were kindly provided
through a Microsoft Azure for Research Award.
Work by Elena Kochkina was partially supported
by the Leverhulme Trust through the Bridges Pro-
gramme.

References
Isabelle Augenstein, Tim Rocktäschel, Andreas Vla-

chos, and Kalina Bontcheva. 2016. Stance Detection
with Bidirectional Conditional Encoding. In Pro-
ceedings of EMNLP.

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, David Warde-Farley, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. arXiv preprint arXiv:1211.5590 .

James Bergstra, Daniel Yamins, and David D Cox.
2013. Making a science of model search: Hyper-
parameter optimization in hundreds of dimensions
for vision architectures. ICML (1) 28:115–123.

Ju-han Chuang and Shukai Hsieh. 2015. Stance classi-
fication on ptt comments. In Proceedings of the 29th
Pacific Asia Conference on Language, Information
and Computation.

Leon Derczynski, Kalina Bontcheva, Maria Li-
akata, Rob Procter, Geraldine Wong Sak Hoi,
and Arkaitz Zubiaga. 2017. Semeval-2017 task
8: Rumoureval: Determining rumour veracity
and support for rumours. In Proceedings of the
11th International Workshop on Semantic Evalu-
ation (SemEval-2017). Association for Computa-
tional Linguistics, Vancouver, Canada, pages 69–76.
http://www.aclweb.org/anthology/S17-2006.

Leon Derczynski, Kalina Bontcheva, Michal Lukasik,
Thierry Declerck, Arno Scharl, Georgi Georgiev,
Petya Osenova, Toms Pariente Lobo, Anna Kolli-
akou, Robert Stewart, et al. 2014. Pheme: comput-
ing veracity: the fourth challenge of big social data.
In Proceedings of ESWC EU Project Networking.

Sander Dieleman, Jan Schlüter, Colin Raffel, Eben Ol-
son, Sren Kaae Sønderby, Daniel Nouri, Daniel Mat-
urana, Martin Thoma, Eric Battenberg, Jack Kelly,
Jeffrey De Fauw, Michael Heilman, Diogo Moit-
inho de Almeida, Brian McFee, Hendrik Weideman,
Gbor Takács, Peter de Rivaz, Jon Crall, Gregory
Sanders, Kashif Rasul, Cong Liu, Geoffrey French,
and Jonas Degrave. 2015. Lasagne: First release.
https://doi.org/10.5281/zenodo.27878.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Michal Lukasik, P. K. Srijith, Duy Vu, Kalina
Bontcheva, Arkaitz Zubiaga, and Trevor Cohn.
2016. Hawkes processes for continuous time se-
quence classification: an application to rumour
stance classification in twitter. In Proceedings of the
54th Meeting of the Association for Computational
Linguistics. Association for Computer Linguistics,
pages 393–398.

Marcelo Mendoza, Barbara Poblete, and Carlos
Castillo. 2010. Twitter under crisis: Can we trust
what we RT? In 1st Workshop on Social Media An-
alytics. SOMA’10, pages 71–79.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Saif M Mohammad, Svetlana Kiritchenko, Parinaz
Sobhani, Xiaodan Zhu, and Colin Cherry. 2016.
Semeval-2016 task 6: Detecting stance in tweets. In
Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval. volume 16.

Rob Procter, Farida Vis, and Alex Voss. 2013. Read-
ing the riots on twitter: methodological innovation
for the analysis of big data. International journal of
social research methodology 16(3):197–214.

Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden-
tifying misinformation in microblogs. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics, pages 1589–1599.

Sarvesh Ranade, Rajeev Sangal, and Radhika Mamidi.
2013. Stance classification in online debates by rec-
ognizing users’ intentions. In Proceedings of the
SIGDIAL 2013 Conference. pages 61–69.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. ELRA, Valletta,
Malta, pages 45–50. http://is.muni.cz/
publication/884893/en.

Li Zeng, Kate Starbird, and Emma S Spiro. 2016.
#unconfirmed: Classifying rumor stance in crisis-
related social media messages. In Tenth Interna-
tional AAAI Conference on Web and Social Media.

Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. En-
quiring minds: Early detection of rumors in social
media from enquiry posts. In Proceedings of the
24th International Conference on World Wide Web.
ACM, pages 1395–1405.

Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob
Procter, and Michal Lukasik. 2016a. Stance classifi-
cation in rumours as a sequential task exploiting the
tree structure of social media conversations. In Pro-
ceedings of COLING, the International Conference
on Computational Linguistics.

479



Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-
dine Wong Sak Hoi, and Peter Tolmie. 2016b.
Analysing how people orient to and spread rumours
in social media by looking at conversational threads.
PloS one 11(3):e0150989.

480


