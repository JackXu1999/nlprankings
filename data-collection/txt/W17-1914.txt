



















































Word Sense Filtering Improves Embedding-Based Lexical Substitution


Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 110–119,
Valencia, Spain, April 4 2017. c©2017 Association for Computational Linguistics

Word Sense Filtering Improves Embedding-Based Lexical Substitution

Anne Cocos∗, Marianna Apidianaki∗† and Chris Callison-Burch∗
∗ Computer and Information Science Department, University of Pennsylvania

† LIMSI, CNRS, Université Paris-Saclay, 91403 Orsay
{acocos,marapi,ccb}@seas.upenn.edu

Abstract

The role of word sense disambiguation in
lexical substitution has been questioned
due to the high performance of vector
space models which propose good sub-
stitutes without explicitly accounting for
sense. We show that a filtering mecha-
nism based on a sense inventory optimized
for substitutability can improve the results
of these models. Our sense inventory
is constructed using a clustering method
which generates paraphrase clusters that
are congruent with lexical substitution an-
notations in a development set. The re-
sults show that lexical substitution can still
benefit from senses which can improve the
output of vector space paraphrase ranking
models.

1 Introduction

Word sense has always been difficult to de-
fine and pin down (Kilgarriff, 1997; Erk et al.,
2013). Recent successes of embedding-based,
sense-agnostic models in various semantic tasks
cast further doubt on the usefulness of word sense.
Why bother to identify senses if even humans can-
not agree upon their nature and number, and if
simple word-embedding models yield good results
without using any explicit sense representation?

Word-based models are successful in various
semantic tasks even though they conflate multiple
word meanings into a single representation. Based
on the hypothesis that capturing polysemy could
further improve their performance, several works
have focused on creating sense-specific word em-
beddings. A common approach is to cluster the
contexts in which the words appear in a corpus
to induce senses, and relabel each word token
with the clustered sense before learning embed-

dings (Reisinger and Mooney, 2010; Huang et al.,
2012). Iacobacci et al. (2015) disambiguate the
words in a corpus using a state-of-the-art WSD
system and then produce continuous representa-
tions of word senses based on distributional infor-
mation obtained from the annotated corpus. Mov-
ing from word to sense embeddings generally im-
proves their performance in word and relational
similarity tasks but is not beneficial in all settings.
Li and Jurafsky (2015) show that although multi-
sense embeddings give improved performance in
tasks such as semantic similarity, semantic rela-
tion identification and part-of-speech tagging, they
fail to help in others, like sentiment analysis and
named entity extraction (Li and Jurafsky, 2015).

We show how a sense inventory optimized for
substitutability can improve the rankings provided
by two sense-agnostic, vector-based lexical sub-
stitution models. Lexical substitution requires
systems to predict substitutes for target word in-
stances that preserve their meaning in context
(McCarthy and Navigli, 2007). We consider a
sense inventory with high substitutability to be one
which groups synonyms or paraphrases that are
mutually-interchangeable in the same contexts. In
contrast, sense inventories with low substitutabil-
ity might group words linked by different types of
relations. We carry out experiments with a syntac-
tic vector-space model (Thater et al., 2011; Apid-
ianaki, 2016) and a word-embedding model for
lexical substitution (Melamud et al., 2015). In-
stead of using the senses to refine the vector rep-
resentations as in (Faruqui et al., 2015), we use
them to improve the lexical substitution rankings
proposed by the models as a post-processing step.
Our results show that senses can improve the per-
formance of vector-space models in lexical substi-
tution tasks.

110



2 A sense inventory for substitution

2.1 Paraphrase substitutability

The candidate substitutes used by our rank-
ing models come from the Paraphrase Database
(PPDB) XXL package (Ganitkevitch et al.,
2013).1 Paraphrase relations in the PPDB are
defined between words and phrases which might
carry different senses. Cocos and Callison-Burch
(2016) used a spectral clustering algorithm to
cluster PPDB XXL into senses, but the clusters
contain noisy paraphrases and paraphrases linked
by different types of relations (e.g. hypernyms,
antonyms) which are not always substitutable. We
use a slightly modified version of their method to
cluster paraphrases where both the number of clus-
ters (senses) and their contents are optimized for
substitutability.

2.2 A measure of substitutability

We define a substitutability metric that quantifies
the extent to which a sense inventory aligns with
human-generated lexical substitution annotations.
We then cluster PPDB paraphrases using the sub-
stitutability metric to optimize the sense clusters
for substitutability.

Given a sense inventory C, we can define the
senses of a target word t as a set of sense clusters,
C(t) = {c1,c2, . . .ck}, where each cluster contains
words corresponding to a single sense of t. Intu-
itively, if a sense inventory corresponds with sub-
stitutability, then each sense cluster ci should have
two qualities: first, words within ci should be in-
terchangeable with t in the same set of contexts;
and second, ci should not be missing any words
that are interchangeable in those same contexts.
We therefore operationalize the definition of sub-
stitutability as follows.

We begin measuring substitutability with a lex-
ical substitution dataset, consisting of sentences
where content words have been manually anno-
tated with substitutes (see example in Table 1). We
then use normalized mutual information (NMI)
(Strehl and Ghosh, 2002) to quantify the level
of agreement between the automatically generated
sense clusters and human-suggested substitutes.
NMI is an information theoretic measure of clus-
ter quality. Given two clusterings U and V over

1PPDB paraphrases come into packages of different sizes
(going from S to XXXL): small packages contain high-
precision paraphrases while larger ones have high coverage.
All are available from paraphrase.org

Sentence Annotated Substitutes (Count)
In this world, one’s
word is a promise.

vow (1), utterance (1), tongue
(1), speech (1)

Silverplate: code word
for the historic mission
that would end World
War II.

phrase (3), term (2), ver-
biage(1), utterance (1), signal
(1), name (1), dictate (1), des-
ignation (1), decree (1)

I think she only heard
the last words of my
speech.

bit (3), verbiage (2), part
(2), vocabulary (1), terminol-
ogy (1), syllable (1), phrasing
(1), phrase (1), patter (1), ex-
pression (1), babble (1), anec-
dote (1)

Table 1: Example annotated sentences for the tar-
get word word.N from the CoInCo (Kremer et al.,
2014) lexical substitution dataset. Numbers after
each word indicate the number of annotators who
made that suggestion.

a set of items, it measures how much each clus-
tering reduces uncertainty about the other (Vinh
et al., 2009) in terms of their mutual information
I(U,V ) and entropies H(U),H(V ):

NMI(U,V ) =
I(U,V )√

H(U)H(V )

To calculate the NMI between a sense inventory
for target word t and its set of annotated substi-
tutes, we first define the substitutes as a clustering,
Bt = {b1,b2, . . .bn}, where bi denotes the set of
suggested substitutes for each of n sentences. Ta-
ble 1, for example, gives the clustered substitutes
for n = 3 sentences for target word t = word.N,
where b1 = {vow, utterance, tongue, speech}. We
then define the substitutability of the sense inven-
tory, Ct , with respect to the annotated substitutes,
Bt , as NMI(Ct ,Bt).2 Given many target words, we
can further aggregate the substitutability of sense
inventory C over the set of targets T in B into a
single substitutability score:

substitutabilityB(C) = ∑
t∈T

NMI(Ct ,Bt)
|T |

2.3 Optimizing for Substitutability
Having defined a substitutability score, we now
automatically generate word sense clusters from
the Paraphrase Database that maximize it. The
idea is to use the substitutability score to choose
the best number of senses for each target word
which will be the number of output clusters (k)
generated by our spectral clustering algorithm.

2In calculating NMI, we ignore words that do not appear
in both Ct and Bt .

111



2.4 Spectral clustering method
2.4.1 Constructing the Affinity Matrix
The spectral clustering algorithm (Yu and Shi,
2003) takes as input an affinity matrix A ∈ Rn×n
encoding n items to be clustered, and an integer
k. It generates k non-overlapping clusters of the n
items. Each entry ai j in the affinity matrix denotes
a similarity measurement between items i and j.
Entries in A must be nonnegative and symmetric.
The affinity matrix can also be thought of as de-
scribing a graph, where the n rows and columns
correspond to nodes, and each entry ai j gives the
weight of an edge between nodes i and j. Because
the matrix must be symmetric, the graph is undi-
rected.

Given a target word t, we call its set of PPDB
paraphrases PP(t). Note that PP(t) excludes t it-
self. In our most basic clustering method, we clus-
ter paraphrases for target t as follows. Given the
length-n set of t’s paraphrases, PP(t), we construct
the n×n affinity matrix A where each shared-index
row and column corresponds to some word p ∈
PP(t). We set entries equal to the cosine similarity
between the applicable words’ embeddings, plus
one: ai j = cos(vi,v j)+1 (to enforce non-negative
similarities). For our implementation we use 300-
dimensional part-of-speech-specific word embed-
dings vi generated using the gensim word2vec
package (Mikolov et al., 2013b; Mikolov et al.,
2013a; Řehůřek and Sojka, 2010).3 In Figure 1a
we show a set of paraphrases, linked by PPDB
relations, and in Figure 1b we show the corre-
sponding basic affinity matrix, encoding the para-
phrases’ distributional similarity.

In order to aid further discussion, we point out
that the affinity matrix used for the basic cluster-
ing method encodes a fully-connected graph G =
{PP(t),EALLPP } with paraphrases PP(t) as nodes,
and edges between every pair of words, EALLPP =
PP(t)×PP(t). As for all variations on the cluster-
ing method, the matrix entries correspond to dis-
tributional similarity.

2.4.2 Masking
The affinity matrix in Figure 1b ignores the graph
structure inherent in PPDB, where edges connect
only words that are paraphrases of one another.
We experiment with enforcing the PPDB structure

3The word2vec parameters we use are a context win-
dow of size 3, learning rate alpha from 0.025 to 0.0001, min-
imum word count 100, sampling parameter 1e−4, 10 negative
samples per target word, and 5 training epochs.

in the affinity matrix through a technique we call
‘masking.’ By masking, we mean allowing pos-
itive values in the affinity matrix only where the
row and column correspond to paraphrases that
appear as pairs in PPDB (Figure 1a). All entries
corresponding to paraphrase pairs that are not con-
nected in the PPDB graph (Figure 1a) are forced to
0.

More concretely, in the masked affinity matrix,
we set each entry ai j for which i and j are not para-
phrases in PPDB to zero. The masked affinity ma-
trix encodes the graph G = {PP(t),EMASKPP } with
edges connecting only pairs of words that are in
PPDB, EMASKPP = {(pi, p j) | pi ∈ P(p j)}. Figure 1c
shows the masked affinity matrix corresponding to
the PPDB structure in Figure 1a.

2.4.3 Optimizing k
Because spectral clustering requires the number
of output clusters, k, to be specified as input, for
each target word we run the clustering algorithm
for a range of k between 1 and the minimum of (n,
20). We then choose the k that maximizes the NMI
of the resulting clusters with the human-annotated
substitutes for that target in the development data.

2.5 Method variations
In addition to using the substitutability score to
choose the best number of senses for each target
word, we also experiment with two variations on
the basic spectral clustering method to increase the
score further: filtering by a paraphrase confidence
score and co-clustering with WordNet (Fellbaum,
1998).

2.5.1 PPDB Score Thresholding
Each paraphrase pair in the PPDB is associated
with a set of scores indicating the strength of
the paraphrase relationship. The recently added
PPDB2.0 Score (Pavlick et al., 2015) was calcu-
lated using a supervised scoring model trained on
human judgments of paraphrase quality.4 Apidi-
anaki (2016) showed that the PPDB2.0 Score itself
is a good metric for ranking substitution candi-
dates in context, outperforming some vector space
models when the number of candidates is high.
With this in mind, we experimented with using a
PPDB2.0 Score threshold to discard noisy PPDB

4The human judgments were used to fit a regression to the
features available in PPDB 1.0 plus numerous new features
including cosine word embedding similarity, lexical overlap
features, WordNet features and distributional similarity fea-
tures.

112



p1

p2

p5

p4

p3

p6

p7

(a) PPDB graph with n = 7
paraphrases PP(t) to be clus-
tered.

p1 p2 p3 p4 p5 p6 p7
p1
p2
p3
p4
p5
p6
p7

(b) Unmasked affinity matrix for input to ba-
sic clustering algorithm, for paraphrases in
Fig 1a. This matrix encodes a fully-connected
graph G = {PP(t),EALLPP }.

p1 p2 p3 p4 p5 p6 p7
p1
p2
p3
p4
p5
p6
p7

(c) Masked affinity matrix for input to clus-
tering algorithm, enforcing the paraphrase
links from the graph in Fig 1a, G =
{PP(t),EMASKPP }.

Figure 1: Unclustered PPDB graph and its corresponding affinity matrices, encoding distributional sim-
ilarity, for input to the basic (1b) and masked (1c) clustering algorithms. Masking zeros-out the values
of all cells corresponding to paraphrases not connected in the PPDB graph. Cell shading corresponds to
the distributional similarity score between words, with darker colors representing higher measurements.

XXL paraphrases prior to sense clustering. Our
objective was to begin the clustering process with
a clean set of paraphrases for each target word,
eliminating erroneous paraphrases that might pol-
lute the substitutable sense clusters. We imple-
mented PPDB score thresholds in a range from 0
to 2.5.

2.5.2 Co-Clustering with WordNet
PPDB is large and inherently noisy. WordNet
has smaller coverage but well-defined semantic
structure in the form of synsets and relations.
We sought a way to marry the high coverage of
PPDB with the clean structure of WordNet by co-
clustering the two resources, in hopes of creating
a sense inventory that is both highly-substitutable
and high-coverage.

The basic unit in WordNet is the synset, a set
of lemmas sharing the same meaning. WordNet
also connects synsets via relations, such as hyper-
nymy, hyponymy, entailment, and ‘similar-to’. We
denote as L(s) the set of lemmas associated with
synset s. We denote as R(s) the set of synsets
that are related to synset s with a hypernym, hy-
ponym, entailment, or similar-to relationship. Fi-
nally, we denote as S(t) the set of synsets to which
a word t belongs. We denote as S+(t) the set of t’s
synsets, plus all synsets to which they are related;
S+(t) = S(t)∪⋃s′∈S(t) S(s′). In other words, S+(t)
includes all synsets to which t is connected by a

path of length at most 2 via one of the relations
encoded in R(s).

For co-clustering, we generate the affinity ma-
trix for a graph with m+n nodes corresponding to
the n words in PP(t) and the m synsets in S+(t),
and edges between every pair of nodes. Because
the edge weights are cosine similarity between
vector embeddings, we need a way to construct an
embedding for each synset in S+(t).5 We there-
fore generate compositional embeddings for each
synset s that are equal to the weighted average of
the embeddings for the lemmas l ∈ L(s), where the
weights are the PPDB2.0 scores between t and l:

vs =
∑l∈L(s) PPDB2.0Score(t, l)× vl

∑l∈L(s) PPDB2.0Score(t, l)

The unmasked affinity matrix used for input to
the co-clustering method, then, encodes the graph
G = {PP(t)∪ S+(t),EALLPP ∪EALLPS ∪EALLSS }, where
EALLPS contains edges between every paraphrase
and synset, and EALLSS contains edges between ev-
ery pair of synsets.

We also define masked versions of the co-
clustering affinity matrix. In a masked affinity
matrix, positive entries are only allowed for en-
tries where the row and column correspond to enti-

5We don’t use the NASARI embeddings (Camacho-
Collados et al., 2015) because these are available only for
nouns.

113



p1

p2

p5

p4

p3

p6

p7

s1 s2

s3s4

(a) Graph showing n = 7 PPDB
paraphrases PP(t) and m = 4
WordNet synsets S+(t) to be
clustered.

p1 p2 p3 p4 p5 p6 p7 s1 s2 s3 s4
p1
p2
p3
p4
p5
p6
p7

s1
s2
s3
s4

(b) Unmasked affinity matrix for input to ba-
sic clustering algorithm, for paraphrases and
synsets in Fig 2a. This matrix encodes a fully-
connected graph G = {PP(t)∪S+(t),EALLPP ∪
EALLPS ∪EALLSS }.

p1 p2 p3 p4 p5 p6 p7 s1 s2 s3 s4
p1
p2
p3
p4
p5
p6
p7

s1
s2
s3
s4

(c) Affinity matrix for input to cluster-
ing algorithm, enforcing the paraphrase-
paraphrase (EMASKPP ) and paraphrase-synset
(EMASKPS ) links from the graph in 2a, but al-
lowing all synset-synset links (EALLPP ).

Figure 2: Paraphrase/synset graph for input to the co-clustering model, and its corresponding affinity
matrices for the basic (2b) and masked (2c) clustering algorithms. Cell shading corresponds to the
distributional similarity score between words/synsets.

ties (paraphrases or synsets) that are connected by
the underlying knowledge base (PPDB or Word-
Net). Just as we defined masking for paraphrase-
paraphrase links (EPP) to allow only positive val-
ues corresponding to paraphrase pairs found in
PPDB, here we separately define masking for
paraphrase-synset (EPS) and synset-synset (ESS)
based on WordNet synsets and relations. When
applying the clustering algorithm, it is possible to
elect to use the masked version for any or all of
EPP, EPS, and ESS. In our experiments we try all
combinations.

For the synset-synset links, we define the
masked version EMASKSS as including only nonzero
edge weights where a hypernym, hyponym, en-
tailment or similar-to relationship connects two
synsets: EMASKSS = {(su,sv) | su ∈ R(sv) or sv ∈
R(su)}. For the paraphrase-synset links, we define
the masked version EMASKPS to include only nonzero
edge weights where the paraphrase is a lemma
in the synset, or is a paraphrase of a lemma in
the synset (excluding the target word): EMASKPS =
{(pi,su) | pi ∈ L(su) or |(P(pi)− t)∩ L(su)|> 0}.
We need to exclude the target word when calcu-
lating the overlap because otherwise all words in
PP(t) would connect to all synsets in S(t). Figure
2 depicts the graph, unnmasked and masked affin-
ity matrices for the co-clustering method.

2.6 Clustering Experiments

2.6.1 Datasets

We run clustering experiments using targets and
human-generated substitution data drawn from
two lexical substitution datasets. The first is the
“Concepts in Context” (CoInCo) corpus (Kremer
et al., 2014), containing over 15K sentences cor-
responding to nearly 4K unique target words. We
divide the CoInCo dataset into development and
test sets by first finding all target words that have
at least 10 sentences. For each of the 327 result-
ing targets, we randomly divide the correspond-
ing sentences into 60% development instances and
40% test instances. The resulting development
and test sets have 4061 and 2091 sentences respec-
tively. We cluster the 327 target words in the re-
sulting subset of CoInCo, performing all optimiza-
tion using the development portion.

In order to evaluate how well our method gen-
eralizes to other data, we also create clusters for
target words drawn from the SemEval 2007 En-
glish Lexical Substitution shared task dataset (Mc-
Carthy and Navigli, 2007). The entire test por-
tion of the SemEval dataset contains 1700 an-
notated sentences for 170 target words. We fil-
ter this data to keep only sentences with one or
more human-annotated substitutes that overlap our
PPDB XXL paraphrase vocabulary. The result-
ing test set, which we use for evaluating SemEval

114



targets, has 1178 sentences and 157 target words.
We cluster each of the 157 targets, using the Co-
InCo development data to optimize substitutabil-
ity for the 32 SemEval targets that also appear in
CoInCo. For the rest of the SemEval targets we
choose a number of senses equal to its WordNet
synset count.

2.6.2 Clustering Method Variations
We try all combinations of the following parame-
ters in our clustering model:

• Clustering method: We try the basic cluster-
ing – clustering Paraphrases Only – and the
WordNet Co-Clustering method.

• PPDB2.0 Score Threshold: We cluster para-
phrases of each target having a PPDB2.0
Score above a threshold, ranging from 0-3.0.

• Masking: When clustering paraphrases only,
we can either use the PP-PP mask or al-
low positive similarities between all words.
When co-clustering, we try all combina-
tions of the PP-PP, PP-SYN, and SYN-SYN
masks.

For each combination, we evaluate the NMI
substitutability of the resulting sense inventory
over our CoInCo and SemEval test instances. The
substitutability results are given in Tables 2 and 3.

3 Filtering Substitutes

3.1 A WSD oracle

We now question whether it is possible to improve
the rankings of current state-of-the-art lexical sub-
stitution systems by using the optimized sense in-
ventory as a filter. Our general approach is to take
a set of ranked substitutes generated by a vector-
based model. Then, we see whether filtering the
ranked substitutes to bring words belonging to the
correct sense of the target to the top of the rankings
would improve the overall ranking results.

Assuming that we have a WSD oracle that is
able to choose the most appropriate sense for a
target word in context, this corresponds to nomi-
nating substitutes from the applicable sense cluster
and elevating them in the list of ranked substitutes
output by the state-of-the-art lexical substitution
system. If sense filtering successfully improves
the quality of ranked substitutes, we can say that
the sense inventory captures substitutability well.

3.2 Ranking Models
Our approach requires a set of rankings produced
by a high-quality lexical substitution model to
start. We generate substitution rankings for each
target/sentence pair in the test sets using a syntac-
tic vector-space model (Thater et al., 2011; Apidi-
anaki, 2016) and a state-of-the-art model based on
word embeddings (Melamud et al., 2015).

The syntactic vector space model of Apidianaki
(2016) (Syn.VSM) demonstrated an ability to cor-
rectly choose appropriate PPDB paraphrases for a
target word in context. The vector features cor-
respond to syntactic dependency triples extracted
from the English Gigaword corpus 6 analyzed with
Stanford dependencies (Marneffe et al., 2006).
Syn.VSM produces a score for each (target, sen-
tence, substitute) tuple based on the cosine sim-
ilarity of the substitute’s basic vector representa-
tion with the target’s contextualized vector (Thater
et al., 2011). The contextualized vector is derived
from the basic meaning vector of the target word
by reinforcing its dimensions that are licensed by
the context of the specific instance under consider-
ation. More specifically, the contextualized vector
of a target is obtained through vector addition and
contains information about the target’s direct syn-
tactic dependents.

The second set of rankings comes from the Ad-
dCos model of Melamud et al. (2015). AddCos
quantifies the fit of substitute word s for target
word t in context C by measuring the semantic
similarity of the substitute to the target, and the
similarity of the substitute to the context:

AddCos(s, t,W ) =
|W |·cos(s, t) + ∑w∈W cos(s,w)

2 · |W |
(1)

The vectors s and t are word embeddings of the
substitute and target generated by the skip-gram
with negative sampling model (Mikolov et al.,
2013b; Mikolov et al., 2013a). The context W
is the set of words appearing within a fixed-width
window of the target t in a sentence (we use a win-
dow (cwin) of 1), and the embeddings c are con-
text embeddings generated by skip-gram. In our
implementation, we train 300-dimensional word
and context embeddings over the 4B words in the
Annotated Gigaword (AGiga) corpus (Napoles et
al., 2012) using the gensim word2vec package

6http://catalog.ldc.upenn.edu/
LDC2003T05

115



(Mikolov et al., 2013b; Mikolov et al., 2013a;
Řehůřek and Sojka, 2010).7

3.3 Substitution metrics

Lexical substitution experiments are usually eval-
uated using generalized average precision (GAP)
(Kishida, 2005). GAP compares a set of predicted
rankings to a set of gold standard rankings. Scores
range from 0 to 1; a perfect ranking, in which all
high-scoring substitutes outrank low-scoring sub-
stitutes, has a score of 1. For each sentence in the
CoInCo and SemEval test sets, we consider the
PPDB paraphrases for the target word to be the
candidates, and we set the test set annotator fre-
quency to be the gold score. Words in PPDB that
were not suggested by annotators receive a gold
score of 0.001. Predicted scores are given by the
two ranking models, Syn.VSM and AddCos.

3.4 Filtering Method

Sense filtering is intended to boost the rank of sub-
stitutes that belong to the most appropriate sense
of the target given the context. We run this exper-
iment as a two-step process.

First, given a target and sentence, we obtain the
PPDB XXL paraphrases for the target word and
rank them using the Syn.VSM and the AddCos
models.8 We calculate the overall unfiltered GAP
score on the test set for each ranking model as the
average GAP over sentences.

Next, we evaluate the ability of a sense inven-
tory to improve the GAP score through filtering.
We implement sense filtering by adding a large
number (10000) to the ranking model’s score for
words belonging to a single sense. We assume
an oracle that finds the cluster which maximally
improves the GAP score using this sense filtering
method. If the sense inventory corresponds well
to substitutability, we should expect this filtering
to improve the ranking by downgrading proposed
substitutes that do not fall within the correct sense
cluster.

We calculate the maximum sense-restricted
GAP score for the inventories produced by each
variation on our clustering model, and compare

7The word2vec training parameters we use are a context
window of size 3, learning rate alpha from 0.025 to 0.0001,
minimum word count 100, sampling parameter 1e−4, 10 neg-
ative samples per target word, and 5 training epochs.

8For the SemEval test set, we rank PPDB XXL para-
phrases having a PPDB2.0 Score with the target of at least
2.54.

this to the unfiltered GAP score for each ranking
model.

3.5 Baselines
We compare the extent to which our optimized
sense inventories improve lexical substitution
rankings to the results of two baseline sense in-
ventories.

• WordNet+: a sense inventory formed from
WordNet 3.0. For each CoInCo target word
that appears in WordNet, we take its sense
clusters to be its synsets, plus lemmas be-
longing to hypernyms and hyponyms of each
synset.

• PPDBClus: a much larger, abeit noisier,
sense inventory obtained by automatically
clustering words in the PPDB XXL package.
To obtain this sense inventory we clustered
paraphrases for all targets in the CoInCo
dataset using the method outlined in Cocos
and Callison-Burch (2016), with PPDB2.0
Score serving as the similarity metric.

We assess the substitutability of these sense
baseline inventories with respect to the human-
annotated substitutes in the CoInCo and SemEval
datasets, and also use them for sense filtering.

Finally, we wish to estimate the impact of
the NMI-based optimization procedure (Section
2.4.3) on the quality of the senses used for fil-
tering. We compare the performance of the opti-
mized CoInCo sense inventory, where the number
of clusters, k, for a target word is defined through
NMI optimization (called ‘Choose-K: Optimize
NMI’), to an inventory induced from CoInCo
where k equals the number of synsets available
for the target word in WordNet (called ‘Choose-
K: #WN Synsets).

4 Results

We report substitutability, and the unfiltered and
best sense-filtered GAP scores achieved using the
paraphrase-only clustering method and the co-
clustering method in Tables 2 and 3.

The average unfiltered GAP scores for the
Syn.VSM rankings over the CoInCo and SemEval
test sets are 0.528 and 0.673 respectively.9 All

9The score reported by Apidianaki (2016) for the
Syn.VSM model with XXL PPDB paraphrases on CoInCo
was 0.56. The difference in scores is due to excluding from
our clustering experiments target words that did not have at
least 10 sentences in CoInCo.

116



Syn.VSM AddCos (cwin=1)
substCoInCo Unfiltered GAP Oracle GAP Unfiltered GAP Oracle GAP

PPDBClus 0.254

0.528

0.661

0.533

0.656
WordNet 0.252 0.655 0.651

Choose-K: # WN Synsets (avg) 0.205 0.639 0.636
Choose-K: # WN Synsets (max, no co-clustering) 0.250* 0.695* 0.690*

Choose-K: # WN Synsets (max, co-clustering) 0.241** 0.690** 0.683**
Choose-K: Optimize NMI (avg) 0.282 0.668 0.662

Choose-K: Optimize NMI (max, no co-clustering) 0.331* 0.719 * 0.714 ***
Choose K: Optimize NMI (max, co-clustering) 0.314** 0.718 **** 0.710 **

Table 2: Substitutablity (NMI) of resulting sense inventories, and GAP scores of the unfiltered and best sense-filtered rankings
produced by the Syn.VSM and AddCos models, for the CoInCo annotated dataset. Configurations for the best-performing sense
inventories were: * Min PPDB Score 2.0, cluster PP’s only, use PP-PP mask; ** Min PPDB Score 2.0, co-clustering, use PP-PP
mask only; *** Min PPDB Score 1.5, cluster PP’s only, use PP-PP mask; **** Min PPDB Score 2.0, co-clustering, use PP-PP,
Syn-Syn masks only

Syn.VSM AddCos (cwin=1)
substSemEval Unfiltered GAP Oracle GAP Unfiltered GAP Oracle GAP

PPDBClus 0.357

0.673

0.855

0.410

0.634
WordNet 0.291 0.774 0.595

Average of all Clustered Sense Inventories 0.367 0.841 0.569
Max basic (no co-clustering) sense inventory 0.448* 0.917* 0.626*

Max co-clustered sense inventory 0.449** 0.906*** 0.612****

Table 3: Substitutablity (NMI) of resulting sense inventories, and GAP scores of the unfiltered and best sense-filtered rankings
produced by the Syn.VSM and AddCos models, for the SemEval07 annotated dataset. Configurations for the best-performing
sense inventories were: * Min PPDB Score 2.31, cluster PP’s only, use PP-PP mask; ** Min PPDB Score 2.54, co-clustering,
use PP-SYN mask only; *** Min PPDB Score 2.54, co-clustering, use PP-SYN mask only; **** Min PPDB Score 2.31,
co-clustering, use PP-SYN mask only.

baseline and cluster sense inventories are capable
of improving these GAP scores when we use the
best sense as a filter. Syntactic models generally
give very good results with small paraphrase sets
(Kremer et al., 2014) but their performance seems
to degrade when they need to deal with larger and
noisier substitute sets (Apidianaki, 2016). Our
results suggest that finding the most appropriate
sense of a target word in context can improve their
lexical substitution results.

The trend in results is similar for the AddCos
rankings. The average unfiltered GAP scores for
the AddCos rankings over the CoInCo and Se-
mEval test sets are 0.533 and 0.410 respectively.
The GAP scores of the unfiltered AddCos rankings
are much lower than after filtering with any base-
line or cluster sense inventory, showing that lexical
subsitutition rankings based on word-embeddings
can also be improved using senses.

To assess the impact of the NMI-based opti-
mization procedure on the results, we compare the
performance of two sense inventories on the Co-
InCo rankings: one where the number of clusters
(k) for a target word is defined through NMI op-
timization and another one, where k is equal to
the number of synsets available for the target word

in WordNet. We find that for both the Syn.VSM
and AddCos ranking models, filtering using the
sense inventory with the NMI-optimized k outper-
forms the results obtained when the inventory with
k equal to the number of synsets is used.

Furthermore, we find that the NMI substi-
tutability score is a generally good indicator of
how much improvement we see in GAP score due
to oracle sense filtering. We calculated the Pear-
son correlation of a sense inventory’s NMI with
its oracle GAP score to be 0.644 (calculated over
all target words in the CoInCo test set, including
GAP results for both the Syn.VSM and AddCos
ranking models). This suggests that NMI is a rea-
sonable measure of substitutability.

We find that for all methods, applying a PPDB-
Score Threshold prior to clustering is an effective
way of removing noisy, non-substitutable para-
phrases from the sense inventory. When we use
the resulting sense inventory for filtering, this ef-
fectively elevates only high-quality paraphrases in
the lexical substitution rankings. This supports the
finding of Apidianaki (2016), who showed that the
PPDB2.0 Score itself is an effective lexical substi-
tution ranking metric when large and noisy para-
phrase substitute sets are involved.

117



Finally, we discover that co-clustering with
WordNet did not produce any significant improve-
ment in NMI or GAP score over clustering para-
phrases alone. This could suggest that the added
structure from WordNet did not improve over-
all substitutability of the resulting sense invento-
ries, or that our co-clustering method did not ef-
fectively incorporate useful structural information
from WordNet.

5 Conclusion

We have shown that despite the high performance
of word-based vector-space models, lexical sub-
stitution can still benefit from word senses. We
have defined a substitutability metric and proposed
a method for automatically creating sense invento-
ries optimized for substitutability. The number of
sense clusters in an optimized inventory and their
contents are aligned with lexical substitution an-
notations in a development set. Using the best
fitting cluster in each context as a filter over the
rankings produced by vector-space models boosts
good substitutes and improves the models’ scores
in a lexical substitution task.

For choosing the cluster that best fits a con-
text, we used an oracle experiment which finds
the maximum GAP score achievable by a sense by
boosting the ranking model’s score for words be-
longing to a single sense. The cluster that achieved
the highest GAP score in each case was selected.
The task of finding the most appropriate sense in
context still remains. But the improvement in lex-
ical substitution results shows that word sense in-
duction and disambiguation can still benefit state-
of-the-art word-based models for lexical substitu-
tion.

Our sense filtering mechanism can be applied to
the output of any vector-space substitution model
at a post-processing step. In future work, we in-
tend to experiment with models that account for
senses during embedding learning. The models of
Huang et al. (2012) and Li and Jurafsky (2015)
learn multi-prototype, or sense-specific, embed-
ding representations and are able to choose the
best-fitted ones for words in context. These mod-
els have up to now been tested in several NLP
tasks but have not yet been applied to lexical sub-
stitution. We will experiment with using the em-
beddings chosen by these models for specific word
instances for ranking candidate substitutes in con-
text. The comparison with the results presented in

this paper will show whether it is preferable to ac-
count for senses before or after actual lexical sub-
stitution.

Acknowledgments

This material is based in part on research spon-
sored by DARPA under grant number FA8750-
13-2-0017 (the DEFT program). The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes. The views
and conclusions contained in this publication are
those of the authors and should not be interpreted
as representing official policies or endorsements
of DARPA and the U.S. Government.

This work has also been supported by the
French National Research Agency under project
ANR-16-CE33-0013.

We would like to thank our anonymous review-
ers for their thoughtful and helpful comments.

References
Marianna Apidianaki. 2016. Vector-space models for

PPDB paraphrase ranking in context. In Proceed-
ings of EMNLP, pages 2028–2034, Austin, Texas.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2015. NASARI: a Novel Ap-
proach to a Semantically-Aware Representation of
Items. In Proceedings of NAACL/HLT 2015, pages
567–577, Denver, Colorado.

Anne Cocos and Chris Callison-Burch. 2016. Clus-
tering paraphrases by word sense. In Proceedings
of NAACL/HLT 2016, pages 1463–1472, San Diego,
California.

Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2013. Measuring word meaning in context. Com-
putational Linguistics, 39(3):511–554, 9.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A. Smith.
2015. Retrofitting Word Vectors to Semantic Lex-
icons. In Proceedings of NAACL/HLT, Denver, Col-
orado.

Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.

Juri Ganitkevitch, Benjamin VanDurme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL, Atlanta, Geor-
gia.

Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Pro-
totypes. In Proceedings of the ACL (Volume 1: Long
Papers), pages 873–882, Jeju Island, Korea.

118



Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. SensEmbed: Learning
Sense Embeddings for Word and Relational Simi-
larity. In Proceedings of the ACL/IJCNLP, pages
95–105, Beijing, China.

Adam Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities, 31(2):91–113.

Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evalua-
tion indicator for information retrieval experiments.
National Institute of Informatics Tokyo, Japan.

Gerhard Kremer, Katrin Erk, Sebastian Padó, and Ste-
fan Thater. 2014. What Substitutes Tell Us - Anal-
ysis of an “All-Words” Lexical Substitution Corpus.
In Proceedings of EACL, pages 540–549, Gothen-
burg, Sweden.

Jiwei Li and Dan Jurafsky. 2015. Do Multi-Sense Em-
beddings Improve Natural Language Understand-
ing? In Proceedings of the EMNLP, pages 1722–
1732, Lisbon, Portugal.

M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating Typed Dependency Parses from Phrase
Structure Parses. In Proceedings of LREC-2006,
Genoa, Italy.

Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Substitu-
tion Task. In Proceedings of SemEval, pages 48–53,
Prague, Czech Republic.

Oren Melamud, Omer Levy, and Ido Dagan. 2015. A
Simple Word Embedding Model for Lexical Substi-
tution. In Proceedings of the 1st Workshop on Vector
Space Modeling for Natural Language Processing,
pages 1–7, Denver, Colorado.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in neural information processing
systems, pages 3111–3119.

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated Gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction (AKBC-WEKEX), pages 95–100, Mon-
treal, Canada.

Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,
Benjamin Van Durme, and Chris Callison-Burch.
2015. PPDB 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. In Proceedings of ACL/IJCNLP,
pages 425–430, Beijing, China.

Radim Řehůřek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50, Valletta, Malta.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-Prototype Vector-Space Models of Word
Meaning. In Proceedings of HLT/NAACL, pages
109–117, Los Angeles, California.

Alexander Strehl and Joydeep Ghosh. 2002. Clus-
ter ensembles—a knowledge reuse framework for
combining multiple partitions. Journal of machine
learning research, 3(Dec):583–617.

Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2011. Word Meaning in Context: A Simple and Ef-
fective Vector Model. In Proceedings of IJCNLP,
pages 1134–1143, Chiang Mai, Thailand.

Nguyen Xuan Vinh, Julien Epps, and James Bailey.
2009. Information theoretic measures for cluster-
ings comparison: is a correction for chance neces-
sary? In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning, pages
1073–1080. ACM.

Stella X. Yu and Jianbo Shi. 2003. Multiclass spec-
tral clustering. In Proceedings of International Con-
ference on Computer Vision (ICCV 03), pages 313–
319. IEEE.

119


