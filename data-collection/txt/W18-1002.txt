



















































Commonsense mining as knowledge base completion? A study on the impact of novelty


Proceedings of the Workshop on Generalization in the Age of Deep Learning, pages 8–16
New Orleans, Louisiana, June 5, 2018. c©2018 Association for Computational Linguistics

Commonsense mining as knowledge base completion? A study on the
impact of novelty

Stanisław Jastrzębski∗
Jagiellonian University

stanislaw.jastrzebski@uj.edu.pl

Dzmitry Bahdanau
MILA

Université de Montréal

Seyedarian Hosseini
MILA

Université de Montréal

Michael Noukhovitch
MILA

Université de Montréal

Yoshua Bengio†
MILA

Université de Montréal

Jackie Chi Kit Cheung
MILA

McGill University

Abstract

Commonsense knowledge bases such as Con-
ceptNet represent knowledge in the form of
relational triples. Inspired by recent work by
(Li et al., 2016), we analyse if knowledge base
completion models can be used to mine com-
monsense knowledge from raw text. We pro-
pose novelty of predicted triples with respect to
the training set as an important factor in inter-
preting results. We critically analyse the dif-
ficulty of mining novel commonsense knowl-
edge, and show that a simple baseline method
outperforms the previous state of the art on
predicting more novel triples.

1 Introduction

Many natural language understanding tasks re-
quire commonsense knowledge in order to re-
solve ambiguities involving implicit assumptions.
Collecting such knowledge and representing it
in a reusable way is thus an important chal-
lenge. There exist several commonsense knowl-
edge bases maintained by experts (CyC) or ac-
quired by crowdsourcing (ConceptNet) which
represent commonsense knowledge as relational
triples (e.g., (“pen“, “UsedFor“, “writing“)) (Liu
and Singh, 2004). Automatic mining of common-
sense knowledge, the focus of this work, aims to
improve the coverage of such resources.

One common way of improving the coverage of
knowledge bases is through knowledge base com-
pletion (KBC), which can be formalized as pre-
dicting the existence of edges between (usually)
pre-existing nodes in the graph. Recent work by Li
et al. (2016) approached commonsense mining as
a KBC task. Their method mines candidate triples

∗Work partially done as intern in MILA
†CIFAR Senior Fellow

from Wikipedia and reranks the triples with a KBC
model in order to extend ConceptNet.

The goal of this paper is to investigate why
recent systems such as the above achieve good
performance, and understand their potential for
mining commonsense. We approach it by break-
ing down the previously reported aggregate re-
sults into the cases in which models perform well
or poorly. We focus in particular on the issue
of the novelty of model predictions with respect
to the triples in the training set. For example, a
triple predicted by a system could be correct be-
cause it generates output with a slightly different
wording or morphological inflection (e.g., (“fish“,
“AtLocation“, “water“) from (“fish“, “AtLoca-
tion“, “in water“)), or it could be correct because
it exhibits some degree of semantic generalization
(e.g., (“fish“, “IsCapableOf“, “swimming“) from
(“fish“, “AtLocation“, “in water“)). Arguably,
the former could be handled by better standard-
ization of data set formats or more comprehensive
model pre-processing, whereas the latter presents
an example of genuine commonsense inference
and novelty. This analysis is especially important
for commonsense mining because of the diversity
of the entities, relations, and linguistic expressions
thereof in current datasets.

The contribution of this paper is two-fold. First,
we test if the KBC task as it is set up in recent
work can gauge a model’s ability to mine novel
commonsense (i.e. find novel commonsense facts
based on some resource). We observe the contrary.
We present a model that performs poorly on KBC
but matches the best model on the task of min-
ing novel commonsense (evaluated by re-ranking
extracted candidate triples from Wikipedia). We
then examine the cause of this discrepancy, and

8



find that around 60% of triples in the KBC test set
used by Li et al. (2016) are minor rewordings of
existing triples in the training set. This suggests
that controlling for the novelty of triples in both
KBC and Wikipedia evaluation is needed.

Second, we present a reassessment of previous
methods in which we control the dataset for nov-
elty, extending the results of Li et al. (2016). We
introduce a simple automated novelty metric and
show that it correlates with human judgment. We
then show that the performance of most models on
both KBC and Wikipedia triple reranking drops
drastically when we evaluate them examples that
are genuinely new according to our metric. Fi-
nally, we demonstrate that a simple baseline model
that does not model all interactions between el-
ements in a triple performs surprisingly well on
both KBC and reranking when we focus on novel
triples.

2 Related work

Knowledge extraction from text corpora is a vast
research area (Banko et al., 2007; Mitchell et al.,
2015), yet work that targets commonsense knowl-
edge specifically are comparatively rare (Gordon,
2014). Our focus is on the specific approach to
mining commonsense knowledge by casting it as
a KBC task, as in Li et al. (2016); Forbes and Choi
(2017).

Knowledge base completion (KBC) is a method
to improve coverage of knowledge base by pre-
dicting non-existing edges between nodes (Nickel
and Tresp, 2011; Socher et al., 2013). A common
modeling approach to KBC is to embed nodes and
the edge into a common representation space, fol-
lowed by a simple prediction model (Socher et al.,
2013).

Recently, Dettmers et al. (2017) observed that
some KBC benchmarks have test set triples that
are simply inversions of triples in their training
sets. Our work draws attention to a related is-
sue in commonsense KBC. Additionally, we find
that simple baseline models achieve strong per-
formances in our setting, in agreement with other
studies of KBC Joulin et al. (2017); Kadlec et al.
(2017).

In Angeli and Manning (2013), triple retrieval
based on distributional similarity is used to com-
plete ConceptNet. Our procedure for determining
the novelty of the triple is similar to methods used
in that work, but we apply it only in the context of

evaluation.

3 Completion vs Mining

Our goal in this section is to analyse the relation
between KBC and commonsense mining tasks fol-
lowing setup of Li et al. (2016).

3.1 Models
All our models take (h, r, t) triples as inputs,
where h and t are sequences of words representing
concepts and r is a relation from the ConceptNet
schema, and output the probability of the triple to
be true. Following Li et al. (2016), we embed h
and t by computing the sums h and t of the re-
spective word vectors.

Levy et al. (2015) showed that in the context
of predicting the hypernymy relation using only
head or only tail can be a strong baseline. To better
understand how complex reasoning is needed for
both KBC and mining tasks, we similarly consider
the two following models, which make strong sim-
plifying assumptions about the dependencies be-
tween elements in a triple. The Factorized model
uses only two-way interactions to compute the
triple score:

s(h, r, t) = α〈Ah+ b1,Bt+ b2〉
+ β〈Ar+ b1,Bt+ b2〉
+ γ〈Ar+ b1,Bh+ b2〉,

(1)

where h, r, and t are d1 dimensional embeddings
of head, relation and tail, A,B are d1 × d2 ma-
trices, b1,b2 are d2 dimensional biases, and α,
β, γ are learned scalars. The Prototypical model
is similar, but considers only the head-to-relation
and tail-to-relation terms (first and third terms in
Eq. 1).

We compare the two new models with the best
model from Li et al. (2016), a single hidden layer
DNN. In that model, the triple score is computed
as:

u(h, t) = φ(Ah+Bt+ b1)

s(h, r, t) = Wu(h, t) + b2,
(2)

where φ is a nonlinearity, A, B are d1 × d2 ma-
trices, b1 is a d2 dimensional bias, W is a d2 di-
mensional vector and b2 is a scalar. Additionally,
we compare against Bilinear of Li et al. (2016)1.
Bilinear model computes the triple score as:

s(h, r, t) = hTMrt, (3)

1It is the only model evaluated against the Wikipedia
ranking task in Li et al. (2016).

9



where Mr is a d1 × d1 dimensional matrix, sep-
arate for each relation in the dataset. All models’
scores are fed into a sigmoid function in order to
compute the final prediction.

3.2 Setup
KBC models are trained using 100, 000 triples
from ConceptNet5 (Speer and Havasi, 2012) that
were extracted from the Open Mind Common
Sense (OMCS) corpus (Speer and Havasi, 2012).
For evaluation, we consider two ways to split the
dataset: a random split, as well as the confidence-
based split proposed by (Li et al., 2016), which
uses triples with the highest ConceptNet confi-
dence scores as a test set2. Following Li et al.
(2016) negative examples are sampled by ran-
domly swapping head, tail or relation component
of each triple. The cross-entropy loss is used, and
models are evaluated using F1 score3. All models
are initialized using skip-gram embeddings that
were pretrained on the OMCS corpus.

The commonsense mining task is based on
a set of 1.7M extracted candidate triples from
Wikipedia by Li et al. (2016). The extracted triples
are ranked using a KBC model, and the top of the
ranking is manually evaluated. We will refer to the
experiments in which we rerank external candidate
triples as mining experiments.

We found that similar hyperparameters and op-
timization methods work well across the models.
We use 1, 000 hidden units, and apply L2 regu-
larization with a weight of 10−6 to the word em-
beddings. All models are optimized using Ada-
grad (Duchi et al., 2010) with a learning rate 0.01
and batch sizes of 200 (DNN) and 600 (Factor-
ized and Prototypical). In Section 3.3, we com-
pare against the scores of a Bilinear model pro-
vided by Li et al. (2016). Experiments are per-
formed using Keras (Chollet et al., 2015) and Ten-
sorFlow (Abadi et al., 2015).

3.3 Comparison of KBC and Wikipedia
evaluations

First, we directly test if the performance of a
model on the KBC task is predictive of its perfor-
mance on the mining task. We follow the min-
ing evaluation protocol from (Li et al., 2016): we

2We note that random test set consists of worse quality
triples than confidence-based split. However, the latter leads
to a serious bias in evaluation. We leave addressing this trade-
off for future work.

3The threshold is selected based on a separate develop-
ment set, as in (Speer and Havasi, 2012).

XXXXXXXXNovelty
Model DNN Factorized Prototypical

Entire 0.892 0.890 0.794
≤ 33% 0.950 0.922 0.911
(33%, 66%] 0.920 0.898 0.839
≥ 66% 0.720 0.821 0.574

Table 1: F1 scores on Li et al. (2016) confidence-
based test set. F1 score is reported on each bucket
(based on the percentile of triple novelty) and the
entire test set.

Bilinear Factorized Prototypical DNN

Wikipedia 2.04 2.61 2.55 2.5

Table 2: Average human assigned score (from 1
to 5) of the top 100 Wikipedia triples ranked by
baselines compared to DNN and Bilinear from Li
et al. (2016).

rank triples by assigned scores and manually eval-
uate the top 100 resulting triples on a scale from 0
(nonsensical) to 4 (true statement). We re-evaluate
their model against our baselines and find that the
knowledge base completion task is a poor indica-
tor of performance on Wikipedia. Even though
the Factorized and Prototypical models achieve
the same or much worse score than DNN on the
KBC task (see the first row of Table 1), their min-
ing performance on the top 100 triples is better
(than both DNN and Bilinear), see Table 2. Triples
were scored by two students and scores were av-
eraged, with 0.81 Pearson correlation and 0.48
kappa inter-annotator agreement.

3.4 Novelty of triples

We hypothesize that the discrepancy reported in
Section 3.3 is due to a strong overlap of the train-
ing and testing sets in the KBC setup of Li et al.
(2016). We perform a human evaluation of the
novelty of the triples in the three test sets with re-
spect to the 100, 000 ConceptNet training set used.
The first is the confidence-based test set used in Li
et al. (2016). We compare it with a random sub-
set of ConceptNet. Finally, we consider a sam-
ple of 300 triples from the top 10, 000 triples of
Wikipedia dataset ordered by the Bilinear model.

For each triple in the three datasets, we fetch
the five closest neighbours using word embedding
distance and categorize them into five categories
based on the closest triple found in the training set:
“same relation and minor rewording” (1), “dif-

10



ferent relation and minor rewording” (2), “same
relation and related word” (3), “different relation
and related word” (4), “no directly related triple”
(5). We ignore a small percentage of triples that
are not describing commonsense knowledge, as
well as false triples (some in the random subset,
and a large percentage in the Wikipedia dataset).

To give a better intuition, we provide example
triples for the confidence-based split of Li et al.
(2016). In Category 1 (defined as “same relation
and minor rewording”), we find (“egg”, “IsA”,
“food”), which has a close analog in the train-
ing set: (“egg”, “IsA”, “type of food”). An ex-
ample of a test triple in Category 3 (defined as
“different relation and related word”) is (“floor”,
“UsedFor”, “walk on”), which has a correspond-
ing triple in the training set (“floor”, “UsedFor”,
“stand on”). In the Appendix, we provide more
examples of triples from each category.

As shown in Table 3, we observe that approxi-
mately 87% examples in the confidence-based test
set fall into the first or second category, while these
categories constitute only 19% of the considered
subset of the Wikipedia triples (even after filtering
out false triples). We argue that not controlling
for the novelty of triples might introduce hard-to-
predict biases in the evaluation.

Finally, to understand the effects of using the
confidence-based split, we also re-evaluate models
on a random split. We observe that scores are con-
sistently lower than on the confidence-based split
(compare the first rows of Tables 1 and 4). In-
terestingly, the overall performance of the DNN
model degrades the most (absolute difference in
F1 score 9%), compared to Prototypical (4%) and
Factorized (7%).

4 Evaluation using novelty metric

Motivated by the described similarity of train and
test sets in the KBC task, we shift our attention
to re-evaluating models on datasets controlled for
novelty, extending results of Li et al. (2016). We
consider the same tasks as in Sec. 3: Concept-
Net5 completion task and commonsense mining
task based on Wikipedia triples.

4.1 Automatically measuring novelty

To approximate novelty, we use word embed-
dings (computed over the OMCS corpus) to calcu-
late distance d(a, b) = ||head(a) − head(b)||2 +
||tail(a) − tail(b)||2, where head and tail are

XXXXXXXXDataset
Novelty 1 2 3 4 5

Wikipedia 14% 5% 17% 8% 44%
Confident 65% 22% 4% 4% 2%
Random 21% 10% 16% 3% 29%

Table 3: Human assigned novelty categories to
triples from 3 different test datasets. High qual-
ity triples are usually trivial. Each column reports
percentage of triples in each category ordered by
novelty. Category 1 corresponds to “same relation
and minor rewording”. Category 5 corresponds to
“no directly related triple”.

represented by the average of word embeddings.
Such a formulation is related to the concept
of paradigmatic similarity (Sahlgren, 2006), and
word embedding-based distance can approximate
paradigmatic similarity (Sun et al., 2015). Two
words are paradigmatically similar if one can be
replaced for the other, while maintaining syn-
tactical correctness of the sentence (e.g. “The
wolf/tiger is a fierce animal“). We observe that
many trivial test triples are characterized by the
existence of a triple in the training set that only
differs by such substitutions.

We observe that the proposed distance metric
is correlated with human assigned novelty scores
(from Sec. 3.4). On the considered datasets Pear-
son correlation between automatic novelty score
and human assigned novelty score is 0.22 to 0.47,
with p-values between 0.03 and 0.004. We ac-
knowledge that the automated metric is simplistic,
for instance it underperforms for the triples con-
taining rare words or long phrases. Nevertheless,
the metric enables detecting a substantial portion
of trivial triples (e.g. morphological variations),
and we leave for future work developing better
measures of novelty.

Using the introduced metric, we can partially
explain the inconsistency in the performance of
Prototypical and Bilinear models between KBC
and mining Wikipedia. We note that the top of the
ranking on Wikipedia consists of mostly very far
(novel) triples (Figure 1), while KBC confidence-
based test set is mostly composed of trivial triples
(as argued in Section 3.4).

4.2 Novelty-binned evaluation of KBC

We now re-evaluate the KBC models using
our proposed novelty metric. First, we exam-
ine the performance on different subsets of the

11



1E+02 1E+03 1E+04 1E+05 1E+06
K

5.5
6.0
6.5
7.0
7.5
8.0

M
ea

n 
di

st
an

ce Prototypical
Bilinear

Figure 1: Mean embedding distance (y axis) of top
K (x axis) of triples in Wikipedia dataset for Bi-
linear (orange) and Prototypical (blue).

XXXXXXXXNovelty
Model DNN Factorized Prototypical

Entire 0.809 0.822 0.755
≤ 33% 0.883 0.874 0.866
(33%, 66%] 0.809 0.812 0.758
≥ 66% 0.725 0.731 0.674

Table 4: F1 scores on random split. F1 score is
reported on each bucket (based on the percentile
of triple novelty) and the entire set.

confidence-based split of ConceptNet5. Specifi-
cally, we split the confidence-based test set into
3 buckets, according to 33% (1.93 distance) and
66% (2.80 distance) quantile of distance to the
training set. Second, we run a similar experiment
but on a random split of the training set (bucket
thresholds at 2.1 and 2.95). Results are reported
in Tables 1 and 4.

As expected, the performance of models de-
grades quickly across buckets. The performance
on the farthest bucket drops from 10 to 20% F1
score with respect to the performance on the clos-
est bucket. We observe that the Factorized model
achieves the strongest performance on the farthest
bucket.

4.3 Novelty-binned evaluation on Wikipedia
Similar to Section 4.2, we analyse splitting candi-
date triples for the mining task using our novelty
metric. We split the Wikipedia dataset into 3 buck-
ets based on 33% (3.21 distance) and 66% (4.22
distance) quantiles of distance to the training set,
and we manually score the top 100 triples in each
bucket on the same scale from 1 to 5.

As in Section 4.2, we note a degradation of per-
formance across buckets for all models (from 1.06
to 0.32 mean human assigned score) and again the
Factorized model achieves the best performance
on the farthest bucket (mean score 2.26 compared
to 1.63 and 1.41). The Factorized model outper-
forms DNN on all buckets despite being a simpler

XXXXXXXXNovelty
Model DNN Factorized Prototypical

≤ 33% 2.47 2.58 2.33
(33%, 66%] 2.34 2.41 2.24
≥ 66% 1.41 2.26 1.63

Table 5: Novelty based evaluation of quality of
mined triples from Wikipedia dataset. Triples are
scored by humans on scale from 1 to 5.

model, which we hypothesize is due to DNN being
more prone to overfitting.

5 Conclusions

Mining genuinely novel commonsense is a chal-
lenging task, and training successful models will
require large training sets (e.g. ConceptNet) and
principled evaluation. We critically assessed the
potential of KBC models for mining common-
sense knowledge, and proposed several first steps
towards a more principled evaluation methodol-
ogy. Future work could focus on developing bet-
ter novelty metrics, and developing new regular-
ization techniques to better generalize to novel
triples.

Acknowledgments

SJ was supported by Grant No. DI 2014/016644
from Ministry of Science and Higher Education,
Poland. Work at MILA was funded by NSERC,
CIFAR and Canada Research Chairs.

References
Martín Abadi et al. 2015. TensorFlow: Large-scale

machine learning on heterogeneous systems. Soft-
ware available from tensorflow.org. https://
www.tensorflow.org/.

Gabor Angeli and Christopher Manning. 2013.
Philosophers are mortal: Inferring the truth of un-
seen facts. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning. Association for Computational Linguis-
tics, Sofia, Bulgaria, pages 133–142. http://
www.aclweb.org/anthology/W13-3515.

Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni.
2007. Open Information Extraction from the
Web. In IJCAI. volume 7, pages 2670–
2676. http://www.aaai.org/Papers/
IJCAI/2007/IJCAI07-429.pdf.

François Chollet et al. 2015. Keras. https://
github.com/keras-team/keras.

12



Tim Dettmers, Pasquale Minervini, Pontus Stene-
torp, and Sebastian Riedel. 2017. Convolu-
tional 2d knowledge graph embeddings. CoRR
abs/1707.01476. http://arxiv.org/abs/
1707.01476.

John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning
and stochastic optimization. Technical Re-
port UCB/EECS-2010-24, EECS Department,
University of California, Berkeley. http:
//www2.eecs.berkeley.edu/Pubs/
TechRpts/2010/EECS-2010-24.html.

Maxwell Forbes and Yejin Choi. 2017. Verb physics:
Relative physical knowledge of actions and objects.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2017,
Vancouver, Canada, July 30 - August 4, Volume 1:
Long Papers. pages 266–276. https://doi.
org/10.18653/v1/P17-1025.

Jonathan Gordon. 2014. Inferential Commonsense
Knowledge from Text. Ph.D. thesis.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Maximilian Nickel, and Tomas Mikolov. 2017. Fast
linear model for knowledge graph embeddings.
CoRR abs/1710.10881. http://arxiv.org/
abs/1710.10881.

Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst.
2017. Knowledge base completion: Baselines strike
back. In Proceedings of the 2nd Workshop on
Representation Learning for NLP, Rep4NLP@ACL
2017, Vancouver, Canada, August 3, 2017.
pages 69–74. https://aclanthology.
info/papers/W17-2609/w17-2609.

Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
970–976.

Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.
2016. Commonsense knowledge base completion.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics, pages 1445–1455. https://doi.org/
10.18653/v1/P16-1137.

H. Liu and P. Singh. 2004. Conceptnet: A practical
commonsense reasoning tool-kit. BT Technology
Journal 22(4):211–226. https://doi.org/
10.1023/B:BTTJ.0000047600.45421.6d.

T. Mitchell, W. Cohen, E. Hruscha, P. Talukdar,
J. Betteridge, A. Carlson, B. Dalvi, M. Gardner,
B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,
T. Mohammad, N. Nakashole, E. Platanios, A. Rit-
ter, M. Samadi, B. Settles, R. Wang, D. Wijaya,

A. Gupta, X. Chen, A. Saparov, M. Greaves, and
J. Welling. 2015. Never-ending learning. In AAAI.
: Never-Ending Learning in AAAI-2015. http:
//www.cs.cmu.edu/~wcohen/pubs.html.

Maximilian Nickel and Volker Tresp. 2011. A three-
way model for collective learning on multi-relational
data. In In Proceedings of the 28th Intl Conf. on
Mach. Learn. Citeseer.

Magnus Sahlgren. 2006. The Word-Space Model:
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words
in High-Dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning With Neural Ten-
sor Networks for Knowledge Base Completion. In
C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K. Q. Weinberger, editors, Advances in
Neural Information Processing Systems 26, Curran
Associates, Inc., pages 926–934.

Robert Speer and Catherine Havasi. 2012. Repre-
senting general relational knowledge in concept-
net 5. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uğur
Doğan, Bente Maegaard, Joseph Mariani, Asun-
cion Moreno, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC’12). European Language Resources Associ-
ation (ELRA), Istanbul, Turkey.

Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and
Xueqi Cheng. 2015. Learning word representa-
tions by jointly modeling syntagmatic and paradig-
matic relations. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Con-
ference on Natural Language Processing (Volume
1: Long Papers). Association for Computational
Linguistics, pages 136–145. http://aclweb.
org/anthology/P15-1014.

A Example triples

In this Appendix we report randomly picked ex-
amples from human assigned novelty categories
considered in the paper for each of the 3 datasets.
Due to large size of the training set, instead of
showing all triples from train set to human an-
notator, we show only 5 closest using embedding
based distance. A triple is classified as belong-
ing to the given category if at least one of the re-
trieved triples is sufficiently related. For exam-
ple, if for (“egg”, “IsA”, “food”) we find triple
(“egg”, “IsA”, “type of food”) in the top 5 closest
examples, we categorize it as belonging to the first
category (“same rel, rephrase”).

13



A.1 Confidence-based split
In this Section we report examples for novelty cat-
egory from Confidence-based split dataset. For
each example we include the 5 examples that were
shown to the human annotator, ordered by close-
ness according to our word embedding metric.

A.1.1 “same rel, rephrase“
• (egg, IsA, food) : (egg, UsedFor, food), (egg,

HasProperty, good for food), (egg, IsA,
type of food), (egg, HasProperty, good for
you), (egg, AtLocation, omletts),

• (book, AtLocation, classroom) : (lot book,
AtLocation, classroom), (physic, AtLocation,
classroom), (teacher aide, AtLocation, class-
room), (desk and chair, AtLocation, class-
room), (test paper, AtLocation, classroom),

• (dog, CapableOf, be pet) : (dog, CapableOf,
be great pet), (dog, CapableOf, be loyal
pet), (dog, CapableOf, be over-fed), (dog,
IsA, good pet), (dog, NotDesires, be with cat),

A.1.2 “different rel, rephrase“
• (window, MadeOf, glass) : (window,

HasProperty, make of glass), (window,
DefinedAs, glass that be stick to window
frame), (abottle, MadeOf, glass), (window,
UsedFor, look out of), (window, UsedFor,
look inside),

• (bury cat, HasSubevent, dig hole) : (bury cat,
HasFirstSubevent, dig hole), (bury cat, Has-
Subevent, dig), (bury cat, HasFirstSubevent,
dig grind), (bury cat, UsedFor, when your cat
be dead), (bury cat, HasPrerequisite, make
sure it be dead),

• (bridge, UsedFor, cross river) : (bridge, Ca-
pableOf, cross river), (bridge, UsedFor, cross
sometihng), (bridge, UsedFor, cross wa-
ter), (bridge, UsedFor, cross over), (bridge,
ReceivesAction, find over river),

A.1.3 “same rel, similar word“
• (cat, CapableOf, hunt mouse) : (cat, Ca-

pableOf, hunt lizard), (cat, NotCapableOf,
like mouse), (cat, UsedFor, kill mouse), (cat,
CapableOf, kill mouse), (cat, Desires, eat
mouse),

• (pilot, CapableOf, land airplane) : (pilot, Ca-
pableOf, carsh airplane), (pilot, CapableOf,

land taildragger), (pilot, CapableOf, work in
airplane), (pilot, CapableOf, land), (pilot, At-
Location, airplane),

• (play sport, HasSubevent, run) : (play
baseball, HasSubevent, run), (play frisbee,
Causes, run), (do some exercise, Has-
Subevent, run), (horse jump high when they,
HasProperty, run), (go for run, HasSubevent,
run),

A.1.4 “different rel, similar word“
• (statue, AtLocation, museum) : (statue, Re-

ceivesAction, see in museum), (statue, IsA,
example of art), (statue, UsedFor, imortalize
someone), (statue, HasProperty, hard to cre-
ate), (statue, CapableOf, be beautiful),

• (son, PartOf, family) : (son, IsA, member
of family), (man and his daughter, IsA, fam-
ily), (son, DefinedAs, child of parent), (son,
AtLocation, his home), (son, IsA, male kid of
his parent),

• (internet, UsedFor, research) : (internet, IsA,
amaze research tool), (go on internet, Used-
For, research), (internet, IsA, research project
of darpa), (internet, UsedFor, do research or
chat), (internet, HasA, lot of information),

A.1.5 “no directly related triple“
• (clerk, CapableOf, stock shelve) : (clerk, Ca-

pableOf, be bag grocery), (clerk, CapableOf,
price item), (clerk, CapableOf, bag gro-
cery), (clerk, CapableOf, enter data), (clerk,
AtLocation, at hotel),

• (human, HasA, five finger on each
hand) : (human, HasA, five toe on each
foot), (human, HasA, arm hand finger fin-
gernail and lunula), (human, HasA, two
hand), (human, CapableOf, write with right
hand), (human, CapableOf, stand on two
leg),

• (cat, CapableOf, corner mouse) : (cat, Not-
CapableOf, like mouse), (cat, CapableOf,
kill mouse), (cat, UsedFor, kill mouse), (cat,
UsedFor, keep mouse away), (cat, AtLoca-
tion, petstore),

A.2 Random split
In this Section we report examples for novelty cat-
egory from Random split dataset. For each exam-
ple we include the 5 examples that were shown to

14



the human annotator, ordered by closeness accord-
ing to our word embedding metric.

A.2.1 “same rel, rephrase“
• (coffee mug, AtLocation, cupboard) : (mug,

AtLocation, cupboard), (coffee cup, AtLoca-
tion, cupboard), (tea cup, AtLocation, cup-
board), (cup and plate, AtLocation, cup-
board), (can of soup, AtLocation, cupboard),

• (man, IsA, person) : (man, IsA, male per-
son), (egoistic person, IsA, person), (woman,
IsA, person), (child, InheritsFrom, per-
son), (child, IsA, person),

• (bookshelf, IsA, for store book) : (bookshelf,
UsedFor, store book), (bookshelf, Used-
For, display and store read mate-
rial), (bookshelf, UsedFor, hold and or-
ganize book), (bookshelf, UsedFor, organize
book), (bookshelf, UsedFor, display book),

A.2.2 “different rel, rephrase“
• (hear sing, HasSubevent, listen) : (hear

sing, HasFirstSubevent, listen), (hear sing,
HasPrerequisite, listen), (hear, HasPrerequi-
site, listen), (hear music, HasPrerequisite,
listen), (hear music, HasSubevent, listen),

• (procreate, HasPrerequisite, find mate)
: (procreate, HasFirstSubevent, find
mate), (procreate, Causes, have to raise your
grandchild), (procreate, HasFirstSubevent,
form will to do so),

• (go outside for even, MotivatedByGoal, see
star) : (go outside for even, HasSubevent,
that you see star), (go to film, UsedFor, see
star), (go outside for even, UsedFor, look at
star), (go outside for even, MotivatedByGoal,
you have date), (go outside for even, UsedFor,
get out of house),

A.2.3 “same rel, similar word“
• (aluminum, IsA, metal) : (aluminum,

IsA, material), (safety-pins, MadeOf,
metal), (titanium, IsA, metal), (quicksilver,
IsA, metal), (plumbum, IsA, metal),

• (cherry, AtLocation, jar) : (vegemite, AtLo-
cation, jar), (beet, AtLocation, jar), (toffee,
AtLocation, jar), (jellybeans, AtLocation,
jar), (moonshine, AtLocation, jar),

• (u.s president, IsA, political leader)
: (u.s president, IsA, in charge of arm
force), (president of something, IsA, it
leader), (president, IsA, leader), (president,
DefinedAs, leader of american govern-
ment), (us president, IsA, important political
figure),

A.2.4 “different rel, similar word“
• (attach case, AtLocation, embassy)

: (attach case, UsedFor, carry paper
and book), (attach case, AtLocation,
office), (attach case, AtLocation, court-
room), (attache case, AtLocation, busi-
nessperson hand), (attache case, CapableOf,
hold important document),

• (catch mumps, Causes, sickness) : (die,
HasSubevent, sickness), (catch mumps, Has-
Subevent, you have fever), (catch mumps,
HasFirstSubevent, get sick), (catch mumps,
MotivatedByGoal, be sick), (cold, IsA, sick-
ness),

• (buy something for love one, Causes, get
lay) : (get in line, MotivatedByGoal, get
lay), (have party, UsedFor, get lay), (get pay,
UsedFor, get lay), (become inebriate, Used-
For, get lay),

A.2.5 “no directly related triple“
• (fall from hot air balloon, CapableOf, kill

you) : (if you drink salt water it, CapableOf,
kill you), (drink sea water, CapableOf, kill
you), (water, CapableOf, kill you), (lighten,
CapableOf, kill you), (pretty thing, Capa-
bleOf, kill you),

• (milk, IsA, part of many food) : (milk, De-
finedAs, product of cow), (milk, ReceivesAc-
tion, produce by female cow), (milk, Capa-
bleOf, come from cow), (milk, ReceivesAc-
tion, make into cheese), (milk, ReceivesAc-
tion, create from cow),

• (some food, ReceivesAction, make from
dead animal) : (some food, HasProperty,
good but some be very dissgusting), (some
food, IsA, healthy and some be not), (some
food, HasProperty, poisonous if prepare im-
properly), (some food, ReceivesAction, grind
before eat), (some food, HasProperty, con-
sider exotic),

15



A.3 Wikipedia

In this Section we report examples for novelty cat-
egory from Wikipedia dataset. For each example
we include the 5 examples that were shown to the
human annotator, ordered by closeness according
to our word embedding metric.

A.3.1 “same rel, rephrase“
• (deep snow, IsA, winter) : (snow, Sym-

bolOf, winter), (snow, AtLocation, win-
ter), (it, IsA, winter), (snowflake, AtLocation,
winter), (nice time of year, IsA, winter time),

• (winter season, HasProperty, cold) : (winter
weather, HasProperty, cold), (in winter it,
HasProperty, cold), (snow fall from sky when
weather, HasProperty, cold), (stethascopes,
HasProperty, cold), (cold weather, Causes,
cold),

• (mathematical logic, HasProperty, log-
ical) : (mathmatics, HasProperty, log-
ical), (human wish for happiness but
happiness, NotHasProperty, logical), (design
computer chip, HasPrerequisite, logical
think), (write program, HasPrerequisite,
logical think), (logic, DefinedAs, set of rule
by which axiom can be manipulate to derive
true statement),

A.3.2 “different rel, rephrase“
• (the house, HasA, room) : (house, MadeOf,

room), (many different way to put furniture,
AtLocation, room), (something you find up-
stairs, IsA, room), (something you find down-
stairs, IsA, room), (family room, IsA, room),

A.3.3 “same rel, similar word“
• (bus system, AtLocation, city) : (subway sys-

tem, AtLocation, city), (bus stop, AtLoca-
tion, city), (bus, AtLocation, city), (bus shel-
ter, AtLocation, city), (bus station, AtLoca-
tion, city),

• (satellite radio, HasA, channel) : (tv, HasA,
channel), (hear news, HasSubevent, change
channel), (watch television, HasSubevent,
change channel), (cnn, IsA, television chan-
nel), (cnn, IsA, tv channel),

• (summer, IsA, hotter weather) : (summer,
HasA, more sunshine than winter), (summer,
IsA, hot than winter), (summer, IsA, warm

than winter), (summer, DefinedAs, season of
baseball), (summer, DefinedAs, warm sea-
son),

A.3.4 “different rel, similar word“
• (liberal democracy, HasProperty, political)

: (democracy, IsA, political system), (liberal
democratic party, InstanceOf, japanese po-
litical party), (feminism, IsA, political ide-
ology), (libertarianism, IsA, political ideol-
ogy), (liberalism, IsA, political ideology),

• (music, UsedFor, musical express) : (music,
CapableOf, be express use musical no-
tation), (music, ReceivesAction, play with
musical instrument), (music, ReceivesAction,
write with musical symbol), (music, Creat-
edBy, instrument or human voice), (music,
CapableOf, express feel),

• (the planet, HasA, mass) : (boston, PartOf,
mass), (matter, HasA, mass), (planet
plutoi, ReceivesAction, discover by
mr), (some planet, HasA, more than one
moon), (magnitude of planet, IsA, quantifi-
able),

A.3.5 “no directly related triple“
• (field, HasA, vector potential) : (field, HasA,

plant grow in them), (field, UsedFor, agri-
cultural pursuit), (field, UsedFor, cultivate
crop), (field, UsedFor, graze livestock), (field,
UsedFor, ride horse),

• (town, HasA, center of commerce) : (town,
ReceivesAction, compose of many neighbor-
hood), (town, HasProperty, likely to have sev-
eral cafe), (town, IsA, small than city), (town,
DefinedAs, prarie dog community), (town,
UsedFor, live in),

• (divorce, HasProperty, mutual consent)
: (divorce, NotHasProperty, more common
than marriage), (divorce, DefinedAs, official
end to marriage), (divorce, IsA, fact of
life), (divorce, DefinedAs, termination of
marriage), (divorce, IsA, when marry couple
separate legallyt),

16


