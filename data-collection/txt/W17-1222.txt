



















































Arabic Dialect Identification Using iVectors and ASR Transcripts


Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–183,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

Arabic Dialect Identification Using iVectors and ASR Transcripts

Shervin Malmasi
Harvard Medical School, USA

Macquarie University, Australia
shervin.malmasi@mq.edu.au

Marcos Zampieri
University of Cologne

Germany
mzampie2@uni-koeln.de

Abstract

This paper presents the systems submit-
ted by the MAZA team to the Arabic Di-
alect Identification (ADI) shared task at
the VarDial Evaluation Campaign 2017.
The goal of the task is to evaluate com-
putational models to identify the dialect
of Arabic utterances using both audio and
text transcriptions. The ADI shared task
dataset included Modern Standard Arabic
(MSA) and four Arabic dialects: Egyp-
tian, Gulf, Levantine, and North-African.
The three systems submitted by MAZA
are based on combinations of multiple ma-
chine learning classifiers arranged as (1)
voting ensemble; (2) mean probability en-
semble; (3) meta-classifier. The best re-
sults were obtained by the meta-classifier
achieving 71.7% accuracy, ranking second
among the six teams which participated in
the ADI shared task.

1 Introduction

The interest in Arabic natural language process-
ing (NLP) has grown substantially in the last
decades. This is evidenced by several publica-
tions on the topic and the dedicated series of work-
shops (WANLP) co-located with major interna-
tional computational linguistics conferences.1

Several Arabic dialects are spoken in North
Africa and in the Middle East co-existing with
Modern Standard Arabic (MSA) in a diglossic
situation. Arabic dialects are used in both spo-
ken and written forms (e.g. user-generated con-
tent) and pose a number of challenges for NLP
applications. Several studies on dialectal variation
of Arabic have been published including corpus

1https://sites.google.com/a/nyu.edu/
wanlp2017/

compilation for Arabic dialects (Al-Sabbagh and
Girju, 2012; Cotterell and Callison-Burch, 2014),
parsing (Chiang et al., 2006), machine translation
of Arabic dialects (Zbib et al., 2012), and finally,
the topic of the ADI shared task, Arabic dialect
identification (Zaidan and Callison-Burch, 2014;
Sadat et al., 2014; Malmasi et al., 2015).

In this paper we present the MAZA entries for
the 2017 ADI shared task which was organized
as part of the VarDial Evaluation Campaign 2017
(Zampieri et al., 2017). The ADI shared task
dataset (Ali et al., 2016) included audio and tran-
scripts from Modern Standard Arabic (MSA) and
four Arabic dialects: Egyptian, Gulf, Levantine,
and North-African.

2 Related Work

There have been several studies published on Ara-
bic dialect identification applied to both speech
and text.2 Examples of Arabic dialect identifica-
tion on speech data include the work by Biadsy et
al. (2009), Biadsy (2011), Biadsy and Hirschberg
(2009), and Bahari et al. (2014). Identifying Ara-
bic dialects in text also became a popular research
topic in recent years with several studies published
about it (Zaidan and Callison-Burch, 2014; Sadat
et al., 2014; Tillmann et al., 2014; Malmasi et al.,
2015).

To our knowledge, however, the 2017 ADI is
the first shared task to provide participants with
the opportunity to carry out Arabic dialect identi-
fication using a dataset containing both audio and
text (transcriptions). The first edition of the ADI
shared task, organized in 2016 as a sub-task of the
DSL shared task (Malmasi et al., 2016c), used a
similar dataset to the ADI 2017 dataset, but in-
cluded only transcriptions.

2See Shoufan and Al-Ameri (2015) for a survey on NLP
methods for processing Arabic dialects including a section on
Arabic dialect identification.

178



3 Methods and Data

We approach this task as a multi-class classifica-
tion problem. For our base classifier we utilize
a linear Support Vector Machine (SVM). SVMs
have proven to deliver very good performance in
discriminating between language varieties and in
other text classification problems, SVMs achieved
first place in both the 2015 (Malmasi and Dras,
2015a) and 2014 (Goutte et al., 2014) editions of
the DSL shared task.3

3.1 Data
The data comes from the aforementioned Arabic
dialect dataset by Ali et al. (2016) used in the 2016
edition of the ADI shared task. It contains audio
and ASR transcripts of broadcast, debate, and dis-
cussion programs from videos by Al Jazeera in
MSA and four Arabic dialects: Egyptian, Gulf,
Levantine, and North-African. In 2016, the or-
ganizers released only the transcriptions of these
videos and in 2017 transcriptions are combined
with audio features providing participants with
an interesting opportunity to test computational
methods that can be used both for text and speech.
We combined all the train/dev data (25,311 sam-
ples). The test set contained 1,492 instances.

3.2 Features
In this section we describe our features and evalu-
ate their performance under cross-validation.

We employ two lexical surface feature types
for this task, as described below. These are ex-
tracted from the transcriptions without any pre-
processing (e.g. case folding or tokenization) on
texts prior to feature extraction. Pre-processing
was not needed as the data are computer-generated
ASR transcripts. We also used the provided iVec-
tor features, as described below.

• Character n-grams: This sub-word feature
uses the constituent characters that make up
the whole text. When used as n-grams, the
features are n-character slices of the text.
Linguistically, these substrings, depending
on the length, can implicitly capture various
sub-lexical features including single letters,
phonemes, syllables, morphemes & suffixes.
Here we examine n-grams of size 1–8.

3See the 2014 and 2015 DSL shared task reports for more
information (Zampieri et al., 2015; Zampieri et al., 2014) and
Goutte et al. (2016) for a comprehensive evaluation on the
first two DSL shared tasks.

• Word n-grams: The surface forms of words
can be used as a feature for classification.
Each unique word may be used as a feature
(i.e. unigrams), but the use of bigram distri-
butions is also common. In this scenario, the
n-grams are extracted along with their fre-
quency distributions. For this study we eval-
uate unigram features.

• iVector Audio Features: Identity vectors or
iVectors are a probabilistic compression pro-
cess for dimensionality reduction. They have
been used in speech processing for dialect
and accent identification (Bahari et al., 2014),
as well as for language identification systems
(Dehak et al., 2011).

We now report our cross-validation results on the
training data. We began by testing individual fea-
ture types, with results displayed in Figure 1.

We observe that many character n-grams out-
perform the word unigram features. Character
4-grams and above obtained higher results than
those obtained using word unigrams. The best
transcript-based results were obtained with char-
acter 6-grams achieving 76.2% accuracy. The
audio-based iVector features, however, performed
substantially better with 85.3% accuracy. This
is a very large difference of almost 10% accu-
racy compared to the performance obtained using
words and characters.

Having demonstrated that these features are
useful for this task, we proceed to describe our
systems in the next section.

3.3 Systems

We created three systems for our submission, as
described below.

3.4 Voting Ensemble (System 1)

The best performing system in the 2015 edition
of the DSL challenge (Malmasi and Dras, 2015a)
used SVM ensembles evidencing the adequacy of
this approach for the task of discriminating be-
tween similar languages and language varieties.
In light of this, we decided to test two ensemble
methods. Classifier ensembles have also proven
to be an efficient and robust alternative in other
text classification tasks such as language identi-
fication (Malmasi and Dras, 2015a), grammatical
error detection (Xiang et al., 2015), and complex
word identification (Malmasi et al., 2016a).

179



85.31%

72.17%

75.27%

75.90%

76.16%

75.22%

73.11%

67.59%

55.09%

37.29%

30 35 40 45 50 55 60 65 70 75 80 85

IVEC

WORD1

CHAR8

CHAR7

CHAR6

CHAR5

CHAR4

CHAR3

CHAR2

CHAR1

Accuracy (%)

Fe
at

ur
e 

Ty
pe

Figure 1: Cross-validation performance for each of our individual feature types.

We follow the methodology described by Mal-
masi and Dras (2015a): we extract a number of
different feature types and train a single linear
model using each feature type. Our ensemble was
created using linear Support Vector Machine clas-
sifiers.4 We used all of the feature types listed in
Section 3.2 to create our ensemble of classifiers.

Each classifier predicts every input and also as-
signs a continuous output to each of the possible
labels. Using this information, we created the fol-
lowing two ensembles.

In the first system each classifier votes for a sin-
gle class label. The votes are tallied and the la-
bel with the highest number5 of votes wins. Ties
are broken arbitrarily. This voting method is very
simple and does not have any parameters to tune.
An extensive analysis of this method and its the-
oretical underpinnings can be found in the work
of (Kuncheva, 2004, p. 112). We submitted this
system as run 1.

3.5 Mean Probability Ensemble (System 2)
Our second system is similar to System 1 above,
but with a different combination method. Instead
of a single vote, the probability estimates for each
class6 are added together and the class label with
the highest average probability is the winner. An

4Linear SVMs have proven effective for text classification
tasks (Malmasi and Dras, 2014; Malmasi and Dras, 2015b).

5This differs with a majority voting combiner where a la-
bel must obtain over 50% of the votes to win. However, the
names are sometimes used interchangeably.

6SVM results can be converted to per-class probability
scores using Platt scaling.

important aspect of using probability outputs in
this way is that a classifier’s support for the true
class label is taken in to account, even when it is
not the predicted label (e.g. it could have the sec-
ond highest probability). This method has been
shown to work well on a wide range of problems
and, in general, it is considered to be simple, intu-
itive, stable (Kuncheva, 2014, p. 155) and resilient
to estimation errors (Kittler et al., 1998) making it
one of the most robust combiners discussed in the
literature. We submitted this system as run 2.

3.6 Meta-classifier (System 3)

In addition to classifier ensembles, meta-classifier
systems have proven to be very competitive for
text classification tasks (Malmasi and Zampieri,
2016) and we decided to include a meta-classifier
in our entry. Also referred to as classifier stacking,
a meta-classifier architecture is generally com-
posed of an ensemble of base classifiers that each
make predictions for all of the input data. Their
individual predictions, along with the gold labels
are used to train a second-level meta-classifier that
learns to predict the label for an input, given the
decisions of the individual classifiers. This setup
is illustrated in Figure 2. This meta-classifier at-
tempts to learn from the collective knowledge rep-
resented by the ensemble of local classifiers.

The first step in such an architecture is to create
the set of base classifiers that form the first layer.
For this we used the same base classifiers as our
ensembles described above.

180



Input
x

Classifier 1
with

Parameters
θ1

Final
Decision

C1

h1(x, θ1)

Ck−1

hk(x, θk)Ck

Ck−1

CT

First Level
Base Classifiers

Second Level
Meta Classifier

C
la

ss
ifi

er
 T

+1
w

ith
P

ar
am

et
er

s 
θ T

+1

hT(x, θT)

Figure 2: An illustration of a meta-classifier architecture. Image reproduced from Polikar (2006).

In this system we combined the probability out-
puts of our seven individual classifier and used
them to train a meta-classifier via cross-validation.
Following Malmasi et al. (2016b), we used a Ran-
dom Forest as our meta-classification algorithm.
We submitted this system as run 3.

4 Results

4.1 Cross-validation Results

We first report the cross-validation results of our
three systems on the training data. Results are
shown in Table 1.

System Accuracy
Majority Class Baseline 0.219

Voting Ensemble (System 1) 0.854
Probability Ensemble (System 2) 0.950

Meta-Classifier (System 3) 0.977

Table 1: Cross-validation results for the Arabic
training data.

We note that all of these methods outperform any
individual feature type, with the meta-classifier
achieving the best result of 97.7%. This is a very
large increase over the weakest system, which is
the voting ensemble with 85.4% accuracy. For the
voting ensemble 1,165 of the 25,311 training sam-
ples (4.60%) were ties that were broken arbitrarily.

This is an issue that can occur when there are an
even number of classifiers in a voting ensemble.

4.2 Test Set Results
Finally, in this section we report the results of
our three submissions generated from the unla-
belled test data. The samples in the test set were
slightly unbalanced with a majority class baseline
of 23.1%. Shared task performance was evalu-
ated and teams ranked according to the weighted
F1-score which provides a balance between preci-
sion and recall. Accuracy, along with macro- and
micro-averaged F1-scores were also reported.

We observe that the meta-classifier achieved the
best result among our three entries, following the
same relative pattern as the cross-validation re-
sults. The meta-classifier system ranked second
among the six teams participating in the ADI task.

In Figure 3 we present the confusion matrix
heat map for the output of our best system, the
meta-classifier. The confusion matrix confirms the
assumption that not all classes presented in the
dataset are equally difficult to identify. For ex-
ample, the system is able to identify MSA utter-
ances with substantially higher performance than
the performance obtained when identifying any of
the four Arabic dialects present in the dataset. We
also observe a higher degree of confusion in dis-
criminating between Gulf and Levantine Arabic
compared to the other dialects and MSA.

181



System Accuracy F1 (micro) F1 (macro) F1 (weighted)
Majority Class Baseline 0.231 — — —
Voting Ensemble (run1) 0.6086 0.6086 0.6032 0.6073

Probability Ensemble (run2) 0.6689 0.6689 0.6671 0.6679
Meta-classifier (run3) 0.7165 0.7165 0.7164 0.7170

Table 2: MAZA Results for the ADI task.

eg
y

gl
f

la
v

m
sa no
r

Predicted label

egy

glf

lav

msa

nor

T
ru

e
 l
a
b
e
l

Confusion Matrix

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Figure 3: Confusion Matrix for Run 3.

Finally, another important observation is that the
test set results are somewhat lower than the cross-
validation results. Although this was not specified
by the task organizers, it may have been the case
that the test data was drawn from a different dis-
tribution as the training data. An analysis of the
most informative features and the misclassified in-
stances in both the training and test sets may pro-
vide an explanation for this difference.

5 Conclusion

We presented three systems trained to identified
MSA and four Arabic dialects using iVectors and
ASR transcripts. The best results were obtained
by a meta-classifier achieving 71.7% accuracy and
ranking second in the ADI shared task 2017. To
the best of our knowledge, this was the first time
that computational methods have been evaluated
on Arabic dialect detection using audio and text.

An important insight is that combining text-
based features from transcripts with audio-based
features can substantially improve performance.
Additionally, we also saw that a meta-classifier
can provide a significant performance boost com-
pared to a classifier ensemble approach.

Acknowledgements

We would like to thank Preslav Nakov and Ahmed
Ali for proposing and organizing the ADI task. We
also thank the VarDial workshop reviewers who
provided us valuable feedback and suggestions to
improve this manuscript.

References
Rania Al-Sabbagh and Roxana Girju. 2012. YADAC:

Yet another Dialectal Arabic Corpus. In Proceed-
ings of LREC.

Ahmed Ali, Najim Dehak, Patrick Cardinal, Sameer
Khurana, Sree Harsha Yella, James Glass, Peter
Bell, and Steve Renals. 2016. Automatic Dialect
Detection in Arabic Broadcast Speech. In Proceed-
ings of INTERSPEECH.

Mohamad Hasan Bahari, Najim Dehak, Lukas Burget,
Ahmed M Ali, Jim Glass, et al. 2014. Non-negative
Factor Analysis of Gaussian Mixture Model Weight
Adaptation for Language and Dialect Recognition.
IEEE/ACM transactions on audio, speech, and lan-
guage processing, 22(7):1117–1129.

Fadi Biadsy and Julia Hirschberg. 2009. Using
Prosody and Phonotactics in Arabic Dialect Identifi-
cation. In Proceedings of INTERSPEECH.

Fadi Biadsy, Julia Hirschberg, and Nizar Habash.
2009. Spoken Arabic Dialect Identification using
Phonotactic Modeling. In Proceedings of the Work-
shop on Computational Approaches to Semitic Lan-
guages.

Fadi Biadsy. 2011. Automatic dialect and accent
recognition and its application to speech recogni-
tion. Ph.D. thesis, Columbia University.

David Chiang, Mona T Diab, Nizar Habash, Owen
Rambow, and Safiullah Shareef. 2006. Parsing Ara-
bic Dialects. In Proceedings of EACL.

Ryan Cotterell and Chris Callison-Burch. 2014. A
Multi-dialect, Multi-genre Corpus of Informal Writ-
ten Arabic. In Proceedings LREC.

Najim Dehak, Pedro A Torres-Carrasquillo, Douglas A
Reynolds, and Reda Dehak. 2011. Language
Recognition via i-vectors and Dimensionality Re-
duction. In Proceedings of INTERSPEECH.

182



Cyril Goutte, Serge Léger, and Marine Carpuat. 2014.
The NRC System for Discriminating Similar Lan-
guages. In Proceedings of the VarDial Workshop.

Cyril Goutte, Serge Léger, Shervin Malmasi, and Mar-
cos Zampieri. 2016. Discriminating Similar Lan-
guages: Evaluations and Explorations. In Proceed-
ings of LREC.

Josef Kittler, Mohamad Hatef, Robert PW Duin, and
Jiri Matas. 1998. On combining classifiers. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 20(3):226–239.

Ludmila I Kuncheva. 2004. Combining Pattern Clas-
sifiers: Methods and Algorithms. John Wiley &
Sons.

Ludmila I Kuncheva. 2014. Combining Pattern Clas-
sifiers: Methods and Algorithms. Wiley, second edi-
tion.

Shervin Malmasi and Mark Dras. 2014. Chinese
Native Language Identification. In Proceedings of
EACL.

Shervin Malmasi and Mark Dras. 2015a. Language
identification using classifier ensembles. In Pro-
ceedings of the LT4VarDial Workshop.

Shervin Malmasi and Mark Dras. 2015b. Multilingual
Native Language Identification. Natural Language
Engineering, pages 1–53.

Shervin Malmasi and Marcos Zampieri. 2016. Ara-
bic Dialect Identification in Speech Transcripts. In
Proceedings of the VarDial Workshop.

Shervin Malmasi, Eshrag Refaee, and Mark Dras.
2015. Arabic Dialect Identification using a Parallel
Multidialectal Corpus. In Proceedings of PACLING.

Shervin Malmasi, Mark Dras, and Marcos Zampieri.
2016a. LTG at SemEval-2016 Task 11: Complex
Word Identification with Classifier Ensembles. In
Proceedings of SemEval.

Shervin Malmasi, Marcos Zampieri, and Mark Dras.
2016b. Predicting Post Severity in Mental Health
Forums. In Proceedings of the CLPsych Workshop.

Shervin Malmasi, Marcos Zampieri, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, and Jörg Tiedemann.
2016c. Discriminating between Similar Languages
and Arabic Dialect Identification: A Report on the
Third DSL Shared Task. In Proceedings of the Var-
Dial Workshop.

Robi Polikar. 2006. Ensemble based systems in deci-
sion making. Circuits and Systems Magazine, IEEE,
6(3):21–45.

Fatiha Sadat, Farnazeh Kazemi, and Atefeh Farzindar.
2014. Automatic Identification of Arabic Language
Varieties and Dialects in Social Media. In Proceed-
ings of the SocialNLP Workshop.

Abdulhadi Shoufan and Sumaya Al-Ameri. 2015. Nat-
ural Language Processing for Dialectical Arabic: A
Survey. In Proceedings of the Arabic NLP Work-
shop.

Christoph Tillmann, Saab Mansour, and Yaser Al-
Onaizan. 2014. Improved Sentence-Level Arabic
Dialect Classification. In Proceedings of the Var-
Dial Workshop.

Yang Xiang, Xiaolong Wang, Wenying Han, and
Qinghua Hong. 2015. Chinese grammatical error
diagnosis using ensemble learning. In Proceedings
of the NLP-TEA Workshop.

Omar F Zaidan and Chris Callison-Burch. 2014. Ara-
bic Dialect Identification. Computational Linguis-
tics.

Marcos Zampieri, Liling Tan, Nikola Ljubešić, and
Jörg Tiedemann. 2014. A Report on the DSL
Shared Task 2014. In Proceedings of the VarDial
Workshop.

Marcos Zampieri, Liling Tan, Nikola Ljubešić, Jörg
Tiedemann, and Preslav Nakov. 2015. Overview
of the DSL Shared Task 2015. In Proceedings of the
LT4VarDial Workshop.

Marcos Zampieri, Shervin Malmasi, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, Jörg Tiedemann, Yves
Scherrer, and Noëmi Aepli. 2017. Findings of the
VarDial Evaluation Campaign 2017. In Proceedings
of the VarDial Workshop.

Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic Di-
alects. In Proceedings of NAACL-HLT.

183


