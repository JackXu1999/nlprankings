



















































Parsing for Grammatical Relations via Graph Merging


Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 26–35,
Vancouver, Canada, August 3 - August 4, 2017. c©2017 Association for Computational Linguistics

Parsing for Grammatical Relations via Graph Merging

Weiwei Sun, Yantao Du and Xiaojun Wan
Institute of Computer Science and Technology, Peking University

The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,duyantao,wanxiaojun}@pku.edu.cn

Abstract

This paper is concerned with building
deep grammatical relation (GR) analysis
using data-driven approach. To deal with
this problem, we propose graph merging, a
new perspective, for building flexible de-
pendency graphs: Constructing complex
graphs via constructing simple subgraphs.
We discuss two key problems in this per-
spective: (1) how to decompose a com-
plex graph into simple subgraphs, and (2)
how to combine subgraphs into a coher-
ent complex graph. Experiments demon-
strate the effectiveness of graph merging.
Our parser reaches state-of-the-art perfor-
mance and is significantly better than two
transition-based parsers.

1 Introduction

Grammatical relations (GRs) represent functional
relationships between language units in a sen-
tence. Marking not only local but also a wide
variety of long distance dependencies, GRs en-
code in-depth information of natural language sen-
tences. Traditionally, GRs are generated as a by-
product by grammar-guided parsers, e.g. RASP
(Carroll and Briscoe, 2002), C&C (Clark and Cur-
ran, 2007b) and Enju (Miyao et al., 2007). Very
recently, by representing GR analysis using gen-
eral directed dependency graphs, Sun et al. (2014)
and Zhang et al. (2016) showed that considerably
good GR structures can be directly obtained using
data-driven, transition-based parsing techniques.
We follow their encouraging work and study the
data-driven approach for producing GR analyses.

The key challenge of building GR graphs is due
to their flexibility. Different from surface syn-
tax, the GR graphs are not constrained to trees,
which is a fundamental consideration in design-

ing parsing algorithms. To deal with this problem,
we propose graph merging, a new perspective, for
building flexible representations. The basic idea is
to decompose a GR graph into several subgraphs,
each of which captures most but not the complete
information. On the one hand, each subgraph is
simple enough to allow efficient construction. On
the other hand, the combination of all subgraphs
enables whole target GR structure to be produced.

There are two major problems in the graph
merging perspective. First, how to decompose
a complex graph into simple subgraphs in a
principled way? To deal with this problem,
we considered structure-specific properties of the
syntactically-motivated GR graphs. One key prop-
erty is their reachability: In a given GR graph,
almost every node is reachable from a same and
unique root. If a node is not reachable, it is dis-
connected from other nodes. This property en-
sures a GR graph to be successfully decomposed
into limited number of forests, which in turn can
be accurately and efficiently built via tree parsing.
We model the graph decomposition problem as an
optimization problem and employ Lagrangian Re-
laxation for solutions.

Second, how to merge subgraphs into one co-
herent structure in a principled way? The prob-
lem of finding an optimal graph that consistently
combines the subgraphs obtained through individ-
ual models is non-trivial. We treat this problem as
a combinatory optimization problem and also em-
ploy Lagrangian Relaxation to solve the problem.
In particular, the parsing phase consists of two
steps. First, graph-based models are applied to as-
sign scores to individual arcs and various tuples of
arcs. Then, a Lagrangian Relaxation-based joint
decoder is applied to efficiently produces globally
optimal GR graphs according to all graph-based
models.

We conduct experiments on Chinese GRBank

26



浦东 近年 来 颁布 实行 了 涉及 经济 领域 的 法规性 文件
Pudong recently issue practice involve economic field regulatory document

root root

comp temp

subj
prt

obj

temp

subj

prt

obj

subj*ldd

obj
nmod

comp
relative

nmod

Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field.

(Sun et al., 2014). Though our parser does not
use any phrase-structure information, it produces
high-quality GR analysis with respect to depen-
dency matching. Our parsers obtain a labeled f-
score of 84.57 on the test set, resulting in an er-
ror reduction of 15.13% over Sun et al. (2014)’s
single system. and 10.86% over Zhang et al.
(2016)’s system. The remarkable parsing result
demonstrates the effectiveness of the graph merg-
ing framework. This framework can be adopted to
other types of flexible representations, e.g. seman-
tic dependency graphs (Oepen et al., 2014, 2015)
and abstract meaning representations (Banarescu
et al., 2013).

2 Background

In this paper, we focus on building GR analy-
sis for Mandarin Chinese. Mandarin is an an-
alytic language that lacks inflectional morphol-
ogy (almost) entirely and utilizes highly config-
urational ways to convey syntactic and semantic
information. This analytic nature allows to repre-
sent all GRs as bilexical dependencies. Sun et al.
(2014) showed that analysis for a variety of com-
plicated linguistic phenomena, e.g. coordination,
raising/control constructions, extraction, topical-
ization, can be conveniently encoded with directed
graphs. Moreover, such deep syntactic depen-
dency graphs can be effectively derived from Chi-
nese TreeBank (Xue et al., 2005) with very high
quality. Figure 1 is an example. In this graph,
“subj*ldd” between the word “涉及/involve” and
the word “文件/documents” represents a long-
distance subject-predicate relation. The arguments
and adjuncts of the coordinated verbs, namely “颁
布/issue” and “实行/practice,” are separately yet
distributively linked to the two heads.

By encoding GRs as directed graphs over
words, Sun et al. (2014) and Zhang et al. (2016)
showed that the data-driven, transition-based ap-

proach can be applied to build Chinese GR struc-
tures with very promising results. This architec-
ture is complementary to the traditional approach
to English GR analysis, which leverages grammar-
guided parsing under deep formalisms, such as
LFG (Kaplan et al., 2004), CCG (Clark and Curran,
2007a) and HPSG (Miyao et al., 2007). We follow
Sun et al.’s and Zhang et al.’s encouraging work
and study the discriminative, factorization models
for obtaining GR analysis.

3 The Idea

The key idea of this work is constructing a
complex structure via constructing simple partial
structures. Each partial structure is simple in the
sense that it allows efficient construction. For in-
stance, projective trees, 1-endpoint-corssing trees,
non-crossing dependency graphs and 1-endpoint-
crossing, pagenumber-2 graphs can be taken as
simple structures, given that low-degree polyno-
mial time parsing algorithms exist (Eisner, 1996;
Pitler et al., 2013; Kuhlmann and Jonsson, 2015;
Cao et al., 2017; Sun et al., 2017). To construct
each partial structure, we can employ mature pars-
ing techniques. To get the final target output, we
also require the total of all partial structures en-
ables whole target structure to be produced. In this
paper, we exemplify the above idea by designing
a new parser for obtaining GR graphs. Take the
GR graph in Figure 1 for example. It can be de-
composed into two tree-like subgraphs, shown in
Figure 2. If we can parse the sentence into sub-
graphs and combine them in a principled way, we
get the original GR graph.

Under this perspective, we need to develop a
principled method to decompose a complex struc-
ture into simple sturctures, which allows us to gen-
erate data to train simple solvers. We also need
to develop a principled method to integrate partial
structures, which allows us to produce coherent

27



浦东 近年 来 颁布 实行 了 涉及 经济 领域 的 法规性 文件
Pudong recently issue practice involve economic field regulatory document

root

root

comp

comp

temp

subj
prt

obj

temp

subj

prt

obj

subj[inverse]

obj

nmod

obj

nmod

comp
relative

nmod

nmod

Figure 2: A graph decomposition for the GR graph in Figure 1. The two subgraphs are shown on two
sides of the sentence respectively. The subgraph on the upper side of the sentence is exactly a tree,
while the one on the lower side is slightly different. The edge from the word “文件/document” to “涉
及/involve” is tagged “[inverse]” to indicate that the direction of the edge in the subgraph is in fact
opposite to that in the original graph.

structures as outputs. We are going to demonstrate
the techniques we use to solve these two problems.

4 Decomposing GR Graphs

4.1 Graph Decomposition as Optimization
Given a sentence s = w1w2 · · ·wn of length n,
we use a vector y of length n2 to denote a graph
on it. We use indices i and j to index the elements
in the vector, y(i, j) ∈ {0, 1}, denoting whether
there is an arc from wi to wj (1 ≤ i, j ≤ n).

Given a graph y, we hope to find m subgraphs
y1, ..., ym, each of which belongs to a specific
class of graphs Gk (k = 1, 2, · · · ,m). Each class
should allow efficient construction. For example,
we may need a subgraph to be a tree or a non-
crossing dependency graph. The combination of
all yk gives enough information to construct y.
Furthermore, the graph decomposition procedure
is utilized to generate training data for building
sub-models. Therefore, we hope each subgraph yk
is informative enough to train a good disambigua-
tion model. To do so, for each yk, we define a
score function sk that indicates the “goodness” of
yk. Integrating all ideas, we can formalize graph
decomposition as an optimization problem,

max.
∑

k sk(yk)
s.t. yi belongs to Gi∑

k yk(i, j) ≥ y(i, j), ∀i, j
The last condition in this optimization problem en-

sures that all edges in y appear at least in one sub-
graph.

For a specific graph decomposition task, we
should define good score functions sk and graph
classes Gk according to key properties of the tar-
get structure y.

4.2 Decomposing GR Graphs into Tree-like
Subgraphs

One key property of GR graphs is their reachabil-
ity: Every node is either reachable from a unique
root or by itself an independent connected com-
ponent. This property allows a GR graph to be
decomposed into limited number of tree-like sub-
graphs. By tree-like we mean if we treat a graph
on a sentence as undirected, it is a tree, or it is a
subgraph of some tree on the sentence. The ad-
vantage of tree-like subgraphs is that they can be
effectively built by adapting data-driven tree pars-
ing techniques. Take the sentence in Figure 1 for
example. For every word, there is at least one path
link the virtual root and this word. Furthermore,
we can decompose the graph into two tree-like
subgraphs, as shown in Figure 2. In this decom-
position, one subgraph is exactly a tree, and the
other is very close to a tree.

We restrict the number of subgraphs to 3. The
intuition is that we use one tree to capture long
distance information and the other two to capture

28



coordination information.1 In other words, we de-
compose each given graph y into three tree-like
subgraphs g1, g2 and g3. The goal is to let g1, g2
and g3 carry important information of the graph
as well as cover all edges in y. The optimization
problem can be written as

max. s1(g1) + s2(g2) + s3(g3)
s.t. g1, g2, g3 are tree-like

g1(i, j) + g2(i, j) + g3(i, j) ≥ y(i, j), ∀i, j
4.2.1 Scoring a Subgraph
We score a subgraph in a first order arc-factored
way, which first scores the edges separately and
then adds up the scores. Formally, the score func-
tion is sk(g) =

∑
ωk(i, j)gk(i, j) (k = 1, 2, 3)

where ωk(i, j) is the score of the edge from i to
j. Under this score function, we can use the Max-
imum Spanning Tree (MST) algorithm (Chu and
Liu, 1965; Edmonds, 1967; Eisner, 1996) to de-
code the tree-like subgraph with the highest score.

After we define the score function, extracting a
subgraph from a GR graph works like this: We
first assign heuristic weights ωk(i, j) (1 ≤ i, j ≤
n) to the potential edges between all the pairs of
words, then compute a best projective tree gk us-
ing the Eisner’s Algorithm:

gk = arg max
g

sk(g) = arg max
g

∑
ωk(i, j)g(i, j).

gk is not exactly a subgraph of y, because there
may be some edges in the tree but not in the graph.
To guarantee we get a subgraph of the original
graph, we add labels to the edges in trees to encode
necessary information. We label gk(i, j) with the
original label, if y(i, j) = 1; with the original la-
bel appended by “∼R” if y(j, i) = 1; with “None”
else. With this labeling, we can have a function
t2g to transform the extracted trees into tree-like
graphs. t2g(gk) is not necessary the same as the
original graph y, but must be a subgraph of it.

4.2.2 Three Variations of Scoring
With different weight assignments, we can extract
different trees from a graph, obtaining different

1 In this paper, we employ projective parsers. The mini-
mal number of sub-graphs is related to the pagenumber of GR
graphs. The pagenumber of 90.96% GR graphs is smaller
than or equal to 2, while the pagenumber of 98.18% GR
graphs is at most 3. That means 3 projective trees are perhaps
good enough to handle Chinese sentences, but 2 projective
trees are not. Due to the empirical results in Table 3, using
three projective trees can handle 99.55% GR arcs. Therefore,
we think three is suitable for our problem.

subgraphs. We devise three variations of weight
assignment: ω1, ω2, and ω3. Each ωk (k is 1,2
or 3) consists of two parts. One is shared by
all, denoted by S, and the other is different from
each other, denoted by V . Formally, ωk(i, j) =
S(i, j) + Vk(i, j) (k = 1, 2, 3 and 1 ≤ i, j ≤ n).

Given a graph y, S is defined as S(i, j) =
S1(i, j) + S2(i, j) + S3(i, j) + S4(i, j), where

S1(i, j) =
{
c1 if y(i, j) = 1 or y(j, i) = 1
0 else

S2(i, j) =
{
c2 if y(i, j) = 1
0 else

S3(i, j) = c3(n− |i− j|)
S4(i, j) = c4(n− lp(i, j))

In the definitions above, c1, c2, c3 and c4 are
coefficients, satisfying c1 � c2 � c3, and lp is a
function of i and j. lp(i, j) is the length of shortest
path from i to j that either i is a child of an ances-
tor of j or j is a child of an ancestor of i. That is
to say, the paths are in the form i ← n1 ← · · · ←
nk → j or i ← n1 → · · · → nk → j. If no such
path exits, then lp(i, j) = n. The intuition behind
the design is illustrated below.

S1 indicates whether there is an edge between i
and j, and we want it to matter mostly;

S2 indicates whether the edge is from i to j, and
we want the edge with correct direction to be
selected more likely;

S3 indicates the distance between i and j, and we
like the edge with short distance because it is
easier to predict;

S4 indicates the length of certain type of path be-
tween i and j that reflects c-commanding re-
lationships, and the coefficient remains to be
tuned.

We want the score V to capture different infor-
mation of the GR graph. In GR graphs, we have
an additional information (as denoted as “*ldd”
in Figure 1) for long distance dependency edges.
Moreover, we notice that conjunction is another
important structure, and they can be derived from
the GR graph. Assume that we tag the edges re-
lating to conjunctions with “*cjt.” The three varia-
tion scores, i.e. V1, V2 and V3, reflect long distance
and the conjunction information in different ways.

29



wp ... wc1 ... wgc2 ... wgc1 ... wc2 ... wl

X*cjt

X*cjt

X*cjt
X*ldd

Figure 3: Examples to illustrate the additional
weights.

V1. First for edges y(i, j) whose label is tagged
with *ldd, we assign V1(i, j) = d. d is a co-
efficient to be tuned on validation data.. When-
ever we come across a parent p with a set of con-
junction children cjt1, cjt2, · · · , cjtn, we find the
rightmost child gc1r of the leftmost child in con-
junction cjt1, and add d to each V1(p, cjt1) and
V1(cjt1, gc1r). The edges in conjunction that are
added additional d’s to are shown in blue in Figure
3.

V2. Different from V1, for edges y(i, j) whose
label is tagged with *ldd, we assign an V2(j, i) =
d. Then for each conjunction structure with
a parent p and a set of conjunction children
cjt1, cjt2, · · · , cjtn, we find the leftmost child
gcnl of the rightmost child in conjunction cjtn,
and add d to each V2(p, cjtn) and V2(cjtn, gcnl).
The concerned edges in conjunction are shown in
green in Figure 3.

V3. We do not assign d’s to the edges with tag
*ldd. For each conjunction with parent p and con-
junction children cjt1, cjt2, · · · , cjtn, we add an
d to V3(p, cjt1), V3(p, cjt2), · · · , and V3(p, cjtn).
4.3 Lagrangian Relaxation with

Approximation
As soon as we get three trees g1, g2 and g3, we get
three subgraphs t2g(g1), t2g(g2) and t2g(g3). As
is stated above, we want every edge in a graph y to
be covered by at least one subgraph, and we want
to maximize the sum of the edge weights of all
trees. Note that the inequality in the constrained
optimization problem above can be replaced by a
maximization, written as

max. s1(g1) + s2(g2) + s3(g3)
s.t. g1, g2, g3 are trees

max{t2g(g1)(i, j), t2g(g2)(i, j),
t2g(g3)(i, j)} = y(i, j),∀i, j

where sk(gk) =
∑
ωk(i, j)gk(i, j)

Let gm = max{t2g(g1), t2g(g2), t2g(g3)},
and by max{g1, g2, g3}we mean to take the maxi-
mum of three vectors pointwisely. The Lagrangian

Algorithm 1: The Tree Extraction Algorithm
Initialization: set u(0) to 0
for k = 0 to K do

g1 ← arg maxg1 s1(g1) + u(k)>g1
g2 ← arg maxg2 s2(g2) + u(k)>g2
g3 ← arg maxg3 s3(g3) + u(k)>g3
if max{g1, g2, g3} = y then

return g1, g2, g3
u(k+1) ←
u(k) − α(k)(max{g1, g2, g3} − y)

return g1, g2, g3

of the problem is

L(g1, g2, g3;u) = s1(g1) + s2(g2) + s3(g3)
+u>(gm − y)

where u is the Lagrangian multiplier.
Then the dual is

L(u) = max
g1,g2,g3

L(g1, g2, g3;u)

= max
g1

(s1(g1) +
1
3
u>gm)

+ max
g2

(s2(g2) +
1
3
u>gm)

+ max
g3

(s3(g3) +
1
3
u>gm)− u>y

According to the duality principle,
maxg1,g2,g3;u minu L(g1, g2, g3) = minu L(u),
so we can find the optimal solution for the
problem if we can find minu L(u). However it
is very hard to compute L(u), not to mention
minu L(u). The challenge is that gm in the three
maximizations must be consistent.

The idea is to separate the overall maximization
into three maximization problems by approxima-
tion. We observe that g1, g2, and g3 are very close
to gm, so we can approximate L(u) by
L′(u) = max

g1,g2,g3
L(g1, g2, g3;u)

= max
g1

(s1(g1) +
1
3
u>g1)

+ max
g2

(s2(g2) +
1
3
u>g2)

+ max
g3

(s3(g3) +
1
3
u>g3)− u>y

In this case, the three maximization problem can
be decoded separately, and we can try to find the
optimal u using the subgradient method.

30



4.4 The Algorithm

Algorithm 1 is our tree decomposition algorithm.
In the algorithm, we use subgradient method to
find minu L′(u) iteratively. In each iteration, we
first compute g1, g2, and g3 to find L′(u), then
update u until the graph is covered by the sub-
graphs. The coefficient 13 ’s can be merged into
the steps α(k), so we omit them. The three sep-
arate problems gk ← arg maxgk sk(gk) + u>gk
(k = 1, 2, 3) can be solved using Eisner’s algo-
rithm, similar to solving arg maxgk sk(gk). In-
tuitively, the Lagrangian multiplier u in our Al-
gorithm can be regarded as additional weights for
the score function. The update of u is to increase
weights to the edges that are not covered by any
tree-like subgraph, so that it will be more likely
for them to be selected in the next iteration.

5 Graph Merging

The extraction algorithm gives three classes of
trees for each graph. We apply the algorithm to
the graph training set, and get three training tree
sets. After that, we can train three parsing models
with the three tree sets. In this work, the parser
we use to train models and parse trees is Mate
(Bohnet, 2010), a second-order graph-based de-
pendency parser.

Let the scores the three models use be
f1, f2, f3 respectively. Then the parsers can
find trees with highest scores for a sentence.
That is solving the following optimization prob-
lems: arg maxg1 f1(g1), arg maxg2 f2(g2) and
arg maxg2 f3(g3). We can parse a given sen-
tence with the three models, obtain three trees,
and then transform them into subgraphs, and com-
bine them together to obtain the graph parse of
the sentence by putting all the edges in the three
subgraphs together. That is to say, we obtain the
graph y = max{t2g(g1), t2g(g2), t2g(g3)}. We
call this process simple merging.

However, the simple merging process omits
some consistency that the three trees extracted
from the same graph achieve, thus losing some
important information. The information is that
when we decompose a graph into three subgraphs,
some edges tend to appear in certain classes of
subgraphs at the same time. We want to retain
the co-occurrence relationship of the edges when
doing parsing and merging. To retain the hidden
consistency, we must do joint decoding instead of
decode the three models separately.

5.1 Capturing the Hidden Consistency

In order to capture the hidden consistency, we add
consistency tags to the labels of the extracted trees
to represent the co-occurrence. The basic idea is
to use additional tag to encode the relationship of
the edges in the three trees. The tag set is T =
{0, 1, 2, 3, 4, 5, 6}. Given a tag t ∈ T , t&1, t&2,
t&4 denote whether the edge is contained in g1,
g2, g3 respectively, where the operator “&” is the
bitwise AND operator. Specially, since we do not
need to consider first bit of the tags of edges in g1,
the second bit in g2, and the third bit in g3, we
always assign 0 to them. For example, if y(i, j) =
1, g1(i, j) = 1, g2(j, i) = 1, g3(i, j) = 0 and
t3(j, i) = 0, we tag g1(i, j) as 2 and g2(j, i) as 1.

When it comes to parsing, we also get labels
with consistency information. Our goal is to guar-
antee the tags in edges of the parse trees for a
same sentence are consistent while graph merg-
ing. Since the consistency tags emerge, for con-
venience we index the graph and tree vector rep-
resentation using three indices. g(i, j, t) denotes
whether there is an edge from word wi to word
wj with tag t in graph g.

The joint decoding problem can be written as a
constrained optimization problem as

max. f1(g1) + f2(g2) + f3(g3)
s.t. g′1(i, j, 2) + g′1(i, j, 6) ≤

∑
t g

′
2(i, j, t)

g′1(i, j, 4) + g′1(i, j, 6) ≤
∑

t g
′
3(i, j, t)

g′2(i, j, 1) + g′2(i, j, 5) ≤
∑

t g
′
1(i, j, t)

g′2(i, j, 4) + g′2(i, j, 5) ≤
∑

t g
′
3(i, j, t)

g′3(i, j, 1) + g′3(i, j, 3) ≤
∑

t g
′
1(i, j, t)

g′3(i, j, 2) + g′3(i, j, 3) ≤
∑

t g
′
2(i, j, t)

∀i, j

where g′k = t2g(gk)(k = 1, 2, 3).
The inequality constraints in the problem are the

consistency constraints. Each of them gives the
constraint between two classes of trees. For exam-
ple, the first inequality says that an edge in g1 with
tag t&2 6= 0 exists only when the same edge in g2
exist. If all of these constraints are satisfied, the
subgraphs achieve the consistency.

5.2 Lagrangian Relaxation with
Approximation

To solve the constrained optimization problem
above, we do some transformations and then ap-
ply the Lagrangian Relaxation to it with approxi-
mation.

31



Let a12(i, j) = g1(i, j, 2) + g1(i, j, 6), then the
first constraint can be written as an equity con-
straint

g1(:, :, 2) + g1(:, :, 6) = a12. ∗ (
∑

t

g2(:, :, t))

where “:” is to take out all the elements in the
corresponding dimension, and “.∗” is to do multi-
plication pointwisely. So can the other inequality
constraints. If we take a12,a13, · · · ,a32 as con-
stants, then all the constraints are linear. The con-
straints thus can be written as

A1g1 +A2g2 +A3g3 = 0

where A1, A2, and A3 are matrices that can be
constructed from a12,a13, · · · ,a32.

The Lagrangian of the optimization problem is

L(g1, g2, g3;u) = f1(g1) + f2(g2) + f3(g3) +
u>(A1g1 +A2g2 +A3g3)

where u is the Lagrangian multiplier. Then the
dual is

L(u) = max
g1,g2,g3

L(g1, g2, g3;u)
= max

g1
(f1(g1) + u>A1g1)

+ max
g2

(f2(g2) + u>A2g2)

+ max
g3

(f3(g3) + u>A3g3)

Again, we use the subgradient method to min-
imize L(u). During the deduction, we take
a12,a13, · · · ,a32 as constants, but unfortunately
they are not. We propose an approximation for the
a’s in each iteration: Using the a’s we got in the
previous iteration instead. It is a reasonable ap-
proximation given that the u’s in two consecutive
iterations are similar and so are the a’s.

5.3 The Algorithm
The pseudo code of our algorithm is shown in Al-
gorithm 2. We know that the score functions f1,
f2, and f3 each consist of first-order scores and
higher order scores. So they can be written as

fk(g) = s1stk (g) + s
h
k(g)

where s1stk (g) =
∑
ωk(i, j)g(i, j) (k = 1, 2, 3).

With this property, each individual problem gk ←
arg maxgk fk(gk)+u

>Akgk can be decoded eas-
ily, with modifications to the first order weights

Algorithm 2: The Joint Decoding Algorithm
Initialization: set u(0), A1, A2, A3 to 0,
for k = 0 to K do

g1 ← arg maxg1 f1(g1) + u(k)>A1g1
g2 ← arg maxg2 f2(g2) + u(k)>A2g2
g3 ← arg maxg3 f3(g3) + u(k)>A3g3
update A1, A2, A3
if A1g1 +A2g2 +A3g3 = 0 then

return g1, g2, g3
u(k+1) ←
u(k) − α(k)(A1g1 +A2g2 +A3g3)

return g1, g2, g3

of the edges in the three models. Specifically, let
wk = u>Ak, then we can modify the ωk in sk to
ω′k, such that ω

′
k(i, j, t) = ωk(i, j, t)+wk(i, j, t)+

wk(j, i, t).
The update of w1, w2, w3 can be understood

in an intuitive way. When one of the constraints
is not satisfied, without loss of generality, say,
the first one for edge y(i, j). We know g1(i, j)
is tagged to represent that g2(i, j) = 1, but it
is not the case. So we increase the weight of
that edge with all kinds of tags in g2, and de-
crease the weight of the edge with tag representing
g2(i, j) = 1 in g1. After the update of the weights,
the consistency is more likely to be achieved.

5.4 Labeled Parsing

For sake of formal concision, we illustrate our al-
gorithms omitting the labels. It is straightforward
to extend the algorithms to labeled parsing. In the
joint decoding algorithm, we just need to extend
the weights w1, w2, w3 for every label that ap-
pears in the three tree sets, and the algorithm can
be deduced similarly.

6 Evaluation and Analysis

6.1 Experimental Setup

We conduct experiments on Chinese GRBank
(Sun et al., 2014), an LFG-style GR corpus for
Mandarin Chinese. Linguistically speaking, this
deep dependency annotation directly encodes in-
formation such as coordination, extraction, rais-
ing, control as well as many other long-range de-
pendencies. The selection for training, develop-
ment, test data is also according to Sun et al.
(2014)’s experiments. Gold standard POS-tags are
used for deriving features for disambiguation.

32



UP UR UF UCompl LP LR LF LCompl

SM

subgraph1 88.63 76.19 81.94 18.09 85.94 73.88 79.46 16.11
subgraph2 88.04 78.20 82.83 17.47 85.31 75.77 80.26 15.43
subgraph3 88.91 81.12 84.84 20.36 86.57 78.99 82.61 17.30

Merged 83.23 88.45 85.76 22.97 80.59 85.64 83.04 19.29

LR

subgraph1 89.76 77.48 83.17 18.60 87.17 75.25 80.77 16.39
subgraph2 89.30 79.18 83.93 18.66 86.68 76.85 81.47 16.56
subgraph3 89.42 81.55 85.31 20.53 87.09 79.43 83.08 17.81

Merged 88.07 85.14 86.58 26.32 85.55 82.70 84.10 21.61

Table 1: Results on development set. SM is for Simple Merging, and LR for Lagrangian Relaxation.

UP UR UF UCompl LP LR LF LCompl
subgraph1 89.80 76.74 82.76 18.69 87.81 75.04 80.93 17.13
subgraph2 89.34 78.66 83.66 18.46 87.26 76.84 81.72 16.97
subgraph3 89.57 81.23 85.19 20.18 87.78 79.61 83.49 18.22

Merged 88.06 85.11 86.56 26.24 86.03 83.16 84.57 22.84
Sun et al. - - - - 83.93 79.82 81.82 -

Zhang et al.[Single] - - - - 82.28 83.11 82.69 -
Zhang et al.[Ensemble] - - - - 84.92 85.28 85.10 -

Table 2: Lagrangian Relaxation Results on test set.

The measure for comparing two dependency
graphs is precision/recall of GR tokens which are
defined as 〈wh, wd, l〉 tuples, wherewh is the head,
wd is the dependent and l is the relation. Labeled
precision/recall (LP/LR) is the ratio of tuples cor-
rectly identified by the automatic generator, while
unlabeled precision/recall (UP/UR) is the ratio re-
gardless of l. F-score is a harmonic mean of pre-
cision and recall. These measures correspond to
attachment scores (LAS/UAS) in dependency tree
parsing. To evaluate our GR parsing models that
will be introduced later, we also report these met-
rics.

6.2 Results of Graph Decomposition

Table 3 shows the results of graph decomposition
on the training set. If we use simple decompo-
sition, say, directly extracting three trees from a
graph, we get three subgraphs. On the training
set, each kind of the subgraphs cover around 90%
edges and 30% sentences. When we merge them
together, they cover nearly 97% edges and over
70% sentences. This indicates that the ability of
a single tree is limited and three trees can cover
most of the edges.

When we apply Lagrangian Relaxation to the
decomposition process, both the edge coverage
and the sentence coverage gain great error reduc-

Coverage Edge Sentence

SD

subgraph1 85.52 28.73
subgraph2 88.42 28.36
subgraph3 90.40 34.37

Merged 96.93 71.66

LR

subgraph1 85.66 29.01
subgraph2 88.48 28.63
subgraph3 90.67 34.72

Merged 99.55 96.90

Table 3: Results of graph decomposition. SD is
for Simple Decomposition and LR for Lagrangian
Relaxation

tion, indicating that Lagrangian Relaxation is very
effective on the task of decomposition.

6.3 Results of Graph Merging

Table 1 shows the results of graph merging on the
development set, and Table 2 on test set. The three
training sets of trees are from the decomposition
with Lagrangian Relaxation and the models are
trained from them. In both tables, simple merging
(SM) refers to first decode the three trees for a sen-
tence then combine them by putting all the edges
together. As is shown, the merged graph achieves
higher f-score than other single models. With La-
grangian Relaxation, the performance of not only

33



the merged graph but also the three subgraphs are
improved, due to capturing the consistency infor-
mation.

When we do simple merging, though the recall
of each kind of subgraphs is much lower than the
precision of them, it is opposite of the merged
graph. This is because the consistency between
three models is not required and the models tend
to give diverse subgraph predictions. When we re-
quire the consistency between the three models,
the precision and recall become comparable, and
higher f-scores are achieved.

The best scores reported by previous work, i.e.
(Sun et al., 2014) and (Zhang et al., 2016) are
also listed in Table 2. We can see that our sub-
graphs already achieve competitive scores, and
the merged graph with Lagrangian Relaxation im-
proves both unlabeled and labeled f-scores sub-
stantially, with an error reduction of 15.13% and
10.86%. We also include Zhang et al.’s parsing re-
sult obtained by an ensemble model that integrate
six different transition-based models. We can see
that parser ensemble is very helpful for deep de-
pendency parsing and the accuracy of our graph
merging parser is sightly lower than this ensemble
model. Given that the architecture of graph merg-
ing is quite different from transition-based pars-
ing, we think system combination of our parser
and the transition-based parser is promising.

7 Conclusion

To construct complex linguistic graphs beyond
trees, we propose a new perspective, namely graph
merging. We take GR parsing as a case study and
exemplify the idea. There are two key problems
in this perspective, namely graph decomposition
and merging. To solve these two problems in a
principled way, we treat both problems as opti-
mization problems and employ combinatorial op-
timization techniques. Experiments demonstrate
the effectiveness of the graph merging framework.
This framework can be adopted to other types of
flexible representations, e.g. semantic dependency
graphs (Oepen et al., 2014, 2015) and abstract
meaning representations (Banarescu et al., 2013).

Acknowledgments

This work was supported by 863 Program of China
(2015AA015403), NSFC (61331011), and Key
Laboratory of Science, Technology and Standard
in Press Industry (Key Laboratory of Intelligent

Press Media Technology). We thank anonymous
reviewers for their valuable comments.

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representa-
tion for sembanking. In Proceedings of the
7th Linguistic Annotation Workshop and Interoper-
ability with Discourse. Association for Computa-
tional Linguistics, Sofia, Bulgaria, pages 178–186.
http://www.aclweb.org/anthology/W13-2322.

Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010). Coling 2010 Or-
ganizing Committee, Beijing, China, pages 89–97.
http://www.aclweb.org/anthology/C10-1011.

Junjie Cao, Sheng Huang, Weiwei Sun, and Xiao-
jun Wan. 2017. Parsing to 1-endpoint-crossing,
pagenumber-2 graphs. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics.

John Carroll and Ted Briscoe. 2002. High pre-
cision extraction of grammatical relations. In
Proceedings of the 19th International Conference
on Computational Linguistics - Volume 1. As-
sociation for Computational Linguistics, Strouds-
burg, PA, USA, COLING ’02, pages 1–7.
https://doi.org/10.3115/1072228.1072241.

Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica pages
14:1396–1400.

Stephen Clark and James Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Lin-
guistics. Association for Computational Linguis-
tics, Prague, Czech Republic, pages 248–255.
http://www.aclweb.org/anthology/P07-1032.

Stephen Clark and James R. Curran. 2007b.
Wide-coverage efficient statistical pars-
ing with CCG and log-linear models.
Computational Linguistics 33(4):493–552.
https://doi.org/10.1162/coli.2007.33.4.493.

J. Edmonds. 1967. Optimum branchings. Journal of
Research of the NationalBureau of Standards pages
71B:233–240.

Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: an exploration. In Proceed-
ings of the 16th conference on Computational lin-
guistics - Volume 1. Association for Computational
Linguistics, Stroudsburg, PA, USA, pages 340–345.

34



Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Main Proceedings. Association for Computational
Linguistics, Boston, Massachusetts, USA, pages
97–104.

Marco Kuhlmann and Peter Jonsson. 2015. Parsing to
noncrossing dependency graphs. Transactions of the
Association for Computational Linguistics 3:559–
570.

Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsu-
jii. 2007. Towards framework-independent
evaluation of deep linguistic parsers. In Ann
Copestake, editor, Proceedings of the GEAF 2007
Workshop. CSLI Publications, CSLI Studies in
Computational Linguistics Online, pages 238–258.
http://www.cs.cmu.edu/ sagae/docs/geaf07miyaoetal.pdf.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinková, Dan Flickinger, Jan
Hajic, and Zdenka Uresová. 2015. Semeval 2015
task 18: Broad-coverage semantic dependency pars-
ing. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015).

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajic, An-
gelina Ivanova, and Yi Zhang. 2014. Semeval 2014
task 8: Broad-coverage semantic dependency pars-
ing. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014). As-
sociation for Computational Linguistics and Dublin
City University, Dublin, Ireland, pages 63–72.
http://www.aclweb.org/anthology/S14-2008.

Emily Pitler, Sampath Kannan, and Mitchell Mar-
cus. 2013. Finding optimal 1-endpoint-crossing
trees. TACL 1:13–24. http://www.transacl.org/wp-
content/uploads/2013/03/paper13.pdf.

Weiwei Sun, Junjie Cao, and Xiaojun Wan. 2017. Se-
mantic dependency parsing via book embedding. In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.

Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding, and
Xiaojun Wan. 2014. Grammatical relations in Chi-
nese: GB-ground extraction and data-driven pars-
ing. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 446–
456. http://www.aclweb.org/anthology/P14-1042.

Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005. The penn Chinese treebank:
Phrase structure annotation of a large corpus.
Natural Language Engineering 11:207–238.
https://doi.org/10.1017/S135132490400364X.

Xun Zhang, Yantao Du, Weiwei Sun, and Xiaojun
Wan. 2016. Transition-based parsing for deep de-
pendency structures. Computational Linguistics
42(3):353–389. http://aclweb.org/anthology/J16-
3001.

35


