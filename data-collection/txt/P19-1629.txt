



















































Discourse Representation Parsing for Sentences and Documents


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6248

Discourse Representation Parsing for Sentences and Documents

Jiangming Liu Shay B. Cohen Mirella Lapata
Institute for Language, Cognition and Computation

School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB

Jiangming.Liu@ed.ac.uk, {scohen,mlap}@inf.ed.ac.uk

Abstract

We introduce a novel semantic parsing task
based on Discourse Representation Theory
(DRT; Kamp and Reyle 1993). Our model
operates over Discourse Representation Tree
Structures which we formally define for sen-
tences and documents. We present a general
framework for parsing discourse structures of
arbitrary length and granularity. We achieve
this with a neural model equipped with a su-
pervised hierarchical attention mechanism and
a linguistically-motivated copy strategy. Ex-
perimental results on sentence- and document-
level benchmarks show that our model outper-
forms competitive baselines by a wide margin.

1 Introduction

Semantic parsing is the task of mapping natural
language to machine interpretable meaning repre-
sentations. Various models have been proposed
over the years to learn semantic parsers from lin-
guistic expressions paired with logical forms, SQL
queries, or source code (Kate et al., 2005; Liang
et al., 2011; Zettlemoyer and Collins, 2005; Ba-
narescu et al., 2013; Wong and Mooney, 2007;
Kwiatkowski et al., 2011; Zhao and Huang, 2015).

The successful application of encoder-decoder
models (Sutskever et al., 2014; Bahdanau et al.,
2015) to a variety of NLP tasks has prompted the
reformulation of semantic parsing as a sequence-
to-sequence learning problem (Dong and Lap-
ata, 2016; Jia and Liang, 2016; Kočiskỳ et al.,
2016), although most recent efforts focus on ar-
chitectures which make use of the syntax of
meaning representations, e.g., by developing tree
or graph-structured decoders (Dong and Lapata,
2016; Cheng et al., 2017; Yin and Neubig, 2017;
Alvarez-Melis and Jaakkola, 2017; Rabinovich
et al., 2017; Buys and Blunsom, 2017).

In this work we focus on parsing formal mean-
ing representations in the style of Discourse Rep-
resentation Theory (DRT; Kamp and Reyle 1993).

a.

k1 :

x1 e1 t1

max(x1)
fall(e1)

Agent(e1, x1)
now(t1)
e1 ≤ t1

k2 : � :

x2 e2

john(x2)
push(e2)

Patient(e2, x2)
male(x1)
e2 ≤ e1

because(k1, k2)

b.
SDRS

max(x1)

k1 k2

DRS

fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1)

DRS

POS

DRS

john(x2) push(e2) Patient(e2, x1) male(x1)
temp_before(e2, e1)

because(k1, k2)

Max fell .
John might push him .

Figure 1: Meaning representation for the discourse
“Max fell. John might push him.” in box-like format
(top) and as a tree (bottom). Red lines indicate termi-
nals corresponding to words and green lines indicate
non-terminals corresponding to sentences. � and POS
are modality operators for possibility.

DRT is a popular theory of meaning representa-
tion (Kamp, 1981; Kamp and Reyle, 1993; Asher,
1993; Asher and Lascarides, 2003) designed to ac-
count for a variety of linguistic phenomena, in-
cluding the interpretation of pronouns and tempo-
ral expressions within and across sentences. The
basic meaning-carrying units in DRT are Dis-
course Representation Structures (DRSs) which
consist of discourse referents (e.g., x1, x2) repre-
senting entities in the discourse and discourse con-
ditions (e.g., max(x1), male(x1)) representing in-
formation about discourse referents. An example
of a two-sentence discourse in box-like format is
shown in Figure 1a. DRT parsing resembles the
task of mapping sentences to Abstract Meaning
Representations (AMRs; Banarescu et al. 2013) in
that logical forms are broad-coverage, they repre-
sent compositional utterances with varied vocabu-



6249

lary and syntax and are ungrounded, i.e., they are
not tied to a specific database from which answers
to queries might be retrieved (Zelle and Mooney,
1996; Cheng et al., 2017; Dahl et al., 1994).

Our work departs from previous general-
purpose semantic parsers (Flanigan et al., 2016;
Foland and Martin, 2017; Lyu and Titov, 2018;
Liu et al., 2018; van Noord et al., 2018b) in that
we focus on building representations for entire
documents rather than isolated utterances, and in-
troduce a novel semantic parsing task based on
DRT. Specifically, our model operates over Dis-
course Representation Tree Structures (DRTSs)
which are DRSs rendered in a tree-style format
(Liu et al. 2018; see Figure 1b). Discourse rep-
resentation parsing has been gaining more atten-
tion lately.1 The semantic analysis of text be-
yond isolated sentences can enhance various NLP
applications such as information retrieval (Zou
et al., 2014), summarization (Goyal and Eisen-
stein, 2016), conversational agents (Vinyals and
Le, 2015), machine translation (Sim Smith, 2017;
Bawden et al., 2018), and question anwsering (Ra-
jpurkar et al., 2018).

Our contributions in this work can be summa-
rized as follows: 1) We formally define Discourse
Representation Tree structures for sentences and
documents; 2) We present a general framework for
parsing discourse structures of arbitrary length and
granularity; our framework is based on a neural
model which decomposes the generation of mean-
ing representations into three stages following a
coarse-to-fine approach (Liu et al., 2018; Dong
and Lapata, 2018); 3) We further demonstrate that
three modeling innovations are key to tree struc-
ture prediction: a supervised hierarchical attention
mechanism, a linguistically-motivated copy strat-
egy, and constraint-based inference to ensure well-
formed DRTS output; 4) Experimental results on
sentence- and document-level benchmarks show
that our model outperforms competitive baselines
by a wide margin. We release our code and DRTS
benchmarks in the hope of driving research in se-
mantic parsing further.2

1The shared task on Discourse Representation Structure
parsing in IWCS 2019. https://sites.google.com/
view/iwcs2019/home

2https://github.com/LeonCrashCode/
TreeDRSparsing

2 Discourse Representation Trees

In this section, we define Discourse Representa-
tion Tree Structures (DRTSs). We adopt the box-
to-tree conversion algorithm of Liu et al. (2018) to
obtain trees which we generalize to multi-sentence
discourse. As shown in Figure 1, the conver-
sion preserves most of the content of DRS boxes,
such as referents, conditions, and their dependen-
cies. Furthermore, we add alignments between
sentences and DRTSs nodes.

A DRTS is represented by a labeled tree over a
domain D = [R, V,C,N ] where R denotes rela-
tion symbols, V denotes variable symbols, C de-
notes constants and N denotes scoping symbols.
Variables V are indexed and can refer to entities x,
events e, states s, time t, propositions p, and seg-
ments k.3 R is the disjoint union of a set of el-
ementary relations Re and segment relations Rs.
The set N is defined as the union of binary scop-
ing symbols Nb and unary scoping symbols Nu,
where Nb = {IMP,OR,DUP}, denoting condi-
tions involving implication, disjunction, and du-
plex,4 and Nu = {POS,NEC,NOT} denoting
modality operators expressing possibility, neces-
sity, and negation.

There are six types of nodes in a DRTS: simple
scoped nodes, proposition scoped nodes, segment
scoped nodes, elementary DRS nodes, segmented
DRS nodes, and atomic nodes. Atomic nodes are
leaf nodes such that their label is an instantiated
relation r ∈ R with argument variables from V or
constants from C.5 Relations can either be unary
or binary. For example, in Figure 1, male(x1) de-
notes an atomic node with a unary relation, while
Patient(e2, x1) denotes a binary relation node.

A simple scoped node can take one of the la-
bels in N . A node that takes a label from Nu has
only one child which is either an elementary or a
segmented DRS node. A binary scope label node
can take two children nodes which are an elemen-
tary or a segmented DRS. A proposition scoped
node can take as label one of the proposition vari-
ables p. Its children are elementary or segmented
DRS nodes. A segment scoped node can take as
label one of the segment variables k and its chil-

3Segment variables originate from Segmented Discourse
Representation Theory (SDRT; Asher and Lascarides 2003),
and denote units connected by discourse relations.

4Duplex represents wh-questions (e.g., who, what, how).
5In our formulation, the only constants used are for denot-

ing numbers. Proper names are denoted by relations, such as
John(x2).

https://sites.google.com/view/iwcs2019/home
https://sites.google.com/view/iwcs2019/home
https://github.com/LeonCrashCode/TreeDRSparsing
https://github.com/LeonCrashCode/TreeDRSparsing


6250

Figure 2: The DRTS parsing framework; words and sentences are encoded with bi-LSTMs; documents are de-
coded in three stages, starting with tree non-terminals, then relations, and finally variables. Decoding makes use
of multi-attention and copying.

dren are elementary or segmented DRS nodes.
An elementary DRS node is labeled with

“DRS” and has children (one or more) which
are atomic nodes (taking relations from Re), sim-
ple scoped nodes, or proposition scoped nodes.
Atomic nodes may use any of the variables except
for segment variables k. Finally, a segmented DRS
node (labeled with “SDRS”) takes at least two
children nodes which are segment scoped nodes
and at least one atomic node (where the variables
allowed are the segment variables that were cho-
sen for the other children nodes and the relations
are taken from Rs). For example, the root node
in Figure 1 is an SDRS node with two segment
variables k1 and k2 and the instantiated relation
is because(k1, k2). The children of the nodes la-
beled with the segment variables are elementary or
segmented DRS nodes. A full DRTS is a tree with
an elementary or segmented DRS node as root.

3 Modeling Framework

We propose a unified framework for sentence-
and document-level semantic parsing based on the
encoder-decoder architecture shown in Figure 2.
The encoder is used to obtain word and sentence
representations while the decoder generates trees
in three stages. Initially, elementary DRS nodes,
segmented DRS nodes, and scoped nodes are gen-
erated. Next, the relations of atomic nodes are
predicted, followed by their variables. In order
to make the framework compatible for discourse
structures of arbitrary length and granularity and
capable of adopting document-level information,
we equip the decoder with multi-attention, a su-
pervised attention mechanism for aligning DRTS
nodes to sentences, and a linguistically-motivated

copy strategy.

3.1 Encoder
Documents (or sentences) are represented as a se-
quence of words 〈d〉,w00,...,〈sepi〉,...,wij ,...,〈/d〉,
where 〈d〉 and 〈/d〉 denote the start and end of doc-
ument, respectively, and 〈sepi〉 denotes the right
boundary of the ith sentence.6

The jth token in the ith sentence of a document
is represented by vector xij = f([ewij ; ēwij ; e`ij ])
which is the concatenation (;) of randomly initial-
ized embeddings ewij , pre-trained word embed-
dings ēwij , and lemma embeddings e`ij (where
f(·) is a non-linear function). Embeddings ewij
and e`ij are randomly initialized and tuned during
training, while ēwij are fixed.

The encoder represents words and sentences in
a unified framework compatible with sentence-
and document-level DRTS parsing. Our experi-
ments employed recurrent neural networks with
long-short term memory units (LSTMs; Hochre-
iter and Schmidhuber 1997), however, there is
nothing inherent in our framework that is LSTM
specific. For instance, representations based on
convolutional (Kim, 2014) or recursive neural net-
works (Socher et al., 2012) are also possible.

Word Representation We encode the input text
with a bidirectional LSTM (biLSTM):

[
←→
hx00 :

←−→
hxmn ] = biLSTM(x00 : xmn),

where
←→
hxij denotes the hidden representation of

the encoder for xij , which denotes the input repre-
sentation of token j in sentence i.

6The left boundary of sentence i is the right boundary of
sentence i − 1, the left boundary of the first sentence is 〈d〉,
and the right boundary of the last sentence is 〈/d〉.



6251

ℎ"#
ℎ"#
$%

sentence-level
representations

word-level
representations

ℎ"#
$&

ℎ"#
$'

ℎ"#
$(

…

𝑔$%

𝑔$&

𝑔$'

𝑔$(

Figure 3: Multi-attention component; linear func-
tions gv(·) transform decoder hidden representations
into different vector spaces, where v shows which lin-
ear function is applied, e.g. hwordyk = g

word(hyk).

Shallow Sentence Representation Each sen-
tence can be represented via the concatenation of
the forward hidden state of its right boundary and
the backward hidden state of its left boundary, i.e.,
hxi = [

−−−−→
hx〈sepi〉

;
←−−−−−−
hx〈sepi−1〉

].

Deep Sentence Representation An alternative
to the shallow sentence representation just de-
scribed, is a biLSTM encoder:

[
←→
hx0 :

←→
hxm ] = biLSTM(hx0 : hxm),

which takes hxi , the shallow sentence representa-
tion, as input.

3.2 Decoder

We generate DRTSs following a three-stage de-
coding process (Liu et al., 2018), where each
stage can be regarded as a sequential prediction
on its own. Based on this, we propose the multi-
attention mechanism to make it possible to deal
with multiple sentences. The backbone of our
tree-generation procedure is an LSTM decoder
which takes encoder representations Hx as in-
put and constructs bracketed trees (i.e., strings) in
a top-down manner, while being equipped with
multi-attention. We first describe this attention
mechanism as it underlies all generation stages
and then move on to present each stage in detail.

3.2.1 Multi-Attention
Multi-attention aims to extract features from dif-
ferent encoder representations and is illustrated in
Figure 3. The hidden representations hyk of the
decoder are fed to various linear functions to ob-
tain vector space representations:

hvyk = g
v(hyk),

where gv(·) is a linear function with the
name v.7 Given encoder representations Hx =
hx0 , hx1 , ...hxm , we extract features by applying
a standard attention mechanism (Bahdanau et al.,
2015) on encoder representations hvyk :

Attnv(hyk , Hx) = Attn(h
v
yk
, Hx) =

m∑
i=1

βvkihxi ,

where weight βvki is computed by:

βvki =
exp(hv

T

yk
hxi)∑

o exp(h
vT
yk
hxo)

.

Multi-attention scores can be also obtained from
the attention weights:

Scorev(hyk , Hx) = [β
v
k0 : β

v
km]

3.2.2 Tree Generation
Stage 1 Our decoder first generates tree non-
terminals yst0 , ..., y

st
k (see Figure 2).

8 The proba-
bilistic distribution of the kth prediction is:

P (ystk |yst<k, Hx) = SoftMax(sstk ),

where Hx refers to the encoder representations
and score sstk is computed as:

sstk =f([hystk ; Attn
word(hystk , [hx00 :hxmn ])

; Attnsent(hystk , [hx0 :hxm ])]),
(1)

where hystk is the hidden representation of the de-
coder in Stage 1, i.e., hystk = LSTM(eystk−1).

9

Stage 2 Given elementary or segmented DRS
nodes generated in Stage 1, atomic nodes
ynd0 , ..., y

nd
k are predicted (see Figure 2), with the

aid of copy strategies which we discuss shortly.
The probabilistic distribution of the kth prediction
is:

P (yndk |ynd<k, Hx, Hyst) = SoftMax([sndk ; s
copy
k ]),

where sndk and s
copy
k are generation and copy

scores, respectively, over the kth prediction.

sndk =f([hyndk
; Attnword(hyndk

, [hx00:hxmn ])

; Attnsent(hyndk
, [hx0:hxm ])])

(2)

7In this paper, v could be “word”, “sent”, “copy”, “st2nd”
(from first to second stage) and “nd2rd” (from second to third
stage), which are used to distinguish linear functions in dif-
ferent roles, as explained later.

8Upper subscripts “st”, “nd”, and “rd” denote Stage 1, 2,
and 3, respectively.

9yst−1 is special token SOS denoting the start of sequence.



6252

s
copy
k = Score

copy(hyndk
, [h

copy
`′0

: h
copy
`′z

]) (3)

where [hcopy`0 : h
copy
`z

] are copy representations used
for copy scoring; and hyndk is the hidden represen-
tation of the decoder in Stage 2, which is obtained
based on how the previous token was constructed:

hndyk =


LSTM(gcopy(h

copy
yndk−1

)) yndk−1 is copied

LSTM(eyndk−1
) yndk−1 is generated

LSTM(gst2nd(hdrs)) k = 0

The generation of atomic nodes in the second
stage is conditioned on hdrs, the decoder hidden
representation of elementary or segmented DRS
nodes from Stage 1 by the linear function gst2nd.

For the generation of atomic nodes, we copy
lemmas from the input text. However, copying
is limited to unary nodes which mostly represent
entities and predicates (e.g., john(x1), eat(e1)),
and correspond almost verbatim to input to-
kens. Binary atomic nodes denote seman-
tic relations between two variables and do
not directly correspond to the surface text.
For example, given the DRTS for the utter-
ance “the oil company is deprived of ...”,
nodes oil(x1) and company(x2) will be copied
from oil and company, while node of(x2, x1) will
not be copied from deprived of.

Copy representations Md = [h
copy
`′0

: h
copy
`′z

] are
constructed for each document d from its encoder
hidden representations [hx00 : hxmn ], by averag-
ing the encoder word representations which have
the same lemma, where `′ ∈ L′ and L′ is the set of
distinct lemmas in document d:

h`′z =
1

N

∑
(ij):`ij=`′z

hxij ,

and N is the number of tokens with lemma `′z .

Stage 3 Finally, we generate terminals, i.e.
atomic node variables yrd0 , ..., y

rd
k (see Figure 2).

The probabilistic distribution of the kth prediction
is:

P (yrdk |yrd<k, Hx, Hynd) = SoftMax(srdk ),

srdk =f([hyrdk
; Attnword(hyrdk

, [hx00 :hxmn ])

; Attnsent(hyrdk
, [hx0 :hxm ])])

(4)

where hyrdk is the decoder hidden representation in
the third stage:

hyrdk
=

{
LSTM(eyrdk−1

) k 6= 0
LSTM(gnd2rd(hatm)) k = 0

Here, the generation of variables is conditioned
upon hatm, the decoder hidden representation of
atomic nodes from the second stage by the linear
function gnd2rd.

3.3 Training

The model is trained to minimize an average cross-
entropy loss objective:

L(θ) = − 1
N

∑
j

log pj , (5)

where pj is the distribution of output tokens, θ are
the parameters of the model. We use stochastic
gradient descent and adjust the learning rate with
Adam (Kingma and Ba, 2014).

4 Extensions

In this section we present two important ex-
tensions to the basic modeling framework out-
lined above. These include a supervised atten-
tion mechanism dedicated to aligning sentences to
tree nodes. This type of alignment is important
when parsing documents (rather than individual
sentences) and may also enhance the quality of the
copy mechanism. Our second extension concerns
the generation of well-formed and meaningful log-
ical forms which is generally challenging for se-
mantic parsers based on sequence-to-sequence ar-
chitectures, even more so when dealing with long
and complex sequences pertaining to documents.

4.1 Supervised Attention

The attention mechanism from Section 3.2.1 can
automatically learn alignments between encoder
and decoder hidden representations. However, as
shown in Figure 1, DRTSs are constructed recur-
sively and alignment information between DRTS
nodes and sentences is available. For this reason,
we propose a method to explicitly learn this align-
ment by exploiting the feature representations af-
forded by multi-attention. Specifically, we obtain
alignment weights via multi-attention:

Scorealign(hyk , [hx0 : hxm ]) = [β
align
k0 : β

align
km ]



6253

where βalignkm = P (ak = m|hyk , [hx0 : hxm ]),
i.e., the probabilistic distribution over alignments
from sentences to the kth prediction in the de-
coder, where ak = m denotes the kth prediction
aligned to the mth sentence. We add an alignment
loss to the objective in Equation (5):

L(θ) = − 1
N

∑
j

log pj +
1

N align

∑
k

log palignk ,

where palignk is the probability distribution of align-
ments. We then use these alignments in two ways.

Alignments as Features Alignments are incor-
porated as additional features in the decoder by
concatenating the aligned sentence representations
with the scoring layers. Equations (1), (2), and (4)
are thus rewritten as:

sstgk = f([hystgk
; Attnword(hystgk

, [hx00 : hxmn ])

hxak ; Attn
sent(hystgk

, [hx0 : hxm ])]),

where stg ∈ {st, nd, rd}, and hxak is the akth sen-
tence representation.

At test time, the scoring layer requires the
alignment information, so we first select the sen-
tence with the highest probability, i.e., a∗k =
arg maxak P (ak|hyk , [hx0 : hxm ]), and then add
its representation hx∗ak to the scoring layer.

Copying from Alignments We use alignment
as a means to modulate which information is
copied. Specifically, we allow copying to take
place only over sentences aligned to elemen-
tary DRS nodes. We construct copy repre-
sentations for each sentence in a document,
i.e., M0, ...,Mi, ...,Mm where Mi = [h

copy
`′i0

:

h
copy
`′iz

], `′iz ∈ L′i, and L′i is the set of distinct lem-
mas in the ith sentence:

h
copy
`′iz

=
1

N

∑
(ij):`ij=`′iz

hxij ,

Given the alignment between elementary DRS
nodes and sentences, we calculate the copying
score by rewriting Equation (3) as:

s
copy
k = Score

copy(hyndk
,Ma)

where a is the index of the sentence that is aligned
to the elementary DRS node.

At test time, when an elementary DRS is gener-
ated during the first stage, we further predict which
sentence the node should be aligned to. The infor-
mation is then passed onto the second stage, and
elements from the aligned sentence can be copied.

step stack valid candidates prediction
1 [] SDRS(, DRS( SDRS(
2 [SDRS(0] k1( k1(
3 [SDRS(0, k1(0] SDRS(, DRS( DRS(
4 [SDRS(0, k1(0, DRS(] simpSNs, ) )
5 [SDRS(0, k1(1] ) )
6 [SDRS(1] k2( k2(
... ... ... ...

Figure 4: Constraint-based inference in decoding
stage 1; simpSNs are simple scoped nodes; subscripts
denote the number of children already constructed.

4.2 Constraint-based Inference

Recall that our decoder consists of three stages,
each of which is a sequence-to-sequence model.
As a result, there is no guarantee that tree output
will be well-formed. To ensure the generation of
syntactically valid trees, at each step, we generate
the set of valid candidates Y validk which do not vi-
olate the DRTS definitions in Section 2, and then
select the highest scoring tree as our prediction:

y∗k = arg max
yk∈Y validk

P (yk|y<k, θ),

where θ are the parameters of the model,
and Y validk the set of valid candidates at step k.

In Stage 1, partial DRTSs are stored in a stack
and for each prediction the model checks the stack
to obtain a set of valid candidates. In the exam-
ple in Figure 4, segment scoped node k1 has a
child already at step 5, so predicting a right bracket
would not violate the definition of DRTS.10 In
stage 2, when generating relations for elementary
DRS nodes, the candidates come from Re and
lemmas that are used for copying; when generat-
ing relations for segmented DRS nodes, the can-
didates only come from Rs. Finally, in stage 3
we generate only two variables for binary relations
and one variable for unary relations. A formal de-
scription is given in the Appendix.

5 Experimental Setup

Benchmarks Our experiments were carried out
on the Groningen Meaning Bank (GMB; Bos et al.
2017) which provides a large collection of English
texts annotated with Discourse Representation
Structures. We preprocessed the GMB into the
tree-based format defined in Section 2 and created
two benchmarks, one which preserves document-
level boundaries, and a second one which treats
sentences as isolated instances. Various statistics

10Similar constraints apply to unary simple scoped nodes.



6254

Sentences Documents
#sent avgw #doc #sent avgs avgw

train 41,563 21.1 7,843 48,599 6.2 135.3
dev 5,173 21.0 991 6,111 6.2 134.0
test 5,451 21.2 1,035 6,469 6.3 137.2

Table 1: Statistics on the GMB sentence- and
document-level benchmarks (avgw denotes the average
number of words per sentence (or document), avgs de-
notes the average number of sentences per document).

on these are shown in Table 1, for the respective
training, development, and testing partitions. We
followed the same data splits as Liu et al. (2018).

Settings We carried out experiments on the
sentence- and document-level GMB benchmarks
in order to evaluate our framework. We used
the same empirical hyper-parameters for sentence-
and document-level parsing. The dimensions of
word and lemma embeddings were 300 and 100,
respectively. The encoder and decoder had two
layers with 300 and 600 hidden dimensions, re-
spectively. The dropout rate was 0.1. Pre-trained
word embeddings (100 dimensions) were gener-
ated with Word2Vec trained on the AFP portion of
the English Gigaword corpus.11

Model Comparison For the sentence-level ex-
periments, we compared our DRTS parser against
Liu et al. (2018) who also perform tree parsing
and have a decoder which first predicts the struc-
ture of the DRS, then its conditions, and finally
its referents. Our parser without the document-
level component is similar to Liu et al. (2018);
a key difference is that our model is equipped
with linguistically-motivated copy strategies. In
addition, we employed a baseline sequence-to-
sequence model (Dong and Lapata, 2016) which
treats DRTSs as linearized trees.

For the document-level experiments, we built
two baseline models. The first one treats doc-
uments as one long string (by concatenating all
document sentences) and performs sentence-level
parsing (DocSent). The second one parses each
sentence in a document with a parser trained on the
sentence-level version of the GMB and constructs
a (flat) document tree by gathering all senten-
tial DRTSs as children of a segmented DRS node
(DocTree). We used the sentence-level DRTS
parser for both baselines. We also compared four
variants of our document-level model: one with

11Models were trained on a single GPU without batches.

SDRS

max(x1)

k1 k2

DRS

fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1)

DRS

POS

DRS

john(x2) push(e2) Patient(e2, x1) male(x1) temp_before(e2, e1)

because(k1, k2)

b0

b1 b2

b3

b0 DRS b1
b0 DRS b2
b0 because b1 b2
b1 max x1
b1 fall e1
b1 Agent e1 x1
b1 now t1
b1 temp_before e1 t1
b2 POS b3
b3 john x2
b3 push e2
b3 Patient e2 x1
b3 male x1
b3 temp_before e2 e1

Figure 5: Clausal form for DRTS corresponding to the
document “Max fell. John might push him.”.

multi-attention and shallow sentence representa-
tions (Shallow); one with multi-attention and deep
sentence representations (Deep); a Deep model
with supervised attention and alignments as fea-
tures (DeepFeat); and finally, a Deep model with
copying modulated by supervised attention (Deep-
Copy). All variants of our DRTS parser and com-
parison models adopt constraint-based inference.

Evaluation We evaluated the output of our
semantic parser using COUNTER (van Noord
et al., 2018a), a recently proposed metric suited
to matching scoped meaning representations.
COUNTER converts DRSs to sets of clauses
and computes precision and recall on matching
clauses. We transformed DRTSs to clauses as
shown in Figure 5. b variables refer to DRS nodes,
and children of DRS nodes correspond to clauses.
We used a hill-climbing algorithm to match vari-
ables between predicted clauses and gold standard
clauses. We report F1 using exact match and par-
tial match. For example, given predicted clauses
“b0 fall e1, b0 Agent e2 x1, b0 push e2” and gold
standard clauses “b0 fall e1, b0 Agent e1 x1”, ex-
act F1 is 0.4 (1/3 precision and 1/2 recall) while
partial F1 is 0.67 (4/7 precision and 4/5 recall).

6 Results

Parsing Sentences Table 2 summarizes results
on the sentence-level semantic parsing task for our
model (DRTS parser), Liu et al.’s (2018) model,
and the sequence-to-sequence baseline (Seq2Seq).
As can be seen, our system outperforms compari-
son models by a wide margin. The better perfor-
mance over Liu et al. (2018) is due to the richer
feature space we exploit and the application of
linguistically-motivated copy strategies.

Parsing Documents Table 3 presents various
ablation studies for the document-level model on
the development set. Deep sentence representa-
tions when combined with multi-attention bring



6255

Models par-F1 exa-F1
Seq2Seq 61.27 51.21
Liu et al. (2018) 74.31 68.72
DRTS parser 80.06 77.85

Table 2: Results (test set) on
sentence-level GMB benchmark.

DRTS parser par-F1 exa-F1
Shallow 66.63 61.74
Deep 71.01 65.42
DeepFeat 71.44 66.43
DeepCopy 75.89 69.45

Table 3: Results (dev set) on
document-level GMB benchmark.

Models par-F1 exa-F1
DocSent 57.10 53.27
DocTree 62.83 58.22
DeepCopy 70.83 66.56

Table 4: Results (test set) on
document-level GMB benchmark.

DeepCopy atomic scoped DRS All
sentences 0.22 0.26 1.78 2.09
documents 3.57 4.54 25.02 30.75

Table 5: Percentage of ill-formed outputs without con-
straints during inference (test set); atomic refers to
atomic nodes, scoped refers to scoped nodes and DRS
referes to DRS nodes (from Section 2) violated.

improvements over shallow representations (+3.68
exact-F1). Using alignments as features and as a
way of highlighting where to copy from yields fur-
ther performance gains both in terms of exact and
partial F1. The best performing variant is Deep-
Copy which combines supervised attention with
copying. Table 4 shows our results on the test
set (see the Appendix for an example of model
output); we compare the best performing DRTS
parser (DeepCopy) against two baselines which
rely on our sentence-level parser (DocSent and
DocTree). The DRTS parser, which has a global
view of the document, outperforms variants which
construct document representations by aggregat-
ing individually parsed sentences.

Influence of Constraints In Table 5, we exam-
ine whether constraint-based inference is helpful.
In particular we show the percentage of ill-formed
DRTSs when constraints are not enforced. We
present results for the sentence- and document-
level parsers overall and broken down according to
the type of DRTS nodes being violated. 30.75% of
document level DRTSs are ill-formed when con-
straints are not imposed during inference. This is
in stark contrast with sentence-level outputs which
are mostly well-formed (only 2.09% display viola-
tions of any kind). We observe that most violations
concern elementary and segmented DRS nodes.

Influence of Document size Figure 6 shows
how our parser (DeepCopy variant) and compar-
ison systems perform on documents of varying
length. Unsurprisingly, we observe that F1 de-
creases with document length and that all systems
have trouble modeling documents with 10 sen-

5 6 7 8 9 10 11
40

50

60

70

document length

F 1
(%

)

DocSent DocTree DeepCopy

Figure 6: Model performance (exact F1%) as a func-
tion of document length (i.e., number of sentences).

tences and beyond. In general, DeepCopy has
an advantage over comparison systems due to the
more sophisticated alignment information and the
fact that it aims to generate global document-level
structures. Our results also indicate that mod-
eling longer documents which are relatively few
in the training set is challenging mainly because
the parser cannot learn reliable representations for
them. Moreover, as the size of documents in-
creases, ambiguity for the resolution of corefer-
ring expressions increases, suggesting that explicit
modeling of anaphoric links might be necessary.

7 Related Work

Le and Zuidema (2012) were the first to train a
data-driven DRT parser using a graph-based rep-
resentation. Recently, Liu et al. (2018) concep-
tualized DRT parsing as a tree structure predic-
tion problem which they modeled with a series of
encoder-decoder architectures. van Noord et al.
(2018b) adapt models from neural machine trans-
lation (Klein et al., 2017) to DRT parsing, also
following a graph-based representation. Previ-
ous work has focused exclusively on sentences,
whereas we design a general framework for pars-
ing sentences and documents and provide a model
which can be used interchangeably for both.

Various mechanisms have been proposed to
improve sequence-to-sequence models including
copying (Gu et al., 2016) and attention (Mikolov



6256

et al., 2013). Our copying mechanism is more spe-
cialized and linguistically-motivated: it considers
the semantics of the input text for deciding which
tokens to copy. While our multi-attention mecha-
nism is fairly general, it extracts features from dif-
ferent encoder representations (word- or sentence-
level) and flexibly integrates supervised and unsu-
pervised attention in a unified framework.

A few recent approaches focus on the align-
ment between semantic representations and in-
put text, either as a preprocessing step (Foland
and Martin, 2017; Damonte et al., 2017) or as a
latent variable (Lyu and Titov, 2018). Instead,
our parser implicitly models word-level align-
ments with multi-attention and explicitly obtains
sentence-level alignments with supervised atten-
tion, aiming to jointly train a semantic parser.

8 Conclusions

In this work we proposed a novel semantic pars-
ing task to obtain Discourse Representation Tree
Structures and introduced a general framework
for parsing texts of arbitrary length and granular-
ity. Experimental results on two benchmarks show
that our parser is able to obtain reasonably accu-
rate sentence- and document-level discourse rep-
resentation structures (77.85 and 66.56 exact-F1,
respectively). In the future, we would like to more
faithfully capture the semantics of documents by
explicitly modeling entities and their linking.

Acknowledgments We thank the anonymous
reviewers for their feedback and Johan Bos
for answering several questions relating to the
GMB. We gratefully acknowledge the support
of the European Research Council (Lapata, Liu;
award number 681760), the EU H2020 project
SUMMA (Cohen, Liu; grant agreement 688139)
and Bloomberg (Cohen, Liu).

References
David Alvarez-Melis and Tommi S. Jaakkola. 2017.

Tree-structured decoding with doubly-recurrent
neural networks. In Proceedings of the 5th In-
ternational Conference on Learning Representation
(ICLR), Toulon, France.

Nicholas Asher. 1993. Reference to abstract objects in
English: a philosophical semantics for natural lan-
guage metaphysics. Studies in Linguistics and Phi-
losophy. Kluwer, Dordrecht.

Nicholas Asher and Alex Lascarides. 2003. Logics of
conversation. Cambridge University Press.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 4th International Conference on Learning Rep-
resentations (ICLR), San Diego, California.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186, Sofia, Bulgaria.

Rachel Bawden, Rico Sennrich, Alexandra Birch, and
Barry Haddow. 2018. Evaluating discourse phe-
nomena in neural machine translation. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1304–1313, New Orleans,
Louisiana.

Johan Bos, Valerio Basile, Kilian Evang, Noortje Ven-
huizen, and Johannes Bjerva. 2017. The gronin-
gen meaning bank. In Nancy Ide and James Puste-
jovsky, editors, Handbook of Linguistic Annotation,
volume 2, pages 463–496. Springer.

Jan Buys and Phil Blunsom. 2017. Robust incremen-
tal neural semantic graph parsing. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1215–1226, Vancouver, Canada.

Jianpeng Cheng, Siva Reddy, Vijay Saraswat, and
Mirella Lapata. 2017. Learning structured natural
language representations for semantic parsing. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 44–55, Vancouver, Canada.

Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, Christine Pao
David Pallett, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis task:
the atis-3 corpus. In Proceedings of the workshop on
ARPA Human Language Technology, pages 43–48,
Plainsboro, New Jersey.

Marco Damonte, Shay B. Cohen, and Giorgio Satta.
2017. An incremental parser for abstract meaning
representation. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Papers,
pages 536–546, Valencia, Spain.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceed-
ings of the 56th Annual Meeting of the Association

http://aclweb.org/anthology/P18-1068
http://aclweb.org/anthology/P18-1068


6257

for Computational Linguistics (Volume 1: Long Pa-
pers), pages 731–742, Melbourne, Australia.

Jeffrey Flanigan, Chris Dyer, Noah A Smith, and Jaime
Carbonell. 2016. Cmu at semeval-2016 task 8:
Graph-based amr parsing with infinite ramp loss. In
Proceedings of the 10th International Workshop on
Semantic Evaluation (SemEval-2016), pages 1202–
1206, San Diego, California.

William Foland and James H Martin. 2017. Abstract
meaning representation parsing using lstm recurrent
neural networks. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
463–472, Vancouver, Canada.

Naman Goyal and Jacob Eisenstein. 2016. A joint
model of rhetorical discourse structure and sum-
marization. In Proceedings of the Workshop on
Structured Prediction for NLP, pages 25–34, Austin,
Texas. Association for Computational Linguistics.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1631–1640, Berlin, Germany.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany.

Hans Kamp. 1981. A theory of truth and semantic
representation. In J. A. G. Groenendijk, T. M. V.
Janssen, and M. B. J. Stokhof, editors, Formal Meth-
ods in the Study of Language, volume 1, pages 277–
322. Mathematisch Centrum, Amsterdam.

Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.

Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artificial Intelligence, pages 1062–
1068, Pittsburgh, Pennsylvania.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1746–1751, Doha, Qatar.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations (ICLR), Banff, Canada.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. Opennmt:
Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72, Vancouver, Canada.

Tomáš Kočiskỳ, Gábor Melis, Edward Grefenstette,
Chris Dyer, Wang Ling, Phil Blunsom, and
Karl Moritz Hermann. 2016. Semantic parsing with
semi-supervised sequential autoencoders. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1078–
1087, Austin, Texas.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1512–1523, Edinburgh, Scotland, UK.

Phong Le and Willem Zuidema. 2012. Learning com-
positional semantics for open domain semantic pars-
ing. In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING),
pages 1535–1552, Mumbai, India.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590–599, Port-
land, Oregon.

Jiangming Liu, Shay B. Cohen, and Mirella Lapata.
2018. Discourse representation structure parsing.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 429–439, Melbourne, Aus-
tralia.

Chunchuan Lyu and Ivan Titov. 2018. Amr parsing as
graph prediction with latent alignment. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 397–407, Melbourne, Australia.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119. Curran Associates,
Inc.

Rik van Noord, Lasha Abzianidze, Hessel Haagsma,
and Johan Bos. 2018a. Evaluating scoped meaning
representations. In Proceedings of the 11th Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2018), Miyazaki, Japan.

Rik van Noord, Lasha Abzianidze, Antonio Toral, and
Johan Bos. 2018b. Exploring neural methods for
parsing discourse representation structures. Trans-
actions of the Association for Computational Lin-
guistics, 6:619–633.

https://doi.org/10.18653/v1/W16-5903
https://doi.org/10.18653/v1/W16-5903
https://doi.org/10.18653/v1/W16-5903
http://aclweb.org/anthology/P18-1040
http://aclweb.org/anthology/P18-1037
http://aclweb.org/anthology/P18-1037
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf


6258

Ella Rabinovich, Noam Ordan, and Shuly Wintner.
2017. Found in translation: Reconstructing phylo-
genetic language trees from translations. In Pro-
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 530–540, Vancouver, Canada.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 784–789,
Melbourne, Australia.

Karin Sim Smith. 2017. On integrating discourse in
machine translation. In Proceedings of the Third
Workshop on Discourse in Machine Translation,
pages 110–121, Copenhagen, Denmark.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211, Jeju Island, Korea.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. In Proceedings of the Deep Learning
Workshop in the 31st International Conference on
Machine Learning, volume 37.

Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 960–967, Prague, Czech Repub-
lic.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440–450, Vancouver, Canada.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the 13th National
Conference on Artificial Intelligence, pages 1050–
1055, Portland, Oregon.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In PProceedings of the 21st Conference
in Uncertainty in Artificial Intelligence, pages 658–
666, Edinburgh, Scotland, UK.

Kai Zhao and Liang Huang. 2015. Type-driven in-
cremental semantic parsing with polymorphism. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1416–1421, Denver, Colorado.

Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2014.
Negation focus identification with contextual dis-
course information. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 522–
530, Baltimore, Maryland. Association for Compu-
tational Linguistics.

A Constraint-based Inference

In this section we provide more formal detail on
how our model applies constraint-based inference.
In order to guide sequential predictions, we de-
fine a State Tracker (ST) equipped with four func-
tions: INITIALIZATION initializes the ST, UP-
DATE updates the ST according to token y, IS-
TERMINATED determines whether the ST should
terminate, and VALID returns the set of valid can-
didates in the current state. The state tracker pro-
vides an efficient interface for applying constraints
during decoding. Sequential inference with the ST
is shown in Algorithm 1; θ are model parameters
and Y validk all possible valid predictions at step k.

A.1 Stage 1

Algorithm 2 implements the ST functions for
Stage 1; DRS denotes an elementary DRS node,
SDRS denotes a segmented DRS node, propSN
is short for proposition scoped node, segmSN is
short for segment scoped node, simpSN is short
for simple scoped node. Function INITIALIZA-
TION (lines 1–4) initializes the ST as an empty
stack with a counter.

Lines 5–15 implement the function UPDATE,
where y is placed on top of the stack if it is not
a CompletedSymbol (lines 6–7) and the counter
is incremented if y is an elementary DRS node
(lines 8–9). The top of the stack is popped if y is
a CompletedSymbol (line 12), i.e., the children of
the node on top of the stack have been generated,
and the stack is updated (line 13).

Lines 16–22 implement the function ISTERMI-
NATED. If the stack is empty, decoding in Stage 1
is completed. Function ISTERMINATED is called
after function UPDATE has been called at least
once (see lines 7–8 in Algorithm 1).

Lines 23–63 implement the function VALID,
which returns the set of valid candidates Y valid

http://aclweb.org/anthology/P18-2124
http://aclweb.org/anthology/P18-2124
https://doi.org/10.18653/v1/W17-4814
https://doi.org/10.18653/v1/W17-4814
https://doi.org/10.3115/v1/P14-1049
https://doi.org/10.3115/v1/P14-1049


6259

Algorithm 1 Inference with ST
1: procedure INFERENCE(ST, θ)
2: INITIALIZATION(ST)
3: k = 0
4: repeat
5: Y validk = VALID(ST)
6: y∗k = argmaxyk∈Y validk P (yk|y<k, θ)
7: UPDATE(ST, y∗k)
8: k = k + 1
9: until ISTERMINATED(ST)

10: return [y∗0 , ..., y∗k−1]
11: end procedure

in the current state. If the stack is empty, which
means that a root of a DRTS should be con-
structed, Y valid only includes elementary and seg-
mented DRS nodes (lines 24–25). We use top
to denote the top node of the stack (line 27).
If top is a proposition scoped node or segment
scoped node, Y valid includes an elementary and
segmented DRS node only if top has no children
(lines 29–30), otherwise Y valid includes Complet-
edSymbol (lines 31–32), showing that the scoped
node should be completed with only one elemen-
tary or segmented DRS node as a child. The same
constraints are applied to unary simple scoped
nodes (lines 34–39). Similarly, binary simple
scoped nodes should only have two elementary or
segmented DRS nodes as children (lines 40–45).

If top is an elementary DRS node, Y valid

is initialized with the set {CompletedSymbol}
(line 47), because it can be completed with-
out any child in Stage 1.12 Furthermore, if
the number of elementary DRS nodes is within
the threshold MAX DRS, top can have more
children, i.e., Y valid includes scoped nodes, ex-
cept segmented scoped nodes (lines 48–49). If
top is a segmented DRS node and has less
than two children, Y valid only includes segment
scoped nodes (lines 52–53). Furthermore, if the
number of elementary DRS nodes is within the
threshold MAX DRS, top can have more chil-
dren, i.e., Y valid includes segmented scoped nodes
(lines 55–57).

A.2 Stage 2

The ST functions for Stage 2 are shown in Algo-
rithm 3. Lines 1–5 implement the function INI-
TIALIZATION, which initializes ST as a relation
counter, a type flag, and a completed flag. The
relation counter records the number of relations
that have been already constructed. The type flag

12Atomic nodes are constructed in Stage 2.

shows the type of nodes, i.e., e for elementary
DRS nodes or s for segmented DRS nodes, based
on which the relations are constructed. The com-
pleted flag checks if the construction is completed.
Lines 6–11 implement the function UPDATE. If
CompletedSymbol is predicted, the completed flag
is set to true, and the completed flag is checked
(lines 12–14, function ISTERMINATED).

Lines 15–24 implement the function VALID.
If the number of constructed relations is zero,
Y valid only includes R (lines 16–17). If the num-
ber of constructed relations is within the thresh-
old MAX RELST.type, it is possible to construct
more relations (lines 18–19). If the number of
children exceeds the threshold, Y valid only in-
cludes CompletedSymbol to complete the con-
struction of relations (lines 20–21).

A.3 Stage 3
Algorithm 4 implements the ST functions for
Stage 3, where Ve includes entity variables, event
variables, state variables, time variables, propo-
sition variables, and constants, and Vs includes
segment variables. Lines 1–5 (INITIALIZATION)
initialize the ST with a variable counter, a type
flag, and a completed flag. The variable counter
records the number of variables that have already
been constructed. The type flag shows the type of
nodes (e for elementary DRS nodes or s for seg-
mented DRS nodes), based on which the variables
are constructed. The completed flag checks if
the construction is completed. Lines 6–11 imple-
ment the function UPDATE. If CompletedSymbol
is predicted, the completed flag is set to true and
checked (lines 12–14, function ISTERMINATED).

Lines 15–28 implement the function VALID. If
no variables are constructed, Y valid only includes
VST.type (lines 16–17). If only one variable is con-
structed and ST.type is a segmented DRS, Y valid

only includes Vs to construct one more variable
because relations in segmented DRS nodes are
binary (lines 21–22). If two variables are con-
structed, Y valid only includes CompletedSymbol
(line 25). Note that indices of variables are in in-
creased order.

B Example Output

We provide example output of our model (DRTS
parser, DeepCopy variant) for the GMB document
below in Figure 7.

European Union energy officials will



6260

hold an emergency meeting next
week amid concerns that the Russian-
Ukrainian dispute over natural gas
prices could affect EU gas supplies. An
EU statement released Friday says the
meeting is aimed at finding a common
approach. It also expresses the Euro-
pean Commission’s concern about the
situation, but says the EU top executive
body remains confident an agreement
will be reached. A Russian cut-off of
supplies to Ukraine will reduce the
amount of natural gas flowing through
the main pipeline toward Europe. But
the commission says there is no risk of a
gas shortage in the short term. German
officials say they are hoping for a quick
resolution to the dispute. Government
spokesman, Ulrich Wilhelm says of-
ficials have been in contact with both
sides at a working level, but will not
mediate.

Algorithm 2 State Tracker for Stage 1
1: procedure INITIALIZATION(ST)
2: ST.stack = []
3: ST.count = 0
4: end procedure
5: procedure UPDATE(ST, y)
6: if y is not CompletedSymbol then
7: ST.stack.push(y)
8: if y is DRS then
9: ST.count += 1

10: end if
11: else
12: ST.stack.pop()
13: ST.stack.top.childnum += 1
14: end if
15: end procedure
16: procedure ISTERMINATED(ST)
17: if ST.stack.empty() then
18: return True
19: else
20: return False
21: end if
22: end procedure
23: procedure VAILD(ST)
24: if ST.stack.empty() then
25: Y Valid = {DRS, SDRS}
26: else
27: top = ST.stack.top
28: if top is propSN or segmSN then
29: if top.childnum = 0 then
30: Y Valid = {DRS, SDRS}
31: else
32: Y Valid = {CompletedSymbol}
33: end if
34: else if top is unary simpSN then
35: if top.childnum = 0 then
36: Y Valid = {DRS, SDRS}
37: else
38: Y Valid = {CompletedSymbol}
39: end if
40: else if top is binary simpSN then
41: if top.childnum ≤ 1 then
42: Y Valid = {DRS, SDRS}
43: else
44: Y Valid = {CompletedSymbol}
45: end if
46: else if top is DRS then
47: Y Valid = {CompletedSymbol}
48: if ST.count < MAX DRS then
49: Y Valid = Y Valid ∪ {propSN, simpSN}
50: end if
51: else if top is SDRS then
52: if top.childnum < 2 then
53: Y Valid = {segmSN}
54: else
55: Y Valid = {CompletedSymbol}
56: if ST.count < MAX DRS then
57: Y Valid = Y Valid ∪ {segmSN}
58: end if
59: end if
60: end if
61: end if
62: return Y Valid

63: end procedure



6261

Algorithm 3 State Tracker for Stage 2
1: procedure INITIALIZATION(ST, type)
2: ST.count = 0
3: ST.completed = False
4: ST.type = type
5: end procedure
6: procedure UPDATE(ST, y)
7: ST.count += 1
8: if y is CompletedSymbol then
9: ST.completed = True

10: end if
11: end procedure
12: procedure ISTERMINATED(ST)
13: return ST.completed
14: end procedure
15: procedure VALID(ST)
16: if ST.count = 0 then
17: Y Valid = RST.type
18: else if ST.count < MAX RELST.type then
19: Y Valid = RST.type ∪ {CompletedSymbol}
20: else
21: Y Valid = {CompletedSymbol}
22: end if
23: return Y Valid

24: end procedure

Algorithm 4 State Tracker for Stage 3
1: procedure INITIALIZATION(ST, type)
2: ST.count = 0
3: ST.completed = False
4: ST.type = type
5: end procedure
6: procedure UPDATE(ST, y)
7: ST.count += 1
8: if y is CompletedSymbol then
9: ST.completed = True

10: end if
11: end procedure
12: procedure ISTERMINATED(ST)
13: return ST.completed
14: end procedure
15: procedure VALID(ST)
16: if ST.count = 0 then
17: Y Valid = VST.type
18: else if ST.count = 1 then
19: if ST.type is elementary DRS then
20: Y Valid = VST.type∪ {CompletedSymbol}
21: else if ST.type is segmented DRS then
22: Y Valid = VST.type
23: end if
24: else
25: Y Valid = {CompletedSymbol}
26: end if
27: return Y Valid

28: end procedure



6262

SD
RS

eu
ro

pe
an

(x
1)

in
(x

2,
x 1

)
un

io
n(

x 3
)

in
(x

2,
x 3

)
en

er
gy

(x
4)

in
(x

2,
x 4

)
of

fic
ia

l(x
2)

em
er

ge
nc

y(
x 5

)
in

(x
6,

x 5
)

m
ee

tin
g(

x 6
)

ho
ld

(e
1) 

Ag
en

t(e
1,

x 2
) 

Th
em

e(
e 1

,x
6) 

we
ek

(x
7) 

on
(e

1,
x 7

) 
ne

xt
(e

1) 
co

nc
er

n(
x 8

) 
Th

em
e(

x 8
,p

1) 
un

de
r(

e 1
,x

8)
no

w(
t 1)

 
te

m
p_

in
cl

ud
ed

(e
1,

t 2)
te

m
p_

be
fo

re
(t 1

,t
2)k 1 D
RS

N
EC

D
RS

p 1 D
RS

PO
S

D
RS

To
pi

c(
s 1

,x
9)

ru
ss

ia
n-

uk
ra

in
ia

n(
s 1

) 
di

sp
ut

e(
x 9

) 
To

pi
c(

s 2
,x

10
)

na
tu

ra
l(s

2) 
ga

s(
x 1

1) 
of

(x
10

,x
11

)
pr

ic
e(

x 1
0)

ov
er

(x
9,

x 1
0) 

eu
(x

12
)

of
(x

13
,x

12
) 

ga
s(

x 1
4)

of
(x

13
,x

14
) 

su
pp

lie
s(

x 1
3)

af
fe

ct
(e

2) 
sti

m
ul

us
(e

2,
x 9

)
Ex

pe
rie

nc
er

(e
2,

x 1
3) 

no
w(

t 1)
 

te
m

p_
in

cl
ud

ed
(e

2,
t 3)

 
te

m
p_

be
fo

re
(t 1

,t
3) 

k 2 D
RS

eu
(x

15
)

of
(x

16
,x

15
)

sta
te

m
en

t(x
16

)
eq

u(
x 1

6,
x 1

7) 
re

le
as

e(
e 3

) 
Th

em
e(

e 3
,x

17
)

fri
da

y(
x 1

8) 
on

(e
3,

x 1
8)

sa
y(

e 4
) 

Ca
us

e(
e 4

,x
16

)
To

pi
c(

e 4
,p

2) 
no

w(
t 1)

 
te

m
p_

in
cl

ud
ed

(e
4,

t 4)
eq

u(
t 4,

t 1)

p 2 D
RS

m
ee

tin
g(

x 1
9) 

ai
m

(e
5)

Th
em

e(
e 5

,x
19

)
To

pi
c(

s 3
,x

20
)

co
m

m
on

(s
3)

ap
pr

oa
ch

(x
20

) 
fin

d(
e 6

)
Ag

en
t(e

6,
x 2

1)
Th

em
e(

e 6
,x

20
)

at
(e

5,
e 6

)
no

w(
t 1)

 t
em

p_
in

cl
ud

ed
(e

5,
t 5)

 
eq

u(
t 5,

t 1)

k 3 D
RS

th
in

g(
x 2

) 
To

pi
c(

s 4
,x

22
) 

eu
ro

pe
an

(s
4)

co
m

m
iss

io
n(

x 2
2) 

of
(x

23
,x

22
)

co
nc

er
n(

x 2
3) 

sit
ua

tio
n(

x 2
4) 

ab
ou

t(x
23

,x
24

) 
ex

pr
es

s(
e 7

) 
Ag

en
t(e

7,
x 2

) 
Th

em
e(

e 7
,x

23
) 

no
w(

t 1)
 

te
m

p_
in

cl
ud

ed
(e

7,
t 6)

eq
u(

t 6,
t 1)

 a
lso

(e
7) 

D
RSk 4

th
in

g(
x 2

) 
sa

y(
e 8

)
Ca

us
e(

e 8
,x

2) 
To

pi
c(

e 8
,p

3) 
no

w(
t 1)

 te
m

p_
in

cl
ud

ed
(e

8,
t 7)

eq
u(

t 7,
t 1)

p 3 D
RS

eu
(x

25
) 

in
(x

26
,x

25
) 

To
pi

c(
s 5

,x
26

)
to

p(
s 5

)
To

pi
c(

s 6
,x

26
)

ex
ec

ut
iv

e(
s 6

) 
bo

dy
(x

26
) 

re
m

ai
n(

e 9
)

Ag
en

t(e
9,

x 2
6) 

To
pi

c(
e 9

,p
4) 

no
w(

t 1)
te

m
p_

in
cl

ud
ed

(e
9,

t 8)
 

eq
u(

t 8,
t 1)

p 4 D
RS

co
nf

id
en

t(e
10

)
Ag

en
t(e

10
,x

26
) 

To
pi

c(
e 1

0,
p 5

)

N
EC

D
RS

ag
re

em
en

t(x
27

) 
re

ac
h(

e 1
1) 

Th
em

e(
e 1

1,
x 2

7) 
no

w(
t 1)

 
Te

m
p_

in
cl

ud
ed

(e
11

,t
9)

Te
m

p_
be

fo
re

(t 1
,t

9)

k 5 D
RS

N
EC

D
RS

ru
ss

ia
(x

28
)

of
(x

29
,x

28
)

cu
t-o

ff(
x 2

9) 
su

pp
lie

s(
x 3

0) 
uk

ra
in

e(
x 3

1)
to

(x
30

,x
31

)
of

(x
29

,x
30

)
am

ou
nt

(x
32

)
To

pi
c(

s 7
,x

33
) 

na
tu

ra
l(s

7)
ga

s(
x 3

3)
of

(x
32

,x
33

)
eq

u(
x 3

2,
x 3

4)
flo

w(
e 1

2)
Th

em
e(

e 1
2,

x 3
4) 

To
pi

c(
s 8

,x
35

) 
m

ai
n(

s 8
) 

pi
pe

lin
e(

x 3
5)

eu
ro

pe
(x

36
) 

to
wa

rd
(x

35
,x

36
)

th
ro

ug
h(

e 1
2,

x 3
5)

re
du

ce
(e

13
)

Ca
us

e(
e 1

3,
x 2

9)
Pa

tie
nt

(e
13

,x
32

)
no

w(
t 1)

te
m

p_
in

cl
ud

ed
(e

13
,t

10
) 

te
m

p_
be

fo
re

(t 1
,t

10
)

k 6 D
RS

co
m

m
iss

io
n(

x 3
7)

sa
y(

e 1
4) 

Ca
us

e(
e 1

4,
x 3

7)
To

pi
c(

e 1
4,

p 5
) 

no
w(

t 1)
te

m
p_

in
cl

ud
ed

(e
14

,t
11

) 
eq

u(
t 11

,t
1) 

bu
t(e

14
)

p 5

D
RS

ris
k(

x 3
8) 

ga
s(

x 3
9

)
of

(x
40

,x
39

) 
sh

or
ta

ge
(x

40
)

Pa
tie

nt
(s

9,
x 4

1) 
sh

or
t(s

9) 
te

rm
(x

41
) 

in
(x

40
,x

41
)

of
(x

38
,x

40
) 

be
(e

15
) 

Ag
en

t(e
15

,x
42

) 
Th

em
e(

e 1
5,

x 3
8)

no
w(

t 1)
 

te
m

p_
in

cl
ud

ed
(e

15
,t

12
) 

eq
u(

t 12
,t

1)

N
O
T

D
RS

k 7 D
RS

ge
rm

an
y(

x 4
3)

of
(x

44
,x

43
) 

of
fic

ia
l(x

44
)

sa
y(

e 1
6)

Ca
us

e(
e 1

6,
x 4

4) 
To

pi
c(

e 1
6,

p 6
)

no
w(

t 1)
 

te
m

p_
in

cl
ud

ed
(e

16
,t

13
) 

eq
u(

t 13
,t

1)

p 6 D
RS

th
in

g(
x 2

)
ho

pe
(e

17
)

Th
em

e(
e 1

7,
x 2

)
To

pi
c(

s 1
0,

x 4
5) 

qu
ic

k(
s 1

0)
re

so
lu

tio
n(

x 4
5) 

di
sp

ut
e(

x 4
6) 

to
(x

45
,x

46
) 

fo
r(

e 1
7,

x 4
5) 

no
w(

t 1)
eq

u(
x 4

7,
t 1)

 
te

m
p_

in
cl

ud
es

(t 1
3,

x 4
7) 

te
m

p_
ov

er
la

p(
e 1

7,
t 13

)

k 8 D
RS

go
ve

rn
m

en
t(x

48
)

fo
r(

x 4
9,

x 4
8) 

sp
ok

es
m

an
(x

49
) 

ul
ric

h(
x 5

0)
eq

u(
x 5

1,
x 5

0)
wi

lh
el

m
(x

51
)

re
l(x

49
,x

51
)

sa
y(

e 1
8) 

Ca
us

e(
e 1

8,
x 4

9)
To

pi
c(

e 1
8,

p 7
) 

no
w(

t 1)
 

te
m

p_
in

cl
ud

ed
(e

18
,t

14
) 

eq
u(

t 14
,t

1)

p 7 D
RS

of
fic

ia
l(x

52
)

be
(e

19
) 

Ag
en

t(e
19

,x
52

)
co

nt
ac

t(x
53

) 
sid

e(
x 5

4)
wi

th
(x

53
,x

54
)

in
(e

19
,x

53
)

wo
rk

(e
20

) 
Pa

tie
nt

(e
20

,x
55

)
le

ve
l(x

55
)

at
(e

19
,x

55
)

no
w(

t 1)
eq

u(
x 5

6,
t 1)

 
te

m
p_

in
cl

ud
es

(e
21

,x
56

) 
te

m
p_

ab
ut

(e
19

,e
21

)

k 9 D
RS

go
ve

rn
m

en
t(x

57
) 

fo
r(

x 5
8,

x 5
7) 

sp
ok

es
m

an
(x

58
) 

ul
ric

h(
x 5

9)
eq

u(
x 6

0,
x 5

9) 
wi

lh
el

m
(x

60
)

re
l(x

58
,x

60
)

N
O
T

D
RS

N
EC

D
RS

m
ed

ia
te

(e
22

)
Ag

en
t(e

22
,x

58
)

no
w(

t 1)
 

te
m

p_
in

cl
ud

ed
(e

22
,t

15
) 

te
m

p_
be

fo
re

(t 1
,t

15
)co

nt
in

ua
tio

n(
k 1

,k
2) 

co
nt

in
ua

tio
n(

k 2
,k

3) 
co

nt
in

ua
tio

n(
k 3

,k
4) 

co
nt

ra
st(

k 3
,k

4) 
co

nt
in

ua
tio

n(
k 4

,k
5) 

co
nt

in
ua

tio
n(

k 5
,k

6)
co

nt
in

ua
tio

n(
k 6

,k
7) 

co
nt

in
ua

tio
n(

k 7
,k

8) 
co

nt
in

ua
tio

n(
k 8

,k
9) 

co
nt

ra
st(

k 8
,k

9) 

Figure 7: Output of DRTS parser (DeepCopy variant) for the document in Section 2.


