



















































Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1879–1889,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Transition-based Dependency Parsing
Using Two Heterogeneous Gated Recursive Neural Networks

Xinchi Chen, Yaqian Zhou, Chenxi Zhu, Xipeng Qiu, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University

School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China

{xinchichen13,zhouyaqian,czhu13,xpqiu,xjhuang}@fudan.edu.cn

Abstract

Recently, neural network based depen-
dency parsing has attracted much interest,
which can effectively alleviate the prob-
lems of data sparsity and feature engineer-
ing by using the dense features. How-
ever, it is still a challenge problem to
sufficiently model the complicated syn-
tactic and semantic compositions of the
dense features in neural network based
methods. In this paper, we propose
two heterogeneous gated recursive neu-
ral networks: tree structured gated re-
cursive neural network (Tree-GRNN) and
directed acyclic graph structured gated
recursive neural network (DAG-GRNN).
Then we integrate them to automati-
cally learn the compositions of the dense
features for transition-based dependency
parsing. Specifically, Tree-GRNN mod-
els the feature combinations for the trees
in stack, which already have partial depen-
dency structures. DAG-GRNN models the
feature combinations of the nodes whose
dependency relations have not been built
yet. Experiment results on two prevalent
benchmark datasets (PTB3 and CTB5)
show the effectiveness of our proposed
model.

1 Introduction

Transition-based dependency parsing is a core task
in natural language processing, which has been
studied with considerable efforts in the NLP com-
munity. The traditional discriminative dependency
parsing methods have achieved great success (Koo
et al., 2008; He et al., 2013; Bohnet, 2010; Huang
and Sagae, 2010; Zhang and Nivre, 2011; Mar-
tins et al., 2009; McDonald et al., 2005; Nivre et
al., 2006; Kübler et al., 2009; Goldberg and Nivre,

(a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN

Figure 1: Sketch of three recursive neural net-
works (RNN). (a) is the standard RNN for con-
stituent tree; (b) is Tree-GRNN for dependency
tree, in which the dashed arcs indicate the depen-
dency relations between the nodes; (c) is DAG-
GRNN for the nodes without given topological
structure.

2013; Choi and McCallum, 2013; Ballesteros and
Bohnet, 2014). However, these methods are based
on discrete features and suffer from the problems
of data sparsity and feature engineering (Chen and
Manning, 2014).

Recently, distributed representations have been
widely used in a variety of natural language pro-
cessing (NLP) tasks (Collobert et al., 2011; De-
vlin et al., 2014; Socher et al., 2013; Turian et al.,
2010; Mikolov et al., 2013b; Bengio et al., 2003).
Specific to the transition-based parsing, the neu-
ral network based methods have also been increas-
ingly focused on due to their ability to minimize
the efforts in feature engineering and the boosted
performance (Le and Zuidema, 2014; Stenetorp,
2013; Bansal et al., 2014; Chen and Manning,
2014; Zhu et al., 2015).

However, most of the existing neural network
based methods still need some efforts in feature
engineering. For example, most methods often se-
lect the first and second leftmost/rightmost chil-
dren of the top nodes in stack, which could miss
some valuable information hidden in the unchosen
nodes. Besides, the features of the selected nodes
are just simply concatenated and then fed into neu-
ral network. Since the concatenation operation is
relatively simple, it is difficult to model the com-

1879



plicated feature combinations which can be man-
ually designed in the traditional discrete feature
based methods.

To tackle these problems, we use two het-
erogeneous gated recursive neural networks, tree
structured gated recursive neural network (Tree-
GRNN) and directed acyclic graph gated struc-
tured recursive neural network (DAG-GRNN), to
model each configuration during transition based
dependency parsing. The two proposed GRNNs
introduce the gate mechanism (Chung et al., 2014)
to improve the standard recursive neural network
(RNN) (Socher et al., 2013; Socher et al., 2014),
and can model the syntactic and semantic compo-
sitions of the nodes during parsing.

Figure 1 gives a rough sketch for the standard
RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN
is applied to the partial-constructed trees in stack,
which have already been constructed according to
the previous transition actions. DAG-GRNN is ap-
plied to model the feature composition of nodes in
stack and buffer which have not been labeled their
dependency relations yet. Intuitively, Tree-GRNN
selects and merges features recursively from chil-
dren nodes into their parent according to their de-
pendency structures, while DAG-GRNN further
models the complicated combinations of extracted
features and explicitly exploits features in differ-
ent levels of granularity.

To evaluate our approach, we experiment on
two prevalent benchmark datasets: English Penn
Treebank 3 (PTB3) and Chinese Penn Treebank 5
(CTB5) datasets. Experiment results show the ef-
fectiveness of our proposed method. Compared to
the parser of Chen and Manning (2014), we re-
ceive 0.6% (UAS) and 0.9% (LAS) improvement
on PTB3 test set, while we receive 0.8% (UAS)
and 1.3% (LAS) improvement on CTB5 test set.

2 Neural Network Based Transition
Dependency Parsing

2.1 Transition Dependency Parsing

In this paper, we employ the arc-standard tran-
sition systems (Nivre, 2004) and examine only
greedy parsing for its efficiency. Figure 2 gives
an example of arc-standard transition dependency
parsing.

In transition-based dependency parsing, the
consecutive configurations of parsing process can
be defined as c(i) = (s(i), b(i), A(i)) which con-
sists of a stack s, a buffer b, and a set of

dependency arcs A. Then, the greedy pars-
ing process consecutively predicts the actions
based on the features extracted from the corre-
sponding configurations. For a given sentence
w1, . . . , wn, parsing process starts from a initial
configuration c(0) = ([ROOT ], [w1, . . . , wn], ∅),
and terminates at some configuration c(2n) =
([ROOT ], ∅, A(2n)], where n is the length of the
given sentence w1:n. As a result, we derive the
parse tree of the sentence w1:n according to the
arcs set A(2n).

In arc-standard system, there are three types of
actions: Left-Arc, Right-Arc and Shift. Denot-
ing sj(j = 1, 2, . . . ) as the jth top element of the
stack, and bj(j = 1, 2, . . . ) as the jth front ele-
ment of the buffer, we can formalize the three ac-
tions of arc-standard system as:

• Left-Arc(l) adds an arc s2 ← s1 with label
l and removes s2 from the stack, resulting a
new arc l(s1, s2). Precondition: |s| ≥ 3 (The
ROOT node cannot be child node).
• Right-Arc(l) adds an arc s2 → s1 with label
l and removes s1 from the stack, resulting a
new arc l(s2, s1). Precondition: |s| ≥ 2.
• Shift removes b1 from the buffer, and adds it

to the stack. Precondition: |b| ≥ 1.
The greedy parser aims to predict the correct

transition action for a given configuration. There
are two versions of parsing: unlabeled and labeled
versions. The set of possible action candidates
T = 2nl + 1 in the labeled version of parsing,
and T = 3 in the unlabeled version, where nl is
number of different types of arc labels.

2.2 Neural Network Based Parser
In neural network architecture, the words, POS
tags and arc labels are mapped into distributed
vectors (embeddings). Specifically, given the
word embedding matrix Ew ∈ Rde×nw , each word
wi is mapped into its corresponding column ewwi ∈
Rde of Ew according to its index in the dictionary,
where de is the dimensionality of embeddings and
nw is the dictionary size. Likewise, The POS and
arc labels are also mapped into embeddings by the
POS embedding matrix Et ∈ Rde×nt and arc la-
bel embedding matrix El ∈ Rde×nl respectively,
where nt and nl are the numbers of distinct POS
tags and arc labels respectively. Correspondingly,
embeddings of each POS tag ti and each arc label
li are etti ∈ Rde and elli ∈ Rde extracted from Et
and El respectively.

1880



Configurations Gold ActionsID Stack Buffer A
0 [ROOT] [He likes story books .] ∅
1 [ROOT He] [likes story books .] Shift
2 [ROOT He likes] [story books .] Shift
3 [ROOT likes] [story books .] A ∪ SUB(likes,He) Left Arc(SUB)
4 [ROOT likes story] [books .] Shift
5 [ROOT likes story books] [.] Shift
6 [ROOT likes books] [.] A∪NMOD(books,story) Left Arc(NMOD)
7 [ROOT likes] [.] A∪OBJ(likes,books) Right Arc(OBJ)
8 [ROOT likes .] ∅ Shift
9 [ROOT likes] ∅ A∪P(likes,.) Right Arc(P)

10 [ROOT] ∅ A∪ROOT(ROOT,likes) Right Arc(ROOT)

Figure 2: An example of arc-standard transition dependency parsing.

s3 s2 s1 b1 b2 b3

s2.lc1 s2.lc2 s2.rc2 s2.rc1… …

… …s2.rc2.lc2s2.rc2.lc1 s2.rc2.rc1s2.rc2.rc2

Stack Buffer

s2.lc1 s1 …… b2 s2.rc2.rc1s2.lc2.rc1

S L R

…

…

Concatenate

h=tanh(W1x+b1)

p=softmax(W2h+b2)

Input x

Hidden units h

Probability of each action p

Sub tree

Figure 3: Architecture of neural network based
transition dependency parsing.

Figure 3 gives the architecture of neu-
ral network based parser. Following
Chen and Manning (2014), a set of el-
ements S from stack and buffer (e.g.
S = {s2.lc2.rc1, s2.lc1, s1, b2, s2.rc2.rc1, . . . })
is chosen as input. Specifically, the information
(word, POS or label) of each element in the set S
(e.g. {s2.lc2.rc1.t, s2.lc1.l, s1.w, s1.t, b2.w, . . . })
are extracted and mapped into their corresponding
embeddings. Then these embeddings are concate-
nated as the input vector x ∈ Rd̂. A special token
NULL is used to represent a non-existent element.

We perform a standard neural network using
one hidden layer with dh hidden units followed by
a softmax layer as:

h = g(W1x + b1), (1)
p = softmax(W2h + b2), (2)

where W1 ∈ Rdh×d̂, b1 ∈ Rdh , W2 ∈ R|T |×dh ,
b2 ∈ R|T |.Here, g is a non-linear function which

can be hyperbolic tangent, sigmoid, cube (Chen
and Manning, 2014), etc.

3 Recursive Neural Network

Recursive neural network (RNN) is one of classi-
cal neural networks, which performs the same set
of parameters recursively on a given structure (e.g.
syntactic tree) in topological order (Pollack, 1990;
Socher et al., 2013).

In the simplest case, children nodes are com-
bined into their parent node using a weight matrix
W which is shared across the whole network, fol-
lowed by a non-linear function g(·). Specifically,
given the left child node vector hL ∈ Rd and right
child node vector hR ∈ Rd, their parent node vec-
tor hP ∈ Rd will be formalized as:

hP = g
(
W
[

hL
hR

])
, (3)

where W ∈ Rd×2d and g is a non-linear function
as mentioned above.

4 Architecture of Two Heterogeneous
Gated Recursive Neural Networks for
Transition-based Dependency Parsing

In this paper, we apply the idea of recursive neu-
ral network (RNN) to dependency parsing task.
RNN needs a pre-defined topological structure.
However, in each configuration during parsing,
just partial dependency relations have been con-
structed, while the remains are still unknown. Be-
sides, the standard RNN can just deal with the bi-
nary tree. Therefore we cannot apply the standard
RNN directly.

Here, we propose two heterogeneous recursive
neural networks: tree structured gated recursive
neural network (Tree-GRNN) and directed acyclic
graph structured gated recursive neural network

1881



s2.lc1 s2.lc2 s2.rc2 s2.rc1… …

s3 s2 s1 b1 b2 b3

…

S L RSoftmax

… …s2.rc2.lc2s2.rc2.lc1 s2.rc2.rc1s2.rc2.rc2

Stack Buffer

Sub tree

DAG-GRNN

Tree-GRNN

Figure 4: Architecture of our proposed depen-
dency parser using two heterogeneous gated recur-
sive neural networks.

(DAG-GRNN). Tree-GRNN is applied to the sub-
trees with partial dependency relations in stack
which have already been constructed according
to the previous transition actions. DAG-GRNN
is employed to model the feature composition of
nodes in stack and buffer which have not been la-
beled their dependency relations yet.

Figure 4 shows the whole architecture of our
model, which integrates two different GRNNs to
predict the action for each parsing configuration.
The detailed descriptions of two GRNNs will be
discussed in the following two subsections.

4.1 Tree Structured Gated Recursive Neural
Network

It is a natural way to merge the information from
children nodes into their parent node recursively
according to the given tree structures in stack. Al-
though the dependency relations have been built,
it is still hard to apply the recursive neural net-
work (as Eq. 3) directly for the uncertain num-
ber of children of each node in stack. By aver-
aging operation on children nodes (Socher et al.,
2014), the parent node cannot well capture the
crucial features from the mixed information of its
children nodes. Here, we propose tree structured
gated recursive neural network (Tree-GRNN) in-
corporating the gate mechanism (Cho et al., 2014;
Chung et al., 2014; Chen et al., 2015a; Chen
et al., 2015b), which can selectively choose the

AVG AVG

σ σσσ

Parent node: p

Child node: p.lc1

σ Reset gate Element-wise multiplication operator AVG Average operator

vp.lcl 1 v
p.lc
r

1vp.lcn 1

Child node: p.lc2

vp.lcl 2 v
p.lc
r

2vp.lcn 2

Child node: p.rc2 Child node: p.rc1

vp.rcr 2v
p.rc
l

2 vp.rcn 2 v
p.rc
l

1 vp.rcr 1v
p.rc
n

1

vpl v
p
n v

p
r

Figure 5: Minimal structure of tree structured
gated recursive neural network (Tree-GRNN). The
solid arrow denotes that there is a weight matrix on
the link, while the dashed one denotes none.

crucial features according to the gate state. Fig-
ure 5 shows the minimal structure of Tree-GRNN
model.

In Tree-GRNN, each node p of trees in stack is
composed of three components: state vector of left
children nodes vpl ∈ Rdc , state vector of current
node vpn ∈ Rdn and state vector of right children
nodes vpr ∈ Rdc , where dn and dc indicate the cor-
responding vector dimensionalities. Particularly,
we represent information of node p as a vector

vp =

 vplvpn
vpr

 , (4)
where vp ∈ Rq and q = 2dc+dn. Specifically, vpn
contains the information of current node including
its word form p.w, pos tag p.t and label type p.l
as shown in Eq. 5, and vpl and v

p
r are initialized

by zero vectors 0 ∈ Rdc , then update as Eq. 6.

vpn = tanh

 ewp.wetp.t
elp.l

 , (5)
where word embedding ewp.w ∈ Rde , pos embed-
ding etp.t ∈ Rde and label embedding elp.l ∈ Rde
are extracted from embedding matrices Ew, Et

and El according to the indices of the correspond-
ing word p.w, pos p.t and label p.l respectively.
Specifically, in the case of unlabeled attachment
parsing, we ignore the last term elp.l in Eq. 5.
Thus, the dimensionality dn of v

p
n varies. In la-

beled attachment parsing case, we set a special to-
ken NULL to represent label p.l if not available
(e.g. p is the node in stack or buffer).

By given node p and its left children nodes p.lci
and right children nodes p.rci, we update the left

1882



children information vpl and right children infor-
mation vpr as

vpl = tanh(Wl
1

NL(p)

∑
i

op.lci � vp.lci + bl),

vpr = tanh(Wr
1

NR(p)

∑
i

op.rci � vp.rci + br),
(6)

where op.lci and op.rci are the reset gates of the
nodes p.lci and p.rci respectively as shown in Eq.
7. In addition, functions NL(p) and NR(p) re-
sult the numbers of left and right children nodes
of node p respectively. The operator � indicates
element multiplication here. Wl ∈ Rdc×q and
Wr ∈ Rdc×q are weight matrices. bl ∈ Rdc and
br ∈ Rdc are bias terms.

The reset gates op.lci and op.rci can be formal-
ized as

op.lci = σ(Wo

[
vp.lci
vpn

]
+ bo),

op.rci = σ(Wo

[
vp.rci
vpn

]
+ bo),

(7)

where σ indicates the sigmoid function, Wo ∈
Rq×(q+dn) and bo ∈ Rq.

By the mechanism above, we can summarize
the whole information into the stack recursively
from children nodes to their parent using the
partial-built tree structure. Intuitively, the gate
mechanism can selectively choose the crucial fea-
tures of a child node according to the gate state
which is derived from the current child node and
its parent.

4.2 Directed Acyclic Graph Structured
Gated Recursive Neural Network

Previous neural based parsing works feed the ex-
tracted features into a standard neural network
with one hidden layer. Then, the hidden units are
fed into a softmax layer, outputting the probability
vector of available actions. Actually, it cannot well
model the complicated combinations of extracted
features. As for the nodes, whose dependency
relations are still unknown, we propose another
recursive neural network namely directed acyclic
graph structured gated recursive neural network
(DAG-GRNN) to better model the interactions of
features.

Intuitively, the DAG structure models the com-
binations of features by recursively mixing the in-
formation from the bottom layer to the top layer

σ σ

Parent node P

Child node L Child node R

New activation node P

Φ Φ Φ 

σ Reset gate
Element-wise multiplication operator

Φ Update  gate

Figure 6: Minimal structure of directed acyclic
graph structured gated recursive neural network
(DAG-GRNN). The solid arrow denotes that there
is a weight matrix on the link, while the dashed
one denotes none.

as shown in Figure 4. The concatenation opera-
tion can be regraded as a mix of features in differ-
ent levels of granularity. Each node in the directed
acyclic graph can be seen as a complicated feature
composition of its governed nodes.

Moreover, we also use the gate mechanism to
better model the feature combinations by introduc-
ing two kinds of gates, namely “reset gate” and
“update gate”. Intuitively, each node in the net-
work seems to preserve all the information of its
governed notes without gates, and the gate mech-
anism similarly plays a role of filter which de-
cides how to selectively exploit the information of
its children nodes, discovering and preserving the
crucial features.

DAG-GRNN structure consists of minimal
structures as shown in Figure 6. Vectors hP , hL,
hR and hP̂ ∈ Rq denote the value of the parent
node P , left child node L, right child node R and
new activation node P̂ respectively. The value of
parent node hP ∈ Rq is computed as:

hP = zP̂ � hP̂ + zL � hL + zR � hR, (8)

where zP̂ , zL and zR ∈ Rq are update gates for
new activation node P̂ , left child node L and right
child node R respectively. Operator � indicates
element-wise multiplication.

1883



The update gates z can be formalized as:

z =

 zP̂zL
zR

 ∝ exp(Wz
 hP̂hL

hR

), (9)
which are constrained by:

[zP̂ ]k + [zL]k + [zR]k = 1, 1 ≤ k ≤ q,
[zP̂ ]k ≥ 0, 1 ≤ k ≤ q,
[zL]k ≥ 0, 1 ≤ k ≤ q,
[zR]k ≥ 0, 1 ≤ k ≤ q,

(10)

where Wz ∈ R3q×3q is the coefficient of update
gates.

The value of new activation node hP̂ is com-
puted as:

hP̂ = tanh(WP̂

[
rL � hL
rR � hR

]
), (11)

where WP̂ ∈ Rq×2q, rL ∈ Rq, rR ∈ Rq. rL
and rR are the reset gates for left child node L
and right child node R respectively, which can be
formalized as:

r =
[

rL
rR

]
= σ(Wr

[
hL
hR

]
), (12)

where Wr ∈ R2q×2q is the coefficient of two reset
gates and σ indicates the sigmoid function.

Intuitively, the reset gates r partially read the
information from the left and right children, out-
putting a new activation node hP̂ , while the up-
date gates z selectively choosing the information
among the the new activation node P̂ , the left child
node L and the right child node R. This gate
mechanism is effective to model the combinations
of features.

Finally, we concatenate all the nodes in the
DAG-GRNN structure as input x of the architec-
ture described in Section 2.2, resulting the proba-
bility vector for all available actions.

4.3 Inference
We use greedy decoding in parsing. At each step,
we apply our two GRNNs on the current config-
uration to extract the features. After softmax op-
eration, we choose the feasible transition with the
highest possibility, and perform the chosen tran-
sition on the current configuration to get the next
configuration state.

In practice, we do not need calculate the Tree-
GRNN over the all trees in the stack on the current

configuration. Instead, we preserve the represen-
tations of trees in the stack. When we need apply a
new transition on the configuration, we update the
relative representations using Tree-GRNN.

5 Training

We use the maximum likelihood (ML) criterion to
train our model. By extracting training set (xi, yi)
from gold parse trees using a shortest stack oracle
which always prefers Left-Arc(l) or Right-Arc(l)
action over Shift, the goal of our model is to mini-
mize the loss function with the parameter set θ:

J(θ) = − 1
m

m∑
i=1

log p(yi|xi; θ)+ λ2m‖θ‖
2
2, (13)

where m is number of extracted training examples
which is as same as the number of all configura-
tions.

Following Socher et al. (2013), we use the diag-
onal variant of AdaGrad (Duchi et al., 2011) with
minibatch strategy to minimize the objective. We
also employ dropout strategy to avoid overfitting.

In practice, we perform DAG-GRNN with
two hidden layers, which gets the best perfor-
mance. We use the approximated gradient for
Tree-GRNN, which only performs gradient back
propagation on the first two layers.

6 Experiments

6.1 Datasets

To evaluate our proposed model, we experiment
on two prevalent datasets: English Penn Treebank
3 (PTB3) and Chinese Penn Treebank 5 (CTB5)
datasets.

• English For English Penn Treebank 3
(PTB3) dataset, we use sections 2-21 for
training, section 22 and section 23 as de-
velopment set and test set respectively. We
adopt CoNLL Syntactic Dependencies (CD)
(Johansson and Nugues, 2007) using the
LTH Constituent-to-Dependency Conversion
Tool.
• Chinese For Chinese Penn Treebank 5

(CTB5) dataset, we follow the same split as
described in (Zhang and Clark, 2008). De-
pendencies are converted by the Penn2Malt
tool with the head-finding rules of (Zhang
and Clark, 2008).

1884



Embedding size de = 50
Dimensionality of child node vector dc = 50
Initial learning rate α = 0.05
Regularization λ = 10−8

Dropout rate p = 20%

Table 1: Hyper-parameter settings.

6.2 Experimental Settings

For parameter initialization, we use random ini-
tialization within (-0.01, 0.01) for all parameters
except the word embedding matrix Ew. Specifi-
cally, we adopt pre-trained English word embed-
dings from (Collobert et al., 2011). And we pre-
train the Chinese word embeddings on a huge un-
labeled data, the Chinese Wikipedia corpus, with
word2vec toolkit (Mikolov et al., 2013a).

Table 1 gives the details of hyper-parameter set-
tings of our approach. In addition, we set mini-
batch size to 20. In all experiments, we only take
s1, s2, s3 nodes in stack and b1, b2, b3 nodes in
buffer into account. We also apply dropout strat-
egy here, and only dropout at the nodes in stack
and buffer with probability p = 20%.

6.3 Results

The experiment results on PTB3 and CTB5
datasets are list in Table 2 and 3 respectively. On
all datasets, we report unlabeled attachment scores
(UAS) and labeled attachment scores (LAS). Con-
ventionally, punctuations are excluded in all eval-
uation metrics.

To evaluate the effectiveness of our approach,
we compare our parsers with feature-based parser
and neural-based parser. For feature-based parser,
we compare our models with two prevalent
parsers: MaltParser (Nivre et al., 2006) and
MSTParser (McDonald and Pereira, 2006). For
neural-based parser, we compare our results with
parser of Chen and Manning (2014). Compared
with parser of Chen and Manning (2014), our
parser with two heterogeneous gated recursive
neural networks (Tree-GRNN+DAG-GRNN) re-
ceives 0.6% (UAS) and 0.9% (LAS) improvement
on PTB3 test set, and receives 0.8% (UAS) and
1.3% (LAS) improvement on CTB5 test set.

Since that speed of algorithm is not the focus of
our paper, we do not optimize the speed a lot. On
CTB (UAS), it takes about 2 days to train Tree-
GRNN+DAG-GRNN model with CPU only. The
testing speed is about 2.7 sentences per second.
All implementation is based on Python.

6.4 Effects of Gate Mechanisms

We adopt five different models: plain
parser, Tree-RNN parser, Tree-GRNN parser,
Tree-RNN+DAG-GRNN parser, and Tree-
GRNN+DAG-GRNN parser. The experiment
results show the effectiveness of our proposed two
heterogeneous gated recursive neural networks.

Specifically, plain parser is as same as parser
of Chen and Manning (2014). The difference
between them is that plain parser only takes the
nodes in stack and buffer into account, which
uses a simpler feature template than parser of
Chen and Manning (2014). As plain parser
omits all children nodes of trees in stack, it
performs poorly compared with parser of Chen
and Manning (2014). In addition, we find
plain parser outperforms MaltParser (standard) on
PTB3 dataset making about 1% progress, while
it performs poorer than MaltParser (standard) on
CTB5 dataset. It shows that the children nodes
of trees in stack is of great importance, especially
for Chinese. Moreover, it also shows the effective-
ness of neural network based model which could
represent complicated features as compacted em-
beddings. Tree-RNN parser additionally exploits
all the children nodes of trees in stack, which is
a simplified version of Tree-GRNN without incor-
porating the gate mechanism described in Section
4.1. In anther word, Tree-RNN omits the gate
terms op.lci and op.rci in Eq. 6. As we can see,
the results are significantly boosted by utilizing
the all information in stack, which again shows
the importance of children nodes of trees in stack.
Although the results of Tree-RNN are compara-
ble to results of Chen and Manning (2014), it not
outperforms parser of Chen and Manning (2014)
in all cases (e.g. UAS on CTB5), which implies
that exploiting all information without selection
might lead to incorporate noise features. More-
over, Tree-GRNN parser further boosts the perfor-
mance by incorporating the gate mechanism. In-
tuitively, Tree-RNN who exploits all the informa-
tion of stack without selection cannot well capture
the crucial features, while Tree-GRNN with gate
mechanism could selectively choose and preserve
the effective features by adapting the current gate
state.

We also experiment on parsers using two
heterogeneous gated recursive neural networks:
Tree-RNN+DAG-GRNN parser and Tree-
GRNN+DAG-GRNN parser. The similarity of

1885



models Dev TestUAS LAS UAS LAS
Malt:standard 90.0 88.8 89.9 88.5

Malt:eager 90.1 88.9 90.1 88.7
MSTParser 92.1 90.8 92.0 90.5

Chen’s Parser 92.2 91.0 92.0 90.7
Plain 91.1 90.0 91.2 89.7

Tree-RNN 92.4 91.0 92.1 90.8
Tree-GRNN 92.6 91.1 92.4 91.0

Tree-RNN+DAG-GRNN 92.8 91.9 92.4 91.5
Tree-GRNN+DAG-GRNN 92.6 91.9 92.6 91.6

Table 2: Performance of different models on PTB3
dataset. UAS: unlabeled attachment score. LAS:
labeled attachment score.

models Dev TestUAS LAS UAS LAS
Malt:standard 82.4 80.5 82.4 80.6

Malt:eager 91.2 79.3 80.2 78.4
MSTParser 84.0 82.1 83.0 81.2

Chen’s Parser 84.0 82.4 83.9 82.4
Plain 81.6 79.3 81.1 78.8

Tree-RNN 83.5 82.5 83.8 82.7
Tree-GRNN 84.2 82.5 84.3 83.1

Tree-RNN+DAG-GRNN 84.5 83.3 84.5 83.1
Tree-GRNN+DAG-GRNN 84.6 83.6 84.7 83.7

Table 3: Performance of different models on
CTB5 dataset. UAS: unlabeled attachment score.
LAS: labeled attachment score.

two parsers is that they all employ the DAG
structured recursive neural network with gate
mechanism to model the combination of features
extracted from stack and buffer. The difference
between them is the former one employs the
Tree-RNN without gate mechanism to model the
features of stack, while the later one employs the
gated version (Tree-GRNN). Again, the perfor-
mance of these two parsers is further boosted,
which shows DAG-GRNN can well model the
combinations of features which is summarized by
Tree-(G)RNN structure. In addition, we find the
performance does not drop a lot in almost cases by
turning off the gate mechanism of Tree-GRNN,
which implies that the DAG-GRNN can help
selecting the information from trees in stack, even
it has not been selected by gate mechanism of
Tree-GRNN yet.

6.5 Convergency Speed

To further analyze the convergency speed of our
approach, we compare the UAS results on devel-
opment sets of two datasets for first ten epoches
as shown in Figure 7 and 8. As plain parser
only take the nodes in stack and buffer into ac-

2 4 6 8 10
0.86

0.88

0.9

0.92

epoches

U
A

S(
%

)

Plain

Tree-RNN

Tree-GRNN

Tree-RNN+DAG-GRNN

Tree-GRNN+DAG-GRNN

Figure 7: Performance of different models on
PTB3 development set. UAS: unlabeled attach-
ment score.

2 4 6 8 10
0.74

0.76

0.78

0.8

0.82

0.84

epoches

U
A

S(
%

)

Plain

Tree-RNN

Tree-GRNN

Tree-RNN+DAG-GRNN

Tree-GRNN+DAG-GRNN

Figure 8: Performance of different models on
CTB5 development set. UAS: unlabeled attach-
ment score.

count, the performance is much poorer than the
rest parsers. Moreover, Tree-GRNN converges
slower than Tree-RNN, which shows that it might
be more difficult to learn this gate mechanism. By
introducing the DAG-GRNN, both Tree-RNN and
Tree-GRNN parsers become faster to converge,
which shows that the DAG-GRNN is of great help
in boosting the convergency speed.

7 Related Work

Many neural network based methods have been
used for transition based dependency parsing.

Chen et al. (2014) and Bansal et al. (2014) used
the dense vectors (embeddings) to represent words
or features and found these representations are

1886



complementary to the traditional discrete feature
representation. However, these two methods only
focus on the dense representations (embeddings)
of words or features.

Stenetorp (2013) first used RNN for transition
based dependency parsing. He followed the stan-
dard RNN and used the binary combination to
model the representation of two linked words. But
his model does not achieve the performance of the
traditional method.

Le and Zuidema (2014) proposed a genera-
tive re-ranking model with Inside-Outside Recur-
sive Neural Network (IORNN), which can pro-
cess trees both bottom-up and top-down. How-
ever, IORNN works in generative way and just es-
timates the probability of a given tree, so IORNN
cannot fully utilize the incorrect trees in k-best
candidate results. Besides, IORNN treats depen-
dency tree as a sequence, which can be regarded
as a generalization of simple recurrent neural net-
work (SRNN) (Elman, 1990).

Although the two methods also used RNN, they
just deal with the binary combination, which is un-
natural for dependency tree.

Zhu et al. (2015) proposed a recursive convolu-
tional neural network (RCNN) architecture to cap-
ture syntactic and compositional-semantic repre-
sentations of phrases and words in a dependency
tree. Different with the original recursive neu-
ral network, they introduced the convolution and
pooling layers, which can model a variety of com-
positions by the feature maps and choose the most
informative compositions by the pooling layers.

Chen and Manning (2014) improved the
transition-based dependency parsing by represent-
ing all words, POS tags and arc labels as dense
vectors, and modeled their interactions with neu-
ral network to make predictions of actions. Their
method only relies on dense features, and is not
able to automatically learn the most useful feature
conjunctions to predict the transition action.

Compared with (Chen and Manning, 2014), our
method can fully exploit the information of all the
descendants of a node in stack with Tree-GRNN.
Then DAG-GRNN automatically learns the com-
plicated combination of all the features, while the
traditional discrete feature based methods need
manually design them.

Dyer et al. (2015) improved the transition-based
dependency parsing using stack long short term
memory neural network and received significant

improvement on performance. They focused on
exploiting the long distance dependencies and in-
formation, while we aims to automatically model
the complicated feature combination.

8 Conclusion

In this paper, we pay attention to the syntac-
tic and semantic composition of the dense fea-
tures for transition-based dependency parsing. We
propose two heterogeneous gated recursive neu-
ral networks, Tree-GRNN and DAG-GRNN. Each
hidden neuron in two proposed GRNNs can be re-
garded as a different combination of the input fea-
tures. Thus, the whole model has an ability to sim-
ulate the design of the sophisticated feature com-
binations in the traditional discrete feature based
methods.

Although the two proposed GRNNs are only
used for the greedy parsing based on arc-standard
transition system in this paper, it is easy to gen-
eralize them to other transition systems and graph
based parsing. In future work, we would also like
to extend our GRNNs for the other NLP tasks.

Acknowledgments

We would like to thank the anonymous review-
ers for their valuable comments. This work was
partially funded by the National Natural Science
Foundation of China (61472088, 61473092), Na-
tional High Technology Research and Develop-
ment Program of China (2015AA015408), Shang-
hai Science and Technology Development Funds
(14ZR1403200).

References
Miguel Ballesteros and Bernd Bohnet. 2014. Au-

tomatic feature selection for agenda-based depen-
dency parsing. In Proceedings of the 25th Interna-
tional Conference on Computational Linguistics.

Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-

1887



putational Linguistics, pages 89–97. Association for
Computational Linguistics.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.

Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature embedding for dependency parsing. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 816–826, Dublin, Ireland, August.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
Chinese word segmentation. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Shiyu Wu, and
Xuanjing Huang. 2015b. Sentence modeling with
gated recursive neural network. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of EMNLP.

Jinho D Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In ACL (1), pages 1052–1062.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, Baltimore, MD, USA, June.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. arXiv preprint arXiv:1505.08075.

Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.

Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
TACL, 1:403–414.

He He, Hal Daumé III, and Jason Eisner. 2013. Dy-
namic feature selection for dependency parsing. In
EMNLP, pages 1455–1464.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086.

Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In 16th Nordic Conference of Computational
Linguistics, pages 105–112. University of Tartu.

Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL.

Sandra Kübler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on
Human Language Technologies, 1(1):1–127.

Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 729–739, Doha, Qatar,
October. Association for Computational Linguistics.

André FT Martins, Noah A Smith, and Eric P Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 342–350. Association for
Computational Linguistics.

Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’05, pages 91–98.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, vol-
ume 6, pages 2216–2219.

1888



Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57. Association
for Computational Linguistics.

Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77–105.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.

Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.

Pontus Stenetorp. 2013. Transition-based dependency
parsing using recursive neural networks. In NIPS
Workshop on Deep Learning.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.

Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562–571.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188–193.

Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing
Huang. 2015. A re-ranking model for dependency
parser with recursive convolutional neural network.
In Proceedings of Annual Meeting of the Association
for Computational Linguistics.

1889


