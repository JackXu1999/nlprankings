



















































A Joint Model for Chinese Microblog Sentiment Analysis


Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 61–67,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing

A Joint Model for Chinese Microblog Sentiment Analysis

Yuhui Cao, Zhao Chen, Ruifeng Xu∗, Tao Chen and Lin Gui
Shenzhen Engineering Laboratory of Performance Robots at Digital Stage,

Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China
caoyuhuiszu@gmail.com xuruifeng@hitsz.edu.cn

Abstract
Topic-based sentiment analysis for Chi-
nese microblog aims to identify the
user attitude on specified topics. In
this paper, we propose a joint model
by incorporating Support Vector Ma-
chines (SVM) and deep neural network
to improve the performance of senti-
ment analysis. Firstly, a SVM Clas-
sifier is constructed using N-gram, N-
POS and sentiment lexicons features.
Meanwhile, a convolutional neural net-
work is applied to learn paragraph rep-
resentation features as the input of an-
other SVM classifier. The classification
results outputted by these two classi-
fiers are merged as the final classifica-
tion results. The evaluations on the
SIGHAN-8 Topic-based Chinese mi-
croblog sentiment analysis task show
that our proposed approach achieves
the second rank on micro average F1
and the fourth rank on macro average
F1 among a total of 13 submitted sys-
tems.

1 Introduction
With the development of the Internet, mi-

croblog has become a popular user-generated
content platform where users share the newest
events or their personal feelings with each
other. Topic-based microblogs are the most
common interactive way for users to share
their opinions towards a specified topic. To
identify the opinions of users, sentiment anal-
ysis techniques are investigated to classify
texts into different categorizations according
to their sentiment polarities.

Most existing sentiment classification tech-
niques are based on machine learning al-
gorithms, such as Support Vector Machine,

Naïve Bayes and Maximum Entropy. The
machine learning based approach uses feature
vectors as the input of classification to pre-
dict the classification results. Thus, feature
engineering, a method for extracting effective
features from texts, plays an important role.
Some commonly used features in sentiment
classification are unigram, bigram and senti-
ment words. However, these features cannot
work well for cross-domain sentiment classifi-
cation because of the lack of domain knowl-
edge.

Danushka Bollegala et al. (2011) used mul-
tiple sources to construct a sentiment sensi-
tive thesaurus to overcome the lack of domain
knowledge. New sentiment words expansion is
another kind of approach to improve the per-
formance of sentiment analysis. Strfano Bac-
cianella et al. (2010) constructed SentiWord-
Net by extending WordNet with sentiment in-
formation. It is now widely used in sentiment
classification for English. As for Chinese senti-
ment analysis, Minlie Huang et al. (2014) pro-
posed a new word detection method by mining
the frequent sentiment word patterns. This
method may discover new sentiment words
from a large scale of unlabeled texts.

With the rapid development of pre-trained
word embedding and deep neural networks,
a new way to represent texts and features
is devloped. Mikolov et al. (2013) showed
that word embedding represents words with
meaningful syntactic and semantic informa-
tion effectively. Recursive neural network pro-
posed by Socher et al. (2011a; 2011b; 2013) is
shown efficient to construct sentence represen-
tations based on the word embedding. Con-
volutional neural networks (CNN), another
deep learn model which achieved success in
image recognition field, was applied to na-
ture language processing with word embed-

61



dings. Yoon Kim (2014) used CNN with pre-
trained word embedding to achieve state-of-
the-art performances on some sentence classi-
fication tasks, including sentiment classifica-
tion. Siwei Lai et al. (2015) incorporated
global information in a recurrent convolutional
neural network. It obtained further improve-
ments comparing to other deep learning mod-
els.

In this paper, we propose a joint model
which incorporates traditional machine learn-
ing based method (SVM) and deep learning
model. Two different classifiers are devel-
oped. One is a word feature based SVM clas-
sifier which uses word unigram, bigram and
sentiment words as features. Another one
is a CNN-based SVM classifier which takes
paragraph representations features learned by
CNN as input features. The classification re-
sults of these two classifiers are integrated to
generate the final classification results. The
evaluations on the SIGHAN-8 Topic-based
Chinese microblog sentiment analysis task
show that our proposed approach achieves the
second rank on micro average F1 and the
fourth rank on macro average F1 among a to-
tal of 13 submitted systems. Furthermore, the
joint classifier strategy brings further perfor-
mance improvement on individual classifiers.

The rest of this paper is organized as follows.
Section 2 presents the design and implemen-
tation of our proposed joint model. Section
3 gives the evaluation results and discussions.
Finally, Section 4 gives the conclusion and fu-
ture research directions.

2 Our Approach

The SIGHAN8 topic- based Chinese polarity
classification task aims to is to classify Chinese
microblog into three topic-related sentiment
classes, namely neutral, positive and nega-
tive. This task may be generally regarded as
a three-category classification problem. The
SVM classifier which has been shown effective
to document classification is adopted as the
core classifier. Here, two different feature rep-
resentation models, namely word-based vector
space model and CNN-based composition rep-
resentation, are adopted to generate the clas-
sification features for two classifiers, respec-
tively. The classification outputs of two clas-

sifiers are integrated to generate the final out-
put.

2.1 Data preprocessing
Chinese microblog text is obviously differ-

ent from formal text. Many microblogs have
noises, including nickname, hashtag, repost
or reply symbols, and URL. Therefore, be-
fore the feature representation and extraction,
preprocessing is performed to filter out noise
text in the microblogs. Meanwhile, the ad-
vertising text and topic-irrelevant microblog
are identified as neutral text. Especially, this
task is designed to identify the topic-relevant
sentiments. Therefore, the information com-
ing from the reply, repost and sharing parts
should be filtered out to avoid their influences
to the sentiment analysis of the microblog au-
thor. Generally speaking, such filtering is
based on rules. The table 1 shows the example
data preprocessing rules with illustrations.

Table 2 shows the rules for identifying the
advertisement and topic-irrelevant microblogs.
The identified microblogs are labeled as neu-
tral for topic-based sentiment classification.

2.2 Word feature based classifier
The word feature based classifier is de-

signed based on the vector model. Firstly,
the new sentiment words from unlabeled sen-
tences data are recognized to expand the sen-
timent lexicon. The classification features are
extracted from the labeled training data and
sentiment lexicon resources. In order to al-
leviate the influences of unbalanced training
data, SMOTE, which is an oversampling algo-
rithm, is applied to training data before clas-
sifier training. Finally, a SVM classifier is
trained on the balanced data. The framework
of word feature based classifier is shown in Fig-
ure 1.

2.2.1 Feature selection
Unigram, Bigram, Uni-Part-of-Speech and

Bi-Part-of-Speech features are selected as the
basic features. CHI-test based feature selec-
tion is applied to obtain the top 20000 fea-
tures. To improve the performance of senti-
ment classification, additional features based
on lexicons including sentiment word lexicons,
negation word lexicons, and adverb word lex-
icons, are incorporated.

62



Rules Raw Text Processed Text
Sharing news with 好看？吗？//【Galaxy S6：三星证明自 好看？吗？
personal comments 己能做出好看的手机】http://t.cn/

RwHRsIb(分享自 @ 今日头条)
Removing HashTag # 三星 Galaxy S6# 三星 GALAXY S6 三星 GALAXY S6，

，挺中意 [酷][酷] [位置] 芒砀路 挺中意 [酷][酷]
Removing URL 699 欧元起传三星 Galaxy S6/S6 Edge 售 699 欧元起传三星 Galaxy

价获证实（分享自 @ 新浪科技） S6/S6 Edge 售价获证实
http://t.cn/RwTo3on （分享自 @ 新浪科技）

Removing nickname 玻璃取代塑料，更美 Galaxy S6 的 5 大 http://t.cn/RwHY6Az
妥协 http://t.cn/RwHY6Az 罗永浩我去 罗永浩我去小米和三星这
小米和三星这是要闹哪样，，，老罗。。不 是要闹哪样，，，老罗。。
能忍啊，，，，，@ 锤子科技营销帐号 @ 罗 不能忍啊，，，，，
永浩

Removing 【视频：三星 S6 对比苹果 iPhone6 【视频：三星 S6 对比苹果
information sources MWC2015 @youtube 科技 】 iPhone6 MWC2015

http://t.cn/RwHQzJ8（来自于优酷安 @youtube 科技 】
卓客户端） http://t.cn/RwHQzJ8

Table 1: Data preprocessing rules with illustrations.

Figure 1: Framework of word feature based
classifier

Rules Type
Including many different Advertisement
topic (“#...#”) tag.
Including many words Advertisement
like “微商”, “商机”,
“想赚钱”,“面膜”.
No actual content Topic-irrelevant

Table 2: Microblog text matching rules.

By analyzing the expressions of the mi-
croblog text in training data, some special ex-
pression features in microblog text are iden-
tified. For example, the continuous punctua-
tions are always used to express a strong feel-
ing and thus, the microblog with continuous
punctuations tends to be subjective. Another
adopted feature for microblog text is the use
of emoticons.

2.2.2 Sentiment lexicon expansion
In microblogs, abundant new or informal

sentiment words are widely used. Normally,
these new sentiment words are short but
meaningful for expressing a strong feeling.
These new sentiment words play an important
role in Chinese microblog sentiment classifica-
tion. Therefore, sentiment word identification
is performed to recognize new sentiment words
as the supplement of sentiment lexicon.

63



Twenty million microblog text collected
from Sina Weibo Platform are used in new
sentiment word detection. Considering that
new words normally cannot be correctly seg-
mented by the existing segmentor, identifying
new words from preliminary segmentation re-
sults together with their POS tags is a feasible
method. Here, potential components for new
words are limited to the segmentation tokens
shorter than three. Using word frequency,
mutual information and context entropy as
the evaluation indicators for words, the most
possible new word candidates are obtained.
With the help of word embedding construction
model, each word in the corpus can be repre-
sented as a low dimension vector together with
its context information. Hence, the distances
between the new words and the existed senti-
ment words corresponding to difference senti-
ment polarity are estimated. The new words
are then classified into one of the three polar-
ity classes by following voting mechanism.

2.2.3 Classification
Two steps are performed to determine the

topic-relevant sentiment for input microblogs.
The first step is to distinguish topic relevant
messages from topic irrelevant messages. Sen-
timent classification is then applied to topic
relevant messages in the second step.

Topic relevant words generated by clus-
tering analysis are employed as distinguish-
able features to filter out topic irrelevant mi-
croblogs because normally the topic irrelevant
microblogs have few intersections with topic
relevant words. Some advertisement posts
consisting of several hot topic hash tags are
also filtered out by considering the number of
hash tag types in the microblog.

The provided labeled dataset is used to train
the SVM classifier with linear kernel. A new
challenge is that the provided training set is
imbalanced. There are about 3973 neutral mi-
croblogs, while the numbers of positive and
negative microblogs are 394 and 538, respec-
tively. In order to reduce the influences of im-
balanced training dataset, the SMOTE algo-
rithm (Chawla et al., 2002) is applied to over-
sampling the samples on minority class. Over-
sampling ratio is set to 10 and 7.4 for positive
class and negative class, respectively. In this
way, the training dataset becomes balanced.

2.3 CNN-based SVM classifier

Figure 2: CNN and SVM joint classifier.

Another classifier is CNN-based SVM clas-
sifier. The classifier framework is shown in
Figure 2. Firstly, continuous bog of word
(CBOW) model (Mikolov et al., 2013) is used
to learn word embeddings from Chinese mi-
croblog text. A deep convolutional neural net-
works (CNN) model is applied to learn dis-
tributed paragraph representation features for
Chinese microblog training and testing data.
Finally, the distributed paragraph representa-
tion features are used in SVM classifier to learn
the probability distribution over sentiment la-
bels.

2.3.1 Word embedding construction
Word embedding, wherein words are pro-

jected from a sparse, 1-of-V encoding (here
V is the vocabulary size) onto a lower di-
mensional vector space via a hidden layer, are
essentially feature extractors that encode se-
mantic features of words in their dimensions.
Mikolov et al. (2013) introduced CBOW
model to learn vector representations which
captures a large number of syntactic and se-
mantic word relationships from unstructured
text data. The main idea of this model is to
find word representations which use the sur-
rounding words in a sentence or a document
to predict current word.

64



In this study, we train the CBOW model by
using 16GB Chinese microblog text. Finally,
we obtain 200-dimension word embeddings for
Chinese microblog text.

2.3.2 CNN-based SVM classifier
In the CNN-based SVM classifier, the input

is a matrix which is composed of the word em-
beddings of microblogs. There are windows
with the lengths of three, four and five words,
respectively. A convolution operation involves
three filters which are applied to these win-
dows to produce new features. After convolu-
tion operation, a max-over-time pooling oper-
ation is applied over these features. The maxi-
mum value is taken as the feature correspond-
ing to this particular filter. The idea is to cap-
ture the most important feature which has the
largest value. Since one feature is extracted
from one filter, the model uses multiple filters
(with varying window sizes) to obtain mul-
tiple features. These features constitute the
distributed paragraph feature representation.
In the last step, a SVM classifier is applied
on these distributed paragraph representation
features to obtain the probability distributions
over labels (positive, negative, and neutral).

2.4 Outputs Merging

Classifier 1 Classifier 2 Final result
positive neutral neutral
negative neutral neutral
neutral positive neutral
neutral negative neutral
positive negative negative
negative positive positive

Table 3: Merging rules for two classifiers.

A set of merging rules is designed to incor-
porate the individual classification results of
the two classifiers for generating the final re-
sult. If the two classification outputs are the
same, naturally, the final output is the same.
If the two classification outputs are different,
the final result is determined from the merge
rules shown in Table 3. Simply speaking, if any
of two classifiers output neutral category, the
final output is neutral. If two classifiers out-
puts positive and negative, respectively, the
final output is the result of CNN-based clas-

sifier. Such a classification outputs merging
strategy is based on the statistical analysis on
the individual classifier performances on train-
ing dataset.

3 Experimental results and analysis

3.1 Data set
In the SIGHAN-8 Chinese sentiment analy-

sis bakeoff dataset, 4905 topic-based Chinese
microblog are provided as training data which
consists of 394 positive, 538 negative and 3973
neutral microblogs corresponding to 5 topics,
namely“央行降息”,“油价”,“日本马桶”,
“三星 S6”and “雾霾”. In the testing data,
there are 19,469 microblogs corresponding to
20 topic, such as “12306 验证码”, “中国政
府也门撤侨”,“何以笙箫默”,“刘翔退役”.

3.2 Metrics
Precision, recall and F1-value are used as

the evaluation metrics, as shown below:

Precision =
SystemCorrect

SystemOutput
(1)

Recall =
SystemCorrect

HumanLabeled
(2)

F1 =
2× Precision×Recall

Precision + Recall
(3)

Where System.Output refers to the
total number of the submitted results,
System.Correct refers to the number of
correctly classified results in the submitted
results, Human.Labeled refers to the total
number of manually labeled results in the
Gold Standard.

The evaluation metrics corresponding to
positive, negative and overall are estimated,
respectively. The corresponding micro-
average and macro-average performances are
then estimated. The micro-average estimates
the average performance of the three evalu-
ation metrics over the entire dataset. The
macro-average estimates the average perfor-
mances of the evaluation metrics on positive,
negative and neutral, respectively.

3.3 Experimental results and analysis
There are two subtasks in SIGHAN-8 topic-

based Chinese microblog polarity classification

65



All Positive Negative
Team Name Precision Recall F1 Precision Recall F1 Precision Recall F1
TICS-dm 0.83 0.83 0.83 0.62 0.51 0.56 0.82 0.46 0.59
NEUDM2 0.74 0.74 0.74 0.31 0.08 0.13 0.44 0.08 0.13

LCYS_TEAM 0.72 0.64 0.68 0.26 0.05 0.09 0.40 0.10 0.16
HLT_HITSZ 0.68 0.68 0.68 0.21 0.40 0.28 0.45 0.60 0.52

Table 4: Performances in restricted resource subtask.

All Positive Negative
Team Name Precision Recall F1 Precision Recall F1 Precision Recall F1
TICS-dm 0.85 0.85 0.85 0.58 0.62 0.60 0.79 0.61 0.69

xk0 0.74 0.74 0.74 0.19 0.01 0.03 0.40 0.05 0.09
NEUDM1 0.74 0.74 0.74 0.26 0.11 0.16 0.46 0.33 0.38

HLT_HITSZ 0.71 0.71 0.71 0.24 0.41 0.30 0.51 0.54 0.53

Table 5: Performances in unrestricted resource subtask.

All Positive Negative
Approach Precision Recall F1 Precision Recall F1 Precision Recall F1
Classifier 1 0.67 0.67 0.67 0.20 0.42 0.27 0.44 0.49 0.46
Classifier 2 0.60 0.60 0.60 0.18 0.61 0.28 0.42 0.67 0.52
Merging 0.71 0.71 0.71 0.24 0.41 0.30 0.51 0.54 0.53

Table 6: Performances by different classifiers in unrestricted resource subtask.

task: restricted resource and unrestricted re-
source subtasks.

Table 4 gives the performances in restricted
resource subtask. The first column lists
the name of participants who achieves higher
macro average F1 values while out system is
named as HLT_HITSZ. It is observed that
our proposed approach achieves better perfor-
mance on negative and positive categories, but
obviously lower performance on neutral cat-
egory. The good performance on the recall
of minority classes showed the effectiveness of
our consideration on imbalanced dataset train-
ing.

The achieved performances in the unre-
stricted resource subtask are listed in Table 5.
Our system achieves about 3% of performance
improvement on each category, respectively.
It shows the contributions of extra training
corpus and merging rules.

In order to validate the effectiveness of
merging rules, the performances of Classifier
1 and Classifier 2 are evaluated, individually.
The achieved performances are given in Ta-
ble 6. It is observed that generally speaking,

Classifier 1 achieves a higher classification pre-
cision because many features are coming from
manually compiled sentiment-related lexicons.
However, these features are limited to training
data so that Classifier 1 achieved a lower re-
call. On the contrary, Classifier 2 may learn
the representation features automatically from
training data which is better for generaliza-
tion. Thus, a good recall is achieved. Mean-
while, the achieved performances show that
our joint model obtains better performances
compared to two individual classifiers which
indicate the effectiveness of our proposed joint
classification strategy.

4 Conclusion

In this work, we propose a joint model for
sentiment topic analysis on Chinese microblog
messages. A word feature based SVM classifier
and a SVM classifier using CNN-based para-
graph representation features are developed,
respectively. To overcome the limitation of
each classifier, their classification outputs are
merged to generate the final output while the
merging rules are based on statistical analy-

66



sis on the performances on training dataset.
Experimental results show that our proposed
joint method achieves better sentiment classi-
fication performance over individual classifiers
which show the effectiveness of the joint clas-
sifier strategy. In future, we intend to study
the way to distinguish the subjective messages
from objective messages for further improving
the sentiment classification performance.

Acknowledgements
This work is supported by the Na-

tional Natural Science Foundation of China
(No.61370165,61203378), National 863 Pro-
gram of China 2015AA015405, the Natural
Science Foundation of Guangdong Province
(No.S2013010014475), Shenzhen Development
and Reform Commission Grant No.[2014]1507,
Shenzhen Peacock Plan Research Grant
KQCX20140521144507925 and Baidu Collab-
orate Research Funding.

References
Stefano Baccianella, Andrea Esuli, and Fabrizio

Sebastiani. 2010. Sentiwordnet 3.0: An en-
hanced lexical resource for sentiment analysis
and opinion mining. In Proceedings of the In-
ternational Conference on Language Resources
and Evaluation (LREC).

Danushka Bollegala, David Weir, and John Car-
roll. 2011. Using multiple sources to construct
a sentiment sensitive thesaurus for cross-domain
sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 132–
141. Association for Computational Linguistics.

Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002.
Smote: synthetic minority over-sampling tech-
nique. Journal of Artificial Intelligence Re-
search, 16:321–357.

Minlie Huang, Borui Ye, Yichen Wang, Haiqiang
Chen, Junjun Cheng, and Xiaoyan Zhu. 2014.
New word detection for sentiment analysis. In
Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 531–541, Baltimore, Maryland,
June. Association for Computational Linguis-
tics.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1746–
1751, October.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao.
2015. Recurrent convolutional neural net-
works for text classification. In Proceedings of
the Twenty-Ninth AAAI Conference on Artifi-
cial Intelligence, January 25-30, 2015, Austin,
Texas, USA., pages 2267–2273.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. In Proceedings
of Workshop at the International Conference on
Learning Representations (ICLR).

Richard Socher, Eric H. Huang, Jeffrey Penning-
ton, Andrew Y. Ng, and Christopher D. Man-
ning. 2011a. Dynamic pooling and unfolding
recursive autoencoders for paraphrase detection.
In Advances in Neural Information Processing
Systems 24: 25th Annual Conference on Neural
Information Processing Systems (NIPS), pages
801–809.

Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning.
2011b. Semi-supervised recursive autoencoders
for predicting sentiment distributions. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 151–161. Association for Computational
Linguistics.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep
models for semantic compositionality over a sen-
timent treebank. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 1631–1642. Associ-
ation for Computational Linguistics.

67


