



















































A Consolidated Open Knowledge Representation for Multiple Texts


Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 12–24,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

A Consolidated Open Knowledge Representation for Multiple Texts
Rachel Wities∗ 1, Vered Shwartz1, Gabriel Stanovsky1, Meni Adler1, Ori Shapira1,
Shyam Upadhyay2, Dan Roth2, Eugenio Martinez Camara3, Iryna Gurevych3,4 and

Ido Dagan1

1Bar-Ilan University, Ramat-Gan, Israel
2University of Illinois at Urbana-Champaign, IL, USA

3Ubiquitous Knowledge Processing Lab (UKP), Technische Universitat Darmstadt
4Ubiquitous Knowledge Processing Lab (UKP-DIPF),

German Institute for Educational Research
{rachelvov,vered1986,gabriel.satanovsky,meni.adler,obspp18}@gmail.com

{upadhya3,danr}@illinois.edu, {camara,gurevych}@ukp.informatik.tu-darmstadt.de
dagan@cs.biu.ac.il

Abstract

We propose to move from Open Infor-
mation Extraction (OIE) ahead to Open
Knowledge Representation (OKR), aim-
ing to represent information conveyed
jointly in a set of texts in an open text-
based manner. We do so by consolidating
OIE extractions using entity and predicate
coreference, while modeling information
containment between coreferring elements
via lexical entailment. We suggest that
generating OKR structures can be a useful
step in the NLP pipeline, to give seman-
tic applications an easy handle on consoli-
dated information across multiple texts.

1 Introduction

Natural language understanding involves identify-
ing, classifying, and integrating information about
events and other propositions mentioned in text.
While much effort has been invested in generic
methods for analyzing single sentences and de-
tecting the propositions they contain, little thought
and effort has been put into the integration step:
how to systematically consolidate and represent
information contributed by propositions originat-
ing from multiple texts. Consolidating such in-
formation, which is typically both complemen-
tary and partly overlapping, is needed to con-
struct multi-document summaries, to combine ev-
idence when answering questions that cannot be
answered based on a single sentence, and to pop-
ulate a knowledge base while relying on multiple
pieces of evidence (see Figure 1 for a motivating

∗Corresponding author

example). Yet, the burden of integrating informa-
tion across multiple texts is currently delegated to
downstream applications, leading to various par-
tial solutions in different application domains.

This paper suggests that a common consolida-
tion step and a corresponding knowledge represen-
tation should be part of the “standard” semantic
processing pipeline, to be shared by downstream
applications. Specifically, we pursue an Open
Knowledge Representation (OKR) framework that
captures the information expressed jointly in mul-
tiple texts while relying solely on the termi-
nology appearing in those texts, without requir-
ing pre-defined external knowledge resources or
schemata.

As we focus in this work on investigating an
open representation paradigm, our proposal fol-
lows and extends the Open Information Extrac-
tion (OIE) approach. We do that by first extract-
ing textual predicate-argument tuples, each cor-
responding to an individual proposition mention.
We then merge these mentions by accounting for
proposition coreference, an extended notion of
event coreference. This process yields consoli-
dated propositions, each corresponding to a single
fact, or assertion, in the described scenario. Simi-
larly, entity coreference links are used to establish
reference to real-world entities. Taken together,
our proposed representation encodes information
about events and entities in the real world, simi-
larly to what is expected from structured knowl-
edge representations. Yet, being an open text-
based representation, we record the various lexical
terms used to describe the scenario. Further, we
model information redundancy and containment
among these terms through lexical entailment.

In this paper we specify our proposed represen-

12



1. Turkey forces down Syrian plane. 4. Turkish PM says plane was carrying ammunition for Syria government.
2. Damascus sends note to Ankara over Syrian plane. 5. Last night Turkish F16s grounded a Syrian passenger jet.
3. Turkey Escalates Confrontation with Syria. 6. Russia angry at Turkey about Russian passengers.

Figure 1: A sample of news headlines, illustrating the need for information consolidation. Two mentions
of the same proposition, for which event coreference holds, are highlighted, with the predicate in bold and
the arguments underlined. Some information is redundant, but may be described at different granularity
levels; for example, different mentions describe the interception target as a plane and as a jet, where jet
entails plane and is accordingly more informative.

tation, while specifying the involved annotation
sub-tasks from which our structures are composed.
We then describe our annotated dataset, of news
headline tweets about 27 news stories, which is
the first to be jointly annotated for all our required
sub-tasks. We also provide initial predicted base-
line results for each of the sub-tasks, pointing at
the limitations of current state of the art.1

Overall, our main contribution is in proposing to
create a consolidated representation for the infor-
mation contained in multiple texts, and in specify-
ing how such representation can be created based
on entity and event coreference and lexical entail-
ment. An accompanying contribution is our an-
notated dataset, which can be used to analyze the
involved phenomena and their interactions, and as
a development and test set for automated gener-
ation of OKR structures. We further note that
while this paper focuses on creating an open repre-
sentation, by consolidating Open IE propositions,
future work may investigate the consolidation of
other semantic sentence representations, for exam-
ple AMR (Abstract Meaning Representation) (Ba-
narescu et al., 2013), while exploiting similar prin-
ciples to those proposed here.

2 Background: Relevant Component
Tasks

In this section we describe the prior annotation
tasks on which we rely in our representation, as
described later in Section 3.

2.1 Open Information Extraction

Open IE (Open Information Extraction) (Etzioni
et al., 2008) is the task of extracting coherent
propositions from a sentence, each comprising a
relation phrase and two or more argument phrases.
For example, (plane, landed in, Ankara).

Open IE has gained substantial and consistent
attention, and many automatic extractors were cre-

1Our dataset, detailed annotation guidelines, the annota-
tion tool and the baseline implementations are available at
https://github.com/vered1986/OKR.

ated (e.g., Fader et al. (2011); Del Corro and
Gemulla (2013)). Open IE’s extractions were also
shown to be effective as intermediate sentence-
level representation in various downstream appli-
cations (Stanovsky et al., 2015; Angeli et al.,
2015). Analogously, we conjecture a similar util-
ity of our OKR structures at the multi-text level.

Open IE does not assign roles to the arguments
associated with each predicate, as in other single-
sentence representations like SRL (Semantic Role
Labeling) (Carreras and Màrquez, 2005; Palmer
et al., 2010). While the former is not consis-
tent in assigning argument slots to the same ar-
guments across different propositions, the latter
requires predefined thematic role ontologies. A
middle ground was introduced by QA-SRL (He
et al., 2015), where predicate-argument structures
are represented using question-answer pairs, e.g.
(what landed somewhere?, plane), (where did
something land?, Ankara).

2.2 Coreference Resolution Tasks

In our representation, we use coreference resolu-
tion to consolidate mentions of the same entity or
the same event across multiple texts.

Entity Coreference Entity coreference resolu-
tion identifies mentions in a text that refer to the
same real-world entity (Soon et al., 2001; Ng and
Cardie, 2002; Bengtson and Roth, 2008; Clark and
Manning, 2015; Peng et al., 2015). In the cross-
document variant, Cross Document Coreference
Resolution (CDCR), mentions of the same entity
can also appear in multiple documents in a cor-
pus (Singh et al., 2011).

Event Coreference Event coreference deter-
mines whether two event descriptions (mentions)
refer to the same event (Humphreys et al., 1997).
Cross document event coreference (CDEC) is a
variant of the task in which mentions may occur in
different documents (Bagga and Baldwin, 1999).

Compared to within document event corefer-
ence (Chen et al., 2009; Araki et al., 2014; Liu et

13



al., 2014; Peng et al., 2016), the problem of cross
document event coreference has been relatively
under-explored (Bagga and Baldwin, 1999; Bejan
and Harabagiu, 2014). Standard benchmarks for
this task are the Event Coreference Bank (ECB)
(Bejan and Harabagiu, 2008) and its extensions,
that also annotate entity coreference: EECB (Lee
et al., 2012) and ECB+ (Cybulska and Vossen,
2014). See (Upadhyay et al., 2016) for more de-
tails on cross document event coreference.

Differently from our dataset described in Sec-
tion 4, ECB and its extensions do not establish
predicate-argument annotations. A secondary line
of work deals with aligning predicates across doc-
ument pairs, as done in Roth and Frank (2012).
PARMA (Wolfe et al., 2013) treated the task as
a token-alignment problem, aligning also argu-
ments, while Wolfe et al. (2015) added joint con-
straints to align predicates and their arguments.

Using Coreference for Consolidation Recog-
nizing that two elements are corefering can help
in consolidating textual information. In discourse
representation theory (DRT), a proposition ap-
plies to all co-referring entities (Kamp et al.,
2011). In recognizing textual entailment (Dagan
et al., 2013), lexical substitution of co-referring el-
ements is useful (Stern and Dagan, 2012). For ex-
ample, in Figure 1, sentence (1) together with the
coreference relation between plane and jet entail
that “Turkey forces down Syrian jet.”

2.3 Lexical Inference

Recognizing lexical inferences is an important
component in semantic tasks, in order to bridge
lexical variability in texts. For instance, in text
summarization, lexical inference can help identi-
fying redundancy, when two candidate sentences
for the summary differ only in terms that hold a
lexical inference relation (e.g. “the plane landed in
Ankara” and “the plane landed in Turkey”). Rec-
ognizing the inference direction, e.g. that Ankara
is more specific than Turkey, can help in selecting
the desired granularity level of the description.

There has been consistent attention to recogniz-
ing lexical inference between terms. Some meth-
ods aim to recognize a general lexical inference
relation (e.g. (Kotlerman et al., 2010; Turney and
Mohammad, 2015)), others focus on a specific se-
mantic relation, mostly hypernymy (Hearst, 1992;
Snow et al., 2005; Santus et al., 2014; Shwartz et
al., 2016), while recent methods classify a pair of

terms to a specific semantic relation out of several
(Baroni et al., 2012; Weeds et al., 2014; Pavlick et
al., 2015; Shwartz and Dagan, 2016). It is worth
noting that most existing methods are indifferent
to the context in which the target terms occur, with
the exception of few works, which were mostly fo-
cused on a narrow aspect of lexical inference, e.g.
lexical substitution (Melamud et al., 2015).

Determining entailment between predicates is a
different sub-task, which has also been broadly
explored (Lin and Pantel, 2001; Duclaye et al.,
2002; Szpektor et al., 2004; Schoenmackers et
al., 2010; Roth and Frank, 2012). Berant et al.
(2010) achieved state-of-the-art results on the task
by constructing a predicate entailment graph opti-
mizing a global objective function. However, per-
formance should be further improved in order to
be used accurately within semantic applications.

3 Proposed Representation

Our Open Knowledge Representation (OKR) aims
to capture the consolidated information expressed
jointly in a set of texts. In some analogy to struc-
tured knowledge bases, we would like the ele-
ments of our representation to correspond to en-
tities in the described scenario and to statements
(propositions) that relate them. Still, in the spirit
of Open IE, we would like the representation to be
open, while relying only on the natural language
terminology in the given texts without referring to
predefined external knowledge.

This section specifies our proposed structure,
with a running example in Figure 2. The specifica-
tion involves two aspects: the first is defining the
component annotation sub-tasks involved in cre-
ating our representation, following those reviewed
in Section 2; the second is specifying how we de-
rive from these component annotations a consoli-
dated representation. These two aspects are inter-
leaved along the presentation, where for each step
we first describe the relevant annotations and then
how we use them to create the corresponding com-
ponent of the representation.

3.1 Entities

To represent entities, we first annotate the text by
entity mentions and coreference. Following the
typical notion for these tasks, an entity mention
corresponds to a word or multi-word expression
that refers to an object or concept in the described
scenario (in the broader sense of “entity”). Ac-

14



Original texts:
(1) Turkey forces down Syrian plane.

(2) Syrian jet grounded by Turkey carried munitions from Moscow.

(3) Intercepted Syrian plane carried ammunition from Russia.

Entities:
E1 = {Turkey}, E2 = {Syrian}, E3 = {plane, jet}
E4 = {munitions, ammunition}, E5 = {Moscow}, E6 = {Russia}
Proposition Mentions (+ coreference + argument alignment):
P1: [a1] forces down [a2] (Turkey [E1], plane [E3])
P2: [a1] [a2] (Syrian [E2], plane [E3]) (implicit)
P2: [a1] [a2] (Syrian [E2], jet [E3]) (implicit)
P1: [a2] grounded by [a1] (jet [E3], Turkey [E1])
P3: [a1] carried [a2] from [a3]

(jet [E3], munitions [E4], Moscow [E5])
P1: intercepted [a2] (jet [E3])
P2: [a1] [a2] (Syrian [E2], plane [E3]) (implicit)
P3: [a1] carried [a2] from [a3]

(plane [E3], ammunition [E6], Russia [E6])
Consolidated Propositions:

P1 :


[a1] forces down [a2]
[a2] grounded by [a1]

intercepted [a2]

 [a1: {E1}, a2: {E3}]
P2: { [a1] [a2] (implicit) } [a1: {E2}, a2: {E3}]
P3: { [a1] carried [a2] from [a3] }

[ a1: {E3}, a2: {E4},
a3: {E5, E6}

]
Entailment Graphs:

Propositions:

P1 :

[a1] forces down [a2] [a2] grounded by [a1]

intercepted [a2]

Entities:
E3: planejet

E4: munitionsammunition

Arguments: P3.a3: RussiaMoscow

[a1] forces
down [a2]

E1 P1 

[a1] carried [a2]
from [a3] 

intercepted [a2]

[a2] 
grounded

by [a1] 

E3
{jet,plane}

E4
{ammunition

,
munitions}E6 {Russia}

E5 {Moscow}

IMPLICIT

E1
{Turkey}

E3
{jet,plane}

a2

a1

a2

a1

a3

E2
{Syrian}

E3
{jet,plane}

a2

a1

P2 

P3 

Turkey

E2 
Syrian

E5 
Moscow

E6 
Russia

E4 
munitions

ammunition

E3 
plane

jet

(a) (b)

Figure 2: An illustration of our OKR formalism (a), with a corresponding graphical view of the consol-
idated structure (b). In (b), dashed lines connect entities to their instantiation within arguments, while
allowing graph-traversal inferences such as: what is the relation between Turkey and Russia? Turkey
intercepted a plane that carried ammunition from Russia (the path from E1 to E6 via the darker dashed
lines).

cordingly, we represent an entity in the described
scenario by the coreference cluster of all its men-
tions. We represent the coreferring cluster of men-
tions by the multiset of its terms, keeping pointers
to each term’s mentions (see Entities in Figure 2;
to avoid clutter, pointers are not presented in the
figure). We note that we take an inclusive view
which regards concepts as entities, for example the
adjective Syrian is considered an entity mention
that may corefer with Syria.

3.2 Proposition Mentions and Consolidated
Propositions

To represent propositions, we first annotate Open
IE style extractions, which we term proposition
mentions. Each mention consists of a predicate
expression, e.g. around verbs or nominalizations,

and a set of arguments (see Proposition Mentions
in Figure 2). We deviate slightly from standard
Open IE formats by representing the predicate ex-
pression as a template, with place holders for the
arguments (marked with brackets in the figure).
This follows the common representation of predi-
cates within predicate inference rules, as in DIRT
(Lin and Pantel, 2001), and allows the span of en-
tity arguments to correspond exactly to the entity
term. Further, as typical in Open IE, modalities
and negations become part of the lexical elements
of the predicate. Notice that at this stage an ar-
gument mention is already associated with its cor-
responding entity. Further, we annotate implicit
predicates when a predication between two enti-
ties is implied, without an explicit predicate ex-
pression, as common for noun modifications (P2

15



in the figure). Nested propositions are represented
by having one proposition mention as an argument
of the other (e.g. “the [plane] was forced to [land
in Ankara]”).

To link different mentions of the same real
world fact, we annotate proposition coreference,
which generalizes the notion of event corefer-
ence to cover all types of predications (e.g., John
is Mary’s brother would co-refer with Mary is
John’s sister). This annotation specifies the coref-
erence relation for a cluster of proposition men-
tions (denoted by the same proposition index Pi in
Figure 2), as well as an alignment of their argu-
ments, (denoted by matching argument indexes
within the same proposition cluster). We then con-
sider a proposition to correspond to a coreference
cluster of proposition mentions, which jointly de-
scribe the referred real-world fact.

Yet, a cluster of co-referring proposition men-
tions does not provide a succinct representation for
the aggregated textual description of a proposition.
To that end, we aggregate the information in the
cluster into a Consolidated Proposition, composed
of a consolidated predicate and consolidated ar-
guments. Similar to entity representation, a con-
solidated predicate is represented by the set of all
predicate expressions appearing in the cluster. A
consolidated argument is specified by the set of
all entities (or propositions, in case of having one
proposition being an argument of another one) that
occupy this argument’s slot in the different men-
tions. As with entities, each element in this rep-
resentation is accompanied by a set of pointers to
all its original mentions (omitted from the figure).
A graphical illustration of this structure is given
in Figure 2(b) (for now, ignore the arrows within
some of the nodes).

A consolidated proposition encodes compactly
all possible textual descriptions for the referred
proposition, which can be generated from its men-
tions taken jointly. Each description can be gener-
ated by picking one possible predicate expression
and then picking one possible lexical choice for
each argument. For example, P1 may be described
as Turkey intercepted a plane, Turkey forces down
a jet etc. Some of these descriptions correspond to
original mentions in the text, while others can be
induced through coreference (as reviewed at the
end of Section 2.2). The representation of a con-
solidated proposition thus does not depend on the
particular way in which lexical choices were split

across the different proposition mentions.

3.3 Lexical Entailment Graphs

The set of descriptions encoded in a consolidated
proposition is highly redundant. To make it more
useful, we would like to model the information
overlap between different lexical choices. For ex-
ample, we want to know that Turkey intercepted a
plane is more general than, or equivalently, is en-
tailed by, Turkey intercepted a jet. To that end, we
annotate the lexical entailment relations between
the elements in each component of our represen-
tation, that is, within each consolidated predicate,
consolidated argument and entity. This yields a
lexical entailment graph within each component
(see figure 2), which models the information con-
tainment relationships between different descrip-
tions.

Notice that in our setting the lexical entailment
relation is considered within the given context (see
Section 2.3). For example, grounded and forced
down may not be generically synonymous, but
they do convey equivalent information in a given
context of forcing a flying plane to land. Contra-
dictions are modeled to a limited extent, by anno-
tating contradiction relations (in context) between
elements of our entailment graphs, for example
when different figures are reported for the number
of casualties in a disaster. This is a natural rep-
resentation, since contradiction is often modeled
within a three-way entailment classification task.
Modeling of broader cases of contradiction is left
for future work.

The entailment graphs yield better modeling
of the supporting text mentions (and their total
count) for each possible description. For exam-
ple, knowing that Moscow entails Russia, we can
assume in P3 two supporting mentions for know-
ing that the ammunition was carried from Russia,
while having only one supporting mention for the
more detailed information regarding Moscow be-
ing the origin. Such frequency support often cor-
relates with confidence and prominence of infor-
mation, which, together with generality modeling,
may be very useful in applications such as multi-
document summarization or question answering.
Finally, the graphical view of our representation
lends itself to graph-based inferences, such as
looking for all connections between two entities,
similar to aggregated inferences over structured
knowledge graphs (see example in Figure 2(b)).

16



# Entities 1262
# Entity mentions 5074
# Entity singeltons 777
# Propositions 1406
# Proposition mentions 4311
# Proposition Singletons 949
Avg. mentions per entity chain 8.86
Avg. distinct lemmas per entity chain 2.00
Avg. mentions per proposition chain 7.35
Avg. distinct lemmas per prop. chain 2.24
Avg. number of elements per arg. chain 1.08

Table 2: Twitter dataset statistics. Distinct lemma
terms per proposition chain were calculated only
on explicit propositions. Average number of el-
ements per argument chain measures how many
distinct entities or propositions were part of the
same argument.

In summary, our open knowledge representa-
tion consists of the following: entities, generated
by detecting entity mentions and coreference; con-
solidated propositions, composed of consolidated
predicates and arguments, which are generated
by detecting proposition mentions and coreference
relations between them; lexical entailment graphs
for entities, consolidated predicates and consoli-
dated arguments, which specify the inference rela-
tions between the elements within each of these
components. This yields a compact representa-
tion of all possible descriptions of the statements
jointly asserted by the set of texts, as induced via
coreference-based inference, while tracking in-
formation containment between different descrip-
tions as well as tracking their (induced) supporting
mentions.

4 News-Related Tweets Dataset

Following the formal definition of our OKR struc-
tures, we compiled a corpus with gold annotations
of our 5 subtasks (listed in Table 1). As outlined in
the previous section, our structures follow deter-
ministically from these annotations. Specifically,
we make use of the news-related tweets collected
in the Twitter Event Detection Dataset (McMinn
et al., 2013), which clusters tweets from major
news networks and other sources discussing the
same event (for example, the grounding of a Syr-
ian plane by the Turkish government). We chose to
annotate news related tweets in this first dataset for
several reasons: (1) they represent self contained
assertions, (2) they tend to be relatively factual
and succinct, and (3) by looking at several news
sources we can obtain a corpus with high redun-
dancy, which our representation aims to address.

We note that while this dataset exhibits a limited
amount of linguistic complexity, making it suit-
able for a first investigation, it still represents a
very practical use case of consolidating informa-
tion in a large stream of tweets about a news story.

This annotation serves two main purposes.
First, it validates the feasibility of our annotation
scheme in terms of annotator requirements, train-
ing and agreement. Second, to the best of our
knowledge, this is the first time these core NLP
annotations are annotated in parallel over the same
texts. Following, this annotation has the potential
of becoming a useful resource spurring future re-
search into joint prediction of these annotations.
For instance, predicate argument structures may
benefit from co-reference signals, and entity ex-
traction systems may exploit signals from lexical
entailment.

Overall, we annotated 1257 tweets from 27
clusters. We release the dataset both in full OKR
format, as well as ECB-like “light” format, con-
taining only the annotated co-reference chains.
Overall corpus statistics are depicted in Table 2.

4.1 Dataset Characteristics
An analysis of the annotations reveals interesting
and unique characteristics of our annotated corpus.

First, the part of speech distribution of enti-
ties and predicates (Table 3) shows that our cor-
pus captures information beyond the current focus
on verb-centric applications and corpora in NLP.
Namely, our corpus contains a vast number of non-
verbal predications (67%), and a relatively large
number of adjectival entities, owing to the fact that
our structure annotates concepts such as “north-
ern” or “Syrian” as entities in an implicit relation.

Second, the average number of unique lemmas
per entity and proposition chains (2.00 and 2.24,
respectively) shows that our corpus exhibits a fair
amount of non-trivial lexical variability.

Finally, roughly 96% of our entailment graphs
(entity and proposition) form a connected compo-
nent. This data provides an interesting potential
for investigating and modeling lexical inference
relations within coreference chains.

4.2 Annotation Procedure and Agreement
The annotation was performed by two native
English speakers with linguistic academic back-
ground, which had 10 hours of in house training.
The entire annotation process took 200 person-
hours using a graphical tool purposely-designed

17



Task
Entity
Ment.

Entity Co-reference
Prop. Mentions Proposition Co-Reference Entailment

Pred. Arg. Predicate Argument Entity Prop
avg. acc MUC B3 CEAF CoNLL F1 avg. acc avg. acc MUC B3 CEAF CoNLL F1 MUC B3 CEAF CoNLL F1 F1 F1

IAA .85 .87 .92 .92 .90 .74 (.93, .72)† .85 .86 .88 .76 .83 .99 .99 .98 .99 .70 .82
Pred .58 .84 .89 .81 .85 .41 (.73, .25)† .37 .47 .67 .56 .56 .93 .97 .94 .95 .44 .56

Table 1: Inter-Annotator Agreement (top) and off-the-shelf state-of-the-art predicted performance (bot-
tom, see Section 5) for the OKR subtasks: (1) Entity mention extraction (for prediction we use F1 score)
(2) Entity co-reference (3) Proposition Extraction (predicate identification and argument detection) (4)
Proposition Co-reference (predicate coreference and argument alignment), and (5) Entailment graphs
(entity and proposition entailment; argument entailment figures are not presented due to very low statis-
tics). † Numbers in parenthesis denote verbal vs. non-verbal predicates, respectively.

to facilitate the incremental annotation for all
subtasks . We employ the QA-SRL annotation
methodology to help determining Open IE pred-
icate and argument spans in the gold standard, for
its intuitiveness for non-expert annotators (He et
al., 2015). Five clusters were annotated indepen-
dently by both annotators and were used to mea-
sure their agreement on the task. The other clus-
ters were annotated by one annotator and reviewed
by an expert.

We measure agreement separately on each an-
notation subtask. After each task in our pipeline
we keep only the consensual annotations. For ex-
ample, we measure entity coreference agreement
only for entity mentions that were annotated by
both annotators. For entity, predicate and argu-
ment mention agreement, we average the accuracy
of the two annotators, each computed while taking
the other as a gold reference.

For entity, predicate, and argument co-reference
we calculated coreference resolution metrics: the
link-based MUC (Vilain et al., 1995), the mention-
based B3 (Bagga and Baldwin, 1998), the entity-
based CEAF, and the widely adopted CoNLL F1
measure which is an average of the three. For en-
tity and proposition entailment we compute the F1
score over the annotated directed edges in each
entailment graph, as is common for entailment
agreement metrics (Berant et al., 2010).

We macro-averaged these scores to obtain an
overall agreement on the 5 events annotated by
both annotators. The agreement scores for the two
annotators are shown in Table 1, and overall show
high levels of agreement. A qualitative analysis of
the more common disagreements between annota-
tors is shown in Table 4.

Overall, this shows that our parallel annotation
is indeed feasible; agreement on each of the sub-
tasks is relatively high and on par with reported
inter-annotator agreement on similar tasks.

POS Nouns Verbs Adj’s Impl. Others

Ent. Dist. .85 .01 .09 – .05
Pred. Dist. .40 .33 .04 .18 .04

Table 3: Entity and Predicate distribution across
part of speech tags: nouns, verbs, adjectives, non-
lexicalized (implicit) and all others.

Disagreement Type Examples

Phrasal verbs [placed to leave]pred. vs. [placed to]pred.[leave]pred.[faces charges]pred. vs. [faces]pred. [charges]arg.

Nominalizations
[suspect]ent. plane vs. [suspect]pred. plane
[terror]ent. attack vs. [terror]pred. attack
U.S. [elections]ent. vs. U.S. [elections]pred.

Entailment fuel→gas vs. gas→fuelscandal→case vs. case→scandal

Table 4: Typical cases of annotator disagreements.
Annotated spans are denoted by square brackets,
subscript denotes label for the mention (predicate,
argument or entity).

5 Baselines

As we have shown in previous sections, our struc-
ture is derived from known “core” NLP tasks, ex-
tended where needed to fit our consolidated repre-
sentation. Subsequently, a readily available means
of automatically recovering OKR is through a
pipeline which uses off-the-shelf models for each
of the subtasks.

To that end, we employ publicly available tools
and simple baselines which approximate the cur-
rent state-of-the-art in each of these subtasks. For
brevity sake, in the rest of the section we briefly
describe each of these baselines. For a more de-
tailed technical description see the OKR repos-
itory (https://github.com/vered1986/
OKR).

For Entity Mention extraction we use the spaCy
NER model2 in addition to annotating all of the
nouns and adjectives as entities. For Proposition
Mention detection we use Open IE propositions
extracted from PropS (Stanovsky et al., 2016),
where non-restrictive arguments were reduced fol-
lowing Stanovsky and Dagan (2016). For Proposi-

2https://spacy.io/

18



tion and Entity coreference, we clustered the entity
mentions based on simple lexical similarity met-
rics (e.g., lemma matching and Levenshtein dis-
tance), shown to be effective on our news tweets.
3

For Argument Mention detection we attach the
components (entities and propositions) as argu-
ments of predicates when the components are
syntactically dependent on them. Argument Co-
reference is simply predicted by marking co-
reference if and only if the arguments are both
mentions of the same entity co-reference chain.
For Entity Entailment purposes we used knowl-
edge resources (Shwartz et al., 2015) and a pre-
trained model for HypeNET (Shwartz et al., 2016)
to obtain a score for all pairs of Wikipedia com-
mon words (unigrams, bigrams, and trigrams). A
threshold for the binary entailment decision was
then calibrated on a held out development set. Fi-
nally, for Predicate Entailment we used the entail-
ment rules extracted by Berant et al. (2012).

5.1 Results and Error Analysis

Using the same metrics used for measuring inter-
annotator agreement, we evaluated how well the
presented models were able to recover the differ-
ent facets of the OKR gold annotations. The per-
formance on the different subtasks is presented in
Table 1 (bottom).

We measure the performance of each compo-
nent separately, while taking the annotations for
all previous steps from the gold human annota-
tions. This allows us to examine the performance
of the current component, alleviating any incurred
errors from previous steps. Thus, we can iden-
tify technological “bottle-necks” – the steps which
most significantly lower predicted OKR accuracy
using current off-the-shelf tools.

First, we noticed that non-verbal predicates
pose a challenge for current verb-centric systems.
This primarily manifests in low scores for iden-
tifying entities, predicates and arguments. Many
entity mention errors are due to nominalizations
mistakenly annotated as entities. When excluding
gold nominalizations, the entity mention baseline
F1 score rises from 0.58 to 0.63. As mentioned

3We chose simple metrics over complex state-of-the-art
entity coreference models since they target different scenarios
from ours: first, they focus on named entities, and are likely
to overlook common nouns like plane and jet. Second, since
we work in the context of the same news story, it is reasonable
to assume that, for example, two mentions of a person with
the same last name belong to the same entity.

earlier (Section 4.2) nominalizations were also one
of the main challenges for the annotators. Further-
more, recognizing nominalizations and other non-
verbal predicates, which are very common in our
dataset (see Table 3), proves to be a difficult task.
Indeed, we see a significant improvement in per-
formance when comparing verbal predicate men-
tion performance to non-verbal performance (ac-
curacy of 0.73 vs. 0.25). Finally, argument iden-
tification was hard mainly because of inconsisten-
cies in verbal versus nominal predicate-argument
structure in dependency trees.4

The low performance in predicate coreference
compared to entity coreference can be explained
by the higher variability of predicate terms. The
argument co-reference task becomes easy given
gold predicate-argument structures, as most argu-
ments are singletons (i.e. composed of a single
element).

Finally, while the performance of the predicate
entailment component reflects the current state-
of-the-art (Berant et al., 2012; Han and Sun,
2016), the performance on entity entailment is
much worse than the current state-of-the-art in this
task as measured on common lexical inference test
sets. We conjecture that this stems from the nature
of the entities in our dataset, consisting of both
named entities and common nouns, many of which
are multi-word expressions, whereas most work in
entity entailment is focused on single word com-
mon nouns. Furthermore, it is worth noting that
our annotations are of naturally occurring texts,
and represent lexical entailment in real world co-
reference chains, as opposed to synthetically com-
piled test sets which are often used for this task.

While several tasks achieve reasonable perfor-
mance on our datasets, most tasks leave room for
improvement. These bottle-necks are bound to
hinder the performance of a pipeline end-to-end
system. Future research into OKR should first tar-
get these areas; either as a pipeline or in a joint
learning framework.

6 Applications and Related Work

The need to consolidate information originating
from multiple texts is common in applications
that summarize multiple text into some struc-
ture, such as multi-document summarization and
knowledge-base population. Currently, there is no

4E.g., “Facebook’s acquisition of Instagram” is repre-
sented differently than “Facebook acquired Instagram”.

19



systematic solution, and the burden of integrating
information across multiple texts is delegated to
downstream applications, leading to partial solu-
tions which are geared to specific applications.

Multi-Document Summarization (MDS)
(Barzilay et al., 1999) is a task whose goal is to
produce a concise summary from a set of related
text documents, such that it includes the most
important information in a non-redundant manner.
While extractive summarization selects salient
sentences from the document collection, abstrac-
tive summarization generates new sentences, and
is considered a more promising yet more difficult
task.

A recent approach for abstractive summa-
rization generates a graphical representation of
the input documents by: (1) parsing each sen-
tence/document into a meaning representation
structure; and (2) merging the structures into a sin-
gle structure that represents the entire summary,
e.g. by identifying coreferring items.

In that sense, this approach is similar to OKR.
However, current methods applying this approach
are still limited. Gerani et al. (2014) parse each
document to discourse tree representation (Joty et
al., 2013), aggregating them based on entity coref-
erence. Yet, they work with a limited set of (dis-
course) relations, and rely on coreference only be-
tween entities, which was detected manually.

Similarly, Liu et al. (2015) parse each input sen-
tence into an individual AMR graph (Banarescu
et al., 2013), and merge those into a single graph
through identical concepts. This work extends the
AMR formalism of canonicalized representation
per entity or event to multiple sentences. How-
ever, they only focus on certain types of named
entities, and collapse two entities based on their
names rather than on coreference.

Event-Centric Knowledge Graphs (ECKG)
(Vossen et al., 2016; Rospocher et al., 2016) is
another related work which represent news arti-
cles as graphs. Event nodes are linked to DBPedia
(Auer et al., 2007), with the goal of enriching en-
tities and events with dynamic knowledge. For ex-
ample, an event describing the interception of the
Syrian plane by Turkey will be linked in DBPedia
to Syria and Turkey.

We propose that OKR can help the described
applications by providing a general underlying
representation for multiple texts, obviating the

need to develop specialized consolidation meth-
ods for each application. We can expect the use
of OKR structures in MDS to shift the research ef-
forts in this task to other components, e.g. gener-
ation, and eventually contribute to improving state
of the art on this task. Similarly, an algorithm cre-
ating the ECKG structure can benefit from build-
ing upon a consolidated structure such as OKR,
rather than working directly on free text.

7 Conclusions

In this paper we advocate the development of rep-
resentation frameworks for the consolidated infor-
mation expressed in a set of texts. The key ingre-
dients of our approach are the extraction of propo-
sition structures which capture individual state-
ments and their merging based on entity and event
coreference. Coreference clusters are proposed as
a handle on real world entities and facts, while
still being self-contained within the textual realm.
Lexical entailment is proposed to model infor-
mation containment between different textual de-
scriptions of the same real world components.

While we developed an “open” KR framework,
future work may investigate the creation of similar
models based on structures that do refer to exter-
nal resources (such as PropBank, as in Abstract
Meaning Representation – AMR). Gradually, fine
grained semantic phenomena may be addressed,
such as factuality, attribution and modeling sub-
events and cross-event relationships. Finally, we
plan to investigate performing the core annotation
sub-tasks via crowdsourcing, for scalability.

Acknowledgments

This work was supported in part by grants from
the MAGNET program of the Israeli Office of the
Chief Scientist (OCS) and the German Research
Foundation through the German-Israeli Project
Cooperation (DIP, grant DA 1600/1-1), and by
Contract HR0011-15-2-0025 with the US Defense
Advanced Research Projects Agency (DARPA).

References
Gabor Angeli, Melvin Jose Johnson Premkumar, and

Christopher D. Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages

20



344–354, Beijing, China, July. Association for Com-
putational Linguistics.

Jun Araki, Zhengzhong Liu, Eduard Hovy, and Teruko
Mitamura. 2014. Detecting subevent structure for
event coreference resolution. In Proceedings of the
Ninth International Conference on Language Re-
sources and Evaluation (LREC-2014), pages 4553–
4558, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. The semantic web. In Lecture Notes in Com-
puter Science, volume 4825, chapter Dbpedia: A
nucleus for a web of open data, pages 722–735.
Springer Berlin Heidelberg.

Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the First International Conference on Language Re-
sources and Evaluation (LREC’98), volume 1, pages
563–566, Granada, Spain. European Language Re-
sources Association (ELRA).

Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1–8, College Park, Maryland,US. Association
for Computational Linguistics.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 23–32, Avignon, France, April. Association
for Computational Linguistics.

Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 550–557, College
Park, Maryland, USA, June. Association for Com-
putational Linguistics.

Cosmin Bejan and Sanda Harabagiu. 2008. A lin-
guistic resource for discovering event structures and
resolving event coreference. In Proceedings of the
Sixth International Conference on Language Re-
sources and Evaluation (LREC’08), pages 2881–
2887, Marrakech, Morocco, May. European Lan-
guage Resources Association (ELRA).

Cosmin A. Bejan and Sanda Harabagiu. 2014. Un-
supervised event coreference resolution. Computa-
tional Linguistics, 40(2):311–347.

Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 294–
303, Honolulu, Hawaii. Association for Computa-
tional Linguistics.

Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1220–1229, Uppsala, Sweden, July. Association for
Computational Linguistics.

Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73–111.

Xavier Carreras and Lluı́s Màrquez. 2005. Intro-
duction to the CoNLL-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 152–164, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.

Zheng Chen, Heng Ji, and Robert Haralick. 2009.
A pairwise event coreference model, feature impact
and evaluation for event coreference resolution. In
Proceedings of the Workshop on Events in Emerg-
ing Text Types, pages 17–22, Borovets, Bulgaria,
September. Association for Computational Linguis-
tics.

Kevin Clark and Christopher D. Manning. 2015.
Entity-centric coreference resolution with model
stacking. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1405–1415, Beijing, China, July. Association
for Computational Linguistics.

Agata Cybulska and Piek Vossen. 2014. Using a
sledgehammer to crack a nut? lexical diversity and
event coreference resolution. In Proceedings of the
Ninth International Conference on Language Re-
sources and Evaluation (LREC-2014), pages 4545–
4552, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).

Ido Dagan, Dan Roth, and Mark Sammons. 2013. Rec-
ognizing textual entailment. Morgan & Claypool
Publishers, San Rafael, CA.

Luciano Del Corro and Rainer Gemulla. 2013.
Clausie: Clause-based open information extraction.
In Proceedings of the 22Nd International Confer-
ence on World Wide Web, WWW ’13, pages 355–
366, Rio de Janeiro, Brazil. Association for Com-
puting Machinery.

21



Florence Duclaye, François Yvon, and Olivier Collin.
2002. Using the web as a linguistic resource for
learning reformulations automatically. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC’02), vol-
ume 2, pages 390–396, Las Palmas, Canary Islands
- Spain. European Language Resources Association
(ELRA).

Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68–74, December.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535–1545, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.

Shima Gerani, Yashar Mehdad, Giuseppe Carenini,
Raymond T. Ng, and Bita Nejat. 2014. Abstractive
summarization of product reviews using discourse
structure. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1602–1613, Doha, Qatar, October.
Association for Computational Linguistics.

Xianpei Han and Le Sun. 2016. Context-sensitive
inference rule discovery: A graph-based method.
In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 2902–2911, Osaka, Japan,
December. The COLING 2016 Organizing Commit-
tee.

Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
643–653, Lisbon, Portugal, September. Association
for Computational Linguistics.

Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In COLING
1992 Volume 2: Proceedings of the 15th Inter-
national Conference on Computational Linguistics,
volume 2, pages 539–545. Association for Compu-
tational Linguistics.

Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information ex-
traction. In Proceedings of a Workshop on Opera-
tional Factors in Practical, Robust Anaphora Reso-
lution for Unrestricted Texts, pages 75–81, Madrid,
Spain, July. Association for Computational Linguis-
tics.

Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 486–496,

Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Hans Kamp, Josef Van Genabith, and Uwe Reyle.
2011. Discourse representation theory. In Hand-
book of philosophical logic, volume 15, pages 125–
394. Springer Netherlands.

Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(04):359–389.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500, Jeju Island, Korea, July. Association for
Computational Linguistics.

Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proceedings of the
Seventh ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’01,
pages 323–328, San Francisco, California. Associa-
tion for Computing Machinery.

Zhengzhong Liu, Jun Araki, Eduard Hovy, and Teruko
Mitamura. 2014. Supervised within-document
event coreference using information propagation. In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014), pages 4539–4544, Reykjavik, Iceland. Euro-
pean Language Resources Association (ELRA).

Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman
Sadeh, and Noah A. Smith. 2015. Toward abstrac-
tive summarization using semantic representations.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1077–1086, Denver, Colorado, May–
June. Association for Computational Linguistics.

Andrew J. McMinn, Yashar Moshfeghi, and Joe-
mon M. Jose. 2013. Building a large-scale cor-
pus for evaluating event detection on twitter. In
Proceedings of the 22Nd ACM International Con-
ference on Information & Knowledge Management,
CIKM ’13, pages 409–418, San Francisco, Califor-
nia, USA. Association for Computing Machinery.

Oren Melamud, Ido Dagan, and Jacob Goldberger.
2015. Modeling word meaning in context with sub-
stitute vectors. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 472–482, Denver, Col-
orado, May–June. Association for Computational
Linguistics.

Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.

22



In Proceedings of 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 104–
111, Philadelphia, Pennsylvania, USA, July. Asso-
ciation for Computational Linguistics.

Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1–103.

Ellie Pavlick, Johan Bos, Malvina Nissim, Charley
Beller, Benjamin Van Durme, and Chris Callison-
Burch. 2015. Adding semantics to data-driven para-
phrasing. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1512–1522, Beijing, China, July. Association
for Computational Linguistics.

Haoruo Peng, Kai-Wei Chang, and Dan Roth. 2015. A
joint framework for coreference resolution and men-
tion head detection. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning, pages 12–21, Beijing, China, July.
Association for Computational Linguistics.

Haoruo Peng, Yangqiu Song, and Dan Roth. 2016.
Event detection and co-reference with minimal su-
pervision. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 392–402, Austin, Texas, November.
Association for Computational Linguistics.

Marco Rospocher, Marieke van Erp, Piek Vossen,
Antske Fokkens, Itziar Aldabe, German Rigau,
Aitor Soroa, Thomas Ploeger, and Tessel Bogaard.
2016. Building event-centric knowledge graphs
from news. Web Semantics: Science, Services and
Agents on the World Wide Web, 37:132–151.

Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics – Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218–
227, Montréal, Canada, 7-8 June. Association for
Computational Linguistics.

Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine
Schulte im Walde. 2014. Chasing hypernyms in
vector spaces with entropy. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 38–42, Gothenburg, Sweden,
April. Association for Computational Linguistics.

Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1088–1098, Cambridge,
MA, October. Association for Computational Lin-
guistics.

Vered Shwartz and Ido Dagan. 2016. Path-based vs.
distributional information in recognizing lexical se-
mantic relations. In Proceedings of the 5th Work-
shop on Cognitive Aspects of the Lexicon (CogALex
- V), pages 24–29, Osaka, Japan, December. The
COLING 2016 Organizing Committee.

Vered Shwartz, Omer Levy, Ido Dagan, and Jacob
Goldberger. 2015. Learning to exploit structured
resources for lexical inference. In Proceedings of
the Nineteenth Conference on Computational Nat-
ural Language Learning, pages 175–184, Beijing,
China, July. Association for Computational Linguis-
tics.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2389–2398, Berlin, Germany, August.
Association for Computational Linguistics.

Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-scale
cross-document coreference using distributed infer-
ence and hierarchical models. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 793–803, Portland, Oregon, USA, June.
Association for Computational Linguistics.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems, volume 17, pages 1297–1304. MIT
Press.

Wee M. Soon, Hwee T. Ng, and Daniel C. Y. Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational linguis-
tics, 27(4):521–544.

Gabriel Stanovsky and Ido Dagan. 2016. Annotating
and predicting non-restrictive noun phrase modifi-
cations. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1256–1265,
Berlin, Germany, August. Association for Compu-
tational Linguistics.

Gabriel Stanovsky, Ido Dagan, and Mausam. 2015.
Open IE as an intermediate structure for semantic
tasks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
pages 303–308, Beijing, China, July. Association for
Computational Linguistics.

Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav
Goldberg. 2016. Getting more out of syntax with
props. arXiv preprint.

23



Asher Stern and Ido Dagan. 2012. Biutee: A mod-
ular open-source system for recognizing textual en-
tailment. In Proceedings of the ACL 2012 System
Demonstrations, pages 73–78, Jeju Island, Korea,
July. Association for Computational Linguistics.

Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages
41–48, Barcelona, Spain, July. Association for Com-
putational Linguistics.

Peter D. Turney and Saif M. Mohammad. 2015. Ex-
periments with three approaches to recognizing lex-
ical entailment. Natural Language Engineering,
21(03):437–476.

Shyam Upadhyay, Nitish Gupta, Christos
Christodoulopoulos, and Dan Roth. 2016. Re-
visiting the evaluation for cross document event
coreference. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th conference on Message understand-
ing, pages 45–52, Columbia, Maryland. Association
for Computational Linguistics.

Piek Vossen, Rodrigo Agerri, Itziar Aldabe, Agata Cy-
bulska, Marieke van Erp, Antske Fokkens, Egoitz
Laparra, Anne-Lyse Minard, Alessio Palmero Apro-
sio, German Rigau, et al. 2016. Newsreader: Us-
ing knowledge resources in a cross-lingual reading
machine to generate more knowledge from mas-
sive streams of news. Knowledge-Based Systems,
110:60–85.

Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,
and Bill Keller. 2014. Learning to distinguish hy-
pernyms and co-hyponyms. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
2249–2259, Dublin, Ireland, August. Dublin City
University and Association for Computational Lin-
guistics.

Travis Wolfe, Benjamin Van Durme, Mark Dredze,
Nicholas Andrews, Charley Beller, Chris Callison-
Burch, Jay DeYoung, Justin Snyder, Jonathan
Weese, Tan Xu, and Xuchen Yao. 2013. Parma: A
predicate argument aligner. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 63–68, Sofia, Bulgaria, August. Association
for Computational Linguistics.

Travis Wolfe, Mark Dredze, and Benjamin Van Durme.
2015. Predicate argument alignment using a global
coherence model. In Proceedings of the 2015 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human

Language Technologies, pages 11–20, Denver, Col-
orado, May–June. Association for Computational
Linguistics.

24


