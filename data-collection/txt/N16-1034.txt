



















































Joint Event Extraction via Recurrent Neural Networks


Proceedings of NAACL-HLT 2016, pages 300–309,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Joint Event Extraction via Recurrent Neural Networks

Thien Huu Nguyen, Kyunghyun Cho and Ralph Grishman
Computer Science Department, New York University, New York, NY 10003, USA

thien@cs.nyu.edu,kyunghyun.cho@nyu.edu,grishman@cs.nyu.edu

Abstract

Event extraction is a particularly challenging
problem in information extraction. The state-
of-the-art models for this problem have ei-
ther applied convolutional neural networks in
a pipelined framework (Chen et al., 2015) or
followed the joint architecture via structured
prediction with rich local and global features
(Li et al., 2013). The former is able to learn
hidden feature representations automatically
from data based on the continuous and gen-
eralized representations of words. The latter,
on the other hand, is capable of mitigating the
error propagation problem of the pipelined ap-
proach and exploiting the inter-dependencies
between event triggers and argument roles via
discrete structures. In this work, we propose
to do event extraction in a joint framework
with bidirectional recurrent neural networks,
thereby benefiting from the advantages of the
two models as well as addressing issues inher-
ent in the existing approaches. We systemati-
cally investigate different memory features for
the joint model and demonstrate that the pro-
posed model achieves the state-of-the-art per-
formance on the ACE 2005 dataset.

1 Introduction

We address the problem of event extraction (EE):
identifying event triggers of specified types and their
arguments in text. Triggers are often single verbs
or normalizations that evoke some events of interest
while arguments are the entities participating into
such events. This is an important and challeng-
ing task of information extraction in natural lan-
guage processing (NLP), as the same event might

be present in various expressions, and an expression
might expresses different events in different con-
texts.

There are two main approaches to EE: (i) the joint
approach that predicts event triggers and arguments
for sentences simultaneously as a structured predic-
tion problem, and (ii) the pipelined approach that
first performs trigger prediction and then identifies
arguments in separate stages.

The most successful joint system for EE (Li et
al., 2013) is based on the structured perceptron al-
gorithm with a large set of local and global fea-
tures1. These features are designed to capture the
discrete structures that are intuitively helpful for EE
using the NLP toolkits (e.g., part of speech tags, de-
pendency and constituent tags). The advantages of
such a joint system are twofold: (i) mitigating the er-
ror propagation from the upstream component (trig-
ger identification) to the downstream classifier (ar-
gument identification), and (ii) benefiting from the
the inter-dependencies among event triggers and ar-
gument roles via global features. For example, con-
sider the following sentence (taken from Li et al.
(2013)) in the ACE 2005 dataset:

In Baghdad, a cameraman died when an Ameri-
can tank fired on the Palestine hotel.

In this sentence, died and fired are the event trig-
gers for the events of types Die and Attack, respec-
tively. In the pipelined approach, it is often simple
for the argument classifiers to realize that camera-

1Local features encapsulate the characteristics for the in-
dividual tasks (i.e, trigger and argument role labeling) while
global features target the dependencies between triggers and ar-
guments and are only available in the joint approach.

300



man is the Target argument of the Die event due to
the proximity between cameraman and died in the
sentence. However, as cameraman is far away from
fired, the argument classifiers in the pipelined ap-
proach might fail to recognize cameraman as the
Target argument for the event Attack with their lo-
cal features. The joint approach can overcome this
issue by relying on the global features to encode the
fact that a Victim argument for the Die event is often
the Target argument for the Attack event in the same
sentence.

Despite the advantages presented above, the joint
system by Li et al. (2013) suffers from the lack of
generalization over the unseen words/features and
the inability to extract the underlying structures for
EE (due to its discrete representation from the hand-
crafted feature set) (Nguyen and Grishman, 2015b;
Chen et al., 2015).

The most successful pipelined system for EE to
date (Chen et al., 2015) addresses these drawbacks
of the joint system by Li et al. (2013) via dy-
namic multi-pooling convolutional neural networks
(DMCNN). In this system, words are represented by
the continuous representations (Bengio et al., 2003;
Turian et al., 2010; Mikolov et al., 2013a) and fea-
tures are automatically learnt from data by the DM-
CNN, thereby alleviating the unseen word/feature
problem and extracting more effective features for
the given dataset. However, as the system by Chen
et al. (2015) is pipelined, it still suffers from the
inherent limitations of error propagation and failure
to exploit the inter-dependencies between event trig-
gers and argument roles (Li et al., 2013). Finally, we
notice that the discrete features, shown to be helpful
in the previous studies for EE (Li et al., 2013), are
not considered in Chen et al. (2015).

Guided by these characteristics of the EE sys-
tems by Li et al. (2013) and Chen et al. (2015),
in this work, we propose to solve the EE problem
with the joint approach via recurrent neural net-
works (RNNs) (Hochreiter and Schmidhuber, 1997;
Cho et al., 2014) augmented with the discrete fea-
tures, thus inheriting all the benefits from both sys-
tems as well as overcoming their inherent issues. To
the best of our knowledge, this is the first work to
employ neural networks to do joint EE.

Our model involves two RNNs that run over the
sentences in both forward and reverse directions to

learn a richer representation for the sentences. This
representation is then utilized to predict event trig-
gers and argument roles jointly. In order to capture
the inter-dependencies between triggers and argu-
ment roles, we introduce memory vectors/matrices
to store the prediction information during the course
of labeling the sentences.

We systematically explore various memory vec-
tor/matrices as well as different methods to learn
word representations for the joint model. The ex-
perimental results show that our system achieves
the state-of-the-art performance on the widely used
ACE 2005 dataset.

2 Event Extraction Task

We focus on the EE task of the Automatic Context
Extraction (ACE) evaluation2. ACE defines an event
as something that happens or leads to some change
of state. We employ the following terminology:
• Event mention: a phrase or sentence in which

an event occurs, including one trigger and an
arbitrary number of arguments.
• Event trigger: the main word that most clearly

expresses an event occurrence.
• Event argument: an entity mention, temporal

expression or value (e.g. Job-Title) that servers
as a participant or attribute with a specific role
in an event mention.

ACE annotates 8 types and 33 subtypes (e.g., At-
tack, Die, Start-Position) for event mentions that
also correspond to the types and subtypes of the
event triggers. Each event subtype has its own set
of roles to be filled by the event arguments. For in-
stance, the roles for the Die event include Place, Vic-
tim and Time. The total number of roles for all the
event subtypes is 36.

Given an English text document, an event extrac-
tion system needs to recognize event triggers with
specific subtypes and their corresponding arguments
with the roles for each sentence. Following the pre-
vious work (Liao and Grishman, 2011; Li et al.,
2013; Chen et al., 2015), we assume that the argu-
ment candidates (i.e, the entity mentions, temporal
expressions and values) are provided (by the ACE
annotation) to the event extraction systems.

2
http://projects.ldc.upenn.edu/ace

301



3 Model

We formalize the EE task as follow. Let W =
w1w2 . . . wn be a sentence where n is the sentence
length and wi is the i-th token. Also, let E =
e1, e2, . . . , ek be the entity mentions3 in this sen-
tence (k is the number of the entity mentions and can
be zero). Each entity mention comes with the offsets
of the head and the entity type. We further assume
that i1, i2, . . . , ik be the indexes of the last words of
the mention heads for e1, e2, . . . , ek, respectively.

In EE, for every token wi in the sentence, we need
to predict the event subtype (if any) for it. If wi is a
trigger word for some event of interest, we then need
to predict the roles (if any) that each entity mention
ej plays in such event.

The joint model for event extraction in this work
consists of two phases: (i) the encoding phase that
applies recurrent neural networks to induce a more
abstract representation of the sentence, and (ii) the
prediction phase that uses the new representation
to perform event trigger and argument role identi-
fication simultaneously for W . Figure 1 shows an
overview of the model.

3.1 Encoding
3.1.1 Sentence Encoding

In the encoding phase, we first transform each to-
ken wi into a real-valued vector xi using the con-
catenation of the following three vectors:

1. The word embedding vector of wi: This is ob-
tained by looking up a pre-trained word embedding
table D (Collobert and Weston, 2008; Turian et al.,
2010; Mikolov et al., 2013a).

2. The real-valued embedding vector for the en-
tity type of wi: This vector is motivated from the
prior work (Nguyen and Grishman, 2015b) and gen-
erated by looking up the entity type embedding table
(initialized randomly) for the entity type of wi. Note
that we also employ the BIO annotation schema to
assign entity type labels to each token in the sen-
tences using the heads of the entity mentions as do
Nguyen and Grishman (2015b).

3. The binary vector whose dimensions corre-
spond to the possible relations between words in the
dependency trees. The value at each dimension of

3From now on, when mentioning entity mentions, we al-
ways refer to the ACE entity mentions, times and values.

this vector is set to 1 only if there exists one edge
of the corresponding relation connected to wi in the
dependency tree of W . This vector represents the
dependency features that are shown to be helpful in
the previous research (Li et al., 2013).

Note that we do not use the relative position fea-
tures, unlike the prior work on neural networks for
EE (Nguyen and Grishman, 2015b; Chen et al.,
2015). The reason is we predict the whole sentences
for triggers and argument roles jointly, thus having
no fixed positions for anchoring in the sentences.

The transformation from the token wi to the
vector xi essentially converts the input sentence
W into a sequence of real-valued vectors X =
(x1, x2, . . . , xn), to be used by recurrent neural net-
works to learn a more effective representation.

3.1.2 Recurrent Neural Networks
Consider the input sequence X =

(x1, x2, . . . , xn). At each step i, we compute
the hidden vector αi based on the current input
vector xi and the previous hidden vector αi−1,
using the non-linear transformation function
Φ: αi = Φ(xi, αi−1). This recurrent compu-
tation is done over X to generate the hidden
vector sequence (α1, α2, . . . , αn), denoted by−−→
RNN(x1, x2, . . . , xn) = (α1, α2, . . . , αn).

An important characteristics of the recurrent
mechanism is that it adaptively accumulates the
context information from position 1 to i into the
hidden vector αi, making αi a rich representa-
tion. However, αi is not sufficient for the event
trigger and argument predictions at position i as
such predictions might need to rely on the con-
text information in the future (i.e, from position i
to n). In order to address this issue, we run a
second RNN in the reverse direction from Xn to
X1 to generate the second hidden vector sequence:←−−
RNN(xn, xn−1, . . . , x1) = (α′n, α′n−1, . . . , α′1) in
which α′i summarizes the context information from
position n to i. Eventually, we obtain the new
representation (h1, h2, . . . , hn) for X by concate-
nating the hidden vectors in (α1, α2, . . . , αn) and
(α′n, α′n−1, . . . , α′1): hi = [αi, α′i]. Note that hi es-
sentially encapsulates the context information over
the whole sentence (from 1 to n) with a greater fo-
cus on position i.

Regarding the non-linear function, the simplest

302



a

died
when

a
tank
fired

in
Baghdad Sentence

Encoding

Trigger

Prediction

Argument

Role

Prediction

Memory

Vectors/Matrices

word

embeddings

entity type

embeddings

depdendecy

tree relations

input sentence

indexes of trigger

and entity mention

candidates

local argument

feature generator

(Li et al., 2013)
memory matrices

hidden

vectors
word embedding

look up

feature representations

prediction outputs

X

Bidirectional

Recurrent

Neural

Network

a

man

man died when tanka fired in Baghdad

local context

vector extraction

entity mention "man"

entity mention "Baghdad"

Figure 1: The joint EE model for the input sentence “a man died when a tank fired in Baghdad” with local context
window d = 1. We only demonstrate the memory matrices Garg/trgi in this figure. Green corresponds to the trigger
candidate “died” at the current step while violet and red are for the entity mentions “man” and “Baghdad” respectively.

form of Φ in the literature considers it as a one-layer
feed-forward neural network. Unfortunately, this
function is prone to the “vanishing gradient” prob-
lem (Bengio et al., 1994), making it challenging to
train RNNs properly. This problem can be alleviated
by long-short term memory units (LSTM) (Hochre-
iter and Schmidhuber, 1997; Gers, 2001). In this
work, we use a variant of LSTM; called the Gated
Recurrent Units (GRU) from Cho et al. (2014).
GRU has been shown to achieve comparable perfor-
mance (Chung et al., 2014; Józefowicz et al., 2015).

3.2 Prediction
In order to jointly predict triggers and argument
roles for W , we maintain a binary memory vector
G

trg
i for triggers, and binary memory matrices G

arg
i

and Garg/trgi for arguments (at each time i). These
vector/matrices are set to zeros initially (i = 0) and
updated during the prediction process for W .

Given the bidirectional representation
h1, h2, . . . , hn in the encoding phase and the
initialized memory vector/matrices, the joint predic-
tion procedure loops over n tokens in the sentence
(from 1 to n). At each time step i, we perform the
following three stages in order:

(i) trigger prediction for wi.

(ii) argument role prediction for all the entity men-
tions e1, e2, . . . , ek with respect to the current
token wi.

(iii) compute Gtrgi , G
arg
i and G

arg/trg
i for the cur-

rent step using the previous memory vec-
tor/matrices Gtrgi−1, G

arg
i−1 and G

arg/trg
i−1 , and the

prediction output in the earlier stages.

The output of this process would be the pre-
dicted trigger subtype ti for wi, the predicted ar-
gument roles ai1, ai2, . . . , aik and the memory vec-
tor/matrices Gtrgi , G

arg
i and G

arg/trg
i for the current

step. Note that ti should be the event subtype ifwi is
a trigger word for some event of interest, or “Other”
in the other cases. aij , in constrast, should be the
argument role of the entity mention ej with respect
to wi if wi is a trigger word and ej is an argument
of the corresponding event, otherwise aij is set to
“Other” (j = 1 to k).

3.2.1 Trigger Prediction
In the trigger prediction stage for the current to-

ken wi, we first compute the feature representation
vector Rtrgi for wi using the concatenation of the fol-
lowing three vectors:
• hi: the hidden vector to encapsulate the global

context of the input sentence.

303



• Ltrgi : the local context vector for wi. Ltrgi is
generated by concatenating the vectors of the
words in a context window d of wi:
L

trg
i = [D[wi−d], . . . , D[wi], . . . , D[wi+d]].

• Gtrgi−1: the memory vector from the previous
step.

The representation vector Rtrgi = [hi, L
trg
i , G

trg
i−1]

is then fed into a feed-forward neural network F trg

with a softmax layer in the end to compute the prob-
ability distribution P trgi;t over the possible trigger sub-
types: P trgi;t = P

trg
i (l = t) = F

trg
t (R

trg
i ) where t is

a trigger subtype. Finally, we compute the predicted
type ti for wi by: ti = argmaxt(P

trg
i;t ).

3.2.2 Argument Role Prediction

In the argument role prediction stage, we first
check if the predicted trigger subtype ti in the previ-
ous stage is “Other” or not. If yes, we can simply set
aij to “Other” for all j = 1 to k and go to the next
stage immediately. Otherwise, we loop over the en-
tity mentions e1, e2, . . . , ek. For each entity mention
ej with the head index of ij , we predict the argument
role aij with respect to the trigger word wi using the
following procedure.

First, we generate the feature representation vec-
torRargij for ej andwi by concatenating the following
vectors:
• hi and hij : the hidden vectors to capture the

global context of the input sentence for wi and
ej , respectively.
• Largij : the local context vector for wi and ej .
L

arg
ij is the concatenation of the vectors of the

words in the context windows of size d for wi
and wij :
L

arg
ij = [D[wi−d], . . . , D[wi], . . . , D[wi+d],

D[wij−d], . . . , D[wij ], . . . , D[wij+d]].
• Bij : the hidden vector for the binary feature

vector Vij . Vij is based on the local argument
features between the tokens i and ij from (Li et
al., 2013). Bij is then computed by feeding Vij
into a feed-forward neural network F binary for
further abstraction: Bij = F binary(Vij).
• Gargi−1[j] and Garg/trgi−1 [j]: the memory vectors for
ej that are extracted out of the memory matri-
ces Gargi−1 and G

arg/trg
i−1 from the previous step.

In the next step, we again use a feed-
forward neural network F arg with a soft-

max layer in the end to transform Rargij =

[hi, hij , L
arg
ij , Bij , G

arg
i−1[j], G

arg/trg
i−1 [j]] into the prob-

ability distribution P trgij;a over the possible argument
roles: P argij;a = P

arg
ij (l = a) = F

arg
a (R

arg
ij ) where

a is an argument role. Eventually, the predicted
argument role forwi and ej is aij = argmaxa(P

arg
ij;a).

Note that the binary vector Vij enriches the fea-
ture representation Rargij for argument labeling with
the discrete structures discovered in the prior work
on feature analysis for EE (Li et al., 2013). These
features include the shortest dependency paths, the
entity types, subtypes, etc.

3.2.3 The Memory Vector/Matrices
An important characteristics of EE is the exis-

tence of the dependencies between trigger labels and
argument roles within the same sentences (Li et al.,
2013). In this work, we encode these dependen-
cies into the memory vectors/matricesGtrgi ,G

arg
i and

G
arg/trg
i (i = 0 to n) and use them as features in

the trigger and argument prediction explicitly (as
shown in the representation vectors Rtrgi and R

arg
ij

above). We classify the dependencies into the fol-
lowing three categories:

1. The dependencies among trigger subtypes:
are captured by the memory vectors Gtrgi (G

trg
i ∈

{0, 1}nT for i = 0, . . . , n, and nT is the number
of the possible trigger subtypes). At time i, Gtrgi in-
dicates which event subtypes have been recognized
before i. We obtain Gtrgi from G

trg
i−1 and the trigger

prediction output ti at time i: G
trg
i [t] = 1 if t = ti

and Gtrgi−1[t] otherwise.
A motivation for such dependencies is that if a

Die event appears somewhere in the sentences, the
possibility for the later occurrence of an Attack event
would be likely.

2. The dependencies among argument roles:
are encoded by the memory matrix Gargi (G

arg
i ∈

{0, 1}k×nA for i = 0, . . . , n, and nA is the number
of the possible argument roles). At time i,Gargi sum-
marizes the argument roles that the entity mentions
has played with some event in the past. In particular,
G

arg
i [j][a] = 1 if and only if ej has the role of a with

some event before time i. Gargi is computed from
G

arg
i−1, and the prediction outputs ti and ai1, . . . , aik

at time i: Gargi [j][a] = 1 if ti 6= “Other” and a = aij ,
and Gargi−1[j][a] otherwise (for j = 1 to k).

304



3. The dependencies between argument roles
and trigger subtypes: are encoded by the memory
matrixGarg/trgi (G

arg/trg
i ∈ {0, 1}k×nT for i = 0 to n).

At time i, Garg/trgi specifies which entity mentions
have been identified as arguments for which event
subtypes before. In particular,Garg/trgi [j][t] = 1 if and
only if ej has been detected as an argument for some
event of subtype t before i. Garg/trgi is computed from
G

arg/trg
i−1 and the trigger prediction output ti at time i:

G
arg/trg
i [j][t] = 1 if ti 6= “Other” and t = ti, and

G
arg/trg
i−1 [j][t] otherwise (for all j = 1 to k).

3.3 Training
Denote the given trigger subtypes and argument
roles forW in the training time as T = t∗1, t∗2, . . . , t∗n
and A = (a∗ij)

j=1,k
i=1,n . We train the network by min-

imizing the joint negative log-likelihood function C
for triggers and argument roles:

C(T,A,X,E) = − logP (T,A|X,E)
=− logP (T |X,E)− logP (A|T,X,E)

=−
n∑

i=1

logP trgi;t∗i

−
n∑

i=1

I(ti 6= “Other”)
k∑

j=1

logP argij;a∗ij

where I is the indicator function.
We apply the stochastic gradient descent algo-

rithm with mini-batches and the AdaDelta update
rule (Zeiler, 2012). The gradients are computed us-
ing back-propagation. During training, besides the
weight matrices, we also optimize the word and en-
tity type embedding tables to achieve the optimal
states. Finally, we rescale the weights whose Frobe-
nius norms exceed a hyperparameter (Kim, 2014;
Nguyen and Grishman, 2015a).

4 Word Representation

Following the prior work (Nguyen and Grishman,
2015b; Chen et al., 2015), we pre-train word em-
beddings from a large corpus and employ them to
initialize the word embedding table. One of the
models to train word embeddings have been pro-
posed in Mikolov et al. (2013a; 2013b) that intro-
duce two log-linear models, i.e the continuous bag-

of-words model (CBOW) and the continuous skip-
gram model (SKIP-GRAM). The CBOW model at-
tempts to predict the current word based on the av-
erage of the context word vectors while the SKIP-
GRAM model aims to predict the surrounding words
in a sentence given the current word. In this work,
besides the CBOW and SKIP-GRAM models, we
examine a concatenation-based variant of CBOW
(C-CBOW) to train word embeddings and compare
the three models to understand their effectiveness for
EE. The objective of C-CBOW is to predict the tar-
get word using the concatenation of the vectors of
the words surrounding it.

5 Experiments

5.1 Resources, Parameters and Dataset
For all the experiments below, in the encoding
phase, we use 50 dimensions for the entity type em-
beddings, 300 dimensions for the word embeddings
and 300 units in the hidden layers for the RNNs.

Regarding the prediction phase, we employ the
context window of 2 for the local features, and the
feed-forward neural networks with one hidden layer
for F trg, F arg and F binary (the size of the hidden lay-
ers are 600, 600 and 300 respectively).

Finally, for training, we use the mini-batch size =
50 and the parameter for the Frobenius norms = 3.

These parameter values are either inherited from
the prior research (Nguyen and Grishman, 2015b;
Chen et al., 2015) or selected according to the vali-
dation data.

We pre-train the word embeddings from the
English Gigaword corpus utilizing the word2vec
toolkit4 (modified to add the C-CBOW model). Fol-
lowing Baroni et al. (2014), we employ the context
window of 5, the subsampling of the frequent words
set to 1e-05 and 10 negative samples.

We evaluate the model with the ACE 2005 corpus.
For the purpose of comparison, we use the same data
split as the previous work (Ji and Grishman, 2008;
Liao and Grishman, 2010; Li et al., 2013; Nguyen
and Grishman, 2015b; Chen et al., 2015). This data
split includes 40 newswire articles (672 sentences)
for the test set, 30 other documents (836 sentences)
for the development set and 529 remaining docu-
ments (14,849 sentences) for the training set. Also,

4
https://code.google.com/p/word2vec/

305



we follow the criteria of the previous work (Ji and
Grishman, 2008; Liao and Grishman, 2010; Li et
al., 2013; Chen et al., 2015) to judge the correctness
of the predicted event mentions.

5.2 Memory Vector/Matrices

This section evaluates the effectiveness of the mem-
ory vector and matrices presented in Section 3.2.3.
In particular, we test the joint model on different
cases where the memory vector for triggers Gtrg and
the memory matrices for arguments Garg/trg and Garg

are included or excluded from the model. As there
are 4 different ways to combine Garg/trg and Garg for
argument labeling and two options to to employGtrg

or not for trigger labeling, we have 8 systems for
comparison in total. Table 1 reports the identifica-
tion and classification performance (F1 scores) for
triggers and argument roles on the development set.
Note that we are using the word embeddings trained
with the C-CBOW technique in this section.

System No Garg/trg Garg Garg/trg+Garg

No Trigger 67.9 68.0 64.6 64.2
Argument 55.6 58.1 55.2 53.1

Gtrg Trigger 63.8 61.0 61.3 66.8
Argument 55.2 56.6 54.7 53.6

Table 1: Performance of the Memory Vector/Matrices
on the development set. No means not using the memory
vector/matrices.

We observe that the memory vector Gtrg is not
helpful for the joint model as it worsens both trig-
ger and argument role performance (considering the
same choice of the memory matrices Garg/trg and
Garg (i.e, the same row in the table) and except in
the row with Garg/trg +Garg).

The clearest trend is that Garg/trg is very effective
in improving the performance of argument labeling.
This is true in both the inclusion and exclusion of
Gtrg. Garg and its combination with Garg/trg, on the
other hand, have negative effect on this task. Finally,
Garg/trg and Garg do not contribute much to the trig-
ger labeling performance in general (except in the
case where Gt, Garg/trg and Garg are all applied).

These observations suggest that the dependencies
among trigger subtypes and among argument roles
are not strong enough to be helpful for the joint
model in this dataset. This is in contrast to the de-

pendencies between argument roles and trigger sub-
types that improve the joint model significantly.

The best system corresponds to the application of
the memory matrixGarg/trg and will be used in all the
experiments below.

5.3 Word Embedding Evaluation

We investigate different techniques to obtain the pre-
trained word embeddings for initialization in the
joint model of EE. Table 2 presents the performance
(for both triggers and argument roles) on the devel-
opment set when the CBOW, SKIP-GRAM and C-
CBOW techniques are utilized to obtain word em-
beddings from the same corpus. We also report the
performance of the joint model when it is initialized
with the Word2Vec word embeddings from Mikolov
et al. (2013a; 2013b) (trained with the Skip-gram
model on Google News) (WORD2VEC). Finally,
for comparison, the performance of the random
word embeddings (RANDOM) is also included. All
of these word embeddings are updated during the
training of the model.

Word Embeddings Trigger Argument
RANDOM 59.9 50.1
SKIP-GRAM 66.7 57.1
CBOW 66.5 53.8
WORD2VEC 66.9 56.4
C-CBOW 68.0 58.1

Table 2: Performance of the Word Embedding Tech-
niques.

The first observation from the table is that RAN-
DOM is not good enough to initialize the word em-
beddings for joint EE and we need to borrow some
pre-trained word embeddings for this purpose. Sec-
ond, SKIP-GRAM, WORD2VEC and CBOW have
comparable performance on trigger labeling while
the argument labeling performance of SKIP-GRAM
and WORD2VEC is much better than that of CBOW
for the joint EE model. Third and most importantly,
among the compared word embeddings, it is clear
that C-CBOW significantly outperforms all the oth-
ers. We believe that the better performance of C-
CBOW stems from its concatenation of the multi-
ple context word vectors, thus providing more infor-
mation to learn better word embeddings than SKIP-
GRAM and WORD2VEC. In addition, the concate-

306



Model Trigger Trigger Identification Argument Argument
Identification (%) + Classification (%) Identification (%) Role (%)
P R F P R F P R F P R F

Li’s basline 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9
Liao’s cross-event† N/A 68.7 68.9 68.8 50.9 49.7 50.3 45.1 44.1 44.6
Hong’s cross-entity† N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3
Li’s structure 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
DMCNN 80.4 67.7 73.5 75.6 63.6 69.1 68.8 51.9 59.1 62.2 46.9 53.5
JRNN 68.5 75.7 71.9 66.0 73.0 69.3 61.4 64.2 62.8 54.2 56.7 55.4

Table 3: Overall Performance on the Blind Test Data. “†” designates the systems that employ the evidences beyond
sentence level.

nation mechanism essentially helps to assign differ-
ent weights to different context words, thereby be-
ing more flexible than CBOW that applies a single
weight for all the context words.

From now on, for consistency, C-CBOW would
be utilized in all the following experiments.

5.4 Comparison to the State of the art

The state-of-the-art systems for EE on the ACE 2005
dataset have been the pipelined system with dy-
namic multi-pooling convolutional neural networks
by Chen et al. (2015) (DMCNN) and the joint sys-
tem with structured prediction and various discrete
local and global features by Li et al. (2013) (Li’s
structure).

Note that the pipelined system in Chen et al.
(2015) is also the best-reported system based on
neural networks for EE. Table 3 compares these
state-of-the-art systems with the joint RNN-based
model in this work (denoted by JRNN). For com-
pleteness, we also report the performance of the fol-
lowing representative systems:

1) Li’s baseline: This is the pipelined system with
local features by Li et al. (2013).

2) Liao’s cross event: is the system by Liao and
Grishman (2010) with the document-level informa-
tion.

3) Hong’s cross-entity (Hong et al., 2011): This
system exploits the cross-entity inference, and is
also the best-reported pipelined system with discrete
features in the literature.

From the table, we see that JRNN achieves the
best F1 scores (for both trigger and argument la-
beling) among all of the compared models. This
is significant with the argument role labeling per-

formance (an improvement of 1.9% over the best-
reported model DMCNN in Chen et al. (2015)),
and demonstrates the benefit of the joint model with
RNNs and memory features in this work. In ad-
dition, as JRNN significantly outperforms the joint
model with discrete features in Li et al. (2013) (an
improvement of 1.8% and 2.7% for trigger and ar-
gument role labeling respectively), we can confirm
the effectiveness of RNNs to learn effective feature
representations for EE.

5.5 Sentences with Multiple Events

In order to further prove the effectiveness of JRNN,
especially for those sentences with multiple events,
we divide the test data into two parts according to
the number of events in the sentences (i.e, single
event and multiple events) and evaluate the perfor-
mance separately, following Chen et al. (2015). Ta-
ble 4 shows the performance (F1 scores) of JRNN,
DMCNN and two other baseline systems, named
Embeddings+T and CNN in Chen et al. (2015).
Embeddings+T uses word embeddings and the tra-
ditional sentence-level features in (Li et al., 2013)
while CNN is similar to DMCNN, except that it ap-
plies the standard pooling mechanism instead of the
dynamic multi-pooling method (Chen et al., 2015).

The most important observation from the table is
that JRNN significantly outperforms all the other
methods with large margins when the input sen-
tences contain more than one events (i.e, the row la-
beled with 1/N in the table). In particular, JRNN
is 13.9% better than DMCNN on trigger labeling
while the corresponding improvement for argument
role labeling is 6.5%, thereby further suggesting the
benefit of JRNN with the memory features. Regard-

307



Stage Model 1/1 1/N all
Embedding+T 68.1 25.5 59.8

Trigger CNN 72.5 43.1 66.3
DMCNN 74.3 50.9 69.1

JRNN 75.6 64.8 69.3
Embedding+T 37.4 15.5 32.6

Argument CNN 51.6 36.6 48.9
DMCNN 54.6 48.7 53.5

JRNN 50.0 55.2 55.4
Table 4: System Performance on Single Event Sentences
(1/1) and Multiple Event Sentences (1/N).

ing the performance on the single event sentences,
JRNN is still the best system on trigger labeling al-
though it is worse than DMCNN on argument role
labeling. This can be partly explained by the fact
that DMCNN includes the position embedding fea-
tures for arguments and the memory matrix Garg/trg

in JRNN is not functioning in this single event case.

6 Related Work

Early research on event extraction has primarily fo-
cused on local sentence-level representations in a
pipelined architecture (Grishman et al., 2005; Ahn,
2006). After that, higher level features has been in-
vestigated to improve the performance (Ji and Gr-
ishman, 2008; Gupta and Ji, 2009; Patwardhan and
Riloff, 2009; Liao and Grishman, 2010; Liao and
Grishman, 2011; Hong et al., 2011; McClosky et al.,
2011; Huang and Riloff, 2012; Li et al., 2013). Be-
sides, some recent research has proposed joint mod-
els for EE, including the methods based on Markov
Logic Networks (Riedel et al., 2009; Poon and Van-
derwende, 2010; Venugopal et al., 2014), structured
perceptron (Li et al., 2013; Li et al., 2014b), and dual
decomposition (Riedel et al. (2009; 2011a; 2011b)).

The application of neural networks to EE is very
recent. In particular, Nguyen and Grishman (2015b)
study domain adaptation and event detection via
CNNs while Chen et al. (2015) apply dynamic
multi-pooling CNNs for EE in a pipelined frame-
work. However, none of these work utilizes RNNs
to perform joint EE as we do in this work.

7 Conclusion

We present a joint model to do EE based on bidirec-
tional RNN to overcome the limitation of the previ-

ous models for this task. We introduce the memory
matrix that can effectively capture the dependencies
between argument roles and trigger subtypes. We
demonstrate that the concatenation-based variant of
the CBOW word embeddings is very helpful for the
joint model. The proposed joint model is empiri-
cally shown to be effective on the sentences with
multiple events as well as yields the state-of-the-art
performance on the ACE 2005 dataset. In the fu-
ture, we plan to apply this joint model on the event
argument extraction task of the KBP evaluation as
well as extend it to other joint tasks such as mention
detection together with relation extraction etc.

References
David Ahn. 2006. The stages of event extraction. In

Proceedings of the Workshop on Annotating and Rea-
soning about Time and Events.

Marco Baroni, Georgiana Dinu, and Germán Kruszewski.
2014. Don’t count, predict! a systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In ACL.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradient
descent is difficult. In Journal of Machine Learning
Research 3.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. In Journal of Machine Learning Re-
search 3.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and
Jun Zhao. 2015. Event extraction via dynamic
multi-pooling convolutional neural networks. In ACL-
IJCNLP.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning phrase represen-
tations using rnn encoder–decoder for statistical ma-
chine translation. In EMNLP.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In arXiv preprint arXiv:1412.3555.

Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In ICML.

Felix Gers. 2001. Long short-term memory in recurrent
neural networks. In PhD Thesis.

Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyus english ace 2005 system description. In
ACE 2005 Evaluation Workshop.

308



Prashant Gupta and Heng Ji. 2009. Predicting unknown
time arguments based on cross-event propagation. In
ACL-IJCNLP.

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long
short-term memory. In Neural Computation.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction. In
ACL.

Ruihong Huang and Ellen Riloff. 2012. Modeling tex-
tual cohesion for event extraction. In AAAI.

Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In ACL.

Rafal Józefowicz, Wojciech Zaremba, and Ilya Sutskever.
2015. An empirical exploration of recurrent network
architectures. In ICML.

Yoon Kim. 2014. Convolutional neural networks for sen-
tence classification. In EMNLP.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event ex-
traction via structured prediction with global features.
In ACL.

Qi Li, Heng Ji, Yu Hong, and Sujian Li. 2014b.
Constructing information networks using one single
model. In EMNLP.

Shasha Liao and Ralph Grishman. 2010. Using docu-
ment level cross-event inference to improve event ex-
traction. In ACL.

Shasha Liao and Ralph Grishman. 2011. Acquiring topic
features to improve event extraction: in pre-selected
and balanced collections. In RANLP.

David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event extraction as dependency parsing.
In BioNLP Shared Task Workshop.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. In ICLR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013b. Distributed representations
of words and phrases and their compositionality. In
NIPS.

Thien Huu Nguyen and Ralph Grishman. 2015a. Rela-
tion extraction: Perspective from convolutional neural
networks. In The NAACL Workshop on Vector Space
Modeling for NLP (VSM).

Thien Huu Nguyen and Ralph Grishman. 2015b. Event
detection and domain adaptation with convolutional
neural networks. In ACL-IJCNLP.

Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In EMNLP.

Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In NAACL-HLT.

Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In EMNLP.

Sebastian Riedel and Andrew McCallum. 2011b. Robust
biomedical event extraction with dual decomposition
and minimal domain adaptation. In BioNLP Shared
Task 2011 Workshop.

Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In BioNLP 2009
Workshop.

Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL.

Deepak Venugopal, Chen Chen, Vibhav Gogate, and Vin-
cent Ng. 2014. Relieving the computational bottle-
neck: Joint inference for event extraction with high-
dimensional features. In EMNLP.

Matthew D. Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method. In CoRR, abs/1212.5701.

309


