



















































A Neural Graph-based Approach to Verbal MWE Identification


Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 114–124
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

114

A Neural Graph-based Approach to Verbal MWE Identification

Jakub Waszczuk & Rafael Ehren & Regina Stodden & Laura Kallmeyer
Heinrich Heine University

Düsseldorf, Germany
(waszczuk|ehren|stodden|kallmeyer)@phil.hhu.de

Abstract

We propose to tackle the problem of ver-
bal multiword expression (VMWE) identi-
fication using a neural graph parsing-based
approach. Our solution involves encoding
VMWE annotations as labellings of depen-
dency trees and, subsequently, applying a neu-
ral network to model the probabilities of dif-
ferent labellings. This strategy can be par-
ticularly effective when applied to discontinu-
ous VMWEs and, thanks to dense, pre-trained
word vector representations, VMWEs unseen
during training. Evaluation of our approach
on three PARSEME datasets (German, French,
and Polish) shows that it allows to achieve per-
formance on par with the previous state-of-
the-art (Al Saied et al., 2018).

1 Introduction

Multiword expressions (MWEs) are defined as
combinations of multiple lexemes whose overall
properties are not readily predictable by those of
their components (Baldwin and Kim, 2010). This
idiosyncrasy makes MWEs a well-known chal-
lenge for NLP and their ubiquity forces us to find
ways to account for them. While all types of
MWEs come with their own set of issues, verbal
MWEs (VMWEs) stand out as a particularly chal-
lenging subclass because of properties like discon-
tinuity, overlap, varying word order, and syntac-
tic or semantic ambiguity. These properties sug-
gest that we have to rely on both syntactic and se-
mantic features to successfully process VMWEs
(Savary et al., 2017). E.g., syntactic informa-
tion can help us catch long-distance dependen-
cies, while semantic information can prove useful
in disambiguating between literal and idiomatic
readings.

One of the main tasks that constitute MWE pro-
cessing is the automatic identification of MWEs

in running text which can be used as a preprocess-
ing step for parsing or machine translation. MWE
identification can be seen as a sequence labeling
task similar to named entity recognition (NER): A
system receives sequences of tokens as input and
outputs the same sequences with annotation labels
added to it (Constant et al., 2017). As in NER,
most parts of the sequence will belong to the neg-
ative class, that is, the majority of words is not part
of an MWE. However, certain issues that occur
when dealing with NEs and MWEs are much more
prevalent in case of the latter. Especially with re-
spect to discontinuity. In the PARSEME 1.0 cor-
pus (Savary et al., 2018), wich comprises datasets
of 18 languages, only three of them have a conti-
nuity rate of over 80% when it comes to VMWEs.
German, the most striking example in this regard,
has a continuity rate of 35.7% and 30.54% of its
discontinuities are longer than three words (Savary
et al., 2017). In (1), the verb-particle construction
teilnehmen ‘take part’ spans over 13 words and
this is not even a particularly excessive example.
Much more could be inserted in between the two
VMWE components nahmen and teil, e.g. a rela-
tive clause, without it sounding marked.

(1) [In]
in

Paris
Paris

selbst
itself

nahmen
took

zur
at the

gleichen
same

Zeit
time

rund
roughly

tausend
thousand

Studenten
students

an
in

einer
a

Kundgebung
rally

in
in

dem
the

Quartier
Quartier

Latin
Latin

teil.
part.

‘In Paris itself roughly a thousand students
took part in a rally in the Quartier Latin.’1

In this paper, we propose a method which iden-
tifies VMWE occurrences directly over depen-
dency structures. Relying on existing dependency
trees greatly simplifies the task, since VMWEs

1From the German set of the PARSEME Shared Task 1.1.



115

are usually connected in such trees (Bejček et al.,
2012), even if they are discontinuous on the sur-
face of word sequences as in (1).

In the same vein as (Waszczuk, 2018), our
method is conceptually divided into two layers.
The first is concerned with encoding VMWE oc-
currences as tree labellings, as well as the inverse
process of decoding the labellings into VMWE an-
notations. In the second layer, a probability model
which allows to discriminate between different
VMWE labellings is used. We propose two prob-
ability models, both based on dense feature repre-
sentations (i.e. pre-trained word embeddings) of
input words. Relying on dense features allows to
easier generalize beyond training data, on the one
hand, and to possibly capture helpful syntactic and
semantic cues, on the other hand.2

The paper is structured as follows. In Sec. 2, we
describe related work on VMWE identification. In
Sec. 3, we give a detailed description of our meth-
ods, in particular the encoding schemata and the
labelling models. In Sec. 4, we summarize the ex-
periments we performed to evaluate our approach.
Finally, we conclude and mention possible future
work in Sec. 5.

2 Related Work

The MWE identification strategies can be broadly
divided into approaches based on deep learning,
sequence labelling, and parsing-based methods.
Because all of these have different advantages
(and are not necessarily mutually exclusive), some
systems also pursue a mix of these approaches,
e.g., the system proposed here uses both (graph-
based) parsing and deep learning methods.

Gharbieh et al. (2017) were one of the first to
apply deep learning to MWE identification. They
tested different network architectures, e.g., a lay-
ered feed-forward network and a recurrent neu-
ral network, and all of them outperformed more
traditional MWE identification methods. The ap-
proaches based on deep learning have the advan-
tage that they can easily leverage pre-trained word
vectors as features (Constant et al., 2017; Taslim-
ipoor and Rohanian, 2018; Ehren et al., 2018). The
method described in this work also relies on pre-
trained word vectors.

Schneider et al. (2014) addressed the task of
MWE identification as a sequence labeling prob-

2Implementation of the methods presented in this work
can be found at https://github.com/kawu/vine.

lem. They proposed a sequence labeling scheme
(IiOoBb) which allows to represent discontinuous
MWEs as well as nested MWEs. The encoding
methods we propose also allow to handle discon-
tinuous and, to a certain degree, nested MWEs,
with the important difference that they apply to
trees rather than sequences.

Previous work on applying parsing-based tech-
niques to MWE identification includes transition-
based (Constant and Nivre, 2016; Al Saied et al.,
2018; Stodden et al., 2018) and graph-based
(Waszczuk, 2018; Boroş and Burtica, 2018) ap-
proaches. As shown by the two PARSEME shared
tasks (Savary et al., 2017; Ramisch et al., 2018a),
both strategies can be very effective, even without
relying on pre-trained word vectors. The method
we propose is graph-based and it resembles the
one of Waszczuk (2018) in that it relies on global
modelling and restricts the labelling decisions to
dependency fragments, and the one of Boroş and
Burtica (2018) in that it relies on a neural archi-
tecture. In comparison with the former, the encod-
ing schemata we propose allow to deal with two
important phenomena the method of (Waszczuk,
2018) could not handle – adjacent and discon-
nected MWE occurrences.

Another way to classify MWE identification ap-
proaches is based on whether the process of MWE
prediction takes place before, during, or after (syn-
tactic and/or semantic) parsing. The joint solu-
tion is typically considered as most promising in
that it can potentially improve both MWE identi-
fication and parsing results (Constant and Nivre,
2016; Le Roux et al., 2014; Nasr et al., 2015;
Simkó et al., 2018). On this scale, our method
clearly fits into the family of post-processing ap-
proaches, since it requires the dependency trees on
input. Nevertheless, it should be straightforward
to extend it to a fully joint solution, notably due to
its similarities with the graph-based, arc-factored,
neural dependency parsing architecture of (Dozat
and Manning, 2017).

3 Methods

In this section we describe the methods and mod-
els used in the proposed MWE identification ap-
proach. In Sec. 3.1, we introduce some basic def-
initions. In Sec. 3.2, we detail the methods of en-
coding MWE occurrences as tree labellings. This
allows to reduce the problem of MWE identifi-
cation to the problem of determining the best la-

https://github.com/kawu/vine


116

belling of the given (dependency) tree. We pro-
pose two solutions to the latter problem, both de-
scribed in Sec. 3.3.

3.1 Basic Definitions

Input Sentence. We define an input sentence of
length n as a sequence w = (wi ∈ Rd)ni=1 of
vector representations (with dimension d) corre-
sponding to the subsequent input words. The indi-
vidual vectors wi can be simply defined as input
word embeddings, but they can also be the result
of preliminary processing (e.g., concatenating the
input word embeddings with hidden POS repre-
sentations).

Dependency Tree. We define a dependency
tree as a directed rooted tree G = (V,E), where
V is a set of nodes and E ⊂ V ×V is a set of arcs.
Given (v, w) ∈ E, we say that w is v’s head and
that v isw’s dependent. For simplicity, we blur the
distinction between dependency nodes and word
identifiers and assume that V = {0, 1, 2, . . . , n},
with 0 representing a dummy root node. We addi-
tionally define inc(i) = {h ∈ V | (h, i) ∈ E} and
out(i) = {j ∈ V | (i, j) ∈ E}.

3.2 Encoding

Our methodology relies fundamentally on the idea
of encoding MWE occurrences as tree labellings.
More precisely, given a sentence and the corre-
sponding dependency tree, the set of MWE occur-
rences present therein is encoded as a labelling of
dependency arcs and nodes. A machine learning
method is used to model the probabilities of differ-
ent labellings, which it learns based on a training
dataset of encoded MWE annotations. MWE iden-
tification requires a reverse procedure of decoding
a given labelling to the set of MWE occurrences.

MWE Occurrence. Each MWE occurrence is
represented as a set of tokens in a particular sen-
tence. We are thus not concerned with determining
the category of a MWE occurrence (in our experi-
ments we train one model per MWE category).

3.2.1 Basic Encoding
In the basic encoding scheme, we assume that ele-
ments of a single MWE occurrence are connected
by dependency arcs. The set of MWE occurrences
in a sentence with a given dependency tree is en-
coded as a single labelling function `E : E → B
defined over the set of arcsE ⊂ V ×V of the tree.

Encoding. `E(v, w) := 1 for a given arc
(v, w) ∈ E iff both v and w belong to a single

Dali im cynk , że nie ma co wychodzić
Gave them zink , that no has what leave

Figure 1: Example of extended encoding applied to
a tree fragment with two Polish idioms, dać komuś
cynk ‘give someone a tip‘ and nie ma co [wychodzić]
‘it is not worth [leaving]‘, adjacent in the dependency
tree. The nodes and arcs labelled with 1 are marked in
bold. The example (simplified) comes from the Polish
dataset of the PARSEME Shared Task 1.1.

MWE occurrence.
Decoding is a two-stage process. First, a copy

of the dependency tree is created in which only
the arcs (v, w) ∈ E such that `E(v, w) = 1 are
preserved. Next, each connected component3 in
the copy of the tree is considered to represent a
distinct MWE occurrence.

Limitations. The basic encoding scheme does
not handle single-token, disconnected, or overlap-
ping MWE occurrences. In the process of encod-
ing, both single-token and disconnected MWE oc-
currences get either discarded or trimmed. Over-
lapping MWE occurrences, on the other hand, get
merged.

3.2.2 Extended Encoding
In the extended encoding scheme, the set of MWE
occurrences is encoded as a pair of labelling func-
tions `V : V → B and `E : E → B.

Encoding. `V (v) := 1 for a given node v ∈ V
iff v is a part of a MWE. `E(v, w) := 1 iff both
v and w belong to the shortest, undirected path
between (any) two component nodes of a single
MWE occurrence.

Thanks to node labelling, the extended encod-
ing scheme allows to represent single-token MWE
occurrences. Arc labelling, on the other hand, fa-
cilitates demarcating adjacent MWE occurrences
(see Fig. 1). Finally, using a hybrid (node and arc)
labelling allows to represent disconnected MWE
occurrences (see Fig. 2).

Decoding. As in the basic encoding scheme,
a copy of the dependency tree consisting of arcs
labelled with 1 is created first. To accommodate
for single-node MWE occurrences, the set of arcs
in this copy is further enriched with {(v, v) : v ∈

3Formally, a connected component is a set of nodes C ⊂
V such that every two nodes in C are connected by an undi-
rected path.



117

La perfusion doit être éffectué . . .
The perfusion must be done . . .

Figure 2: Example of extended encoding applied to
a tree fragment with a disconnected French light-verb
construction. The nodes and arcs labelled with 1 are
marked in bold. The example (simplified) comes from
the French dataset of the PARSEME Shared Task 1.1.

V, `V (v)}. Finally, each connected component in
the resulting structure is considered as a distinct
MWE. However, given a particular MWE compo-
nent C, only the nodes v ∈ C such that `V (v) = 1
are marked as the MWE’s elements.

Node and arc labellings can be, in general,
inconsistent. An example is a labelling with
`E(v, w) = 1 for some (v, w) ∈ E and `V (v) = 0
for every v ∈ V . Determining an optimal, con-
sistent labelling for a given sentence is therefore a
problem of structured prediction.

Limitations. While the extended encoding
scheme is more powerful than the basic one, it still
cannot deal with certain phenomena. A notable
limitation is the inability to represent overlapping
MWE occurrences. Another, more practical draw-
back is that encoding two MWE occurrence com-
ponents placed far from each other in the depen-
dency tree entails labelling the entire list of arcs in
between (not necessarily related to the MWE) with
1’s. Such a situation can in particular occur when
the dependency structure is obtained automatically
in pre-processing. Joint modelling of dependency
structures and MWEs (to which, we believe, our
work can be extended) would in principle allevi-
ate this issue.

3.3 Labelling

We consider two labelling models in this work.
The first, local model (see Sec. 3.3.2) relies on
the basic encoding scheme (see Sec. 3.2.1) and as-
sumes independence between the labelling deci-
sions for the individual dependency arcs. The sec-
ond, global model (described in Sec. 3.3.3) adopts
the extended encoding scheme (see Sec. 3.2.2) and
relaxes the independence assumptions of the first
model.

3.3.1 Notation
Given two vectors x ∈ Rn and y ∈ Rm, we use
[x;y] ∈ Rn+m to denote the vector concatenation

of x and y.
The definitions provided below are set in the

context of a specific input sentencew and the cor-
responding dependency tree G = (V,E). These
should be understood as implicit arguments of the
individual functions defined below.

3.3.2 Local Model
Score. We define the score vector Φ(i, j) ∈ R2 of
the dependency arc (i, j) ∈ E as:

Φ(i, j) = MLP([wi;wj ]), (1)

where MLP is a feed-forward network with a sin-
gle hidden layer followed by a leaky rectifier and
an output layer with two units. The two output
scores represent the arc’s affinity of not being and
being labelled as a MWE component, respectively.

Probability. We define the probability distribu-
tion P (`E(i, j) | w, G) based on the scores of the
arc (i, j) using SOFTMAX:

P (`E(i, j) | w, G) = SOFTMAX(Φ(i, j)) (2)

Prediction. To determine a most probable
labelling for a given sentence, we rely on the
adopted independence assumption and set the out-
put label `E(i, j) to 1 iff P (`E(i, j) = 1 |
w, G) > α for each (i, j) ∈ E separately, where α
is a threshold over which an arc is considered as a
MWE. In the rest of this paper, we simply assume
α = 0.5.

3.3.3 Global Model
The second labelling model we consider in this
work is a global model in which the score is as-
signed to the labelling of the entire dependency
tree. This model is based on the extended encod-
ing scheme (see Sec. 3.2.2) and it is concerned
with both node and arc labellings.

Compound Labels. We introduce a compound
labelling function ` : E → B3 as:

`(i, j) = (`V (i), `E(i, j), `V (j)) (3)

This function combines the label of the given
arc with the labels of the arc’s source and target
nodes. Modelling ` enables making predictions
about both node and arc labellings and, conse-
quently, handling the extended encoding scheme.
Moreover, node labels are shared between dif-
ferent compound labels, which allows to capture
inter-label interactions and enables global scoring.



118

To facilitate the interpretation of compound la-
bels, for a given x = `(i, j) we denote xdep =
`V (i), xarc = `E(i, j), and xhed = `V (j).

Node Score. We define the node score φV (i) ∈
R of the dependency node i ∈ V as:

φV (i) = MLP′(wi)1, (4)

where MLP′ is a feed-forward network with a sin-
gle hidden layer followed by a leaky rectifier and
a single-unit output layer (with the resulting value
accessed via 1). The score represents the node’s
affinity of being labeled as a MWE component.

Arc Score. We define the arc score φE(i, j) ∈
R8 of the arc (i, j) ∈ E as an 8-element vector
whose individual values correspond to the scores
of the different `(i, j) labelling combinations. Put
differently, there are 8 different ways of labelling
i, j, and (i, j), and φE(i, j) provides the score for
each of these 8 possibilities.

The score vector φE(i, j) is calculated using
a network consisting of a single hidden layer
and two output layers. The output layers con-
tain 8 and 3 units, respectively. The 8 elements
of the first output vector γ ∈ R8 correspond
to the scores of the different labelling combina-
tions. The 3 elements of the second output vec-
tor δ = (δ1, δ2, δ3) ∈ R3 correspond to the
scores of (i) labelling i with 1, (ii) labelling (i, j)
with 1, and (iii) labelling j with 1, respectively.
These scores are combined to produce the fi-
nal score vector using the binary masks m1 =
(0, 0, 0, 0, 1, 1, 1, 1), m2 = (0, 0, 1, 1, 0, 0, 1, 1),
andm3 = (0, 1, 0, 1, 0, 1, 0, 1):

φE(i, j) = γ + δ1m1 + δ2m2 + δ3m3 (5)

For instance, δ1m1 is the scalar multiplication of
the mask m1 by the 1-st element of the vector δ,
i.e., δ1m1 = (0, 0, 0, 0, δ1, δ1, δ1, δ1). Hence, δ1
impacts (equally) all the 4 elements of φE(i, j)
which correspond to labelling the node i with 1.

The motivation behind using both γ and δ is
that it can be useful to look at the nodes i, j and the
arc (i, j) both jointly and separately, depending on
the situation. For instance, if the arc (i, j) belongs
to a single MWE, than the score of labelling both
i and j with 1 should be high, while the score
of labelling only one of them with 1 should be
low. This relation can be easily expressed with
γ. Conversely, δ may be better in expressing the
pattern where one of the nodes i, j can be easily
pinpointed as a MWE element, while the other is

hard to judge without looking at the other neigh-
boring arcs.

Formally, given the input vector representation
[wi;wj ] of the arc (i, j) ∈ E, the hidden vector
and the two output vectors are calculated as fol-
lows:

h = σ(A1[wi;wj ] + b1) (6a)

γ = A2h+ b2 (6b)

δ = A3h+ b3, (6c)

where A1,A2,A3 are matrices, b1, b2, b3 are the
corresponding bias vectors, and σ is the element-
wise activation function (leaky rectifier).

Arc Scoring Restrictions. We additionally fix
the arc score of each compound label (x, 0, y) for
any x, y ∈ B to 0. In practice, this means that the
non-MWE nodes surrounding a MWE candidate
do not influence the choice of marking this can-
didate as a MWE. This allows to avoid overfitting
which could result from relying too much on the
surrounding, non-MWE words.

Casting. The set of compound labels B3 and
the set of score vector indices {1, 2, . . . , 8} are iso-
morphic. In a slight abuse of notation, we there-
fore treat elements of B3 and {1, 2, . . . , 8} inter-
changeably, assuming implicit cast between the
objects of these two types.

Local Score. We define the local score vector
φ(i, j) ∈ R8 for a given arc (i, j) ∈ E as the sum
of the (i, j)’s arc score and the i’s node score:

φ(i, j)x = φV (i)xdep + φE(i, j)x, (7)

where x ∈ {1, 2, . . . , 8} represents a particular
compound label, φV (i)xdep is the node score of
the dependent, activated only if it is labelled with
1 (xdep = 1), and φE(i, j)x is the arc score of the
compound label x with respect to the arc (i, j).

Global Score. The score Φ(`) ∈ R of a given
labelling ` of the entire tree is defined as the sum
of the local scores implied by the global labelling:

Φ(`) =
∑

(i,j)∈E
φ(i, j)`(i,j) (8)

Since we assume the dependency structure to be a
tree, summing the local scores of all the arcs in the
graph is equivalent to summing the node scores of
all the nodes and the arc scores of all the arcs in
the tree, modulo the dummy root node.4

4For simplicity, we assume that the node score of the
dummy root is 0. In practice, either this score needs to be
explicitely handled, or labelling the root with 1 should be
prohibited (the dummy root cannot be a part of a MWE).



119

Probability. We define the probability of a par-
ticular compound labelling ` as:

P (` | w, G) = exp(Φ(`))∑
`′ exp(Φ(`

′))
, (9)

where the calculation of Z =
∑

`′ exp(Φ(`
′)),

the so called partition function (Goldberg, 2017,
p. 224), involves summing over all the possible
compound labellings of the given tree.5 The global
model is therefore an instance of a log-linear
model which combines the scores determined by
the non-linear MLP component. A similar solu-
tion can be found in (Durrett and Klein, 2015).

Due to the arc-factored nature of the model, it
is possible to calculate P (` | w, G) – in particu-
lar, the partition Z – efficiently, without summing
over the exponentially many labellings. This can
be done using a variant of the standard inside algo-
rithm, which can be specified using the following
recursive function inside : V × B→ R:

inside(j, x) =
∏

i∈inc(j)

∑
y∈B3 : yhed=x

exp(φ(i, j)y)× inside(i, ydep) (10)

The partition factor Z is then equal to
inside(r, 0) + inside(r, 1), where r is the
root of the dependency tree.

Prediction involves determining a highest-
probability (see Eq. 9) or, equivalently, a highest-
score (see Eq. 8) labelling. This can be achieved
using a variant of the inside algorithm:

g(j, x) =
∑

i∈inc(j)

max
y : yhed=x

φ(i, j)y + g(i, ydep)

Constraints. Any labelling consistent with the
extended encoding scheme (see Sec. 3.2.2) must
satisfy the property that, if a node i is labeled with
0, then either zero or more than one arc among
{(h, i) | h ∈ inc(i)} ∪ {(i, j) | j ∈ out(i)} is
labelled with 1. Put differently, all the nodes on
the border of a given MWE occurrence must be
marked as its elements.

While this constraint is not directly reflected in
the definition of the probability (see Eq. 9), we use
it in our implementation of the global model for
both prediction and (optionally) training.

4 Experiments

We now describe the experiments we performed in
order to evaluate the methods described in Sec. 3.

5The labellings must be internally consistent, i.e., com-
pound labels must agree on the labels of the nodes they share.

4.1 Dataset

All the experiments were run on the German (DE),
French (FR), and Polish (PL) datasets of the edi-
tion 1.1 of the PARSEME corpus (Ramisch et al.,
2018b). This highly multilingual corpus was cre-
ated in the context of a shared task on the au-
tomatic identification of VMWEs and consists
of annotated datasets of 20 different languages.
The individual datasets are collections of sen-
tences which were, among other things, tokenized,
part-of-speech (POS) tagged, lemmatized, and en-
riched with dependency information. While FR
contains solely manual dependency annotations,
the dependencies in DE were annotated partly
manually and partly automatically. Besides auto-
matic annotations, PL includes dependencies that
were converted from a manually annotated con-
stituency treebank.

The VMWE annotation comprises the identi-
fication of the words that belong to a VMWE
instance, as well as the categorization of the
identified instances. The categories used in
the PARSEME annotation framework are light-
verb constructions (LVCs), verbal idioms (VIDs),
inherently reflexive verbs (IRV), verb-particle
constructions (VPC), multi-verb constructions
(MVC), and inherently adpositional verbs (IAV).

Our implementation of the global model is cur-
rently a prototype and it takes a relatively long
time to train a model.6 We therefore focused on
a few languages which come from different fami-
lies and cover a large spectrum of VMWE-related
phenomena. This way, we hope we can test our
system on a variety of problems despite the small
number of languages. For instance, DE contains a
large amount of VPCs, a verb class very common
in Germanic, but almost non-existent in Romance
or Slavic languages. These VPCs also account for
most of the single-token VMWEs in DE which do
not occur in FR or PL. The Polish dataset covers
a reasonable amount of IAVs,7 which are rather
challenging for our models because of their lack
of connectivity in the dependency structures.

4.2 Pre-processing

The first pre-processing step we used in our ex-
periments involved removing (multiword) tokens,

6Around 16 hours were required to train a single global
model on the 220465 tokens of the Polish dataset using four
cores of a 2.40GHz Xeon E5-2630 machine.

7IAVs were an experimental PARSEME category and a
lot of languages in the PARSEME corpus do not cover them.



120

such as the contraction du of de le ‘of the‘ in
French, from consideration.8 In the PARSEME
datasets, only the expanded forms (i.e., de le rather
than du) are annotated at the level of dependency
structures and VMWEs.

The second pre-processing operation consisted
in adding a dummy root node (with special POS
and dependency relation values) to each depen-
dency structure, to enforce that it is actually a tree
(as required by the global model, see Sec. 3.3.3).
This was particularly important for the German
dataset, in which some of the dependency struc-
tures did not satisfy this property.

The two previous steps are carried out auto-
matically by our VMWE identification systems.
However, we also performed one full-fledged pre-
processing operation of adding the missing lem-
mas9 in the French test set. Even though having no
impact on the results of the proposed systems, this
step was necessary to obtain reliable comparison
with the benchmark system (see Sec. 4.3), config-
ured to use lemma information in case of French.

4.3 Benchmark System
As a benchmark, we use the system of Al Saied
et al. (2018), henceforth called ATILF, a
transition-based tagger relying on support vector
machines and hand-crafted features for classifi-
cation. The hand-crafted features are separately
specified for each language. ATILF addresses sev-
eral VMWE challenges at the same time – it is able
to handle single-token, discontinuous, nested, and
(some forms of) overlapping VMWEs. Without
relying on word embeddings or any other exter-
nal resources, the benchmark system yields state-
of-the-art results on the PARSEME corpus 1.1
(Taslimipoor and Rohanian, 2018).10

4.4 System Implementation
In this subsection we detail the implementation of
the proposed systems.

4.4.1 Input
To each word in the input sentence a vector repre-
sentation is assigned. This representation consists

8The discarded tokens are restored after identification in
order to allow for comparison with gold data.

9Provided by the shared task’s organizers via http://
groups.google.com/group/verbalmwe.

10ATILF was originally developed for the edition 1.0 of the
PARSEME shared task. We therefore converted the relevant
files of the PARSEME corpus 1.1 to the format supported by
the tool. We used the published, default feature configura-
tions for the individual languages.

of the concatenation of the corresponding (i) Fast-
Text word embedding (Mikolov et al., 2018), (ii)
POS embedding, and (iii) dependency label em-
bedding. The latter correspond to the dependency
label of the arc connecting the word with its de-
pendency head. The size of the FastText word em-
beddings is 300. We chose the size of 25 for both
POS and dependency embeddings, which should
be sufficient given the small number of values they
can take. The POS and dependency label em-
bedding vectors are both learned during training,
while the FastText embeddings are kept intact.

4.4.2 Network Dimensions
In both labelling models, the size of the network’s
input layer is determined by the size of the input
vector representations. All the scoring networks
contain one hidden layer with 200 units, followed
by element-wise leaky ReLU. The size of the hid-
den layer was chosen during preliminary experi-
ments on the French dataset.

4.4.3 Training
Objective. For both labelling models consid-
ered in this work we define the training objective
for a given tree (V,E) as the sum of the cross-
entropies between the target and the estimated dis-
tributions for the individual arcs (i, j) ∈ E. In
case of the local model the arc labelling distribu-
tions P (`E(i, j) | w, G) (see Eq. 2) are used, and
in case of the global model – the marginal com-
pound labelling distributions P (`(i, j) | w, G).
The marginal distributions can be defined in terms
of the global probability (see Eq. 9) and calculated
efficiently using the inside-outside algorithm.

Backpropagation. In order to use P (`(i, j) |
w, G) as a part of the training objective, the
inside-outside algorithm needs to be specified in
a backpropagation-enabled way. We achieve this
by using a library11 which automatically deter-
mines the way to backpropagate the gradient from
the output to the input of the inside-outside algo-
rithm. Conveniently, this requires no changes in
the structure of the algorithm itself. This is similar
to how the inside algorithm can be extended with
its outside counterpart automatically, using auto-
matic differentiation (Eisner, 2016).

Optimization. We used stochastic gradient de-
scent (SGD) to train the models for the individual
datasets and VMWE categories. We used mini-

11https://backprop.jle.im/

http://groups.google.com/group/verbalmwe
http://groups.google.com/group/verbalmwe
https://backprop.jle.im/


121

DE FR PL AVG
P R F P R F P R F P R F

ATILF MWE-based 71.56 46.71 56.52 82.69 71.38 76.62 85.23 68.35 75.86 79.82 62.15 69.67Tok-based 76.43 45.72 57.21 85.73 72.96 78.83 88.69 67.9 76.92 83.61 62.19 70.99

Local MWE-based 49.64 27.15 35.10 71.04 62.08 66.67 75.54 53.98 62.97 65.41 47.98 55.36Tok-based 68.22 39.78 50.25 80.03 68.12 73.60 79.45 54.37 64.56 75.90 54.09 63.17

Global MWE-based 68.48 47.70 56.24 84.92 70.75 77.19 80.83 64.66 71.84 78.08 61.04 68.52Tok-based 72.74 47.83 57.72 86.84 73.24 79.47 83.13 66.19 73.69 80.90 62.42 70.47

Table 1: General results per language and system on the development data.

DE FR PL AVG
P R F P R F P R F P R F

ATILF MWE-based 70.82 39.96 51.09 74.57 61.24 67.25 80.94 60.19 69.04 75.44 53.80 62.81Tok-based 76.03 39.69 52.16 79.83 65.93 72.22 83.21 59.48 69.37 79.69 55.03 65.10

Local MWE-based 54.36 26.31 35.45 60.26 55.42 57.74 74.46 60.00 66.45 63.03 47.24 54.00Tok-based 70.3 36.82 48.38 73.96 62.08 67.50 78.95 59.57 67.90 74.48 52.82 61.81

Global MWE-based 69.72 44.38 54.23 74.57 60.64 66.89 82.01 66.41 73.39 75.43 57.14 65.02Tok-based 74.52 44.10 55.41 78.56 63.54 70.25 83.85 66.06 73.90 78.98 57.90 66.82

Table 2: General results per language and system on the test data.

batches of size 30 and the training length of 60
epochs. We did not apply drop-out.

We used the Adam variant (Kingma and Ba,
2015) of the SGD algorithm with the default pa-
rameters: initial stepsize α0 = 0.001, the expo-
nential decay rates β1 = 0.9 and β2 = 0.999,
and � = 10−8. We additionally used a gradu-
ally decreasing stepsize α = α0×ττ+t , where t ∈
[0, 60] is the epoch number (fractional) and τ =
15. The suitable hyperparameter values were de-
termined during preliminary experiments on the
French dataset.

4.5 Evaluation Results

We evaluated the two implemented systems and
the benchmark (ATILF) system on the develop-
ment and the test parts of the German (DE), French
(FR), and Polish (PL) PARSEME datasets.12 We
trained one local and three global models per lan-
guage and per VMWE category. Training sepa-
rate models for different categories allows to par-
tially handle the issue of overlapping VMWE in-
stances, as well as to simplify the architecture (no
need to encode the categories in terms of tree la-
bellings). The benchmark system predicts all the
categories in one pass. We used the official evalu-
ation script13 provided by the shared task organiz-
ers to calculate all the scores.

12In contrast to several systems participating in the
PARSEME shared task 1.1, we didn’t use the development
parts for training. This is important in that the development
set can contain VMWEs unseen in the training part.

13Available at https://gitlab.com/parseme/
sharedtask-data/tree/master/1.1/bin/.

As mentioned above, we trained three global
models per language and per VMWE category.
One model was obtained using constrained train-
ing and two models using unconstrained training
(see Sec. 3.3.3). We observed that the results
between different training runs can differ signifi-
cantly. For instance, the LVC.full identification F-
scores differed by almost 3% on the FR develop-
ment set between the two unconstrained models.
We therefore used all three models (for each lan-
guage and VMWE category) to calculate ensemble
node scores (see Eq. 4) and ensemble arc scores
(see Eq. 5) by simply summing up the correspond-
ing scores coming from the three models. Such
ensemble averaging should have a smoothing ef-
fect and alleviate the issue of diverging results.

The general results of the three systems on the
development and the test sets are presented in
Tab. 1 and Tab. 2, respectively. The benchmark
system and the global model achieve comparable
results: ATILF has better overall performance on
the development sets, the global model – on the
test sets. The results of the local model are consis-
tently lower than those of the global model, which
shows the usefulness of extended encoding com-
bined with global scoring. Nevertheless, the local
model achieves very competitive results, compa-
rable to those obtained with the best systems par-
ticipating in the PARSEME shared task 1.1.

More detailed evaluation results are presented
in Tab. 3 and Tab. 4. The former shows the per-
formance of the systems across different VMWE-
related challenges, while the latter presents their

https://gitlab.com/parseme/sharedtask-data/tree/master/1.1/bin/
https://gitlab.com/parseme/sharedtask-data/tree/master/1.1/bin/


122

Contin-
uous

Discon-
tinuous

Multi-
token

Single-
token

Seen-in-
train

Unseen-
in-train

Variant-
of-train

Identical-
to-train

ATILF 72.19 44.79 60.26 69.08 82.15 18.9 71.87 92.72
Local 56.68 47.96 56.37 0.0 72.29 29.59 68.06 75.88

Global 72.58 53.30 62.67 69.89 81.65 32.28 74.07 89.23

Table 3: Results (MWE-based F-scores) per VMWE challenge averaged over the three language test sets. The
single-token score is only calculated on German because single-token VMWEs do not occur in the other languages.

VID LVC.full VPC.full IRV IAV

DE
ATILF 39.29 19.23 64.55 28.57 -
Local 33.67 21.87 40.29 30.77 -

Global 35.56 22.95 72.40 32.84 -
# 37% 8% 42% 8% 0%

FR
ATILF 64.47 60.9 - 73.53 -
Local 51.08 53.25 - 75.93 -

Global 66.12 61.29 - 78.47 -
# 43% 32% 0% 22% 0%

PL
ATILF 46.73 50.81 - 86.08 60.0
Local 13.01 64.86 - 85.71 0.0

Global 35.51 65.62 - 87.32 69.57
# 14% 29% 0% 48% 6%

Table 4: Results (MWE-based F-scores) for the se-
lected VMWE categories on the test sets. The last row
per language reports the percentage of occurrences per
category in the test data.

results for the VMWE categories occurring most
frequently in the three test sets. These results
clearly show that our approach performs partic-
ularly well for both discontinuous and unseen
VMWEs. Despite its relative simplicity, the lo-
cal model also yields better results than the bench-
mark system in these two categories. The global
model under-performs in the identification of the
identical-to-train VMWEs. This also applies to the
local model, which does not perform very well in
the category of seen-in-train VMWEs in general.

Concerning the VMWE categories, VIDs
proved challenging for both our models, especially
in DE and PL. This may be due to the arc-factored
nature of our approach, which may be inadequate
for handling VIDs, often composed from more
than two words. On the other hand, our approach
proved very effective in case of LVCs (especially
in PL) and, somewhat surprisingly, in case of
IAVs, consistently disconnected in the PARSEME
dependency structures.

5 Conclusions and Future Work

In this paper we propose a neural, graph-based
VMWE identification method relying on the idea
that VMWE annotations can be represented as la-
bellings of dependency trees. Couching the task of
VMWE identification as syntax-driven labelling

allows to transparently handle the issue of discon-
tinuity, a challenging property of VMWEs which
makes sequential models poorly adapted for this
task. Relying on neural scoring and pre-trained
word embeddings, on the other hand, facilitates
identifying unseen VMWEs. While the idea of ap-
plying parsing-based (in particular, graph-based)
methods to VMWE identification is not novel, we
show that combining it with a neural scoring com-
ponent and supplying the system with pre-trained
word embeddings allows to achieve overall results
on par with state-of-the-art on all three PARSEME
datasets we experimented with (German, French,
and Polish), and to surpass it as far as handling
discontinuous and unseen VMWEs is concerned.

Some VMWE-related challenges, such as over-
lapping instances, are not very well supported by
the encoding schemata we propose. Their en-
hancement is thus one of the major points for
future work. Furthermore, we believe that the
system could better support certain classes of
VMWEs, in particular verbal idioms, often con-
tinuous and consisting of several lexemes. It is
difficult to identify such VMWEs by focusing on
pairs of input tokens at a time. Possible solutions
to this issue include adding a BiLSTM layer in or-
der to contextualize the input word embeddings, as
well as increasing the scope of the factors under-
lying the global model. We plan to explore these
possibilities in future work as well. Finally, we
would like to extend our model to a joint depen-
dency parsing and VMWE identification solution,
and to experimentally check how effective it can
be in the setting where the dependency structures
are not available on input.

Acknowledgments

We thank the anonymous reviewers for their valu-
able comments. The work presented in this pa-
per was funded by the German Research Founda-
tion (DFG) within the CRC 991 and the Beyond
CFG project, as well as by the Land North Rhine-
Westphalia within the NRW-Forschungskolleg
Online-Partizipation.



123

References
Hazem Al Saied, Marie Candito, and Matthieu Con-

stant. 2018. A transition-based verbal multiword ex-
pression analyzer. In Stella Markantonatou, Carlos
Ramisch, Agata Savary, and Veronika Vincze, edi-
tors, Multiword expressions at length and in depth:
Extended papers from the MWE 2017 workshop,
pages 209–226. Language Science Press., Berlin.

Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. Handbook of natural language pro-
cessing, 2:267–292.

Eduard Bejček, Jarmila Panevová, Jan Popelka, Pavel
Straňák, Magda Ševčı́ková, Jan Štěpánek, and
Zdeněk Žabokrtský. 2012. Prague Dependency
Treebank 2.5 – a revisited version of PDT 2.0.
In Proceedings of COLING 2012, pages 231–246,
Mumbai, India. The COLING 2012 Organizing
Committee.

Tiberiu Boroş and Ruxandra Burtica. 2018. GBD-NER
at PARSEME shared task 2018: Multi-word expres-
sion detection using bidirectional long-short-term
memory networks and graph-based decoding. In
Proceedings of the Joint Workshop on Linguistic An-
notation, Multiword Expressions and Constructions
(LAW-MWE-CxG-2018), pages 254–260, Santa Fe,
New Mexico, USA. Association for Computational
Linguistics.

Mathieu Constant, Gülşen Eryiğit, Johanna Monti,
Lonneke van der Plas, Carlos Ramisch, Michael
Rosner, and Amalia Todirascu. 2017. Multiword ex-
pression processing: A survey. Comput. Linguist.,
43(4):837–892.

Matthieu Constant and Joakim Nivre. 2016. A
transition-based system for joint lexical and syn-
tactic analysis. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 161–
171, Berlin, Germany. Association for Computa-
tional Linguistics.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings. Open-
Review.net.

Greg Durrett and Dan Klein. 2015. Neural CRF pars-
ing. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguis-
tics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Pa-
pers), pages 302–312, Beijing, China. Association
for Computational Linguistics.

Rafael Ehren, Timm Lichte, and Younes Samih. 2018.
Mumpitz at PARSEME shared task 2018: A bidirec-
tional LSTM for the identification of verbal multi-
word expressions. In Proceedings of the Joint Work-
shop on Linguistic Annotation, Multiword Expres-
sions and Constructions (LAW-MWE-CxG-2018),

pages 261–267, Santa Fe, New Mexico, USA. As-
sociation for Computational Linguistics.

Jason Eisner. 2016. Inside-outside and forward-
backward algorithms are just backprop (tutorial pa-
per). In Proceedings of the Workshop on Structured
Prediction for NLP, pages 1–17, Austin, TX. Asso-
ciation for Computational Linguistics.

Waseem Gharbieh, Virendrakumar Bhavsar, and Paul
Cook. 2017. Deep learning models for multiword
expression identification. In Proceedings of the
6th Joint Conference on Lexical and Computational
Semantics (*SEM 2017), pages 54–64, Vancouver,
Canada. Association for Computational Linguistics.

Yoav Goldberg. 2017. Neural network methods for nat-
ural language processing. Synthesis Lectures on Hu-
man Language Technologies, 10(1):1–309.

Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
A method for stochastic optimization. In Proceed-
ings of the Third International Conference on Learn-
ing Representations (ICLR), San Diego, California,
USA.

Joseph Le Roux, Antoine Rozenknop, and Matthieu
Constant. 2014. Syntactic parsing and compound
recognition via dual decomposition: Application
to french. In Proceedings of COLING 2014, the
25th International Conference on Computational
Linguistics: Technical Papers, pages 1875–1885,
Dublin, Ireland. Dublin City University and Asso-
ciation for Computational Linguistics.

Tomas Mikolov, Edouard Grave, Piotr Bojanowski,
Christian Puhrsch, and Armand Joulin. 2018. Ad-
vances in pre-training distributed word representa-
tions. In Proceedings of the International Confer-
ence on Language Resources and Evaluation (LREC
2018).

Alexis Nasr, Carlos Ramisch, José Deulofeu, and
André Valli. 2015. Joint dependency parsing and
multiword expression tokenization. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 1116–1126, Bei-
jing, China. Association for Computational Linguis-
tics.

Carlos Ramisch, Silvio Ricardo Cordeiro, Agata
Savary, Veronika Vincze, Verginica Barbu Mititelu,
Archna Bhatia, Maja Buljan, Marie Candito, Polona
Gantar, Voula Giouli, Tunga Güngör, Abdelati
Hawwari, Uxoa Iñurrieta, Jolanta Kovalevskaitė, Si-
mon Krek, Timm Lichte, Chaya Liebeskind, Jo-
hanna Monti, Carla Parra Escartı́n, Behrang Qasem-
iZadeh, Renata Ramisch, Nathan Schneider, Ivelina
Stoyanova, Ashwini Vaidya, and Abigail Walsh.
2018a. Edition 1.1 of the PARSEME shared task on
automatic identification of verbal multiword expres-
sions. In Proceedings of the Joint Workshop on Lin-
guistic Annotation, Multiword Expressions and Con-
structions (LAW-MWE-CxG 2018), Santa Fe, New

https://doi.org/10.5281/zenodo.1469561
https://doi.org/10.5281/zenodo.1469561
https://www.aclweb.org/anthology/C12-1015
https://www.aclweb.org/anthology/C12-1015
http://www.aclweb.org/anthology/W18-4928
http://www.aclweb.org/anthology/W18-4928
http://www.aclweb.org/anthology/W18-4928
http://www.aclweb.org/anthology/W18-4928
https://doi.org/10.1162/COLI_a_00302
https://doi.org/10.1162/COLI_a_00302
http://www.aclweb.org/anthology/P16-1016
http://www.aclweb.org/anthology/P16-1016
http://www.aclweb.org/anthology/P16-1016
https://openreview.net/forum?id=Hk95PK9le
https://openreview.net/forum?id=Hk95PK9le
https://doi.org/10.3115/v1/P15-1030
https://doi.org/10.3115/v1/P15-1030
http://www.aclweb.org/anthology/W18-4929
http://www.aclweb.org/anthology/W18-4929
http://www.aclweb.org/anthology/W18-4929
https://doi.org/10.18653/v1/W16-5901
https://doi.org/10.18653/v1/W16-5901
https://doi.org/10.18653/v1/W16-5901
http://www.aclweb.org/anthology/S17-1006
http://www.aclweb.org/anthology/S17-1006
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1412.6980.pdf
https://www.aclweb.org/anthology/C14-1177
https://www.aclweb.org/anthology/C14-1177
https://www.aclweb.org/anthology/C14-1177
https://www.aclweb.org/anthology/L18-1008
https://www.aclweb.org/anthology/L18-1008
https://www.aclweb.org/anthology/L18-1008
http://www.aclweb.org/anthology/P15-1108
http://www.aclweb.org/anthology/P15-1108
https://aclweb.org/anthology/W18-4925
https://aclweb.org/anthology/W18-4925
https://aclweb.org/anthology/W18-4925


124

Mexico, USA. Association for Computational Lin-
guistics.

Carlos Ramisch, Silvio Ricardo Cordeiro, Agata
Savary, Veronika Vincze, Verginica Barbu Mi-
titelu, Archna Bhatia, Maja Buljan, Marie Can-
dito, Polona Gantar, Voula Giouli, Tunga Güngör,
Abdelati Hawwari, Uxoa Iñurrieta, Jolanta Ko-
valevskaitė, Simon Krek, Timm Lichte, Chaya
Liebeskind, Johanna Monti, Carla Parra Escartı́n,
Behrang QasemiZadeh, Renata Ramisch, Nathan
Schneider, Ivelina Stoyanova, Ashwini Vaidya, Abi-
gail Walsh, Cristina Aceta, Itziar Aduriz, Jean-
Yves Antoine, Špela Arhar Holdt, Gözde Berk,
Agnė Bielinskienė, Goranka Blagus, Loic Boizou,
Claire Bonial, Valeria Caruso, Jaka Čibej, Matthieu
Constant, Paul Cook, Mona Diab, Tsvetana Dim-
itrova, Rafael Ehren, Mohamed Elbadrashiny, Hevi
Elyovich, Berna Erden, Ainara Estarrona, Agge-
liki Fotopoulou, Vassiliki Foufi, Kristina Geeraert,
Maarten van Gompel, Itziar Gonzalez, Antton Gur-
rutxaga, Yaakov Ha-Cohen Kerner, Rehab Ibrahim,
Mihaela Ionescu, Kanishka Jain, Ivo-Pavao Jazbec,
Teja Kavčič, Natalia Klyueva, Kristina Kocijan,
Viktória Kovács, Taja Kuzman, Svetlozara Leseva,
Nikola Ljubešić, Ruth Malka, Stella Markantona-
tou, Héctor Martı́nez Alonso, Ivana Matas, John Mc-
Crae, Helena de Medeiros Caseli, Mihaela Onofrei,
Emilia Palka-Binkiewicz, Stella Papadelli, Yannick
Parmentier, Antonio Pascucci, Caroline Pasquer,
Maria Pia di Buono, Vandana Puri, Annalisa Raf-
fone, Shraddha Ratori, Anna Riccio, Federico San-
gati, Vishakha Shukla, Katalin Simkó, Jan Šnajder,
Clarissa Somers, Shubham Srivastava, Valentina
Stefanova, Shiva Taslimipoor, Natasa Theoxari,
Maria Todorova, Ruben Urizar, Aline Villavicencio,
and Leonardo Zilio. 2018b. Annotated corpora and
tools of the PARSEME shared task on automatic
identification of verbal multiword expressions (edi-
tion 1.1). LINDAT/CLARIN digital library at the
Institute of Formal and Applied Linguistics (ÚFAL),
Faculty of Mathematics and Physics, Charles Uni-
versity.

Agata Savary, Marie Candito, Verginica Barbu Mi-
titelu, Eduard Bejek, Fabienne Cap, Slavomr pl, Sil-
vio Ricardo Cordeiro, Glen Eryiit, Voula Giouli,
Maarten van Gompel, Yaakov HaCohen-Kerner,
Jolanta Kovalevskait, Simon Krek, Chaya Liebe-
skind, Johanna Monti, Carla Parra Escartn, Lon-
neke van der Plas, Behrang QasemiZadeh, Carlos
Ramisch, Federico Sangati, Ivelina Stoyanova, and
Veronika Vincze. 2018. PARSEME multilingual
corpus of verbal multiword expressions. In Stella
Markantonatou, Carlos Ramisch, Agata Savary, and
Veronika Vincze, editors, Multiword expressions at
length and in depth: Extended papers from the MWE
2017 workshop, pages 87–149. Language Science
Press., Berlin.

Agata Savary, Carlos Ramisch, Silvio Cordeiro, Fed-
erico Sangati, Veronika Vincze, Behrang Qasem-
iZadeh, Marie Candito, Fabienne Cap, Voula Giouli,
Ivelina Stoyanova, and Antoine Doucet. 2017. The

PARSEME shared task on automatic identification
of verbal multiword expressions. In Proceedings of
the 13th Workshop on Multiword Expressions (MWE
2017), pages 31–47, Valencia, Spain. Association
for Computational Linguistics.

Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. Transactions of the Association for Compu-
tational Linguistics, 2:193–206.

Katalin Ilona Simkó, Viktria Kovcs, and Veronika
Vincze. 2018. Identifying verbal multiword ex-
pressions with POS tagging and parsing techniques.
In Stella Markantonatou, Carlos Ramisch, Agata
Savary, and Veronika Vincze, editors, Multiword ex-
pressions at length and in depth: Extended papers
from the MWE 2017 workshop, pages 227–243. Lan-
guage Science Press., Berlin.

Regina Stodden, Behrang QasemiZadeh, and Laura
Kallmeyer. 2018. TRAPACC and TRAPACCS at
PARSEME shared task 2018: Neural transition tag-
ging of verbal multiword expressions. In Proceed-
ings of the Joint Workshop on Linguistic Annotation,
Multiword Expressions and Constructions (LAW-
MWE-CxG-2018), pages 268–274, Santa Fe, New
Mexico, USA. Association for Computational Lin-
guistics.

Shiva Taslimipoor and Omid Rohanian. 2018.
SHOMA at PARSEME shared task on automatic
identification of VMWEs: Neural multiword ex-
pression tagging with high generalisation. arXiv
preprint arXiv:1809.03056.

Jakub Waszczuk. 2018. TRAVERSAL at PARSEME
shared task 2018: Identification of verbal multiword
expressions using a discriminative tree-structured
model. In Proceedings of the Joint Workshop on
Linguistic Annotation, Multiword Expressions and
Constructions (LAW-MWE-CxG-2018), pages 275–
282, Santa Fe, New Mexico, USA. Association for
Computational Linguistics.

http://hdl.handle.net/11372/LRT-2842
http://hdl.handle.net/11372/LRT-2842
http://hdl.handle.net/11372/LRT-2842
http://hdl.handle.net/11372/LRT-2842
https://doi.org/10.5281/zenodo.1469563
https://doi.org/10.5281/zenodo.1469563
https://doi.org/10.18653/v1/W17-1704
https://doi.org/10.18653/v1/W17-1704
https://doi.org/10.18653/v1/W17-1704
https://transacl.org/ojs/index.php/tacl/article/view/281
https://transacl.org/ojs/index.php/tacl/article/view/281
https://transacl.org/ojs/index.php/tacl/article/view/281
https://doi.org/10.5281/zenodo.1469563
https://doi.org/10.5281/zenodo.1469563
http://www.aclweb.org/anthology/W18-4930
http://www.aclweb.org/anthology/W18-4930
http://www.aclweb.org/anthology/W18-4930
https://arxiv.org/pdf/1809.03056.pdf
https://arxiv.org/pdf/1809.03056.pdf
https://arxiv.org/pdf/1809.03056.pdf
http://www.aclweb.org/anthology/W18-4931
http://www.aclweb.org/anthology/W18-4931
http://www.aclweb.org/anthology/W18-4931
http://www.aclweb.org/anthology/W18-4931

