



















































Is this Sentence Difficult? Do you Agree?


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2690–2699
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2690

Is this Sentence Difficult? Do you Agree?

Dominique Brunato�, Lorenzo De Mattei•
Felice Dell’Orletta�, Benedetta Iavarone?, Giulia Venturi�

•Dipartimento di Informatica, Università di Pisa
?Scuola Normale Superiore, Pisa

�Istituto di Linguistica Computazionale “Antonio Zampolli” (ILC-CNR), Pisa
ItaliaNLP Lab - www.italianlp.it

{dominique.brunato,felice.dellorletta,giulia.venturi}@ilc.cnr.it
lorenzo.demattei@di.unipi.it beneiavarone@gmail.com

Abstract

In this paper, we present a crowdsourcing-
based approach to model the human percep-
tion of sentence complexity. We collect a large
corpus of sentences rated with judgments of
complexity for two typologically-different lan-
guages, Italian and English. We test our ap-
proach in two experimental scenarios aimed to
investigate the contribution of a wide set of
lexical, morpho-syntactic and syntactic phe-
nomena in predicting i) the degree of agree-
ment among annotators independently from
the assigned judgment and ii) the perception
of sentence complexity.

1 Introduction

Linguistic complexity is a well-studied and mul-
tifaceted notion for which several measures have
been proposed in different frameworks ranging
from First and Second Language Acquisition, lan-
guage typology and readability assessment. Such
measures depend on the perspective from which
linguistic complexity is considered. According to
one established distinction, linguistic complexity
should be divided into an absolute vs a relative
notion (Miestamo, 2008). While the former is
driven by theory and aims at assessing the com-
plexity of a language according to some formal
properties of the linguistic system, the latter de-
fines complexity in relation to the language user
(e.g. speaker, listener or learner) thus considering
complexity in terms of processing difficulty. From
this second perspective, sentence complexity is
analyzed in terms of cognitive load, which can be
inferred using both off-line (e.g. complexity judg-
ments, error rates on comprehension test, prefer-
ence for a structure over a meaning-equivalent one
in elicited production tasks) and online process-
ing measures (e.g. eye-tracking data such as to-
tal gaze time, fixation duration and pupil dilation).

To operationalize factors underlying sentence pro-
cessing performance, several complexity metrics
have been proposed which consider properties of
single word and sentence, as well as experience-
based expectations. Word-level predictors shown
to correlate with greater processing difficulties are
e.g. word frequency, age of acquisition, root fre-
quency effect, orthographic neighbourhood fre-
quency. At syntactic level, a well-studied mea-
sure of sentence complexity takes into account de-
pendency length (Gibson, 1998, 2000), which has
been used to explain a wide range of psycholin-
guistic phenomena, such as the subject/object rel-
ative clauses asymmetry or the garden path effect
in main verb/reduced-relative ambiguities (Gor-
don et al., 2001; Staub et al., 2010), as well as vari-
ations in word order patterns (Gildea and Temper-
ley, 2010), also in a diachronic perspective (Gu-
lordava and Merlo, 2015). Alternatively, process-
ing difficulty has been explained in terms of sur-
prisal (Hale, 2001). Computational models to cal-
culate lexical and syntactic surprisal have been de-
veloped by e.g. Roark et al. (2009) using a broad-
coverage probabilistic PCFG parser and Demberg
and Keller (2009), who introduced Prediction The-
ory, which aims at unifying Dependency Length
Theory with syntactic surprisal, by making use of
a psycholinguistically-motivated version of tree-
adjoining grammar.

Unlike more conventional studies on human
sentence processing carried out in experimental
settings, in this study we rely on crowdsourcing
methods to investigate how people perceive sen-
tence complexity. The reliability of crowdsourced
data for linguistics and computational linguistics
research is well acknowledged as shown in the sur-
vey by Munro et al. (2010) proving that the qual-
ity of findings obtained from the crowd is com-
parable, if not higher, to controlled laboratory ex-
periments. In addition, crowdsourcing reaches a



2691

broader population, in terms of age, education,
profession etc. and it is thus more suitable to catch
the “layman” intuition of sentence complexity. For
these reasons, this method has been used in recent
works in the field of readability and text simpli-
fication; it is the case of Lasecki et al. (2015);
Clercq et al. (2013); Brunato et al. (2016) where
the crowd was asked to evaluate the level of com-
plexity or the degree of informativeness of simpli-
fied sentences compared to the original one.

In our study, we adopted a similar perspective
relying on a crowdsourcing approach to collect a
wide resource containing multiple annotations of
sentence complexity given by humans. Unlike tra-
ditional studies which typically assess either lex-
ical or structural complexity phenomena, we fo-
cused on the analysis of a wide set of linguistic
features to investigate how all contribute to human
perception of sentence complexity. This choice
is also motivated by previous studies focused on
the “form” of a text all related to the assess-
ment of complexity, e.g. readability assessment
(Collins-Thompson, 2015), first language acqui-
sition (Sagae et al., 2005) and Native Language
Identification (Malmasi et al., 2017).

2 Our Contributions

Our contribution to the study of sentence complex-
ity is multiple:

• we address two research questions aimed to
investigate the role played by a set of linguis-
tic phenomena in characterizing a) the agree-
ment among annotators when they rated the
sentences independently from the assigned
score and b) the human perception of com-
plexity.

• we introduce a new crowdsourcing-based
method to assess how people perceive sen-
tence complexity and we test it for two lan-
guages;

• we collect two corpora of sentences anno-
tated by humans with a judgment of complex-
ity;

The two research questions refer to two phenom-
ena that are by definition highly subjective and dif-
ficult to define. Our study intends to address this
vagueness providing the following main contribu-
tions: i) detecting the main linguistic phenom-
ena involved in the prediction of agreement and

ii) which phenomena characterize a sentence that
is perceived complex by a high number of human
subjects.

All the data discussed here are made available
at www.italianlp.it/resources/.

3 Approach

We collected a dataset of rated sentences through
a crowdsourcing task in which annotators were
asked to give a score of complexity to a sentence.
The task was carried out on two languages, Ital-
ian and English, which have different morpho-
syntactic and syntactic properties such as morpho-
logical richness and word order freedom. This
choice was aimed to investigate whether there are
linguistic complexity parameters shared by typo-
logically different languages. Starting from the
collected rated sentences, we automatically ex-
tracted a wide set of features spanning across mul-
tiple levels of linguistic description, which have
been acknowledged in the literature on human sen-
tence processing to be involved in sentence com-
plexity. The contribution of these features in mod-
eling the perception of sentence complexity was
tested in two different scenarios: i) a classifi-
cation experiment to assess which features con-
tribute more in the automatic prediction of the de-
gree of agreement among annotators and which
features vary in a statistically significant way be-
tween agreed and not-agreed sentences; ii) a re-
gression experiment to evaluate if the considered
features allow predicting the complexity judgment
assigned by humans and how they contribute to the
prediction.

In what follows, we introduce the three main in-
gredients of our approach, i.e. the set of linguis-
tic features (Section 3.1), the datasets of sentences
(Section 3.2) and the crowdsourcing task (Section
3.3). In the rest of the paper, we describe the ex-
perimental scenarios raised by our two research
questions and discuss the results (Sections 4 and
5).

3.1 Linguistic Features

The set of features considered in this study cap-
tures different aspects of sentence complexity.

Raw text features:
word length, i.e. average number of characters
per words (char tok in all tables and figures that
follow) and sentence length, i.e. average number



2692

of words per sentence (n tokens), which are typi-
cally used as a proxy of lexical and syntactic com-
plexity in traditional readability metrics (Collins-
Thompson, 2015);

Morpho-syntactic features:
distribution of part-of-speech types; type/token
ratio, calculated as the ratio between the number
of lexical types, the number of tokens, in terms of
both lemma and forms (ttr form, ttr lemma); ver-
bal features, i.e. the distribution of verbs accord-
ing to mood (verbs mood), tense (verbs tense)
and persons (verbs num per), and lexical density
(lex density), calculated as the ratio of content
words (verbs, nouns, adjectives and adverbs) to
the total tokens in a text. Psycholinguistic stud-
ies highlight that higher lexical density implies
greater cognitive load (Gibson, 1993);

Syntactic features:
probability of syntactic dependency types e.g.
subject, direct object, modifier, etc., calculated as
the distribution of each type out of the total de-
pendency types. Some syntactic relations have
been shown to be harder to process, e.g. object-
relative clauses and prepositional-phrase attach-
ments (Gibson and Pearlmutter, 1994; Gibson,
2000), or the subject and object relations espe-
cially in free word-order languages;
distribution of verbal roots, i.e. the distribution
of verbal roots out of the total of sentence roots. A
lower percentage of verbal roots implies a higher
number of nominal sentences which have a less-
standard structure due to verb ellipsis thus possi-
bly causing processing ambiguity;
parse tree depth features: the depth of the whole
parse tree (max depth), calculated in terms of the
longest path from the root of the dependency tree
to some leaf; the depth of embedded complement
chains governed by a nominal head and including
either prepositional complements or nominal and
adjectival modifiers, calculated as the total num-
ber of prepositional chains (n prep chains) and the
average depth of chains (prep chain l); the distri-
bution of embedded complement chains by depth,
calculated as the number of chains out of the total
number of chains in a sentence (prep depth). All
these features are related to length factors and cor-
relate with processing difficulty (Frazier, 1985), as
in the case of long sequences of embedded prepo-
sitional complements;
verbal predicate features: the distribution of ver-

bal head (verb head); the arity of verbs, meant
as the average number of instantiated dependency
links sharing the same verbal head covering both
arguments and modifiers verb arity); the distribu-
tion of verbal head by arity, calculated as the total
number of verbal heads with the same arity in a
sentence (verb head arity); the relative ordering
of subject and object with respect to the verbal
head (order subj and order obj);
subordination features include the distribution of
main vs. subordinate clauses (n subord clauses
and n princ clauses; the average depth of
chains of embedded subordinate clauses, calcu-
lated as the total number of subordinate chains
(n subord chain) and the average depth of sub-
ordinate chains (subord chain l); the distribution
of embedded subordinate clauses chains by depth,
calculated as the number of chains out of the to-
tal number of chains in a sentence (subord depth).
We also calculated the order of the subordi-
nate clause with respect to the main clause (or-
der subord), since according to e.g. (Miller and
Weinert, 1998), sentences containing subordinate
clauses in postverbal than in preverbal position are
easier to process;
length of dependency links calculated as the
number of words between the syntactic head and
the dependent: the feature includes the length of
all dependency links (links len) and of the maxi-
mum dependency links (max links l). It is widely
known that long-distance constructions cause cog-
nitive load (Gibson, 1998; Gildea and Temperley,
2010);
clause length measured as the number of tokens
occurring within a clause (token clause). Syntac-
tic metrics relying on this feature, such as the T-
Unit (Hunt, 1966), are widely used e.g. in first and
second language acquisition to assess the develop-
ment of syntactic competence.

3.2 Data

The experiments were carried out on a subset of
sentences extracted from two manually revised
treebanks. We chose this kind of data in order to
prevent possible errors produced by the automatic
annotation of sentences. Specifically, we consid-
ered the newspaper section of the Italian Universal
Dependency Treebank (UDT) (Simi et al., 2014)
and the automatically converted Wall Street Jour-
nal section of the Penn Treebank (McDonald et al.,
2013). Since we wanted to investigate the human



2693

perception of complexity with respect to standard
language, we didn’t use the English version of the
UDT containing different genres of web media
(e.g. blogs, emails). Although the two selected
treebanks have different annotation schemes, the
annotation scheme of the UDT project (McDon-
ald et al., 2013) is based on an evolution of (uni-
versal) Stanford dependencies (de Marneffe et al.,
2006). This allowed us to compare linguistic phe-
nomena correlated with sentence complexity mini-
mizing possible cross-linguistic differences due to
not uniform principles of sentence structure rep-
resentation. In order to reduce the influence of
lexicon on the study of sentence complexity we
pruned from the two treebanks those sentences
containing low-frequency lemmas with respect to
a lemma frequency list that we automatically ex-
tracted from a large reference corpus, excluding
numerals and proper nouns. For what concerns
Italian, we used as a reference corpus PAISÁ (Ly-
ding et al., 2014), which is one of the biggest cor-
pus of authentic contemporary Italian texts. For
English, we selected a large corpus of sentences
from the Wall Street Journal (Nivre et al., 2007).
For both languages, all the sentences contained in
the two treebanks were grouped into 6 bins based
on a different sentence length, i.e. 10, 15, 20, 25,
30, 35 tokens (only for Italian with a range of +/-
1 tokens each). This was meant to investigate if
some linguistic features that are known to corre-
late with sentence length (e.g. parse tree depth fea-
tures and dependency links) still play an influence
on sentence complexity judgments when sentence
length is controlled. Sentences in each subset were
then ranked according to the sum of the average
frequency of their lemmas. We extracted for each
bin the first 200-top ranked sentences, with the ex-
ception of Italian for which the last bin contains
123 sentences. As a result of the whole selection
process, we obtained 1,200 sentences for English
and 1,123 for Italian used for experiments.

3.3 Collection of Judgments of Complexity

To collect human complexity judgments, we ad-
ministered a crowdsourcing task through the plat-
form CrowdFlower1. For each language we re-
cruited 20 native speakers who were asked to read
a sentence and rate how difficult it was on a 7-
point scale where 1 means “very easy” and 7 “very
difficult”. Sentences were randomly ordered and

1www.crowdflower.com

presented on distinct pages containing five sen-
tences each. To improve the quality of the col-
lected annotations we chose workers with a “high
quality” level assigned by the platform on the ba-
sis of their performance in previous tasks and we
set a minimum of ten seconds to complete a page.
We computed the Krippendorff’s alpha reliability
corresponding to the number of annotators who as-
signed the same judgment. We obtained a reliabil-
ity of 26% for Italian and 24% for English.

4 Studying the Agreement between
Human Judgments

Our first research question concerned the inves-
tigation of linguistic phenomena characterizing
the agreement among annotators in assigning the
same judgment of complexity to a sentence. To
this end, we split the whole set of rated sentences
into ten sets corresponding to the number of an-
notators giving a judgment of complexity within a
same range, hereafter referred to degrees of agree-
ment2. Figure 1 reports the number of sentences
for each degree of agreement. For both languages,
if we consider a minimum number of 10 agree-
ing annotators, very few sentences were discarded
(∼50 for Italian and 70 for English). As the num-
ber of agreeing annotators increases, the number
of sentences progressively decreases but we still
have a considerable number of sentences (∼600)
when 14 annotators agree.

10 12 14 16 18 20
Number of annotators

0

200

400

600

800

1000

1200

Nu
m
be

r o
f s

en
te
nc
es

ITA
ENG
ITA
ENG

Figure 1: Number of sentences at different degrees
of agreement.

To study the linguistic phenomena characteriz-
ing the agreement, we firstly extracted the features
described in Section 3.1 from sentences on which
annotators agreed (agreed sentences) and from the
rest of sentences (not-agreed sentences); we as-
sessed if the difference is statistically significant

2Each range was calculated in terms of standard deviation
from the mean judgment values given to each sentence.



2694

using Wilcoxon Rank-sum test. This was done for
each agreement threshold.

We then performed a feature selection process
to identify the features that maximize the accu-
racies of a classifier in predicting agreed vs not-
agreed sentences. To create a ranking of feature
relevance, we used the Recursive Feature Elimi-
nation (RFE) algorithm implemented in the Scikit-
learn library (Pedregosa et al., 2011), using Linear
SVM as estimator algorithm, and we dropped 1
feature in each iteration. We evaluated the clas-
sifier performance using a 3-fold cross validation
method. At the end of this process we selected the
top ranked features. This procedure was iterated
10 times for each degree of agreement.

In order to evaluate the accuracy of the SVM
classifier we computed a baseline corresponding
to the performance of the classifier using a most-
likely class classification method, where each sen-
tence is always classified into the most likely class.

Table 1 reports the features that vary in a sta-
tistically significant way (Xin table) and the ones
selected in classification (marked with ?) for both
languages and degrees of agreement levels. As it
can be seen, there is an opposite trend between
the statistically significant features and those se-
lected by the classifier as the degree of agreement
increases. For what concerns the Wilcoxon test,
very few features have significantly different val-
ues at lower degrees of agreement. That is to say
that very few features are involved in discriminat-
ing the agreed vs not-agreed sentences, especially
when the agreement is lower than 14.

For both languages, raw text features (n tokens
and char tok) vary significantly at all degrees
of agreement. Interestingly, these two features
are not considered by the classifier which uses
more complex syntactic features, such as features
related to subordination (e.g. subord depth) and
nominal modification (e.g. prep chain l). Syntac-
tic features start to vary significantly as the agree-
ment increases, e.g. parse tree depth features such
as the depth of the whole parse tree (max depth)
and the complement chains (dep mark), and fea-
tures related to the use of subordination. Com-
paring the two languages we also found a number
of differences. For example, at the lowest agree-
ment (degree 10), features of all types turned out
to vary significantly for English, while the Ital-
ian agreed and not-agreed sentences do not vary
for any features. At higher agreement, Italian

agreed sentences are characterized by the varia-
tion of two language-specific features: the posi-
tion of the object with respect to the verb head
(order obj) and some verbal morphological fea-
tures (verbs num pers, verbs tense), which also
contributes to the classification only for Italian.

Table 2 reports the accuracy of SVM classi-
fier for each degree of agreement3 and the base-
line. At lower degrees of agreement (i.e. <14)
the classifier achieves lower accuracy compared
to the baseline showing that the selected features
do not contribute to discriminate agreed vs not-
agreed sentences. Instead, these features start to
have a greater impact for the classification of sen-
tences at degrees 14, 15, 16, 17. This means that at
these degrees of agreement the values of the fea-
tures characterizing the agreed sentences are sig-
nificantly different from those of the not-agreed
sentences. In addition, even though for these sen-
tences a very high number of features are consid-
ered statistically significant by the Wilcoxon test
the classifier needs less features to assign the cor-
rect class (as shown in Table 1).

5 Correlation of Linguistic Features with
Sentence Complexity

The second research question aims to model the
human perception of complexity studying the cor-
relation between the set of linguistic features ex-
tracted from sentences and the judgments of com-
plexity assigned to each sentence. We firstly cal-
culated the average complexity judgments for the
six bins of sentences of the same length (i.e. 10,
15, 20, 25, 30, 35 tokens). As expected, long
sentences were judged as more complex for both
languages even though all sentences were always
rated as more complex for Italian (see Figure 2).

We then calculated the Spearman’s rank correla-
tion coefficient between the values of each feature
and the average judgments of complexity thus ob-
taining a ranking of features. The correlation was
computed at two distinct degrees of agreement, i.e.
10 and 14. We chose these two thresholds since at
10 the agreed sentences correspond to almost all
the rated sentences and at 14 the SVM classifier
starts to outperform the baseline (see Table 2). Be-
sides, at 14 we still have a quite large set of agreed
sentences allowing a reliable statistical study of
the features (see Figure 1). Only at threshold 10

3The accuracy was computed as the average classification
score of the 10 best results of the feature selection process.



2695

Feature
Agreement

10 11 12 13 14 15 16 17
IT EN IT EN IT EN IT EN IT EN IT EN IT EN IT EN

char tok ? ? ? ? ? - - X X X? X? X? X? X? X? X?
cpos ADJ ? ? ? ? ? - - - X? - X? X? X? X? X? X
cpos ADP ? ? ? ? ? - - ? - - - - X X X X
cpos ADV ? - ? - ? - - - - - ? - ? - ? -
cpos AUX ? - ? - ? - X - - - - - X? - X -
cpos CONJ ? ? ? ? ? - - ? - X X X? X? X X X
cpos PRON ? - ? - ? - - - X - X? - X - X -
cpos DET - ? - ? - - - X? - X? - X? - X? - X?
cpos NUM - ? - X? - X - X? - X? - X? - X? - X?
cpos PROPN ? - ? - ? - - - X - ? - X? - -
cpos PUNCT ? - ? - ? - X - - - X? - X - X? -
cpos SCONJ ? - ? - ? - - - - - X? - X - X -
cpos VERB - ? - ? - X - X? - X? - X? - X? - X
dep acl - - ? - ? - X - X - X - X - X -
dep acl:relcl - - ? - ? - - - ? - X - X? - X -
dep adpobj - ? - ? - - - ? - - - - - - - X
dep advcl ? - ? - ? - - - X - X - X? - X -
dep amod ? X? ? ? ? X - X? X X X X? X? X? X X?
dep appos - ? - ? - - - - - ? - - - - - -
dep attr - ? - ? - - - - - - - X? - X? - X
dep aux - - ? - ? - X - X - - - X? - X -
dep case ? - ? - ? - - - ? - - - X - X -
dep cc ? ? ? ? ? - - - - X? X X? X? X? X X
dep ccomp - ? - ? - - - - - X - X - X - X
dep compmod - ? - ? - - - - - X? - ? - X? - X?
dep conj ? ? ? ? ? - - X? - X? X? X? X? X? X X?
dep det - ? - ? - - - ? - X? - X? - X? - X?
dep dobj ? - ? - ? - - - - - X - X? - X -
dep mark ? ? ? ? ? - X ? X ? X? ? X? X X X
dep nmod ? ? ? ? X? - X - X - - X? X? X X X
dep nsubj - X? - X? - X - X - X? - X? - X? - X?
dep num - ? - ? - X - X - X? - X? - X? - X?
dep partmod - ? - ? - - - - - - - X - X - X
dep poss - ? - ? - - - - - X - X - X - X
dep punct ? - ? - ? - X - - - X? - X - X -
dep rcmod - ? - ? - - - ? - - - X? - X? - X
dep xcomp ? - ? - ? - - - - - - - X - X -
lex density - ? - ? - - - X? - X? - X - X? - X?
links len - X? ? ? X? X X X X? X X X? X X X X
max depth - ? ? ? X? - X X X X X X X X? X X
max links l - X? ? X? X? X X X X X X X X X X X
n prep chains ? X? X? ? X? X X X X X X X X X? X X
n principal clauses - ? ? ? ? - X X X X? X X X X X X
n subord chain ? ? ? ? ? X X - X? X? X? X X? X X X
n subord clauses ? - ? - ? - X - X? - X? - X? - X? -
n tokens - X? X? X? X? X X X X X X X X X X X
order obj - - ? - ? - - - - - X - X - X -
order subj - - ? - ? - - - ? - - - X - X -
order subord ? ? ? ? ? - X X X X? X X X X X X
prep chain l - ? ? ? ? - X - X X X? X X? X X X
prep depth - X? ? ? X X X X X X? X X? X X? X X?
subord depth ? ? ? ? ? - X? ? X? X? X? X? X? X? X? X?
token clause - ? ? ? ? - - - - - - - X X X X
ttr form - X? ? ? X? X X X X X? X? X? X? X? X X?
ttr lemma ? X? ? X? ? X X X? X X? X? X? X? X? X? X?
verb arity ? ? ? ? X? - X X X X X X X X X X
verb head arity ? ? ? ? ? ? ? ? X? ? X? X? X? X? X? X?
verb head ? ? ? ? X? - X X X X X X X X X X
verbs num pers ? - ? - ? - X? - X? - X? - X? - X? -
verbs tense ? - ? - X? - X - X? - X? - X? - X? -

Table 1: Linguistic features that vary statistically (X) and the ones selected by the SVM classifier in at
least 50% of the 10 runs (?) for Italian and English at different degrees of agreement.

Baseline Accuracy (%) – SVM Classifier Accuracy (%)
10 11 12 13 14 15 16 17

Italian 95.4-95.4 91-90.8 80.6-80.5 66.7-66 51.9-59.1 66.8-68.8 79-80.7 87-87.1
English 94-94 86.8-86.8 83.6-77.4 66.3-66.1 53.9-60 60.7-71.8 70.9-79.3 80.4-84.6

Table 2: Baseline and SVM classifier accuracy at different degrees of human agreement.

we also calculated the ranking of the features with
respect to the six bins of sentences of the same
length (L10, L15, L20, L25, L30, L35). Figure
3 reports the ranking of features with p <0.05.
Positive numbers mean that the higher the feature
value the more complex the sentence was judged
(i.e. the feature ranked +1 is the top-ranked one
since it is the most positively correlated). Instead,
negative numbers mean that the lower the feature

value, the more complex the sentence was judged
(i.e. the feature ranked -1 is the highest nega-
tively correlated). In both languages, the corre-
lation between the top 20 ranked features and the
complexity judgment is extremely high, ranging
from 0.30 to 0.85 when we consider sentences at
agreement 14. At the two agreement thresholds,
for all lengths (columns T10, T14), they concern
not only sentence length but also deep syntactic



2696

10 15 20 25 30 35
Sentence length

1.5

2.0

2.5

3.0

3.5

4.0

4.5

M
ea

n 
ju
dg

m
en

t

ITA
ENG
ITA
ENG

Figure 2: Mean complexity judgment at different
sentence length.

features, in terms of e.g. the depth of the whole
parse tree (max depth), the length of dependency
links (links len), and features related to subordi-
nation (e.g. n subord clauses). Specifically, the
1st-ranked feature in Italian (parse tree depth) and
the one in English (sentence length) have a cor-
relation of 0.64 and 0.84 respectively. Nominal
modification (n prep chains) is also highly corre-
lated (Italian rs=0.59, English 0.54) and similarly
ranked in the two languages at 3rd position. The
distribution of verbs num pers makes the sentence
harder only for Italian; this is possibly related to
the higher complexity of verbal morphology since
the 3rd person verb in impersonal structures might
increase the ambiguity of the sentence with re-
spect to the referent. Only in English, sentence
complexity is affected by the distribution of car-
dinal numbers (cpos NUM) and the dependency
type “numeric modifier” (dep num), in line with
the difficulty of numerical information shown in
readability studies (Bautista and Saggion, 2014).
Conversely, the verbal arity and the relative or-
dering of subjects with respect to the verb have
a lower position in the negative ranking, suggest-
ing that these features make a sentence easier: this
might be due to a more fixed predicate-argument
structure and word order in this language.

If we focus on sentences of the same length,
features considered as a proxy of lexical complex-
ity are in the top positions in both languages. It
is the case of the average word length (char tok)
and the lexical density (lex density) only for En-
glish. Interestingly, while for English the major-
ity of features are similarly ranked in all bins of
sentences of the same length, for Italian we ob-
serve differences between the rankings of features
extracted from sentences ≤ and ≥20 token long.
Namely, when the average sentence length is ≥20

tokens, features related to subordination make the
sentence more complex.

5.1 Predicting Human Complexity
Judgments

To asses the contribution of the linguistic fea-
tures to predict the judgment of sentence com-
plexity we trained a linear SVM regression model
with default parameters. We performed a 3-fold
cross validation over each subset of agreed sen-
tences at agreement 10 and 14. We measured two
performance metrics: the mean absolute error to
evaluate the accuracy of the model to predict the
same complexity judgment assigned by humans;
the Spearman correlation to evaluate the correla-
tion between the ranking of features produced by
the regression model with the ranking produced by
the human judgments. Table 3 reports the results
and the average score of the two metrics. As it can
be seen, the model is very accurate and achieves a
very high correlation (>0.56 with p <0.001) with
an average error difference (avg mean abs err) be-
low 1. In particular, the model obtained higher
performance in predicting the ranking of features
extracted from sentences at agreement 14. This
might be due to the fact these sentences are charac-
terized by a more uniform distribution of linguistic
phenomena and that these phenomena contribute
to predict the same judgment of complexity. This
is in line with the results obtained by the SVM
classifier in predicting agreement (Table 2). This
is particularly the case of English and it possibly
suggests that the set of sentences similarly judged
by humans are characterized by a lower variability
of the values of the features.

IT-10 IT-14 EN-10 EN-14
mean abs err 1 0.77 0.78 0.71 0.68
Spearman 1 0.57 0.64 0.68 0.64
mean abs err 2 0.79 0.80 0.70 0.70
Spearman 2 0.55 0.63 0.67 0.73
mean abs err 3 0.85 0.75 0.77 0.60
Spearman 3 0.55 0.64 0.61 0.71
avg mean abs err 0.80 0.78 0.72 0.66
avg Spearman 0.56 0.63 0.65 0.69

Table 3: Performance of the linear SVM regression
model and the avg score at different agreements.

6 Discussion and Conclusion

In this paper, we introduced a method to model
the human perception of sentence complexity re-
lying on a new corpus of Italian and English sen-



2697

1

−5

−2

−3

−1

3

2

−4

1

7

6

2

−1

−2

5

4

3

1

4

−6

−7

2

−4

−5

9

−3

7

−1

3

−2

8

6

5

1

5

−4

−1

6

4

−3

−7

−2

−5

2

−6

3

1

7

−5

−2

11

9

5

4

−3

−4

3

2

6

−1

8

10

5

17

22

−4

−9

−8

6

9

11

20

13

−6

8

1

−3

−1

−7

−2

−5

−10

14

19

15

10

3

23

7

18

4

2

21

12

16

30

28

48

47

33

−6

−9

−10

35

−7

29

51

31

21

18

26

24

41

45

49

5

17

38

22

9

40

19

39

50

16

43

46

4

23

52

13

−1

−8

34

11

1

−5

3

−4

8

2

27

36

12

32

14

10

6

15

37

−2

−3

53

7

20

42

25

44

31

23

47

51

34

−8

41

−6

30

37

20

19

28

18

40

48

42

2

21

35

26

6

38

17

43

16

49

50

4

29

44

13

−1

−7

32

12

1

−4

3

−5

9

5

22

36

11

27

14

10

7

15

33

−2

−3

46

8

25

45

24

39

L10 L15 L20 L25 L30 L35 TOT10 TOT14

verbs_tense

verbs_num_pers

verbs_mood

verb_head

verb_arity

verbal_head_arity

ttr_lemma

ttr_form

token_clause

subord_depth

subord_clauses

subord_chain_l

prep_depth

prep_chain_l

order_subord

order_subj

order_obj

n_tokens

n_subord_chain

n_principal_clauses

n_prep_chains

max_links_l

max_depth

links_len

dep_xcomp

dep_vocative

dep_root

dep_punct

dep_nummod

dep_nsubjpass

dep_nsubj

dep_nmod

dep_neg

dep_name

dep_mark

dep_foreign

dep_expl

dep_dobj

dep_det:poss

dep_det

dep_conj

dep_ccomp

dep_cc

dep_case

dep_auxpass

dep_aux

dep_appos

dep_amod

dep_advmod

dep_advcl

dep_acl:relcl

dep_acl

cpos_VERB

cpos_SCONJ

cpos_PUNCT

cpos_PROPN

cpos_PRON

cpos_NUM

cpos_NOUN

cpos_DET

cpos_CONJ

cpos_AUX

cpos_ADV

cpos_ADP

cpos_ADJ

char_tok

(a) Italian.

−1

6

−3

2
−4
−5

−7
−2

5

−1

−11

3

−10

−8
1

4

−12

−9

−6

3
6

−24
−5

1
−10

−1
−14

5

10

−4
−25
−23
−21

7
−20

8
−6

−17

−22

−18

−9
−26

4
12

−12
2
11

−19

−11

9
−3
−16

−8
−13
−7

−2
−15

2

3

−2
−7

−1
−9

−4

1

−3

−5

−6

−8
4

1
−7
−2
−3

4

−1

6

5

3

7

−5
−8

2

−10

−9
8

−6

−4

−4

2
−9

−2

−1

−6

−7

−10

1

3

−12

−8

−5

4
−3
−11

39
−10
34
40
32
−7
47
17

49
−6

38
41
2
45
35
30
6
33

−11
−12

14
24
8
13
28
20
18

48

29
46

22
21

10
5

26
27

31
42
−2
43
36
4
7

−5
3

−4
12
1
37
−9
19
9
23
11
16
44
−3
−1
−8
15
25

42
−12
36
40
30
−7
43
16

−5

38
39
2
44
41
33
8
32
−9
−8

15
28
9
13
29
14
22

47
48
31
45

21
23

7
4

27
25

26

−2
46
37
5
6

−6
3

−4
11
1
34

−10
19
10
20
12
17
35
−3
−1
−11
18
24

L10 L15 L20 L25 L30 L35 TOT10 TOT14

verb_head_arity

verb_head

verb_arity

ttr_lemma

ttr_form

token_clause

subord_depth

subord_clauses

prep_len

prep_depth

order_subord

order_subj

order_obj

n_tokens

n_subord_chain

n_principal_clauses

n_prep_chains

max_links_l

max_depth

links_len

lex_density

dep_xcomp

dep_ROOT

dep_rel

dep_rcmod

dep_prt

dep_poss

dep_partmod

dep_parataxis

dep_p

dep_num

dep_nsubjpass

dep_nsubj

dep_nmod

dep_neg

dep_mwe

dep_mark

dep_iobj

dep_infmod

dep_expl

dep_dobj

dep_det

dep_dep

dep_conj

dep_compmod

dep_ccomp

dep_cc

dep_auxpass

dep_aux

dep_attr

dep_appos

dep_amod

dep_advmod

dep_advcl

dep_adpobj

dep_adpmod

dep_adpcomp

dep_adp

dep_acomp

cpos_VERB

cpos_PRT

cpos_PRON

cpos_NUM

cpos_NOUN

cpos_DET

cpos_CONJ

cpos_ADP

cpos_ADJ

cpos_.

char_tok

(b) English.

Figure 3: Features correlating with human judgments at different sentence lengths and with respect to the
sentences at agreement 10 (TOT 10) and 14 (TOT 14).

tences rated with human complexity judgments.
We tested the contribution of a wide set of lin-
guistic features automatically extracted from these
sentences in two experimental scenarios. The first
one highlighted that we can reliably predict the de-
gree of agreement between human annotators, in-
dependently from the assigned judgment of com-
plexity: given the high subjectivity of the task, this
is a quite notable result that to our knowledge has
never been reported. We observed in particular
that deep syntactic features related to e.g. the use
of subordination and nominal modification play a
main role in the automatic prediction of human
agreement. This is true for the two languages even
though we found that some features resulted to be
more relevant in the classification of agreed Ital-

ian sentences, e.g. the relative ordering of the ob-
ject. Interestingly, we also noticed that the classi-
fier needs few features to predict agreed sentences
when more than half of annotators shares the same
judgment.

In the second experiment, we studied the cor-
relation between linguistic features and complex-
ity judgments. The resulting ranking highlighted
the key role played by syntactic phenomena: fea-
tures related to sentence structure are among the
top-ranked features characterizing sentences that
were rated highly complex by a given number of
agreeing annotators. When sentence length was
controlled, the relevance of the considered fea-
tures changes in particular for Italian: e.g. features
concerning the use of subordination make the sen-



2698

tence more complex when sentence length is ≥20
tokens. As showed by the results of the regression
model, the set of studied features contribute sig-
nificantly to automatically predict the human judg-
ment of sentence complexity.

In addition, the presented corpus can be use-
ful for different applications. From a psycholin-
guistic perspective, it can be used for compari-
son with data collected through controlled experi-
mental scenarios assessing sentence complexity in
terms of cognitive measures (offline and online),
which are also more constrained and costly to ac-
quire in large-dimensions. The corpus also allows
to study whether features of linguistic complexity
are implied in modeling other properties of texts,
such as the level of engagement or subjectivity.
From a NLP perspective, the corpus can be ex-
ploited to train systems able to predict people’s
perception of complexity. For example, it can sup-
port a range of related tasks, such as the devel-
opment of linguistically-informed algorithms for
the automatic assessment of text difficulty, as well
as in Natural Language Generation tasks, going
from text simplification to the automatic genera-
tion/evaluation of highly-engaging texts.

References
Susana Bautista and Horacio Saggion. 2014. Can nu-

merical expressions be simpler? Implementation
and demostration of a numerical simplification sys-
tem for spanish. In Proceedings of LREC, the 9th In-
ternational Conference on Language Resources and
Evaluation.

D. Brunato, A. Cimino, F. Dell’Orletta F., and G. Ven-
turi. 2016. PaCCSS–IT: A parallel corpus of
complex–simple sentences for automatic text simpli-
fication. In Proceedings of Conference on Empirical
Methods in Natural Language Processing (EMNLP
2016), pages 10–18.

O. De Clercq, V. Hoste, B. Desmet, and P. Van Oosten.
2013. Using the crowd for readability prediction.
Natural Language Engineering, pages 1–33.

Kevyn Collins-Thompson. 2015. Computational as-
sessment of text readability: A survey of current
and future research. Recent Advances in Automatic
Readability Assessment and Text Simplification. Spe-
cial issue of International Journal of Applied Lin-
guistics, 165(2):97–135.

Vera Demberg and Frank Keller. 2009. A computa-
tional model of prediction in human parsing: Uni-
fying locality and surprisal effects. In Proceedings
of CogSci 2009, the 31st Annual Conference of the
Cognitive Science Society, pages 1888–1893.

Lin Frazier. 1985. Syntactic complexity. Natural Lan-
guage Parsing: Psychological, Computational, and
Theoretical Perspectives, Cambridge: Cambridge
University Press.

Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 24(11):1–76.

Edward Gibson. 2000. The dependency locality the-
ory: A distance–based theory of linguistic complex-
ity. W.O.A. Marants and Y. Miyashita (Eds.), Image,
Language and Brain, Cambridge, MA: MIT Press.

Edward Gibson and Neal J. Pearlmutter. 1994. A
corpus-based analysis of psycholinguistic con-
straints on prepositional-phrase attachment. Per-
spectives on sentence processing.

Timothy R. Gibson. 1993. Towards a discourse theory
of abstracts and abstracting. Monographs in Sys-
temic Linguistics, Nottingham: Department of En-
glish Studies, University of Nottingham.

Daniel Gildea and David Temperley. 2010. Do gram-
mars minimize dependency length? Cognitive Sci-
ence, 34(2):286–310.

Peter C. Gordon, Randall Hendrick, and Marcus John-
son. 2001. Memory interference during language
processing. Journal of Experimental Psychol-
ogy: Learning, Memory and Cognition, 27(6):1411–
1423.

Kristina Gulordava and Paola Merlo. 2015. Diachronic
trends in word order freedom and dependency length
in dependency-annotated corpora of latin and an-
cient greek. In Proceedings of Depling 2015, the
Third International Conference on Dependency Lin-
guistics.

John Hale. 2001. A probabilistic earley parser as a psy-
cholinguistic model. In Proceedings of the NAACL,
pages 159–166.

Kellogg W. Hunt. 1966. Recent measures in syntactic
development. Elementary English, 43(7):732–739.

W. S. Lasecki, L. Rello, and J. P. Bigham. 2015. Mea-
suring text simplification with the crowd. In Pro-
ceedings of the 12th Web for All Conference (W4A
15).

V. Lyding, E. Stemle, C. Borghetti, M. Brunello,
S. Castagnoli, F. Dell’Orletta, H. Dittmann,
A. Lenci, and V. Pirrelli. 2014. The PAISÀ corpus
of Italian web texts. In Proceedings of the 9th Web
as Corpus Workshop (WAC-9) EACL, pages 36–43.

S. Malmasi, E. Keelan, A. Cahill, J. Tetreault, R. Pugh,
C. Hamill, D. Napolitano, and Y. Qian. 2017. A
report on the 2017 native language identification
shared task. In Proceedings of the 12th Workshop
on Building Educational Applications Using NLP.



2699

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC-
2006), pages 449–454.

R. McDonald, J. Nivre, Y. Quirmbach-brundage,
Y. Goldberg, D. Das, K. Ganchev, K. Hall, S. Petrov,
H. Zhang, O. Tackstrom, C. Bedini, N. B. Castelló,
and J. Lee. 2013. Universal dependency annota-
tion for multilingual parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 92–97.

M. Miestamo. 2008. Grammatical complexity in a
cross–linguistic perspective. M. Miestamo, K. Sin-
nemaki and F. Karlsson (eds.), Language Com-
plexity: Typology, Contact, Change, Amsterdam:
Benjamins:23–41.

Jim Miller and Regina Weinert. 1998. Spontaneous
spoken language. Syntax and discourse. Oxford,
Clarendon Press.

R. Munro, S. Bethard, V. Kuperman, V.T. Lai, R. Mel-
nick, C. Potts, T. Schnoebelen, and H. Tily. 2010.
Crowdsourcing and language studies: The new gen-
eration of linguistic data. In Proceedings of the
Workshop on Creating Speech and Language Data
with Amazons Mechanical Turk, pages 122–130.

J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the EMNLP-CoNLL, pages 915–932.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Brian Roark, A. Bachrach, C. Cardenas, and C. Pallier.
2009. Deriving lexical and syntactic expectation-
based measures for psycholinguistic modeling via
incremental top–down parsing. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 324–333.

K. Sagae, A. Lavie, and B. MacWhinne. 2005. Au-
tomatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the ACL.

Maria Simi, Cristina Bosco, and Simonetta Monte-
magni. 2014. Less is more? Towards a reduced
inventory of categories for training a parser for the
italian stanford dependencies. In Proceedings of
the 9th International Conference on Language Re-
sources and Evaluation, (LREC’14), pages 83–90.

A. Staub, S.J. White, D. Drieghe, E.C. Hollway, and
K. Rayner. 2010. Distributional effects of word fre-
quency on eye fixation durations. Journal of Exper-
imental Psychology: Human Perception and Perfor-
mance.


