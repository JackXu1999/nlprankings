



















































A System for Identifying and Exploring Text Repetition in Large Historical Document Corpora


Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 330–333,
Gothenburg, Sweden, 23-24 May 2017. c©2017 Linköping University Electronic Press

A System for Identifying and Exploring Text Repetition
in Large Historical Document Corpora

Aleksi Vesanto,1 Asko Nivala,2,3 Tapio Salakoski,1 Hannu Salmi,2 and Filip Ginter1
1Turku NLP Group, Department of FT

2Cultural History
3Turku Institute for Advanced Studies

University of Turku, Finland
first.last@utu.fi

Abstract

We present a software for retrieving and
exploring duplicated text passages in low
quality OCR historical text corpora. The
system combines NCBI BLAST, a soft-
ware created for comparing and aligning
biological sequences, with the Solr search
and indexing engine, providing a web in-
terface to easily query and browse the
clusters of duplicated texts. We demon-
strate the system on a corpus of scanned
and OCR-recognized Finnish newspapers
and journals from years 1771 to 1910.

1 Introduction

The task of finding repeated passages from old
newspapers and magazines is relevant to the his-
torians who study the spread of news in time
and space. The underlying corpora – in our
case scanned and OCR-transcribed newspapers
and journals, some over 200 years old – pose a
number of technical challenges. Firstly, the size
of the corpora is large, in the millions of pages
range. And, more importantly, the text produced
using OCR is often of poor quality – sometimes
nearly unreadable as shown in Figures 1 and 2.
This makes the corpora inaccessible to commonly
used fast methods such as Passim (Smith et al.,
2014) which rely on identifying seed overlaps that
are several full words in length, a rare occurrence
in our data whose error rate has been estimated to
25-30% in terms of words, depending on period of
print (Kettunen et al., 2016).

In this demo, we present a system for identi-
fying text repetitions and forming their clusters
using BLAST (Altschul et al., 1990), a software
developed to compare and align biological se-
quences. To browse and search these clusters, we
index them using Solr, an open-source search en-
gine, and provide a web interface that is capable of

searching and visualizing these repeated text clus-
ters and their associated metadata.

We demonstrate the software and its web in-
terface on a corpus of OCR scanned old Finnish
newspapers and journals from years 1771 to 1910,
around 3 million pages in total.

2 Software Architecture

2.1 Data Preprocessing and Indexing

NCBI BLAST is built for fuzzy-aligning protein
and nucleotide sequences and querying massive
sequence databases. As such, it seems an ideal
tool for the task, but the assumption of working
with biological data is ubiquitous throughout the
BLAST codebase, and it cannot be easily used for
matching arbitrary alphabets. Therefore, to apply
BLAST in our setting, we need to first encode our
whole corpus into protein sequences composed of
an alphabet of 23 amino acids. As we are limited
by the number of distinct amino acids, we can only
map the 23 most common lowercase letters in our
corpus to distinct amino acids. We then lowercase
our corpus and replace all characters using this
mapping. Characters that do not have an equiv-
alent in our mapping are discarded – and naturally
restored later. This encoding also simultaneously
works as a preprocessing method, as the docu-
ments have a lot of noise in them in the form of ar-
bitrary characters and spaces. These characters are
not among the 23 most common letters, so they are
discarded in the encoding process. Interestingly,
although space is the most used character in the
corpus, we found that discarding spaces neverthe-
less makes the BLAST query process more than
twice as fast and the hits we find are also slightly
longer. Once encoded into protein sequences, the
documents are indexed using BLAST for a subse-
quent fast retrieval.

330



cru− i . _ , i , 1 i l and m a l f i l l y t o l i g i h t mndcr C ; , b a l n c r ; a s ; ! i i t ’ t f i n , t h e worHd , : , : i ( t 1 i ( t c \ ’ i ; ; i l a t i o c o n l t i i i i e
Cihr lC ’ t i i l i f ’ u l i f o l l i ’ t −’ a i n d f I r v a n t i i t c ( / . i i l i l c ’ s e n c l . An11e i l . T7 jeJZa / n l / t / h P / r i / ? / l J 9 ’ , iC J−e i n g now ,
d e a r l y h b e l ed bhrc− i . i h r c i , t h a t t h i s ( Chi / i ! by Balp− ; 1 1 i r e . n e g n c i r a t e , ; l a n d g r a f t e d i n t o h i l e l : ; , dy o f C h r i t l t
’ s C h i l l r e h , l e t i i S i x e t h a n k l s un to A l n i g h t y ( , o d l : o r t h c l e b e l l n e i t s

c r u c i f i e d , and m l a n f u l l y t o f i g h t u n d l e r h i ; banne ! aigm~ n i t f i n , t h e v o r l d J , and t h e d e v i l ; an−d t o c o n t i n u o e C h r i f t ’ s f a ~
i t h f a l f o l d l i e r and f e r v i a n t un to h i s l i f e ’ s end . , Amlen . Theni j h ~ a l l t h e P r i e s 3 fay , S E EiIN~ G now . d e a r l y r b e l o v e d i

b r e i h r e n , t h t h i s h i i s by B a p t i f m r g n e a e a n r f t d i t t h e bod ly o f C h r i f i ~ s Chuirch , l e t us g i v e t h a ~ nks un to A l m l i g h l t y God
f a t t h e i i e b e n e f i t s

Figure 1: A hit pair from a run with ECCO dataset. (OCR-scanned books from 18th century)

Multa t \ ä@tä f y N l k Ã Ď s i i i k c h t a l o s t u , c t , Äb o u i l Äs i , 3 wic ! l ä t i c i u n ’ t > t , mi t ä ä>« , » v a a l i i l u i f t t i i l o i s t a M,m< i ä
T s h i r a g a u i s s a , Âl’el ä f i : f ö f3 > i ’ ö i e t t ä u i U f a t f p ä im −u h k a i s i l o u i i H v i a r a t , m i i n t o fu ^ t i a a n i ’ f a t i f e f i − f u f f o t a i » lÃĎuja
THi r o i n i n , p u u t a r h a s s a j a , i p i c i ’ i l i t s i hwi ’ t t < i i ö i i fmmiamcrk ^ i U i j a anoo » » imi lyMla ,

Mutta t ä s t ä synk ä s t ä k o h t a l o s t a e i Äbbu l Äs i b » i e l ä t i e n n y t mi t ä än , vaan » i e t t i i l o i s t a e l ämä ä T s h i r a g a n i S s a . Sek ä s i s » S t ä « t t
ä u lk oa p ä i n u h k a s i v a t « a a r a t . mu t t a s u l t t a a n i k a t s e l i l u k k o t a i s t e l u j a T f h i i a a a n i n p u u t a r h a s s a j a p a l k i t s i v o i t t a j a n
l u n n i c n n e r l e i l l ä j a a rÂ ř v o n i m i t y k s i l l ä .

Figure 2: A hit pair from Finnish newspapers.

2.2 Clustering

Every document in the corpus, 3 million pages
in our case, is subsequently matched by BLAST
against the entire indexed collection – i.e. all pair-
wise comparisons are calculated. The matching
document pairs contain the starting and ending
offsets from each document, which we use to con-
nect and combine pairs that share a text passage
with a sufficient overlap. Because the matching
is fuzzy and the texts are very noisy, if the same
text passage is reused in a number of documents,
each of the identified document pairs will mark
a slightly different beginning and end of the pas-
sage. For instance, if a passage from a document
A is reused in documents B and C, the offsets in
A will be slightly different between the A-B and
A-C pairs. To deal with the problem, we calcu-
late consensus indexes that combine all passages
in one document from individual document pairs
that are close to each other – in our example the
two passages in A from the A-B and A-C pairs.
We do this by averaging the starting and ending
indexes of passages that overlap by at least 80%,
obtaining consensus passages.

After identifying all the distinct consensus pas-
sages for each document, we create a graph with
the consensus passages in individual documents as
nodes, and edges between corresponding passage
pairs. Subsequently, we extract all connected com-
ponents from the graph, providing us with an ini-
tial estimate of clusters of documents that share
a passage. The identification of passage clus-
ters through connected components in the graph
can be seen as a high recall method. A stray
edge – not uncommon in the noisy data – may

connect two otherwise disjoint clusters together.
To deal with this, we separate these clusters us-
ing community detection. To this end, we apply
the Louvain method (Blondel et al., 2008) which
identifies communities within the connected com-
ponents of the graph and we subdivide those
connected components that have several distinct,
highly-connected communities (subcomponents).
This removes the likely stray edges that were con-
necting them. After this subdivision, we obtain the
final clusters and the nodes within them are the re-
peated text passages we seek.

2.3 Finnish newspapers

We applied our system to old OCR-scanned
Finnish newspapers and journals from years 1771
to 1910, around 3 million pages in total. We
found nearly 8 million passage clusters containing
around 49 million repeating passages. We only
considered hits that are 300 characters or longer,
as the shorter hits would either be too fractioned
to be useful or they are just boilerplate text. The
most computationally intensive part of the pro-
cess is running the BLAST queries, which took
150,000 CPU-core hours. Clearly, a dataset of
this size requires an access to a cluster computer,
which is not surprising given the complexity in-
volved in fuzzy-matching 3 million pages of text
against each other. This computationally intensive
step however only needs to be performed once and
its results can be reused at a later point.

3 Web User Interface

For the user interface, we index our data with Solr.
More specifically, we index the data as nested doc-

331



Figure 3: A screenshot showing the user interface.

uments, where the parent document is the clus-
ter and child documents are the hits within that
cluster. Solr is capable of querying the data very
efficiently, easily allowing for a swift, real-time
search. Solr has built-in support for Apache Ve-
locity Template Engine and out of the box it pro-
vides a simple browse page where one can browse
the query results. Using this template engine, we
implement an easy-to-use interface suitable to the
nature of the data.

A screenshot of a result page is shown in Fig-
ure 3. At the top, a search field allows a free text
search. Below is a field for direct search by clus-
ter number. This will result in all hits that belong
to that cluster as well as other information about
the cluster, such as average length of hits, number
of hits and the year of its first occurrence. On the
right, we see a small snippet of the results. For ev-
ery matching hit we can see the name of the orig-
inal file, date when that issue was published, the
name of the newspaper or journal, URL for view-
ing the original scanned document online, cluster
number and the text itself with the query hits high-
lighted. Clicking the cluster number link shows
all hits, i.e. occurrences of the same repeated text
passage, within the cluster. Finally on the left we
have a facet view, currently giving an overview of
hits from a specific magazine. The rich query lan-
guage employed by Solr gives us the capability of
performing fuzzy and proximity search, which is
especially useful in our case of low-quality OCR-
recognized documents.

As one would expect from a mature search en-
gine like Solr, querying this large collection of re-

peated text clusters is effortless and real-time. For
instance, querying for kissa, the Finnish word for
a cat, found over 23,000 results, returning the first
page of 10 results in 38ms.

4 Conclusions

The ability to identify text repetition in large his-
torical corpora is of great importance to histori-
ans, and the problem of fuzzy match in large text
collections is of a broader interest in corpus re-
search and NLP in general. We have presented
a fully implemented and currently deployed soft-
ware and web interface to identify repeated text
passages in large corpora of poor OCR quality, and
present them through a simple web interface. We
have shown that the BLAST algorithm works effi-
ciently in identifying regions of similarity in his-
torical text corpora, even in cases where the qual-
ity of OCR is extremely low, for instance where
the original text has been printed with Gothic type-
set (Fraktur), or with poor paper and ink quality.
The development of new tools for text reuse detec-
tion is essential for further enhancement of the use
of scanned historical documents, and work with
noisy corpora in general.

Acknowledgments

The work was supported by the research consor-
tium Computational History and the Transforma-
tion of Public Discourse in Finland, 1640-1910,
funded by the Academy of Finland. Computa-
tional resources were provided by CSC — IT Cen-
tre for Science, Espoo, Finland.

332



References
Stephen F. Altschul, Warren Gish, Webb Miller, Eu-

gene W. Myers, and David J. Lipman. 1990. Basic
local alignment search tool. Journal of Molecular
Biology, 215(3):403–410, Oct.

Vincent D Blondel, Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal
of Statistical Mechanics: Theory and Experiment,
2008(10):P10008.

Kimmo Kettunen, Tuula Pääkkönen, and Mika Koisti-
nen. 2016. Between diachrony and synchrony:
Evaluation of lexical quality of a digitized historical
finnish newspaper and journal collection with mor-
phological analyzers. In Baltic HLT.

David A. Smith, Ryan Cordell, Elizabeth Maddock
Dillon, Nick Stramp, and John Wilkerson. 2014.
Detecting and modeling local text reuse. In Pro-
ceedings of the 14th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries, JCDL ’14, pages 183–
192, Piscataway, NJ, USA. IEEE Press.

333


