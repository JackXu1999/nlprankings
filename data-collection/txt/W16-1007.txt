



















































CaTeRS: Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures


Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 51–61,
San Diego, California, June 17, 2016. c©2016 Association for Computational Linguistics

CaTeRS: Causal and Temporal Relation Scheme
for Semantic Annotation of Event Structures

Nasrin Mostafazadeh1, Alyson Grealish1, Nathanael Chambers2,
James Allen1,4, Lucy Vanderwende3

1 University of Rochester, 2 United States Naval Academy,
3 Microsoft Research, 4 The Institute for Human & Machine Cognition

{nasrinm,agrealis,james}@cs.rochester.edu

nchamber@usna.edu, lucyv@microsoft.com

Abstract

Learning commonsense causal and temporal
relation between events is one of the major
steps towards deeper language understanding.
This is even more crucial for understanding
stories and script learning. A prerequisite
for learning scripts is a semantic framework
which enables capturing rich event structures.
In this paper we introduce a novel seman-
tic annotation framework, called Causal and
Temporal Relation Scheme (CaTeRS), which
is unique in simultaneously capturing a com-
prehensive set of temporal and causal relations
between events. By annotating a total of 1,600
sentences in the context of 320 five-sentence
short stories sampled from ROCStories cor-
pus, we demonstrate that these stories are in-
deed full of causal and temporal relations.
Furthermore, we show that the CaTeRS an-
notation scheme enables high inter-annotator
agreement for broad-coverage event entity an-
notation and moderate agreement on semantic
link annotation.

1 Introduction

Understanding events and their relations in natural
language has become increasingly important for var-
ious NLP tasks. Most notably, story understand-
ing (Charniak, 1972; Winograd, 1972; Turner, 1994;
Schubert and Hwang, 2000) which is an extremely
challenging task in natural language understanding,
is highly dependent on understanding events and
their relations. Recently, we have witnessed a re-
newed interest in story and narrative understanding
based on the progress made in core NLP tasks.

Perhaps the biggest challenge of story under-
standing (and story generation) is having common-
sense knowledge for the interpretation of narrative
events. This commonsense knowledge can be best
represented as scripts. Scripts present structured
knowledge about stereotypical event sequences to-
gether with their participants. A well known script
is the Restaurant Script, which includes the events
{Entering, Sitting down, Asking for menus, Choos-
ing meals, etc.}, and the participants {Customer,
Waiter, Chef, Tables, etc.}. A large body of work in
story understanding has focused on learning scripts
(Schank and Abelson, 1977). Given that develop-
ing hand-built scripts is extremely time-consuming,
there is a serious need for automatically induced
scripts (Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Balasubramanian et al., 2013;
Cheung et al., 2013; Nguyen et al., 2015). It is evi-
dent that various NLU applications (text summariza-
tion, co-reference resolution and question answer-
ing, among others) can benefit from the rich infer-
ential capabilities that structured knowledge about
events can provide.

The first step for any script learner is to decide
on a corpus to drive the learning process. The
most recent resource for this purpose is a corpus
of short commonsense stories, called ROCStories
(Mostafazadeh et al., 2016), which is a corpus of
40,000 short commonsense everyday stories 1. This
corpus contains high quality2 five-sentence stories

1These stories can be found here: http://cs.
rochester.edu/nlp/rocstories

2Each of these stories have the following major characteris-
tics: is realistic, has a clear beginning and ending where some-
thing happens in between, does not include anything irrelevant

51



that are full of stereotypical causal and temporal re-
lations between events, making them a perfect re-
source for learning narrative schemas.

One of the prerequisites for learning scripts from
these stories is to extract events and find inter-event
semantic relations. Earlier work (Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009; Pichotta
and Mooney, 2014; Rudinger et al., 2015) defines
verbs as events and uses TimeML-based (Puste-
jovsky et al., 2003) learning for temporal ordering
of events. This clearly has many shortcomings, in-
cluding, but not limited to (1) not capturing a wide
range of non-verbal events such as ‘earthquake’, (2)
not capturing a more comprehensive set of semantic
relations between events such as causality, which is
a core relation in stories.

In this paper we formally define a new compre-
hensive semantic framework for capturing stereo-
typical event-event temporal and causal relations in
commonsense stories, the details of which can be
found in Sections 2-4. Using this semantic frame-
work we annotated 320 stories sampled from ROC-
Stories to extract inter-event semantic structures.
Our inter-annotator agreement analysis, presented in
Section 5 shows that this framework enables high
event entity annotation agreement and promising
inter-event relation annotation agreement. We be-
lieve that our semantic framework better suits the
goals of the task of script learning and story un-
derstanding, which can potentially enable learning
richer and more accurate scripts. Although this work
focuses on stories, the CaTeRS annotation frame-
work for capturing inter-event relations can be ap-
plied to other genres.

2 Event Entities

Our semantic framework captures the set of event
entities and their pairwise semantic relations, which
together form an inter-connected network of events.
In this Section we define event entities and discuss
their annotation process.

2.1 Definition

Event is mainly used as a term referring to any sit-
uation that can happen, occur, or hold. The def-
inition and detection of events has been a topic

to the core story.

of interest in various NLP applications. However,
there is still no consensus regarding the span of
events and how they should be annotated. There has
been some good progress in domain-specific anno-
tation of events, e.g., recent Clinical TempEval task
(Bethard, 2013) and THYME annotation scheme
(Styler et al., 2014), however, the detection of events
in broad-coverage natural language has been an on-
going endeavor in the field.

One of the existing definitions for event is pro-
vided in the TimeML annotation schema (Puste-
jovsky et al., 2003):

“An event is any situation (including a process or state) that
happens, occurs, or holds to be true or false during some
time point (punctual) or time interval (durative).”

According to this definition, adjectival states such
as ‘on board’ are also annotated as events. As we
are focused on the task of narrative structure learn-
ing, we want to capture anything that ‘happens and
occurs’, but not including holds. Formally, we there-
fore define an event as follows:

“An event is any situation (including a process or state) that
happens or occurs either instantaneously (punctual) or dur-
ing a period of time (durative).”

In order to make the event annotation less subjec-
tive, we specifically define an event to be any lexical
entry under any of the following ontology types in
the TRIPS ontology3 (Allen et al., 2008):

– Event-of-state: e.g., have, lack.

– Event-of-change: e.g., kill, delay, eat.

– Event-type: e.g., effort, procedure, turmoil,
mess, fire.

– Physical-condition: all medical disorders and
conditions, e.g., cancer, heart attack, stroke, etc.

– Occurring: e.g., happen, occur.

– Natural-phenomenon: e.g., earthquake, tsunami.
This ontology has one of the richest event hierar-

chies, which perfectly serves our purpose of broad-
coverage event extraction.

2.2 How to annotate events?
After pinpointing an event entity according to the
formal definition presented earlier, one should an-
notate the event by selecting the corresponding span

3http://www.cs.rochester.edu/research/
trips/lexicon/browse-ont-lex-ajax.html

52



in the sentence. Here, we define the event span to
be the head of the main phrase which includes the
complete event. For example, in the sentence ‘I
[climbed] the tree’, we annotate ’climb’, the head
of the verb phrase, while the complete event is
‘[climb] tree’ . This implies that only main events
(and not their dependents) are annotated. Annotat-
ing the head word enables us to delegate the deci-
sion about adding the dependent children to a post-
process, which can be tuned per application.

Moreover, no verbs which take the role of an aux-
iliary verb are annotated as events. For instance,
in the sentence ‘She had to [change] her jeans’ the
main event is ‘change’. For multi-word verbs such
as ‘turn out or ‘find out’, the entire span should be
selected as the event. The annotators can consult
lexicons such as WordNet (Miller., 1995) for distin-
guishing multi-word verbs from verbs with preposi-
tional phrases adjuncts.

2.2.1 The Case for Embedded Events
Another important controversial issue is what to

do with embedded events, where one event takes an-
other event as its core argument (if neither of the
verbs are auxiliary). For instance, consider the fol-
lowing example:

(1) Sam [wanted]e1 to [avoid]e2 any trouble, so
he [drove]e3 slowly.

According to our event entity definition, there are
three events in example 1, e1 and e2 and e3, all of
which should be annotated. However, more compli-
cated is the case of a main event in an embedded
event construction which signals any of the seman-
tic relations in the annotation scheme. Consider the
sentence in example 2. In this sentence, there are
also three events according to our definition of event
entities, where (cause (die)) is an embedded event
construction. In this case the verb ‘cause’ simply
signals a causal relation between e1 and e3, which
will be captured by our existing semantic relation (to
be described in Section 3), and so we do not annotate
the verb ‘cause’ as an event.

Likewise, the sentence in example 3 showcases
another embedded event construction, (cause (ex-
plosion)), so the event ‘cause’ should not be anno-
tated.

(2) The [explosion]e1 caused him to [die]e3.

(3) The [fire]e1 caused an [explosion]e2.

The same rule applies to the synonyms of these
verbs in addition to other verbs that signal a temporal
or causal relation, including but not limited to {start,
begin, end, prevent, stop, trigger}, which hereinafter
we call ‘aspectual verbs’4. It is important to note
that the above rule for aspectual verbs can be applied
only to embedded event constructions and may be
overridden. Consider example 4. In this example, it
is clear that the event ‘prevent’ plays a key semantic
role in the sentence and should be annotated as an
event since it is the only viable event that can be
semantically connected to other events such as e3.

(4) John [prevented]e1 the vase from [falling]e2
off the table, I was [relieved]e3.

2.3 The Case for Copulas

A copula is a verb which links the subject of a sen-
tence with a predicate, such as the verb ‘is’ which
links ‘this suit’ to the predicate ‘dark blue’ in the
sentence ‘This suit is dark blue’. Many such con-
structions assign a state to something or someone
which holds true for some duration. The question is
what to specify as the event entity in such sentences.
According to our definitions, an adjective such as
‘blue’ is not an event (that is, it does not occur or
happen), but after many rounds of pilot annotations,
we concluded that annotating the predicate adjective
or predicate nominal best captures the core semantic
information. Thus, the sentences 5-6 will be anno-
tated as follows:

(5) He was really [hungry]e1.

(6) He [ate]e1 a juicy burger.

It is important to emphasize that annotating states
such as ‘hungry’ as an event is only done in the case
of copulas, and, for example in sentence 6, the ad-
jective ‘juicy’ will not be annotated as an event.

Our annotation of light verb constructions (e.g.,
do, make, have) is consistent with the annotation of
copulas and auxiliary verbs. Whenever the seman-
tic contribution of the verb is minimal and the non-
verb element of the construction is an event in the

4These verbs are the same as aspectual events characterized
by TimeML, which include ‘INITIATES’, ‘CULMINATES’,
‘TERMINATES’, ‘CONTINUES’ and ‘REINITIATES’.

53



TRIPS ontology, we annotate the non-verb element
as the event. Thus, we annotate the noun predicate
’offer’ in the sentence ‘Yesterday, John made an of-
fer to buy the house for 350,000’, similarly to the
way Abstract Meaning Representation (AMR) drops
the light verb and promotes the noun predicate (Ba-
narescu et al., 2013). This annotation is also close to
the PropBank annotation of copulas and light verbs
(Bonial et al., 2014), where they annotate the noun
predicate and predicate adjective as the event; how-
ever, PropBank includes an explicit marking of the
verb as either a light verb or a copula verb.

3 The Semantic Relations Between Event
Entities

A more challenging problem than event entity de-
tection is the identification of the semantic relation
that holds between events. Events take place in
time, hence temporal relations between events are
crucial to study. Furthermore, causality plays a cru-
cial role in establishing semantic relation between
events, specifically in stories. In this Section, we
provide details on both temporal and causal seman-
tic relations.

3.1 Temporal Relation

Time is the main notion for anchoring the changes
of the world triggered by sequences of events. Of
course having temporal understanding and tempo-
ral reasoning capabilities is crucial for many NLP
applications such as question answering, text sum-
marization and many others. Throughout the years
the issue of temporal analysis and reasoning in nat-
ural language has been addressed via different ap-
proaches. Allen’s Interval Algebra (Allen, 1984) is
one theory for representing actions and introduces a
set of 13 distinct, exhaustive, and qualitative tempo-
ral relations that can hold between two time inter-
vals. The first three columns of Table 1 list these
13 temporal relations together with their visualiza-
tion, which includes 6 main relations and their corre-
sponding inverses –together with the ‘equal’ relation
which does not have an inverse.

Based on Interval Algebra, a new markup lan-
guage for annotating events and temporal expres-
sions in natural language was proposed (Pustejovsky
et al., 2003), named TimeML. This schema is de-

signed to address problems in event and tempo-
ral expression markup. It covers two major tags:
‘EVENT’ and ‘TIMEX3’. The EVENT tag is used
to annotate elements in a text that represent events
such as ‘kill’ and ‘crash’. TIMEX is mainly used
to annotate explicit temporal expressions, such as
times, dates and durations. One of the major features
introduced in TimeML was the LINK tag. These
tags encode the semantic relations that exist between
the temporal elements annotated in a document. The
most notable LINK is TLINK: a Temporal Link rep-
resenting the temporal relationship between entities
(events and time expressions). This link not only
encodes a relation between two entities, but also
makes a partial ordering between events. There
are 14 TLINK relations in TimeML, adding ‘si-
multaneous’ to the list of temporal links between
events. The fourth column of Table 1 shows the cor-
respondence between Allen relations and TimeML
TLINKs. Furthermore, in the fifth column we in-
clude the THYME annotation schema (to be dis-
cussed in Section 6).

We propose a new set of temporal relations for
capturing event-event relations. Our final set of tem-
poral relations are shown in the sixth column of Ta-
ble 15. As compared with TimeML, we drop the
relations ‘simultaneous’, ‘begins’ and ‘ends’. ‘Si-
multaneous’ was not a part of the original Allen
relations. Generally it is hard to be certain about
two events occurring exactly during the same time
span, starting together and ending together. Indeed,
the majority of events which are presumed ‘simulta-
neous’ in TimeML annotated corpora are either (1)
EVENT-TIMEX relations which are not event-event
relations, or (2) wrongly annotated and should be
the ‘overlapping’ relation, e.g., in the following sen-
tence from TimeBank corpus the correct relation for
the two events e1 and e2 should be ‘overlap’:

She [listened]e1 to music while [driving]e2.
We acknowledge that having ’simultaneous’ can
make the annotation framework more comprehen-
sive and may apply in few certain cases of punc-
tual events, however, such cases are very rare in
our corpus, and in the interest of a more compact
and less ambiguous annotation, we did not include

5Since a main temporal relation and its inverse have a re-
flexive relation, the annotation is carried out only on the main
temporal relation.

54



Allen Visualization Allen - Inverse TimeML THYME CaTeRS

X Before Y
X Y

Y After X Before Before Before

X Meets Y
X Y

Y Is Met X IBefore (Immediately) - -

X Overlaps Y
X

Y Y Is overlapped by X - Overlaps Overlaps

X Finishes Y
X

Y Y Is finished by X Ends Ends-on -

X Starts Y
X

Y Y Is started by X Begins Begins-on -

X Contain Y
X
Y Y During X During Contains Contains

X Equals Y X/Y - Identity Identity Identity

-
X
Y - Simultaneous - -

Table 1: The correspondence of temporal relation sets of different annotation frameworks.

it. THYME also dropped ‘simultaneous’ for similar
reasons.

As for the ‘begins’ and ‘ends’, our multiple pilot
studies, indicated that these relations are more accu-
rately captured by one of our causal relations (next
subsection) or the relation ‘overlaps’. We believe
that our simplified set of 4 temporal relations can be
used for any future broad-coverage inter-event tem-
poral relation annotation. We also drop the temporal
relation ‘IBefore’, given that this relation usually re-
flects on causal relation between two events which
will be captured by our causal links.

3.2 Causal Relation
Research on the extraction of event relations has
concerned mainly the temporal relation and time-
wise ordering of events. A more complex semantic
relationship between events is causality. Causality
is one of the main semantic relationships between
events where an event (CAUSE) results in another
event (EFFECT) to happen or hold. It is clear that
identifying the causal relation between events is cru-
cial for numerous applications, including story un-
derstanding. Predicting occurrence of future events
is the major benefit of causal analysis, which can
itself help risk analysis and disaster decision mak-

ing. There is an obvious connection between causal
relation and temporal relation: by definition, the
CAUSE event starts ‘BEFORE’ the EFFECT event.
Hence, predicting causality between two events also
requires/results in a temporal prediction.

It is challenging to define causality in natural lan-
guage. Causation, as commonly understood as a
notion for understanding the world in the philoso-
phy and psychology, is not fully predicated in nat-
ural language (Neeleman and Koot, 2012). There
have been several attempts in the field of psychol-
ogy for modeling causality, e.g., the counterfactual
model (Lewis, 1973) and the probabilistic contrast
model (Cheng and Novick, 1992). Leonard Talmy’s
seminal work (Talmy, 1988) in the field of cogni-
tive linguistics models the world in terms of seman-
tic categories of how entities interact with respect
to force (Force Dynamics). These semantic cate-
gories include concepts such as the employment of
force, resistance to force and the overcoming of re-
sistance, blockage of a force, removal of blockage,
and etc. Force dynamics provides a generalization
over the traditional linguistic understanding of cau-
sation by categorizing causation into ‘letting’, ‘help-
ing’, ‘hindering’ and etc. Wolff and Song (Wolff

55



and Song, 2003) base their theory of causal verbs
on force dynamics. Wolff proposes (Wolff, 2007)
that causation includes three main types of causal
concepts: ‘Cause’, ‘Enable’ and ‘Prevent’. These
three causal concepts are lexicalized through distinc-
tive types of verbs (Wolff and Song, 2003) which are
as follows:

– Cause-type verbs: e.g. cause, start, prompt,
force.

– Enable-type verbs: e.g. allow, permit, enable,
help.

– Prevent-type verbs: e.g. block, prevent, hinder,
restrain.
Wolff’s model accounts for various ways that

causal concepts are lexicalized in language and we
base our annotation framework on this model. How-
ever, we will be looking at causal relation be-
tween events more from a ‘commonsense reason-
ing’ perspective than linguistic markers. We de-
fine cause, enable and prevent for commonsense co-
occurrence of events, inspired by mental model the-
ory of causality (Khemlani et al., 2014), as follows:

– A Cause B: In the context, If A occurs, B most
probably occurs as a result.

– A Enable B: In the context, If A does not oc-
cur, B most probably does not occur (not enabled
to occur).

– A Prevent B: In the context, If A occurs, B
most probably does not occur as a result.

where In the context refers to the underlying con-
text in which A and B occur, such as a story. This
definition is in line with the definition of CAUSE
and PRECONDITION presented in the RED anno-
tation guidelines (Ikuta et al., 2014) (to be discussed
in Section 6).

In order to better understand the notion of com-
monsense causality, consider the sentences 7-9.

(7) Harry [fell]e1 and [skinned]e2 his knee.

(8) Karla [earned]e1 more money and finally
[bought]e2 a house.

(9) It was [raining]e1 so hard that it prevented
me from [going]e26 to the school.

6As discussed earlier, here the embedded event construction
is (prevent (going)) where only the event ‘going’ will be anno-
tated.

In the above three sentences, the relation between e1
and e2 is ‘cause’, ‘enable’ and ‘prevent’ in order.

It is important to note that our scope of lexical se-
mantic causality only captures events causing events
(Davidson, 1967), however, capturing individuals as
cause (Croft, 1991) is a another possible extension.

3.2.1 Temporal Implications of Causality
The definition of causality implies that when A

Causes B, then A should start before B in order to
have triggered it. It is important to note that for
durative events the temporal implication of causal-
ity is mainly about the start of the causal event,
which should be before the start of the event which
is caused. Consider the following example:

(10) The [fire]e1 [burned down]e2 the house.

In this example there is a ‘cause’ relation between
e1 and e2, where ‘fire’ clearly does not finish before
starting of the ‘burn’ event. So the temporal rela-
tion between the two events is ‘overlaps’. Here we
conclude that when ‘A cause/enable/prevent B’, we
know as a fact that As start is before Bs start, but
there is no restriction on their relative ending. This
implies that a cause relation can have any of the two
temporal relations: before and overlaps.

All the earlier examples of causal concepts we ex-
plored involved an event causing another event to
happen or to start (in case of a durative event). How-
ever, there are examples of causality which involve
not starting but ending an ongoing event. Consider
the sentence 11. In order to capture causality rela-
tions between pairs of events such as e1 and e2, we
introduce a Cause-to-end relation, which can have
one of the three temporal implications: before, over-
laps, and during. Hence, in sentence 11, the relation
between e1 and e2 will be Cause-to-end (overlap).

(11) The [famine]e1 ended the [war]e2.

3.3 How to annotate semantic relations
between events?

In summary, the disjunctive7 set of 13 semantic re-
lations between events in our annotation framework
are as follows:

7This implies that only one of these semantic relations can
be selected per each event pair.

56



Figure 1: Semantic annotation of a sample story.

– 9 causal relations: Including ‘cause (be-
fore/overlaps)’, ‘enable (before/overlaps)’,
‘prevent (before/overlaps)’, ‘cause-to-end (be-
fore/overlaps/during)’

– 4 temporal relations: Including ‘Before’,
‘Overlaps’, ‘Contains’, ‘Identity’.

The semantic relation annotation between two
events should start with deciding about any causal
relations and then, if there was not any causal rela-
tion, proceed to choosing any existing temporal re-
lation.

4 Annotating at Story level

It has been shown (Bittar et al., 2012) that tempo-
ral annotation can be most properly carried out by
taking into account the full context for sentences, as
opposed to TimeML, which is a surface-based an-
notation. The scope and goal of this paper very well
aligns with this observation. We carry out the an-
notation at the story level, meaning that we annotate
inter-event relations across the five sentences of a
story. It suffices to do the event-event relation speci-
fication minimally given the transitivity of temporal
relations. For example for three consecutive events
e1 e2 e3 one should only annotate the ‘before’ re-
lation between e1 and e2, and e2 and e3, since the
‘before’ relation between e1 and e3 can be inferred
from the other two relations. The event-event rela-

Figure 2: Frequency of semantic links in our dataset.

tions can be from/to any sentence in the story. It is
important to emphasize here that the goal of annota-
tion is to capture commonsensical relations between
events, so the annotation effort should be driven by
intuitive commonsensical relation one can pinpoint
throughout a story.

Consider an example of a fully annotated story
shown in Figure 1. As you can see, all of the
semantic annotations reflect commonsense relation
between events given the underlying story, e.g.,
‘cracked a joke’ cause-before ‘embarrassed’.

5 Annotated Dataset Analysis

We randomly sampled 320 stories (1,600 sentences)
from the ROCStories Corpus. This set covers a va-
riety of everyday stories, with titles ranging from
‘got a new phone’ to ‘Left at the altar’. We pro-
vided our main expert annotator with the annota-
tion guidelines, with the task of annotating each of
the 320 stories at story level. The annotation task
was set up on Brat tool8. On average, annotation
time per story was 11 minutes. These annotations
can be found through http://cs.rochester.
edu/nlp/rocstories/CaTeRS/.

5.1 Statistics
Overall, we have annotated 2,708 event entities and
2,715 semantic relations. Figure 2 depicts the dis-
tribution of different semantic relations in our an-
notation set. For this Figure we have removed any
semantic relations with frequency less than 5. As
you can see, the temporal relation before is the
most common relation, which reflects on the natu-
ral reporting of the sequence of events throughout a

8http://brat.nlplab.org/

57



story. There are overall 488 various causal links, the
most frequent of which is cause-before. Given these
statistics, it is clear that capturing causality along
with temporal aspects of stories is crucial.

Our annotations also enable some deeper analy-
sis of the narrative flow of the stories. One major
question is if the text order of events appearing in
consecutive sentences mirrors the real-world order
of events. Although the real-world order of events
is more complex than just a sequence of before re-
lations, we can simplify our set of semantic links
to make an approximation: we count the number of
links (from any9 type) which connect an event entity
appearing in position X to an event entity appear-
ing in position X − i. We found that temporal or-
der does not match the text order 23% of the time.
This reinforces the quality of the narrative flow in
the ROCStories corpus. Moreover, 23% is statisti-
cally significant enough to motivate the requirement
for temporal ordering models for these stories.

5.2 Inter-annotator Agreement

In order to compute inter-annotator agreement on
our annotation framework, we shared one batch of
20 stories between four expert annotators. Our an-
notation task consists of two subtasks: (1) entity
span selection: choosing non-overlapping event en-
tity spans for each story, (2) semantic structure an-
notation: building a directed graph (most commonly
connected) on top of the entity spans.

5.2.1 Agreement on Event Entities

Given that there are no prefixed set of event en-
tity spans for straight-forward computation of inter-
annotator agreement, we do the following: among
all the annotators, we aggregate the spans of the an-
notated event entity as the annotation object (Art-
stein and Poesio, 2008). Then, if there exists a span
which is not annotated by one of the coders (anno-
tators) it will be labeled as ‘NONE’ for its category.
The agreement according to Fleiss’s Kappa κ = 0.91,
which shows substantial agreement on event entity
annotation. Although direct comparison of κ val-
ues is not possible, as a point of reference, the event

9This is based on the fact that any relation such as ‘A enable-
before B’ or ‘A overlaps B’ can be naively approximated to ‘A
before B’.

span annotation of the most recent clinical TempE-
val (Bethard et al., 2015) was 0.81.

5.2.2 Agreement on Semantic Links
Decisions on semantic links are dependent on two

things (1) decisions on event entities; (2) the deci-
sion about the other links. Hence, the task of anno-
tating event structures is in general a hard task. In
order to relax the dependency on event entities, we
fix the set of entities to be the ones that all annota-
tors have agreed on. Following the other discourse
structure annotation tasks such as Rhetorical Struc-
ture Theory (RST), we aggregate all the relations
captured by all annotators as the annotation object,
then labeling ‘NONE’ as the category for coders
who have not captured this relation. The agreement
according to Fleiss’s Kappa κ = 0.49 without ap-
plying basic closure and κ = 0.51 with closure10,
which shows moderate agreement. For reference,
the agreement on semantic link annotation in the
most recent clinical TempEval was 0.44 without clo-
sure and 0.47 with closure.

6 Related Work

One of the most recent temporal annotation schemas
is Temporal Histories of Your Medical Event
(THYME) (Styler et al., 2014). This annotation
guideline was devised for the purpose of establish-
ing timelines in clinical narratives, i.e. the free
text portions contained in electronic health records.
In their work, they combine the TimeML annota-
tion schema with Allen Interval Algebra, identifying
the five temporal relations BEFORE, OVERLAP,
BEGINS-ON, ENDS-ON, and CONTAINS. Of note
is that they adopt the notion of narrative contain-
ers (Pustejovsky and Stubbs, 2011), which are time
slices in which events can take place, such as DOC-
TIME (time of the report) and before DOCTIME.
As such, the THYME guideline focuses on ordering
events with respect to specific time intervals, while
in our work, we are only focused on the relation
between two events, without concern for ordering.
Their simplification of temporal links is similar to
ours, however, our reasoning for simplification takes

10Temporal closure (Gerevini et al., 1995) is a reasoning for
deriving explicit relations to implicit relations, applying rules
such as transitivity.

58



into account the existence of causality, which is not
captured by THYME.

Causality is a notion that has been widely stud-
ied in psychology, philosophy, and logic. However,
precise modeling and representation of causality in
NLP applications is still an open issue. A formal
definition of causality in lexical semantics can be
found in (Hobbs, 2005). Hobbs introduces the no-
tion of “causal complex”, which refers to some col-
lection of eventualities (events or states) for which
holding or happening entails the happening of ef-
fect. In part, our annotation work is motivated to
learn what the causal complexes are for a given event
or state. The Penn Discourse Tree Bank (PDTB)
corpus (Prasad et al., 2008) addresses the annota-
tion of causal relations, annotating semantic rela-
tions that hold between exactly two Abstract Ob-
jects (called Arg1 and Arg2), expressed either ex-
plicitly via lexical items or implicitly via adjacency
in discourse. In this paper, we present a semantic
framework that captures both explicit and implicit
causality, but no constraint of adjacency is imposed,
allowing commonsense causality to be captured at
the inter-sentential level (story).

Another line of work annotates temporal and
causal relations in parallel (Steven Bethard and Mar-
tin, 2008). Bethard et al. annotated a dataset
of 1,000 conjoined-event temporal-causal relations,
collected from Wall Street Journal corpus. Each
event pair was annotated manually with both tem-
poral (BEFORE, AFTER, NO-REL) and causal re-
lations (CAUSE, NO-REL). For example, sentence
12 is an entry in their dataset. This dataset makes no
distinction between various types of causal relation.

(12) Fuel tanks had leaked and contaminated the soil.
- (leaked BEFORE contaminated)
- (leaked CAUSED contaminated).

A recent work (Mirza and Tonelli, 2014) has
proposed a TimeML-style annotation standard for
capturing causal relations between events. They
mainly introduce ‘CLINK’, analogous to ‘TLINK’
in TimeML, to be added to the existing TimeML link
tags. Under this framework, Mirza et al (Mirza and
Tonelli, 2014) annotates 318 CLINKs in TempEval-
3 TimeBank. They only annotate explicit causal
relations signaled by linguistic markers, such as
{because of, as a result of, due to, so, therefore,

thus}. Another relevant work is Richer Event De-
scriptions (RED) (Ikuta et al., 2014), which com-
bines event coreference and THYME annotations,
and also introduces cause-effect annotation in adja-
cent sentences to achieve a richer semantic represen-
tation of events and their relations. RED also distin-
guishes between ‘PRECONDITION’ and ‘CAUSE’,
similarly to our ‘ENABLE’ and ‘CAUSE’ rela-
tions. These can be in the context of BEFORE
or OVERLAP, but they do not include PREVENT
and CAUSE-TO-END. Our set of comprehensive 9
causal relations distinguishes between various tem-
poral implications, not covered by any of the related
work.

7 Conclusion

In this paper we introduced a novel framework
for semantic annotation of event-event relations in
commonsense stories, called CaTeRS. We annotated
1,600 sentences throughout 320 short stories sam-
pled from ROCStories corpus, capturing 2,708 event
entities and 2,715 semantic relations, including 13
various types of causal and temporal relations. This
annotation scheme is unique in capturing both tem-
poral and causal inter-event relations. We show that
our annotation scheme enables high inter-annotator
agreement for event entity annotation. This is due to
our clear definition of events which (1) is linked to
lexical entries in TRIPS ontology, removing prob-
lems caused by each annotator devising his or her
own notion of event, (2) captures only the head of
the underlying phrase.

Our inter-annotator analysis of semantic link an-
notation shows moderate agreement, competitive
with earlier temporal annotation schemas. We are
planning to improve the agreement on semantic
links even further by setting up two-stage expert an-
notation process, where we can pair annotators for
resolving disagreements and modifying the annota-
tion guideline. A comprehensive study of tempo-
ral and causal closure in our framework is a future
work.

We believe that our semantic framework for tem-
poral and causal annotation of stories can better
model the event structures required for script and
narrative structure learning. Although this work fo-
cuses on stories, our annotation framework for cap-

59



turing inter-event relations can be applicable to other
genres. It is important to note that this framework is
not intended to be a comprehensive analysis of all
temporal and causal aspects of text, but rather we
focus on those temporal and causal links that sup-
port learning stereotypical narrative structures. As
with any annotation framework, this framework will
keep evolving over time, the updates of which can
be followed through http://cs.rochester.
edu/nlp/rocstories/CaTeRS/.

Acknowledgments

We thank Martha Palmer and the anonymous re-
viewers for their invaluable and extensive comments
on this paper. This work was supported in part
by Grant W911NF-15-1-0542 with the US Defense
Advanced Research Projects Agency (DARPA), the
Army Research Office (ARO) and Office of Naval
Research (ONR).

References

James F. Allen, Mary Swift, and Will de Beaumont.
2008. Deep semantic analysis of text. In Proceedings
of the 2008 Conference on Semantics in Text Process-
ing, STEP ’08, pages 343–354, Stroudsburg, PA, USA.
Association for Computational Linguistics.

James F. Allen. 1984. Towards a general theory of action
and time. Artif. Intell., 23(2):123–154, July.

Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput. Lin-
guist., 34(4):555–596, December.

Niranjan Balasubramanian, Stephen Soderland, Oren Et-
zioni Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In EMNLP, pages
1721–1731.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation for
sembanking.

Steven Bethard, Leon Derczynski, Guergana Savova,
James Pustejovsky, and Marc Verhagen. 2015.
Semeval-2015 task 6: Clinical tempeval. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015), pages 806–814, Denver,
Colorado, June. Association for Computational Lin-
guistics.

Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference

on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10–14, Atlanta, Georgia, USA. Association for Com-
putational Linguistics.

Andr Bittar, Caroline Hagge, Vronique Moriceau, Xavier
Tannier, and Charles Teissdre. 2012. Temporal an-
notation: A proposal for guidelines and an experiment
with inter-annotator agreement. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Thierry De-
clerck, Mehmet Uur Doan, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).

Claire Bonial, Julia Bonn, Kathryn Conger, Jena D.
Hwang, and Martha Palmer. 2014. Propbank: Se-
mantics of new predicate types. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Ninth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC’14), Reykjavik, Iceland, may. European
Language Resources Association (ELRA).

Nathanael Chambers and Daniel Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Kath-
leen McKeown, Johanna D. Moore, Simone Teufel,
James Allan, and Sadaoki Furui, editors, ACL, pages
789–797. The Association for Computer Linguistics.

Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ’09,
pages 602–610, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Eugene Charniak. 1972. Toward a model of children’s
story comprehension. December.

Patricia W. Cheng and Laura R. Novick. 1992. Covari-
ation in natural causal induction. Psychological Re-
view, 99(2):365382.

Jackie Cheung, Hoifung Poon, and Lucy Vanderwende.
2013. Probabilistic frame induction. In ACL.

William. Croft. 1991. Syntactic categories and gram-
matical relations : the cognitive organization of infor-
mation / William Croft. University of Chicago Press
Chicago.

Donald Davidson. 1967. Causal relations. Journal of
Philosophy, 64(21):691–703.

60



Alfonso Gerevini, Lenhart K. Schubert, and Stephanie
Schaeffer. 1995. The temporal reasoning tools time-
graph i-ii. International Journal on Artificial Intelli-
gence Tools, 4(1-2):281–300.

Jerry R. Hobbs. 2005. Toward a useful concept of
causality for lexical semantics. Journal of Semantics,
22(2):181–209.

Rei Ikuta, Will Styler, Mariah Hamang, Tim O’Gorman,
and Martha Palmer. 2014. Challenges of adding cau-
sation to richer event descriptions. In Proceedings
of the Second Workshop on EVENTS: Definition, De-
tection, Coreference, and Representation, pages 12–
20, Baltimore, Maryland, USA, June. Association for
Computational Linguistics.

Sangeet Khemlani, Aron K Barbey, and Philip Nicholas
Johnson-Laird. 2014. Causal reasoning with mental
models. Frontiers in Human Neuroscience, 8(849).

David Lewis. 1973. Counterfactuals. Blackwell Pub-
lishers, Oxford.

G. Miller. 1995. Wordnet: A lexical database for english.
In In Communications of the ACM.

Paramita Mirza and Sara Tonelli. 2014. An analysis
of causality between events and its relation to tem-
poral information. In COLING 2014, 25th Interna-
tional Conference on Computational Linguistics, Pro-
ceedings of the Conference: Technical Papers, August
23-29, 2014, Dublin, Ireland, pages 2097–2106.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A corpus
and cloze evaluation for deeper understanding of com-
monsense stories. In Proceedings of NAACL HLT,
San Diego, California, June. Association for Compu-
tational Linguistics.

Ad Neeleman and Hans Van De Koot. 2012. The theta
system: Argument structure at the interface. The Lin-
guistic Expression of Causation, pages 20–51.

Kiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret, and
Romaric Besançon. 2015. Generative event schema
induction with entity disambiguation. In Proceedings
of the 53rd annual meeting of the Association for Com-
putational Linguistics (ACL-15).

Karl Pichotta and Raymond J Mooney. 2014. Statisti-
cal script learning with multi-argument events. EACL
2014, page 220.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K. Joshi, and Bon-
nie L. Webber. 2008. The penn discourse treebank
2.0. In LREC. European Language Resources Associ-
ation.

James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceed-
ings of the 5th Linguistic Annotation Workshop, LAW

V ’11, pages 152–160, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

James Pustejovsky, Jos Castao, Robert Ingria, Roser
Saur, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. Timeml: Robust specification of event
and temporal expressions in text. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5.

Rachel Rudinger, Pushpendre Rastogi, Francis Ferraro,
and Benjamin Van Durme. 2015. Script induction as
language modeling. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-15).

Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: an Inquiry into Hu-
man Knowledge Structures. L. Erlbaum, Hillsdale, NJ.

Lenhart K. Schubert and Chung Hee Hwang. 2000.
Episodic logic meets little red riding hood: A com-
prehensive, natural representation for language un-
derstanding. In Natural Language Processing and
Knowledge Representation: Language for Knowledge
and Knowledge for Language. MIT/AAAI Press.

Sara Klingenstein Steven Bethard, William Corvey
and James H. Martin. 2008. Building a cor-
pus of temporal-causal structure. In Bente Mae-
gaard Joseph Mariani Jan Odijk Stelios Piperidis
Daniel Tapias Nicoletta Calzolari (Conference Chair),
Khalid Choukri, editor, Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC’08), Marrakech, Morocco, may.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.

IV William F. Styler, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet C. de Groen, Brad Er-
ickson, Timothy Miller, Chen Lin, Guergana Savova,
and James Pustejovsky. 2014. Temporal annotation in
the clinical domain. Transactions of the Association
for Computational Linguistics, 2:143–154.

Leonard Talmy. 1988. Force dynamics in language and
cognition. Cognitive Science, 12(1):49–100.

Scott R. Turner. 1994. The creative process: A computer
model of storytelling. Hillsdale: Lawrence Erlbaum.

Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Inc., Orlando, FL, USA.

Phillip Wolff and Grace Song. 2003. Models of cau-
sation and the semantics of causal verbs. Cognitive
Psychology, 47(3):276–332.

Phillip Wolff. 2007. Representing causation. Journal of
Experiment Psychology: General, 136:82111.

61


