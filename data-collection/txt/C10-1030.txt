Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 259–267,

Beijing, August 2010

259

Generating Learner-Like Morphological Errors in Russian

Markus Dickinson
Indiana University

md7@indiana.edu

Abstract

To speed up the process of categorizing
learner errors and obtaining data for lan-
guages which lack error-annotated data,
we describe a linguistically-informed
method for generating learner-like mor-
phological errors, focusing on Russian.
We outline a procedure to select likely er-
rors, relying on guiding stem and sufﬁx
combinations from a segmented lexicon to
match particular error categories and rely-
ing on grammatical information from the
original context.

Introduction

1
Work on detecting grammatical errors in the lan-
guage of non-native speakers covers a range of
errors, but it has largely focused on syntax in
a small number of languages (e.g., Vandeven-
ter Faltin, 2003; Tetreault and Chodorow, 2008).
In more morphologically-rich languages, learn-
ers naturally make many errors in morphology
(Dickinson and Herring, 2008). Yet for many lan-
guages, there is a major bottleneck in system de-
velopment: there are not enough error-annotated
learner corpora which can be mined to discover
the nature of learner errors, let alone enough data
to train or evaluate a system. Our perspective is
that one can speed up the process of determin-
ing the nature of learner errors via semi-automatic
means, by generating plausible errors.

We set out to generate linguistically-plausible
morphological errors for Russian, a language with
rich inﬂections. Generating learner-like errors has
practical and theoretical beneﬁts. First, there is
the issue of obtaining training data; as Foster and

Andersen (2009) state, “The ideal situation for a
grammatical error detection system is one where a
large amount of labelled positive and negative ev-
idence is available.” Generated errors can bridge
this gap by creating realistic negative evidence
(see also Rozovskaya and Roth, 2010). As for
evaluation data, generated errors have at least one
advantage over real errors, in that we know pre-
cisely what the correct form is supposed to be, a
problem for real learner data (e.g., Boyd, 2010).

By starting with a coarse error taxonomy, gen-
erating errors can improve categorization. Gener-
ated errors provide data for an expert—e.g., a lan-
guage teacher—to search through, expanding the
taxonomy with new error types or subtypes and/or
deprecating error types which are unlikely. Given
the lack of real learner data, this has the potential
to speed up error categorization and subsequent
system development. Furthermore, error genera-
tion techniques can be re-used, adjusting the er-
rors for different learner levels, ﬁrst languages,
and so forth.

The error generation process can beneﬁt by us-
ing linguistic properties to mimic learner varia-
tions. This can lead to more realistic errors, a ben-
eﬁt for machine learning (Foster and Andersen,
2009), and can also provide feedback for the lin-
guistic representation used to generate errors by,
e.g., demonstrating under which linguistic condi-
tions certain error types are generated and under
which they are not.

We are speciﬁcally interested in generating
Russian morphological errors. To do this, we need
a knowledge base representing Russian morphol-
ogy, allowing us to manipulate linguistic proper-
ties. After outlining the coarse error taxonomy

260

(section 2), we discuss enriching a part-of-speech
(POS) tagger lexicon with segmentation informa-
tion (section 3). We then describe the steps in er-
ror generation (section 4), highlighting decisions
which provide insight for the analysis of learner
language, and show the impact on POS tagging in
section 5.

2 Error taxonomy
Russian is an inﬂecting language with relatively
free word order, meaning that morphological syn-
tactic properties are often encoded by afﬁxes. In
(1a), for example, the verb начина needs a suf-
ﬁx to indicate person and number, and ет is the
third person singular form.1 By contrast, (1b) il-
lustrates a paradigm error: the sufﬁx ит is third
singular, but not the correct one. Generating such
a form requires having access to individual mor-
phemes and their linguistic properties.

(1) a. начина+ет

begin-3s

[nachina+et]

b. *начина+ит

begin-3s

[nachina+it]
(diﬀ. verb paradigm)

This error is categorized as a sufﬁx error in ﬁg-
ure 1, expanding the taxonomy in Dickinson and
Herring (2008). Stem errors are similarly catego-
rized, with Semantic errors deﬁned with respect
to a particular context (e.g., using a different stem
than required by an activity).

For formation errors (#3), one needs to know
how stems relate.
For instance, some verbs
change their form depending on the sufﬁx, as in
(2). In (2c), the stem and sufﬁx are morpholog-
ically compatible, just not a valid combination.
One needs to know that мож is a variant of мог.

(2) a. мог+ут
can-3p

[mog+ut]

b. мож+ет

can-3s

[mozh+et]

c. *мож+ут

can-3p

[mozh+ut] (#3)
(wrong formation)

Using a basic lexicon without such knowledge,
it is hard to tell formation errors apart from lex-

1For examples, we write the Cyrillic form and include a

Roman transliteration (SEV 1362-78) for ease of reading.

0. Correct: The word is well-formed.
1. Stem errors:

(a) Stem spelling error
(b) Semantic error

2. Sufﬁx errors:

(a) Sufﬁx spelling error
(b) Lexicon error:

i. Derivation error: The wrong POS is

used (e.g., a noun as a verb).

ii. Inherency error: The ending is for a
different subclass (e.g., inanimate as
an animate noun).

(c) Paradigm error: The ending is from the

wrong paradigm.

3. Formation errors: The stem does not follow

appropriate spelling/sound change rules.

4. Syntactic errors: The form is correct, but
used in an in appropriate syntactic context
(e.g., nominative case in a dative context)

• Lexicon incompleteness: The form may be

possible, but is not attested.

Figure 1: Error taxonomy

If мо-
icon incompleteness (see section 4.2.2).
жут (2c) is generated and is not in the lexicon,
we do not know whether it is misformed or simply
unattested. In this paper, we group together such
cases, since this allows for a simpler and more
quickly-derivable lexicon.

We have added syntactic errors, whereas Dick-
inson and Herring (2008) focused on strictly mor-
phological errors. Learners make syntactic errors
(e.g., Rubinstein, 1995; Rosengrant, 1987), and
when creating errors, a well-formed word may re-
sult. In the future, syntactic errors can be subdi-
vided (Boyd, 2010).

This classiﬁcation is of possible errors, making
no claim about the actual distribution of learner
errors, and does not delve into issues such as
errors stemming from ﬁrst language interference
(Rubinstein, 1995). Generating errors from the
possible types allows one to investigate which
types are plausible in which contexts.

261

It should be noted that we focus on inﬂec-
tional morphology in Russian, meaning that we
focus on sufﬁxes. Preﬁxes are rarely used in Rus-
sian as inﬂectional markers; for example, preﬁxes
mark semantically-relevant properties for verbs of
motion. The choice of preﬁx is thus related to
the overall word choice, an issue discussed under
Random stem generation in section 4.2.4.

3 Enriching a POS lexicon
To create errors, we need a segmented lexicon
with morphological information, as in (3). Here,
the word могу (mogu, ‘I am able to’) is split into
stem and sufﬁx, with corresponding POS tags.2

(3) a. мог,Vm-----a-p,у,Vmip1s-a-p

b. мож,Vm-----a-p,ет,Vmip3s-a-p
c. мог,Vm-----a-p,NULL,Vmis-sma-p

The freely-available POS lexicon from Sharoff
et al. (2008), speciﬁcally the ﬁle for the POS
tagger TnT (Brants, 2000), contains full words
(239,889 unique forms), with frequency informa-
tion. Working with such a rich database, we only
need segmentation, providing a quickly-obtained
lexicon (cf. ﬁve years for a German lexicon in
Geyken and Hanneforth, 2005).

In the future, one could switch to a different
tagset, such as that in Hana and Feldman (2010),
which includes reﬂexivity, animacy, and aspect
features. One could also expand the lexicon, by
adapting algorithms for analyzing unknown words
(e.g., Mikheev, 1997), as suggested by Feldman
and Hana (2010). Still, our lexicon continues the
trend of linking traditional categories used for tag-
ging with deeper analyses (Sharoff et al., 2008;
Hana and Feldman, 2010).3

3.1 Finding segments/morphemes
We use a set of hand-crafted rules to segment
words into morphemes, of the form: if the tag is x
and the word ends with y, make y the sufﬁx. Such
rules are easily and quickly derivable from a text-
book listing of paradigms. For certain exceptional

2POS tags are from the compositional tagset in
Sharoff et al. (2008). A full description is at: http://

corpus.leeds.ac.uk/mocky/msd-ru.html.

3This lexicon now includes lemma information, but each

word is not segmented (Erjavec, 2010).

cases, we write word-speciﬁc rules. Additionally,
we remove word, tag pairs indicating punctuation
or non-words (PUNC, SENT, -).

One could use a sophisticated method for lem-
matizing words (e.g., Chew et al., 2008; Schone
and Jurafsky, 2001), but we would likely have
to clean the lexicon later; as Feldman and Hana
(2010) point out, it is difﬁcult to automatically
guess the entries for a word, without POS in-
formation. Essentially, we write precise rules to
specify part of the Russian system of sufﬁxes; the
lexicon then provides the stems for free.

We use the lexicon for generating errors, but
it should be compatible with analysis. Thus, we
focus on sufﬁxes for beginning and intermediate
learners. We can easily prune or add to the rule
set later. From an analysis perspective, we need to
specify that certain grammatical properties are in
a tag (see below), as an analyzer is to support the
provision of feedback. Since the rules are freely
available,4 changing these criteria for other pur-
poses is straightforward.

3.1.1 Segmentation rules

We have written 1112 general morphology
rules and 59 rules for the numerals ‘one’ through
‘four,’ based on the Nachalo textbooks (Ervin
et al., 1997). A rule is simply a tag, sufﬁx pair.
For example, in (4), Ncmsay (Noun, common,
masculine, singular, accusative, animate [yes])
words should end in either а (a) or я (ya).

(4) a. Ncmsay, а
b. Ncmsay, я

A program consults this list and segments a
word appropriately, requiring at least one charac-
ter in the stem. In the case where multiple sufﬁxes
match (e.g., ени (eni) and и (i) for singular neuter
locative nouns), the longer one is chosen, as it is
unambiguously correct.

We add information in 101 of

the 1112
rules. All numerals, for instance, are tagged as
Mc-s (Numeral, cardinal, [unspeciﬁed gender],
singular). The tagset in theory includes properties
such as case; they just were not marked (see foot-
note 6, though). Based on the ending, we add all

4http://cl.indiana.edu/

˜boltundevelopment/

262

possible analyses. Using an optional output tag,
in (5), Mc-s could be genitive (g), locative (l),
or dative (d) when it ends in и (i). These rules
increase ambiguity, but are necessary for learner
feedback.

(5) a. Mc-s, и, Mc-sg
b. Mc-s, и, Mc-sl
c. Mc-s, и, Mc-sd

In applying the rules, we generate stem tags, en-
coding properties constant across sufﬁxes. Based
on the word’s tag (e.g., Ncmsay, cf. (4)) a stem
is given a more basic tag (e.g., Ncm--y).

3.2 Lexicon statistics
To be ﬂexible for future use, we have only en-
riched 90% of the words (248,014), removing ev-
ery 10th word. Using the set of 1112 rules results
in a lexicon with 190,450 analyses, where analy-
ses are as in (3). For these 190,450 analyses, there
are 117 sufﬁx forms (e.g., я, ya) corresponding to
808 sufﬁx analyses (e.g., <я, Ncmsay>). On av-
erage 3.6 sufﬁx tags are observed with each stem-
tag pair, but 22.2 tags are compatible, indicating
incomplete paradigms.

4 Generating errors
4.1 Basic procedure
Taking the morpheme-based lexicon, we generate
errors by randomly combining morphemes into
full forms. Such randomness must be constrained,
taking into account what types of errors are likely
to occur.

The procedure is given in ﬁgure 2 and de-
tailed in the following sections. First, we use the
contextually-determined POS tag to restrict the
space of possibilities. Secondly, given that ran-
dom combinations of a stem and a sufﬁx can result
in many unlikely errors, we guide the combina-
tions, using a loose notion of likelihood to ensure
that the errors fall into a reasonable distribution.
After examining the generated errors, one could
restrict the errors even further. Thirdly, we com-
pare the stem and sufﬁx to determine the possible
types of errors. A full form may have several dif-
ferent interpretations, and thus, lastly, we select
the best interpretation(s).

1. Determine POS properties of the word to be

generated (section 4.2.1).

2. Generate a full-form, via guided random
stem and sufﬁx combination (section 4.2.4).
3. Determine possible error analyses for the full

form (section 4.2.2).

4. Select the error type(s) from among multiple

possible interpretations (section 4.2.3).

Figure 2: Error generation procedure

By trying to determine the best error type in
step 4,
the generation process can provide in-
sight into error analysis. This is important, given
that sufﬁxes are highly ambiguous; for example,
ой (-oj) has at least 6 different uses for adjec-
tives. Analysis is not simply generation in reverse,
though. Importantly, error generation relies upon
the context POS tag for the intended form, for
the whole process. To morphologically analyze
the corrupted data, one has to POS tag corrupted
forms (see section 5).

4.2 Corruption
We use a corpus of 5 million words automatically
tagged by TnT (Brants, 2000) and freely avail-
able online (Sharoff et al., 2008).5 Because we
want to make linguistically-informed corruptions,
we corrupt only the words we have information
for, identifying the words in the corpus which are
found in the lexicon with the appropriate POS
tag.6 We also select only words which have in-
ﬂectional morphology: nouns, verbs, adjectives,
pronouns, and numerals.7

4.2.1 Determining word properties (step 1)

We use the POS tag to restrict the properties of
a word, regardless of how exactly we corrupt it.
Either the stem and its tag or the sufﬁx and its tag

5See http://corpus.leeds.ac.uk/mocky/.
6We downloaded the TnT lexicon in 2008, but the corpus
in 2009; although no versions are listed on the website, there
are some discrepancies in the tags used (e.g., numeral tags
now have more information). To accommodate, we use a
looser match for determining whether a tag is known, namely
checking whether the tags are compatible. In the future, one
can tweak the rules to match the newer lexicon.

7Adverbs inﬂect for comparative forms, but we do not

consider them here.

263

can be used as an invariant, to guide the gener-
ated form (section 4.2.4). In (6a), for instance, the
adjective (Af) stem or plural instrumental sufﬁx
(Afp-pif) can be used as the basis for genera-
tion.

(6) a. Original: серыми (serymi, ‘gray’)

7→ сер/Af+ыми/Afp-pif

b. Corrupted: сер+ой (seroj)

The error type is deﬁned in terms of the original
word’s POS tag. For example, when we generate a
correctly-formed word, as in (6b), it is a syntactic
error if it does not match this POS tag.

4.2.2 Determining error types (step 3)

Before discussing word corruption in step 2
(section 4.2.4), we need to discuss how error types
are determined (this section) and how to han-
dle multiple possibilities (section 4.2.3), as these
steps help guide step 2. After creating a corrupted
word, we elucidate all possible interpretations in
step 3 by comparing each sufﬁx analysis with the
stem.
If the stem and sufﬁx form a legitimate
word (in the wrong context), it is a syntactic er-
ror. Incompatible features means a derivation or
inherency error, depending upon which features
are incompatible.
If the features are compati-
ble, but there is no attested form, it is either a
paradigm error—if we know of a different sufﬁx
with the same grammatical features—or a forma-
tion/incompleteness issue, if not.

This is a crude morphological analyzer (cf.
Dickinson and Herring, 2008), but bases its anal-
yses on what is known about the invariant part of
the original word. If we use ыми (ymi) from (6a)
as an invariant, for instance, we know to treat it as
a plural instrumental adjective ending, regardless
of any other possible interpretations, because that
is how it was used in this context.

4.2.3 Selecting the error type (step 4)

Corrupted forms may have many possible anal-
yses. For example, in (6b), the sufﬁx ой (oj)
has been randomly attached to the stem сер (ser).
With the stem ﬁxed as an adjective,
the suf-
ﬁx could be a feminine locative adjective (syn-
tactic error), a masculine nominative adjective

(paradigm error), or an instrumental feminine
noun (derivation error). Given what learners are
likely to do, we can use some heuristics to restrict
the set of possible error types.

First, we hypothesize that a correctly-formed
word is more likely a correct form than a mis-
formed word. This means that correct words
and syntactic errors—correctly-formed words in
the wrong context—have priority over other error
types. For (6b), for instance, the syntactic error
outranks the paradigm and derivation errors.

even if misformed,

Secondly, we hypothesize that a contextually-
is
appropriate word,
more likely the correct
interpretation than a
contextually-inappropriate word. When we have
cases where there is: a) a correctly-formed word
not matching the context (a syntactic error), and
b) a malformed word which matches the context
(e.g., a paradigm error), we list both possibilities.
Finally, derivation errors seem less likely than
the others (a point conﬁrmed by native speakers),
giving them lower priority. Given these heuristics,
not only can we rule out error types after gener-
ating new forms, but we can also split the error
generation process into different steps.

4.2.4 Corrupting selected words (step 2)

Using these heuristics, we take a known word
and generate errors based on a series of choices.
For each choice, we randomly generate a num-
ber between 0 and 1 and choose based on a given
threshold. Thresholds should be reset when more
is known about error frequency, and more deci-
sions added as error subtypes are added.

Decision #1: Correct forms The ﬁrst choice is
whether to corrupt the word or not. Currently, the
threshold is set at 0.5. If we corrupt the word, we
continue on to the next decision.

Decision #2: Syntactic errors We can either
generate a syntactic or a morphological error. On
the assumption that syntactic errors are more com-
mon, we currently set a threshold of 0.7, generat-
ing syntactic errors 70% of the time and morpho-
logical form errors 30% of the time.

To generate a correct form used incorrectly, we
extract the stem from the word and randomly se-
lect a new sufﬁx. We keep selecting a sufﬁx until

264

we obtain a valid form.8 An example is given in
(7): the original (7a) is a plural instrumental ad-
jective, unspeciﬁed for gender; in (7b), it is singu-
lar nominative feminine.
(7) a. серыми

gray
Afp-pif

b. серая

Afpfsnf

глазами
eyes
Ncmpin
глазами
Ncmpin

.
.
SENT
.
SENT

One might consider ensuring that each error
differs from the original in only one property. Or
one might want to co-vary errors, such that, in
this case, the adjective and noun both change from
instrumental to nominative. While this is eas-
ily accomplished algorithmically, we do not know
whether learners obey these constraints. Generat-
ing errors in a relatively unbounded way can help
pinpoint these types of constraints.

While the form in (7b) is unambiguous, syntac-
tic errors can have more than one possible analy-
sis. In (8), for instance, this word could be cor-
rupted with an -ой (-oj) ending, indicating fem-
inine singular genitive, instrumental, or locative.
We include all possible forms.
(8) серой

глазами
Ncmpin

.
SENT

Afpfsg.Afpfsi.Afpfsl

Likewise, considering the heuristics in sec-
tion 4.2.3, generating a syntactic error may lead
to a form which may be contextually-appropriate.
Consider (9): in (9a), the verb-preposition com-
bination requires an accusative (Ncnsan). By
changing -о to -е, we generate a form which could
be locative case (Ncnsln, type #4) or, since -
е can be an accusative marker, a misformed ac-
cusative with the incorrect paradigm (#2c). We
list both possibilities.
(9) a. . . . смотрел

. . . (he) looked
. . . Vmis-sma-p

в
into
Sp-a

небо
the sky
Ncnsan

b. . . . в

. . . Sp-a

небе
Ncnsan+2c.Ncnsln+4

Syntactic errors obviously conﬂate many dif-
ferent error types. The taxonomy for German
8We ensure that we do not generate the original form, so

that the new form is contextually-inappropriate.

from Boyd (2010), for example, includes selec-
tion, agreement, and word order errors. Our syn-
tactic errors are either selection (e.g., wrong case
as object of preposition) or agreement errors (e.g.,
subject-verb disagreement in number). However,
without accurate syntactic information, we cannot
divvy up the error space as precisely. With the
POS information, we can at least sort errors based
on the ways in which they vary from the original
(e.g., incorrect case).

Finally, if no syntactic error can be derived, we
revert to the correct form. This happens when the
lexicon contains only one form for a given stem.
Without changing the stem, we cannot generate a
new form which is veriﬁably correct.

Decision #3: Morphological errors The next
decision is: should we generate a true morpholog-
ical error or a spelling error? We currently bias
this by setting a 0.9 threshold. The process for
generating morphological errors (0.9) is described
in the next few sections, after which spelling er-
rors (0.1) are described. Surely, 10% is an un-
derestimate of the amount of spelling errors (cf.
Rosengrant, 1987); however, for reﬁning a mor-
phological error taxonomy, biasing towards mor-
phological errors is appropriate.

Decision #4: Invariant morphemes When cre-
ating a context-dependent morphological error,
we have to ask what the unit, or morpheme, is
upon which the full form is dependent. The ﬁnal
choice is thus to select whether we keep the stem
analysis constant and randomize the sufﬁx or keep
the sufﬁx and randomize the stem. Consider that
the stem is the locus of a word’s semantic proper-
ties, and the (inﬂectional) sufﬁx reﬂects syntactic
properties. If we change the stem of a word, we
completely change the semantics (error type #1b).
Changing the sufﬁx, on the other hand, creates a
morphological error with the same basic seman-
tics. We thus currently randomly generate a sufﬁx
90% of the time.

Random sufﬁx generation Randomly attach-
ing a sufﬁx to a ﬁxed stem is the same procedure
used above to generate syntactic errors. Here,
however, we force the form to be incorrect, not
allowing syntactic errors. If attaching a sufﬁx re-

265

sults in a correct form (contextually-appropriate
or not), we re-select a random sufﬁx.

Similarly, the intention is to generate inherency
(#2bii), paradigm (#2c), and formation (#3) errors
(or lexicon incompleteness). All of these seem
to be more likely than derivation (#2bi) errors, as
discussed in section 4.2.3. If we allow any sufﬁx
to combine, we will overwhelmingly ﬁnd deriva-
tion errors. As pointed out in Dickinson and Her-
ring (2008), such errors can arise when a learner
takes a Russian noun, e.g., душ (dush, ‘shower’)
and attempts to use it as a verb, as in English, e.g.,
душу (dushu) with ﬁrst person singular morphol-
ogy. In such cases, we have the wrong stem be-
ing used with a contextually-appropriate ending.
Derviation errors are thus best served with ran-
dom stem selection, as described in the next sec-
tion. To rule out derivation errors, we only keep
sufﬁx analyses which have the same major POS as
the stem.

For some stems, particular types of errors are
impossible to generate. a) Inherency errors do not
occur for underspeciﬁed stems, as happens with
adjectives. For example, нов- (nov-, ‘new’) is an
adjective stem which is compatible with any ad-
jective ending. b) Paradigm errors cannot occur
for words whose sufﬁxes in the lexicon have no al-
ternate forms; for instance, there is only one way
to realize a third singular nominative pronoun. c)
Lexicon incompleteness cannot be posited for a
word with a complete paradigm. These facts show
that the generated error types are biased, depend-
ing upon the POS and the completeness of the lex-
icon.

Random stem generation Keeping the sufﬁx
ﬁxed and randomly selecting a stem ties the gen-
erated form to the syntactic context, but changes
the semantics. Thus, these generated errors are
ﬁrstly semantic errors (#1b), featuring stems in-
appropriate for the context, in addition to having
some other morphological error. The fact that,
given a context, we have to generate two errors
lends weight to the idea that these are less likely.
A randomly-generated stem will most likely
be of a different POS class than the sufﬁx, re-
sulting in a derivation error (#2bi). Further, as
with all morphological errors, we restrict the gen-

erated word not to be a correctly-formed word,
and we do not allow the stem or the sufﬁx to be
closed class items.
It makes little sense to put
noun inﬂections on a preposition, for example,
and derivation errors involve open class words.9

Spelling errors For spelling errors, we create an
error simply by randomly inserting, deleting, or
substituting a single character in the word.10 This
will either be a stem (#1a) or a sufﬁx (#2a) error. It
is worth noting that since we know the process of
creating this error, we are able to compartmental-
ize spelling errors from morphological ones. An
error analyzer, however, will have a harder time
distinguishing them.

5 Tagging the corpus
Figure 3 presents the distribution of error types
generated, where Word refers to the number of
words with a particular error type, as opposed to
the count of error type+POS pairs, as each word
can have more than one POS for an error type (cf.
(9b)). For the 780,924 corrupted words, there are
2.67 error type+POS pairs per corrupted word. In-
herency (#2bii) errors in particular have many tags
per word, since the same sufﬁx can have multiple
similar deviations from the original (cf. (8)). Fig-
ure 3 shows that we have generated roughly the
distribution we wanted, based on our initial ideas
of linguisic plausibility.

Type
1a
2a
2bii
2c
4
3+

Word
19,661
6,560
150,710
94,211
524,269
83,763

POS
19,661
6,560
749,292
94,211
721,051
208,208

Type
1b-2bi
1b-2bii
1b-2c
1b-3+

Word
11,772
5,529
279
1,770

POS
11,772
5,529
279
1,770

1b-all

19,350

19,350

Figure 3: Distribution of generated errors

Without an error detection system, it is hard to
gauge the impact of the error generation process.
Although it is not a true evaluation of the error
generation process, as a ﬁrst step, we test a POS

9Learners often misuse, e.g., prepositions, but these er-
rors do not affect morphology. Future work should examine
the relation between word choice and derivation errors, in-
cluding changes in preﬁxes.

10One could base spelling errors on known or assumed
phonological confusions (cf. Hovermale and Martin, 2008).

266

ment: only 17.3% of these words are correctly
tagged (compared to 62% for stem spelling er-
rors). With an ill-formed sufﬁx, the tagger simply
does not have reliable information. To improve
tagging for morphological errors, one should in-
vestigate which linguistic properties are being in-
correctly tagged (cf. sub-tagging in Hana et al.,
2004) and what roles distributional, morphologi-
cal, or lexicon cues should play in tagging learner
language (see also D´ıaz-Negrillo et al., 2010).

6 Conclusions and Outlook
We have developed a general method for gener-
ating learner-like morphological errors, and we
have demonstrated how to do this for Russian.
While many insights are useful for doing error
analysis (including our results for POS tagging
the resulting corpus), generation proceeds from
knowing grammatical properties of the original
word. Generating errors based on linguistic prop-
erties has the potential to speed up the process of
categorizing learner errors, in addition to creating
realistic data for machine learning systems. As a
side effect, we also added segmentation to a wide-
coverage POS lexicon.

There are several directions to pursue. The
most immediate step is to properly evaluate the
quality of generated errors. Based on this analysis,
one can reﬁne the taxonomy of errors, and thereby
generate even more realistic errors in a future iter-
ation. Additionally, building from the initial POS
tagging results, one can work on generally analyz-
ing the morphology of learner language, includ-
ing teasing apart what information a POS tagger
needs to examine and dealing with multiple hy-
potheses (Dickinson and Herring, 2008).

Acknowledgements
I would like to thank Josh Herring, Anna Feld-
man, Jennifer Foster, and three anonymous re-
viewers for useful comments on this work.

tagger against the newly-created data. This helps
test the difﬁculty of tagging corrupted forms, a
needed step in the process of analyzing learner
language. Note that for providing feedback, it
seems desirable to have the POS tagger match
the tag of the corrupted form. This is a different
goal than developing POS taggers which are ro-
bust to noise (e.g., Bigert et al., 2003), where the
tag should be of the original word.

To POS tag, we use the HMM tagger TnT
(Brants, 2000) with the model from http://
corpus.leeds.ac.uk/mocky/.
The re-
sults on the generated data are in ﬁgure 4, using
a lenient measure of accuracy: a POS tag is cor-
rect if it matches any of the tags for the hypoth-
esized error types. The best performance is for
uncorrupted known words,11 but notable is that,
out of the box, the tagger obtains 79% precision
on corrupted words when compared to the gener-
ated tags, but is strongly divergent from the orig-
inal (no longer correct) tags. Given that 67%
( 524,269
780,924) of words have a syntactic error—i.e., a
well-formed word in the wrong context—this in-
dicates that the tagger is likely relying on the form
in the lexicon more than the context.

Gold Tags

Corrupted
Unchanged:

Known
Unknown

Overall

Original

Error

# words
3.8% 79.0% 780,924

92.1% 92.1% 965,280
81.9% 81.9% 3,484,909
72.1% 83.4% 5,231,113

Figure 4: POS tagging results, comparing tagger
output to Original tags and Error tags

It is difﬁcult to break down the results for cor-
rupted words by error type, since many words are
ambiguous between several different error types,
and each interpretation may have a different POS
tag. Still, we can say that words which are syn-
tactic errors have the best tagging accuracy. Of
the 524,269 words which may be syntactic er-
rors, TnT matches a tag in 96.1% of cases. Sufﬁx
spelling errors are particularly in need of improve-

11Known here refers to being in the enriched lexicon, as

these are the cases we speciﬁcaly did not corrupt.

267

References
Bigert, Johnny, Ola Knutsson and Jonas Sj¨obergh
(2003). Automatic Evaluation of Robustness
and Degradation in Tagging and Parsing.
In
Proceedings of RANLP-2003. Borovets, Bul-
garia, pp. 51–57.

Boyd, Adriane (2010).

EAGLE: an Error-
Annotated Corpus of Beginning Learner Ger-
man. In Proceedings of LREC-10. Malta.

Brants, Thorsten (2000). TnT – A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-00.
Seattle, WA, pp. 224–231.

Chew, Peter A., Brett W. Bader and Ahmed Abde-
lali (2008). Latent Morpho-Semantic Analysis:
Multilingual Information Retrieval with Char-
acter N-Grams and Mutual Information. In Pro-
ceedings of Coling 2008. Manchester, pp. 129–
136.

D´ıaz-Negrillo, Ana, Detmar Meurers, Salvador
Valera and Holger Wunsch (2010). Towards
interlanguage POS annotation for effective
learner corpora in SLA and FLT. Language Fo-
rum .

Dickinson, Markus and Joshua Herring (2008).
Developing Online ICALL Exercises for Rus-
sian. In The 3rd Workshop on Innovative Use
of NLP for Building Educational Applications.
Columbus, OH, pp. 1–9.

Erjavec, Tomaˇz (2010). MULTEXT-East Ver-
sion 4: Multilingual Morphosyntactic Speciﬁ-
cations, Lexicons and Corpora. In Proceedings
of LREC-10. Malta.

Ervin, Gerard L., Sophia Lubensky and Donald K.
Jarvis (1997). Nachalo: When in Russia . . . .
New York: McGraw-Hill.

Feldman, Anna and Jirka Hana (2010).

A
Resource-light Approach to Morpho-syntactic
Tagging. Amsterdam: Rodopi.

Foster, Jennifer and Oistein Andersen (2009).
GenERRate: Generating Errors for Use in
Grammatical Error Detection. In The 4th Work-
shop on Innovative Use of NLP for Building Ed-
ucational Applications. Boulder, CO, pp. 82–
90.

Geyken, Alexander and Thomas Hanneforth
(2005). TAGH: A Complete Morphology for
German Based on Weighted Finite State Au-

tomata. In FSMNLP 2005. Springer, pp. 55–66.
Hana, Jirka and Anna Feldman (2010). A Posi-
In Proceedings of

tional Tagset for Russian.
LREC-10. Malta.

Hana, Jirka, Anna Feldman and Chris Brew
(2004). A Resource-light Approach to Russian
Morphology: Tagging Russian using Czech
resources.
In Proceedings of EMNLP-04.
Barcelona, Spain.

Hovermale, DJ and Scott Martin (2008). Devel-
oping an Annotation Scheme for ELL Spelling
Errors.
In Proceedings of MCLC-5 (Midwest
Computational Linguistics Colloquium). East
Lansing, MI.

Mikheev, Andrei (1997). Automatic Rule Induc-
tion for Unknown-Word Guessing. Computa-
tional Linguistics 23(3), 405–423.

Rosengrant, Sandra F. (1987). Error Patterns in
Written Russian. The Modern Language Jour-
nal 71(2), 138–145.

Rozovskaya, Alla and Dan Roth (2010). Training
Paradigms for Correcting Errors in Grammar
and Usage. In Proceedings of HLT-NAACL-10.
Los Angeles, California, pp. 154–162.

Rubinstein, George (1995). On Case Errors Made
in Oral Speech by American Learners of Rus-
sian. Slavic and East European Journal 39(3),
408–429.

Schone, Patrick and Daniel Jurafsky (2001).
Knowledge-Free Induction of Inﬂectional Mor-
phologies. In Proceedings of NAACL-01. Pitts-
burgh, PA.

Sharoff, Serge, Mikhail Kopotev, Tomaˇz Erjavec,
Anna Feldman and Dagmar Divjak (2008). De-
signing and evaluating Russian tagsets. In Pro-
ceedings of LREC-08. Marrakech.

Tetreault, Joel and Martin Chodorow (2008). The
Ups and Downs of Preposition Error Detection
in ESL Writing.
In Proceedings of COLING-
08. Manchester.

Vandeventer Faltin, Anne (2003). Syntactic error
diagnosis in the context of computer assisted
language learning. Th`ese de doctorat, Univer-
sit´e de Gen`eve, Gen`eve.

