


































Abduction for Discourse Interpretation: A Probabilistic Framework

Ekaterina Ovchinnikova
USC/ISI

4676 Admiralty Way
Marina del Rey, CA 90292

katya@isi.edu

Andrew S. Gordon
USC/ICT

12015 Waterfront Drive
Los Angeles, CA 90094-2536
gordon@ict.usc.edu

Jerry Hobbs
USC/ISI

4676 Admiralty Way
Marina del Rey, CA 90292

hobbs@isi.edu

Abstract

Abduction allows us to model interpre-
tation of discourse as the explanation of
observables, given additional knowledge
about the world. In an abductive frame-
work, many explanations can be con-
structed for the same observation, requir-
ing an approach to estimate the likelihood
of these alternative explanations. We show
that, for discourse interpretation, weighted
abduction has advantages over alternative
approaches to estimating the likelihood
of hypotheses. However, weighted ab-
duction has no probabilistic interpretation,
which makes the estimation and learning
of weights difficult. To address this, we
propose a formal probabilistic abductive
framework that captures the advantages
weighted abduction when applied to dis-
course interpretation.

1 Introduction

In this paper, we explore discourse interpretation
based on a mode of inference called abduction,
or inference to the best explanation. Abduction-
based discourse processing was studied inten-
sively in the 1980s and 1990s (Charniak and Gold-
man, 1989; Hobbs et al., 1993). This framework is
appealing because it is a realization of the obser-
vation that we understand new material by linking
it with what we already know. It instantiates in
discourse understanding the more general princi-
ple that we understand our environment by com-
ing up with the best explanation for the observ-
ables in the environment. Hobbs et al. (1993) show
that abductive proofs can be efficiently exploited
for a whole range of natural language pragmat-
ics problems, such as word sense disambiguation,
anaphora and metonymy resolution, interpretation
of noun compounds and prepositional phrases, and

detection of discourse relations. As applied to
discourse interpretation, abduction was shown to
have advantages over deduction, a more classical
mode of inference (Ovchinnikova, 2012). One se-
rious advantage concerns treatment of incomplete
knowledge. In the cases when it is impossible to
provide it with all the knowledge which is relevant
for interpretation of a particular piece of text, de-
ductive reasoners fail to find a prove. Instead of
a deterministic yes/no proof abduction provides a
way of measuring in how far the input formula was
proven and which of its parts could not be proven.

In the early 90s, research on abduction-based
discourse processing resulted in good theoretical
work and in interesting small-scale systems, but
it faced three difficulties: 1) parsers were slow
and not accurate enough, so that inference had
no place to start, 2) inference processes were nei-
ther efficient nor accurate enough, 3) there was
no large knowledge base designed for discourse
processing applications. In the last two decades,
the first of these difficulties has been addressed by
progress in statistical parsing, e.g. (McClosky et
al., 2006; Huang, 2008; Bos, 2011). Recently, ef-
ficient reasoning techniques were developed that
overcome the second difficulty (Inoue and Inui,
2011; Inoue et al., 2012b). Finally, it has been
shown that there exists sufficient knowledge about
the world – at a level of precision that enables
its translation into formal logic – available in a
variety of resources (Ovchinnikova et al., 2011;
Ovchinnikova, 2012). These advances have re-
cently been capitalized upon in several large-scale
applications of abduction to discourse processing
tasks (Inoue and Inui, 2011; Ovchinnikova et al.,
2011; Ovchinnikova, 2012; Inoue et al., 2012a).

In an abductive framework, often many expla-
nations can be provided for the same observation.
In order to find the best solution for our pragmatic
problem, we need to be able to choose the best,
i.e. the most probable, explanation. Several ap-

42



proaches were proposed for estimating the likeli-
hood of alternative abductive explanations: cost-
based abduction (Charniak and Shimony, 1990),
weighted abduction (Hobbs et al., 1993), abduc-
tion based on Bayesian Networks (Pearl, 1988;
Charniak and Goldman, 1989; Raghavan and
Mooney, 2010), abduction based on Markov Logic
Networks (Kate and Mooney, 2009).

In this paper, we show that weighted abduc-
tion employing a cost propagation mechanism (see
Section 3) and favoring low-cost explanations has
certain features relevant for discourse processing
that other approaches do not have (see Section 4).
The main such feature is the approach to unifica-
tion, i.e. associating two entities with each other,
so that their common properties only need to be
proved or assumed once (see Section 2). Weighted
abduction favors explanations with the maximum
number of unifications. Thus, it favors those ex-
planations that link parts of observations together
and supports discourse coherence, which is crucial
for discourse interpretation.

There is not yet any work on linking weights in
weighted abduction to probabilities, which makes
the estimation and learning of the weights difficult.
In this paper, we show that the original cost prop-
agation mechanism in weighted abduction as in-
formally introduced in (Hobbs et al., 1993) cannot
be interpreted in terms of probabilities. However,
we can still capture features of weighted abduc-
tion desirable for discourse processing in a formal
probabilistic framework based on Bayesian Net-
works. As a result, we obtain a theoretically sound
probabilistic abductive framework favoring expla-
nations relevant for discourse interpretation.

2 Abduction

Abduction is inference to the best explanation.
Formally, logical abduction is defined as follows:
Given: Background knowledge B, observations
O, where both B and O are sets of first-order log-
ical formulas,
Find: A hypothesis H such that H ∪B |= O,H ∪
B "|=⊥, where H is a set of first-order logical for-
mulas.

Observation O is usually a conjunction of ex-
istentially quantified propositions (Charniak and
Goldman, 1989; Hobbs et al., 1993; Raghavan and
Mooney, 2010):

∃x1, ..., xk, ..., y1, ..., yl(q1(x1, ..., xk) ∧ ...
∧qn(y1, ..., yl)).

We extend the notion of observation by allow-
ing inequalities (x "= y) as conjuncts. Sometimes
inequalities follow from the natural language syn-
tax. For example, if we read There is a cat on the
mat. Another cat is on the table, we immediately
know that there are two different cats mentioned.
This text can be logically represented as follows:

∃x1, x2, y1, y2(cat(x1) ∧ on(x1, y1) ∧mat(y1)∧
cat(x2) ∧ on(x2, y2) ∧ table(y2) ∧ x1 "= x2)).

Background knowledge B is a set of first-order
logic formulas. In order to keep the inference
process computationally tractable, B is often re-
stricted to a set of Horn clauses (Charniak and Shi-
mony, 1990; Hobbs et al., 1993; Kate and Mooney,
2009; Raghavan and Mooney, 2010). Thus, each
background axiom has the form

P1 ∧ ... ∧ Pn → Q,

where all variables on the left-hand side are uni-
versally quantified with the widest possible scope
and all variables occurring on the right-hand side
only are existentially quantified. We weaken this
restriction allowing multiple literals on the right-
hand side of the background axioms because of
the importance of the context and compositional-
ity for discourse interpretation. For example, in
order to express the fact that a testing process can
be called “dry run”, we use the following axiom:

∀x, y, e, z, u(process(x) ∧ of(x, e) ∧ test(e, z, u)
→ dry(x) ∧ run(x)).

Breaking this axiom into two different axioms
(one implying that the process is dry and the other
implying that it is a run) will result in loosing the
binding of the arguments of dry and run.

We allow inequalities (x "= y) as conjuncts in
the background axioms. Inequalities can be used
to represent incompatibility. For example, the ax-
iom below represents the fact that the arguments
of the relation parent of refer to different objects:

∀x, y(parent of(x, y) → x "= y).

The two main inference operations in abduction
are backchaining and unification. Backchaining
is the introduction of new assumptions given an
observation and background knowledge. For ex-
ample, given O = q(A) and B = {∀x(p(x) →
q(x))}, there are two candidate hypotheses: H1 =

43



q(A) and H2 = p(A). We say that p(A) explains
q(A) in H2. If an atomic proposition is included
in a hypothesis (hypothesized) and not explained,
then it is assumed, e.g., q(A) is assumed in H1.

Unification is merging of propositions with the
same predicate name by assuming that their argu-
ments are same.1 For example, O = ∃x, y(p(x) ∧
p(y) ∧ q(y)). Given this observation, the proposi-
tions p(x) and p(y) are unifiable. Thus, there is a
hypothesis H = ∃x(p(x) ∧ q(y) ∧ x = y).

Both operations (backchaining and unification)
can be applied as many times as possible to gener-
ate a possibly infinite set of hypotheses. The gen-
eration of the set of hypotheses H initialized as an
empty set can be formalized as follows.

Backchaining∧n
i=1 Pn →

∧m
j=1Qj ∈ B and O ∧H |=

∧m
j=1Qj

and O ∧H ∧
∧n

i=1 Pn %|=⊥, where H ∈ H
H := H ∪ {H ∧

∧n
i=1 Pn}

Unification
O ∧H |= p(X) ∧ p(Y ) and

O ∧H ∧X = Y %|=⊥, where H ∈ H
H := H ∪ {H ∧X = Y }

3 Estimating Hypothesis Likelihood

Often many hypotheses can be constructed for the
same observation. In order to find the best solution
for our pragmatic problem, we need to choose the
best, i.e. the most probable, hypothesis. Several
approaches were proposed for estimating the like-
lihood of alternative abductive explanations.

Charniak and Shimony (1990) propose cost-
based abduction. In this framework, the likelihood
of a hypothesis depends on the probability of the
assumed atomic propositions to be true.

Another popular approach to abduction is based
on Bayesian Networks (Pearl, 1988; Charniak and
Goldman, 1989; Raghavan and Mooney, 2010). In
this framework, abductive explanations are repre-
sented by a directed graph constituting a Bayesian
net, such that the nodes of the graph correspond to
atomic predications and the edges connect expla-
nations with the predications they explain. Each
node has an associated conditional probability
P (A|B), where B is an explanation of A. Given
the constructed Bayesian net, the best abductive
hypothesis is selected using standard methods,

1Note that the abduction unification mechanism is differ-
ent from how unification is usually understood in computer
science and logic, because it allows us to assume equalities
of constants.

which assign values to the unobserved nodes in the
network that maximize the posterior probability of
the joint assignment given the observations.

One more approach developed by (Kate and
Mooney, 2009) is based on Markov Logic Net-
works (MLNs) (Richardson and Domingos, 2006).
In this approach, a weight is assigned to each
background axiom that reflects the strength of a
constraint it imposes on the set of possible worlds.
The higher the weight, the lower the probabil-
ity of a world that violates the axiom. An MLN
can be viewed as a set of templates for construct-
ing Markov networks. Originally, MLNs employ
deductive reasoning. Kate and Mooney (2009)
adapt MLNs for abductive inferences by intro-
ducing reverse implications for every axiom in
the knowledge base and adding mutual exclusiv-
ity constraints on the transformed axioms.

Finally, weighted abduction (Hobbs et al.,
1993) proposes a cost propagation mechanism for
selecting best hypotheses. In this framework, each
atomic observation is assigned a positive real-
valued cost. Atomic antecedents in the back-
ground axioms are assigned positive real-valued
weights. If an axiom α = P → Q is applied
then the cost of each newly introduced literal p in
P is equal to the sum of the costs of the literals
in Q multiplied by the weight of p in α. For ex-
ample, given the axiom ∀x(p(x)0.9 ∧ s(y)0.1 →
q(x)) and the observation q(A)$10, the literal p(A)
costs $10 × 0.9 = $9 and the literal s(y) costs
$10 × 0.1 = $1. When two literals are unified,
the result of their unification is assigned the min-
imum of their costs. For example, given the ob-
servation p(x)$10 ∧ p(y)$20 there is a hypothesis
x = y$10. The cost of the hypothesis is equal to
the sum of the costs of the assumptions. Each uni-
fication reduces the overall cost of the hypothe-
sis, while an application of an axiom can increase
or decrease the overall cost depending on whether
its total weight is less or greater than 1. There is
not yet any work on interpreting the weighted ab-
duction cost propagation in terms of probabilities.
Therefore the minimal cost hypothesis does not
necessarily correspond to the most probable one.

All mentioned approaches to estimating the
likelihood of abductive hypotheses have a com-
mon problem. The problem is that they all imply
certain assumptions that cannot be proved or dis-
proved practically because of the absence of the
gold standard (collection of correct proof graphs)

44



that is obviously very difficult to obtain. Cost-
based abduction implies that the likelihood of a
hypothesis depends on the joint likelihood of the
assumptions only and that the assumptions are mu-
tually independent. Abduction based on Bayesian
Networks implies that the truth of the literals de-
pends on their direct explanations only. MNL-
based abduction implies that the probability of a
background axiom to hold does not depend on the
observation. All mentioned framework imply that
unifications always hold.

In order to successfully apply abductive infer-
ence to pragmatic tasks, we should formulate the
underlying independence assumptions with a good
understanding of our domain of interest (in our
case, it is discourse interpretation) and design a
probabilistic framework correspondingly.

4 Abduction for Discourse Processing

Weighted abduction has three features, missing in
other abduction-based frameworks, that are espe-
cially relevant for discourse processing. In this
section, we discuss these features.

Unification The first feature is related to the uni-
fication inference. Weighed abduction prefers hy-
potheses with the maximum number of unifica-
tions. Therefore, it favors those explanations that
link parts of observations together and thus sup-
port discourse coherence.

Suppose we want to construct an interpretation
for the sentence John composed a sonata. The
verb compose has two readings, 1) the “put to-
gether” reading (e.g., The party composed a com-
mittee, and 2) the “create art” reading. Suppose
there are the following axioms:

1) put together(e, x1, x2)∧ collection(x2) →
compose(e, x1, x2)

2) create art(e, x1, x2)∧ work of art(x2) →
compose(e, x1, x2)

3) sonata(x) → work of art(x)
Axioms (1) and (2) correspond to the two read-

ings of compose. Axiom (3) states that a sonata
is a work of art. Weighted abduction favors Ax-
iom (2) over (1) for the observed sentence, because
unification of sonata resulting from the applica-
tion of Axioms 2 and 3 with the observable sonata
reveals the implicit discourse redundancy and sup-
ports linking the meanings of compose and sonata.

As mentioned above, weighted abduction im-
plies unconditional unification. In the discourse
interpretation context, unification is one of the

principal methods by which coreference is re-
solved. A naive approach to coreference in an
inference-based framework is to unify proposi-
tions having the same predicate names unless it
implies logical contradictions (Hobbs et al., 1993;
Bos, 2011). However, in situations when knowl-
edge necessary for establishing contradictions is
missing, the naive procedure results in overmerg-
ing. For example, given O = ∃x, y(animal(x) ∧
animal(y)), we do not want to assume that x
equals y when dog(x) ∧ cat(y) are observed. For
John runs and Bill runs, with the observations
O = ∃x, y(John(x)∧run(x)∧Bill(y)∧run(y)),
we do not want to assume that John and Bill are
the same individual just because they are both
running. If we had complete knowledge about
incompatibility (dog and cat are disjoint, people
have unique first names), the overmerging prob-
lem might not occur because of logical contradic-
tions. However, it is not plausible to assume that
we would have an exhaustive knowledge base. A
proposal to introduce weighted unification is de-
scribed in (Inoue et al., 2012a), where unification
costs depend on the semantic relation (synonymy
vs. antonymy), modality and polarity, and shared
properties of the unified literals.

Observations costs The second feature con-
cerns the unequal treatment of atomic observa-
tions depending on their initial cost. Hobbs et
al. (1993) mention that costs reflect the demand
for propositions to be proved. Those propositions
that are most likely to be linked referentially to
other parts of the discourse are expensive to as-
sume. This idea is illustrated by an example pro-
vided in (Blythe et al., 2011). Suppose there are
two sentences.

The smart man is tall.
The tall man is smart.
The logical representation for each of them is

∃x(smart(x) ∧ tall(x) ∧ man(x)). But cer-
tain syntactic features attached to propositions
(e.g., definite article) influence the probability of
the propositions to be explained or assumed. In
the first sentence we want to prove smart(x) to
anchor the sentence referentially. Then tall(x)
is new information to be assumed. Blythe et
al. (2011) suggest having a high cost on smart(x)
to force the proof procedure to find this referential
anchor. The cost on tall(x) will be low, to allow it
to be assumed without expending effort in trying
to locate that fact in background knowledge. For

45



the second sentence, the case is the reverse.
Suppose we know that educated people are

smart and big people are tall, and furthermore that
John is educated and Bill is big and both of them
are men. This knowledge is formalized as follows:
∀x(educated(x) → smart(x))
∀x(big(x) → tall(x))
educated(John), big(Bill),man(John),
man(Bill)
In weighted abduction, the best interpretation

for the first sentence is that the smart man is John,
because he is educated, and the cost for assuming
he is tall is paid. The interpretation to avoid is one
that says x is Bill; he is tall because he is big, and
the cost of assuming he is smart is paid. Weighted
abduction with its differential costs on observables
favors the first and disfavors the second.

Weighted conjuncts in the antecedents The
third feature of weighted abduction is related to
the weights of the conjuncts in the antecedents of
the background axioms. Hobbs et al. (1993) say
that the weights correspond to the “semantic con-
tribution” each conjunct makes to its consequent
and discuss the following example:
∀x(car(x)∧ no-top(x) → convertible(x))
Hobbs et al. (1993) assume that car contributes

more to convertible than no-top, therefore the for-
mer should have a higher weight forcing its ex-
planation. Thus, given a convertible mentioned
in text, we will probably intend to link it to some
other mentioning of a car rather than to a mention-
ing of an object with no top.

5 Graph Representation of Hypotheses

In this section, we introduce a formalization allow-
ing us to estimate probabilities of abductive hy-
potheses in Section 6. We follow (Charniak and
Shimony, 1990) and represent the set of all pos-
sible hypotheses as an AND/OR directed acyclic
graph (AODAG).

Definition 1 An AODAG is a 3-tuple < G, l, o >,
where:

1. G is a directed acyclic graph, G = (V,E).

2. l is a function from V to {AND, OR}, called
the label. A node labeled AND is called an
AND node, etc.

3. o ⊆ V is a set of observed nodes.

q(x, y)

1

p(y)

Up

x = y

p(x)

2

r(x)

s(z)

Figure 1: AODAG for the running example.

Consider an observation O = ∃x, y(q(x, y) ∧
r(x)) and the background knowledge B:

1) ∀y(p(y) → ∃x(q(x, y)))
2) ∀x, z(p(x) ∧ s(z) → ∃y(q(x, y) ∧ r(x)))

The AODAG in Fig. 1 is constructed by ap-
plying backchaining and unification to observation
O. The nodes marked with a double circle repre-
sent inference operations: backchaining using Ax-
ioms 1 (“1” node) and 2 (“2” node) as well as uni-
fication (“Up” node). Note that all operation nodes
are AND nodes. All literal nodes are OR nodes.
The notation u ↘ v is used to say that u is an im-
mediate parent of v. In our example, node “1” is a
parent of q(x, y) or 1 ↘ q(x, y).

Definition 2 A truth assignment for an AODAG is
a function f from V to {T, F}. A truth assignment
is a model if the following conditions hold:

1. If v ∈ o then f(v) = T .

2. If v (∈ o and v is an AND node then one of
the following statements hold:

(a) f(v) = F and ∃u ↘ v : f(u) = F .
(b) f(v) = T and ∀u ↘ v : f(u) = T .

3. If v (∈ o and v is an OR node then one of the
following statements hold:

(a) f(v) = T and ∃u ↘ v : f(u) = T .

46



(b) f(v) ∈ {T, F} and ∀u ↘ v : f(u) =
F .

4. If ∃v1, .., vn such that for all i ∈ {1, .., n} : vi
is xi = xi+1 and ∃v0 equal to x1 %= xn+1
then f(v0) ∧ f(v1) ∧ ... ∧ f(vn) = F .

Condition 1 in Definition 2 ensures that observ-
ables are true in every model. Condition 2 ensures
that an operation node is true if the result of this
operation is true. Otherwise, an operation node
is false. Condition 3 ensures that a literal node
is true if one of its explanations is true. Other-
wise, it can be either true or false. We rely on the
“open world” assumption, i.e., we do not assume
that the knowledge base contains all possible facts
about the world. Thus, assumptions can be made
without explanations. Condition 4 rules out incon-
sistencies that result from an equality and an in-
equality of the same variables. It rules out truth
assignments that assign T to both equality chains
x1 = x2... = xn+1 and an inequality x1 %= xn+1.

It is easy to see that the set of hypotheses corre-
sponds to the set of models of the AODAG. Given
Definition 2, the truth assignment
M = {(q(x, y), T ), (r(x), T ), (1, T ), (p(y), T ),
(2, F ), (p(x), F ), (s(z), F ), (U,F ), (x = y, F )}
is a model of the example AODAG. It corresponds
to the hypothesis p(y) ∧ r(x). The nodes in a
model that are assigned the truth value T and have
no parents with the truth value T are called as-
sumptions in this model. If u ↘ v and both u
and v are assigned the truth value T in a model,
then u explains v in this model. For example, r(x)
is an assumption in the model M above, whereas
q(x, y) is explained by Axiom 1 in M .

6 Probabilities and Independence
Assumptions

Now we are ready to estimate the likelihood of ab-
ductive hypotheses relevant for discourse interpre-
tation. Let us associate a random variable from
the set {X1, ..., Xn} with each of v nodes in an
AODAG. The variables Xi (i ∈ {1, ..., n}) take
values from the set {T, F}. If f(vi) = T then
Xi = T ; otherwise Xi = F . The joint probability
distribution of the set {X1, ..., Xn} is as follows:

P (X1, ..., Xn) =
n∏

i=1

P (Xi|πi), (1)

where Xi is conditioned on πi that denotes all
other variables from the set {X1, ..., Xn} on

which Xi depends. The question is how to define
πi for each Xi. In order to do it, we need to make
independence assumptions.

As discussed in Section 4, the cost propagation
mechanism in weighted abduction results in the
following model preferences:

1. Other things being equal, a model that results
from application of more reliable axioms is
favored.

2. Other things being equal, a model that con-
tains more true unification nodes is favored.

3. Other things being equal, a model that ex-
plains referential observables is favored.

Let us formulate independence assumptions re-
flecting the above model preferences. We can use
the local Markov property: each variable is condi-
tionally independent of its non-descendants given
its immediate parent variables. But we also need a
special account for unifications, because any true
unification raises the likelihood of the correspond-
ing model.

One option is to say that every axiom node in
an AODAG also depends on its parent unification
nodes. For example, nodes 1 and 2 in the example
AODAG depend on the node Up. However, given
more observables there could be more unifications
resulting from axiom applications. For example, if
we add observable s(t) then the application of Ax-
iom 2 can result in one more unification (t = z).
Given a set of golden AODAG models, one can
compute all possible unifications resulting from a
particular axiom. Alternatively, we can say that it
does not matter unifications of which literals result
from an axiom; the only thing that matters is how
many unifications are there. In order to implement
this second option, we introduce one more type
of random variables associated with an AODAG:
numbUv is associated with each axiom node v. It
takes values from the set N and stands for the num-
ber of true unifications that are parents of v.

In order to account for referentiality, we intro-
duce another type of random variables Ref v asso-
ciated with each literal node v in an AODAG. It
takes values from the set {T, F}. If v is a referen-
tial observable or it has a referential observable as
its child, then Ref v = T ; otherwise Ref v = F .
Each axiom application depends on whether its
immediate children are referential or not.

We associate random variables Xnode name with
each node of our example AODAG. In addition,

47



Xq(x,y)

X1

Xp(y)

XUp

Xx=y

Xp(x)

X2

Xr(x)

Xs(z)numbU1

numbU2

Ref q(x,y)

Ref r(x)

Figure 2: Bayesian network for the running exam-
ple AODAG.

we introduce random variables numbU1, numbU2,
Ref q(x,y) and Ref r(x). Fig. 2 shows the cor-
responding Bayesian network for the example
AODAG that has the following joint probability
distribution:

P (Xx=y) ∗ P (XUp |Xx=y) ∗ P (Xp(y)|XUp)∗
P (Xp(x)|XUp) ∗ P (Xs(z)) ∗ P (numbUp(y)|XUp)∗

P (X1|Xp(y), numbUp(y), Refq(x,y))∗
P (X2|Xp(x), Xs(z), numbUp(x),s(z), Refq(x,y), Refr(x))∗

P (Xq(x,y)|X1, X2) ∗ P (Xr(x)|X2)∗
P (numbUp(x),s(z)|XUp) ∗ P (Refq(x,y)) ∗ P (Refr(x))

Now we can estimate the probability of all ab-
ductive hypotheses or compute the best hypothe-
sis a using standard method for computing Most
Probable Explanation (Pearl, 1988) that maxi-
mizes the posterior probability of the joint as-
signment given the observations (values of vari-
ables Xq(x,y), Xr(x), Refq(x,y), Refr(x) in our ex-
ample). If conditional probability tables need to
be learned, we can use standard algorithms: Ex-
pectation Maximization (Dempster et al., 1977;
Langseth and Bangsø, 2001; Ramoni and Sebas-
tiani, 2001) and Markov Chain Monte Carlo meth-
ods (Liao and Ji, 2009).

7 Linking Costs and Weights in
Weighted Abduction to Probabilities

Section 6 gives us a probabilistic approach to ab-
duction that preserves the relevant discourse inter-

pretation features of weighted abduction, so now
we want to see what are the relationships between
weights and probabilities across these two frame-
works. Consider our running example again. Sup-
pose cost(q(x, y)) = c1, cost(r(x)) = c2, weight
of p(y) in Axiom 1 is w1, and weights of p(x) and
s(z) in Axiom 2 are w2 and w3 correspondingly.
There are 5 hypotheses for the given observation.
According to the cost propagation scheme, the hy-
potheses are assigned the following costs.

H1 = q(x, y) ∧ r(x)
cost(H1) = c1 + c2
H2 = p(y) ∧ r(x)
cost(H2) = w1 ∗ c1 + c2
H3 = p(x) ∧ s(z)
cost(H3) = w2 ∗ (c1 + c2) + w3 ∗ (c1 + c2)
H4 = p(y) ∧ p(x) ∧ s(z)
cost(H4) = w1∗c1+w2∗(c1+c2)+w3∗(c1+c2)
H5 = p(y) ∧ p(x) ∧ s(z) ∧ y = x
cost(H5) = min(w1 ∗ c1, w2 ∗ (c1 + c2)) + w3 ∗
(c1 + c2)

The corresponding AODAG has 5 models:

M1 = {(q(x, y), T ), (r(x), T ), (1, F ), (p(y), F ),
(2, F ), (p(x), F ), (s(z), F ), (U,F ), (x = y, F )}
M2 = {(q(x, y), T ), (r(x), T ), (1, T ), (p(y), T ),
(2, F ), (p(x), F ), (s(z), F ), (U,F ), (x = y, F )}
M3 = {(q(x, y), T ), (r(x), T ), (1, F ), (p(y), F ),
(2, T ), (p(x), T ), (s(z), T ), (U,F ), (x = y, F )}
M4 = {(q(x, y), T ), (r(x), T ), (1, T ), (p(y), T ),
(2, T ), (p(x), T ), (s(z), T ), (U,F ), (x = y, F )}
M5 = {(q(x, y), T ), (r(x), T ), (1, T ), (p(y), T ),
(2, T ), (p(x), T ), (s(z), T ), (U, T ), (x = y, T )}

Our goal is to find function g such that

∀i ∈ {1, .., 5} : cost(Hi) = g(P (Mi)). (2)

The hypothesis cost is a sum of the assumption
costs (e.g., cost(H1) = c1 + c2). Can we de-
rive costs of atomic literals from the probabilities
of these literals to be assumed? The smaller the
cost, the bigger the probability that the literal is
assumed. The event when no axioms are applied
to the literal node v is denoted by Assume(v). If
we set g to the negative logarithm, then summing
costs will be equal to multiplying probabilities:

cost(v) = −log(P (Assume(v))). (3)

Model M1 refers to the event when no axioms
are applied: Assume(q(x, y)) ∩ Assume(r(x)).
Obviously, the events Assume(q(x, y)) and

48



Assume(r(x)) are not independent, because Ax-
iom 2 is applicable to both q(x, y) and r(x).
Therefore we get the following contradiction:

cost(H1) = cost(q(x, y)) + cost(r(x)) =

−log(P (Assume(q(x, y)) ∗ P (Assume(r(x)))
#=

−log(P (Assume(q(x, y) ∩Assume(r(x))) =
−log(P (M1)).

We cannot link the sum of costs of atomic literals
to the product of the probabilities of these liter-
als to be assumed, because the assumption events
are not independent. Therefore we have to reject
Eq. 3. Suppose we selected c1 and c2 so that

c1 + c2 = −log(P (Assume(q(x, y))∩
Assume(r(x))) = −log(P (M1)).

Can we then link axiom weights to probabilities?
Model M3 refers to the situation when only Ax-
iom 2 is applied. It has the following probability2:

P (M3) = P (X1 = F ∩X2 = T∩
Xp(y) = F ∩Xp(x) = T ∩Xs(z) = T∩

XUp = F |Xq(x,y) = T,Xr(x) = T ).

Since cost(H3) = (w2 + w3) ∗ (c1 + c2), we can
try to link w2 + w3 to the probability of Axiom 2
to be applied. But in order to compute P (M3) the
value of cost(H3) is also required to accommo-
date the probability of Axiom 1 not to be applied.
Thus, instead of one axiom weight for each axiom
α we need to have a table of conditional weights
depending on all other axioms that can be applied
in combination with α. This is not the case in
weighted abduction.

The discussion above shows that we need condi-
tional probabilities that cannot be linked to atomic
literal costs and weights, because variables as-
signed to the atomic literal nodes are not indepen-
dent. The question remains open if it is possible to
tune weights and costs so that least cost hypothe-
ses in weighted abduction correspond to the most
pragmatically relevant (and the most probable) ex-
planations. This is an empirical question and the
answer to it depends on a particular application.

The fact that costs and weights in weighted ab-
duction cannot be linked to probabilities does not
make the framework inapplicable to discourse in-
terpretation or any other task. One can see costs

2For simplicity, we ignore the referential variables.

and weights as being parameters that need to be
tuned in a practical setting. Inoue and Inui (2011)
show that it is possible to represent weighted ab-
duction as a linear constraint optimization prob-
lem and learn costs and weights in a large-margin
learning procedure (Inoue et al., 2012b) including
unification cost learning (Inoue et al., 2012a).

However, the problem remains how to set prior
values for costs and weights before starting the
learning. Furthermore, it is impossible to interpret
learned values, which results in the choice of the
best hypothesis being unpredictable.

8 Conclusion

Abduction allows us to model interpretation of
discourse as the explanation of observables given
knowledge about the world. In an abductive
framework, many explanations can be constructed
for the same observation. Therefore, an approach
to estimating the likelihood of the alternative ex-
planations is required.

In this paper, we showed that the cost propa-
gation mechanism in weighted abduction has ad-
vantages over alternative approaches when applied
to discourse interpretation. However, costs and
weights in weighted abduction have no proba-
bilistic interpretation, which makes their estima-
tion and learning difficult. We proposed a formal
framework for computing likelihood of abductive
hypotheses with an account of variable inequali-
ties and probabilistic unification. We discussed
independence assumptions relevant for discourse
processing. We showed that the cost propagation
mechanism cannot be interpreted in terms of prob-
abilities, but that features of weighted abduction
relevant for discourse interpretation can be still
captured in a probabilistic framework.

Future work concerns implementation of the
probabilistic abductive framework proposed in
Section 6 and its comparison with weighted ab-
duction as tested on specific discourse processing
tasks, such as recognizing textual entailment or
coreference resolution; see (Ovchinnikova et al.,
2011; Inoue et al., 2013) and (Inoue et al., 2012a)
for applications of abduction to these tasks.

Acknowledgments

We thank Chris Wienberg for his valuable com-
ments. This research was supported by ONR grant
N00014-13-1-0286.

49



References
J. Blythe, J. R. Hobbs, P. Domingos, R. J. Kate, and

R. J. Mooney. 2011. Implementing weighted ab-
duction in markov logic. In Proc. of IWCS’11, pages
55–64, Oxford, England.

J. Bos. 2011. A survey of computational semantics:
Representation, inference and knowledge in wide-
coverage text understanding. Language and Lin-
guistics Compass, 5(6):336–366.

E. Charniak and R. P. Goldman. 1989. A Semantics
for Probabilistic Quantifier-Free First-Order Lan-
guages, with Particular Application to Story Under-
standing. In N. S. Sridharan, editor, IJCAI’89, pages
1074–1079. Morgan Kaufmann.

E. Charniak and S. E. Shimony. 1990. Probabilistic
semantics for cost-based abduction. In Proc. of the
8th National Conference on AI, pages 106–111.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety: Series B, 39:1–38.

J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin.
1993. Interpretation as abduction. Artificial Intel-
ligence, 63:69–142.

L. Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL’08,
pages 586–594.

N. Inoue and K. Inui. 2011. ILP-Based Reasoning for
Weighted Abduction. In Proc. of AAAI Workshop on
Plan, Activity and Intent Recognition.

N. Inoue, E. Ovchinnikova, K. Inui, and J. R. Hobbs.
2012a. Coreference Resolution with ILP-based
Weighted Abduction. In Proc. of COLING’12,
pages 1291–1308.

N. Inoue, K. Yamamoto, Y. Watanabe, N. Okazaki, and
K. Inui. 2012b. Online large-margin weight learn-
ing for first-order logic-based abduction. In Proc.
of the 15th Information-Based Induction Sciences
Workshop, pages 143–150.

N. Inoue, E. Ovchinnikova, K. Inui, and J. R. Hobbs.
2013. Weighted abduction for discourse processing
based on integer linear programming. In Plan, Ac-
tivity, and Intent Recognition.

R.J. Kate and R. J. Mooney. 2009. Probabilistic ab-
duction using markov logic networks. In Proc. of
PAIR’09, Pasadena, CA.

H. Langseth and O. Bangsø. 2001. Parameter learning
in object-oriented bayesian networks. Ann. Math.
Artif. Intell., 32(1-4):221–243.

W. Liao and Q. Ji. 2009. Learning bayesian net-
work parameters under incomplete data with domain
knowledge. Pattern Recognition, 42(11):3046–
3056.

D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proc. of HLT-
NAACL’06.

E. Ovchinnikova, N. Montazeri, T. Alexandrov, J. R
Hobbs, M. McCord, and R. Mulkar-Mehta. 2011.
Abductive Reasoning with a Large Knowledge Base
for Discourse Processing. In Proc. of IWCS’11,
pages 225–234, Oxford, UK.

E. Ovchinnikova. 2012. Integration of World Knowl-
edge for Natural Language Understanding. Atlantis
Press, Springer.

J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.

S. Raghavan and R. Mooney. 2010. Bayesian abduc-
tive logic programs. In Proc. of Star-AI’10, pages
82–87, Atlanta, GA.

M. Ramoni and P. Sebastiani. 2001. Robust learning
with missing data. Machine Learning, 45(2):147–
170.

M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1-2):107–136.

50


