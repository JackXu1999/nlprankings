















































S3 - Statistical Sandhi Splitting


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 301–308,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

S3 - Statistical Sam. dhi Splitting

Abhiram Natarajan and Eugene Charniak
Brown Laboratory for Linguistic Information Processing

Brown University, USA
abhiram.nat@gmail.com, ec@cs.brown.edu

Abstract

The problem of Sam. dhi-Splitting is cen-
tral to computational processing of San-
skrit texts. Currently the best-known al-
gorithm for this task, given a chunk, gen-
erates all possible splits and chooses the
Maximum-a-Posteriori estimate as the fi-
nal answer. Our contributions to the task
of Sam. dhi-Splitting are two-fold. Firstly,
we improve upon the current algorithm
by proposing a principled modification
of the posterior probability function to
achieve better results. Secondly, we pro-
pose an algorithm based on Bayesian
Word-Segmentation methods. We find that
the unsupervised version of our algorithm
achieves a better precision than the cur-
rent algorithm with the original probabilis-
tic model. We then present a supervised
version of our algorithm that outperforms
all previous methods/models.

1 Introduction

Sanskrit, considered to be among the oldest lan-
guages in the world, is elaborate in its oral spec-
ifications. It possesses a set of euphonic rules
called Sam. dhi1 rules, which when applied, cause
phonological changes at word or morph bound-
aries. These rules are enumerated in Pān. ini’s
As.t.ādhyāyī, and have been devised with the aim
of achieving epigrammatic brevity. There are two
kinds of Sam. dhi:

1. Internal Sam. dhi: Euphonic transformation at
morph boundaries; W1x + yW2 ⇒ W1zW2,
where x and y are the final and initial
segments of W1 and W2, respectively, and z
respresents the smoothed phonetic transfor-
mation of x and y together

1Literally means “Putting Together”.

E.g: dīpena udvejayati ⇒
dīpenodvejayati

2. External Sam. dhi: Phonetic change at word
boundaries; W1x + W2 ⇒ W1x′ + W2,
where the final segment of the first word x
changes to x′

E.g: utthitah. vidyādharah. ⇒
utthito vidyādharah.

We shall refer to the process of making the eu-
phonic transformations as Sam. dhising and the text
formed as a result as Sam. dhied text. The process
of undoing the euphonic transformations shall be
referred to as Analysis or Sam. dhi-Splitting2, and
the text formed as a result will be referred to as
Analysed text.

It is easy to see that the task of Sam. dhi-
Splitting is ambiguous. For instance, any of
{(ca,api), (cā,api), (ca,āpi), (cā,āpi)} could have
combined to form cāpi. Contextual knowl-
edge is necessary for perfect analysis of Sanskrit
text (Hellwig, 2009), and current methods have
not evolved enough to be able to supply context.
However the field of Statistical Machine Learning,
by making reasonable assumptions, allows us to
provide approximations of these processes. Pre-
vious work (Beesley, 1998; Hyman, 2007) indi-
cates that Finite State Transducers (FST) could be
used to generate morphologically valid splits. Mit-
tal (2010) considers the FST approach as well as
an approach based on Optimality Theory (Prince
and Smolensky, 1993), by defining a posterior
probability function to choose among all morpho-
logically valid splits of a chunk (OT1). More re-
cently, Kumar et al. (2010) report findings using
a different posterior probability function with the
same Optimality Theory approach (OT2).

Firstly, we derive our own posterior probability
function, which is different from OT1 and OT2.

2We shall use these terms interchangeably.

301



We observe better results. Secondly, and more
importantly, we present a Sam. dhi-Splitting tech-
nique based on the Bayesian Word-Segmentation
methods presented by Goldwater et al. (2006a).
These methods in turn are based on the Dirich-
let process. The Dirichlet process is a continu-
ous multivariate distribution used in nonparamet-
ric Bayesian Statistics, often as a convenient prior
distribution. Gibbs sampling is used to sample
from the posterior distribution of analysed texts
given the Sam. dhied text.

The paper is organised as follows. In Sec-
tion 2, we discuss the Optimality Theory approach
in detail. In Section 3, we introduce our frame-
work for Sam. dhi-Splitting. We present both the
unsupervised and supervised versions of our al-
gorithm in this section. Section 4 then provides
specific details of our implementations and Sec-
tion 5 contrasts the results obtained by all the
methods/models considered. Section 6 concludes
the paper.

2 Current Methods

The literature on Sam. dhi-Splitting is too huge for
us to do justice. We shall stick to the best pub-
lished results so far, which have been obtained by
the OT method (Kumar et al., 2010; Mittal, 2010).
Procedure 1 is a pseudocode representation of the
OT method.

Procedure 1 : Sam. dhi-Splitting using OT
1: for each chunk ∈ D do
2: S← getAllPossibleSplits(chunk)
3: S′ ← {x : x ∈ S, x is morphologically

valid}
4: l← arg maxs{P̂ (s) : s ∈ S′}
5: print l

The procedure is quite straightforward. Each
candidate chunk is recursively broken (Line 2) to
generate all possible splits. Each split is passed
through a morphological analyser and the splits
that contain one or more invalid morphs are dis-
carded (Line 3). Finally, the Maximum a pos-
teriori (MAP) estimate is chosen as the answer
(Line 4).
P̂ (s) is the posterior probability function,

which we derive as follows. Consider a morpho-
logically valid split s =< c1, . . . , cm >, where
c1 . . . cm are its constituents, which was obtained

after applying rules r =< r1, . . . , rm−1 >3 on a
chunk C. We would need to find the most probable
split, which is given by arg maxs∈S′ P (s|C). Us-
ing the noisy channel model framework (Shannon,
1948), we get

arg max
s∈S′

P (s|C) = arg max
s∈S′

P (C|s)× P (s) (1)

We know that s is a result of applying r on C at
specific points. Now, P (C|s) reads as the condi-
tional probability of obtaining C given that r was
applied at exactly those points. Clearly, this is 1.
Thus the MAP estimate would be a split that max-
imises the probability of the prior

l = arg max
s∈S′

P̂ (s) (2)

Note that we use the notation P̂ instead of P be-
cause we can never know the true value of P (s),
but can only estimate it from our training set.
Let us now turn our attention to P̂ (s). We read
P̂ (s) as the probability of generating the morphs
< c1, . . . , cm >. Thus we get:

P̂ (s) = P̂ (c1)× P̂ (c2 | c1)× P̂ (c3 | c1, c2)× . . .

=
m∏

j=1

P̂ (cj) [Assuming Independence]

(3)

Equation 3 describes a generative model in that
it shows us how we can generate the morphs <
c1, . . . , cm >. Mittal (2010) defines P̂ (s) as

∏m−1
i=1

(
P̂ (ci) + P̂ (ci+1)

)
× P̂ (ri)

m
(4)

Kumar et al. (2010) define P̂ (s) as

(∏m
i=1 P̂ (ci)

)
×
(∏m−1

j=1 P̂ (rj)
)

m
(5)

In each of these models, the maximum likeli-
hood estimators for P̂ (ci) and P̂ (rj) are used, i.e.
P̂ (ci) is set to the relative frequency of ci in the
training set, and P̂ (rj) to the relative frequency of
the usage of rj in the training set.

3Rule ri is applied between ci and ci+1

302



Note that unlike Equation 4 and Equation 5,
Equation 3 does not have rule probabilities. This
can be explained by keeping in mind that Equa-
tion 3 describes a generative model. If we gen-
erated morphs and knew that they had to be com-
bined to form a chunk, the rules are uniquely deter-
mined. In Section 5, we shall compare the results
obtained by using each of Equations 3, 4 and 5 on
standard datasets.

3 Sam. dhi-Splitting based on the Dirichlet
Process

Our method is similar to the two-stage modelling
framework described by Goldwater et al. (2006a;
2006b). The framework has two components, a
morph generator which generates morphs likely to
be found in a lexicon, from some probability dis-
tribution, and an adaptor which determines how
often each of these morphs occur. Section 3.1 and
Section 3.2 describe the unsupervised version of
our algorithm while Section 3.3 describes the su-
pervised version.

3.1 Foundations

In the realm of the framework, a sam. dhied chunk
C = c1 c2 . . . cm can be viewed to be created as
follows:

1. A generator Ps generates a super-set se-
quence of morphs4: M = M1 . . .Mn from
a probability distribution Ps.

2. The adaptor Pγ generates a sequence of inte-
gers: Z = z1 . . . zm, each of which are iden-
tifiers of one particular item from M. This
means that 1 ≤ zi ≤ n, and zi = x ⇒ ci =
Mx.

Once the morphs of a chunk are generated,
Sam. dhi rules are applied between them to form
C. Henceforth TwoStage(Pγ , Ps) shall be used
to denote a two-stage framework with Pγ as the
adaptor and Ps as the generator. Let us move to
the Chinese Restaurant Process, which we use as
our adaptor.

4In the case of Internal Sam. dhi, we consider morph
boundaries and in the case of External Sam. dhi, we consider
word boundaries. The theoretical foundations hold good for
both cases. Thus the reader must always keep in mind that the
concepts presented in this section apply at word boundaries
too.

3.1.1 Chinese Restaurant Process
Having studied many corpora of natural language
utterances, Zipf (1932) made a famous empirical
observation that word frequencies follow a power-
law distribution, i.e., the frequency of a word is
inversely proportional to its frequency rank. This
means that the most frequent word in a corpus oc-
curs twice as often as the second most frequent
word, thrice as often as the third most frequent
word, and so on. Mathematically speaking, if wr
is the r-ranked word in a corpus, then

f(wr) ∝
1

rc
(6)

where f(wr) denotes the frequency of occurrence
of wr, and c ≈ 1. The veracity of this law has
hence been verified by a study on present-day En-
glish (Kucera and Francis, 1967).

Typically, power-law distributions are produced
by stochastic processes in which outcomes ac-
crue probability based on the probability they al-
ready have. Such processes are called preferential
attachment processes. Let us turn our attention
to the Chinese Restaurant Process (Aldous et al.,
1985) (CRP), which is one such process. The pro-
cess is best explained by specifying how to draw a
sample from it. Consider a restaurant with an infi-
nite number of tables, each with infinite capacity.
Customers enter the restaurant, one at a time, and
seat themselves at a table of their choice. They
choose an occupied table with probability propor-
tional to the number of people already present at
the table, or an unoccupied table with probability
proportional to some real-valued scalar parameter
α. The first customer always sits at the first table.
Then onwards, the ith customer sits at a table Ti,
and Ti follows the distribution:

P (Ti = k |Previous Customers)

=





nk
i−1+α 1 ≤ k ≤ Ni−1

α
i−1+α k = Ni−1 + 1

(7)

where nk is the number of people occupying table
k and Ni−1 is total number of tables occupied by
the previous (i− 1) customers.

When the CRP is combined with a morph gen-
erator, it can be used to generate a power-law dis-
tribution over morphs. Such a model can be de-
scribed as TwoStage(CRP (α), Ps). We could

303



view this as restaurant where each table represents
a morph. Every customer is also labelled with a
morph and can either sit only at tables which are
labelled with the same morph, or at an unoccu-
pied table. A customer at a table represents one
occurence of the morph that is represented by the
table. To get the probability distribution of the ith

morph, we must sum over all the tables labelled
with that morph:5

P (ci = c | c−i)
= P (assign customer to any of the c tables)

+ P (assign customer to a new table)

=
n
c−i
c

i− 1 + α + Ps(c)×
α

i− 1 + α

=
n
c−i
c + α× Ps(c)
i− 1 + α (8)

where c−i = c1 . . . ci−1 and n
c−i
c represents the

number of occurrences of c in c−i.
Let us examine Equation 8. It is in accordance

with the principle of preferential attachment. This
is because the probability of generating a morph
that has already been generated increases as more
instances of the morph are observed. Also note
that the sparseness of the distribution generated in-
creases with an increase in the value of the param-
eter α. This is explained by the fact that α×Ps(c)
is constant and it reduces w.r.t. the first summand
over time. What this means is that the probability
of generating a novel morph decreases (but never
dissapears fully) as more data is observed.

Let us now turn our attention to the genera-
tor. For simplicity, we choose a unigram phoneme
distribution for Ps. If a morph ci contains the
phonemes a1 . . . ad

Ps(ci) = pΦ(1− pΦ)d−1
d∏

j=1

P (aj) (9)

where pΦ is the probability of a Sam. dhi rule be-
ing applied at any juncture. The reasoning for the
equation is quite straightforward - the Sam. dhi rule
is not applied (d− 1) times, and applied at the dth
phoneme. The unsupervised version of the algo-
rithm uses an uniform distribution over phonemes.

5Note that two different tables may represent the same
morph.

3.1.2 Dirichlet Process Model
Given a coin with an unknown bias, say p, what
would be a suitable distribution that reflects our
expectations about p? It would be the Beta dis-
tribution. The Dirichlet distribution is a gener-
alisation of the Beta distribution, in that it may
have more than just two dimensions. The Dirich-
let process is an infinite dimensional version of the
Dirichlet distribution. Each sample from a Dirich-
let process returns a distribution over an infinite
set of random variables.

Let us consider a Dirichlet process with its pa-
rameters as α and Ps. A sample from this process,
say G, will consist of a set of all possible morphs
and their corresponding probabilities. Now, we
can generate each morph ci in the corpus from the
distributionG. This can be represented as follows:

ci ∼ G
G ∼ DP (α, Ps) (10)

Following the argument in Goldwater (2006),
it is not hard to find that the model described
by TwoStage(CRP (α), Ps) is equivalent to the
model represented by Equation 10.

We must also keep in mind that, analogous to
Equation 8, the probability of ith morph ci = c is
given by

P (ci = c | c−i) =
n
c−i
c + α× Ps(c)
i− 1 + α (11)

where c−i = c1 . . . ci−1, and n
c−i
c denotes the

number of occurrences of c in c−i. Going ahead
a little, we realise that our input corpus is a set
of sentences, where each sentence contains a set
of Sam. dhied chunks, where each chunk is in turn
formed by Sam. dhising separate morphs. The pro-
gram does not need to separate the chunks from
each other, all it needs to do is to separate a chunk
into its constituent morphs. Referring to Equa-
tion 3, we recall our generative model view of how
a Sam. dhied chunk is generated. We describe the
process by the following PCFG:6

P (ri = rbranch | r−i): S → S C
P (ri = rend | r−i): S → C
Ps(ci = c | c−i): C → c

We shall derive an equation for P (ri =
rbranch | r−i) in the next section.

6Probabilistic Context-Free Grammar

304



3.2 Gibbs Sampling for Sam. dhi Analysis
As suggested in Gilks et al. (1996), we use Gibbs
Sampling to sample from the posterior distrubition
of Sam. dhi Analyses. One iteration of the algo-
rithm causes the program control to consider ev-
ery possible splitting point in the data and form
two hypotheses, say H1 and H2. These hypothe-
ses contain the same structure, except at the point
of consideration. If the point of consideration is
part of a chunk c, H2 splits c into two individual
morphs (say c1 and c2) whileH1 does not. LetH∩
denote the structure common to bothH1 andH2.

Given the way Gibbs sampling works, we know
that we only need the relative probabilities of H1
and H2. Also, we must recall that the order of
arrival of customers in the metaphorical Chinese
Restaurant does not affect seating probability, as
shown by Aldous et al. (1985). This means that we
could considerH∩ to have already been generated.
From Equation 11 we get

P
(
H1 |H∩

)
= P

(
c |H∩

)

=
nH
∩

c + αPs(c)

|H∩|+ α (12)

We also derive

P
(
H2 |H∩

)

= P
(
rbranch ∪ c1 ∪ c2 |H∩

)

= P
(
rbranch |H∩

)
× n

H∩
c1 + αPs(c1)

|H∩|+ α

× n
H∩
c2 + I(c1 = c2) + αPs(c2)

|H∩|+ 1 + α (13)

The extra ’1’ in the denominator of the third
item of the product is because after c1, we have
an additional morph. Also, I is the indicator
function, which is 1 when the expression in-
side its parathesis is true. Let us now exam-
ine P (rbranch |H∩). Clearly, each time we can
choose only one among {rbranch, rend}, so we
have P (rbranch) + P (rend) = 1. The number of
r rules applied at the point of consideration would
be (|H∩| + 1) - one each for the |H∩| morphs,
and an extra one for the chunk under considera-
tion. We apply a Beta(ρ2 ,

ρ
2 ) prior to get:

7

P
(
rbranch |H∩

)
=

nH
∩

rbranch
+ ρ2

|H∩|+ 1 + ρ (14)

7Henceforth, please substitute this expression in Equa-
tion 13

The algorithm works as follows - we begin with
a random analysis of the corpus. After that, using
Equations 12 and 13, we sample every potential
boundary point. An iteration is said to have com-
pleted when the sampler samples over the entire
corpus. We run multiple iterations of this process,
until convergence.

Simulated annealing (Aarts and Korst, 1989) is
used to facilitate choosing of low probability tran-
sitions early in the algorithm, lest it gets stuck at a
local maximum.

3.3 Supervised Version
As mentioned earlier, we use Gibbs Sampling to
sample from the posterior distribution. Let us re-
call the relevant equations - Equations 12 and 13.
In the supervised version of our algorithm, we use
available training data (say Dtrain) to our advan-
tage. Firstly, we defineH∩

′
= H∩ ∪Dtrain.

Also, instead of assuming a uniform distribu-
tion over phonemes, we infer phoneme probabili-
ties from Dtrain:

P
′
s(ci) = pΦ(1− pΦ)d−1

d∏

j=1

P̂ (aj) (15)

Let Π denote the set of all phonemes found in
Dtrain. Given a phoneme ax, we set P̂ (ax) to be
the maximum likelihood estimate:

P̂ (ax) =
nax + α0
n∗ + α0|Π|

(16)

where nax is the number of times ax occurs in
Dtrain, n∗ =

∑
p∈Π np, and α0 is the unigram

smoothing constant.
Thus we get

P
(
H1 |H∩

′)
=
nH
∩
′

c + αP
′
s(c)

|H∩′ |+ α
(17)

and

P
(
H2 |H∩

′)

=
nH
∩
′

rbranch
+ ρ2

|H∩′ |+ 1 + ρ
× n

H∩
′

c1 + αP
′
s(c1)

|H∩′ |+ α

× n
H∩
′

c2 + I(c1 = c2) + αP
′
s(c2)

|H∩′ |+ 1 + α
(18)

305



The supervised version of the algorithm now
works exactly the same way as the unsupervised
version, except that Gibbs sampling is done using
Equations 17 and 18.

4 Implementation Details

As a part of the Consortium project in India, a cor-
pus of nearly 25000 parallel strings of Sam. dhied
and analysed text has been formed (say D1). OT
trains on the 25000 string dataset and tests on a
separate dataset of 2148 Sam. dhied Chunks (say
D2). We see that the test set used in OT only
contains examples of Internal Sam. dhi. Thus, we
merge the test set with the original 25000 string
dataset (D = D1 ∪ D2) and generate random
samples of training data and test data. We use 34
of the data for training (Dtrain) and 14 for test-
ing (Dtest). Note that Dtest can have instances
of Internal Sam. dhi, External Sam. dhi as well as no
Sam. dhi8 at all. The scores we report are averaged
over 5 random samples of Dtrain and Dtest from
D.

We also use the morphological analyser (Ak-
shar Bharati, 2006) provided by the apertium
group.

As for the parameters of the model, we fixed
ρ = 2. We found that a decrease in pΦ improved
recall, but caused long words to get concatenated.
At the same time higher values of pΦ tended to
cause over segmentation. It was also observed that
higher α values resulted in higher lexicon recall,
once again only up to a point. Although it was not
possible to arrive a single best value for either α
or pΦ; we fixed them to be pΦ = 0.5 and α =
20. The sampler is annealed with the following
temperature schedule - 1γ = 0.1 to 1.0, in steps of
0.1.

For evaluation, standard statistical measures of
Precision, Recall and F-Score were used. Preci-
sion indicates how many among the items found
are correct; Recall indicates how many among the
correct items are found. F-score is the harmonic
mean of the Precision and the Recall.

Our implementations work with the Sanskrit
Library Phonetic Basic format (SLP1). Since
the morphological analyser uses the Hyderabad-

8This explains why our implementations of OT achieve
lower scores than what is reported in their paper. Readers
must note that our implementation of OT, when trained on D1
and tested on D2, achieved nearly the same accuracy reported
in their paper. Thus we believe our implementation of OT is
accurate.

Tirupati format (WX), we use the transcoding
tools on the Sanskrit Library Website (Scharf and
Hyman, 2010). One may refer Huet (2009) for
more details about the formats.

5 Results

Let OT1, OT2, OT3 denote versions of Proce-
dure 1, using Equations 4, 5, 3 as the posterior
probability functions, respectively. Precision is
calculated as the percentage of correct segmen-
tations among all the segmentations made by the
algorithm. Recall is calculated as the percentage
of correct segmentations made by the algorithm
among all the correct segmentations that need to
be made. F-Score is the harmonic mean of the
Precision and Recall. Table 1 shows the scores
obtained by our implementations of different ver-
sions of OT.

Method Precision Recall F-Score
OT1 54.61 62.08 58.11
OT2 63.96 68.74 66.26
OT3 70.52 66.61 68.51

Table 1: Accuracies obtained by different versions
of OT; OT3 outperforms OT1 and OT2 in Preci-
sion and F-Score

As we can see, using Equation 3 as the poste-
rior probability function while obtaining the MAP
estimate, we improve the previous best F-Score by
nearly 3.5%. We shall now examine the results ob-
tained by four different versions of the framework
described in Section 3. Let these versions be de-
noted by NC1 . . . NC4:

1. NC1 denotes the Two-Stage Framework in its
most basic state. The algorithm possesses no
linguistic knowledge whatsoever.

2. Kessler (1994) shows that each Sanskrit syl-
lable must have only one Sonority peak. NC2
uses this knowledge to effect. The method
is largely similar to NC1 except for the way
in which sampling is done. We only sample
when we are sure that the segments we gener-
ate do not violate Sonority Hierarchy. Refer
Appendix A for more details about sonority
hierarchy.

3. NC3 uses a morphological analyser. Sam-
pling is only done when we know that the

306



segment we are leaving behind is morpholog-
ically valid.

4. NC4 uses a morphological analyser as well as
training data. Sampling is done using Equa-
tions 17 and 18.

Method Precision Recall F-Score
NC1 31.74 41.22 35.86
NC2 50.98 40.43 45.1
NC3 66.55 56.13 60.9
NC4 76.21 64.84 70.07

Table 2: Accuracies obtained by different versions
of NC; NC4 Outperforms NC1...3, as well OT1...3
in Precision and F-Score

Results obtained by these versions are shown in
Table 2. NC4 improves upon the F-Score achieved
by OT2 by nearly 6%.

6 Conclusion

In this paper, we have presented an algorithm for
Sam. dhi-Splitting which draws on expedients of
Bayesian statistics. It is highly flexible in that it
works even in the absence of (a) morphological
analyser (b) training data. Also, one must remem-
ber that an entire corpus, as it is, can be fed to the
algorithm, as opposed to OT which would require
each Sam. dhied chunk separately. Finally, the fact
that the algorithm runs in polynomial time, as op-
posed to OT which takes exponential time, further
asserts the efficacy of the method.

References
Emile Aarts and Jan Korst. 1989. Simulated annealing

and Boltzmann machines: a stochastic approach to
combinatorial optimization and neural computing.
John Wiley & Sons, Inc., New York, NY, USA.

V Sheeba Akshar Bharati, Amba P. Kulkarni. 2006.
Building a wide coverage sanskrit morphological
analyzer: A practical approach.

David Aldous, Illdar Ibragimov, Jean Jacod, and David
Aldous. 1985. Exchangeability and related topics.
In École d’Été de Probabilités de Saint-Flour XIII -
1983, volume 1117 of Lecture Notes in Mathemat-
ics, pages 1–198. Springer Berlin / Heidelberg.

Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations. In Proceedings of the
Workshop on Computational Approaches to Semitic
Languages, Semitic ’98, pages 50–57. Association
for Computational Linguistics.

W.R. Gilks, S. Richardson, and D.J. Spiegelhalter.
1996. Markov chain Monte Carlo in practice.
Chapman & Hall/CRC.

Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006a. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 673–680, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006b. Interpolating between types and
tokens by estimating power-law generators. In In
Advances in Neural Information Processing Systems
18, page 18.

Sharon Goldwater. 2006. Nonparametric Bayesian
models of lexical acquisition. Ph.D. thesis, Brown
University.

Oliver Hellwig. 2009. Sanskrittagger: A stochastic
lexical and pos tagger for sanskrit. In Sanskrit Com-
putational Linguistics, pages 266–277. Springer-
Verlag, Berlin, Heidelberg.

Gérard Huet. 2009. Sanskrit computational linguistics.
chapter Formal Structure of Sanskrit Text: Require-
ments Analysis for a Mechanical Sanskrit Proces-
sor, pages 162–199. Springer-Verlag, Berlin, Heidel-
berg.

Malcolm Hyman. 2007. From Paninian Sandhi to
Finite State Calculus. In First International San-
skrit Computational Linguistics Symposium, Roc-
quencourt France. INRIA Paris-Rocquencourt.

Brett Kessler. 1994. Sandhi and syllables in classi-
cal sanskrit. In The proceedings of the Twelfth West
Coast Conference on Formal Linguistics, pages 35–
50, Stanford, CA, USA. CSLI Publications.

H. Kucera and W. N. Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.

Anil Kumar, Vipul Mittal, and Amba Kulkarni. 2010.
Sanskrit compound processor. In Sanskrit Compu-
tational Linguistics, volume 6465 of Lecture Notes
in Computer Science, pages 57–69. Springer Berlin
/ Heidelberg.

Vipul Mittal. 2010. Automatic sanskrit segmentizer
using finite state transducers. In Proceedings of the
ACL 2010 Student Research Workshop, ACLstudent
’10, pages 85–90. Association for Computational
Linguistics.

Alan Prince and Paul Smolensky. 1993. Optimality
theory: Constraint interaction in generative gram-
mar. Technical report, Rutgers University and Uni-
versity of Colorado.

307



Peter M. Scharf and Malcolm D. Hyman. 2010.
Transcoding. http://sanskrit1.ccv.
brown.edu/tomcat/sl/TranscodeText.

Claude E. Shannon. 1948. A Mathematical Theory of
Communication, volume 27. Bell System Technical
Journal, Champaign, IL, USA.

G. K. Zipf. 1932. Selective Studies and the Principle
of Relative Frequency in Language. Harvard Uni-
versity Press.

A Sonority Hierarchy

Sonority hierarchy is as follows: Vowels >
Semivowels > Nasals > Spirants > Voiced Stops
> Unvoiced Stops

• Vowels = AaEOeoiIuUfFxX

• Semivowels = yvrl

• Nasals = NYRnmM

• Spirants = hSzsH

• Voiced Stops = gGjJqQdDbB

• Unvoiced Stops = kKcCwWtTpP’

A syllable is said possess a sonority violation
if it has more than one sonority peak (e.g. ’rjA’).
However, at the beginning of a syllable, we must
rate Spirants at the same level as Unvoiced Stops
(e.g. ’sTApayati’ is valid).

308


