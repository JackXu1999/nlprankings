



















































Domain-Specific Image Captioning


Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20,
Baltimore, Maryland USA, June 26-27 2014. c©2014 Association for Computational Linguistics

Domain-Specific Image Captioning

Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)

Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu

Abstract

We present a data-driven framework for
image caption generation which incorpo-
rates visual and textual features with vary-
ing degrees of spatial structure. We pro-
pose the task of domain-specific image
captioning, where many relevant visual
details cannot be captured by off-the-shelf
general-domain entity detectors. We ex-
tract previously-written descriptions from
a database and adapt them to new query
images, using a joint visual and textual
bag-of-words model to determine the cor-
rectness of individual words. We imple-
ment our model using a large, unlabeled
dataset of women’s shoes images and nat-
ural language descriptions (Berg et al.,
2010). Using both automatic and human
evaluations, we show that our caption-
ing method effectively deletes inaccurate
words from extracted captions while main-
taining a high level of detail in the gener-
ated output.

1 Introduction

Broadly, the task of image captioning is: given a
query image, generate a natural language descrip-
tion of the image’s visual content. Both the im-
age understanding and language generation com-
ponents of this task are challenging open problems
in their respective fields. A wide variety of ap-
proaches have been proposed in the literature, for
both the specific task of caption generation as well
as related problems in understanding images and
text.

Typically, image understanding systems use su-
pervised algorithms to detect visual entities and
concepts in images. However, these typically re-
quire accurate hand-labeled training data, which
is not available in most specific domains. Ideally,

1. Extract existing human-authored caption according to
similarity of coarse visual features.

Query Image Nearest-Neighbor

Nearest-neighbor caption: This sporty sneaker clog keeps
foot cool and comfortable and fully supported.

2. Estimate correctness of extracted words using domain-
specific joint model of text and visual bag-of-word features.

This sporty sneaker clog keeps foot cool and comfortable and
fully supported.

3. Compress extracted caption to adapt its content while
maintaining grammatical correctness.

Output: This clog keeps foot comfortable and supported.

a domain-specific image captioning system would
learn in a less supervised fashion, using captioned
images found on the web.

This paper focuses on image caption genera-
tion for a specific domain – images of women’s
shoes, collected from online shopping websites.
Our framework has three main components. We
extract an existing description from a database
of human-captions, by projecting query images
into a multi-dimensional space where structurally
similar images are near each other. We also
train a joint topic model to discover the latent
topics which generate both captions and images.
We combine these two approaches using sentence
compression to delete modifying details in the ex-
tracted caption which are not relevant to the query
image.

Our captioning framework is inspired by sev-
eral recent approaches at the intersection of Nat-
ural Language Processing and Computer Vision.
Previous work such as Farhadi et al. (2010) and
Ordonez et al. (2011) explore extractive methods
for image captioning, but these rely on general-
domain visual detection systems, and only gener-

11



ate extractive captions. Other models learn corre-
spondences between domain-specific images and
natural language captions (Berg et al., 2010; Feng
and Lapata, 2010b) but cannot generate descrip-
tions for new images without the use of auxil-
iary text. Kuznetsova et al. (2013) propose a
sentence compression model for editing image
captions, but their compression objective is not
conditioned on a query image, and their system
also requires general-domain visual detections.
This paper proposes an image captioning frame-
work which extends these ideas and culminates in
the first domain-specific image caption generation
system.

More broadly, our goal for image caption gener-
ation is to work toward less supervised captioning
methods which could be used to generate detailed
and accurate descriptions for a variety of long-tail
domains of captioned image data, such as in nature
and medicine.

2 Related Work

Our framework for domain-specific image cap-
tioning consists of three main components: ex-
tractive caption generation, image understanding
through topic modeling, and sentence compres-
sion. 1 These methods have previously been ap-
plied individually to related tasks such as gen-
eral domain image captioning and annotation. We
briefly describe some of the related work:

2.1 Extractive Caption Generation

In previous work on image caption extraction, cap-
tions are generated by retrieving human-authored
descriptions from visually similar images. Farhadi
et al. (2010) and Ordonez et al. (2011) retrieve
whole captions to apply to a query image, while
Kuznetsova et al. (2012) generate captions using
text retrieved from multiple sources. The descrip-
tions are related to visual concepts in the query
image, but these models use visual similarity to
approximate textual relevance; they do not model
image and textual features jointly.

2.2 Image Understanding

Recent improvements in state-of-the-art visual ob-
ject class detections (Felzenszwalb et al., 2010)

1A research proposal for this framework and other image
captioning ideas was previously presented at NAACL Stu-
dent Research Workshop in 2013 (Mason, 2013). This paper
presents a completed project including implementation de-
tails and experimental results.

have enabled much recent work in image caption
generation (Farhadi et al., 2010; Ordonez et al.,
2011; Kulkarni et al., 2011; Yang et al., 2011;
Mitchell et al., 2012; Yu and Siskind, 2013). How-
ever, these systems typically rely on a small num-
ber of detection types, e.g. the twenty object cate-
gories from the PASCAL VOC challenge.2 These
object categories include entities which are com-
monly described in general domain images (peo-
ple, cars, cats, etc) but these require labeled train-
ing data which is not typically available for the vi-
sually relevant entities in specific domains.

Our caption generation system employs a multi-
modal topic model from our previous work (Ma-
son and Charniak, 2013) which generates descrip-
tive words, but lacks the spatial structure needed
to generate a full sentence caption. Other previ-
ous work uses topic models to learn the semantic
correspondence between images and labels (e.g.
Blei and Jordan (2003)), but learning from natural
language descriptions is considerably more diffi-
cult because of polysemy, hypernymy, and mis-
alginment between the visual content of an im-
age and the content humans choose to describe.
The MixLDA model (Feng and Lapata, 2010b;
Feng and Lapata, 2010a) learns from news images
and natural language descriptions, but to generate
words for a new image it requires both a query
image and query text in the form of a news arti-
cle. Berg et al. (2010) use discriminative models
to discover visual attributes from online shopping
images and captions, but their models do not gen-
erate descriptive words for unseen images.

2.3 Sentence Compression

Typical models for sentence compression (Knight
and Marcu, 2002; Furui et al., 2004; Turner and
Charniak, 2005; Clarke and Lapata, 2008) have a
summarization objective: reduce the length of a
source sentence without changing its meaning. In
contrast, our objective is to change the meaning of
the source sentence, letting its overall correctness
relative to the query image determine the length
of the output. Our objective differs from that of
Kuznetsova et al. (2013), who compress image
caption sentences with the objective of creating a
corpus of generally transferrable image captions.
Their compression objective is to maximize the
probability of a caption conditioned on the source

2http://pascallin.ecs.soton.ac.uk/
challenges/VOC/

12



Two adjustable buckle
straps top a classic rubber
rain boot grounded by a
thick lug sole for excellent
wet-weather traction.

Available in Plus Size. Faux
snake skin flats with a large
crossover buckle at the toe.
Padded insole for a comfort-
able all day fit.

Glitter-covered elastic up-
per in a two-piece dress san-
dal style with round open
toe. Single vamp strap with
contrasting trim matching
elasticized heel strap criss-
crosses at instep.

Explosive! These white
leather joggers are sure to
make a big impression. De-
tails count, including a toe
overlay, millennium trim
and lightweight raised sole.

Table 1: Example data from the Attribute Discovery Dataset (Berg et al., 2010). See Section 3.

image, while our objective is conditioned on the
query image that we are generating a caption for.
Additionally, their model also relies on general-
domain trained visual detections.

3 Dataset and Preprocessing

The dataset we use is the women’s shoes sec-
tion of the publicly available Attribute Discov-
ery Dataset3 from Berg et al. (2010), which con-
sists of product images and captions scraped from
the shopping website Like.com. We use the
women’s shoes section of the dataset which has
14764 captioned images. Product descriptions de-
scribe many different attributes such as styles, col-
ors, fabrics, patterns, decorations, and affordances
(activities that can be performed while wearing the
shoe). Some examples are shown in Table 1.

For preprocessing in our framework, we first de-
termine an 80/20% train test split. We define a tex-
tual vocabulary of “descriptive words”, which are
non-function words – adjectives, adverbs, nouns
(except proper nouns), and verbs. This gives us
a total of 9578 descriptive words in the training
set, with an average of 16.33 descriptive words per
caption.

4 Image Captioning Framework

4.1 Extraction

To repeat, our overall process is to first find a cap-
tion sentence from our database to use as a tem-
plate, and then correct the template sentences us-
ing sentence compresion. We compress by remov-

3http://tamaraberg.com/
attributesDataset/index.html

ing details that are probably not correct for the test
image. For example, if the sentence describes “a
red slipper” but the shoe in the query image is yel-
low, we want to remove “red” and keep the rest.

As in this simple example, the basic paradigm
for compression is to keep the head words of
phrases (“slipper”) and remove modifiers. Thus
we want to extraction stage of our scheme to be
more likely to find a candidate sentence with cor-
rect head words, figuring that the compression
stage can edit the mistakes. Our hypothesis is that
headwords tend to describe more spatially struc-
tured visual concepts, while modifier words de-
scribe those that are more easily represented using
local or unstructured features.4 Table 2 contains
additional example captions with parses.

GIST (Oliva and Torralba, 2001) is a com-
monly used feature in Computer Vision which
coarsely localizes perceptual attributes (e.g. rough
vs smooth, natural vs manmade). By computing
the GIST of the images, we project them into a
multi-dimensional Euclidean space where images
with semantically similar structures are located
near each other. Thus the extraction stage of our
caption generation process selects a sentence from
the GIST nearest-neighbor to the query image.5

4.2 Joint Topic Model

The second component of our framework incorpo-
rates visual and textual features using a less struc-
tured model. We use a multi-modal topic model

4For example, the color “red” can be described using a
bag of random pixels, while a “slipper” is a spatial configura-
tion of parts in relationship to each other.

5See Section 5.1 for additional implementation details.

13



Table 2: Example parses of women’s shoes descriptions. Our hypothesis is that the headwords in phrases
are more likely to describe visual concepts which rely on spatial locations or relationships, while modi-
fiers words can be represented using less-structured visual bag-of-words features.

to learn the latent topics which generate bag-of-
words features for an image and its caption.

The bag-of-words model for Computer Vision
represents images as a mixture of topics. Mea-
sures of shape, color, texture, and intensity are
computed at various points on the image and clus-
tered into discrete “codewords” using the k-means
algorithm.6 Unlike text words, an individual code-
word has little meaning on its own, but distri-
butions of codewords can provide a meaningful,
though unstructured, representation of an image.

An image and its caption do not express exactly
the same information, but they are topically re-
lated. We employ the Polylingual Topic Model
(Mimno et al., 2009), which is originally used to
model corresponding documents in different lan-
guages that are topically comparable, but not par-
allel translations. In particular, we employ our
previous work (Mason and Charniak, 2013) which
extends this model to topically similar images and
natural language captions. The generative process
for a captioned image starts with a single topic
distribution drawn from concentration parameter
α and base measure m:

θ ∼ Dir(θ, αm) (1)

Modality-specific latent topic assignments zimg

and ztxt are drawn for each of the text words and
codewords:

zimg ∼ P (zimg|θ) =
∏
n

θ
zimgn

(2)

6While space limits a more detailed explanation of visual
bag-of-word features, Section 5.2 provides a brief overview
of the specific visual attributes used in this model.

ztxt ∼ P (ztxt|θ) =
∏
n

θztxtn (3)

Observed words are generated according to their
probabilities in the modality-specific topics:

wimg ∼ P (wimg|zimg,Φimg) = φimg
wimgn |zimgn

(4)

wtxt ∼ P (wtxt|ztxt,Φtxt) = φtxtwtxtn |ztxtn (5)

Given the uncaptioned query image qimg and
the trained multi-modal topic model, it is now pos-
sible to infer the shared topic proportion for qimg

using Gibbs sampling:

P (zn = t|qimg, z\n,Φimg, αm)

∝ φimg
qimgn |t

(Nt)\n + αmt∑
tNt − 1 + α

(6)

4.3 Sentence Compression

Let w = w1, w2, ..., wn be the words in the ex-
tracted caption for qimg. For each word, we de-
fine a binary decision variable δ, such that δi = 1
if wi is included in the output compression, and
δi = 0 otherwise. Our objective is to find values
of δ which generate a caption for qimg which is
both semantically and grammatically correct.

We cast this problem as an Integer Linear Pro-
gram (ILP), which has previously been used for
the standard sentence compression task (Clarke
and Lapata, 2008; Martins and Smith, 2009). ILP
is a mathematical optimization method for deter-
mining the optimal values of integer variables in
order to maximize an objective given a set of con-
straints.

14



4.3.1 Objective
The ILP objective is a weighted linear combina-
tion of two measures which represent the correct-
ness and fluency of the output compression:

Correctness: Recall in Section 3 we defined
words as either descriptive words or function
words. For each descriptive word, we estimate
P (wi|qimg), using topic proportions estimated us-
ing Equation 6:

P (wi|qimg) =
∑

t

P (wi|ztxtt )P (zt|qimg) (7)

This is used to find I(wi), a function of the likeli-
hood of each word in the extracted caption:

I(wi) =

{
P (wi|qimg)− P (wi), if descriptive
0, function word

(8)
This function considers the prior probability of wi
because frequent words often have a high posterior
probability even when they are inaccurate. Thus
the sum

∑n
i=1 δi · I(wi) is the overall measure of

the correctness of a proposed caption conditioned
on qimg.

Fluency: We formulate a trigram language
model as an ILP, which requires additional binary
decision variables: αi = 1 if wi begins the out-
put compression, βij = 1 if the bigram sequence
wi, wj ends the compression, γijk = 1 if the tri-
gram sequence wi, wj , wk is in the compression,
and a special “start token” δ0 = 1. This language
model favors shorter sentences, which is not nec-
essarily the objective for image captioning, so we
introduce a weighting factor, λ, to lessen the ef-
fect.

Here is the combined objective, using P to rep-
resent logP :

max z =

(
n∑

i=1

αi · P (wi|start)

+
n−2∑
i=1

n−1∑
j=i+1

n∑
k=j+1

γijk · P (wk|wi, wj)

+
n−1∑
i=0

n∑
j=i+1

βij · P (end|wi, wj)
)
· λ

+
n∑

i=1

δi · I(wi) (9)

Sequential

1.)
∑

i αi = 1

2.) δk − αk −
∑k−2

i=0

∑k−1
j=1 γijk = 0

∀k : k ∈ 1...n
3.) δj−∑j−1i=0 ∑nk=j+1 γijk−∑j−1i=0 βij = 0

∀j : j ∈ 1...n
4.)

∑n−1
j=i+1

∑n
k=j+1 γijk −

∑n
j=i+1 βij −∑i−1

h=0 βhi − δi = 0 ∀i : i ∈ 1...n
5.)

∑n−1
i=0

∑n
j=i+1 βij = 1

Modifier
1. If head of the extracted sentence= wi, then
δi = 1
2. If wi is head of a noun phrase, then δi = 1
3. Punctuation and coordinating conjunctions
follow special rules (below). Otherwise, if
headof(wi) = wj , then δi ≤ δj

Other 1.
∑

i δi ≥ 3
2. Define valid use of puncutation and coordi-
nating conjunctions.

Table 3: Summary of ILP constraints.

4.3.2 ILP Constraints

The ILP constraints ensure both the mathematical
validity of the model, and the grammatical correct-
ness of its output. Table 3 summarizes the list of
constraints. Sequential constraints are defined as
in Clarke (2008) ensure that the ordering of the tri-
grams is valid, and that the mathematical validity
of the model holds.

5 Implementation Details

5.1 Extraction

GIST features are computed using code by Oliva
and Torralba (2001)7. GIST is computed with im-
ages converted to grayscale; since color features
tend to act as modifiers in this domain. Nearest-
neighbors are selected according to minimum dis-
tance from qimg to both a regularly-oriented and a
horizontally-flipped training image.

Only one sentence from the first nearest-
neighbor caption is extracted. In the case of multi-
sentence captions, we select the first suitable sen-
tence according to the following criteria 1.) has
at least five tokens, 2.) does not contain NNP or
NNPS (brand names), 3.) does not fail to parse
using Stanford Parser (Klein and Manning, 2003).
If the nearest-neighbor caption does not have any
sentences meeting these criteria, caption sentences
from the next nearest-neighbor(s) are considered.

7http://people.csail.mit.edu/torralba/
code/spatialenvelope/

15



5.2 Joint Topic Model

We use the Joint Topic Model that we imple-
mented in our previous work; please see Mason
and Charniak (2013) for the full model and imple-
mentation details. The topic model is trained with
200 topics using the polylingual topic model im-
plementation from MALLET8. Briefly, the code-
words represent the following attributes:

SHAPE: SIFT (Lowe, 1999) describes the
shapes of detected edges in the image, using de-
scriptors which are invariant to changes in rotation
and scale.

COLOR: RGB (red, green, blue) and HSV (hue,
saturation, value) pixel values are sampled from a
central area of the image to represent colors.

TEXTURE: Textons (Leung and Malik, 2001)
are computed by convolving images with Gabor
filters at multiple orientations and scales, then
sampling the outputs at random locations.

INTENSITY: HOG (histogram of gradients)
(Dalal and Triggs, 2005) describes the direction
and intensity of changes in light. These features
are computed on the image over a densely sam-
pled grid.

5.3 Compression

The sentence compression ILP is implemented us-
ing the CPLEX optimization toolkit9. The lan-
guage model weighting factor in the objective is
λ = 10−3, which was hand-tuned according to
observed output. The trigram language model
is trained on training set captions using Berke-
leyLM (Pauls and Klein, 2011) with Kneser-Ney
smoothing. For the constraints, we use parses
from Stanford Parser (Klein and Manning, 2003)
and the “semantic head” variation of the Collins
headfinder Collins (1999).

6 Evaluation

6.1 Setup

We compare the following systems and baselines:
KL (EXTRACTION): The top performing ex-

tractive model from Feng and Lapata (2010a), and
the second-best captioning model overall. Using
estimated topic distributions from our joint model,
we extract the source with minimum KL Diver-
gence from qimg.

8http://mallet.cs.umass.edu/
9http://www-01.ibm.com/

software/integration/optimization/
cplex-optimization-studio/

ROUGE-2 Average 95% Confidence int.
KL (EXTRACTION)
P .06114 ( .05690 - .06554 )
R .02499 ( .02325 - .02686)
F .03360 ( .03133 - .03600 )
GIST (EXTRACTION)
P .10894 ( .09934 - .11921 )
R .05474 ( .04926 - .06045)
F .06863 ( .06207 - .07534)
LM-ONLY (COMPRESSION)
P .13782 ( .12602 - .14864 )
R .02437 ( .02193 - .02700 )
F .03864 ( .03512 - .04229)
SYSTEM (COMPRESSION)
P .16752 (.15679 -.17882 )
R .05060 ( .04675 - .05524 )
F .07204 ( .06685 - .07802 )

Table 4: ROUGE-2 (bigram) scores. The pre-
cision of our system compression (bolded) sig-
nificantly improves over the caption that it com-
presses (GIST), without a significant decrease in
recall.

GIST (EXTRACTION): The sentence extracted
using GIST nearest-neighbors, and the uncom-
pressed source for the compression systems.

LM-ONLY (COMPRESSION): We include this
baseline to demonstrate that our model is effec-
tively conditioning output compressions on qimg,
as opposed to simply generalizing captions as in
Kuznetsova et al. (2013)10. We modify the com-
pression ILP to ignore the content objective and
only maximize the trigram language model (still
subject to the constraints).

SYSTEM (COMPRESSION): Our full system.
Unfortunately, we cannot compare our system

against prior work in general-domain image cap-
tioning, because those models use visual detec-
tion systems which train on labeled data that is not
available in our domain.

6.2 Automatic Evaluation
We perform automatic evaluation using similar-
ity measures between automatically generated and
human-authored captions. Note that currently
our system and baselines only generate single-
sentence captions, but we compare against entire

10Technically their model is conditioned on the source im-
age, in order to address alignment issues which are not appli-
cable in our setup.

16



BLEU@1
KL (EXTRACTION) .2098

GIST (EXTRACTION) .4259
LM-ONLY (COMPRESSION) .4780
SYSTEM (COMPRESSION) .4841

Table 5: BLEU@1 scores of generated captions
against human authored captions. Our model
(bolded) has the highest BLEU@1 score with sig-
nificance.

held-out captions in order to increase the amount
of text we have to compare against.

ROUGE (Lin, 2004) is a summarization eval-
uation metric which has also been used to eval-
uate image captions (Yang et al., 2011). It is
usually a recall-oriented measure, but we also re-
port precision and f-measure because our sen-
tence compressions do not improve recall. Table 4
shows ROUGE-2 (bigram) scores computed with-
out stopwords.

We observe that our system very significantly
improves ROUGE-2 precision of the GIST ex-
tracted caption, without significantly reducing re-
call. While LM-Only also improves precision
against GIST extraction, it indiscriminately re-
moves some words which are relevant to the
query image. We also observe that GIST extrac-
tion strongly outperforms the KL model, which
demonstrates the importance of visual structure.

We also report BLEU (Papineni et al., 2002)
scores, which are the most popularly accepted au-
tomatic metric for captioning evaluation (Farhadi
et al., 2010; Kulkarni et al., 2011; Ordonez et
al., 2011; Kuznetsova et al., 2012; Kuznetsova
et al., 2013). Results are very similar to the
ROUGE-2 precision scores, except the difference
between our system and LM-Only is less pro-
nounced because BLEU counts function words,
while ROUGE does not.

6.3 Human Evaluation

We perform human evaluation of compressions
generated by our system and LM-Only. Users are
shown the query image, the original uncompressed
caption, and a compressed caption, and are asked
two questions: does the compression improve the
accuracy of the caption, and is the compression
grammatical.

We collect 553 judgments from six women who
are native English-speakers and knowledgeable

Query Image GIST Nearest-Neighbor

Extraction: Shimmering snake-embossed leather upper in a
slingback evening dress sandal style with a round open toe.

Compression: Shimmering upper in a slingback evening
dress sandal style with a round open toe.

Query Image GIST Nearest-Neighbor

Extraction: This sporty sneaker clog keeps foot cool and
comfortable and fully supported.

Compression: This clog keeps foot comfortable and sup-
ported.

Query Image GIST Nearest-Neighbor

Extraction: Italian patent leather peep-toe ballet flat with a
signature tailored grosgrain bow.

Compression: leather ballet flat with a signature tailored
grosgrain bow.

Query Image GIST Nearest-Neighbor

Extraction: Platform high heel open toe pump with horsebit
available in silver guccissima leather with nickel hardware
with leather sole.

Compression: Platform high heel open toe pump with
horsebit available in leather with nickel hardware with
leather sole.

Table 6: Example output from our full system.
Red underlined words indicate the words which
are deleted by our compression model.

17



SYSTEM LM-ONLY
Yes No Yes No

Compression
improves
accuracy

63.2% 36.8% 42.6% 57.4%

Compression is
grammatical

73.1% 26.9% 82.2% 17.8%

Table 7: Human evaluation results.

about fashion.11 Users were recruited via email
and did the study over the internet.

Table 7 reports the results of the human evalu-
ation. Users report 63.2% of SYSTEM compres-
sions improve accuracy over the original, while
the other 36.8% did not improve accuracy. (Keep
in mind that a bad compression does not make the
caption less accurate, just less descriptive.) LM-
ONLY improves accuracy for less than half of the
captions, which is significantly worse than SYS-
TEM captions (Fisher exact test, two-tailed p less
than 0.01).

Users find LM-Only compressions to be slightly
more grammatical than System compressions, but
the difference is not significant. (p > 0.05)

7 Conclusion

We introduce the task of domain-specific image
captioning and propose a captioning system which
is trained on online shopping images and natu-
ral language descriptions. We learn a joint topic
model of vision and text to estimate the correct-
ness of extracted captions, and use a sentence
compression model to propose a more accurate
output caption. Our model exploits the connection
between image and sentence structure, and can be
used to improve the accuracy of extracted image
captions.

The task of domain-specific image caption
generation has been overlooked in favor of the
general-domain case, but we believe the domain-
specific case deserves more attention. While
image captioning can be viewed as a complex
grounding problem, a good image caption should
do more than label the objects in the image. When
an expert looks at images in a specific domain, he
or she makes inferences that would not be made by
a non-expert. Providing this information to non-

11About 15% of output compressions are the same for both
systems, and about 10% have no deleted words in the output
compression. We include the former in the human evaluation,
but not the latter.

Query Image GIST Nearest-Neighbor

Extraction: Classic ballet flats with decorative canvas
strap and patent leather covered buckle.

Compression: Classic ballet flats covered.
Query Image GIST Nearest-Neighbor

Extraction: This shoe is the perfect shoe for you , fea-
turing an open toe and a lace up upper with a high heel
, and a two tone color .

Compression: This shoe is the shoe , featuring an open toe
and upper with a high heel .

Table 8: Examples of bad performance. The top
example is a parse error, while the bottom exam-
ple deletes modifiers that are not part of the image
description.

expert users in the form of an image caption will
greatly expand the utility for automatic image cap-
tioning.

References
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.

2010. Automatic attribute discovery and charac-
terization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV’10, pages 663–676, Berlin, Heidel-
berg. Springer-Verlag.

David M. Blei and Michael I. Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and development in informaion retrieval, SIGIR ’03,
pages 127–134, New York, NY, USA. ACM.

James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. J. Artif. Int. Res., 31(1):399–
429, March.

James Clarke. 2008. Global Inference for Sentence
Compression: An Integer Linear Programming Ap-
proach. Dissertation, University of Edinburgh.

Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.

18



N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
886 –893 vol. 1, june.

Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV’10, pages 15–29,
Berlin, Heidelberg. Springer-Verlag.

P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2010. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627–1645.

Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1239–1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In
HLT-NAACL, pages 831–839.

Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,
and Chiori Hori. 2004. Speech-to-text and speech-
to-speech summarization of spontaneous speech.
IEEE TRANS. ON SPEECH AND AUDIO PRO-
CESSING, 12(4):401–408.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91–107, July.

Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601–1608.

Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.

Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.

T. Leung and J. Malik. 2001. Representing and rec-
ognizing the visual appearance of materials using
three-dimensional textons. International Journal of
Computer Vision, 43(1):29–44.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.

D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The
Proceedings of the Seventh IEEE International Con-
ference on, volume 2, pages 1150 –1157 vol.2.

André F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ’09, pages 1–9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

R. Mason and E. Charniak. 2013. Annotation of online
shopping images without labeled training examples.
Workshop on Vision and Language (WVL).

Rebecca Mason. 2013. Domain-independent caption-
ing of domain-specific images. NAACL Student Re-
search Workshop.

David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2 - Volume
2, EMNLP ’09, pages 880–889, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daumé III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).

Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145–175.

V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

19



Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
Portland, Oregon, June. Association for Computa-
tional Linguistics.

Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 290–297, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Yezhou Yang, Ching Lik Teo, Hal Daumé III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.

Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53–63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.

20


