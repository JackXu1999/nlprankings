



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 263–268
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2041

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 263–268
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2041

Discourse Annotation of Non-native Spontaneous Spoken Responses
Using the Rhetorical Structure Theory Framework

Xinhao Wang1, James V. Bruno2, Hillary R. Molloy1, Keelan Evanini2, Klaus Zechner2
Educational Testing Service

190 New Montgomery St #1500, San Francisco, CA 94105, USA
2660 Rosedale Road, Princeton, NJ 08541, USA

xwang002, jbruno, hmolloy, kevanini, kzechner@ets.org

Abstract

The availability of the Rhetorical Struc-
ture Theory (RST) Discourse Treebank
has spurred substantial research into dis-
course analysis of written texts; how-
ever, limited research has been conducted
to date on RST annotation and parsing
of spoken language, in particular, non-
native spontaneous speech. Considering
that the measurement of discourse coher-
ence is typically a key metric in human
scoring rubrics for assessments of spo-
ken language, we initiated a research ef-
fort to obtain RST annotations of a large
number of non-native spoken responses
from a standardized assessment of aca-
demic English proficiency. The result-
ing inter-annotator κ agreements on the
three different levels of Span, Nuclear-
ity, and Relation are 0.848, 0.766, and
0.653, respectively. Furthermore, a set
of features was explored to evaluate the
discourse structure of non-native sponta-
neous speech based on these annotations;
the highest performing feature showed a
correlation of 0.612 with scores of dis-
course coherence provided by expert hu-
man raters.

1 Introduction

The spread of English as the global language of
education and commerce is continuing, and there
is a strong interest in developing assessment sys-
tems that can automatically score spontaneous
speech from non-native speakers with the goals
of reducing the burden on human raters, improv-
ing reliability, and generating feedback that can
be used by language learners. Discourse coher-
ence, which refers to the conceptual relations be-

tween different units within a response, is an im-
portant aspect of communicative competence, as
is reflected in human scoring rubrics for assess-
ments of non-native English (ETS, 2012). How-
ever, discourse-level features have rarely been in-
vestigated in the context of automated speech scor-
ing. This study aims to construct a discourse-
level annotation of non-native spontaneous speech
in the framework of Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988), which can
then be used in automated discourse analysis and
coherence measurement for non-native spoken re-
sponses, thereby improving the validity of the au-
tomated scoring systems.

RST is a descriptive framework that has been
widely used in the analysis of discourse organiza-
tion of written texts (Taboada and Mann, 2006b),
and has been applied to various natural lan-
guage processing tasks, including language gen-
eration, text summarization, and machine trans-
lation (Taboada and Mann, 2006a). In particu-
lar, the availability of RST annotations on a se-
lection of 385 Wall Street Journal articles from
the Penn Treebank1 (Carlson et al., 2001) has
facilitated RST-based discourse analysis of writ-
ten texts, since it provides a standard benchmark
for comparing the performance of different tech-
niques for document-level discourse parsing (Joty
et al., 2013; Feng and Hirst, 2014).

Another important application of RST closely
related to our research is the automated evaluation
of discourse in student essays. For example, one
study used features for each sentence in an essay
to reflect the status of its parent node as well as its
rhetorical relation based on automatically parsed
RST trees with the goal of providing feedback to
students about the discourse structure in the essay
(Burstein et al., 2003). Another study compared

1https://catalog.ldc.upenn.edu/
LDC2002T07

263

https://doi.org/10.18653/v1/P17-2041
https://doi.org/10.18653/v1/P17-2041


features derived from deep hierarchical discourse
relations based on RST parsing and features de-
rived from shallow discourse relations based on
Penn Discourse Treebank (PDTB) (Prasad et al.,
2008) parsing in the task of essay scoring and
demonstrated the effectiveness of deep discourse
structure in better differentiation of text coherence
(Feng et al., 2014).

Related work has also been conducted to anno-
tate discourse relations in spoken language, which
is produced and processed differently from writ-
ten texts (Rehbein et al., 2016), and often lacks
explicit discourse connectives that are more fre-
quent in written language. Instead of the rooted-
tree structure that is employed in RST, the annota-
tion scheme with shallow discourse structure and
relations from the PDTB (Prasad et al., 2008) has
been generally used for spoken language (Demi-
rahin and Zeyrek, 2014; Stoyanchev and Banga-
lore, 2015). For example, Tonelli et al. adapted the
PDTB annotation scheme to annotate discourse
relations in spontaneous conversations in Italian
(Tonelli et al., 2010) and Rehbein et al. compared
two frameworks, PDTB and CCR (Cognitive ap-
proach to Coherence Relations) (Sanders et al.,
1992), for the annotation of discourse relations in
spoken language (Rehbein et al., 2016).

In contrast to these previous studies, this study
focuses on monologic spoken responses produced
by non-native speakers within the context of a lan-
guage proficiency assessment. A discourse an-
notation scheme based on the RST framework
was selected due to the fact that it can effectively
demonstrate the deep hierarchical discourse struc-
ture across an entire response, rather than focusing
on the local coherence of adjacent units.

2 Data

This study obtained manual RST annotations on
a corpus of 600 spoken responses drawn from a
large-scale, high-stakes standardized assessment
of English for non-native speakers, the TOEFL R©

Internet-based Test (TOEFL R© iBT), which as-
sesses English communication skills for academic
purposes (ETS, 2012). The speaking section of the
TOEFL iBT assessment contains six tasks, each
of which requires the test taker to provide an un-
scripted spoken response 45 or 60 seconds in du-
ration. The corpus used in this study includes
100 responses from each of six different test ques-
tions that comprise two different speaking tasks:

1) Independent questions: providing an opinion
based on personal experience (N = 200 responses)
and 2) Integrated questions: summarizing or dis-
cussing material provided in a reading and/or lis-
tening passage (N = 400 responses). The spo-
ken responses were all manually transcribed using
standard punctuation and capitalization. The av-
erage number of words per response is 104.4 (st.
dev. = 34.4) and the average number of sentences
is 5.5 (st. dev. = 2.1).

The spoken responses were all provided with
holistic English proficiency scores on a scale of 1
to 4 by expert human raters in the context of opera-
tional, high-stakes scoring for the spoken language
assessment. The scoring rubrics address the fol-
lowing three main aspects of speaking proficiency:
delivery (pronunciation, fluency, prosody), lan-
guage use (grammar and lexical choice), and topic
development (content and coherence). In order to
ensure a sufficient quantity of responses from each
proficiency level, 25 responses were selected ran-
domly from each of the 4 score points for each of
the 6 test questions.

The current study builds on a previous study
that investigated approaches for modeling dis-
course coherence in non-native spontaneous
speech (but which did not consider the hierarchical
rhetorical structure of speech) (Wang et al., 2013).
In that study, each spoken response in the same
corpus that was used for the current study was
provided with global discourse coherence scores.
Two expert annotators (not drawn from the pool
of expert human raters who provided the holistic
scores) provided each response with a score on
a scale of 1 to 3 based on the orthographic tran-
scriptions of the spoken response. The three score
points were defined as follows: 3 = highly co-
herent (contains no instances of confusing argu-
ments or examples), 2 = somewhat coherent (con-
tains some awkward points in which the speaker’s
line of argument is unclear), 1 = barely coherent
(the entire response was confusing and hard to
follow). In addition, the annotators were specif-
ically required to ignore disfluencies and gram-
matical errors as much as possible. The inter-
annotator agreement for these coherence scores
was κ = 0.68. These discourse coherence scores
are reused in the current study (along with the
holistic profiency scores presented above) to eval-
uate the performance of features measuring dis-
course coherence based on the RST annotations.

264



3 Annotation

3.1 Guidelines
We used a modified version of the tagging ref-
erence manual for the RST Discourse Treebank
(Carlson and Marcu, 2001) for this study. Ac-
cording to these guidelines, annotators segment
a transcribed spoken response into Elementary
Discourse Unit (EDU) spans of text (correspond-
ing to clauses or clause-like units), and indi-
cate rhetorical relations between non-overlapping
spans which typically consist of a nucleus (the
most essential information in the rhetorical re-
lation) and a satellite (supporting or background
information). In contrast to well-formed writ-
ten text, non-native spontaneous speech frequently
contains ungrammatical sentences, disfluencies,
fillers, hesitations, false starts, and unfinished ut-
terances. In some cases, these spoken responses
do not constitute coherent, well-formed discourse.
On the other hand, spoken responses are relatively
shorter and comprise simpler discourse structures
with fewer relations, which simplifies the RST an-
notation task in comparison to written text. In
order to account for these differences, we cre-
ated an addendum to the RST Discourse Treebank
manual introducing the following additional rela-
tions: Disfluency relations (in which the disflu-
ent span is the satellite and the corresponding flu-
ent span is the nucleus), Awkward relations (cor-
responding to portions of the response where the
speaker’s discourse structure is infelicitous; awk-
ward relations are based on pre-existing relations,
such as awkward-Reason, if the intended relation
is clear but is expressed incoherently or are labeled
as awkward-Other if there is no clear relation
between the awkward EDU and the surrounding
discourse), Unfinished Utterance relations (repre-
senting EDUs at the end of a response that are in-
complete because the test taker ran out of time in
which the incomplete span is the satellite and the
root node of the discourse tree is the nucleus), and
Discourse Particle relations (such as you know and
right, which are satellites of adjacent spans).

The discourse annotation tool used in the RST
Discourse Treebank2 was also adopted for this
study. Using this tool, annotators incrementally
build hierarchical discourse trees in which the
leaves are the EDUs and the internal nodes cor-
respond to contiguous spans of text. When the an-

2Downloaded from http://www.isi.edu/
licensed-sw/RSTTool/index.html

notators assign the rhetorical relation for a node of
the tree, they provide the relation’s label (drawn
from the pre-defined set of relations in the anno-
tation guidelines) and also indicate whether the
spans that comprise the relation are nuclei or satel-
lites. Figure 1 shows an example of an annotated
RST tree for a response with a proficiency score
of 1. This response includes three disfluencies
(EDUs 3, 6, and 9), which are satellites of the
corresponding repair nuclei. In addition, the re-
sponse also includes an awkward Comment-Topic
relation between EDU 2 and the node combin-
ing EDUs 3-11, indicated by awkward-Comment-
Topic-2; in this multinuclear relation, the annota-
tor judged that the second branch of the relation
was awkward, which is indicated by the 2 that was
appended to the relation label.

3.2 Pilot Annotation

The manual annotations were provided by two ex-
perts with prior experience in various types of data
annotation on both text and speech. First, a pi-
lot annotation was conducted to train and cali-
brate the annotators based on 48 training samples
drawn from the TOEFL R© Practice Online (TPO)
product3, which offers practice tests simulating
the TOEFL iBT testing experience with authentic
test questions. The training samples were selected
from a TPO test form with 6 test questions and
were balanced according to test question and pro-
ficiency score, i.e., 2 responses from each score
level for each question.

Human annotators were trained in a two-step
process: 1) after a comprehensive study of the an-
notation guidelines described in Section 3.1, the
two annotators were initially trained with 16 TPO
responses (8 responses from an Independent ques-
tion and 8 responses from an Integrated question)
by first performing independent annotation and
then resolving all disagreements through a discus-
sion of the guidelines; 2) another round of training
was conducted on an additional set of 32 TPO re-
sponses (8 responses from an Independent ques-
tion and 24 responses from an Integrated ques-
tion). Each annotator first annotated this set of 32
responses independently; the two annotators sub-
sequently conducted a thorough joint review and
discussion of each other’s annotations in order to
resolve all disagreements on this set.

In order to measure the human agreement on

3https://toeflpractice.ets.org/

265



Figure 1: Example of an annotated RST tree on a response with a proficiency score of 1.

the EDU segmentation task, we first converted the
segmentation sequences into 0/1 sequences: for
each word in a response, 1 is assigned if a seg-
ment boundary exists after the word; otherwise,
0 is assigned. The inter-annotator agreement rate
on the EDU segmentations of the 32 pilot samples
(from stage 2) was κ = 0.876. On the hierarchi-
cal tree building task, inter-annotator agreement
was evaluated on the levels of Span (assignment
of discourse segment), Nuclearity (assignment of
nucleus vs. satellite), and Relation (assignment of
rhetorical relation) using κ, as described in (Marcu
et al., 1999); on the 32 samples, the κ values are
0.861, 0.769, and 0.631 for the three levels, re-
spectively.

3.3 Formal Annotation

For the formal annotation on the full set of 600
TOEFL spoken responses, 120 responses from 6
test questions (5 responses from each score level
from each question) were selected for double an-
notation, and the remaining 480 responses re-
ceived a single annotation.

The 120 double-annotated responses were split
into two batches of equal size and the two annota-
tors each performed EDU segmentation indepen-
dently on one of the batches. Subsequently, the
annotators reviewed each other’s EDU segmen-
tations and adjudicated all disagreements to ob-
tain gold-standard EDU segmentation for the 120
responses in the double-annotation set. The av-
erage number of EDUs per response in the two
batches in this set of 120 responses were 15.1 and
14.1. The annotators subsequently performed the
remaining steps of RST annotation (assigning the
relations, nuclearity, and hierarchical structure) in-
dependently on all 120 responses using the adjudi-
cated EDU segmentations. Table 1 shows that the
κ agreements on the three levels of Span, Nucle-

Table 1: Human agreement on RST annotations in
terms of κ and F1-Measure.

Span Nuclearity Relation
κ 0.848 0.766 0.653

F1-Measure 0.872 0.724 0.522

Table 2: The average number of awkward relations
appearing in responses from each of the four pro-
ficiency score levels.

1 2 3 4
Annotator 1 3.2 1.1 1.1 0.3
Annotator 2 2.1 1.2 0.7 0.3

arity, and Relation are 0.848, 0.766, and 0.653, re-
spectively. Besides the κ evaluation, the standard
ways of F1-Measure on three levels of Span, Nu-
clearity, and Relation (Marcu, 2000), commonly
used to evaluate the performance of RST parsers,
are also reported in Table 1. The F1-measures
were calculated according to each pair of trees
from two annotators on the same sample and then
averaged across all samples, i.e., a macroaveraged
F1-measure.

The human agreement results also indicate that
two annotators tend to agree better on responses
from speakers with higher speaking proficiency
levels. This is demonstrated by positive correla-
tions between the F1 agreement scores and the hu-
man proficiency ratings: 0.197 for Span annota-
tions, 0.210 for Nuclearity, and 0.188 for Relation.

In addition, we also examined the distribution
of the manually identified awkward relations. As
shown in Table 2, awkward points occur with
higher frequency in responses with lower profi-
ciency scores.

266



4 Discourse Features

The ultimate aim of this line of research is to
use an RST-annotated corpus to investigate fea-
tures for automatically assessing discourse struc-
ture in spontaneous non-native speech. Using
the annotated discourse trees, we extracted sev-
eral different features based on the distribution
of relations and the structure of the trees, includ-
ing the number of EDUs (n edu), the number
of relations (n rel), the number of awkward re-
lations (n awk rel), the number of rhetorical re-
lations, i.e., relations that were neither classified
as awkward nor as a disfluency (n rhe rel), the
number of different types of rhetorical relations
(n rhe relTypes), the percentage of rhetorical rela-
tions (perc rhe rel) out of all relations, the depth of
the RST trees (tree depth), and the ratio between
n edu and tree depth (ratio nedu depth). Table 3
lists the Pearson correlation coefficients of these
features with both the holistic proficiency scores
and the discourse coherence scores and demon-
strates the effectiveness of these features. The
n rhe rel feature achieves the highest correlation
of 0.691 with the holistic proficiency scores, and
the normalized feature perc rhe rel achieves the
highest correlation of 0.612 with the discourse co-
herence scores. It is interesting to note that RST-
based discourse features generally have higher
correlations with the holistic speaking proficiency
scores than with the more specific discourse co-
herence scores. This result is somewhat unex-
pected, since the holistic proficiency scores are
based only partially on discourse coherence and
also cover other aspects of speaking proficiency,
such as pronunciation, fluency, grammar, and vo-
cabulary. One potential explanation for the higher
correlations could be the difference in score range
(1-4 for the holistic proficiency scores and 1-3 for
the discourse scores). In addition, as described in
Section 2, the data set used in this study was cre-
ated using a stratified random sample with an even
distribution of holistic scores (which may increase
the features’ correlations with holistic scores), but
this constraint does not apply to the discourse co-
herence scores.

5 Conclusion and Future Work

In this study, we obtained discourse coherence an-
notations based on Rhetorical Structure Theory
for a corpus of 600 non-native spontaneous spo-
ken responses drawn from a standardized assess-

Table 3: Pearson correlation coefficients (r) of dis-
course features with both the holistic proficiency
scores as well as the discourse coherence scores.
For the 120 double-annotated responses, the aver-
aged feature values were used.

Features Proficiency Coherence
n edu 0.58 0.397
n rel 0.584 0.396
n awk rel -0.396 -0.509
n rhe rel 0.691 0.541
n rhe relTypes 0.64 0.557
perc rhe rel 0.589 0.612
tree depth 0.365 0.25
ratio nedu depth 0.529 0.367

ment of non-native English. The RST annotation
results show that the annotators achieved similar
inter-annotator agreement rates as have been re-
ported in previous studies that investigated well-
formed written text (Marcu et al., 1999). In ad-
dition, we demonstrate the potential of using fea-
tures derived from these RST annotations for as-
sessing non-native spoken English through moder-
ately high correlations with both holistic speaking
proficiency scores and discourse coherence scores;
the highest performing feature when evaluated on
the discourse coherence scores provided by expert
raters was the percentage of rhetorical relations in
the entire spoken response (perc rhe rel), with a
correlation of 0.612.

In the future, we will continue this work by ad-
dressing the following main research questions: a)
how can we develop additional effective features
from the discourse trees; b) how well can an auto-
matic discourse parser trained on the obtained an-
notations perform; c) how well will the proposed
features perform when extracted using an auto-
matic RST parser; d) how well will the features
perform when using an automated speech recog-
nizer (rather than human transcribers) to obtain the
textual transcriptions of a spoken response.

References
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.

Finding the write stuff: Automatic identification of
discourse structure in student essays. IEEE Intelli-
gent Systems 18(1):32–39.

Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging reference manual. Technical Report ISI-TR-
545, ISI Technical Report.

267



Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurows. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In 2nd SIGDIAL Workshop on Discourse and Dia-
logue. Aalborg, Denmark, pages 1–10.

In Demirahin and Deniz Zeyrek. 2014. Annotating
discourse connectives in spoken Turkish. In The
8th Liguistic Annotation Workshop. Dublin, Ireland,
pages 105–109.

ETS. 2012. The official guide to the TOEFL R© test.
Fourth Edition, McGraw-Hill .

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics. Baltimore, Maryland, pages 511–521.

Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst.
2014. The impact of deep hierarchical discourse
structures in the evaluation of text coherence. In
Proceedings of COLING 2014, the 25th Interna-
tional Conference on Computational Linguistics:
Technical Papers. Dublin, Ireland, pages 940–949.

Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics. Sofia, Bulgaria, pages 486–496.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text - Interdisciplinary
Journal for the Study of Discourse (Text) 8(3):243–
281.

Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.

Daniel Marcu, Estibaliz Amorrortu, and Magdalena
Romera. 1999. Experiments in constructing a cor-
pus of discourse trees. In ACL Workshop on Stan-
dards and Tools for Discourse Tagging. College
Park, Maryland, pages 48–57.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, and Livio Robaldo. 2008. The Penn Dis-
course TreeBank 2.0. In The 6th International
Conference on Language Resources and Evaluation
(LREC). Marrakech, Morocco, pages 2961–2968.

Ines Rehbein, Merel Scholman, and Vera Demberg.
2016. Annotating discourse relations in spoken lan-
guage: A comparison of the PDTB and CCR frame-
works. In The Tenth International Conference on
Language Resources and Evaluation (LREC 2016).
Portorož, Slovenia, pages 1039–1046.

Ted J. M. Sanders, Wilbert P. M. Spooren, and Leo
G. M. Noordman. 1992. Toward a taxonomy of co-
herence relations. Discourse Processes 15(1):1–35.

Svetlana Stoyanchev and Srinivas Bangalore. 2015.
Discourse in customer care dialogues. Poster pre-
sented at the Workshop of Identification and Anno-
tation of Discourse Relations in Spoken Language.
Saarbrücken, Germany.

Maite Taboada and William C. Mann. 2006a. Appli-
cations of Rhetorical Structure Theory. Discourse
Studies 8(4):567–588.

Maite Taboada and William C. Mann. 2006b. Rhetor-
ical Structure Theory: Looking back and moving
ahead. Discourse Studies 8(3):423–459.

Sara Tonelli, Giuseppe Riccardi, Rashmi Prasad, and
Aravind Joshi. 2010. Annotation of discourse re-
lations for conversational spoken dialogs. In The
Seventh International Conference on Language Re-
sources and Evaluation (LREC’10). Valetta, Malta,
pages 2084–2090.

Xinhao Wang, Keelan Evanini, and Klaus Zechner.
2013. Coherence modeling for the automated as-
sessment of spontaneous spoken responses. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. At-
lanta, Georgia, pages 814–819.

268


	Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework

