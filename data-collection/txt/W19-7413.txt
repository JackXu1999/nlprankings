

















































A folksonomy-based approach for profiling human perception on word
similarity

GuanI Wu
Department of Statistics,

University of California, Los Angeles
guani@g.ucla.edu

Ker-Chau Li
Department of Statistics,

University of California, Los Angeles
ISS, Academia Sinica

kcli@stat.sinica.edu.tw

Abstract

Automatic assessment of word similarity has
long been considered as one important chal-
lenge in the development of Artificial Intel-
ligence. People often have a big disagree-
ment on how similar a pair of words is. Yet
most word similarity prediction methods, tak-
ing either the knowledge-based approach or
the corpus-based approach, only attempt to es-
timate an average score of human raters. The
distribution aspect of similarity for each word-
pair has been methodologically neglected, thus
limiting their downstream applications in Nat-
ural Language Processing. Here, utilizing
the category information of Wikipedia, we
present a method to model similarity between
two words as a probability distribution. Our
method leverages unique features of folkson-
omy. The success of our method in describ-
ing the diversity of human perception on word
similarity is evaluated against the rater dataset
WordSim-353. Our method can be extended to
compare documents.

1 Introduction

Making machine understand human language is
one of the ultimate goals in the development of
Artificial Intelligence (Christopher D. Manning,
2015). In order to reach the goal, many different
Natural Language Processing (NLP) tasks were
designed. Among them, one of the fundamental
upstream tasks is to automatically assess similari-
ties between words. The performance of this task
has directly impacts on many downstream NLP
applications such as Question Answering, Infor-
mation Retrieval, Topic Modeling, and Text Clus-
tering (Sandhya and Govardhan, 2012; Nathawith-
arana et al., 2016; Wei et al., 2015), etc.

Methods automatically assessing word similar-
ity generally fall into two categories, knowledge-
based and corpus-based approaches (Harispe
et al., 2015). The corpus-based approach was
founded on the maxim “You should know a word

by the companies it keeps (Firt, J. R., 1957), which
has shown remarkable performance on different
word-similarity tests. Landauer et al. proposed
Latent Semantic Analysis (LSA) that employs sin-
gular value decomposition to generate vectors as
word representations (Thomas K Landauer et al.,
1998). Since then, many methods were proposed
to generate word vectors. Bengio et al. pub-
lished a series of papers using neural network tech-
niques (Yoshua Bengio et al., 2003). The team
of Tomas Mikolov proposed the continuous bag
of words (CBOW) and skip grams (also known
as Word2vec) (Tomas Mikolov et al., 2013) and
Jeffrey et al. proposed GloVe (Pennington et al.,
2014). These methods need to be fed with a large
corpus to train models in order to generate word
vectors. To obtain a similarity score between two
words, the dot product of the two word vectors is
computed.

Instead of the dependence on which corpus to
use, the knowledge-based approach requires a pre-
existing knowledge base. WordNet is the most
common knowledge base employed by the major-
ity of methods developed in this realm. Word-
Net collects over 150,000 English words, and or-
ganizes them into cognitive synonyms (synsets).
These synsets are connected through conceptual,
semantic and lexical relations such as hyponyms,
hypernyms, meronyms, holonyms (George A.
Miller, 1995). Wu and Palmer proposed a method
that exploited ontology/taxonomy to compute sim-
ilarity scores based on Least Common Subsumer
(LCS) (Zhibiao Wu and Martha Palmer, 1994).
Many methods based on LCS, known as the edge-
counting-based approach, were proposed (T. Sli-
mani et al., 2006; Yuhua Li et al., 2003; Hadj Taieb
et al., 2014). Another type of knowledge base ap-
proach used features of words to assess the simi-
larities (Amos Tversky, 1977; Andrea Rodriguez
and Max J Egenhofer, 2003; Euripides G.M. Pe-
trakis et al., 2006).



The performance of computed similarity has
to be evaluated against human raters, but hu-
man raters often display considerable disagree-
ment in assigning similarity scores. As an exam-
ple, see Figure 1 for the distribution of 16 raters’
scores assigned to the pair of life and lesson from
WordSim-353 (Finkelstein et al., 2002). Such rat-
ing disagreements are quite common. However,
most word-similarity methodologies attempt to es-
timate only the “average” score of human rating.
The distribution aspect has been methodologically
neglected, thus limiting their downstream applica-
tions in NLP.

Figure 1: The histogram of similarity scores assigned
by 16 raters to the pair of life and lesson.

2 Rater Disagreement on
Word-Similarity

WordSim-353 is composed of two datasets:
WordSim-353.1, a list of 153 word-pairs rated
by 13 persons, and WordSim-353.2, a list of 200
word-pairs rated by 16 persons. We computed the
Pearson correlation coefficient and the weighted
Cohen’s kappa coefficient for the similarity scores
between any two raters. The results are shown
in Figure 2 and Figure 3 after we ordered raters
by hierarchical clustering. Rater disagreement on
word-similarity is evident.

The important message we like to deliver is
two-fold. First, the computer-imputed single sim-
ilarity score has grossly simplified the human be-
havior. Second, using average rater score to eval-
uate the performance of different word-similarity
prediction algorithms is itself a problematic eval-
uation approach.

(a) WordSim-353.1 (b) WordSim-353.2

Figure 2: Weighted Cohen’s kappa coefficient matrices
for WordSim-353.1 and WordSim-353.2.

(a) WordSim-353.1 (b) WordSim-353.2

Figure 3: Pearson correlation matrices for WordSim-
353.1 and WordSim-353.2.

3 Leveraging Folksonomy for
Distribution Quantification of Word
Similarity

To reflect the more realistic human behaviors, we
propose that in lieu of assigning a single simi-
larity score, a better computer task would be to
assign a probability distribution to each word-
pair, (p0, p1, . . . , pd, . . . , pδ), where pd denotes
the probability of similarity score d, and δ is the
highest allowable score. To evaluate the perfor-
mance of a computer algorithm, we should em-
ploy common statistical criteria that are designed
for the distribution against distribution compari-
son.

3.1 Category Information of Wikipedia

Wikipedia organizes the categories of articles via
folksonomy, which is a collaborative tagging sys-
tem allowing users to tag articles with multiple
category notions (Aniket Kittur and Ed H. Chi,
Bongwon Suh, 2009). Links between categories
do not impose any specification on relations such
as is-a, is-part-of, is-an-example-of, etc. Figure
4 illustrated how Wikipedia category is organized
into a Directed Acyclic Graph (DAG). It is typi-
cal to find multiple roots linking to the title of an



article.
In contrast to the traditional centralized

classification, folksonomy may directly re-
flect the diversity of article contributors in
their personal styles of vocabulary manage-
ment, which in turn are influenced by a variety
of factors including cultural, social or per-
sonal bias. At this writing, about 70,000
editors—from expert scholars to casual readers—
regularly edit Wikipedia. (March 2, 2019
https://en.wikipedia.org/wiki/Wikipedia:About)

Figure 4: An example of Wikipedia category structure,
where rectangle indicates a title of an article, and el-
lipses are categories. The graph is drawn based on the
data downloaded from https://wiki.dbpedia.org/data-
set-36.

3.2 Distribution Quantification of
Word-Similarity

We propose a method to assign a probability distri-
bution to a pair of words (W1,W2). First, we find
the set of conceptual paths X = {X1, . . . , XN}
linking to W1, and also find the set of concep-
tual paths Y = {Y1, . . . , YM} linking to W2. We
delete paths in X that are disconnected from any
path in Y , and vice versa. We then compute a sim-
ilarity score cij for each path pair (Xi, Yj) to gen-
erate a matrix as shown in Table 1. The probability
of similarity score d, denoted by pd, is set to be the
proportion of path pairs with cij = d.

We propose Equation 1 to calculate the similar-
ity score for (Xi, Yj).

sim(Ci, Cj) = 1−
(Ki +Kj)

Li + Lj
∝ Li+Lj−Ki−Kj (1)

As illustrated by Figure 5, Li is the number of

HHH
HHHY

X
X1 X2 . . . XN

Y1 c11 c12 . . . c1N
Y2 c21 c22 . . . c2N
... . . . . . .

. . .
...

YM cM1 cM2 . . . cMN

Table 1: Matrix of Similarity Degrees Between Sets of
Conceptual Paths.

nodes on the path from Ci to its root node Ri, and
Lj is the number of nodes on the path from Cj to
its root node Rj . Ki is the number of nodes on the
path fromCi toCk, andKj is the number of nodes
on the path from Cj to Ck.

Ck

Nk

Ni

Nj

Ri

Rj

Ci

Cj

Ki

Li-Ki

Kj

Lj-Kj
Ck

Ci

Cj

Figure 5: Calculating similarity between two concep-
tual paths via node counting.

In our implementation, we set Li and Lj as con-
stants and let Li = Lj = L. There are two rea-
sons. First, nodes that are too far away fromCi,Cj
are often un-informative. Second, due to the large
number of conceptual paths in X and Y , we must
alleviate computational complexity. This leads to

cij = 2L−Ki −Kj (2)

3.3 Implementation
Since there are over one million categories con-
tained in Wikipedia, it would be a challenge to
collect data directly from Wikipedia. Fortunately,
DBpedia has collected and organized Wikipedia
data in a way easier for us to use (Auer et al.,
2007). We downloaded two datasets, article-
categories and skos-categories; the former keeps
the links between articles and categories, and
the latter stores links between categories. Since
the downloaded databases are stored in Triple-
store format, subject-predicate-object, we set up
Apache Jena Fuseki as an in-house SPARQL
server for access by our main program. Figure



Figure 6: The flowchart of main program.

6 illustrated how we implement our method. Af-
ter inputing a pair of target words (W1,W2), the
program will start with stemming the words, and
check if they can be found in article-categories.
If not, the program will search the disambigua-
tion database and return a category closest to
the target word. After stemming, the program
sends the linked categories as the input to Search
Subcategories. This phase recursively searches
superior categories of given categories until the
search reaches the maximum number of depth we
set initially. Once the search is done, the sys-
tem generates a plain file in Jason format for
displaying the output as a taxonomy-like graph
on the website. Through the same procedure,
the program generates the other plain file in the
same format for the other target word. Finally,
we use the distribution quantification method de-
scribed earlier to generate the probability distribu-
tion (p0, p1, . . . , pd, . . . , pδ) for (W1,W2).

We developed a website to implement our
method, http://ws.stat.sinica.edu.tw/wikiCat.
Given a pair of words, it provides a summary
table and two taxonomy-like graphs for the input
words as shown in Figure 7. Every node in

the graph represents a category, and it can be
clicked to show its superior categories hidden
underneath. The column “Proportions” gives
the similarity distribution for the query (Life,
Lesson). Compared to Figure 1, the agreement
with the human raters is quite good. The time for
executing a query varies around 2 seconds to 30
seconds.

4 Experiment

We use WordSim-353 to evaluate the performance
of our method. We set L = 5 in order to be
consistent with the scale used in WordSim-353
(from 0 to 10), so that our program will yield
a probability distribution (p0, p1, ..., p10) for each
word-pair(W1,W2). To see how our probability
distribution agrees with the score distribution of
WordSim-353 raters, Kolmogorov-Smirnov statis-
tic (K-S statistic) between two distributions is
used. We perform the following procedure 1000
times to get a p-value. A p-value smaller than
0.05 indicates significant disagreement between
the two distributions.

1. Simulating 13 (16, respectively) scores from

http://ws.stat.sinica.edu.tw/wikiCat


Figure 7: A screen shot of the developed website.

the distribution (p0, p1, ..., p10) for the word
pair (W1,W2) from WordSim-353.1 (from
WordSim-353.2, respectively).

2. Computing Kolmogorov-Smirnov distance
between (p0, p1, ..., p10) and the distribution
of simulated scores.

After 1000 simulations, the p-value for
(W1,W2) is given by the proportion of times
that the observed K-S statistic exceeds the sim-
ulated K-S distance. As it turns, around 50% of
word-pairs showed agreement between human
rating and our computer rating (Figure 8). Given
that the raters of WordSim-353 were from a
generation before the inception of Wikipedia, we
consider this result supports the potential of our
folksonomy-based approach in reflecting human
judgment diversity. Figure 9 showed some cases
that our folksonomy-based method agreed very
well with human rating.

Figure 8: Histograms of p-values for WordSim-353.1
and WordSim-353.2. 53.59% of word-pairs have p-
values greater than 0.05 in WordSim-353.1 and 48%
in WordSim-353.2.

We further split the word pairs into two groups,
AG (agreement, word pairs with p-value > 0.05)
and DIS (disagreement, word pairs with p-value
< 0.05). We examined the variance of human
rater scores for each word-pair and plot the dis-
tribution for AG group and DIS group separately
for comparison (Figure 10). We found AG group
of word pairs tend to have larger variance than the
DIS group. This indicates our approach may over-
estimate the degree of divergence in human rating,
provided that the small group of raters participat-
ing WordSim-353 did not under-represent the true
diversity of human behavior.

5 Application in Document Similarity
Comparison

Our method can be extended for comparing
documents. As a word can be mapped to
multiple conceptual paths, a document will be
mapped to an even bigger set of conceptual
paths. As an example, we select three docu-
ments (talk.politics.178908, talk.politics.178860
and sci.med.59319) from The 20 Newsgroups
dataset (Lang, 1995). We further employed tf-
idf (term frequency-inverse document frequency)
(Salton and McGill, 1986) to extract the feature
words of documents. Only top 10 words with
highest tf-idf were kept (Table 2). We merge con-
ceptual paths of these words to form a bigger set
of representative conceptual paths for each docu-
ment. Then we applied the same procedure as de-
scribed in 3.2 to yield a probability distribution of
similarity scores between two documents.

In this example, we set L = 4 to yield a proba-
bility distribution (p0, p1, . . . , p8) for comparing
two documents as shown in Table 3. Here PP
is talk.politics.178908 v.s. talk.politics.178860,



Figure 9: Eight cases that our method agreed well with
human rating. The red lines are CDF by human rating
and the blue lines are CDF by our folksonomy-based
method.

talk.politics
178908

talk.politics
178860

sci.med
59319

president oath widex
masks garrett resound
attorney gain aids
federal ingres programmable
gas nixon hearing
reno powers loss
yesterday office ear
departments personal ahead
janet monetary sloping
children indictment reprogramed

Table 2: Lists of top 10 words with highest tf-idf
scores.

PM1 is talk.politics.178908 v.s. sci.med.59319
and PM2 is talk.politics.178860 v.s.
sci.med.59319. Evidently, the probability distri-
butions for (talk.politics.178908, sci.med.59319)

Figure 10: Boxplots for variances of similarity scores
across 13 raters (WorSim-353.1 ) and 16 raters
(WordSim-353.2). Word-pairs are split into two
groups, AG (agreement, p > 0.05) and DIS (disagree-
ment, p < 0.05).

PP PM1 PM2
0 0 0 0
1 0 0 0
2 0.1236742 0.2240363 0.2725498
3 0.1616162 0.3133787 0.3924248
4 0.1674242 0.245805 0.2225693
5 0.1511995 0.2126984 0.1124561
6 0.1440657 0.004081633 0
7 0.1337121 0 0
8 0.1183081 0 0

Table 3: Probability distributions of document
similarity for comparing talk.politics.178908,
talk.politics.178860 and sci.med.59319.

and (talk.politics.178860, sci.med.59319) have
low probabilities on high similarity scores (6, 7,
8). In contrast, we observe relatively higher prob-
abilities being assigned to high similarity scores
for (talk.politics.178908, talk.politics.178860).

6 Conclusion

Human perception on word similarity can be very
discordant. Against the common trend of assign-
ing a single score of similarity by most computer
algorithms, we request a new computer task of
assigning a probability distribution of similarity
for each word pair. Leveraging the rich infor-
mation embroidered behind the principle of free
expression and empowered by user diversity of
folksonomy, we design an approach that exploited
the category tagging system of Wikipedia articles
to perform the task. The good performance of
our method is illustrated against two word sim-
ilarity datasets with scores assigned by human

GuanI Wu




raters. Our way of using Wikipedia (via folkson-
omy) is very different from many others; for ex-
ample, the method of Explicit Semantic Analysis
(Gabrilovich and Markovitch, 2007) treated arti-
cles in Wikipedia as a document corpus and pro-
duced only a single similarity score. For future
works, we plan to modify our word similarity scor-
ing formula by path-dependent weight adjustment
for broadening the application in document com-
parison. It would also be worthwhile to apply our
method to other languages for comparing the pos-
sible differences between languages in assigning
similarity distributions.

Acknowledgments

This work was supported in part by grants from
Academia Sinica, Taiwan, AS-104-TP-A07 and
National Science Foundation, USA, NSF, DMS-
1513622, and by MIB, Institute of Statistical Sci-
ence, Academia Sinica. The content is solely the
responsibility of the authors and does not neces-
sarily represent the official views of NSF.

References

Amos Tversky. 1977. Features of Similarity. Psyco-
logical Review, 84(4):327–352.

Andrea Rodriguez and Max J Egenhofer. 2003. De-
termining Semantic Similarity among Entity Classes
from Different Ontologies. IEEE Transactions on
Knowledge and Data Engineering, 15(2):442–456.

Aniket Kittur and Ed H. Chi, Bongwon Suh. 2009.
What’s in Wikipedia?: mapping topics and conflict
using socially annotated category structure. In The
SIGCHI Conference on Human Factors in Comput-
ing Systems, pages 1509–1512.

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A Nucleus for a Web of Open Data.
In Proceedings of the 6th International The Seman-
tic Web and 2Nd Asian Conference on Asian Seman-
tic Web Conference, ISWC’07/ASWC’07, pages
722–735, Berlin, Heidelberg. Springer-Verlag.

Christopher D. Manning. 2015. Computational Lin-
guistics and Deep Learning. Computational Lin-
guistics, 41(4):701–707.

Euripides G.M. Petrakis, Giannis Varelas, Angelos
Hliaoutakis, and Paraskevi Raftopoulou. 2006. X-
Similarity: Computing Semantic Similarity between
concepts from different ontologies. Journal of Digi-
tal Information Management, 4(4):233–237.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Trans. Inf. Syst., 20(1):116–
131.

Firt, J. R. 1957. A Synopsis of Linguistic Theory 1930-
55. Studies in Linguistic Analysis(special volume of
the Philological Society), pages 1–32.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI’07, pages 1606–1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.

George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41.

Mohamed Ali Hadj Taieb, Mohamed Ben Aouicha, and
Abdelmajid Ben Hamadou. 2014. Ontology-based
Approach for Measuring Semantic Similarity. Eng.
Appl. Artif. Intell., 36(C):238–261.

Sébastien Harispe, Sylvie Ranwez, Stefan Janaqi, and
Jacky Montmain. 2015. Semantic Similarity from
Natural Language and Ontology Analysis. Synthesis
Lectures on Human Language Technologies, 8(1):1–
254.

Ken Lang. 1995. Newsweeder: Learning to filter net-
news. In Proceedings of the Twelfth International
Conference on Machine Learning, pages 331–339.

Nilupulee Nathawitharana, Damminda Alahakoon, and
Daswin De Silva. 2016. Using semantic relatedness
measures with dynamic self-organizing maps for im-
proved text clustering. 2016 International Joint
Conference on Neural Networks (IJCNN), pages
2662–2671.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Gerard Salton and Michael J McGill. 1986. Introduc-
tion to modern information retrieval.

Nadella Sandhya and A. Govardhan. 2012. Analysis
of Similarity Measures with WordNet Based Text
Document Clustering. In Proceedings of the In-
ternational Conference on Information Systems De-
sign and Intelligent Applications 2012 (INDIA 2012)
held in Visakhapatnam, India, January 2012, pages
703–714. Springer Berlin Heidelberg.

T. Slimani, B. Ben Yaghlane, and K. Mellouli. 2006.
A New Similarity Measure based on Edge Count-
ing. In World Academy of Science, Engineering and
Technology, volume 17, pages 232–236.

http://dl.acm.org/citation.cfm?id=1785162.1785216
https://doi.org/10.1145/503104.503110
https://doi.org/10.1145/503104.503110
http://dl.acm.org/citation.cfm?id=1625275.1625535
http://dl.acm.org/citation.cfm?id=1625275.1625535
https://doi.org/10.1016/j.engappai.2014.07.015
https://doi.org/10.1016/j.engappai.2014.07.015
https://doi.org/10.2200/S00639ED1V01Y201504HLT027
https://doi.org/10.2200/S00639ED1V01Y201504HLT027
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162


Thomas K Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse Processes, 25:259–284.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Workshop at Interna-
tional Conference on Learning Representations.

Tingting Wei, Yonghe Lu, Huiyou Chang, Qiang Zhou,
and Xianyu Bao. 2015. A semantic approach for text
clustering using WordNet and lexical chains. Expert
Systems with Applications, 42(4):2264–2275.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Yuhua Li, Zuhair A. Bandar, and David McLean. 2003.
An Approach for Measuring Semantic Similarity be-
tween Words Using Multiple Information Sources.
IEEE Transactions on Knowledge and Data Engi-
neering, 15(4):871–882.

Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In ACL 94 Proceedings of
the 32nd annual meeting on Association for Compu-
tational Linguistics. Association for Computational
Linguistics Stroudsburg.

https://doi.org/10.1016/j.eswa.2014.10.023
https://doi.org/10.1016/j.eswa.2014.10.023

