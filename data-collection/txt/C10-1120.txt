Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1065–1073,

Beijing, August 2010

1065

Semi-supervised dependency parsing using generalized tri-training

Anders Søgaard and Christian Rishøj

Center for Language Technology

University of Copenhagen

{soegaard|crjensen}@hum.ku.dk

Abstract

Martins et al. (2008) presented what to
the best of our knowledge still ranks as
the best overall result on the CONLL-
X Shared Task datasets.
The paper
shows how triads of stacked dependency
parsers described in Martins et al. (2008)
can label unlabeled data for each other in
a way similar to co-training and produce
end parsers that are signiﬁcantly better
than any of the stacked input parsers.
We evaluate our system on ﬁve datasets
from the CONLL-X Shared Task and ob-
tain 10–20% error reductions, incl. the
best reported results on four of them.
We compare our approach to other semi-
supervised learning algorithms.

1 Introduction

Semi-supervised learning of structured variables
is a difﬁcult problem that has received consid-
erable attention recently, but most results have
been negative (Abney, 2008). This paper uses
stacked learning (Wolpert, 1992) to reduce struc-
tured variables, i.e. dependency graphs, to multi-
nomial variables,
i.e. attachment and labeling
decisions, which are easier to manage in semi-
supervised learning scenarios, and which can
later be combined into dependency trees using
parsing algorithms for arc-factored dependency
parsing. Our approach thus combines ensemble-
based methods and semi-supervised learning.

Ensemble-based methods such as stacked
learning are used to reduce the instability of clas-
siﬁers, to average out their errors and to com-
bine the strengths of diverse learning algorithms.

Ensemble-based methods have attracted a lot of
attention in dependency parsing recently (Sagae
and Lavie, 2006; Hall et al., 2007; Nivre and
McDonald, 2008; Martins et al., 2008; Fishel
and Nivre, 2009; Surdeanu and Manning, 2010).
Nivre and McDonald (2008) were ﬁrst to intro-
duce stacking in the context of dependency pars-
ing.

Semi-supervised learning is typically moti-
vated by data sparseness. For many classiﬁ-
cation tasks in natural language processing, la-
beled data can be in short supply but unla-
beled data is more readily available.
Semi-
supervised methods exploit unlabeled data in ad-
dition to labeled data to improve performance
on classiﬁcation tasks.
If the predictions of a
learner l on unlabeled data are used to improve
a learner l′ in semi-supervised learning, the ro-
bustness of learning will depend on the stabil-
ity of l. Combining ensemble-based and semi-
supervised methods may thus lead to more ro-
bust semi-supervised learning.

Ensemble-based and semi-supervised meth-
ods are some of the areas that receive most at-
tention in machine learning today, but relatively
little attention has been given to combining these
methods (Zhou, 2009). Semi-supervised learn-
ing algorithms can be categorized with respect
to the number of views, i.e. the number of fea-
ture sets, and the number of learners used to in-
form each other (Hady and Schwenker, 2008).
Self-training and expectation maximization are
perhaps the best known semi-supervised learn-
ing algorithms (Abney, 2008). They are both
single-view and single-learner algorithms. Since
there is thus only a single perspective on data,

1066

selecting unlabeled data points with predictions
is a difﬁcult task. There is an imminent danger
that the learner ampliﬁes its previous mistakes,
and while several techniques such as balancing
and throttling have been developed to avoid such
caveats, using single-view and single-learner al-
gorithms often requires both caution and experi-
ence with the modeling task at hand.

Algorithms with multiple views on data are
known to be more robust. This insight led to the
development of co-training (Blum and Mitchell,
1998), a two-view method where views inform
each other, but it also paved the way for the inte-
gration of ensemble-based and semi-supervised
methods, i.e. for methods with multiple learners.
It was mentioned that relatively little work has
been devoted to this topic, but there are notable
exceptions:

Bennett et al. (2003) generalized boosting to
semi-supervised learning in a seminal paper,
where the idea of iterative or recursive ensembles
was also introduced. Li and Zhou (2005) intro-
duce tri-training, a form of co-training that trains
an ensemble of three learners on labeled data and
runs them on unlabeled data.
If two learners
agree on their labeling of a data point, the data
point is added to the labeled data of the third
learner with the prediction of the ﬁrst two. Di-
daci and Roli (2006) extend self-training and co-
training to multiple learners. Li and Zhou (2007)
generalize tri-training to larger ensembles of ran-
dom trees. The technique is also known as co-
forests. Hady and Schwenker (2008) general-
ize existing ensemble-based methods for semi-
supervised learning scenarios; in particular they
embed ensembles in a form of co-training that is
shown to maintain the diversity of the ensemble
over time. Milidiu and Duarte (2009) generalize
boosting at start to semi-supervised learning.

This paper applies a generalization of tri-
training to two classiﬁcation problems, attach-
ment and labeling. The attachment classiﬁer’s
weights are used for arc-factored dependency
parsing, and the labeling classiﬁer’s weights are
then used to label the dependency tree delivered
by the parser.

Semi-supervised dependency parsing has at-

tracted a lot of attention recently (Koo et al.,
2008; Wang et al., 2008; Suzuki et al., 2009),
but there has, to the best of our knowledge, been
no previous attempts to apply tri-training or re-
lated combinations of ensemble-based and semi-
supervised methods to any of these tasks, ex-
cept for the work of Sagae and Tsujii (2007)
discussed in Sect. 2.6. However,
tri-training
has been applied to Chinese chunking (Chen et
al., 2006), question classiﬁcation (Nguyen et al.,
2008) and POS tagging (Søgaard, 2010).

We compare generalized tri-training to other
semi-supervised learning algorithms, incl. self-
training, the original tri-training algorithm based
on bootstrap samples (Li and Zhou, 2005),
co-forests (Li and Zhou, 2007) and semi-
supervised support vector machines (Sindhwani
and Keerthi, 2006).

Sect. 2 introduces dependency parsing and
stacked learning. Stacked learning is general-
ized to dependency parsing, and previous work is
brieﬂy surveyed. We then describe how stacked
dependency parsers can be further stacked as in-
put for two end classiﬁers that can be combined
to produce dependency structures. These two
classiﬁers will learn multinomial variables (at-
tachment and labeling) from a combination of
labeled data and unlabeled data using a gener-
alization of tri-training. Sect. 3 describes our ex-
periments. We describe the data sets, and how
the unlabeled data was prepared. Sect. 4 presents
our results. Sect. 5 presents an error analysis and
discusses the results in light of other results in
the literature, and Sect. 6 concludes the paper.

2 Background and related work

2.1 Dependency parsing
Dependency parsing models a sentence as a tree
where words are vertices and grammatical func-
tions are directed edges (dependencies). Each
word thus has a single incoming edge, except
one called the root of the tree. Dependency pars-
ing is thus a structured prediction problem with
trees as structured variables. Each sentence has
exponentially many possible dependency trees.
Our observed variables are sentences with words
labeled with part-of-speech tags. The task for

1067

each sentence is to ﬁnd the dependency tree that
maximizes an objective function which in our
case is learned from a combination of labeled
and unlabeled data.

More formally, a dependency tree for a
sentence x = w1, . . . , wn is a tree T =
h{0, 1, . . . , n}, Ai with A ⊆ V × V the set of
dependency arcs. Each vertex corresponds to
a word in the sentence, except 0 which is the
root vertex, i.e. for any i ≤ n hi, 0i 6∈ A.
Since a dependency tree is a tree it is acyclic.
A tree is projective if every vertex has a continu-
ous projection, i.e. if and only if for every arc
hi, ji ∈ A and node k ∈ V , if i < k < j
or j < k < i then there is a subset of arcs
{hi, i1i,hi1, i2i, . . . ,hik−1, iki} ∈ A such that
ik = k.
In this paper we use a maximum spanning tree
algorithm, the so-called Chu-Liu-Edmonds algo-
rithm (CLE) (Edmonds, 1967) to turn the pre-
dictions of our semi-supervised classiﬁers into a
dependency tree.

2.2 Stacked learning
Stacked generalization, or simply stacking, was
ﬁrst proposed by Wolpert (1992). Stacking is an
ensemble-based learning method where multiple
weak classiﬁers are combined in a strong end
classiﬁer. The idea is to train the end classiﬁer
directly on the predictions of the input classiﬁers.
Say each input classiﬁer ci with 1 ≤ i ≤
n receives an input x and outputs a prediction
ci(x). The end classiﬁer then takes as input
hx, c1(x), . . . , cn(x)i and outputs a ﬁnal predic-
tion c0(hx, c1(x), . . . , cn(x)i). Training is done
by cross-validation. In sum, stacking is training
a classiﬁer on the output of classiﬁers.

2.3 Stacked dependency parsing
Stacked learning can be generalized to structured
prediction tasks such as dependency parsing. Ar-
chitectures for stacking dependency parsers typi-
cally only use one input parser, but otherwise the
intuition is the same: the input parser is used to
augment the dependency structures that the end
parser is trained and evaluated on.

Nivre and McDonald (2008) ﬁrst showed how
the MSTParser (McDonald et al., 2005) and the

MaltParser (Nivre et al., 2007) could be im-
proved by stacking each parser on the predic-
tions of the other. Martins et al. (2008) general-
ized their work, considering more combinations
of parsers, and stacking the end parsers on non-
local features from the predictions of the input
parser, e.g. siblings and grand-parents.
In this
work we use three stacked dependency parsers
for each language: mst2 (p1), malt/mst2 (p2) and
malt/mst1 (p3).

The notation ”malt/mst2” means that

the
second-order MSTParser has been stacked on the
MaltParser. The capital letters refer to feature
conﬁgurations. Conﬁguration D stacks a level 1
parser on several (non-local) features of the pre-
dictions of the level 0 parser (along with the in-
put features): the predicted edge, siblings, grand
parents and predicted head of candidate modiﬁer
if predicted edge is 0. Conﬁguration E stacks
a level 1 parser on the features in conﬁguration
D and all the predicted children of the candi-
date head. The chosen parser conﬁgurations are
those that performed best in Martins et al. (2008)
across the different datasets.

2.4 Stacking stacked dependency parsing
The input features of the input classiﬁers in
stacked learning x can of course be removed
from the input of the end classiﬁer.
It is also
possible to stack stacked classiﬁers. This leaves
us with four strategies for recursive stacking;
namely to constantly augment the feature set,
with level n classiﬁers trained on the predictions
of the classiﬁers at all n − 1 lower levels with or
without the input features x, or simply to train a
level n classiﬁer on the predictions of the level
n − 1 classiﬁers with or without x.
In this work we stack stacked dependency
parsers by training classiﬁers on the output of
three stacked dependency parsers and POS tags.
Consequently, we use one of the features from x.
Note that we train classiﬁers and not parsers on
this new level 2.

The reduction is done the following way: First
we train a classiﬁer on the relative distance from
a word to its head to induce attachments. For
example, we may obtain the following features
from the predictions of our level 1 parsers:

1068

label

1
0

p1
1
0

p2
-1
0

p3
1
0

POS
NNP
VBD

In the second row all input parsers, p1−3 in
columnsaa 2–4, agree that the verb is the root of
the sentence. Column 1 tells us that this is cor-
rect.
In the ﬁrst row, two out of three parsers
agree on attaching the noun to the verb, which
again is correct. We train level 2 classiﬁers on
feature vectors produced this way. Note that or-
acle performance of the ensemble is no upper
bound on the accuracy of a classiﬁer trained on
level 1 predictions this way, since a classiﬁer
may learn the right decision from three wrong
predictions and a POS tag.

Second we train a classiﬁer to predict depen-
dency relations. Our feature vectors are similar
to the ones just described, but now contain de-
pendency label predictions, e.g.:

POS
label
SBJ
NN
ROOT ROOT ROOT COORD VBN

p1
SBJ

p2
SBJ

p3
SBJ

2.5 Generalized tri-training
Tri-training was originally introduced in Li and
Zhou (2005). The method involves three learners
that inform each other.

Let L denote the labeled data and U the
unlabeled data. Assume that three classiﬁers
c1, c2, c3 have been trained on L. In the origi-
nal algorithm, the three classiﬁers are obtained
by applying the same learning algorithm to three
bootstrap samples of the labeled data; but in gen-
eralized algorithms, three different learning al-
gorithms are used. An unlabeled datapoint in
U is labeled for a classiﬁer, say c1, if the other
two classiﬁers agree on its label, i.e. c2 and c3.
Two classiﬁers inform the third. If the two clas-
siﬁers agree on a labeling, we assume there is a
good chance that they are right. In the original
algorithm, learning stops when the classiﬁers no
longer change; in generalized tri-training, a ﬁxed
stopping criterion is used. The three classiﬁers
are combined by voting. Li and Zhou (2005)
show that under certain conditions the increase
in classiﬁcation noise rate is compensated by the
amount of newly labeled data points.

The most

important condition is that

the
If the three clas-

three classiﬁers are diverse.

ci ← train classiﬁer (li, L)

for i ∈ {1..3} do
for x ∈ U do

1: for i ∈ {1..3} do
2:
3: end for
4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: until stopping criterion is met
15: apply c1

Li ← ∅
if cj(x) = ck(x)(j, k 6= i) then
end if
end for
ci ← train classiﬁer (li, L ∪ Li)

Li ← Li ∪ {(x, cj (x)}

end for

Figure 1: Generalized tri-training.

siﬁers are identical, tri-training degenerates to
self-training. As already mentioned, Li and
Zhou (2005) obtain this diversity by training
classiﬁers on bootstrap samples. In their exper-
iments, they consider classiﬁers based on deci-
sion trees, BP neural networks and na¨ıve Bayes
inference.

In this paper we generalize the tri-training al-
gorithm and use three different learning algo-
rithms rather than bootstrap samples to create
diversity: a na¨ıve Bayes algorithm (no smooth-
ing), random forests (Breiman, 2001) (with 100
unpruned decision trees) and an algorithm that
induces unpruned decision trees. The overall al-
gorithm is sketched in Figure 1 with li a learning
algorithm.

Our weights are those of the random forest
classiﬁer after a ﬁxed number of rounds. The
attachment classiﬁer iterates once over the unla-
beled data, while the dependency relations clas-
siﬁer uses three iterations. The optimal number
of iterations could of course be estimated on de-
velopment data instead. Given the weights for an
input sentence we use CLE to ﬁnd its most likely
dependency tree.

2.6 Related work

This paper uses stacking rather than voting to
construct ensembles, but voting has been more

1069

widely used in dependency parsing than stack-
ing. Voting was ﬁrst introduced in dependency
parsing in Zeman and Zabokrtsky (2005). Sagae
and Lavie (2006) later used weighted voting and
reparsing, i.e. using CLE to ﬁnd the dependency
tree that reﬂects the maximum number of votes.
They also showed that binning the vote over
part-of-speech tags led to further improvements.
This set-up was adopted by Hall et al. (2007) in
the best performing system in the CONLL 2007
Shared Task. Fishel and Nivre (2009) later ex-
perimented with binning the vote on other fea-
tures with modest improvements.

Semi-supervised dependency parsing has only
recently been explored, and failures have been
more frequent
There are,
however, noteable exceptions such as Koo et
al.
(2008), Suzuki et
al. (2009) and Sagae and Gordon (2009).

(2008), Wang et al.

than successes.

The semi-supervised methods employed in
these experiments are very different from more
traditional scenarios such as self-training and co-
training. Two approaches (Koo et al., 2008;
Sagae and Gordon, 2009) use clusters obtained
from large amounts of unlabeled data to augment
their labeled data by introducing new features,
and two approaches (Wang et al., 2008; Suzuki et
al., 2009) combine probability distributions ob-
tained from labeled data with probability distri-
butions obtained from unlabeled data.

Successes with self-training and co-training
are rare, and several authors report negative re-
sults, e.g. Spreyer and Kuhn (2009). A note-
able exception in constituent-based parsing is the
work of McClosky et al. (2006) who show that
self-training is possible if a reranker is used to
inform the underlying parser.

They ﬁrst

Sagae and Tsujii (2007) participated in (and
won) the CONLL 2007 Shared Task on do-
main adaptation.
trained a max-
imum entropy-based transition-based depen-
dency parser on the out-of-domain labeled data
and an SVM-based transition-based dependency
parser on the reversed out-of-domain labeled
data. The two parsers parse the in-domain la-
beled data (reversed, in the case of the SVM-
based parser). Identical analyses are added to the

original training set. The ﬁrst parser is retrained
and used to parse the test data. In sum, the au-
thors do one round of co-training with the fol-
lowing selection criterion: If the two parsers pro-
duce the same dependency structures for a sen-
tence, the dependency structure is added to the
labeled data. This criterion is also the selection
criterion in tri-training.

3 Experiments

3.1 Data
We use ﬁve datasets from the CONLL-X Shared
Task (Buchholz and Marsi, 2006).1 Lemmas and
morphological features (FEATS) are ignored,
since we only add POS and CPOS tags to un-
labeled data.
For German and Swedish, we
use 100,000 sentences from the Leipzig Corpora
Collection (Biemann et al., 2007) as unlabeled
data. For Danish, Dutch, and Portuguese we
use 100,000 sentences from the Europarl cor-
pus (Koehn, 2005). The data characteristics are
provided in Figure 2. The unlabeled data were
POS tagged using the freely available SVMTool
(Gimenez and Marquez, 2004) (model 4, left-
right-left).

3.2 Algorithm
Once our data has been prepared, we train the
stacked dependency parsers and use them to la-
bel training data for our classiﬁers (∼4,000 to-
kens), our test data and our unlabeled data. This
gives us three sets of predictions for each of the
three data sets. Using the features described in
Sect. 2.4 we then construct data for training our
two triads of classiﬁers (for attachment and de-
pendency relations). The entire architecture can
be depicted as in Figure 3.

We ﬁrst stack three dependency parsers as
described in Martins et al. (2008). We then
stack three classiﬁers on top of these dependency
parsers (and POS tags): a na¨ıve Bayes classiﬁer,
a random forest, and a decision tree. Finally,

1The CONLL-X Shared Task consists of 12 datasets,
but we did not have consistently tokenized unlabeled data
for Arabic, Chinese, Japanese, Slovene and Turkish. Mar-
tins et al. (2008) ignore Czech. Our experiment with the
Spanish dataset crashed unexpectedly. We will post results
on the website as soon as possible.

1070

Danish

Dutch

German

Portuguese

Swedish

train
unl (Europarl)
test
train
unl (Europarl)
test
train
unl (LCC)
test
train
unl (Europarl)
test
train
unl (LCC)
test

tokens
94,386
2,422,144
5,852
195,069
2,336,176
5,585
699,610
1,763,281
5,694
206,678
2,882,967
5,867
191,467
1,727,068
5,656

sents
5,190
100,000
322
13,349
100,000
386
39,216
100,000
357
9,071
100,000
288
11,042
100,000
389

tokens/sents
18.2
24.2
18.2
14.6
23.4
14.5
17.8
17.6
15.9
22.3
28.8
22.8
17.4
17.3
14.5

POSs DEPRELs
52
-
-
26
-
-
46
-
-
55
-
-
56
-
-

24
-
-
13
-
-
52
-
-
21
-
-
37
-
-

Figure 2: Characteristics of the data sets.

tri-training

stacking

stacking

...

nb

forests

tree

mst2/mst2 malt/mst2 malt/mst1

mst2

malt

mst1

Figure 3: Tri-training stacked classiﬁers.

we tri-train these three stacked classiﬁers and for
each test sentence output the weights provided
by the random forest classiﬁer. These weights
are used to ﬁnd the best possible dependency tree
using CLE.

3.3 Baselines
The best of the stacked input parsers is of course
our natural baseline.

The original

Since we have generalized tri-training, we
also compare generalized tri-training to the orig-
inal tri-training algorithm based on bootstrap
samples.
tri-training algorithm
is run with the same decomposition and the
same features as our generalized tri-training al-
gorithm. We use the learning algorithm orig-
inally used in Li and Zhou (2005), namely
C4.5. We also compare our results to self-
training (no pool, no growth rate) and co-forests
(Li and Zhou, 2007). Finally, we compare our

results to semi-supervised support vector ma-
chines (S3VMs) (Sindhwani and Keerthi, 2006).
Since S3VMs produce binary classiﬁers, and
one-vs.-many combination would be very time-
consuming, we train a binary classiﬁer that pro-
duces a probability that any candidate arc is cor-
rect and do greedy head selection. We optimized
the feature set and included a total of seven fea-
tures (head POS, dependent POS, dependent left
neighbor POS, distance+direction, predictions of
the three classiﬁers).

4 Results

Our results are presented in Figure 4. Labeled
(LAS) and unlabeled attachment scores (UAS)
and labeling accuracy (LA) are deﬁned as usual
and include punctuation signs unless otherwise
noted. Difference (∆) in LAS, error reduction
and p-value compare our results to the best input
stacked parser (malt/mst2, excerpt for Swedish).
Generalized tri-training (tri-training-CLE),
i.e. using CLE to ﬁnd the best well-formed de-
pendency trees given the weights provided by
our tri-trained random forest classiﬁer, leads to
highly signiﬁcant improvements on all data sets
(p < 0.001) with an average error reduction of
14,9%. The results for the other semi-supervised
learning algorithms are presented in Figure 5.
We only used 10% of the unlabeled data (10k
sentences) in this experiment and only did un-
labeled parsing, but it is quite evident that these
learning strategies seem less promising than gen-

1071

Danish
mst2
malt/mst2
malt/mst1
tri-training-CLE
tri-training-CLE (excl. pnc.)
CONLL-X best (excl. pnc.)
Martins et al. (excl. pnc.)
Dutch
mst2
malt/mst2
malt/mst1
tri-training-CLE
tri-training-CLE (excl. pnc.)
CONLL-X best (excl. pnc.)
Martins et al. (excl. pnc.)
German
mst2
malt/mst2
malt/mst1
tri-training-CLE
tri-training-CLE (excl. pnc.)
CONLL-X best (excl. pnc.)
Martins et al. (excl. pnc.)
Portuguese
mst2
malt/mst2
malt/mst1
tri-training-CLE
tri-training-CLE (excl. pnc.)
CONLL-X best (excl. pnc.)
Martins et al. (excl. pnc.)
Swedish
mst2
malt/mst2
malt/mst1
tri-training-CLE
tri-training-CLE (excl. pnc.)
CONLL-X best (excl. pnc.)
Martins et al. (excl. pnc.)
AV

LAS(%) UAS(%)

84.64
86.36
86.11
87.76
87.54
84.79
86.79

80.27
81.00
80.72
83.42
81.73
79.19
81.61

87.32
88.06
88.04
90.41
90.30
87.34
88.66

84.83
85.39
85.00
88.03
89.18
87.60
88.46

81.82
84.42
84.74
86.83
86.66
84.58
85.16

89.11
90.50
90.23
92.11
92.61
90.58

-

84.32
84.58
84.17
88.18
86.97
83.57

-

89.88
90.53
90.50
93.22
93.49
90.38

-

88.44
88.80
88.39
91.89
93.69
91.36

-

87.36
89.57
89.83
92.04
92.45
89.50

-

LA(%)
91.35
92.09
91.87
92.87
91.68
89.22

-

84.96
85.46
85.34
87.82
86.61
83.89

-

93.05
93.52
93.48
94.61
93.87
92.11

-

92.04
92.59
92.23
93.54
92.43
91.54

-

87.29
88.68
89.07
90.65
89.58
87.39

-

EM(%) ∆ LAS
24.84
27.64
25.78
27.95

1.40

err.red(%)

p-value

10.26

<0.0001

23.32
24.35
26.17
28.00

35.85
40.06
38.10
43.14

25.69
28.13
25.69
29.86

27.76
31.62
31.11
32.65

2.42

12.74

<0.0001

2.35

19.68

<0.0001

2.64

18.07

<0.0001

2.09

13.70

<0.0001

2.18

14.89

Figure 4: Results on CONLL-X datasets. Scores are including punctuation unless otherwise noted.
∆ and p-value is difference with respect to best input parser.

UAS
Danish
Dutch
German
Portuguese
Swedish
AV

malt-mst2

90.50
84.58
90.53
88.80
89.83
88.80

S3VMs
90.47
85.34
90.15
65.64
81.46
82.61

self-training

orig-tri-training

co-forests

89.68
84.06
89.83
87.60
89.09
88.05

89.66
83.83
89.92
87.62
89.20
88.05

88.79
83.97
88.47
87.06
88.65
87.44

tri-training

90.60
86.07
90.81
89.16
90.22
89.37

tri-training[full]

92.21
88.06
93.20
91.87
92.24
91.52

Figure 5: Comparison of different semi-supervised learning algorithms (10% of unlabeled data)
using 2-fold CV and no reparsing, UAS including punctuation.

1072

eralized tri-training.

5 Error analysis and discussion

2

Error reductions are higher with dependencies
to the root node and long distance dependencies
than with local dependencies. The table below
lists the labeled attachment F1-scores for the ﬁve
datasets binned on dependency length. The av-
erage error reduction is the same for root depen-
dencies and long distance dependencies (length
>7), but signiﬁcantly lower for local dependen-
cies. This seems to indicate that large amounts of
data are necessary for the parser to recover long
distance dependencies.
1

root
98.45
41.34
83.65
28.39
97.33
26.65
96.23
22.47
96.37
32.85
30.34

4–7
88.17
15.75
82.40
17.00
92.42
25.25
84.80
22.56
88.42
25.97
21.31

>7
90.93
21.92
81.54
31.88
93.94
38.97
87.11
26.97
89.57
31.50
30.25

92.09
13.92
88.60
20.72
94.28
17.46
95.17
24.86
93.46
15.04
18.40

96.21
10.69
94.47
16.74
96.47
19.77
97.05
19.56
95.67
14.10
16.17

Da(F1)
– err.red
Du(F1)
– err.red
Ge(F1)
– err.red
Po(F1)
– err.red
Sw(F1)
– err.red
AV err.red
Our results for Danish, Dutch, German and
Portuguese are to the best of our knowledge the
best reported results in the literature. Zhang and
Chan (2009) obtain a LAS of 87.20 for Swedish
with transition-based parsing based on reinforce-
ment learning. They evaluate their system on
a subset of the CONLL-X datasets and obtain
their (by far) best improvement on the Swedish
dataset. They speculate that ”the reason might
be that [long distance dependencies] are not pop-
ular in Swedish”. Since our parser is particu-
larly good at long distance dependencies, this
may also explain why a supervised parser outper-
forms our system on this dataset. Interestingly,
our unlabeled attachment score is a lot better
than the one reported by Zhang and Chan (2009),
namely 92.45 compared to 91.84.

Generally, our UASs are better than our LASs.
Since we separate attachment and labeling out
in two independent steps, improvements in UAS
and improvements in LA do not necessarily lead
to improvements in LAS. While our average er-
ror reduction in LAS is 14.9%, our average error
reductions in UAS is 23.6%. The average error

reduction in LA is 14.0%. In two-stage depen-
dency parsers or dependency parsers with joint
models, improvements in UAS are typically fol-
lowed by comparable improvements in LAS.

6 Conclusion

This paper showed how the stacked depen-
dency parsers introduced in Martins et al. (2008)
can be improved by inference from unlabeled
data. Brieﬂy put, we stack three diverse clas-
siﬁers on triads of stacked dependency parsers
and let
them label unlabeled data for each
other in a co-training-like architecture. Our
average error reductions in LAS over the best
of our stacked input parsers is 14.9%;
in
UAS, it is 23.6%. The code is available at
http://cst.dk/anders/tridep.html.

References

Abney, Steven. 2008. Semi-supervised learning for

computational linguistics. Chapman & Hall.

Bennett, Kristin, Ayhan Demiriz, and Richard
Maclin. 2003. Exploiting unlabeled data in en-
semble methods. In KDD.

Biemann, Chris, G. Heyer, U. Quasthoff, and
M. Richter. 2007. The Leipzig corpora collection.
In Corpus Linguistics.

Blum, Avrim and Tom Mitchell. 1998. Combining
labeled and unlabeled with-co-training. In COLT.

Breiman, Leo. 2001. Random forests. Machine

Learning, 45:5–32.

Buchholz, Sabine and Erwin Marsi. 2006. CONLL-
X shared task on multilingual dependency parsing.
In CONLL.

Chen, Wenliang, Yujie Zhang, and Hitoshi Isahara.
2006. Chinese chunking with tri-training learning.
In Computer processing of oriental languages,
pages 466–473. Springer, Berlin, Germany.

Didaci, Luca and Fabio Roli.

2006. Using co-
training and self-training in semi-supervised mul-
tiple classiﬁer systems.
In SSPR& SPR, pages
522–530. Springer, Berlin, Germany.

Edmonds, J. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71:233–240.

1073

Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser.
Natural Language Engineering, 13(2):95–135.

Sagae, Kenji and Andrew Gordon. 2009. Cluster-
ing words by syntactic similarity improves depen-
dency parsing of predicate-argument structures. In
IWPT.

Sagae, Kenji and Alon Lavie. 2006. Parser combina-

tion by reparsing. In HLT-NAACL.

Sagae, Kenji and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In EMNLP-CONLL.

Sindhwani, Vikas and Sathiya Keerthi. 2006. Large
In ACM SI-

scale semi-supervised linear SVMs.
GIR.

Søgaard, Anders. 2010. Simple semi-supervised

training of part-of-speech taggers. In ACL.

Spreyer, Kathrin and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using in-
complete and noisy training data. In CONLL.

Surdeanu, Mihai and Christopher Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In NAACL.

Suzuki, Jun, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. Semi-supervised convex
training for dependency parsing. In EMNLP.

Wang, Qin, Dekang Lin, and Dale Schuurmans.
2008. Semi-supervised convex training for depen-
dency parsing. In ACL.

Wolpert, David. 1992. Stacked generalization. Neu-

ral Networks, 5:241–259.

Zeman, Daniel and Zdenˇek ˇZabokrtsk´y. 2005. Im-
proving parsing accuracy by combining diverse
dependency parsers. In IWPT.

Zhang, Lidan and Kwok Chan. 2009. Dependency
parsing with energy-based reinforcement learning.
In IWPT.

Zhou, Zhi-Hua. 2009. When semi-supervised learn-

ing meets ensemble learning. In MCS.

Fishel, Mark and Joakim Nivre. 2009. Voting and
In

stacking in data-driven dependency parsing.
NODALIDA.

Gimenez, Jesus and Lluis Marquez. 2004. SVM-
Tool: a general POS tagger generator based on
support vector machines. In LREC.

Hady, Mohamed and Friedhelm Schwenker. 2008.
Co-training by committee. International Journal
of Software and Informatics, 2:95–124.

Hall, Johan, Jens Nilsson, Joakim Nivre, Gulsen
Eryigit, Beata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended? In
CONLL.

Koehn, Philipp. 2005. Europarl: a parallel corpus for

statistical machine translation. In MT-Summit.

Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In ACL.

Li, Ming and Zhi-Hua Zhou. 2005. Tri-training:
exploiting unlabeled data using three classiﬁers.
IEEE Transactions on Knowledge and Data En-
gineering, 17(11):1529–1541.

Li, Ming and Zhi-Hua Zhou.

Improve
computer-aided diagnosis with machine learning
techniques using undiagnosed samples.
IEEE
Transactions on Systems, Man and Cybernetics,
37(6):1088–1098.

2007.

Martins, Andr´e, Dipanjan Das, Noah Smith, and Eric
In

Xing. 2008. Stacking dependency parsers.
EMNLP.

McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.

McDonald, Ryan, Fernando Pereira, Kiril Ribarov,
and Jan Hajiˇc. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT-
EMNLP.

Milidiu, Ruy and Julio Duarte.

Improv-
ing BAS committee performance with a semi-
supervised approach. In European Symposium on
Artiﬁcial Neural Networks.

2009.

Nguyen, Tri, Le Nguyen, and Akira Shimazu. 2008.
Using semi-supervised learning for question clas-
siﬁcation. Journal of Natural Language Process-
ing, 15:3–21.

Nivre, Joakim and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In ACL-HLT.

