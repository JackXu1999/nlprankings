



















































A Context-Aware Topic Model for Statistical Machine Translation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 229–238,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

A Context-Aware Topic Model for Statistical Machine Translation

Jinsong Su1, Deyi Xiong2∗, Yang Liu3, Xianpei Han4, Hongyu Lin1,
Junfeng Yao1, Min Zhang2

Xiamen University, Xiamen, China1

Soochow University, Suzhou, China2

Tsinghua University, Beijing, China3

Institute of Software, Chinese Academy of Sciences, Beijing, China4

{jssu, hylin, yao0010}@xmu.edu.cn
{dyxiong, minzhang}@suda.edu.cn
liuyang2011@tsinghua.edu.cn
xianpei@nfs.iscas.ac.cn

Abstract

Lexical selection is crucial for statistical ma-
chine translation. Previous studies separately
exploit sentence-level contexts and document-
level topics for lexical selection, neglecting
their correlations. In this paper, we propose
a context-aware topic model for lexical selec-
tion, which not only models local contexts and
global topics but also captures their correla-
tions. The model uses target-side translations
as hidden variables to connect document top-
ics and source-side local contextual words. In
order to learn hidden variables and distribu-
tions from data, we introduce a Gibbs sam-
pling algorithm for statistical estimation and
inference. A new translation probability based
on distributions learned by the model is inte-
grated into a translation system for lexical se-
lection. Experiment results on NIST Chinese-
English test sets demonstrate that 1) our model
significantly outperforms previous lexical se-
lection methods and 2) modeling correlations
between local words and global topics can fur-
ther improve translation quality.

1 Introduction

Lexical selection is a very important task in statis-
tical machine translation (SMT). Given a sentence
in the source language, lexical selection statistically
predicts translations for source words, based on vari-
ous translation knowledge. Most conventional SMT
systems (Koehn et al., 2003; Galley et al., 2006;
Chiang, 2007) exploit very limited context informa-
tion contained in bilingual rules for lexical selection.

∗Corresponding author.

{stance,  attitude ...}lìchǎng

duì    gāi    wèntí    zhōngguó  bǎochí    zhōnglì    lìchǎng

[Economy topic,  Politics topic ...]

{problem,  issue ...}wèntí 

Figure 1: A Chinese-English translation example to il-
lustrate the effect of local contexts and global topics as
well as their correlations on lexical selection. Each black
line indicates a set of translation candidates for a Chinese
content word (within a dotted box). Green lines point to
translations that are favored by local contexts while blue
lines show bidirectional associations between global top-
ics and their consistent target-side translations.

Previous studies that explore richer information for
lexical selection can be divided into two categories:
1) incorporating sentence-level contexts (Chan et al.,
2007; Carpuat and Wu, 2007; Hasan et al., 2008;
Mauser et al., 2009; He et al., 2008; Shen et al.,
2009) or 2) integrating document-level topics (Xi-
ao et al., 2011; Ture et al., 2012; Xiao et al., 2012;
Eidelman et al., 2012; Hewavitharana et al., 2013;
Xiong et al., 2013; Hasler et al., 2014a; Hasler et al.,
2014b) into SMT. The methods in these two strands
have shown their effectiveness on lexical selection.

However, correlations between sentence- and
document-level contexts have never been explored
before. It is clear that local contexts and global top-
ics are often highly correlated. Consider a Chinese-
English translation example presented in Figure 1.
On the one hand, if local contexts suggest that the
source word “á|/lı̀chǎng” should be translated in-

229



to “stance”, they will also indicate that the topic
of the document where the example sentence oc-
curs is about politics. The politics topic can be fur-
ther used to enable the decoder to select a correc-
t translation “issue” for another source word “¯
K/wèntǐ”, which is consistent with this topic. On
the other hand, if we know that this document main-
ly focuses on the politics topic, the candiate trans-
lation “stance” will be more compatible with the
context of “á|/lı̀chǎng” than the candiate transla-
tion “attitude”. This is because neighboring source-
side words “¥I/zhōnguó” and “¥á/zhōnglı̀” of-
ten occur in documents that are about international
politics. We believe that such correlations between
local contextual words and global topics can be used
to further improve lexical selection.

In this paper, we propose a unified framework to
jointly model local contexts, global topics as well as
their correlations for lexical selection. Specifically,

• First, we present a context-aware topic mod-
el (CATM) to exploit the features mentioned
above for lexical selection in SMT. To the best
of our knowledge, this is the first work to joint-
ly model both local and global contexts for lex-
ical selection in a topic model.

• Second, we present a Gibbs sampling algorith-
m to learn various distributions that are related
to topics and translations from data. The trans-
lation probabilities derived from our model are
integrated into SMT to allow collective lexical
selection with both local and global informtion.

We validate the effectiveness of our model on a
state-of-the-art phrase-based translation system. Ex-
periment results on the NIST Chinese-English trans-
lation task show that our model significantly outper-
forms previous lexical selection methods.

2 Context-Aware Topic Model

In this section, we describe basic assumptions and
elaborate the proposed context-aware topic model.

2.1 Basic Assumptions

In CATM, we assume that each source document d
consists of two types of words: topical words which
are related to topics of the document and contextual

words which affect translation selections of topical
words.

As topics of a document are usually represented
by content words in it, we choose source-side nouns,
verbs, adjectives and adverbs as topical words. For
contextual words, we use all words in a source sen-
tence as contextual words. We assume that they are
generated by target-side translations of other words
than themselves. Note that a source word may be
both topical and contextual. For each topical word,
we identify its candidate translations from training
corpus according to word alignments between the
source and target language. We allow a target trans-
lation to be a phrase of length no more than 3 words.
We refer to these translations of source topical word-
s as target-side topical items, which can be either
words or phrases. In the example shown in Figure
1, all source words within dotted boxes are topical
words. Topical word “á|/lı̀chǎng” is supposed to
be translated into a target-side topical item “stance”,
which is collectively suggested by neighboring con-
textual words “ ¥I/zhōngguó”, “¥á/zhōnglı̀”
and the topic of the corresponding document.

In our model, all target-side topical items in a doc-
ument are generated according to the following two
assumptions:

• Topic consistency assumption: All target-side
topical items in a document should be consis-
tent with the topic distribution of the document.
For example, the translations “issue”, “stance”
tend to occur in documents about politics topic.

• Context compatibility assumption: For a top-
ical word, its translation (i.e., the counter-
part target-side topical item) should be com-
patible with its neighboring contextual word-
s. For instance, the translation “stance” of
“á|/lı̀chǎng” is closely related to contextu-
al words “¥I/zhōnguó” and “¥á/zhōnglı̀”.

2.2 Model
The graphical representation of CATM, which visu-
alizes the generative process of training data D, is
shown in Figure 2. Notations of CATM are present-
ed in Table 1. In CATM, each document d can be
generated in the following three steps1:

1In the following description,Dir(.),Mult(.) andUnif(.)
denote Dirichlet, Multinomial and Uniform distributions, re-

230



Symbol Meaning
α hyperparameter for θ
β hyperparameter for φ
γ hyperparameter for ψ
δ hyperparameter for ξ
f topical word
c contextual word
ẽ target-side topical item

ẽ′
a sampled target-side topical item used to
generate a source-side contextual word

θ the topic distribution of document

φ
the distribution of a topic over target-side
topical items

ψ

the translation probability distribution of a
target-side topical item over source-side topical
words

ξ

the generation probability distribution of a
target-side topical item over source-side
contextual words

Nz topic number
Nd document number
Nf the number of topical words
Nc the number of contextual words
Nẽ the number of target-side topical items
Nf,d the number of topical words in d
Nc,d the number of contextual words in d

Table 1: Notations in CATM.

1. Sample a topic distribution θd∼Dir(α).
2. For each position i that corresponds to a topical

word fi in the document:
(a) Sample a topic zi∼Mult(θd).
(b) Conditioned on the topic zi, sample a

target-side topical item ẽi∼Mult(φzi).
(c) Conditioned on the target-side topi-

cal item ẽi, sample the topical word
fi∼Mult(ψẽi).

3. For each position j that corresponds to a contex-
tual word cj in the document:
(a) Collect all target-side topical items ẽs that

are translations of neighboring topical
words within a window centered at cj
(window size ws).

(b) Randomly sample an item from ẽs,
ẽ′j∼Unif(ẽs).

(c) Conditioned on the sampled target-side
topical item ẽ′j , sample the contextual
word cj∼Mult(ξẽ′j ).

To better illustrate CATM, let us revisit the example
in Figure 1. We describe how CATM generates top-

spectively.

Nd

Nc,dNf,d

Nẽ

α

θ

z

ẽ ẽ′

f c

ψγ δξ

Nz

β

ϕ

Figure 2: Graphical representation of our model.

ical words “¯K/wèntı́”, “á|/lı̀chǎng”, and con-
textual word “¥á/zhōnglı̀” in the following steps:

Step 1: The model generates a topic dis-
tribution for the corresponding document as
{economy0.25, politics0.75}.

Step 2: Based on the topic distribution, we
choose “economy” and “politics” as topic assign-
ments for “¯K/wèntı́” and “á|/lı̀chǎng” respec-
tively; Then, according to the distributions of the t-
wo topics over target-side topical items, we generate
target-side topical items “issue” and “stance”; Final-
ly, according to the translation probability distribu-
tions of these two topical items over source-side top-
ical words, we generate source-side topical words
“¯K/wèntı́” and “á|/lı̀chǎng” for them respec-
tively.

Step 3: For the contextual word “¥á/zhōnglı̀”,
we first collect target-side topical items of its neigh-
boring topical words such as “¯K/wèntı́”, “�
±/bǎochı́” and “á|/lı̀chǎng” to form a target-
side topical item set {“issue”,“keep”, “stance”},
from which we randomly sample one item “stance”.
Next, according to the generation probability dis-
tribution of “stance” over source contextual words,
we finally generate the source contextual word “¥
á/zhōnglı̀”.

In the above generative process, all target-side
topical items are generated from the underlying top-
ics of a source document, which guarantees that se-
lected target translations are topic-consistent. Ad-

231



ditionally, each source contextual word is derived
from a target-side topical item given its generation
probability distribution. This makes selected target
translations also compatible with source-side local
contextual words. In this way, global topics, topical
words, local contextual words and target-side topi-
cal items are highly correlated in CATM that exactly
captures such correlations for lexical selection.

3 Parameter Estimation and Inference

We propose a Gibbs sampling algorithm to learn var-
ious distributions described in the previous section.
Details of the learning and inference process are p-
resented in this section.

3.1 The Probability of Training Corpus
According to CATM, the total probability of train-
ing data D given hyperparameters α, β, γ and δ is
computed as follows:

P (D;α, β, γ, δ) =
∏
d

P (fd, cd;α, β, γ, δ)

=
∏
d

∑̃
ed
P (ẽd|α, β)P (fd|ẽd, γ)P (cd|ẽd, δ)

=
∫
φ P (φ|β)

∫
ψ P (ψ|γ)

∏
d

∑̃
ed
P (fd|ẽd, ψ)

× ∫ξ P (ξ|δ) ∑̃
e′d

P (ẽ′d|ẽd)p(cd|ẽ′d, ξ)

× ∫θ P (θ|α)P (ẽd|θ, φ)dθdξdψdφ (1)
where fd and ẽd denote the sets of topical words and
their target-side topical item assignments in docu-
ment d, cd and ẽ′d are the sets of contextual word-
s and their target-side topical item assignments in
document d.

3.2 Parameter Estimation via Gibbs Sampling
The joint distribution in Eq. (1) is intractable to
compute because of coupled hyperparameters and
hidden variables. Following Han et al, (2012),
we adapt the well-known Gibbs sampling algorith-
m (Griffiths and Steyvers, 2004) to our model. We
compute the joint posterior distribution of hidden
variables, denoted by P (z, ẽ, ẽ′|D), and then use this
distribution to 1) estimate θ, φ, ψ and ξ, and 2) pre-
dict translations and topics of all documents in D.

Specifically, we derive the joint posterior distribu-
tion from Eq. (1) as:

P (z, ẽ, ẽ′|D) ∝ P (z)P (ẽ|z)P (f|ẽ)P (ẽ′|ẽ)P (c|ẽ′) (2)

Based on the equation above, we construct a Markov
chain that converges to P (z, ẽ, ẽ′|D), where each s-
tate is an assignment of a hidden variable (includ-
ing topic assignment to a topical word, target-side
topical item assignment to a source topical or con-
textual word.). Then, we sequentially sample each
assignment according to the following three condi-
tional assignment distributions:

1. P (zi = z|z−i, ẽ, ẽ′,D): topic assignment dis-
tribution of a topical word given z−i that denotes all
topic assignments but zi, ẽ and ẽ′ that are target-side
topical item assignments. It is updated as follows:

P (zi = z|z−i, ẽ, ẽ′,D) ∝
CDZ(−i)dz + α

CDZ(−i)d∗+Nzα
×

CZẼ(−i)zẽ + β

CZẼ(−i)z∗+Nẽβ
(3)

where the topic assignment to a topical word is de-
termined by the probability that this topic appears in
document d (the 1st term) and the probability that
the selected item ẽ occurs in this topic (the 2nd ter-
m).

2. P (ẽi = ẽ|z, ẽ−i, ẽ′,D): target-side topical item
assignment distribution of a source topical word giv-
en the current topic assignments z, the current item
assignments of all other topical words ẽ−i, and the
current item assignments of contextual words ẽ′. It
is updated as follows:

P (ẽi = ẽ|z, ẽ−i, ẽ′,D) ∝
CZẼ(−i)zẽ + β

CZẼ(−i)z∗ +Nẽβ

×
CẼF(−i)ẽf + γ

CẼF(−i)ẽ∗ +Nfγ
× (

CWẼ(−i)wẽ + 1

CWẼ(−i)wẽ
)C

WẼ′
wẽ (4)

where the target-side topical item assignment to a
topical word is determined by the probability that
this item is from the topic z (the 1st term), the prob-
ability that this item is translated into the topical
word f (the 2nd term) and the probability of con-
textual words within a ws word window centered at
the topical word f , which influence the selection of
the target-side topical item ẽ (the 3rd term). It is
very important to note that we use a parallel corpus
to train the model. Therefore we directly identify
target-side topical items for source topical words via
word alignments rather than sampling.

232



3. P (ẽ′i = ẽ|z, ẽ, ẽ′−i,D): target-side topical item
assignment distribution for a contextual word given
the current topic assignments z, the current item as-
signments of topical words ẽ, and the current item
assignments of all other contextual words ẽ′−i. It is
updated as follows:

P (ẽ′i = ẽ|z, ẽ, ẽ′−i,D) ∝
CWẼwẽ
CWẼw∗

×
CẼC(−i)ẽc + δ

CẼC(−i)ẽ∗ +Nc δ
(5)

where the target-side topical item assignment used
to generate a contextual word is determined by the
probability of this item being assigned to generate
contextual words within a surface window of size
ws (the 1st term) and the probability that contextu-
al words occur in the context of this item (the 2nd
term).

In all above formulas,CDZdz is the number of times
that topic z has been assigned for all topical words
in document d, CDZd∗ =

∑
z C

DZ
dz is the topic number

in document d, and CZẼzẽ , C
ẼF
ẽf , C

WẼ
wẽ , C

WẼ′
wẽ and

CẼCẽc have similar explanations. Based on the above
marginal distributions, we iteratively update all as-
signments of corpus D until the constructed Markov
chain converges. Model parameters are estimated
using these final assignments.

3.3 Inference on Unseen Documents
For a new document, we first predict its topics and
target-side topical items using the incremental Gibb-
s sampling algorithm described in (Kataria et al.,
2011). In this algorithm, we iteratively update top-
ic assignments and translation assignments of an
unseen document following the same process de-
scribed in Section 3.2, but with estimated model pa-
rameters.

Once we obtain these assignments, we estimate
lexical translation probabilities based on the sam-
pled counts of target-side topical items. Formal-
ly, for the position i in the document correspond-
ing to the content word f , we collect the sampled
count that translation ẽ generates f , denoted by
Csam(ẽ, f). This count can be normalized to form a
new translation probability in the following way:

p(ẽ|f) = Csam(ẽ, f) + k
Csam + k ·Nẽ,f (6)

where Csam is the total number of samples during
inference and Nẽ,f is the number of candidate trans-
lations of f . Here we apply add-k smoothing to re-
fine this translation probability, where k is a tunable
global smoothing constant. Under the framework of
log-linear model (Och and Ney, 2002), we use this
translation probability as a new feature to improve
lexical selection in SMT.

4 Experiments

In order to examine the effectiveness of our mod-
el, we carried out several groups of experiments on
Chinese-to-English translation.

4.1 Setup
Our bilingual training corpus is from the FBIS cor-
pus and the Hansards part of LDC2004T07 cor-
pus (1M parallel sentences, 54.6K documents, with
25.2M Chinese words and 29M English words).
We first used ZPar toolkit2 and Stanford toolkit3 to
preprocess (i.e., word segmenting, PoS tagging) the
Chinese and English parts of training corpus, and
then word-aligned them using GIZA++ (Och and
Ney, 2003) with the option “grow-diag-final-and”.
We chose the NIST evaluation set of MT05 as the
development set, and the sets of MT06/MT08 as test
sets. On average, these three sets contain 17.2, 13.9
and 14.1 content words per sentence, respectively.
We trained a 5-gram language model on the Xinhua
portion of Gigaword corpus using the SRILM Toolk-
it (Stolcke, 2002).

Our baseline system is a state-of-the-art SMT sys-
tem, which adapts bracketing transduction gram-
mars (Wu, 1997) to phrasal translation and equip-
s itself with a maximum entropy based reordering
model (MEBTG) (Xiong et al., 2006). We used the
toolkit4 developed by Zhang (2004) to train the re-
ordering model with the following parameters: it-
eration number iter=200 and Gaussian prior g=1.0.
During decoding, we set the ttable-limit as 20, the
stack-size as 100. The translation quality is eval-
uated by case-insensitive BLEU-4 (Papineni et al.,
2002) metric. Finally, we conducted paired boot-
strap sampling (Koehn, 2004) to test the significance
in BLEU score differences.

2http://people.sutd.edu.sg/∼yue zhang/doc/index.html
3http://nlp.stanford.edu/software
4http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html

233



Model MT05
CATM (± 6w) 33.35
CATM (± 8w) 33.43
CATM (± 10w) 33.42
CATM (± 12w) 33.49
CATM (± 14w) 33.30

Table 2: Experiment results on the development set using
different window sizes ws.

To train CATM, we set the topic number Nz as
25.5 For hyperparameters α and β, we empirically
set α=50/Nz and β=0.1, as implemented in (Grif-
fiths and Steyvers, 2004). Following Han et al.
(2012), we set γ and δ as 1.0/Nf and 2000/Nc, re-
spectively. During the training process, we ran 400
iterations of the Gibbs sampling algorithm. For doc-
uments to be translated, we first ran 300 rounds in a
burn-in step to let the probability distributions con-
verge, and then ran 1500 rounds where we collected
independent samples every 5 rounds. The longest
training time of CATM is less than four days on our
server using 4GB RAM and one core of 3.2GHz
CPU. As for the smoothing constant k in Eq. (6),
we set its values to 0.5 according to the performance
on the development set in additional experiments.

4.2 Impact of Window Size ws
Our first group of experiments were conducted on
the development set to investigate the impact of the
window size ws. We gradually varied window size
from 6 to 14 with an increment of 2.

Experiment results are shown in Table 2. We
achieve the best performance when ws=12. This
suggests that a ?12-word window context is suf-
ficient for predicting target-side translations for am-
biguous source-side topical words. We therefore set
ws=12 for all experiments thereafter.

4.3 Overall Performance

In the second group of experiments, in addition to
the conventional MEBTG system, we also compared
CATM with the following two models:

Word Sense Disambiguation Model (WSDM)
(Chan et al., 2007). This model improves lexical s-
election in SMT by exploiting local contexts. For

5We try different topic numbers from 25 to 100 with an in-
crement of 25 each time. We find thatNz=25 produces a slight-
ly better performance than other values on the development set.

each content word, we construct a MaxEnt-based
classifier incorporating local collocation and sur-
rounding word features, which are also adopted by
Chan et al. (2007). For each candidate translation
ẽ of topical word f , we use WSDM to estimate
the context-specific translation probability P (ẽ|f),
which is used as a new feature in SMT system.

Topic-specific Lexicon Translation Model
(TLTM) (Zhao and Xing, 2007). This model
focuses on the utilization of document-level context.
We adapted it to estimate a lexicon translation
probability as follows:

p(f |ẽ, d) ∝ p(ẽ|f, d) · p(f |d)
=

∑
z
p(ẽ|f, z) · p(f |z) · p(z|d) (7)

where p(ẽ|f, z) is the lexical translation probabil-
ity conditioned on topic z, which can be calculat-
ed according to the principle of maximal likelihood,
p(f |z) is the generation probability of word f from
topic z, and p(z|d) denotes the posterior topic distri-
bution of document d.

Note that our CATM is proposed for lexical se-
lection on content words. To show the strong effec-
tiveness of our model, we also compared it against
the full-fledged variants of the above-mentioned two
models that are built for all source words. We refer
to them as WSDM (All) and TLTM (All), respec-
tively.

Table 3 displays BLEU scores of different lexical
selection models. All models outperform the base-
line. Although we only use CATM to predict trans-
lations for content words, CATM achieves an aver-
age BLEU score of 26.77 on the two test sets, which
is higher than that of the baseline by 1.18 BLEU
points. This improvement is statistically significant
at p<0.01. Furthermore, we also find that our model
performs better than WSDM and TLTM with signif-
icant improvements. Finally, even if WSDM (All)
and TLTM (all) are built for all source words, they
are still no better than than CATM that selects de-
sirable translations for content words. These exper-
iment results strongly demonstrate the advantage of
CATM over previous lexical selection models.

5 Analysis

In order to investigate why CATM is able to outper-
form previous models that explore only local contex-

234



Model Local Context Global Topic MT06 MT08 Avg
Baseline × × 29.66↓↓ 21.52↓↓ 25.59
WSDM

√ × 30.62↓ 22.05↓↓ 26.34
WSDM (All)

√ × 30.92 22.27 26.60
TLTM × √ 30.27↓↓ 21.70↓↓ 25.99

TLTM (All) × √ 30.33↓↓ 21.58↓↓ 25.96
CATM

√ √
30.97 22.56 26.77

Table 3: Experiment results on the test sets. Avg = average BLEU scores. WSDM (All) and TLTM (All) are models
built for all source words. ↓: significantly worse than CATM (p<0.05), ↓↓: significantly worse than CATM (p<0.01)

.

tual words or global topics, we take a deep look in-
to topics, topical items and contextual words learned
by CATM and empirically analyze the effect of mod-
eling correlations between local contextual words
and global topics on lexical selection.

5.1 Outputs of CATM

We present some examples of topics learned by
CATM in Table 4. We also list five target-side topi-
cal items with the highest probabilities for each top-
ic, and the most probable five contextual words for
each target-side topical item. These examples clear-
ly show that target-side topical items tightly connect
global topics and local contextual words by captur-
ing their correlations.

5.2 Effect of Correlation Modeling

Compared to previous lexical selection models,
CATM jointly models both local contextual words
and global topics. Such a joint modeling also en-
ables CATM to capture their inner correlations at the
model level. In order to examine the effect of corre-
lation modeling on lexical selection, we compared
CATM with its three variants: � CATM (Contex-
t) that only uses local context information. We de-
termined target-side topical items for content words
in this variant by setting the probability distribution
that a topic generates a target-side topical item to be
uniform;� CATM (Topic) that explores only glob-
al topic information. We identified target-side topi-
cal items for content words in the model by setting
ws as 0, i.e., no local contextual words being used
at all. � CATM (Log-linear) is the combination
of the above-mentioned two variants (� and�) in
a log-linear manner, which does not capture corre-
lations between local contextual words and global
topics at the model level.

Model MT06 MT08 Avg
CATM (Context) 30.46 ↓↓ 22.02 ↓↓ 26.24

CATM (Topic) 30.20 ↓↓ 21.90 ↓↓ 26.05
CATM (Log-linear) 30.59 ↓ 22.24 ↓ 26.42

CATM 30.97 22.56 26.77

Table 5: Experiment results on the test sets. CATM (Log-
linear) is the combination of CATM (Context) and CATM
(Topic) in a log-linear manner.

Results in Table 5 show that CATM performs sig-
nificantlly better than both CATM (Topic) and CAT-
M (Context). Even compared with CATM (Log-
linear), CATM still achieves a significant improve-
ment of 0.35 BLEU points (p<0.05). This validates
the effectiveness of capturing correlations for lexical
selection at the model level.

6 Related Work

Our work is partially inspired by (Han and Sun,
2012), where an entity-topic model is presented for
entity linking. We successfully adapt this work to
lexical selection in SMT. The related work mainly
includes the following two strands.

(1) Lexical Selection in SMT. In order to explore
rich context information for lexical selection, some
researchers propose trigger-based lexicon models to
capture long-distance dependencies (Hasan et al.,
2008; Mauser et al., 2009), and many more re-
searchers build classifiers to select desirable trans-
lations during decoding (Chan et al., 2007; Carpuat
and Wu, 2007; He et al., 2008; Liu et al., 2008).
Along this line, Shen et al. (2009) introduce four
new linguistic and contextual features for translation
selection in SMT. Recently, we have witnessed an
increasing efforts in exploiting document-level con-
text information to improve lexical selection. Xiao
et al. (2011) impose a hard constraint to guarantee

235



Topic Target-sideTopical Items Source-side Contextual Words

refugee

UNHCR J¬(refugee) ¯?(office) ;
(commissioner) ¯Ö(affair) p?(high-level)
republic é(union) ¬Ì(democracy) �?(government) d=(Islam) ¥(Central Africa)
refugee J¬(refugee) £(return) 6l¤(displaced) e(repatriate) �o(protect)
Kosovo r÷Fæ(Metohija) ¸S(territory) Å(crisis) Û³(situation) l�æ(Serbia)
federal �ÚI(republic) Hd.Å(Yugoslavia) ¢»(Kosovo) �?(government) �Û(authority)

military

military *	
(observer) 1Ä(action) {I(USA) <
(personnel) Üè(army)
missile (defense) XÚ(system) {I(USA) u�(launch) q(*)

United States ¥I(China) F�(Japan) ��(Taiwan) �¯(military) NMD(National Missile Defense)
system éÜI(United Nations) ïá(build) I(country) I[(country) &E(information)

war Ô�(war) |(∗) ­.(world) uÄ(wage) °�(gulf)

economy

country uÐ¥(developing) u(developed) ³(Africa) uÐ(development) ¥(China)
development ±Y(sustainable) ²L(economy) r?(promote) �¬(society) ¯�(situation)
international �¬(society) |(organization) Ü(coorporation) I[(country) éÜI(United Nations)

economic �¬(society) uÐ(development) O(growth) I[(country) �¥z(globalization)
trade uÐ(development) IS(international) ­.(world) Ý](investment) :(point)

cross-strait
relation

Taiwan ¥I(China) º(mainland) �Û(authority) {I(USA) Ó(compatriot)
China `(say) {I(USA) ��(Taiwan) �K(principle) ü(*)

relation uÐ(development) W(*) ¥(China) ü(*) I(country)
cross-strait ü(*) 'X(relation) ��(Taiwan) W(*) �6(exchange)

issue )û(settlement) ?Ø(discuss) ¯K(issue) ­(important) ��(Taiwan)

Table 4: Examples of topics, topical items and contextual words learned by CATM with Nz=25 and Ws=12. Chinese
words that do not have direct English translations are denoted with ”*”. Here “q” and “|” are Chinese quantifiers
for missile and war, respectively; “ü” and “W” together means cross-starit.

the document-level translation consistency. Ture et
al. (2012) soften this consistency constraint by in-
tegrating three counting features into decoder. Also
relevant is the work of Xiong et al.(2013), who use
three different models to capture lexical cohesion for
document-level SMT.

(2) SMT with Topic Models. In this strand, Zhao
and Xing (2006, 2007) first present a bilingual top-
ical admixture formalism for word alignment in
SMT. Tam et al. (2007) and Ruiz et al. (2012) apply
topic model into language model adaptation. Su et
al. (2012) conduct translation model adaptation with
monolingual topic information. Gong et al. (2010)
and Xiao et al. (2012) introduce topic-based similar-
ity models to improve SMT system. Axelrod et al.
(2012) build topic-specific translation models from
the TED corpus and select topic-relevant data from
the UN corpus to improve coverage. Eidelman et al.
(2012) incorporate topic-specific lexical weights in-
to translation model. Hewavitharana et al. (2013)
propose an incremental topic based translation mod-
el adaptation approach that satisfies the causality
constraint imposed by spoken conversations. Hasler
et al. (2014) present a new bilingual variant of LDA
to compute topic-adapted, probabilistic phrase trans-
lation features. They also use a topic model to learn

latent distributional representations of different con-
text levels of a phrase pair (Hasler et al., 2014b).

In the studies mentioned above, those by Zhao
and Xing (2006), Zhao and Xing (2007), Hasler et
al. (2014a), and Hasler et al. (2014b) are most relat-
ed to our work. However, they all perform dynam-
ic translation model adaptation with topic models.
Significantly different from them, we propose a new
topic model that exploits both local contextual word-
s and global topics for lexical selection. To the best
of our knowledge, this is first attempt to capture cor-
relations between local words and global topics for
better lexical selection at the model level.

7 Conclusion and Future Work

This paper has presented a novel context-aware topic
model for lexical selection in SMT. Jointly modeling
local contexts, global topics and their correlations in
a unified framework, our model provides an effec-
tive way to capture context information at differen-
t levels for better lexical selection in SMT. Experi-
ment results not only demonstrate the effectiveness
of the proposed topic model, but also show that lex-
ical selection benefits from correlation modeling.

In the future, we want to extend our model from
the word level to the phrase level. We also plan to

236



improve our model with monolingual corpora.

Acknowledgments

The authors were supported by National Natural Sci-
ence Foundation of China (Grant Nos 61303082 and
61403269), Natural Science Foundation of Jiang-
su Province (Grant No. BK20140355), National
863 program of China (No. 2015AA011808), Re-
search Fund for the Doctoral Program of Higher
Education of China (No. 20120121120046), the
Special and Major Subject Project of the Industri-
al Science and Technology in Fujian Province 2013
(Grant No. 2013HZ0004-1), and 2014 Key Project
of Anhui Science and Technology Bureau (Grant
No. 1301021018). We also thank the anonymous
reviewers for their insightful comments.

References

Amittai Axelrod, Xiaodong He, Li Deng, Alex Acero,
and Mei-Yuh Hwang. 2012. New methods and eval-
uation experiments on translating TED talks in the I-
WSLT benchmark. In Proc. of ICASSP 2012, pages
4945-4648.

Rafael E. Banchs and Marta R. Costa-jussà. 2011. A
Semantic Feature for Statistical Machine Translation.
In Proc. of SSSST-5 2011, pages 126-134.

David M. Blei. 2003. Latent Dirichlet Allocation. Jour-
nal of Machine Learning, pages 993-1022.

Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. In Proc. of EMNLP 2007, pages 61-72.

Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proc. of ACL 2007, pages 33-40.

David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, pages 201-228.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better Hypothesis Testing for Statis-
tical Machine Translation: Controlling for Optimizer
Instability. In Proc. of ACL 2011, short papers, pages
176-181.

George Doddington. 2002. Translation Quality Using N-
gram Cooccurrence Statistics. In Proc. of HLT 2002,
138-145.

Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic Models for Dynamic Transla-
tion Model Adaptation. In Proc. of ACL 2012, Short
Papers, pages 115-119.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proc. of ACL
2006, pages 961-968.

Zhengxian Gong and Guodong Zhou. 2010. Improve
SMT with Source-side Topic-Document Distributions.
In Proc. of SUMMIT 2010.

Thomas L. Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. In Proc. of the National Academy of
Sciences 2004.

Xianpei Han and Le Sun. 2012. An Entity-Topic Model
for Entity Linking. In Proc. of EMNLP 2012, pages
105-115.

Saša Hasan, Juri Ganitkevitch, Hermann Ney, and Jesús
Andrés-Ferrer 2008. Triplet Lexicon Models for S-
tatistical Machine Translation. In Proc. of EMNLP
2008, pages 372-381.

Eva Hasler, Phil Blunsom, Philipp Koehn, and Bar-
ry Haddow. 2014. Dynamic Topic Adaptation for
Phrase-based MT. In Proc. of EACL 2014, pages 328-
337.

Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic Topic Adaptation for SMT
using Distributional Profiles. In Proc. of WMT 2014,
pages 445-456.

Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improv-
ing Statistical Machine Translation using Lexicalized
Rule Selection. In Proc. of COLING 2008, pages 321-
328.

Sanjika Hewavitharana, Dennis Mehay, Sankara-
narayanan Ananthakrishnan, and Prem Natarajan.
2013. Incremental Topic-based TM Adaptation for
Conversational SLT. In Proc. of ACL 2013, Short
Papers, pages 697-701.

Saurabh S. Kataria, Krishnan S. Kumar, and Rajeev Ras-
togi. 2011. Entity Disambiguation with Hierarchical
Topic Models. In Proc. of KDD 2011, pages 1037-
1045.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Proc.
of NAACL-HLT 2003, pages 127-133.

Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388-395.

Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum Entropy based Rule Selection Model
for Syntax-based Statistical Machine Translation. In
Proc. of EMNLP 2008, pages 89-97.

Arne Mauser, Saša Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Proc. of
EMNLP 2009, pages 210-218.

237



Franz Joseph Och and Hermann Ney. 2002. Discrimi-
native Training and Maximum Entropy Models for S-
tatistical Machine Translation. In Proc. of ACL 2002,
pages 295-302.

Franz Joseph Och and Hermann Ney. 2003. A Systemat-
ic Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 2003(29), pages 19-
51.

Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL 2003,
pages 160-167.

Franz Joseph Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 2004(30), pages
417-449.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2007. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proc. of ACL
2002, pages 311-318.

Nick Ruiz and Marcello Federico. 2012. Topic Adapta-
tion for Lecture Translation through Bilingual Latent
Semantic Models. In Proc. of the Sixth Workshop on
Statistical Machine Translation, pages 294-302.

Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical Ma-
chine Translation. In Proc. of EMNLP 2009, pages
72-80.

Andreas Stolcke. 2002. Srilm - An Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-
904.

Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xi-
aodong Shi, Huailin Dong, and Qun Liu. 2012. Trans-
lation Model Adaptation for Statistical Machine Trans-
lation with Monolingual Topic Information. In Proc.
of ACL 2012, pages 459-468.

Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4), pages 187-
207.

Ferhan Ture, DouglasW. Oard, and Philip Resnik. 2012.
Encouraging Consistent Translation Choices. In Proc.
of NAACL-HLT 2012, pages 417-426.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.

Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level Consistency Verification in
Machine Translation. In Proc. of MT SUMMIT 2011,
pages 131-138.

Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A Topic Similarity Model for Hi-
erarchical Phrase-based Translation. In Proc. of ACL
2012, pages 750-758.

Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for S-
tatistical Machine Translation. In Proc. of ACL 2006,
pages 521-528.

Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lü,
and Qun Liu. 2013. Modeling Lexical Cohesion for
Document-Level Machine Translation. In Proc. of IJ-
CAI 2013, pages 2183-2189.

Deyi Xiong and Min Zhang. 2014. A Sense-Based
Translation Model for Statistical Machine Translation.
In Proc. of ACL 2014, pages 1459-1469.

Bing Zhao and Eric P.Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proc. of ACL/COLING 2006, pages 969-976.

Bing Zhao and Eric P.Xing. 2007. HM-BiTAM: Bilin-
gual Topic Exploration, Word Alignment, and Trans-
lation. In Proc. of NIPS 2007, pages 1-8.

238


