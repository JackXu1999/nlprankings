















































Potts Model on the Case Fillers for Word Sense Disambiguation


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1382–1386,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Potts Model on the Case Fillers for Word Sense Disambiguation

Hiroya Takamura
Tokyo Institute of Technology

takamura@pi.titech.ac.jp

Manabu Okumura
Tokyo Institute of Technology
oku@pi.titech.ac.jp

Abstract

We propose a new method for word sense
disambiguation for verbs. In our method,
sense-dependent selectional preference of
verbs is obtained through the probabilistic
model on the lexical network. The mean-
field approximation is employed to com-
pute the state of the lexical network. The
outcome of the computation is used as fea-
tures for discriminative classifiers. The
method is evaluated on the dataset of the
Japanese word sense disambiguation.

1 Introduction

Polysemous words can be obstacles to many ap-
plications of natural language processing such as
information retrieval, question answering, and ma-
chine translation. The task of distinguishing word
senses given a token of a polysemous word and
its context is often referred to as word sense dis-
ambiguation and has been studied by many re-
searchers (Agirre and Edmonds, 2006). Among
a number of word sense disambiguation tasks
including noun sense disambiguation, adjective
sense disambiguation, and named entity disam-
biguation, we focus on the verb sense disambigua-
tion with the supervised setting, where we are sup-
posed to construct a classifier given labeled train-
ing instances.

It is learned in the previous work that case fillers
are often good clues for the verb sense disam-
biguation. Consider, for example, the following
two sentences:

1. “He drove a car to the next town.”
2. “He drove the dogs away.”

The “drive” in Sentence 1 means “operate (a ve-
hicle)”, while “drive” in Sentence 2 means “urge
(something to move)”. Although there might be
long contexts to these instances, looking at the

case fillers “car” and “dog” alone would lead to
the correct interpretations of the meanings. This
kind of preference on nouns as case fillers is called
selectional preference, which in this case de-
pends on sense. It is sometimes impractical, how-
ever, to expect that the training dataset covers all
the nouns as case fillers, if case fillers of the target
verb are diverse. The purpose of this article is to
propose a method for propagating the information
on the case fillers in the training dataset to other
nouns, so that the sense of polysemous words is
correctly disambiguated. In our method, the infor-
mation propagation is implemented as estimation
of the state of the probabilisitc model on the lexi-
cal network. One advantage of our method is that
we can overcome the difficulty caused by the noise
contained in the lexical network.

2 Related Work

Recent work on general word sense disambigua-
tion is summarized in the book edited by Agirre
and Edmonds (2006). We focus on the work of the
verb sense disambiguation.

The idea of using case frames for the verb sense
disambiguation is not novel, and dates back to
90’s. Fujii et al. (1998) proposed a method for
the verb sense disambiguation, which is based on
the k-nearest neighbors. In their method, each in-
stance is represented as a case frame and the sim-
ilarity of two instances is calculated as a weighted
sum of the similarities of case fillers. Fujii et al.
also proposed a framework of active learning.

To disambiguate verb senses, Chen and
Palmer (2009) proposed to use linguistic and se-
mantic features including the voice of the given
sentence, the presence of a PP adjunct, and the
named entity tags. Dligach and Palmer (2008) pro-
posed to use co-occurrence with other verbs as fea-
tures. Wagner et al. (2009) proposed to use verb
clusters generated with statistics on verb subcate-
gorization.

1382



3 Potts Model

We introduce the probability model that we will
use for our task. This model gives the probabil-
ity distribution over a set of nodes associated with
random variables, where some pairs of variables
are dependent on each other. If the variables can
have more than two values and there is no order re-
lation between the values, the network comprised
of such variables is called Potts model (Wu, 1982),
which has been used in applications such as image
restoration (Tanaka and Morita, 1996) and rumor
transmission (Liu et al., 2001).

Suppose a network consisting of nodes and
weighted edges is given. Let c denote the value
of a node, and wij the weight between i and j.
Energy function H(c) is represented as

H(c) = −β
∑

ij

wijδ(ci, cj)−α
∑

i∈L
δ(ci, ai), (1)

where β is a constant called the inverse-
temperature, L is the index set for the observed
variables, ai is the value of an observed variable i,
and α is a positive constant representing a weight
on labeled data. δ returns 1 if two arguments are
equal to each other, 0 otherwise. The state is pe-
nalized if ci (i ∈ L) is different from ai. The prob-
ability distribution of the network is represented as
P (c) = exp{−H(c)}/Z, where Z is a normaliza-
tion factor.

Instead of minimizing H(c), we attempt to min-
imize the free energy, which is defined to be the
sum of H(c) and the negative entropy. How-
ever, this minimization is computationally hard.
We hence resort to the mean-field approximation
method (Nishimori, 2001), in which P (c) is re-
placed by factorized function ρ(c) =

∏
i ρi(ci).

The evergy function with the factorized probabil-
ity function is called variational free energy:

F (c) =
∑

c
ρ(c)H(c) −

∑

c
−ρ(c) log ρ(c)

= −α
∑

i

∑

ci

ρi(ci)δ(ci, ai)

−β
∑

ij

∑

ci,cj

ρi(ci)ρj(cj)wijδ(ci, cj)

−
∑

i

∑

ci

−ρi(ci) log ρi(ci). (2)

By minimizing F (c) under the condition that
∀i, ∑ci ρi(ci) = 1, we obtain the following fixed
point equation for i ∈ L:

ρi(c) =
exp(αδ(c, ai) + β

∑
j
wijρj(c))∑

n
exp(αδ(n, ai) + β

∑
j
wijρj(n))

. (3)

The fixed point equation for i /∈ L can be obtained
by removing αδ(c, ai) from above. This fixed
point equation is solved by an iterative computa-
tion. In the actual implementation, we represent ρi
with a linear combination of the discrete Tcheby-
cheff polynomials (Tanaka and Morita, 1996). De-
tails on the Potts model and its computation can be
found in the literature (Nishimori, 2001).

4 Proposed Method

4.1 Construction of Lexical Networks

We follow the work by Takamura et al. (2005) to
construct a lexical network. We link two words if
one word appears in the gloss of the other. Each
link belongs to one of two groups: the same-
orientation links SL and the different-orientation
links DL. If a negation word appears in the gloss
of an entry word, the words after the negation
word are linked to the entry word with DL. Oth-
erwise, those words are linked with SL. In case
of Japanese, the auxiliaries “nai” and “nu” are re-
garded as negation words.

We next set weights W = (wij) to links :

wij =





1√
d(i)d(j)

(lij ∈ SL)
− 1√

d(i)d(j)
(lij ∈ DL)

0 otherwise

, (4)

where lij denotes the link between word i and
word j, and d(i) denotes the degree of word
i, which indicates the number of words linked
with word i. We call this network the gloss net-
work (G). We construct another network, the the-
saurus network (T), by linking synonyms and hy-
pernyms in a thesaurus. We also merge the two
networks above to construct another network the
gloss-thesaurus network (GT).

4.2 Use of Potts Model

We estimate the tendency of each word to be the
case filler for a verb sense. For example, “car”
would have a high tendency to be the case filler for
“drive” with the sense “operate (a vehicle)”. Such
tendency is measured as the probability Pi(c) over
senses c for noun ni. We will use the Potts model
for this purpose. Namely, the local approximate

1383



probability ρi(c), equivalently the mariginal prob-
ability

∑
c ρ(c), is regarded as probability Pi(c).

The values of each random variable are senses
of the target verb. For each case and each verb, we
estimate the probability of the sense assignments
on the whole set of nodes by means of the mean-
field approximation. The case fillers of the train-
ing instances are used as observed variables, with
their index set being L in Equation (2). Pi(c) is
estimated for each case.

In our experiments on Japanese, surface cases
are employed: wo (accusative), ga (nominative),
ni (dative/locative), de (locative/instrumental), no
(genitive/others), e (locative/illative), to (comita-
tive), kara (elative), yori (comparative), made
(terminative). Although our model is also applica-
ble to deep cases, we focus on surface cases since
deep case recognition itself is a challenging task.

4.3 Estimation of β
In some pieces of previous work (Takamura et al.,
2005; Takamura et al., 2007), it has been shown
that the optimal β can be obtained by estimating
the critical temperature, at which phase transition
occurs from paramagnetic phase (variables are
randomly oriented) to ferromagnetic phase (most
of the variables have the same value). We follow
these pieces of previous work.

In practice, when the maximum of the spa-
tial averages of the approximated probabilities
maxc

∑
i ρi(c)/N exceeds a threshold during in-

creasing β, we consider that the phase transition
has occurred. We select the value of β slightly be-
fore the phase transition.

4.4 Discriminative Training
Probability ρi(c) is expected to be a strong clue for
sense disambiguation. However, it is not clear how
we can effectively use the probabilities of different
cases c; ρi(c) for some cases would be reliable,
while some others less reliable. In addition, the
word tokens that appear before and after the target
word also give evidence for senses. We therefore
use a discriminative approach to construct a clas-
sifier with those various clues as features. Sup-
port vector machines are employed in this work,
although other classifiers are also applicable.

Note that even if a case filler in the test instance
did not appear in the training data, the feature ρi(c)
corresponding to this case filler conveys the infor-
mation on the selectional preference of the verb
against the case filler.

5 Experiments

5.1 Experimental Settings
The proposed method for verb sense disambigua-
tion is evaluated on the white paper part of BC-
CWJ corpus, the first balanced corpus of contem-
porary written Japanese (Maekawa, 2008), which
was also used as a test set for SemEval-2 Japanese
word sense disambiguation task (Okumura et al.,
2010). The dataset used in this research was cre-
ated by the preliminary annotation for SemEval-
2. The senses are defined in the Iwanami Kokugo
Jiten (Nishio et al., 1994), a Japanese dictio-
nary. Among three levels of sense IDs defined in
this dictionary, the middle-level sense was used
in the empirical evaluation, which is the same
level of senses used in the SemEval-2. From the
dataset above, we selected most ambiguous 14
verbs whose empirical sense distributions (i.e., es-
timated with the maximum likelihood principle)
have a high entropy and appear more than 100
times in the dataset. The statistics of this dataset
is shown in the middle columns of Table 1. 5-fold
cross-validation was employed for each verb.

TinySVM version 0.091 was used for SVM
training and classification. The linear kernel was
used as a kernel function of SVM. The soft-margin
parameter C indicating the tradeoff between the
model complexity and the training error was tuned
to the best value among 0.01, 0.1, 1, 10 for each
method. The glosses of the Iwanami Kokugo Jiten
Japanese dictionary are used to construct a lexi-
cal network G (gloss network). Japanese Word-
Net version 0.9 (Bond et al., 2009) was used to
construct a lexical network T (thesaurus network).
The numbers of nodes in G, T , and GT are respec-
tively 35225, 66218, and 87038. Due to a large
number of polysemous words in the thesaurus,
many synsets of Japanese WordNet are connected.

Two baselines are evaluated on the test datasets.
Baseline 1 is SVMs trained with basic features:
the target verb itself and 3 words before and after
the target verb. Baseline 2 is also SVMs trained
with the basic features above and the case filler
features: the nouns that appear as case fillers.

The proposed methods are SVMs trained with
the basic features, the case filler features, and the
lexical network features: the probability ρi(c) in-
troduced in Section 4 for each case and each sense
c when wi appears as a case filler.

1http://chasen.org/˜taku/software/
TinySVM/

1384



Table 1: Classification accuracy on hard verbs (%)

target English # of # of baselines G T GT
word translation instances senses 1 2
ataru hit, correspond 463 7 83.2 85.3 87.9 88.1 87.7
dasu take out, cause 173 3 73.4 75.1 74.6 79.2 75.1
deru go out, appear 189 3 86.2 86.8 89.4 87.8 87.8
kiku listen, be effective 141 2 87.2 87.2 87.2 86.5 87.2

susumu proceed, advance 931 2 81.8 83.1 84.2 83.5 84.4
average – 379.4 3.4 82.0 83.5 84.7 84.6 84.8

Table 2: Average classification accuracy on easy verbs (%)

# of # of baselines G T GT
instances senses 1 2

average on easy verbs 1216.9 3.0 94.6 94.8 95.0 95.1 95.0
total average 917.8 3.1 93.1 93.4 93.7 93.8 93.7

The classification accuracy, i.e., the number of
correctly classified instances divided by the total
number of instances, is employed as evaluation
measure. Averages in this paper are micro aver-
ages. The methods are evaluated first for hard
verbs, for which the classification accuracy of the
baseline method is less than 90 %. There are
5 hard verbs; the other 9 verbs are easy verbs,
for which the classification accuracy is already
equal to or better than 90 %. Note that the pro-
posed method aims at hard verbs as mentioned in
Section 1, although we would like the proposed
method not to degrade the classification perfor-
mance for easy verbs.

5.2 Results

The classification result for hard verbs is shown
in Table 1. Baseline 2 is better than baseline 1,
meaning that the simple information on the word
as a case filler improves the classification perfor-
mance. The proposed methods on lexical net-
works G, T , and GT mostly outperformed base-
lines with a few exceptions. On average, the pro-
posed method on GT increases the classification
accuracy by 2.8 points compared with the base-
line 1, 1.3 points compared with the baseline 2.
This result shows that the information propagation
on the lexical network offers useful clues for verb
sense disambiguation. Table 2 shows that the pro-
posed method is at least comparable to the base-
lines when it is applied to easy verbs. The ac-
curacy for each easy verb was omitted from the

table due to the space limitation, and only the av-
erage values are written in the table. These results
also show that the difference of lexical networks
does not have a significant effect on the average
classification performance.

In order to gain more intuitive understanding on
the method, we give an example of the computa-
tional result for case wo (accusative) of verb dasu.
We used all of the 173 instances that are avail-
able. Nouns that have high probability ρi(c) for
c = sense1 (take out, let out, send, pay) are, for
example, price, application, permission, demand,
request, wish, command, permit, and certificate.
Nouns that have high ρi(c) for c = sense2 (show)
are, for example, speed, advance, agility, effect,
driving force, breakthrough, and activity.

6 Conclusion

We proposed a new method for word sense dis-
ambiguation for verbs. In our method, sense-
dependent selectional preference of verbs was ob-
tained through the probabilistic model on the lex-
ical network. The mean-field approximation was
employed to compute the state of the lexical net-
work. The outcome of the computation was used
as features for discriminative classifiers. The
method is evaluated on the dataset of the Japanese
word sense disambiguation.

Future work includes the use of words that are
not on the lexical network, the incorporation of in-
teractions between multiple cases, and theoretical
study of the Potts model for lexical network.

1385



References
Eneko Agirre and Philip Glenny Edmonds, editors.

2006. Word Sense Disambiguation: Algorithms And
Applications. Springer-Verlag.

Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka
Uchimoto, Takayuki Kuribayashi, and Kyoko Kan-
zaki. 2009. Enhancing the japanese wordnet. In
Proceedings of the 7th Workshop on Asian Language
Resources (in conjunction with ACL-IJCNLP 2009).

Jinying Chen and Martha S. Palmer. 2009. Improv-
ing English verb sense disambiguation performance
with linguistically motivated features and clear sense
distinction boundaries. Language Resources and
Evaluation, 43:181–208.

Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Short Papers, HLT-Short
’08, pages 29–32, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, and
Hozumi Tanaka. 1998. Selective sampling for
example-based word sense disambiguation. Com-
putational Linguistics, 24(4):573–597.

Zhongzhu Liu, Jun Luo, and Chenggang Shao. 2001.
Potts model for exaggeration of a simple rumor
transmitted by recreant rumormongers. Physical
Review E, 64:046134,1–046134,9.

Kikuo Maekawa. 2008. Balanced corpus of con-
temporary written japanese. In Proceedings of the
6th Workshop on Asian Language Resources, pages
101–102.

Hidetoshi Nishimori. 2001. Statistical Physics of Spin
Glasses and Information Processing. Oxford Uni-
versity Press.

Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu-
tani. 1994. Iwanami Japanese Dictionary (5th edi-
tion). Iwanami-shoten.

Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In Proceedings of the 5th Interna-
tional Workshop on Evaluating Word Sense Disam-
biguation Systems, pages 69–74.

Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 133–140.

Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2007. Extracting semantic orientations of phrases
from dictionary. In Proceedings of the Human
Language Technologies: The Annual Conference
of the North American Chapter of the Association

for Computational Linguistics (NAACL-HLT2007),
pages 292–299.

Kazuyuki Tanaka and Tohru Morita. 1996. Applica-
tion of cluster variation method to image restoration
problem. In J.L. Morán-López and J.M. Sanchez,
editors, Theory and Applications of the Cluster Vari-
ation and Path Probability Methods, pages 353–373.
Plenum Press, New York.

Wiebke Wagner, Helmut Schmid, and Sabine Schulte
im Walde. 2009. Verb sense disambiguation using
a predicate-argument-clustering model. In Proceed-
ings of the CogSci Workshop on Distributional Se-
mantics beyond Concrete Concepts.

Fa-Yueh Wu. 1982. The potts model. Reviews of Mod-
ern Physics, 54(1):235–268.

1386


