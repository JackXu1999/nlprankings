



















































Generating Diverse Translations with Sentence Codes


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1823–1827
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1823

Generating Diverse Translations with Sentence Codes

Raphael Shu
The University of Tokyo

shu@nlab.ci.i.u-tokyo.ac.jp

Hideki Nakayama
The University of Tokyo

nakayama@ci.i.u-tokyo.ac.jp

Kyunghyun Cho
New York University

CIFAR Azrieli Global Scholar
kyunghyun.cho@nyu.edu

Abstract

Users of machine translation systems may de-
sire to obtain multiple candidates translated
in different ways. In this work, we attempt
to obtain diverse translations by using sen-
tence codes to condition the sentence gener-
ation. We describe two methods to extract
the codes, either with or without the help of
syntax information. For diverse generation,
we sample multiple candidates, each of which
conditioned on a unique code. Experiments
show that the sampled translations have much
higher diversity scores when using reasonable
sentence codes, where the translation quality
is still on par with the baselines even under
strong constraint imposed by the codes. In
qualitative analysis, we show that our method
is able to generate paraphrase translations with
drastically different structures. The proposed
approach can be easily adopted to existing
translation systems as no modification to the
model is required.

1 Introduction

When using machine translation systems, users
may desire to see different candidate translations
other than the best one. In this scenario, users usu-
ally expect the system to show candidates with dif-
ferent sentence structures.

To obtain diverse translations, conventional
neural machine translation (NMT) models allow
one to sample translations using the beam search
algorithm, however, they usually share similar
sentence structures. Recently, various methods (Li
et al., 2016; Xu et al., 2018) are proposed for di-
verse generation. These methods encourage the
model to use creative vocabulary to achieve high
diversity. Although producing creative words ben-
efits tasks in the dialog domain, when applied
to machine translation, it can hurt the translation
quality by changing the original meaning.

In this work, we are interested in generating
multiple valid translations with high diversity. To
achieve this, we propose to construct the codes
based on semantics-level or syntax-level informa-
tion of target-side sentences.

To generate diverse translations, we constrain
the generation model by specifying a particular
code as a semantic or syntactic assignment. More
concretely, we prefix the target-side sentences
with the codes. Then, an NMT model is trained
with the original source sentences and the prefixed
target sentences. As the model generates tokens
in left-to-right order, the probability of emitting
each word is predicted conditioned on the assigned
code. As each assignment is supposed to corre-
spond to a sentence structure, the candidate trans-
lations sampled with different assignments are ex-
pected to have high diversity.

We can think such model as a mixture-of-expert
translation model where each expert is capable of
producing translations with a certain style indi-
cated by the code. In the inference time, code as-
signments are given to the model so that a selec-
tion of experts are picked to generate translations.

The key question is how to extract such sentence
codes. Here, we explore two approaches. First, a
simple unsupervised method is tested, which clus-
ters the sentence embeddings and use the cluster
ids as the code assignments. Next, to capture only
the structural variation of sentences, we turn to
syntax. We encode the structure of constituent
parse trees into discrete codes with a tree auto-
encoder.

Experiments on two machine translation
datasets show that a set of highly diverse transla-
tions can be obtained with reasonable mechanism
for extracting the sentence codes, while the
sampled candidates still have BLEU scores on par
with the baselines.



1824

2 Proposed Approach

2.1 Extracting Sentence Codes

Our approach produces diverse translations by
conditioning sentence generation with the sen-
tence codes. Ideally, we would like the codes to
capture the information about the sentence struc-
tures rather than utterances. To extract such codes
from target sentences, we explore two methods.

Semantic Coding Model The first method ex-
tracts sentence codes from unsupervisedly learned
semantic information. We cluster the sentence em-
beddings produced by pre-trained models into a
fixed number of clusters, then use the cluster ids
as discrete priors to condition the sentence gener-
ation. In this work, we test two semantic coding
models. The first model is based on BERT (De-
vlin et al., 2018), where the vectors corresponding
to the “[CLS]” token are clustered.

The second model produces sentence embed-
dings by averaging FastText word embeddings
(Bojanowski et al., 2017). Comparing to the hid-
den states of BERT, word embeddings are ex-
pected to contain less syntactic information as the
word order is ignored during training.

Syntactic Coding Model To explicitly capture
the syntactic diversity, we also consider to derive
the sentence codes from the parse trees produced
by a constituency parser. As the utterance-level
information is not desired, the terminal nodes are
removed from the parse trees.

To obtain the sentence codes, we use a
TreeLSTM-based auto-encoder similar to Socher
et al. (2011), which encodes the syntactic infor-
mation into a single discrete code. As illustrated
in Fig. 1 (a), a TreeLSTM cell (Tai et al., 2015)
computes a recurrent state based on a given input
vector and the states of Ni child nodes:

hi = fcell(xi, hi1, hi2, ..., hiNi ; θ). (1)

The tree auto-encoder model is shown in Fig. 1
(c), where the encoder computes a latent tree rep-
resentation. As the decoder has to unroll the vec-
tor representation following a reversed tree struc-
ture to predict the non-terminal labels, the stan-
dard TreeLSTM equation cannot be directly ap-
plied. To compute along the reversed tree, we
modify Eq. 1 for computing the hidden state of the
j-th child node given the parent-node state hi:

hij = fdec(hi; θj), (2)

self-attention
feed-forward Nx

(b) source encoder (c) tree auto-encoder

mean pool

discrete code

+

+

NP VP

NPVBD

S

VP

VBD NP

NP

S

the cat bit him

(a) TreeLSTM cell

Label

hi1
<latexit sha1_base64="5Vvyf+Bf+wOqV4BOkHkduYgiiz8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqhg2mhNLtiBoUXGLDciuwnWikcSSwFY1vZ37rCbXhSj7YSYJhTIeSDzij1knNUS/jwbRXrvhVfw6ySoKcVCBHvVf+6vYVS2OUlglqTCfwExtmVFvOBE5L3dRgQtmYDrHjqKQxmjCbXzslZ07pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq5EILll1dJ86Ia+NXg/rJSu8njKMIJnMI5BHAFNbiDOjSAwSM8wyu8ecp78d69j0VrwctnjuEPvM8fffmPDw==</latexit><latexit sha1_base64="5Vvyf+Bf+wOqV4BOkHkduYgiiz8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqhg2mhNLtiBoUXGLDciuwnWikcSSwFY1vZ37rCbXhSj7YSYJhTIeSDzij1knNUS/jwbRXrvhVfw6ySoKcVCBHvVf+6vYVS2OUlglqTCfwExtmVFvOBE5L3dRgQtmYDrHjqKQxmjCbXzslZ07pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq5EILll1dJ86Ia+NXg/rJSu8njKMIJnMI5BHAFNbiDOjSAwSM8wyu8ecp78d69j0VrwctnjuEPvM8fffmPDw==</latexit><latexit sha1_base64="5Vvyf+Bf+wOqV4BOkHkduYgiiz8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqhg2mhNLtiBoUXGLDciuwnWikcSSwFY1vZ37rCbXhSj7YSYJhTIeSDzij1knNUS/jwbRXrvhVfw6ySoKcVCBHvVf+6vYVS2OUlglqTCfwExtmVFvOBE5L3dRgQtmYDrHjqKQxmjCbXzslZ07pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq5EILll1dJ86Ia+NXg/rJSu8njKMIJnMI5BHAFNbiDOjSAwSM8wyu8ecp78d69j0VrwctnjuEPvM8fffmPDw==</latexit><latexit sha1_base64="5Vvyf+Bf+wOqV4BOkHkduYgiiz8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqhg2mhNLtiBoUXGLDciuwnWikcSSwFY1vZ37rCbXhSj7YSYJhTIeSDzij1knNUS/jwbRXrvhVfw6ySoKcVCBHvVf+6vYVS2OUlglqTCfwExtmVFvOBE5L3dRgQtmYDrHjqKQxmjCbXzslZ07pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq5EILll1dJ86Ia+NXg/rJSu8njKMIJnMI5BHAFNbiDOjSAwSM8wyu8ecp78d69j0VrwctnjuEPvM8fffmPDw==</latexit>

hi2
<latexit sha1_base64="oU4N2owU9MMPtxx7wPH567xUBE4=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hd0g6DHoxWME84BkCbOTTjJmdmaZmRXCkn/w4kERr/6PN//GSbIHTSxoKKq66e6KEsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jUo1wwZTQul2RA0KLrFhuRXYTjTSOBLYisa3M7/1hNpwJR/sJMEwpkPJB5xR66TmqJfx6rRXKvsVfw6ySoKclCFHvVf66vYVS2OUlglqTCfwExtmVFvOBE6L3dRgQtmYDrHjqKQxmjCbXzsl507pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq6EILll1dJs1oJ/Epwf1mu3eRxFOAUzuACAriCGtxBHRrA4BGe4RXePOW9eO/ex6J1zctnTuAPvM8ff36PEA==</latexit><latexit sha1_base64="oU4N2owU9MMPtxx7wPH567xUBE4=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hd0g6DHoxWME84BkCbOTTjJmdmaZmRXCkn/w4kERr/6PN//GSbIHTSxoKKq66e6KEsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jUo1wwZTQul2RA0KLrFhuRXYTjTSOBLYisa3M7/1hNpwJR/sJMEwpkPJB5xR66TmqJfx6rRXKvsVfw6ySoKclCFHvVf66vYVS2OUlglqTCfwExtmVFvOBE6L3dRgQtmYDrHjqKQxmjCbXzsl507pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq6EILll1dJs1oJ/Epwf1mu3eRxFOAUzuACAriCGtxBHRrA4BGe4RXePOW9eO/ex6J1zctnTuAPvM8ff36PEA==</latexit><latexit sha1_base64="oU4N2owU9MMPtxx7wPH567xUBE4=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hd0g6DHoxWME84BkCbOTTjJmdmaZmRXCkn/w4kERr/6PN//GSbIHTSxoKKq66e6KEsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jUo1wwZTQul2RA0KLrFhuRXYTjTSOBLYisa3M7/1hNpwJR/sJMEwpkPJB5xR66TmqJfx6rRXKvsVfw6ySoKclCFHvVf66vYVS2OUlglqTCfwExtmVFvOBE6L3dRgQtmYDrHjqKQxmjCbXzsl507pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq6EILll1dJs1oJ/Epwf1mu3eRxFOAUzuACAriCGtxBHRrA4BGe4RXePOW9eO/ex6J1zctnTuAPvM8ff36PEA==</latexit><latexit sha1_base64="oU4N2owU9MMPtxx7wPH567xUBE4=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hd0g6DHoxWME84BkCbOTTjJmdmaZmRXCkn/w4kERr/6PN//GSbIHTSxoKKq66e6KEsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jUo1wwZTQul2RA0KLrFhuRXYTjTSOBLYisa3M7/1hNpwJR/sJMEwpkPJB5xR66TmqJfx6rRXKvsVfw6ySoKclCFHvVf66vYVS2OUlglqTCfwExtmVFvOBE6L3dRgQtmYDrHjqKQxmjCbXzsl507pk4HSrqQlc/X3REZjYyZx5Dpjakdm2ZuJ/3md1A6uw4zLJLUo2WLRIBXEKjJ7nfS5RmbFxBHKNHe3EjaimjLrAiq6EILll1dJs1oJ/Epwf1mu3eRxFOAUzuACAriCGtxBHRrA4BGe4RXePOW9eO/ex6J1zctnTuAPvM8ff36PEA==</latexit>

...
<latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit>

xi
<latexit sha1_base64="fhCcXp/PIFsLrr08ImxUPJe8y0A=">AAAB6nicdVDJSgNBEK2JW4xb1KOXxiB4GnqGRJNb0IvHiGaBZAg9nZ6kSc9Cd48YhnyCFw+KePWLvPk3dhZBRR8UPN6roqqenwiuNMYfVm5ldW19I79Z2Nre2d0r7h+0VJxKypo0FrHs+EQxwSPW1FwL1kkkI6EvWNsfX8789h2TisfRrZ4kzAvJMOIBp0Qb6ea+z/vFEraxW62UXYRtt4JrTs2QCnZqZ2Xk2HiOEizR6Bffe4OYpiGLNBVEqa6DE+1lRGpOBZsWeqliCaFjMmRdQyMSMuVl81On6MQoAxTE0lSk0Vz9PpGRUKlJ6JvOkOiR+u3NxL+8bqqDqpfxKEk1i+hiUZAKpGM0+xsNuGRUi4khhEpubkV0RCSh2qRTMCF8fYr+Jy3XdrDtXJdL9YtlHHk4gmM4BQfOoQ5X0IAmUBjCAzzBsyWsR+vFel205qzlzCH8gPX2Cch/jiA=</latexit><latexit sha1_base64="fhCcXp/PIFsLrr08ImxUPJe8y0A=">AAAB6nicdVDJSgNBEK2JW4xb1KOXxiB4GnqGRJNb0IvHiGaBZAg9nZ6kSc9Cd48YhnyCFw+KePWLvPk3dhZBRR8UPN6roqqenwiuNMYfVm5ldW19I79Z2Nre2d0r7h+0VJxKypo0FrHs+EQxwSPW1FwL1kkkI6EvWNsfX8789h2TisfRrZ4kzAvJMOIBp0Qb6ea+z/vFEraxW62UXYRtt4JrTs2QCnZqZ2Xk2HiOEizR6Bffe4OYpiGLNBVEqa6DE+1lRGpOBZsWeqliCaFjMmRdQyMSMuVl81On6MQoAxTE0lSk0Vz9PpGRUKlJ6JvOkOiR+u3NxL+8bqqDqpfxKEk1i+hiUZAKpGM0+xsNuGRUi4khhEpubkV0RCSh2qRTMCF8fYr+Jy3XdrDtXJdL9YtlHHk4gmM4BQfOoQ5X0IAmUBjCAzzBsyWsR+vFel205qzlzCH8gPX2Cch/jiA=</latexit><latexit sha1_base64="fhCcXp/PIFsLrr08ImxUPJe8y0A=">AAAB6nicdVDJSgNBEK2JW4xb1KOXxiB4GnqGRJNb0IvHiGaBZAg9nZ6kSc9Cd48YhnyCFw+KePWLvPk3dhZBRR8UPN6roqqenwiuNMYfVm5ldW19I79Z2Nre2d0r7h+0VJxKypo0FrHs+EQxwSPW1FwL1kkkI6EvWNsfX8789h2TisfRrZ4kzAvJMOIBp0Qb6ea+z/vFEraxW62UXYRtt4JrTs2QCnZqZ2Xk2HiOEizR6Bffe4OYpiGLNBVEqa6DE+1lRGpOBZsWeqliCaFjMmRdQyMSMuVl81On6MQoAxTE0lSk0Vz9PpGRUKlJ6JvOkOiR+u3NxL+8bqqDqpfxKEk1i+hiUZAKpGM0+xsNuGRUi4khhEpubkV0RCSh2qRTMCF8fYr+Jy3XdrDtXJdL9YtlHHk4gmM4BQfOoQ5X0IAmUBjCAzzBsyWsR+vFel205qzlzCH8gPX2Cch/jiA=</latexit><latexit sha1_base64="fhCcXp/PIFsLrr08ImxUPJe8y0A=">AAAB6nicdVDJSgNBEK2JW4xb1KOXxiB4GnqGRJNb0IvHiGaBZAg9nZ6kSc9Cd48YhnyCFw+KePWLvPk3dhZBRR8UPN6roqqenwiuNMYfVm5ldW19I79Z2Nre2d0r7h+0VJxKypo0FrHs+EQxwSPW1FwL1kkkI6EvWNsfX8789h2TisfRrZ4kzAvJMOIBp0Qb6ea+z/vFEraxW62UXYRtt4JrTs2QCnZqZ2Xk2HiOEizR6Bffe4OYpiGLNBVEqa6DE+1lRGpOBZsWeqliCaFjMmRdQyMSMuVl81On6MQoAxTE0lSk0Vz9PpGRUKlJ6JvOkOiR+u3NxL+8bqqDqpfxKEk1i+hiUZAKpGM0+xsNuGRUi4khhEpubkV0RCSh2qRTMCF8fYr+Jy3XdrDtXJdL9YtlHHk4gmM4BQfOoQ5X0IAmUBjCAzzBsyWsR+vFel205qzlzCH8gPX2Cch/jiA=</latexit>

Figure 1: Architecture of the TreeLSTM-based auto-
encoder with a discretization bottleneck for learning
the sentence codes.

where the internal implementation of the recurrent
function is same as Eq. 1, however, each node has
a different parameterization depending on its posi-
tion among siblings. Note that in the decoder side,
no input vectors are fed to the recurrent computa-
tion. Finally, the decoder states are used to predict
target labels, whereas the model is optimized with
cross-entropy loss.

As the source sentence already provides hints
on the target-side sentence structure, we feed the
source information to the tree auto-encoder to en-
courage the latent representation to capture the
syntax that cannot be inferred from the source sen-
tence. To obtain the sentence codes from the latent
tree representation, we apply improved semantic
hashing (Kaiser and Bengio, 2018) to the hidden
state of the root node, which discretizes the vec-
tor into a 8-bit code (binary vector). When per-
forming improved semantic hashing, the forward
pass computes two operations: binarization and
saturated sigmoid, resulting in two vectors. One
of these two vectors are randomly selected for
the next computation. However, in the backward
pass, the gradient always flows through the vector
produced by saturated sigmoid. As the model is
trained together with the bottleneck, the codes are
optimized directly to minimize the loss function.



1825

2.2 Diverse Generation with Code
Assignment

Once we obtain the sentence codes, we prefix the
target-side sentences in the training data with the
corresponding codes. The resultant target sentence
has a form of “〈c12〉 〈eoc〉 Here is a translation.”.
The “〈eoc〉” token separates the code and words.

We train a regular NMT model with the modi-
fied training dataset. To generate diverse transla-
tions, we first obtain top-K codes from the proba-
bility distribution of code prediction. In detail, we
selectK sentence codes with the highest probabil-
ities. Then, conditioning on each code, we let the
beam search continue to generate the sentence, re-
sulting in K translations conditioned on different
codes.

3 Related Work

Existing works for diverse text generation can be
categorized into two major categories. The ap-
proaches in the first categoriy sample diverse se-
quences by varying a hidden representation. Jain
et al. (2017) generates diverse questions by in-
jecting Gaussian noise to the latent in a VAE for
encouraging the creativity of results. Xu et al.
(2018) learns K shared decoders, conditioned on
different pattern rewriting embeddings. The for-
mer method is evaluated by assessing the ability
of generating unique and unseen results, whereas
the latter is evaluated with the number of unique
uni/bi-grams and the divergence of word distri-
butions produced by different decoders. Inde-
pendent to this work, Shen et al. (2019) also ex-
plores mixture-of-expert models with an ensemble
of learners. The paper discusses multiple training
strategies and found the multiple choice learning
works best.

The second category of approaches attempts to
improve the diversity by improving decoding algo-
rithms. Li et al. (2016) modifies the scoring func-
tion in beam search to encourage the algorithm
to promote hypotheses containing words from dif-
ferent ancestral hypotheses, which is also evalu-
ated with the number of unique uni/bi-grams. Ku-
likov et al. (2018) uses an iterative beam search
approach to generate diverse dialogs.

Comparing to these works, we focus on generat-
ing translations with different sentence structures.
We still use beam search to search for best words
in every decoding steps under the constraint of
code assignment. Our approach also comes with

the advantage that no modification to the NMT
model architecture is required.

4 Experiments

4.1 Experimental Settings

We evaluate our models on two machine transla-
tion datasets: ASPEC Japanese-to-English dataset
(Nakazawa et al., 2016) and WMT14 German-
to-English dataset. The datasets contain 3M and
4.5M bilingual pairs respectively. For the ASPEC
Ja-En dataset, we use the Moses toolkit (Koehn
et al., 2007) to tokenize the English side and Kytea
(Neubig et al., 2011) to tokenize the Japanese side.
After tokenization, we apply byte-pair encoding
(Sennrich et al., 2016) to segment the texts into
subwords, forcing the vocabulary size of each lan-
guage to be 40k. For WMT14 De-En dataset, we
use sentencepiece (Kudo and Richardson, 2018) to
segment the words to ensure a vocabulary size of
32k.

In evaluation, we report tokenized BLEU for
ASPEC Ja-En dataset. For WMT14 De-En
dataset, BLEU scores are generated using Sacre-
Bleu toolkit (Post, 2018). For models that produce
sentence codes during decoding, the codes are re-
moved from translation results before evaluating
BLEU scores.

4.2 Obtaining Sentence Codes

For the semantic coding model based on BERT, we
cluster the hidden state of “[CLS]” token into 256
clusters with k-means algorithm. The cluster ids
are then used as sentence codes. For models using
FastText Embeddings, pre-trained vectors (Com-
mon Crawl, 2M words) are used. Please note
that the number of clusters is a hyperparameter,
here we choose the number of clusters to match
the number of unique codes in the syntax-based
model.

To train the syntax coding model, we parse
target-side sentences with Stanford CFG parser
(Klein and Manning, 2003). The TreeLSTM-
based auto-encoder is implemented with DGL,1

which is trained using AdaGrad optimizer for
faster convergence. We found it helpful to pre-
train the model without the discretization bottle-
neck for achieving higher label accuracy.

1https://www.dgl.ai/



1826

Model BLEU Oracle DP

ASPEC Ja → En
Transformer Baseline 27.1 - 22.4
+Diverse Dec (Li et al., 2016) 26.9 - 26.2

+ Random Codes 27.0 - 4.9
+ Semantic Coding (BERT) 26.8 28.8 30.6
+ Semantic Coding (FastText) 27.3 28.5 31.1
+ Syntactic Coding 27.4 29.5 39.8

WMT14 De → En
Transformer Baseline 29.4 - 28.2
+Diverse Dec (Li et al., 2016) 29.1 - 31.0

+ Random Codes 29.5 - 3.8
+ Semantic Coding (BERT) 29.3 29.4 21.7
+ Semantic Coding (FastText) 28.5 29.2 28.8
+ Syntactic Coding 29.3 30.7 33.0

Table 1: Results for different approaches. The
BLEU(%) are reported for the first sampled candi-
date. Oracle BLEU scores are produced with reference
codes. Diversity scores (DP) are evaluated with Eq. 3.

4.3 Quantitive Evaluation of Diversity
As we are interested in the diversity among sam-
pled candidates, the diversity metric based on the
divergence between word distributions (Xu et al.,
2018) can not be applied in this case. In order
to qualitatively evaluate the diversity of generated
translations, we propose to use a BLEU-based dis-
crepancy metric. Suppose Y is a list of candidate
translations, we compute the diversity score with

DP(Y ) =
1

|Y |(|Y | − 1)
∑
y∈Y

∑
y′∈Y,y′ 6=y

1−∆(y, y′), (3)

where ∆(y, y′) returns the BLEU score of two
candidates. The equation gives a higher diversity
score when each candidate contains more unique
n-grams.

4.4 Experiment Results
We use Base Transformer architecture (Vaswani
et al., 2017) for all models. The results are summa-
rized in Table 1. We sample three candidates with
different models, and report the averaged diversity
score. The BLEU(%) is reported for the candi-
date with highest confidence (log-probability). A
detailed table with BLEU scores of all three can-
didates can be found in supplementary material,

Tg以上の温度で Iを消去できた。 (Japanese)

A

1. It is possible to eliminate I at

temperatures above Tg .

2. It is possible to eliminate I at

temperatures higher than Tg .

3. It is possible to eliminate I at the

temperature above Tg .

B

1. above Tg , I was able to be eliminated .

2. It was found that the photoresists were

eliminated at temperatures above Tg .

3. at the temperature above Tg , I was able

to be eliminated .

C

1. I could be eliminated at temperatures

above Tg .

2. I was removed at temperatures above Tg .

3. It was possible to eliminate I at

temperatures above Tg .

Table 2: A comparison of candidates produced by
beam search (A), semantic coding model based on
BERT (B) and syntactic coding model (C) in Ja-En task

where the BLEU scores of the second and third
candidates are on par with the baseline.

We compare the proposed approach to three
baselines. The first baseline samples three candi-
dates using standard beam search. We also tested
the diverse decoding approach (Li et al., 2016).
The coefficient γ is chosen to maximize the diver-
sity with no more than 0.5 BLEU degradation. The
third baseline uses random codes for conditioning.

As shown in the table, the model based on
BERT sentence embeddings achieves higher diver-
sity in ASPEC dataset, which contains only formal
texts. However, it fails to deliver similar results
in WMT14 dataset, which is more informal. This
may be due to the difficulty in clustering BERT
vectors which were never trained to work with
clustering. The model using FastText embeddings
is shown to be more robust across the datasets, al-
though it also fails to outperform the diverse de-
coding baseline in WMT14 dataset.

In contrast, syntax-based models achieve much
higher diversity in both datasets. We found the
results generated by this model has more diverse
structures rather than word choices. By compar-
ing the BLEU scores, no significant degradation is
observed in translation quality. As a control exper-
iment, using random codes does not contributes to
the diversity. As a confirmation that the sentence
codes have strong impact on sentence generation,
the models using codes derived from references
(oracle codes) achieve much higher BLEU scores.



1827

5 Analysis and Conclusion

Table 2 gives samples of the candidate translations
produced by the models conditioning on different
discrete codes, compared to the candidates pro-
duced by beam search. We can see that the can-
didate translations produced by beam search has
only minor grammatical differences. In contrast,
the translation results sampled with the syntactic
coding model have drastically different grammars.
By examining the results, we found the syntax-
based model tends to produce one translation in
active voice and another in passive voice.

To summarize, we show a diverse set of trans-
lations can be obtained with sentence codes when
a reasonable external mechanism is used to pro-
duce the codes. When a good syntax parser exists,
the syntax-based approach works better in terms of
diversity. The source code for extracting discrete
codes from parse trees will be publicly available.

Acknowledgement

The research results have been achieved by ”Re-
search and Development of Deep Learning Tech-
nology for Advanced Multilingual Speech Trans-
lation”, the Commissioned Research of National
Institute of Information and Communications
Technology (NICT), JAPAN. This work was par-
tially supported by JSPS KAKENHI Grant Num-
ber JP16H05872, Samsung Advanced Institute
of Technology (Next Generation Deep Learning:
from pattern recognition to AI) and Samsung Elec-
tronics (Improving Deep Learning using Latent
Structure).

References
Piotr Bojanowski, Edouard Grave, Armand Joulin, and

Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. CoRR, abs/1810.04805.

Unnat Jain, Ziyu Zhang, and Alexander G. Schwing.
2017. Creativity: Generating diverse questions us-
ing variational autoencoders. 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR), pages 5415–5424.

Lukasz Kaiser and Samy Bengio. 2018. Dis-
crete autoencoders for sequence models. CoRR,
abs/1801.09797.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.

Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
EMNLP.

Ilya Kulikov, Alexander H. Miller, Kyunghyun Cho,
and Jason Weston. 2018. Importance of a search
strategy in neural dialogue modelling. CoRR,
abs/1811.00907.

Jiwei Li, Will Monroe, and Daniel Jurafsky. 2016. A
simple, fast diverse decoding algorithm for neural
generation. CoRR, abs/1611.08562.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. Aspec: Asian
scientific paper excerpt corpus. In LREC.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In ACL, pages
529–533.

Matt Post. 2018. A call for clarity in reporting bleu
scores. In WMT.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. CoRR, abs/1508.07909.

Tianxiao Shen, Myle Ott, Michael Auli, and
Marc’Aurelio Ranzato. 2019. Mixture models for
diverse machine translation: Tricks of the trade.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in neural in-
formation processing systems, pages 801–809.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In ACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Qiongkai Xu, Juyan Zhang, Lizhen Qu, Lexing Xie,
and Richard Nock. 2018. D-page: Diverse para-
phrase generation. CoRR, abs/1808.04364.

http://arxiv.org/abs/1902.07816
http://arxiv.org/abs/1902.07816

