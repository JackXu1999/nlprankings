



















































Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 482–491
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Magnets for Sarcasm:
Making Sarcasm Detection Timely, Contextual and Very Personal

Aniruddha Ghosh
University College Dublin, Ireland.
Aniruddha.Ghosh@ucdconnect.ie

Tony Veale
University College Dublin, Ireland.

Tony.Veale@ucd.ie

Abstract

Sarcasm is a pervasive phenomenon in so-
cial media, permitting the concise commu-
nication of meaning, affect and attitude.
Concision requires wit to produce and wit
to understand, which demands from each
party knowledge of norms, context and a
speaker’s mindset. Insight into a speaker’s
psychological profile at the time of pro-
duction is a valuable source of context for
sarcasm detection. Using a neural archi-
tecture, we show significant gains in de-
tection accuracy when knowledge of the
speaker’s mood at the time of production
can be inferred. Our focus is on sarcasm
detection on Twitter, and show that the
mood exhibited by a speaker over tweets
leading up to a new post is as useful a cue
for sarcasm as the topical context of the
post itself. The work opens the door to an
empirical exploration not just of sarcasm
in text but of the sarcastic state of mind.

1 Introduction

Oscar Wilde memorably described sarcasm as “the
lowest form of wit but the highest form of intelli-
gence.” Though sarcasm lacks the sophistication
of irony, and does little to conceal the speaker’s
disdain for a target, it is a figurative device that re-
quires as much intelligence from its consumers as
its producers. The concision with which sarcasm
and irony allow speakers to conflate propositional
content and affective stance makes it a perva-
sive mode of communication in the 140-character
tweets of Twitter. By combining an overtly posi-
tive attitude with a meaning that is more deserv-
ing of scorn, sarcasm allows speakers to commu-
nicate disappointment about a state of affairs that
bites (or etymologically “cuts the flesh”) of an ad-

dressee. It conveys the feeling the speaker would
wish to experience (“I love it when ...”) with the
state of affairs that up-ends this feeling (“... my
friends forget my birthday”). It often combines
politeness with mockery to disguise the appear-
ance of hostility while heightening its effect on
a listener (Brown and Levinson, 1978; Dews and
Winner, 1995). It establishes a wry environment
(Dews and Winner, 1999) that has its roots in so-
cial norms and the speaker’s state of mind.

Psychological theories of irony, such as echoic
reminder theory (Kreuz and Glucksberg, 1989)
and implicit display theory (Utsumi, 2000b) have
yet to fully translate into text-analytic methods.
Neuropsychology researchers who have sought
patterns of brain activity to identify the neural cor-
relates of sarcasm note that an understanding of
sarcasm is highly dependent not just on the con-
text of an utterance but on the state-of-mind and
personality of the speaker, as well as on facial
expressions and prosody (Shamay-Tsoory et al.,
2005). Without the latter markers, purely textual
detection must depend largely on the content and
context of an utterance, though speaker personal-
ity and state-of-mind can also be approximated via
text-analytic means. Probabilistic classification
models that exploit textual cues – such as the jux-
taposition of positive sentiment and negative sit-
uations (Riloff et al., 2013), discriminative words
and punctuation marks (Davidov et al., 2010), and
emoticon usage (González-Ibánez et al., 2011) –
have achieved good performance across domains,
yet these models typically suffer from an absence
of psychological insight into a speaker and topi-
cal insight into the context of utterance production.
Kreuz and Link (2002) argue that the likelihood of
sarcasm is proportional to the amount of knowl-
edge shared by speaker and audience, which in-
cludes knowledge of the world and knowledge of
the speaker and audience. Personality is defined

482



by Olver and Mooradian (2003) as the “enduring
characteristics of the individual” though mood –
which is changeable – is perhaps just as useful if
sampled in a timely fashion. The difference be-
tween personality and mood can be likened to that
between climate and weather. Tausczik & Pen-
nebaker (2010) have developed a Twitter-based
mood analysis web service at AnalyzeWords.com
which uses a variety of psycholinguistic crite-
ria and the LIWC (Linguistic Inquiry and Word
Count) resource1 to quantify the recent mood – i.e.
the recent weather – of a user along 11 dimensions
ranging from Arrogance/Remoteness to Anger and
Analyticity. To exploit the stable personality of an
online user, Celli et al. (2016) sought a correla-
tion between Big Five personality traits (Costa and
McCrae, 2008) and the LIWC-quantifiable dimen-
sions found in re-tweets amongst Twitter users.
(Rajadesingan et al., 2015) have also shown how
relevant aspects of personality can be acquired
from a speaker’s past tweets. Since personality
and mood can each influence the detection pro-
cess, they underpin our first research question: To
what extent can the quantifiable dimensions of ei-
ther lead to a better understanding of sarcasm? Re-
liable detection depends as much on the context of
an utterance – which provides the motivation for
sarcasm – as its content. Consider e.g.:

Speaker Utterance: @MSNBC of course all of those jobs
will be in China
In reply to @realDonaldTrump: I will be the greatest jobs-
producing president that God ever created.

The speaker’s sarcastic intent cannot be grasped
without knowledge of the larger context. This is-
sue provides our second research question: How
can we usefully incorporate utterance context into
a neural network model of sarcasm detection? Sar-
casm is ubiquitous but always in flux, relying on
a changing swirl of socially relevant viewpoints.
The following tweet is sarcastic by virtue of its
echoic mockery of a widely ventilated opinion:

Time to get my Sunday dose of #fakenews from the failing
@nytimes.

This begs the third research question that we ex-
plore in the following sections: How can we train
our sarcasm detection model to exploit evolving
social norms and public opinions?

1https://liwc.wpengine.com/

2 Related Work and Ideas

Sarcasm has been extensively researched by lin-
guists and psychologists (Gibbs and Clark, 1992;
Gibbs and Colston, 2007; Kreuz and Glucksberg,
1989; Utsumi, 2000a), yet due to the limited avail-
ability of stimuli, sarcasm detection in text has re-
lied chiefly on the recognition of stock patterns
and lexical cues. Sarcasm often highlights failed
expectations by engaging in a pragmatic pretense
that is designed to be seen through (Campbell and
Katz, 2012), so cues such as interjections, intensi-
fiers, punctuation and markers of non-veridicality
and hyperbole play a crucial role in recognizing
sarcastic intent. Likewise, stock plaudits such as
“yay!” or “great!” are common in sarcastic prod-
uct reviews (Tsur et al., 2010), while hashtags such
as #sarcasm, as compressed vehicles for user in-
tent, are often used to self-annotate sarcastic texts
(Davidov et al., 2010). Liebrecht et al. (2013) used
topic-specific information and n-grams as discrim-
inative features, while (Lukin and Walker, 2013)
showed that phrases such as “no way”, “Oh re-
ally?” and “not so much” serve to flag a sarcastic
intent when used with specific linguistic patterns.

Capelli et al. (1990); Woodland and Voyer
(2011) suggest that contextual awareness is a nec-
essary precursor to identifying sarcasm. Sarcasm
is a response to a motivating context that appears
to force a rueful incongruity between a text and
its context. Exploiting the principle of inferabil-
ity (see Kreuz (1996)), Bamman and Smith (2015)
modeled shared common knowledge by extracting
features from context, the author, and the audi-
ence. Khattri et al. (2015) identified sarcasm by
seeking a strong contrast in affect toward named
entities in current vs. historical tweets, while Ra-
jadesingan et al. (2015) also exploited a contrast
in statistically-derived author traits across current
and historical tweets. Zhang et al. (2016) use sim-
ilar sources of contextual information to show the
effectiveness of a neural network over more tra-
ditional approaches involving manually-selected,
discrete features, claiming that automatic feature
induction can uncover more subtle markers of sar-
casm. Amir et al. (2016) argue that sarcasm de-
tection hinges on speaker modeling, and exploited
user embedding to quantify incongruity between
utterances and the behavioral traits of their au-
thors. These methods measure the disparity be-
tween an utterance and expectations arising from
knowledge of context or speaker or both together.

483



We build on this double-grounding for sarcasm
to improve detection in a neural network model
of sarcasm and thereby address our first two re-
search questions. We model the speaker at the time
of utterance production using mood indicators de-
rived from the most recent prior tweets, and model
context using features derived from the proximate
cause of the new utterance, the tweet to which
an utterance is a response. For our third research
question, we present a novel feedback-based anno-
tation scheme that engages authors of training/test
tweets in a process of explicit annotation, feeding
new examples back into the model. Section 3 out-
lines the kind and source of features exploited in
the model. Section 4 outlines our methods of data
collection and annotation. Section 5 presents the
neural network model, while section 6 & 7 present
our experimental set-up and analysis of results. Fi-
nally, section 8 offers some closing remarks.

3 Psychological dimensions and Sarcasm

We cannot perceive a user’s state-of-mind directly
on Twitter, but we might infer one’s current dis-
position from an analysis of recent tweets, as lin-
guistic expressions tend to be congruent with an
author’s state-of-mind (Campbell and Katz, 2012).
An informative if low-res psychological portrait is
sketched by web services such as AnalyzeWords
(Tausczik and Pennebaker, 2010), which analyzes
the most recent 1000-words or so of a Twitter user
using LIWC to score the user on 11-dimensions:
Upbeat, Worried, Angry, Depressed, Plugged in,
Personable, Arrogant, Spacy, Analytic, Sensory
and In-the-moment. Sarcasm is often perceptible
in the incongruity between utterance and context
(Joshi et al., 2015) but it can also be conveyed by
an incongruity between text and recent mood.

To understand the relationship between these 11
dimensions (each scored 0..100) and a propensity
for sarcasm, we performed a k-Nearest Neighbors
(KNN) clustering of the Twitter users that provide
the tweets of our sarcastic data set. The Analyze-
Words snapshot of each user was taken at the time
of that user’s tweet in the dataset. A value of 30 for
k was chosen empirically to ensure a decent size
for the clusters. By calculating Spearman corre-
lations between each group and the 11 Analyze-
Words dimensions, we estimated the affinity for
sarcasm of different dimensions. Unsurprisingly,
we observed that clusters showing a high correla-
tion with negative dimensions, such as Angry, also

tend to use positive expressions such as ’funny”
and ’wow” to mark sarcasm. Here is an example:

@realDonaldTrump They can all fit in your head? Wow!
Have you seen someone about this?

Unless one knows that @realDonaldTrump of-
ten elicits anger, or that the author scored 83 (of
100) for Angry, this tweet might seem quite posi-
tive. Valence shifters such as “not” might also sug-
gest literal positivity if not for the implicit anger of
the author. At the time of the following tweet An-
alyzeWords scored its author as Angry=98.

@realdonaldtrump funny the founder of the birther move-
ment is saying that he’s not racist #trumpbirther

Polarizing figures such as @realDonaldTrump
are magnets for sarcasm on Twitter. By identifying
these magnets, we can better detect the sarcasm of
a tweet that offers plaudits for negative qualities.
We use AnalyzeWords to obtain the popular affec-
tive feelings for common addressees by averaging
the affective dimensions of the users that tweet
at them. The top 5 magnets for sarcasm in our
data-set of 18K sarcastic tweets are @hillaryClin-
ton, @realDonaldTrump, @bernieSanders, @AP
and @megynKelly. Of these, @hillaryClinton is
the biggest target for Angry tweets while @meg-
ynKelly is the biggest target for Analytic tweets.

Addresses in the political domain score high for
both angry tweets and analytic tweets: people an-
alyze the news and shoot the messenger. We see
much less analyticity – a tendency to use com-
plex expressions linked with logical connectives
– in tweets about popular entertainers. To mock
such targets, users tend to use affective words that
contrast with overall public opinion. The magnet
with the highest mean Angry score for the tweets
that target him is @realDonaldTrump, yet 63% of
the affective words in the tweets that target him in
the data-set are positive. Knowing that @realDon-
aldTrump is a magnet for anger can help a sarcasm
detector overcome this positive bias.

4 Dataset Construction

Tweets with sarcastic intent are often misclassi-
fied due to a lack of shared context or knowledge
between speaker and annotator. Opposing social
beliefs and a dearth of topical or personal knowl-
edge can lead to serious misjudgments. Relevant
tweet sets can be harvested by searching sarcasm
specific hashtags (e.g. #sarcasm). This approach

484



overlooks tweets that are not explicitly tagged as
sarcastic by their authors. Thus we have devised a
feedback-based system that contacts tweet authors
directly after-the fact to ask for their authoritative
self-annotations for a potentially sarcastic tweet.

4.1 Data collection
To collect annotations from authors for their own
tweets, we used a Twitterbot named @onlinesar-
casm to exploit the “retweet with comment” func-
tion in Twitter. The bot chooses randomly from
tweets that are addressed to any of 700 top Twitter
users (as listed by TwitterCounter.com), as we ex-
pect high-profile figures to be magnets for sarcasm
from others. The bot retweets a chosen tweet (si)
to its author, appending a yes/no question (qi) as a
comment to elicit a reply.

At the time of retweeting (si), the 11 Analyze-
Words.com dimensions (awi) of the tweet’s author
(ui) are saved, along with the context tweet (sj) by
author (uj) that provoked (si). Authors respond to
the bot by favoriting/retweeting the bot’s request
or via a reply (rei) containing #Yes or #No. Au-
thor responses often contain more than a simple
#Yes or #No response, and so, after observing a
series of responses the following linguistic rules
were used to extract the training annotations:

• If the number of retweets (ri) or likes (li) for
qi is non-zero or rei contains #Yes, then si is
deemed positive for sarcasm.

• If rei contains #No or an explicit mention of
’not sarcastic’ or ’no sarcasm’ or ’truth’, then
si is deemed negative for sarcasm.

We discarded any si lacking a context tweet sj .
Using author feedback, a data set of 40K tweets
was collected, comprising 18K tweets acknowl-
edged as sarcastic and 22K deemed non-sarcastic.
For another test set, we collected 1200 tweets: 550
tweets acknowledged as sarcastic by their authors
and 650 acknowledged to be non-sarcastic.

4.2 External datasets
In addition to our own training and test sets, whose
annotations come directly from tweet authors, we
also used 5 Twitter datasets where tweet informa-
tion, fetched by tweet identifier, contains identifier
of context tweet (Ptáček et al., 2014; Bamman and
Smith, 2015; Rajadesingan et al., 2015; Cliche,
2014), from which motivating contexts can be dis-
cerned for each. (This contextual requirement pre-

vents us from considering even more of the avail-
able sarcasm datasets.) For the context tweets sj
for each si in these sets we collected the most re-
cent linked tweets of si. To obtain the 11 Ana-
lyzeWords.com dimensions for tweet authors, we
collect the 50 tweets of ui posted just prior to si,
and use the LIWC to estimate the 11 dimensions
(Anger, Arrogance, etc.) from those tweets. As
AnalyzeWords.com does not provide retrospective
analyses, and as its code is not public, we reverse-
engineered a substitute using the LIWC by follow-
ing the creators’ guidelines in (Tausczik and Pen-
nebaker, 2010). For subsequent evaluations, the
5 external datasets were split into 3 parts each:
80% for training, 10% for development/tuning,
and 10% for testing.

5 The Neural Network Model

Ghosh and Veale (2016) described an Artificial
Neural Network (ANN) model built around layers
of CNNs (Convolutional Neural Networks) and
LSTMs (Long Short Term Memory) for sarcasm
detection to efficiently capture contrasting text sig-
nals of sarcasm within a tweet. We build here on
this model as shown in Fig.1, adding input fea-
tures for the psychological profile of the author
and the context of the tweet to those for the tweet
itself. The LSTM layer (Hochreiter and Schmid-
huber, 1997) captures dependencies amongst non-
adjacent contrasting signals for sarcasm within
each si. We extend this architecture to include a
context tweet sj for each si, but instead of con-
catenating sj and si at the input layer, we stitch
them together after the LSTM layer. The text input
layer is initialized with embeddings from Google’s
Word2Vec model (Mikolov et al., 2013) with a di-
mension setting of 300. To further integrate fea-
tures reflecting the state of mind of the speaker
at utterance-time, the values awi (i = 1...11) for
each si are concatenated with the feature vector of
sj & si in the merge layer. We use a bi-directional
LSTM (BLSTM) and forego a maxpooling layer
to increase throughput to the BLSTM. We prevent
overfitting using a dropout layer with a dropout
rate of 0.25 after the BLSTM layers. The con-
catenation layer combines the feature maps of the
source and context tweets (si & sj) along with a
vector of aw1...11 for the author ui. The concate-
nation yields a merge layer of size <f(2(|s|m+1)+l)
where f , s, m and l are, respectively, the num-
ber of BLSTM units, the length of the input se-

485



Figure 1: A Neural Architecture for Detecting Sarcasm in Contextualized Utterances

quence, the width of the CNN filter and the length
of aw. Notice that the features for a tweet si and
its immediate context sj – which we consider the
proximate cause of the sarcasm (if any) in si – are
concatenated only after they have passed through
separate sets of CNN and LSTM layers (CNN1 +
BLSTM1 and CNN2 + BLSTM2). It is important
to keep a tweet and its context separate for as long
as possible, as the model is designed to recognize
an inherent incongruity between each. This incon-
gruity becomes diffuse if the inputs are combined
too soon. EAW is the embedding layer for the 11
AnalyzeWords dimensions; it combines the vectors
of sj , si and aw, and passes the concatenated fea-
tures to a Deep Neural Network (DNN) to discrim-
inate both classes (sarcasm vs. non-sarcasm). The
code2 is developed using Keras3.

6 Evaluation and Experimental Setup

Success with a neural architecture requires apt in-
put features and an equally apt selection of hyper-
parameters. After performing a grid search over
hyper-parameters, the best configuration of the
CNN, LSTM and DNN layers places 1280 hidden
memory units into each layer and uses a CNN fil-
ter width of 3. A simple baseline will use only the
textual content of a tweet si without a context sj
or an affective profile aw of the author ui. To ap-
preciate the contribution of different input sources
of information we trained the network on different
combinations of these sources.

2https://github.com/AniSkywalker/SarcasmDetection
3https://keras.io/

6.1 Addressee information

If si is addressed to uj , this information can pro-
vide additional insights into si’s tone. In the
TTIA (Target Tweet Including Addressee) setting
the name of the addressee (but not an estimation of
the public opinion of the addressee, as so few ad-
dresses are actually famous) is added to the base-
line along with si. If the addressee is a magnet for
sarcasm, aspects of this magnetism should still im-
press themselves on the network during training.

6.2 Contextual Information

In a variant of the baseline called CT (Context
Tweet), the features of the tweet sj to which the
target si is a response are also added as inputs to
the model, to be stitched together with the features
of si at the concatenation layer. Changes in perfor-
mance with and without CT will allow us to esti-
mate the value of context in sarcasm detection.

6.3 Author Profile Information

The 11-dimensional AnalyzeWords snapshot aw
for author ui at the time si is posted offers valu-
able insights into the intent of ui. In the PD (Psy-
chological Dimensions) configuration, the 11 af-
fective dimensions awi are added to the model.
They pass through an embeddings layer to be com-
bined with utterance (and possibly context) fea-
tures at the concatenation layer. To determine the
relative contribution of each dimension awi to de-
tection competence, we trained the model in two
extra modes. In the first, we fed the model with
false values for each awi, varying values from 0 to

486



100, to observe the effects on accuracy when e.g.
Angry is over- or under-estimated for ui. In the
second extra mode, we excised each awi, one at a
time in different training runs, to quantify its lack
on the model.

6.4 Automatic adaptation

Online sarcasm is often used to comment on the
vagaries of politics and current affairs. As topical-
ity is of the essence, we expect a model that regu-
larly acquires new author-annotated training data
to bootstrap itself will adapt better to the times
and yield better results. To estimate the benefits of
bootstrapping we tested the model on an evolving
version of the data-set that acquired new training
data each week for a month in August 2016.

7 Results & Analysis

Table 3 shows the recall (R), precision (P) and f-
score (F1) for our model, called Sarcasm Magnet,
with alternate configurations on different datasets.
The configuration for each setup is given in the
second row (e.g. the addition of context tweets
requires the use of two LSTMs and two CNNs).
Setup TTEA is the baseline which uses only the
text of a target tweet; it excludes addressee han-
dles, context tweets (CT) and the psychological
dimensions (PD) of authors. Setup TTIA adds ad-
dressee handles to the baseline to give our model
a small boost, mostly in recall. Setup TTEA+CT
adds context tweets to the baseline, yielding a sig-
nificant boost since a good deal of sarcasm is con-
versational in nature. In this setup the most sig-
nificant improvement in recall was observed with
the Bamman dataset (Bamman and Smith, 2015).
In setup TTIA+CT, which uses context and ad-
dressee handles, no significant improvement over
TTEA+CT is observed, except for precision on
Bamman’s dataset. In setup TTEA+PD, the affec-
tive profile of each author at tweet-time is added
to the baseline to yield a significant boost in per-
formance almost as large as that for TTIA+CT.
Setup TTIA+CT+PD includes all available infor-
mation sources (addressee, content, and psycho-
logical profile). This column reports (in paren-
theses) the performance on each dataset by the
dataset creator’s own system, which is either pub-
licly available or re-implemented from their paper.
The results show that Sarcasm Magnet beats the
state of the art for these data-sets.

Table 1 shows the effect on the model’s perfor-

Dimension omit-
ted

Precision Recall F-score

None omitted 0.9 0.89 0.9
Sensory 0.85 0.93 0.89
Plugged In 0.84 0.84 0.84
Depressed 0.78 0.96 0.86
Angry 0.81 0.95 0.87
Spacy/Valley girl 0.78 0.97 0.87
Worried 0.79 0.97 0.87
Arrogant/Distant 0.84 0.85 0.84
Analytic 0.86 0.83 0.84
In-the-moment 0.84 0.86 0.85
Upbeat 0.86 0.91 0.88
Personable 0.87 0.88 0.88

Table 1: Performance of the model when a specific
dimension awi is omitted from training.

mance in the absence of specific awi values. A
boost in recall and a drop in precision shows the
bias of the model shifting towards sarcasm when
the space of non-sarcastic tweets overlaps with
that of sarcastic tweets in the absence of an awi
that confirms literal intent. So political tweets may
be mis-classified as sarcasm in the absence of val-
ues for Angry, Depressed, and Worried, suggest-
ing that sarcastic authors often seem less angry,
depressed or worried. A drop in precision and
recall when Arrogant/Remote, Analytic, Plugged
in and In-the-moment dimensions are absent sug-
gests sarcastic people to be more socially active
and aware, and smarter but more arrogant.

7.1 A Tale of Two Contexts

The CT and PD additions each bring significant
improvements in F-score, yet when added jointly
they bring no significant increases over either used
individually. For each is a form of context drawn
from different sources that reflects different intu-
itions but which ultimately offers much the same
insights. The impact of the 11 aw dimensions
is lower on the 5 external datasets than for the
new feedback-based dataset, no doubt because the
AnalyzeWords.com snapshot of authors in the lat-
ter could be taken directly at tweet-time, whilst
for the former it was retrospectively approximated
using our own jerry-rigged version based on the
LIWC. If the official web service were to allow
retrospective analyses of Twitter users at specific
times we are confident the improvements on the
external datasets would mirror those on our own
dataset. For now it is interesting to note the ef-
fectiveness of the AnalyzeWords.com service at af-
fectively profiling Twitter users at specific times,

487



which is to say, at specific contexts in their Twitter
time-lines.The service boils down the most recent
tweets (approx. 1000 words in total) to 11 dimen-
sions that are more than simple functions of the
lexical scores in the LIWC. Rather, it analyzes the
selected text as a coherent product of a coherent
mind-set, to measure a local propensity for hostil-
ity, optimism, depression, emotional detachment
and preference for reason. To use our earlier anal-
ogy, AnalyzeWords.com forecasts the psychologi-
cal weather around an author, not the user’s stable
climate. Though we may often speak of a “sarcas-
tic personality” as a stable aspect of some speak-
ers, most users of sarcasm will not fall into this
category. As such, insight into the recent mind-
set of an author is more valuable to a detector than
knowledge of one’s personality overall.

7.2 Rolling With The Punches

Our feedback-annotated dataset was collected dur-
ing a fertile period for sarcasm online: the heights
of the 2016 US presidential campaign. The main
body of the new dataset was collected and anno-
tated (as described earlier) in the early summer of
2016. During the month of August we acquired
additional annotated training data in four weekly
tranches, to incrementally retrain the model to an
evolving political and social context. As shown in

Week Precision Recall F-score
Week 1 0.751 0.752 0.752
week 2 0.790 0.752 0.771
Week 3 0.798 0.775 0.786
Week 4 0.839 0.869 0.85

Table 2: Bootstrapping gains (August, 2016)

Table 2, each weekly tranche of extra training data
yielded increased dividends in terms of F-score
and precision or recall when evaluating the model
on the same test set (of 1,200 tweets, 550 sar-
castic and 650 non-sarcastic). The new annotated
data harvested in the final week of August yielded
the biggest dividends, especially in Recall, per-
haps in a reflection of the growing bitterness of the
campaign and of frantic campaigning in (and on-
line commentary about) the pivotal swing states.
As the candidates were revealing more of them-
selves to voters, the voters were revealing more of
themselves to our model. Specifically, in week 1,
4719 sarcastic and 5361 non-sarcastic tweets were
added for training; in week 2, an additional 3179
and 6901 were added; in week 3, 3571 and 6509;

and in a week 4 reversal, 6504 and 3574 tweets.

8 Conclusions & Future work

Context is vital to the understanding of the fruits of
any figurative device, whether metaphor, irony or
sarcasm. We have explored two sources of contex-
tual information in this work: the linguistic con-
text of the utterance itself – which we take to be
another utterance that is the proximate cause of the
text under consideration – and the psychological
context of the utterance’s author – which we take
to be the mind-set that is apparent in the author’s
most recent writings on Twitter. Each source of
context is ultimately grounded in a text and un-
derstood in text-analytic terms. It is perhaps not
so surprising then that each kind of context yields
similarly large improvements to a neural model of
sarcasm detection when added in isolation, but no
large improvements over either alone when both
are combined in a single model.

This work makes three principle contributions
to the computational analysis of sarcasm. First,
as outlined above, it shows how different kinds of
context – from the linguistic to the psychological
– can be usefully incorporated to yield improved
detection. Second, it shows how accurate anno-
tation of training data can be automated on Twit-
ter by going directly to the source of each training
text, to obtain a definitive answer as to its figura-
tive status. So the resulting neural model does not
learn to approximate the reasoning of independent
human annotators but the mind-set and intent of
the authors themselves. Thirdly, and perhaps most
usefully for future work by others, this feedback-
based dataset will be made available for use by
other researchers and in other evaluations. Im-
portantly, this dataset is not merely a collection of
yes/no annotated texts, even if the yeses and nos
come from authoritative sources. For each text in
the dataset, we can provide the linguistic context
to which it is a response, and furthermore, we can
provide a psychological snapshot of the author at
the time the tweet was posted on Twitter. In the
end we believe this is the most valuable contribu-
tion of the work, as it will allow others to incorpo-
rate an understanding of personality and mind-set
into their own models of that most personal and
moody of figurative devices, sarcasm.
Acknowledgements: This work was funded by
Science Foundation Ireland (SFI) via the ADAPT
centre for Digital Content Technology.

488



D
at

as
et

A
lte

rn
at

e
C

on
fig

ur
at

io
ns

of
th

e
Sa

rc
as

m
M

ag
ne

t*
m

od
el

T
T

E
A

T
T

IA
T

T
E

A
+

C
T

T
T

IA
+

C
T

T
T

E
A

+
PD

T
T

IA
+

PD
T

T
IA

+
C

T
+

PD

C
N

N
1

C
N

N
1

C
N

N
1

+
C

N
N

2
C

N
N

1
+

C
N

N
2

C
N

N
1

+
L

ST
M

1
C

N
N

1
+

L
ST

M
1

C
N

N
1

+
C

N
N

2

+
L

ST
M

1
+

L
ST

M
1

+
L

ST
M

1
+

L
ST

M
2

+
L

ST
M

1
+

L
ST

M
2

+
E

A
W

+
C

L
+

E
A

W
+

C
L

+
L

ST
M

1
+

L
ST

M
2

+
D

N
N

+
D

N
N

+
C

L
+

D
N

N
+

C
L

+
D

N
N

+
D

N
N

+
D

N
N

+
E

A
W

+
C

L
+

D
N

N

(P
tá

če
k

et
al

.,
20

14
)

(b
al

an
ce

d
da

ta
se

t)

P
0.

82
1

0.
83

2
0.

90
8

0.
92

0.
85

7
0.

86
0.

94
7

R
0.

82
1

0.
83

2
0.

90
8

0.
92

0.
85

7
0.

86
0.

94
7

(S
:1

5K
,N

S:
17

K
)

F1
0.

82
1

0.
83

2
0.

90
8

0.
92

0.
85

7
0.

86
0.

94
72

(0
.9

46
6)

(P
tá

če
k

et
al

.,
20

14
)

(u
nb

al
an

ce
d

da
ta

se
t)

P
0.

81
4

0.
81

3
0.

92
6

0.
93

7
0.

85
1

0.
84

3
0.

94
6

R
0.

83
2

0.
83

3
0.

93
0.

93
0.

83
3

0.
83

8
0.

93
3

(S
:1

5K
,N

S:
39

K
)

F1
0.

82
3

0.
82

3
0.

92
8

0.
93

3
0.

84
2

0.
84

0.
94

(0
.9

24
)

(B
am

m
an

an
d

Sm
ith

,2
01

5)

P
0.

89
6

0.
90

0.
88

6
0.

91
9

0.
82

5
0.

83
5

0.
9

(0
.8

57
)

R
0.

65
1

0.
67

2
0.

81
9

0.
81

7
0.

80
3

0.
82

7
0.

85
8

(0
.8

72
)

(S
:8

K
,N

S:
7K

)
F1

0.
75

4
0.

77
0.

85
1

0.
86

5
0.

81
4

0.
83

1
0.

87
8

(0
.8

64
)

(C
lic

he
,2

01
4)

P
0.

78
8

0.
8

0.
87

4
0.

89
3

0.
88

4
0.

88
3

0.
89

6

R
0.

75
1

0.
76

9
0.

84
2

0.
84

3
0.

81
2

0.
81

7
0.

86
2

(S
:5

0K
,N

S:
10

0K
)

F1
0.

76
9

0.
78

4
0.

85
8

0.
86

7
0.

84
6

0.
84

9
0.

87
9

(0
.6

)

(R
aj

ad
es

in
ga

n
et

al
.,

20
15

)

P
0.

95
7

0.
95

7
0.

95
7

0.
95

7
0.

95
8

0.
95

8
0.

95
6

R
0.

80
7

0.
80

7
0.

80
7

0.
80

7
0.

86
1

0.
86

1
0.

90
5

(S
:6

K
,N

S:
6K

)
F1

0.
87

5
0.

87
5

0.
87

5
0.

87
5

0.
90

7
0.

90
7

0.
93

(0
.9

03
)

Sa
rc

as
m

M
ag

ne
t*

(t
hi

s
pa

pe
r)

P
0.

73
3

0.
73

1
0.

84
0.

84
1

0.
84

6
0.

85
3

0.
90

R
0.

71
7

0.
73

2
0.

83
3

0.
85

8
0.

85
2

0.
86

1
0.

89

F1
0.

72
5

0.
73

2
0.

83
6

0.
84

9
0.

84
8

0.
85

6
0.

90

Ta
bl

e
3:

E
va

lu
at

io
n

of
Sa

rc
as

m
M

ag
ne

t(
P

-p
re

ci
si

on
,R

-r
ec

al
l,

F1
-f

-s
co

re
,T

T
E

A
-t

ar
ge

tt
w

ee
te

xc
lu

di
ng

ad
dr

es
se

e;
T

T
IA

-t
ar

ge
tt

w
ee

ti
nc

lu
di

ng
ad

dr
es

se
e;

C
T

-
C

on
te

xt
Tw

ee
t;

PD
-

Ps
yc

ho
lo

gi
ca

ld
im

en
si

on
s;

S
-

sa
rc

as
tic

;N
S

-
no

n-
sa

rc
as

tic
).

A
ll

re
su

lts
ar

e
fo

r
th

e
Sa

rc
as

m
M

ag
ne

tm
od

el
;w

he
n

av
ai

la
bl

e,
re

su
lts

ob
ta

in
ed

by
ot

he
ra

ut
ho

rs
on

th
ei

ro
w

n
da

ta
se

ts
ar

e
in

pa
re

nt
he

se
s.

*S
ar

ca
sm

M
ag

ne
ti

s
th

e
na

m
e

of
th

e
cu

rr
en

ts
ys

te
m

an
d

its
as

so
ci

at
ed

da
ta

se
t.

489



References
Silvio Amir, Byron C Wallace, Hao Lyu, and Paula

Carvalho Mário J Silva. 2016. Modelling context
with user embeddings for sarcasm detection in so-
cial media. arXiv preprint arXiv:1607.00976 .

David Bamman and Noah A Smith. 2015. Contex-
tualized sarcasm detection on twitter. In Ninth
International AAAI Conference on Web and So-
cial Media. http://homes.cs.washington.edu/ na-
smith/papers/bamman+smith.icwsm15.pdf.

Penelope Brown and Stephen C Levinson. 1978.
Universals in language usage: Politeness
phenomena. Cambridge University Press.
http://www.mpi.nl/publications/escidoc-66660/.

John D Campbell and Albert N Katz. 2012. Are there
necessary conditions for inducing a sense of sar-
castic irony? Discourse Processes 49(6):459–480.
http://dx.doi.org/10.1080/0163853X.2012.687863.

Carol A Capelli, Noreen Nakagawa, and Cary M
Madden. 1990. How children understand
sarcasm: The role of context and intona-
tion. Child Development 61(6):1824–1841.
https://www.jstor.org/stable/1130840.

Fabio Celli, Arindam Ghosh, Firoj Alam, and Giuseppe
Riccardi. 2016. In the mood for sharing contents:
Emotions, personality and interaction styles in the
diffusion of news. Information Processing & Man-
agement 52(1):93–98.

Mathieu Cliche. 2014. The sarcasm detector. http:
//www.thesarcasmdetector.com/.

Paul T Costa and Robert R McCrae. 2008. The re-
vised neo personality inventory (neo-pi-r). The
SAGE handbook of personality theory and assess-
ment 2:179–198.

Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010. Semi-supervised recognition of sarcasm
in twitter and amazon. In Proceedings of the
Fourteenth Conference on Computational Natu-
ral Language Learning. Association for Computa-
tional Linguistics, Uppsala, Sweden, pages 107–
116. http://www.aclweb.org/anthology/W10-2914.

Shelly Dews and Ellen Winner. 1995. Mut-
ing the meaning a social function of
irony. Metaphor and Symbol 10(1):3–19.
http://dx.doi.org/10.1207/s15327868ms1001 2.

Shelly Dews and Ellen Winner. 1999. Obligatory pro-
cessing of literal and nonliteral meanings in verbal
irony. Journal of pragmatics 31(12):1579–1599.
http://psycnet.apa.org/psycinfo/1999-01341-003.

Aniruddha Ghosh and Tony Veale. 2016. Fracking
sarcasm using neural network. In Proceedings of
NAACL-HLT . pages 161–169.

Deanna W. Gibbs and Herbert H. Clark. 1992. Coordi-
nating beliefs in conversation. Journal of Memory
and Language 31(2):183–194. doi:10.1016/0749-
596X(92)90010-U.

Raymond W. Gibbs and Herbert L. Colston.
2007. Irony in language and thought: A
cognitive science reader. Psychology Press.
https://books.google.ie/books?isbn=0805860622.

Roberto González-Ibánez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in
twitter: a closer look. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies: short papers-Volume 2. Association
for Computational Linguistics, pages 581–586.
http://www.aclweb.org/anthology/P11-2102.

Sepp Hochreiter and Jürgen Schmidhu-
ber. 1997. Long short-term memory.
Neural computation 9(8):1735–1780.
http://dl.acm.org/citation.cfm?id=1246450.

Aditya Joshi, Vinita Sharma, and Pushpak Bhat-
tacharyya. 2015. Harnessing context in-
congruity for sarcasm detection. In As-
sociation for Computational Linguistics
Volume 2: Short Papers. pages 757–762.
https://www.aclweb.org/anthology/P/P15/P15-
2124.pdf.

Anupam Khattri, Aditya Joshi, Pushpak Bhat-
tacharyya, and Mark Carman. 2015. Your sentiment
precedes you: Using an author’s historical tweets to
predict sarcasm. In Proceedings of the 6th Workshop
on Computational Approaches to Subjectivity, Sen-
timent and Social Media Analysis. Association for
Computational Linguistics, Lisboa, Portugal, pages
25–30. http://aclweb.org/anthology/W15-2905.

Roger J Kreuz. 1996. The use of verbal irony: Cues
and constraints. Metaphor: Implications and appli-
cations pages 23–38.

Roger J Kreuz and Sam Glucksberg. 1989. How to
be sarcastic: The echoic reminder theory of verbal
irony. Journal of Experimental Psychology: Gen-
eral 118(4):374. http://dx.doi.org/10.1037/0096-
3445.118.4.374.

Roger J Kreuz and Kristen E Link. 2002. Asym-
metries in the use of verbal irony. Journal of
Language and Social Psychology 21(2):127–143.
http://dx.doi.org/10.1177/02627X02021002002.

Christine Liebrecht, Florian Kunneman, and An-
tal Van den Bosch. 2013. The perfect solu-
tion for detecting sarcasm in tweets #not. In
Proceedings of the 4th Workshop on Computa-
tional Approaches to Subjectivity, Sentiment and
Social Media Analysis. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 29–37.
http://www.aclweb.org/anthology/W13-1605.

490



Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for on-
line dialogue. In Proceedings of the Workshop on
Language Analysis in Social Media. Association for
Computational Linguistics, Atlanta, Georgia, pages
30–40. http://www.aclweb.org/anthology/W13-
1104.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed
representations of words and phrases and their
compositionality. In Advances in neural infor-
mation processing systems. pages 3111–3119.
https://papers.nips.cc/paper/5021-distributed-
representations-of-words-and-phrases-and-their-
compositionality.pdf.

James M. Olver and Todd A. Mooradian. 2003.
Personality traits and personal values: a con-
ceptual and empirical integration. Personality
and Individual Differences 35(1):109 – 125.
https://doi.org/http://dx.doi.org/10.1016/S0191-
8869(02)00145-9.

Tomáš Ptáček, Ivan Habernal, and Jun Hong.
2014. Sarcasm detection on czech and en-
glish twitter. In Proceedings of COLING 2014,
the 25th International Conference on Compu-
tational Linguistics: Technical Papers. Dublin
City University and Association for Computa-
tional Linguistics, Dublin, Ireland, pages 213–223.
http://www.aclweb.org/anthology/C14-1022.

Ashwin Rajadesingan, Reza Zafarani, and Huan Liu.
2015. Sarcasm detection on twitter: A be-
havioral modeling approach. In Proceedings of
the Eighth ACM International Conference on Web
Search and Data Mining. ACM, pages 97–106.
http://dl.acm.org/citation.cfm?id=2685316.

Ellen Riloff, Ashequl Qadir, Prafulla Surve,
Lalindra De Silva, Nathan Gilbert, and Rui-
hong Huang. 2013. Sarcasm as contrast be-
tween a positive sentiment and negative situa-
tion. In Empirical Methods on Natural Lan-
guage Processing. volume 13, pages 704–714.
http://www.anthology.aclweb.org/D/D13/D13-
1066.pdf.

SG Shamay-Tsoory, Rachel Tomer, and Judith
Aharon-Peretz. 2005. The neuroanatomical basis
of understanding sarcasm and its relationship to
social cognition. Neuropsychology 19(3):288.
http://www.apa.org/pubs/journals/releases/neu-
193288.pdf.

Yla R Tausczik and James W Pennebaker. 2010.
The psychological meaning of words: Liwc and
computerized text analysis methods. Journal
of language and social psychology 29(1):24–54.
http://dx.doi.org/10.1177/0261927X09351676.

Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm-a great catchy name: Semi-supervised recog-

nition of sarcastic sentences in online product re-
views. In Proceedings of the Fourth International
Conference on Weblogs and Social Media.

A. Utsumi. 2000a. Verbal irony as implicit display
of ironic environment: Distinguishing ironic ut-
terances from nonirony. Journal of Pragmatics
http://www.utm.se.uec.ac.jp/ utsumi/paper/jop2000-
utsumi.pdf.

Akira Utsumi. 2000b. Verbal irony as implicit dis-
play of ironic environment: Distinguishing ironic
utterances from nonirony. Journal of Pragmatics
32(12):1777–1806.

Jennifer Woodland and Daniel Voyer. 2011. Con-
text and intonation in the perception of sar-
casm. Metaphor and Symbol 26(3):227–239.
http://dx.doi.org/10.1080/10926488.2011.583197.

Meishan Zhang, Yue Zhang, and Guohong Fu.
2016. Tweet sarcasm detection using deep neu-
ral network. In Proceedings of COLING 2016,
the 26th International Conference on Computa-
tional Linguistics: Technical Papers. The COL-
ING 2016 Organizing Committee, pages 2449–
2460. http://aclweb.org/anthology/C16-1231.

491


