



















































Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1349–1354,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1349

Simpler and Faster Learning of Adaptive Policies
for Simultaneous Translation

Baigong Zheng 1,∗ Renjie Zheng 2,∗ Mingbo Ma 1 Liang Huang 1,2
1Baidu Research, Sunnyvale, CA, USA

2Oregon State University, Corvallis, OR, USA
{baigongzheng, mingboma}@baidu.com zrenj11@gmail.com

Abstract

Simultaneous translation is widely useful but
remains challenging. Previous work falls into
two main categories: (a) fixed-latency poli-
cies such as Ma et al. (2019) and (b) adap-
tive policies such as Gu et al. (2017). The for-
mer are simple and effective, but have to ag-
gressively predict future content due to diverg-
ing source-target word order; the latter do not
anticipate, but suffer from unstable and ineffi-
cient training. To combine the merits of both
approaches, we propose a simple supervised-
learning framework to learn an adaptive pol-
icy from oracle READ/WRITE sequences gen-
erated from parallel text. At each step, such
an oracle sequence chooses to WRITE the
next target word if the available source sen-
tence context provides enough information to
do so, otherwise READ the next source word.
Experiments on German↔English show that
our method, without retraining the underlying
NMT model, can learn flexible policies with
better BLEU scores and similar latencies com-
pared to previous work.

1 Introduction

Simultaneous translation outputs target words
while the source sentence is being received, and is
widely useful in international conferences, negoti-
ations and press releases. However, although there
is significant progress in machine translation (MT)
recently, simultaneous machine translation is still
one of the most challenging tasks. This is because
it is hard to balance translation quality and latency,
especially for the syntactically divergent language
pairs, such as English and Japanese.
Researchers previously study simultaneous

translation as a part of real-time speech-to-speech
translation system (Yarmohammadi et al., 2013;
Bangalore et al., 2012; Fügen et al., 2007; Sridhar

∗These authors contributed equally.

et al., 2013; Jaitly et al., 2016; Graves et al., 2013).
Recent simultaneous translation research focuses
on obtaining a strategy, called a policy, to decide
whether to wait for another source word (READ
action) or emit a target word (WRITE action). The
obtained policies fall into two main categories:
(1) fixed-latency policies (Ma et al., 2019; Dalvi
et al., 2018) and (2) context-dependent adaptive
policies (Grissom II et al., 2014; Cho and Esipova,
2016; Gu et al., 2017; Alinejad et al., 2018;
Arivazhagan et al., 2019; Zheng et al., 2019a). As
an example of fixed-latency policies, wait-k (Ma
et al., 2019) starts by waiting for the first few
source words and then outputs one target word
after receiving each new source word until the
source sentence ends. It is easy to see that this
kind of policy will inevitably need to guess the
future content, which can often be incorrect.
Thus, an adaptive policy (see Table 1 as an
example), which can decides on the fly whether
to take READ action or WRITE action, is more
desirable for simultaneous translation. Moreover,
the widely-used beam search technique become
non-trivial for fixed policies (Zheng et al., 2019b).
To represent an adaptive policy, previous work

shows three different ways: (1) a rule-based de-
coding algorithm (Cho and Esipova, 2016), (2)
an original MT model with an extended vocabu-
lary (Zheng et al., 2019a), and (3) a separate pol-
icy model (Grissom II et al., 2014; Gu et al., 2017;
Alinejad et al., 2018; Arivazhagan et al., 2019).
The decoding algorithm (Cho and Esipova, 2016)
applies heuristicsmeasures and does not exploit in-
formation in the hidden representation, while the
MT model with an extended vocabulary (Zheng
et al., 2019a) needs guidance from a restricted dy-
namic oracle to learn an adaptive policy, whose
size is exponentially large so that approximation
is needed. A separate policy model could avoid
these issues. However, previous policy-learning



1350

German Ich bin mit dem Bus nach Ulm gekommen
Gloss I am with the bus to Ulm come
Action R W R R R R W W W R R R W W W W

Translation I took the bus to come to Ulm
Table 1: An example for READ/WRITE action sequence. R represents READ and W represents WRITE.

methods either depends on reinforcement learn-
ing (RL) (Grissom II et al., 2014; Gu et al., 2017;
Alinejad et al., 2018), which makes the training
process unstable and inefficient due to exploration,
or applies advanced attention mechanisms (Ari-
vazhagan et al., 2019), which requires its train-
ing process to be autoregressive, and hence in-
efficient. Furthermore, each such learned policy
cannot change its behaviour according to different
latency requirements at testing time, and we will
need to train multiple policy models for scenarios
with different latency requirements.
To combine the merits of fixed and adaptive

policies, and to resolve the mentioned drawbacks,
we propose a simple supervised learning frame-
work to learn an adaptive policy, and show how
to apply it with controllable latency. This frame-
work is based on sequences of READ/WRITE ac-
tions for parallel sentence pairs, so we present a
simple method to generate such an action sequence
for each sentence pair with a pre-trained neural
machine translation (NMT) model.1 Our experi-
ments on German↔English dataset show that our
method, without retraining the underlying NMT
model, leads to better policies than previous meth-
ods, and achieves better BLEU scores than (the re-
trained) wait-k models at low latency scenarios.
2 Generating Action Sequences

In this section, we show how to generate action se-
quences for parallel text. Our simultaneous trans-
lation policy can take two actions: READ (receive
a new source word) and WRITE (output a new tar-
get word). A sequence of such actions for a sen-
tence pair (s, t) defines one way to translate s into
t. Thus, such a sequence must have |t| number
of WRITE actions. However, not every action se-
quence is good for simultaneous translation. For
instance, a sequence without any READ action
will not provide any source information, while a
sequence with all |s| number of READ actions be-

1Previous work (He et al., 2015; Niehues et al., 2018)
shows that carefully generated parallel training data can help
improve simultaneous MT or low-latency speech translation.
This is different from our work in that we generates action
sequences for training policy model instead of parallel trans-
lation data for MT model.

fore all WRITE actions usually has large latency.
Thus the ideal sequences for simultaneous transla-
tion should have the following two properties:

• there is no anticipation during translation,
i.e. when choosing WRITE action, there is
enough source information for the MT model
to generate the correct target word;

• the latency is as low as possible, i.e. the
WRITE action for each target word appears
as early as possible.

Table 1 gives an example for such a sequence.
Algorithm 1 Generating Action Sequence
Input: sentence pair (s, t), integer r, modelM
ids ← 1, idt ← 1
Seq ← [R]
while idt ≤ |t| do

if rankM (tidt|s≤ids) ≤ r or ids = |s| thenSeq ← Seq +[W ]
idt ← idt + 1

else
Seq ← Seq +[R]
ids ← ids + 1return Seq

In the following, we present a simple method to
generate such an action sequence for a sentence
pair (s, t) using a pre-trained NMT model, as-
suming this model can make reasonable prediction
given incomplete source sentence. Our method is
based on this observation: if the rank of the next
ground-truth target word is high enough in the pre-
diction of the model, then this implies that there
is enough source-side information for the model
to make a correct prediction. Specifically, we se-
quentially input the source words to the pre-trained
model, and use it to predict next target word. If the
rank of the gold target word is high enough, wewill
append a WRITE action to the sequence and then
try the next target word; otherwise, we append a
READ action and input a new source word. Let
r be a positive integer, M be a pre-trained NMT
model, s≤i be the source sequence consisting of the
first i words of s, and rankM (tj|s≤i) be the rank of
the target word tj in the prediction of model M



1351

given sequence s≤i. Then the generating process
can be summarized as Algorithm 1.

Although we can generate action sequences bal-
ancing the two wanted properties with appropriate
value of parameter r, the latency of generated ac-
tion sequence may still be large due to the word or-
der difference between the two sentences. To avoid
this issue, we filter the generated sequences with
the latencymetric Average Lagging (AL) proposed
by Ma et al. (2019), which quantifies the latency in
terms of the number of source words and avoids
some flaws of other metrics like Average Propor-
tion (AP) (Cho and Esipova, 2016) and Consec-
utive Wait (CW) (Gu et al., 2017). Another is-
sue we observed is that, the pre-trained model may
be too aggressive for some sentence pair, meaning
that it may write all target words without seeing
the whole source sentence. This may be because
the model is also trained on the same dataset. To
overcome this, we only keep the action sequences
that will receive all the source words before the last
WRITE action. After the filtering process, each ac-
tion sequence has AL less than a fixed constant �
and receives all source words.
3 Supervised-Learning Framework for

Simultaneous Translation Policy
Given a sentence pair and an action sequence for
this pair, we can apply supervised learning method
to learn a parameterized policy for simultaneous
translation. For the policy to be able to choose
the correct action, its input should include infor-
mation from both source and target sides. Since
we use Transformer (Vaswani et al., 2017) as our
underlying NMT model in this work, we need to
recompute encoder hidden states for all previous
seen source words, which is the same as done for
wait-k model training (Ma et al., 2019).2 The pol-
icy input oi at step i consists of three components
from this model:

• ℎsi : the last-layer hidden state from the en-coder for the first source word at step i;
• ℎti: the last-layer hidden state from the de-coder for the first target word at step i;3
2In our experiments, the decoder and policy model com-

bined need about 0.0445 seconds on average to generate one
target word (this might include multiple READ’s), while it
takes on average only about 0.0058 seconds to recompute all
encoder states for each new source word. So we think this
re-computation might not be a serious issue for system effi-
ciency.

3The hidden states of the first target word will be re-
computed at each step.

• ci: cross-attention scores at step i for the cur-
rent input target word on all attention layers
in decoder, averaged over all current source
words.

That is oi = [ℎsi , ℎti, ci].Let ai be the i-th action in the given action se-
quence a. Then the decision of our policy on the
i-th step depends on all previous inputs o≤i and all
taken actions a<i. We want to maximize the proba-
bility of the next action ai given those information:

max p�(ai|o≤i, a<i)
where p� is the action distribution of our policy pa-
rameterized by �.
4 Decoding with Controllable Latency

To apply the learned policy for simultaneous trans-
lation, we can choose at each step the action with
higher probability . However, different scenarios
may have different latency requirements. Thus,
this greedy policy may not always be the best
choice for all situations. Here we present a simple
way to implicitly control the latency of the learned
policy without retraining the policy model.
Let � be a probability threshold. For each step

in translation, we choose READ action only if the
probability of READ is greater than �; otherwise
we choose WRITE action. Thus, this threshold
balances the tradeoff between latency and trans-
lation quality: with larger �, the policy prefers to
take WRITE actions, providing lower latency; and
with smaller �, the policy prefers to take READ ac-
tions, and provides more conservative translation
with larger latency.

2 4 6 8 10
AL

22

23

24

25

26

27

BL
EU

r=5 =3
r=50 =3
r=50 =7
r=50 =

Figure 1: Translation quality against latency on
DE→EN dev set. Lines are obtained with different
probability thresholds �. Markers represent � = 0.5.

5 Experiments

Dataset We conduct experiments on
English↔German (EN↔DE) simultaneous
translation. We use the parallel corpora from



1352

2 4 6 8 10 12
AL

20
22
24
26
28
30

BL
EU

29 2 4 6 8 10 12
AL

16
18
20
22
24
26
28

BL
EU

27

Figure 2: Comparing performances of different methods on testing sets. Left: DE→EN. Right: EN→DE. The
shown pairs are results of greedy decoding (solid shapes) and beam search (empty shapes, beam-size = 5). ◆:
wait-kmodels for k ∈ {1, 2, 3, 4, 5, 6},●: test-time wait-k for k ∈ {1, 2, 3, 4, 5, 6}, ◂: our SL model with threshold
� ∈ {0.65, 0.6, 0.55, 0.5, 0.45, 0.4}, ★: full-sentence translation model, ■: RL with CW = 2, ▾: RL with CW =
5, ▴: RL with CW = 8, ×: WID for s0 ∈ {2, 4, 6} and � ∈ {2, 4}, +: WIW for s0 ∈ {2, 4, 6} and � ∈ {1, 2}.

WMT 15 for training, newstest-2013 for valida-
tion and newstest-2015 for testing.4 All datasets
are tokenized and segmented into sub-word units
with byte-pair encoding (BPE) (Sennrich et al.,
2016), and we only use the sentence pairs of
lengths less than 50 (on both sides) for training.
Model Configuration We use Transformer-base
(Vaswani et al., 2017) as our NMT model and our
implementation is based on PyTorch-based Open-
NMT (Klein et al., 2017). We add an <eos> to-
ken on the source side, which is not included
in the original OpenNMT codebase. Our recur-
rent policy model consists of one GRU layer with
512 units, one fully-connected layer of dimen-
sion 64 followed by ReLU activation, and one
fully-connected layer of dimension 2 followed by
a softmax function to produce the action distribu-
tion. We use BLEU (Papineni et al., 2002) as the
translation quality metric and Averaged Lagging
(AL) (Ma et al., 2019) as the latency metric.
Effects of Generated Action Sequences We
first analyze the effects of the two parameters in the
generation process of action sequences: the rank
r and the filtering latency �. We fix � = 3 and
choose the rank r ∈ {5, 50}; thenwe fix r = 50 and
choose the latency � ∈ {3, 7,∞} to generate action
sequences on DE→EN direction.Figure 1 shows
the performances of resulting models with differ-
ent probability thresholds �. We find that smaller �
helps achieve better performance and our model is
not very sensitive to values of rank. Therefore, in
the following experiments, we report results with

4http://www.statmt.org/wmt15/translation-task.html

r = 50 and � = 3.
Performance Comparison We compare our
method on EN↔DE directions with different
methods: greedy decoding algorithms Wait-If-
Worse/Wait-If-Diff (WIW/WID) of Cho and Es-
ipova (2016), RL method of Gu et al. (2017), wait-
kmodels and test-time wait-kmethods of Ma et al.
(2019). Both of WIW and WID only use the
pre-trained NMT model, and the algorithm ini-
tially reads s0 number of source words and chooses
READ only if the probability of the most likely
target word decreases when given another � num-
ber of source words (WIW) or the most likely tar-
get word changes when given another � number
of source words (WID). For RL method, we use
the same kind of input and architecture for pol-
icy model.5 Test-time wait-kmeans decoding with
wait-k policy using the pre-trained NMT model.
All methods share the same underlying pre-trained
NMT model except for the wait-k method, which
retrains the NMT model from scratch, but with the
same architecture as the pre-trained model.
Figure 2 shows the performance comparison.

Our models on both directions can achieve higher
BLEU scores with the similar latency than WIW,
WID, RL model and test-time wait-k method, im-
plying that our method learns a better policy model
than the other methods when the underlying NMT
model is not retrained.6 Compared with wait-
k models, our models with beam search achieve

5We only report results obtained with CW-based reward
functions for they outperform those with AP-based ones.

6Our test-time wait-k results are better than those in Ma
et al. (2019), because the added source side <eos> token helps
the full-sentence model to learn when the source sentence

http://www.statmt.org/wmt15/translation-task.html


1353

German die deutsche bahn will im kommenden jahr die kin- zi- g- tal- – bahn- strecke verbessern .
gloss the German train want in the coming year the Kinzigtal – railroad track improve .
wait-3 deutsche bahn wants to make the cinema show a success this coming year .
wait-5 deutsche bahn wants to introduce the kin- zi- g- tal railway line next year .

test-time deutsche bahn wants the german railways to be the kin- z- ig- tal railway line in the
wait-3 coming year .
test-time the german railways wants to take the german train to the german railways in the
wait-5 coming year .
SL the german railway wants the kinzigtal railway

policy to be improved
next year .

RL the german railways wants the german railway will improve the kinzigtal railway
policy next year .

Table 2: German-to-English example from validation set.
higher BLEU scores when latency AL is small,
which we think will be the most useful scenarios of
simultaneous translation. Furthermore, this figure
also shows that our model can achieve good per-
formance on different latency conditions by con-
trolling the threshold �, so we do not need to train
multiple models for different latency requirements.
We also provide a translation example in Table 2 to
compare different methods.
Learning Process Analysis We analyze two as-
pects of the learning processes of different meth-
ods: stability and training time. Figure 3 shows
the learning curves of the training processes of
RL method and our SL method, averaged over
four different runs with different random seeds on
DE→EN direction. We can see that the training
process of our method is more stable and con-
verges faster than the RL method. Although there
are some steps where the RL training process can
achieve better BLEU scores than our SL method,
the corresponding latencies are usually very big,
which are not appropriate for simultaneous trans-
lation. We present the training time of different
methods in Table 3. Our method only need about
12 hours to train the policy model with 1 GPU,
while the wait-k method needs more than 600
hours with 8 GPUs to finish the training process,
showing that our method is very efficient. Note
that this table does not include the time needed to
generate action sequences. The time for this pro-
cess could be very flexible since we can parallelize
this by dividing the training data into separating
parts. In our experiments, we need about 2 hours
to generate all action sequences in parallel.
6 Conclusions

We have proposed a simple supervised-learning
framework to learn an adaptive policy based on
ends. Without this token, test-time wait-k generates either
very short target sentences ormany punctuations for small k’s.

0

10

20

AL

0 10 20 30 40 50 60

5
10
15
20
25

BLEU
RL training
SL training
Reported RL model (CW=5 Fig.2)

Figure 3: Learning curves averaged over four indepen-
dent training runs on DE→EN direction. The x-axis
represents training steps (× 50).
DE→EN pre-train: 64 wait-1 wait-3 wait-5SL: +12 RL: +14.3 938 966 945
EN→DE pre-train: 68 wait-1 wait-3 wait-5SL: +11 RL: +11.2 665 665 630

Table 3: Training time (in hours) of different methods.
Wait-k training uses 8 GPUs, while others use 1 GPU.
The RL time is the average time needed to obtain the
three RL models in Figure 2.

generated action sequences for simultaneous trans-
lation, which leads to faster training and better
policies than previous methods, without the need
to retrain the underlying NMT model.
Acknowledgments

We thank Hairong Liu for helpful discussion,
Kaibo Liu for helping training baseline mod-
els, and the anonymous reviewers for sugges-
tions. We also thank Kaibo Liu for making the
AL script available at https://github.com/
SimulTrans-demo/STACL.

References
AshkanAlinejad,MaryamSiahbani, andAnoop Sarkar.

2018. Prediction improves simultaneous neural ma-

https://github.com/SimulTrans-demo/STACL
https://github.com/SimulTrans-demo/STACL
https://www.aclweb.org/anthology/D18-1337


1354

chine translation. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 3022–3027.

Naveen Arivazhagan, Colin Cherry, Wolfgang
Macherey, Chung-Cheng Chiu, Semih Yavuz,
Ruoming Pang, Wei Li, and Colin Raffel. 2019.
Monotonic infinite lookback attention for simultane-
ous machine translation. In Proceedings of the 57th
Conference of the Association for Computational
Linguistics, ACL 2019, pages 1313–1323.

Srinivas Bangalore, Vivek Kumar Rangarajan Srid-
har, Prakash Kolan, Ladan Golipour, and Aura
Jimenez. 2012. Real-time incremental speech-to-
speech translation of dialogs. In Proc. of NAACL-
HLT.

Kyunghyun Cho and Masha Esipova. 2016. Can neu-
ral machine translation do simultaneous translation?
arXiv preprint arXiv:1606.02012.

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, and
Stephan Vogel. 2018. Incremental decoding and
training methods for simultaneous translation in neu-
ral machine translation. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 493–499.

Christian Fügen, Alex Waibel, and Muntsin Kolss.
2007. Simultaneous translation of lectures and
speeches. Machine translation, 21(4):209–252.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international
conference on acoustics, speech and signal process-
ing, pages 6645–6649. IEEE.

Alvin Grissom II, He He, Jordan Boyd-Graber, John
Morgan, and Hal Daumé III. 2014. Don’t until the
final verb wait: Reinforcement learning for simul-
taneous machine translation. In Proceedings of the
2014 Conference on empirical methods in natural
language processing (EMNLP), pages 1342–1352.

Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor O. K. Li. 2017. Learning to translate in real-
time with neural machine translation. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2017, pages 1053–1062.

He He, Alvin Grissom II, John Morgan, Jordan Boyd-
Graber, and Hal Daumé III. 2015. Syntax-based
rewriting for simultaneous machine translation. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages 55–
64.

Navdeep Jaitly, David Sussillo, Quoc V Le, Oriol
Vinyals, Ilya Sutskever, and Samy Bengio. 2016.
An online sequence-to-sequence model using partial
conditioning. In Advances in Neural Information
Processing Systems, pages 5067–5075.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M Rush. 2017. Opennmt:
Open-source toolkit for neural machine translation.
arXiv preprint arXiv:1701.02810.

Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,
Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,
Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and
Haifeng Wang. 2019. STACL: Simultaneous trans-
lation with implicit anticipation and controllable la-
tency using prefix-to-prefix framework. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3025–3036.

Jan Niehues, Ngoc-Quan Pham, Thanh-Le Ha,
Matthias Sperber, and Alex Waibel. 2018. Low-
latency neural speech translation. In Interspeech
2018, 19th Annual Conference of the Interna-
tional Speech Communication Association., pages
1293–1297.

Kishore Papineni, Salim Roukos, ToddWard, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318, Philadephia, USA.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1715–1725.

Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas
Bangalore, Andrej Ljolje, and Rathinavelu Chengal-
varayan. 2013. Segmentation strategies for stream-
ing speech translation. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 230–238.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Mahsa Yarmohammadi, Vivek Kumar Rangara-
jan Sridhar, Srinivas Bangalore, and Baskaran
Sankaran. 2013. Incremental segmentation and
decoding strategies for simultaneous translation.
In Proceedings of the Sixth International Joint
Conference on Natural Language Processing.

Baigong Zheng, Renjie Zheng, Mingbo Ma, and Liang
Huang. 2019a. Simultaneous translation with flexi-
ble policy via restricted imitation learning. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5816–
5822.

Renjie Zheng, Mingbo Ma, Baigong Zheng, and Liang
Huang. 2019b. Speculative beam search for simulta-
neous translation. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language
Processing and 9th International Joint Conference
on Natural Language Processing.

https://www.aclweb.org/anthology/D18-1337
https://doi.org/10.18653/v1/N18-2079
https://doi.org/10.18653/v1/N18-2079
https://doi.org/10.18653/v1/N18-2079
https://aclanthology.info/papers/E17-1099/e17-1099
https://aclanthology.info/papers/E17-1099/e17-1099
https://doi.org/10.18653/v1/P16-1162
https://doi.org/10.18653/v1/P16-1162

