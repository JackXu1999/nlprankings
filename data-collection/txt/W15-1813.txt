






















The Effect of Author Set Size in Authorship Attribution for Lithuanian

Jurgita Kapočiūtė-Dzikienė
Vytautas Magnus University
K. Donelaičio 58, LT-44248,

Kaunas, Lithuania
jurgita.k.dz@gmail.com

Ligita Šarkutė
Kaunas University of Technology

K. Donelaičio 73, LT-44029,
Kaunas, Lithuania

ligita.sarkute@ktu.lt

Andrius Utka
Vytautas Magnus University
K. Donelaičio 58, LT-44248,

Kaunas, Lithuania
utka@hmf.vdu.lt

Abstract

This paper reports the first authorship at-
tribution results based on the effect of the
author set size using automatic compu-
tational methods for the Lithuanian lan-
guage. The aim is to determine how fast
authorship attribution results are deterio-
rating while the number of candidate au-
thors is gradually increasing: i.e. start-
ing from 3, going up to 5, 10, 20, 50, and
100. Using supervised machine learning
techniques we also investigated the influ-
ence of different features (lexical, char-
acter, morphological, etc.) and language
types (normative parliamentary speeches
and non-normative forum posts).

The experiments revealed that the effec-
tiveness of the method and feature types
depends more on the language type rather
than on the number of candidate authors.
The content features based on word lem-
mas are the most useful type for the nor-
mative texts, due to the fact that Lithua-
nian is a highly inflective, morphologi-
cally and vocabulary rich language. The
character features are the most accurate
type for forum posts, where texts are too
complicated to be effectively processed
with external morphological tools.

1 Introduction

Authorship Attribution (AA) is the task of iden-
tifying who, from a set of candidate authors, is
an actual author of a given anonymous text docu-
ment. This prediction is based on a human “stylo-
metric fingerprint” notion: i.e. a specific, individ-
ual, persistent, and uncontrolled habit to express
thoughts with a unique set of linguistic means.
Van Halteren (2005) has gone so far as to name

this phenomenon a “human stylome” in the de-
liberate analogy to the DNA “genome”. How-
ever, Juola (2007) argues that such strict implica-
tions may not be absolutely correct, because the
“genome” is stable, but the human style tends to
evolve over time. Nevertheless a “stylome” can
still be added to human biometrics, next to voice,
gait, keystroke dynamics, handwriting, etc.

Starting from Mendenhall (1887) AA is one
of the oldest computational linguistics problems,
which is especially highly topical nowadays. For
a long time in the past the main AA applica-
tions were restricted to the literary texts only.
But the constant influx of anonymous electronic
text documents, especially on the Internet, and
the popularity of automatic methods opened the
gate to a number of new applications in foren-
sic analysis and electronic commerce. In addi-
tion to literary research the practical problems
from the plagiarism detection (Stamatatos, 2011),
the identification of harassment and threaten-
ing (Tan et al., 2013) to tracking authors of mali-
cious source code (Alrabaee et al., 2014) gained
even greater prominence. This led to experi-
ments with different datasets, such as e-mails (de
Vel et al., 2001; Abbasi and Chen, 2008), web
forum messages (Solorio et al., 2011), online
chats (Cristani et al., 2012; Inches et al., 2013), In-
ternet blogs (Koppel et al., 2011) or tweets (Sousa-
Silva et al., 2011; Schwartz et al., 2013), which, in
turn, contributed to a progress of the development
of computational linguistic methods that are able
to cope with the emerged problems.

Despite that many computational linguistic
tasks can be solved accurately only relying on ef-
forts of domain-experts, it is very time consuming,
expensive, and perhaps the most limiting way for
AA, moreover, which provides no explicit mea-
sure how attributions are made. The alternative
way is a manually composed set of rules capable
to take attribution decisions automatically. Unfor-

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 87



tunately, rule-based systems usually are very com-
plex, unwieldy, and thus not robust to any changes
in the domain, language or author characteristics,
therefore it is rather difficult to make any updates.
Moreover, when dealing with hundreds (e.g. in
Luyckx and Daelemans (2008), Luyckx (2010))
or thousands of candidate authors (e.g. in Kop-
pel et al. (2011) 10,000 authors; in Narayanan et
al. (2012) – 100,000) the possibility to create an
effective rule set goes far beyond human potential
limits. Ultimately, AA task can be solved using the
machine learning (Sebastiani, 2002): i.e. by train-
ing the classifiers and later using them to predict
the authorship of unseen texts. Moreover, it can
be easily adjusted to new applications or domains
and even generalized well to drifts in the author
characteristics. Due to all these advantages, the
machine learning paradigm became dominant and
remained the most popular till nowadays. There-
fore our focus in this paper is also on the machine
learning methods.

2 Related Work

Despite rare attempts to deal with unlabeled data,
e.g. Nasir et al. (2014), Qian et al. (2014), a typ-
ical AA problem fits the standard paradigm of
the supervised machine learning. It means that
the training dataset containing texts of known au-
thors is available and can be used to create the
model able to predict the authorship of unknown
texts from the same closed-set of the candidate au-
thors in the future. Algorithmically, it involves a
variety of different methods (for the detailed re-
view see Stamatatos (2009)) ranging from prob-
abilistic approaches (Seroussi et al., 2011), com-
pression models (Oliveira et al., 2013) to Vec-
tor Space Models (Stamatatos, 2008). In gen-
eral all methods can be distinguished according
to whether they treat each training text individ-
ually (instance-based) or cumulatively by con-
catenating texts written by the same author into
one (profile-based). Intuitively, profile-based ap-
proaches should have advantages over instance-
based when text documents are very concise,
thus concatenation helps to create sufficiently
long document for capturing its style; but on
the other hand instance-based approaches are bet-
ter suited for the sparse data scenario. Some
comparative experiments on the AA after test-
ing Decision Trees (DTs), Back Propagation Neu-
ral Networks (BPNNs) and Support Vector Ma-

chines (SVMs) revealed that SVMs and BPNNs
achieved significantly better performance com-
pared to DTs (Zheng et al., 2006). Zhao and Zo-
bel (2005) proved that k-Nearest Neighbor (kNN)
approach produces better results compared to both
Naı̈ve Bayes (NB) and DTs. Jockers and Wit-
ten (2010) report that Delta method outperforms
popular SVMs. Savoy (2012) proposes new clas-
sification scheme based on the specific vocabulary
and experimentally proves that it performs bet-
ter than Principal Component Analysis (PCA) and
slightly better than Delta approach; Savoy (2013)
also shows that LDA (Latent Dirichlet Allocation)
classification scheme can surpass two classical
AA approaches – i.e. Delta rule and chi-squared
distance. Nevertheless, the precise comparison of
methods is still difficult due to the lack of suitable
benchmark data. Besides, the results are affected
not only by the selected classification method it-
self, but by preprocessing techniques, author set
sizes, language characteristics, etc. However the
most crucial factor is probably the selected type of
features.

The first modern work in AA (different from
traditional human-expert techniques) was de-
scribed by Mosteller and Wallace (1963). They
demonstrated promising AA results on The Fed-
eralist papers using Bayesian methods applied on
frequencies of a small set of function words (in-
cluding articles, prepositions and conjunctions) as
stylistic features in the text. Since this pioneering
study and until 1990s AA was based on quanti-
tative features (so-called style markers) such as a
sentence or word length, syllables per word, type-
token ratio, vocabulary richness functions, lex-
ical repetition, etc. In fact all these stylomet-
ric features are considered to be suitable only for
homogeneous long texts (>1,000 words) and for
datasets where the number of candidate authors is
limited. Lately other feature types– in particular,
lexical, syntactic, semantic, or character –treating
texts as the sequence of tokens or characters be-
came more popular. A huge number of these fea-
tures have been presented so far, but we will focus
only on the most popular and the most accurate
ones. The most common example of the lexical
feature type is a simple bag-of-words represen-
tation which is considered to be topic-dependent
therefore should be avoided when the distribution
over authors coincides the distribution over differ-
ent topics (not to solve topic-classification prob-

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 88



lem instead of AA). Besides token n-grams are
also considered to capture content-specific instead
of stylistic information. The most popular topic-
neutral lexical solution, carrying no semantic in-
formation, is the function words (articles, conjunc-
tions, prepositions, pronouns, etc.). Various au-
thors use different lists of function words, varying
from 150 (Abbasi and Chen, 2005) to 675 (Arg-
amon et al., 2007) words, but providing very lit-
tle information about how these lists were com-
posed. The effectiveness of syntactic and seman-
tic features usually rely on the accuracy of exter-
nal linguistic tools (e.g. part-of-speech taggers,
parsers) or exhaustiveness of additional data re-
sources (e.g. thesauruses or databases). Although
used alone they hardly can outperform lexical fea-
tures, but often improve the results used in the
combination (Gamon, 2004). However, charac-
ter features (character n-grams, in particular) are
considered the most important document repre-
sentation type in authors’ style detection: they
are topic-neutral, language-independent, able to
capture style through lexical and contextual in-
formation, and are tolerant to grammatical errors.
Application-specific features are highly dependent
on the solvable problem, e.g. positions of hash-
tags, smileys, punctuation are important style de-
tectors in tweets (Sousa-Silva et al., 2011).

The majority of surveyed research works deal
with Germanic languages, providing no guidance
what could work the best with morphologically
rich, highly inflective, derivationally complex,
and relatively free word order languages such as
Lithuanian. Starting from 1971 (Pikčilingis, 1971)
lots of descriptive linguistic works are done on
the AA for the Lithuanian language (the review
in Žalkauskaitė (2012)). Besides, the pioneer-
ing and as far as we know the only work us-
ing automatic methods on the Lithuanian texts is
described in (Kapočiūtė-Dzikienė et al., 2014).
However, their experiments have been made only
with the normative Lithuanian language, few au-
thors, and small training data; therefore findings
are not robust to make the generalizations about
which method is the best and which feature type is
the most reliable for solving AA problem in gen-
eral. Consequently in this research we will try to
overcome all mentioned shortcomings by experi-
menting with different language types (normative
and Internet forum data) and increasing number of
candidate authors (up to one hundred).

3 Methodology

In essence, AA problem is a task which can be
formally described as follows.

The dataset D contains text documents di at-
tributed to a closed-set of candidate authors (de-
fined as classes) C = {c j}.

The training dataset DT (where DT ⊂D) is com-
posed of training instances: i.e. documents di with
a known authorship c j: {〈di,c j〉}.

The function ϕ determines the mapping (about
characteristics in styles of the authors) how each
di is attributed to c j in DT .

Our goal is using DT to train a classifier and
to create the model ϕ’, which could be as close
approximation of ϕ as possible.

3.1 The Datasets
All our experiments were carried out on 2 datasets
to make sure that findings generalize over different
domains and language types:

• ParlTranscr1 (see Table 1) contains unedited
transcripts of parliamentary speeches and de-
bates, thus representing formal spoken but
normative Lithuanian language. All tran-
scripts are from regular parliamentary ses-
sions and cover the period of 7 parliamen-
tary terms starting on March 10, 1990 and
ending on December 23, 2013. Very long
(>1,000 words) and very short (<100 words)
texts were removed from the dataset to avoid
speeches written by non-parliamentarians,
but by someone else and to avoid less infor-
mative text samples, respectively. Afterwards
we selected 100 authors with the largest num-
ber of texts, but making sure that the selected
candidates are distributed over different par-
liamentary terms (to avoid topic classifica-
tion) and party groups (to avoid ideology-
based classification).

• LRytas2 (see Table 2) contains forum data full
of informal words, foreign language inser-
tions, word shortenings, emoticons, and di-
acritic eliminations, thus represents the infor-
mal non-normative Lithuanian language. The
forum has 11 general topics (such as “Busi-
ness”, “Politics”, “Sports”, etc.). Very short
texts (<10 words) were not included into

1Downloaded from http://www3.lrs.lt/pls/inter/w5 sale.
2Crawled on March 19, 2014 from

http://forum.lrytas.lt/forum show.pl.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 89



the dataset. Afterwards we selected 100 au-
thors having the largest number of texts, but
making sure that selected candidates would
be distributed over different topics (to avoid
topic classification).

3.2 Classification

In this paper we focus on the supervised machine
learning techniques (Kotsiantis, 2007) applied to
the text categorization (Sebastiani, 2002) and used
for the AA (Stamatatos, 2009).

The aim of our task is to find a method, which
could distinguish the distinct authors from each
other by creating a model for the best approx-
imation of the authors’ style. For this reason
we explored two supervised machine learning ap-
proaches:

• Support Vector Machine (SVM) (introduced
by Cortes and Vapnik (1995)) is a dis-
criminative instance-based approach, which
is currently the most popular text classifi-
cation technique, efficiently handling a high
dimensional feature spaces (e.g. maximum
∼295 thousand features in the imbalanced
100 authors ParlTrascr dataset, ∼84,4 thou-
sand in LRytas); sparseness of the feature
vectors (only ∼215 non-zero feature values
among ∼295 thousand in ParlTranscr and
∼42 among ∼84,4 thousand in LRytas); and
does not perform aggressive feature selec-
tion, which may result in a loss of infor-
mation and degrade the accuracy (Joachims,
1998).

• Naı̈ve Bayes Multinomial (NBM) (introduced
by Lewis and Gale (1994)) is a generative
profile-based approach, which is often se-
lected due to its simplicity: Naı̈ve Bayes as-
sumption about the feature independence al-
lows parameters of each feature to be learned
separately; the method performs especially
well when the number of features having
equal significance is large; it is very fast and
does not require huge data storage resources;
besides, this Bayesian method is often se-
lected as the baseline approach.

However, it is important to notice that the choice
of classification algorithm is not more important
than the choice of feature types by which texts
have to be represented.

3.3 Feature Extraction

In our research we explored the impact of the most
popular or/and accurate individual and compound
feature types, covering stylistic, character, lexical,
and morpho-syntactic levels:

• usm – ultimate style markers: average sen-
tence and word length in a text document;
standardized type/token ratio (STTR). Al-
though we assume that this archaic stylomet-
ric feature type will definitely give very poor
classification results, it still has to be tested
for comparison reasons.

• chrN – document-level character n-grams:
context-free character feature type (where
N = [2;7] in our experiments). It considers
successions of N characters including spaces
and punctuation marks, e.g., chr7 of phrase
“authorship attribution” produces the follow-
ing character n-grams: “authors”, “uthorsh”,
“thorshi”, “horship”, “orship ”, “rship a”,
etc.3 By many researchers this feature type
was proved to be one of the best (or even the
best) to tackle AA problems.

• fwd – function words: the content-free lexi-
cal feature type which includes prepositions,
pronouns, conjunctions, particles, interjec-
tions, and onomatopoeias. Instead of relying
on the pre-established and stable list of the
function words, we identified them by apply-
ing the Lithuanian morphological analyzer-
lemmatizer “Lemuoklis” (Zinkevičius, 2000;
Daudaravičius et al., 2007). This feature
type by consensus is considered as the topic-
neutral and was proved to be a relatively good
identifier of the writing style by many re-
searchers.

• lexN – token n-grams: the most popular
content-specific lexical feature type which in-
volves a bag-of-words (N = 1) or interpola-
tion of token n-grams (N = [2;3] in our ex-
periments), e.g., lex1 of the phrase “author-
ship attribution problem” produces 3 bag-
of-words: “authorship”, “attribution”, and
“problem”; lex2: 3 bag-of-words plus token
bigrams “authorship attribution”, and “attri-
bution problem”; lex3: 3 bag-of-words, 2 to-

3This and the following examples will be given in English
instead of Lithuanian for the clarity reasons.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 90



Numb. of
classes

Numb. of text
documents

Numb. of
tokens

Numb. of distinct
tokens (types)

Numb. of distinct
lemmas

Avg. numb of
tokens in a doc.

3
600 156,107 21,439 8,608 260.18

16,804 3,457,093 107,950 35,525 205.73

5
1,000 239,288 27,983 10,864 239.29

22,476 4,585,493 132,623 42,620 204.02

10
2,000 451,638 38,952 14,076 225.82

34,307 6,821,083 157,409 49,470 198.82

20
4,000 927,411 63,456 21,310 231.85

50,532 10,254,271 204,043 61,443 202.93

50
10,000 2,475,615 107,029 33,308 247.56
77,005 16,478,475 254,966 75,563 213.99

100
20,000 4,728,411 151,836 45,441 236.42
98,999 21,295,515 295,046 86,770 215.11

Table 1: Composition of ParlTranscr: the upper value in each cell represents a balanced dataset (200
instances in each class), the lower – imbalanced (full). The set of authors is identical in the both datasets.

Numb. of
classes

Numb. of text
documents

Numb. of
tokens

Numb. of distinct
tokens (types)

Numb. of distinct
lemmas

Avg. numb of
tokens in a doc.

3
30 1,252 792 615 41.73

3,567 137,768 30,830 16,726 38.62

5
50 1,722 1,049 781 34.44

4,579 166,512 36,267 19,271 36.36

10
100 3,913 2,191 1,572 39.13

6,209 244,947 49,648 26,603 39.45

20
200 8,876 4,287 2,910 44.38

8,470 351,285 63,363 33,377 41.47

50
500 21,942 8,980 5,725 43.88

11,155 468,466 76,861 40,057 42.00

100
1,000 44,375 15,290 9,443 44.38

12,888 545,405 84,482 44,211 42.32

Table 2: Composition of LRytas: the upper value in each cell represents a balanced dataset (10 instances
in each class), the lower – imbalanced (full). The set of authors is identical in the both datasets.

ken n-grams plus one trigram “authorship at-
tribution problem”.

• lemN – n-grams of token lemmas: the
content-specific lexical feature type which
involves lemmas based on the word tokens
(N = 1) or their interpolation (N = [2;3]
in our experiments). “Lemuoklis” replaces
words with their lemmas, transforms recog-
nized generic words into the lower-case and
replaces all numbers with a special tag. We
assume that this feature type should reduce
the number of types significantly (especially
for ParlTranscr) which should result in cre-
ation of more robust models and higher clas-

sification accuracy.

• posN – n-grams of part-of-speech tags: the
content-free morpho-syntactic feature type
which involves coarse-grained part-of-speech
tags based on word tokens (N = 1) or their
interpolation (N = [2;3] in our experiments).
Coarse-grained part-of-speech tags (such as
noun, verb, adjective, etc.) are also deter-
mined by “Lemuoklis”.

• lexposN, lemposN, lexmorfN, lemmorfN –
the aggregated features which involve uni-
grams (N = 1) of concatenated features or
their interpolation (N = [2;3] in our ex-
periments): lex&pos, lem&pos, lex&morf,

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 91



lex&morf, respectively, where morf indicates
the string of the concatenated fine-grained
morphological values for case, gender, tense,
mood, etc., determined by “Lemuoklis”,
e.g., lexpos2 of phrase “interesting problem”
produces two unigrams “interesting ADJ”,
“problem NOUN” plus one bigram “interest-
ing ADJ problem NOUN”.

4 Experimental Set-Up and Results

Our aim is to explore different classification meth-
ods (see Section 3.2), feature types (see Sec-
tion 3.3) and to answer the main questions:

• How the author set size affects results, when
having 3, 5, 10, 20, 50, and 100 candidate au-
thors? The candidate author selection is done
depending on the number of their texts: the
authors with the most texts are selected first.

• How the language type influences results,
when ParlTranscr contains texts of norma-
tive, but LRytas of non-normative language?

All experiments were carried out with the strat-
ified 10-fold cross-validation and evaluated us-
ing accuracy and micro/macro average F-score
metrics. Since F-scores showed the same ac-
curacy trend in all our experiments, we do not
present them in the following figures and tables.
For each dataset random (∑P2(c j)) and majority
(maxP(c j)) baselines (where P(c j) is the prob-
ability of class c j) were calculated, but only the
higher values were presented in the following fig-
ures. In order to determine whether the differences
between obtained values are statistically signifi-
cant we performed McNemar’s (McNemar, 1947)
test with one degree of freedom.

In our experiments we used chi-squared feature
extraction method, SMO polynomial kernel (be-
cause it gave the highest accuracy in our prelim-
inary control experiments) with SVM and NBM
implementations in the WEKA machine learning
toolkit (Hall et al., 2009), version 3.6. All remain-
ing parameters were set to their default values.

For the effect of used method see Figure 1 and
feature type see Table 3 and Table 4.

5 Discussion

Zooming into the results presented in Figure 1, al-
lows us to report the following statements:

All obtained results are reasonable and appro-
priate for our solving task, because they exceed

random and majority baselines. However, SVM is
a much better selection, as it always outperformed
NBM, except for a couple of cases when the both
methods achieved the same accuracy.

If compared the same number of candidate au-
thors, the accuracy of LRytas is always much
lower compared to ParlTranscr. This could be
due to the language type, text length, and train-
ing dataset size. The comprehensive expert analy-
sis revealed that parliamentarians use official lan-
guage with the larger but more steady dictionary.
Moreover, their speeches or debates are carefully
transcribed, thus there are no grammatical errors
and diacritic eliminations. Whereas in LRytas dif-
ferent forum texts posted by even the same author
are written in different manners, thus the quality
of texts varies (sometimes more typing errors or
abbreviations). Since the non-normative language
is always much harder to deal with, the accuracy is
lower. The second reason is the length of classified
texts: it is always easier to predict the author from
longer text samples. As we can see from the Ta-
ble 1 and Table 2 the texts in ParlTranscr are more
than 5 times longer compared to LRytas texts. Be-
sides, in our experiments we were using 10-fold
cross-validation, thus having 9/10 of all text sam-
ples for training, e.g., when dealing with the im-
balanced datasets and 100 candidate authors, Parl-
Transc has 7 times more text documents and 3
times more different tokens (types) compared to
LRytas (see Table 1 and Table 2). Consequently,
the bigger variety in the training data helps to cre-
ate more comprehensive models which in turn are
more robust in the classification stage.

When increasing the number of candidate au-
thors, we are also making the task more difficult,
thus the accuracy is gradually dropping. However
the decline is much steeper for LRytas compared
to ParlTranscr, e.g. the increase from 3 to 100
candidate authors using SVM for balanced and
imbalanced ParlTranscr produces the decrease of
26.9% and 22.9%, respectively; while for LRytas
it is 46.6% and 40.1%. Having more candidate
authors the task becomes more difficult, therefore
all previously mentioned problems (language type,
text length, training dataset size), become even
more detriment.

The balancing decreases training data, thus neg-
atively affects AA results for LRytas and SVM’s
results with 100 authors for ParlTranscr (this con-
firms a statement in Manning and Schütze (1999)),

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 92



Figure 1: The accuracy (y axis) dependence on the number of the candidate authors (x axis). Grey
columns represent NBM, white – SVM, black lower parts represent higher of the random/majority base-
lines. Each column shows the maximum achieved accuracy over all explored feature types.

but has opposite effect on ParlTranscr. This might
happened due to a successful random selection of
instances for the balanced dataset. In the imbal-
anced experiment the major 3 authors already has
the texts which are not appropriate to express their
style as good as in the balanced dataset, thus a neg-
ative influence not only persists, but may increase
when adding more authors to the dataset.

In our experiments we tested all the most popu-
lar currently known feature types (29 in total) used
for AA. Zooming into the results reported in Ta-
ble 3 and Table 4 allows us to make the following
statements.

When analyzing the normative Lithuanian lan-
guage (as it is in ParlTranscr) the content informa-
tion is very important for achieving high classifi-
cation accuracy. Moreover, the feature type based
on word lemmas is marginally the best in 8 of 12
times (with 5, 10, 20, 50 and 100 candidate au-
thors with balanced and 10, 50 and 100 authors
with imbalanced dataset). When having 5 or 20
authors with imbalanced dataset a bit longer pat-
terns N = 2 and N = 3, respectively, of word lem-
mas give the best results. Despite that the part-
of-speech information when used alone is defi-
nitely not the best selection, but in concatenation
with lemmas (when N = 2) it can boost the per-

formance and become the best feature type with
3 authors. Considering information about statisti-
cal significance between different results, and ig-
noring small variations depending on the number
of authors, we can state that in general the best
feature type for ParlTranscr dataset is based on
the lemma and part-of-speech information. It is
not surprising due to the fact that we were deal-
ing with the Lithuanian language which is highly
inflective, morphologically and vocabulary rich;
moreover we were dealing with the normative lan-
guage; therefore morphological tools were maxi-
mally helpful for this dataset.

When dealing with forum posts in LRytas, the
picture is absolutely different. Marginally the best
feature type in most of the cases is not based
on the content information, thus, it is not based
on the lemma information. Document-level char-
acter bigrams give the best results in 9 of 12
cases with the small exceptions (100 candidate au-
thors with balanced and 3 and 5 authors with im-
balanced datasets), where the credit is given to
the content lemma information again. It is not
surprising, since we were dealing with the non-
normative Lithuanian language texts full of errors,
diacritic eliminations, and words out of the stan-
dard Lithuanian language dictionary; moreover,

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 93



Feature type ParlTranscr (balanced) ParlTranscr3 5 10 20 50 100 3 5 10 20 50 100
usm 0.488 0.430 0.272 0.155 0.069 0.037 0.581 0.435 0.299 0.202 0.133 0.103
fwd 0.792 0.763 0.662 0.518 0.392 0.324 0.801 0.753 0.642 0.555 0.461 0.398

chr3 0.938 0.945 0.901 0.819 0.699 0.627 0.904 0.890 0.804 0.747 0.680 0.633
chr4 0.938 0.945 0.893 0.813 0.685 0.601 0.906 0.887 0.802 0.743 0.674 0.625
lex1 0.953 0.936 0.900 0.816 0.708 0.635 0.927 0.911 0.832 0.774 0.706 0.659

lem1 0.960 0.961 0.922 0.862 0.760 0.699 0.931 0.920 0.850 0.796 0.746 0.706
lem2 0.962 0.958 0.910 0.852 0.753 0.691 0.932 0.922 0.847 0.797 0.740 0.702
lem3 0.957 0.954 0.914 0.849 0.753 0.690 0.933 0.921 0.847 0.797 0.737 0.701
pos3 0.742 0.715 0.643 0.509 0.359 0.261 0.807 0.755 0.655 0.558 0.439 0.364

lexpos1 0.962 0.943 0.906 0.815 0.705 0.637 0.926 0.912 0.835 0.774 0.708 0.659
lempos1 0.960 0.956 0.918 0.851 0.750 0.690 0.934 0.921 0.847 0.795 0.742 0.701
lempos2 0.968 0.954 0.913 0.841 0.741 0.682 0.935 0.919 0.846 0.795 0.738 0.698
lexmorf1 0.953 0.941 0.900 0.812 0.703 0.632 0.922 0.911 0.831 0.771 0.705 0.657

lemmorf1 0.958 0.936 0.907 0.822 0.708 0.646 0.925 0.913 0.835 0.778 0.715 0.671

Table 3: Accuracy values with SVM and various feature types for ParlTrascr dataset. Only the best
results in terms of N of each feature type are reported. The best results for different author set sizes (in
columns) are in bold; results that do not statistically significant differ from the best result are underlined.

Feature type LRytas (balanced) LRytas3 5 10 20 50 100 3 5 10 20 50 100
usm 0.267 0.360 0.250 0.085 0.038 0.027 0.443 0.343 0.251 0.183 0.139 0.121
fwd 0.300 0.280 0.170 0.105 0.078 0.045 0.587 0.459 0.375 0.293 0.225 0.197

chr2 0.500 0.520 0.350 0.260 0.180 0.135 0.698 0.584 0.537 0.445 0.354 0.309
lex1 0.467 0.400 0.320 0.175 0.136 0.103 0.695 0.578 0.512 0.397 0.323 0.281

lem1 0.433 0.380 0.310 0.165 0.172 0.128 0.696 0.604 0.525 0.418 0.336 0.230
lem2 0.400 0.280 0.210 0.205 0.152 0.127 0.687 0.583 0.506 0.409 0.328 0.287
pos1 0.400 0.340 0.220 0.180 0.106 0.060 0.609 0.486 0.407 0.304 0.233 0.202
pos2 0.367 0.280 0.260 0.150 0.082 0.063 0.649 0.536 0.469 0.353 0.275 0.238

lexpos1 0.467 0.440 0.290 0.225 0.128 0.097 0.689 0.570 0.500 0.394 0.316 0.281
lempos1 0.467 0.380 0.280 0.140 0.144 0.137 0.692 0.592 0.526 0.413 0.337 0.298
lempos2 0.367 0.260 0.200 0.190 0.146 0.118 0.697 0.586 0.508 0.407 0.327 0.289
lexmorf1 0.400 0.400 0.240 0.220 0.124 0.087 0.695 0.571 0.501 0.396 0.315 0.276

lemmorf1 0.367 0.340 0.240 0.140 0.130 0.105 0.703 0.570 0.506 0.395 0.318 0.278

Table 4: Accuracy values with SVM and various feature types for LRytas dataset. For other notations
see the caption of Table 3.

even in forums for the registered users the iden-
tity of the author is not 100% certain. Despite all
these findings about character n-grams, we cannot
strongly state that it is the very best feature type for
our non-normative texts, because the differences
between other content-based feature types are not
always statistically significant.

6 Conclusions and Future Work

In this paper we report the first authorship attribu-
tion results based on the exploration of the effect
of the author set size when dealing with normative
and non-normative Lithuanian language texts and
using supervised machine learning techniques.

We experimentally have determined that the ef-
fect of feature types depend more on the language
type used in the dataset than on the number of
candidate authors. Using parliamentary data (thus
normative Lithuanian language) the best feature

types are based on the morpho-syntactic informa-
tion generated by the external grammatical tools.
The results exceed baseline by 62.7% and reach
even 70.6% of accuracy with 100 of candidate
authors. Using forum posts (thus non-normative
texts) the best feature types are however based on
the character n-grams. The results exceed baseline
by 20.7% and reach 30.9% of accuracy.

In the future research we are planning to fur-
ther expand the number of candidate authors up
to several thousands or even tens of thousands; to
experiment more with non-normative Lithuanian
language (blog data, tweets, etc.) and to reveal if
the same statements about feature types and meth-
ods are still valid.

Acknowledgments

Research was funded by a grant (No. LIT-8-69)
from the Research Council of Lithuania.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 94



References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying

Authorship Analysis to Extremist-Group Web Fo-
rum Messages. IEEE Intelligent Systems, 20(5):67–
75.

Ahmed Abbasi and Hsinchun Chen. 2008. Writer-
prints: A Stylometric Approach to Identity-Level
Identification and Similarity Detection in Cy-
berspace. ACM Transactions on Information Sys-
tems, 26(2):1–29.

Saed Alrabaee, Noman Saleem, Stere Preda, Lingyu
Wang, and Mourad Debbabi. 2014. OBA2: An
Onion approach to Binary code Authorship Attribu-
tion. Digital Investigation, 11(1):S94–S103.

Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic Text Classification Using Functional
Lexical Features: Research Articles. Journal of the
American Society for Information Science and Tech-
nology, 58(6):802–822.

Corina Cortes and Vladimir Vapnik. 1995. Support-
Vector Networks. Machine Learning, 20(3):273–
297.

Marco Cristani, Giorgio Roffo, Cristina Segalin,
Loris Bazzani, Alessandro Vinciarelli, and Vittorio
Murino. 2012. Conversationally-inspired Stylomet-
ric Features for Authorship Attribution in Instant
Messaging. In Proceedings of the 20th ACM In-
ternational Conference on Multimedia, pages 1121–
1124.

Vidas Daudaravičius, Erika Rimkutė, and Andrius
Utka. 2007. Morphological annotation of the
Lithuanian corpus. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing: Information Extraction and Enabling Technolo-
gies, pages 94–99.

Olivier de Vel, Alison M. Anderson, Malcolm W. Cor-
ney, and George M. Mohay. 2001. Mining e-Mail
Content for Author Identification Forensics. SIG-
MOD Record, 30(4):55–64.

Michael Gamon. 2004. Linguistic Correlates of
Style: Authorship Classification with Deep Linguis-
tic Analysis Features. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 611–617.

Mark Hall, Eibe Frank, Holmes Geoffrey, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10–18.

Giacomo Inches, Morgan Harvey, and Fabio Crestani.
2013. Finding Participants in a Chat: Authorship
Attribution for Conversational Documents. In In-
ternational Conference on Social Computing, pages
272–279.

Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with many Rel-
evant Features. In 10th European Conference on
Machine Learning, volume 1398, pages 137–142.

Matthew L. Jockers and Daniela M. Witten. 2010. A
Comparative Study of Machine Learning Methods
for Authorship Attribution. Literary and Linguistic
Computing, 25(2):215–223.

Patrick Juola. 2007. Future Trends in Authorship At-
tribution. In Advances in Digital Forensics III IFIP
– The International Federation for Information Pro-
cessing, volume 242, pages 119–132.

Jurgita Kapočiūtė-Dzikienė, Andrius Utka, and Ligita
Šarkutė. 2014. Feature Exploration for Authorship
Attribution of Lithuanian Parliamentary Speeches.
In 17th International Conference on Text, Speech,
and Dialogue, pages 93–100.

Moshe Koppel, Jonathan Schler, and Shlomo Arga-
mon. 2011. Authorship attribution in the wild. Lan-
guage Resources and Evaluation, 45(1):83–94.

Sotiris B. Kotsiantis. 2007. Supervised Machine
Learning: A Review of Classification Techniques.
In Proceedings of the 2007 Conference on Emerging
Artificial Intelligence Applications in Computer En-
gineering: Real Word AI Systems with Applications
in eHealth, HCI, Information Retrieval and Perva-
sive Technologies, pages 3–24.

David D. Lewis and William A. Gale. 1994. A Se-
quential Algorithm for Training Text Classifiers. In
17th Annual International ACM-SIGIR Conference
on Research and Development in Information Re-
trieval, pages 3–12.

Kim Luyckx and Walter Daelemans. 2008. Author-
ship Attribution and Verification with Many Authors
and Limited Data. In Proceedings of the 22Nd Inter-
national Conference on Computational Linguistics,
volume 1, pages 513–520.

Kim Luyckx. 2010. Scalability Issues in Authorship
Attribution. Ph.D. thesis, University of Antwerp,
Belgium.

Christopher D. Manning and Hinrich Schütze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, MA, USA.

Quinn Michael McNemar. 1947. Note on the
Sampling Error of the Difference Between Corre-
lated Proportions or Percentages. Psychometrika,
12(2):153–157.

Thomas Corwin Mendenhall. 1887. The Characteris-
tic Curves of Composition. Science, 9:237–246.

Frederik Mosteller and David L. Wallace. 1963. In-
ference in an authorship problem. Journal Of The
American Statistical Association, 58(302):275–309.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 95



Arvind Narayanan, Hristo Paskov, Neil Zhenqiang
Gong, John Bethencourt, Emil Stefanov, Eui
Chul Richard Shin, and Dawn Song. 2012. On
the Feasibility of Internet-Scale Author Identifica-
tion. In Proceedings of the 2012 IEEE Symposium
on Security and Privacy, pages 300–314.

Jamal Abdul Nasir, Nico Görnitz, and Ulf Brefeld.
2014. An Off-the-shelf Approach to Authorship
Attribution. The 25th International Conference on
Computational Linguistics, pages 895–904.

Walter Ribeiro Oliveira, Edson Justino, and Luiz S.
Oliveira. 2013. Comparing compression models
for authorship attribution. Forensic Science Inter-
national, 228(1-3):100–104.

Juozas Pikčilingis. 1971. Kas yra stilius?[What is the
style?]. Vaga, Vilnius, Lithuania. (in Lithuanian).

Tieyun Qian, Bing Liu, Li Chen, and Zhiyong Peng.
2014. Tri-Training for Authorship Attribution with
Limited Training Data. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, volume 2, pages 345–351.

Jacques Savoy. 2012. Authorship Attribution: A Com-
parative Study of Three Text Corpora and Three
Languages. Journal of Quantitative Linguistics,
19(2):132–161.

Jacques Savoy. 2013. Authorship Attribution Based
on a Probabilistic Topic Model. Information Pro-
cessing and Management, 49(1):341–354.

Roy Schwartz, Oren Tsur, Ari Rappoport, and Moshe
Koppel. 2013. Authorship Attribution of Micro-
Messages. In Empirical Methods in Natural Lan-
gauge Processing, pages 1880–1891.

Fabrizio Sebastiani. 2002. Machine Learning in Au-
tomated Text Categorization. ACM Computing Sur-
veys, 34(1):1–47.

Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.
2011. Authorship Attribution with Latent Dirichlet
Allocation. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning,
pages 181–189.

Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and
Manuel Montes-y Gómez. 2011. Modality Specific
Meta Features for Authorship Attribution in Web
Forum Posts. In The 5th International Joint Confer-
ence on Natural Language Processing, pages 156–
164.

Rui Sousa-Silva, Gustavo Laboreiro, Luı́s Sarmento,
Tim Grant, Eugénio C. Oliveira, and Belinda Maia.
2011. ’twazn me!!! ;(’ automatic authorship analy-
sis of micro-blogging messages. In Proceedings of
the 16th International Conference on Natural Lan-
guage Processing and Information Systems, pages
161–168.

Efstathios Stamatatos. 2008. Author Identification:
Using Text Sampling to Handle the Class Imbalance
Problem. Information Processing and Management,
44(2):790–799.

Efstathios Stamatatos. 2009. A Survey of Modern Au-
thorship Attribution Methods. Journal of the As-
sociation for Information Science and Technology,
60(3):538–556.

Efstathios Stamatatos. 2011. Plagiarism Detection
Using Stopword N-Grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512–2527.

Enhua Tan, Lei Guo, Songqing Chen, Xiaodong Zhang,
and Yihong Zhao. 2013. UNIK: Unsupervised So-
cial Network Spam Detection. In Proceedings of the
22Nd ACM International Conference on Conference
on Information & Knowledge Management, pages
479–488.

Hans Van Halteren, R. Harald Baayen, Fiona Tweedie,
Marco Haverkort, and Anneke Neijt. 2005. New
Machine Learning Methods Demonstrate the Exis-
tence of a Human Stylome. Journal of Quantitative
Linguistics, 12(1):65–77.

Gintarė Žalkauskaitė. 2012. Idiolekto požymiai elek-
troniniuose laiškuose. [Idiolect signs in e-mails].
Ph.D. thesis, Vilnius University, Lithuania. (in
Lithuanian).

Ying Zhao and Justin Zobel. 2005. Effective and Scal-
able Authorship Attribution Using Function Words.
In Proceedings of the Second AIRS Asian Informa-
tion Retrieval Symposium, pages 174–189.

Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan
Huang. 2006. A framework for authorship identi-
fication of online messages: Writing-style features
and classification techniques. Journal of the Ameri-
can Society for Information Science and Technology,
57(3):378–393.

Vytautas Zinkevičius. 2000. Lemuoklis – mor-
fologinei analizei [Morphological analysis with
Lemuoklis]. In Darbai ir Dienos, volume 24, pages
246–273. (In Lithuanian).

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 96


