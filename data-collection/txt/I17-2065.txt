



















































A Multi-task Learning Approach to Adapting Bilingual Word Embeddings for Cross-lingual Named Entity Recognition


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 383–388,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

A Multi-task Learning Approach to Adapting Bilingual Word
Embeddings for Cross-lingual Named Entity Recognition

Dingquan Wang1 Nanyun Peng3∗ Kevin Duh12
1 Center for Language and Speech Processing, Johns Hopkins University

2 Human Language Technology Center of Excellence, Johns Hopkins University
3 Information Sciences Institute, University of Southern California

wdd@cs.jhu.edu, npeng@isi.edu, kevinduh@cs.jhu.edu

Abstract

We show how to adapt bilingual word em-
beddings (BWE’s) to bootstrap a cross-
lingual name-entity recognition (NER)
system in a language with no labeled data.
We assume a setting where we are given
a comparable corpus with NER labels for
the source language only; our goal is to
build a NER model for the target lan-
guage. The proposed multi-task model
jointly trains bilingual word embeddings
while optimizing a NER objective. This
creates word embeddings that are both
shared between languages and fine-tuned
for the NER task. As a proof of concept,
we demonstrate this model on English-to-
Chinese transfer using Wikipedia.

1 Introduction

Cross-lingual transfer is an important technique
for building natural language processing (NLP)
systems for low-resource languages, where la-
beled examples are scarce. The main idea is to
transfer labels or models from high-resource lan-
guages. Representative techniques include (a) pro-
jecting labels (or information derived from labels)
across parallel corpora (Yarowsky et al., 2011;
Das and Petrov, 2011; Che et al., 2013; Zhang
et al., 2016), and (b) training universal models us-
ing unlexicalized features (McDonald et al., 2011;
Täckström et al., 2012; Zirikly and Hagiwara,
2015) or bilingual word embeddings (Xiao and
Guo, 2014; Gouws and Søgaard, 2015).

Here, we focus on the bilingual word embed-
ding (BWE) approach. In particular, we are in-
terested in leveraging recent advances in learn-
ing BWE from comparable corpora (Hermann and

∗This research was majorly conducted when the author
was at Johns Hopkins University.

…

Stochastic Merging 

Comparable 
Document

Pseudo-bilingual 
Document

Bilingual Word 
Embeddings

…

…

…

… d

Skip-gram 
Model for BWEs

Unigram Model
for NER

ct

ct�1 ct+1 …

NER Training 
Sentence

x1, x2, · · · , xl
y1, y2, · · · , yl

yt

xt⇥ ⇤

c(S)1 , c
(S)
2 , · · · , c

(S)
J

c(T )1 , c
(T )
2 , · · · , c

(T )
K

c1, c2, · · · , cJ+K

V[v]V[1]

Figure 1: Our multi-task framework, which trains
bilingual word embeddings from comparable cor-
pora while optimizing an NER objective on the
high-resource language. The NER part of the
model is then tested on a low-resource language.

Blunsom, 2014; Vulic and Moens, 2015; Gouws
and Søgaard, 2015). A comparable corpus is a col-
lection of document pairs written in different lan-
guages but talking about the same topic (e.g. in-
terconnected Wikipedia articles). The advantage
of comparable corpora is that they may be more
easily acquired in the language and domain of in-
terest. However, cross-lingual transfer on compa-
rable corpora is more difficult than on parallel cor-
pora, due to the difficulty in finding high-quality
word translation equivalences.

Our contributions are two-fold: First, we inves-
tigate cross-lingual transfer on an NER task, and
found that pre-trained BWE’s do not necessarily
help out-of-the-box. This corroborates results in
the monolingual setting, where it is widely rec-
ognized that training task-specific embeddings is
helpful for the downstream tasks like NER (Peng
and Dredze, 2015; Ma and Hovy, 2016).

Second, we propose a multi-task learning
framework that utilizes comparable corpora to
jointly train BWE’s and the downstream NER task
(Figure 1). We experimented with a Wikipedia

383



corpus, training a NER model from labeled En-
glish articles (high-resource) and testing it on Chi-
nese articles (low-resource)1. The challenge with
training task-specific embeddings in cross-lingual
transfer is that the task in which we have labels
(English NER) is not equivalent to the task we care
about (Chinese NER). Despite this, we demon-
strate improvements on NER F-scores with our
multi-task model.

2 The Multi-task Framework

Assumptions : We assume two resources: First
is a comparable corpus where S (”source”) refers
to the high-resource language and T (”target”)
refers to the low-resource language. The compa-
rable corpus is denoted as C = {(c(S)i , c(T )i ) | i ∈
[1,M ]}, where each (c(S)i , c(T )i ) is a tuple of com-
parable documents written in S and T , and M is
the size (total number of tuples) of C.

We also assume a labeled NER corpus on the
high-resource language, which may be disjoint
from C. Let X(S) = {x(S)i | i ∈ [1, N (S)]} and
Y (S) = {y(S)i | i ∈ [1, N (S)]} together form the
NER training examples of S, where each y(S)i is
the gold tag sequence of sentence x(S)i , and N

(S)

is the number of training examples.

Training : Given X(S) and Y (S) and
C the training objective (loss) L is:

α Ln
X(S),Y (S)

(V,Λ) + (1− α)Lm
C

(V,Θ) (1)

Ln is the loss for training the NER tagger in
S, Lm is the loss for training the BWE’s, and
α ∈ [0, 1] is coefficient for balancing these two
losses. Λ, V and Θ are the model parameters,
where Λ is Ln-specific parameter, Θ is the Lm-
specific parameter and V is the d×v-shape BWE’s
that shared are by both Ln and Lm. v is the size
of the joint vocabulary V and V is formed by con-
catenating2 the vocabulary of S and T , d is the
dimension of the word embedding. Figure 1 gives
a visualization of the framework.

Evaluation : At test time, given X(T ) – the raw
sentences of T , we evaluate the F1 score of Ȳ (T )

predicted by the trained model {V∗,Λ∗} against
the true label Y (T ). Note that this model is trained

1Chinese can be considered a high-resource language for
NER, but we use it as a proof-of-concept and do not use any
existing Chinese resources.

2Same word in different languages is treated separately.

on NER labels in S only, so it is imperative for the
learned BWE’s to map S and T words with the
same NER label into nearby spaces.

Our framework (Equation 1) is flexible to differ-
ent definitions of Ln and Lm objectives. Below,
we describe a specific instantiation that fits well
with cross-lingual NER.

2.1 Design of Ln
Given the labeled training data X(S), Y (S) of S,
we optimize the conditional log-likelihood as Ln
(superscripts S are suppressed for readability):

Ln
X,Y

(V,Λ) =
1
N

N∑
i=1

log pV,Λ(yi | xi) (2)

pV,Λ(y | x) is the conditional probability of y
given x parameterized by V and Λ such that

pV,Λ(y | x) = exp(sV,Λ(x,y))∑
y′∈Y exp(sV,Λ(x,y′))

(3)

. sV,Λ(. . . ) is a score function of a sentence and
its possible NER-tag sequence. Y is the set of all
possible NER-tag sequences.

Unigram Model : Given a sentence x ∈ X with
length l and its label y ∈ Y , the score function of
unigram model is simply:

sV,Λ(x,y) =
∑

t∈[1,l]
V[xt]> ·W [yt] + b[yt] (4)

where Λ def= {W, b}, W is a d × n-shape matrix
that maps the word vector V[xt] into the NER-tag
space, n is the number of NER-tag types, which is
7 as will be explained in §3.1. b is the bias vector
with size n. The reason why we call this model
unigram is it only looks at the word itself without
its context.3

2.2 Design of Lm
Here we adopt the method of Vulic and Moens
(2015), which is the only mechanism for training
on comparable documents as far as we know: First
we transform C into a pseudo-bilingual corpora C′
by a stochastic merging process of two documents
as shown in Alg. 1. The idea is to mix together
the two documents in different languages into a
single document (where S and T words are inter-
spersed), then apply a standard monolingual word
embedding algorithm.

3Besides unigram, we also tried LSTM+CRF (Lample
et al., 2016) for longer context. Despite good results in mono-
lingual NER, it did poorly in our cross-lingual experiments.

384



Algorithm 1 Stochastic merging of two docu-
ments, where len(c) returns the number of tokens
of document c, c[i] is the ith token of document c.

Input: Comparable Document: c(S), c(T )
Output: Pseudo-bilingual Document: c

1: c← []; i1 ← 0; i2 ← 0
2: r ← len(c(S))

len(c(S))+len(c(T ))

3: while i1 < len(c(S)) and i2 < len(c(T )) do
4: p ∼ Uniform[0, 1]
5: if i2 == len(c(T )) or p < r then
6: c. append(c(S)[i1])
7: i1 ← i1 + 1
8: else
9: c. append(c(T )[i2])

10: i2 ← i2 + 1return c

We use the standard skip-gram objective
(Mikolov et al., 2013) by considering each
pseudo-bilingual document as a single sentence.
Lm
C

(V,Θ) is given by:

mean
c∈C′

mean
t∈[1,len(c)]

∑
−w≤j≤w

j 6=0

log pV,Θ(ct+j |ct) (5)

, where w is the word window, ct is the tth token
of c. pV,Θ(. . . ) is the standard context probability
parameterized by V and Θ such that

pV,Θ(cO|cI) = V[cI ]
> ·V′[cO]∑

c′O∈V V[cI ]
> ·V′[c′O]

(6)

Θ def= {V′}, where V′ is the context embedding
with size d × v. In the implementation, we use
negative sampling to save the computation, since
we desire using a large vocabulary to handle as
many words as possible for cross-lingual transfer.

2.3 Optimization
Full Joint (FJ) Training : First optimize Ln by
updating V and Λ. Then optimize Lm by updating
V, Θ. Repeat.

Half-fixed Joint (HFJ) Training : Same as FJ
Training, except in the Lm optimization step, the
English word embeddings are fixed and only the
Chinese word embeddings are updated. The mo-
tivation is to anchor the English embeddings to fit
the NER objective Ln, and encourage the Chinese
embedding (of comparable documents) to move
towards this anchor.

Inspired by Lample et al. (2016), for both ap-
proaches, words with frequency 1 in the NER data
are replaced by OOV with probability 0.5 to so
that embedding OOV could be optimized.

3 Experiments

3.1 Data

We use the EN-ZH portion of the Wikipedia Com-
parable Corpora4. For experiment purposes, we
sampled 19K document pairs5 as our comparable
corpora C. The NER labeled data on English (S)
is obtained by collecting the first paragraph of
each English document in C as X(S), and labeling
it with Stanford NER tagger (Finkel et al., 2005)
to generate Y (S).6

For the NER test data in Chinese (T ), we sep-
arately sampled 1K documents and collected the
first sentence as X(T ). We ran automatic word
segmentation7 and manually labeled X(T ) to gen-
erate Y (T ). The English side of these 1K tuple
is treated as held-out data for tuning NER hyper-
parameters, and is labeled with the same Stanford
NER tagger. We use the BIO tagging scheme for
3 basic named-entity types (“LOC” for location,
“ORG” for organization and “PER” for person),
so the output space is 7 tags. The data statistics
are shown in Table 1. The size of BWE’s is about
1M with 514K being Chinese words.

C X(S) X(T )
#Document Tuple 19K - -
#Sentence 1.8M 45K 1K
#Token 24M 994K 20K

Table 1: Data statistics.

3.2 Results

We compare our multi-task model with FJ and HFJ
alternate training8 against a baseline where BWE’s

4http://linguatools.org/tools/corpora/
wikipedia-comparable-corpora/

5We started from 20K pairs in totall, then we first sampled
out 1K from the Chinese side for the final evaluation and left
19K for training. We consider it as a reasonable number com-
pared to Vulic and Moens (2015), which used 14K pairs for
Spanish-English and 19K for Italian-English.

6We treat these automatic NER tags as ”gold” labels to
simulate a scenario where we want cross-lingual transfer on
T to do at least as well as on S. But naturally our model can
also use human annotations if available.

7https://github.com/fxsjy/jieba
8For training the BWE’s, we borrow the same hyperpa-

rameters as Vulic and Moens (2015) – learning rate of 0.025,

385



d Baseline HFJ FJ
64 6.54 13.5 7.01
128 14.95 20.27 17.14
256 13.69 24.29 16.53
512 21.23 17.7 20.14

Table 2: The F1 scores on the Chinese held-out
data averaged over multiple restarts. d is the di-
mension of the BWE’s. All the BWE’s are ini-
tialized by the output of Vulic and Moens (2015)
trained on C. “Baseline” fixes the BWE’s and only
trains Λ, “HFJ” and “FJ” are proposed joint train-
ing methods described in §2.3

are pre-trained on C, then held fixed when train-
ing a NER tagger. This baseline corresponds to a
two-step procedure where word embeddings pre-
trained on comparable corpora is used as features
when training an NER.9

Table 2 shows the F1 scores. We observe the
joint training methods (HFJ & FJ) outperform the
baseline method with embedding size 64, 128, and
256. For example, for d = 256, HFJ achieves
an F1 score of 24%, compared to the baseline of
13%, implying that jointly tuning the embedding
on both comparable corpora and NER objectives
(HFJ) is better than fine-tuning only NER objec-
tive after training on comparable corpora (base-
line). Further, HFJ results are better than FJ, im-
plying when optimizing Lm, it is better to tune the
Chinese embedding toward an anchored English
embedding, rather than allow both to be updated.

We note that the trend is different for d = 512:
it might be that as the NER model grows larger,
there is a risk of overfitting10 the English NER
data and losing generality on Chinese NER. We
observe similar trends when we replaced the uni-

negative sampling with 25 samples and subsampling rate of
value 1e-4. The dropout rate of 0.3 is decided by the best
F1 score of the language-specific NER tagger on the English
held-out data. The coefficient α for balancing two Ln and
Lm should presumably be chosen by tuning on the labelled
data for Chinese, which is not available in our setting. So we
heuristically set it to 0.5 by assuming they are equally help-
ful. Our fully unsupervised setting has no NER training data
available on the Chinese side for tuning. To prevent the train-
ing from overfitting to the English data, we heuristically early
stop after 10000 pairs of alternating updates of Lm and Ln.

9Another possible baseline is using only Ln for training,
this will not work because Ln only consists of English data,
so the Chinese embeddings will stay random when English
embedding are optimized, resulting random outputs on the
Chinese side.

10When training with d = 512, the F1 on training data is
consistently > 60 for the last epochs, which is about 50 with
d = 256.

gram model with a LSTM+CRF (see §2.1) in Ln.
This degraded results for all systems, e.g. with
d = 256, “Baseline” F1 dropped from 13.69 to
6.07, and “FJ” dropped from 16.53 to 8.62.

It is interesting to see the impact of joint train-
ing on BWE’s. In Table 3, given a Chinese query,
we show the most similar words in English re-
turned by computing the Euclidean distance be-
tween BWE’s (d = 256). While both pre-trained
and jointly-trained BWE’s retrieve correct English
translations, jointly-trained BWE’s retrieves more
words with the same NER type. For example,
“Spanish”, and “Greek” – the adjective forms of
“Spain” and “Greece”, rank highly with the pre-
trained BWE’s due to semantic similarity. But
these may degrade NER since these adjectives are
not labeled ”LOC”. Our multi-task model miti-
gates this confusion.

4 Conclusion & Future Work

We show how a multi-task learning approach can
help adapt bilingual word embeddings (BWE’s)
to improve cross-lingual transfer. Joint train-
ing of BWE’s encourages the BWE’s to be task-
specific, and outperforms the baseline of using
pre-trained BWE’s. We showed promising re-
sults on the challenging task of cross-lingual NER
on comparable corpora, where the target language
has no labels. Future work will aim to improve
the absolute F1 scores by combining limited la-
bels in the low-resource languages, via exploiting
document structure in Wikipedia (Richman and
Schone, 2008; Steinberger et al., 2011; Tsai et al.,
2016; Ni and Florian, 2016; Pan et al., 2017).
While we only focus on the most difficult case
where the source language and target languages
are not in the same family, and a bilingual dictio-
nary is not available in this paper, it is interesting
to study how this technique could be applied when
the different levels of supervision are available on
various language pairs in the future.

Acknowledgment

We thank Mark Dredze and Mo Yu for the useful
discussion and Jason Eisner for the suggestion of
some relevant previous work. Finally, we thank
the anonymous reviewers for the high-quality sug-
gestions.

386



Query Type Top 8 results in English
NBA
NBA

ORG
NBA, rebounds, Knicks, Lakers, Lewiston-Porter, 76ers, guard-forward, Celtics
NBA, Lakers, Gervin, rebounds, Celtics, Cavaliers, Knicks, All-Defensive

西班牙
Spain

LOC
Spain, Spanish, Nogueruelas, Rosanes, Mazarete, Ólvega, Marquesado, Montija
Spain, Rosanes, Cenicientos, Madrid, Sorita, Alcahozo, Nogueruelas, Villaralto

希腊
Greece

LOC
Greece, Greek, Achaia, annalistic, heroized, Gigantomachy, Hecabe, river-god
Hachadoor, Greece, Demoorjian, Safranbolu, Scicli, Holasovice, Sighisoara, Litomysl

卡卡
Kaka

PER
Kakashi, Moure, Uzumaki, cosplayed, Uchiha, humanizing, Yens, hilarious
Kakashi, Kaka, Moure, Nedved, Suazo, Batistuta, Uzumaki, Quagliarella

Table 3: Top similar words in the English given a Chinese query. “Type” is the gold named-entity type
of the query. For each query, the upper row is calculated with the baseline BWE’s, and the lower row is
calculated with HFJ BWE’s. The words with the same type as the query are bold-faced, and we observe
more of these cases with HFJ.

References
Wanxiang Che, Mengqiu Wang, Christopher D. Man-

ning, and Ting Liu. 2013. Named entity recogni-
tion with bilingual constraints. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 52–62, Atlanta,
Georgia. Association for Computational Linguistics.

Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ’11, pages 600–609, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Rose Jenny Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), pages 363–370. Association for Compu-
tational Linguistics.

Stephan Gouws and Anders Søgaard. 2015. Sim-
ple task-specific bilingual word embeddings. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1386–1390, Denver, Colorado. Association
for Computational Linguistics.

Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of ICLR.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270. Association for Computational Lin-
guistics.

Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end
sequence labeling via bi-directional lstm-cnns-crf.
CoRR, abs/1603.01354.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 62–72, Edinburgh, Scotland, UK. Asso-
ciation for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Jian Ni and Radu Florian. 2016. Improving multilin-
gual named entity recognition with wikipedia entity
type mapping. pages 1275––1284.

Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual name tagging and linking for 282 languages.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics.

Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for chinese social media with jointly
trained embeddings. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).

Alexander E Richman and Patrick Schone. 2008. Min-
ing wiki resources for multilingual named entity
recognition. In ACL, pages 1–9.

Ralf Steinberger, Bruno Pouliquen, Mijail Kabadjov,
Jenya Belyaeva, and Erik van der Goot. 2011.
Jrc-names: A freely available, highly multilingual
named entity resource. RECENT ADVANCES IN,
page 104.

387



Oscar Täckström, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 conference of the North American chap-
ter of the association for computational linguistics:
Human language technologies, pages 477–487. As-
sociation for Computational Linguistics.

Chen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016.
Cross-lingual named entity recognition via wikifi-
cation. In Proceedings of The 20th SIGNLL Con-
ference on Computational Natural Language Learn-
ing, pages 219–228, Berlin, Germany. Association
for Computational Linguistics.

Ivan Vulic and Marie-Francine Moens. 2015. Bilin-
gual word embeddings from non-parallel document-
aligned data applied to bilingual lexicon induc-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics (ACL
2015). ACL.

Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
parsing. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learning,
pages 119–129, Ann Arbor, Michigan. Association
for Computational Linguistics.

D. Yarowsky, G. Ngai, and R. Wicentowski. 2011. In-
ducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings of
the First International Conference on Human Lan-
guage Technology Research.

Dongxu Zhang, Boliang Zhang, Xiaoman Pan, Xi-
aocheng Feng, Heng Ji, and Weiran Xu. 2016. Bi-
text name tagging for cross-lingual entity annotation
projection. page 461–470.

Ayah Zirikly and Masato Hagiwara. 2015. Cross-
lingual transfer of named entity recognizers without
parallel corpora. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers), pages 390–396, Beijing, China. Associa-
tion for Computational Linguistics.

388


