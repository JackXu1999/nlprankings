



















































A Tutorial Markov Analysis of Effective Human Tutorial Sessions


Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 30–34
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

30

A Tutorial Markov Analysis of Effective Human Tutorial Sessions

Nabin Maharjan and Vasile Rus
Department of Computer Science / Institute for Intelligent Systems

The University of Memphis
Memphis, TN, USA

{nmharjan,vrus}@memphis.edu

Abstract

This paper investigates what differentiates
effective tutorial sessions from less effec-
tive sessions. Towards this end, we charac-
terize and explore human tutors’ actions in
tutorial dialogue sessions by mapping the
tutor-tutee interactions, which are streams
of dialogue utterances, into streams of ac-
tions, based on the language-as-action the-
ory. Next, we use human expert judgment
measures, evidence of learning (EL) and
evidence of soundness (ES), to identify ef-
fective and ineffective sessions. We per-
form sub-sequence pattern mining to iden-
tify sub-sequences of dialogue modes that
discriminate good sessions from bad ses-
sions. We finally use the results of sub-
sequence analysis method to generate a tu-
torial Markov process for effective tutorial
sessions.

1 Introduction

Identifying effective instructional strategies, i.e.,
strategies that induce learning gains, has been
a key research question in the Intelligent Tutor-
ing Systems (ITSs) community. There are two
common approaches used to address this research
question: (1) hypothesize and validate through
experimentation strategies guided by sound ped-
agogical theory (Aleven et al., 2001; Rus et al.,
2017a) and (2) discover strategies employed by
expert tutors (Boyer et al., 2011; Rus et al., 2015;
Ohlsson et al., 2007). In our work, we adopt the
latter approach which typically consists of mining
patterns associated with successful tutorial ses-
sions in large collections of recorded human tu-
toring sessions. (Boyer et al., 2011; Cade et al.,
2008; Rus et al., 2015; Ohlsson et al., 2007).

It is important to note that discovering effective

tutoring strategies by studying the strategies used
by expert tutors is challenging because what char-
acterizes tutoring expertise is still an open ques-
tion to some degree (Rus et al., 2015). A tutor who
employs sound strategies may appear less expert
when working with students having low abilities
or lacking in motivation. On the other hand, an
average tutor may seem expert if s/he only works
with high ability and highly motivated students.
We lack student ability and prior knowledge infor-
mation in our data and therefore focus on effective
tutoring rather than expert tutoring. Effective tu-
toring refers to tutoring that yields learning gains.
In sum, we study in this paper strategies of effec-
tive tutors as reflected in effective tutorial sessions.

In this paper, we worked with an annotated cor-
pus of human tutoring sessions from which we
identified effective sessions based on human ex-
pert judgments (see Section 5). We mapped tu-
torial sessions onto sequences of dialogue acts
and dialogue modes (Cade et al., 2008), explained
later, using a predefined coding taxonomy (see
Section 3). We then conducted sub-sequence pat-
tern mining to identify sub-sequences of dialogue
modes that occur in effective tutoring sessions
but not in ineffective tutoring sessions. We used
these distinctive sub-sequences of modes to build a
Markov process for effective tutorial sessions. Fi-
nally, using the tutorial Markov process, we ana-
lyzed and searched for dialogue mode patterns as-
sociated with effective tutoring sessions.

2 Related Work

Discovering the structure of tutorial dialogues and
tutors’ strategies has been a main goal of the in-
telligent tutoring research community from the
very beginning because such tutorial session struc-
tures and strategies must be understood in order
to be replicated in the development of effective



31

ITSs. Graesser et al. (1995) proposed a five-step
general structure of collaborative problem solv-
ing during tutoring. Cade et al. (2008) examined
likely sequences of dialogue modes in expert tu-
toring. Boyer and colleagues (2009; 2010) applied
hidden Markov models to discover effective dia-
logue modes inherent in the tutoring sessions. Rus
et al. (2017b) used a supervised machine learning
method to automatically map tutorial sessions into
dialogue acts, sub-acts and modes and then ana-
lyzed human tutoring sessions using profile com-
parison and sequence logos to discover effective
tutorial strategies in terms of dialogue acts and
modes. Our work further contributes to this area
of research by characterizing effective tutorial ses-
sions in terms of dialogue mode sequential pat-
terns and tutorial Markov processes.

3 Coding Taxonomy

The role of the coding taxonomy is to help us
map tutors and tutees’ utterances in tutorial di-
alogues onto actions, i.e., dialogue acts, based
on the language-as-action theory (Austin, 1975;
Searle, 1969) according to which when we say
something we do something. For example, the ut-
terance: “There is an useful idea called ‘conserva-
tion of energy”’ is categorized as an Assertion dia-
logue act, i.e., the utterance is making an assertion.
Because the assertion is about “conservation of
energy”, a Concept, we consider this as a special-
ized assertion about a concept, i.e., an Assertion-
Concept dialogue act-subact combination.

We group sequences of dialogue acts and sub
acts into higher level constructs, i.e., dialogue
modes. Dialogue modes represent contiguous
sequences of dialogue acts-subacts that together
serve particular pedagogical purposes, e.g., a se-
quence of hints in the form of questions may re-
flect a scaffolding instructional strategy in which
the tutee works mostly by herself on the current
instructional task while the tutor offers help, when
needed, through such hints.

The dialogue act and mode taxonomies are
adapted to our context from a set of earlier tax-
onomies which were created to analyze a large
corpus of online tutoring sessions conducted by
human tutors in the domains of Algebra and
Physics (Morrison et al., 2014). There are 17
top level expert-defined dialogue act categories.
Each dialogue act category may have 4 to 22
subcategories or sub-acts. For example, we dis-

Annotation Agreement (%) Kappa
Act 77 0.72
Act-subact 62 0.60
Mode 44 0.37
Mode∗ 53.8 0.48
Mode∗∗ 64.3 0.60

Table 1: Average Inter Annotator Agreement Be-
tween Two Independent Annotators. Mode∗ and
Mode∗∗ represent dialogue mode agreement be-
tween verifier and first annotator and, verifier and
second annotator respectively.

tinguish Assertions that reference aspects of the
tutorial process itself (Assertion:Process); do-
main concepts (Assertion:Concept), or the the
use of lower-level mathematical calculations (As-
sertion:Calculation). Further, we have a set of
17 dialogue modes: Assessment, Closing, Fad-
ing, ITSupport, Metacognition, MethodID, Mod-
eling, OffTopic, Opening, ProblemID, ProcessNe-
gotiation,RapportBuilding, RoadMap, SenseMak-
ing, Scaffolding, SessionSummary and Telling. A
detailed description of the dialogue modes is de-
scribed by Morrison and colleagues (Morrison
et al., 2014).

4 Data

A large corpus of about 19K tutorial sessions
between professional human tutors and actual
college-level, adult students on Algebra domain
was collected via an online human tutoring ser-
vice. 500 tutorial sessions containing 31,299 ut-
terances were randomly selected for annotation.
The sessions were manually labeled by a team of 6
subject matter experts (SMEs). They were trained
on the taxonomy of dialogue acts, sub-acts, and
modes. Each session was manually tagged by two
independent annotators without looking at each
other’s tags to eliminate any labeling bias prob-
lems. The tags of the two independent annotators
were double-checked by a verifier who resolved
discrepancies in tags, if any. It should be noted
that though the average inter-annotator agreement
is apparently low, the final agreement of annota-
tors with the verifier is higher (see Table 1). The
verifier also happened to be the designer of our
dialogue taxonomy. In this paper, dialogue mode
should be interpreted as dialogue mode-switch.



32

Mode sub-sequences (p-value in bracket)
Fading-Closing (0.0002)
Scaffolding-Scaffolding (0.0008)
Fading (0.0009)
Scaffolding-Fading (0.0055)
Fading-Fading (0.01)
ProblemID-Fading (0.02)
ProblemID-Scaffolding-Scaffolding(0.0362)
Closing (0.05)
Fading-RapportBuilding (0.05)
Fading-ProcessNegotiation (0.06)
Fading-Scaffolding (0.07)
Fading-ProcessNegotiation-Closing (0.07)
Fading-Fading-Scaffolding (0.08)
ProblemID-Fading-Scaffolding (0.10)
Fading-Scaffolding-Fading (0.14)
Scaffolding-Closing (0.15)
Opening-ProblemID-Fading (0.18)
RapportBuilding (0.18)
Fading-Scaffolding-Closing (0.18)
Scaffolding-Fading-Scaffolding (0.20)
Scaffolding-RapportBuilding (0.23)
Fading-Scaffolding-ProcessNegotiation (0.26)
RapportBuilding-Closing(0.37)

Table 2: Discriminant mode sub-sequences.

5 Markov Analysis of Tutorial Sessions

In order to identify effective sessions, the SMEs
also rated each tutorial session using a 1-5 scale (5
being best score) along two dimensions: evidence
of learning (EL) and evidence of soundness (ES).
The ES score is supposed to measure the degree to
which the tutor applied pedagogically sound tac-
tics. On the other hand, the EL score indicates
whether there is strong evidence that the student
learned from the tutoring session. The ES and EL
distinction was designed in order to separate con-
founding factors such as learners’ engagement in
the session. For instance, a tutor may apply sound
pedagogy but the student may not learn as all they
might be interested is to find a quick answer to
their (homework) problem. It should be noted that
most of the sessions we had access to were in the
context of homework help. That is, students start a
session by asking for help with a particular prob-
lem. While EL and ES were supposed to cap-
ture different things, the EL and ES scores were
found to be highly correlated (Pearson co-efficient
of 0.7).

We used both EL and ES to capture overall qual-

ity of tutoring sessions. We categorized all ses-
sions having ES and EL scores ≤ 2 as ineffective,
and all sessions rated with ES = 5 and EL ≥ 4 as
effective.

We conducted discriminant mode sub-sequence
analysis using Traminer package in R. It should
be noted that a sub-sequence is not necessarily
a contiguous sequence of observations, however,
the order of the observations is preserved. For ex-
ample, (Fading)-(Closing) is a valid sub-sequence
of dialogue modes formed from the (Fading)-
(ProcessNegotiation)-(Closing) contiguous ses-
sion fragment. We generated sub-sequences up to
length 7 from all annotated tutorial sessions.

The Traminer algorithm first finds the most
frequent sub-sequences by counting their distinct
occurrences and then applies a Chi-squared test
(Bonferroni-adjusted) to identify sub-sequences
that are statistically more (or less) frequent in each
group. We used a p-value < 0.4 to generate a suf-
ficient number of likely distinctive sub-sequences
of modes (Table 2). Once the significant sub-
sequences were identified, we generated a state
transition matrix, explained next.

5.1 State Transition Matrix

We created a state transition matrix with modes
as the states. We ignored sub-sequences of
unit length as they don’t indicate an observed
transition. For sub-sequences spanning more
than two states, we split them into multiple bi-
gram sub-sequences. For example, we obtained
bigram sub-sequences Opening-ProblemID and
ProblemID-Fading from the Opening-ProblemID-
Fading sub-sequence. We discarded self-transition
paths since modes are actually mode switches
in our case. Therefore, we discarded transition
path Scaffolding-Scaffolding from the ProblemID-
Scaffolding-Scaffolding sub-sequence.

We used confident scores of discriminant sub-
sequences to compute transition probabilities. The
confidence score of a discriminant sub-sequence
is the probability that the sub-sequence is com-
ing from an effective session, i.e., 1 - p-value.
We computed a confidence score of a path as
the confidence score of the discriminant sub-
sequence the path belongs to. For example,
for Opening-ProblemID-Fading (0.18), the con-
fidence score of paths Opening-ProblemID and
ProblemID-Fading is 1-0.18 = 0.82. We weighted
an edge as the cumulative sum of its confi-



33

Figure 1: Tutorial Markov Process for effective tutorial sessions.

dence scores from all the sub-sequences where the
path is present. For example, path ProblemID-
Fading is present in sub-sequences ProblemID-
Fading (0.02) and Opening-ProblemID-Fading
(0.18). So, the weight of the path ProblemID-
Fading is: 0.98 + 0.82 = 1.8. Finally, we nor-
malized the weight of each path A-B to represent
transition probability by dividing it by the sum
of the weights of all possible transitions from A.
For example, the weight of the path ProblemID-
Fading is normalized by dividing it by the sum
total of weights of all the transitions from Prob-
lemID state.

5.2 Tutorial Markov Interpretation
Figure 1 shows the state transition graph of the
underlying Markov process corresponding to the
above state transition matrix. In the figure, the
states are dialogue modes whereas transitions
are generated using only the discriminant sub-
sequences of modes. Each path has been labeled
with the corresponding transition probabilities.

The Markov process reveals that any sequence
of modes it can generate starts with an Open-
ing and ends with a Closing state and is likely
to have a large number of Scaffolding - Fad-
ing switches/transitions. This result partly sup-
ports theoretical expert tutoring models based
on the modeling-scaffolding-fading paradigm (Ro-
goff and Lave, 1984). The high occurrences of
these modes provide evidence that effective tu-
tors monitor and engage students more and pro-
vide help only when needed. Cade et al. (2008)
also found that Scaffolding was a highly occur-

ring mode in expert tutoring. They found a rela-
tively low occurrence of the Fading mode, which
they suggested might be explained by time con-
straints, i.e. the tutoring session prevented tutors
from spending too much time in the Fading mode.

The Markov process also resembles to some
degree Graesser’s (Graesser et al., 1995) 5-step
dialogue framework, which captures the tutorial
phases prevalent in collaborative problem-solving
tutoring: i) Tutor asks question, ii) Student an-
swers question, iii) Tutor gives short feedback,
iv)Tutor and student collaboratively improve the
quality of the answer, v) Tutor assesses student’s
understanding. One probable effective tutorial
path from the Markov process, which might be
comparable to Graesser’s framework, is Opening
- ProblemId - Fading - Scaffolding - Fading -
ProcessNegotiation - Closing. Indeed, the sub-
path ProblemId - Fading - Scaffolding - Fading
- ProcessNegotiation resembles Graesser’s 5-step
framework.

The first 3 phases in Graesser’s framework don’t
align with the initial modes of the suggested learn-
ing path. This might be because of the differ-
ence in the tutoring environment. Graesser as-
sumed tutor-driven sessions, which with a tutor
first asking a question or presenting a problem for
the learner to solve, followed by a student answer,
etc. In our case, it is the students who are seeking
help from tutors on specific problems. Initially, in
our case, the tutor works together with the student
to understand the problem (ProblemId). Then, the
tutor fades, allowing the student to work on the
problem by herself (Fading). The tutor may switch



34

between Scaffolding and Fading to provide help
(Scaffolding), only when needed. In this sense,
the last two elements in Graesser’s framework can
be considered to be aligned with the Scaffolding -
Fading pattern.

The additional benefit of this Markov process
representation is that it suggests multiple possible
paths or meta-strategies that can lead to learning
gains.

6 Conclusion

We used human expert judgment scores to iden-
tify effective and ineffective tutoring sessions. We
conducted discriminant mode sub-sequence anal-
ysis based on which we generated a Markov pro-
cess for effective tutorial sessions. We found that
sequences of dialogue modes derived from the
Markov process are most likely to have many Scaf-
folding and Fading modes. Furthermore, the in-
ferred Markov process suggests a new model for
tutoring when students ask for help as opposed to
tutor-driven sessions, which was modeled in the
past. Our future work is to expand our understand-
ing of the effective strategies in effective tutorial
sessions while accounting for other factors such
as students’ prior knowledge.

Acknowledgments

This work was partially supported by The Univer-
sity of Memphis and a contract from the Advanced
Distributed Learning Initiative of the United States
Department of Defense.

References
Vincent Aleven, Octav Popescu, and Kenneth R

Koedinger. 2001. Towards tutorial dialog to sup-
port self-explanation: Adding natural language un-
derstanding to a cognitive tutor. In Proceedings of
Artificial Intelligence in Education, pages 246–255.

John Langshaw Austin. 1975. How to do things with
words. Oxford university press.

Kristy Elizabeth Boyer, Eunyoung Ha, Michael D Wal-
lis, Robert Phillips, Mladen A Vouk, and James C
Lester. 2009. Discovering tutorial dialogue strate-
gies with hidden markov models. In AIED, pages
141–148.

Kristy Elizabeth Boyer, Robert Phillips, Amy Ingram,
Eun Young Ha, Michael Wallis, Mladen Vouk, and
James Lester. 2010. Characterizing the effective-
ness of tutorial dialogue with hidden markov mod-
els. In International Conference on Intelligent Tu-
toring Systems, pages 55–64. Springer.

Kristy Elizabeth Boyer, Robert Phillips, Amy Ingram,
Eun Young Ha, Michael Wallis, Mladen Vouk, and
James Lester. 2011. Investigating the relationship
between dialogue structure and tutoring effective-
ness: a hidden markov modeling approach. Interna-
tional Journal of Artificial Intelligence in Education,
21(1-2):65–81.

Whitney Cade, Jessica Copeland, Natalie Person, and
Sidney DMello. 2008. Dialogue modes in expert tu-
toring. In Intelligent tutoring systems, pages 470–
479. Springer.

Arthur C Graesser, Natalie K Person, and Joseph P
Magliano. 1995. Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied cognitive
psychology, 9(6):495–522.

Donald Morrison, Benjamin Nye, Borhan Samei,
Vivek Varma Datla, Craig Kelly, and Vasile Rus.
2014. Building an intelligent pal from the tutor. com
session database phase 1: Data mining. In Educa-
tional Data Mining 2014.

Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow,
Davide Fossati, Xin Lu, and Trina C Kershaw. 2007.
Beyond the code-and-count analysis of tutoring dia-
logues. Artificial intelligence in education: Build-
ing technology rich learning contexts that work,
158:349.

Barbara Ed Rogoff and Jean Ed Lave. 1984. Everyday
cognition: Its development in social context. Har-
vard University Press.

Vasile Rus, Rajendra Banjade, Nobal Niraula, Eliz-
abeth Gire, and Donald Franceschetti. 2017a. A
study on two hint-level policies in conversational in-
telligent tutoring systems. In Innovations in Smart
Learning, pages 171–181. Springer.

Vasile Rus, Nabin Maharjan, and Rajendra Banjade.
2015. Unsupervised discovery of tutorial dialogue
modes in human-to-human tutorial data. In Pro-
ceedings of the Third Annual GIFT Users Sympo-
sium, pages 63–80.

Vasile Rus, Nabin Maharjan, Tamang Lasang, Michael
Yudelson, Susan Berman, Fancsali Stephen, and
Steve Ritter. 2017b. An analysis of human tutors
actions in tutorial dialogues. In Proceedings of
the Thirtieth International Florida Artificial Intelli-
gence Research Society Conference.

John R Searle. 1969. Speech acts: An essay in the phi-
losophy of language, volume 626. Cambridge uni-
versity press.


