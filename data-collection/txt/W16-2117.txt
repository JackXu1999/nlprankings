



















































Towards a text analysis system for political debates


Proceedings of the 10th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH), pages 134–139,
Berlin, Germany, August 11, 2016. c©2016 Association for Computational Linguistics

Towards a text analysis system for political debates

Dieu-Thu Le
IMS, Institute for NLP
University of Stuttgart

Germany

Ngoc Thang Vu
IMS, Institute for NLP
University of Stuttgart

Germany
{dieu-thu.le,thang.vu,andre.blessing}@ims.uni-stuttgart.de

Andre Blessing
IMS, Institute for NLP
University of Stuttgart

Germany

Abstract

Social scientists and journalists nowadays
have to deal with an increasingly large
amount of data. It usually requires ex-
pensive searching and annotation effort to
find insight in a sea of information. Our
goal is to build a discourse analysis sys-
tem which can be applied to large text col-
lections. This system can help social sci-
entists and journalists to analyze data and
validate their research theories by provid-
ing them with tailored machine learning
methods to alleviate the annotation effort
and exploratory facilities and visualization
tools. We report initial experimental re-
sults in a case study related to discourse
analysis in political debates.

1 Introduction

The overall goal of our project is to develop an in-
teractive research environment for text collections
that (a) puts state-of-the-art text analysis models
from Computational Linguistics in the hands of
social scientists or data journalists, allowing them
to quickly tailor search facilities and filters to their
research goal, i.e., finding and categorizing textual
passages in the collection that instantiate a rele-
vant position towards an issue under exploration.
The environment furthermore (b) relates the cat-
egorized positions, or claims, to the uttering ac-
tors, capturing dates of utterance, the relation to
relevant mentioned entities, and (c) provides ex-
ploratory facilities and visualization tools for per-
forming time-series analysis and network analy-
sis on aggregated text-analytical results, includ-
ing differential analysis against trends observed
in previous legislation processes. By keeping all
backward links from aggregated results to the in-
dividual underlying text sources, the environment

readily supports (d) a critical assessment of the
analysis and (e) a transparent presentation of the
data basis of a news story.

A major side-effect of the project is to engage
in an exchange among two different explorative
points of view towards large heterogeneous data
collections: social scientists and journalists on
the one hand have certain intuitions and strate-
gies how to proceed when they first approach a
collection which they suspect to contain some
newsworthy evidence. They cannot know how-
ever which substeps in their strategy can be sup-
ported or taken over by sufficiently reliable au-
tomatic means. Computational linguists on the
other hand have a wide range of analytical tools
at their disposal, they know how to adapt them
to specifics of some application context, and they
are able to combine tools to solve more complex
structural questions about a text. However, ideas
for completely novel types of complex analyti-
cal questions about a text collection have to come
from outside of Computational Linguistics - so
professional investigators of novel questions are
highly interesting partners for developing explo-
rative strategies.

In the next sections, we will report the first ex-
perimental results, which were carried out on an
already annotated dataset to illustrate how the sys-
tem could be used to assist social scientists and
journalists to analyze data.

2 Approach

Argumentation mining is an arising research topic
(Peldszus and Stede, 2013; Moens, 2013) which
models argumentation in textual content. Most
theories propose that each argumentation consists
of two parts: i) the premise and ii) the conclu-
sion/claim. For discourse network analysis only
claims and the actor behind is relevant. Further-

134



more, our first analysis of existing labeled data
showed that there are large divergences in the
way claims are annotated in the different com-
munities. Thus, we have chosen a task-driven
approach, instead of a theory-driven approach,
which is defined by actual questions of the jour-
nalists and social scientists on large text collec-
tions. Which means, that we follow a supervised
approach since we use a seed of already annotated
text segments. Nevertheless the annotation1 pro-
cess is also well-defined by complex codebooks
(Koopmans, 2002).

3 Case Study: The debate of nuclear
power phase-out

In March 2011, Japanese earthquake and tsunami
caused a nuclear accident in Fukushima, which
prompted a critical re-thinking of nuclear power.
Germany witnessed a radical political change to-
wards an accelerated phasing out of nuclear re-
actors as an immediate reaction to the disaster.
The sudden changes in decisions could not be ex-
plained by traditional political science theories. A
few months before the accident, an agreement re-
lated to prolonging of nuclear energy use had been
made, but was quickly withdrawn after the energy
debate and set the final exit date to the year 2022.

A political science group in Bremen (Haunss et
al., 2013) has proposed using discourse network
analysis to find a plausible explanation. They ex-
amined articles in two Germany newspapers pub-
lished during this time. They argued that actor
centrality, consistency and cohesion of discourse
coalition could be used to explain the fast devel-
opment in political changes.

4 Problem statement

The problems of identifying factors for text analy-
sis of the political science group could be stated in
machine learning tasks as follows (Figure 1):

Claim vs. Non-claim classification In our case
study, claims are defined to be sentences related to
political opinions and decisions of actors, while
non-claims are general statements without con-
tent about political decision. In the first step,
claims are extracted from articles. We train a claim
classification that learns from some pre-annotated
claims and help the annotators to automatically
find other relevant claims.

1Social scientists use often the term coding instead of an-
notation.

Figure 1: A computational linguistic pipeline for
text analysis with main steps: Claim, actor extrac-
tion and event detection

Actor extraction One major part of the dis-
course analysis is to identify actors associated to
each claim. We argue that using Named Entity
recognition, the system can propose possible can-
didates for each claim and help annotators to select
correct actors faster. The names of actors are usu-
ally mentioned within a claim itself or within the
article where the claim is stated. By proposing a
ranked list of named entities of type Person and
Organization, the annotators can browse through
the list of suggestions and select the correct one.

Topic estimation, trend and event detection In
this pipeline, we use topic models (Blei et al.,
2003) as a way to browse and summarize articles
by dates and find out which topics/events are im-
portant. Firstly, a topic model is estimated from
all articles. After that, we use this model to in-
fer topics for claims grouped by dates. The topic
distribution over time can be used to detect impor-
tant events and to have an overview of what topics
were discussed during which time.

5 Models and experiments

5.1 Term extraction
Figure 2 shows top terms that appear in claims and
non-claims using term frequency (TF) and term
extraction (TE). In term frequency, we counted
how many times a term appears in all claims or
non-claims. In term extraction, we compare how
important a term is in the dataset in compared to
the term appearing in a reference corpus, which is
a collection of online German news articles.

The first glance at the top extracted terms from
claims and non-claims suggests that terms in both
categories are very similar. A traditional bag-of-

135



Figure 2: Term extraction from claims and unla-
beled data

word approach may not be sufficient to distinguish
them to suggest appropriate claims for the annota-
tors. Following, we present our claim classifica-
tion method using deep learning to automatically
detect important features for finding claims.

5.2 Claim classification

5.2.1 Settings
Claim classification can be considered as a sen-
tence classification task. Hence, we applied con-
volutional neural networks (CNNs) - a state-of-
the-art method (Kalchbrenner et al., 2014; Kim,
2014) for this task. CNNs perform a discrete con-
volution on an input matrix with a set of differ-
ent filters. The input matrix represents a sentence,
i.e. each column of the matrix stores the word
embedding of the corresponding word. Word em-
bedding can be randomly initialised or pre-trained
with unsupervised training method. In both cases,
we fine-tuned the embeddings during the network
training. By applying a filter with a width of e.g.
three columns, three neighbouring words (trigram)
are convolved. Afterwards, the convolution re-
sults are pooled. In this work, our model used
filters of width 3-5 with 100 filters each. Fol-
lowing (Collobert et al., 2011), we perform max-
pooling which extracts the maximum value for
each filter and, thus, the most informative n-gram
for the following steps. Finally, the resulting val-
ues are concatenated and used for claim classifi-
cation. To train the network we used stochastic
gradient descent with a mini-batch size of 50 and
AdaDelta (Zeiler, 2012) to adapt learning rate af-
ter each epoch. We pre-trained word embeddings
with word2vec2 using 99M German sentences col-
lected from the news and Wikipedia. Motivated by
the fact that claims are independent from person or

2https://code.google.com/archive/p/word2vec/

organization, we replaced all named entities with
NE tags to improve the generalization of the net-
work.

5.2.2 Results
In total, we have 1,837 sentences which are man-
ually annotated as claims and 12,033 non-claim
sentences. It is, however, not clear whether non-
claim sentences are manually cross checked (if all
non-claim sentences contain no claim at all). Fur-
thermore to balance the claims:non-claim ratio,
we randomly picked only 1,837 non-claim sen-
tences. Table 1 summarized the average F1-scores
on a 10-fold cross-validation with different experi-
mental setups. Our results revealed that using pre-
trained word embeddings and replacing all named
entities with their corresponding tags are useful to
improve the final performance.

Table 1: F1 score for claim classification
Systems F1-score
using random initialized word embs 67.5%
+ replace NEs 68.5%
using pretrained word embs 70.3%
+ replace NEs 70.6%

5.3 Named Entities
We applied Named Entity recognition using Con-
ditional Random Field explained in (Finkel et al.,
2005) and the German model prepared by (Faruqui
and Padó, 2010) to recognize entities in all claims.
We used Person and Organization named entities
to prepare a list of suggested actors for each claim.

We carried out two experiments: in the first one,
only sentences where claims are annotated were
used to extract named entities from; and in the sec-
ond one, we further expanded to all sentences in
articles that contain claims. The results are shown
in Table 2, where 71.2% of actors could be found
within the suggested named entity list extracted
from articles where claims are annotated.

Table 2: Percentage of actors detected using NER
in claims

using only sentences containing claims 51%
using articles containing claims 71.2%

5.4 Topic browsing - trend detection
Firstly, we estimated a topic model with 20 top-
ics from all articles. Then we grouped claims by
dates and inferred topics for these claims. We pro-
vide a visualization tool for social scientists to per-
form time-series analysis. Figures 3, 4, 5 show
the topic distribution of claims over time. Figure

136



Figure 5: Topic timeline of claims related to CDU and Angela Merkel

Figure 3: Discussion related to energy changing
and energy companies

3 shows that discussions related to the topic of en-
ergy changing heated up after the nuclear catastro-
phe in Japan, which involves statements of energy
companies, their reactions and debates on prob-
lems such as payments in the energy and climate
funds, finding repositories for nuclear waste. Im-
portant events related to the setup of security and
ethic commissions to examine the safety of nu-
clear reactors can be spotted from Figure 4.

Finally, we grouped claims based on actors and
do topic inference for these claims over time. Fig-
ure 5 shows an example of a topic timeline for the
CDU party and Angela Merkel. Some events re-
lated to the election results and nuclear company
reactions to the government can be spotted from
the timeline (e.g., election in Baden-Württemberg
(BW) - the first time CDU lost the presidential
mandate, final decision of the federal state regard-
ing nuclear phaseout, an energy company suing
the government).

Figure 4: Timeline of discussion related to secu-
rity and ethic commissions

6 Related work

Textual content analysis in social science is still
a handcrafted discipline which requires manual
annotations (Baumgartner et al., 2008; Bruycker
and Beyers, 2015; Koopmans and Statham, 1999).
The main drawback besides the expensive man-
ual work is that for each research questions the
whole process has to be repeated. In contrast
to other content analysis systems (Bamman and
Smith, 2015; Qiu et al., 2015; Levy et al., 2014;
Slonim et al., 2014) our approach can be seen as
a bottom-up task-driven approach instead of a top-
down approach based on the theory of argumenta-
tion (Moens, 2013).

7 Conclusions

In this paper, we have presented our first experi-
mental results on building a tool to facilitate re-
search in political and social science using dis-
course analysis. In particular, we focus on three
tasks involving claim extraction, actor identifica-

137



tion and timeline visualization for detecting im-
portant events and topics. In our case study, all
data has been manually annotated. Our initial re-
sults show that this manual annotation process can
be accelerated with the assistance of tailored state-
of-the-art machine learning systems: for claim ex-
traction, a fine-tuned word embedding system can
achieve up to 70% F1-score when taking into ac-
count automatically tagged persons and organiza-
tions; for actor extraction, 71% of actors can be
found using named entity recognition. Finally, we
show how topic timelines could be used to spot
important events related to the debate.

Acknowledgments

This research was supported by CRETA - Cen-
ter for Reflected Text Analytics funded by the
German Federal Ministry of Education and Re-
search (BMBF) and by the project DebateExplorer
funded by the VolkswagenStiftung.

References
David Bamman and Noah A. Smith. 2015. Open ex-

traction of fine-grained political statements. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 76–85,
Lisbon, Portugal, September. Association for Com-
putational Linguistics.

Frank R Baumgartner, Suzanna L De Boef, and Am-
ber E Boydstun. 2008. The decline of the death
penalty and the discovery of innocence. Cambridge
University Press.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.

Iskander De Bruycker and Jan Beyers. 2015. Balanced
or biased? interest groups and legislative lobbying in
the european news media. Political Communication,
32(3):453–474.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Richard Eckart de Castilho, Chris Biemann, Iryna
Gurevych, and Seid Muhie Yimam. 2014. We-
banno: a flexible, web-based annotation tool for
clarin. Proceedings of the CLARIN Annual Confer-
ence (CAC) 2014, October.

Manaal Faruqui and Sebastian Padó. 2010. Train-
ing and evaluating a german named entity recog-
nizer with semantic generalization. In Proceedings
of KONVENS 2010, Saarbrücken, Germany.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 363–370, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Sebastian Haunss, Matthias Dietz, and Frank
Nullmeier. 2013. Der Ausstieg aus der Atom-
energie. Diskursnetzwerkanalyse als Beitrag zur
Erklärung einer radikalen Politikwende. Zeitschrift
für Diskursforschung, 1(3):288–316.

Nal Kalchbrenner, Edward Grefenstette, and Phil
Blunsom. 2014. A convolutional neural net-
work for modelling sentences. arXiv preprint
arXiv:1404.2188.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Ruud Koopmans and Paul Statham. 1999. Political
claims analysis: integrating protest event and polit-
ical discourse approaches. Mobilization: An Inter-
national Quarterly, 4(2):203–221.

Ruud Koopmans. 2002. Codebook for the
analysis of political mobilisation and com-
munication in european public spheres.
http://europub.wzb.eu/Data/Codebooks

Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud
Aharoni, and Noam Slonim. 2014. Context de-
pendent claim detection. In COLING, pages 1489–
1500.

Marie-Francine Moens. 2013. Argumentation mining:
Where are we now, where do we want to be and how
do we get there? In Proceedings of the 5th 2013
Forum on Information Retrieval Evaluation, page 2.
ACM.

Andreas Peldszus and Manfred Stede. 2013. From ar-
gument diagrams to argumentation mining in texts:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence (IJCINI), 7(1):1–31.

Minghui Qiu, Yanchuan Sim, Noah A. Smith, and Jing
Jiang. 2015. Modeling user arguments, interactions,
and attributes for stance prediction in online debate
forums. In Suresh Venkatasubramanian and Jieping
Ye, editors, Proceedings of the 2015 SIAM Interna-
tional Conference on Data Mining, Vancouver, BC,
Canada, April 30 - May 2, 2015, pages 855–863.
SIAM.

Srikrishna Raamadhurai, Oskar Kohonen, and Teemu
Ruokolainen. 2014. Creating custom taggers by in-
tegrating web page annotation and machine learn-
ing. In Proceedings of the Conference System
Demonstrations , COLING, pages 15–19.

138



Noam Slonim, Ehud Aharoni, Carlos Alzate Perez,
Roy Bar-Haim, Yonatan Bilu, Lena Dankin,
Iris Eiron, Daniel Hershcovich, Shay Hummel,
Mitesh M. Khapra, Tamar Lavee, Ran Levy, Paul
Matchen, Anatoly Polnarov, Vikas C. Raykar, Ruty
Rinott, Amrita Saha, Naama Zwerdling, David
Konopnicki, and Dan Gutfreund. 2014. Claims
on demand - an initial demonstration of a sys-
tem for automatic detection and polarity identifi-
cation of context dependent claims in massive cor-
pora. In Lamia Tounsi and Rafal Rak, editors, COL-
ING 2014, 25th International Conference on Com-
putational Linguistics, Proceedings of the Confer-
ence System Demonstrations, August 23-29, 2014,
Dublin, Ireland, pages 6–9. ACL.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

139


