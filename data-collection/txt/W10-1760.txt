










































Integration of Multiple Bilingually-Learned Segmentation Schemes into Statistical Machine Translation


Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Integration of Multiple Bilingually-Learned Segmentation Schemes
into Statistical Machine Translation

Michael Paul and Andrew Finch and Eiichiro Sumita
MASTAR Project

National Institute of Information and Communications Technology
Hikaridai 2-2-2, Keihanna Science City

619-0288 Kyoto, Japan
michael.paul@nict.go.jp

Abstract

This paper proposes an unsupervised
word segmentation algorithm that identi-
fies word boundaries in continuous source
language text in order to improve the
translation quality of statistical machine
translation (SMT) approaches. The
method can be applied to any language
pair where the source language is unseg-
mented and the target language segmen-
tation is known. First, an iterative boot-
strap method is applied to learn multi-
ple segmentation schemes that are consis-
tent with the phrasal segmentations of an
SMT system trained on the resegmented
bitext. In the second step, multiple seg-
mentation schemes are integrated into a
single SMT system by characterizing the
source language side and merging iden-
tical translation pairs of differently seg-
mented SMT models. Experimental re-
sults translating five Asian languages into
English revealed that the method of in-
tegrating multiple segmentation schemes
outperforms SMT models trained on any
of the learned word segmentations and
performs comparably to available state-of-
the-art monolingually-built segmentation
tools.

1 Introduction

The task of word segmentation, i.e., identifying
word boundaries in continuous text, is one of the
fundamental preprocessing steps of data-driven
NLP applications like Machine Translation (MT).
In contrast to Indo-European languages like En-
glish, many Asian languages like Chinese do not
use a whitespace character to separate meaningful
word units. The problems of word segmentation
are:

(1) ambiguity, e.g., for Chinese, a single charac-
ter can be a word component in one context,
but a word by itself in another context.

(2) unknown words, i.e., existing words can be
combined into new words such as proper
nouns, e.g. “White House”.

Purely dictionary-based approaches like (Cheng
et al., 1999) addressed these problems by max-
imum matching heuristics. Recent research on
unsupervised word segmentation focuses on ap-
proaches based on probabilistic methods. For ex-
ample, (Brent, 1999) proposes a probabilistic seg-
mentation model based on unigram word distri-
butions, whereas (Venkataraman, 2001) uses stan-
dard n-gram language models. An alternative non-
parametric Bayesian inference approach based on
the Dirichlet process incorporating unigram and
bigram word dependencies is introduced in (Gold-
water et al., 2006).

The focus of this paper, however, is to
learn word segmentations that are consistent with
phrasal segmentations of SMT translation mod-
els. In case of small translation units, e.g. sin-
gle Chinese or Japanese characters, it is likely
that such tokens have been seen in the training
corpus, thus these tokens can be translated by
an SMT engine. However, the contextual infor-
mation provided by these tokens might not be
enough to obtain a good translation. For exam-
ple, a Japanese-English SMT engine might trans-
late the two successive characters “

�
” (“white”)

and “ � ” (“bird”) as “white bird”, while a human
would translate “

� � ” as “swan”. Therefore, the
longer the translation unit, the more context can be
exploited to find a meaningful translation. On the
other hand, the longer the translation unit, the less
likely it is that such a token will occur in the train-
ing data due to data sparseness of the language
resources utilized to train the statistical translation
models. Therefore, a word segmentation that is

400



“consistent with SMT models” is one that identi-
fies translation units that are small enough to be
translatable, but large enough to be meaningful in
the context of the given input sentence, achieving a
trade-off between the coverage and the translation
task complexity of the statistical models in order to
improve translation quality.

The use of monolingual probabilistic models
does not necessarily yield a better MT perfor-
mance (Chang et al., 2008). However, improve-
ments have been reported for approaches taking
into account not only monolingual, but also bilin-
gual information, to derive a word segmentation
suitable for SMT. Due to the availability of lan-
guage resources, most recent research has focused
on optimizing Chinese word segmentation (CWS)
for Chinese-to-English SMT. For example, (Xu et
al., 2008) proposes a Bayesian Semi-Supervised
approach for CWS that builds on (Goldwater et al.,
2006). The generative model first segments Chi-
nese text using an off-the-shelf segmenter and then
learns new word types and word distributions suit-
able for SMT. Similarly, a dynamic programming-
based variational Bayes approach using bilingual
information to improve MT is proposed in (Chung
and Gildea, 2009). Concerning other languages,
for example, (Kikui and Yamamoto, 2002) ex-
tended Hidden-Markov-Models, where hidden n-
gram probabilities were affected by co-occurring
words in the target language part for Japanese
word segmentation.

Recent research on SMT is also focusing on the
usage of multiple word segmentation schemes for
the source language to improve translation qual-
ity. For example, (Zhang et al., 2008) combines
dictionary-based and CRF-based approaches for
Chinese word segmentation in order to avoid out-
of-vocabulary (OOV) words. Moreover, the com-
bination of different morphological decomposi-
tion of highly inflected languages like Arabic or
Finnish is proposed in (de Gispert et al., 2009) to
reduce the data sparseness problem of SMT ap-
proaches. Similarly, (Nakov et al., 2009) utilizes
SMT engines trained on different word segmenta-
tion schemes and combines the translation outputs
using system combination techniques as a post-
process to SMT decoding.

In order to integrate multiple word segmenta-
tion schemes into the SMT decoder, (Dyer et al.,
2008) proposed to generate word lattices covering
all possible segmentations of the input sentence

and to decode the lattice input. An extended ver-
sion of the lattice approach that does not require
the use (and existence) of monolingual segmenta-
tion tools was proposed in (Dyer, 2009) where a
maximum entropy model is used to assign prob-
abilities to the segmentations of an input word to
generate diverse segmentation lattices from a sin-
gle automatically learned model.

The method of (Ma and Way, 2009) also uses
a word lattice decoding approach, but they itera-
tively extract multiple word segmentation schemes
from the training bitext. This dictionary-based
approach uses heuristics based on the maximum
matching algorithm to obtain an agglomeration of
segments that are covered by the dictionary. It uses
all possible source segmentations that are consis-
tent with the extracted dictionary to create a word
lattice for decoding.

The method proposed in this papers differs from
previous approaches in the following points:

• it works for any language pair where the
source language is unsegmented and the tar-
get language segmentation is known.

• it can be applied for the translation of a
source language where no linguistically mo-
tivated word segmentation tools are available.

• it applies machine learning techniques to
identify segmentation schemes that improve
translation quality for a given language pair.

• it decodes directly from unsegmented text us-
ing segmentation information implicit in the
phrase-table to generate the target and thus
avoids issues of consistency between phrase-
table and input representation.

• it uses segmentations at all iterative levels of
the bootstrap process, rather than only those
from the final iteration allowing the consid-
eration of segmentations from many levels of
granularity.

Word segmentations are learned using a parallel
corpus by aligning character-wise source language
sentences to word units separated by a white-
space in the target language. Successive characters
aligned to the same target words are merged into a
larger source language unit. Therefore, the granu-
larity of the translation unit is defined in the given
bitext context. In order to minimize the side ef-
fects of alignment errors and to achieve segmenta-
tion consistency, a Maximum-Entropy (ME) algo-
rithm is applied to learn the source language word

401



segmentation that is consistent with the transla-
tion model of an SMT system trained on the re-
segmented bitext. The process is iterated until
no further improvement in translation quality is
achieved. In order to integrate multiple word seg-
mentation into a single SMT system, the statisti-
cal translation models trained on differently seg-
mented source language corpora are merged by
characterizing the source side of each translation
model, summing up the probabilities of identical
phrase translation pairs, and rescoring the merged
translation model (see Section 2).

The proposed segmentation method is applied
to the translation of five Asian languages, i.e.,
Japanese, Korean, Thai, and two Chinese dialects
(Standard Mandarin and Taiwanese Mandarin),
into English. The utilized language resources
and the outline of the experiments are summa-
rized in Section 3. The experimental results re-
vealed that the proposed method outperforms not
only a baseline system that translates character-
ized source language sentences, but also all SMT
models trained on any of the learned word seg-
mentations. In addition, the proposed method
achieves translation results comparable to SMT
models trained on linguistically segmented bitext.

2 Word Segmentation

The word segmentation method proposed in this
paper is an unsupervised, language-independent
approach that treats the task of word segmentation
as a phrase-boundary tagging task. This method
uses a parallel text corpus consisting of initially
unigram segmented source language character se-
quences and whitespace-separated target language
words. The initial bitext is used to train a stan-
dard phrase-based SMT system (SMTchr). The
character-to-word alignment results of the SMT
training procedure1 are exploited to identify suc-
cessive source language characters aligned to the
same target language word in the respective bitext
and to merge these characters into larger transla-
tion units, defining its granularity in the given bi-
text context.

The obtained translation units are then used to
learn the word segmentation that is most consis-
tent with the phrase alignments of the given SMT
system. First, each character of the source lan-
guage text is annotated with a word-boundary in-

1For the experiments presented in Section 3, the GIZA++
toolkit was used.

dicator where only two tags are used, i.e, “E”
(end-of-word character tag) and “I” (in-word
character tag). The annotations are derived from
the SMT training corpus as described in Figure 1.

(1) proc annotate-phrase-boundaries( Bitext ) ;
(2) begin
(3) for each (Src, Trg) in {Bitext} do
(4) A← align(Src, Trg) ;
(5) for each i in {1, . . . , len(Src)-1} do
(6) Trgi ← get-target(Src[i], A) ;
(7) Trgi+1 ← get-target(Src[i+1], A) ;
(8) if null(Trgi) or Trgi 6= Trgi+1 then
(9) (∗ aligned to none or different target ∗)
(10) SrcME ← assign-tag(Src[i],′ E′) ;
(11) else
(12) (∗ aligned to the same target ∗)
(13) SrcME ← assign-tag(Src[i],′ I ′) ;
(14) fi ;
(15) CorpusME ← add(SrcME) ;
(16) od ;
(17) (∗ last source token ∗)
(18) LastSrcME ← assign-tag(Src[len(Src)],′ E′) ;
(19) CorpusME ← add(LastSrcME) ;
(20) od ;
(21) return( CorpusME ) ;
(22) end ;

Figure 1: ME Training Data Annotation

Using these alignment-based word boundary
annotations, a Maximum-Entropy (ME) method is
applied to learn the word segmentation consistent
with the SMT translation model (see Section 2.1),
to resegment the original source language corpus,
and to retrain a phrase-based SMT engine that will
hopefully achieve a better translation performance
than the initial SMT engine. This process should
be repeated as long as an improvement in transla-
tion quality is achieved. Eventually, the concate-
nation of succeeding translation units will result in
overfitting, i.e., the newly created token can only
be translated in the context of rare training data ex-
amples. Therefore, a lower translation quality due
to an increase of untranslatable source language
phrases is to be expected (see Section 2.2).

However, in order to increase the coverage and
to reduce the translation task complexity of the
statistical models, the proposed method integrates
multiple segmentation schemes into the statistical
translation models of a single SMT engine so that
longer translation units are preferred for transla-
tion, if available, and smaller translation units can
be used otherwise (see Section 2.3).

2.1 Maximum-Entropy Tagging Model

ME models provide a general purpose machine
learning technique for classification and predic-

402



Lexical Context Features < t0, w−2 > < t0, w−1 >
< t0, w0 >

< t0, w+1 > < t0, w+2 >

Tag Context Features < t0, t−1 > < t0, t−1, t−2 >

Table 1: Feature Set of ME Tagging Model

tion. They are versatile tools that can handle
large numbers of features, and have shown them-
selves to be highly effective in a broad range of
NLP tasks including sentence boundary detection
or part-of-speech tagging (Berger et al., 1996).

A maximum entropy classifier is an exponential
model consisting of a number of binary feature
functions and their weights (Pietra et al., 1997).
The model is trained by adjusting the weights to
maximize the entropy of the probabilistic model
given constraints imposed by the training data. In
our experiments, we use a conditional maximum
entropy model, where the conditional probability
of the outcome given the set of features is modeled
(Ratnaparkhi, 1996). The model has the form:

p(t, c) = γ
K∏

k=0

α
fk(c,t)
k · p0

where:
t is the tag being predicted;
c is the context of t;
γ is a normalization coefficient;
K is the number of features in the model;
fk are binary feature functions;
ak is the weight of feature function fk;
p0 is the default model.

The feature set is given in Table 1. The lexical
context features consist of target words annotated
with a tag t. w0 denotes the word being tagged and
w
−2, . . . , w+2 the surrounding words. t0 denotes

the current tag, t
−1 the previous tag, etc. The tag

context features supply information about the con-
text of previous tag sequences. This conditional
model can be used as a classifier. The model is
trained iteratively, and we used the improved iter-
ative scaling algorithm (IIS) (Berger et al., 1996)
for the experiments presented in Section 3.

2.2 Iterative Bootstrap Method

The proposed iterative bootstrap method to learn
the word segmentation that is consistent with an
SMT engine is summarized in Figure 2. After
the ME tagging model is learned from the ini-
tial character-to-word alignments of the respec-
tive bitext ((1)-(4)), the obtained ME tagger is

SRC text

TRG text

unigram
segmented SRC

(1)   characterize

evalchrdecodeSMTchr

SRCtoken
TRGword

alignment

(3)   extract

ME1
classifier

Iteration 1
segmented SRC

(4)
annotate

(5)
resegment

SMT1  
eval1  decode

SMTJ-1

SRCtoken
TRGword

alignment

ME2
classifier

(7)   extract

(8)
annotate

Iteration 2
segmented SRC

(9)
resegment

…

evalJ-1decode

SRCtoken
TRGword

alignment

MEJ-1
classifier

extract

Iteration J-1
segmented SRC

SMTJ
evalJ

(6) train

(2) train

(J-1) train

(J) train
…

…

Selected Word
Segmenter

…

better

better

worse

Figure 2: Iterative Bootstrap Method

applied to resegment the source language side of
the unsegmented parallel text corpus ((5)). This
results in a resegmented bitext that can be used
to retrain and reevaluate another engine SMT1
((6)), achieving what is hoped to be a better trans-
lation performance than the initial SMT engine
(SMTchr).

The unsupervised ME tagging method can also
be applied to the token-to-word alignments ex-
tracted during the training of the SMT1 engine
to obtain an ME tagging model ME1 capable of
handling longer translation units ((7)-(8)). Such
a bootstrap method iteratively creates a sequence
of SMT engines SMTi ((9)-(J)), each of which
reduces the translation complexity, because larger
chunks can be translated in a single step leading
to fewer word order or word disambiguation er-
rors. However, at some point, the increased length
of translation units learned from the training cor-
pus will lead to overfitting, resulting in reduced
translation performance when translating unseen
sentences. Therefore, the bootstrap method stops
when the J th resegmentation of the training cor-
pus results in a lower automatic evaluation score
for the unseen sentences than the one for the previ-
ous iteration. The ME tagging model MEJ−1 that
achieved the highest automatic translation scores
is then selected as the best single-iteration word
segmenter.

2.3 Integration of Multiple Segmentations

The integration of multiple word segmentation
schemes is carried out by merging the transla-
tion models of the SMT engines trained on the
characterized and iteratively learned segmentation
schemes. This process is performed by linearly in-
terpolating the model probabilities of each of the

403



models. In our experiments, equal weights were
used; however, it might be interesting to inves-
tigate varying the weights according to iteration
number, as the latter iterations may contain more
useful segmentations.

In addition, we also remove the internal seg-
mentation of the source phrases. The advantages
are twofold. Primarily it allows decoding directly
from unsegmented text. Moreover, the segmenta-
tion of the source phrase can differ between mod-
els at differing iterations; removing the source seg-
mentation at this stage makes the phrase pairs in
the translations models at various stages in the it-
erative process consistent with one another. Con-
sequently, duplicate bilingual phrase pairs appear
in the phrase table. These duplicates are combined
by normalizing their model probabilities prior to
model interpolation.

The rescored translation model covers all trans-
lation pairs that were learned by any of the
iterative models. Therefore, the selection of
longer translation units during decoding can re-
duce the complexity of the translation task. On the
other hand, overfitting problems of single-iteration
models can be avoided because multiple smaller
source language translation units can be exploited
to cover the given input parts and to generate trans-
lation hypotheses based on the concatenation of
associated target phrase expressions. Moreover,
the merging process increases the translation prob-
abilities of the source/target translation parts that
cover the same surface string but differ only in
the segmentation of the source language phrase.
Therefore, the more often such a translation pair is
learned by different iterative models, the more of-
ten the respective target language expression will
be exploited by the SMT decoder.

The translation of unseen data using the merged
translation models is carried out by (1) character-
izing the input text and (2) applying the SMT de-
coding in a standard way.

3 Experiments
The effects of using different word segmentations
and integrating them into an SMT engine are in-
vestigated using the multilingual Basic Travel Ex-
pressions Corpus (BTEC), which is a collection
of sentences that bilingual travel experts consider
useful for people going to or coming from other
countries (Kikui et al., 2006). For the word seg-
mentation experiments, we selected five Asian
languages that do not naturally separate word

BTEC train set dev set test set
# of sen 160,000 1,000 1,000
en voc 15,390 1,262 1,292

len 7.5 7.1 7.2
ja voc 17,168 1,407 1,408

len 8.5 8.2 8.2
ko voc 17,246 1,366 1,365

len 8.0 7.7 7.8
th voc 7,354 1,081 1,053

len 7.8 7.3 7.4
zh voc 11,084 1,312 1,301

len 7.1 6.4 6.5

Table 2: Language Resources

units, i.e., Japanese (ja), Korean (ko), Thai (th),
and two dialects of Chinese (Standard Mandarin
(zh) and Taiwanese Mandarin (tw)).

Table 2 summarizes the characteristics of the
BTEC corpus used for the training (train) of the
SMT models, the tuning of model weights and
stop conditions of the iterative bootstrap method
(dev), and the evaluation of translation quality
(test). Besides the number of sentences (sen)
and the vocabulary (voc), the sentence length
(len) is also given as the average number of
words per sentence. The given statistics are ob-
tained using commonly-used linguistic segmenta-
tion tools available for the respective language,
i.e., CHASEN (ja), WORDCUT (th), ICTCLAS
(zh), HanTagger (ko). No segmentation was avail-
able for Taiwanese Mandarin and therefore no
meaningful statistics could be obtained.

For the training of the SMT models, standard
word alignment (Och and Ney, 2003) and lan-
guage modeling (Stolcke, 2002) tools were used.
Minimum error rate training (MERT) was used to
tune the decoder’s parameters and performed on
the dev set using the technique proposed in (Och
and Ney, 2003). For the translation, a multi-stack
phrase-based decoder was used.

For the evaluation of translation quality, we ap-
plied standard automatic metrics, i.e., BLEU (Pap-
ineni et al., 2002) and METEOR (Lavie and Agar-
wal, 2007). We have tested the statistical signif-
cance of our results2 using the bootstrap method
reported in (Zhang et al., 2004) that (1) performs a
random sampling with replacement from the eval-
uation data set, (2) calculates the evaluation metric
score of each engine for the sampled test sentences
and the difference between the two MT system
scores, (3) repeats the sampling/scoring step itera-

22000 iterations were used for the analysis of the auto-
matic evaluation results in this paper. All reported differences
in evaluation scores are statistically significant.

404



tively, and (4) applies the Student’s t-test at a sig-
nificance level of 95% confidence to test whether
the score differences are significant.

In addition, human assessment of translation
quality was carried out using the Ranking metrics.
For the Ranking evaluation, a human grader was
asked to “rank each whole sentence translation
from Best to Worst relative to the other choices
(ties are allowed)” (Callison-Burch et al., 2007).
The Ranking scores were obtained as the average
number of times that a system was judged better
than any other system and the normalized ranks
(NormRank) were calculated on a per-judge ba-
sis for each translation task using the method of
(Blatz et al., 2003).

Section 3.1 compares the proposed method to
the baseline system that translates characterized
source language sentences and to the SMT en-
gines that are trained on iteratively learned as well
as language-dependent linguistic word segmenta-
tions. The effects of the iterative learning method
are summarized in Section 3.2.

3.1 Effects of Word Segmentation

The automatic evaluation scores of the SMT en-
gines trained on the differently segmented source
language resources are given in Table 3, where
“character” refers to the baseline system of using
character-segmented source text; “single-best”3 is
the SMT engine that is trained on the corpus seg-
mented by the best-performing iteration of the
bootstrap approach; “proposed” is the SMT engine
whose models integrate multiple word segmen-
tation schemes; and “linguistic” uses language-
dependent linguistically motivated word segmen-
tation tools. The reported scores are calculated as
the mean score of all metric scores obtained for the
iterative sampling method used for statistical sig-
nificance testing and listed as percentage figures.

The results show that the proposed method out-
performs the character (single-best) system for
each of the involved languages achieving gains
of 2.0 to 9.1 (0.4 to 1.6) BLEU points and 2.0
to 5.9 (0.7 to 4.6) METEOR points, respectively.
However, the improvements depend on the source
language. For example, the smallest gains were
obtained for Standard Mandarin, because single
characters frequently form words of their own,
thus resulting in more ambiguity than Japanese,

3This approximates the approach of (Ma and Way, 2009)
and is given as a way of showing the effect of segmentation
at multiple levels of granularity.

where consecutive hiragana or katakana charac-
ters can form larger meaningful units.

Comparing the proposed method towards lin-
guistically motivated segmenters, the results show
that the proposed method outperforms the SMT
engines using linguistic segmentation tools for
tasks such as translating Korean and Standard
Mandarin into English. Slightly lower evaluation
scores were achieved for the automatically learned
word segmentation for Japanese, although the re-
sults of the proposed method are quite similar.
This is a suprisingly strong result, given the ma-
turity of the linguistically motivated segmenters,
and given that our segmenters use only the bilin-
gual corpus used to train the SMT systems.

The Thai-English experiments expose some is-
sues that are related to the definition of what
a “character” is. Our segmentation schemes
are learned directly from the bitext without any
language-specific information, and can cope well
with most languages. However, Thai seems to be
an exceptional case in our experiments, because
(1) the Thai script is a segmental writing system
which is based on consonants but in which vowel
notation is obligatory, so that the characterization
of the baseline system affects vowel dependen-
cies, (2) it uses tone markers that are placed above
the consonant, but are treated as a single charac-
ter in our approach, and (3) vowels sounding after
a consonant are non-sequential and can occur be-
fore, after, above, or below a consonant increasing
the number of word form variations in the training
corpus and reducing the accuracy of the learned
ME tagging models. This is an interesting result
that motivates further study on how to incorpo-
rate features on language scripts into our machine
learning framework. For example, Japanese is
written in three different scripts (kanji, hiragana,
katakana). Therefore, the script class of each char-
acter could be used as an additional feature to ob-
tain the initial segmentation of the training corpus.

Finally, the results for Taiwanese Mandarin,
where no linguistic tool was available to segment
the source language text, shows that the proposed
method can be applied successfully for the trans-
lation of any language where no linguistically-
motivated segmentation tools are available.

Table 4 summarizes the subjective evaluation
results which were carried out by a paid evalua-
tion expert who is a native speaker of English. The
NormRank results confirm the findings of the au-

405



BLEU
source word segmentation

language character single-best proposed linguistic
ja 36.93 39.65 41.25 41.46
ko 34.72 37.32 38.51 37.19
th 41.42 50.16 50.53 56.68
zh 36.59 37.02 38.61 38.13
tw 45.71 50.95 52.21 –

METEOR
source word segmentation

language character single-best proposed linguistic
ja 59.78 60.95 65.45 66.03
ko 58.45 60.06 64.31 63.04
th 67.22 71.22 72.58 79.02
zh 61.77 62.38 63.80 62.72
tw 70.14 73.64 74.38 –

Table 3: Automatic Evaluation

NormRank
source word segmentation

language character single-best proposed linguistic
ja 2.76 2.85 3.18 3.12
ko 2.68 2.90 3.17 3.09
th 2.65 2.95 3.05 3.43
zh 2.87 3.01 3.07 3.04
tw 2.83 2.86 3.24 –

Table 4: Subjective Evaluation

tomatic evaluation. In addition, for Japanese, the
translation outputs of the proposed method were
judged better than those of the linguistically seg-
mented SMT model.

3.2 Effects of Bootstrap Iteration

In order to get an idea of the robustness of the pro-
posed method, the changes in system performance
for each source language during the iterative boot-
strap method is given in Figure 3. The results for
BLEU and METEOR show that all languages reach
their best performance after the first or second it-
eration and then slightly, but consistently decrease
with the increased number of iterations. The rea-
son for this is the effect of overfitting caused by
the concatenation of source tokens that are aligned
to longer target phrases, resulting in the segmenta-
tion of longer translation units.

The changes in the vocabulary size and the word
length are summarized in Figure 4. The amount of
words extracted by the proposed method is much
larger than the one of the baseline system, increas-
ing the vocabulary size by a factor of 10 for Stan-
dard Mandarin and Taiwanese Mandarin, 30 for
Japanese and Korean, and 100 for Thai. It is also
larger than the vocabulary obtained for the linguis-
tic tools by a factor of 1.5 to 2.5 for all investigated

Change in BLEU

20.00

30.00

40.00

50.00

60.00

iteration

BL
EU

 (%
) � �� �� �

� �� �

Change in METEOR

40.00

50.00

60.00

70.00

80.00

iteration

M
ET

EO
R 

(%
) � 	
 �� 

� � �

chr 1        2       3      4         5      6      7        8 9       10

chr 1       2       3       4        5      6      7        8  9       10

Figure 3: Change in System Performance

Change in Vocabulary Size

0
20000

40000
60000

80000
100000

120000

iteration

V
oc

ab
ul

ar
y 

Si
ze � �� �� �� �� �

chr 1        2       3      4         5      6      7        8 9       10

Change in Average Vocabulary Length

0.0

5.0

10.0

15.0

20.0

25.0

30.0

iteration

V
oc

ab
ul

ar
y 

Le
ng

th � ����� �� ��  
chr 1        2       3      4         5      6      7        8 9       10

Figure 4: Change in Vocabulary Size and Length

languages. The average vocabulary length also in-
creased for each iteration whereby the length of
the translation units learned after 10 iterations al-
most doubles the word size of the initial iteration.

The overfitting problem of the iterative boot-
strap method is illustrated in the increase of out-
of-vocabulary words, i.e. source language words
contained in the unseen evaluation data set that
cannot be translated by the respective SMT. The
results given in Figure 5 show a large increase
in OOV for the first three iterations, resulting in
lower translation qualities as listed in Figure 3.

Table 5 illustrates translation examples using
different segmentation schemes for the Japanese-
English translation task. The SMT engines that
output the best translations are marked with an as-
terisk. In the first example, the concatenation of “!#"%$'&)(

” (already midnight) by the single-best
segmentation scheme leads to an OOV word, thus
only a partial translation can be achieved. How-
ever, the problem can be resolved using the pro-
posed method. The second example is best trans-
lated using the single-best word segmentation that
correctly handles the sentence coordination. The

406



Change in OOV Words

0
100
200
300
400
500
600

iteration

O
O

V
 W

or
ds � �� �� �� �� �

chr 1        2       3      4         5      6      7        8 9       10

Figure 5: Change in Out-of-Vocabulary Size

baseline system omits the sentence coordination
information, resulting in an unacceptable transla-
tion. The third examples illustrates that longer to-
kens reduce the translation complexity and thus
can be translated better than the other segmenta-
tion that cause more ambiguities.

4 Conclusions

This paper proposes a new language-independent
method to segment languages that do not use
whitespace characters to separate meaningful
word units in an unsupervised manner in order to
improve the performance of a state-of-the-art SMT
system. The proposed method does not need any
linguistic information about the source language
which is important when building SMT systems
for the translation of relatively resource-poor lan-
guages which frequently lack morphological anal-
ysis tools. In addition, the development costs
are far less than those for developing linguistic
word segmentation tools or even paying humans
to segment the data sets manually, since only the
bilingual corpus used to train the SMT system is
needed to train the segmenter.

The effectiveness of the proposed method was
investigated for the translation of Japanese, Ko-
rean, Thai, and two Chinese dialects (Standard
Mandarin and Taiwanese Mandarin) into English
for the domain of travel conversations. The auto-
matic evaluation of the translation results showed
consistent improvements of 2.0 to 9.1 BLEU points
and 2.0 to 5.9 METEOR points compared to a
baseline system that translates characterized input.
Moreover, it improves the best performing SMT
engine of the iterative learning procedure by 0.4
to 1.6 BLEU points and 0.7 to 4.6 METEOR points.

In addition, the proposed method achieved
translation results similar to SMT models trained
on bitext segmented with linguistically motivated
tools, even outperforming these for Korean, Chi-
nese, and Japanese in the human evaluation, al-
though no external information and only the given
bitext was used to train the segmentation models.

linguistic seg:
�	�

/ 
 / ���� / � / ��� / ����� / ��� / ��

trans: Yes. Let’s see. It’s midnight.

character∗ seg:
�

/
�

/ 
 / � / � /  / � / � / � / � / � / � / � /
� / � / 


trans: Yes. Well, it’s already midnight.
single-best seg:

�	� 
 / ���� / � / ��������� / � / ����

trans: Yes. Let ’s see.

proposed∗ seg:
�

/
�

/ 
 / � / � /  / � / � / � / � / � / � / � /
� / � / 


trans: Yes. Well, it’s already midnight.

linguistic seg: � � �"! / # / $&%(' / ) / ��� / # / � /
'	' / * / + / , � / - / .0/ 12' / 


trans: I’d like a pair of jeans.
Could you recommend a good shop?

character seg: � / � / � / ! / # / $ / % / ' / ) / � / � / # / � /
' / ' / * / + / , / � / - / . / / / 1 / ' / 


trans: Could you recommend a good ’d like
a pair of jeans.

single-best∗ seg: � � �"! / #3$&% / ' / )3�	�	#4� /
'	' / * / +5, � / -6.0/ 12' / 


trans: I’d like some jeans.
Could you recommend a good shop?

proposed seg: � / � / � / ! / # / $ / % / ' / ) / � / � / # / � /
' / ' / * / + / , / � / - / . / / / 1 / ' / 


trans: I ’d like a pair of jeans and
could you recommend a good shop?

linguistic seg: 7&8 / ) / 9�: / ;<� / = / ��> / ;<� / ? / 

trans: Will it be ready by this afternoon?

character seg: 7 / 8 / ) / 9 / : / ; / � / = / � / > / ; / � /
? / 


trans: It’ll be ready by this afternoon?
single-best seg: 7&8@) / 9�:A;<� / = / ��>4; / �	?4


trans: Will it be ready by this afternoon?
proposed∗ seg: 7 / 8 / ) / 9 / : / ; / � / = / � / > / ; / � /

? / 

trans: Can you have these ready by this

afternoon?

Table 5: Sample Translations

The experiments using Thai are interesting be-
cause the script is a segmental writing system us-
ing tone markers and vowel dependencies. This
exposed some issues that are related to the defini-
tion of what a “character” is and motivates further
study on how to incorporate features on language
scripts into our machine learning framework.

References

Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to
NLP. Computational Linguistics, 22(1):39–71.

John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for statistical machine translation. In Final
Report of the JHU Summer Workshop.

Michael Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34:71–105.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Jan Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of the 2nd Workshop on SMT, pages 136–
158, Prague, Czech Republic.

407



Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese Word Segmen-
tation for Machine Translation Performance. In
Proc. of the 3rd Workshop on SMT, pages 224–232,
Columbus, USA.

Kwok-Shing Cheng, Gilbert Young, and Kam-Fai
Wong. 1999. A study on word-based and integrat-
bit Chinese text compression algorithms. American
Society of Information Science, 50(3):218–228.

Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised Tokenization for Machine Translation. In
Proc. of the EMNLP, pages 718–726, Singapore.

Adrian de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alternative
Morphological Decompositions. In Proc. of HLT,
Companion Volume, pages 73–76, Boulder, USA.

Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Transla-
tion. In Proc. of ACL, pages 1012–1020, Columbus,
USA.

Christopher Dyer. 2009. Using a maximum entropy
model to build segmentation lattices for MT. In
Proc. of HLT, pages 406–414, Boulder, USA.

Sharon Goldwater, Thomas Griffith, and Mark John-
son. 2006. Contextual Dependencies in Unsuper-
vised Word Segmentation. In Proc. of the ACL,
pages 673–680, Sydney, Australia.

Geninchiro Kikui and Hirofumi Yamamoto. 2002.
Finding Translation Pairs from English-Japanese
Untokenized Aligned Corpora. In Proc. of the Work-
shop on Speech-to-Speech Translation, pages 23–30,
Philadephia, USA.

Geninchiro Kikui, Seiichi Yamamoto, Toshiyuki
Takezawa, and Eiichiro Sumita. 2006. Com-
parative study on corpora for speech translation.
IEEE Transactions on Audio, Speech and Language,
14(5):1674–1682.

Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In
Proc. of the 2nd Workshop on SMT, pages 228–231,
Prague, Czech Republic.

Yanjun Ma and Andy Way. 2009. Bilingually Moti-
vated Domain-Adapted Word Segmentation for Sta-
tistical Machine Translation. In Proc. of the 12th
EACL, pages 549–557, Athens, Greece.

Preslav Nakov, Chang Liu, Wei Lu, and Hwee Tou Ng.
2009. The NUS SMT System for IWSLT 2009. In
Proc. of IWSLT, pages 91–98, Tokyo, Japan.

Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Statistical Alignment Models. Com-
putational Linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
40th ACL, pages 311–318, Philadelphia, USA.

Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing Features of Random
Fields. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(4):380–393.

Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proc. of the
EMNLP, pages 133–142, Pennsylvania, USA.

Andreas Stolcke. 2002. SRILM an extensible lan-
guage modeling toolkit. In Proc. of ICSLP, pages
901–904, Denver, USA.

Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3):351–372.

Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian Semi-Supervised Chi-
nese Word Segmentation for SMT. In Proc. of the
COLING, pages 1017–1024, Manchester, UK.

Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting Bleu/NIST Scores: How Much Im-
provement do We Need to Have a Better System? In
Proc. of the LREC, pages 2051–2054, Lisbon, Por-
tugal.

Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved Statistical Machine Translation by
Multiple Chinese Word Segmentation. In Proc. of
the 3rd Workshop on SMT, pages 216–223, Colum-
bus, USA.

408


