










































Cluster-based Web Summarization


International Joint Conference on Natural Language Processing, pages 1124–1128,
Nagoya, Japan, 14-18 October 2013.

Cluster-based Web Summarization

Yves Petinot and Kathleen McKeown and Kapil Thadani
Department of Computer Science

Columbia University
503 Computer Science Building

1214 Amsterdam Avenue
New York, New York, 10027

{ypetinot|kathy|kapil}@cs.columbia.edu

Abstract

We propose a novel approach to abstrac-
tive Web summarization based on the ob-
servation that summaries for similar URLs
tend to be similar in both content and
structure. We leverage existing URL clus-
ters and construct per-cluster word graphs
that combine known summaries while ab-
stracting out URL-specific attributes. The
resulting topology, conditioned on URL
features, allows us to cast the summariza-
tion problem as a structured learning task
using a lowest cost path search as the de-
coding step. Early experimental results on
a large number of URL clusters show that
this approach is able to outperform previ-
ously proposed Web summarizers.

1 Introduction

Abstract Web summaries, which describe the top-
ics and functionalities of Web pages at an abstract
level, play an essential part in the discovery of new
sites and services on the Web. Such summaries are
intrinsically difficult to obtain using the content of
Web pages. As such, the most successful meth-
ods for generating them are effectively extractive
and based on the identification of likely abstractive
content from linking pages. These URL-centric
techniques however require a significant amount
of redundancy in linking content (Delort et al.,
2003).

In this paper, we propose a summary-centric ap-
proach to Web summarization based on the ob-
servation that summaries for similar URLs exhibit
both similar content and structure. This simi-
larity is apparent when analyzing summaries from
a single ODP category, examples of which are

shown in Table 1. We can see there that sum-
maries of semantically related URLs tend to share
common concepts and differ mostly at the level of
target-specific attributes. Given a previously un-
seen URL U , such a cluster could thus be used as a
source of potentially relevant terms for that URL’s
summary. In particular, these relevant terms in-
clude abstract terms, which may not otherwise be
observed in the input data. We propose a graph-
based summarization framework that can leverage
this phenomenon.

2 Proposed Framework

Given a reference cluster C, we represent the
space for summary generation as a graph GC =
(VC , EC). This graph is obtained by fusing train-
ing summaries in C into a word graph. Each sum-
mary gi is mapped to a path between the shared
source and sink nodes. Each word gji is thus
mapped to a node Nk and each pair of neighboring
words (gji , g

j+1
i ) to a directed edge (Nk, Nl). No-

tably, we add nodes as needed to guarantee that in-
dividual summaries are cycle-free. Figure 1 shows
a simple summary graph.

2.1 Node Alignment

During the graph construction, nodes from distinct
paths are iteratively combined to elicit the struc-
tural and content commonalities between sum-
maries in the reference cluster. Following (Fil-
ippova, 2010) unambiguous nodes — i.e. nodes
whose surface form match exactly and for which
only one candidate exists — are always aligned,
while ambiguous nodes are aligned to the candi-
date node with maximum context overlap. When
there is no context overlap, a new node is added to
the graph.

1124



URL Abstract Summary
http://www.qgazette.com/ Published weekly for the Queens, New York community. Includes information on politics, religion,

dining, seniors, events, archives, classifieds and subscription details.
http://www.queenschronicle.com/ Local Queens NY news classifieds.
http://www.rockawave.com/ Published weekly in Rockaway, featuring local news, sports, community calendar, classified ad section,

archives and subscription and advertising details.
http://www.observer.com/ Online version of the newspaper, providing coverage of local politics and media, Real Estate, fashion,

and the Arts.
http://www.nytimes.com/ Online edition of the newspaper’s news and commentary. [Registration required]

Table 1: Sample entries form the ODP category /News/Newspapers/Regional/United States/New York.
All entries in this category describe sites of news organization located in the New-York area.

2.2 Template Slots

The content of reference summaries is likely to
be only partially relevant to a previously unseen
URL. In particular, certain paths in the summary
graph may contain nodes whose surface form is
target-specific. We use the following heuristic to
decide on the presence of template slots in a sum-
mary:

• Adverbs, adjectives and named entities are
treated as slots.
• Any term occurring in at least 25% of paths

will not be treated as a slot;

Similarly to (Barzilay and Lee, 2003), slot identi-
fication is performed prior to alignment.

3 Features

In this section we present the feature sets used to
condition the summary graph topology on a tar-
get URL U . Two aspects of the graph need to be
trained, namely edge costs and slot locations. We
introduce features for both.

3.1 Edge Feature Templates

We use the following feature templates to repre-
sent the compatibility between U — represented
by its text modalities, as listed in Table 2 — and a
specific edge in the summary graph.

Edge prior Probability of appearance of edge
eij in reference paths (summaries).

Edge appearance + Modality Frequency of
edge eij in each modality Mk. We consider that
eij appears in Mk if its source and sink co-occur
in Mk.

Source/Sink prior Probability of Source/Sink
node Ni in reference paths (summaries).

Source/Sink appearance + Modality Indicates
whether Source/Sink node Ni occurs in Mk.

Modality + n-gram Compatibility between the
edge eij , the modality Mk and the n-gram nl,
where n is in the range [1, 3].

3.2 Slot Features
To allow for the use of supervised methods in
learning optimal edge costs we need a graph
whose structure remains unchanged from one
training instance to the next. The fillers of slot
locations are thus described using features that are
not surface-based:

Semantic Type We only consider fillers com-
patible with the host slot.

Modality appearance Frequency of filler can-
didate in modality Mk.

Content HTML Context HTML (Style +
Structural) context of filler candidate in the con-
tent of U .

4 Learning Model

Using the summary graph topology and the fea-
tures defined above we express the abstract Web
summarization task as a structured learning prob-
lem. Specifically, we seek to obtain edge weights
such that the cost of individual reference sum-
maries — which, as we saw earlier, are mapped
to paths — is minimized. Given a set of features
F , the optimal set of feature weights w∗ is such
that:

w∗ = arg min
w

∑
g∈R(U)

Costw(g) (1)

Since our core constraint in building the summary
graph is that each summary should map to a cycle-
free path, identifying the optimal summary given
a set of edge weights reduces to solving a lowest
cost path problem:

w∗ = arg min
w

∑
g∈R(U)

|g|∑
i=1

∑
fk∈F

Costfk(egi−1gi)

(2)

1125



Modality Feature Types Description

URL content n-gram (n ∈ [1, 3]) n-grams generated from the target page content
n-gram + context n-gram with HTML context (immediately surrounding HTML tag)

URL title 1-gram 1-grams generated from the target URL title
URL words 1-gram 1-grams generated from the target URL string (e.g. “nytimes”, “com”)

URL anchortext n-gram (n ∈ [1, 3]) n-grams generated from the anchortext for the target URL

Table 2: Modality and features used to represent a target URL U .

In this setting, generation is achieved by running
the decoder using the optimal set of edge weights.

4.1 Structured Perceptron

One way to solve this learning problem is to use
a structured perceptron algorithm (Collins, 2002)
where weights w control the linear contribution of
individual features to the aggregate cost of edges.

Costfk(eij) = w
eij
k · fk (3)

The structured perceptron algorithm is shown in
Algorithm 1. At every iteration, decoding is
achieved via a search for the shortest path based
on the edge costs induced by the current weights
w∗. Following recent work on structured learning
(Huang et al., 2012), we do not require this search
to be exact, but to guarantee that each iteration
results in a valid update. Our decoder thus uses
beam search (beam size b = 5) combined with an
early update procedure, the latter helping in sig-
nificantly speeding up model training.

Algorithm 1 Structured Perceptron
Require: {ui, gi}ni=1
1: w∗ ← {0}
2: for i = 1→ T do
3: for j = 1→ n do
4: g∗ ← ShortestPathGw∗ (u

i)
5: w∗ ← w∗ + φ(ui, gi)− φ(ui, g∗)
6: end for
7: end for

4.2 Slot Filling

During the inference phase, we substitute slot lo-
cations with alternate paths, each containing a can-
didate filler for the slot. Each of these paths is
associated with the slot features discussed ear-
lier, however the feature weights of each slot are
shared, thus allowing the learning algorithm to
converge towards appropriate weights for filler se-
lection.

5 Evaluation

We compare the performance of our model against
a reimplementation of the two summarization al-

gorithms - content-based and context-based - pro-
posed in Delort et al. (2003). We apply our sum-
marization algorithms and the baselines to a ran-
dom sample of 56 ODP categories comprising at
least 50 entries. For each category, we split the
set of available summaries into training (90%) and
testing (10%), train the summarization algorithms
on the training set, and report (macro) average per-
formance on the testing set.

ROUGE (Lin, 2004) results comparing both
our proposed summarization model and the base-
line systems to the ODP ground truth are pro-
vided in Table 3. The summary-graph model,
both with and without slot-filling, shows signif-
icant improvements compared to the baselines
in terms of ROUGE-1 and ROUGE-L scores.
ROUGE-2 performance, however, is not on par
with the baselines. Long-distance sequence sim-
ilarity (ROUGE-L) being higher, we believe this
could indicate the inability of our model to cap-
ture target-specific bi-grams that have little or no
support in the training summaries. Allowing the
topology of the summary-graph to adapt to its tar-
get, for instance by introducing missing edges sup-
ported by the input data, should help alleviate this
issue. Finally, the performance of the model with
slot filling shows slight improvement over the ba-
sic model, however we observed that the system
frequently fails in extracting slot fillers. Future
work will focus on acquiring more filler candi-
dates and better features to model them.

6 Previous and Related Work

The work presented in this paper is linked to previ-
ous research in Web summarization and T2T lan-
guage generation. Most works on the former have
been on extractive methods, owing to the complex-
ity of Web content but also to the need for com-
pressed versions of Web pages. Other works have
in fact eluded the question of generation to instead
focus on the extraction of salient keywords from
Web sites (Glover et al., 2002; Zhang et al., 2004).
In the context of Web search engines, the com-
pression task is constrained further by the amount
of SERP real estate available for any single snip-

1126



and

fashion

art

and/2

and/1

spring

community

edition

in

coverage

politics

calendar

classifieds

weekly

rundown

[eog]

section

sports

version

[adverb]

details

provides

online the/1

commentaryfeaturing

local

events

columns

archives

[NAMED ENTITY SLOT/1]

obituaries

[adjective]

[bog]

news

subscription

school

providing

column

of

recipes

required

[NAMED ENTITY SLOT/2]

s

wine

advertisingpublished

newspaper

the

of/1

[NAMED ENTITY SLOT]

Figure 1: Summary graph associated with a subset of /News/Newspapers/Regional/United States/New York.
The path for the URL http://www.nytimes.com is shown in red

Summarization System ROUGE-1 ROUGE-2 ROUGE-L
Delort - Content 0.07163 0.04492 0.06574
Delort - Context 0.06783 0.03979 0.06197
Summary-graph 0.16222† 0.02775 0.14370†

Summary-graph + slot filling 0.16832† 0.02702 0.14729†

Table 3: Performance of summarization algorithms. † indicates statistically significant improvements
(according to a paired t-test with p < 0.05) compared to the provided baselines.

pet and how content may be truncated (Clarke et
al., 2007). Sun et al. (2005), in particular, lever-
aged the ODP hierarchy to mitigate data scarcity
for certain URLs, however they did not exploit
ODP summaries themselves. Several efforts have
focused on producing Web summaries using the
content of linking pages as a source of descrip-
tive content. Amitay and Paris (2000) assumes full
summaries can be readily found on a single page
linking to the target site. Delort et al. (2003) makes
less stringent assumptions and seeks to combine
descriptive content from multiple linking pages.
Closer to our work, Berger and Mittal (2000) pro-
posed a generative solution embracing the noisi-
ness of Web data and trained directly over ODP
(URL,summary) pairs. Finally our work relates
to T2T generation as we seek to generate well-

formed sentences without resorting to semantic
representations of either the input or output con-
tents. Graph-based models similar to ours have
been used for tasks ranging from string reconstruc-
tion (Wan et al., 2009) to sentence fusion and com-
pression (Filippova and Strube, 2008; Filippova,
2010).

7 Conclusion and Future Work

We have introduced a word graph model for the
task of Web summarization. and showed that per-
cluster word graphs make it possible to combine
abstractive and extractive behaviors. A limitation
of our model is the need for existing reference
clusters from which we build our summary graphs.
Future work will investigate the dynamic produc-
tion of such clusters.

1127



References
Einat Amitay and Cecile Paris. 2000. Automatically

summarising web sites - is there a way around it? In
CIKM 2000, pages 173–179.

Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
NAACL-HLT, 2003, pages 16–23.

A. Berger and V. Mittal. 2000. Ocelot: a system
for summarizing web pages. In Proceedings of the
23rd Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval (SIGIR’00), pages 144–151.

Charles L.A. Clarke, Eugene Agichtein, Susan Dumais,
and Ryen W. White. 2007. The influence of caption
features on clickthrough patterns in web search. In
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieva, pages 135–142.

Michael Collins. 2002. Discrimative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002, page ...

Jean-Yves Delort, Bernadette Bouchon-Meunier, and
Maria Rifqi. 2003. Enhanced web document sum-
marization using hyperlinks. In Hypertext 2003,
pages 208–215.

K. Filippova and M. Strube. 2008. Sentence fusion
via dependency graph compression. In Proceed-
ings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing. Honolulu,
Hawaii, Association for Computational Linguistics,
page 177–185.

Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of COLING 2010, pages 322–330.

Eric J. Glover, Kostas Tsioutsiouliklis, Steve
Lawrence, David M. Pennock, and Gary W.
Flake. 2002. Using web structure for classifying
and describing web pages. In Proceedings of WWW
2002, pages 562–569.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter for the Association for Computational
Linguistics: Human Language Technologies, pages
142–151.

Chin-Yew Lin. 2004. Rouge: a package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), Barcelona, Spain, July 25 - 26.

Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page
summarization using clickthrough data. In SIGIR
2005, pages 194–201.

Stephen Wan, Mark Dras, Robert Dale, and Cécile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of the 12th Conference
of the European Chapter of the ACL, pages 852–860.

Y. Zhang, N. Zincir-Heywood, and E. Milios. 2004.
World wide web site summarization. In Web Intelli-
gence and Agent Systems, 2(1), pages 39–53.

1128


