



















































Proceedings of the...


D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 101–106,
Goa, India. December 2014. c©2014 NLP Association of India (NLPAI)

LMSim : Computing Domain-specific Semantic Word
Similarities Using a Language Modeling Approach

Sachin Pawar1,2 Swapnil Hingmire1,3 Girish K. Palshikar1

{sachin7.p, swapnil.hingmire, gk.palshikar}@tcs.com
1Systems Research Lab, Tata Consultancy Services Ltd., Pune, India

2Department of CSE, IIT Bombay, Mumbai, India
3Department of CSE, IIT Madras, Chennai, India

Abstract

We propose a method to compute
domain-specific semantic similarity be-
tween words. Prior approaches for
finding word similarity that use lin-
guistic resources (like WordNet) are
not suitable because words may have
very specific and rare sense in some
particular domain. For example, in
customer support domain, the word
escalation is used in the sense of
“problem raised by a customer” and
therefore in this domain, the words
escalation and complaint are se-
mantically related. In our approach,
domain-specific word similarity is cap-
tured through language modeling. We
represent context of a word in the form
of a set of word sequences containing
the word in the domain corpus. We
define a similarity function which com-
putes weighted Jaccard similarity be-
tween the set representations of two
words and propose a dynamic program-
ming based approach to compute it ef-
ficiently. We demonstrate effectiveness
of our approach on domain-specific cor-
pora of Software Engineering and Agri-
culture domains.

1 Introduction

In text clustering, we expect to cluster these
two sentences together - Ricestar HT is a
good product for sprangletop control.

and Barnyardgrass can be controlled
by usage of Clincher. We observe that
though these two sentences do not share any
content word, still they are semantically simi-
lar to each other. The similarity between these
two sentences will not be high unless the clus-
tering algorithm takes into account that the

words within the pairs Ricestar-Clincher1

and sprangletop-Barnyardgrass2 are
semantically similar. In addition to text
clustering, discovering semantically similar
words has rich applications in the fields of
Information Retrieval, Question Answering,
Machine Translation, Spelling correction, etc.

We propose an algorithm which assigns high
similarity for words, which are highly replace-
able by each other in their context with-
out affecting its syntax and “meaning”. In
above example, the words sprangletop and
Barnyardgrass can be safely replaced by each
other while preserving the syntax and “mean-
ing” of the original sentences.

Approaches for computing similarity be-
tween words can be broadly classified into two
types - i) Approaches using linguistic resources
like WordNet (Miller, 1995) or thesaurus (Bu-
danitsky and Hirst, 2006) and ii) Approaches
using statistical properties of words in a cor-
pus (Blei et al., 2003; Halawi et al., 2012;
Brown et al., 1992).

In this paper, our focus is to develop a
word similarity algorithm which discovers sim-
ilar words for a specific domain. We mainly
focus on two domains - Software Engineering
(SE) and Agriculture. Our motivation behind
choosing these domains is that these are rel-
atively unexplored and as per our knowledge
they lack domain-specific lexical resources.

WordNet-based approaches are not suitable
for some domains because words may have
very specific and rare sense in those domains.
Moreover, creating lexical resources like Word-
Net for a certain domain is a quite challenging
task as it requires extensive human efforts, ex-
pertise, time and cost. In SE domain, we can
say that the words Oracle and DB2 are simi-

1both are names of herbicides
2both are names of weeds101



lar because they both are Database Manage-
ment Softwares. However, the word DB2 is not
present in WordNet and the sense of the word
Oracle as a “database management software”
is not covered in WordNet.

Corpus-based approaches for finding word
similarity using topic models (Blei et al., 2003)
or Distributional Similarity (Lee, 1999) make
use of word context to capture its meaning.
Though these approaches use higher order
word co-occurrence, they do not consider order
of other words within the context of a word.
Our hypothesis is that considering word order
results in better word similarities and this is
evident from our experimental results. Brown
Word clustering algorithm (Brown et al., 1992)
considers order of words and learns a language
model based on word clusters. However, this
algorithm is sensitive to number of clusters.
In this paper, we use word-based Language
Model to capture the context of words in the
form of word sequences (preserving word or-
der) and to assign weights for each of the
word sequences by computing their probabil-
ities. The major contributions of this paper
are - i) a new algorithm (LMSim) to compute
domain-specific word similarity and ii) an ef-
ficient dynamic programming based algorithm
for fast computation of LMSim.

2 Related Work

Various WordNet-based approaches are pro-
posed for computing similarity between two
words which are surveyed by Budanitsky and
Hirst (2006). Pedersen et al. (2007) adapted
WordNet-based word similarity measures to
the biomedical domain using SNOMED-CT,
which is an ontology of medical concepts.

In order to overcome several limitations of
WordNet, Gabrilovich and Markovitch (2007)
proposed Explicit Semantic Analysis (ESA),
that represents and compares the meaning of
texts in a high-dimensional space of concepts
derived from Wikipedia.

Halawi et al. (2012) proposed an approach
for learning word-word relatedness, where
known pairs of related words can be provided
as an input to impose constraints on the learn-
ing process. For both the domains that we are
focusing on - SE and Agriculture domain, we
do not have any such prior knowledge about

Notation Details
D Domain-specific text corpus
K Length of context window

C(wi,K) Set of context words for the word wi
wsji j

th sequence of context words for wi
λ(wsji ) Weight of the word sequence ws

j
i

n(wi, wj) Frequency of bigram 〈wi, wj〉 in D
n(wi) Frequency of word wi in D

Pf (wi|wj) Prob. of observing wi after wj
Pb(wi|wj) Prob. of observing wi before wj
Cseq(wi,K) Set of all possible K length word

sequences (formed using words in
C(wi,K)) that start or end in wi

Table 1: Details of notations used in this paper

related word pairs.

3 Computing Word Similarity

We take help of a statistical language model to
represent the context of each of the words.

3.1 Language Model

A statistical language model (Jurafsky and
Martin, 2000) assigns a probability to a se-
quence of words using n-gram statistics es-
timated from a text corpus. Using bigram
statistics, the probability of the word sequence
w1, w2, w3, w4 is computed as follows,

P (w1, w2, w3, w4) = P (w1)P (w2|w1)P (w3|w2)P (w4|w3)

The main motivation behind using a Lan-
guage Model is that it not only provides a way
to capture the context information for words
but also retains the word order information of
context words. Moreover, using a Language
Model, we can weigh each word sequence in
the context of a particular word by computing
its probability as explained above.

In our work, we have used a bigram (L0 = 2)
Language Model along with Laplace smooth-
ing. Our algorithm is currently designed for
bigram model and in future we plan to extend
it to higher order (L0 > 2) models.

3.2 LMSim : Algorithm to Compute
Language Model based Similarity

For finding semantic similarity between two
any words w1 and w2 (refer Table 1 for no-
tations), sets of context words (C(w1,K) and
C(w2,K)) are created by collecting all the
words falling in the window of ±K in the given
text corpus. Let I be the set of words which
appear in the context of both w1 and w2, i.e.
I = C(w1,K) ∩ C(w2,K).102



For w1 For w2
Forward Backward Forward Backward
w1, c1, c2 c1, c2, w1 w2, c1, c2 c1, c2, w2
w1, c1, c3 c1, c3, w1 w2, c1, c3 c1, c3, w2
w1, c2, c1 c2, c1, w1 w2, c2, c1 c2, c1, w2
w1, c2, c3 c2, c3, w1 w2, c2, c3 c2, c3, w2
w1, c3, c1 c3, c1, w1 w2, c3, c1 c3, c1, w2
w1, c3, c2 c3, c2, w1 w2, c3, c2 c3, c2, w2

Table 2: Example word sequences considered

The context of w1 and w2 is then defined
as set of word sequences of length K starting
at or ending at either w1 or w2. The word
sequences are formed by considering all pos-
sible combinations of the words in the set I.
If |I| = M , then MK word sequences starting
at w1 or w2 are considered and M

K word se-
quences ending at w1 or w2 are considered. If
I = {c1, c2, c3} and K = 2, then the word se-
quences created for w1 and w2 are as shown in
the table 2. Each word sequence is also asso-
ciated with a weight which corresponds to its
Language Model probability. For a forward
sequence w1, c1, c2, its weight is set as follows,

λw1,c1,c2 = Pforward(c1, c2|w1) = Pf (c1|w1)Pf (c2|c1)

Pf (c1|w1) is computed by using the bigram
statistics learnt from the text corpus, i.e. it
is the ratio of number of times the word c1
succeeds w1 in the corpus to the number of
times w1 occurs in the corpus.

Pf (c1|w1) = n(w1, c1)
n(w1)

Similarly, for a backward sequence c1, c2, w1,
we set its weight as,

λc1,c2,w1 = Pbackward(c1, c2|w1) = Pb(c2|w1)Pb(c1|c2)

Pb(c2|w1) is computed as the ratio of number
of times the word c2 precedes w1 in the corpus
to the number of times w1 occurs in the corpus.

Pb(c2|w1) = n(c2, w1)
n(w1)

The context of a word is a set of word
sequences (ws’s) and their corresponding
weights (λ’s) as follows,

Cseq(w1,K) = {〈ws11, λ(ws11)〉, · · · , 〈ws1N , λ(ws1N )〉}

Cseq(w2,K) = {〈ws21, λ(ws21)〉, · · · , 〈ws2N , λ(ws2N )〉}

It is to be noted that the word sequence
ws1i (in Cseq(w1,K)) corresponds to ws

2
i (in

Cseq(w2,K)) and they both are equal except
that ws1i starts or ends in w1 whereas ws

2
i

starts or ends in w2.

Word pair simjaccard simcontext LMSim
client,

0.1545 0.754 0.1165
complaint
succession,

0.2019 0.9651 0.1949
plan
collaboration,

0.4259 0.2643 0.1126
realization
mapping,

0.4672 0.2951 0.1379
publication

Table 3: Word pairs having low LMSim

We define similarity based on Language
Model between w1 and w2 as follows,

simjaccard(w1, w2) =

∑N
i=1 min(λ(ws

1
i ), λ(ws

2
i ))∑N

i=1 max(λ(ws
1
i ), λ(ws

2
i ))

(1)

The similarity simJaccard between w1 and w2
is nothing but the weighted Jaccard similar-
ity (Surgey, 2010) between Cseq(w1,K) and
Cseq(w2,K). Here, we treat ws

1
i and ws

2
i as

equal while computing Jaccard similarity as
noted earlier.

As we are constructing the word sequences
by only using the words in the intersection of
the contexts of the two words, we weigh down
the similarity by multiplying it with the com-
mon context factor, simcontext defined as,

simcontext(w1, w2) = max

( |I|
|C(w1,K)|

,
|I|

|C(w2,K)|

)

LMSim, the similarity between two words w1
and w2 is computed as,

LMSim(w1, w2) = simjaccard(w1, w2)∗simcontext(w1, w2)
3.2.1 Why Are Both simjaccard and

simcontext Important?

Considering only simcontext can be misleading,
as many co-occurring word pairs will have a
high simcontext. This is evident for the top
2 word pairs in Table 3 which have got high
simcontext even though they are not similar.
As simjaccard for these pairs is very low, LM-
Sim is reduced as desired.

On the other hand, due to smaller com-
mon context, considering only simjaccard can
be misleading. In Table 3, we can observe that
the bottom 2 word pairs have relatively high
simjaccard, but due to their smaller common
context, their simcontext is low which results
in low LMSim as desired.

3.3 Efficient Computation of LMSim

For computing simjaccard (Eq. 1), the sum of
N terms is required to be computed. Here,103



Figure 1: All possible forward word sequences
(intersection scenario)

N = 2 ·MK where K is the context window
considered and M is the number of words in
the context of both the words in consideration
for computing similarity. Even for a moderate
size corpus, the value ofM can be in thousands
and even for a small window of size 3, N will
be 10003 which is a huge number. Hence, it is
infeasible to actually enumerate all the N dis-
tinct word sequences. We propose an efficient,
dynamic programming based approach similar
to Viterbi Algorithm (Forney, 1973) for fast
computation of Language Model similarity.

Figure 1 shows all possible forward paths
for words w1 and w2 formed using words in
the set I = {c1, c2, · · · cM}. It depicts the sce-
nario where the numerator (intersection of two
sets with weighted members) of Eq. 1 is being
computed. All possible backward paths can be
constructed in similar way.

At each level (L in the figure 1 which varies
from 1 to K), for each context word, we store
sum of probabilities of all the word sequences
ending in that particular word. This is similar
to the Viterbi algorithm used for finding most
probable state sequence for HMMs. The only
difference is that, at each level for each state,
Viterbi algorithm stores maximum of proba-
bilities of all state sequences ending in that
particular state whereas our algorithm stores
the sum of probabilities of all word sequences
ending in a particular word. This algorithm
to compute simjaccard reduces the number of
computations from O(MK) to O(KM2).

4 Experimental Analysis

In this section, we describe details about ex-
perimental evaluation of our algorithm.

4.1 Datasets

We use text corpora from following domains:
1. SE domain: A collection of 138159 task
descriptions/expectations for various roles
performed in Software services industry con-
taining 1276514 words.
2. Agriculture domain: A set of 30533 doc-
uments in English containing 999168 sentences
and approximately 19 million words. It was
collected using crawler4j3 by crawling various
agriculture news sites.

There are no publicly available datasets of
similar words for the SE and Agriculture do-
mains. We selected some domain-specific word
pairs for these domains and obtained human
assigned similarity scores for these pairs. For
the SE domain, we had 500 word pairs an-
notated for similarity scores (0:No similarity,
1:Weak, 2:High) by 4 human annotators fa-
miliar with the domain terminologies. The
average of the similarity scores assigned by
all the annotators is then considered as the
gold-standard similarities for these word pairs.
Similarly, we created a standard dataset4 of
200 word pairs in Agriculture domain.

4.2 Comparison with Other
Approaches

1. Distributional Similarity (DistSim):
For each word, a TF-IDF vector is constructed
which encodes how the other words co-occur
with that particular word. The similarity be-
tween any two words is then computed as the
cosine similarity between their corresponding
TF-IDF vectors.
2. Latent Dirichlet Allocation (LDA):
LDA (Blei et al., 2003) is a probabilistic gen-
erative model that can be used to uncover the
underlying semantic structure of a document
collection. LDA gives per-word per-document
topic assignments that can be used to find a
likely set of topics and represent each docu-
ment in the collection in the form of topic pro-
portions. We find probability distribution of a
word over topics using the number of times
the word is assigned to a topic. We com-
pute similarity between words wi and wj as
the Jensen-Shannon (JS) divergence between
their respective probability distributions over

3code.google.com/p/crawler4j/
4contact authors for the datasets104



topics. We experimented with different num-
ber of topics and reported the results of best
performing number of topics.
3. WordNet and Wikipedia based sim-
ilarities: We used WordNet-based approach
“Lin similarity” (Lin, 1998) for computing
word similarities. We used its NLTK imple-
mentation5 with Brown corpus for IC statis-
tics. This algorithm computes similarity be-
tween two synsets (senses). As we are not
disambiguating sense of the words in a given
word pair, we compute Lin similarity between
all possible combinations of senses of the two
words and consider the maximum of these sim-
ilarities as overall similarity between words.
We consider that the words not present in
WordNet have similarity score of 0 with any
other word. We also compare LMSim with
Wikipedia based ESA6.
4. Brown Word Clustering: We used
the Brown word clustering implementation7

by Percy Liang. Each word cluster is rep-
resented by a binary (0/1) string indicating
the path taken from the root to the word in
consideration in the hierarchical word cluster-
ing output. The algorithm does not explic-
itly compute the word similarities, hence for
any two words w1 and w2 having binary string
cluster representations s1 and s2, we compute
similarity as,

simbrown(w1, w2) =
|CommonPrefix(s1, s2)|

Average(|s1|, |s2|)

Here, |si| indicates the length of the binary
string si. The only parameter required for
Brown word clustering algorithm is the num-
ber of clusters to form. We got the best results
with 1000 clusters for SE domain and 500 clus-
ters for Agriculture domain.

4.3 Results

We use each algorithm discussed above to
assign similarity scores to each word pair
in our gold-standard dataset. Performance
of each algorithm (see Table 4) is judged
by computing correlation between an algo-
rithm assigned word similarities with the
gold-standard word similarities. We could not
compare LMSim with LDA for Agriculture

5www.nltk.org
6http://treo.deri.ie/easyesa/
7http://github.com/percyliang/brown-cluster

Algorithm
Correlation with
Gold-standard

SE Agriculture
Lin Similarity 0.1675 0.2303

ESA 0.1527 0.3940
DistSim 0.3278 0.3645

LDA 0.4738 NA
Brown 0.479 0.5945
LMSim 0.5639 0.6229

Table 4: Relative performance of algorithms

dataset due to large size of the corpus. We
can observe here that for both the datasets,
LMSim performs better than WordNet and
Wikipedia based approaches because of ab-
sence of many domain-specific words/senses
in these resources. At the same time, LMSim
performs better than DistSim, LDA and
Brown word clustering algorithm, so we can
say that LMSim encodes the context infor-
mation in better way through forward and
backward context word sequences. Table 5
and Table 6 show some of the highly similar
word pairs discovered for both the domains.
Following are some key observations of the
experimental results.
1. Importance of Word Order : LM-
Sim scores over other context based word
similarity approaches like DistSim and LDA
because it encodes word order of the context
words using Language Model. For example, a
word pair asset - wiki incorrectly assigned
high similarity scores by both DistSim and
LDA, because these two words co-occur quite
frequently. Some example text fragments in
which they usually co-occur are as follows:
-Number of Assets to be logged into wiki ...

-No. of assets created in team Wiki..

At least one word not present in WordNet
Word Pair Comment
clarity, epm Clarity is a tool for EPM (En-

terprise Project Management)
sit ,uat SIT:System Integration Testing,

UAT:User Acceptance Testing
solution, POCs POC : “Proof of concept”
.Net, Java Types of softwares
Both words present in WordNet
Word Pair Comment
people, resource “person” sense of resource not

present in WN
complaint,
escalation

“complaint” sense of
escalation absent in WN

test, regression Regression is a kind of testing

Table 5: Examples from SE domain105



At least one word not present in WordNet
Word Pair Comment
cruiser, gaucho Both are insecticides
clincher, regiment Both are herbicides
ethanol, biodiesel ethanol is used in pro-

ducing biodiesel
fusarium, verticillium Names of fungi
glyphosate, herbicide Glyphosate is a herbi-

cide
Both words present in WordNet
Word Pair Comment
farmer, producer “farmer” sense of pro-

ducer is absent in WN
subsoil, topsoil Different layers of soils

Table 6: Examples from Agriculture domain

-Number of assets posted to Wiki..

For this word pair, LMSim assigns a low
similarity score as they do not tend to share
similar context word sequences.
2. Limitations of LMSim: We ob-
served that LMSim assigns high similarity
for synonyms (client-customer), antonyms
(dry-wet) and siblings (spring-summer,
.Net-Java), but for some hypernymy rela-
tions (like consultant-associate) it assigns
a low similarity score because of strict replace-
ability constraint. In future, we will revise
our algorithm to overcome this limitation.

5 Conclusions and Future Work

We proposed a new algorithm LMSim using
a Language Modeling approach for computing
domain-specific word similarities. We demon-
strated the performance of LMSim on two dif-
ferent domains - Software Engineering (SE)
domain and Agriculture domain. To the best
of our knowledge this is the first attempt for
discovering similar words in these domains.

The important advantages of LMSim over
previous approaches are - i) it does not require
any linguistic resources (like WordNet) hence
it saves cost, time and human efforts involved
in creating such resources, ii) LMSim incorpo-
rates the word order information within con-
text of a word resulting in better estimates
of word similarity compared to other corpus-
based approaches that ignore word order. We
proposed an efficient dynamic programming
based algorithm for computing LMSim. Our
experiments show that LMSim better corre-
lates with human assigned word similarities as
compared to other approaches.

In future, we plan to revise LMSim by ex-
tending it to higher order language models,
trying better smoothing techniques and us-
ing other information like POS tags for hav-
ing better estimates of word probabilities. We
also plan to extend our algorithm to work for
Indian languages as lexical resources for these
languages are not widely available. We would
like to experiment with other domains like Me-
chanical Engineering, Legal domain etc. We
believe that our algorithm can facilitate con-
struction of domain-specific ontologies.

References

D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
Dirichlet Allocation. The Journal of Machine
Learning Research, 3:993–1022, March 2003.

A. Budanitsky and G. Hirst. Evaluating wordnet-
based measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13–47, 2006.

G. D. Forney Jr. The viterbi algorithm. Proceed-
ings of the IEEE, 61(3):268–278, 1973.

E. Gabrilovich and S. Markovitch. Computing
semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI, volume 7,
pages 1606–1611, 2007.

G. Halawi, G. Dror, E. Gabrilovich, and Y. Ko-
ren. Large-scale learning of word relatedness
with constraints. In SIGKDD, pages 1406–1414.
ACM, 2012.

S. Ioffe. Improved consistent sampling, weighted
minhash and l1 sketching. Data Mining
(ICDM), 2010 IEEE 10th International Confer-
ence on. IEEE, 2010, pages 246–255, 2010.

D. Jurafsky and J. H. Martin. Speech & Language
Processing. Pearson Education India, 2000.

L. Lee. Measures of distributional similarity. In
ACL 1999, pages 25–32.

D. Lin. An information-theoretic definition of sim-
ilarity. In ICML, vol. 98, pages 296–304, 1998.

G. A. Miller. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38(11):39–
41, 1995.

T. Pedersen, S.V. Pakhomov, S. Patwardhan and
C.G. Chute. Measures of semantic similarity
and relatedness in the biomedical domain. Jour-
nal of biomedical informatics, 40(3):288–299,
2007.

Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra,
V. J. D., & Lai, J. C. Class-based n-gram mod-
els of natural language. In Computational lin-
guistics, 18(4), pages 467–479.106


