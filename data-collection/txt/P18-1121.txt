



















































Guess Me if You Can: Acronym Disambiguation for Enterprises


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1308–1317
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1308

Guess Me if You Can: Acronym Disambiguation for Enterprises
Yang Li1∗, Bo Zhao2, Ariel Fuxman1, Fangbo Tao3

1Google, Mountain View, CA, USA
2Pinterest, San Francisco, CA, USA
3Facebook, Menlo Park, CA, USA

{zheda2006liyang, bo.zhao.uiuc, afuxman, fangbo.tao}@gmail.com

Abstract

Acronyms are abbreviations formed from
the initial components of words or phrases.
In enterprises, people often use acronyms
to make communications more efficient.
However, acronyms could be difficult to
understand for people who are not fa-
miliar with the subject matter (new em-
ployees, etc.), thereby affecting produc-
tivity. To alleviate such troubles, we
study how to automatically resolve the true
meanings of acronyms in a given con-
text. Acronym disambiguation for enter-
prises is challenging for several reasons.
First, acronyms may be highly ambigu-
ous since an acronym used in the enter-
prise could have multiple internal and ex-
ternal meanings. Second, there are usu-
ally no comprehensive knowledge bases
such as Wikipedia available in enterprises.
Finally, the system should be generic to
work for any enterprise. In this work
we propose an end-to-end framework to
tackle all these challenges. The frame-
work takes the enterprise corpus as input
and produces a high-quality acronym dis-
ambiguation system as output. Our dis-
ambiguation models are trained via dis-
tant supervised learning, without requiring
any manually labeled training examples.
Therefore, our proposed framework can be
deployed to any enterprise to support high-
quality acronym disambiguation. Experi-
mental results on real world data justified
the effectiveness of our system.

1 Introduction

Acronyms are abbreviations formed from the ini-
tial components of words or phrases (e.g., “AI”
from “Artificial Intelligence”). As acronyms can
shorten long names and make communications

∗Work done while authors were at Microsoft Research.

more efficient, they are widely used at almost ev-
erywhere in enterprises, including notifications,
emails, reports and social network posts. Figure 1
shows a sample enterprise social network post. As
we can see, acronyms are frequently used there.

Someone

Figure 1: Acronyms in Enterprises
Despite the fact that acronyms can make com-

munications more efficient, sometimes they could
be difficult to understand, especially for people
who are not familiar with the specific areas, such
as new employees and patent lawyers. We ran-
domly sampled 1000 documents from a Microsoft
question answering forum and found out that only
7% of the acronyms co-occur with the corre-
sponding meanings in the same document, which
means 93% of the time when the user does not
understand an acronym, she will need to find clues
outside of the document. Therefore, it is partic-
ularly useful to develop a system that can auto-
matically resolve the true meanings of acronyms
in enterprise documents. Such system could be
run online as a querying tool to handle any ad-
hoc document, or run offline to annotate acronyms
with their true meanings in a large corpus. In the
offline mode, the true meanings can be further in-
dexed by an enterprise search engine, so that when
users search for the true meaning, documents con-
taining the acronym can also be found.

The enterprise acronym disambiguation task
is challenging due to the high ambiguity of
acronyms, e.g., “SP” could stand for “Service
Pack”, “SharePoint” or “Surface Pro” in Mi-
crosoft. And there is one additional challenge
compared with previous disambiguation tasks: in
an enterprise document, an acronym could refer



1309

to either an internal meaning (concepts created by
the enterprise that may or may not be found out-
side) or an external meaning (all concepts that are
not internal). For example, regarding the acronym
“AI”, “Asset Intelligence” is an internal meaning
mainly used only in Microsoft, while “Artificial
Intelligence” is an external meaning widely used
in public. A good acronym disambiguation sys-
tem should be able to handle both internal and ex-
ternal meanings. As we will explain in details, it
is important to make such distinction and different
strategies are needed for such two cases.

For internal meanings, there are some previ-
ous work on word sense disambiguation (Navigli,
2009) and acronym disambiguation (Feng et al.,
2009; Pakhomov et al., 2005; Pustejovsky et al.,
2001; Stevenson et al., 2009; Yu et al., 2006) on
a closed-domain corpus. The main challenge here
is that there are rarely any domain-specific knowl-
edge bases available in enterprises, therefore all
the signals for disambiguation (including poten-
tial meanings, and their popularity scores, context
representations, etc.) need to be mined from plain
text. Training data should also be automatically
generated to make the system easily scale out to all
enterprises. Compared with previous work, we de-
veloped a more comprehensive and advanced set
of features in the disambiguation model, and also
used a much less restrictive way to discover mean-
ing candidates and training data, so that both pre-
cision and recall can be improved. Moreover, one
main limitation of all previous work is that they
do not distinguish internal and external meanings.
They merely rely on the enterprise corpus to dis-
cover information about external meanings, which
we observe is quite ineffective. The reason is that
for popular external meaning like “Artificial Intel-
ligence”, people often directly use its acronym in
enterprises without explanation, therefore there is
limited information about the connection between
the acronym and the external meaning in the en-
terprise corpus. On the other hand, there are much
more such information available in the public do-
main, which should be leveraged by the system.

If we consider utilizing a public knowledge base
such as Wikipedia to better handle external mean-
ings of acronyms, the problem becomes very re-
lated to the well studied Entity Linking (Ji and
Grishman, 2011; Cucerzan, 2007; Dredze et al.,
2010; Hoffart et al., 2011; Li et al., 2013, 2016;
Ratinov et al., 2011; Shen et al., 2012) prob-

lem, which is to map entity mentions in texts to
their corresponding entities in a reference knowl-
edge base (e.g. Wikipedia). But our disam-
biguation task is different from the entity link-
ing task, because the system also needs to handle
internal meanings which are not covered by any
knowledge bases, and ultimately needs to decide
whether an acronym refers to an internal meaning
or an external meaning. It is nontrivial to combine
the information mined from the enterprise corpus
and the public knowledge base so that the system
can get the best of both worlds. For instance, we
have tried to run an internal disambiguator (lever-
aging information mined from enterprise corpus)
and then resort to a public entity linking system if
the internal one’s confidence is low, but the perfor-
mance is very poor. Even for external meanings,
it is important to leverage signals from the enter-
prise corpus since the context surrounding them
could be quite different from that in the external
world, and context is one of the most important
factor for disambiguation. For example, in pub-
lic world, when people mention “Operating Sys-
tem” they mainly talk about how to install or use
it; while within Microsoft, when people mention
“Operating System” most of the time they focus
on how to design or implement it.

In this work, we design a novel, end-to-end
framework to address all the above challenges.
Our framework takes the enterprise corpus and
certain public knowledge base as input and pro-
duces a high-quality acronym disambiguation sys-
tem as output. The models are all trained via dis-
tant supervised learning, therefore our system re-
quires no manually labeled training examples and
can be easily deployed to any enterprises.

2 Problem Statement

The Enterprise Acronym Disambiguation prob-
lem is comprised of two sub-problems. The first
one is Acronym Meaning Mining (Adar, 2004; Ao
and Takagi, 2005; Park and Byrd, 2001; Schwartz
and Hearst, 2002; Jain et al., 2007; Larkey et al.,
2000; Nadeau and Turney, 2005; Taneva et al.,
2013), which aims at mining acronym/meaning
pairs from the enterprise corpus. Each meaning
m should contain the full name expansion e, pop-
ularity score p (indicating how often m is used
as the genuine meaning of acronym a) and con-
text words W (i.e. words frequently used in con-
text of the meaning). The popularity score and



1310

context words can provide critical information for
making disambiguation decisions. The second one
is Meaning Candidate Ranking, whose goal is to
rank the candidate meanings associated with the
target acronym a and select the genuine meaning
m based on the given context.

In this paper we assume the acronyms for dis-
ambiguation are provided as input to the system,
either by the user or by an existing acronym de-
tection module. We do not try to optimize the
performance of acronym detection (e.g. identify-
ing acronyms beyond the simple capitalized rule,
or distinguishing cases where a capitalized term is
not an acronym but a regular English word, such
as “OK”). The task of acronym detection is also
interesting and important. But due to the space
limit, it is beyond the scope of this paper.

3 Framework

We propose a novel end-to-end framework to solve
the Enterprise Acronym Disambiguation problem.
Our framework takes the enterprise corpus as input
and produces a high-quality acronym disambigua-
tion system as output. Figure 2 shows the details
of our proposed framework. In the mining module,
we will sequentially perform Candidates Genera-
tion, Popularity Calculation, Candidates Dedupli-
cation and Context Harvesting on the input enter-
prise corpus. The details of these steps will be dis-
cussed in Section 4. After mining steps, we will
get an acronym/meaning repository storing all the
mined acronym/meaning pairs. Feed this reposi-
tory together with the training data (automatically
generated via distant supervision from the enter-
prise corpus) to the training module, we will get a
candidate ranking model, a confidence estimation
model and a final selection model. These models
form the final acronym disambiguator and will be
used in the testing module for actual acronym dis-
ambiguation. In the testing module, given the tar-
get acronym along with some context as input, the
system will output the predicted meaning. Note
that the mining and training module run offline
once for the entire corpus or periodically when the
corpus update, while the testing can be run online
repeatedly for processing new documents.

4 Acronym Meaning Mining

4.1 Candidates Generation
As there is no reference dictionary or knowledge
base available in enterprise telling us the potential

Figure 2: Framework
meanings of acronyms, we have to extract them
from plain text. We propose a strategy called
Hybrid Generation to balance extraction accuracy
and coverage. Namely, we treat a phrase as a
meaning candidate for an acronym if: (1) the ini-
tial letters of the phrase match the acronym and
the phrase and the acronym co-occur in at least
one document in the enterprise corpus; or (2) it is
a valid candidate for the acronym in public knowl-
edge bases (e.g. Wikipedia). The insight of this
strategy is that the valid candidates missed by con-
dition (1) are mainly public meanings which can
be found in public knowledge bases. With this
strategy we can make our system understand both
the internal world and the external world.

4.2 Popularity Calculation
As mentioned in Section 2, for each candidate
meaning, we need to calculate its popularity score,
which reveals how often the candidate meaning is
used as the genuine meaning of the acronym. In
previous research on Entity Linking, popularity is
calculated as the fraction of times a candidate be-
ing the target page for an anchor text in a reference
knowledge base (e.g. Wikipedia). However, in en-
terprises, we do not have a knowledge base with
anchor links. Thus we cannot calculate popularity
in the same way. Here we propose to calculate two
types of popularity to mimic the effect.

1. Marginal Popularity.
MP (mi) =

Count(mi)∑n
j=1Count(mj)

, (1)

where m1, m2, . . ., mn are the meaning can-
didates of acronym a and Count(mi) is the
number of occurrences for mi in the corpus.

2. Conditional Popularity.

CP (mi) =
Count(mi, a)∑n
j=1Count(mj , a)

, (2)

where m1, m2, . . ., mn are the meaning
candidates of acronym a and Count(mi, a)
is the number of document-level co-
occurrences for mi and a in the corpus.



1311

Conditional Popularity can more reasonably re-
veal how often the acronym is used to represent
each meaning candidate. However, due to the data
sparsity issue in enterprises, many valid candi-
dates may get zero value for conditional popularity
since they may never co-occur with the acronyms
in the enterprise corpus. The Marginal Popular-
ity does not have this problem since it is calcu-
lated from the raw counts of the candidates. Yet
on the other hand, high marginal popularity score
does not necessarily indicate high correlation be-
tween the candidate and the acronym. It is unclear
how to combine the two scores into one popular-
ity score, so we use both of them as features in the
disambiguation model.

4.3 Candidates Deduplication
In enterprises, people often create many vari-
ants (including abbreviations, plurals or even mis-
spellings) for the same meaning, therefore many
mined meaning candidates are actually equivalent.
For example, for the meaning “Certificate Author-
ity” of the acronym “CA”, the variants include
“Cert Auth”, “Certificate Authorities” and many
others. It is important to deduplicate these vari-
ants before sending them to the disambiguation
module. The deduplication helps aggregate disam-
biguation evidences and reduce noises. We design
several heuristic rules1 to perform the deduplica-
tion. Experiments show that the rules can accu-
rately group the variants together. After grouping,
we sort the variants within the same group based
on their marginal popularity. The candidate with
the largest marginal popularity is selected as the
canonical candidate for the group. Other variants
in the group will be deleted from the candidate list
and their popularity scores will be aggregated to
the canonical candidate. We maintain a table to
record the variants for each canonical candidate.

4.4 Context Harvesting
In this step, we aim to harvest context words for
each meaning candidate. These context words
could be used to calculate context similarity with
the query context. For each meaning candidate m,
we put its canonical form and all its variants (from
the variants table in Section 4.3) into set S. Then
we scan the enterprise corpus, each time we find
a match of any e ∈ S, we harvest the words in a

1Due to space limitations, the detailed rules are omitted.
Example rules are “word overlap percentage after stemming
> 0.8”, “corresponding component words share same prefix”.

... Basically,     

using direct AD 

Import fails 

if Sharepoint 

Code Analysis

is configured to 

run over SSL ...

Acronym: CA

Ground Truth: Code Analysis

Context:

... Basically, using   

direct AD Import  

fails if Sharepoint 

CA is configured to 

run over SSL ...

Figure 3: Distant Supervision Example

width-W word window surrounding e as the con-
text words of m. In our experiments we set win-
dow size as 30 after trying to vary the window size
from 10 to 50 and finding 30 gives the best result.

As mentioned before, some popular public
meanings might be mentioned very rarely by their
full names in the enterprise corpus since people di-
rectly use their acronyms most of the time. There-
fore, the above context harvesting process can
only get very few context words for those public
meanings. To alleviate this, for each public mean-
ing we add its Wikipedia page’s content as com-
plementary context. By doing so, we ensure al-
most all valid candidates get a reasonable amount
of context words.

5 Meaning Candidate Ranking

5.1 Candidate Ranking

We first train a candidate ranking model to rank
candidates with respect to the likelihood of being
the genuine meaning for the target acronym.

5.1.1 Training Data Generation

In order to train a robust ranking model, we need
to get adequate amount of labeled training data.
Manually labeling is obviously too expensive and
it requires a lot of domain knowledge, which
severely limits our framework’s generalization ca-
pability. To tackle this problem, we propose to au-
tomatically generate training data via distant su-
pervision. The intuition is that since acronyms
and the corresponding meanings are semantically
equivalent, people use them interchangeably in en-
terprises. Therefore we can fetch documents con-
taining the meaning, replace the meaning with the
corresponding acronym and treat the meaning as
ground truth. Figure 3 shows an example of this
automatic training data generation process.

5.1.2 Training Algorithm

Any learning-to-rank algorithms can be used here.
In our system we utilize the LambdaMART algo-
rithm (Burges, 2010) to train the model.



1312

5.1.3 Features
Now we explain the features we developed for
the candidate ranking model. First, we have the
Marginal Popularity score and Conditional Popu-
larity score as two context-independent features,
which could compensate for each other. However,
as discussed in the previous section, some popu-
lar public meanings (e.g., “Artificial Intelligence”)
can be rarely mentioned in enterprise corpus by
their full names, therefore both their marginal pop-
ularity score and conditional popularity score can
be very low. To address this, we add a third feature
called Wiki Popularity, which is calculated from
Wikipedia anchor texts to capture how often an
acronym refers to a public meaning in Wikipedia.
The fourth feature we adopt is Context Similarity.
We convert the harvested context for the mean-
ing and the query context of the target acronym
into TFIDF vectors and then compute their co-
sine similarity2. We also include two features
(i.e. LeftNeighborScore and RightNeighborScore)
to capture the effect of the immediate neighbor-
ing words, which are more important than further
context words since immediate words could form
phrases with the acronym. For example, if we see
an acronym “SP” followed by the word “2”, then
likely it stands for “Service Pack”. However, if
we see “SP” followed by “2003”, then probably its
genuine meaning is “SharePoint”. The last feature
we use is FullNamePercentage. This feature is de-
fined as the percentage of the meaning candidate’s
component words appearing in the context of the
target acronym. Table 1 summarizes the features
used to train the candidate ranking model.

5.2 Confidence Estimation
After getting the ranking results, we propose to ap-
ply a confidence estimation step to decide whether
to trust the top ranked answer. There are two moti-
vations behind. First, our candidate generation ap-
proach is not perfect, therefore we could encounter
cases in which the genuine meaning is not in our
candidates. For such cases, the top ranked answer
is obviously incorrect. Second, our training data is
biased towards the internal meanings since exter-
nal meanings may rarely appear with full names.

2One popular alternative to measure context similarity is
using word embeddings (Mikolov et al., 2013; Li et al., 2015).
In our system we experimented replacing TFIDF cosine sim-
ilarity with word embedding similarity, or adding word em-
bedding similarity as an additional feature, but both hurt the
disambiguation accuracy. So we only included the TFIDF co-
sine similarity as the context similarity feature in our system.

As a result, the learned ranking model may lack
the capability to properly rank the external mean-
ings. In such cases, we would better have the sys-
tem return nothing rather than directly provide a
wrong answer to mislead the user. In this step, we
train a confidence estimation model, which will
estimate the top result’s confidence.

5.2.1 Training Data Generation
Similar to the ranker training, here the training
data is also automatically generated. We run the
learned ranker on some distant labeled data (gen-
erated from a different corpus), and then check if
the top ranked answer is correct or not. If it is
correct, we generate a positive training example;
otherwise we make a negative training example.

5.2.2 Training Algorithm
Any classification algorithms can be used here. In
our system we utilize the MART boosted tree al-
gorithm (Friedman, 2000) to train the model.

5.2.3 Features
We design 7 features (summarized in Table 2) to
train the confidence estimation model. There are
two intuitions behind: (1) If the top-ranked an-
swer’s ranking score is very small, or the top-
ranked answer’s score is close to the second-
ranked answer’s score, then the ranking is not
very confident; (2) If the acronym has a domi-
nating candidate in the public domain (e.g., “Per-
sonal Computer” is the dominating candidate
for “PC”), and the candidates’ Wiki popularity
distribution is significantly different from their
marginal/conditional popularity distributions, then
the ranker’s output is not very confident. The first
intuition covers the first 3 features, while the sec-
ond intuition covers the last 4 features.

5.3 Final Selection
We have discussed that one particular motivation
for confidence estimation is that the candidate
ranking stage has some bias so it does not always
rank public meanings at top when they are cor-
rect. Therefore, assuming the confidence estima-
tion model can remove incorrect top-ranked result,
we still need one additional step to decide if any
public meaning is correct, which we call a final se-
lection model. In this step, we determine whether
to return the most popular public meaning (based
on Wiki Popularity) as the final answer, and this
step is only triggered when the confidence estima-
tor judges that the ranking result is unconfident.



1313

Feature Description
MarginalPopularity The meaning candidate’s marginal popularity score

ConditionalPopularity The meaning candidate’s conditional popularity score
WikiPopularity The meaning candidate’s Wiki popularity score

ContextSimilarity TFIDF cosine similarity between meaning context and acronym context
LeftNeighborScore Probability of acronym and meaning sharing the same immediate left word

RightNeighborScore Probability of acronym and meaning sharing the same immediate right word
FullNamePercentage Percentage of meaning candidate’s component words appearing in acronym context

Table 1: Candidate Ranking Features

Feature Description
Top1Score Top 1 ranked meaning candidate’s ranking score

Top1&2ScoreDiff Difference between 1st and 2nd ranked meaning candidates’ ranking score
Top1&2CtxSimDiff Difference between 1st and 2nd ranked meaning candidates’ context similarity score
Top1WikiPopularity Top 1 ranked meaning candidate’s Wiki popularity score
MaxWikiPopularity Max Wiki popularity score among all the meaning candidates
MaxWP&MPGap Max gap between Wiki and marginal popularity among all the meaning candidates
MaxWP&CPGap Max gap between Wiki and conditional popularity among all the meaning candidates

Table 2: Confidence Estimation Features

The goal of the final selection model is simi-
lar to that of the confidence estimation model. In
confidence estimation, we judge whether the top-
ranked answer is correct; while in final selection,
we check whether the most popular external mean-
ing is correct. Thanks to this similarity, we can
reuse the data, features and training algorithm in
confidence estimation model. We take the same
training data in Section 5.2.1 and update the labels
correspondingly: if the genuine answer is the most
popular external meaning, we generate a positive
example; otherwise we make a negative one.

6 Experiments

6.1 Data

6.1.1 Mining and Training Corpus
We use both the Microsoft Answer Corpus (MAC)
and the Microsoft Yammer Corpus (MYC) as the
mining corpus. These corpus are kindly shared
to us by Microsoft for research purpose. MAC
contains 0.3 million web pages from a Microsoft
internal question answering forum. MYC is con-
sisted of 6.8 million posts from Microsoft’s Yam-
mer social network. In total, our mining module
harvested 5287 acronyms and 17258 meaning can-
didates from this joint corpus.

For model training, the confidence estimation
model and final selection model need to be trained
on a different corpus than the candidate ranking
model. So we train the candidate ranking model

on MAC, with 12500 training examples being au-
tomatically generated; and train the confidence es-
timation and final selection model on MYC, with
40000 training instances being generated.

6.1.2 Evaluation Datasets
We prepared four datasets3 for evaluation pur-
poses. The first one Manual is obtained from the
recent pages of Microsoft answer forum. Note
these pages are disjoint from those used as min-
ing/training corpus. We randomly sampled 300
pages and filtered out pages which do not con-
tain ambiguous acronyms. After filtering, 240 test
cases were left and we manually labeled them.

The second one Distant is generated via distant
labeling on Microsoft Office365 documents. We
sampled 2000 documents which contain at least
one occurrence of a meaning candidate. Then
we replaced the meanings with the corresponding
acronyms and treat the meanings as ground truths.
We manually checked through this dataset to re-
move some bad cases (e.g., “AS” for “App Store”).
This resulted in a test set of 1949 test cases.

Comparing the Manual dataset with the Dis-
tant dataset, the Manual one, though in smaller
size, can more accurately evaluate the system per-
formance, since the target acronyms in it are sam-
pled from the real distribution, while in the Distant
dataset acronyms are artificially generated from

3Due to data confidentiality issue, we were unable to di-
rectly release these datasets.



1314

randomly sampled meanings.
We also want to compare our method with the

state-of-the-art Entity Linking (EL) systems based
on public knowledge bases such as Wikipedia.
However, it is unfair to directly compare as most
enterprise specific meanings are unknown to them.
Therefore, we need to only consider cases where
the true meaning is a public meaning covered by
both our system and the compared system. By fil-
tering the distant dataset from Office365, we get
the third dataset JoinW (1659 test cases) for com-
paring with the Wikifier (Ratinov et al., 2011), and
the fourth dataset JoinA (237 test cases) for com-
paring with AIDA (Hoffart et al., 2011).

6.2 Compared Methods

6.2.1 Ablations of Our System
We compare the following ablations of our system,
to illustrate the effectiveness of the features and
components.

• Internal Popularity (IP): Only the internal
popularity features (i.e., marginal popularity
and conditional popularity).

• Popularity (P): The internal popularity features
plus Wiki popularity features.

• Popularity+Context (P+C): The popularity
features plus context similarity feature.

• Popularity+Context+Neighbbor (P+C+N):
The popularity features, context similarity
feature and immediate neighbor features.

• Popularity+ Context+ Neighbbor+ Fullname
(a.k.a. Candidate Ranker, or CR): Using all
the features in candidate ranking module.

• Candidate Ranker + Confidence Estimator
(CR+ CE): Using the candidate ranking model
plus the confidence estimation model.

• Candidate Ranker + Confidence Estimator +
Final Selector (a.k.a. Acronym Disambigua-
tor, or AD): Using the candidate ranking model,
the confidence estimation model and the final
selection model. Full version of our system.

6.2.2 State-of-the-art EL Systems
We also compare our method with two state-of-
the-art Entity Linking (EL) systems.

• Wikifier: a popular EL system using machine
learning to combine various features together.

• AIDA: a robust EL system using mention-entity
graph to find the best mention-entity mapping.

6.3 Quality of Mined Acronyms/Meanings

We first conduct experiments to evaluate the
quality of the acronym/meaning pairs harvested
through our offline mining module. Out of the
17258 mined pairs, we randomly sampled 2000
of them and asked 5 domain experts to manually
check their validness. An acronym/meaning pair
is considered as valid if the majority of the ex-
perts think the acronym is indeed used to abbrevi-
ate the meaning. For example, (AS, Analysis Ser-
vice) is a valid pair, but (AS, App Store) is consid-
ered as invalid because people will not actually use
AS to represent App Store. Among the sampled
2000 pairs, 94.5% are labeled as valid, indicat-
ing our offline mining module can accurately ex-
tract acronym/meaning pairs from enterprise cor-
pus. It is hard to precisely evaluate the cover-
age/recall of our mining method, since it is very
difficult to obtain the complete meaning list for a
given acronym. To get a rough idea, we randomly
picked up 100 acronyms and asked the 5 domain
experts to enumerate the valid meanings for these
acronyms. In total we got 230 valid meanings and
all of them are covered by the mined pairs.

6.4 Disambiguation Performance

We first conduct experiments to evaluate the dis-
ambiguation performance of our ranking model,
and compare the helpfulness of the features used
in the model. Figure 4 shows the precision
(i.e., percentage of correctly disambiguated cases
among all predicted cases), recall (i.e., percentage
of correctly disambiguated cases among all test
cases) and F1 (i.e., harmonic mean of precision
and recall) of the compared methods on the Man-
ual dataset and the Distant dataset. In terms of the
helpfulness of the features, the context similarity
feature and the immediate neighbor features con-
tribute most to the performance gain. Other fea-
tures are less helpful, yet still bring improvements
to the overall performance.

Next we conduct experiments to illustrate the
effectiveness of the confidence estimation module
and the final selection module in our system. Fig-
ure 5 shows the precision, recall and F1 of the
compared system configurations on the Manual
and Distant dataset. As can be seen, the confi-
dence estimation module can improve precision at
the cost of hurting recall. Fortunately, the final
selection module can recover some recall losses
without sacrificing too much on precision. In



1315

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Manual	  Dataset Distant	  Dataset

Precision

IP P P+C P+C+N CR

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Manual	  Dataset Distant	  Dataset

Recall

IP P P+C P+C+N CR

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Manual	  Dataset Distant	  Dataset

F1

IP P P+C P+C+N CR

Figure 4: Ranking Performance

0.7

0.75

0.8

0.85

0.9

0.95

Manual Dataset Distant Dataset

Precision

CR CR+CE AD

0.7

0.75

0.8

0.85

0.9

0.95

Manual Dataset Distant Dataset

Recall

CR CR+CE AD

0.7

0.75

0.8

0.85

0.9

0.95

Manual Dataset Distant Dataset

F1

CR CR+CE AD

Figure 5: Effectiveness of Confidence Estimator and Final Selector

terms of the F1 measure, the final system achieves
the best performance.

Note that the ablation P+C naturally corre-
sponds to the existing acronym disambiguation
approaches (Feng et al., 2009; Pakhomov et al.,
2005; Pustejovsky et al., 2001; Stevenson et al.,
2009; Yu et al., 2006) mainly relying on context
words and domain specific resources. These ap-
proaches do not specifically distinguish internal
and external meanings. They merely rely on the
internal corpus to discover information about ex-
ternal meanings, which is quite ineffective in the
scenario of enterprise acronym disambiguation (as
discussed in Section 1). In comparison, our system
(AD) is able to leverage public resources together
with the internal corpus to better handle the prob-
lem and therefore significantly outperforms them.

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Precision Recall F1

Wikifier AD

(a) Wikifier vs. AD

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Precision Recall F1

AIDA AD

(b) AIDA vs. AD

Figure 6: Comparison with EL Systems

6.5 Comparison with EL Systems

We also compare our system (AD) with two state-
of-the-art Entity Linking (EL) systems: Wikifier
and AIDA. As explained in Section 6.1.2, we

made two datasets (i.e., JoinW and JoinA) for fair
comparisons. Figure 6(a) and Figure 6(b) present
the comparison of our AD system against Wikifer
and AIDA, respectively. As we can see from the
figures, AD significantly outperforms both Wiki-
fier and AIDA on all three measures. The reason is
that even for public meanings (e.g., Operating Sys-
tem) indexed by Wikifier and AIDA, the usage of
them could be quite different in enterprises (e.g.,
inside Microsoft people talk more about design-
ing Operating System rather than how to install it).
Wikifier and AIDA utilize information from public
knowledge bases (e.g., Wikipedia) to generate fea-
tures, therefore can hardly capture such enterprise-
specific signals. In contrast, our AD system mines
disambiguation features directly from the enter-
prise corpus and utilizes them together with the
public signals. As a result, it can more accurately
represent the characteristics of the enterprise and
lead to much better disambiguation performances.

7 Related Work

Acronym meaning discovery has received a lot of
attentions in vertical domains (mainly in biomed-
ical). Most of the proposed approaches (Adar,
2004; Ao and Takagi, 2005; Park and Byrd, 2001;
Schwartz and Hearst, 2002; Wren et al., 2002) uti-
lized generic rules or text patterns (e.g. brack-
ets, colons) to discover acronym meanings. These
methods are usually based on the assumption that



1316

acronyms are co-mentioned with the correspond-
ing meanings in the same document. However, in
enterprises, this assumption rarely holds. Enter-
prises themselves are closed ecosystems, so it is
very common for people to define the acronyms
somewhere and use them elsewhere. As a result,
such methods cannot be used for acronym mean-
ing discovery in enterprises.

Recently, there have been a few works (Jain
et al., 2007; Larkey et al., 2000; Nadeau and Tur-
ney, 2005; Taneva et al., 2013) on automatically
mining acronym meanings by leveraging Web data
(e.g., query sessions, click logs). However, it is
hard to apply them directly to enterprises, since
most data in enterprises are raw text and therefore
the query sessions/click logs are rarely available.

Acronym disambiguation can be seen as a spe-
cial case for the Entity Linking (EL) (Ji and Gr-
ishman, 2011; Dredze et al., 2010) problem. Ap-
proaches that link entity mentions to Wikipedia
date back to Bunescu et. al’s work (Bunescu and
Paşca, 2006). They computed the cosine similar-
ity between the text around the mention and the
entity candidate’s Wikipedia page. The referent
entity with the maximum similarity score is se-
lected as the disambiguation result. Cucerzan’s
work (Cucerzan, 2007) is the first one to real-
ize the effectiveness of using topical coherence to
globally perform EL. In that work, the topical co-
herence between the referent entity candidate and
other entities within the same context is calculated
based on their overlaps in categories and incom-
ing links in Wikipedia. Recently, several meth-
ods (Hoffart et al., 2011; Li et al., 2013, 2016;
Ratinov et al., 2011; Shen et al., 2012; Cheng and
Roth, 2013) also tried to enrich “context similar-
ity” and “topical coherence” using hybrid strate-
gies. Shen et. al (Shen et al., 2015) provided
a comprehensive survey for the techniques used
in EL. However, these EL techniques cannot be
used for acronym disambiguation in enterprises,
since most enterprise meanings are not covered by
public knowledge bases, and there are rarely any
domain-specific knowledge bases available in en-
terprises. Automatic knowledge base construction
(Suchanek et al., 2013) is promising, but the qual-
ity is far from applicable. Moreover, the struc-
tural information (e.g. entity taxonomy, cross-
document hyperlinks) within Wikipedia, is rarely
available in enterprises.

Most of the previous work (Feng et al., 2009;

Pakhomov et al., 2005; Pustejovsky et al., 2001;
Stevenson et al., 2009; Yu et al., 2006) on acronym
disambiguation heavily rely on context words
and domain specific resources. In comparison,
our method explored a more comprehensive set
of domain-independent features. Moreover, our
method used a much less restrictive way to dis-
cover meaning candidates and training data, which
is far more general than the methods relying on
strict definition patterns (Schwartz and Hearst,
2002). Another particular limitation of all these
previous work is that they do not distinguish inter-
nal and external meanings. They merely rely on
the internal corpus to discover information about
external meanings, which is quite ineffective.

8 Conclusions

In this paper, we studied the Acronym Disam-
biguation for Enterprises problem. We proposed
a novel, end-to-end framework to solve this prob-
lem. Our framework takes the enterprise cor-
pus as input and produces a high-quality acronym
disambiguation system as output. The disam-
biguation models are trained via distant supervised
learning, without requiring any manually labeled
training examples. Different from all the previ-
ous acronym disambiguation approaches, our sys-
tem is capable of accurately resolving acronyms
to both enterprise-specific meanings and public
meanings. Experimental results on Microsoft en-
terprise data demonstrated that our system can
effectively construct acronym/meaning reposito-
ries from scratch and accurately disambiguate
acronyms to their meanings with over 90% pre-
cision. Furthermore, our proposed framework can
be easily deployed to any enterprises without re-
quiring any domain knowledge.

References
Eytan Adar. 2004. Sarad: A simple and robust abbre-

viation dictionary. Bioinformatics, 20(4):527–533.

Hiroko Ao and Toshihisa Takagi. 2005. Alice: an algo-
rithm to extract abbreviations from medline. Jour-
nal of the American Medical Informatics Associa-
tion, 12(5):576–586.

Razvan Bunescu and Marius Paşca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of EACL, pages 9–16.

Christopher JC Burges. 2010. From ranknet to lamb-
darank to lambdamart: An overview. Learning,
11:23–581.



1317

Xiao Cheng and Dan Roth. 2013. Relational inference
for wikification. In Proceedings of EMNLP, pages
1787–1796.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of EMNLP-CoNLL, pages 708–716.

Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings of
COLING, pages 277–285.

Shicong Feng, Yuhong Xiong, Conglei Yao, Liwei
Zheng, and Wei Liu. 2009. Acronym extraction and
disambiguation in large-scale organizational web
pages. In Proceedings of CIKM, pages 1693–1696.

Jerome H Friedman. 2000. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 29:1189–1232.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of EMNLP, pages
782–792.

Alpa Jain, Silviu Cucerzan, and Saliha Azzam. 2007.
Acronym-expansion recognition and ranking on the
web. In Information Reuse and Integration, pages
209–214.

Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of ACL, pages 1148–1158.

Leah S Larkey, Paul Ogilvie, M Andrew Price, and
Brenden Tamilio. 2000. Acrophile: an automated
acronym extractor and server. In Proceedings of
ACM conference on Digital libraries, pages 205–
214.

Chao Li, Lei Ji, and Jun Yan. 2015. Acronym disam-
biguation using word embedding. In Proceedings of
AAAI, pages 4178–4179.

Yang Li, Shulong Tan, Huan Sun, Jiawei Han, Dan
Roth, and Xifeng Yan. 2016. Entity disambiguation
with linkless knowledge bases. In Proceedings of
WWW, pages 1261–1270.

Yang Li, Chi Wang, Fangqiu Han, Jiawei Han, Dan
Roth, and Xifeng Yan. 2013. Mining evidences
for named entity disambiguation. In Proceedings of
SIGKDD, pages 1070–1078.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in NIPS, pages 3111–3119.

David Nadeau and Peter D Turney. 2005. A supervised
learning approach to acronym identification. In Pro-
ceedings of CSCSI, pages 319–329.

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Comput. Surv., 41(2):10:1–10:69.

Serguei Pakhomov, Ted Pedersen, and Christopher G
Chute. 2005. Abbreviation and acronym disam-
biguation in clinical discourse. In AMIA Annual
Symposium Proceedings, pages 589–593.

Youngja Park and Roy J Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP, pages 126–133.

James Pustejovsky, Jose Castano, Brent Cochran,
Maciej Kotecki, Michael Morrell, and Anna
Rumshisky. 2001. Extraction and disambiguation
of acronym-meaning pairs in medline. Medinfo,
10(2001):371–375.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of ACL,
pages 1375–1384.

Ariel S Schwartz and Marti A Hearst. 2002. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Biocomputing, pages 451–462.

Wei Shen, Jianyong Wang, and Jiawei Han. 2015. En-
tity linking with a knowledge base: Issues, tech-
niques, and solutions. Knowledge and Data Engi-
neering, IEEE Transactions on, 27(2):443–460.

Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Linden: linking named entities with knowl-
edge base via semantic knowledge. In Proceedings
of WWW, pages 449–458.

Mark Stevenson, Yikun Guo, Abdulaziz Al Amri,
and Robert Gaizauskas. 2009. Disambiguation of
biomedical abbreviations. In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processing, pages 71–79.

Fabian Suchanek, James Fan, Raphael Hoffmann, Se-
bastian Riedel, and Partha Pratim Talukdar. 2013.
Advances in automated knowledge base construc-
tion. SIGMOD Records.

Bilyana Taneva, Tao Cheng, Kaushik Chakrabarti, and
Yeye He. 2013. Mining acronym expansions and
their meanings using query click log. In Proceed-
ings of WWW, pages 1261–1272.

Jonathan D Wren, Harold R Garner, et al. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated construc-
tion of comprehensive acronym-definition dictionar-
ies. Methods of information in medicine, 41(5):426–
434.

Hong Yu, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based ap-
proach for automatically disambiguating biomedical
abbreviations. ACM Transactions on Information
Systems, 24(3):380–404.


