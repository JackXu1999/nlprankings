



















































KnowledgeNet: A Benchmark Dataset for Knowledge Base Population


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 749–758,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

749

KnowledgeNet: A Benchmark Dataset for Knowledge Base Population

Filipe Mesquita, Matteo Cannaviccio,
Jordan Schmidek, Paramita Mirza

Diffbot Technologies Corp.
Menlo Park, California

{filipe,matteo,jay,paramita}@diffbot.com

Denilson Barbosa
Department of Computing Science

University of Alberta
Edmonton, Canada

denilson@ualberta.ca

Abstract

KnowledgeNet is a benchmark dataset for the
task of automatically populating a knowledge
base (Wikidata) with facts expressed in natural
language text on the web. KnowledgeNet pro-
vides text exhaustively annotated with facts,
thus enabling the holistic end-to-end evalua-
tion of knowledge base population systems as
a whole, unlike previous benchmarks that are
more suitable for the evaluation of individ-
ual subcomponents (e.g., entity linking, rela-
tion extraction). We discuss five baseline ap-
proaches, where the best approach achieves an
F1 score of 0.50, significantly outperforming a
traditional approach by 79% (0.28). However,
our best baseline is far from reaching human
performance (0.82), indicating our dataset is
challenging. The KnowledgeNet dataset and
baselines are available at https://github.
com/diffbot/knowledge-net

1 Introduction

Knowledge Bases (KBs) are valuable resources
for developing intelligent applications, including
search, question answering, data integration, and
recommendation systems. High-quality KBs still
rely almost exclusively on human-curated struc-
tured or semi-structured data. Such a reliance on
human curation is a major obstacle to the creation
of comprehensive, always-up-to-date KBs.

KB population (KBP) is the task of automati-
cally augmenting a KB with new facts. Tradition-
ally, KBP has been tackled with datasets for in-
dividual components to be arranged in a pipeline,
typically: (1) entity discovery and linking (Ji et al.,
2017; Shen et al., 2015) and (2) relation extrac-
tion (Angeli et al., 2015; Zhang et al., 2017). En-
tity discovery and linking seeks to recognize and
disambiguate proper names in text that refer to en-
tities (e.g., people, organizations and locations) by
linking them to a reference KB. Relation extrac-

tion seeks to detect facts involving two entities (or
an entity and a literal, such as a number or date).

KnowledgeNet is a benchmark dataset for pop-
ulating a KB (Wikidata) with facts expressed in
natural language on the web. KnowledgeNet facts
are of the form (subject; property; object), where
subject and object are linked to Wikidata. For in-
stance, the dataset contains text expressing the fact
(Gennaro Basile1; RESIDENCE; Moravia2), in the
passage:

“Gennaro Basile was an Italian painter,
born in Naples but active in the German-
speaking countries. He settled at Brünn,
in Moravia, and lived about 1756...”

KnowledgeNet’s main goal is to evaluate the
overall task of KBP rather than evaluating its sub-
components in separate. We refer to this type of
evaluation as end-to-end. The dataset supports the
end-to-end evaluation of KBP systems by exhaus-
tively annotating all facts in a sentence. For in-
stance, the dataset contains all RESIDENCE facts
(two) from the sentence “He settled at Brünn, in
Moravia, and lived about 1756”. This allows our
evaluation to assess precision and recall of RESI-
DENCE facts extracted from this sentence.

A popular initiative to evaluate KBP is the
Text Analysis Conference, or TAC (Getman et al.,
2018). TAC evaluations are performed manually
and are hard to reproduce for new systems. Un-
like TAC, KnowledgeNet employs an automated
and reproducible way to evaluate KBP systems
at any time, rather than once a year. We hope a
faster evaluation cycle will accelerate the rate of
improvement for KBP.

In addition to providing an evaluation bench-
mark, KnowledgeNet’s long-term goal is to pro-
vide exhaustively annotated training data at large

1http://www.wikidata.org/wiki/Q1367602
2http://www.wikidata.org/wiki/Q43266

https://github.com/diffbot/knowledge-net
https://github.com/diffbot/knowledge-net
http://www.wikidata.org/wiki/Q1367602
http://www.wikidata.org/wiki/Q43266


750

scale. Our goal for the coming years is to annotate
100,000 facts for 100 properties. To accomplish
this goal, we propose a new framework for anno-
tating facts with high accuracy and low effort.

Contributions. Our contributions are as fol-
lows. We introduce KnowledgeNet, a benchmark
dataset for end-to-end evaluation of KBP systems
(Section 3). We propose a new framework for
exhaustive yet efficient annotation of facts (Sec-
tion 3.1). We implement five baseline approaches
that build upon state-of-the-art KBP systems (Sec-
tion 4). Finally, we present an experimental anal-
ysis of our baseline approaches, comparing their
performance to human performance (Section 5).

2 Related Work

KBP has traditionally been tackled with pipeline
systems. For instance, Stanford’s TAC 2015
winning system employs the following pipeline:
named entity recognition (NER)→ entity linking
→ relation extraction (Angeli et al., 2015). Stan-
ford’s latest TAC system continues to use the same
pipeline architecture with one additional com-
ponent: coreference resolution (Chaganty et al.,
2017).

The main shortcoming of pipeline systems is er-
ror propagation. Mistakes made by components
in the beginning of the pipeline are propagated to
the final output of the system, negatively affecting
the overall precision and recall. For instance, our
experiments show that the pipeline employed by
Stanford’s TAC 2015 winning system can achieve
a maximum recall of 0.32 in KnowledgeNet3.

End-to-end systems (Liu et al., 2018; Miwa and
Bansal, 2016) are a promising solution for ad-
dressing error propagation. However, a major
roadblock for the advancement of this line of re-
search is the lack of benchmark datasets. We hope
KnowledgeNet can help support this line of re-
search.

2.1 Datasets
TAC is a series of evaluation workshops organized
as several tracks by NIST (Getman et al., 2018).
The Cold Start track provides an end-to-end eval-
uation of KBP systems, while other tracks focus
on subtasks (e.g., entity disambiguation and link-
ing). The Cold Start track is the current standard

3Maximum recall is the recall of candidate facts, which
are used as input to the last component of the pipeline (rela-
tion extraction).

to evaluate KBP systems. To compete in this track,
participants have a limited time window to submit
the results of their KBP systems. After this win-
dow, the systems are evaluated by pooling facts
extracted by the contestants. Despite its effec-
tiveness for running a contest, this methodology
has been shown to be biased against new systems,
which are not part of the pooling (Chaganty et al.,
2017). TAC also manually evaluates a system’s
“justification”, a span of text provided as evidence
for a fact. A correct fact with an incorrect justifica-
tion is considered invalid. Therefore, reproducing
TAC’s evaluation for new systems is challenging.

We propose KnowledgeNet as an automated
and reproducible alternative to TAC’s evaluation.
Before creating KnowledgeNet, we considered us-
ing one of the datasets presented in Table 1. We
compare these datasets according to five criteria
that we consider desirable for a KBP benchmark
dataset:

• Human annotation: the dataset should be
annotated by (multiple) humans to support
accurate evaluation.

• Exhaustive annotation: for each property,
the dataset should exhaustively enumerate all
facts of that property that are expressed in the
text. Exhaustive annotation allows measuring
true precision and recall for a system.

• Text annotation: the dataset should con-
tain text spans for entities involved in a fact.
This allows evaluating whether the text ex-
presses an extracted fact (as alternative to
TAC’s manual justification assessments).

• Links to reference KB: the dataset should
contain a link to the reference KB for entities
involved in every fact (or indicate that such
an entity doesn’t exist in the KB). This allows
the evaluation to confirm that the reference
KB is being correctly populated.

• Cross-sentence facts: the dataset should
contain facts involving entities whose names
never appear in the same sentence. This is be-
cause a significant portion of facts expressed
in text require coreference resolution of en-
tity mentions spanning multiple sentences.

ACE 20054 is a popular dataset for end-to-
end relation extraction systems (Li and Ji, 2014;

4https://catalog.ldc.upenn.edu/
LDC2006T06

https://catalog.ldc.upenn.edu/LDC2006T06
https://catalog.ldc.upenn.edu/LDC2006T06


751

Dataset KnowledgeNet ACE TAC TACRED FewRel DocRED GoogleRE T-REx
Human annotation yes yes yes yes yes yes yes no
Exhaustive annotation yes yes no no no no no no
Exhaus. anno. sentences 9,000 11,000 N/A N/A N/A N/A N/A N/A
Text span annotation yes yes no yes yes yes no yes
Links to reference KB yes yes yes no yes no yes yes
Cross-sentence facts yes yes yes no no yes yes yes
Annotated facts 13,000 8,000 84,000 22,000 56,000 56,000 40,000 11M
Properties 15 18 41 41 100 96 5 353
New KB facts annotated 77% 100% 100% 100% 0% 100% 0% 0%

Table 1: A dataset comparison according to our criteria for a desirable KBP benchmark dataset. “Exhaus. anno.
sentences” shows the number of exhaustively annotated sentences. “New KB facts annotated” shows the percent-
age of annotated facts that can be found in the reference KB. Most datasets contain only facts with no links to a
reference KB (100% new facts) or contain only facts that exist in the KB (0% new facts).

Miwa and Bansal, 2016). According to our crite-
ria, ACE might seem the most complete bench-
mark dataset for end-to-end evaluation of KBP.
It exhaustively annotates every sentence of 599
documents with mentions, coreference chains and
facts for 18 properties. Also, ACE has been inde-
pendently extended with links to Wikipedia (Ben-
tivogli et al., 2010). However, a closer look at
ACE’s annotations reveals that most of them are
ill-suited for general-purpose KBs. These anno-
tations include facts about broad properties (e.g.,
part-whole, physical location) or mentions that do
not refer to named entities (e.g., “two Moroccan
men”, “women on the verge of fainting”, “African
immigrants who came ashore Thursday”). Per-
haps not coincidently, we are unaware of any work
using it for the purpose of evaluating a KBP sys-
tem.

Our annotation framework (Section 3.1) is in-
spired by ACE’s framework but tailored towards
KBP. First, we only annotate mentions that re-
fer to named entities. Second, while our anno-
tation is exhaustive, we focus on annotating sen-
tences rather than documents, eliminating the need
to annotate every fact described in the entire doc-
ument. Such a requirement creates a significant
imbalance in the number of annotations per prop-
erty. For instance, the most popular property from
ACE has 1,400 annotated facts, while the majority
of properties from ACE have less than 300 anno-
tated facts. This might explain why most relation
extraction evaluations use only 6 properties from
ACE.

Annotating every sentence with facts for all
properties is also detrimental to the incremental
nature of KnowledgeNet. Adding one property
to the dataset would require an annotator to re-
annotate every sentence in the dataset. In contrast,

our framework selects a limited set of sentences
to be annotated for a particular property. The re-
maining sentences are ignored during annotation
and evaluation. As a consequence, our annotation
framework allows incremental annotation of new
properties and is better suited for the goal of anno-
tating 100,000 facts for 100 properties.

Datasets employing non-exhaustive annotation.
Recent datasets like T-REx automatically anno-
tate facts in text as a way to produce training data
cheaply. This is performed by aligning facts in
the KB to sentences referring to them (Elsahar
et al., 2018). Other datasets go further and use hu-
man annotators to label every alignment as correct
or incorrect. These semi-supervised datasets in-
clude TACRED (Zhang et al., 2017), GoogleRE5,
FewRel (Han et al., 2018) and DocRED (Yao
et al., 2019). Annotations created in this way are
useful for training KBP systems. However, they
do not provide an exhaustive annotation of facts,
which is needed for end-to-end evaluation of KBP.
For instance, Zhang et al. (2017) train their KBP
system with TACRED, but rely on TAC to evaluate
the system.

3 KnowledgeNet Dataset

This section discusses the first release of Knowl-
edgeNet and our annotation framework. The doc-
uments in this first release are either DBpedia ab-
stracts (i.e., first paragraphs of a Wikipedia page)
or short biographical texts about a person or or-
ganization from the web. These web texts were
collected using the Diffbot Knowledge Graph6.

Table 2 presents the number of annotated facts
for each property. We chose 9,073 sentences

5https://code.google.com/archive/p/
relation-extraction-corpus/downloads

6https://www.diffbot.com/

https://code.google.com/archive/p/relation-extraction-corpus/downloads
https://code.google.com/archive/p/relation-extraction-corpus/downloads
https://www.diffbot.com/


752

Property Facts Sent. Relevant
DATE OF BIRTH (PER–DATE) 761 731 468
DATE OF DEATH (PER–DATE) 664 512 347
RESIDENCE (PER–LOC) 1,456 796 387
BIRTHPLACE (PER–LOC) 1137 936 407
NATIONALITY (PER–LOC) 639 801 396
EMPLOYEE OF (PER–ORG) 1,625 650 543
EDUCATED AT (PER–ORG) 951 463 335
POLITICAL AFF. (PER–ORG) 635 537 318
CHILD OF (PER–PER) 888 471 296
SPOUSE (PER–PER) 1,338 504 298
DATE FOUNDED (ORG–DATE) 500 543 315
HEADQUARTERS (ORG–LOC) 880 564 296
SUBSIDIARY OF (ORG–ORG) 544 481 299
FOUNDED BY (ORG–PER) 764 558 346
CEO (ORG–PER) 643 526 350
Total 13,425 9,073 5,423

Table 2: KnowledgeNet properties and their number of
annotated facts and sentences. “Relevant” indicates the
number of relevant sentences (i.e., those with one or
more annotated facts). Subjects and objects belong to
one of the following types: person, organization, loca-
tion and date.

from 4,991 documents to be exhaustively anno-
tated with facts about a particular property of in-
terest. Because our annotation is exhaustive, neg-
ative examples of facts can be automatically gen-
erated. In total, KnowledgeNet comprises 13,425
facts from 15 properties.

Holdout test set. We split the documents into
five folds in a round-robin manner, keeping the
fifth fold (20% of the dataset) as the test set. To
preserve the integrity of the results, we will release
the test set without annotations and will provide
a service through which others can evaluate their
KBP systems. In our experiments, we used folds
1-3 for training and fold 4 for development and
validation, including hyperparameter tuning.

3.1 Dataset Annotation

The dataset has been generated by multiple an-
notators using a new multi-step framework. We
conjecture our framework can help annotators pro-
duce higher quality annotations by allowing them
to focus on one small, more specific task at a time.
The annotation consists of four different steps: (1)
fetch sentences, (2) detect mentions, (3) classify
facts and (4) link entities.

Step 1: Fetch sentences. We employ two meth-
ods of choosing a sentence for annotation. The
first method leverages T-REx’s automatic align-
ments (Elsahar et al., 2018) to find sentences that
are likely to describe facts from Wikidata. The

(a) Interface to detect mentions of an entity type.

(b) Interface to classify facts.

(c) Interface to link a mention to a Wikidata entity.

Figure 1: Interface for Steps 2-4 of our framework.
Step 1 fetches sentences to be exhaustively annotated
for one property. The remaining steps guide annota-
tors to detect entity mentions, facts and links in each
sentence.



753

second method chooses sentences that contain a
keyword that might indicate the presence of a fact
for a property (e.g., “born” for DATE OF BIRTH).
We have chosen these keywords by leveraging
Wikidata’s “also known as” values for properties
as well as WordNet synonyms. By using these
keywords, we prevent the dataset to be exclusively
annotated with facts that are known in Wikidata.
In fact, only 23% of facts annotated in this release
are present in Wikidata.

For each fetched sentence, an annotator decides
whether the sentence is relevant for the property
of interest (i.e., whether this sentence describes
one or more facts for this property). Relevant sen-
tences go through steps 2 through 4; while irrel-
evant sentences are kept to be used for detecting
incorrectly extracted facts (i.e., false positives).

It is worth noting that this step might not fetch
some relevant sentences. Our framework does not
require all relevant sentences to be annotated and
does not penalize systems for extracting facts from
sentences that were not annotated.

Step 2: Detect mentions. In this step, we ask
annotators to highlight entity names (Figure 1a).
We consider only entities whose type is relevant
to the property being annotated. For instance, an
annotator will only be asked to highlight names
of people and organizations when annotating the
property FOUNDED BY. Pronouns are automati-
cally annotated with a gazetteer. To decrease the
likelihood of missing a mention, we consider the
union of mentions highlighted by two annotators
for the following step.

Step 3: Classify facts. We ask annotators to
classify a candidate fact (i.e., a pair of mentions)
in a sentence as a positive or negative example for
a property (Figure 1b). Each candidate fact is an-
notated by at least two annotators. A third anno-
tator breaks the tie when the first two annotators
disagree.

We follow ACE’s reasonable reader rule,
which states that a fact should only be annotated
when there is no reasonable interpretation of the
sentence in which the fact does not hold. In other
words, annotators are asked to only annotate facts
that are either explicitly stated in the sentence or
inferred with absolute certainty from the sentence
alone (i.e., without using external world knowl-
edge).

Step 4: Link entities. Finally, we ask annota-
tors to link every mention involved in a fact to
a single Wikidata entity. In this step, annotators
can read the entire document and resolve mentions
(e.g., pronouns) that refer to names in other sen-
tences. Every mention is annotated by at least two
annotators. When there is disagreement, we ask
other annotators to join in the process until con-
sensus is reached. In total, excluding the proper-
ties having literal objects (Table 2) we can assign
a link to both subject and object for 52% of the
facts.

Inter-annotator agreement. A total of five an-
notators have contributed to KnowledgeNet so far.
In Step 3, the initial two annotators have anno-
tated 33,165 candidate facts with 96% agreement.
They disagreed on 1,495 candidate facts, where
599 have been deemed positive by a third anno-
tator. In Step 4, the initial two annotators have an-
notated 13,453 mentions with agreement of 93%.
The remaining 7% of mentions were resolved with
additional annotators.

Timing. On average, annotating a sentence for
one property takes 3.9 minutes. This total time in-
cludes two annotators (plus additional annotators
for tiebreaking). It also includes inspecting sen-
tences that express no facts and therefore do not go
through steps 2-4 (but are included in the dataset
and are helpful for assessing false positives). The
most expensive step is Step 3 (40% of the total
time), followed by Step 4 (28%), Step 2 (22%) and
Step 1 (10%).

3.2 Limitations
Our first release is comparable to other bench-
marks in size (e.g., ACE 2005), but it is perhaps
insufficient to train data-hungry models. This is
by design. Most organizations do not have the re-
sources to produce tens of thousands of examples
for each property of interest. As we expand the
number of properties to achieve our goal of anno-
tating 100,000 facts, we expect to keep the number
of facts per property to around a thousand. In this
way, we hope to promote approaches that can learn
from multiple properties, requiring less annota-
tions per property. We also hope to promote ap-
proaches using KnowledgeNet together with semi-
supervised or unsupervised datasets for training.

Another limitation of our first release is the fo-
cus on individual sentences. Currently, our frame-
work can only annotate a fact when the subject and



754

the object are explicitly mentioned by a name or
pronoun in a sentence. Others have reported that
the majority of facts fall into this category. For
example, the authors of DocRED report that 41%
of facts require reasoning over multiple sentences
in a document (Yao et al., 2019). This indicates
that a fact’s subject and object are mentioned by
their full name in a single sentence 59% of the
time. The percentage of facts that can be anno-
tated in KnowledgeNet is significantly higher than
59%. This is because our framework can also an-
notate facts that require resolving (partial) names
and pronouns referring to full names in other sen-
tences. These facts are particularly common in our
document collection.

4 Baseline Approaches

This section presents five baseline approaches for
KBP. We evaluate these approaches and compare
their performance relative to human annotators in
Section 5.

Figure 2 illustrates the architecture shared by
our five baseline approaches. We start by splitting
a given document into sentences. For each sen-
tence, we detect entity mentions using a named
entity recognizer (NER) and a gazetteer for pro-
noun mentions and their type (e.g., person, orga-
nizations, location). We also detect coreference
chains, that is, groups of mentions within a docu-
ment that refer to the same entity. Figure 2 illus-
trates how coreference chains help disambiguate
pronouns and partial names by clustering them to-
gether with the full name of an entity. Finally, we
link these coreference chains to Wikidata entities.

Next, we produce candidate facts by consider-
ing pair of mentions from the same sentence, as il-
lustrated in Figure 2. The relation extraction com-
ponent makes the final decision on whether a can-
didate fact is expressed by the text.

4.1 Relation Extraction

Figure 2 illustrates our relation extraction model.
This model follow the literature by using a Bi-
LSTM network (Miwa and Bansal, 2016; Xu et al.,
2015; Zhou et al., 2016; Zhang et al., 2017), which
is effective in capturing long-distance dependen-
cies between words. We train a single multi-task
model for all properties using both positive exam-
ples (i.e., annotated facts) and automatically gen-
erated negative examples.

The model outputs two values for each property.

The first value represents the likelihood of the sub-
ject and object mentions (i.e., text spans) to be cor-
rect, while the second value represents the likeli-
hood of the subject and object links to be correct.
We learn individual thresholds for each value and
property. When both values are above the thresh-
old, the system outputs the fact with links. When
the first value is above the threshold and the sec-
ond value is below the threshold, we output the
fact without links.

Features. Figure 3 illustrates features encoding
syntactic and positional information, which are
concatenated to the embedding of every word.

1. Enriched NER: NER label for names (us-
ing a NER system) and pronouns (using
gazetteers for each type).

2. Mention distance: distance between each
word and the subject and object mention, fol-
lowing Zhang et al. (2017).

3. Shortest dependency path (SDP) length:
number of edges in the SDP between the
word and the subject and object.

4. SDP distance: number of edges separating
the word to the closest word in the SDP be-
tween the subject and object.

5. Coreference confidence: confidence score
of the coreference resolution system that a
word refers to the subject and object.

6. Coreference distance: distance to the clos-
est mention in the coreference chain of the
subject and object.

7. Coreference SDP length: number of edges
in the SDP between the word and the closest
mention in the subject and object chain.

8. Coreference SDP distance: number of
edges separating the word to the closest word
in the SDP between the subject and object
coreference chains.

9. KB entity types: entity types for the subject
and object from Wikidata.

10. KB properties: the property p where (sub-
ject; p; object) exists in Wikidata (when both
the subject and object have links).



755

+

nlp & entity linking

input document

generate candidates

She frequently collaborates with her husband Christopher Nolan

She frequently collaborates with her husband Christopher Nolan

She frequently collaborates with her husband Christopher Nolan

She frequently collaborates with her husband Christopher Nolan

Bi LSTM

sigmoid

Emma Thomas (born 9 December 1971) is a 
British film producer, known for co-producing films 
such as The Prestige (2006) and Interstellar (2014). 
She frequently collaborates with her husband 
Christopher Nolan.

She frequently collaborates with her husband Christopher Nolan.

Emma Thomas (born 9 December 1971) is a British film producer, 
known for co-producing films such as The Prestige (2006) and 
Interstellar (2014). 

Emma Thomas
(Q242951)

United Kingdom 
(Q145)

Christopher Nolan
(Q25191)

FCL

spouse (P26)

human (Q5)

instance of (P31)

human (Q5)

instance of (P31)

human (Q5) human (Q5)

spouse (P26)

Wikidata

candidate pairs
of mentions 

with links

processed 
sentences

0.4
CH

ILD
_O

F

SP
OU

SE

CE
O

NA
TIO

NA
LIT

Y

FO
UN

DE
D_

BY

0.4 0.70.9 0.10.2 0.30.2 0.30.4…

… 

Q242951

Q25191Q242951

Q242951

Q242951

Q242951 Q25191 Q25191

Q25191 Q25191

Q25191

Figure 2: The architecture of our baseline approaches, illustrated with an example. Red arrows and boxes represent
coreference chains and blue arrows represent links to Wikidata. The subject and object of candidate facts are
highlighted in bold (blue) and italics (orange), respectively.

She frequently collaborates with her husband Christopher Nolan

1 B-PER B-PER I-PERB-PER

input sequence (+ dependecies)

0 -6 1 -5 2 -4 3 -3 4 -2 5 -1 6 0 7 0

3 3 2 1 0 00 0

2

8

0 4 2 4 1 3 2 2 4 2 3 1 5 0 4 0

0 4 2 4 1 3 2 2 0 2 1 1 3 0 2 0

3

7

0 1 0 0 1 00 04
1 -1 -1 -1 -1 -1 -1 -1 0.7 -1 -1 0.2 -1 1 -1 15
0 -6 1 -5 2 -4 -1 -3 0 -2 1 -1 2 0 3 06

Figure 3: Features representing the relationships be-
tween the words. Significant relationships with the sub-
ject and object are highlighted in blue and orange, re-
spectively.

Features 9 and 10 are generated by querying
Wikidata and are relative to a single entity pair. We
concatenate those features to the Bi-LSTM output,
as illustrated in Figure 2.

4.2 Baseline Approaches

We propose five baselines obtained by improving
the candidate generation and relation extraction
components.

Baseline 1. Our first baseline is a standard
pipeline approach inspired by the TAC 2015 Cold
Start winning system (Angeli et al., 2015). It gen-
erates candidate mentions by using NER and the
pronoun gazetteers. For mentions of the correct
type (e.g., person for the property SPOUSE), the sys-
tem then links these mentions using an entity link-
ing system. The relation extraction component
uses features 1-4.

Baseline 2. Our second baseline adds corefer-
ence resolution. This baseline is inspired by Stan-
ford’s TAC 2017 system (Chaganty et al., 2017).
We leverage coreference chains to both increase
the number of candidate mentions linked to KB
entities (e.g., pronouns) as well as to introduce ad-
ditional features. This model uses features 1-8.

Baseline 3. Our third baseline adds features 9
and 10 to the relation extraction model. These fea-
tures leverage Wikidata information for the linked
entities, such as entity types and known facts.

Baseline 4. Our fourth baseline seeks to de-
crease error propagation by allowing more candi-
date facts to be evaluated by the relation extraction
component. This is done in two ways. First, Base-
line 4 uses all mentions regardless of their NER
type when creating candidate facts. Second, this
baseline adds a candidate link to mentions that had
no candidate link in Baseline 1-3 (due to incor-
rect coreference chains). This is done by choos-
ing a link outside of the mention’s coreference
chain that maximizes a combination of entity link-
ing score and coreference resolution score.

Baseline 5. Our final baseline seeks to improve
the relation extraction component by employing
BERT’s pre-trained representations (Devlin et al.,
2018) in addition to all other features. To pro-
duce a contextual representation for every word,
we learn a linear weighted combination of BERT’s
12 layers, following Peters et al. (2019).



756

4.3 Implementation
All our baseline systems follow the same ar-
chitecture (Figure 2). We use spaCy7 for the
NLP pipeline (sentence splitting, tokenization,
POS tagging, dependency parsing, NER), Hug-
ging Face’s coreference resolution system8, and
the Diffbot Entity Linker9 for entity linking.

For relation extraction we implement a standard
BiLSTM network with two 500-dimensional hid-
den layers. We use spaCy pre-trained word em-
beddings (size 300) concatenated with additional
features illustrated in Figure 3. The output of the
BiLSTM network is concatenated with features
from Wikidata (Features 9-10).

We train all the networks using mini-batches of
128 examples and Adam optimizer (Kingma and
Ba, 2015) with a learning rate of 0.001. We use
the fourth fold of the dataset as validation set, se-
lecting the model that minimize the loss function
value. The same validation set is used to find
thresholds for the output values that maximize the
F1 score for each property.

5 Experiments

Table 3 presents the performance of our baseline
systems compared to the human performance. We
report precision (P ), recall (R) and F-score (F1):

P =
correctly extracted facts

extracted facts
,

R =
correctly extracted facts

annotated facts
,

F1 =
2 · P ·R
P +R

.

We evaluate our baseline systems from two per-
spectives. The text evaluation deems an extracted
fact correct when the text spans of the subject and
object overlap with the text spans of a ground truth
fact. The link evaluation deems an extracted fact
correct when the links of the subject and object
match the links of a ground truth fact. In the link
evaluation, we consider only facts where both the
subject and object links are present.

Human performance. To measure the human
performance on the end-to-end KBP task, one of
our annotators was asked to enumerate all facts de-
scribed in a sample of the test sentences. We re-
port the performance of our annotator in Table 3.

7https://spacy.io/
8https://huggingface.co/coref/
9https://diffbot.com/

System Text evaluation Link evaluationP R F1 P R F1
Baseline 1 0.44 0.64 0.52 0.31 0.26 0.28
Baseline 2 0.49 0.64 0.55 0.37 0.32 0.34
Baseline 3 0.47 0.66 0.55 0.35 0.37 0.36
Baseline 4 0.60 0.65 0.62 0.51 0.48 0.49
Baseline 5 0.68 0.70 0.69 0.53 0.48 0.50
Human 0.88 0.88 0.88 0.81 0.84 0.82

Table 3: The performance of our baseline approaches
is well below human performance.

A closer look at the annotator’s mistakes shows
that 32% of the mistakes are due to incorrect an-
notations in KnowledgeNet (i.e., the annotator is
actually correct). The remaining mistakes (68%)
are mostly due to the annotator entering an in-
correct fact (30%) or missing a link on a correct
fact (18%). These results show that our annota-
tion framework produces significantly better anno-
tations than individual annotators working without
our framework.

Baseline performance. Table 3 presents the
performance of our baselines. Our best baseline
(Baseline 5) significantly outperforms the standard
pipeline approach (Baseline 1) in both the text
and link evaluation. However, the performance of
Baseline 5 is well below the human performance.
The most impactful improvements over Baseline
1 are due to (a) incorporating coreference when
choosing candidate links for pronouns in Baseline
2; (b) allowing more candidate facts and links to
be classified by the relation extraction component
in Baseline 4; and (c) incorporating BERT’s pre-
trained model in Baseline 5.

Table 4 shows the “maximum recall” for each
baseline (i.e., recall of candidate facts used as in-
put for the relation extraction component). These
results indicate that error propagation significantly
limits recall. Our best baseline shows higher max-
imum recall due to coreference resolution (intro-
duced in Baseline 2) and removing the filtering of
candidate facts based on NER types (introduced
in Baseline 4). The low maximum recall for link
evaluation is mainly due to incorrect candidate
links, which can only be omitted (but not fixed)
in our baselines.

6 Conclusion

We introduce KnowledgeNet, an end-to-end
benchmark dataset for populating Wikidata with
facts expressed in natural language text on the

https://spacy.io/
https://huggingface.co/coref/
https://diffbot.com/


757

System Text evaluation Link evaluationMaximum Recall Maximum Recall
Baseline 1 0.80 0.33
Baseline 2 0.80 0.37
Baseline 3 0.80 0.37
Baseline 4 0.90 0.59
Baseline 5 0.90 0.59

Table 4: The relation extraction component’s recall is
limited by error propagation. Maximum recall is the re-
call of the candidate facts used as input for the relation
extraction component on the dev set.

web. We build KnowledgeNet using a new multi-
step framework that helps human annotators to
produce high-quality annotations efficiently. We
also introduce five baseline systems and evaluate
their performance. Our best baseline outperforms
a traditional pipeline approach by 79% (F1 score
of 0.50 vs. 0.28). Human performance is sig-
nificantly higher (0.82), indicating that Knowled-
geNet can support further research to close this
gap.

Our experiments show that the traditional
pipeline approach for KB population is notably
limited by error propagation. Performance gains
achieved by our best baseline are mainly due to
more candidates being passed along to the final
pipeline component (relation extraction), allow-
ing this component to fix errors made by previous
components. A closer inspection reveals that even
our best baseline is fairly limited by error propa-
gation and can only achieve a maximum recall of
0.59. These results indicate that end-to-end mod-
els might be a promising alternative to the tradi-
tional pipeline approach.

Acknowledgments

We would like to thank Veronica Romualdez and
Geraldine Fajardo for their diligent annotation
work. We would also like to thank Mike Tung,
Zhaochen Guo, Sameer Singh and the anonymous
reviewers for their helpful comments. This work
was supported by the Natural Sciences and En-
gineering Research Council of Canada (NSERC)
and Diffbot.

References

Gabor Angeli, Victor Zhong, Danqi Chen, Arun Te-
jasvi Chaganty, Jason Bolton, Melvin Jose Johnson
Premkumar, Panupong Pasupat, Sonal Gupta, and
Christopher D. Manning. 2015. Bootstrapped self

training for knowledge base population. In TAC.
NIST.

Luisa Bentivogli, Pamela Forner, Claudio Giu-
liano, Alessandro Marchetti, Emanuele Pianta, and
Kateryna Tymoshenko. 2010. Extending English
ACE 2005 corpus annotation with ground-truth links
to Wikipedia. In Proceedings of the 2nd Workshop
on The People’s Web Meets NLP: Collaboratively
Constructed Semantic Resources, pages 19–27, Bei-
jing, China. Coling 2010 Organizing Committee.

Arun Chaganty, Ashwin Paranjape, Percy Liang, and
Christopher D. Manning. 2017. Importance sam-
pling for unbiased on-demand evaluation of knowl-
edge base population. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1038–1048, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL-HLT.

Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon Hare, Frederique
Laforest, and Elena Simperl. 2018. T-REx: A large
scale alignment of natural language with knowledge
base triples. In Proceedings of the 11th Language
Resources and Evaluation Conference, Miyazaki,
Japan. European Language Resource Association.

Jeremy Getman, Joe Ellis, Stephanie Strassel, Zhiyi
Song, and Jennifer Tracey. 2018. Laying the
Groundwork for Knowledge Base Population: Nine
Years of Linguistic Resources for TAC KBP. In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018), Miyazaki, Japan. European Language Re-
sources Association (ELRA).

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A
large-scale supervised few-shot relation classifica-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018, pages 4803–
4809.

Heng Ji, Xiaoman Pan, Boliang Zhang, Joel Nothman,
James Mayfield, Paul McNamee, and Cash Costello.
2017. Overview of TAC-KBP2017 13 languages en-
tity discovery and linking. In Proceedings of the
2017 Text Analysis Conference, TAC 2017, Gaithers-
burg, Maryland, USA, November 13-14, 2017.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

https://www.aclweb.org/anthology/W10-3503
https://www.aclweb.org/anthology/W10-3503
https://www.aclweb.org/anthology/W10-3503
https://doi.org/10.18653/v1/D17-1109
https://doi.org/10.18653/v1/D17-1109
https://doi.org/10.18653/v1/D17-1109
https://www.aclweb.org/anthology/L18-1544
https://www.aclweb.org/anthology/L18-1544
https://www.aclweb.org/anthology/L18-1544
https://aclanthology.info/papers/D18-1514/d18-1514
https://aclanthology.info/papers/D18-1514/d18-1514
https://aclanthology.info/papers/D18-1514/d18-1514
https://tac.nist.gov/publications/2017/additional.papers/TAC2017.KBP_Entity_Discovery_and_Linking_overview.proceedings.pdf
https://tac.nist.gov/publications/2017/additional.papers/TAC2017.KBP_Entity_Discovery_and_Linking_overview.proceedings.pdf
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980


758

Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 402–412, Baltimore, Maryland. Asso-
ciation for Computational Linguistics.

Yue Liu, Tongtao Zhang, Zhicheng Liang, Heng Ji,
and Deborah L. McGuinness. 2018. Seq2rdf: An
end-to-end application for deriving triples from nat-
ural language text. In Proceedings of the ISWC
2018 Posters & Demonstrations, Industry and Blue
Sky Ideas Tracks co-located with 17th International
Semantic Web Conference (ISWC 2018), Monterey,
USA, October 8th - to - 12th, 2018.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. pages 1105–1116.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Matthew E. Peters, Sebastian Ruder, and Noah A.
Smith. 2019. To tune or not to tune? adapt-
ing pretrained representations to diverse tasks. In
Proceedings of the 4th Workshop on Representa-
tion Learning for NLP, RepL4NLP@ACL 2019, Flo-
rence, Italy, August 2, 2019., pages 7–14.

Pouya Pezeshkpour, Liyan Chen, and Sameer
Singh. 2018. Embedding multimodal relational
data for knowledge base completion. CoRR,
abs/1809.01341.

Wei Shen, Jianyong Wang, and Jiawei Han. 2015. En-
tity linking with a knowledge base: Issues, tech-
niques, and solutions. IEEE Trans. Knowl. Data
Eng., 27(2):443–460.

Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
2017. Knowledge graph embedding: A survey of
approaches and applications. IEEE Trans. Knowl.
Data Eng., 29(12):2724–2743.

Peng Xu and Denilson Barbosa. 2019. Connecting lan-
guage and knowledge with heterogeneous represen-
tations for neural relation extraction. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL-
HLT 2019, Minneapolis, Minnesota, USA, June 2-7,
2019, Volume 2 (Short Papers), page 4.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1785–1794, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,
Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,
and Maosong Sun. 2019. Docred: A large-scale
document-level relation extraction dataset. In Pro-
ceedings of the 57th Conference of the Association
for Computational Linguistics, ACL 2019, Florence,
Italy, July 28- August 2, 2019, Volume 1: Long Pa-
pers, pages 764–777.

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D. Manning. 2017. Position-
aware attention and supervised data improve slot fill-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 35–45, Copenhagen, Denmark. Association
for Computational Linguistics.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 207–212, Berlin, Germany. Association for
Computational Linguistics.

A Beyond Binary Relationships

While it would be convenient to express all facts
as (subject; property; object) triples, this is not al-
ways possible. Many facts require further annota-
tions to be sufficiently and accurately expressed
in the KB. Take for instance (United States;
head of government; Barack Obama), which only
holds true in the past.

Qualifiers allow facts to be expanded or contex-
tualized beyond what can be expressed with binary
relationships. More specifically, qualifiers can be
used to constrain the validity of a fact in time or
space, e.g., (employment fact; end time; 2017);
represent n-ary relationships, e.g., (casting fact;
character role; Tony Stark); and track provenance.

This release contains 4,518 facts annotated
with three temporal qualifiers: IS CURRENT,
START TIME and END TIME. We use one of our
baseline system to obtain facts to be annotated
with qualifiers, along with the the sentence where
each fact was found. Given a fact and a sentence,
human annotators must decide the value of a quali-
fier (true or false for IS CURRENT or a time expres-
sion for START TIME, END TIME). A third option
unclear can be chosen in the case of uncertainty.
To be included in the dataset, each fact must be
annotated by two annotators in agreement. While
preliminary experiments show promising results
for qualifier extraction, they are out-of-scope of
this work.

https://doi.org/10.3115/v1/P14-1038
https://doi.org/10.3115/v1/P14-1038
http://ceur-ws.org/Vol-2180/paper-37.pdf
http://ceur-ws.org/Vol-2180/paper-37.pdf
http://ceur-ws.org/Vol-2180/paper-37.pdf
https://doi.org/10.18653/v1/P16-1105
https://doi.org/10.18653/v1/P16-1105
https://doi.org/10.18653/v1/P16-1105
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://www.aclweb.org/anthology/W19-4302/
https://www.aclweb.org/anthology/W19-4302/
http://arxiv.org/abs/1809.01341
http://arxiv.org/abs/1809.01341
http://dblp.uni-trier.de/db/journals/tkde/tkde27.html#ShenWH15
http://dblp.uni-trier.de/db/journals/tkde/tkde27.html#ShenWH15
http://dblp.uni-trier.de/db/journals/tkde/tkde27.html#ShenWH15
https://doi.org/10.1109/TKDE.2017.2754499
https://doi.org/10.1109/TKDE.2017.2754499
https://doi.org/10.18653/v1/D15-1206
https://doi.org/10.18653/v1/D15-1206
https://doi.org/10.18653/v1/D15-1206
https://www.aclweb.org/anthology/P19-1074/
https://www.aclweb.org/anthology/P19-1074/
https://doi.org/10.18653/v1/D17-1004
https://doi.org/10.18653/v1/D17-1004
https://doi.org/10.18653/v1/D17-1004
https://doi.org/10.18653/v1/P16-2034
https://doi.org/10.18653/v1/P16-2034
https://doi.org/10.18653/v1/P16-2034

