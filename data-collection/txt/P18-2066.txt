




































Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 414–419
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

414

Document Embedding Enhanced Event Detection with Hierarchical and
Supervised Attention

Yue Zhao, Xiaolong Jin, Yuanzhuo Wang, Xueqi Cheng
CAS Key Laboratory of Network Data Science and Technology,

Institute of Computing Technology, Chinese Academy of Science (CAS);
School of Computer and Control Engineering, The University of CAS

zhaoyue@software.ict.ac.cn;
{jinxiaolong, wangyuanzhuo, cxq}@ict.ac.cn

Abstract

Document-level information is very im-
portant for event detection even at sen-
tence level. In this paper, we propose
a novel Document Embedding Enhanced
Bi-RNN model, called DEEB-RNN, to de-
tect events in sentences. This model first
learns event detection oriented embed-
dings of documents through a hierarchi-
cal and supervised attention based RNN,
which pays word-level attention to event
triggers and sentence-level attention to
those sentences containing events. It then
uses the learned document embedding to
enhance another bidirectional RNN model
to identify event triggers and their types
in sentences. Through experiments on
the ACE-2005 dataset, we demonstrate the
effectiveness and merits of the proposed
DEEB-RNN model via comparison with
state-of-the-art methods.

1 Introduction

Event Detection (ED) is an important subtask of
event extraction. It extracts event triggers from in-
dividual sentences and further identifies the type
of the corresponding events. For instance, accord-
ing to the ACE-2005 annotation guideline, in the
sentence “Jane and John are married”, an ED sys-
tem should be able to identify the word “married”
as a trigger of the event “Marry”. However, it may
be difficult to identify events from isolated sen-
tences, because the same event trigger might rep-
resent different event types in different contexts.

Existing ED methods can mainly be categorized
into two classes, namely, feature-based methods
(e.g., (McClosky et al., 2011; Hong et al., 2011;
Li et al., 2014)) and representation-based methods
(e.g., (Nguyen and Grishman, 2015; Chen et al.,

2015; Liu et al., 2016a; Chen et al., 2017)). The
former mainly rely on a set of hand-designed fea-
tures, while the latter employ distributed repre-
sentation to capture meaningful semantic informa-
tion. In general, most of these existing methods
mainly exploit sentence-level contextual informa-
tion. However, document-level information is also
important for ED, because the sentences in the
same document, although they may contain differ-
ent types of events, are often correlated with re-
spect to the theme of the document. For example,
there are the following sentences in ACE-2005:

... I knew it was time to leave. Isn’t that a
great argument for term limits? ...
If we only examine the first sentence, it is hard to
determine whether the trigger “leave” indicates a
“Transport” event meaning that he wants to leave
the current place, or an “End-Position” event in-
dicating that he will stop working for his current
organization. However, if we can capture the con-
textual information of this sentence, it is more
confident for us to label “leave” as the trigger of
an “End-Position” event. Upon such observation,
there have been some feature-based studies (Ji
and Grishman, 2008; Liao and Grishman, 2010;
Huang and Riloff, 2012) that construct rules to
capture document-level information for improving
sentence-level ED. However, they suffer from two
major limitations. First, the features used therein
often need to be manually designed and may in-
volve error propagation due to natural language
processing; Second, they discover inter-event in-
formation at document level by constructing infer-
ence rules, which is time-consuming and is hard to
make the rule set as complete as possible. Besides,
a representation-based study has been presented
in (Duan et al., 2017), which employs the PV-DM
model to train document embeddings and further
uses it in a RNN-based event classifier. How-
ever, as being limited by the unsupervised training



415

Figure 1: The schematic diagram of the DEEB-RNN model for ED at sentence level.

process, the document-level representation cannot
specifically capture event-related information.

In this paper, we propose a novel Document
Embedding Enhanced Bi-RNN model, called
DEEB-RNN, for ED at sentence level. This model
first learns ED oriented embeddings of documents
through a hierarchical and supervised attention
based bidirectional RNN, which pays word-level
attention to event triggers and sentence-level at-
tention to those sentences containing events. It
then uses the learned document embeddings to fa-
cilitate another bidirectional RNN model to iden-
tify event triggers and their types in individual
sentences. This learning process is guided by a
general loss function where the loss correspond-
ing to attention at both word and sentence levels
and that of event type identification are integrated.
It should be mentioned that although the atten-
tion mechanism has recently been applied effec-
tively in various tasks, including machine transla-
tion (Zhang et al., 2017), question answering (Hao
et al., 2017), document summarization (Tan et al.,
2017), etc., this is the first study, to the best of
our knowledge, which adopts a hierarchical and
supervised attention mechanism to learn ED ori-
ented embeddings of documents.

We evaluate the developed DEEB-RNN model
on the benchmark dataset, ACE-2005, and sys-
tematically investigate the impacts of differ-
ent supervised attention strategies on its perfor-
mance. Experimental results show that the DEEB-
RNN model outperforms both feature-based and

representation-based state-of-the-art methods in
terms of recall and F1-measure.

2 The Proposed Model

We formalize ED as a multi-class classification
problem. Given a sentence, we treat every word
in it as a trigger candidate, and classify each can-
didate to a certain event type. In the ACE-2005
dataset, there are 8 event types, further being di-
vided into 33 subtypes, and a “Not Applicable
(NA)” type. Without loss of generality, in this pa-
per we regard the 33 subtypes as 33 event types.
Figure 1 presents the schematic diagram of the
proposed DEEB-RNN model, which contains two
main modules:

1. The ED Oriented Document Embedding
Learning (EDODEL) module, which learns
the distributed representations of documents
from both word and sentence levels via the
well-designed hierarchical and supervised at-
tention mechanism.

2. The Document-level Enhanced Event Detec-
tor (DEED) module, which tags each trigger
candidate with an event type based on the
learned embedding of documents.

2.1 The EDODEL Module
To learn the ED oriented embedding of a docu-
ment, we apply the hierarchical and supervised at-
tention network presented in Figure 1, which con-
sists of a word-level Bi-GRU (Schuster and Pali-
wal, 2002) encoder with attention on event triggers



416

(a)

(b)

Figure 2: Examples of the gold word- and
sentence-level attention without normalization.
(a) Word-level attention. “Indicated” is a candi-
date trigger; (b) Sentence-level attention. The sen-
tences in purple contain trigger words.

and a sentence-level Bi-GRU encoder with atten-
tion on sentences with events. Given a document
with L sentences, DEEB-RNN learns its embed-
ding for detecting events in all sentences.

Word-level embeddings Given a sen-
tence si (i = 1, 2, ..., L) consisting of words
{wit|t = 1, 2, ..., T}. For each word wit, we first
concatenate its embedding wit and its entity type
embedding1 eit (Nguyen and Grishman, 2015)
as the input git of a Bi-GRU and thus obtain the
bidirectional hidden state hit:

hit = [
−−−−→
GRUw(git),

←−−−−
GRUw(git)]. (1)

We then feed hit to a perceptron with no bias to
get uit = tanh(Wwhit) as a hidden representa-
tion of hit and also obtain an attention weight
αit = u

T
itcw, which should be normalized through

a softmax function. Here, similar to that in (Yang
et al., 2016), cw is a vector representing the word-
level context of wit, which is initialized at random.
Finally, the embedding of the sentence si can be
obtained by summing up hit with their weights:

si =
T∑
t=1

αithit. (2)

To pay more attention to trigger words than other
words, we construct the gold word-level attention
signals α∗i for the sentence si, as illustrated in Fig-
ure 2a. We can then take the square error as the
general loss of the attention at word level to super-
vise the learning process:

Ew(α
∗,α) =

L∑
i=1

T∑
t=1

(α∗it − αit)2. (3)

1The words in the ACE-2005 dataset are annotated with
their entity types (annotated as “NA” if they are not an entity).

Sentence-level embeddings Given the sentence
embeddings {si|i = 1, 2, ..., L}, we first get the
hidden state qi via a Bi-GRU:

qi = [
−−−→
GRUs(si),

←−−−
GRUs(si)]. (4)

Then we feed qi to a perceptron with no bias to get
the hidden representation ti = tanh(Wsqi) and
also obtain an attention weight βi = tTi cs to be
normalized via softmax. Similarly, cs represents
the sentence-level context of si to be randomly ini-
tialized. We eventually obtain the document em-
bedding d as:

d =
L∑
i=1

βisi. (5)

We also think that the sentences containing event
should obtain more attention than other ones.
Therefore, similar to the case at word level, we
construct the gold sentence-level attention signals
β∗ for the document d, as illustrated in Figure 2b,
and further take the square error as the general loss
of the attention at sentence level to supervise the
learning process:

Es(β
∗,β) =

L∑
i=1

(β∗i − βi)2. (6)

2.2 The DEED Module
We employ another Bi-GRU encoder and a soft-
max output layer to model the ED task, which can
handle event triggers with multiple words. Specif-
ically, given a sentence sj (j = 1, 2, ..., L) in doc-
ument d, for each of its word wjt (t = 1, 2, ..., T ),
we concatenate its word embedding wjt and entity
type embedding ejt with the corresponding docu-
ment embedding d as the input rjt of the Bi-GRU
and thus obtain the hidden state fjt:

fjt = [
−−−→
GRUe(rjt),

←−−−
GRUe(rjt)]. (7)

Finally, we get the probability vector ojt with K
dimensions through a softmax layer for wjt, where
the k-th element, o(k)jt , of ojt indicates the proba-
bility of classifying wjt to the k-th event type. The
loss function, J(y,o), can thus be defined in terms
of the cross-entropy error of the real event type yjt
and the predicted probability o(k)jt as follows:

J(y,o) = −
L∑

j=1

T∑
t=1

K∑
k=1

I(yjt = k)log o
(k)
jt , (8)

where I(·) is the indicator function.



417

2.3 Joint Training of the DEEB-RNN model
In the DEEB-RNN model, the above two modules
are jointly trained. For this purpose, we define the
joint loss function in the training process upon the
losses specified for different modules as follows:

J(θ)=
∑
∀d∈ϕ

(J(y,o)+λEw(α
∗,α)+µEs(β

∗,β)),

(9)
where θ denotes, as a whole, the parameters used
in DEEB-RNN, ϕ is the training document set, and
λ and µ are hyper-parameters for striking a bal-
ance among J(y,o), Ew(α∗,α) and Es(β∗,β).

3 Experiments

3.1 Datasets and Settings
We validate the proposed model through compar-
ison with state-of-the-art methods on the ACE-
2005 dataset. In the experiments, the validation
set has 30 documents from different genres, the
test set has 40 documents and the training set con-
tains the remaining 529 documents. All the data
preprocessing and evaluation criteria follow those
in (Ghaeini et al., 2016).

Hyper-parameters are tuned on the validation
set. We set the dimension of the hidden layers cor-
responding to GRUw, GRUs, and GRUe to 300,
200, and 300, respectively, the output size of Ww
and Ws to 600 and 400, respectively, the dimen-
sion of entity type embeddings to 50, the batch
size to 25, the dropout rate to 0.5. In addition,
we utilize the pre-trained word embeddings with
300 dimensions from (Mikolov et al., 2013) for
initialization. For entity types, their embeddings
are randomly initialized. We train the model using
Stochastic Gradient Descent (SGD) over shuffled
mini-batches and using dropout (Krizhevsky et al.,
2012) for regularization.

3.2 Baseline Models
In order to validate the proposed DEEB-RNN
model through experimental comparison, we
choose the following typical models as the base-
lines.

Sentence-level is a feature-based model pro-
posed in (Hong et al., 2011), which regards entity-
type consistency as a key feature to predict event
mentions.

Joint Local is a feature-based model developed
in (Li et al., 2013), which incorporates such fea-
tures that explicitly capture the dependency among
multiple triggers and arguments.

Methods λ µ P R F1
Bi-GRU - - 66.2 72.3 69.1

DEEB-RNN 0 0 69.3 75.2 72.1
DEEB-RNN1 1 0 70.9 76.7 73.7
DEEB-RNN2 0 1 72.3 74.5 73.4
DEEB-RNN3 1 1 72.3 75.8 74.0

Table 1: Experimental results with different atten-
tion strategies.

JRNN is a representation-based model pro-
posed in (Nguyen et al., 2016), which exploits the
inter-dependency between event triggers and argu-
ment roles via discrete structures.

Skip-CNN is a representation-based model pre-
sented in (Nguyen and Grishman, 2016), which
proposes a novel convolution to exploit non-
consecutive k-grams for event detection.

ANN-S2 is a representation-based model devel-
oped in (Liu et al., 2017), which explicitly exploits
argument information for event detection via su-
pervised attention mechanisms.

Cross-event is a feature-based model proposed
in (Liao and Grishman, 2010), which learns rela-
tions among event types from training corpus and
futher helps predict the occurrence of events.

PSL is a feature-based model developed in (Liu
et al., 2016b), which encods global information
such as event-event association in the form of logic
using the probabilistic soft logic model.

DLRNN is a representation-based model pro-
posed in (Duan et al., 2017), which automatically
extracts cross-sentence clues to improve sentence-
level event detection.

3.3 Impacts of Different Attention Strategies

In this section, we conduct experiments on the
ACE-2005 dataset to demonstrate the effective-
ness of different attention strategies.

Bi-GRU is the basic ED model, which does not
employ document-level embeddings.

DEEB-RNN uses the document embeddings
and computes attentions without supervision, in
which hyper-parameters λ and µ are set to 0.

DEEB-RNN1/2/3 means they uses the gold at-
tention signals as supervision information. Specif-
ically, DEEB-RNN1 uses only the gold word-level
attention signal (λ = 1 and µ = 0), DEEB-RNN2
uses only the gold sentence-level attention signal
(λ = 0 and µ = 1), whilst DEEB-RNN3 employs
the gold attention signals at both word and sen-



418

Methods P R F1
Sentence-level (2011) 67.6 53.5 59.7

Joint Local (2013) 73.7 59.3 65.7
JRNN (2016) 66.0 73.0 69.3

Skip-CNN (2016) N/A N/A 71.3
ANN-S2 (2017) 78.0 66.3 71.7

Cross-event (2010)† 68.7 68.9 68.8
PSL (2016)† 75.3 64.4 69.4

DLRNN (2017)† 77.2 64.9 70.5
DEEB-RNN1† 70.9 76.7 73.7
DEEB-RNN2† 72.3 74.5 73.4
DEEB-RNN3† 72.3 75.8 74.0

Table 2: Comparison between different methods.
† indicates that the corresponding ED method uses
information at both sentence and document levels.

tence levels (λ = 1 and µ = 1).
Table 1 compares these methods, where we

can observe that the methods with document em-
beddings (i.e., the last four) significantly outper-
form the pure Bi-GRU method, which suggests
that document-level information is very benefi-
cial for ED. An interesting phenomenon is that, as
compared to DEEB-RNN, DEEB-RNN2 changes
the precision-recall balance. This is because of
the following reasons. On one hand, as com-
pared to DEEB-RNN, DEEB-RNN2 uses the gold
sentence-level attention signal, indicating that it
pays special attention to the sentences containing
events with event triggers. In this way, the Bi-
RNN model for learning document embeddings
will filter out the sentences containing events but
without explicit event triggers. That means the
events detected by DEEB-RNN2 are basically the
ones with explicit event triggers. Therefore, as
compared to DEEB-RNN, the precision of DEEB-
RNN2 is improved; On the other hand, the above
strategy may result in less learning of words,
which are event triggers but do not appear in the
training dataset. Therefore, those sentences with
such event triggers cannot be detected. The recall
of DEEB-RNN2 is thus lowered, as compared to
DEEB-RNN. Moreover, DEEB-RNN3 shows the
best performance, indicating that the gold atten-
tion signals at both word and sentence levels are
useful for ED.

3.4 Performance Comparison

Table 2 presents the overall performance of all
methods on ACE-2005. We can see that dif-
ferent versions of DEEB-RNN consistently out-

perform the existing state-of-the-art methods in
terms of both recall and F1-measure, while their
precision is comparable to that of others. The
better performance of DEEB-RNN can be ex-
plained by the following reasons: (1) Compared
with feature-based methods, including Sentence-
level, Joint Local, and representation-based meth-
ods, including JRNN, Skip-CNN and ANN-S2,
our method exploits document-level information
(i.e., the ED oriented document embeddings) from
both word and sentence levels in a document by
the supervised attention mechanism, which en-
hance the ability of identifying trigger words;
(2) Compared with feature-based methods using
document-level information, such as Cross-event,
PSL, our method can automatically capture event
types in documents via a end-to-end Bi-RNN
based model without manually designed rules; (3)
Compared with representation-based methods us-
ing document-level information, such as DLRNN,
our method can learn event detection oriented em-
beddings of documents through the hierarchical
and supervised attention based Bi-RNN network.

4 Conclusions and Future Work

In this study, we proposed a hierarchical and su-
pervised attention based and document embedding
enhanced Bi-RNN method, called DEEB-RNN,
for event detection. We explored different strate-
gies to construct gold word- and sentence-level at-
tentions to focus on event information. Experi-
ments on the ACE-2005 dataset demonstrate that
DEEB-RNN achieves better performance as com-
pared to the state-of-the-art methods in terms of
both recall and F1-measure. In this paper, we can
strike a balance between sentence and document
embeddings by adjusting their dimensions. In the
future, we may improve the DEEB-RNN model to
automatically determine the weights of sentence
and document embeddings.

Acknowledgments

This work is supported by National Key Re-
search and Development Program of China under
grants 2016YFB1000902 and 2017YFC0820404,
and National Natural Science Foundation of China
under grants 61772501, 61572473, 61572469, and
91646120. We are grateful to Dr. Liu Kang of the
Institute of Automation, Chinese Academy of Sci-
ences for very helpful discussion on event detec-
tion.



419

References
Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and

Jun Zhao. 2017. Automatically labeled data genera-
tion for large scale event extraction. In Association
for Computational Linguistics, pages 409–419.

Yubo Chen, Liheng Xu, Kang Liu, daojian zeng, and
jun zhao. 2015. Event extraction via dynamic multi-
pooling convolutional neural networks. In Associa-
tion for Computational Linguistics, pages 167–176.

Shaoyang Duan, Ruifang He, and Wenli Zhao. 2017.
Exploiting document level information to improve
event detection via recurrent neural networks. In
Proceedings of the Eighth International Joint Con-
ference on Natural Language Processing - Volume
1), pages 352–361.

Reza Ghaeini, Xiaoli Fern, Liang Huang, and Prasad
Tadepalli. 2016. Event nugget detection with
forward-backward recurrent neural networks. In As-
sociation for Computational Linguistics, pages 369–
373.

Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He,
Zhanyi Liu, Hua Wu, and Jun Zhao. 2017. An end-
to-end model for question answering over knowl-
edge base with cross-attention combining global
knowledge. In Association for Computational Lin-
guistics, pages 221–231.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Association for Computational Linguistics, pages
1127–1136.

Ruihong Huang and Ellen Riloff. 2012. Modeling tex-
tual cohesion for event extraction. In AAAI, pages
1664–1670.

Heng Ji and Ralph Grishman. 2008. Refining event
extraction through cross-document inference. In As-
sociation for Computational Linguistics, pages 254–
262.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Proceedings of the
25th International Conference on Neural Informa-
tion Processing Systems - Volume 1, pages 1097–
1105.

Qi Li, Heng Ji, Yu Hong, and Sujian Li. 2014. Con-
structing information networks using one single
model. In Empirical Methods in Natural Language
Processing, pages 1846–1851.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Association for Computational Linguistics,
pages 73–82.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event

extraction. In Association for Computational Lin-
guistics, pages 789–797.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and Jun
Zhao. 2016a. Leveraging framenet to improve auto-
matic event detection. In Association for Computa-
tional Linguistics, pages 2134–2143.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In
Association for Computational Linguistics, pages
1789–1798.

Shulin Liu, Kang Liu, Shizhu He, and Jun Zhao. 2016b.
A probabilistic soft logic based approach to exploit-
ing latent and global information in event classifica-
tion. In Proceedings of the Thirtieth AAAI Confer-
ence on Artificial Intelligence, pages 2993–2999.

David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing for bionlp 2011. In Association for Com-
putational Linguistics, pages 41–45.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, pages 3111–3119.

Thien Nguyen, Kyunghyun Cho, and Ralph Grishman.
2016. Joint event extraction via recurrent neural net-
works. In NAACL, pages 300–309.

Thien Nguyen and Ralph Grishman. 2015. Event de-
tection and domain adaptation with convolutional
neural networks. In IJCNLP, pages 365–371.

Thien Nguyen and Ralph Grishman. 2016. Modeling
skip-grams for event detection with convolutional
neural networks. In Empirical Methods in Natural
Language Processing, pages 886–891.

M. Schuster and K.K. Paliwal. 2002. Bidirectional re-
current neural networks. IEEE Transactions on Sig-
nal Processing, 45:2673–2681.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017.
Abstractive document summarization with a graph-
based attentional neural model. In Association for
Computational Linguistics, pages 1171–1181.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
NAACL, pages 1480–1489.

Jinchao Zhang, Mingxuan Wang, Qun Liu, and Jie
Zhou. 2017. Incorporating word reordering knowl-
edge into attention-based neural machine transla-
tion. In Association for Computational Linguistics,
pages 1524–1534.


