











































Jack the Reader – A Machine Reading Framework


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 25–30
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

25

Jack the Reader – A Machine Reading Framework
Dirk Weissenborn1, Pasquale Minervini2, Tim Dettmers3, Isabelle Augenstein4,

Johannes Welbl2, Tim Rocktäschel5, Matko Bošnjak2, Jeff Mitchell2,
Thomas Demeester6, Pontus Stenetorp2, Sebastian Riedel2

1German Research Center for Artificial Intelligence (DFKI), Germany
2University College London, United Kingdom

3Università della Svizzera italiana, Switzerland
4University of Copenhagen, Denmark

5University of Oxford, United Kingdom
6Ghent University - imec, Ghent, Belgium

Abstract

Many Machine Reading and Natural Lan-
guage Understanding tasks require reading
supporting text in order to answer ques-
tions. For example, in Question Answer-
ing, the supporting text can be newswire
or Wikipedia articles; in Natural Language
Inference, premises can be seen as the sup-
porting text and hypotheses as questions.
Providing a set of useful primitives operat-
ing in a single framework of related tasks
would allow for expressive modelling, and
easier model comparison and replication.
To that end, we present Jack the Reader
(JACK), a framework for Machine Read-
ing that allows for quick model prototyp-
ing by component reuse, evaluation of new
models on existing datasets as well as in-
tegrating new datasets and applying them
on a growing set of implemented baseline
models. JACK is currently supporting (but
not limited to) three tasks: Question An-
swering, Natural Language Inference, and
Link Prediction. It is developed with the
aim of increasing research efficiency and
code reuse.

1 Introduction

Automated reading and understanding of textual
and symbolic input, to a degree that enables ques-
tion answering, is at the core of Machine Read-
ing (MR). A core insight facilitating the develop-
ment of MR models is that most of these tasks can
be cast as an instance of the Question Answering
(QA) task: an input can be cast in terms of ques-
tion, support documents and answer candidates,
and an output in terms of answers. For instance,
in case of Natural Language Inference (NLI), we
can view the hypothesis as a multiple choice ques-

tion about the underlying premise (support) with
predefined set of specific answer candidates (en-
tailment, contradiction, neutral). Link Prediction
(LP) – a task which requires predicting the truth
value about facts represented as (subject, predi-
cate, object)-triples – can be conceived of as an in-
stance of QA (see Section 4 for more details). By
unifying these tasks into a single framework, we
can facilitate the design and construction of multi-
component MR pipelines.

There are many successful frameworks such as
STANFORD CORENLP (Manning et al., 2014),
NLTK (Bird et al., 2009), and SPACY1 for NLP,
LUCENE2 and SOLR3 for Information Retrieval,
and SCIKIT-LEARN4, PYTORCH5 and TENSOR-
FLOW (Abadi et al., 2015) for general Machine
Learning (ML) with a special focus on Deep
Learning (DL), among others. All of these frame-
works touch upon several aspects of Machine
Reading, but none of them offers dedicated sup-
port for modern MR pipelines. Pre-processing and
transforming MR datasets into a format that is us-
able by a MR model as well as implementing com-
mon architecture building blocks all require sub-
stantial effort which is not specifically handled by
any of the aforementioned solutions. This is due to
the fact that they serve a different, typically much
broader purpose.

In this paper, we introduce Jack the Reader
(JACK), a reusable framework for MR. It allows
for the easy integration of novel tasks and datasets
by exposing a set of high-level primitives and a
common data format. For supported tasks it is
straight-forward to develop new models without
worrying about the cumbersome implementation

1https://spacy.io
2https://lucene.apache.org
3http://lucene.apache.org/solr/
4http://scikit-learn.org
5http://pytorch.org/

https://spacy.io
https://lucene.apache.org
http://lucene.apache.org/solr/
http://scikit-learn.org
http://pytorch.org/


26

of training, evaluation, pre- and post-processing
routines. Declarative model definitions make the
development of QA and NLI models using com-
mon building blocks effortless. JACK covers a
large variety of datasets, implementations and pre-
trained models on three distinct MR tasks and sup-
ports two ML backends, namely PYTORCH and
TENSORFLOW. Furthermore, it is easy to train,
deploy, and interact with MR models, which we
refer to as readers.

2 Related Work

Machine Reading requires a tight integration of
Natural Language Processing and Machine Learn-
ing models. General NLP frameworks include
CORENLP (Manning et al., 2014), NLTK (Bird
et al., 2009), OPENNLP6 and SPACY. All these
frameworks offer pre-built models for standard
NLP preprocessing tasks, such as tokenisation,
sentence splitting, named entity recognition and
parsing.

GATE (Cunningham et al., 2002) and
UIMA (Ferrucci and Lally, 2004) are toolk-
its that allow quick assembly of baseline NLP
pipelines, and visualisation and annotation via
a Graphical User Interface. GATE can utilise
NLTK and CORENLP models and additionally
enable development of rule-based methods using
a dedicated pattern language. UIMA offers a
text analysis pipeline which, unlike GATE, also
includes retrieving information, but does not offer
its own rule-based language. It is further worth
mentioning the Information Retrieval frameworks
APACHE LUCENE and APACHE SOLR which
can be used for building simple, keyword-based
question answering systems, but offer no ML
support.

Multiple general machine learning frame-
works, such as SCIKIT-LEARN (Pedregosa et al.,
2011), PYTORCH, THEANO (Theano Develop-
ment Team, 2016) and TENSORFLOW (Abadi
et al., 2015), among others, enable quick proto-
typing and deployment of ML models. However,
unlike JACK, they do not offer a simple framework
for defining and evaluating MR models.

The framework closest in objectives to JACK
is ALLENNLP (Gardner et al., 2017), which is
a research-focused open-source NLP library built
on PYTORCH. It provides the basic low-level
components common to many systems in addition

6https://opennlp.apache.org

JTREADER
• Save, Load
• Setup, Train

INPUT
• Vocabulary building
• Embeddings
• Batching
• Data to tensors

MODEL
• TensorFlow/PyTorch

OUTPUT
• Human-readable output

Support

Question

Candidates

Answer(s)

Query

Evidence

Response

Figure 1: Our core abstraction, the JTREADER.
On the left, the responsibilities covered by the IN-
PUT, MODEL and OUTPUT modules that compose
a JTREADER instance. On the right, the data for-
mat that is used to interact with a JTREADER (dot-
ted lines indicate that the component is optional).

to pre-assembled models for standard NLP tasks,
such as coreference resolution, constituency pars-
ing, named entity recognition, question answer-
ing and textual entailment. In comparison with
ALLENNLP, JACK supports both TENSORFLOW
and PYTORCH. Furthermore, JACK can also learn
from Knowledge Graphs (discussed in Section 4),
while ALLENNLP focuses on textual inputs. Fi-
nally, JACK is structured following a modular ar-
chitecture, composed by input-, model-, and out-
put modules, facilitating code reuse and the inclu-
sion and prototyping of new methods.

3 Overview

In Figure 1 we give a high-level overview of
our core abstraction, the JTREADER. It is a
task-agnostic wrapper around three typically task-
dependent modules, namely the input, model and
output modules. Besides serving as a container
for modules, a JTREADER provides convenience
functionality for interaction, training and serialisa-
tion. The underlying modularity is therefore well
hidden from the user which facilitates the applica-
tion of trained models.

3.1 Modules and Their Usage

Our abstract modules have the following high-
level responsibilities:

• INPUT MODULES: Pre-processing that trans-
forms a text-based input to tensors.

• MODEL MODULES: Implementation of the
actual end-to-end MR model.

• OUTPUT MODULES: Converting predictions
into human readable answers.

https://opennlp.apache.org


27

The main design for building models in JACK
revolves around functional interfaces between the
three main modules: the input-, model-, and out-
put module. Each module can be viewed as a thin
wrapper around a (set of) function(s) that addi-
tionally provides explicit signatures in the form of
tensor ports which can be understood as named
placeholders for tensors.

The use of explicit signatures helps validate
whether modules are correctly implemented and
invoked, and to ensure correct behaviour as well as
compatibility between modules. Finally, by imple-
menting modules as classes and their interaction
via a simple functional interface, JACK allows for
the exploitation of benefits stemming from the use
of object oriented programming, while retaining
the flexibility offered by the functional program-
ming paradigm when combining modules.

Given a list of training instances, correspond-
ing to question-answer pairs, a input module is re-
sponsible for converting such instances into ten-
sors. Each produced tensor is associated with a
pre-defined tensor port – a named placeholder for
a tensor – which can in turn be used in later mod-
ules to retrieve the actual tensor. This step typ-
ically involves some shallow forms of linguistic
pre-processing such as tokenisation, building vo-
cabularies, etc. The model module runs the end-
to-end MR model on the now tensorised input and
computes a new mapping of output tensor ports to
newly computed tensors. Finally, the joint tensor
mappings of the input- and model module serve
as input to the output module which produces a
human-readable answer. More in-depth documen-
tation can be found on the project website.

3.2 Distinguishing Features

Module Reusability. Our shallow modularisa-
tion of readers into input-, model- and output mod-
ules has the advantage that they can be reused
easily. Most of nowadays state-of-the-art MR
models require the exact same kind of input pre-
processing and produce output of the same form.
Therefore, existing input- and output modules that
are responsible for pre- and post-processing can be
reused in most cases, which enables researchers
to focus on prototyping and implementing new
models. Although we acknowledge that most of
the pre-processing can easily be performed by
third-party libraries such as CORENLP, NLTK
or SPACY, we argue that additional functional-

ity, such as building and controlling vocabular-
ies, padding, batching, etc., and connecting the
pre-processed output with the actual model im-
plementation pose time intensive implementation
challenges. These can be avoided when work-
ing with one of our currently supported tasks –
Question Answering, Natural Language Inference,
or Link Prediction in Knowledge Graphs. Note
that modules are typically task specific and not
shared directly between tasks. However, utilities
like the pre-processing functions mentioned above
and model building blocks can readily be reused
even between tasks.

Supported ML Backends. By decoupling mod-
elling from pre- and post-processing we can easily
switch between backends for model implementa-
tions. At the time of writing, JACK offers support
for both TENSORFLOW and PYTORCH. This al-
lows practitioners to use their preferred library for
implementing new MR models and allows for the
integration of more back-ends in the future.

Declarative Model Definition. Implementing
different kinds of MR models can be repetitive,
tedious, and error-prone. Most neural architec-
tures are built using a finite set of basic building
blocks for encoding sequences, and realising inter-
action between sequences (e.g. via attention mech-
anisms). For such a reason, JACK allows to de-
scribe these models at a high level, as a composi-
tion of simpler building blocks 7, leaving concrete
implementation details to the framework.

The advantage of using such an approach is
that is very easy to change, adapt or even cre-
ate new models without knowing any implemen-
tation specifics of JACK or its underlying frame-
works, such as TENSORFLOW and PYTORCH.
This solution also offers another important advan-
tage: it allows for easy experimentation of auto-
mated architecture search and optimisation (Au-
toML). JACK already enables the definition of new
models purely within configuration files without
writing any source code. These are interpreted by
JACK and support a (growing) set of pre-defined
building blocks. In fact, many models for differ-
ent tasks in JACK are realised by high-level archi-
tecture descriptions. An example of an high-level
architecture definition in JACK is available in Ap-
pendix A.

7For instance, see https://github.com/uclmr/
jack/blob/master/conf/nli/esim.yaml

https://github.com/uclmr/jack/blob/master/conf/nli/esim.yaml
https://github.com/uclmr/jack/blob/master/conf/nli/esim.yaml


28

Dataset Coverage. JACK allows parsing a large
number of datasets for QA, NLI, and Link Pre-
diction. The supported QA datasets include
SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi
et al., 2017), NewsQA (Trischler et al., 2017),
and QAngaroo (Welbl et al., 2017). The sup-
ported NLI datasets include SNLI (Bowman et al.,
2015), and MultiNLI (Williams et al., 2018).
The supported Link Prediction datasets include
WN18 (Bordes et al., 2013), WN18RR (Dettmers
et al., 2018), and FB15k-237 (Toutanova and
Chen, 2015).

Pre-trained Models. JACK offers several pre-
trained models. For QA, these include FastQA,
BiDAF, and JackQA trained on SQuAD and Triv-
iaQA. For NLI, these include DAM and ESIM
trained on SNLI and MultiNLI. For LP, these in-
clude DistMult and ComplEx trained on WN18,
WN18RR and FB15k-237.

4 Supported MR Tasks

Most end-user MR tasks can be cast as an instance
of question answering. The input to a typical ques-
tion answering setting consists of a question, sup-
porting texts and answers during training. In the
following we show how JACK is used to model our
currently supported MR tasks.

Ready to use implementations for these tasks
exist which allows for rapid prototyping. Re-
searchers interested in developing new models can
define their architecture in TENSORFLOW or PY-
TORCH, and reuse existing of input- and output
modules. New datasets can be tested quickly on a
set of implemented baseline models after convert-
ing them to one of our supported formats.

Extractive Question Answering. JACK sup-
ports the task of Extractive Question Answering
(EQA), which requires a model to extract an an-
swer for a question in the form of an answer span
comprising a document id, token start and -end
from a given set of supporting documents. This
task is a natural fit for our internal data format,
and is thus very easy to represent with JACK.

Natural Language Inference. Another popu-
lar MR task is Natural Language Inference, also
known as Recognising Textual Entailment (RTE).
The task is to predict whether a hypothesis is en-
tailed by, contradicted by, or neutral with respect
to a given premise. In JACK, NLI is viewed as

an instance of multiple-choice Question Answer-
ing problem, by casting the hypothesis as the ques-
tion, and the premise as the support. The answer
candidates to this question are the three possible
outcomes or classes – namely entails, contradicts
or neutral.

Link Prediction. A Knowledge Graph is a set of
(s, p, o) triples, where s, o denote the subject and
object of the triple, and p denotes its predicate:
each (s, p, o) triple denotes a fact, represented as
a relationship of type p between entities s and
o, such as: (LONDON, CAPITALOF, UK). Real-
world Knowledge Graphs, such as Freebase (Bol-
lacker et al., 2007), are largely incomplete: the
Link Prediction task consists in identifying miss-
ing (s, p, o) triples that are likely to encode true
facts (Nickel et al., 2016).

JACK also supports Link Prediction, because
existing LP models can be cast as multiple-choice
Question Answering models, where the question
is composed of three words – a subject s, a predi-
cate p, and an object o. The answer candidates to
these questions are true and false.

In its original formulation of the Link Predic-
tion task, the support is left empty. However, JACK
facilitates enriching the questions with additional
support – consisting, for instance, of the neigh-
bourhood of the entities involved in the question,
or sentences from a text corpus that include the
entities appearing in the triple in question. Such a
setup can be interpreted as an instance of NLI, and
existing models not originally designed for solv-
ing Link Prediction problems can be trained ef-
fortlessly.

5 Experiments

Experimental setup and results for different mod-
els on the three above-mentioned MR tasks are
reported in this section. Note that our re-
implementations or training configurations may
not be entirely faithful.We performed slight mod-
ifications to original setups where we found this
to perform better in our experiments, as indicated
in the respective task subsections. However, our
results still vary from the reported ones, which
we believe is due to the extensive hyper-parameter
engineering that went into the original settings,
which we did not perform. For each experiment, a
ready to use training configuration as well as pre-
trained models are part of JACK.



29

Model Original F1 JACK F1 Speed #Params

BiDAF 77.3 77.8 1.0x 2.02M
FastQA 76.3 77.4 2.2x 0.95M
JackQA – 79.6 2.0x 1.18M

Table 1: Metrics on the SQuAD development set
comparing F1 metric from the original implemen-
tation to that of JACK, number of parameters, and
relative speed of the models.

Model Original JACK

cBiLSTM (Rocktäschel et al., 2016) – 82.0
DAM (Parikh et al., 2016) 86.6 84.6
ESIM (Chen et al., 2017) 88.0 87.2

Table 2: Accuracy on the SNLI test set achieved
by cBiLSTM, DAM, and ESIM.

Question Answering. For the Question An-
swering (QA) experiments we report results for
our implementations of FastQA (Weissenborn
et al., 2017), BiDAF (Seo et al., 2016) and, in ad-
dition, our own JackQA implementations. With
JackQA we aim to provide a fast and accurate
QA model. Both BiDAF and JackQA are re-
alised using high-level architecture descriptions,
that is, their architectures are purely defined within
their respective configuration files. Results of our
models on the SQuAD (Rajpurkar et al., 2016)
development set along with additional run-time
and parameter metrics are presented in Table 1.
Apart from SQuAD, JACK supports the more re-
cent NewsQA (Trischler et al., 2017) and Trivi-
aQA (Joshi et al., 2017) datasets too.

Natural Language Inference. For NLI, we re-
port results for our implementations of condi-
tional BiLSTMs (cBiLSTM) (Rocktäschel et al.,
2016), the bidirectional version of conditional
LSTMs (Augenstein et al., 2016), the Decompos-
able Attention Model (DAM, Parikh et al., 2016)
and Enhanced LSTM (ESIM, Chen et al., 2017).
ESIM was entirely implemented as a modular NLI
model, i.e. its architecture was purely defined in a
configuration file – see Appendix A for more de-
tails. Our models or training configurations con-
tain slight modifications from the original which
we found to perform better than the original setup.
Our results are slightly differ from those reported,
since we did not always perform an exhaustive
hyper-parameter search.

Dataset Model MRR Hits@3 Hits@10

WN18 DistMult 0.822 0.914 0.936ComplEx 0.941 0.936 0.947

WN18RR DistMult 0.430 0.443 0.490ComplEx 0.440 0.461 0.510

FB15k-237 DistMult 0.241 0.263 0.419ComplEx 0.247 0.275 0.428

Table 3: Link Prediction results, measured
using the Mean Reciprocal Rank (MRR) and
Hits@10, for DistMult (Yang et al., 2015), and
ComplEx (Trouillon et al., 2016).

Link Prediction. For Link Prediction in Knowl-
edge Graphs, we report results for our implemen-
tations of DistMult (Yang et al., 2015) and Com-
plEx (Trouillon et al., 2016) on various datasets.
Results are otlined in Table 3.

6 Demo

We created three tutorial Jupyter notebooks at
https://github.com/uclmr/jack/
tree/master/notebooks to demo JACK’s
use cases. The quick start notebook shows how to
quickly set up, load and run the existing systems
for QA and NLI. The model training notebook
demonstrates training, testing, evaluating and
saving QA and NLI models programmatically.
However, normally the user will simply use the
provided training script from command line. The
model implementation notebook delves deeper
into implementing new models from scratch by
writing all modules for a custom model.

7 Conclusion

We presented Jack the Reader (JACK), a shared
framework for Machine Reading tasks that will
allow component reuse and easy model transfer
across both datasets and domains.

JACK is a new unified Machine Reading frame-
work applicable to a range of tasks, developed
with the aim of increasing researcher efficiency
and code reuse. We demonstrate the flexibility
of our framework in terms of three tasks: Ques-
tion Answering, Natural Language Inference, and
Link Prediction in Knowledge Graphs. With fur-
ther model additions and wider user adoption,
JACK will support faster and reproducible Ma-
chine Reading research, enabling a building-block
approach to model design and development.

https://github.com/uclmr/jack/tree/master/notebooks
https://github.com/uclmr/jack/tree/master/notebooks


30

References
Martín Abadi et al. 2015. TensorFlow: Large-

scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
https://www.tensorflow.org/.

Isabelle Augenstein, Tim Rocktäschel, Andreas Vla-
chos, and Kalina Bontcheva. 2016. Stance detec-
tion with bidirectional conditional encoding. In Pro-
ceedings or EMNLP. pages 876–885.

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
ing text with the natural language toolkit. " O’Reilly
Media, Inc.".

Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts.
2007. Freebase: A shared database of structured
general human knowledge. In Proceedings of AAAI.
pages 1962–1963.

Antoine Bordes, Nicolas Usunier, Alberto García-
Durán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of NIPS. pages
2787–2795.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of EMNLP. pages 632–642.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
natural language inference. In Proceedings of ACL.
pages 1657–1668.

Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an Architecture for Development of Robust HLT
applications. In Proceedings of ACL.

Tim Dettmers, Minervini Pasquale, Stenetorp Pon-
tus, and Sebastian Riedel. 2018. Convolutional 2d
knowledge graph embeddings. In Proceedings of
AAAI.

David Ferrucci and Adam Lally. 2004. Uima: an archi-
tectural approach to unstructured information pro-
cessing in the corporate research environment. Nat-
ural Language Engineering 10(3-4):327–348.

Matt Gardner et al. 2017. AllenNLP: A Deep Seman-
tic Natural Language Processing Platform. White
paper.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of ACL. pages 1601–1611.

Christopher Manning et al. 2014. The Stanford
CoreNLP Natural Language Processing Toolkit. In
Proceedings of ACL: System Demonstrations. pages
55–60.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2016. A review of relational
machine learning for knowledge graphs. In Pro-
ceedings of IEEE. volume 104, pages 11–33.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of EMNLP. pages 2249–2255.

Fabian Pedregosa et al. 2011. Scikit-learn: Machine
learning in Python. JMLR 12(Oct):2825–2830.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ Questions
for Machine Comprehension of Text. In Proceed-
ings of EMNLP. pages 2383–2392.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomas Kocisky, and Phil Blunsom. 2016.
Reasoning about Entailment with Neural Attention.
In Proceedings of ICLR.

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. In Proceedings of
ICLR.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints abs/1605.02688.

Kristina Toutanova and Danqi Chen. 2015. Observed
versus latent features for knowledge base and text
inference. CVSC workshop, ACL pages 57–66.

Adam Trischler et al. 2017. NewsQA: A machine com-
prehension dataset. Rep4NLP workshop, ACL .

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Complex
embeddings for simple link prediction. In Proceed-
ings of ICML. pages 2071–2080.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017. Making neural QA as simple as possible but
not simpler. In Proceedings of CoNLL. pages 271–
280.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2017. Constructing datasets for multi-hop
reading comprehension across documents. CoRR
abs/1710.06481.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2018. A broad-coverage challenge corpus for
sentence understanding through inference. In Pro-
ceedings of NAACL.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding Entities and
Relations for Learning and Inference in Knowledge
Bases. In Proceedings of ICLR.

https://www.tensorflow.org/
https://www.tensorflow.org/
https://www.tensorflow.org/
https://www.tensorflow.org/

