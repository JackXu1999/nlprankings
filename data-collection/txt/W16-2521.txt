



















































Evaluating word embeddings with fMRI and eye-tracking


Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 116–121,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

Evaluating word embeddings with fMRI and eye-tracking

Anders Søgaard
University of Copenhagen

soegaard@hum.ku.dk

Abstract

The workshop CfP assumes that down-
stream evaluation of word embeddings is
impractical, and that a valid evaluation
metric for pairs of word embeddings can
be found. I argue below that if so, the
only meaningful evaluation procedure is
comparison with measures of human word
processing in the wild. Such evaluation is
non-trivial, but I present a practical proce-
dure here, evaluating word embeddings as
features in a multi-dimensional regression
model predicting brain imaging or eye-
tracking word-level aggregate statistics.

What’s the meaning of embeddings? In or-
der to decide how to evaluate word embeddings,
we first need to decide what word embeddings
are supposed to encode. If we assume that word
embeddings are primarily representations of the
meaning of words, it makes sense to consult lexi-
cal semantic theories.

Here’s a very, very, very (very, . . . ) crude
characterization of lexical semantics: Researchers
disagree whether words are defined by their co-
occurrences (Firth, 1957), the contexts in which
they are used (Wittgenstein, 1953), how they
are organized in the brain (Miller and Fellbaum,
1992), or the referents they denote in the real
world (Montague, 1973). I realize this is a ridicu-
lously simplistic reduction of modern lexical se-
mantics, but I think it suffices for our discussion
of how best to evaluate word embeddings.1

Any metrics here? From (one or more of) these
theories we want to derive a valid evaluation met-
ric. In my view, a valid metric satisfies two prin-
ciples: (i) that it measures what we want to mea-
sure (adequacy), and (ii) that it cannot easily be

1See the discussion in the last paragraph.

hacked. What I mean by (i) is that we want word
embeddings to capture the meaning of words; and
by (ii), that the reason we want to play the eval-
uation game is because it isn’t obvious what the
meaning of a word is. If the meaning of a word
was given directly by its character sequence, I
would not be writing this paper, and this workshop
would not have been proposed. The question then
is, do any of the four theories above provide us
with a valid metric for the general quality of word
embeddings?

Below, I argue that none of the four theories
leave us with fully valid evaluation metrics, ex-
cept maybe COGNITIVE LEXICAL SEMANTICS.
I suggest evaluating embeddings by direct com-
parison with brain-imaging and eye-tracking data
rather than word association norms, as an alterna-
tive approach to COGNITIVE LEXICAL SEMAN-
TICS. I show that state-of-the-art embeddings cor-
relate poorly with such data, but argue that this is
nevertheless the only valid metric left on the table,
if downstream evaluation is not an option – and
that, practically, we can evaluate embeddings by
the error of a multi-dimensional regression model
predicting brain imaging or eye-tracking data us-
ing the embeddings as features.

Co-occurrence theory In CO-OCCURRENCE
THEORY, the meaning of a word is defined by
its co-occurrences with other words – e.g., the
meaning of big is given by its co-occurrence with
words such as house and small, i.e., its value in a
co-occurrence matrix. Word embeddings should
therefore predict lexical co-occurrences, which
can be evaluated in terms of perplexity or word er-
ror rate. This was how embeddings were evaluated
in the early papers, e.g., (Mikolov et al., 2010).
But note that constructing co-occurrence matri-
ces is also an integral part of standard approaches
to inducing embeddings (Levy et al., 2015). In

116



fact for any definition of a word’s company, we
can built co-occurrence matrices tailored to max-
imize our objective. The associated metrics can
thus be ”hacked” in the sense that the encodings
used for evaluation, can also be used for induction.
Just like with other intrinsic evaluation metrics in
unsupervised learning, co-occurrence-based eval-
uation easily bites its own tail. As soon as we
have defined a word’s company, the quality of the
embeddings depends solely on the quality of the
data. The evaluation strategy becomes the induc-
tion strategy, and the validity of the embeddings is
by postulate, not by evidence. In other words, the
metric can be hacked. Note that whether such a
metric is adequate (measuring meaning) remains
an open question.

Sprachspiel theory In SPRACHSPIEL THEORY,
the meaning of a word is defined by its usage,
i.e., the situations in which it is used. In Wittgen-
stein’s words, only someone who already knows
how to do something with it, can significantly ask
a name. Obviously, it is hard to parameterize con-
texts, but explicit semantic analysis (Gabrilovich
and Markovitch, 2009) presents a simple approx-
imation, e.g., thinking of Wikipedia sites as con-
texts. Learning word representations from in-
verted indexings of Wikipedia is encoding a sit-
uational lexical semantics, albeit in a somewhat
simplistic way. The meaning of big, for exam-
ple, is defined by the Wikipedia entries it occurs
in, i.e., its value in a term-document (or term-
topic or term-frame or . . . ) matrix. The ques-
tion then is: How well do our embeddings dis-
tinguish between different contexts? See earlier
work on using embeddings for document classi-
fication, for example. However, such an encod-
ing has also been proposed as an approach to in-
ducing embeddings (Søgaard et al., 2015). While
this proposal adopts a specific encoding of term-
document matrices, similar encodings can be built
for any definition of a Sprachspiel. Any such met-
ric can thus be ”hacked” or build into the model,
directly. Note, again, that whether such a metric
is adequate (measuring meaning) remains an open
question.

Cognitive lexical semantics How well does our
embeddings align with our mental representations
of words? Obviously, we do not have direct ac-
cess to our mental representations, and most re-
searchers have relied on word associations norms

instead.2 In matrix terms, COGNITIVE LEXICAL
SEMANTICS defines the meaning of a word as a
vector over vertices in an ontology or a mental lex-
icon. The hypothesis is that our mental lexicon is
organized as a undirected, colored, weighted net-
work, and the meaning of words are defined by
the edges connecting them to other words. The
meaning of big, for example, is in a synonym re-
lation with large, an antonym of small, etc. Such
networks are typically informed by word associa-
tion norms and corpus linguistic evidence. Using
Wordnets for evaluating word embeddings was re-
cently proposed by Tsvetkov et al. (2015).

However, again, Faruqui and Dyer (2015) re-
cently proposed this as a learning strategy, encod-
ing words by their occurrence in Wordnet. Us-
ing mental lexica as gold standard annotation thus
suffers from the same problem as defining the
meaning of words by their co-occurrencies or dis-
tributions over situations or documents; the de-
rived metrics can be hacked. Also, there’s a
number of problems with using Wordnets and the
like for evaluating word embeddings. The most
obvious ones are low coverage and low inter-
annotator agreement in such resources. Moreover,
as shown by Juergens (2014), some inter-annotator
disagreements are not random (errors), but reflect
different, linguistically motivated choices. There
are different ways to structure word meanings that
lead to different semantic networks. Different lex-
icographic theories suggest different ways to do
this. This means that our resources are theoret-
ically biased. After all, while psycholinguistic
priming effects and word association norms sug-
gest that semantically similar words are retrieved
faster than orthographically similar words, there is
to the best of my knowledge no bullet-proof evi-
dence that our brain does not order words alpha-
betically (or some other obscure way) in the men-
tal lexicon.

Do we have alternatives? Our limited under-
standing of the brain makes evaluating COGNI-
TIVE LEXICAL SEMANTICS non-trivial – at least
if we want to go beyond lexicographic representa-
tions of the mental lexicon. If we accept lexico-
graphic resources as approximations of the mental
lexicon, we can use these resources for training, as

2See Faruqui et al. (2016; Batchkarov et al. (2016; Chiu
et al. (2016) for critiques of using word association norms.
The problem with word association norms is inadequacy (and
statistical power): They conflate several types of similarity,
e.g., synonymy and antonymy, and they are culture-specific.

117



well as evaluation, in the same way as we do eval-
uation in other supervised learning problems. If
we don’t, we have to resort to alternatives. Below
we consider one, namely direct evaluation against
brain imaging (and eye tracking) data.

Denotational semantics At first sight, DENO-
TATIONAL SEMANTICS seems to assume discrete
word representations (sets). Obviously, however,
some words have overlapping sets of referents.
Can we evaluate our embeddings by how well
they predict such overlaps? DENOTATIONAL SE-
MANTICS, in matrix terms, defines the meaning
of a word as its distribution over a set of refer-
ents (e.g., its occurrences in Amazon product de-
scriptions). While learning embeddings of words
from their distribution over Amazon product de-
scriptions has, to the best of our knowledge, not
yet been proposed, this would be easy to do. DE-
NOTATIONAL SEMANTICS is thus very similar to
SPRACHSPIEL THEORY from an evaluation point
of view; if we fix the set of referents, e.g., Ama-
zon products, evaluation again becomes similar to
evaluation in other supervised learning problems.

Brain imaging, anyone? If we accept the
premise in the call for papers for this workshop –
that down-stream evaluation of word embeddings
is impractical and all over the map – we also ac-
cept the conclusion that we are interested in em-
beddings, not only for practical purposes, but as
models of cognitive lexical semantics. It seems
that this motivates focusing on evaluation proce-
dures such as correlation with word association
norms or evaluation against mental lexica. How-
ever, lexicographic resources are sparse and the-
oretically biased, and word association norms are
unreliable. What do we do?

If we could measure the semantic processing as-
sociated with a word in brain imaging, this would
give us a less biased access to the cognitive lex-
ical semantics of words. If we assume such data
is available, there are two possible approaches to
evaluating word embeddings against such data:

(a) Studying the correlation between distances in
word embedding space and EEG/fMRI/etc.
space; or, perhaps more robustly, the P@k
predicting nearest neighbors EEG/fMRI/etc.
using embeddings.

(b) Evaluating the squared error of a regression
model trained to associate the input word em-
beddings with EEG/fMRI/etc.

Note that we have reasons to think such met-
rics are not entirely inadequate, since we know
humans understand words when they read them.
fMRI data, for example, may contain a lot of noise
and other types of information, but semantic word
processing is bound to the contribute to the signal,
one way or the other.

At last, a few experiments I present some ini-
tial experiments doing both (a) and (b). We evalu-
ate the EW30 and SENNA embeddings (Collobert
et al., 2011) against fMRI data from Wehbe et al.
(2015), using the token-level statistics derived in
Barrett et al. (2016), and eye-tracking data from
the Dundee Corpus (Barrett and Søgaard, 2015).

My first experiment is a simple one, merely
to show how uncorrelated raw fMRI and eye-
tracking data are with state-of-the-art embeddings.
I deliberately pick a very simple prediction prob-
lem. Specifically, we randomly sample 9 words
that are shared between the cognitive gold stan-
dard data and the two sets of embeddings we wish
to evaluate. For each of the 9 words, I compare
nearest neighbors, computing P@1 for both our
embedding models.

I convert the fMRI data and the eye-tracking
data to vectors of aggregate statistics following
the suggestions in Barrett and Søgaard (2015) and
Barrett et al. (2016). Table 1 presents the nearest
neighbors (out of the 9 randomly selected words)
in the gold data, as well as the two word em-
beddings. The P@1 for both embeddings is 2/9.
If I increase the size of the candidate set to 50,
and do three random runs, scores drop to 4% and
3.3%, respectively. For comparison, the embed-
dings agree on the nearest neighbors in 9, 10, and
10 words across three random runs. On the other
hand, this is expected, since the embedding algo-
rithms have obvious similarities, while the brain
imaging data is entirely independent of the embed-
dings. If I run the same experiment on the gaze
data, using a candidate set of 50 random words,
scores are even lower (0–1/50). The P@1 agree-
ments between the fMRI data and the eye-tracking
recordings across three runs are also very low (0,
2, and 2 in 50).

If I look at the nearest neighbors across the
full dataset, manually, the picture is also blurred.
Sometimes, the brain imaging data has odd near-
est neighbors, say teachers for having, when
EW30 had giving, for example, which is intu-
itively much closer. In other cases, the gold stan-

118



(a)

0 20 40 60 80 100
0.010

0.015

0.020

0.025

0.030

0.035

0.040

ew30.curve
senna.curve

(b)

0 20 40 60 80 100
0.015

0.020

0.025

0.030

0.035

0.040

0.045

0.050

0.055

0.060

ew30.gaze.curve
senna.gaze.curve

Figure 1: Learning curve fitting state-of-the-art embeddings to token-level fMRI (a) and eye-tracking (b)
statistics (x-axis: learning iterations, y-axis: squared mean error)

Target Nearest neighbors

SENNA EW30 GOLD

rolling nervous pig pig
madly out nervous house
rise hold hold anytime
house hold pig anytime
nervous rolling rolling hold
hold house rise managed
managed hold out hold
out madly managed pig
pig rolling rolling rolling

Table 1: Nearest neighbors within a random sam-
ple of nine words. We underline the nearest neigh-
bors in SENNA and EW30 embeddings when they
agree with the fMRI gold data.

dard nearest neighbors are better than state-of-the
art, or defendable alternatives. Table 2 lists a few
examples, comparing against EW30, and whether
the gold standard makes intuitive sense (to me).

However, it is not clear, a priori, that the
embeddings should correlate perfectly with brain
imaging data. The brain may encode these signals
in some transformed way. I therefore ran the fol-
lowing experiment:

For words w in a training split, I train a
deep neural regression model to reconstruct the
fMRI/gaze vector from the input embedding,
which I evaluate by its squared error on a held-out
test split. All vectors are normalized to the (0,1)-
range, leading to squared distances in the (0,2)-
range. The training split is the first 100 words in

Target EW30 GOLD Okay?

students teachers mistake No
creep drift long No
peace death eat Maybe
tight nasty hold Maybe
squeak twisted broke Yes
admiring cursing stunned Yes
amazed delighted impressed Yes

Table 2: Examples of nearest neighbors (over full
dataset) for EW30 and fMRI embeddings. Man-
ual judgments (Okay?) reflect whether the fMRI
nearest neighbors made intuitive sense.

the common vocabulary (of the two embeddings
and the gold standard); the test split the next 100
words. Sampling from the common vocabulary is
important; comparisons across different vocabu-
laries is a known problem in the word embeddings
literature. I use SGD and a hidden layer with 100
dimensions.

I present a learning curve for the first 100 it-
erations fitting the embeddings to the fMRI data
in Figure 1a. Observe that the EW30 embeddings
give us a much better fit than the SENNA embed-
dings. Interestingly, the better fit is achieved with
fewer dimensions (30 vs. 50). This suggests that
the EW30 embeddings capture more of the differ-
ences in the brain imaging data. See the same ef-
fect with the eye-tracking data in Figure 1b.

What I am saying . . . Under the assumption
that downstream evaluation of word embeddings
is impractical, I have argued that correlating with

119



human word processing data is the only valid type
of evaluation left on the table. Since brain imaging
and eye-tracking data are very noisy signals, cor-
relating distances does not provide sufficient sta-
tistical power to compare systems. For that reason
I have proposed comparing embeddings by testing
how useful they are when trying to predict human
processing data. I have presented some prelimi-
nary experiments, evaluating state-of-the-art em-
beddings by how useful they are for predicting
brain imaging and eye-tracking data using a deep
neural regression model. The test is made avail-
able at the website:

http://cst.dk/anders/fmri-eval/

where users can upload pairs of embeddings and
obtain learning curves such as the ones above. I
believe this type of evaluation is the most mean-
ingful task-independent evaluation of word em-
beddings possible right now. Note that you can
also do nearest neighbor queries (and t-SNE visu-
alizations) with the output of such a model.

More advanced theories? Our proposal was in
part motivated by a crude simplification of lex-
ical semantics. Of course more advanced theo-
ries exist. For example, Marconi (1997) says lexi-
cal competence involves both an inferential aspect,
i.e., learning a semantic network of synonymy and
hyponymy relations, as well as a referential aspect,
which is in charge of naming and application. In
this framework, a word is defined by its edges in a
semantic network and its denotation and/or the sit-
uations in which it can be used. Technically, how-
ever, this is a simple concatenation of the vectors
described above. Again, the derived metrics are
easily hacked. In other words, if Marconi (1997) is
right, evaluation reduces to settling on the defini-
tion of the semantic network and of denotation or
language games, and finding representative data.
From a metrics point of view, any evaluation based
on such a theory would be a vicious circle.

Acknowledgments

This work was supported by ERC Starting Grant
No. 313695, as well as research grant from the
Carlsberg Foundation.

References
Maria Barrett and Anders Søgaard. 2015. Reading be-

havior predicts syntactic categories. In CoNLL.

Maria Barrett, Joachim Bingel, and Anders Søgaard.
2016. Extracting token-level signals of syntactic
processing from fmri - with an application to pos in-
duction. In ACL.

Miroslav Batchkarov, Thomas Kober, Jeremy Reffin,
Julie Weeds, and David Weir. 2016. A critique of
word similarity as a method for evaluating distribu-
tional semantic models. In RepEval.

Billy Chiu, Anna Korhonen, and Sampo Pyysalo.
2016. Intrinsic evaluation of word vectors fails to
predict extrinsic performance. In RepEval.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Manaal Faruqui and Chris Dyer. 2015. Non-
distributional word vector representations. In ACL.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems with evaluation
of word embeddings using word similarity tasks. In
RepEval.

John Firth. 1957. Papers in Linguistics 1934-1951.
Oxford University Press.

Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation for natural
language processing. Journal of Artificial Intelli-
gence Research, pages 443–498.

David Juergens. 2014. An analysis of ambiguity in
word sense annotations. In LREC.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. TACL, 3:211–225.

Diego Marconi. 1997. Lexical Competence. MIT
Press.

Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.

George Miller and Christiane Fellbaum. 1992. Seman-
tic networks of English. In Lexical and conceptual
semantics. Blackwell.

Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In Formal phi-
losophy. Yale University Press.

Anders Søgaard, Željko Agić, Héctor Martı́nez Alonso,
Barbara Plank, Bernd Bohnet, and Anders Jo-
hannsen. 2015. Inverted indexing for cross-lingual
nlp. In ACL.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation of
word vector representations by subspace alignment.
In EMNLP.

120



Leila Wehbe, Brian Murphy, Partha Talukdar, Alona
Fyshe, Aaditya Ramdas, and Tom Mitchell. 2015.
Simultaneously uncovering the patterns of brain re-
gions involved in different story reading subpro-
cesses. PLoS ONE, 10(3).

Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwell Publishing.

121


