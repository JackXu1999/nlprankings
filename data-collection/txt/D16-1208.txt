



















































Modified Dirichlet Distribution: Allowing Negative Parameters to Induce Stronger Sparsity


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1986–1991,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Modified Dirichlet Distribution: Allowing Negative Parameters to Induce
Stronger Sparsity∗

Kewei Tu
School of Information Science and Technology

ShanghaiTech University, Shanghai, China
tukw@shanghaitech.edu.cn

Abstract

The Dirichlet distribution (Dir) is one of the
most widely used prior distributions in statis-
tical approaches to natural language process-
ing. The parameters of Dir are required to be
positive, which significantly limits its strength
as a sparsity prior. In this paper, we propose
a simple modification to the Dirichlet distribu-
tion that allows the parameters to be negative.
Our modified Dirichlet distribution (mDir) not
only induces much stronger sparsity, but also
simultaneously performs smoothing. mDir is
still conjugate to the multinomial distribution,
which simplifies posterior inference. We in-
troduce two simple and efficient algorithms
for finding the mode of mDir. Our experi-
ments on learning Gaussian mixtures and un-
supervised dependency parsing demonstrate
the advantage of mDir over Dir.

1 Dirichlet Distribution

The Dirichlet distribution (Dir) is defined over prob-
ability vectors x = 〈x1, . . . , xn〉 with positive pa-
rameter vector α = 〈α1, . . . , αn〉:

Dir(x;α) =
1

B(α)

n∏

i=1

xαi−1i

where the normalization factor B(α) is the multi-
variate beta function. When the elements in α are
larger than one, Dir can be used as a smoothness
prior that prefers more uniform probability vectors,
with larger α values inducing more smoothness.

∗This work was supported by the National Natural Science
Foundation of China (61503248).

When the elements inα are less than one, Dir can be
seen as a sparsity prior that prefers sparse probabil-
ity vectors, with smaller α values inducing stronger
sparsity. To better understand its sparsity preference,
we take the logarithm of Dir:

log Dir(x;α) =
n∑

i=1

(αi − 1) log xi + constant

Since αi − 1 is negative, the closer xi is to zero,
the higher the log probability becomes. The coef-
ficient αi − 1 controls the strength of the sparsity
preference. However, αi is required to be positive
in Dir because otherwise the normalization factor
becomes divergent. Consequently, the strength of
the sparsity preference is upper bounded. This be-
comes problematic when a strong prior is needed,
for instance, when the training dataset is large rela-
tive to the model size (e.g., in unsupervised learning
of an unlexicalized probabilistic grammar) and thus
the likelihood may dominate the posterior without a
strong prior.

2 Modified Dirichlet Distribution

We make a simple modification to the Dirichlet dis-
tribution that allows the parameters in α to become
negative. To handle the divergent normalization fac-
tor, we require that each xi must be lower bounded
by a small positive constant �. Our modified Dirich-
let distribution (mDir) is defined as follows.

mDir(x;α, �) =

{
0 if ∃i, xi < �

1
Z(α,�)

∏n
i=1 x

αi−1
i otherwise

where we require 0 < � ≤ 1n and do not require
αi to be positive. With fixed values of α and �, the

1986



unnormalized probability density is always bounded
and hence the normalization factor Z(α, �) is finite.

It is easy to show that mDir is still conjugate to the
multinomial distribution. Similar to Dir, mDir can
be used as a smoothness/sparsity prior depending on
the values of α. Because α is no longer required
to be positive, we can achieve very strong sparsity
preference by using a highly negative vector of α.
Note that here we no longer achieve sparsity in its
strict sense; instead, by sparsity we mean most ele-
ments in x reach their lower bound �. Parameter �
can thus be seen as a smoothing factor that prevents
any element in x to become too small. Therefore,
with proper parameters, mDir is able to simultane-
ously achieve sparsity and smoothness. This can be
useful in many applications where one wants to learn
a sparse multinomial distribution in an iterative way
without premature pruning of components.

2.1 Finding the Mode
If ∀i, αi − 1 ≤ 0, then the mode of mDir can be
shown to be:

xi =

{
1− (n− 1)� if i = arg maxi αi
� otherwise

Otherwise, we can find the mode with Algorithm 1.
The algorithm first lets xi = � if αi ≤ 1 and other-
wise lets xi be proportional to αi − 1. It then looks
for variables in x that are less than �, increases them
to �, and renormalizes the rest of the variables. The
renormalization may decrease some additional vari-
ables below �, so the procedure is repeated until all
the variables are larger than or equal to �.

Theorem 1. If ∃i, αi > 1, then Algorithm 1 cor-
rectly finds a mode of mDir(x;α, �).

Proof. First, we can show that for any i such that
αi ≤ 1, we must have xi = � at the mode. This is
because if xi > �, then we can increase the probabil-
ity density by first decreasing xi to � (hence increas-
ing xαi−1i ), and then increasing some other variable
xj with αj > 1 to satisfy the normalization condi-
tion (hence also increasing xαj−1j ). This is consis-
tent with the output of the algorithm.

Once we fix the value to � for any variable xi
s.t. αi ≤ 1, the log probability density function
becomes strictly concave on the simplex specified
by the linear constraints

∑
i xi = 1 and xi ≥ �.

Algorithm 1 Mode-finding of mDir(x;α, �)
1: S ← {i|αi ≤ 1}
2: T ← ∅
3: repeat
4: T ← T ⋃S
5: for i ∈ T do
6: xi ← �
7: end for
8: z ←∑i/∈T (αi − 1)
9: for i /∈ T do

10: xi ← αi−1z × (1− �|T |)
11: end for
12: S ← {i|xi < �}
13: until S = ∅
14: return 〈x1, . . . , xn〉

The strict concavity can be proved by showing that
the log probability density function is twice differ-
entiable and the Hessian is negative definite at any
point of the simplex.

With a concave function and linear constraints,
the KKT conditions are sufficient for optimality. We
need to show that the output of the algorithm satis-
fies the following KKT conditions:

• Stationarity: ∀i, αi−1xi = −µi + λ

• Primal feasibility: ∀i, xi ≥ � and
∑

i xi = 1

• Dual feasibility: ∀i, µi ≥ 0

• Complementary slackness: ∀i, µi(xi − �) = 0

Let x(k)i and T
(k) be the values of xi and T after

k iterations of the algorithm. Suppose the algorithm
terminates after K iterations. So the output of the
algorithm is 〈x(K)1 , . . . , x

(K)
n 〉, which we will prove

satisfies the KKT conditions.
For any i s.t. x(K)i > �, we set µi = 0 and λ =

αi−1
x
(K)
i

to satisfy all the conditions involving x(K)i .

For any j s.t. x(K)j = �, suppose x
(k)
j < �, i.e., xj

falls below � in iteration k and is set to � afterwards.
Pick some i s.t. i /∈ T (K). After iteration k and k+1
respectively, we have:

x
(k)
i

αi − 1
=

1− �‖T (k)‖ −∑j′∈T (k+1)\T (k) x
(k)
j′∑

j′ /∈T (k+1) αj′ − 1

x
(k+1)
i

αi − 1
=

1− �‖T (k+1)‖∑
j′ /∈T (k+1) αj′ − 1

1987



Algorithm 2 Fast mode-finding of mDir(x;α, �)
1: 〈αk1 , . . . , αkn〉 ← 〈α1, . . . , αn〉 in ascending order
2: sn ← αkn − 1
3: for i = n− 1, . . . , 1 do
4: si = si+1 + αki − 1 . So si =

∑
j≥i(αkj − 1)

5: end for
6: t← 0
7: for i = 1, . . . , n do
8: xki ←

αki−1
si
× (1− � t)

9: if xki < � then
10: xki ← � , t← t+ 1
11: end if
12: end for
13: return 〈x1, . . . , xn〉

Because for any j′ ∈ T (k+1)\T (k) we have x(k)j′ < �,
from the two equations above we can deduce that
x
(k)
i > x

(k+1)
i , i.e., xi monotonically decreases over

iterations. Therefore,

(αj − 1)×
x
(K)
i

αi − 1
< (αj − 1)×

x
(k)
i

αi − 1
= x

(k)
j < �

So we get

αj − 1
�

<
αi − 1
x
(K)
i

= λ

So we set µj = λ − αj−1� and all the conditions
involving x(K)j are also satisfied. The proof is now
complete.

The worst-case time complexity of Algorithm 1
is O(n2), but in practice when � is small, the algo-
rithm almost always terminates after only one iter-
ation, leading to linear running time. We also pro-
vide a different mode-finding algorithm with better
worst-case time complexity Θ(n log n) (Algorithm
2). It differs from Algorithm 1 in that the elements
of α are first sorted, so we can finish computing x
in one pass. It can be more efficient than Algorithm
1 when both � and n are larger. Its correctness can
be proved in a similar way to that of Algorithm 1.

2.2 Related Distribution

The closest previous work to mDir is the pseudo-
Dirichlet distribution (Larsson and Ugander, 2011).
It also allows negative parameters to achieve
stronger sparsity. However, the pseudo-Dirichlet

distribution is no longer conjugate to the multino-
mial distribution. Consequently, its maximum a pos-
teriori inference becomes complicated and has no
time-complexity guarantee.

3 Learning Mixtures of Gaussians

We first evaluate mDir in learning mixtures of Gaus-
sians from synthetic data. The ground-truth model
contains two bivariate Gaussian components with
equal mixing probabilities (Figure 5(a)). From the
ground-truth we sampled two training datasets of 20
and 200 data points. We then tried to fit a Gaussian
mixture model with five components.

Three approaches were tested: maximum like-
lihood estimation using expectation-maximization
(denoted by EM), which has no sparsity preference;
mean-field variational Bayesian inference with a Dir
prior over the mixing probabilities (denoted by VB-
Dir), which is the most frequently used inference
approach for Dir with α < 1; maximum a posteri-
ori estimation using expectation-maximization with
a mDir prior over the mixing probabilities (denoted
by EM-mDir). The Dir and mDir priors that we used
are both symmetric, i.e., all the elements in vectorα
have the same value, denoted by α. For mDir, we
set � = 10−5. We ran each approach under each
parameter setting for 300 times with different ran-
dom initialization and then reported the average re-
sults. During learning, we pruned a Gaussian com-
ponent whenever its covariance matrix becomes nu-
merically singular (which means the component is
estimated from only one or two data samples).

Figure 1–4 show the average test set log likeli-
hood and the effective numbers of mixture compo-
nents of the models learned with different values of
parameter α from 20 and 200 samples respectively.
For VB-Dir, we show the results with the α value as
low as 10−5. Further decreasing α did not improve
the results. It can be seen that both VB-Dir and
EM-mDir can achieve better test set likelihood and
lower effective numbers of components than EM
with proper α values. EM-mDir outperforms VB-
Dir even with positive α values, and its performance
is further boosted when α becomes negative. The
improvement of EM-mDir when α becomes neg-
ative is smaller in the 20-sample case than in the
200-sample case. This is because when the train-

1988



‐1.448

‐1.446

‐1.444
0 0.5 1 1.5

EM‐mDir VB‐Dir EM

Figure 1: Test set log likelihood vs. the value of α (20 training
samples)

‐1.448

‐1.446

‐1.444
0 0.5 1 1.5

EM‐mDir VB‐Dir EM

Figure 2: Effective number of components vs. the value of α
(20 training samples)

ing dataset is small, a small positive α value may
already be sufficient in inducing enough sparsity.

Figure 5(b)–(e) show the typical models learned
by VB-Dir and EM-mDir. When the training dataset
is small, both Dir and mDir are effective sparsity
priors that help prune unnecessary mixture compo-
nents, though mDir can be more effective with a neg-
ative α value. When the training dataset is large,
however, the Dir prior is overwhelmed by the like-
lihood in posterior inference and cannot effectively
prune mixture components. On the other hand, with
a highly negative α value, mDir is still effective as a
sparsity prior.

4 Unsupervised Dependency Parsing

Unsupervised dependency parsing aims to learn a
dependency grammar from unannotated text. Pre-
vious work has shown that sparsity regularization
improves the performance of unsupervised depen-
dency parsing (Johnson et al., 2007; Gillenwater et
al., 2010). In our experiments, we tried to learn a
dependency model with valence (DMV) (Klein and
Manning, 2004) from the Wall Street Journal cor-
pus, with section 2-21 for training and section 23

‐1.448

‐1.446

‐1.444
0 0.5 1 1.5

EM‐mDir VB‐Dir EM

Figure 3: Test set log likelihood vs. the value of α (200 training
samples)

‐1.448

‐1.446

‐1.444
0 0.5 1 1.5

EM‐mDir VB‐Dir EM

Figure 4: Effective number of components vs. the value of α
(200 training samples)

for testing. Following previous work, we used sen-
tences of length ≤ 10 with punctuation stripped off.
Since DMV is an unlexicalized model, the number
of dependency rules is small relative to the training
corpus size. This suggests that a strong prior can
be helpful in counterbalancing the influence of the
training data.

We tested six approaches. With a mDir prior,
we tried EM, hard EM, and softmax-EM with σ =
0.5 (Tu and Honavar, 2012) (denoted by EM-mDir,
HEM-mDir, SEM-mDir). With a Dir prior, we tried
variational inference, hard variational inference, and
softmax variational inference with σ = 0.5 (Tu
and Honavar, 2012) (denoted by VB-Dir, HVB-Dir,
SVB-Dir). Again, we used symmetric Dir and mDir
priors. For mDir, we set � = 10−4 by default.

Figure 6 shows the directed accuracy of parsing
the test corpus using the learned dependency mod-
els. It can be seen that with positive α values, Dir
and mDir have very similar accuracy under the stan-
dard, hard and softmax versions of inference respec-
tively. With negative α values, the accuracy of EM-
mDir decreases; but for HEM-mDir and SEM-mDir,
the accuracy is significantly improved with moder-

1989



0 1 2 3
0

0.5

1

1.5

2

2.5

3

(a) Ground-truth

0 1 2 3
0

0.5

1

1.5

2

2.5

3

(b) VB-Dir, α = 10−5
0 1 2 3

0

0.5

1

1.5

2

2.5

3

(c) EM-mDir, α = −2
0 1 2 3

0

0.5

1

1.5

2

2.5

3

(d) VB-Dir, α = 10−5
0 1 2 3

0

0.5

1

1.5

2

2.5

3

(e) EM-mDir, α = −30
Figure 5: The ground-truth model and four typical models learned by VB-Dir and EM-mDir. (b),(c): 20 training samples. (d),(e):
200 training samples.

0

0.2

0.4

0.6

0.8

0 0.2 0.4 0.6 0.8 1 1.2

EM‐mDir HEM‐mDir SEM‐mDir VB‐Dir HVB‐Dir SVB‐Dir
r

0.4

0.45

0.5

0.55

0.6

0.65

‐70 ‐60 ‐50 ‐40 ‐30 ‐20 ‐10 0
0.4

0.45

0.5

0.55

0.6

0.65

0 0.25 0.5 0.75 1

Figure 6: Parsing accuracy vs. the value of α

ately negative α values. HEM-mDir consistently
produces accuracy around 0.63 with a large range of
α values (from -10 to -40), which is on a par with the
best published results in learning the original DMV
model (Cohen and Smith, 2009; Gillenwater et al.,
2010; Berg-Kirkpatrick et al., 2010), even though
these previous approaches employed more sophis-
ticated features and advanced regularization tech-
niques than ours.

Figure 7 shows the degree of sparsity of the
learned dependency grammars. We computed the
percentage of dependency rules with probabilities
below 10−3 to measure the degree of sparsity. It can
be seen that even with positive α values, mDir leads
to significantly more sparse grammars than Dir does.
With negative values of α, mDir can induce even
more sparsity.

Figure 8 plots the parsing accuracy with different
values of parameter � in mDir (α is set to -20). The
best accuracy is achieved when � is neither too large
nor too small. This is because if � is too large, the
probabilities of dependency rules become too uni-
form to be discriminative. On the other hand, if �
is too small, then the probabilities of many depen-
dency rules may become too small in the early stages
of learning and never be able to recover. Similar ob-
servation was made by Johnson et al. (2007) when

0

0.2

0.4

0.6

0.8

0 0.2 0.4 0.6 0.8 1 1.2

EM‐mDir HEM‐mDir SEM‐mDir VB‐Dir HVB‐Dir SVB‐Dir

Dir

0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9

‐70 ‐60 ‐50 ‐40 ‐30 ‐20 ‐10 0
0.4

0.45
0.5

0.55
0.6

0.65
0.7

0.75
0.8

0.85
0.9

0 0.25 0.5 0.75 1

Figure 7: Sparsity of the learned grammars vs. the value of α

r

0.4

0.45

0.5

0.55

0.6

0.65

1.E‐08 1.E‐06 1.E‐04 1.E‐02

EM‐mDir HEM‐mDir SEM‐mDir

Figure 8: Parsing accuracy vs. the value of �

doing maximum a posteriori estimation with a Dir
prior (hence with � = 0).

5 Conclusion

We modify the Dirichlet distribution to allow nega-
tive values of parameterα so that it induces stronger
sparsity when used as a prior of a multinomial
distribution. A second parameter � is introduced
which prevents divergence of the normalization fac-
tor and also acts as a smoothing factor. Our modified
Dirichlet distribution (mDir) is still conjugate to the
multinomial distribution. We propose two efficient
algorithms for finding the mode of mDir. Our ex-
periments on learning Gaussian mixtures and unsu-
pervised dependency parsing show the advantage of
mDir over the Dirichlet distribution.

1990



References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,

John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582–590. Association for
Computational Linguistics.

Shay B. Cohen and Noah A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in un-
supervised grammar induction. In HLT-NAACL, pages
74–82.

Jennifer Gillenwater, Kuzman Ganchev, João Graça, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in de-
pendency grammar induction. In ACL ’10: Proceed-
ings of the ACL 2010 Conference Short Papers, pages
194–199, Morristown, NJ, USA. Association for Com-
putational Linguistics.

Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for pcfgs via markov
chain monte carlo. In HLT-NAACL, pages 139–146.

Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL.

Martin O Larsson and Johan Ugander. 2011. A concave
regularization technique for sparse mixture models. In
Advances in Neural Information Processing Systems,
pages 1890–1898.

Kewei Tu and Vasant Honavar. 2012. Unambiguity reg-
ularization for unsupervised learning of probabilistic
grammars. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 1324–1334. Association for Computational
Linguistics.

1991


