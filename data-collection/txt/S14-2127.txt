



















































ULisboa: Identification and Classification of Medical Concepts


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 711–715,
Dublin, Ireland, August 23-24, 2014.

ULisboa: Identification and Classification of Medical Concepts

André Leal+, Diogo Gonçalves+, Bruno Martins∗, and Francisco M. Couto+
+LASIGE, Faculdade de Ciências, Universidade de Lisboa, 1749-016 Lisboa, Portugal.

∗INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Portugal
{aleal,dgoncalves}@lasige.di.fc.ul.pt , bruno.g.martins@ist.ul.pt , fcouto@di.fc.ul.pt

Abstract

This paper describes our participation on
Task 7 of SemEval 2014, which fo-
cused on the recognition and disambigua-
tion of medical concepts. We used an
adapted version of the Stanford NER sys-
tem to train CRF models to recognize tex-
tual spans denoting diseases and disor-
ders, within clinical notes. We consid-
ered an encoding that accounts with non-
continuous entities, together with a rich
set of features (i) based on domain spe-
cific lexicons like SNOMED CT, or (ii)
leveraging Brown clusters inferred from a
large collection of clinical texts. Together
with this recognition mechanism, we used
a heuristic similarity search method, to
assign an unambiguous identifier to each
concept recognized in the text.

Our best run on Task A (i.e., in the recog-
nition of medical concepts in the text)
achieved an F-measure of 0.705 in the
strict evaluation mode, and a promising
F-measure of 0.862 in the relaxed mode,
with a precision of 0.914. For Task B (i.e.,
the disambiguation of the recognized con-
cepts), we achieved less promising results,
with an accuracy of 0.405 in the strict
mode, and of 0.615 in the relaxed mode.

1 Introduction

Currently, many off-the-shelf named entity recog-
nition solutions are available, and these can be
used to recognize mentions in clinical notes de-
noting diseases and disorders. We decided to use
the Stanford NER tool (Finkel et al., 2005) to train
CRF models based on annotated biomedical text.

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

The use of unsupervised methods for inferring
word representations is nowadays also known to
increase the accuracy of entity recognition mod-
els (Turian et al., 2010). Thus, we also used Brown
clusters (Brown et al., 1992; Turian et al., 2009)
inferred from a large collection of non-annotated
clinical texts, together with domain specific lexi-
cons, to build features for our CRF models.

An important challenge in entity recognition re-
lates to the recognition of overlapping and non-
continuous entities (Alex et al., 2007). In this
paper, we describe how we modified the Stan-
ford NER system to be able to recognize non-
continuous entities, through an adapted version of
the SBIEO scheme (Ratinov and Roth, 2009).

Besides the recognition of medical concepts, we
also present the strategy used to map each of the
recognized concepts into a SNOMED CT identi-
fier (Cornet and de Keizer, 2008). This task is
particularly challenging, since there are many am-
biguous cases. We describe our general approach
to address the aforementioned CUI mapping prob-
lem, based on similarity search and on the infor-
mation content of SNOMED CT concept names.

2 Task and Datasets

Task 7 of SemEval 2014 actually consisted of two
smaller tasks: recognition of mentions of medi-
cal concepts (Task A) and mapping each medical
concept, recognized in clinical notes, to a unique
UMLS CUI (Task B). In the first task, recogni-
tion of medical concepts, systems have to detect
continuous and discontinuous medical concepts
that belong to the UMLS semantic group disor-
ders. The second task, concerning with normal-
ization and mapping, is limited to UMLS CUIs
of SNOMED CT codes (i.e., although the UMLS
meta-thesaurus integrates several resources, we
are only interested in SNOMED CT). Each con-
cept that was previously recognized can have a
unique CUI associated to it, or none at all (CUI-

711



LESS). The goal here is to disambiguate the con-
cepts and choose the right CUI for each case. For
supporting the recognition and CUI mapping of
medical concepts, we retrieved the disorders sub-
set of SNOMED CT directly from UMLS1.

The evaluation can be done in a strict or a in
a relaxed way. For the case of strict evaluation,
an exact match must be achieved in the recogni-
tion, by having correct start and end offsets, within
the text, for the continuous concepts, and a cor-
rect set of start and end offsets for the discontin-
uous concepts. In the relaxed evaluation, there is
some space for errors in the offset values from the
recognition task. If there is some overlap between
the concepts, then the result is considered a partial
match, otherwise it is a recognition error.

A set of annotated biomedical texts was given
to the participants, separated in three categories:
trial, development and training. We also received a
final test set, and a large set of non-annotated texts.
All the provided texts were initially converted into
a common tokenized format, to be used as input
to the tools that we considered for developing our
approach. After processing, we converted the re-
sults back into the format used by SemEval 2014,
this way generating the official runs.

3 Entity Recognition

Our entity recognition approach was based on the
usage of the Stanford NER software, which em-
ploys a linear chain Conditional Random Field
(CRF) approach for building probabilistic mod-
els based on training data (Finkel et al., 2005).
In Stanford NER, model training is based on the
L-BFGS algorithm, and model decoding is made
through the Viterbi algorithm.

This tool requires all input texts to be tokenized
and encoded according to a named entity recog-
nition scheme such as SBIEO (Ratinov and Roth,
2009), characterized by only being able to recog-
nize continuous entities. As we also need to rec-
ognize non-continuous entities, we modified the
Stanford NER software to use a SBIEON encod-
ing scheme. This new scheme has the following
specific token-tag associations:

S: Single, that indicates if the token individually
constitutes an entity to be recognized.

B: Begin, identifying the beginning of the entity.
This tag is only given to the first word of the

1http://www.nlm.nih.gov/research/umls/

entity, being followed is most cases by tokens
labeled as being inside the entity.

I: Inside, representing the continuation of a non
single word entity (i.e., the middle tokens).

E: Ending, representing the last word in the case
of entities composed by more than one word.

N: Non-Continuous, which identifies all the
words that are between the beginning and the
end of an entity, but that do not belong to it.
This label specifically allows us to model the
in-between tokens of non-continuous entities.

O: Other, which is associated to all other words
that are not part of entities.

We developed a Java parser that converts the
biomedical text, provided to the participants, into
a tokenized format. This tokenized format, in the
case of the annotated texts, associates individual
tokens to their SBIEON or SBIEO tags, so that the
datasets can be used as input to train CRF models.

3.1 Concept Recognition Models

As we said, SBIEON tokenization differs from
SBIEO by the fact that the first one gives support
to non-continuous entities. Based on these two in-
put schemes, we generated two different models:

Only continuous entities: A 2nd-order CRF
model was trained based on the SBIOE entity en-
coding scheme, which only recognizes continuous
entities. Non-continuous and overlapping entities
will thus, in this case, only be partially modeled
(i.e., we only considered the initial span of text as-
sociated to the non-continuous entities).

Non-continuous entities: A 2nd-order CRF
model was trained based on the SBIOEN entity
encoding scheme, accepting continuous and non-
continuous entities, although still not supporting
the case of overlapping entities. In these last cases,
only the first entity in each of the overlapping
groups will be modeled correctly, while the oth-
ers will only be partially modeled (i.e., by only
considering the non-overlapping spans).

Our CRF models relied on a standard set of fea-
ture templates that includes (i) word tokens within
a window of size 2, (ii) the token shape (e.g., if it
is uppercased, capitalized, numeric, etc.), (iii) to-
ken prefixes and suffixes, (iv) token position (e.g.,
at the beginning or ending of a sentence), and (v)
conjunctions of the current token with the previ-
ous 2 tags. Besides these standard features, we
also considered (a) domain-specific lexicons, and
(b) word representations based on Brown clusters.

712



3.2 Word Clusters

In addition to the annotated training dataset, par-
ticipants were also provided with 403876 non-
annotated texts, containing a total of 828509 to-
kens. We used this information to induce general-
ized cluster-based word representations.

Brown et al. proposed a greedy agglomera-
tive hierarchical clustering procedure that groups
words to maximize the mutual information of bi-
grams (Brown et al., 1992). According to Brown’s
clustering procedure, clusters are initialized as
consisting of a single word each, and are then
greedily merged according to a mutual informa-
tion criterion, based on bi-grams, to form a lower-
dimensional representation of a vocabulary that
can mitigate the feature sparseness problem. In
the context of named entity recognition studies,
several authors have previously noted that using
these types of cluster-based word representations
can indeed result in improvements (Turian et al.,
2009). The hierarchical nature of the clustering
allows words to be represented at different levels
in the hierarchy and, in our case, we considered
1000 different clusters of similar words.

We specifically used the set of training docu-
ments, together with the non-annotated documents
that were provided by the organizers, to induce
word representations based on Brown’s clustering
procedure, using an open-source implementation
that follows the description given by (Turian et al.,
2010). The word clusters are latter used as features
within the Stanford NER package, by considering
that each word can be replaced by the correspond-
ing cluster, this way adding some other back-off
features to the models (i.e., features that are less
sparse, in the sense that they will appear more fre-
quently associated to some of the instances).

4 Disambiguating Concepts

For mapping entities to concept IDs (Task B), we
used a heuristic method based on similarity search,
supported on Lucene indexes (MacCandless et al.,
2010). We look for SNOMED CT concepts that
have a high n-gram overlap with the entity name
occurring in the text, together with the information
content of each SNOMED CT concept.

In our implementation, we used Lucene to re-
trieve candidate SNOMED CT concepts according
to different string distance algorithms: the NGram
distance (Kondrak, 2005) first, then according to
the Jaro-Winkler distance (Winkler, 1990), and fi-

nally according to the Levenshtein distance. The
most similar candidate is chosen as the disam-
biguation. The specific order for the similar-
ity metrics was based on the intuition that met-
rics based on individual character-level matches
are probably not as informative as metrics based
on longer sequences of characters, although they
can be useful for dealing with spelling variations.
However, for future work, we plan to explore more
systematic approaches (e.g., based on learning to
rank) for combining multiple similarity metrics.

Additionally to the aforementioned similarity
metrics, a measure of the Information Content (IC)
of each SNOMED CT concept was also employed,
to further disambiguate the mappings (i.e., to se-
lect the SNOMED CT identifier that is more gen-
eral, and thus more likely to be associated to a par-
ticular concept descriptor). Notice that the IC of
a concept corresponds to a measure of its speci-
ficity, where higher values correspond to more
specific concepts, and lower values to more gen-
eral ones. Given the frequency freq(c) for each
concept c in a corpus (i.e., the same corpus that
was used to infer the word clusters), the informa-
tion content of this concept can be computed from
the ratio between its frequency (including its de-
scendants) and the maximum frequency of all con-
cepts (Resnik, 1995):

IC(c) = − log
(

freq(c)
maxFreq

)
In the formula, maxFreq represents the maximum
frequency of a concept, i.e. the frequency of the
root concept, when it exists. The frequency of a
concept can be computed using an extrinsic ap-
proach that counts the exact matches of the con-
cept names on a large text corpus.

5 Evaluation Experiments

We submitted three distinct runs to the SemEval
competition. These runs were as follows:

Run 1: A SBIOEN model was used to recog-
nize non-continuous entities. This model was
trained using only the annotated texts from
the provided training set. We also used some
domain specific lexicons like SNOMED CT,
or lists with names for drugs and diseases
retrieved from DBPedia. Finally, the recog-
nition model also used Brown clusters gen-
erated from the non-annotated datasets pro-
vided in the competition.

713



For assigning a SNOMED CT identifier to
each entity, we used the disambiguation tech-
nique supported by Lucene indexes. In
this specific run we used all the considered
heuristics for similarity search.

Run 2: A simpler model based on the SBIOE
scheme was used in this case, which can only
recognize continuous entities. The same fea-
tures from Run 1 were used for training the
recognition model.

For assigning the SNOMED CT identifier to
each entity, we also used the same strategy
that was presented for Run 1.

Run 3: A similar SBIOE model to that from Run
2 was used for the recognition.

For assigning the corresponding SNOMED
CT identifier to each entity, we in this case
limited the heuristic rules that were used.
Instead of using the string similarity algo-
rithms, we used only exact matches, together
with the information content measure and the
neighboring terms for disambiguation.

6 Results and Discussion

We present our official results in Table 1, which
highlights our best results for each task.

Specifically in Task A, we achieved encourag-
ing results. Run 1 achieved an F-measure of 0.705
in the strict evaluation, and of 0.862 in the relaxed
evaluation. Since Runs 2 and 3 used the same
recognition strategy (i.e., models that attempted to
capture only the continuous entities), we obtained
the same results for Task A in both these runs. Ta-
ble 1 also shows that our performance in Task B
was significantly lower than in Task A.

As we can see in the table, our first run was the
one with the best results for Task A. The model
used on this run recognizes non-continuous enti-
ties, and this is perhaps the main reason for the
higher results (i.e., the other two runs used the
same features for the recognition models).

On what concerns the results of Task B, it is im-
portant to notice the distinct results from the first
and second runs, which used exactly the same dis-
ambiguation strategy. The differences in the re-
sults are a consequence from the use of a different
recognition model in Task A. We can see that the
ability to recognize non-continuous entities leads
to the generation of worse mappings, when con-
sidering our specific disambiguation strategy. Our

last run is the best in terms of the performance over
Task B, but the difference is subtle.

7 Conclusions and Future Work

This paper described our participation in Task 7 of
the SemEval 2014 competition, which was divided
into two subtasks, namely (i) the recognition of
continuous and non-continuous medical concepts,
and (ii) the mapping of each recognized concept to
a SNOMED CT identifier.

For the first task, we used the Stanford NER
software (Finkel et al., 2005), modified by us
to recognize not only continuous, but also non-
continuous entities. This was possible by intro-
ducing the SBIEON scheme, derived from the tra-
ditional SBIEO encoding. To increase the accu-
racy and precision of the recognition we have also
used domain specific lexicons and Brown clusters
inferred from non-annotated documents.

For the second task, we used a heuristic method
based on similarity search, for matching concepts
in the text against concepts from SNOMED CT,
together with a measure of information content
to disambiguate the cases of term polysemy in
SNOMED CT. We implemented our disambigua-
tion approach through the Lucene software frame-
work (MacCandless et al., 2010).

In the first task (Task A) we achieved some par-
ticularly encouraging results, showing that an off-
the-shelf NER system can be easily adapted to
the recognition of medical concepts in biomedi-
cal text. Our specific modifications to the Stanford
NER system, in order to support the recognition of
non-continuous entity names, indeed increased the
precision and recall on Task A. However, our ap-
proach for the disambiguation of the recognized
concepts (Task B) performed much worse, achiev-
ing an accuracy of 0.615 in the case of the relaxed
evaluation. Future developments will therefore fo-
cus on improving the component that addressed
the entity disambiguation subtask.

Specifically on what regards future work, we
plan to experiment with the usage of machine
learning methods for the disambiguation subtask,
instead of relying on a purely heuristic approach.
We are interested in experimenting with the us-
age of Learning to Rank (L2R) methods, similar to
those employed on the DNorm system (Leaman et
al., 2013), to optimally combine different heuris-
tics such as the ones that were used in our current
approach. A L2R model can be used to rank candi-

714



Task A Task B
Strict Evaluation Relaxed Evaluation Strict Relaxed

Run Precision Recall F-measure Precision Recall F-measure Accuracy Accuracy
1 0.753 0.663 0.705 0.914 0.815 0.862 0.402 0.606
2 0.752 0.660 0.703 0.909 0.806 0.855 0.404 0.612
3 0.752 0.660 0.703 0.909 0.806 0.855 0.405 0.615

Table 1: Our official results for Tasks A and B of the SemEval challenge focusing on clinical text.

date disambiguations (e.g., retrieved through sim-
ilarity search with basis on Lucene) according to a
combination of multiple criteria, and we can then
choose the top candidate as the disambiguation.

Additionally, we plan to use ontology-based
similarity measures to validate and improve the
mappings (Couto and Pinto, 2013). For example,
by assuming that all entities in a given span of text
are semantically related with each other, we can
use ontology relations to filter likely misannota-
tions (Grego and Couto, 2013; Grego et al., 2013).

Acknowledgments

The authors would like to thank Fundação para a
Ciência e Tecnologia (FCT) for the financial sup-
port of SOMER (PTDC/EIA-EIA/119119/2010),
LASIGE (PEst-OE/EEI/UI0408/2014) and
INESC-ID (Pest-OE/EEI/LA0021/2013).

We also would like to thank our colleagues
Berta Alves, for her support in evaluating devel-
opment errors, and Luı́s Atalaya, for the develop-
ment of some of the data pre-processing scripts.

References
Beatrice Alex, Barry Haddow, and Claire Grover.

2007. Recognising nested named entities in biomed-
ical text. In Proceedings of the ACL-07 Workshop
on Biological, Translational, and Clinical Language
Processing, pages 65–72.

Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467–479.

Ronald Cornet and Nicolette de Keizer. 2008. Forty
years of SNOMED: A literature review. BMC
Medical Informatics and Decision Making, 8(Suppl
1:S2):1–6.

Francisco M. Couto and Helena Sofia Pinto. 2013. The
next generation of similarity measures that fully ex-
plore the semantics in biomedical ontologies. Jour-
nal of Bioinformatics and Computational Biology,
11(05):1–11.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs

sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370.

Tiago Grego and Francisco M Couto. 2013. Enhance-
ment of chemical entity identification in text using
semantic similarity validation. PloS ONE, 8(5):1–9.

Tiago Grego, Francisco Pinto, and Francisco Couto.
2013. LASIGE: using conditional random fields and
chebi ontology. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation, pages
660–666.

Grzegorz Kondrak. 2005. N-gram similarity and dis-
tance. In Proceedings of the 12th International
Conference String Processing and Information Re-
trieval, pages 115–126.

Robert Leaman, Rezarta Islamaj Doğan, and Zhiy-
ong Lu. 2013. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909–2917.

Michael MacCandless, Erik Hatcher, and Otis Gospod-
netić. 2010. Lucene in Action. Manning Publica-
tions Company.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning, pages 147–155.

Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448–453.

Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In Pro-
ceedings of the NIPS-09 Workshop on Grammar In-
duction, Representation of Language and Language
Learning, pages 1–8.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394.

William E Winkler. 1990. String comparator metrics
and enhanced decision rules in the Fellegi-Sunter
model of record linkage. In Proceedings of the Sec-
tion on Survey Research of the American Statistical
Association, pages 354–359.

715


