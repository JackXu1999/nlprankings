



















































End-to-end Relation Extraction using Neural Networks and Markov Logic Networks


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 818–827,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

End-to-end Relation Extraction using Neural Networks and Markov
Logic Networks

Sachin Pawar1,2, Pushpak Bhattacharyya2, and Girish K. Palshikar1

1TCS Research, Tata Consultancy Services, Pune
2Indian Institute of Technology Bombay, Mumbai
{sachin7.p,gk.palshikar}@tcs.com

pb@cse.iitb.ac.in

Abstract

End-to-end relation extraction refers to
identifying boundaries of entity mentions,
entity types of these mentions and appro-
priate semantic relation for each pair of
mentions. Traditionally, separate predic-
tive models were trained for each of these
tasks and were used in a “pipeline” fash-
ion where output of one model is fed as
input to another. But it was observed that
addressing some of these tasks jointly re-
sults in better performance. We propose a
single, joint neural network based model
to carry out all the three tasks of bound-
ary identification, entity type classifica-
tion and relation type classification. This
model is referred to as “All Word Pairs”
model (AWP-NN) as it assigns an appro-
priate label to each word pair in a given
sentence for performing end-to-end rela-
tion extraction. We also propose to re-
fine output of the AWP-NN model by us-
ing inference in Markov Logic Networks
(MLN) so that additional domain knowl-
edge can be effectively incorporated. We
demonstrate effectiveness of our approach
by achieving better end-to-end relation ex-
traction performance than all 4 previous
joint modelling approaches, on the stan-
dard dataset of ACE 2004.

1 Introduction

The task of relation extraction (RE) deals with
identifying whether any pre-defined semantic re-
lation holds between a pair of entity mentions in
the given sentence. Pure relation extraction tech-
niques (Zhou et al., 2005; Jiang and Zhai, 2007;
Bunescu and Mooney, 2005; Qian et al., 2008)
assume that for a sentence, gold-standard entity

mentions (i.e. boundaries as well as types) in it are
known. In contrast, end-to-end relation extraction
deals with plain sentences without assuming any
knowledge of entity mentions in them. The task
of end-to-end relation extraction consists of three
sub-tasks: i) identifying boundaries of entity men-
tions, ii) identifying entity types of these mentions
and iii) identifying appropriate semantic relation
for each pair of mentions. First two sub-tasks
correspond to the Entity Detection and Tracking
task defined by the the Automatic Content Extrac-
tion (ACE) program (Doddington et al., 2004) and
the third sub-task corresponds to the Relation De-
tection and Characterization (RDC) task. ACE
standard defined 7 entity types1: PER (person),
ORG (organization), LOC (location), GPE (geo-
political entity), FAC (facility), VEH (vehicle) and
WEA (weapon). It also defined 7 coarse level
relation types2: EMP-ORG (employment), PER-
SOC (personal/social), PHYS (physical), GPE-
AFF (GPE affiliation), OTHER-AFF (PER/ORG
affiliation), ART (agent-artifact) and DISC (dis-
course).

Traditionally, the three sub-tasks of end-to-end
relation extraction are carried out serially in a
“pipeline” fashion. In this case, the errors in any
sub-task affect subsequent sub-tasks. Another dis-
advantage of this “pipeline” approach is that it
allows only one-way information flow, i.e. the
knowledge about entities is used for identifying re-
lations but not vice versa. Hence to overcome this
problem, several approaches (Roth and Yih, 2004;
Roth and Yih, 2002; Singh et al., 2013; Li and Ji,
2014) were proposed which carried out these sub-
tasks jointly rather than in “pipeline” manner.

We propose a new approach which combines

1www.ldc.upenn.edu/sites/www.ldc.
upenn.edu/files/english-edt-v4.2.6.pdf

2www.ldc.upenn.edu/sites/www.ldc.
upenn.edu/files/english-rdc-v4.3.2.PDF

818



Entity Mention Boundaries Entity Type
His (0, 0) PER
sister (1, 1) PER
Mary Jones (2, 3) PER
United Kingdom (7, 8) GPE

Table 1: Expected output of end-to-end relation
extraction system for entity mentions

the powers of Neural Networks and Markov Logic
Networks to jointly address all the three sub-tasks
of end-to-end relation extraction. We design the
“All Word Pairs” neural network model (AWP-
NN) which reduces solution of these three sub-
tasks to predicting an appropriate label for each
word pair in a given sentence. End-to-end rela-
tion extraction output can then be constructed eas-
ily from these labels of word pairs. Moreover,
as a separate prediction is made for each word
pair, there may be some inconsistencies among
the labels. We address this problem by refining
the predictions of AWP-NN by using inference in
Markov Logic Networks so that some of the in-
consistencies in word pair labels can be removed
at the sentence level.

The specific contributions of this work are : i)
modelling boundary detection problem by intro-
ducing a special relation type WEM and ii) a sin-
gle, joint neural network model for all three sub-
tasks of end-to-end relation extraction. The paper
is organized as follows. Section 2 provides a de-
tailed problem definition. Section 3 describes our
AWP-NN model in detail, followed by Section 4
which describes how the predictions of AWP-NN
model are revised using inference in MLNs. Sec-
tion 5 provides experimental results and analysis.
Finally, we conclude in Section 6 with a short note
on future work.

2 Problem Definition

Given a sentence as an input, an end-to-end rela-
tion extraction system should produce a list of en-
tity mentions within it. For each entity mention,
its boundaries and entity type should be identi-
fied. Also, for each pair of valid entity mentions,
it should decide whether any pre-defined semantic
relation holds between them.

Consider the sentence : His0 sister1
Mary2 Jones3 went4 to5 the6 United7
Kingdom8 .9 Here, end-to-end relation extrac-
tion should produce the output as shown in the
tables 1 and 2.

Entity Mention Pair Relation Type
His, sister PER-SOC
His, Mary Jones PER-SOC
sister, United Kingdom PHYS
Mary Jones, United Kingdom PHYS

Table 2: Expected output of end-to-end relation
extraction system for relations

3 All Word Pairs Model (AWP-NN)

We propose a single, joint model for addressing
all three sub-tasks of end-to-end relation extrac-
tion : i) identifying boundaries of entity mentions,
ii) identifying entity types of these mentions and
iii) identifying appropriate semantic relation for
each pair of mentions. We refer to this model as
AWP-NN, i.e. All Word Pairs model using Neu-
ral Networks. Here, annotations of all these three
sub-tasks can be represented by assigning an ap-
propriate label to each pair of words. It is not nec-
essary to assign label to all possible word pairs;
rather ith word is paired with jth word only when
j ≥ i. AWP-NN model is motivated from the
table representation idea proposed by Miwa and
Sasaki (2014) but differs significantly from it in
following ways:

1. boundary identification is modelled with the
help of a special relation type (WEM) instead
of BIO (Begin, Inside, Other) encoding or
BILOU (Begin, Inside, Last, Unit, Other) en-
coding

2. neural network model for prediction of ap-
propriate label for each word pair instead of
structured prediction

Labels predicted by the AWP-NN model for
each word pair can then be used to construct the
end-to-end relation extraction output as described
in tables 1 and 2.

Consider the example sentence from Section 2.
Table 3 shows true annotations of all word pairs in
this sentence as required for training the AWP-NN
model. Labels used for these annotations can be
grouped into the following 5 logical clusters:

1. PER, ORG, GPE, LOC, FAC, VEH and
WEA : Represent entity type of head word
of an entity mention when both the words in
a word pair are the same

2. OTH : Represents words which are not head
words of any entity mention and both the
words in a word pair are the same

819



His sister Mary Jones went to the United Kingdom .
His PER PER-SOC NULL PER-SOC NULL NULL NULL NULL NULL NULL

sister PER NULL NULL NULL NULL NULL NULL PHYS NULL
Mary OTH WEM NULL NULL NULL NULL NULL NULL
Jones PER NULL NULL NULL NULL PHYS NULL
went OTH NULL NULL NULL NULL NULL
to OTH NULL NULL NULL NULL
the OTH NULL NULL NULL

United OTH WEM NULL
Kingdom GPE NULL

. OTH

Table 3: Annotation of all word pairs as per the AWP-NN model

3. EMP-ORG, PHYS, OTHER-AFF, EMP-
ORG-R, PHYS-R, OTHER-AFF-R3, PER-
SOC, GPE-AFF and ART : Represent rela-
tion type between head words of any two en-
tity mentions

4. NULL : Indicates that no pre-defined seman-
tic relation exists between the words in the
word pair

5. WEM (Within Entity Mention) : Indicates
that the words in the word pair belong to the
same entity mention and one of the word is
the head word of that mention

3.1 Features for the AWP-NN model
Previous work (Zhou et al., 2005; Jiang and Zhai,
2007; Bunescu and Mooney, 2005; Qian et al.,
2008) in relation extraction establishes the im-
portance of both lexical and syntactic features.
Hence, we designed features to capture informa-
tion about word sequences, POS tags and depen-
dency structure. As each word pair constitutes
a separate instance for classification, features are
of three types: i) features characterizing individ-
ual word in a word pair, ii) features characterizing
properties of both the words at a time and iii) fea-
tures based on feedback, i.e. predictions of pre-
ceding instances.

3.1.1 Individual word features
These features are generated separately for both
the words in a word pair.
1. Word itself and its POS tag
2. Previous word and previous POS tag
3. Next word and next POS tag
4. Parent / Governor of the word in the depen-
dency tree, the corresponding dependency relation
type and POS tag of the parent

3EMP-ORG-R, PHYS-R and OTHER-AFF-R correspond
to relation types EMP-ORG, PHYS and OTHER-AFF in the
reverse direction, respectively.

3.1.2 Word pair features
These features are generated for a word pair (say
〈Wi, Wj〉) as a whole.
1. Words distance (WD): Number of words in the
sentence between the words Wi and Wj
2. Tree distance (TD): Number of words on the
path leading from Wi to Wj in the sentence’s de-
pendency tree
3. Common Ancestor (CA): Lowest common an-
cestor of the two words in the dependency tree
4. Ancestor Position (AP): It indicates the posi-
tion of the common ancestor with respect to the
two words of a word pair. Different possible po-
sitions of the ancestor are - left of Wi, Wi itself,
between Wi and Wj , Wj itself and right of Wj .
5. Dependency Path (DP1, DP2, · · · , DPK) : Se-
quence of dependency relation types (ignoring di-
rections) on the dependency path leading from Wi
to Wj in the sentence’s dependency tree.

3.1.3 Feedback features
These features are based on predictions of the
preceding instances. Unlike other sequence la-
belling problems such as Named Entity Recog-
nition where each word gets a label and there is
natural order / sequence of instances (i.e. words),
there is no natural order / sequence of instances
(i.e. word pairs) for AWP-NN model. Hence,
for each instance we identify its two preceding
instances and define two corresponding feedback
features (FB1 and FB2). Let 〈Wi, Wj〉 be an in-
stance representing a word pair in a sentence hav-
ing N words such that 1 ≤ i, j ≤ N and i ≤ j.
There are following two cases for identifying two
preceding instances of 〈Wi, Wj〉:
• If i = j then both the preceding instances are

same i.e. 〈Wi−1, Wi−1〉. Feedback features:
FB1 = FB2 = LabelOf(〈Wi−1, Wi−1〉)
• If i < j then the preceding instances are

820



〈Wi, Wi〉 and 〈Wj , Wj〉. Feedback features:
FB1 = LabelOf(〈Wi, Wi〉) and FB2 =
LabelOf(〈Wj , Wj〉)

Label predictions of the preceding instances are
then represented using one-hot encoding and used
as features. During training, true labels of the pre-
ceding instances are used but while decoding, the
predicted labels of these instances are used. Hence
during decoding, predictions for word pairs of the
form 〈Wi, Wi〉 (diagonal word pairs in the table 3)
are obtained first, starting from i = 1 to N . Pre-
dictions of other word pairs can be obtained later,
as predictions of their preceding instances would
then be available.

3.2 Architecture of the AWP-NN model

Figure 1 shows various major components in the
architecture of the AWP-NN model.

3.2.1 Embedding Layers
Most of the features used by the model are discrete
in nature such as words, POS tags, dependency re-
lation types and ancestor position. These discrete
features have to be mapped to some numerical rep-
resentation and embedding layers are used for this
purpose. We have employed following embedding
layers to represent various types of features:
Word embedding layer: It maps each word
to a real-valued vector of some fixed dimen-
sions. We initialize this layer with the pre-trained
100 dimensional GloVe word vectors4 learned on
Wikipedia corpus. All the different features which
are expressed in the form of words (W1, W2,
NW1, PW1, NW2, PW2, Pa1, Pa2 and CA in
the figure 1) share the same word embedding layer.
During training, the initial embeddings get fine-
tuned for our supervised classification task.
POS embedding layer: It maps each distinct POS
tag to some real-valued vector representation. All
the different features which are expressed in the
form of POS tags (T1, T2, NT1, PT1, NT2, PT2,
PaT1 and PaT2 in the figure 1) share the same
embedding layer.
Dependency relation type embedding layer:
It maps each distinct dependency relation type
to some real-valued vector representation.
Both the features based on dependency types
(DR1, DR2, DP1, · · · , DPK in the figure 1) also
share the same embedding layer.

4http://nlp.stanford.edu/projects/glove/

AP embedding layer: It maps each distinct an-
cestor position to some real-valued vector repre-
sentation.
WD/TD embedding layer: Even though word
distance (WD) and tree distance (TD) are numer-
ical features, we used embeddings to represent
each distinct value for them as range of values of
these features is large. It was observed to be better
than directly providing them as inputs to the neural
network.

In our experiments, we used 20 dimensions for
POS embeddings, 40 for dependency relation type
embeddings and 5 dimensions for AP, WD and TD
embeddings. Unlike word embeddings these were
initialized randomly during training.

3.2.2 Hidden Layers
First hidden layer is divided in 3 parts. First
two parts of 60 nodes each are connected to only
the features capturing first and second word, re-
spectively. These nodes are expected to capture
higher level abstract features of both the words
separately. In order to force these two parts to
learn similar abstract features, the weights matrix
is shared among them. The third part of the first
hidden layer consisting of 500 nodes is connected
to all the input features except dependency path,
i.e. individual word features of two words, word
pair features and feedback features. Output of this
part is further given as input to the second hidden
layer of 250 units. Output of the second hidden
layer is fed to the final softmax layer. Also, out-
puts of the first two parts of the first hidden layer
are directly connected to the final softmax layer.
As the dependency path is represented as a se-
quence of dependency relation types, it is fed to a
separate LSTM layer. Output of the LSTM layer is
directly connected to the final softmax layer. Soft-
max layer consists of 19 nodes, each representing
one of the possible prediction label described ear-
lier.

4 Inference using Markov Logic
Networks

Pawar et al. (2016) presented an approach for
end-to-end relation extraction which uses Markov
Logic Networks (MLN) (Richardson and Domin-
gos, 2006) to obtain globally consistent output by
combining local outputs of individual classifiers.
They developed separate classifiers for identifying
mention boundaries, predicting entity types and

821



Figure 1: AWP-NN model architecture for predicting appropriate label for the given word pair. (W1, W2:
words in the word pair; NW1, PW1, NW2, PW2, NT1, PT1, NT2, PT2: next and previous words/POS
tags of W1 and W2; Pa1, DR1, Pa2, DR2: parents and corresponding dependency relation types of W1
and W2 in the dependency tree; PaT1, PaT2: POS tags of the parents of W1 and W2; FB1, FB2: Pre-
dictions of the preceding instances; CA: Lowest common ancestor of W1 and W2 in the dependency tree;
TD: Tree distance; WD: Words distance; AP : Ancestor position; DP1, DP2, · · · , DPK : Sequence of
dependency relation types on the dependency path leading from W1 to W2; Embedding layers for words,
POS and dependency relations are shown separately for clarity, but are shared throughout the network.

predicting relation types. Outputs of these classi-
fiers may be inconsistent. E.g., if PER-SOC rela-
tion is predicted by the local relation classifier for
an entity pair and the local entity classifier predicts
entity type as ORG for one of the entity mentions,
then there is an inconsistency. Because PER-SOC
relation can only exist between two PER entity
mentions. Such domain knowledge can be easily
incorporated in the form of first-order logic rules
in MLN. For each sentence, predictions of individ-
ual classifiers are represented in an MLN as first-
order logic rules where weights of these rules are
proportional to the prediction probabilities. The
consistency constraints among the relation types
and entity types can be represented in the form of
first-order logic rules with infinite weights. Now,

the inference in such an MLN generates a globally
consistent output with maximum weighted satisfi-
ability of the rules.

AWP-NN is a single joint model which cap-
tures boundaries of mentions, their types and re-
lations among them. As the same parameters are
shared for all entity as well as relation type pre-
dictions, we expect the model to learn dependen-
cies among relation and entity types. However, as
it makes separate predictions for each word pair,
there might be some inconsistencies among the la-
bels as described above. We adopt the MLN-based
approach of Pawar et al. (2016) for handling these
inconsistencies and generate a globally consistent
output. For this adoption, we consider the AWP-
NN predictions for the words pairs where a word

822



Domains:
Let N be the number of words in the sentence in consideration.

word = {1, 2, · · · , N}, etype = {PER, ORG, LOC, GPE, WEA, FAC, V EH, OTH}
rtype = {EMPORG, GPEAFF, OTHERAFF, PERSOC, PHY S, ART, NULL, WEM}

Evidence Predicates:
ET (word, etype) : AWP-NN predictions for word pairs 〈Wi, Wj〉 where i = j
RT (word, word, rtype) : AWP-NN predictions for word pairs 〈Wi, Wj〉 where i < j

Query Predicates:
ETFinal(word, etype) : Global predictions for word pairs 〈Wi, Wj〉 where i = j
RTFinal(word, word, rtype) : Global predictions for word pairs 〈Wi, Wj〉 where i < j

Some examples of generic rules:

RTFinal(x, y, EMPORG) ∧ ETFinal(x, PER)⇒ (ETFinal(y, ORG) ∨ ETFinal(y, GPE)).
RTFinal(x, y, PERSOC)⇒ (ETFinal(x, PER) ∧ ETFinal(y, PER)).

Table 4: Domains and Predicates used for constructing MLN for any given sentence

is paired with itself (diagonal entries in table 3),
as entity type predictions. Whereas all other word
pairs where a word is paired with any subsequent
word in the sentence, are considered as relation
type predictions. Table 4 describes the domains
and predicates required for generating an MLN for
any given sentence. Unlike Pawar et al. (2016)
which considers all predicted mentions in their
entity domain, we consider all words in our word
domain. But to keep the size of the MLN in check,
we keep only those words in the word domain
which are part of interesting word pairs. A word
pair is an interesting word pair, if it can potentially
represent a relation i.e. if AWP-NN model assigns
a probability more than some threshold (say 0.01)
for any non-NULL relation type. All the generic
rules (with infinite weights) described in (Pawar et
al., 2016) are used for imposing constraints among
the relation and entity types. Also, we added fol-
lowing additional generic rules for specifying con-
straints for our WEM relation type, which cap-
tures information about mention boundaries.

RTFinal(x, y,WEM)⇒
(ETFinal(x, OTH) ∨ ETFinal(y,OTH)).

RTFinal(x, y,WEM)⇒
(!ETFinal(x, OTH)∨!ETFinal(y,OTH)).
By definition, the WEM relation holds between a
head word of an entity mention and other words of
that entity mention. Additionally, head word of an
entity mention is labelled with appropriate entity

label and other words are labelled with entity type
OTH. The above rules state that if there is WEM
relation between two words x and y then at least
one of them should have label OTH and at least
one of them should have entity type label, i.e. a
label from domain etype other than OTH.

Similarly, all the sentence-specific rules (with
finite weights proportional to AWP-NN predic-
tion probabilities) described in (Pawar et al., 2016)
are also generated for representing predictions of
the AWP-NN model. We use Constant Multiplier
(CM) as the weights assignment strategy. Follow-
ing rule would be generated for each entity type E
(from etypes) for each word pair 〈Wi, Wi〉, with
the weight 10 · PrAWP−NN (E|〈Wi, Wi〉) where
Emax is the predicted entity type with the highest
probability:

ET (i, Emax)⇔ ETFinal(i, E)
Similarly, following rule would be generated
for each relation type R (from rtypes) for
each word pair 〈Wi, Wj〉, with the weight 10 ·
PrAWP−NN (R|〈Wi, Wj〉) where Rmax is the
predicted relation type with the highest probabil-
ity:

RT (i, Rmax)⇔ RTFinal(i, R)
Using these generic and sentence-specific rules,

an MLN is constructed for each sentence. The best
values of ETFinal and RTFinal (query predi-
cates) for each word pair are obtained by using the

823



inference in this MLN with ET and RT as evi-
dence predicates based on AWP-NN’s predictions.

5 Experimental Analysis

5.1 Dataset

ACE 2004 dataset (Doddington et al., 2004) is the
most widely used dataset5 for reporting relation
extraction performance. We use this dataset to
demonstrate effectiveness of our approach for end-
to-end relation extraction using AWP-NN model
and MLN inference. We perform 5-fold cross-
validation on this dataset where the folds are
formed at the document level. We follow the same
assumptions made by (Chan and Roth, 2011;
Li and Ji, 2014; Pawar et al., 2016), which are
- ignore the DISC relation, do not consider im-
plicit relations (resulting due to intra-sentence co-
references) as false positives and use coarse-level
entity and relation types.
Direction of Relations: Out of 6 coarse-level
relation types that we are considering, we need
not model direction for relation types PER-SOC,
GPE-AFF and ART. Because in case of these rela-
tions, given the entity types of their arguments, the
direction of relation is not necessary or becomes
implicit. As PER-SOC is a social relation between
two PER entity mentions, the direction is not nec-
essary. For GPE-AFF, as entity type of one of the
arguments is always GPE, the direction becomes
implicit. Also, the relation type ART always holds
between an agent (PER, ORG or GPE) and an ar-
tifact (FAC, WEA or VEH), hence the direction
is implicit. Whereas for relation like EMP-ORG
which also represents subsidiary relationship be-
tween two ORG entity mentions, it is important to
model the relation direction explicitly. Consider
following sentence fragments:

• ..the fisheries section of the Gulf
Coast Research Laboratory..

• ..company that owned Road & Track..

Here, EMP-ORG relation exists between ORG
entity mentions fisheries section and Gulf
Coast Research Laboratory. Whereas, EMP-
ORG-R relation holds between that and Road &
Track.

Hence, we consider 9 distinct relation types:
EMP-ORG, EMP-ORG-R, PHYS, PHYS-R,
OTHER-AFF, OTHER-AFF-R, PER-SOC,

5We haven’t yet acquired a more recent ACE 2005 dataset

GPE-AFF and ART. Hence, the overall dataset
contained 4074 instances6 of valid relation types.

5.2 Implementation details

We used Keras (Chollet, 2015) for implementing
our AWP-NN model. The model was trained for
40 epochs using batch size of 64 instances. We
used Dropout (Srivastava et al., 2014) for regu-
larization with probability 0.5 for hidden layers
and 0.1 for embedding layers. We used the tool
Alchemy7 for MLN inference. The value of K
(maximum length of dependency path, see Figure
1) was set to be 4, hence all word pairs having
length of dependency path more than 4 were as-
sumed to have NULL label.

5.3 Results

Table 5 shows the comparative performances
(in terms of micro-F1 measure) for various ap-
proaches. The results are divided in three different
sections:
1. only entity extraction: It includes boundary
identification as well as entity type classification.
2. only relation extraction: It includes relation
type classification for each pair of predicted en-
tity mentions. It is a relaxed version of end-to-
end relation extraction problem where correct re-
lation label for an entity mention pair is counted as
a true positive even if entity types of one or both
the mentions are identified incorrectly.
3. entity+relation extraction: It is end-to-end re-
lation extraction which includes boundary identifi-
cation, entity type classification and relation type
classification. Here, correct relation label for an
entity mention pair is counted as a true positive
only if boundaries and entity types of both the
mentions are identified correctly.

It can be observed in the table 5 that end-to-
end relation extraction performance of our AWP-
NN model is better than all the 4 previous ap-
proaches (Chan and Roth, 2011; Li and Ji, 2014;
Pawar et al., 2016; Miwa and Bansal, 2016) on the
ACE 2004 dataset. However, the AWP-NN+MLN
approach which uses MLN inference to revise
AWP-NN predictions during decoding, achieves
the best performance.

6279 instances of type DISC were not considered. Addi-
tionally, 21 relation instances were not contained in a single
sentence as per our sentence detection algorithm.

7https://alchemy.cs.washington.edu/

824



Approach Entity Extraction Relation Extraction Entity+Relation
P R F P R F P R F

(Chan and Roth, 2011) 42.9 38.9 40.8
(Li and Ji, 2014) 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3
(Pawar et al., 2016) 79.0 80.1 79.5 57.9 45.6 51.0 52.4 41.3 46.2
(Miwa and Bansal, 2016) 80.8 82.9 81.8 48.7 48.1 48.4
AWP-NN 81.1 79.7 80.4 60.3 48.1 53.5 55.6 44.4 49.3
AWP-NN + MLN 81.2 79.7 80.5 61.1 47.9 53.7 56.7 44.5 49.9

Table 5: Performance of various approaches on the ACE 2004 dataset. The numbers are micro-averaged
and obtained after 5-fold cross-validation. Actual folds used by each algorithm may differ.

5.3.1 Statistical Significance
As neural networks are initialized randomly, if we
train a neural network model multiple times, dif-
ferent predictions are obtained each time. Hence,
it is important to establish the statistical signifi-
cance of the performance. We train our AWP-NN
model 30 times independently and obtain 30 dif-
ferent values for precision, recall and F1 score.
The numbers shown in table 5 are average val-
ues over these 30 runs. Also, in order to estab-
lish that the F1 score of AWP-NN model is signif-
icantly higher than the best previous F1 score of
48.4% (by Miwa and Bansal (2016)), we conduct
one tailed one sample t-test. Here, mean and stan-
dard deviation of sample of 30 F1 scores by AWP-
NN are 49.3 and 0.44, respectively. This leads to
p-value of 1.23×10−12, hence establishing the sta-
tistical significance of AWP-NN’s performance.

5.4 Analysis of results

5.4.1 Effect of using MLN
We analyzed the effect of using MLN by ob-
serving the individual sentences where errors of
AWP-NN were being corrected by MLN. As an
example, consider the following sentence:
Lemieux0 rescued1 his2 team3 from4
bankruptcy5 last6 season7 by8
exchanging9 deferred10 salary11
for12 an13 ownership14 stake15 .16

End-to-end relation extraction output produced
by the AWP-NN model for this sentence is shown
in the tables 6 and 7. Only error in this output
is that entity type of the mention team should be
ORG instead of PER as it refers to some profes-
sional team. After MLN inference, the entity type
of team is corrected to ORG. This happens be-
cause of high-confidence EMP-ORG relations be-
tween Lemieux and team and between his and
team. As both Lemieux and his are of type

PER with high confidence, global inference using
MLN8 forces type of team to be ORG to ensure
compatibility of relation and entity types.

Entity Mention Boundaries Entity Type
Lemieux (0, 0) PER
his (2, 2) PER
team (3, 3) PER

Table 6: End-to-end relation extraction output (en-
tity mentions) produced by the AWP-NN model

Entity Mention Pair Relation Type
Lemieux, team EMP-ORG
his, team EMP-ORG

Table 7: End-to-end relation extraction output (re-
lations) produced by the AWP-NN model

The AWP-NN model was able to outperform
(see table 5) all 4 previous approaches without
the help of MLN. One reason behind this may
be that the AWP-NN model itself was sufficient
to learn most of the dependencies among the en-
tity and relation types. However, MLN helped to
improve the performance of AWP-NN by 0.6 F1.
Though considerable improvement was observed
in the precision value, the recall improvement was
not significant. In other words, MLN was ob-
served to be more effective for reducing false pos-
itives than false negatives.

5.4.2 Difficult to identify entities
We observed that for some entity mentions, it
is very difficult to identify their entity types as
the key information required for identification
lies outside the current sentence. Currently, our
approach does not use any information outside the
sentences, such as document level co-reference

8Detailed MLN rules & inference results for this
sentence can be found at: www.cse.iitb.ac.in/
˜sachinpawar/MLN/sentence.html

825



information. Usually these difficult to classify
entity mentions are pronoun mentions. Some
examples are as follows:
1. Though, I think that if they
could stifle the entire peace
process at the moment, then that
is what they’d like to do.
2. It is a partially victory for
both sides.
Here, in the first sentence, it is difficult to iden-
tify (even for humans) whether entity type of
they is PER (e.g. set of leaders) or GPE (e.g.
countries). Also, in the second sentence, entity
type of sides can be any of PER, ORG or GPE
depending on the context. In future, we plan to
capture document level information for correctly
predicting types of such mentions.

6 Related Work

There have been multiple lines of research for
jointly modelling and extracting entities and re-
lations. Integer Linear Programming (ILP) based
approaches (Roth and Yih, 2004; Roth and Yih,
2007) were the earliest ones. Here, various lo-
cal decisions are associated with suitable “cost”
values and they are represented using an integer
linear program. The optimal solution to this inte-
ger linear program provides the best global output.
Another significant lines of research were Prob-
abilistic Graphical Models (Roth and Yih, 2002;
Singh et al., 2013), Card-pyramid parsing (Kate
and Mooney, 2010) and Structured Prediction (Li
and Ji, 2014; Li et al., 2014; Miwa and Sasaki,
2014).

Four previous approaches (Miwa and Sasaki,
2014; Li and Ji, 2014; Pawar et al., 2016; Miwa
and Bansal, 2016) are the most similar to our ap-
proach in the sense that they all address the prob-
lem of end-to-end relation extraction without as-
suming gold-standard entity mention boundaries
like the earlier approaches. Our idea of labelling
“all word pairs” is similar to the table representa-
tion idea of Miwa and Sasaki (2014). The major
difference is that they identify boundaries of men-
tions through BIO encoding of labels whereas we
try to capture boundaries by treating them as an
additional relation type WEM. Also, they perform
structured prediction with beam search to find op-
timal label assignment to the table, whereas we
opt for neural network based classification. The
idea of using MLNs to incorporate domain knowl-

edge and perform joint inference to obtain glob-
ally consistent output was proposed by Pawar et
al. (2016). The current state-of-the-art approach
for end-to-end relation extraction is by Miwa and
Bansal (2016), who employ LSTM-RNN based
model for addressing this problem.

7 Conclusion and Future Work

We proposed a novel approach for end-to-end re-
lation extraction which carries out its all three sub-
tasks (identifying entity mention boundaries, their
entity types and relations among them) jointly by
using a neural network based model. We proposed
a “All Word Pairs” neural network model (AWP-
NN) which reduces solution of these three sub-
tasks to predicting an appropriate label for each
word pair in a given sentence. End-to-end relation
extraction output is then constructed from these la-
bels of word pairs. We further improved output of
the AWP-NN model by using inference in Markov
Logic Networks so that some of the inconsisten-
cies in word pair labels can be removed at the sen-
tence level.

We demonstrated effectiveness of our ap-
proaches (AWP-NN and AWP-NN+MLN) on the
standard dataset of ACE 2004. They outper-
formed all 4 previously reported joint modelling
approaches (Chan and Roth, 2011; Li and Ji, 2014;
Pawar et al., 2016; Miwa and Bansal, 2016) for
end-to-end relation extraction. Since all three sub-
tasks share the same AWP-NN model parameters,
many inter-task dependencies are captured effec-
tively by the AWP-NN itself (without MLN) and
this can be validated by the fact that AWP-NN it-
self performs better than all other joint models.
However, MLN certainly helps to further improve
the end-to-end relation extraction performance by
correcting some errors in predictions of the AWP-
NN model.

In future, we plan to incorporate some addi-
tional features (e.g. document level co-reference
information) in the AWP-NN model for improving
its performance further. Also, deeper analysis of
the errors is required to have a better understand-
ing about which characteristics are better captured
by the AWP-NN model as compared to the MLN
and vice versa. This will help these two to com-
plement each other in a better way.

826



References
Razvan Bunescu and Raymond Mooney. 2005. A

shortest path dependency kernel for relation extrac-
tion. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 724–
731, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.

Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 551–560, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Franois Chollet. 2015. keras. https://github.
com/fchollet/keras.

George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The Automatic Content
Extraction (ACE) Program-Tasks, Data, and Evalu-
ation. In LREC, volume 2, page 1.

Jing Jiang and ChengXiang Zhai. 2007. A system-
atic exploration of the feature space for relation ex-
traction. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 113–120,
Rochester, New York, April. Association for Com-
putational Linguistics.

Rohit J. Kate and Raymond Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the Fourteenth Conference on
Computational Natural Language Learning, pages
203–212, Uppsala, Sweden, July. Association for
Computational Linguistics.

Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 402–412, Baltimore, Maryland, June.
Association for Computational Linguistics.

Qi Li, Heng Ji, Yu HONG, and Sujian Li. 2014.
Constructing information networks using one single
model. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1846–1851, Doha, Qatar, October.
Association for Computational Linguistics.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1105–1116,
Berlin, Germany, August. Association for Compu-
tational Linguistics.

Makoto Miwa and Yutaka Sasaki. 2014. Modeling
joint entity and relation extraction with table repre-
sentation. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1858–1869, Doha, Qatar, October.
Association for Computational Linguistics.

Sachin Pawar, Pushpak Bhattacharyya, and Girish Pal-
shikar. 2016. End-to-end relation extraction using
markov logic networks. In Proceedings of the 17th
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics (CICLing
2016), LNCS 9624. Springer.

Longhua Qian, Guodong Zhou, Fang Kong, Qiaom-
ing Zhu, and Peide Qian. 2008. Exploiting con-
stituent dependencies for tree kernel-based semantic
relation extraction. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 697–704, Manchester, UK,
August. Coling 2008 Organizing Committee.

Matthew Richardson and Pedro Domingos. 2006.
Markov Logic Networks. Machine learning, 62(1-
2):107–136.

Dan Roth and Wen-tau Yih. 2002. Probabilistic rea-
soning for entity & relation recognition. In Proceed-
ings of the 19th international conference on Compu-
tational linguistics-Volume 1, pages 1–7. ACL.

Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Hwee Tou Ng and Ellen Riloff, ed-
itors, HLT-NAACL 2004 Workshop: Eighth Confer-
ence on Computational Natural Language Learning
(CoNLL-2004), pages 1–8, Boston, Massachusetts,
USA, May 6 - May 7. Association for Computa-
tional Linguistics.

Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a linear pro-
gramming formulation. Introduction to statistical
relational learning, pages 553–580.

Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping
Zheng, and Andrew McCallum. 2013. Joint infer-
ence of entities, relations, and coreference. In Pro-
ceedings of the 2013 workshop on Automated knowl-
edge base construction, pages 1–6. ACM.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’05), pages 427–434, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.

827


