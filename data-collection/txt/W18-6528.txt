



















































LSTM Hypertagging


Proceedings of The 11th International Natural Language Generation Conference, pages 210–220,
Tilburg, The Netherlands, November 5-8, 2018. c©2018 Association for Computational Linguistics

210

LSTM Hypertagging

Reid Fu
Department of Computer Science

The Ohio State University
fu.349@buckeyemail.osu.edu

Michael White
Department of Linguistics
The Ohio State University
mwhite@ling.osu.edu

Abstract

Hypertagging, or supertagging for surface
realization, is the process of assigning lex-
ical categories to nodes in an input seman-
tic graph. Previous work has shown that
hypertagging significantly increases real-
ization speed and quality by reducing the
search space of the realizer. Building on
recent work using LSTMs to improve ac-
curacy on supertagging for parsing, we de-
velop an LSTM hypertagging method for
OpenCCG, an open source NLP toolkit
for CCG. Our results show significant im-
provements in both hypertagging accuracy
and downstream realization performance.

1 Introduction

Hypertagging, or supertagging for surface real-
ization, is the process of assigning lexical cate-
gories to nodes in a semantic graph as a step in
grammar-based surface realization. It significantly
increases realization speed and quality by reduc-
ing the search space of the realizer. Espinosa
et al. (2008) built the original single-stage maxi-
mum entropy hypertagger for CCG in OpenCCG,1

which obtained a single-best hypertagging accu-
racy of 93.6%. This hypertagger was later ex-
panded into a two-stage model, which obtained a
hypertagging accuracy of 95.1%.

Recent work has used neural networks to
make significant improvements in supertagging
for parsing (i.e., predicting lexical categories for
a sequence of words), improving upon earlier
work with maximum entropy supertagging (Cur-
ran et al., 2006). Lewis and Steedman (2014) used
a feedforward neural network to obtain 91.3% cat-
egory accuracy in the CCGBank (Hockenmaier

1http://openccg.sf.net

and Steedman, 2007) development section (Sec-
tion 00). Lewis et al. (2016) improved on this re-
sult using an LSTM, obtaining 94.9% category ac-
curacy on the development section and 94.7% on
the test section (Section 23) of the CCGBank.

Our work uses techniques from Lewis et al.
(2016) to implement an improved hypertagger. To
do so, we first linearize the input graph using
a method adapted from Konstas et al.’s (2017)
approach to generating from Abstract Mean-
ing Representations (AMRs).2 Unlike in their
work though, we found that the input ordering
method substantially impacted hypertagging accu-
racy, with an English-like ordering yielding sub-
stantial improvements over random ordering while
substantially trailing oracle ordering.

We evaluated the LSTM hypertagger on both
tagging accuracy and its downstream effect on re-
alization performance. Our results show signif-
icant improvement over the original hypertagger
on both. We obtained 96.47% on tagging accu-
racy (up from 95.1%) and an increase in realiza-
tion BLEU scores from 0.8429 with the original
hypertagger to 0.8683 with the neural hypertag-
ger. As expected, the LSTM hypertagger yielded
large gains in accuracy on the difficult cases of un-
seen predicates and predicates not seen with the
gold tag in training, helping to achieve a 7.8% in-
crease in sentences with grammatically complete
derivations. A human evaluation confirmed that
using the LSTM hypertagger yielded significant
improvements in adequacy and fluency, especially
in cases where the LSTM hypertagger was essen-
tial for obtaining a complete derivation.

This paper is structured as follows. Sec-
tion 2 provides background on surface realization
with CCG, the maximum entropy hypertagger and
LSTM supertagging. Section 3 describes our fea-

2https://amr.isi.edu/

http://openccg.sf.net
https://amr.isi.edu/


211

tures and model along with our approach to input
linearization. The results and analysis appear in
Section 4. Related work is discussed in Section 5,
including where grammar-based realization stands
in the current research landscape. Section 6 con-
cludes.

2 Background

2.1 Surface Realization with OpenCCG

The OpenCCG realizer generates surface strings
for input semantic dependency graphs (or logi-
cal forms) using a chart-based algorithm (White,
2006) for Combinatory Categorial Grammar
(Steedman, 2000) together with a hypertagger for
probabilistically assigning lexical categories to
lexical predicates in the input, as noted above.
An example input appears in Figure 1. In the
figure, nodes correspond to discourse referents
labeled with lexical predicates, and dependency
relations between nodes encode argument struc-
ture; gold standard CCG lexical categories (i.e,
what the hypertagger learns to predict) are also
shown. Note that semantically empty function
words such as infinitival-to are missing. Gener-
ally speaking, the semantic dependency graphs are
more abstract than unordered dependency trees,
but more detailed than AMRs. The grammar is
extracted from a version of the CCGbank (Hock-
enmaier and Steedman, 2007) enhanced for re-
alization, where the enhancements include: bet-
ter analyses of punctuation (White and Rajkumar,
2008); less error prone handling of named entities
(Rajkumar et al., 2009); re-inserting quotes into
the CCGbank; and assignment of consistent se-
mantic roles across diathesis alternations (Boxwell
and White, 2008), using PropBank (Palmer et al.,
2005).

As in other work with OpenCCG (e.g., Duan
and White, 2014), we use OpenCCG’s realization
ranking model off the shelf in order to select pre-
ferred outputs from the chart; in particular, we use
White & Rajkumar’s (2009; 2012) averaged per-
ceptron realization ranking model augmented with
a large-scale 5-gram model based on the Giga-
word corpus. The ranking model makes choices
addressing all three interrelated sub-tasks tradi-
tionally considered part of the surface realization
task in natural language generation research (Re-
iter and Dale, 2000): inflecting lemmas with gram-
matical word forms, inserting function words and
linearizing the words in a grammatical and natural

aa1

he
h3

he
h2

<Det>

<Arg0>
<Arg1>

<TENSE>pres

<NUM>sg

<Arg0>

w1
want.01

m1

<Arg1>

<GenRel>

<Arg1>

<TENSE>pres

p1point

h1
have.03

make.03

<Arg0>

s[b]\np/np

np/n

np

n

s[dcl]\np/np

s[dcl]\np/(s[to]\np)

np

Figure 1: Example OpenCCG semantic depen-
dency input for he has a point he wants to make,
with gold standard lexical categories for each node

order.
Notably, to improve word ordering decisions,

White & Rajkumar (2012) demonstrated that in-
corporating a feature into the ranker inspired by
Gibson’s (2000) dependency locality theory can
deliver statistically significant improvements in
automatic evaluation scores, better match the dis-
tributional characteristics of sentence orderings,
and significantly reduce the number of serious or-
dering errors. With function words, Rajkumar
and White (2011) showed that they could improve
upon the earlier model’s predictions for when to
employ that-complementizers using features in-
spired by Jaeger’s (2010) work on using the princi-
ple of uniform information density, which holds
that human language use tends to keep informa-
tion density relatively constant in order to opti-
mize communicative efficiency.

Finally, to reduce the number of subject-verb
agreement errors, Rajkumar and White (2010) ex-
tended the earlier model with features enabling it
to make correct verb form choices in sentences
involving complex coordinate constructions and
with expressions such as a lot of where the correct
choice is not determined solely by the head noun.
They also improved animacy agreement with rela-
tivizers, reducing the number of errors where that
or which was chosen to modify an animate noun
rather than who or whom (and vice-versa), while
also allowing both choices where corpus evidence
was mixed.



212

2.2 Original Hypertagger

The original MaxEnt hypertagger uses three gen-
eral types of features from logical forms: lexical
features, graph structural features, and node at-
tribute features. Lexical features are the words as-
sociated with the logical form nodes. Graph struc-
tural features are those pertaining to word depen-
dency relations, and include the number of chil-
dren (and argument children) of each node as well
as the names of dependency relations. Node at-
tribute features are those pertaining to semantic or
syntactic features of words, and include tense and
number.

The published single-stage MaxEnt hypertagger
has an intermediate stage in which it predicts POS
tags. These POS tags are then included in the fea-
ture set used to predict supertags. The two-stage
hypertagger stacks on an additional stage in which
predicted supertags in the local graph context are
used as features for making final predictions.

The realizer uses the hypertagger in an iterative
β-best algorithm in which the realizer repeatedly
queries the hypertagger for β-best tags. The hy-
pertagger has a list of β-values sorted from most
restrictive to least. It first returns a β-best list of
supertags for the most restrictive beta. If the re-
alizer fails to find a complete realization with the
returned supertags, it asks for the supertags associ-
ated with the next most restrictive beta, and so on
until either a complete realization is found, time
runs out, or there are no more betas in the list.

2.3 LSTM Supertagger

The model from Lewis et al. (2016) is summarized
in Figure 2. Start and end tokens are added to
each sentence. Each word in each sentence (in-
cluding each start and end token) is mapped to a
50-element word embedding. Word embeddings
are initialized using pre-trained word embeddings
from Turian et al. (2010). Embeddings for fea-
tures of the word are concatenated to the word’s
50-element embedding. The concatenated embed-
dings are used as input to a stacked, bi-directional
LSTM with depth 2. Lewis et al. used 1-4 charac-
ter prefixes and suffixes as their features.

The LSTM cell used is a variant that has cou-
pled input and forget gates. If a cell is at position
t, we refer to the cell at t-1 as the previous cell and
the cell at t+1 as the next cell. Each cell takes cell
state ct−1 and hidden state ht−1 from the previ-
ous cell, and xt from the previous layer, passing ct

Figure 2: LSTM model used in Lewis et al.
(2016) and our hypertagger (image from Lewis
et al.). Concatenated embedding representations
of each word are passed to stacked, bi-directional
LSTM that reads sentence in both directions. Out-
puts of the directional LSTM’s are combined. Ap-
plying a softmax over the combined outputs yields
the probability distributions over supertags.

and ht to the next cell, and ht to the next layer. ct
and ht are calculated as follows, with * indicating
component-wise product:

it = σ(Wi[ct−1, ht−1, xt] + bi) (1)

c̃t = tanh(Wc[ht−1, xt] + bc̃) (2)

ot = σ(Wo[c̃t, ht−1, xt] + bo) (3)

ct = it ∗ c̃t + (1− it)ct−1 (4)
ht = ot ∗ tanh(ct) (5)

The model is trained using stochastic gradient
descent with minibatch size of 1, learning rate of
0.01, and momentum of 0.7. The input layer has
a dropout probability of 0.5. Order of sentences
is shuffled after every epoch. We retained these
settings for our hypertagger experiments.3

The outputs of the LSTM are passed to another
hidden layer, a bias is added, and a rectified lin-
ear function is applied. The result is a set of log-
its that give the probability distributions over su-
pertags for each word when passed to a softmax.

3 Approach

3.1 Features

We use a subset of the features from the original
MaxEnt hypertagger (shown below). If a node has
fewer than five parent or child relations, the val-

3As an anonymous reviewer points out, the training would
be faster with a larger minibatch size, and inference would
be faster using highly optimized implementations of standard
LSTM cells.



213

ues of the missing relations were set to the empty
string. This kept the size of the input constant.

• Lexical Features: Lemmatized words asso-
ciated with elementary predication nodes

• Node Attribute Features: Named entity cat-
egory, determiner, mood, number, particle,
tense

• Graph Structural Features: Number of
children, number of argument children,
names of 5 parent relations, names of 5 child
relations

Our hypertagger does not include a stage for
predicting POS tags or for using the predicted su-
pertags as features in a stacked model. The re-
current nature of the bi-LSTMs means that the
node-level representations informing supertag pre-
diction can be propagated to nearby nodes in the
model.

Features were extracted from logical forms cor-
responding to Section 00 and Sections 02 to 21.
Section 00 was used as the development set, and
has 1883 sentences and 36,247 nodes. Sections 02
to 21 were used as the training set, and have a total
of 35,765 sentences and 680,705 nodes.

3.2 Input Linearization
Each logical form was converted to a sequence of
nodes, with each node containing all features dis-
cussed in the last sub-section. We refer to this con-
version process as input linearization. Early ex-
periments showed that the order of nodes in the
linearized sequence made a significant difference
in accuracy. Oracle linearization, in which nodes
are ordered according to the order of words in the
original sentence, outperformed random lineariza-
tion by about 5%. Depth-first search lineariza-
tion, in which the parent node came before all
the child nodes and the child nodes were in ran-
dom order relative to each other, improved over
random linearization by about 2%. These exper-
iments showed that more English-like input lin-
earizations led to higher performance.

To approximate English ordering, we developed
a heuristic of how nodes and their children should
be ordered. The heuristic was applied recursively
from the root node(s) of the logical form down to
the leaves. The child ordering is shown below.
The heuristic performed around 3% higher than
random linearization, and around 2% lower than
oracle linearization.

1. Determiner (Det) child

2. Possessor (GenOwn) child

3. First argument (Arg0)

4. One or two word modifiers

5. Parent node

6. Remaining arguments in order of argument
number (Arg1, ..., Arg5)

7. Remaining children sorted ascending by sub-
tree size. If two child subtrees have the same
size, the one with a lesser sum of predicate
name lengths comes before the other.

Konstas et al. (2017) were able to improve their
model by adding parentheses and relation names
into their AMR linearization sequence. We ex-
perimented with both. Adding parentheses around
subsequences corresponding to subtrees of size
five or more consistently resulted in an improve-
ment of 0.2%. Adding relation names to the lin-
earization sequence resulted in either no improve-
ment or a slight decrease in accuracy.

3.3 Model

We adapted the bi-LSTM model from Lewis et al.
(2016) to use our features and list of possible su-
pertags. Each feature value was mapped to a fea-
ture embedding. Embeddings for lexical features
were initialized using pre-trained word embed-
dings. Embeddings for other features were ran-
domly initialized 8-element vectors. The embed-
dings for other features were concatenated onto
the embeddings for lexical features, and the con-
catenated embeddings were used as input to the
bi-LSTM.

We experimented with two sets of pre-trained
word embeddings: those that were used in Lewis
et al., which were of size 50, and a custom
set, which were of size 100. Early experiments
showed that performance was similar for both sets.
Later experiments exclusively used the set used in
Lewis et al.

Sentences with linearized sequences longer than
180 tokens (nodes and parentheses) were filtered
out of the training and development sets. This fil-
ter removed one sentence from the development
set, leaving it with 1882 sentences and 36,103
nodes.



214

Our list of possible supertags was originally of
size 1210. We experimented with filtering the
tags: tags that occurred fewer than 5 times in the
training set were not listed as possible supertags.
Instead, when a rare tag is encountered, it was re-
placed with the unknown tag and the associated
word was counted as wrong in the accuracy. We
found that filtering tags boosted performance. Our
filtered list of possible supertags is of size 528.
There were 1543 words in the training set and 88
words in the development set with gold tags fil-
tered out.

The model was trained on the training set, and
evaluated on the development set every 2 minutes
during training. Training was run until either 7
hours had elapsed, or accuracy had not improved
for 30 evaluations. Typically, training ran between
30 and 36 epochs.

3.4 Realization

The realizer is implemented in Java, while the
LSTM model is implemented in Python. To es-
tablish communication between the realizer and
the hypertagger, we used a server thread to run
the LSTM model. The realizer got supertags from
the hypertagger by running a client thread, then
parsing the results returned by it. The LSTM hy-
pertagger was otherwise used in the same way as
the original MaxEnt hypertagger, with the realizer
calling the hypertagger in an iterative β-best algo-
rithm.

4 Results and Discussion

The LSTM hypertagger was tested against the
original, baseline hypertagger on both tagging ac-
curacy and downstream effect on realizer perfor-
mance. Tagging accuracy was evaluated on Sec-
tion 00, and realizer performance was evaluated on
Sections 00 and 23. The lists of β-values for the
LSTM hypertagger and baseline two-stage Max-
Ent hypertagger were adjusted so that both would
on average return about the same number of tags
per word as the published single-stage MaxEnt hy-
pertagger at each corresponding β-level. Section
00 was used to tune the list of β-values for the
LSTM hypertagger.

4.1 Hypertagging Accuracy

The comparison in hypertagging accuracy is sum-
marized in Table 1. In comparison to previously
published results, the LSTM hypertagger achieves

a nearly 3% absolute increase in single-best tag-
ging accuracy, and achieves 99% accuracy at a
multitagging level of only 1.2 tags per predicate,
in comparison to 3.2 tags per predicate previously.
Single-best tagging accuracy for the LSTM hyper-
tagger had a mean of 96.384% and a variance of
0.031% (statistics taken over the six most recent
runs). Figures in Table 1 are for one of the higher-
performing runs.

We were interested in how the hypertaggers
would perform on hard cases such as predicates
that appeared in the development set but not in
training, as well as predicates that appeared in
training, but not with the correct tag in the de-
velopment set. On these hard cases, our model
obtained accuracies of 98.69% and 80.13%, re-
spectively, while the two-stage MaxEnt hypertag-
ger obtained accuracies of 94.58% and 69.96%,
respectively. These improvements represent very
large respective reductions in error of 76% and
34%. These results are summarized in Table 2.

The difference in accuracy between the LSTM
hypertagger with English-like input linearization
and the LSTM hypertagger with oracle input lin-
earization is largely attributed to imperfections in
the English-like linearization. There were many
cases in which the English-like input lineariza-
tion reversed the order between two phrases (e.g.
take these events place ago years, you but have
to recognize) or between two words in a phrase
(e.g. three times than more, among of us those).
In many of these cases, the hypertagger still pre-
dicted the correct tags despite the word order
switches. For the words that the hypertagger did
not tag correctly, when the word order switch was
still grammatical, the assigned supertag was often
similar enough to the gold tag to not significantly
impact realization. By contrast, when the word
order switch was ungrammatical, the assigned su-
pertag was sometimes very different from the gold
tag, which impacted realization negatively.

4.2 Realization Performance

The comparison in realization performance is
summarized in Table 3. Using the LSTM hyper-
tagger, the realizer obtained complete derivations
for more than 6% more realizations (more than
100 more logical forms) in both the development
and test sections. The increase in the number of
complete derivations helped achieve a more than
2.5% absolute increase in BLEU scores for both



215

Tags
Pred

LSTM MaxEnt1
β Accuracy β Accuracy

1 1.0 96.5 1.0 93.6
1.1 0.13 98.5 0.16 95.8
1.2 0.04 99.0 0.05 96.6
1.5 7e-3 99.4 5.8e-3 97.9
1.8 2.8e-3 99.5 1.75e-3 98.4
2.2 1.27e-3 99.6 6.25e-4 98.7
3.2 3.65e-4 99.7 1.25e-4 99.0
3.9 2e-4 99.7 5.8e-5 99.1

Table 1: Comparison of tagging accuracies between LSTM hypertagger and published single-stage
MaxEnt hypertagger on the development section (Section 00) of CCGBank. Results (in percentages) are
for per-predicate tagging accuracies.

LSTM MaxEnt2
Unseen Predicates 98.69 94.58

Unseen Predicate-Tag Pairs 80.13 69.96

Table 2: Comparison of tagging accuracies between LSTM hypertagger and unpublished two-stage
MaxEnt hypertagger on hard cases in the development set, namely predicates that are not seen in training
and predicates that are seen in training but not with correct supertag.

sections.

Most of the incomplete or suboptimal realiza-
tions made with the LSTM hypertagger occurred
due to one or more of the following reasons. Some
sentences had several words in which the correct
tag was in the list of β-best tags for the least re-
strictive beta, but had a low probability. This
caused the realizer to time out due to the large
size of the search space. Others had a single
word in which the correct tag was not in the β-
best tag list for the least restrictive beta. Instead
there were many tags in the list that were simi-
lar to the gold tag, but that did not allow a com-
plete realization. For example, the word recog-
nize in the sentence but you have to recognize that
these events took place 35 years ago had a gold
tag of s[b]\np/s[em] (a bare verb subcategorizing
for an embedded clause), but had a single-best as-
signed tag of s[b]\np (a bare intransitive verb),
making it impossible to derive a constituent con-
taining the embedded clause. These errors were
likely caused by imperfections in the English-like
input linearization. For the example above, the in-
put linearization placed recognize at the end of the
sequence, with no subsequent predicates to help
predict the gold tag. Another reason for incom-
plete or suboptimal realizations is that the hyper-

tagger can’t predict certain low-frequency tags, so
some words will not have the correct tag in their
β-best tag list regardless of the β value.

4.3 Human Evaluation

We also did a targeted human evaluation to de-
termine whether the improvements in hypertag-
ging accuracy generally led to noticeable improve-
ments in realization quality, especially where it en-
abled a complete realization to be found.4 Our
procedure was as follows. We randomly chose
100 sentences from the devset where the realiza-
tions differed between using the LSTM hypertag-
ger and the baseline two-stage MaxEnt hypertag-
ger. 50 of the sentences were ones for which
one system got a complete realization but the
other did not—where we expected to find sub-
stantial differences—while the other 50 were ones
for which either both systems got a complete re-
alization or both did not get a complete realiza-
tion. We generated a spreadsheet with the fol-
lowing columns: Reference sentence, Realization
A, Realization B, Adequacy, and Fluency. Which
system was A for each sentence was randomized,

4The complete set of examples and the evalua-
tion script are available in the following supplement to
the paper: https://osf.io/a4czs/?view_only=
b7a0a49046a0408bb844ff7ea63c4e08

https://osf.io/a4czs/?view_only=b7a0a49046a0408bb844ff7ea63c4e08
https://osf.io/a4czs/?view_only=b7a0a49046a0408bb844ff7ea63c4e08


216

Section
LSTM MaxEnt2

BLEU Complete Exact BLEU Complete Exact
00 0.8783 90.38 49.36 0.8458 83.86 45.62
23 0.8683 87.75 48.35 0.8429 81.69 44.16

Table 3: Comparison in realization performance between systems using the LSTM and two-stage Max-
Ent hypertaggers. The ‘Complete’ column indicates the percentage of logical forms realized with a com-
plete (non-fragmentary) derivation, while the ‘Exact’ column indicates the percentage of realizations that
exactly matched the reference sentence.

as was the order of sentences. A separate key
file kept track of which system was A for each
sentence, and which sentence belonged to which
of the above sets of 50. Two linguists who had
no familiarity with the research evaluated the re-
alizations, marking in the Adequacy and Fluency
columns whether they believed A or B was better
in adequacy and fluency, respectively, or whether
they were equally good for one or both measures.
Raw agreement was relatively high, with the two
judges agreeing on adequacy 70% of the time and
fluency 73% of the time. However, nearly all
the disagreements involved cases where only one
judge found the pair of realizations to be the same
on adequacy or fluency; on the subset of items
where neither judge found the pair to be equal,
agreement was 96% for adequacy and 95% for flu-
ency.

The results of the human evaluations are sum-
marized in Table 4. For the sentences where only
one system produced a complete realization (Set
1), the LSTM hypertagger system outperformed
the baseline one most of the time on both ade-
quacy and fluency. For the other sentences (Set
2), the two systems were mostly tied on adequacy
and fluency, but when the realizations were of dis-
tinct quality, the LSTM hypertagger system usu-
ally outperformed the original one. All differ-
ences in the counts of Better/Worse judgments
were highly significant (p < 0.001, sign test).

Examples of the changes yielded by the LSTM
hypertagger appear in Table 5, where the first two
examples improve both adequacy and fluency, the
next example makes adequacy and fluency worse,
and the final one leaves adequacy and fluency the
same. With wsj 0080.21, it seems that the two-
stage MaxEnt system failed to match the subject
with the verb, yielding a realization where respond
doesn’t have a subject and them is not clearly
linked with its antecedent. With wsj 0004.8, the
LSTM system switched yields and nevertheless,

making a realization that’s different from the orig-
inal sentence, but still grammatical. The two-stage
MaxEnt system seemed to have trouble combining
the words yields, nevertheless, and may, making a
realization that gives an awkward order for these
words and splits the sentence with the phrase said
Brenda Malizia Negus at an awkward place; more-
over, with may appearing initially, the sentence
can be read as a wish rather than a declarative
statement. With wsj 0097.19, the LSTM system
incorrectly inverts the main subject and verb, and
makes several other word order mistakes. Finally,
wsj 0037.9 is an example where leaving out the
complementizer that or the contraction does not
substantially affect adequacy or fluency (though
the reference sentence arguably makes the best
choices here). More generally, while the choice
whether to include a that-complementizer occa-
sionally made a crucial difference, they were a
frequent source of insubstantial differences, along
with contractions and adverbial placement. There
were also cases where both realizations made dis-
tinct but important mistakes that yielded equally
bad realizations.

5 Related Work

Hypertagging can potentially benefit other
grammar-based methods using lexicalized gram-
mars, e.g. using HPSG (Velldal and Oepen, 2005;
Carroll and Oepen, 2005; Nakanishi et al., 2005)
or TAG (Gardent and Perez-Beltrachini, 2017).
Much recent work in NLG (Wen et al., 2015;
Dušek and Jurcicek, 2016; Mei et al., 2016; Kid-
don et al., 2016; Konstas et al., 2017; Wiseman
et al., 2017) has made use of neural sequence-
to-sequence methods for generation rather than
grammar-based methods. The learning flexibility
of neural methods make it possible to develop
very knowledge lean systems, but they continue
to suffer from a tendency to hallucinate content
and have not been used with texts exhibiting



217

Set
Adequacy Fluency

Better Same Worse Better Same Worse
1 (±complete) 84 12 4 88 7 5
2 (=complete) 31 62 7 37 52 11

Table 4: Results (counts of judgments) of human evaluations of realizations, which indicate how often
the new system produced better, same, and worse realizations for the given aspect. Set 1 is the set of
sentences in which one system had a complete realization while the other did not. Set 2 is the set of
sentences in which either both systems had complete realizations or both did not.

wsj 0080.21 it was n’t clear how NL and Mr. Simmons would respond if Georgia Gulf spurns them again .
LSTM [same]
MAXENT2 it was n’t clear how to would respond if Georgia Gulf spurns them again NL and Mr. Simmons .

wsj 0004.8 nevertheless , said Brenda Malizia Negus , editor of Money Fund Report , yields may blip up again before
they blip down because of recent rises in short-term interest rates .

LSTM yields nevertheless may blip up again before they blip down because of recent rises in short-term interest
rates , said Brenda Malizia Negus , editor of Money Fund Report .

MAXENT2 may nevertheless yields , said Brenda Malizia Negus , editor of Money Fund Report , again blip up
before they blip down because of recent rises in short-term interest rates .

wsj 0097.19 -lrb- Morgan Stanley last week joined a growing list of U.S. securities firms that have stopped doing index
arbitrage for their own accounts . -rrb-

LSTM last week joined Morgan Stanley . U.S. securities firms that growing a list of has stopped doing index
arbitrage for their own accounts

MAXENT2 -lrb- Morgan Stanley last week joined a growing list of U.S. securities firms that have stopped doing
index arbitrage for their own accounts .

wsj 0037.9 if “ a Wild Sheep Chase ” carries an implicit message for international relations , it ’s that the Japanese
are more like us than most of us think .

LSTM if “ a Wild Sheep Chase ” carries an implicit message for international relations , it ’s the Japanese are
more like us than most of us think .

MAXENT2 if “ a Wild Sheep Chase ” carries an implicit message for international relations , it is that the Japanese
are more like us than most of us think .

Table 5: Examples of devset sentences where the LSTM hypertagger improved adequacy/fluency (top),
made it worse (middle) or left it the same (bottom).

the full complexity of genres such as news text.
Approaches based on dependency grammar (Guo
et al., 2008; Bohnet et al., 2010, 2011; Zhang
and Clark, 2015; Liu et al., 2015; Puduppully
et al., 2016, 2017; King and White, 2018) are
also simpler than constraint-based grammar ap-
proaches, making them more robust to unexpected
inputs and easier to deploy across languages, but
it is difficult to determine whether they can fully
substitute for precise grammars because these
approaches have not used compatible inputs.

Although approaches using constraint-based
grammars are clearly more difficult to imple-
ment and deploy, there is some evidence that they
are beneficial for parsing, while for realization
the question remains largely open. For parsing,
Buys and Blunsom (2017) have recently shown
that even though their incremental neural seman-
tic graph parser substantially outperforms standard

attentional sequence-to-sequence models, it still
lags 4-6% behind an HPSG parser using a sim-
ple log-linear model (Toutanova et al., 2005) on
a variety of parsing accuracy measures on Deep-
Bank (Flickinger et al., 2012), a conversion of the
Penn Treebank to Minimal Recursion Semantics
(Copestake et al., 2005, MRS). The MRS repre-
sentations in DeepBank are qualitatively similar to
the OpenCCG semantic graphs used in this work,
which are again qualitatively similar to the deep
representations used in the First Surface Realiza-
tion Shared Task (Belz et al., 2010, 2011). On
the deep shared task representations, Bohnet et al.
(2011) achieved a BLEU score of 0.7943, which
Puduppully et al. (2017) later improved upon with
a score of 0.8077. These scores are substantially
lower than our BLEU score of 0.8683 reported
here, though since the inputs are not exactly the
same, the BLEU scores are of course not directly



218

comparable.
Given the flexibility of neural methods, it would

be interesting in future work to examine how well
neural sequence-to-sequence generation methods
would fare in a direct, head-to-head comparison
using the kinds of detailed, deep inputs used with
HPSG and CCG. To the extent that neural ap-
proaches continue to hallucinate content and fail to
observe constraints and preferences implemented
by grammar-based approaches in such a compari-
son, it would also be worthwhile to investigate ad-
ditional ways of combining neural and grammar-
based methods.

6 Conclusion

We have implemented a new LSTM hypertag-
ger that significantly outperforms the existing
OpenCCG hypertagger on both tagging accuracy
and its downstream effect on realization perfor-
mance. Since we have observed that the order in
which input nodes are linearized substantially af-
fects tagging accuracy, in future work we would
like to explore whether graph-based neural tagging
methods could yield further improvements in per-
formance. Another direction of interest is explor-
ing ways of incorporating hypertagging into ar-
chitectures that synergistically combine grammar-
based and neural generation methods.

Acknowledgments

We thank the OSU Clippers Group, Alan Ritter
and the anonymous reviewers for helpful com-
ments and discussion, and Sarah Ewing and Amad
Hussain for their assistance with the evaluation.
This work was supported in part by NSF grant IIS-
1319318.

References

Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 217–226. Association for Computational Lin-
guistics.

Anja Belz, Mike White, Josef van Genabith, Deirdre
Hogan, and Amanda Stent. 2010. Finding common
ground: Towards a surface realisation shared task.
In Proceedings of INLG-10, Generation Challenges,
pages 267–272.

Bernd Bohnet, Simon Mille, Benoı̂t Favre, and Leo
Wanner. 2011. Stumaba : From deep representa-
tion to surface. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation,
pages 232–235. Association for Computational Lin-
guistics.

Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level re-
alizer. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 98–106. Coling 2010 Organizing Com-
mittee.

Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-
08.

Jan Buys and Phil Blunsom. 2017. Robust incre-
mental neural semantic graph parsing. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1215–1226. Association for Computa-
tional Linguistics.

John Carroll and Stefan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proc. IJCNLP-05.

Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics: An
introduction. Research on language and computa-
tion, 3(2-3):281–332.

James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proc. COLING-ACL ’06.

Manjuan Duan and Michael White. 2014. That’s Not
What I Meant! Using Parsers to Avoid Structural
Ambiguities in Generated Text. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 413–423, Baltimore, Maryland. Asso-
ciation for Computational Linguistics.

Ondřej Dušek and Filip Jurcicek. 2016. Sequence-to-
sequence generation for spoken dialogue via deep
syntax trees and strings. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
45–51. Association for Computational Linguistics.

Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183–191, Columbus, Ohio. Association for
Computational Linguistics.

Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank. A dynamically annotated treebank of the
Wall Street Journal. In Proceedings of the 11th In-
ternational Workshop on Treebanks and Linguistic
Theories, pages 85–96.

http://www.aclweb.org/anthology/W11-2832
http://www.aclweb.org/anthology/W11-2832
http://www.aclweb.org/anthology/W11-2832
http://www.aclweb.org/anthology/W11-2835
http://www.aclweb.org/anthology/W11-2835
http://www.aclweb.org/anthology/C10-1012
http://www.aclweb.org/anthology/C10-1012
http://www.aclweb.org/anthology/C10-1012
https://doi.org/10.18653/v1/P17-1112
https://doi.org/10.18653/v1/P17-1112
http://www.aclweb.org/anthology/P14-1039
http://www.aclweb.org/anthology/P14-1039
http://www.aclweb.org/anthology/P14-1039
https://doi.org/10.18653/v1/P16-2008
https://doi.org/10.18653/v1/P16-2008
https://doi.org/10.18653/v1/P16-2008
http://www.aclweb.org/anthology/P/P08/P08-1022
http://www.aclweb.org/anthology/P/P08/P08-1022


219

Claire Gardent and Laura Perez-Beltrachini. 2017.
A statistical, grammar-based approach to micro-
planning. Computational Linguistics, 43(1).

Edward Gibson. 2000. Dependency locality theory:
A distance-based theory of linguistic complexity.
In Alec Marantz, Yasushi Miyashita, and Wayne
O’Neil, editors, Image, Language, brain: Papers
from the First Mind Articulation Project Symposium.
MIT Press, Cambridge, MA.

Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for
general purpose sentence realisation. In Proc.
COLING-08.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355–396.

T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage information density. Cognitive
Psychology, 61(1):23–62.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with neu-
ral checklist models. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 329–339. Association for
Computational Linguistics.

David King and Michael White. 2018. The osu real-
izer for srst ’18: Neural sequence-to-sequence in-
flection and incremental locality-based linearization.
In Proceedings of the First Workshop on Multilin-
gual Surface Realisation, pages 39–48. Association
for Computational Linguistics.

Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin
Choi, and Luke Zettlemoyer. 2017. Neural amr:
Sequence-to-sequence models for parsing and gen-
eration. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 146–157. Associa-
tion for Computational Linguistics.

Mike Lewis, Kenton Lee, and Luke Zettlemoyer. 2016.
Lstm ccg parsing. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 221–231. Association for
Computational Linguistics.

Mike Lewis and Mark Steedman. 2014. Improved
CCG parsing with semi-supervised supertagging.
TACL.

Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based syntactic linearization. In
Proceedings of NAACL, Denver, Colorado, USA.

Hongyuan Mei, Mohit Bansal, and R. Matthew Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-fine alignment. In
Proceedings of the 2016 Conference of the North

American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 720–730. Association for Computational Lin-
guistics.

Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.

Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).

Ratish Puduppully, Yue Zhang, and Manish Shrivas-
tava. 2016. Transition-based syntactic linearization
with lookahead features. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Li nguistics: Human
Language Technologies, pages 488–493, San Diego,
California. Association for Computational Linguis-
tics.

Ratish Puduppully, Yue Zhang, and Manish Shrivas-
tava. 2017. Transition-based deep input lineariza-
tion. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
643–654. Association for Computational Linguis-
tics.

Rajakrishnan Rajkumar and Michael White. 2010. De-
signing agreement features for realization ranking.
In Proc. Coling 2010: Posters, pages 1032–1040,
Beijing, China.

Rajakrishnan Rajkumar and Michael White. 2011.
Linguistically motivated complementizer choice in
surface realization. In Proceedings of the UC-
NLG+Eval: Language Generation and Evaluation
Workshop, pages 39–44, Edinburgh, Scotland. As-
sociation for Computational Linguistics.

Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proc. NAACL HLT 2009
Short Papers.

Ehud Reiter and Robert Dale. 2000. Building Natural-
Language Generation Systems. Cambridge Univer-
sity Press.

Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.

Kristina Toutanova, Christopher D Manning, Dan
Flickinger, and Stephan Oepen. 2005. Stochas-
tic hpsg parse disambiguation using the redwoods
corpus. Research on Language and Computation,
3(1):83–105.

Erik Velldal and Stefan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT-
Summit X.

http://www.ling.uni-potsdam.de/~vasishth/Papers/Gibson-Cognition2000.pdf
http://www.ling.uni-potsdam.de/~vasishth/Papers/Gibson-Cognition2000.pdf
http://dx.doi.org/10.1016/j.cogpsych.2010.02.002
http://dx.doi.org/10.1016/j.cogpsych.2010.02.002
http://aclweb.org/anthology/D16-1032
http://aclweb.org/anthology/D16-1032
http://aclweb.org/anthology/W18-3605
http://aclweb.org/anthology/W18-3605
http://aclweb.org/anthology/W18-3605
https://doi.org/10.18653/v1/P17-1014
https://doi.org/10.18653/v1/P17-1014
https://doi.org/10.18653/v1/P17-1014
https://doi.org/10.18653/v1/N16-1026
https://doi.org/10.18653/v1/N16-1086
https://doi.org/10.18653/v1/N16-1086
http://www.aclweb.org/anthology/N16-1058
http://www.aclweb.org/anthology/N16-1058
http://aclweb.org/anthology/E17-1061
http://aclweb.org/anthology/E17-1061
http://www.aclweb.org/anthology/C10-2119
http://www.aclweb.org/anthology/C10-2119
http://www.aclweb.org/anthology/W11-2706
http://www.aclweb.org/anthology/W11-2706


220

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
1711–1721. Association for Computational Linguis-
tics.

Michael White. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language & Computation,
4(1):39–75.

Michael White and Rajakrishnan Rajkumar. 2008. A
more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Coling
2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 17–24.

Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 410–419,
Singapore. Association for Computational Linguis-
tics.

Michael White and Rajakrishnan Rajkumar. 2012.
Minimal dependency length in realization ranking.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 244–255, Jeju Island, Korea. Association for
Computational Linguistics.

Sam Wiseman, Stuart M. Shieber, and Alexander M.
Rush. 2017. Challenges in data-to-document gener-
ation. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.

Yue Zhang and Stephen Clark. 2015. Syntax-based
word ordering using learning-guided search. Com-
putational Linguistics, 41(3).

https://doi.org/10.18653/v1/D15-1199
https://doi.org/10.18653/v1/D15-1199
http://www.aclweb.org/anthology/D/D09/D09-1043
http://www.aclweb.org/anthology/D/D09/D09-1043
http://www.aclweb.org/anthology/D12-1023
https://arxiv.org/abs/1707.08052
https://arxiv.org/abs/1707.08052

