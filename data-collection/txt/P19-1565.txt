



















































Target-Guided Open-Domain Conversation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5624–5634
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5624

Target-Guided Open-Domain Conversation

Jianheng Tang1,2, Tiancheng Zhao2, Chenyan Xiong3, Xiaodan Liang1∗,
Eric P. Xing2,4, Zhiting Hu2,4∗

1Sun Yat-sen University, 2Carnegie Mellon University, 3Microsoft Research AI, 4Petuum Inc.

{sqrt3tjh,xdliang328}@gmail.com, {tianchez,zhitingh}@cs.cmu.edu
chenyan.xiong@microsoft.com, eric.xing@petuum.com

Abstract

Many real-world open-domain conversation
applications have specific goals to achieve dur-
ing open-ended chats, such as recommenda-
tion, psychotherapy, education, etc. We study
the problem of imposing conversational goals
on open-domain chat agents. In particular, we
want a conversational system to chat naturally
with human and proactively guide the conver-
sation to a designated target subject. The prob-
lem is challenging as no public data is avail-
able for learning such a target-guided strat-
egy. We propose a structured approach that
introduces coarse-grained keywords to con-
trol the intended content of system responses.
We then attain smooth conversation transition
through turn-level supervised learning, and
drive the conversation towards the target with
discourse-level constraints. We further derive
a keyword-augmented conversation dataset for
the study. Quantitative and human evaluations
show our system can produce meaningful and
effective conversations, significantly improv-
ing over other approaches1.

1 Introduction

Creating intelligent agent that can carry out open-
domain conversation with human is a long-lasting
challenge. Impressive progress has been made,
advancing from early rule-based systems, e.g.,
Eliza (Weizenbaum et al., 1966), to recent end-to-
end neural conversation models that are trained on
massive data (Shang et al., 2015; Li et al., 2015)
and make use of background knowledge (Fang
et al., 2018; Qin et al., 2019; Liu et al., 2018).

However, current open-domain systems still
struggle to conduct engaging conversations (Ram

∗corresponding authors
1Data and code are publicly available at

https://github.com/squareRoot3/
Target-Guided-Conversation

Not so good. I am really tired.

Oh, I’m sorry to hear. why?

I have too much work to do.

What kind of work is it?

I am writing a computer program.

Interesting. I read about coding from a 
book.

work

book

Target:  
e-books

Hi there, how are you doing?

Really? You are smart.

e-books

sorry

I prefer e-books over paperback books.

Figure 1: Target-Guided Open-Domain Conversation.
The agent is given a target subject e-books which is
unknown to the human. The goal is to guide the con-
versation naturally to the target. Utterance keywords
are highlighted in red (agent) and blue (human) and in
italic.

et al., 2018), and often generate inconsistent or un-
controlled results. Further, many practical open-
domain dialogue applications do have specific
goals to achieve even though the conversations are
open-ended, e.g., accomplishing nursing goals in
therapeutic conversation, inspiring ideas in educa-
tion, making recommendation and persuasion, and
so forth. Thus, there is a strong demand to enable
the integration of goals and strategy into open-
domain dialogue systems, and it imposes chal-
lenges to both: first, how to define the goal for
an open-domain chat system; and second, how to
encode dialogue strategy into the response produc-
tion process. It is also crucial to attain a gen-
eral method that is not tailored towards special-
ized goals that require domain-specific handcraft-
ing and annotations (Yarats and Lewis, 2018; He
et al., 2018; Li et al., 2018).

https://github.com/squareRoot3/Target-Guided-Conversation
https://github.com/squareRoot3/Target-Guided-Conversation


5625

This paper makes a step towards open-domain
dialogue agents with conversational goals. In
particular, we want the system to chat naturally
with humans on open domain topics and proac-
tively guide the conversation to a designated target
subject. For example, in Figure 1, given a tar-
get e-books and an arbitrary starting topic such
as tired, the agent drives the conversation in a
natural way following a high-level logical back-
bone, and effectively reaches the target in the end.
Such a target-guided conversation setup is general-
purpose and can entail a large variety of practi-
cal applications as above. The above problem is
difficult in that the agent has to balance well be-
tween chatting naturally and achieving the target;
and moreover, to the best of our knowledge, there
is no public dataset available for learning target-
guided dialogue.

This paper proposes a solution to the task. We
decouple the whole system into separate modules
and address the challenges at different granularity.
Specifically, we explicitly model and control the
intended content of each system response by in-
troducing coarse-grained utterance keywords. We
then impose a discourse-level rule that encour-
ages the keywords to approach the end target dur-
ing the course of the conversation; and we attain
smooth conversation transition at each dialogue
turn through turn-level supervised learning. To
this end, we further derive a keyword-augmented
conversation dataset from an existing daily-life
chat corpus (Zhang et al., 2018) and use it for
learning keyword transitions and utterance pro-
duction.

We study different keyword transition ap-
proaches, including pairwise PMI-based transi-
tion, neural-based prediction, and a hybrid kernel-
based method. We conduct quantitative and hu-
man evaluations to measure the performance of
sub-modules and the whole system. Our agent is
able to generate meaningful and effective conver-
sations with a decent success rate of reaching the
targets, improving over other approaches in differ-
ent respects. We show target-guided open-domain
conversation is a promising and potentially impor-
tant direction for future research.

2 Related Work

The past end-to-end dialogue research can be
broadly divided into two categories: task-oriented
dialogue systems and chat-oriented (a.k.a open-

domain) systems. For task-oriented dialogue sys-
tems, the system is designed to accomplish spe-
cific goals, e.g., providing bus schedule (Raux
et al., 2005; Young et al., 2007; Dhingra et al.,
2017). Besides information giving, other tasks
have been extensively studied, such as negotia-
tions (DeVault et al., 2015; Lewis et al., 2017; He
et al., 2018; Cao et al., 2018), symmetric collabo-
rations (He et al., 2017), etc.

On the other hand, chat-oriented dialogue sys-
tems have been created to model open-domain
conversations without specific goals. Prior work
has been focusing on developing novel neural
architectures that improve next utterance gener-
ation or retrieval task performance by training
on large open-domain chit-chat dataset (Sordoni
et al., 2015; Serban et al., 2016; Zhou et al., 2016;
Wu et al., 2018). However, despite the steady im-
provement over model architectures, the current
systems can still suffer from a range of limitations,
e.g., dull responses, inconsistent persona (Li et al.,
2016a), etc.

The commercial chatbot XiaoIce (Zhou et al.,
2018) and the first Amazon Alexa challenge win-
ner (Fang et al., 2018) have stressed to im-
prove engagement with users. Also, to encourage
discourse-level strategy, prior work has developed
different system action representations that enable
the model to reason at the dialogue level. One line
of work has utilized latent variable models (Zhao
et al., 2017; Yarats and Lewis, 2018; Zhao et al.,
2019) to infer a latent representation of system re-
sponses, which separates the natural language gen-
eration process from decision-making. Another
approach has created hybrid systems to incorpo-
rate hand-crafted coarse-grained actions (Williams
et al., 2017; He et al., 2018) as a part of the neural
dialogue systems. These systems have typically
focused on specific domains such as price negoti-
ation and movie recommendation. Building upon
the prior work, this paper creates novelty in terms
of both defining goals for open-domain chatting
and creating system actions representations. Our
structured solution use predicted keywords as a
non-parametric representation of the intended con-
tent for the next system response.

Due to the lack of full supervision data, the so-
lution proposed in this work divides the task into
two competitive sub-objectives, each of which can
be conquered with either direct supervision or sim-
ple rules. Such a divide-and-conquer approach



5626

represents a general means of addressing complex
task objectives with no end-to-end supervision
available. A similar approach has been adopted in
other contexts, such as text style transfer (Hu et al.,
2017; Shen et al., 2017; Yang et al., 2018) and con-
tent manipulation (Lin et al., 2019), where content
fidelity as a sub-objective is achieved with simple
auto-encoding training, while the competitive na-
ture of multiple sub-objectives jointly drives the
models to learn desired behaviors.

3 Task Definition: Target-guided
Open-domain Conversation

We first formally define the task of target-guided
open-domain conversation. We also establish the
key notations used in the rest of the paper.

Briefly, given a target, we want a chat agent
to converse with human starting from an arbitrary
initial topic, and lead the conversation to the tar-
get in the end. In this paper, we define a target
to be a word (e.g., an entity name McDonald, or
a common noun book, etc.) and denote it as t.
We note that a target can also be formulated in
other more complex forms depending on specific
applications. The target is only presented to the
agent and is unknown to the human. The conver-
sation starts with an initial topic which is usually
randomly picked by the human. At each dialogue
turn where the agent wants to make a response, it
has access to the conversation history consisting
of a sequence of utterances by either the human or
the agent, x1:n = {x1, . . . ,xn}. The agent then
produces an utterance xn+1 as a response, aim-
ing to satisfy (1) transition smoothness by mak-
ing the response natural and appropriate in the cur-
rent conversation context, and (2) target achieve-
ment by driving the conversation to reach the des-
ignated target. Specifically, we consider a target is
achieved when either the human or the agent men-
tions the target or similar word in an utterance—
such a definition is simple and allows easy mea-
surement of the success rate. Again, other more
complex and meaningful measures could be con-
sidered for specific practical applications.

The above two objectives are complementary
and competitive. On one hand, an agent cannot
simply bring up the target content regardless of
the conversation context. For example, given a tar-
get cat and conversation history {Human: I went
to a movie.}, a response like Do you like cat? is
typically not a smooth transition, even though it

quickly reaches the target. On the other hand,
the agent must avoid being trapped in open-ended
chats by producing only smooth yet reactive re-
sponses. Instead, it has to proactively lead the con-
versation to approach the target.

The competitive nature of the two desiderata
requires the agent to grasp a conversation strat-
egy that balances well between different factors.
To the best of our knowledge, there is no public
large data that fits the new problem setting and
permits end-to-end learning of such a discourse-
level strategy in open domain. Instead, we usually
only have access to those open-ended conversation
data where interlocutors conversed freely without
a specified end target.

To this end, we propose to break down the prob-
lem, leverage partial supervisions and introduce
more structures for a solution. In the following, we
first present our approach to the task (section 4),
and then introduce a large open-ended conversa-
tion dataset used for building the conversational
agent (section 5).

4 The Proposed Approach

We explore a solution that addresses the two
desiderata separately. In particular, we main-
tain smooth conversation transition by turn-level
supervised learning on open-domain chat data,
and we inject target-guiding behavior with a rule-
based guiding strategy. Further, to enable effec-
tive control over the transition and guiding strat-
egy, we decouple the decision-making process and
utterance generation by explicitly modeling the in-
tended coarse-grained keywords in the next sys-
tem utterance.

Thus the system consists of several core mod-
ules, including a turn-level keyword transition
predictor (section 4.1), a discourse-level target-
guiding strategy (section 4.2), and a response re-
triever (section 4.3).

4.1 Turn-level Keyword Transition

Given the conversation history at each dialogue
turn, this module aims to predict keywords of the
next response that is appropriate in the conversa-
tion context. This part is agnostic to the end target,
and therefore aligns with the conventional chit-
chat objective. We thus can use any open-ended
chat data with extracted utterance keywords to
learn the prediction module in a supervised man-
ner. We present such a dataset that we posit is par-



5627

I play basketball, do you play?

Yes, I also like basketball.

Discourse-level 
Target-Guided Strategy

Do you like rap music? I listen 
to a lot of rap music.

Target: dance Turn-level Keyword 
Transition

Conversation
History

Keyword Augmented
Response Retrieval

dance
1.0

basketball
0.47

music
0.65

sport
0.40

Candidate
Keyword Set

cat
0.45

video
0.55

study
0.36

Keyword 
Predictor

sport music cat

0.03 

0.11 

0.07 

…
…

Response
Retrieval

Keyword 
Selection

music
party
0.62

Figure 2: Solution Overview. The left panel shows an on-going conversation with a designated target dance.
The discourse-level target-guided module (right panel, section 4.2) first picks a set of valid candidate keywords
for the next system response. The turn-level keyword transition module (middle panel, section 4.1) computes a
distribution over candidate keywords. The most likely valid keyword (music) is then selected, and fed into the
keyword-augmented response retrieval module (middle panel, section 4.3) for producing the next response.

ticularly suitable for the learning in section 5.
Architecturally, we study three different ap-

proaches as representative paradigms for predict-
ing the next-turn keyword distribution, including
pairwise keyword linear transition, neural-based
prediction, and kernel-based method.

Pairwise PMI-based Transition The most
straightforward way for keyword transition is to
construct a keyword pairwise matrix that charac-
terizes the association between keywords in the
observed conversation data. We use pointwise mu-
tual information (PMI) (Church and Hanks, 1990)
as the measure, which, given two keywords wi and
wj , computes likeliness of wj → wi with

PMI(wi, wj) = log p(wi|wj)/p(wi), (1)

where p(wi|wj) is the ratio of transitioning to wi
in the next turn given wj in the current turn, and
p(wi) is the ratio of wi occurrence. Both quantities
can be directly counted from the conversation data
beforehand. At test time, we first use a keyword
extractor (section 5) to extract keywords of the
current utterance. Assuming all these keywords
are independent, for each candidate next keyword,
we sum up their PMI scores w.r.t the candidate.
The resulting candidate scores are then normalized
to obtain a distribution over keywords in the next
turn.

The approach enjoys simplicity and inter-
pretability, yet can suffer from data sparsity and
perform poorly with a priori unseen transition
pairs.

Neural-based Prediction The second approach
predicts the next keywords with a neural network
in an end-to-end manner. More concretely, we first
use a recurrent network to encode the conversation
history, and feed the resulting features to a predic-
tion layer to obtain a distribution over keywords
for the next turn. The network is learned by maxi-
mizing the likelihood of observed keywords in the
data. The neural approach is straightforward, but
can rely on a large amount of data for learning.

Hybrid Kernel-based Method We further
study a hybrid approach that combines neural fea-
ture extraction with pairwise closeness measuring.
Specifically, given a pair of a current keyword and
a candidate next keyword, we follow (Xiong et al.,
2017) by first measuring the cosine similarity of
their normalized word embeddings, and feeding
the quantity to a kernel layer consisting of K
RBF kernels. The output of the kernel layer is a
K-dimension kernel feature vector, which is then
fed to a single-unit dense layer for a candidate
score. The score is finally normalized across
all candidate keywords to yield the candidate
probability distribution. If the current turn has
multiple keywords, the corresponding multiple
K-dimension kernel features are first summed
up before feeding to the dense layer. Thus, the
intermediate kernel layer serves as a soft aggre-
gation mechanism to account for multiple-to-one
keyword transition. The parameters are learned
in the same way as in the neural-based prediction



5628

method. Our empirical study shows the hybrid
approach provides the strongest performance.

4.2 Discourse-level Target-Guided Strategy
This module aims to fulfill the end target by proac-
tively driving the discussion topic forward in the
course of the conversation. As noted above, there
is typically no data available for direct learning of
such a strategy. Fortunately, the augmentation of
interpretable coarse-grained keywords enables us
to apply a simple yet effective rule to this end.

We constrain that the keyword of each turn must
move strictly closer to the end target compared to
those of preceding turns. Figure 2, right part, illus-
trates the rule at a particular step. Given the key-
word Basketball of the current turn and its close-
ness score (0.47) to the target Dance, the only
valid candidate keywords for the next turn are
those with higher target closeness, such as Party
with a closeness score of 0.62. On the other hand,
transitioning from Basketball to Sport is not al-
lowed in the context as it does not move towards
the target. More concretely, we use cosine similar-
ity between normalized word embeddings as the
measure of keyword closeness.

At each turn for predicting the next keyword,
the above constraint first collects a set of valid can-
didates, and the turn-level transition module sam-
ples or picks the most likely one the from the set
according to the keyword distribution. In this way,
the predicted keyword for next response can be
both a smooth transition and an effective step to-
wards the target.

4.3 Keyword-augmented Response Retrieval
The final module in the system aims to produce
a response conditioning on both the conversa-
tion history and the predicted keyword. In this
work, we use a retrieval-based approach, though
a generation-based method can also be readily
plugged in.

The architecture of the module is adapted from
the previous work (Wu et al., 2016) with aug-
mented keyword conditioning. More concretely,
we use recurrent networks to encode the input con-
versation history and keyword, as well as each
of the candidate responses from a database (e.g.,
all utterances in the training set). We then com-
pute the element-wise product between the candi-
date feature with the history feature, and between
the candidate feature with the keyword feature, re-
spectively. The resulting two vectors are concate-

Train Val Test

#Conversations 8,939 500 500
#Utterances 101,935 5,602 5,317

#Keyword types 2,678 2,080 1,571
#Avg. keywords 2.1 2,1 1.9

Table 1: Data Statistics. The last row is the average
number of keywords in each utterance. The vocabulary
size is around 19K.

A: Hi ! I am from India . where are you from?
B: I’m from Portland. I just got back from a long walk.
A: I just got back from coaching swimming at the pool.

Walking where ?

B: I like to walk in parks for good health. No soft
drinks for me either!

... ...

Table 2: An Example Conversation. Only the first 4
utterances are shown. Keywords of each utterance are
marked with underline.

nated and fed to a final single-unit dense layer with
sigmoid to get the matching probability of the can-
didate response.

Same as the turn-level transition module, the
conditional response retrieval module can also be
learned with open-ended conversation data in a
supervised manner. That is, we maximize the
likelihood of observed response given its history
and predicted keyword, while minimizing the like-
lihood of randomly sampled negative responses.
Section 5 presents more details of the data and
negative responses.

5 Dataset

We next describe a large conversation dataset that
can be useful for studying the task and has been
used in our solution. The dataset is derived from
the PersonaChat corpus (Zhang et al., 2018) where
crowdworkers were asked to chat naturally with
given persona. The conversations cover a broad
range of topics such as work, family, personal in-
terest, etc; and the discussion topics change fre-
quently during the course of the conversations.
These properties make the conversations partic-
ularly suitable for learning smooth, natural tran-
sitions at each turn. Note that, however, the
conversations do not necessarily suit for learn-
ing discourse-level strategies, as they were origi-
nally created without end targets and do not ex-
hibit target-guided behaviors.



5629

To adapt the corpus for turn-level keyword tran-
sition in our new setting, we obtain all conver-
sations while discarding the associated persona
information. We then augment the data by au-
tomatically extracting keywords of each utter-
ance. Specifically, we apply a rule-based keyword
extractor which combines TF-IDF and Part-Of-
Speech features for scoring word salience. More
details are provided in supplementary materials.
We re-split the data into train/valid/test sets, where
the test set contains 500 conversations with rela-
tively frequent keywords. Table 1 lists the data
statistics. An example conversation with the ex-
tracted keywords is shown in Table 2.

The resulting dataset is used in our solution for
training both the turn-level transition module (sec-
tion 4.1) and the response retrieval module (sec-
tion 4.3). We follow the retrieval-based chit-chat
literature (Wu et al., 2016) and randomly sample
19 negative responses for each turn as the negative
responses for training.

6 Experiments

6.1 Experimental Setup

Baselines and Comparison Systems We evalu-
ate a diverse set of approaches for comparison and
ablation study.

Retrieval (Wu et al., 2016) is the conventional
retrieval-based chitchat system which does not
permit an end target and is not augmented with
coarse-grained utterance keywords. The system
thus cannot be deployed for target-guided conver-
sation, and is used to provide reference perfor-
mance in terms of different metrics in the exper-
iments. The model architecture is adapted from
the prior work, the same as used in our full system
except for the keyword conditioning part.

Retrieval-Stgy augments the above base re-
trieval system with the proposed target-guided
strategy (section 4.2). Specifically, it first extracts
the keywords of current utterance with the extrac-
tor used in section 5, and applies the target-guided
rule to obtain a set of candidate keywords. The
base retrieval model is then used to retrieve a re-
sponse containing at least one keyword from the
keyword set. Such a pipeline approach achieves a
strong baseline performance, as shown in the fol-
lowing.

Ours As in section 4.1, our full system has sev-
eral variants in the turn-level keyword transition
module, including the PMI, Neural, and Kernel

methods. For comparison, we also use a Random
method which randomly picks a keyword for next
response.

Training Details We use the same configuration
for the common parts of all agents. We apply a
single-layer GRU (Chung et al., 2014) in all en-
coders. Both the word embedding and hidden di-
mensions are set to 200. We use GloVe (Penning-
ton et al., 2014) to initialize word embeddings. We
apply Adam optimization (Kingma and Ba, 2014)
with an initial learning rate of 0.001 and anneal-
ing to 0.0001 in 10 epochs. Systems are imple-
mented with a text generation toolkit Texar (Hu
et al., 2019).

6.2 Turn-level Evaluation

We first evaluate the performance of each conver-
sation turn, in terms of both turn-level keyword
prediction and response selection. That is, we dis-
able the discourse-level target constraint, and fo-
cus on measuring how accurate the systems can
predict the next keyword and retrieve the correct
response on the test set of the conversation data.
The evaluation largely follows the protocol of pre-
vious chit-chat systems (e.g., Wu et al., 2016),
and validates the effect of the keyword-augmented
conversation production.

Evaluation metrics For the keyword prediction
task, we measure three metrics: (1) Rw@K: key-
words recall at position K (= 1, 3, 5) in all (over
2600) possible keywords, (2) P@1: precision at
the first position, and (3) Cor.: the word embed-
ding based correlation score (Liu et al., 2016).

For the response selection task, we randomly
sample 19 negative responses for each test case,
and calculate R20@K, i.e., recall at position K in
the 20 candidate (positive and negative) responses,
as well as MRR, the mean reciprocal rank.

Results Table 3 shows the evaluation results.
Our system with Kernel transition module outper-
forms all other systems in terms of all metrics on
both two tasks, expect for R20@3 where the sys-
tem with PMI transition performs best. The Ker-
nel approach can predict the next keywords more
precisely. In the task of response selection, our
systems that are augmented with predicted key-
words significantly outperform the base Retrieval
approach, showing predicted keywords are helpful
for better retrieving responses by capturing coarse-
grained information of the next utterances. Inter-



5630

Keyword Prediction Response Retrieval
System Rw@1 Rw@3 Rw@5 P@1 Cor. R20@1 R20@3 R20@5 MRR

Retrieval - - - - - 0.5196 0.7636 0.8622 0.6661
Ours-Random 0.0005 0.0015 0.0025 0.0009 0.4995 0.5187 0.7619 0.8631 0.6650

Ours-PMI 0.0585 0.1351 0.1872 0.0871 0.7974 0.5441 0.7839 0.8716 0.6847
Ours-Neural 0.0609 0.1324 0.1825 0.1006 0.8075 0.5395 0.7801 0.8790 0.6816
Ours-Kernel 0.0642 0.1431 0.1928 0.1191 0.8164 0.5486 0.7827 0.8845 0.6914

Table 3: Results of Turn-level Evaluation.

System Succ. (%) #Turns

Retrieval 9.8 3.26
Retrieval-Stgy 67.2 6.56

Ours-PMI 47.4 5.12
Ours-Neural 51.6 4.29
Ours-Kernel 75.0 4.20

Table 4: Results of Self-Play Evaluation.

estingly, the system with Random transition has
a close performance to the base Retrieval model,
indicating that the erroneous keywords can be ig-
nored by the system after training.

6.3 Target-guided Conversation Evaluation

We next evaluate system performance in the pro-
posed target-guided conversation setup, with both
automatic simulation-based evaluation and human
evaluation.

6.3.1 Self-Play Simulation
Following the experimental settings in prior
work (Lewis et al., 2017; Li et al., 2016b), we
developed a task simulator to automatically pro-
duce target-guided conversations. Specifically, we
use the base Retrieval agent to play the role of
human which retrieves a response without know-
ing the end target. The simulator randomly picks
a keyword as the end target, and an utterance as
the starting point. Each agent then chats with
the Retrieval system, trying to guide the conver-
sation to the given target. To automatically eval-
uate whether the target is achieved, we use Word-
Net (Miller, 1998) to identify keywords that are
semantically close to the end target. More con-
cretely, if a keyword in an utterance (by either the
agent under test or Retrieval) has a WordNet in-
formation content similarity score higher than 0.9,
we consider the target is successfully achieved. To
avoid infinite conversation without ever reaching
the target, we set a maximum allowed number of
turns, which is 8 in our experiment. That is, an
agent that does not achieve the target after produc-
ing 8 responses is considered to fail in the case.

System Succ. (%) Smoothness

Retrieval 18 3.26
Retrieval-Stgy 66 3.24

Ours-PMI 52 3.00
Ours-Neural 56 2.94
Ours-Kernel 76 3.40

Table 5: Results of the Human Rating.

Kernel
Better(%)

No
Prefer(%)

Kernel
Worse(%)

Retrieval-Stgy 34 40 26
Ours-PMI 52 22 26

Ours-Neural 56 28 16

Table 6: Results of Human Ranking.

We measure the success rate of achieving the tar-
gets (Succ.) and the average number of turns used
to reach a target (#Turns).

Table 4 shows the results of 500 simulations
for each of the comparison systems. Our sys-
tem with Kernel transition obtains the highest suc-
cess rate, significantly improving over other ap-
proaches. The success rate of the base Retrieval
agent is lower than 10%, which proves that a
chitchat agent without a target-guided strategy can
hardly accomplish our task. The Retrieval-Stgy
agent has a relatively high success rate, while tak-
ing more turns (6.56) to accomplish this. This is
partially due to the lack of coarse-grained keyword
modeling and transition. We further note that, in
the Kernel system, around 81% of predicted key-
words eventually occur in the produced utterances,
indicating that the predicted keywords have a great
impact on the retrieval module.

6.3.2 Human Evaluation
We finally perform human evaluation for a more
thorough system comparison in terms of differ-
ent aspects. Specifically, we use the DialCrowd
toolkit (Lee et al., 2018) to setup human evalua-
tion interfaces, and undertook two types of human
studies as below.

The first evaluation is to measure the system



5631

Agent: Retrieval-Stgy Target: Dance Agent: Ours-Kernel Target: Dance

A: Hi, how are you today?
H: Fine. Just finished riding in the countryside.
A: That sounds relaxing. I love the outdoors.
H: I rode along a long black river.
A: Nice , do you like the lake? I do I go often.
H: Yes. I like swimming in the lake.
A: I’ve never been surfing but used to do a lot of traveling

across the us playing guitar.
H: Nice. I also like playing guitar and other musical

instruments.
A: Oh that’s nice, I’m more into classical music.
H: I am interested in pop music.
A: Oh cool! I really like music and singing.
H: I enjoy singing. Although I cannot sing well.
A: Music is life. Dancing to anything with a beat is the

best.

A: Hi, how are you today?
H: Fine. Just finished riding in the countryside.
A: I love riding my bike. It is great exercise and fun.

(bike)
H: I rode along a long black river.
A: That sounds very exciting. You can ride bikes, listen to

musics there too. (music)
H: That’s great. My favorite singer is Taylor Swift.
A: Yes I love to sing! I can sing a lot of songs, can you?

(sing)
H: Yes, although I cannot sing well.
A: Dancing? I love dancing! Singing too!

Agent: Retrieval-Stgy Target: McDonald’s Agent: Ours-Kernel Target: McDonald’s

A: Hello, how are you doing?
H: Hi, I am great just finishing up my homework. How are

you?
A: Oh nothing much just reading.
H: Which kind of books are you reading?
A: Reading comic books, I love them. What about you?
H: I enjoy reading sci-fi and romantic stories.
A: That sounds helpful for writing.
H: Yeah, but I do not good at writing articles
A: Those were good books, what do you like to eat?
H: Vegetables and fruit salad are my favorite.
A: That reminds me of spicy food.
H: Oh spicy food is also very delicious.
A: Nothing better than bacon for breakfast.
H: I often eating eggs for breakfast.
A: I love cheese tacos too.
H: It’s too late now, and I’ll ride my bike to home.

A: Hello, how are you doing?
H: Hi, I am great just finishing up my homework. How are

you?
A: Just got done eating some chicken and rice after I got

done with my gym session. (eat)
H: I am gonna eat some vegetable for dinner.
A: So you are vegetarian? I am from los angeles so there

are lots of vegetarian restaurants. (restaurant)
H: I am not a vegetarian. Fried chicken is my favorite.
A: Oh boy, when I go to McDonald’s, I always get fries

for my 2 dogs.

Table 7: Example conversations between human (H) and two different agents (A), with the same targets and starting
utterances. Keywords selected or predicted by the agents are highlighted in red and italic, and keywords mentioned
by human are highlighted in blue and italic. As keywords predicted by the Kernel agent do not necessarily occur
in the retrieved utterances, we put them to the end of each sentence. Targets achieved at the end of conversations
are underlined. We present the examples in case-sensitive format for readability. All tokens are in lowercase in the
program.

performance in terms of the two key desiderata,
namely target achievement and transition smooth-
ness, respectively. We first build 50 test cases,
each of which has a target and a starting utter-
ance. In each test case, a human turker is asked
to converse with a randomly selected agent. The
agent informs the turker when it thought the target
is achieved or has reached the maximum number
of turns (which is set to 8). Then the turker is pre-
sented with the designated target, and is asked to
judge whether the target has been achieved, as well
as rate transition smoothness during the conversa-
tion with a score ranging from 1 (strongly bad) to
5 (strongly good). All agents are evaluated on all
test cases.

Table 5 shows the results of the first evalua-
tion. Our Kernel agent clearly outperforms all

other comparison systems in terms of both suc-
cess rate and transition smoothness. Note that the
success rate results of all agent are consistent with
those in simulation (Table 4). Comparing the base
Retrieval agent and the augmented Retrieval-Stgy
agent, we can see that Retrieval-Stgy has almost
the same smoothness with Retrieval but achieves a
much higher success rate. This validates that our
discourse-level strategy (section 4.2) is indeed ef-
fective for target-guided conversations.

The second evaluation compares our best-
performing Kernel agents with other agents side-
by-side. Specifically, we ask a human turker to
converse with the Kernel agent and a randomly
selected comparison agent in the same test case.
We then ask the turker to rank the two conversa-
tions by considering all the criteria. Turkers can



5632

Agent: Ours-Kernel Target: Listen

A: Hi! How are you? I am very exciting about my new
job!

H: Fine. What is your new job?
A: That sounds exciting. (sound)
H: I like reading books.
A: What kind of books do you read? (book)
H: I read comics.
A: I do not have TV. (TV)
H: Watching animation is fun.
A: What position d you play? (play)
H: I play a lot of sports.
A: Oh really? What kind of music? Listen to classical.

Table 8: An erroneous example.

also choose “no preference” if the conversations
are equally good or bad. To avoid any bias, in each
test case, we randomly pick one from the pair of
agents to converse first, and we let the turker to de-
cide when to stop to avoid revealing the target too
early. As above, we evaluate on 50 test cases for
each pair of agents.

Table 6 shows the results of the second eval-
uation. We see that our Kernel system consis-
tently outperforms the comparison methods with
30-50% wins.

6.4 Qualitative Study

We take a close look at the model performance by
studying the conversation examples from different
agents in human evaluation.

Table 7 shows the conversations between hu-
man and agents given targets dance and McDon-
ald’s, respectively. We can see that, in general,
our Kernel agent can accomplish the task in fewer
turns than the Retrieval-Stgy agent. In the first
case, the Kernel agent guides the conversation
from ride to the crucial topic music smoothly and
quickly, and then achieves the target word dance
naturally. In contrast, the Retrieve-Stgy agent is
trapped in open-ended chats for the first three turns
and does not reach the target until the 7th turn.
In the second case, the target McDonald’s is rela-
tively uncommon in our dataset. The kernel agent
succeeded to achieve the target in the 4th turn
while the Retrieval-Stgy agent failed to reach the
target within the maximally allowed number of
turns.

Table 8 shows a failure case by our Kernel
agent. Although the agent successfully achieved
the target, it sometimes makes non-smooth key-
word transition without a clear logic. For instance,
the final utterance of the agent, though reaching

the target listen, is not appropriate in the conversa-
tion context (e.g., in the presence of human’s pre-
ceding keyword sports).

7 Conclusions & Discussions

We have studied the problem of target-guided
open-domain conversation, where an agent con-
verses naturally with the human and proactively
guides the conversation to a designated end tar-
get. We propose a modular solution with coarse-
grained keywords as a logical backbone, and use
partial supervision and heuristic rules to achieve
the task. We also derive a dataset for the study.
Quantitative and human evaluations demonstrate
promising and improved results of our approach.

This work presents an initial attempt to bridge
the gap between open-domain chit-chat and task-
oriented dialogue. A target-guided agent can be
deployed in practice to converse with users engag-
ingly and guide the users to trigger task-oriented
systems (e.g., reserving a restaurant) in the end.
An open-domain agent with control over the con-
versation strategy and end target can also be use-
ful in education, psychotherapy, and others as dis-
cussed in section 1. Our treatment of utterance ac-
tion and conversation target through simple key-
words can be preliminary in terms of complex
real applications. It would be exciting to explore
more sophisticated modeling to enable more fine-
grained control on both sentence (Hu et al., 2017)
and discourse levels (Williams et al., 2017; Fang
et al., 2018).

References
Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z

Leibo, Karl Tuyls, and Stephen Clark. 2018. Emer-
gent communication through negotiation. In ICLR.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.

David DeVault, Johnathan Mell, and Jonathan Gratch.
2015. Toward natural turn-taking in a virtual human
negotiation agent. In 2015 AAAI Spring Symposium
Series.

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.



5633

Towards end-to-end reinforcement learning of dia-
logue agents for information access. In ACL.

Hao Fang, Hao Cheng, Maarten Sap, Elizabeth Clark,
Ari Holtzman, Yejin Choi, Noah A Smith, and Mari
Ostendorf. 2018. Sounding board: A user-centric
and content-driven social chatbot. In NAACL System
Demonstrations.

He He, Anusha Balakrishnan, Mihail Eric, and Percy
Liang. 2017. Learning symmetric collaborative di-
alogue agents with dynamic knowledge graph em-
beddings. In ACL.

He He, Derek Chen, Anusha Balakrishnan, and Percy
Liang. 2018. Decoupling strategy and gener-
ation in negotiation dialogues. arXiv preprint
arXiv:1808.09637.

Zhiting Hu, Haoran Shi, Bowen Tan, Wentao Wang,
Zichao Yang, Tiancheng Zhao, Junxian He, Lianhui
Qin, Di Wang, et al. 2019. Texar: A modularized,
versatile, and extensible toolkit for text generation.
In ACL System Demonstrations.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Toward con-
trolled generation of text. In ICML.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Kyusong Lee, Tiancheng Zhao, Alan W Black, and
Maxine Eskenazi. 2018. DialCrowd: A toolkit for
easy dialog system assessment. In SIGDAIL, pages
245–248.

Mike Lewis, Denis Yarats, Yann N Dauphin, Devi
Parikh, and Dhruv Batra. 2017. Deal or no deal?
end-to-end learning for negotiation dialogues. arXiv
preprint arXiv:1706.05125.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A persona-based neural con-
versation model. arXiv preprint arXiv:1603.06155.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016b. Deep re-
inforcement learning for dialogue generation. arXiv
preprint arXiv:1606.01541.

Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz,
Vincent Michalski, Laurent Charlin, and Chris Pal.
2018. Towards deep conversational recommenda-
tions. In NeurIPS, pages 9748–9758.

Shuai Lin, Wentao Wang, Zichao Yang, Haoran Shi,
Frank Xu, Xiaodan Liang, Eric Xing, and Zhiting
Hu. 2019. Toward unsupervised text content manip-
ulation. arXiv preprint arXiv:1901.09501.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

Shuman Liu, Hongshen Chen, Zhaochun Ren, Yang
Feng, Qun Liu, and Dawei Yin. 2018. Knowledge
diffusion for neural dialogue generation. In ACL,
volume 1, pages 1489–1498.

George Miller. 1998. WordNet: An electronic lexical
database. MIT press.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong
Liu, Xiang Gao, Bill Dolan, Yejin Choi, and Jian-
feng Gao. 2019. Conversing by reading: Contentful
neural conversation with on-demand machine read-
ing. In ACL.

Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu
Venkatesh, Raefer Gabriel, Qing Liu, Jeff Nunn,
Behnam Hedayatnia, Ming Cheng, Ashish Nagar,
et al. 2018. Conversational AI: The science behind
the Alexa prize. arXiv preprint arXiv:1801.03604.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Let’s go pub-
lic! taking a spoken dialog system to the real world.
In Ninth European conference on speech communi-
cation and technology.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. arXiv preprint arXiv:1503.02364.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In NeurIPS.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714.

Joseph Weizenbaum et al. 1966. ELIZA—a computer
program for the study of natural language communi-
cation between man and machine. Communications
of the ACM, 9(1):36–45.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. In ACL.



5634

Hua Wu, Yi Liu, Ying Chen, Wayne Xin Zhao, Daxi-
ang Dong, Dianhai Yu, Xiangyang Zhou, and Lu Li.
2018. Multi-turn response selection for chatbots
with deep attention matching network. In ACL.

Yu Wu, Wei Wu, Chen Xing, Ming Zhou, and
Zhoujun Li. 2016. Sequential matching network:
A new architecture for multi-turn response selec-
tion in retrieval-based chatbots. arXiv preprint
arXiv:1612.01627.

Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan
Liu, and Russell Power. 2017. End-to-end neural
ad-hoc ranking with kernel pooling. In SIGIR, pages
55–64. ACM.

Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and
Taylor Berg-Kirkpatrick. 2018. Unsupervised text
style transfer using language models as discrimina-
tors. In NeurIPS.

Denis Yarats and Mike Lewis. 2018. Hierarchical text
generation and planning for strategic dialogue. In
ICML.

Stephanie Young, Jost Schatzmann, Karl Weilhammer,
and Hui Ye. 2007. The hidden information state ap-
proach to dialog management. In Acoustics, Speech
and Signal Processing, 2007. ICASSP 2007. IEEE

International Conference on, volume 4, pages IV–
149. IEEE.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you
have pets too? arXiv preprint arXiv:1801.07243.

Tiancheng Zhao, Kaige Xie, and Maxine Eskenazi.
2019. Rethinking action spaces for reinforcement
learning in end-to-end dialog agents with latent vari-
able models. arXiv preprint arXiv:1902.08858.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. In ACL.

Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung
Shum. 2018. The design and implementation of xi-
aoice, an empathetic social chatbot. arXiv preprint
arXiv:1812.08989.

Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao,
Dianhai Yu, Hao Tian, Xuan Liu, and Rui Yan. 2016.
Multi-view response selection for human-computer
conversation. In EMNLP.


