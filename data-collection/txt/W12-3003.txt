










































Unsupervised Content Discovery from Concise Summaries


Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 13–18,
NAACL-HLT, Montréal, Canada, June 7-8, 2012. c©2012 Association for Computational Linguistics

Unsupervised Content Discovery from Concise Summaries

Horacio Saggion
Universitat Pompeu Fabra

Department of Information and Communication Technologies
TALN Group

C/Tanger 122 - Campus de la Comunicación
Barcelona - 08018

Spain

Abstract

Domain adaptation is a time consuming and
costly procedure calling for the development
of algorithms and tools to facilitate its automa-
tion. This paper presents an unsupervised al-
gorithm able to learn the main concepts in
event summaries. The method takes as input a
set of domain summaries annotated with shal-
low linguistic information and produces a do-
main template. We demonstrate the viability
of the method by applying it to three different
domains and two languages. We have eval-
uated the generated templates against human
templates obtaining encouraging results.

1 Introduction

Our research is concerned with the development of
techniques for knowledge induction in the field of
text summarization. Our goal is to automatically in-
duce the necessary knowledge for the generation of
concise event summaries such as the one shown in
Figure 1. This kind of summaries, which can be
found on the Web and in text collections, contain
key information of the events they describe. Pre-
vious work in the area of text summarization (De-
Jong, 1982; Oakes and Paice, 2001; Saggion and
Lapalme, 2002) addressed the problem of generat-
ing this type of concise summaries from texts, re-
lying on information extraction and text generation
techniques. These approaches were difficult to port
to new domains and languages because of the efforts
needed for modelling the underlying event template
structure. In this paper we propose a method for
learning the main concepts in domain summaries in

an unsupervised iterative procedure. The proposed
algorithm takes a set of unannotated summaries in
a given domain and produces auto-annotated sum-
maries which can be used for training information
extraction and text generation systems. Domain
adaptation is essential for text summarization and in-
formation extraction, and the last two decades have
seen a plethora of methods for supervised, semi-
supervised, and unsupervised learning from texts.

2001 August 24: Air Transat Flight 236 runs out
of fuel over the Atlantic Ocean and makes an
emergency landing in the Azores. Upon land-
ing some of the tires blow out, causing a fire that
is extinguished by emergency personnel on the
ground. None of the 304 people on board the
Airbus A330-200 were seriously injured.

Figure 1: Summary in the aviation domain annotated with
chunks

For example, in (Li et al., 2010) clustering is
applied to generate templates for specific entity
types (actors, companies, etc.) and patterns are
automatically produced that describe the informa-
tion in the templates. In (Chambers and Juraf-
sky, 2009) narrative schemas are induced from
corpora using coreference relations between par-
ticipants in texts. Transformation-based learning
is used in (Saggion, 2011) to induce templates
and rules for non-extractive summary generation.
Paraphrase templates containing concepts and typ-
ical strings were induced from comparable sen-
tences in (Barzilay and Lee, 2003) using multi-
sentence alignment to discover “variable” and fixed

13



structures. Linguistic patterns were applied to
huge amounts of non-annotated pre-classified texts
in (Riloff, 1996) to bootstrap information extrac-
tion patterns. Similarly, semi-supervised or unsu-
pervised methods have been used to learn ques-
tion/answering patterns (Ravichandran and Hovy,
2002) or text schemas (Bunescu and Mooney, 2007).
One current paradigm to learn from raw data is open
information extraction (Downey et al., 2004; Banko,
2009), which without any prior knowledge aims at
discovering all possible relations between pairs of
entities occurring in text. Our work tries to learn the
main concepts making up the template structure in
domain summaries, similar to (Chambers and Juraf-
sky, 2011). However, we do not rely on any source
of external knowledge (i.e. WordNet) to do so.

This paper presents an iterative-learning algo-
rithm which is able to identify the key components
of event summaries. We will show that the algorithm
can induce template-like representations in various
domains and languages. The rest of the paper is or-
ganized in the following way: In Section 2 we in-
troduce the dataset we are using for our experiments
and describe how we have prepated it for experimen-
tation. Then, in Section 3 we provide an overview
of our concept induction learnig algorithm while in
Section 4 we explain how we have instantiated the
algorithm for the experiments presented in this pa-
per. Section 5 describe the experiments and results
obtained and Section 6 discusses our approach com-
paring it with past research. Finally, in Section 7 we
close the paper with conclusions and future work.

2 Data and Data Preparation

The dataset used for this study – part of the CON-
CISUS corpus (Saggion and Szasz, 2012) – consists
of a set of 250 summaries in Spanish and English for
three different domains: aviation accidents, rail ac-
cidents, and earthquakes. This dataset makes it pos-
sible to compare the performance of learning proce-
dures across languages and domains. Based on com-
monsense, a human annotator developed an annota-
tion schema per domain to describe in a template-
like representation the essential elements (i.e., slots)
of each event. For example, for the aviation accident
domain these essential elements were: the date of
the accident, the number of victims, the airline, the

aircraft, the location of the accident, the flight num-
ber, the origin and destination of the flight, etc. The
dataset was then annotated following the schema us-
ing the GATE annotation tool. The human annota-
tions are used for evaluation of the concept discov-
ery algorithm. Each document in the dataset was au-
tomatically annotated using tools for each language.
We relied on basic processing steps to identify sen-
tences, words and word-roots, parts of speech, noun-
chunks, and named entities using the GATE system
for English (Maynard et al., 2002) and TreeTagger
for Spanish (Schmid, 1995).

Algorithm 1 Iterative Learning Algorithm: Main
1: Given: C: Corpus of Summaries Annotated with Chunks
2: Returns: LIST: A list of concepts discovered by the algorithm
3: begin
4: LIST← ∅;
5: while (EXIST CONCEPTS TO LEARN) do
6: CONCEPT← LEAN CONCEPT(C);
7: if (not FILTER CONCEPT(CONCEPT)) then
8: LIST← LIST ∪ CONCEPT;
9: end if

10: REMOVE USED CHUNKS(C);
11: end while
12: end

3 Learning Algorithm

The method is designed to learn the conceptual in-
formation in the summaries by extension (i.e., the
set of strings that make up the concept in a given
corpus) and by intension (i.e., an algorithm able to
recognise the concept members in new documents
in the domain) (Buitelaar and Magnini, 2005). Con-
cept extensions identified by our method in the En-
glish summaries in the aviation domain are listed in
Table 3. Each summary in the corpus can be seen as
a sequence of strings and chunks as shown in Fig-
ure 1 (named entities and noun chunks are shown in
boldface and they may overlap). The procedure to
learn a concept in the corpus of summaries is given
in pseudocode in Algorithm 2 which is repeatedly
invoked by a main algorithm to learn all concepts
(Algorithm 1).

The idea of the algorithm is rather simple, at
each iteration a document is selected for learning,
and from this document a single chunk (i.e., a noun
chunk or a named entity) available for learning is
selected as a seed example of a hypothetical concept
(the concept is given a unique name at each itera-

14



Concept Extension
1 Boeign 737-400; Boeign 777-

200ER; Airbus 300; ...
2 August 16; December 20; February

12; ...
3 Colombia; Algiers; Brazil; Mar-

seille; ...
4 102; 107; 145; 130; ...
5 Flight 243; Flight 1549; Flight

1907; ...
6 1988; 1994; 2001; ...

Table 1: Concepts Discovered in the Aviation Domain.
They correspond (in order) to the type of aircraft, date
of the incident, place of the accident, number of victims,
flight number, and year of the accident.

tion). The document is annotated with this seed as
a target concept and a classifier is trained using this
document. The trained classifier is then applied to
the rest of the documents to identify instances of the
hypothetical concept. If the classifier is unsuccess-
ful in identifying new instances, then the chunk used
in the current iteration is discarded from the learning
process, but if the classifier is successful and able to
identify instances of the hypothetical concept, then
the “best” annotated document is selected and added
to the training set. The classifier is re-trained using
the new added document and the process is repeated
until no more instances can be identified. A hypo-
thetical concept is kept only if there is enough sup-
port for it across the set of documents. The main
procedure calls the basic algorithms a number of
times while there are concepts to be learnt (or all
chunks have been used). The stopping criteria is the
number of concepts which could possibly be learnt,
an estimation of which is the average number of
chunks in a document.

4 Algorithm Instantiation

Experiments were carried out per domain and lan-
guage to assess the suitability of the algorithm to
the conceptual learning task. A number of points
in Algorithm 2 need clarification: the selection of a
document in line 4 of the algorithm can be carried
out using different informed procedures; for the ex-
periments described here we decided to select the
document with more available hypotheses, i.e., the
document with more chunks. For the selection of a

Algorithm 2 Iterative Learning Algorithm: Learn
Concept

1: Given: C: Corpus of summaries automatically annotated with
named entities and chunks

2: Returns: CONCEPT: A concept by extension and a trained algo-
rithm to discover instances of the concept in text

3: begin
4: DOC← SELECT DOCUMENT(C);
5: DOC← ANNOTATE WITH TARGET(DOC);
6: REST← C \ {DOC};
7: TRAINSET← {DOC};
8: CONTINUE← true;
9: while ((EXIST DOCUMENTS TO LEARN) AND CON-

TINUE) do
10: TRAIN(CLASSIFIER,TRAINSET);
11: APPLY(CLASSIFIER,REST);
12: if (DOCUMENT LEANED(REST)) then
13: BESTDOC← SELECT BEST(REST);
14: TRAINSET← TRAINSET ∪ {BESTDOC};
15: REST← REST \ {BESTDOC};
16: CLEAN(REST);
17: else
18: CONTINUE← false;
19: end if
20: end while
21: CONCEPT←< EXTENSION(TRAINSET); CLASSIFIER >;
22: return CONCEPT;
23: end

chunk to start the learning procedure in line 5 of the
algorithm we select the next available chunk in text
order. The classifier we used in line 10 of the al-
gorithm is instantiated to Support Vector Machines
(SVMs) which are distributed with the GATE sys-
tem (Li et al., 2004). The features we use for rep-
resenting the instance to be learnt are very superfi-
cial for these experiments: lemmas, parts-of-speech
tags, orthography, and named entity types of the
words surrounding the target concept to be learnt.
The SVMs provide as output a class together with
a probability which is essential to our method. We
use this probability for selecting the best document
in line 13 of the algorithm: the instance predicted
with the highest probability is located and the docu-
ment where this instance occurs is returned as “best
document”. In case no instances are learned (e.g.,
else in line 17), the iteration ends returning the ex-
tension learnt so far. Concerning Algorithm 1: in
line 5 (the while) we use as stopping criteria for
the maximum number of concepts to learn the av-
erage number of chunks in the corpus. In line 7, the
FILTER CONCEPT function evaluates the concept,
keeping it only if two criteria are met: (i) there are
not “too many” repetitions of a string in the discov-
ered concept and (ii) the discovered concept covers

15



a reasonable number of documents. With criteria
(i) we filter out a concept which contains repeated
strings: a concept could be formed simply by group-
ing together all repeated phrases in the set of doc-
uments (i.e. “the earthquake” or “the accident” or
“the plane”). While these phrases could be relevant
in the target domain they do not constitute a key con-
cept in our interpretation. Strings which are repeated
in the concept extension are more like the “backbone
structure” of the summaries in the domain. In our
experiments both criteria are experimental variables
and we vary them from 10% to 100% at 20% inter-
vals. In Section 5 we will present results for the best
configurations.

5 Experiments and Results

In order to evaluate the discovered concepts we have
treated learning as information extraction. In or-
der to evaluate them in this context we first need
to map each learnt concept onto one of the human
concepts. The mapping, which is based on the con-
cept extension, is straightforward: a discovered con-
cept is mapped onto the human concept with which
it has a majority of string matches. Note that we
match the discovered text offsets in the analysed
documents and not only the identified strings. In or-
der to evaluate the matching procedure we have used
precision, recall, and f-score measures comparing
the automatic concept with the human concept. Note
that we use a lenient procedure – counting as correct
strings those with a partial match. This is justified
since discovering the exact boundaries of a concept
instance is a very difficult task. Table 2 shows some
examples of the human annotated instances and re-
lated discovered one. It can be appreciated that the
learnt concepts have a reasonable match degree with
the human annotated ones.

Table 3 gives information extraction results per
domain and language for the best configuration of
the algorithm. The best scores are generally ob-
tained when coverage is set to 10% of the number
of summaries, except for the learning of conceptual
information in Spanish for the earthquake domain
where the system performs better for 10% summary
coverage. The parameter controlling string repeti-
tion in the concept extension should be kept small.
The obtained results are quite satisfactory consider-

Annotated Instance Discovered
Instance

PMTair (Airline) PMTair Flight
Boeing 777-200ER (Type-
OfAircraft)

Boeing 777

the Margalla Hills north-
est of Islamabad (Place)

Margalla

transporte de mercancias
(TypeOfTrain)

mercancias

29 abril 1997 (DateOfAc-
cident)

29 abril

Table 2: Examples of Concept Extensions Partially
Matched

Spanish
Domain (% rep, % cov) Prec. Rec. F
Aviation Accident (10%, 10%) 0.53 0.57 0.60
Rail Accident (10%, 10%) 0.66 0.67 0.66
Earthquake (10%, 30%) 0.41 0.30 0.35

English
Domain Prec. Rec. F
Aviation Accident (10%, 10%) 0.67 0.64 0.66
Rail Accident (30%, 10%) 0.52 0.33 0.44
Earthquake (10%, 10%) 0.40 0.19 0.26

Table 3: Performance in terms of Precision, Recall, and
F-Score per Domain and Language. % rep and % cov are
the repetition and coverage parameters used.

ing the small dataset and the limited use of linguistic
resources during learning. These results compare fa-
vorably to cross-validation results obtained using su-
pervised machine learning techniques (Saggion and
Szasz, 2011). Learning from the earthquake do-
main appears to be more challenging given the more
verbose characteristics of these texts. Even though
space restricions prevent us from showing all evalu-
ation results, in Table 4 we present detailed results
for the two domains and languages. Note that the
concepts listed constitute the slots of the induced do-
main template.

6 Discussion

Similar to active learning information extraction
techniques (Ciravegna and Wilks, 2003), the con-
cept discovery algorithm presented here is inspired
by techniques like learning by reading, where un-
familiar expressions in one document can be “ex-
plained” by association to expressions in similar

16



English - Aviation Accidents
Concept Precision Recall F-score
Airline 0.90 0.90 0.90
DateOfAccident 0.90 0.93 0.92
FlightNumber 0.91 0.94 0.92
NumberOfVictims 0.41 0.30 0.35
Place 0.34 0.54 0.42
TypeOfAccident 0.42 0.76 0.54
TypeOfAircraft 0.73 0.75 0.74
Year 0.94 0.97 0.95
All 0.67 0.64 0.66

Spanish - Train Accidents
Concept Precision Recall F-score
DateOfAccident 1.00 1.00 1.00
NumberOfVictims 0.97 0.91 0.94
Place 0.43 0.76 0.55
Survivors 0.55 0.96 0.70
TypeOfAccident 0.74 0.63 0.68
TypeOfTrain 0.35 0.30 0.32
All 0.66 0.67 0.66

Spanish - Earthquakes
Concept Precision Recall F-score
Country 0.53 0.36 0.43
DateOfEarthquake 0.96 0.94 0.95
Fatalities 0.37 0.28 0.32
Magnitude 0.54 0.32 0.40
Region 0.16 0.56 0.25
All 0.35 0.35 0.35

Table 4: Learning Evaluation in the Train and Aviation
Accident and Earthquake Domains (Spanish and English
Dataset)

document contexts. However, and unlike active
learning, human intervention is unnecessary in our
approach. Although the algorithm achieves reason-
ably lenient performance, strict (hard) evaluation in-
dicates that in each experimental condition perfor-
mance drops when a strict match is required. This
is expected given the difficulty of finding the right
instance boundaries based only on automatic chunk-
ing information. For this reason, we intend to carry
out additional experiments based on richer domain
independent features from a syntactic parser. We
have identified a number of reasons why some con-
cept instances can not be correctly associated with
their concepts. In the aviation domain, for example,
numeric expressions constitute the extensions of dif-
ferent concepts including: number of victims, crew
members, and number of survivors; it is a rather

common feature in the aviation domain to include
these different concepts together in one sentence,
making their “separation” complicated. Same ex-
planations apply to other tested domains: for exam-
ple locations playing the role of origin and destina-
tion of a given train or airplace are also sometimes
confused. Our work demonstrates the possibility of
learning conceptual information in several domains
and languages, while previous work (Chambers and
Jurafsky, 2011) has addressed sets of related do-
mains (e.g., MUC-4 templates) in English. Learn-
ing full conceptualizations from raw data is a daunt-
ing and difficult enterprise (Biemann, 2005). Here,
we provide a short-cut by proposing a method able
to learn the essential concepts of a domain by re-
lying on summaries which are freely available on
the Web. Our method is able to produce concep-
tualizations from a few documents in each domain
and language unlike recent open domain informa-
tion extraction which requires massive amount of
texts for relation learning (Banko, 2009). Our al-
gorithm has a reasonable computational complex-
ity, unlike alignment-based or clustering-based ap-
proaches (Barzilay and Lee, 2003), which are com-
putationally expensive.

7 Conclusions and Outlook

Domain adaptation is a time consuming and costly
procedure calling for the development of algorithms
and tools to facilitate its automation. In this paper
we have presented a novel algorithm for learning
information content in event summaries. The ap-
proach is fully unsupervised and based on the appli-
cation of an iterative algorithm which grows a con-
cept extension step-by-step. We have also proposed
an instantiation of the algorithm and demonstrated
its applicability to learning conceptual information
in three different domains and two languages. We
have obtained encouraging results, with the proce-
dure able to model the main conceptual information
in the summaries with lenient F-scores ranging from
0.25 to 0.66 F-scores depending on the language and
domain. There are, however, a number of avenues
that should be further explored such as the use of
a richer document representation based on syntactic
information and the development of additional pro-
cedures to improve instance boundary recognition.

17



Acknowledgements

We are grateful to the Advanced Research Fellow-
ship RYC-2009-04291 from Programa Ramón y Ca-
jal 2009, Ministerio de Economı́a y Competitividad,
Secretarı́a de Estado de Investigación, Desarrollo e
Innovación, Spain. We thank Biljana Drndarevic
who helped proofreading the paper.

References
M. Banko. 2009. Open Information Extraction for the

Web. Ph.D. thesis, University of Washington.
R. Barzilay and L. Lee. 2003. Learning to paraphrase: an

unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, NAACL ’03, pages 16–23, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

C. Biemann. 2005. Ontology Learning from Text: A
Survey of Methods. LDV Forum, 20(2):75–93.

P. Buitelaar and B. Magnini. 2005. Ontology learning
from text: An overview. In In Paul Buitelaar, P., Cimi-
ano, P., Magnini B. (Eds.), Ontology Learning from
Text: Methods, Applications and Evaluation, pages 3–
12. IOS Press.

R.C. Bunescu and R.J. Mooney. 2007. Learning to ex-
tract relations from the web using minimal supervi-
sion. In ACL.

N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
ACL/AFNLP, pages 602–610.

N. Chambers and D. Jurafsky. 2011. Template-Based In-
formation Extraction without the Templates. In ACL,
pages 976–986.

F. Ciravegna and Y. Wilks. 2003. Designing Adaptive
Information Extraction for the Semantic Web in Amil-
care. In S. Handschuh and S. Staab, editors, Annota-
tion for the Semantic Web. IOS Press, Amsterdam.

G. DeJong. 1982. An Overview of the FRUMP Sys-
tem. In W.G. Lehnert and M.H. Ringle, editors, Strate-
gies for Natural Language Processing, pages 149–
176. Lawrence Erlbaum Associates, Publishers.

D. Downey, O. Etzioni, S. Soderland, and D. S. Weld.
2004. Learning Text Patterns for Web Information
Extraction and Assessment. In Proceedings of AAAI
2004 Workshop on Adaptive Text Extraction and Min-
ing (ATEM’04).

Y. Li, K. Bontcheva, and H. Cunningham. 2004. An
SVM Based Learning Algorithm for Information Ex-
traction. Machine Learning Workshop, Sheffield.

P. Li, J. Jiang, and Y. Wang. 2010. Generating Templates
of Entity Summaries with an Entity-Aspect Model and
Pattern Mining. In Proceedings of ACL, Uppsala.
ACL.

D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-
gion, K. Bontcheva, and Y. Wilks. 2002. Architectural
Elements of Language Engineering Robustness. Jour-
nal of Natural Language Engineering – Special Issue
on Robust Methods in Analysis of Natural Language
Data, 8(2/3):257–274.

Michael P. Oakes and Chris D. Paice. 2001. Term ex-
traction for automatic abstracting. In D. Bourigault,
C. Jacquemin, and M-C. L’Homme, editors, Recent
Advances in Computational Terminology, volume 2 of
Natural Language Processing, chapter 17, pages 353–
370. John Benjamins Publishing Company.

D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In ACL,
pages 41–47.

E. Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. Proceedings of the Thir-
teenth Annual Conference on Artificial Intelligence,
pages 1044–1049.

H. Saggion and G. Lapalme. 2002. Generat-
ing Indicative-Informative Summaries with SumUM.
Computational Linguistics, 28:497–526, December.

H. Saggion and S. Szasz. 2011. Multi-domain Cross-
lingual Information Extraction from Clean and Noisy
Texts. In Proceedings of the 8th Brazilian Sympo-
sium in Information and Human Language Technol-
ogy, Cuiabá, Brazil. BCS.

H. Saggion and S. Szasz. 2012. The CONCISUS Corpus
of Event Summaries. In Proceedings of the 8th Lan-
guage Resources and Evaluation Conference (LREC),
Istanbul, Turkey. ELDA.

H. Saggion. 2011. Learning predicate insertion rules for
document abstracting. In Lecture Notes in Computer
Science, volume 6609, pages 301–312.

H. Schmid. 1995. Improvements in part-of-speech tag-
ging with an application to german. In In Proceedings
of the ACL SIGDAT-Workshop, pages 47–50.

18


