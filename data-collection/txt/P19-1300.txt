



















































Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3113–3124
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3113

Unsupervised Multilingual Word Embedding with Limited Resources
using Neural Language Models

Takashi Wada1, Tomoharu Iwata2,3, and Yuji Matsumoto1,3

1Nara Institute of Science and Technology
2NTT Communication Science Laboratories

3RIKEN Center for Advanced Intelligence Project (AIP)
1{wada.takashi.wp7,matsu}@is.naist.jp

2tomoharu.iwata.gy@hco.ntt.co.jp

Abstract

Recently, a variety of unsupervised methods
have been proposed that map pre-trained word
embeddings of different languages into the
same space without any parallel data. These
methods aim to find a linear transformation
based on the assumption that monolingual
word embeddings are approximately isomor-
phic between languages. However, it has been
demonstrated that this assumption holds true
only on specific conditions, and with limited
resources, the performance of these methods
decreases drastically. To overcome this prob-
lem, we propose a new unsupervised multi-
lingual embedding method that does not rely
on such assumption and performs well under
resource-poor scenarios, namely when only
a small amount of monolingual data (i.e.,
50k sentences) are available, or when the do-
mains of monolingual data are different across
languages. Our proposed model, which we
call ‘Multilingual Neural Language Models’,
shares some of the network parameters among
multiple languages, and encodes sentences of
multiple languages into the same space. The
model jointly learns word embeddings of dif-
ferent languages in the same space, and gen-
erates multilingual embeddings without any
parallel data or pre-training. Our experi-
ments on word alignment tasks have demon-
strated that, on the low-resource condition,
our model substantially outperforms existing
unsupervised and even supervised methods
trained with 500 bilingual pairs of words. Our
model also outperforms unsupervised meth-
ods given different-domain corpora across lan-
guages. Our code is publicly available1.

1 Introduction

Learning cross-lingual or multilingual word em-
bedding has been recognised as a very impor-
tant research topic in natural language processing

1https://github.com/twadada/multilingual-nlm

(NLP). Its objective is to map monolingual word
embeddings of different languages into a com-
mon space, and this research has been applied to
many multilingual tasks such as machine transla-
tion (Zou et al., 2013) and bilingual named entity
recognition (Rudramurthy et al., 2016). It also en-
ables the transfer of knowledge from one language
into another (Xiao and Guo, 2014; Adams et al.,
2017).

A number of supervised and unsupervised
methods have been proposed that obtain cross-
lingual word embeddings. Both supervised and
unsupervised methods aim to find such a linear
transformation that maps word embeddings in a
source language into a target language space. Su-
pervised methods employ bilingual dictionaries to
learn the mapping (Mikolov et al., 2013b; Xing
et al., 2015; Smith et al., 2017; Artetxe et al.,
2018a), while unsupervised ones utilise the sim-
ilarities or distance of word embeddings spaces
across different languages (Conneau et al., 2018;
Zhang et al., 2017a; Xu et al., 2018; Artetxe et al.,
2018b).

Since the common objective of most of the
supervised and unsupervised methods is to find
an orthogonal linear mapping between languages,
they heavily rely on the assumption that mono-
lingual word embeddings are approximately iso-
morphic. However, Søgaard et al. (2018) have
found that this assumption does not hold true in
general, and demonstrated that it requires three
specific conditions for the unsupervised method
of Conneau et al. (2018) to perform well. The
conditions are: Languages to align are linguisti-
cally similar; Monolingual word embeddings are
trained by the same algorithms; And the domains
of the monolingual corpora are similar across lan-
guages. In particular, the last condition is hard
to assume when dealing with resource-poor lan-
guages, for which unsupervised methods can be



3114

beneficial in reality.
To overcome the limitations of previous work,

we propose a new unsupervised multilingual word
embedding method called Multilingual Neural
Language Model (MNLM). In what follows, we
summarise our main contributions and novelty of
our proposed model:
Contributions

• We have discovered another limitation of the
existing unsupervised methods: They do not
perform well under the low-resource condi-
tion, namely when only small monolingual
corpora (i.e., 50k sentences) are available in
source and/or target languages. We have also
confirmed that word embeddings are far from
being isomorphic across languages under this
condition, indicating that the conventional
approachs are effective only for resource-rich
languages. This is a serious problem since
unsupervised learning is supposed to be ben-
eficial when dealing with low-resource lan-
guages.

• We propose a new unsupervised multilin-
gual word embedding method that overcomes
the limitations of the existing methods. Our
approach can successfully obtain multilin-
gual word embeddings under the challeng-
ing conditions when only small monolingual
corpora are available, or when the domains
of the monolingual corpora are different
across languages (we define these conditions
as ‘low-resource condition’ and ‘different-
domain condition’, respectively).

Novelty of Our Proposed Model
Whereas the existing unsupervised methods aim

to map pre-trained word embeddings between lan-
guages based on the strong assumption that mono-
lingual word embeddings are approximately iso-
morphic, our method does not require such as-
sumption or pre-trained word embeddings; in-
stead, it learns multilingual word embeddings
jointly using forward and backward LSTM lan-
guage models (Mikolov et al., 2010). Our model
shares the language models among multiple lan-
guages and aims to learn a common sequential
structure of different languages such as a com-
mon basic word order rule (e.g., subject-verb-
object). The word embeddings of each lan-
guage are trained independently, but sharing the
LSTM networks encourages the embeddings to be

mapped into the same space, generating multilin-
gual word embeddings. Our experiments show
that our unique approach makes it possible to ob-
tain multilingual word embeddings with limited
resources.

2 Related Work

Mikolov et al. (2013b) have proposed to obtain
cross-lingual word representations by learning a
linear mapping between two monolingual word
embedding spaces. Later, Xing et al. (2015)
have shown that enforcing an orthogonality con-
straint on the mapping improves the performance,
and that offers a closed form Procrustes solution
obtained from the singular value decomposition
(SVD) of Y XT

W ∗ = arg min
W

‖WX − Y ‖2 = UV T,

s.t. UΣV T = SVD(Y XT),
(1)

where W is a mapping matrix and Σ is a diagonal
matrix.

Following this work, a variety of unsupervised
methods have been proposed that obtain cross-
lingual representations without any bilingual su-
pervision. Zhang et al. (2017a) have proposed an
unsupervised method that obtains the linear trans-
formation using adversarial training (Goodfellow
et al., 2014): during the training, a discriminator is
trained to distinguish between the mapped source
embeddings and the target embeddings, while the
mapping matrix is trained to fool the discrimina-
tor. Conneau et al. (2018) employ a similar ap-
proach to Zhang et al. (2017a); they acquire an
initial matrix using adversarial training and re-
fine it by solving the orthogonal Procrustes prob-
lem. Zhang et al. (2017b) and Xu et al. (2018)
obtain cross-lingual representations by minimis-
ing the earth-mover’s distance and Sinkhorn dis-
tance, respectively. Artetxe et al. (2018b) pro-
pose an unsupervised self-learning method. Their
method starts from roughly aligning words across
languages using structural similarities of word em-
bedding spaces, and refines the word alignment
by repeating a robust self-learning method until
convergence. They show that their approach is
more effective than Zhang et al. (2017a) and Con-
neau et al. (2018) when languages to align are
distant or monolingual corpora are not compara-
ble across language. Recently, Chen and Cardie
(2018) and Alaux et al. (2018) have proposed un-



3115

supervised multilingual word embedding meth-
ods. Their methods map word embeddings of
more than two languages into a common space by
capturing the inter-dependencies among multiple
languages.

3 Our Model

3.1 Overview
We propose a new unsupervised multilingual word
embeddings method called Multilingual Neural
Language Model. Fig.1 briefly illustrates our pro-
posed model. The model consists of bidirectional
language models similar to ELMo (Peters et al.,
2018), and most of the parameters are shared
among multiple languages. In what follows, we
summaries which parameters are shared across
languages or specific to each language:

• Shared Parameters

–
−→
f and

←−
f : LSTM networks which per-

form as forward and backward language
models, independently.

– EBOSfwd and EBOSbkw : The embed-
dings of initial inputs to the forward
and backward language models, respec-
tively.

– WEOS: The linear mapping for
<EOS>, which is used to calculate the
probability of the end of a sentence at
every time-step.

• Specific Parameters to Language `

– E`: Word embeddings of language `
– W `: Linear projection of language `,

which is used to calculate the probabil-
ity distribution of the next word.

The LSTMs
−→
f and

←−
f are shared among mul-

tiple languages and capture a common language
structure. On the other hand, the word embed-
dings E` and linear projection W ` are specific to
each language `. Since different languages are en-
coded by the same LSTM functions, similar words
across different languages should have a similar
representation so that the shared LSTMs can en-
code them effectively. For instance, suppose our
model encodes an English sentence ‘He drives a
car.’ and its Spanish translation ‘El conduce un
coche.’ In these sentences, each English word
corresponds to each Spanish one in the same or-
der. Therefore, these equivalent words would have

Figure 1: Illustration of our proposed model Multilin-
gual Neural Language Models.

similar representations so that the shared language
models can encode the English and Spanish sen-
tences effectively. Although in general, each lan-
guage has its different grammar rules, the shared
language models are trained to roughly capture
the common structure such as common basic word
order rules (e.g., subject-verb-object) among dif-
ferent languages. Sharing <BOS> and <EOS>
symbols ensures that the beginning and end of the
hidden states are in the same space regardless of
language, which encourages the model to obtain
multilingual representations.

The limitation of our model is that it is
only applicable to the languages that have com-
mon word order rules such as subject-verb-object
and subject-object-verb. Although this limita-
tion may sound somewhat significant, our exper-
iments show that our model performs well not
only for closely related language pairs such as
French-English but also for linguistically distant
languages such as English-Finnish2 and Turkish-
Japanese. In fact, our experiments show that it is
extremely difficult for the existing unsupervised
methods as well as for our model to align very
distant languages which have different word order,
such as English and Japanese.

3.2 Network Structure

Suppose a sentence with N words in language `,
〈w`1..., w`N 〉. The forward and backward language
models calculate the probability of a next word w`t

2Finnish is often considered as a non-Indo-European
synthetic language, whereas English is often regarded as an
Indo-European analytic language.



3116

given the previous words:

p(w`1..., w
`
N ) =

N∏
t=1

p(w`t |w`1..., w`t−1). (2)

p(w`1..., w
`
N ) =

N∏
t=1

p(w`t |w`t+1..., w`N ). (3)

The tth hidden states h`t of the forward and back-
ward language models are calculated based on the
previous hidden state and word embedding,

−→
h `t =

−→
f (
−→
h `t−1, x

`
t−1), (4)

←−
h `t =

←−
f (
←−
h `t+1, x

`
t+1), (5)

x`t =


EBOSfwd if t = 0 ,
EBOSbkw if t = N+1,
E`(w`t) otherwise,

(6)

where
−→
f (·) and

←−
f (·) are the standard LSTM

functions. Note that the same word embedding
function E` is used among the forward and back-
ward language models. The probability distribu-
tion of the upcoming word w`t is calculated by
the forward and backward models independently
based on their current hidden state:

p(w`t |w`1..., w`t−1) = softmax(g`(
−→
h `t))), (7)

p(w`t |w`t+1..., w`N ) = softmax(g`(
←−
h `t)), (8)

g`(h`t) = [W
`(h`t),W

EOS(h`t)], (9)

where [x, y] means the concatenation of x and y.
WEOS andW ` are matrices with the size of (1×d)
and (V ` × d), where d is the size of hidden state
and V ` is the vocabulary size of language ` ex-
cluding <EOS>. As with the word embeddings,
those matrices are shared among the forward and
backward language models.

The proposed model is trained by maximising
the log likelihood of the forward and backward di-
rections for each language `:

L∑
l=1

S`∑
i=1

N i∑
t=1

log p(w`i,t|w`i,1...w`i,t−1;
−→
θ )

+ log p(w`i,t|w`i,t+1...w`i,N i ;
←−
θ ),

where L and S` denote the numbers of languages
and sentences of language `.

−→
θ and

←−
θ denote the

parameters for the forward and backward LSTMs−→
f and

←−
f , respectively.

4 Experiments

4.1 Data and Experimental Conditions
We trained our model and baselines under the fol-
lowing two conditions:

1. low-resource condition: Only small mono-
lingual corpora are available.

2. different-domain condition: Relatively
large monolingual corpora are available but
their domains are different across languages.

On each condition, we conducted cross-lingual
and multilingual embedding experiments, respec-
tively.

4.1.1 Cross-lingual Word Embedding
In the experiments of cross-lingual embedding,
we evaluated the quality of cross-lingual embed-
dings between seven pairs of source-target lan-
guages: {German, Spanish, French, Russian,
Czech, Finnish, Japanese}-English. For the low-
resource condition, we used subsets of News
Crawl monolingual corpora3. We used 50k sen-
tences for source languages, and either 50k or 1M
sentences for the target language (i.e., English).
This condition simulates two realistic scenarios;
the case when analysing inter-dependencies be-
tween multiple minor languages, or between mi-
nor and major languages.

For the different-domain condition, we added
{Tamil, Turkish}-Japanese pairs and North
Saami-{Finnish, English} pairs to the seven pairs
described above. North Saami is one of the minor
languages spoken in northern Finland, Sweden
and Norway, and it is so close to Finnish that
transfer learning between them is very effective
in dependency parsing (Lim et al., 2018). Note
that the basic word order of Tamil, Turkish and
Japanese is subject-object-verb (SOV), while
the one of the other languages is SVO. We used
Europarl corpus (Koehn, 2005) for English,
Wikipedia for Japanese, SIKOR North Saami
corpus4 for North Saami, and news data for the
other languages5. We extracted 1M sentences

3downloaded from http://www.statmt.org
and http://wortschatz.uni-leipzig.de/en/
download

4https://dataverse.no/dataset.xhtml?
persistentId=doi:10.18710/8AK7KZ

5The vocabulary sizes of Europarl and News Crawl
corpora in English are significantly different (79,258 v.s.
265,368 words), indicating the major differences between
these domains

http://www.statmt.org
http://wortschatz.uni-leipzig.de/en/download
http://wortschatz.uni-leipzig.de/en/download
https://dataverse.no/dataset.xhtml?persistentId=doi:10.18710/8AK7KZ
https://dataverse.no/dataset.xhtml?persistentId=doi:10.18710/8AK7KZ


3117

from these corpora except for North Saami,
for which we used the whole corpus which
contains 0.75M sentences. This different-domain
condition also simulates the cases of analysing
inter-dependencies among minor languages; large
monolingual data containing up to 1M sentences
may be available in each language, but it is hard
to assume that their domains are similar across
languages.

4.1.2 Multilingual Word Embedding
We trained multilingual word embeddings among
the four linguistically similar languages: German,
Spanish, French, and English. We conducted ex-
periments under the following three conditions:
(a) 50k sentences in News Crawl are used for each
language; (b) 50k sentences in News Crawl are
used for German, Spanish, French and 1M for En-
glish; (c) 1M sentences in News Crawl are used
for German, Spanish, French and 1M sentences in
Europarl for English.

4.2 Evaluation

In our experiment, we evaluated cross-lingual and
multilingual word embeddings on the word align-
ment tasks. In the cross-lingual experiments, we
used 1000 unique pairs of words in the dictionar-
ies and we report p@5 in each language. That is,
for each word in the 1000 source words, we ex-
tracted the 5 most similar words from the 1000
target words and checked how often the correct
translation is included in them. In the multilin-
gual experiments, we extracted 500 words aligned
among English, French, Spanish and German and
evaluated p@5 of ‘joint’ alignment among the four
languages. That is, for each English word we ex-
tracted the 5 most similar words in French, Ger-
man and Spanish independently, and evaluated
how often the correct translation of the English
word is included in all of the three languages. In
most language pairs, these 1000 and 500 words
were extracted from bilingual dictionaries pub-
lished by Conneau et al. (2018) so that they did not
contain any unknown words in all the training set-
tings6. For North Saami-{Finnish, English}, we
used the North Saami-Finnish dictionary7 used by
Lim et al. (2018) and aligned it with a Finnish-
English dictionary published by Conneau et al.

6For {Tamil, Turkish}-Japanese, we aligned the {Tamil,
Turkish}-English dictionaries with the Japanese-English dic-
tionary.

7https://github.com/jujbob/multilingual-models

(2018) to build a North Saami-English dictionary.
When only 50k sentences were used both for

the source and target languages, we trained all the
models three times with different random seeds
and calculated the average precision in both the
cross-lingual and multilingual experiments. This
is because unsupervised learning with small data
can be unstable.

4.3 Baseline

Baseline models aim to map pre-trained word em-
beddings of different languages into a common
space. For a fair comparison to our model, we
used word2vec (Mikolov et al., 2013a), that pre-
train word embeddings at a token level. We used
their code with the default setting8 except for the
embedding size and minimum frequency, which
were set the same as our model. Note that these
pre-trained embeddings were used only by base-
line models, not by ours.

As baselines of cross-lingual word embedding
methods, we chose Xu et al. (2018), Artetxe et al.
(2018b), and Conneau et al. (2018) with and with-
out normalisation. We also compared our model
against (weakly) supervised cross-lingual word
embedding methods (Artetxe et al., 2018a). The
supervised methods exploited 500 pairs of equiva-
lent words that are not used in the evaluation data9,
and weakly supervised methods exploited pseudo
bilingual pairs of words (auto seeds): the words
with the same spellings among different languages
were deemed as equivalent words. We trained the
cross-lingual baselines and our model in each lan-
guage pair.

As baselines of multilingual word embedding
models, we used Chen and Cardie (2018) with
or without auto seeds. We also compared our
model against the cross-lingual baselines. While
Chen and Cardie (2018) and our model jointly
train multilingual word embeddings, the cross-
lingual models independently map the word em-
beddings of German, Spanish, and French into
the English embedding space. Regarding Artetxe
et al. (2018b) and Artetxe et al. (2018a), we omit-
ted the re-weighting, whitening, and normalisation
processes in the multilingual experiments10. This

8The code is at https://code.google.com/
archive/p/word2vec, and the default algorithm is Con-
tinuous Bag of Words (CBOW) with its window size 5

9These 500 words were also extracted in the same way
as explained in 4.2.

10To omit these processes, we used ‘–orthogonal’ option

https://code.google.com/archive/p/word2vec
https://code.google.com/archive/p/word2vec


3118

src de es fr ru cs fi ja

Method
data size(tgt)

50k 1M 50k 1M 50k 1M 50k 1M 50k 1M 50k 1M 50k 1M

(weakly) supervised
Artetxe et al. (2018b)+char 5.6 2.5 12.1 5.1 9.2 4.0 2.9 1.4 5.0 0.5 1.3 1.6 2.1 9.2
Artetxe et al. (2018a)+dict 9.6 9.7 15.0 19.7 13.3 19.5 5.7 8.0 5.5 8.0 3.8 5.0 6.1 11.2
Conneau et al. (2018)+dict 11.1 9.7 18.0 20.4 19.2 20.7 4.7 5.2 7.1 4.8 1.7 3.2 7.5 18.7

unsupervised
Xu et al. (2018) 3.9 0.7 6.8 0.5 4.4 0.2 1.4 1.3 2.7 0.3 0.9 0.5 1.9 0.6
Artetxe et al. (2018b) 3.9 0.6 7.5 0.8 6.5 1.0 1.0 1.0 0.7 1.1 1.1 1.7 1.6 1.3
Conneau et al. (2018) 3.0 0.8 11.0 0.2 7.8 0.4 1.0 0.4 1.1 0.4 0.5 0.5 1.3 0.4
Conneau et al. (2018)+norm 2.1 0.7 11.3 0.7 9.2 0.3 0.7 0.3 0.6 0.2 0.6 0.3 1.7 0.5
OURS 14.2 20.8 26.1 37.5 21.8 35.3 13.6 14.1 13.8 18.8 12.7 12.4 2.3 2.3

Table 1: The precision p@5 of the cross-lingual word alignment task on the low-resource condition. We used
50k sentences for the source languages and either 50k or 1M sentences for the target language (English). The
best scores among the (weakly) supervised or unsupervised methods are bold-faced, and the best scores of all the
methods are underlined.

Method
src-tgt

de-en es-en fr-en ru-en cs-en fi-en tr-ja ta-ja ja-en se-fi se-en

(weakly) supervised
Artetxe et al. (2018b)+char 49.0 59.5 59.6 10.7 38.6 18.2 40.4 28.6 11.6 32.6 14.2
Artetxe et al. (2018a)+dict 35.6 49.7 49.4 38.5 38.5 28.3 25.8 46.6 24.5 42.9 20.2
Conneau et al. (2018)+dict 53.6 66.8 67.6 53.1 54.0 43.4 41.1 36.2 34.1 43.8 32.5

unsupervised
Xu et al. (2018) 0.8 3.2 32.7 0.8 6.9 3.2 5.8 0.1 0.6 20.4 1.6
Artetxe et al. (2018b) 5.6 47.4 47.1 9.0 3.1 1.2 3.7 1.5 1.8 13.6 0.3
Conneau et al. (2018) 0.8 0.7 1.7 0.5 0.9 1.6 1.3 0.6 1.4 13.1 0.7
Conneau et al. (2018)+norm 0.7 2.5 0.6 0.6 0.3 0.2 0.1 1.3 0.9 23.2 0.2
OURS 26.4 54.9 54.0 22.7 26.8 19.2 18.1 10.4 1.8 37.9 18.3

Table 2: The precision p@5 of cross-lingual word alignment task on the different-domain condition. The best
scores among the (weakly) supervised or unsupervised methods are bold-faced, and the best scores of all the
methods are underlined.

is because these processes transform both source
and target word embeddings, and that makes it im-
possible to map word embeddings of multiple lan-
guages into a single embedding space.

To implement these baselines, we used the code
published by the authors11,12,13(Conneau et al.,
2018; Artetxe et al., 2018b; Xu et al., 2018)

4.4 Training Settings

In the cross-lingual and multilingual experiments,
we trained our model among two and four lan-

in their code.
11https://github.com/facebookresearch/

MUSE
12https://github.com/artetxem/vecmap
13https://github.com/xrc10/

unsup-cross-lingual-embedding-transfer.

guages, respectively. When the size of the source
and target corpora were different, we conducted
oversampling to generate the same number of
mini-batches for source and target languages. We
trained our model for 10 epochs with the mini-
batch size 64, and stopped training when the train-
ing loss saturates (i.e., when the loss decreases by
less than 1% compared to the previous epoch). For
each iteration, our model alternately read mini-
batches of each language and updated its param-
eters. We set the size of word embeddings as 300,
and used two-layer LSTM networks for the for-
ward and backward language models, respectively.
We set the size of the hidden state as 300 and
1024 for the low-resource and different-domain
conditions. Dropout (Srivastava et al., 2014) is ap-

https://github.com/facebookresearch/MUSE
https://github.com/facebookresearch/MUSE
https://github.com/artetxem/vecmap
https://github.com/xrc10/unsup-cross-lingual-embedding-transfer.
https://github.com/xrc10/unsup-cross-lingual-embedding-transfer.


3119

Method
Condition

(a) (b) (c)

(weakly) supervised
Artetxe et al. (2018b)+char 2.3 0.6 44.6
Artetxe et al. (2018a)+dict 5.6 8.6 37.2
Conneau et al. (2018)+dict 6.4 7.0 51.6
Chen and Cardie (2018)+char 5.2 2.8 53.4

unsupervised
Xu et al. (2018) 1.1 0.0 0.0
Artetxe et al. (2018b) 0.9 0.0 5.2
Conneau et al. (2018) 1.3 0.0 0.0
Conneau et al. (2018)+norm 0.3 0.0 0.0
Chen and Cardie (2018) 1.0 0.0 3.0
OURS 10.4 16.2 37.0

Table 3: The precision p@5 of multilingual word align-
ment task on the three different conditions (a), (b),
and (c) described in 4.1.2. The best scores among the
(weakly) supervised or unsupervised methods are bold-
faced, and the best scores of all the methods are under-
lined.

plied to the hidden state with a rate of 0.3. We
used SGD (Bottou, 2010) as an optimiser with
the learning rate 1.0. All of the parameters of
our model including word embeddings were uni-
formly initialised in [-0.1, 0.1], and gradient clip-
ping (Pascanu et al., 2013) was used with the clip-
ping value 5.0. We included those words in vo-
cabulary that were used at least 3, 5, and 20 times
for 50k, 100k-250k, and 1M sentences in News
Crawl and Wikipedia. For Europarl and SIKOR
North Saami corpora, we set the threshold as 10.
We fed the most 15,000 frequent words to train
Xu et al. (2018) and the discriminator in Conneau
et al. (2018).

As a preprocess, we tokenized the monolin-
gual corpora using Moses toolkit14 for European
languages and Polyglot15 for Tamil, Turkish and
Japanese. We also lowercased all the corpora.

4.5 Results

4.5.1 Cross-lingual Word Embedding
Table 1 illustrates the results of the cross-lingual
word alignment task under the low-resource con-
dition. The methods with ‘+char’ use character
information to obtain a pseudo dictionary, and the
ones with ‘+dict’ use a gold dictionary that con-

14https://github.com/moses-smt/
mosesDecoder

15https://polyglot.readthedocs.io/en/
latest/Tokenization.html

tains 500 pairs of words. The table shows that
our model substantially outperforms the unsuper-
vised baseline models in all of the language pairs.
Our model also achieves better results than su-
pervised methods except in the Japanese-English
pair, which has different word order (SOV v.s
SVO). Another interesting finding is that when
the size of the target corpus increases from 50k
to 1M sentences, our model improves its perfor-
mance whereas the performance of the unsuper-
vised baseline models drops substantially. For in-
stance, when the size of the target corpus increases
from 50k to 1M, Conneau et al. (2018) decreases
the precision in Spanish-English from 11.0 to 0.2,
while our model increases the precision from 26.1
to 37.5.

Table 2 shows the results on the different-
domain condition. It shows that our method
achieves better results overall than the unsuper-
vised baseline models. The extremely poor per-
formance of Conneau et al. (2018) under this con-
dition is compatible with the results reported by
Søgaard et al. (2018). Regarding the Japanese-
English pair, none of the unsupervised methods
including ours perform well, demonstrating that it
is difficult to align languages without any super-
vision if the basic word order is different. Super-
vised methods, on the other hand, perform well in
all the languages and outperform our model. This
result indicates that even if domains of monolin-
gual corpora are different across languages, the
conventional approach of learning a linear trans-
formation can be effective with (weak) bilingual
supervision.

Impact of Data Size

To evaluate the effects of the data size on the
model performances, we increased the size of both
source and target corpora from 50k to 250k by 50k
sentences. All of these sentences were extracted
from News Crawl. Fig. 2 illustrates how p@5
changes depending on the data size. It shows that
our model overall performs better than the base-
lines, especially among the distant language pairs
such as Finnish-English. Although Artetxe et al.
(2018b) report positive results on word alignment
tasks between Finnish and English, our experi-
ments show that their method requires much larger
monolingual corpora such as Wikipedia on both
the source and target sides to achieve good perfor-
mance.

https://github.com/moses-smt/mosesDecoder
https://github.com/moses-smt/mosesDecoder
https://polyglot.readthedocs.io/en/latest/Tokenization.html
https://polyglot.readthedocs.io/en/latest/Tokenization.html


3120

50 100 150 200 250

5

10

15

20

25

30

35

German-English

50 100 150 200 250

10

20

30

40

50

Spanish-English

50 100 150 200 250

10

20

30

40

50

French-English

50 100 150 200 250
0

5

10

15

20

25

Russian-English

50 100 150 200 250
0

5

10

15

20

25

30

35
Czech-English

50 100 150 200 250
0

5

10

15

20

25

30
Finnish-English

OURS
Xu et al. (2018)

Artex et al. (2018b)
Conneau et al. (2017)

Conneau et al. (2017) + normalize

Figure 2: The change in p@5 achieved by the unsuper-
vised methods on word alignment tasks. The x-axis de-
notes the number of sentences (thousand) in the source
and target corpora, and the y-axis denotes the average
precision p@5 over three runs for each method.

src-tgt

lang
(src) de es fr ru cs fi ja

same domain
50k-50k 9.7 10.6 12.4 6.5 7.5 6.6 6.5
250k-250k 18.5 23.4 24.6 12.7 17.5 12.5 11.7
1M-1M 20.6 28.0 29.6 18.3 23.2 17.0 15.4
50k-1M 6.1 9.5 10.9 4.3 4.1 3.7 7.1

different domain
1M-1M 15.7 19.5 22.2 17.9 19.5 15.2 13.6

Table 4: The ratio (%) of the monolingual word embed-
dings being roughly isomorphic across a source and tar-
get language (English). Each row describes the number
of sentences in source and target corpora used to train
word embeddings, and each column denotes the source
language.

4.5.2 Multilingual Word Embedding

Table 3 describes the results under the three con-
ditions described in 4.1.2. It shows that our
model substantially outperforms the unsupervised
and supervised baseline models under the low-
resource conditions (a) and (b). As in the case
of 4.5.1, when the size of the English corpus in-
creases from 50k (a) to 1M (b), our model im-
proves its performance while the unsupervised
baselines perform worse. Under the different-
domain condition (c), our model also achieves
much better results than the unsupervised base-
lines, but cannot outperform supervised methods.

POS

lang
(src) de es fr ru cs fi

ADJ 25.2 36.8 35.5 23.1 38.6 20.7
ADV 68.8 82.6 71.9 82.6 81.6 66.2
NOUN 24.5 53.8 51.1 13.2 16.4 9.2
VERB 16.1 66.7 73.6 34.4 34.9 19.7

Table 5: The ratio (%) of correctly matched POS tags
using our model under the different-domain condition.
For each language, the best and worst ratios among the
four POS tags are bold-faced and underlined.

5 Analysis

5.1 Validation of Isomorphism

Our experiments show that our model substan-
tially outperforms both supervised and unsuper-
vised methods under the low-resource condition.
We conjecture that this large improvement is ow-
ing to our unique approach of obtaining multilin-
gual word embeddings; unlike the conventional
approach, our method does not assume that word
embedding spaces are approximately isomorphic
across languages. In fact, when word embeddings
are trained with small data, they should contain
a lot of noises and are unlikely to be isomorphic
across languages. This suggests that it would be
extremely difficult to learn a linear mapping across
languages using the existing unsupervised meth-
ods.

To verify this hypothesis, we investigated how
likely monolingual word embeddings were more
or less isomorphic across languages. For each pair
of a language ` and English, we sampled 10 pairs
of equivalent words from a bilingual dictionary
and built non-directed adjacency matrices of the
nearest neighbour graphs G(`) and G(en) inde-
pendently. Then, we conducted an element-wise
comparison of the two matrices and deemed them
as roughly isomorphic if more than 80% of the el-
ements are the same. Table 4 shows how often the
graphs were roughly isomorphic over 1,000 sam-
ples. The row indicates the size of the source and
target corpora. It clearly shows that monolingual
corpora trained on small data (i.e. 50k sentences)
are far from being isomorphic between any lan-
guage pair, and the linguistically distant languages
such as Finnish-English and Japanese-English are
less isomorphic than close languages. This re-
sult clearly explains why the existing unsupervised
methods do not perform well on the low-resource



3121

condition, or among distant language pairs. An-
other intriguing finding is that word embeddings
trained with 50k and 1M sentences in a source
and target languages are overall less isomorphic
than those trained with 50k source and target
sentences. This result explains why the perfor-
mance of the unsupervised baseline methods de-
creases given the additional target data in 4.5.1 and
4.5.2. Our method, on the other hand, can effec-
tively utilise the additional data to improve its per-
formance, demonstrating its robustness under the
low-resource condition.

5.2 POS tags of Matched Words

To analyse the performance of our model, we
checked Part-of-Speech (POS) tags of the English
words used in the word alignment task and investi-
gated what kind of words were correctly matched
by our model. Since a word is given without any
context in the word alignment task and it is not
possible to infer its POS tag, we assigned to each
word its most frequent POS tag in Brown Corpus
(Kucera and Francis, 1967). For instance, since
‘damage’ is used as a noun more often than as a
verb in Brown Corpus, we define its POS tag as
‘noun’. Table 5 shows p@5 of the word alignment
task grouped by the four major POS tags, namely
adjective, adverb, verb, and noun16. It clearly in-
dicates that an adverb can be easily matched in ev-
ery language pair. This would be because there are
less adverbs than other tags in the evaluation data,
and also because there are common word order
rules about an adverb among all the languages: an
adverb usually comes before an adjective to mod-
ify it, and when modifying a verb, it comes either
before or after it. Refer to the Appendix B for the
statistics regarding word order in each language.
Among French, Spanish and English, the match-
ing accuracy of a noun and verb is very high, and
their word order is in fact very similar; as shown
in the Appendix B, the basic word order of these
languages is strictly subject-verb-object, and that
makes it easy to align words among them. On
the other hand, the word order between a noun
and adjective is very different among these lan-
guages, explaining why the precision of matching
adjectives is lower than the other tags. As for the
other languages, they have more flexible word or-
der than English and that makes it difficult to align

16When there are X nouns and Y of them are matched
correctly in the alignment task, the ratio is 100Y

X
%

words across languages. For instance, in German,
Russian and Czech a subject sometimes comes af-
ter a verb, and in German and Finnish an object
can come before a verb. These findings clearly in-
dicate that our model employs sequential similari-
ties among different languages to obtain multilin-
gual word embeddings without any supervision.

6 Conclusion

In this paper, we proposed a new unsupervised
multilingual word embedding approach. Whereas
conventional methods aim to map pre-trained
word embeddings into a common space, ours
jointly generates multilingual word embeddings
by extracting a common language structure among
multiple languages. Our experiments on word
alignment tasks have demonstrated that our pro-
posed model substantially outperforms the exist-
ing cross-lingual and multilingual unsupervised
models under resource-poor conditions, namely
when only small data are available or when do-
mains of corpora are different across languages.
Under the first condition, our model even outper-
forms supervised methods trained with 500 bilin-
gual pairs of words. By analysing the nearest
neighbour graphs of monolingual word embed-
dings, we have verified that word embeddings are
far from being isomorphic when they are trained
on small data, explaining why existing unsuper-
vised methods did not perform well on the low-
resource condition. We have also found that the
performance of our model is closely related to
word order rules, and our model can align words
very well when they are used in a similar order
across different languages. Our future work is to
exploit character and subword information in our
model and see how those information affect the
performance in each language pair. It would be
also interesting to investigate how our approach
compares to the baselines given a large amount of
data such as Wikipedia.

7 Acknowledgement

We are grateful to all the anonymous reviewers for
their insightful comments and advice.

References
Oliver Adams, Adam Makarucha, Graham Neubig,

Steven Bird, and Trevor Cohn. 2017. Cross-lingual
word embeddings for low-resource language model-
ing. In Proceedings of the 15th Conference of the

http://aclweb.org/anthology/E17-1088
http://aclweb.org/anthology/E17-1088
http://aclweb.org/anthology/E17-1088


3122

European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
937–947. Association for Computational Linguis-
tics.

Jean Alaux, Edouard Grave, Marco Cuturi, and Ar-
mand Joulin. 2018. Unsupervised hyperalign-
ment for multilingual word embeddings. CoRR,
abs/1811.01124.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018a. Generalizing and improving bilingual word
embedding mappings with a multi-step framework
of linear transformations. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intel-
ligence, pages 5012–5019.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018b. A robust self-learning method for fully un-
supervised cross-lingual mappings of word embed-
dings. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 789–798. Association
for Computational Linguistics.

Léon Bottou. 2010. Large-scale machine learning
with stochastic gradient descent. In Proceedings
of the 19th International Conference on Compu-
tational Statistics (COMPSTAT’2010), pages 177–
187, Paris, France. Springer.

Xilun Chen and Claire Cardie. 2018. Unsupervised
multilingual word embeddings. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 261–270, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Interna-
tional Conference on Learning Representations.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative
adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 27, pages 2672–2680. Curran Associates,
Inc.

Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.

H. Kucera and W. N. Francis. 1967. Computational
analysis of present-day American English. Brown
University Press.

KyungTae Lim, Niko Partanen, and Thierry Poibeau.
2018. Multilingual Dependency Parsing for Low-
Resource Languages: Case Studies on North Saami
and Komi-Zyrian. Miyazaki, Japan. ELRA.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579–2605.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations (Workshop).

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. CoRR, abs/1309.4168.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, At-
lanta, GA, USA, 16-21 June 2013, pages 1310–1318.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. pages 2227–2237, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

V Rudramurthy, Mitesh M. Khapra, and Pushpak
Bhattacharyya. 2016. Sharing network parameters
for crosslingual named entity recognition. CoRR,
abs/1607.00198.

Samuel L. Smith, David H. P. Turban, Steven Hamblin,
and Nils Y. Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. In International Conference on Learning
Representations.

Anders Søgaard, Sebastian Ruder, and Ivan Vulić.
2018. On the limitations of unsupervised bilingual
dictionary induction. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 778–
788. Association for Computational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
parsing. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learning,
pages 119–129. Association for Computational Lin-
guistics.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational

http://arxiv.org/abs/1811.01124
http://arxiv.org/abs/1811.01124
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16935/16781
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16935/16781
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16935/16781
http://aclweb.org/anthology/P18-1073
http://aclweb.org/anthology/P18-1073
http://aclweb.org/anthology/P18-1073
http://leon.bottou.org/papers/bottou-2010
http://leon.bottou.org/papers/bottou-2010
http://aclweb.org/anthology/D18-1024
http://aclweb.org/anthology/D18-1024
http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
http://mt-archive.info/MTS-2005-Koehn.pdf
http://mt-archive.info/MTS-2005-Koehn.pdf
https://hal.archives-ouvertes.fr/hal-01856178
https://hal.archives-ouvertes.fr/hal-01856178
https://hal.archives-ouvertes.fr/hal-01856178
http://www.jmlr.org/papers/v9/vandermaaten08a.html
https://arxiv.org/pdf/1309.4168.pdf
https://arxiv.org/pdf/1309.4168.pdf
http://jmlr.org/proceedings/papers/v28/pascanu13.html
http://jmlr.org/proceedings/papers/v28/pascanu13.html
https://www.aclweb.org/anthology/N18-1202
https://www.aclweb.org/anthology/N18-1202
https://arxiv.org/pdf/1607.00198.pdf
https://arxiv.org/pdf/1607.00198.pdf
http://aclweb.org/anthology/P18-1072
http://aclweb.org/anthology/P18-1072
http://jmlr.org/papers/v15/srivastava14a.html
http://jmlr.org/papers/v15/srivastava14a.html
https://doi.org/10.3115/v1/W14-1613
https://doi.org/10.3115/v1/W14-1613
https://doi.org/10.3115/v1/W14-1613
https://doi.org/10.3115/v1/N15-1104
https://doi.org/10.3115/v1/N15-1104


3123

Linguistics: Human Language Technologies, pages
1006–1011. Association for Computational Linguis-
tics.

Ruochen Xu, Yiming Yang, Naoki Otani, and Yuexin
Wu. 2018. Unsupervised cross-lingual transfer of
word embedding spaces. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2465–2474. Association
for Computational Linguistics.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017a. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1959–1970. Association for Computational Linguis-
tics.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Earth mover’s distance minimization
for unsupervised bilingual lexicon induction. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
1934–1945. Association for Computational Linguis-
tics.

Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393–1398.
Association for Computational Linguistics.

dep-head(rel) en de es fr ru cs fi
N-V(nsubj) 93.7 77.5 89.0 95.4 80.4 77.1 88.2
N-V(obj) 0.7 56.3 0.5 0.2 4.0 10.0 20.8
ADJ-N 98.9 99.9 30.3 30.9 99.3 93.9 100.0
ADV-ADJ 98.3 93.6 95.0 99.2 96.6 94.9 98.8
ADV-V 75.6 65.8 68.7 61.2 79.1 80.4 47.9

Table 6: The ratio (%) of a dependent being put be-
fore its head. N, V, ADJ, and ADV denote noun, verb,
adjective and adverb, respectively. The dependency re-
lation of ADJ-N is amod, and the one of ADV-ADJ and
ADV-V is advmod. Refer to the download page of PUD
for the definition of the dependency relations.

A Visualisation

Figure 3 visualises the multilingual word em-
beddings obtained by our model and (Chen and
Cardie, 2018) under the low-resource condition.
It shows the most frequent 1000 words in Span-
ish, French, German and English. The figure
clearly shows that the word embeddings obtained
by (Chen and Cardie, 2018) form some clusters
based on their languages. In particular, many of
the German words are mapped near the centre of

15 10 5 0 5 10 15

15

10

5

0

5

10

15
OURS

Spanish
French
German
English

10 5 0 5 10 15
15

10

5

0

5

10

Chen and Cardie (2018)
Spanish
French
German
English

Figure 3: Scatter plot of multilingual word embeddings
of French, English, German and Spanish obtained by
our model and Chen and Cardie (2018) under the low-
resource condition. The embeddings are reduced to 2D
using tSNE (van der Maaten and Hinton, 2008).

the figure and make a large cluster. On the other
hand, the word embeddings trained by our model
are not clustered by language, indicating that our
model successfully maps word embeddings into a
common space.

B Word Order

To obtain statistics about word order rules in each
language, we used Parallel Universal Dependen-
cies (PUD) treebanks17. PUD contains 1000 par-
allel sentences aligned among 18 languages, and
those sentences are annotated morphologically
and syntactically according to Google universal
annotation guidelines. Since these sentences are
aligned among all the languages, it is possible
to compare the syntactical differences across lan-
guages.

Table 6 shows the ratio of a dependent being
put before its head in PUD treebanks in each lan-
guage. As can be seen, the word order of ADV-

17available at http://universaldependencies.org/

http://aclweb.org/anthology/D18-1268
http://aclweb.org/anthology/D18-1268
https://doi.org/10.18653/v1/P17-1179
https://doi.org/10.18653/v1/P17-1179
http://aclweb.org/anthology/D17-1207
http://aclweb.org/anthology/D17-1207
http://aclweb.org/anthology/D13-1141
http://aclweb.org/anthology/D13-1141


3124

ADJ (advmod) is very similar among all the lan-
guage pairs: an adverb is put before an adverb
to modify it. The order of ADV-V (advmod) is
rather flexible regardless of language, indicating
that an adverb can modify a verb from either left or
right. These common word order rules of adverbs
explain why our model successfully matched ad-
verbs very well in every language pair. The table
also indicates that the word order of N-V is very
similar among English, Spanish and French and
the basic word order is strictly subject-verb-object.
This explains why our model performed well over-
all among these languages. However, the word
order of ADJ-N is significantly different among
these languages, and that would lead to the low
performance of our model in matching adjectives.


