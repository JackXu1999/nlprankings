



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1353–1363
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1124

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1353–1363
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1124

Joint Optimization of User-desired Content in Multi-document
Summaries by Learning from User Feedback

Avinesh P.V.S and Christian M. Meyer
Research Training Group AIPHES and UKP Lab

Computer Science Department, Technische Universität Darmstadt
www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de

Abstract

In this paper, we propose an extractive
multi-document summarization (MDS)
system using joint optimization and active
learning for content selection grounded in
user feedback. Our method interactively
obtains user feedback to gradually im-
prove the results of a state-of-the-art inte-
ger linear programming (ILP) framework
for MDS. Our methods complement fully
automatic methods in producing high-
quality summaries with a minimum num-
ber of iterations and feedbacks. We
conduct multiple simulation-based exper-
iments and analyze the effect of feedback-
based concept selection in the ILP setup in
order to maximize the user-desired content
in the summary.

1 Introduction

The task of producing summaries from a clus-
ter of multiple topic-related documents has gained
much attention during the Document Understand-
ing Conference1 (DUC) and the Text Analysis
Conference2 (TAC) series. Despite a lot of re-
search in this area, it is still a major challenge to
automatically produce summaries that are on par
with human-written ones. To a large extent, this
is due to the complexity of the task: a good sum-
mary must include the most relevant information,
omit redundancy and irrelevant information, sat-
isfy a length constraint, and be cohesive and gram-
matical. But an even bigger challenge is the high
degree of subjectivity in content selection, as it can
be seen in the small overlap of what is considered

1http://duc.nist.gov/
2http://www.nist.gov/tac/

important by different users. Optimizing a sys-
tem towards one single best summary that fits all
users, as it is assumed by current state-of-the-art
systems, is highly impractical and diminishes the
usefulness of a system for real-world use cases.

In this paper, we propose an interactive concept-
based model to assist users in creating a personal-
ized summary based on their feedback. Our model
employs integer linear programming (ILP) to max-
imize user-desired content selection while using a
minimum amount of user feedback and iterations.
In addition to the joint optimization framework us-
ing ILP, we explore pool-based active learning to
further reduce the required feedback. Although
there have been previous attempts to assist users in
single-document summarization, no existing work
tackles the problem of multi-document summaries
using optimization techniques for user feedback.
Additionally, most existing systems produce only
a single, globally optimal solution. Instead, we
put the human in the loop and create a personal-
ized summary that better captures the users’ needs
and their different notions of importance.

Need for personalization. Table 1 shows the
ROUGE scores (Lin, 2004) of multiple existing
summarization systems, namely TF*IDF (Luhn,
1958), LexRank (Erkan and Radev, 2004), Text-
Rank (Mihalcea and Tarau, 2004), LSA (Gong
and Liu, 2001), KL-Greedy (Haghighi and Van-
derwende, 2009), provided by the sumy package3

and ICSI4 (Gillick and Favre, 2009; Boudin et al.,
2015), a strong state-of-the-art approach (Hong
et al., 2014) in comparison to the extractive up-
per bound on DUC’04 and DBS. DUC’04 is an
English dataset of abstractive summaries from ho-

3https://github.com/miso-belica/sumy
4https://github.com/boudinfl/sume

1353

https://doi.org/10.18653/v1/P17-1124
https://doi.org/10.18653/v1/P17-1124


Figure 1: Lexical overlap of a reference summary (cluster D31043t in DUC 2004) with the summary
produced by ICSI’s state-of-the-art system (Boudin et al., 2015) and the extractive upper bound

DUC’04 DBS
Systems R1 R2 SU4 R1 R2 SU4

TF*IDF .292 .055 .086 .377 .144 .144
LexRank .345 .070 .108 .434 .161 .180
TextRank .306 .057 .096 .400 .167 .167
LSA .294 .045 .081 .394 .154 .147
KL-Greedy .336 .072 .104 .369 .133 .134
ICSI .374 .090 .118 .452 .183 .190
UB .472 .210 .182 .848 .750 .532

Table 1: ROUGE-1 (R1), ROUGE-2 (R2), and
ROUGE-SU4 (SU4) scores of multiple systems
compared to the extractive upper bound (UB)

mogenous news texts, whereas DBS (Benikova
et al., 2016) is a German dataset of cohesive ex-
tracts from heterogeneous sources from the educa-
tional domain (see details in section 4.1). For each
dataset, we compute an extractive upper bound
(UB) by optimizing the sentence selection which
maximizes ROUGE-2, i.e., the occurrence of bi-
grams as in the reference summary (Cao et al.,
2016). Although some systems achieve state-of-
the-art performance, their scores are still far from
the extractive upper bound of individual reference
summaries as shown in Figure 1. This is due to
low inter-annotator agreement for concept selec-
tion: Zechner (2002) reports, for example, κ = .13
and Benikova et al. (2016) κ = .23. Most systems
try to optimize for all reference summaries instead
of personalizing, which we consider essential to
capture user-desired content.

Need for user feedback. The goal of concept
selection is finding the important information

within a given set of source documents. Although
existing summarization algorithms come up with
a generic notion of importance, it is still far from
the user-specific importance as shown in Figure 1.
In contrast, humans can easily assess importance
given a topic or a query. One way to achieve
personalized summarization is thus by combining
the advantages of both human feedback and the
generic notion of importance built in a system.
This allows users to interactively steer the summa-
rization process and integrate their user-specific
notion of importance.

Contributions. In this work, (1) we propose a
novel ILP-based model using an interactive loop
to create multi-document user-desired summaries,
and (2) we develop models using pool-based ac-
tive learning and joint optimization techniques
to collect user feedback on identifying important
concepts of a topic. In order to encourage the
community to advance research and replicate our
results, we provide our interactive summarizer im-
plementation as open-source software.5.

Our proposed method and our new interactive
summarization framework can be used in multiple
application scenarios: as an interactive annotation
tool, which highlights important sentences for the
annotators, as a journalistic writing aid that sug-
gests important, user-adapted content from multi-
ple source feeds (e.g., live blogs), and as a medical
data analysis tool that suggests key information as-
sisting a patient’s personalized medical diagnosis.

The rest of the paper is structured as follows:
In section 2, we discuss related work. Section 3

5https://github.com/UKPLab/
acl2017-interactive_summarizer

1354



introduces our computer-assisted summarization
framework using the concept-based optimization.
Section 4 describes our experiment data and setup.
In section 5, we then discuss our results and an-
alyze the performance of our models across dif-
ferent datasets. Finally, we conclude the paper in
section 6 and discuss future work.

2 Related Work

Previous works related to our research address ex-
tractive summarization as a budgeted subset selec-
tion problem, computer-assisted approaches, and
personalized summarization models.

Bugeted subset selection. Extractive summa-
rization systems that compose a summary from
a number of important sentences from the source
documents are by far the most popular solution
for MDS. This task can be modeled as a budgeted
maximum coverage problem. Given a set of sen-
tences in the document collection, the task is to
maximize the coverage of the subset of sentences
under a length constraint. The scoring function
estimates the importance of the content units for
a summary. Most previous works consider sen-
tences as content units and try different scoring
functions to optimize the summary.

One of the earliest systems by McDonald
(2007) models a scoring function by simultane-
ously maximizing the relevance scores of the se-
lected content units and minimizing their pairwise
redundancy scores. They solve the global opti-
mization problem using an ILP framework. Later,
several state-of-the-art results employed an ILP to
maximize the number of relevant concepts in the
created summary: Gillick and Favre (2009) use
an ILP with bigrams as concepts and hand-coded
deletion rules for compression. Berg-Kirkpatrick
et al. (2011) combine grammatical features relat-
ing to the parse tree and use a maximum-margin
SVM trained on annotated gold-standard com-
pressions. Woodsend and Lapata (2012) jointly
optimize content selection and surface realiza-
tion, Li et al. (2013) estimate the weights of the
concepts using supervised methods, and Boudin
et al. (2015) propose an approximation algorithm
to achieve the optimal solution. Although these
approaches achieve state-of-the-art performance,
they produce only one globally optimal summary
which is impractical for various users due to the
subjectivity of the task. Therefore, we research in-
teractive computer-assisted approaches in order to

produce personalized summaries.

Computer-assisted summarization. The ma-
jority of the existing computer-assisted summa-
rization tools (Craven, 2000; Narita et al., 2002;
Orǎsan et al., 2003; Orǎsan and Hasler, 2006)
present important elements of a document to the
user. Creating a summary then requires the hu-
man to cut, paste, and reorganize the important el-
ements in order to formulate a final text. The work
by Orǎsan and Hasler (2006) is closely related to
ours, since they assist users in creating summaries
for a source document based on the output of a
given automatic summarization system. However,
their system is neither interactive nor does it con-
sider the user’s feedback in any way. Instead, they
suggest the output of the state-of-the-art (single-
document) summarization method as a summary
draft and ask the user to construct the summary
without further interaction.

Personalized summarization. While most pre-
vious work focuses on generic summaries, there
have been a few attempts to take a user’s prefer-
ences into account. The study by Berkovsky et al.
(2008) shows that users prefer personalized sum-
maries that precisely reflect their interests. These
interests are typically modeled with the help of a
query (Park and An, 2010) or keyword annotations
reflecting the user’s opinions (Zhang et al., 2003).

In another strand of research, Dı́az and Gervás
(2007) create user models based on social tag-
ging and Hu et al. (2012) rank sentences by com-
bining informativeness scores with a user’s in-
terests based on fuzzy clustering of social tags.
Extending the use of social content, another re-
cent work showed how personalized review sum-
maries (Poussevin et al., 2015) can be useful in
recommender systems beyond rating predictions.
Although these approaches show that personal-
ized summaries are more useful than generic sum-
maries, they do not attempt to iteratively refine a
summary in an interactive user–system dialog.

3 Approach

The goal of our work is maximizing the user-
desired content in a summary within a minimum
number of iterations. To this end, we propose an
interactive loop that alternates the automatic cre-
ation of a summary and the acquisition of user
feedback to refine the next iteration’s summary.

1355



3.1 Summary Creation

Our starting point is the concept-based ILP sum-
marization framework by Boudin et al. (2015). Let
C be the set of concepts in a given set of source
documents D, ci the presence of the concept i in
the resulting summary, wi a concept’s weight, `j
the length of sentence j, sj the presence of sen-
tence j in the summary, and Occij the occurrence
of concept i in sentence j. Based on these defini-
tions, we formulate the following ILP:

max
∑

iwici (1)

∀j. ∑j`jsj ≤ L (2)
∀i, j. ∑jsjOccij ≥ ci (3)
∀i, j. sjOccij ≤ ci (4)
∀i. ci ∈ {0, 1} (5)
∀j. sj ∈ {0, 1} (6)

The objective function (1) maximizes the oc-
currence of concepts ci in the summary based on
their weights wi. The constraint formalized in (2)
ensures that the summary length is restricted to a
maximum length L, (3) ensures the selection of
all concepts in a sentence sj if sj has been selected
for the summary. Constraint (4) ensures that a con-
cept is only selected if it is present in at least one
of the selected sentences.

The two key factors for the performance of this
ILP are defining the concept set C and a method
to estimate the weights wi ∈ W . Previous works
have used word bigrams as concepts (Gillick and
Favre, 2009; Li et al., 2013; Boudin et al., 2015)
and either use document frequency (i.e. the num-
ber of source documents containing the concept)
as weights (Woodsend and Lapata, 2012; Gillick
and Favre, 2009) or estimate them using a su-
pervised regression model (Li et al., 2013). For
our implementation, we likewise use bigrams as
concepts and document frequency as weights, as
Boudin et al. (2015) report good results with this
simple strategy. Our approach is, however, not
limited to this setup, as our interactive approach
allows for any definition of C and W , including
potentially more sophisticated weight estimation
methods, e.g., based on deep neural networks. In
section 5.2, we additionally analyze how other no-
tions of concepts can be integrated into our ap-
proach.

3.2 Interactive Summarization Loop

Algorithm 1 provides an overview of our interac-
tive summarization approach. The system takes
the set of source documents D as input, derives
the set of concepts C, and initializes their weights
W . In line 5, we start the interactive feedback
loop iterating over t = 0, . . . , T . We first create
a summary St (line 6) by solving the ILP and then
extract a set of concepts Qt (line 7), for which
we query the user in line 11 As the user feed-
back in the current time step, we use the concepts
It ⊆ Qt that have been considered important by
the user. For updating the weights W in line 12,
we may use all feedback collected until the cur-
rent time step t, i.e., It0 =

⋃t
j=0 Ij and the set of

concepts Qt0 =
⋃t
j=0Qj seen by the user (with

Q−10 = ∅). If there are no more concepts to query
(i.e., Qt = ∅), we stop the iteration and return the
personalized summary St.

Algorithm 1 Interactive summarizer
1: procedure INTERACTIVESUMMARIZER()
2: input: Documents D
3: C ← extractConcepts(D)
4: W ← conceptWeights(C)
5: for t = 0...T do
6: St ← getSummary(C,W )
7: Qt ← extractConcepts(St)−Qt−10
8: if Qt = ∅ then
9: return St

10: else
11: It ← obtainFeedback(St, Qt)
12: W ← updateWeights(W, It0, Qt0)
13: end if
14: end for
15: end procedure

3.3 User Feedback Optimization

To optimize the summary creation based on
user feedback, we iteratively change the concept
weights in the objective function of the ILP setup.
We define the following models:

Accept model (ACCEPT). This model presents
the current summary St with highlighted concepts
Qt to a user and asks him/her to select all impor-
tant concepts It. We assign the maximum weight
MAX to all concepts in It and consider the re-
maining Qt − It as unimportant by setting their
weight to 0 (see equation 7 and 8). The intuition

1356



behind this baseline is that the modified scores
cause the ILP to prefer the user-desired concepts
while avoiding unimportant ones.

∀i ∈ It0. wi = MAX (7)
∀i ∈ Qt0 − It0. wi = 0 (8)

Joint ILP with User Feedback (JOINT). The
ACCEPT model fails in cases where the user could
not accept concepts that never appear in one of the
St summaries. To tackle this, in our JOINT model,
we change the objective function of the ILP in or-
der to create St by jointly optimizing importance
and user feedback. We thus replace the equation
(1) with:

max

{∑
i 6∈Qt0 wici −

∑
i∈Qt0 wici if t ≤ τ∑

iwici if t > τ
(9)

Equation (9) maximizes the use of concepts for
which we yet lack feedback (i 6∈ Qt0) and min-
imizes the use of concepts for which we already
have feedback (i ∈ Qt0). In this JOINT model, we
use an exploration phase t = 0 . . . τ to collect the
feedback, which terminates when the user does not
return any important concepts (i.e., It = ∅). In the
exploratory phase, the minus term in the equation
9 helps to reduce the score of the sentences whose
concepts have received feedback already. In other
words, it causes higher scores for sentences con-
sisting of concepts which yet lack feedback. Af-
ter the exploration step, we fall back to the orig-
inal importance-based optimization function from
equation (1).

Active learning with uncertainty sampling
(AL). Our JOINT model explores well in terms
of prioritizing the concepts which yet lack user
feedback. However, it gives equal probabilities
to all the unseen concepts. The AL model em-
ploys pool-based active learning (Kremer et al.,
2014) during the exploration phase in order to pri-
oritize concepts for which the model is most un-
certain. We distinguish the unlabeled concept pool
Cu = {Φ(x̃1),Φ(x̃2), ...,Φ(x̃N )} and the labeled
concept poolC` = {(Φ(x1), y1), (Φ(x2), y2), . . . ,
(Φ(xN ), yN )}, where each concept xi is repre-
sented as a d-dimensional feature vector Φ(xi) ∈
Rd. The labels yi ∈ {−1, 1} are 1 for all important
concepts in It0 and−1 for all unimportant concepts
in Qt0 − It0. Initially, the labeled concept pool C`

is small or empty, whereas the unlabeled concept
pool Cu is relatively large.

The learning algorithm is presented with a C =
C` ∪ Cu and is first called to learn a decision
function f (0) : Rd → R, where the function
f (0)(Φ(x̃)) is taken to predict the label of the input
vector Φ(x̃). Then, in each tth iteration, where t =
1, 2, . . . , τ , the querying algorithm selects an in-
stance of x̃t ∈ Cu for which the learning algorithm
is least certain. Thus, our learning goal of active
learning is to minimize the expected loss L (i.e.,
hinge loss) with limited querying opportunities to
obtain a decision function f (1), f (2), . . . , f (τ) that
can achieve low error rates:

minE(Φ(x),y)∈C`
[
L(f (t)(Φ(x)), y)

]
(10)

As the learning algorithm, we use a support vec-
tor machine (SVM) with a linear kernel. To obtain
the probability distribution over classes we use
Platt’s calibration (Platt, 1999), an effective ap-
proach for transforming classification models into
a probability distribution. Equation (11) shows the
probability estimates for f (t), where f (t) is the un-
calibrated output of the SVM in the tth iteration
and A, B are scalar parameters that are learned by
the calibration algorithm. The uncertainty scores
are calculated as described in the equation (12) for
all the concepts which lack feedback (Cu).

p(y | f (t)) = 1
1 + exp(Af (t) +B)

(11)

ui = 1− max
y∈{−1,1}

p(y | f (t)) (12)

For our AL model, we now change the objec-
tive function in order to create St by multiplying
uncertainty scores ui to the weights wi. We thus
replace the objective function from (9) with

max

{∑
i 6∈Qt0 uiwici if t ≤ τ∑
i wici if t > τ

(13)

Active learning with positive sampling (AL+).
One way to sample the unseen concepts is using
uncertainty as in AL, but another way is to actively
choose samples for which the learning algorithm
predicts as a possible important concept. In AL+,
we introduce the notion of certainty (1−ui) for the
positively predicted samples (f (t)(Φ(x̃i)) = 1) in

1357



Dataset Lang Topics Summary type Length

DBS de 10 Coherent extracts ≈ 500 words
DUC’01 en 30 Abstracts 100 words
DUC’02 en 59 Abstracts 100 words
DUC’04 en 50 Abstracts 100 words

Table 2: Statistics of the MDS datasets used

the objective function (1) for producing St

max

{∑
i 6∈Qt0 (1− ui)`iwici if t ≤ τ∑
i wici if t > τ

(14)

where `i =

{
0 if f (t)(Φ(x̃i)) = −1
1 if f (t)(Φ(x̃i)) = 1

(15)

4 Experimental Setup

4.1 Data
For our experiments, we mainly focus on the
DBS corpus, which is an MDS dataset of coher-
ent extracts created from heterogeneous sources
about multiple educational topics (Benikova et al.,
2016). This corpus is well-suited for our evalu-
ation setup, since we are able to easily simulate a
user’s feedback based on the overlap between gen-
erated and reference summary.

Additionally, we carry out experiments on the
most commonly used evaluation corpora pub-
lished by DUC/NIST from the generic multi-
document summarization task carried out in
DUC’01, DUC’02 and DUC’04. The documents
are all from the news domain and are grouped into
various topic clusters. Table 2 shows the proper-
ties of these corpora.

For evaluating the summaries against the refer-
ence summary we use ROUGE (Lin, 2004) with
the parameters suggested by (Owczarzak et al.,
2012) yielding high correlation with human judg-
ments (i.e., with stemming and without stopword
removal).6 Since DBS summaries do not have a
fixed length, we use a variable length parameter
L for evaluation, where L denotes the length of
the reference summary. All results are averaged
across all topics and reference summaries.

4.2 Data Pre-processing and Features
To pre-process the datasets, we perform tokeniza-
tion and stemming with NLTK (Loper and Bird,
2002) and constituency parsing with the Stanford
parser (Klein and Manning, 2003) for English and

6-n 4 -m -a -x -c 95 -r 1000 -f A -p 0.5 -t 0 -2 -4 -u

German. The parse trees will be used in sec-
tion 5.2 below to experiment with a syntactically
motivated concept notion.

As a concept’s feature representation Φ for
our active learning setups AL and AL+, we
use pre-trained word embeddings. We use the
Google News embeddings with 300 dimensions
by Mikolov et al. (2013) for English and the 100-
dimensional news- and Wikipedia-based embed-
dings by Reimers et al. (2014) for German. Ad-
ditionally, we add TF*IDF, number of stop words,
presence of named entities, and word capitaliza-
tion as features. Discrete features, such as part-of-
speech tags, are mapped into the word representa-
tion via lookup tables.

4.3 Oracle-Based Simulation of User
Feedback

The presence of a human in the loop typically de-
mands for a user study based evaluation, but to
collect sufficient data for various settings of our
models would be too expensive. Therefore, we
resort to an oracle-based approach, where the or-
acle is a system simulating the user by generat-
ing the feedback based on reference outputs. This
idea has been widely used in the development of
interactive systems (González-Rubio et al., 2012;
Knowles and Koehn, 2016) for studying the prob-
lem and exhibiting solutions in a theoretical and
controlled environment.

To simulate user feedback in our setting, we
consider all concepts It ⊆ Qt from the system-
suggested summary St as important if they are
present in the reference summary. Let Ref be the
set of concepts in the reference summary. In the
tth iteration, we return It = Qt ∩Ref as the simu-
lated user feedback. Thus, the goal of our system
is to reach the upper bound for a user’s reference
summary within a minimal number of iterations.

We limit our experiments to ten iterations, since
it appears unrealistic that users are willing to par-
ticipate in more feedback cycles. Petrie and Bevan
(2009) even report only three to five iterations.

5 Results and Analysis

5.1 Methods

Table 3 shows the evaluation results of our four
models. When evaluating a summarization sys-
tem, it is common to report the mean ROUGE
scores across clusters using all the reference sum-
maries. However, since we aim at personalizing

1358



Datasets ICSI UB ACCEPT JOINT AL AL+R1 R2 SU4 R1 R2 SU4 R1 R2 SU4 R1 R2 SU4 R1 R2 SU4 R1 R2 SU4

Concept Notion: Bigrams
DBS .451 .183 .190 .848 .750 .532 .778 .654 .453 .815 .707 .484 .833 .729 .498 .828 .721 .500
DUC’04 .374 .090 .118 .470 .212 .185 .442 .176 .165 .444 .180 .166 .440 .178 .160 .427 .166 .154
DUC’02 .350 .085 .110 .474 .216 .187 .439 .178 .161 .444 .182 .165 .448 .188 .165 .448 .184 .170
DUC’01 .333 .073 .105 .450 .213 .181 .414 .171 .156 .418 .167 .149 .435 .186 .163 .426 .181 .158
Concept Notion: Content Phrases
DBS .403 .135 .154 .848 .750 .532 .691 .531 .430 .742 .597 .419 .776 .652 .448 .767 .629 .440
DUC’04 .374 .090 .118 .470 .212 .185 .441 .176 .160 .441 .179 .162 .444 .180 .162 .422 .164 .150
DUC’02 .350 .085 .110 .474 .216 .187 .436 .181 .162 .444 .183 .165 .446 .185 .168 .442 .182 .162
DUC’01 .333 .073 .105 .450 .213 .181 .410 .165 .153 .417 .170 .156 .433 .182 .161 .420 .179 .154

Table 3: ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE SU-4 (SU4) achieved by our models after the
tenth iteration of the interactive loop in comparison to the upper bound and the basic ILP setup

Datasets ACCEPT JOINT AL AL+#F #F #F #F

Concept Notion: Bigrams
DBS 313 296 348 342
DUC’04 15 14 16 14
DUC’02 14 13 15 15
DUC’01 13 11 13 13
Concept Notion: Content Phrases
DBS 110 114 133 145
DUC’04 8 9 10 10
DUC’02 7 7 8 6
DUC’01 7 7 8 6

Table 4: Average amount of user feedback (#F)
considered by our models at the end of the tenth
iteration of the interactive summarization loop

the summary for an individual user, we evaluate
our models based on the mean ROUGE scores
across clusters per reference summary. In Table 4,
we additionally evaluate the models based on the
amount of feedback (#F = |IT0 |) taken by the or-
acles to converge to the upper bound within ten
iterations.

To examine the system performance based on
user feedback, we analyze our models’ perfor-
mance on multiple datasets. The results in Table 3
show that our idea of interactive multi-document
summarization allows users to steer a general sum-
mary towards a personalized summary consis-
tently across all datasets. From the results, we
can see that the AL model starts from the concept-
based ILP summarization and nearly reaches the
upper bound for all the datasets within ten itera-
tions. AL+ performs similar to AL in terms of
ROUGE, but requires less feedback (compare Ta-
ble 4). Furthermore, the ACCEPT and JOINT
models get stuck in a local optimum due to the
less exploratory nature of the models.

5.2 Concept Notion
Our interactive summarization approach is based
on the scalable global concept-based model which
uses bigrams as concepts. Thus, it is intuitive to
use bigrams for collecting user feedback as well.7

Although our models reach the upper bound when
using bigram-based feedback, they require a sig-
nificantly large number of iterations and much
feedback to converge, as shown in Table 4.

To reduce the amount of feedback, we also con-
sider content phrases to collect feedback. That
is, syntactic chunks from the constituency parse
trees consisting of non-function words (i.e., nouns,
verbs, adjectives, and adverbs). For DBS be-
ing extractive dataset, we use bigrams and con-
tent phrases as concepts, both for the objective
function in equation (1) and as feedback items,
whereas for the DUC datasets, the concepts are
always bigrams for both the feedback types (bi-
grams/content phrases). For DUC being abstrac-
tive, in the case of feedback given on content
phrases, they are projected back to the bigrams to
change the concept weights in order to have more
overlap of simulated feedback. Table 4 shows
feedbacks based on the content phrases reduces
the number of feedbacks by a factor of 2. Further-
more, when content phrases are used as concepts
for DBS, the performance of the models is lower
compared to bigrams, as seen in Table 3.

5.3 Datasets
Figure 2 compares the ROUGE-2 scores and the
amount of feedback used over time when applied
to the DBS and the DUC’04 corpus. We can see
from the figure that all models show an improve-
ment of +.45 ROUGE-2 after merely 4 iterations

7We prune bigrams consisting of only functional words.

1359



0 1 2 3 4 5 6 7 8 9 10
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

R
O

U
G

E
2

Upper bound
Active+
Active
Joint
Accept

0 1 2 3 4 5 6 7 8 9 10
# Iterations

0

50

100

150

200

250

300

350

#
Fe

ed
ba

ck
s

0 1 2 3 4 5 6 7 8 9 10
0.08

0.10

0.12

0.14

0.16

0.18

0.20

0.22

R
O

U
G

E
2

Upper bound
Active+
Active
Joint
Accept

0 1 2 3 4 5 6 7 8 9 10
# Iterations

0

2

4

6

8

10

12

14

16

#
Fe

ed
ba

ck
s

Figure 2: Analysis for the models over the DBS (left) and DUC’04 (right) datasets

on DBS. For DUC’04, the improvements are +.1
ROUGE-2 after ten iterations, which is relatively
notable considering the lower upper bound of .21
ROUGE-2. This is primarily because DBS is a
corpus of cohesive extracts, whereas DUC’04 con-
sists of abstractive summaries. As a result, the ora-
cles created using abstractive reference summaries
have lower overlap of concepts as compared to that
of the oracles created using extractive summaries.

For DBS, it becomes clear that the JOINT
model converges faster with an optimum amount
of feedback as compared to other models. AC-
CEPT takes relatively more feedbacks than
JOINT, but performs low in terms of ROUGE
scores. The best performing models are AL and
AL+, which reach closest to the upper bound.
This is clearly due to the exploratory nature of the
models which use semantic representations of the
concepts to predict uncertainty and importance of
possible concepts for user feedback.

For DUC’04, the JOINT model reaches the
closest to the upper bound, closely followed by
AL. The JOINT model consistently stays above all

other models and it gathers more important con-
cepts due to optimizing feedbacks for concepts
which lack feedback. Interestingly, AL+ performs
rather worse in terms of both ROUGE scores and
gathering important concepts. The primary reason
for this is the fewer feedback collected from the
simulation due to the abstractive property of ref-
erence summaries, which makes the AL+ model’s
prediction inconsistent.

5.4 Personalization

Figure 3 shows the performance of different mod-
els in comparison to two different oracles for the
same document cluster. For DBS, the JOINT,
AL, and AL+ models consistently converge to the
upper bound in 4 iterations for different oracles,
whereas ACCEPT takes longer for one oracle and
does not reach the upper bound for the other.

For DUC’04, JOINT and AL show consistent
performance across the oracles, whereas AL+ per-
forms worse than the state-of-the-art system (iter-
ation 0) for oracle created using abstractive sum-
maries as shown in Figure 3 (right) for User:1.

1360



0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8
R

O
U

G
E

2
User:1

0 1 2 3 4 5 6 7
# Iterations

0.2

0.3

0.4

0.5

0.6

0.7

0.8

R
O

U
G

E
2

User:2

Upper bound
Active+
Active
Joint
Accept

0.02

0.04

0.06

0.08

0.10

0.12

0.14

0.16

R
O

U
G

E
2

User:1

0 1 2 3 4 5 6 7
# Iterations

0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

0.40

R
O

U
G

E
2

User:2

Upper bound
Active+
Active
Joint
Accept

Figure 3: Analysis of models over cluster 7 from DBS (left) and cluster d30051t from DUC’04 (right)
respectively for different oracles

However, for User:2, we observe a ROUGE-2 im-
provement of +.1 indicating that the predictions
of the active learning system are better if there is
more feedback. Nevertheless, we expect that in
practical use, the human summarizers may give
more feedback similar to DBS in comparison to
DUC’04 simulation setting.

6 Conclusion and Future Work

We propose a novel ILP-based approach using in-
teractive user feedback to create multi-document
user-desired summaries. In this paper, we investi-
gate pool-based active learning and joint optimiza-
tion techniques to collect user feedback for iden-
tifying important concepts for a summary. Our
models show that interactively collecting feedback
consistently steers a general summary towards a
user-desired personalized summary. We empiri-
cally checked the validity of our approach on stan-
dard datasets using simulated user feedback and
observed that our framework shows promising re-
sults in terms of producing personalized multi-

document summaries.
As future work, we plan to investigate more

sophisticated sampling strategies based on active
learning and concept graphs to incorporate lexical-
semantic information for concept selection. We
also plan to look into ways to propagate feedback
to similar and related concepts with partial feed-
back, to reduce the total amount of feedback. This
is a promising direction as we have shown that in-
teractive methods help to create user-desired per-
sonalized summaries, and with minimum amount
of feedbacks, it has propitious use in scenarios
where user-adapted content is a requirement.

Acknowledgments

This work has been supported by the German Re-
search Foundation as part of the Research Training
Group Adaptive Preparation of Information from
Heterogeneous Sources (AIPHES) under grant
No. GRK 1994/1. We also acknowledge the use-
ful comments and suggestions of the anonymous
reviewers.

1361



References
Darina Benikova, Margot Mieskes, Christian M.

Meyer, and Iryna Gurevych. 2016. Bridging the gap
between extractive and abstractive summaries: Cre-
ation and evaluation of coherent extracts from het-
erogeneous sources. In Proceedings of the 26th In-
ternational Conference on Computational Linguis-
tics (COLING). Osaka, Japan, pages 1039–1050.
http://aclweb.org/anthology/C16-1099.

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan
Klein. 2011. Jointly learning to extract and
compress. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL/HLT). Portland, OR, USA, pages 481–490.
http://aclweb.org/anthology/P11-1049.

Shlomo Berkovsky, Timothy Baldwin, and Ingrid Zuk-
erman. 2008. Aspect-based personalized text sum-
marization. In Adaptive Hypermedia and Adaptive
Web-Based Systems. Proceedings of the 5th Interna-
tional Conference, Springer, Berlin/Heidelberg, vol-
ume 5149 of Lecture Notes in Computer Science,
pages 267–270. https://doi.org/10.1007/978-3-540-
70987-9 31.

Florian Boudin, Hugo Mougard, and Benoit Favre.
2015. Concept-based summarization using in-
teger linear programming: From concept prun-
ing to multiple optimal solutions. In Pro-
ceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing
(EMLNP). Lisbon, Portugal, pages 1914–1918.
http://aclweb.org/anthology/D15-1220.

Ziqiang Cao, Chengyao Chen, Wenjie Li, Sujian
Li, Furu Wei, and Ming Zhou. 2016. TG-
Sum: Build Tweet Guided Multi-Document Sum-
marization Dataset. In Proceedings of the Thir-
tieth AAAI Conference on Artificial Intelligence
(AAAI). Phoenix, AZ, USA, pages 2906–2912.
http://www.aaai.org/ocs/index.php/AAAI/AAAI16.

T. C. Craven. 2000. Abstracts produced using
computer assistance. Journal of the American
Society for Information Science 51(8):745–756.
https://doi.org/10.1002/(SICI)1097-4571(2000)51:8
<745::AID-ASI70>3.0.CO;2-Z.

Alberto Dı́az and Pablo Gervás. 2007. User-
model based personalized summarization. In-
formation Process Management 43(6):1715–1734.
https://doi.org/10.1016/j.ipm.2007.01.009.

Günes Erkan and Dragomir R. Radev. 2004.
LexRank: Graph-based Lexical Centrality As
Salience in Text Summarization. Journal of
Artificial Intelligence Research 22(1):457–479.
https://www.jair.org/papers/paper1523.html.

Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Langauge Processing. Boulder, CO, USA, pages
10–18. http://aclweb.org/anthology/W09-1802.

Yihong Gong and Xin Liu. 2001. Generic text sum-
marization using relevance measure and latent se-
mantic analysis. In Proceedings of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR). New Orleans, LA, USA, pages 19–25.
https://doi.org/10.1145/383952.383955.

Jesús González-Rubio, Daniel Ortiz-Martı́nez, and
Francisco Casacuberta. 2012. Active learning for
interactive machine translation. In Proceedings
of the 13th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics (EACL). Avignon, France, pages 245–254.
http://aclweb.org/anthology/E12-1025.

Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL). Boulder, CO, USA,
pages 362–370. http://aclweb.org/anthology/N09-
1041.

Kai Hong, John M. Conroy, Benoı̂t Favre, Alex
Kulesza, Hui Lin, and Ani Nenkova. 2014. A repos-
itory of state of the art and competitive baseline
summaries for generic news summarization. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC). Reyk-
javik, Iceland, pages 1608–1616. http://www.lrec-
conf.org/proceedings/lrec2014/summaries/1093.html.

Po Hu, Donghong Ji, Chong Teng, and Yujing Guo.
2012. Context-enhanced personalized social sum-
marization. In Proceedings of the 24th Inter-
national Conference on Computational Linguis-
tics (COLING). Mumbai, India, pages 1223–1238.
http://www.aclweb.org/anthology/C12-1075.

Dan Klein and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. In Pro-
ceedings of the 41st Annual Meeting on
Association for Computational Linguis-
tics (ACL). Sapporo, Japan, pages 423–430.
https://doi.org/10.3115/1075096.1075150.

Rebecca Knowles and Philipp Koehn. 2016. Neural
interactive translation prediction. In Proceedings
of the Conference of the Association for Machine
Translation in the Americas (AMTA).

Jan Kremer, Kim Steenstrup Pedersen, and Christian
Igel. 2014. Active learning with support vector
machines. Wiley Interdisciplinary Reviews: Data
Mining and Knowledge Discovery 4(4):313–326.
https://doi.org/10.1002/widm.1132.

Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ILP for extractive summariza-
tion. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Sofia, Bulgaria, pages 1004–
1013. http://aclweb.org/anthology/P13-1099.

1362



Chin-Yew Lin. 2004. ROUGE: A Package for Au-
tomatic Evaluation of Summaries. In Text Sum-
marization Branches Out: Proceedings of the
ACL-04 Workshop. Barcelona, Spain, pages 74–81.
http://aclweb.org/anthology/W04-1013.

Edward Loper and Steven Bird. 2002. NLTK: The
Natural Language Toolkit. In Proceedings of the
ACL-02 Workshop on Effective Tools and Method-
ologies for Teaching Natural Language Process-
ing and Computational Linguistics. pages 63–70.
https://doi.org/10.3115/1118108.1118117.

H. P. Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of Re-
search and Development 2(2):159–165.
https://doi.org/10.1147/rd.22.0159.

Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Ad-
vances in Information Retrieval. Proceedings of the
29th European Conference on IR Research (ECIR),
Springer, Berlin/Heidelberg, volume 4425 of Lec-
ture Notes in Computer Science, pages 557–564.
https://doi.org/10.1007/978-3-540-71496-5 51.

Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Text. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP). Barcelona, Spain,
pages 404–411. http://aclweb.org/anthology/W04-
3252.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. CoRR abs/1301.3781.
http://arxiv.org/abs/1301.3781.

Masumi Narita, Kazuya Kurokawa, and Takehito
Utsuro. 2002. A Web-based English Abstract
Writing Tool Using a Tagged E–J Parallel Cor-
pus. In Proceedings of the Third International
Conference on Language Resources and Evalua-
tion (LREC). Las Palmas, Spain. http://www.lrec-
conf.org/proceedings/lrec2002/sumarios/137.htm.

Constantin Orǎsan and Laura Hasler. 2006. Computer-
aided Summarisation: What the User Really Wants.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC).
Genoa, Italy, pages 1548–1551. http://www.lrec-
conf.org/proceedings/lrec2006/summaries/52.html.

Constantin Orǎsan, Ruslan Mitkov, and Laura Hasler.
2003. CAST: a computer-aided summarisation tool.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics (EACL). Budapest, Hungary, pages 135–
138. http://aclweb.org/anthology/E03-1066.

Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in sum-
marization. In Proceedings of Workshop on Eval-
uation Metrics and System Comparison for Auto-
matic Summarization. Montréal, Canada, pages 1–9.
http://aclweb.org/anthology/W12-2601.

Sun Park and Dong Un An. 2010. Auto-
matic Query-based Personalized Summarization
That Uses Pseudo Relevance Feedback with NMF.
In Proceedings of the 4th International Con-
ference on Ubiquitous Information Management
and Communication (ICUIMC). pages 61:1–61:7.
https://doi.org/10.1145/2108616.2108690.

Helen Petrie and Nigel Bevan. 2009. The evaluation
of accessibility, usability, and user experience. In
Constantine Stephanidis, editor, The Universal Ac-
cess Handbook, Boca Raton: CRC Press, Human
Factors and Ergonomics, chapter 20, pages 1–16.
https://doi.org/10.1201/9781420064995-c20.

John C. Platt. 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regularized
likelihood methods. In Advances In Large Margin
Classifiers. MIT Press, pages 61–74.

Mickaël Poussevin, Vincent Guigue, and Patrick Gal-
linari. 2015. Extended recommendation framework:
Generating the text of a user review as a personalized
summary. In Proceedings of the 2nd Workshop on
New Trends on Content-Based Recommender Sys-
tems co-located with 9th ACM Conference on Rec-
ommender Systems (RecSys 2015), Vienna, Austria,
September 16-20, 2015. pages 34–41. http://ceur-
ws.org/Vol-1448/paper7.pdf.

Nils Reimers, Judith Eckle-Kohler, Carsten Schnober,
Jungi Kim, and Iryna Gurevych. 2014. GermEval-
2014: Nested Named Entity Recognition with Neu-
ral Networks. In Workshop Proceedings of the 12th
Edition of the KONVENS Conference. Hildesheim,
Germany, pages 117–120.

Kristian Woodsend and Mirella Lapata. 2012. Multi-
ple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP/CoNLL). Jeju Island, Korea,
pages 233–243. http://aclweb.org/anthology/D12-
1022.

Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Journal of Computational Linguistics 28(4):447–
485. https://doi.org/10.1162/089120102762671945.

Haiqin Zhang, Zheng Chen Wei-ying Ma, and
Qingsheng Cai. 2003. A study for docu-
ments summarization based on personal annota-
tion. In Proceedings of the HLT-NAACL 03
on Text Summarization Workshop. pages 41–48.
https://doi.org/10.3115/1119467.1119473.

1363


	Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback

