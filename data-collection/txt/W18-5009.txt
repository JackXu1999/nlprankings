



















































Automatic Token and Turn Level Language Identification for Code-Switched Text Dialog: An Analysis Across Language Pairs and Corpora


Proceedings of the SIGDIAL 2018 Conference, pages 80–88,
Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics

80

Automatic Token and Turn Level Language Identification for
Code-Switched Text Dialog: An Analysis Across Language Pairs and

Corpora

Vikram Ramanarayanan
Educational Testing Service R&D
90 New Montgomery St, #1500

San Francisco, CA
vramanarayanan@ets.org

Robert Pugh
Educational Testing Service R&D
90 New Montgomery St, #1500

San Francisco, CA
rpugh@ets.org

Abstract

We examine the efficacy of various
feature–learner combinations for lan-
guage identification in different types of
text-based code-switched interactions –
human-human dialog, human-machine
dialog, as well as monolog – at both the
token and turn levels. In order to examine
the generalization of such methods across
language pairs and datasets, we analyze
ten different datasets of code-switched
text. We extract a variety of character- and
word-based text features and pass them
into multiple learners, including condi-
tional random fields, logistic regressors,
and recurrent neural networks. We further
examine the efficacy of character-level
embedding and GloVe features in im-
proving performance and observe that our
best-performing text system significantly
outperforms the majority vote baseline
across language pairs and datasets.

1 Introduction

Code-switching refers to multilingual speakers’
alternating use of two or more languages or lan-
guage varieties within the context of a single con-
versation or discourse in a manner consistent with
the syntax and phonology of each variety (Mil-
roy and Muysken, 1995; Wei, 2000; MacSwan,
2004; Myers-Scotton, 2006). Increasing global-
ization and the continued rise of multilingual so-
cieties around the world makes research and de-
velopment of automated tools for the processing
of code-switched speech a very relevant and in-
teresting problem for the scientific community.
In our case, an important additional motivating
factor for studying and developing tools to elicit

and process code-switched or crutched1 language
comes from the education domain, specifically
language learning. Recent findings in the litera-
ture suggest that strategic use of code-switching
of bilinguals’ L1 and L2 in instruction serves mul-
tiple pedagogic functions across lexical, cultural,
and cross-linguistic dimensions, and could en-
hance students’ bilingual development and maxi-
mize their learning efficacy (Wheeler, 2008; Jiang
et al., 2014). This seems to be a particularly effec-
tive strategy especially when instructing low profi-
cient language learners (Ahmad and Jusoff, 2009).
Therefore, the understanding of code-switched di-
alog and development of computational tools for
automatically processing code-switched conversa-
tions would provide an important pedagogic aid
for teachers and learners in classrooms, and po-
tentially even enhance learning at scale and per-
sonalized learning.

Automated processing of code-switched text di-
alog poses an interesting, albeit challenging prob-
lem for the scientific community. This is be-
cause the hurdles observed during traditional di-
alog processing tasks such as spoken language
understanding (SLU), natural language generation
(NLG) and dialog management (DM) are exacer-
bated in the case of code-switched text where the
language the speaker is using at any given instant
is not known apriori. Integrating an explicit lan-
guage identification (or LID) step into the process-
ing pipeline can potentially alleviate these issues.
Take for example a use case of designing conver-
sational applications for non-native English lan-
guage learners (ELLs) from multiple native lan-
guage (or L1) backgrounds. Many such learners
tend to “crutch” on their L1 while speaking in the
target language (or L2) that they are learning, es-

1Crutching refers to language learners relying on one
language to fill in gaps in vocabulary or knowledge in the
other (OConnor and Crawford, 2015).



81

pecially if they are low proficiency learners (Little-
wood and Yu, 2011), resulting in mixed-language
speech. In such a case, LID becomes particu-
larly important for SLU and DM, where the dialog
designer/language expert may want the conversa-
tional agent to perform different dialog actions de-
pending on whether the speaker used his/her L1
alone, the L2 alone, or a mixture of both during
the previous turn.

Researchers have made significant progress in
the automated processing of code-switched text
in recent years (Solorio et al., 2014; Bali et al.,
2014; Molina et al., 2016). While Joshi (Joshi,
1982) had already proposed a formal computa-
tional linguistics framework to analyze and parse
code-switched text in the early eighties, it was not
until recently that significant strides were made
in the large-scale analysis of code-switched text.
These have been facilitated by burgeoning mul-
tilingual text corpora (thanks largely to the rise
of social media) and corpus analysis studies (see
for example Solorio et al., 2014; Bali et al., 2014;
Molina et al., 2016), which have in turn facili-
tated advances in automated processing. Particu-
larly relevant to our work is prior art on predicting
code-switch points (Solorio and Liu, 2008) and
language identification (Barman et al., 2014; King
and Abney, 2013). Researchers have made much
progress on LID in code-switched text (tweets, in
particular) thanks to recent workshops dedicated
to the topic (Solorio et al., 2014; Molina et al.,
2016). One of the top-performing systems used
character n-gram, prefix and suffix features, letter
case and special character features and explored
logistic regression and conditional random field
(CRF) learners to achieve the best performance
for Spanish-English codeswitched text (Shirvani
et al., 2016). Yet another successful system lever-
aged bi-directional long short term memory net-
works (BLSTMs) and CRFs (along with word and
character embedding features) on both Spanish-
English and Standard Arabic-Egyptian language
pairs (Samih et al., 2016).

While there is comparatively less work in
the literature on automated analysis of code-
switched speech and dialog, the number of cor-
pora and studies is steadily growing in sev-
eral language pairs – for instance, Mandarin–
English (Li et al., 2012; Lyu et al., 2015),
Cantonese–English (Chan et al., 2005) and Hindi–
English (Dey and Fung, 2014). As far as

dialog is concerned, the Bangor Corpus con-
sists of human-human dialog conversations in
Spanish–English, Welsh–English and Spanish–
Welsh (Donnelly and Deuchar, 2011). More
recently, Ramanarayanan and Suendermann-Oeft
(2017) also proposed a multimodal dialog corpus
of human-machine Hindi–English and Spanish–
English code-switched data. In order to under-
stand how turn-level LID systems for dialog per-
form across different languages and corpora, this
paper explores the efficacy of different text-based
features on multiple human–human and human–
machine dialog corpora of code-switched data in
multiple language pairs. To that end, this pa-
per builds on other recent work that examined
this phenomenon for the Bangor Miami Corpus
of English–Spanish human–human dialog (Rama-
narayanan et al., 2018) and expands it significantly
(note however, that this study does not examine
speech data). To our knowledge, this is the first
such comprehensive exploration of turn-level LID
performance in human-human code-switched text
dialog. With that in mind, the specific contribu-
tions of this paper are to examine:

1. The performance of: (i) a range of text fea-
tures (including word- and character-level
embedding features) for (ii) both word-level
and turn-level LID;

2. How generalizable these features are across
different datasets comprising different lan-
guage pairs and styles of codeswitched text
– human-human dialog, human-machine dia-
log and monolog (tweets);

3. Turn-level LID performance by (i) using
word-level LID followed by aggregation over
the entire turn v.s. (ii) directly training clas-
sifiers at the turn-level.

The rest of this paper is organized as follows:
Section 2 describes the various corpora used for
our turn-level LID experiments. We then elucidate
the various featuresets and learners we explored
in Sections 3 and 4 respectively, followed by de-
tails of the experimental setup in Section 5. Next,
Section 6 presents the results of our LID experi-
ments as well as analyses of performance numbers
across featureset-learner combinations, language
pairs and dataset style. Finally, we conclude with a
discussion of current observations and an outlook
for future work in Section 7.



82

2 Data

We used a total of ten code-switched corpora for
our experiments across language pairs and inter-
action type, summarized briefly below. Note that
although some of these corpora contain speech as
well, we only consider the text transcripts for the
purposes of this paper.

• Bangor University in Wales has assem-
bled three corpora of human-human code-
switched dialog2: (i) The Miami corpus of
code-switched English and Spanish, (ii) the
Siarad corpus of English and Welsh, and (iii)
the Patagonia corpus of Spanish and Welsh.

• The SEAME corpus3 comprises approxi-
mately 192 hours of Mandarin-English code-
switching human-human dialog from 156
speakers with associated transcripts (Lyu
et al., 2015). The speakers were gender-
balanced (49.7% female, 50.3% male) and
between 19 and 33 years of age. Over 60% of
the speakers were Singaporean; the rest were
Malaysian.

• The HALEF corpora of code-switched
human-machine dialog comprise English–
Hindi and English–Spanish language pairs.
In each language pair, bilingual human
participants were encouraged to use code-
switched speech as they interacted with a
cloud-based multimodal dialog system to or-
der food and drink from a virtual coffee shop
barista. For more details, see Ramanarayanan
and Suendermann-Oeft (2017).

• Finally, in addition to these dialog corpora,
we also used monolog corpora for compar-
ison – four Twitter datasets used in the 1st

shared task on language identification held at
EMNLP 2016 (Solorio et al., 2014). These
consisted of code-switched tweets in the
following language pairs: English–Spanish,
English–Mandarin, English–Nepalese, and
Modern Standard Arabic–Egyptian Arabic.

The transcripts were processed by performing
whitespace tokenization on each turn, and remov-
ing event descriptions (such as “&=laugh”) and

2http://bangortalk.org.uk/
3https://catalog.ldc.upenn.edu/ldc2015s04 Ta
bl

e
1:

St
at

is
tic

s
of

th
e

di
ffe

re
nt

co
de

-s
w

itc
hi

ng
co

rp
or

a
co

ns
id

er
ed

in
th

is
pa

pe
r.

N
ot

e
th

at
H

2H
st

an
ds

fo
r

hu
m

an
-t

o-
hu

m
an

w
hi

le
H

2M
st

an
ds

fo
r

hu
m

an
-t

o-
m

ac
hi

ne
.

L
an

gu
ag

e
Pa

ir
E

N
G

-S
PA

E
N

G
-C

H
I

E
N

G
-W

E
L

W
E

L
-S

PA
E

N
G

-H
IN

E
N

G
–N

E
P

M
SA

–E
G

Y
C

or
pu

s
N

am
e

B
an

go
r

H
A

L
E

F
Tw

itt
er

SE
A

M
E

Tw
itt

er
B

an
go

r
B

an
go

r
H

A
L

E
F

Tw
itt

er
Tw

itt
er

Ty
pe

of
in

te
ra

ct
io

n
H

2H
H

2M
Te

xt
H

2H
Te

xt
H

2H
H

2H
H

2M
Te

xt
Te

xt
di

al
og

di
al

og
m

on
ol

og
di

al
og

m
on

ol
og

di
al

og
di

al
og

di
al

og
m

on
ol

og
m

on
ol

og
N

um
be

ro
ft

ur
ns

co
lle

ct
ed

39
66

0
58

2
10

49
1

11
01

45
91

0
61

00
2

34
43

6
72

7
96

62
51

15
U

tte
ra

nc
e-

le
ve

ll
an

gu
ag

e
E

N
G

:6
5%

E
N

G
:3

6%
E

N
G

:5
4%

E
N

G
:2

6%
E

N
G

:7
%

E
N

G
:3

%
W

E
L

:7
7%

E
N

G
:3

2%
E

N
G

:1
2%

M
SA

:7
4%

us
e

or
co

de
sw

itc
hi

ng
SP

A
:2

9%
SP

A
:5

1%
SP

A
:1

8%
C

H
I:

20
%

C
H

I:
40

%
W

E
L

:8
6%

SP
A

:1
8%

H
IN

:3
5%

N
E

P:
15

%
E

G
Y

:6
%

pe
rc

en
ta

ge
C

S:
6%

C
S:

13
%

C
S:

28
%

C
S:

54
%

C
S:

53
%

C
S:

11
%

C
S:

5%
C

S:
33

%
C

S:
73

%
C

S:
20

%



83

unintelligible tokens. For the Twitter datasets, in
order to enable cross-dataset comparison, we nor-
malized the tag sets by creating an “other” class
that included all tokens not belonging to either of
the two relevant languages (NEs, ambiguous to-
kens, etc).

3 Feature Extraction

3.1 Low-Level Text Features
Following earlier work (Shirvani et al., 2016;
Samih et al., 2016), we experimented with the fol-
lowing low-level binary text features that capture
the presence or absence of the following:

• Word n-grams: We used a bag-of-words
representation, trying uni- and bi-grams.

• Character n-grams: The set of unique char-
acter n-grams (1 ≤ n ≤ 4), without crossing
word-boundaries. For example, the word se-
quence “la sal” would produce the following
character n-grams {‘l’, ‘a’, ‘s’, ‘al’, ‘la’, ‘sa’,
‘sal’}.
• Character Prefixes/Suffixes: All affixes

with length ≤ 3. For example, the word
“intricate” would have prefixes {”i”, ”in”,
”int”}, and suffixes {”ate”, ”te”, and ”e”}.
• Dictionary Lookup: We examine whether

each word exists in a dictionary for either
one of the code-switched languages. Dic-
tionaries for English, Spanish, and Welsh,
were sourced from GNU Aspell4. Dictionar-
ies from other languages were not used either
because they were not available or the dictio-
nary’s orthography differed from that used in
our data.

We also extracted turn length (in number of words)
and used that as an additional feature.

3.2 Embedding Features
We also examined the utility of different combina-
tions of the following embedding features:

• word2vec based pre-trained word embed-
dings (Mikolov et al., 2013). These models
are shallow, two-layer neural networks that
represent (embed) words in a continuous vec-
tor space where semantically similar words
are embedded close to each other. In order

4http://aspell.net/

to pre-train word2vec models while analyz-
ing the code-switched corpus of a particular
language pair, we utilized other corpora (if
they existed) for that same language pair (in
other words, we were not able to analyze the
effect of these features for language pairs that
had just one exemplar corpus, like English–
Welsh).

• char2vec based pre-trained character em-
beddings. These features are similar to
word2vec, but are applied at the character
level. In order to generate these embeddings,
we run standard Word2Vec with skip-grams,
except characters take the place of words and
words take the place of sentences, enabling
us to learn character contexts within words.
Jaech et al. (2016) used a similar feature they
termed “char2vec”, which is however dif-
ferent from our implementation; it involves
learning a character-based word vector using
a convolutional neural network.

• GloVe based pre-trained word embeddings
(Pennington et al., 2014). GloVe is an un-
supervised learning algorithm for obtaining
vector representations for words, which cap-
ture linear substructures of interest in the
word vector space. It is a global log bilin-
ear regression model that combines the ad-
vantages of the two major methods: matrix
factorization of global corpus co-occurence
statistics and local context window methods.
As in the word2vec case, to obtain pre-trained
vectors for a corpus in a given language pair,
we used the other corpora for that pair to train
aggregated global word-word co-occurrence
statistics.

• GloVe based pre-trained character embed-
dings. This is the GloVe algorithm applied
at the character level. To our knowledge, our
paper is the first such application of these fea-
tures for language identification.

• No pre-training: In this case, we learned
word and/or character embeddings from
scratch, i.e., we randomly initialized the vec-
tors and trained these embeddings using the
training partition of the data for each cross-
validation fold.



84

4 Machine Learning Methods

Following previous work in this area, we exam-
ined the utility of the following learners:

• Logistic Regression: The simplest method
we investigated was just logistic regression
with L2-regularization to generate language
label probabilities using the various combi-
nations of the features described in Section
3.1.

• CRFs: In this case, instead of modeling lan-
guage tagging decisions for each word in-
dependently, we model them jointly using a
conditional random field or CRF (Lafferty
et al., 2001).

• Bidirectional LSTMs: Long short term
memory networks or LSTMS are a special
kind of recurrent neural network that is ca-
pable of learning long-term dependencies
(Hochreiter and Schmidhuber, 1997). They
do so using several gates that control the
proportion of the input to give to the mem-
ory cell, and the proportion from the previ-
ous state to forget5. We implemented the
Stack LSTM architecture first proposed by
Dyer et al. (2015), in which the LSTM is
augmented with a “stack pointer.” While se-
quential LSTMs model sequences from left
to right, Stack LSTMs permit embedding of
a stack of objects that are both added to (us-
ing a push operation) and removed from (us-
ing a pop operation). This allows the Stack
LSTM to work like a stack that maintains
a “summary embedding” of its contents. In
our case, we use this architecture to model a
summary embedding of characters within an
model of word embedding sequences. In ad-
dition to this Stack BiLSTM, following Lam-
ple et al. (2016), we used a combination of
a Stack BiLSTM with a CRF, where instead
of directly using the softmax output from the
Stack BiLSTM, we use a CRF to predict the
final language tag for each word by taking
into account neighboring tags.

An novel feature of our experiments is the ex-
amination of the utility of pre-trained GloVe
and char2vec in improving performance of
the system proposed for named entity recog-
nition in Lample et al. (2016).

5Also see http://colah.github.io/posts/2015-08-
Understanding-LSTMs

5 Experiments

We conducted 10-fold cross-validation experi-
ments for all datasets. For each dataset, we first
extracted the word and character level features de-
scribed in Section 3. We then tried the follow-
ing approaches to predicting one of 3 classes –
English, Spanish or Code-switched – at the turn-
level: (i) Used a CRF to make word-level pre-
dictions, and aggregated them to form a turn-
level prediction; (ii) aggregated the features at the
turn level and try a variety of learners, including
logistic regression and deep neural networks to
make language predictions at the turn level; (iii)
fed word- and character-embedding combinations
(both with and without pre-training) to a Stacked-
BiLSTM-CRF system and made an LID predic-
tion for each turn. We experimented with differ-
ent learner configurations and parameter settings
and summarize the best performing featureset and
learner combination in the Results section. We
used a grid search method to find optimal char-
acter embedding size for each dataset (among val-
ues of 25, 50 and 100). For the Stack-BiLSTM
system, given the large number of architectural
parameters to optimize (number of LSTM layers
and recurrent units, type of optimizer, dropout,
gradient clipping/normalization, minibatch size, to
name a few), we chose to use the choices recom-
mended by Reimers and Gurevych (2017), who
evaluated over 50,000 different setups and found
that some parameters, like pre-trained embeddings
or the last layer of the network, have a large im-
pact on the performance, while other parameters,
like the number of LSTM layers or the number of
recurrent units, are of relatively minor importance.
We set the word-embedding size to 100 and used
25 and 100 recurrent units in the character-level
and word-level BiLSTMs, respectively, following
Lample et al. (2016).

6 Observations and Analysis

Table 2 lists the best performing turn-level LID
systems, including the feature sets and model de-
tails. In each cell of the table, the top value indi-
cates the overall weighted average F1 score, while
the bottom value (in parentheses) indicates the F1
score of the code-switched class. We decided to
list the latter value since this class is easily con-
fusable with the other two, and better F1 scores
for this class might give an insight into which al-
gorithms are better at capturing the characteristics



85

System Weighted Average F1 Scores for Each Dataset

Featureset
Machine ENG-SPA ENG-CHI ENG-WEL WEL-SPA ENG-HIN ENG–NEP MSA–EGY
Learner Bangor HALEF Twitter SEAME Twitter Bangor Bangor HALEF Twitter Twitter

Word n-grams, Char n-grams,
Affixes, Length & Dictionary
lookup

Logistic
Regression

0.9525 0.9324 0.8143 0.9931 0.5786 0.9647 0.9706 0.8765 0.8442 0.7556

(0.6820) (0.7576) (0.6839) (0.9937) (0.6272) (0.8531) (0.6762) (0.8235) (0.9023) (0.4511)

Word n-grams, Char n-grams,
Affixes, Length & Dictionary
lookup

CRF ag-
gregated to
turn

0.9696 0.9584 0.8912 0.9977 0.7393 0.9676 0.9800 0.9022 0.9367 0.7280

(0.8381) (0.8874) (0.8247) (0.9979) (0.7457) (0.8639) (0.7982) (0.8553) (0.9568) (0.3216)

Word and Char Embeddings
(both from scratch)

Stacked
Bi-LSTM
+ CRF

0.966 0.9759 0.884 0.999 0.742 0.9606 0.977 0.894 0.932 0.747

(0.8345) (0.9560) (0.8256) (0.9991) (0.7268) (0.8469) (0.7828) (0.8536) (0.9525) (0.4227)

Pre-trained Word Embeddings
(’word2vec’ in blue, otherwise
’GloVe’) and Char Embeddings
(from scratch)

Stacked
Bi-LSTM
+ CRF

0.9671 0.9708 0.8950 0.999 0.7270 — — — — —

(0.8438) (0.9308) (0.8421) (0.9987) (0.7104) — — — — —

2 Pre-trained Word and Char
Embeddings (’word2vec’ in
blue, otherwise ’GloVe’)

Stacked
Bi-LSTM
+ CRF

0.9692 0.976 0.8953 0.999 0.7332 — — — — —

(0.8506) (0.9560) (0.8424) (0.9992) (0.7173) — — — — —

Best-performing turn predic-
tions

Stacked
Bi-LSTM

0.9621 0.9394 0.8587 — 0.6089 0.9485 0.9501 0.8514 0.7906 0.7564

(0.7962) (0.8235) (0.7770) — (0.6821) (0.7869) (0.7277) (0.7889) (0.8710) (0.5280)

Majority Baseline 0.49 0.34 0.38 0.38 0.37 0.79 0.67 0.18 0.61 0.63

Random Baseline 0.38 0.34 0.35 0.35 0.37 0.43 0.41 0.34 0.39 0.40

Best performance on 1st codeswitching challenge — — 0.822 — 0.894 — — — 0.977 0.417

Best performance on 2nd codeswitching challenge — — 0.913 — — — — — — 0.83

Table 2: Weighted average F1 scores obtained by different featureset–learner combinations on each
codeswitching dataset. Notice that datasets are organized first by language pair, and then according to
type of interaction (human-human vs. human-machine vs. Twitter). Each cell of the table contains two
numbers: the overall weighted F1 score on top and the F1 score of the code-switched class in paren-
theses at the bottom. Note that we obtained performance numbers for pre-trained word and character
embeddings only for language pairs with more than one dataset, i.e., ENG-SPA and ENG–CHI. Also
shown for benchmarking purposes are the best tweet-level performance numbers from the 1st and 2nd

codeswitching challenges for some of the Twitter datasets. However, note that this is not a completely
fair comparison, because the train-test partitions in our case are different: we used only the train data
from the 1st code-switching challenge in order to perform 10-fold cross-validation experiments. Also
see the text for more details.

System Weighted Average Token-Level F1 Scores for Each Dataset

Featureset
Machine ENG-SPA ENG-CHI ENG-WEL WEL-SPA ENG-HIN ENG–NEP MSA–EGY
Learner Bangor HALEF Twitter SEAME Twitter Bangor Bangor HALEF Twitter Twitter

Word n-grams, Char n-grams,
Affixes, Length & Dictionary
lookup

CRF 0.9774 0.9772 0.9111 0.9989 0.9513 0.9824 0.9774 0.9343 0.9601 0.5804

Word and Char Embeddings
(both from scratch)

Stacked
Bi-LSTM +
CRF

0.9883 0.9721 0.9394 0.9993 0.9476 0.9820 0.9922 0.9290 0.9579 0.5791

Pre-trained Word Embeddings
(’word2vec’ in blue, otherwise
’GloVe’) and Char Embeddings
(from scratch)

Stacked
Bi-LSTM +
CRF

0.9814 0.9784 0.9437 0.9992 0.9429 — — — — —

Pre-trained Word and Char Em-
beddings (’word2vec’ in blue,
otherwise ’GloVe’)

Stacked
Bi-LSTM +
CRF

0.9819 0.9788 0.9370 0.9993 0.9478 — — — — —

Best performance on 1st codeswitching challenge — — 0.94 — 0.892 — — — 0.959 0.936

Best performance on 2nd codeswitching challenge — — 0.973 — — — — — — 0.876

Table 3: Weighted average F1 scores for token-level predictions after 10-fold crossvalidation. Also
shown for benchmarking purposes are the best token-level performance numbers from the 1st and 2nd

codeswitching challenges. However, note that this is not a fair comparison, because the train-test parti-
tions in our case are different: we used only the train data from the 1st code-switching challenge in order
to perform 10-fold cross-validation experiments. Also see the text for more details.



86

of this class. At the outset, we observe that all text
systems significantly outperform the majority vote
baseline (where we assign the language labels of
all turns in the test set to the majority class) and
the random baseline (where the language labels of
all test set turns are assigned at random) by a huge
margin.

One of the primary research questions we
wanted to study (see the penultimate paragraph
of Section 1) was how different featureset-learner
combinations performed across different language
pairs. We see that no particular featureset-learner
combination dominated overall performance-wise,
with results varying depending on the dataset and
language pair in question. Interestingly, in the
case of English–Spanish, where there were 3 dif-
ferent datasets of code-switched text, using pre-
trained word and character embeddings performed
at or above par all other systems. In other words,
in the presence of sufficient amounts of data for
pre-training, using pre-trained embedding-based
systems yields the best results. Even though the
overall F1 score of all embedding-based Stack
Bi-LSTM systems is similar, notice that the F1
score of the code-switched class improves when
we use both pre-trained word and character em-
beddings. This suggests that pretrained charac-
ter embeddings are particularly useful in captur-
ing the characteristics of code-switched language.
While GloVe-based character embeddings were
more useful for the human-human (Bangor) and
monolog (Twitter) datasets, word2vec was better
for the HALEF dataset of human-machine dialog.
For English–Mandarin corpora, on the other hand,
while the embedding–Stack BiLSTM–CRF com-
bination still performed best, using pre-trained
embeddings did not seem to make any significant
additional impact.

Another research question of interest dealt with
whether we obtained a better turn-level LID per-
formance by (i) using word-level LID followed by
aggregation over the entire turn, or (ii) directly
training classifiers at the turn-level. Our results
seem to suggest that the former is better than the
latter across all code-switched text datasets with
one notable exception. In the case of the Modern
Standard Arabic–Egyptian Arabic Twitter dataset,
using a Stacked BiLSTM with embedding features
and a direct softmax layer for turn-level predic-
tions (i.e., without an additional CRF aggregation
step) performed best.

For all other remaining language pairs (each
of which had just one dataset), the simpler CRF
classifier (where predictions were aggregated to a
turn) with a more standard featureset (word and
character n-grams, affixes, turn length and dictio-
nary lookup) yielded the best results. That this
simpler CRF system performed competently even
in the other cases relative to the Stack-BiLSTM
systems suggests that the former is perhaps a bet-
ter choice when one does not have large amounts
of training data, particularly for pre-training. On
a related note, it is also worth pointing out that
unsurprisingly, performance numbers across the
board are influenced by the amount of data in each
dataset, i.e., more data leads to higher F1 scores.

Yet another research question dealt with the per-
formance across datasets for human–human di-
alog vs. human–machine dialog vs. monolog
tweets. We observe, in general, a decrease in
overall weighted F1 score as one moves from
human–human dialog to human–machine dialog
to monolog tweet data. One possible reason for
this is that Twitter data in particular consists of
many “other” non-language tokens (such as named
entities, ambiguous tokens, etc.), which, on re-
moval or non-consideration, might lead to differ-
ent phrase structures in the resulting data6.

The final question we asked was to examine
token-level prediction performance, in order to
benchmark ourselves against prior art in this area.
Table 3 lists these results. We find that perfor-
mance trends in this case roughly mirror those ob-
served at the turn-level.

Performances from the 1st and 2nd Code-
switching Workshop Challenge results are pro-
vided in each table to provide some comparison
with our systems. However, it should be noted
that these comparisons are not exact. Our results
are from 10-fold cross-validation on the training
data used in the Workshop challenge, not on the
held-out test sets. Additionally, because we pulled
the data from Twitter years after the 1st Work-
shop, some of the tweets initially intended for the
dataset were no longer available. For the tweet-
level performance, we report results on three-class

6A big part of the errors made by crowd-sourcing an-
notators who assigned tag labels for the Twitter datasets in-
volve named entities, probably because the annotators do not
take the context into account in an effort to be fast and col-
lect money quickly. The problem is exacerbated in the MSA-
EGY set due to the fact that there is inherently considerable
amount of data overlap due to homographs between the two
varieties of the language (Molina et al., 2016).



87

classification (language 1 vs. language 2 vs. code-
switched), whereas the Code-switching Workshop
performances are based on binary classification
(monolingual vs. code-switched). Furthermore,
as mentioned earlier, in order to enable cross-
dataset comparison, we normalized the tag sets
by creating an ”other” class that included all to-
kens not belonging to either of the two relevant
languages (NEs, ambiguous tokens, etc). Taking
these points into consideration, our systems per-
form competitively with the submissions to the
Code-switching Workshop Challenges. The only
exception is in the case of the MSA-EGY dataset,
where while our tweet-level performance is com-
petitive, our token-level performance far underper-
forms the state-of-the-art. We suspect that dataset
imbalance could play a role, as well as the fact that
we didn’t use any external resources for this lan-
guage pair.

7 Discussion and Outlook

We have presented an experimental evaluation of
different text-based featuresets in performing lan-
guage identification (LID) at both the turn and
token levels in code-switched text interactions.
We studied the generalizability of various sys-
tems both across language pair and dataset type—
human–human, human–machine and monolog—
by examining 10 different datasets of code-
switched text. While our best text-based sys-
tems performed either at or above par with the
state of the art in the field, we found that the
use of both pre-trained word and character-based
embedding features, and the latter in particular
(either through char2vec or GloVe), were partic-
ularly useful at capturing the characteristics of
code-switched speech (with the caveat that the fea-
ture extraction process requires sufficient data for
pre-training). We further observed that a perfor-
mance drop depending on the style of interaction,
as we move from human–human dialog to human–
machine dialog to monolog tweets.

Going forward, we will explore a number of po-
tential avenues for improving the performance of
the text-based LID systems. Chief among these is
to investigate strategies for dealing with little or
no code-switched data (or indeed, overall train-
ing data) for a given language pair, and how to
improve the performance of deep learning algo-
rithms for such datasets. In addition, we would
like to perform a deeper error analysis of the al-

gorithms on different featuresets to obtain a bet-
ter understanding of how best to select a feature-
learner combination for the LID task.

Finally, as mentioned earlier, one of the key ex-
citing R&D directions that this work informs is in
building code-switching dialog systems. For in-
stance, integrating an explicit language identifi-
cation step into the spoken language understand-
ing (SLU) could help enhance the system perfor-
mance. Over and above such applications, such
an LID module might also help inform pragmatic
considerations during dialog management and the
language generation module for the generation of
appropriate mixed-language output.

8 Acknowledgements

We would like to thank Julia Hirschberg and other
attendees at the Interspeech 2018 Special Ses-
sion on Speech Techologies for Code-switching in
Multilingual Communities for useful discussions
and illuminating ideas on the processing of code-
switched speech, including available datasets.

References
Badrul Hisham Ahmad and Kamaruzaman Jusoff.

2009. Teachers code-switching in classroom in-
structions for low english proficient learners. En-
glish Language Teaching 2(2):49.

Kalika Bali, Yogarshi Vyas, Jatin Sharma, and Mono-
jit Choudhury. 2014. i am borrowing ya mixing?
an analysis of english-hindi code mixing in face-
book. Proceedings of the First Workshop on Com-
putational Approaches to Code Switching, EMNLP
2014 page 116.

Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code mixing: A challenge
for language identification in the language of social
media. EMNLP 2014 13.

Joyce YC Chan, PC Ching, and Tan Lee. 2005. Devel-
opment of a cantonese-english code-mixing speech
corpus. In INTERSPEECH. pages 1533–1536.

Anik Dey and Pascale Fung. 2014. A hindi-english
code-switching corpus. In LREC. pages 2410–2413.

Kevin Donnelly and Margaret Deuchar. 2011. The ban-
gor autoglosser: a multilingual tagger for conversa-
tional text. ITA11, Wrexham, Wales .

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. arXiv preprint arXiv:1505.08075 .



88

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Aaron Jaech, George Mulcaire, Mari Ostendorf, and
Noah A Smith. 2016. A neural model for language
identification in code-switched tweets. In Proceed-
ings of The Second Workshop on Computational Ap-
proaches to Code Switching. pages 60–64.

Yih-Lin Belinda Jiang, Georgia Earnest Garcı́a, and
Arlette Ingram Willis. 2014. Code-mixing as a bilin-
gual instructional strategy. Bilingual Research Jour-
nal 37(3):311–326.

Aravind K Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of
the 9th conference on Computational linguistics-
Volume 1. Academia Praha, pages 145–150.

Ben King and Steven P Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In HLT-NAACL.
pages 1110–1119.

John D Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning. Morgan
Kaufmann Publishers Inc., pages 282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT . pages 260–270.

Ying Li, Yue Yu, and Pascale Fung. 2012. A mandarin-
english code-switching corpus. In LREC. pages
2515–2519.

William Littlewood and Baohua Yu. 2011. First lan-
guage and target language in the foreign language
classroom. Language Teaching 44(1):64–77.

Dau-Cheng Lyu, Tien-Ping Tan, Eng-Siong Chng,
and Haizhou Li. 2015. Mandarin–english code-
switching speech corpus in south-east asia: Seame.
Language Resources and Evaluation 49(3):581–
600.

Jeff MacSwan. 2004. Code switching and grammatical
theory. The handbook of bilingualism 46:283.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Lesley Milroy and Pieter Muysken. 1995. One speaker,
two languages: Cross-disciplinary perspectives on
code-switching. Cambridge University Press.

Giovanni Molina, Nicolas Rey-Villamizar, Thamar
Solorio, Fahad AlGhamdi, Mahmoud Ghoneim, Ab-
delati Hawwari, and Mona Diab. 2016. Overview
for the second shared task on language identification
in code-switched data. EMNLP 2016 page 40.

Carol Myers-Scotton. 2006. Codeswitching with en-
glish: types of switching, types of communities.
World Englishes: Critical Concepts in Linguistics
4(3):214.

Brendan H OConnor and Layne J Crawford. 2015. An
art of being in between: The promise of hybrid lan-
guage practices. In Research on Preparing Inservice
Teachers to Work Effectively with Emergent Bilin-
guals, Emerald Group Publishing Limited, pages
149–173.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Vikram Ramanarayanan, Robert Pugh, Yao Qian, and
David Suendermann-Oeft. 2018. Automatic Turn-
Level Language Identification for Code-Switched
Spanish-English Dialog. In Proc. of the IWSDS
Workshop 2018, Singapore.

Vikram Ramanarayanan and David Suendermann-
Oeft. 2017. Jee haan, I’d like both, por favor:
Elicitation of a Code-Switched Corpus of Hindi–
English and Spanish–English Human–Machine Di-
alog. Proc. Interspeech 2017 pages 47–51.

Nils Reimers and Iryna Gurevych. 2017. Optimal hy-
perparameters for deep lstm-networks for sequence
labeling tasks. arXiv preprint arXiv:1707.06799 .

Younes Samih, Suraj Maharjan, Mohammed Attia,
Laura Kallmeyer, and Thamar Solorio. 2016. Multi-
lingual code-switching identification via lstm recur-
rent neural networks. EMNLP 2016 page 50.

Rouzbeh Shirvani, Mario Piergallini, Gauri Shankar
Gautam, and Mohamed Chouikha. 2016. The
howard university system submission for the shared
task in language identification in spanish-english
codeswitching. In Proceedings of The Second Work-
shop on Computational Approaches to Code Switch-
ing. pages 116–120.

Thamar Solorio, Elizabeth Blair, Suraj Mahar-
jan, Steven Bethard, Mona Diab, Mahmoud
Gohneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-
lia Hirschberg, Alison Chang, et al. 2014. Overview
for the first shared task on language identification
in code-switched data. In Proceedings of the First
Workshop on Computational Approaches to Code
Switching. Citeseer, pages 62–72.

Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, pages 973–981.

Li Wei. 2000. The bilingualism reader. Psychology
Press.

Rebecca S Wheeler. 2008. Code-switching. EDUCA-
TIONAL LEADERSHIP .


