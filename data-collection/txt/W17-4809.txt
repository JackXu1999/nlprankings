



















































Combining the output of two coreference resolution systems for two source languages to improve annotation projection


Proceedings of the Third Workshop on Discourse in Machine Translation, pages 67–72,
Copenhagen, Denmark, September 8, 2017. c©2017 Association for Computational Linguistics.

Combining the output of two coreference resolution systems for two
source languages to improve annotation projection

Yulia Grishina
Applied Computational Linguistics

FSP Cognitive Science
University of Potsdam

grishina@uni-potsdam.de

Abstract

Although parallel coreference corpora can
to a high degree support the development
of SMT systems, there are no large-scale
parallel datasets available due to the com-
plexity of the annotation task and the vari-
ability in annotation schemes. In this
study, we exploit an annotation projec-
tion method to combine the output of two
coreference resolution systems for two
different source languages (English, Ger-
man) in order to create an annotated cor-
pus for a third language (Russian). We
show that our technique is superior to pro-
jecting annotations from a single source
language, and we provide an in-depth
analysis of the projected annotations in or-
der to assess the perspectives of our ap-
proach.

1 Introduction

Most of the recent work on exploiting corefer-
ence relations in Machine Translation focused on
improving the translation of anaphoric pronouns
(Le Nagard and Koehn, 2010; Hardmeier and Fed-
erico, 2010; Guillou, 2012; Novák et al., 2015;
Guillou and Webber, 2015), disregarding other
types of coreference relations, one of the reasons
being the lack of annotated parallel corpora as
well as the variability in the annotated data. How-
ever, this could be alleviated by exploiting anno-
tation projection across parallel corpora to create
more linguistically annotated resources for new
languages. More importantly, applying annotation
projection using several source languages would
support the creation of corpora less biased to-
wards the peculiarities of a single source annota-
tion scheme.

In our study, we aim at exploring the usability

of annotation projection for the transfer of auto-
matically produced coreference chains. In partic-
ular, our idea is that using several source annota-
tions produced by different systems could improve
the performance of the projection method. Our
approach to the annotation projection builds upon
the approach recently introduced by (Grishina and
Stede, 2017), who experimented with projecting
manually annotated coreference chains from two
source languages to the target language. However,
our goal is slightly different: We are interested
in developing a fully automatic pipeline, which
would support the automatic creation of paral-
lel annotated corpora in new languages. There-
fore, in contrast to (Grishina and Stede, 2017), we
use automatic source annotations produced by two
state-of-the-art coreference systems, and we com-
bine the output of our projection method for two
source languages (English and German) to obtain
target annotations for a third language (Russian).
Through performing the error analysis of the pro-
jected annotations, we investigate the most com-
mon projection errors and assess the benefits and
drawbacks of our method.

The paper is organized as follows: Section 2
presents an overview of the related work and Sec-
tion 3 describes the experimental setup. In Section
4, we give a detailed error analysis and discuss the
results of our experiment. The conclusions and the
avenues for future research are presented in Sec-
tion 5.

2 Related work

Annotation projection is a method that allows
for automatically transferring annotations from a
well-studied (source) language to a low-resource
(target) language in a parallel corpus in order to
automatically obtain annotated data. It was first
introduced in the work of (Yarowsky et al., 2001)

67



News Stories Total
EN DE EN DE EN DE

Markables 486 621 429 414 915 1035
Chains 125 200 57 68 182 268

Table 1: Number of markables and coreference chains in the automatic annotations

MUC B3 CEAFm Avg.
P R F1 P R F1 P R F1 P R F1

berkeley (EN) 49.5 41.4 45.0 38.9 27.8 32.1 45.9 40.4 42.9 44.7 36.5 40.0
CorZu (DE) 66.9 59.2 62.5 59.2 41.3 46.6 52.4 52.8 52.3 59.5 51.1 53.8

Table 2: Evaluation of the automatic source annotations

and then extensively exploited for different kinds
of linguistic tasks, including coreference resolu-
tion. Specifically, several studies used annotation
projection to acquire annotated data, such as (Pos-
tolache et al., 2006; Rahman and Ng, 2012; Mar-
tins, 2015; Grishina and Stede, 2015).

Thereafter, (Grishina and Stede, 2017) pro-
posed a multi-source method for annotation pro-
jection: They used a manually annotated trilin-
gual coreference corpus and two source languages
(English-German, English-Russian) to transfer an-
notations to the target language (Russian and
German, respectively). Although their approach
showed promising results, it was based on trans-
ferring manually produced annotations, which are
typically not available for other languages and,
more importantly, can not be acquired large-scale
due to the complexity of the annotation task.

3 Annotation projection experiment

In our experiment, we propose a fully automatic
projection setup: First, we perform coreference
resolution on the source language data and then
we implement the single- and multi-source ap-
proaches to transfer the automatically produced
annotations. We use the English-German-Russian
unannotated corpus of (Grishina and Stede, 2017)
as the basis for our experiment, which contains
texts in two genres – newswire texts (229 sen-
tences per language) and short stories (184 sen-
tences per language). Furthermore, we use manual
annotations present in the corpus as the gold stan-
dard for our evaluation. It should be noted that
the manual annotations were performed accord-
ing to the parallel coreference annotation guide-
lines of (Grishina and Stede, 2016) that are in
general compatible with the annotation of the the
OntoNotes corpus (Hovy et al., 2006) and are
therefore suitable for our evaluation.

3.1 Coreference resolution on the source
language data

Since the main goal of this experiment is to as-
sess the quality of the projection of automatic an-
notations, first we need to automatically label the
source language data. For the English side of
the corpus, we chose the Berkeley Entity Reso-
lution system (Durrett and Klein, 2014), which
was trained on the English part of the OntoNotes
corpus (Hovy et al., 2006) and achieves the aver-
age F1 of 61.71 on the OntoNotes dataset (Dur-
rett and Klein, 2014). For the German side of the
corpus, we use the state-of-the-art CorZu system
(Tuggener, 2016) to obtain the source annotations,
which achieves the average of 66.9 F1 on the Ger-
man part of the SemEval 2010 dataset (Klenner
and Tuggener, 2011).

Corpus statistics for the English and German
datasets are presented in Table 1. Interestingly,
CorZu was able to resolve slightly more mark-
ables and coreference chains in total than Berke-
ley (1035 vs. 915, 268 vs. 182 respectively). In
particular, the numbers of found markables and
chains in English and German diverge for the
newswire texts, which further supports the claim
that this part of the corpus contains more complex
coreference relations than the short stories1.

To estimate the quality of the automatically
produced annotations, we evaluate the resulting
dataset against the manually annotated English
and German parts of the corpus (Table 2). As one
can see from this table, CorZu and Berkeley do not
perform equally good on our dataset: the average
F1 of 53.8 for German as compared to the average
F1 of 40.0 for English.

1As already stated in (Grishina and Stede, 2017), the
newswire texts contain a larger percentage of complex noun
phrases than the short stories.

68



MUC B3 CEAFm Avg.
P R F1 P R F1 P R F1 P R F1

en-ru 51.7 32.6 39.8 40.6 19.6 26.0 45.7 31.3 37.0 46.0 27.8 34.3
de-ru 55.5 23.6 32.8 42.1 13.0 19.1 43.0 25.3 31.6 46.9 20.6 27.8
en,de-ru
Setting 1 58.5 33.6 42.5 43.9 19.8 26.9 55.7 30.3 39.1 52.7 27.9 36.2
Setting 2 85.2 14.9 24.7 76.8 7.8 13.8 75.8 17.1 27.6 79.3 13.3 22.0
Setting 3 49.4 36.1 41.5 35.9 22.1 26.7 38.3 35.2 36.5 41.2 31.1 34.9

Table 3: Projection results from English and German into Russian

3.2 Annotation projection strategies
For our experiment, we implement a direct projec-
tion method for coreference as described in (Gr-
ishina and Stede, 2015). Our method works as fol-
lows: For each markable on the source side, we
automatically select all the corresponding tokens
on the target side aligned to it, and we then take
the span between the first and the last word as the
new target markable, which has the same corefer-
ence chain number as the source one. Since the
corpus was already sentence- and word-aligned2,
we use the available alignments to transfer the an-
notations.

Thereafter, we re-implement the multi-source
approach as described in (Grishina and Stede,
2017). In particular, they (a) looked at disjoint
chains coming from different sources and (b) used
the notion of chain overlap to measure the simi-
larity between two coreference chains that contain
some identical mentions3. In our experiment, we
apply the following strategies from (Grishina and
Stede, 2017):

1. Setting 1 (‘add’): disjoint chains from one
source language are added to all the chains
projected from the other source language;

2. Setting 2 (‘unify-intersect’): the intersection
of mentions for overlapping chains is se-
lected.

3. Setting 3 (‘unify-concatenate’): chains that
overlap are treated as one chain starting from
a certain percentage of overlap.

For both single- and multi-source approaches,
we deliberately rely solely on word alignment in-
formation to project the annotations, in order to
keep our approach easily transferable to other lan-
guages.

2Sentence alignment was performed using HunAlign
(Varga et al., 2007); word alignments were computed with
GIZA++ (Och and Ney, 2003) on a parallel newswire corpus
(Grishina and Stede, 2015).

3Computed as Dice coefficient.

3.3 Results

To evaluate the projection results, we computed
the standard coreference metrics – MUC (Vilain
et al., 1995), B-cubed (Bagga and Baldwin, 1998)
and CEAF (Luo, 2005) – and their average for
each of the approaches (Table 3). As one can see
from the table, the quality of projections from En-
glish to Russian outperforms the quality of projec-
tions from German to Russian by 6.5 points F1.
Moreover, while Precision number are quite simi-
lar, projections from English exhibit higher Recall
numbers.

As for the multi-source settings, we were able to
achieve the highest F1 of 36.2 by combining dis-
joint chains (Setting 1), which is 1.9 point higher
than the best single-source projection scores and
constitutes almost 62% of the quality of the pro-
jection of gold standard annotations reported in
(Grishina and Stede, 2017). We were able to
achieve the highest Precision scores by intersect-
ing the overlapping chains (Setting 2) and the
highest Recall by concatenating them (Setting 3).

Finally, we evaluate the annotations coming
from English and German against each other, in
order to estimate their comparability and the per-
centage of overlap. Interestingly, we achieve 52.0
F1, with Precision being slightly higher than Re-
call (53.9 vs. 50.2), which shows the dissimilarity
between the two projections.

4 Error analysis and discussion

Analyzing the errors coming from each of the
source languages, we first looked at the percent-
age of transferred mentions (Table 4): Using our
method we were able to automatically transfer
82.7% of all the source markable from English
and only 57.6% of all the source markables from
German; similarly, the percentage of the trans-
ferred chains is lower for German than for En-
glish. Interestingly, while CorZu performs bet-
ter on the source dataset than Berkeley, the results
for the annotations projected from a single source

69



are the opposite: Annotation projection from En-
glish to Russian performs better than from German
to Russian. Our hypothesis is that the reason for
the lower percentage of transferred annotations is
the lower quality of word alignments for German-
Russian as compared to English-Russian. Further-
more, since the original language of the texts was
English, we presume that the German and Russian
translations are closer to English and less similar
to each other.

English German
# % # %

Markables 757 82.7 596 57.6
Chains 182 100.0 227 84.7

Table 4: Transferred chains and markables

Since we do not have access to any gold align-
ment data, we estimate the quality of the word
alignments by computing the number of unaligned
tokens. Not surprisingly, we see a higher percent-
age of unaligned words for German-Russian than
for English-Russian: 17.03% vs. 14.96% respec-
tively, which supports our hypothesis regarding
the difference in the alignment quality for the two
pairs. Furthermore, we computed the distribution
of unaligned words: The highest percentage of
unaligned tokens disregarding punctuation marks
are prepositions; pronouns constitute only 3% and
5% of all unaligned words for the alignments be-
tween English-Russian and German-Russian re-
spectively. However, these numbers do not con-
stitute more than 5% of the overall number of pro-
nouns in the corpus.

Following the work of (Grishina and Stede,
2017), we analyse the projection accuracy for
common nouns (‘Nc’), named entities (‘Np’) and
pronouns (‘P’) separately4: Table 5 shows the per-
centage of correctly projected markables of each
type out of all the projected markables of this type.
Our results conform to the results of (Grishina and
Stede, 2017): For both languages, pronouns ex-
hibit the highest projection quality, while common
and proper nouns are projected slightly less ac-
curately, which is probably due to the fact that
pronouns typically consist of single tokens and
are better aligned than multi-token common and
proper names. Overall, for all the markables, the
projection accuracy for English-Russian is around

4Using the automatic POS annotations already present in
the corpus and provided by TreeTagger (Schmid, 2013).

10% better than projection accuracy for German-
Russian.

en-ru de-ru
Nc 64.5 60.7
Np 70.5 66.6
P 83.6 76.5
All 65.1 55.6

Table 5: Projection accuracy for common nouns,
proper nouns and pronouns (%)

Moreover, we compare the projected annota-
tions across the two genres. Interestingly, the re-
sults for the two languages vary: While the av-
erage coreference scores for English-Russian are
quite comparable (news: 34.2 F1, stories: 33.3
F1), the scores for German-Russian differ consid-
erably (news: 30.8 F1, stories: 20.8 F1). We at-
tribute this difference to the quality of the source
annotations and the performance of the source
coreference resolvers on different genres of texts.

5 Summary and outlook

In this study, we assessed the applicability of an-
notation projection in a scenario where we have
access to two coreference resolvers in two source
languages, the output of which is projected to a
third language in a low-resource setting. Our re-
sults have shown that projection from two source
languages is able to reach 62% of the quality of
the projection of manual annotations and improves
the projection scores by 1.9 F1. Moreover, using
the output of two completely different coreference
resolution systems, we observed the similar ten-
dencies as while projecting gold standard annota-
tions: Projection from English to Russian achieves
higher scores than projection from German to Rus-
sian, and pronouns have the highest projection ac-
curacy.

Another important finding is that better source
annotations does not necessarily result in better
projection scores, which can be explained by the
different quality of word alignments for both lan-
guage pairs. Having investigated this issue, we
conclude that alignments between German and
Russian contain more unaligned units than the
alignments between English and Russian. Our
next steps include examining the alignment quality
in more detail, which would require establishing a
gold standard set of alignments (for at least noun
phrases).

70



Overall, we envision our future work in exploit-
ing more than two source annotations as well as
multiple coreference resolution systems for a sin-
gle source language to improve the source coref-
erence annotations. Specifically, we plan on ap-
plying our method on other language pairs and
datasets, in order to explore its generalizabililty
for a wider range of languages. Furthermore,
we are interested in exploiting our approach as a
first step to create coreference annotated corpora
in new languages by providing automatically pro-
jected target coreference chains to human annota-
tors for a subsequent validation.

References
Amit Bagga and Breck Baldwin. 1998. Entity-

based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 1. Association for Computational Linguis-
tics, pages 79–85.

Greg Durrett and Dan Klein. 2014. A joint model for
entity analysis: Coreference, typing, and linking. In
Transactions of the Association for Computational
Linguistics.

Yulia Grishina and Manfred Stede. 2015. Knowledge-
lean projection of coreference chains across lan-
guages. In Proceedings of the 8th Workshop on
Building and Using Comparable Corpora, Beijing,
China. Association for Computational Linguistics,
page 14.

Yulia Grishina and Manfred Stede. 2016. Parallel
coreference annotation guidelines. University of
Potsdam.

Yulia Grishina and Manfred Stede. 2017. Multi-source
annotation projection of coreference chains: Assess-
ing strategies and testing opportunities. In Sec-
ond Workshop on Coreference Resolution Beyond
OntoNotes. Association for Computational Linguis-
tics, page 41.

Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics. Association for Compu-
tational Linguistics, pages 1–10.

Liane Guillou and Bonnie Webber. 2015. Analysing
ParCor and its translations by state-of-the-art SMT
systems. In Proceedings of the Second Workshop on
Discourse in Machine Translation. Association for
Computational Linguistics, page 24.

Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-

chine translation. In IWSLT (International Work-
shop on Spoken Language Translation); Paris,
France; December 2nd and 3rd, 2010.. pages 283–
289.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human lan-
guage technology conference of the NAACL, Com-
panion Volume: Short Papers. Association for Com-
putational Linguistics, pages 57–60.

Manfred Klenner and Don Tuggener. 2011. An incre-
mental entity-mention model for coreference resolu-
tion with restrictive antecedent accessibility. In Pro-
ceedings of the international conference on Recent
Advances in Natural Language Processing. pages
178–185.

Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and Metrics (MATR). As-
sociation for Computational Linguistics, pages 252–
261.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics, pages 25–32.

André Martins. 2015. Transferring coreference re-
solvers with posterior regularization. In Proceed-
ings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics. volume 1, pages
1427–1437.

Michal Novák, Dieke Oele, and Gertjan van Noord.
2015. Comparison of coreference resolvers for deep
syntax translation. In Proceedings of the Second
Workshop on Discourse in Machine Translation. As-
sociation for Computational Linguistics, page 17.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics 29(1):19–51.

Oana Postolache, Dan Cristea, and Constantin Orasan.
2006. Transferring coreference chains through word
alignment. In Proceedings of 5th international
conference on Language Resources and Evaluation
(LREC).

Altaf Rahman and Vincent Ng. 2012. Translation-
based projection for multilingual coreference reso-
lution. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. Association for Computational Linguis-
tics, pages 720–730.

Helmut Schmid. 2013. Probabilistic part-of speech
tagging using decision trees. In New methods in lan-
guage processing. Routledge, page 154.

71



Don Tuggener. 2016. Incremental Coreference Resolu-
tion for German. Ph.D. thesis, University of Zurich.

Dániel Varga, Péter Halácsy, András Kornai, Viktor
Nagy, László Németh, and Viktor Trón. 2007. Paral-
lel corpora for medium density languages. Amster-
dam Studies in the Theory and History of Linguistic
Science Series 4 292:247.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Pro-
ceedings of the 6th conference on Message under-
standing. Association for Computational Linguis-
tics, pages 45–52.

David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research. Associa-
tion for Computational Linguistics, pages 1–8.

72


