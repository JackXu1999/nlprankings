















































Indexing Spoken Documents with Hierarchical Semantic Structures: Semantic Tree-to-string Alignment Models


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 509–517,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Indexing Spoken Documents with Hierarchical Semantic Structures:
Semantic Tree-to-string Alignment Models

Xiaodan Zhu & Colin Cherry
Institute for Information Technology
National Research Council Canada

{Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca

Gerald Penn
Department of Computer Science

University of Toronto
gpenn@cs.toronto.edu

Abstract

This paper addresses a semantic tree-to-
string alignment problem: indexing spo-
ken documents with known hierarchical
semantic structures, with the goal to help
index and access such archives. We pro-
pose and study a number of alignment
models of different modeling capabilities
and time complexities to provide a com-
prehensive understanding of these unsu-
pervised models and hence the problem it-
self.

1 Introduction

The inherent difficulties in efficiently accessing
spoken documents raise the need for ways to better
organize such archives. Such a need parallels with
the consistently increasing demand for and avail-
ability of audio content on web pages and other
digital media, which, in turn, should come as no
surprise, with speech being one of the most basic,
most natural forms of human communication.

When intended to be read, written documents
are almost always presented as more than unin-
terrupted text strings; e.g., indicative structures
such as section/subsection headings and tables-of-
contents are standard constituents created manu-
ally to help readers, whereas structures of this kind
are rarely aligned with spoken documents, which
has raised little concern—in most time of history,
speech has not been ready for navigation, un-
til very recently, when recording, delivering, and
even automatic transcription were possible.

Navigating audio documents is often inherently
much more difficult than browsing text. An ob-
vious solution, in relying on human beings’ abil-
ity of reading text, is to conduct a speech-to-text
conversion through ASR, which in turn raises a
new set of problems to be considered. First, the
convenience and efficiency of reading transcripts

are affected by errors produced in transcription
channels, though if the goal is only to browse
the most salient parts, recognition errors in ex-
cerpts can be reduced by considering ASR con-
fidence (Xie and Liu, 2010; Hori and Furui, 2003;
Zechner and Waibel, 2000) and the quality of ex-
cerpts can be improved from various perspectives
(Zhang et al., 2010; Xie and Liu, 2010; Zhu et
al., 2009; Murray, 2008; Zhu and Penn, 2006;
Maskey and Hirschberg, 2005). Even if transcrip-
tion quality were not a problem, browsing lengthy
transcripts is not straightforward, since, as men-
tioned above, indicative browsing structures are
barely manually created for and aligned with spo-
ken documents. Ideally, such semantic structures
should be inferred directly from the spoken doc-
uments themselves, but this is known to be diffi-
cult even for written texts, which are often more
linguistically well-formed and less noisy than au-
tomatically transcribed text. This paper studies
a less ambitious problem: we align an already-
existing hierarchical browsing structure, e.g., the
electronic slides of lecture recordings, with the se-
quential transcripts of the corresponding spoken
documents, with the aim to help index and access
such archives. Specifically, we study a number of
semantic tree-to-string alignment models with dif-
ferent modeling capabilities and time complexities
in order to obtain a comprehensive understanding
of these models and hence the indexing task itself.

Semantic Structures of Spoken Documents
Much previous work, similar to its written-text
counterpart, has attempted to find certain flat
structures of spoken documents such as topic and
slide boundaries (Malioutov et al., 2007; Zhu et
al., 2008), which, however, involve no hierarchical
structures of a spoken document, thought as will
be shown in this paper, topic-segmentation mod-
els can be considered in our alignment task. Re-
search has also resorted to other multimedia chan-
nels, e.g., video (Fan et al., 2006), to detect slide

509



transitions. This type of approaches, however, are
unlikely to recover semantic structures more de-
tailed than slide boundaries.

Zhu et al. (2010) investigate the problem of
aligning electronic slides with lecture transcripts
by first sequentializing bullet trees on slides
with a pre-order walk before conducting align-
ment, through which the problem is reduced to
a string-to-string alignment problem and conven-
tional methods such as DTW (dynamic time warp-
ing) based alignment can then be directly ap-
plicable. A pre-order walk of bullet tree on
slides is actually a natural choice, since speak-
ers of presentations often follow such an order
to develop their talks, i.e., they discuss a parent
bullet first and then each of its children in se-
quence. However, although some remedies may
be taken (Zhu et al., 2010), sequentializing the hi-
erarchies before alignment, in principle, enforces
a full linearity/monotonicity between transcripts
and slide trees, which violates some basic proper-
ties of the problem that we will discuss. More re-
cently, the work of (Zhu, 2011) proposes a graph-
partitioning based model (revisited in Section 4)
and shows that the model outperforms a bullet-
sequentializing model.

With this previous work available, several im-
portant questions, however, are still open in ob-
taining a comprehensive understanding of the se-
mantic tree-to-string alignment task. First of all,
a basic question is associated with different ways
of exploiting the semantic trees when performing
alignment, which, as will be studied comprehen-
sively in this paper, results in models of different
modeling capabilities and time complexities. Sec-
ond, all the models discussed above consider only
similarities between bullets and transcribed utter-
ances, while similarities among utterances, which
directly underline a cohesion model, are generally
ignored. We will show in this paper that the state-
of-the-art topic-segmentation model (Malioutov
and Barzilay, 2006) can be inherently incorporated
into the graph-partitioning-based alignment mod-
els. Third, the different alignment objectives, e.g.,
that of the graph-partitioning models versus that of
basic DTW-based models, are entangled together
with different ways of exploiting the bullet tree
structures in (Zhu, 2011). In this paper, we discuss
two more quadratic-time models to bridge the gap.

Specifically, this paper studies nine different
models, with the aim to provide a comprehensive

understanding of the questions discussed above.
In the remainder of the paper, we will first review
the related work (Section 2) and more formally de-
scribe our problem (Section 3). Then we revisit
the graph-partitioning alignment model (Section
4), before present all the alignment models we will
study (Section 5). We describe our experiment set-
up in Section 7 and results in Section 8, and draw
our conclusions in Section 9.

2 Related Work

Alignment of parallel texts In general, research
on finding correspondences between parallel texts
pervades both spoken and written language pro-
cessing, e.g., in training statistical machine trans-
lation models, identifying relationship between
human-written summaries and their original texts
(Jing, 2002), force-aligning speech and transcripts
in ASR, and grounding text with database facts
(Snyder and Barzilay, 2007; Chen and Mooney,
2008; Liang et al., 2009). Our problem here,
however, is distinguished in several major aspects,
which need to be considered in our modeling.
First, it involves segmentation—alignment is con-
ducted together with the decision of the corre-
sponding segment boundaries on transcripts; in
other words, we are not finally concerned with
the specific utterances that a bullet is aligned to,
but the region of utterances. In such a sense,
graph partitioning seems intuitively to be more
relevant than models optimizing a full-alignment
score. Second, unlike a string-to-string alignment
task, the problem involves hierarchical tree struc-
tures. This allows for different ways of combining
tree traversal with the alignment process, as will
be studied in detail in this paper. Third, the hi-
erarchical structures as well as the texts on them
are fixed and unique to each document (here a lec-
ture) and knowledge is little generalizable across
different documents. We accordingly keep our so-
lution in an unsupervised framework. Fourth, the
length of transcripts and that of the hierarchies are
very imbalanced, and the former can be as long
as tens of thousands of utterances or hundreds of
thousands of words, which requires a careful con-
sideration of a model’s time complexity.

Building Tables-of-contents on Written Text
Learning semantic structures of written text has
been studied in a number of specific tasks, which
include, but not limited to, those finding seman-
tic representations for individual sentences and

510



those constructing hierarchical structures among
sentences or larger text blocks. A notable effort of
the latter kind, for example, is the work of (Brana-
van et al., 2007), which aims at the ultimate goal
of building tables-of-contents for written texts,
though the problem was restricted to generating
titles for each text span by assuming the availabil-
ity of the structures of tables-of-contents and their
alignments with text spans. Our work here can be
thought of as an inverse problem, in which a spe-
cific type of semantic hierarchical structures are
known, and we need to establish their correspon-
dence with the spoken documents.

Rhetoric Analysis In general, analyzing dis-
course structures can provide thematic skeletons
(often represented as trees) of a document as well
as relationship between the nodes in the trees. Ex-
amples include the widely known discourse pars-
ing work of (Marcu, 2000). However, when the
task involves the understanding of high-level dis-
course, it becomes more challenging than finding
local discourse conveyed on small spans of text;
e.g., the latter is more likely to benefit from the
presence of discourse markers. Specifically for
spoken documents, speech recognition errors, ab-
sence of formality and thematic boundaries, and
less linguistically well-formedness of the spoken
language, will further impair the conditions on
which an reliable discourse-analysis algorithm is
often built. In this paper, we study a less ambi-
tious but naturally occurring problem.

3 Problem

We are given a speech sequence U =
u1, u2, ..., uN , where ui is an utterance, and
the corresponding hierarchical structure, which,
in our work here, is a sequence of lecture
slides containing a set of slide titles and bullets,
B = {b1, b2, ..., bM}, organized in a tree structure
T (ℜ,ℵ,Ψ), where ℜ is the root of the tree that
concatenates all slides of a lecture; i.e., each slide
is a child of the root ℜ and each slide’s bullets
form a subtree. In the rest of this paper, the word
bullet means both the title of a slide (if any) and
any bullet in it. ℵ is the set of nodes of the tree
(both terminal and non-terminals, excluding the
root ℜ), each corresponding to a bullet bm in the
slides. Ψ is the edge set. With the definitions,
our task is herein to find the triple (bi, uj , uk),
denoting that a bullet bi is mapped to a region
of lecture transcripts that starts from the jth

utterance uj and ends at the kth, inclusively. Con-
strained by the tree structure, the transcript region
corresponding to an ancestor bullet contains those
corresponding to its descendants; i.e., if a bullet
bi is the ancestor of another bullet bn in the tree,
the acquired boundary triples (bi, uj1 , uk1) and
(bn, uj2 , uk2) should satisfy j1 ≤ j2 and k1 ≥ k2.

4 Graph-partitioning Models: A Revisit

To facilitate our discussion later in this paper,
we briefly revisit the graph-partitioning alignment
model proposed in (Zhu, 2011), which, inspired
by (Malioutov and Barzilay, 2006; Shi and Malik,
2000), extended a graph-partitioning model to find
the correspondence between the bullets on elec-
tronic slides and transcribed utterances.

Consider a general, simple two-set partitioning
case, in which a boundary is placed on a graph
G = (V,E) to separate its vertices V into two
sets, A and B, with all the edges between these
two sets being removed. The objective, as we
have mentioned above, is to minimize the follow-
ing normalized-cut score:

Ncut(A,B) =
cut(A,B)

assoc(A,V )
+

cut(A,B)

assoc(B,V )
(1)

In equation (1), cut(A,B) is the total weight of
the edges being cut, i.e., those connecting A with
B, while assoc(A,V ) and assoc(B,V ) are the
total weights of the edges that connect A with all
vertices V , and B with V , respectively. In general,
minimizing such a normalized-cut score has been
shown to be NP-complete. In our problem, how-
ever, the solution is constrained by the linearity of
segmentation on transcripts, similar to that in topic
segmentation (Malioutov and Barzilay, 2006). In
such a situation, a polynomial-time algorithm ex-
ists (Zhu, 2011).

Consider a set of sibling bullets, b1, ..., bm, that
appear on the same level of a bullet tree and share
the same parent bp. For the time being, we as-
sume the corresponding region of transcripts has
already been identified for bp, say u1, ..., un. We
connect each bullet in b1, ..., bm with utterances
in u1, ..., un by their similarity, which results in
a bipartite graph. Our task here is to place m − 1
boundaries onto the bipartite graph to partition the
graph into m bipartite graphs and obtain triples,
e.g., (bi, uj , uk), to align bi to uj , ..., uk , where
bi ∈ {b1, ..., bm} and uj , uk ∈ {u1, ..., un} and
j <= k. Since we have all descendant bullets to

511



help the partitioning, when constructing the bipar-
tite graph, we actually include also all descendant
bullets of each bullet bi, but ignoring their orders
within each bi. We find optimal normalized cuts in
a dynamic-programming process with the follow-
ing recurrence relation:

C[i, k] = min
j≤k

{C[i− 1, j] +D[i, j + 1, k]} (2)

In equation (2), C[i, k] is the optimal/minimal
normalized-cut value of aligning the first i sib-
ling bullets, b1, ..., bi, with the first k utterances,
u1, ..., uk . It is computed by updating C[i − 1, j]
with D[i, j + 1, k], for all possible j s.t. j ≤ k,
where D[i, j + 1, k] is a normalized-cut score for
the triple (bi, uj+1, uk) and is defined as follows:

D[i, j + 1, k] =
cut(Ai,j+1,k, V \ Ai,j+1,k)

assoc(Ai,j+1,k, V )
(3)

where Ai,j+1,k is the vertex set that contains the
bullet bi (including its descendant bullets, if any, as
discussed above) and the utterances uj+1, ..., uk ;
V \ Ai,j+1,k is its complement set.

Different from the topic segmentation prob-
lem (Malioutov and Barzilay, 2006), the graph-
partitioning alignment model needs to remem-
ber the normalized-cut values between any region
uj , ..., uk and any bullet bi in our task, which re-
quires to use the additional subscript i in Ai,j+1,k,
while in topic segmentation, the computation of
both cut(.) and assoc(.) is only dependant on the
left boundary j and right boundary k. Also, the
similarity matrix here is not symmetric as in topic
segmentation, but m by n, where m is the number
of bullets, while n is the number of utterances.

As far as time complexity is concerned, the
graph-partitioning models discussed above are
quadratic with regards to N , i.e., O(MN2), where
M ≪ N ; M and N denoting the number of bul-
lets and utterances, respectively, with the loop ker-
nel computing and filling D[i, j, k] in equation 3,
which is a M × N × N matrix. Zhu (2011) ap-
plied the algorithm deterministically in traversing
a bullet tree top-down: starting from the root, the
normalized-cut algorithm finds the corresponding
regions of transcripts for all the direct children of
the root, fixes the regions, and repeats this process
recursively to partition lower-level bullets. This
whole algorithm is still quadratic O(MN2) but
outperforms a bullet-sequentializing baseline.

5 Alignment Models

Now, we discuss the models that we will study fur-
ther in this paper to address the problems rise ear-
lier in the introduction section.

5.1 The O(MN4) Models

As discussed, Zhu (2011) proposed a graph-
partitioning alignment model and applied it in a
deterministic way along with a top-down traversal
of bullet trees. Though such models could be very
competitive in performance, an important ques-
tion, however, is with regard to the performance of
models that can optimize a global score rather than
local ones on each set of sibling bullets, which
requires a study of models with more modeling
capability (containing the deterministic hierarchi-
cal models as a special case) and with higher time
complexities.

Naively, searching all possible partitions to op-
timizing a global score needs to consider an ex-
ponential space in terms of the number of tran-
scribed utterances, while applying dynamic pro-
gramming similar to those used in syntactic pars-
ing would keep the solution to be polynomial. In
this section, we introduce such alignment models;
or in another viewpoint, we formulate the align-
ment task in a parsing-like setting. A dynamic
programming approach, e.g., that used in a con-
ventional CYK parser, can be adapted to solve
this problem, in which one can replace the splitter
moving in each text span in the classic CYK with
the quadratic bipartite-graph partitioning model
discussed above. However, in our task here, the
trees, unlike in a general parsing task, are given
and fixed, meaning that the cells of a parsing ta-
ble can be filled in a fixed order, i.e., a post order,
so that the search speed can be improved by some
constant.

Figure 1 shows an algorithm, in which we insert
the bipartite graph partitioning model that works
on sibling bullets (as discussed in Section 4) into
a parsing search process (line (12)). We call this
model PrsCut. Note that there are more than one
way to conducting such a search, but they should
yield the same results once the objective function,
e.g., the normalized-cut score here, is the same.

Specifically, the Main function in Figure 1 takes
as input an M ×N similarity matrix, where, same
as before, M and N denote the number of bul-
lets and transcribed utterances in a lecture, re-
spectively. The Main function first computes the

512



Figure 1: An algorithm of optimizing a global
normalized-cut score.

cutCostTab, which saves the D[i, j + 1, k] val-
ues defined by equation (3). Then the parsing
table prsTab is built with a post-order traversal
algorithm Build-Parsing-Tab, followed by a de-
coding process that finds the optimal partition-
ing tree. As sketched in Figure 1, the Build-
Parsing-Tab algorithm builds a 3-dimensional ta-
ble prsTab, each cell saving a value that linearly
combines the corresponding cutCostTab value of
the current node/bullet curNd and the optimal par-
titioning score bestScr value calculated on its de-
scendent, if any (see line (13)); or if the current
node curNd is a leaf itself, its bestScr score is
zero; in such case, the prsTab value is initialized
with the cutCostTab value (line (5)). The recur-
sive algorithm traverse the bullet tree in a post-
order walk, which, as discussed above, utilizes the
given, fixed bullet tree structures to fill the pars-
ing table prsTab. The weight w in line (13) is
set in a held-out data and note that if w is set to
be 0, the model degrades to be the deterministic
hierarchical model discussed in (Zhu, 2011) and
referred to as HieCut below in Section 5.2, since
in this case the prsTab is same as costCustTab.
As far as time complexity is concerned, the whole
algorithm is O(MN4), shown by the nested for-
loops of line (10)-(15) that contain the O(MN2)
bigraph-partitioning alignment in line (12). Simi-

larly, we can insert a standard DTW-based align-
ment model into the line (12) here, which we call
the PrsBase model. Note that the real algorithm
is a little more complicated; e.g., we need to allow
a parent bullet to have a different starting position
than its first child, same as in (Zhu, 2011).

5.2 The O(MN2) models

Sequential Alignment Models As discussed ear-
lier, in a simplified situation, our problem here can
be formulated as a sequential alignment problem,
based on a fairly reasonable assumption (Zhu et
al., 2010): a speaker follows a pre-order walk of
a bullet tree to develop the talk, i.e., discussing
a parent bullet first, followed by each of its chil-
dren in sequence. Accordingly, the models first
sequentialize bullet trees with a pre-order walk
before conducting alignment, through which the
problem is reduced to a string-to-string alignment
problem and conventional methods such as DTW-
like alignment can then be applicable. Such a pre-
order walk has also been assumed by (Branavan et
al., 2007) to reduce the search space in their table-
of-contents generation task, a problem in which a
tree hierarchy has already been aligned with a span
of written text, while the title of each node on the
tree needs to be generated.

With this formulation, we first included here the
baseline model in (Zhu et al., 2010), which ap-
plies a typical DTW-based alignment. We refer
to the model as SeqBase. In addition, we applied
the graph-partitioning based models discussed in
(Zhu, 2011) to align the sequentialized bullets and
the corresponding transcribed utterances, and we
call this model SeqCut. The motivation of study-
ing SeqCut is to further understand the benefit of
graph-partitioning based models. For example, it
allows us to disentangle the benefit of the deter-
ministic graph-partitioning models in (Zhu, 2011):
whether the benefit is due to the modeling advan-
tage of the proposed partitioning objective or its
avoiding sequentializing bullet trees.

In principle, sequentializing bullet trees before
alignment enforces a full linearity/monotonicity
between transcripts and these bullet trees, which,
though based on a reasonable assumption and is
fairly effective (as will be shown in our com-
prehensive comparison later), misses some basic
properties of the problem. For example, the gen-
erative process of lecture speech, with regards to
a hierarchical structure (here, bullet trees), is char-

513



acterized in general by a speaker’s producing de-
tailed content for each bullet when discussing it,
during which sub-bullets, if any, are talked about
recursively. By the nature of the problem, words
in a bullet could be repeated multiple times, even
when the speaker traverses to talk about the de-
scendant bullets in the depth of the sub-trees. That
is, the content of a bullet could be mentioned not
only before its children but also very likely when
the speaker traverses to talk descendant bullets, if
any, which violate the pre-order-walk assumption.

Though with shortcomings, an important ben-
efit of formulating the task as a sequential-
alignment problem is its computational efficiency:
solutions can be acquired in quadratic time. This
is of particular importance for this task, consider-
ing that the length of a document, such as a lecture
or a book, is often long enough to make less effi-
cient algorithms practically intractable. A natural
question to be ask is therefore whether we can, in
principle, model the problem better, but still keep
the time complexity quadratic, i.e., O(MN2).

Deterministic Hierarchical Models Determinis-
tically deciding bullets’ boundaries on transcribed
utterances when traversing the bullet tree can keep
the solution within a quadratic time complexity
and avoid a sequentialization of bullet trees be-
forehand. For example, in (Zhu, 2011), the graph-
partitioning alignment model, as discussed above,
is applied in such a deterministic way; the model
recursively traverses a bullet tree by first determin-
ing transcript boundaries of the direct children of
the root, fixing the boundaries found, and then de-
termining boundaries for the descendant bullets re-
cursively1. We refer to this model as HieCut in
this paper. Note that though working deterministi-
cally, this models utilize the similarities associated
with all descendant bullets of the current sibling
bullets under concern, to find the optimal bound-
aries between these siblings. In addition, we in-
clude a standard DTW-based alignment model in
such a deterministic-decision process, called the
HieBase model in the remainder of this paper.

One major benefit of the deterministic hierar-
chical alignment models is their time complex-
ity: still quadratic, same as the sequential align-
ment model discussed above, though models like
HieCut can achieve a very competitive perfor-

1A pre-order walk can be used here (not for sequentializ-
ing bullet trees though); other top-down transversing methods
are also applicable, e.g., a breadth-first search, once a parent
bullet is visited before its children.

mance, which we will discuss in detail later. Also,
the deterministic hierarchical models need less
memories than the corresponding O(MN4) mod-
els and even the sequential models. For example,
the memory needed by HieCut is proportional to
the maximal number of sibling bullets in a tree,
not the total number of bullets.

6 The Topic-segmentation Model

Up to now, we have discussed a variety of align-
ment models with different model capabilities and
time complexities, which, however, consider only
similarities between bullets and utterances. Cohe-
sion in text or speech, by itself, often evidenced by
the change of lexical distribution (Hearst, 1997),
can also indicate topic or subtopic transitions, even
among subtle subtopics (Malioutov and Barzilay,
2006). In our problem here, when a lecturer dis-
cusses a bullet, the words used are likely to be
different from those used in another bullet, sug-
gesting that the spoken documents themselves,
when ignoring the alignment model above for the
time being, could potentially indicate the seman-
tic boundaries that we are interested in here. Par-
ticularly, the cohesion conveyed by the repeti-
tion of the words that appear in transcripts but
not in slides could be additionally helpful; this
is very likely to happen considering the signifi-
cant imbalance of text lengths between bullets and
transcripts, from which the alignment models by
themselves may suffer.

C[i, k] = min
j≤k

{C[i− 1, j] + λ1D[i, j + 1, k]

+(1− λ1)S[j + 1, k]} (4)

where,

S[j + 1, k] =
cut(Aj+1,k, V \Aj+1,k)

assoc(Aj+1,k, V )
(5)

In fact, a state-of-the-art topic-segmentation
model (Malioutov and Barzilay, 2006) (also called
a cohesion model in this paper) can be nat-
urally incorporated into the graph-partitioning
alignment models that we have discussed. That
is, we can augment the SeqCut, HieCut, and
PrsCut models with the cohesion models to form
three new models SeqCutTpc, HieCutTpc, and
PrsCutTpc, respectively. To achieve this, we
modify equation (2) to equation (4), where S[j +
1, k] is calculated as in (Malioutov and Barzilay,

514



2006), which denotes the normalized partition cost
of the segment from utterance uj+1 to uk, inclu-
sively. For complexity, since the cohesion model
is O(MN2), linearly combining it would not in-
crease the time complexities of the corresponding
polynomial alignment models, which are at least
O(MN2) by themselves.

7 Experiment Set-up

Corpus Our experiment uses a corpus of four
50-minute university lectures taught by the same
instructor, which contain 119 slides composed
of 921 bullets. The automatic transcripts of the
speech contain approximately 30,000 word to-
kens, roughly equal to a 120-page double-spaced
essay in length. The lecturer’s voice was recorded
with a head-mounted microphone with a 16kHz
sampling rate and 16-bit samples, while students’
comments and questions were not recorded. The
speech is split into utterances by pauses longer
than 200ms, resulting in around 4000 utterances.
The slides and automatic transcripts of one lec-
ture were used as the development set. In practice,
each lecture is divided into three roughly equally-
long pieces in all our experiments discussed be-
low, for pragmatic computational consideration of
calculating the O(MN4) models quickly enough.

Building the Graphs The transcripts were
generated with the SONIC toolkit (Pellom,
2001), with the models trained as suggested by
(Munteanu et al., 2007), in which one language
model was trained on SWITCHBOARD and the
other used also corpus obtained from the Web
through searching the words on slides. Both bul-
lets and automatic transcripts were stemmed with
the Porter stemmer and stopwords were removed.
The similarities between bullets and utterances
and those between utterances were calculated with
different distance metrics, i.e., cosine, exponential
cosine (Malioutov and Barzilay, 2006) for topic
segmentation, and a normalized word-overlapping
score used in summarization (Radev et al., 2004),
from which we chose the one (regular cosine) that
optimizes our baseline. Our graph-partitioning
models then used exactly the same setting. The
lexical weighting is same as in (Malioutov et al.,
2007), for which we split each lecture into M
chunks, the number of bullets. Finally, we ob-
tained a M-by-N bullet-utterance similarity matrix
and a N-by-N utterance-utterance matrix to opti-
mize the alignment model and topic-segmentation

model, respectively, while M and N , as already
mentioned, denote the number of bullets and ut-
terances of a lecture, respectively.

Evaluation Metric The metric used in our eval-
uation is straightforward—automatically acquired
boundaries on transcripts for each slide bullet are
compared against the corresponding gold-standard
boundaries to calculate offsets measured in num-
ber of words, counted after stopwords having been
removed, which are then averaged over all bound-
aries to evaluate model performance. Though one
may consider that different bullets may be of dif-
ferent importance, in this paper we do not use
any heuristics to judge this and we treat all bul-
lets equally in our evaluation. Note that topic
segmentation research often uses metrics such
as Pk and WindowDiff (Malioutov and Barzilay,
2006; Beeferman et al., 1999; Pevsner and Hearst,
2002). Our problem here, as an alignment prob-
lem, has an exact 1-to-1 correspondence between
a gold and automatic boundary, in which we can
directly measure the exact offset of each bound-
ary.

8 Experimental Results

Alignment Models Table 1 presents the exper-
imental results obtained on the automatic tran-
scripts generated by the ASR models discussed
above, with WERs of 0.43 and 0.48, respectively,
which are typical for lectures and conference pre-
sentations in realistic and less controlled situa-
tions (Leeuwis et al., 2003; Hsu and Glass, 2006;
Munteanu et al., 2007).

The results show that among the four quadratic
models, i.e., the first four models in the table,
HieCut achieves the best performance. The
results also suggest that the improvement of
HieCut over SeqBase comes from two aspects.
First, the normalized-cut objective used in the
graph-partitioning based model seems to outper-
form that used in the baseline, indicated by the bet-
ter performance of SeqCut over SeqBase, since
both take as input the same, sequentialized bul-
let sequence and the corresponding transcribed
utterances. The DTW-based objective used in
SeqBase corresponds to finding the optimal path
that maximizes the similarity score between the
bullet sequence and the transcripts. Second, the
better performance of HieCut and SeqCut shows
that HieCut further benefits from avoiding se-
quentializing the bullet trees. However, this two

515



aspects of benefit do not come independently,
since the former (performance of an alignment ob-
jective) can significantly affect the latter (whether
a model can benefit from avoiding sequentializ-
ing bullets). This is evident in the inferior per-
formance of HieBase. Manual analysis of its
errors shows that HieBase is less accurate than
HieCut on higher-level bullets and the errors in
turn severely impair the decisions made on lower-
level bullets in the deterministic decision process:
the errors propagate severely in such a determinis-
tic process.

Models WER=0.43 WER=0.48

SeqBase 15.19 18.44

SeqCut 12.87 16.16

HieBase 21.06 24.25

HieCut 12.13 15.95

PrsBase 15.05 18.18

PrsCut 12.05 15.20

Table 1: The performances of different alignment
models.

A closer examination of errors made by
HieBase suggests that in a DTW-based align-
ment, a large subtree is likely to be aligned to
a region larger than it should be, particularly for
higher-level bullets (e.g., slides), where the sub-
tree sizes vary more, e.g., some slides contain-
ing much textual content and others containing
little. It seems that HieCut could counteract
this effect with its capability of normalizing par-
tition sizes (see the denominators in both equa-
tion (1) and (3)). The usefulness of the normal-
ization has also been discussed in other tasks such
as image segmentation (Shi and Malik, 2000).
Compared with those of HieBase, segments in
the SeqBase model are smaller (all non-leaf bul-
lets do not include its descendants after being se-
quentialized) and the pre-order walk constrains the
alignment range of bullets, which often avoid er-
rors of long offsets. Again, the HieCut model is
quadratic in time, it uses less memories than the
O(MN4) models and even the SeqCut model,
and it achieves a very competitive overall perfor-
mance.

The results in Table 1 also shows that the
(O(MN4)) models, which conduct a more thor-
ough search, improve the performance in all situa-
tions.

Effect of Topic-segmentation Models The effect
of the topic-segmentation model is presented in
Table 2. To facilitate reading, we also copy here
the relevant results from Table 1. The results
show that incorporating text cohesion addition-
ally reduces the errors consistently for all models,
though the specific improvement varies.

Models WER=0.43 WER=0.48

SeqCut 12.87 16.16

SeqCutTpc 12.77 15.14

HieCut 12.13 15.95

HieCutTpc 11.82 15.28

PrsCut 12.05 15.20

PrsCutTpc 11.34 14.62

Table 2: The effect of topic-segmentation models.

9 Conclusions

In addressing the semantic tree-to-string align-
ment problem described, this paper proposes and
studies a number of models with different mod-
eling capabilities and time complexities. Exper-
imental results show that among the quadratic
alignment models (O(MN2)), HieCut consis-
tently achieves the best performance, while the
O(MN4) models that optimize a global ob-
jective score further improve the performance,
though such models are, pragmatically, much
more computationally expensive. This paper also
relates alignment models with topic-segmentation
models by showing that a state-of-the-art topic-
segmentation models can be inherently incorpo-
rated into the graph-partitioning based alignment
models. The experimental results show the benefit
of considering such cohesion knowledge.

References

D. Beeferman, A. Berger, and J. Lafferty. 1999. Statis-
tical models for text segmentation. Machine Learn-
ing, 34(1-3):177–210.

S. Branavan, Deshpande P., and Barzilay R. 2007.
Generating a table-of-contents: A hierarchical dis-
criminative approach. In Proc. of Annual Meeting
of the Association for Computational Linguistics.

D.L. Chen and R.J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In Proc. of International Conference on Machine
Learning.

516



Q. Fan, K. Barnard, A. Amir, A. Efrat, and M. Lin.
2006. Matching slides to presentation videos using
sift and scene background. In Proc. of ACM Inter-
national Workshop on Multimedia Information Re-
trieval, pages 239–248.

M. Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.

C. Hori and S. Furui. 2003. A new approach to auto-
matic speech summarization. IEEE Transactions on
Multimedia, 5(3):368–378.

B. Hsu and J. Glass. 2006. Style and topic language
model adaptation using hmm-lda. In Proc. of Con-
ference on Empirical Methods in Natural Language
Processing.

H. Jing. 2002. Using hidden markov modeling to de-
compose human-written summaries. Computational
Linguistics, 28(4):527–543.

E. Leeuwis, M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the ted corpus
lectures. In Proc. of IEEE International Conference
on Acoustics, Speech and Signal Processing.

P. Liang, M. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proc. of Annual Meeting of the Association for Com-
putational Linguistics.

I. Malioutov and R. Barzilay. 2006. Minimum cut
model for spoken lecture segmentation. In Proc.
of International Conference on Computational Lin-
guistics and Annual Meeting of the Association for
Computational Linguistics.

I. Malioutov, A. Park, R. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. of Annual
Meeting of the Association for Computational Lin-
guistics, pages 504–511.

D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. The MIT Press.

S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features
for speech summarization. In Proc. of European
Conference on Speech Communication and Technol-
ogy, pages 621–624.

C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modelling for automatic lecture tran-
scription. In Proc. of Annual Conference of the In-
ternational Speech Communication Association.

G. Murray. 2008. Using Speech-Specific Character-
istics for Automatic Speech Summarization. Ph.D.
thesis, University of Edinburgh.

B. L. Pellom. 2001. Sonic: The university of colorado
continuous speech recognizer. Tech. Rep. TR-CSLR-
2001-01, University of Colorado.

L. Pevsner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19–36.

D. Radev, H. Jing, M. Stys, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40:919–938.

J. Shi and J. Malik. 2000. Normalized cuts and im-
age segmentation. IEEE Trans. Pattern Anal. Mach.
Intell., 22.

B. Snyder and R. Barzilay. 2007. Database-text align-
ment via structured multilabel classification. In
Proc. of International Joint Conference on Artificial
Intelligence.

S. Xie and Y. Liu. 2010. Using confusion networks
for speech summarization. In Proc. of International
Conference on Human Language Technology and
Annual Meeting of North American Chapter of the
Association for Computational Linguistics.

K. Zechner and A. Waibel. 2000. Minimizing word er-
ror rate in textual summaries of spoken language. In
Proc. of Applied Natural Language Processing Con-
ference and Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 186–193.

J. Zhang, H. Chan, and P. Fung. 2010. Extrac-
tive speech summarization using shallow rhetorical
structure modeling. IEEE Transactions on Audio,
Speech and Language Processing, 18:1147–1157.

X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proc. of International
Conference on Spoken Language Processing, pages
1531–1534.

X. Zhu, X. He, C. Munteanu, and G. Penn. 2008. Us-
ing latent dirichlet allocation to incorporate domain
knowledge for topic transition detection. In Proc.
of Annual Conference of the International Speech
Communication Association.

X. Zhu, G. Penn, and F. Rudzicz. 2009. Summarizing
multiple spoken documents: Finding evidence from
untranscribed audio. In Proc. of Annual Meeting of
the Association for Computational Linguistics.

X. Zhu, C. Cherry, and G. Penn. 2010. Imposing
hierarchical browsing structures onto spoken doc-
uments. In Proc. of International Conference on
Computational Linguistics.

X. Zhu. 2011. A normalized-cut model for aligning
hierarchical browsing structures with spoken docu-
ments. In Proc. of the Fifteenth Conference on Com-
putational Natural Language Learning (to appear).

517


