















































Reduction of Search Space to Annotate Monolingual Corpora


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1457–1461,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Reduction of Search Space to Annotate Monolingual Corpora

Prajol Shrestha Christine Jacquin
Laboratore d’Informatique de Nantes-Atlantique (LINA)

Université de Nantes
44322 Nantes Cedex 3, France

{prajol.shrestha;christine.jacquin;beatrice.daille}@univ-nantes.fr

Beatrice Daille

Abstract

Monolingual corpora which are aligned
with similar text segments (paragraphs,
sentences, etc.) are used to build and test a
wide range of natural language processing
applications. The drawback wanting to use
them is the lack of publicly available an-
notated corpora which obligates people to
make one themselves. The annotation pro-
cess is a time consuming and costly task.
This paper describes a new corpus-based
measure to significantly reduce the search
space for a faster and easier manual an-
notation process for monolingual corpora.
This measure can be used in making align-
ments on different types of text segments.
The performance of this measure is eval-
uated on a manually annotated paragraph
corpus, whose alignments are freely avail-
able, with promising results.

1 Introduction

In the field of Natural Language Processing
(NLP), annotated monolingual corpora are used to
build and test a wide range of applications such as
information retrieval, summarization, plagiarism
detection, dictionary building and so on. With
a number of applications to be built, the field of
NLP requires a wide range of monolingual cor-
pus with different annotations on different level of
text segments. Our focus is on the fast and easy
way of aligning short text segments based on sim-
ilarity. These annotations are usually done man-
ually by annotators as done by Hatzivassiloglou
et al. (1999) where a corpus with alignments be-
tween similar short texts are created by two or
more annotators who look at each possible short
text pair independently and analyse them to make
a decision on whether each pair should be aligned
as similar or not. Finally, the annotators discuss

the disagreements between their annotations and
come to an agreement with reasoning. A cor-
pus containing n number of short texts will gener-
ate n(n−1)2 number of short text pairs for compar-
ing similarities which becomes a tedious and time
consuming task even if a corpus contains a few
hundred of short texts. For example, the corpus
we use consists of 239 paragraphs, explained in
section 3.1, generating a total of 28,441 text pairs
to compare.

There are few publicly available annotated cor-
pus, some are manually annotated like the TDT
corpus1 for topic detection and tracking and the
METER corpus (Gaizauskas et al., 2001) for de-
tection of text reuse and some are automatically
annotated like the PAN-PC-10 (Barrón-Cedeño et
al., 2010) for plagiarism detection and the MSRPC
(Dolan and Brockett, 2005) for paraphrase detec-
tion. Annotating a corpus automatically is easier
and faster than manual annotation but they have
a major limitation which allows the corpus to in-
clude only a subset of the problem which prevents
the corpus to represent many of the naturally oc-
curring instances. This limitation in turn might
cause some incompleteness issues on the applica-
tions built on it as mentioned by Barrón-Cedeño
et al. (2010) and Dolan and Brockett (2005). To
reduce this effect of coverage in a corpus, annota-
tions on corpus are done manually.

We propose manual annotation to be done in
two phases. At first, the number of pairs to com-
pare are reduced and then manual annotation is
done. We present a corpus-based measure to auto-
matically reduce the search space for manual an-
notation making the annotation process faster and
easier. We evaluate this measure using a manually
annotated paragraph corpus2, created by reducing
the search space manually.

1http://projects.ldc.upenn.edu/TDT-Pilot/
2Alignments can be freely downloaded from :

http://www.projet-depart.org/public/LINA-PAL-1.0.tar.gz

1457



2 Search Space Reduction

In this section, we show how we reduce the search
space manually to find similar short texts (e.g.
thematic segments, paragraphs, sentences) and
present a measure to automatize this manual pro-
cess. Similarity is a vague concept and its defi-
nition depends on the application for which it is
intended. The most general intuition of similar-
ity is that, two short texts are similar if they have
something in common (Lin, 1998). This intuition
includes paraphrases, reused text, plagiarized text
and so on as similar texts. Each application spec-
ifies what commonality is required to call it sim-
ilar and we believe our reduction of search space
will be useful for all the application based on this
general intuition of similarity. This reduction of
search space is the first phase towards manual an-
notation. This phase produces candidate simi-
lar pairs which is a subset of the total short text
pairs within which all the actual similar pairs are
present. The number of candidate similar pairs
will be less than the total number of short text pairs
which allows many annotators in the second phase
to efficiently annotate the small set of text pairs
manually in less human hours.

The manual reduction of search space is done
by going through all the possible short text pairs
and selecting the candidate similar pairs using a
criteria which states that: each short text in a can-
didate similar pair consists at least one common
entity (Shrestha, 2011a). This criteria for selection
theoretically guarantees that all the actual similar
pairs will be present in the candidate similar pairs
because for two short texts to be similar they must
have at least one entity in common. The entities
that we use are noun, noun phrase, and transitive
verb (Loberger and Shoup, 2009). Two entities are
said to be common when they both have the same
meaning or in other words share the same concept
for example, the entities ‘crashed’, ‘rammed into a
wall’, ‘fatal impact’ can all be mapped to the con-
cept ‘crashed’ and the entities ‘Prince Charles’,
‘heir to the British throne’ can be mapped to the
concept ‘Prince Charles’. The context within the
short text also helps to identify the concept that the
entity represents.

The selection of candidate similar pairs is eas-
ier and faster because the analysis of the pairs is
not required unlike when selecting actual similar
pairs. This manual reduction is used while build-
ing the paragraph corpus for evaluation. As this

phase is done manually, the annotator can remove
a selected candidate similar pair if a decision of it
not being useful can be taken easily and without
any doubt to further reduce the search space.

2.1 Short text Vector Space Measure (SVSM)

We present a corpus-based measure called Short
text Vector Space Measure (SVSM) (Shrestha,
2011b) based on Vector Space Model (VSM)
(Salton et al., 1975) to reduce the search space.
SVSM assigns a value to each text pair and text
pairs having a value greater than a threshold is
considered as candidate similar pairs. For sim-
plicity reasons, we explain the method using sen-
tences. For each sentence a sentence vector is cre-
ated from term vectors. Given a corpus C of n
sentences and m unique terms, the term vector, ~tj ,
for term tj is a vector created with n number of
possible dimensions where each dimension repre-
sents a unique sentence. The presence of the term
in a sentence is indicated by its sentence id and
the term’s inverse document frequency, idf , here a
document is a sentence, as shown below:

~tj = [(S1, idfj), (S5, idfj), ..., (Si, idfj)]

where Si is the sentence id where the term tj is
present, i ∈ 1, .., n and idfj is the idf value of
term tj . This term vector is a reduced vector space
representation where sentences that do not con-
tain the term is absent which saves space. The di-
mension of the matrix formed by term vectors can
be further reduced using Latent Semantic Anal-
ysis (Deerwester et al., 1990) or Principle Com-
ponent Analysis (Jolliffe, 1986) but are not used
here. Once we have the term vectors we can create
sentence vectors by adding the term vectors of the
terms present in that sentence. For a sentence con-
sisting of terms t1, t2, .., tk, the dimension, di, of
the sentence vector corresponding to the sentence
Si will be:

di = Σ
k
j=1;tj∈Siidfj

where idfj is the idf value of the term j and i ∈
1, .., n. This term vector shows the different senses
that the term may have. Here, the sense of the term
means the idea with which it can be related to.
Our assumption is that sentences are independent
to each other making each sentence presenting a
unique idea and therefore, each term present in a
sentence is related to this idea. This assumption
like the assumption of VSM (Wong et al., 1987)
is unrealistic but the effect of this assumption can

1458



-William and Harry, with their father Prince Charles and their grandmother Queen Elizabeth, are thought likely to remain in
seclusion at Balmoral Castle in Scotland until Saturday’s ceremony.
-The royal family remained at Balmoral in Scotland Tuesday, with reports that Charles and his younger son Prince Harry went
for a walk in the afternoon. It was not clear when they would return to London.
-Dodi Al Fayed’s father, Harrods Department Store owner Mohammed Al Fayed, arrived here immediately after learning of his
son’s death.
-Bernard Dartevelle, a lawyer for Mohamed Al Fayed, Dodi Fayed’s wealthy businessman father and also the owner of the
Hotel Ritz, said the revelation “changes absolutely nothing.” He spoke of an “ambience of harassment” created around Diana
and Fayed by the constant presence of paparazzi.

Table 1: Examples of similar and dissimilar paragraph pairs. The first block consists of a similar paragraph pair whereas the
second block consists of a dissimilar paragraph pair.

be reduced using clustering techniques like hierar-
chical clustering (Han and Kamber, 2006) to group
sentences that give the same idea or in other words
similar sentences.

This method is similar to the method of Kauf-
mann (2000) using lexical cohesion but includes
more information which are i) the importance of
each term using its idf; ii) the co-occurrence of
terms by adding up the idf value in term vectors
while creating sentence vectors; and iii) the dis-
tribution of term along various sentences as the
dimensions of the sentence vector is equal to the
number of sentences present in the corpus. Using
these sentence vectors we can now compute the
similarity value between two sentences using the
cosine similarity measure (Barron-Cedeno et al.,
2009). In this method, other types of short text
can be used in place of sentences.

3 Experiments and Results

3.1 Corpus

The corpus used for experiments was made from
12 articles on the same topic, the death of Di-
ana, from the Linguistic Data Consortium’s (LDC)
North American News Text Corpus (LDC Cat-
alog number: LDC95T21). The articles con-
tain newswire text from three different news ser-
vices which were published within two consecu-
tive days. The articles contained 239 paragraphs,
each of which contains more than 10 non stop-
words, which produces 28,441 paragraph pairs for
comparisons.

3.2 Manual Alignment

We have manually aligned 28,441 paragraph pairs
from the corpus based on similarity. The align-
ment was done in two phases as explained in sec-
tion 1. The first phase was performed by one an-
notator who selected 3,418 candidate similar para-
graph pairs from a total of 28,441 paragraph pairs

which took about 71 hours of work.
The second phase was done manually by two

annotators who independently selected similar
pairs from the candidate pairs. The similarity
definition given to the annotators is an intuitive
definition which states that two paragraphs are
similar if one of the main information that the
paragraph conveys is common. This definition
is slightly different from the definition given by
Shrestha (2011a) based on sub-topics. There exist
few definitions on text similarity but they are all
specific to the size of the text segment (Barzilay,
2003) or entities within the sentences (Hatzivas-
siloglou and Klavans, 2001) which make them un-
suitable for a general text similarity definition. In
Table 1, we present a positive and a negative exam-
ple to further explain the definition. The first block
presents a positive example whose main informa-
tion in common is that the royal family will remain
at Balmoral Castle. The paragraph pair in the sec-
ond block is not similar even though the informa-
tion about Dodi’s father is a businessman is com-
mon because the main information conveyed by
the paragraphs is different. We used kappa statis-
tics (Carletta, 1996; Cohen, 1960) to evaluate the
annotations made by the annotators in the second
phase. Kappa statistics is defined as k = PA−PE1−PE
where, in our case PA=0.959, which is the proba-
bility of two annotators agreeing in practice and
PE=0.918, which is the expected probability of
the two annotators agreeing, and k=0.5, indicating
a moderate agreement (Artstein and Poesio, 2008).
The error between the annotators is about 5% due
to the intuitive definition of similarity. The annota-
tors jointly resolved annotation disagreements be-
tween them by reasoning.

The second phase produced 144 similar para-
graph pairs and took about 20 hours for both an-
notators. The total time that took to annotate the
corpus manually was about 91 hours. If we had
directly tried to find the actual similar paragraph

1459



CS SVSM Overlap
T Retri. Rec. Retri. Rec. T Retri. Rec.
0 15415 100 28406 100 0 15415 100
0.1 957 72.92 17245 100 1 6618 96.53
0.2 169 36.11 7253 97.92 2 2991 87.5
0.3 51 17.36 3009 93.06 3 1434 77.78
0.4 14 6.25 1218 76.39 4 735 63.89
0.5 4 2.08 412 53.47 5 398 50.69
0.6 2 1.39 134 27.78 6 197 32.64

Table 2: Rec. (Recall) and Retri. (Retrieved pairs) of meth-
ods CS, SVSM, and stem overlap according to T (Threshold).

pairs without phase one, with an assumption that
the time taken per paragraph pair (≈21 sec) is the
same as in the second phase, it would take about
166 hours. The total time saved is 75 hours of
work.

3.3 Automatic Selection of Candidate Pairs
The manual alignment method is still time con-
suming and difficult as manual effort has to be
done. SVSM, presented in section 2.1, is used to
reduce the search space for annotators. Its perfor-
mance is compared with stem overlap (Overlap)
and cosine similarity measure (CS) with TF*IDF
as weights (Salton and McGill, 1983). For each
method, stop-words were removed and the re-
maining words were stemmed using the snowball
stemmer3. We decide a paragraph pair is a candi-
date similar pair if the value given by a method ex-
ceeds a threshold. Table 2 shows the performance
based on recall compared to the manually selected
actual similar pairs of section 3.2 and the number
of retrieved paragraph pairs by each method on the
total paragraph pairs at different thresholds.

If we look at the table, the best result with 100%
Recall is given by CS and Overlap methods with
15,415 retrieved pairs but still this is a large num-
ber. Using automatic methods, we would like
to optimize our threshold so that we can reduce
the retrieved paragraph pairs as much as possible
without losing much of the actual similar para-
graph pairs. According to the optimization issue
SVSM is the best among the three methods at
threshold 0.3 with 3009 retrieved paragraph pairs
almost equal to the manually selected candidate
pairs and with a recall of 93.06%. Another prop-
erty we would like in a method for automatic re-
duction of search space is the slow rate of decrease
in recall making sure with a small variation of
threshold the recall will not have a drastic change.
The rate of decrease in recall is shown in Figure

3http://snowball.tartarus.org/

1 where four highest varying recall are plotted for
each method. These values are boldfaced in Ta-
ble 2. From Figure 1 we can see that SVSM is
the method that has the most gradual decrease in
recall making it the most suitable method for au-
tomatic reduction of search space. CS on the other
hand is the least suitable with a sharp decrease
in recall showing that similarity measures based
only on term overlap is not suitable to find simi-
lar short text as discussed by Abdalgader (2011).
Using this method at the threshold 0.3 we can re-
duce the time for manual annotation to about 17.5
hours (3009x21) with a loss of about 10 similar
paragraph pairs only.

Figure 1: The rate of decrease in recall as the retrieved
paragraph decreases.

4 Conclusion and Future Work

We present an automatic method using SVSM to
reduce the total number of paragraph pairs from
which actual similar paragraph pairs are manu-
ally selected. Using the manual method we re-
duced the 28,441 total paragraph comparisons to
only 3,418 paragraph comparisons from which
144 paragraph pairs were aligned as similar. This
shows that 99.5% of the effort in selecting the
similar paragraph is wasted in terms of the differ-
ence between the end number of aligned paragraph
pairs and the total initial pairs. Using the man-
ual method we were able to save 75 hours of hu-
man work which can be further increased to 148.5
hours by using the automatic method in expense
of few similar pairs. In future, the reliability of the
threshold will be tested on other corpus and the
present manually annotated corpus will be pop-
ulated with more manually selected similar para-
graph pairs using the automatic method.

1460



Acknowledgements. This work is supported
by the French Region Pays de Loire in the con-
text of the DEPART project (http://www.projet-
depart.org/).

References
Khaled Abdalgader and Andrew Skabar. 2011. Short-

text similarity measurement using word sense dis-
ambiguation and synonym expansion. AI 2010:Ad-
vances in Artificial Intelligence, Lecture Notes in
Computer Science, 6464:435–444.

Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.

Alberto Barron-Cedeno, Andreas Eiselt, and Paolo
Rosso. 2009. Monoligual text similarity measures:
A comparison of models over wikipedia articles re-
visions. Proceedings of ICON-2009: 7th Interna-
tional Conference on Natural Language Processing.

Alberto Barrón-Cedeño, Martin Potthast, Paolo Rosso,
Benno Stein, and Andreas Eiselt. 2010. Corpus and
Evaluation Measures for Automatic Plagiarism De-
tection. Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC 10).

Regina Barzilay. 2003. Sentence alignment for mono-
lingual comparable corpora. In 18th Conference of
the 2003 conference on Empirical methods in natu-
ral language processing, pages 25–32.

Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. In Computational
Linguistics, pages 249–254.

Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. In Educational and Psyhological
Measurement, pages 37–46.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.

William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. 3rd International Workshop on Paraphras-
ing (IWP2005).

Robert Gaizauskas, Jonathan Foster, Yorick Wilks,
John Arundel, Paul Clough, and Scott Piao. 2001.
The meter corpus: A corpus for analysing journalis-
tic text reuse. pages 214–223.

Jiawei Han and Micheline Kamber. 2006. Data Min-
ing: Concepts and Techniques. Number Edition,
Second. Morgan Kaufmann.

Vasileios Hatzivassiloglou and Judith L. Klavans.
2001. Simfinder: A flexible clustering tool for sum-
marization. In Proceedings of NAACL Workshop of
Automati Summarization, pages 203–212.

Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combi-
nations via machine learning. In Proceedings of the
1999 joint sigdat conference on empirical methods
in natural language processing and very large cor-
pora, pages 203–212.

I T Jolliffe. 1986. Principal component analysis.
Chemometrics and Intelligent Laboratory Systems,
2(1-3):37–52.

Stefan Kaufmann. 2000. Second-order cohesion.
Computational Intelligence, 16(4):511–524.

Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In ICML, pages 296–304.

Gordon Loberger and Kate Shoup. 2009. Websters
New World English Grammar Handbook. Wiley,
Hoboken.

Gerard Salton and Michael J. McGill. 1983. Introduc-
tion to Modern Informational Retrieval. McGraw-
Hill.

Gerard Salton, Anita Wong, and C S Yang. 1975. A
vector space model for automatic indexing. Com-
munications of the ACM, 18(11):613–620.

Prajol Shrestha. 2011a. Alignment of monolingual
corpus by reduction of the search space. In Proceed-
ings of the 18th Conference on the Traitement Au-
tomatique des Langues Naturelles, volume 1, pages
543–551.

Prajol Shrestha. 2011b. Corpus-based methods for
short text similarity. In Proceedings of the 15th Ren-
contre des Etudiants Chercheurs en Informatique
pour le Traitement automatique des Langues, vol-
ume 2, pages 297–302.

S K M Wong, W Ziarko, V V Raghavan, and P C N
Wong. 1987. On modeling of information retrieval
concepts in vector spaces. ACM Transactions on
Database Systems TODS, 12(2):299–321.

1461


