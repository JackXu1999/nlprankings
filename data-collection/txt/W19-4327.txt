



















































Best Practices for Learning Domain-Specific Cross-Lingual Embeddings


Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 230–234
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

230

Best Practices for Learning Domain-Specific Cross-Lingual Embeddings

Lena Shakurova1, 2, Beata Nyari1, Chao Li1, Mihai Rotaru1

1 Textkernel B.V., Amsterdam, Netherlands
2 Radboud University, Nijmegen, Netherlands

{shakurova,nyari,chaoli,rotaru}@textkernel.nl

Abstract

Cross-lingual embeddings aim to represent
words in multiple languages in a shared vector
space by capturing semantic similarities across
languages. They are a crucial component for
scaling tasks to multiple languages by trans-
ferring knowledge from languages with rich
resources to low-resource languages. A com-
mon approach to learning cross-lingual em-
beddings is to train monolingual embeddings
separately for each language and learn a lin-
ear projection from the monolingual spaces
into a shared space, where the mapping relies
on a small seed dictionary. While there are
high-quality generic seed dictionaries and pre-
trained cross-lingual embeddings available for
many language pairs, there is little research
on how they perform on specialised tasks. In
this paper, we investigate the best practices
for constructing the seed dictionary for a spe-
cific domain. We evaluate the embeddings on
the sequence labelling task of Curriculum Vi-
tae parsing and show that the size of a bilin-
gual dictionary, the frequency of the dictionary
words in the domain corpora and the source
of data (task-specific vs generic) influence the
performance. We also show that the less train-
ing data is available in the low-resource lan-
guage, the more the construction of the bilin-
gual dictionary matters, and demonstrate that
some of the choices are crucial in the zero-shot
transfer learning case.

1 Introduction

Expanding Natural Language Processing (NLP)
models to new languages typically involves cre-
ating completely new data sets for each language
which comes with challenges such as acquir-
ing and annotating the data. To avoid these te-
dious and costly tasks, one can use cross-lingual
embeddings to enable knowledge transfer from
languages with sufficient training data to low-
resource languages.

Cross-lingual embeddings aim to represent

words in multiple languages in a shared vector
space by capturing semantic similarities across
languages. Based on the assumption that the em-
bedding spaces of different languages exhibit a
similar structure (Mikolov et al., 2013), previ-
ous work proposed to learn a linear transforma-
tion which projects independently learned mono-
lingual spaces into a single shared space, using
a seed translation dictionary (Faruqui and Dyer,
2014). Although more advanced techniques in-
volving jointly optimising monolingual and cross-
lingual objectives were proposed, most of these
solutions require some form of cross-lingual su-
pervision via parallel data (Guo et al., 2015; Kle-
mentiev et al., 2012; Xiao and Guo, 2014; Her-
mann and Blunsom, 2014; Søgaard et al., 2015;
Vulic and Moens, 2015). However, for applica-
tions targeting a specific domain (in our case, hu-
man resources) there is often little to no parallel
data available, so simple alignment-based meth-
ods relying on only a small translation dictionary
remain an attractive choice.

We adopt the Multilingual CCA framework
(Ammar et al., 2016), and evaluate the cross-
lingual embedding on a sequence labelling task
in Curriculum Vitae parsing domain. We use this
framework as it only requires an easier to acquire
seed dictionary. Previous work has shown that
the quality of this dictionary influences the cross-
lingual embeddings (Vulić and Korhonen, 2016).
However, to the best of our knowledge, there has
been no extensive research on the choice of a seed
dictionary in a non-generic domain. In addition,
little attention was paid to how the quality of the
bilingual dictionary affects performance as some
labelled data from the target language is added.

In this paper, we investigate the best practices
to create a seed dictionary for training domain-
specific cross-lingual embeddings. We measure
the impact of different choices of the dictionary
creation on the downstream task: the dictionary



231

size, the source of the words and their frequency,
in both zero-shot and joint training scenarios.

2 Related work

Offline linear map induction methods The ear-
liest approach to induce a linear mapping from
the monolingual embedding spaces into a shared
space was introduced in (Mikolov et al., 2013).
They propose to learn the mapping by optimising
the least squares objective on the monolingual em-
bedding matrices corresponding to translational
equivalent pairs. Subsequent research aimed to
improve the mapping quality by optimising dif-
ferent objectives such as max-margin (Lazaridou
et al., 2015) and by introducing an orthogonal-
ity constraint to the bilingual map to enforce self-
consistency (Xing et al., 2015; Smith et al., 2017).
(Artetxe et al., 2016) provide a theoretical analy-
sis to existing approaches and in a follow-up re-
search (Artetxe et al., 2018) they propose to learn
principled bilingual mappings via a series of linear
transformations.

An extensive survey of different approaches, in-
cluding offline and online methods can be found in
(Ruder, 2017).

The role of bilingual dictionary A common
way to select a bilingual dictionary is by using
either automatic translations of frequent words or
word alignments. For instance, (Faruqui and Dyer,
2014) select the target word to which the source
word is most frequently aligned in parallel cor-
pora. (Mikolov et al., 2013) use the 5,000 most
frequent words from the source language with
their translations. To investigate the impact of
the dictionary on the embedding quality, (Vulić
and Korhonen, 2016) evaluate different factors and
conclude that carefully selecting highly reliable
symmetric translation pairs improves the perfor-
mance of benchmark word-translation tasks. The
authors also demonstrate that increasing the lexi-
con size over 10,000 pairs show a slow and steady
decrease in performance.

3 Task

In this work, we look at the Curriculum Vitae
(CV) parsing task: extraction of relevant informa-
tion (e.g. name, job titles, etc) from a given CV
and converting it into a structured format. This
task can be cast as a cascaded sequence labelling
problem (Yu et al., 2005) consisting of two steps:
section segmentation and extraction of pre-defined

entities, similar to named entity recognition task
(NER). In the first step, a model segments the en-
tire CV into sections such as personal informa-
tion, education, experience or skills. In the sec-
ond step, for each section, a dedicated model ex-
tracts entities specific to that section such as name,
phone number, etc. from personal section and de-
gree level, institution, etc. from education sec-
tion. For all models, we use the standard BIO
approach (Begin, Inside, Outside) to sequence la-
belling (Ramshaw and Marcus, 1995). For brevity,
in this paper, we present the results of extracting 2
entities from the experience section: job title and
organisation name.

4 Methodology

We conduct the experiments for German-English
and Dutch-English cross-lingual embeddings.
Given a bilingual seed dictionary, we use the
learned CCA linear projection (see Section
2) between the monolingual vector spaces to
project German/Dutch embeddings into the En-
glish space. The projected embeddings are then
fed into the sequence labelling model. The se-
quence labelling model is always trained in the
English space using either English training data
(zero-shot) or English training data combined with
projected German/Dutch training data. The model
is tested using projected German/Dutch embed-
dings and German/Dutch test data. We experi-
ment with several factors in the construction of the
bilingual dictionary: source of data, size, and the
frequency of the bilingual dictionary entries in the
domain corpus.

4.1 Training data

Monolingual embeddings For each language,
we train monolingual word2vec embeddings
(Mikolov et al., 2013) on normalised CV data. The
dimension of embeddings is 150, vocabulary size
is 169k, 503k and 286k for English, German and
Dutch respectively (minimum frequency 5).

Corpora In our experiments, we use English
as high resource language and German and Dutch
as low resource. The number of annotated doc-
uments is 4342 for English, 1947 for German and
2383 for Dutch. Having enough resources for Ger-
man/Dutch also allows us to study the impact of
increasing the amount of training data. Each doc-
ument contains on average 11 entities. We split
our data into train, development and test set with



232

proportions of 70, 15 and 15% accordingly.

4.2 Bilingual dictionary factors

Source of data (IDP vs MUSE vs domain): We
want to investigate the impact of constructing the
bilingual dictionary from domain-specific words
versus employing generic seed dictionaries: 1)
from Facebook’s MUSE project 1 2) from The
Internet Dictionary Project (IDP) 2. MUSE dic-
tionaries were specifically created for developing
cross-lingual embeddings (Lample et al., 2017),
whereas IDP dictionaries were produced for the
purpose of making royalty-free translating dictio-
naries accessible to the Internet community. For
the domain-specific dictionary, we picked top fre-
quent words (see below) from the source monolin-
gual corpus (German/Dutch) and translated the se-
lected words into English using Yandex Translate
API 3. Stop words were removed and the words
shorter than three characters were filtered out due
to their unreliable translation.

Frequency of bilingual dictionary entries
(high vs lower): We compared choosing most fre-
quent words to those selected from a lower fre-
quency range (between top 5-10%) in our domain-
specific corpus. It has been observed by previous
research that due to the fact that frequent terms are
over-represented in commonly used seed dictio-
naries, the performance of cross-lingual mappings
is much lower on rare words (Nakashole, 2018).
Motivated by this finding we wanted to analyse the
downstream effect of adding rarer terms to the dic-
tionary.

Size of bilingual dictionary (1k vs 5k vs 10k):
We compared seed dictionaries of different size:
1.000, 5.000 and 10.000. Understanding the im-
pact of this factor is important as larger dictionar-
ies are more expensive to create.

Validation: Previous research suggests using
back-translation as a verification step for a trans-
lation pair. We skipped this because we noticed
that certain words are crucial to be included in
the seed dictionary and despite their translation
being correct often they would be invalidated be-
cause of synonyms or suffixes (e.g. persönliche
→ personal → persönlich). Instead, we filter
words whose translations do not reach a frequency

1https://github.com/facebookresearch/
MUSE

2http://www.june29.com/IDP/
3https://pypi.org/project/

yandex-translater/

threshold in the English corpus, where this thresh-
old is tuned on a validation set.

4.3 Model Architecture

Our sequence labelling model is a stacked Bidirec-
tional LSTM with a CRF layer based on (Huang
et al., 2015) with a pre-trained embedding layer.
We used Adam optimiser and trained for 150
epochs. The network’s hyperparameters are tuned
on the English development set.

4.4 Evaluation metrics

As extrinsic evaluation metric of the cross-lingual
embeddings, we use the average F1 score across
the 2 entities we extract (job title and organisation
name). As intrinsic evaluation metric, we use the
precision at 1 (P@1) measured on the MUSE test
sets consisting of 1,500 translation pairs.

5 Results and discussion

Table 1 presents our results on how the 3 bilingual
dictionary factors influence the downstream task
performance and the precision@1 score. We start
with the best practices from previous work (top 5k
frequent words) and change one factor at a time
choosing the best performing setting when moving
to the next factor.

From the first set of rows, we see that using in-
domain seed words improves the task performance
over generic dictionaries. This effect is amplified
in the zero-shot transfer learning scenario. We also
see that using a bilingual dictionary (MUSE) em-
ployed by previous NLP research performs much
better than typical free online resource dictionary
(IDP). These observations are particularly impor-
tant in industry settings where it is a common prac-
tice to use free open-source resources. We also
see that the intrinsic metric (P@1) yields very low
scores and it is uncorrelated with the task metric
e.g. it ranks MUSE and IDP in the reverse order.
This highlights the importance of verifying cross-
lingual embeddings on the downstream task.

We also observe that choosing less frequent
seed words degrades the performance in the zero-
shot case. Qualitative analysis shows that includ-
ing certain high-frequency words can be crucial
for our task: these words are typically section
header words (e.g. Persönliche Angaben (Personal
Information)) or common context words of the en-
tities of interest (e.g. Erfahrung (experience)).
Since these words tend to occur in similar contexts

https://github.com/facebookresearch/MUSE
https://github.com/facebookresearch/MUSE
http://www.june29.com/IDP/
https://pypi.org/project/yandex-translater/
https://pypi.org/project/yandex-translater/


233

Factor combinations DE - EN NL-EN
Joint training Zero shot P@1 Joint training Zero shot P@1

IDP + 5k 79.5 61.5 1.1 - - -
MUSE + 5k 80.4 72.1 0.8 81.4 77.2 2.1
domain + 5k + high freq 81.1 75.8 1.7 81.5 79.1 2.5
domain + 5k + high freq 81.1 75.8 1.7 81.4 79.1 2.5
domain + 5k + lower freq 81.0 70.2 1.0 80.9 71.6 1.9
domain + 10k + high freq 81.5 76.8 1.2 81.6 79.3 2.8
domain + 5k + high freq 81.1 75.8 1.7 81.4 79.1 2.5
domain + 1k + high freq 80.1 72.2 1.2 79.3 77.8 1.6

Table 1: Average F1 and precision@1 score for bilingual dictionary experiments. Joint training uses 200 documents
from the low resource language.

Low resource data DE - EN NL - EN
Monolingual Cross-lingual gain Monolingual Cross-lingual gain

None (zero-shot) - +75.8 - +79.1
200 CVs 77.0 +4.1 75.1 +6.3
500 CVs 83.9 +0.2 80.9 +1.3
Full set 87.1 +0.0 83.5 +0.3

Table 2: Gain from knowledge transfer, averaged F1 score. Full set is 1363 for German and 1678 for Dutch.

as the entities, they tend to be confused with these
entities in the zero-shot setting if they are not in
the dictionary. Being common words, their mean-
ing is quickly picked up when jointly training with
some German/Dutch data.

In terms of vocabulary size, we notice that even
with a smaller 1k domain-specific dictionary we
tend to get a competitive performance. Using 5k
terms seems sufficient, although in line with (Vulić
and Korhonen, 2016) we observe that a larger vo-
cabulary (10k) gives only a slight improvement.

By analysing neighbourhoods of non-seed Ger-
man words projected in the English space, we no-
ticed that even though the nearest English neigh-
bours are related words (e.g. job title words),
often the distances are quite big. Our intuition
is that, specifically for sequence labelling tasks,
adding some training data from the low-resource
language allows the BLSTM model to the learn
about these nearby neighbourhoods and account
for the leeway created by imperfect cross-lingual
projections.

We investigate the impact of increasing the size
of low-resource language data in Table 2. For
these experiments, we use the best performing
seed dictionary (5k high-frequency words from
domain corpus). The results demonstrate that
with a strong English-only CV parsing model and

cross-lingual embeddings we achieve comparable
results to a model trained on only 15% of the low-
resource language. We also observe that the gain
of transfer learning diminishes as we jointly train
with an increasing amount of German data.

6 Conclusions and future work

In this paper, we investigate the best practices
for constructing a bilingual dictionary for learning
domain-specific cross-lingual embeddings. We
show that for our CV parsing task, the dictionary
should be created from top frequency domain-
specific words. A dictionary size of 5k tends to be
sufficient, with limited gains coming from adding
more words. We also show that the less training
data is available in the low-resource language, the
more these best practices matter.

In future work, we plan to extend our research to
cover other language pairs (e.g. Slavic languages)
or more distant pairs (e.g. English-Russian). We
also plan to look at cross-lingual subwords em-
beddings which become crucial for languages with
more complex morphology.

References
Waleed Ammar, George Mulcaire, Yulia Tsvetkov,

Guillaume Lample, Chris Dyer, and Noah A. Smith.



234

2016. Massively Multilingual Word Embeddings.
arXiv e-prints, page arXiv:1602.01925.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
pages 2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.
Generalizing and improving bilingual word embed-
ding mappings with a multi-step framework of lin-
ear transformations. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
pages 5012–5019.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 462–471. Association
for Computational Linguistics.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1234–1244, Beijing, China. Association for Com-
putational Linguistics.

Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual distributed representations without word
alignment. In ICLR 2014.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. Cite
arxiv:1508.01991.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING
2012, pages 1459–1474, Mumbai, India. The COL-
ING 2012 Organizing Committee.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora only.
arXiv preprint arXiv:1711.00043.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In ACL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013. Exploiting Similarities among Languages
for Machine Translation. arXiv e-prints, page
arXiv:1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. CoRR, abs/1310.4546.

Ndapa Nakashole. 2018. NORMA: Neighborhood sen-
sitive maps for multilingual word embeddings. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
512–522, Brussels, Belgium. Association for Com-
putational Linguistics.

Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
CoRR, cmp-lg/9505040.

Sebastian Ruder. 2017. A survey of cross-lingual em-
bedding models. CoRR, abs/1706.04902.

Samuel L. Smith, David H. P. Turban, Steven Hamblin,
and Nils Y. Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax.

Anders Søgaard, Željko Agić, Héctor Martı́nez Alonso,
Barbara Plank, Bernd Bohnet, and Anders Jo-
hannsen. 2015. Inverted indexing for cross-lingual
NLP. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1713–1722, Beijing, China. Association for
Computational Linguistics.

Ivan Vulić and Anna Korhonen. 2016. On the role
of seed lexicons in learning bilingual word embed-
dings. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 247–257, Berlin, Ger-
many. Association for Computational Linguistics.

Ivan Vulic and Marie-Francine Moens. 2015. Bilingual
distributed word representations from document-
aligned comparable data. CoRR, abs/1509.07308.

Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
parsing. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learning,
pages 119–129, Ann Arbor, Michigan. Association
for Computational Linguistics.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In HLT-NAACL.

Kun Yu, Gang Guan, and Ming Zhou. 2005. Resume
information extraction with cascaded hybrid model.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ’05,
pages 499–506, Stroudsburg, PA, USA. Association
for Computational Linguistics.

http://arxiv.org/abs/1602.01925
https://doi.org/10.18653/v1/D16-1250
https://doi.org/10.18653/v1/D16-1250
https://doi.org/10.3115/v1/E14-1049
https://doi.org/10.3115/v1/E14-1049
https://doi.org/10.3115/v1/E14-1049
https://doi.org/10.3115/v1/P15-1119
https://doi.org/10.3115/v1/P15-1119
http://arxiv.org/abs/1508.01991
http://arxiv.org/abs/1508.01991
https://www.aclweb.org/anthology/C12-1089
https://www.aclweb.org/anthology/C12-1089
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1309.4168
http://arxiv.org/abs/1309.4168
http://arxiv.org/abs/1310.4546
http://arxiv.org/abs/1310.4546
http://arxiv.org/abs/1310.4546
https://www.aclweb.org/anthology/D18-1047
https://www.aclweb.org/anthology/D18-1047
http://arxiv.org/abs/cmp-lg/9505040
http://arxiv.org/abs/1706.04902
http://arxiv.org/abs/1706.04902
http://arxiv.org/abs/1702.03859
http://arxiv.org/abs/1702.03859
http://arxiv.org/abs/1702.03859
https://doi.org/10.3115/v1/P15-1165
https://doi.org/10.3115/v1/P15-1165
https://doi.org/10.18653/v1/P16-1024
https://doi.org/10.18653/v1/P16-1024
https://doi.org/10.18653/v1/P16-1024
http://arxiv.org/abs/1509.07308
http://arxiv.org/abs/1509.07308
http://arxiv.org/abs/1509.07308
https://doi.org/10.3115/v1/W14-1613
https://doi.org/10.3115/v1/W14-1613
https://doi.org/10.3115/v1/W14-1613
https://doi.org/10.3115/1219840.1219902
https://doi.org/10.3115/1219840.1219902

