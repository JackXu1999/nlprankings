



















































ConSSED at SemEval-2019 Task 3: Configurable Semantic and Sentiment Emotion Detector


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 175–179
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

175

ConSSED at SemEval-2019 Task 3: Configurable Semantic and Sentiment
Emotion Detector

Rafał Poświata
National Information Processing Institute

al. Niepodległości 188b, 00-608 Warsaw, Poland
rafal.poswiata@opi.org.pl

Abstract

This paper describes our system participating
in the SemEval-2019 Task 3: EmoContext:
Contextual Emotion Detection in Text. The
goal was to for a given textual dialogue, i.e. a
user utterance along with two turns of context,
identify the emotion of user utterance as one
of the emotion classes: Happy, Sad, Angry or
Others. Our system: ConSSED is a configu-
rable combination of semantic and sentiment
neural models. The official task submission
achieved a micro-average F1 score of 75.31
which placed us 16th out of 165 participating
systems.

1 Introduction

Emotion detection is crucial in developing a
“smart” social (chit-chat) dialogue system (Chen
et al., 2018). Like many sentence classification
tasks, classifying emotions requires not only un-
derstanding of single sentence, but also capturing
contextual information from entire conversations.
For the competition we were invited to create a
system for emotion detection of user utterance
from short textual dialogue i.e. a user utterance
along with two turns of context (Chatterjee et al.,
2019b). The number of emotion classes has been
limited to four (Happy, Sad, Angry and Others).

The rest of the paper is organized as follows.
Section 2 briefly shows the related work. Section
3 elaborates on our approach. It shows preproces-
sing step and architecture of our system. Section 4
describes the data set, used word embeddings and
hyper-parameters, adopted research methodology
and experiments with results. Finally, Section 5
concludes our work.

2 Related Work

Detection of emotions in dialogues can be divided
into two types: based only on the text of the dialo-

gue (Chen et al., 2018) and based on many chan-
nels (video, speech, motion capture of a face, text
transcriptions) (Busso et al., 2008). Regardless of
the type, the most common solution is the use of
neural networks, in particular variations of Recur-
rent Neural Networks, such as LSTMs (Hochreiter
and Schmidhuber, 1997), BiLSTMs (Schuster and
Paliwal, 1997) and GRUs (Cho et al., 2014) or Co-
nvolutional Neural Networks (Krizhevsky et al.,
2012). Our solution uses LSTMs and BiLSTMs
and is based on the ideas from SS-BED system
(Chatterjee et al., 2019a).

3 Our Approach

Figure 1 provides an overview of our approach.
We wanted to create a system that would bene-
fit from the advantages of semantic and sentiment
embeddings (like SS-BED). At the same time,
it would be easily configurable both in terms of
the selection of parameters/network architecture
as well as the change of applied embeddings, both
static and dynamic. In the next subsections, we
describe in details our approach.

3.1 Preprocessing

For the preprocessing, we adjusted the ekphrasis
tool (Baziotis et al., 2017). We use this tool for
tokenization and to do the following:

• Normalize URLs, emails, per-
cent/money/time/date expressions and
phone numbers.

• Annotate emphasis and censored words and
phrases with all capitalized letters.

• Annotate and reduce elongated (e.g. Whaaaat
becomes <elongated> What) and repeated
words (e.g. !!!!!!!!! becomes <repeated> !).



176

Text

Preprocessing

Semantic

Embedding
LSTM / BiLSTM  LSTM / BiLSTM 

Angry

Text

Preprocessing

Sentiment

Embedding
LSTM / BiLSTM  LSTM / BiLSTM 

Others Class
Regularizer

Semantic Recurrent Network (SemRN) 

Sentiment Recurrent Network (SenRN) 

utterances

Softmax

Fully Connected 
Network 

Sad

Happy

Others

Figure 1: High level architecture of Configurable Semantic and Sentiment Emotion Detector (ConSSED).

• Unpack hashtags (e.g. #GameTime becomes
<hashtag> game time </hashtag>) and con-
tractions (e.g. “didn’t” becomes “did not”).

• Simplify emoticons e.g. :-] is changed to :).

We also prepare and apply dictionaries with com-
mon abbreviations and mistakes to reduce vocabu-
lary size and deal with Out of Vocabulary (OOV)
issue.

3.2 Model
Our model contains four parts: Semantic Recur-
rent Network (SemRN), Sentiment Recurrent Ne-
twork (SenRN), Fully Connected Network and
Others Class Regularizer. SemRN and SenRN are
independent of each other and have similar archi-
tecture: Text Preprocessing, suitable Word Em-
bedding and 2-layer LSTM or bidirectional LSTM
(BiLSTM) - which is configurable. Outputs of
those two modules are concatenated and become
input for Fully Connected Network. This network
has one hidden layer and Softmax layer which re-
presents probabilities of classes. The last element
of our model is Others Class Regularizer (used
only during the prediction on validation/test set).

3.3 Others Class Regularizer
This component was created due to the fact that
a real-life distribution is about 4% for each of
Happy, Sad and Angry class and the rest is Others
class. This component works by grouping records
into three sets, depending on whether they are pre-
dicted as Happy, Sad or Angry. Next, for all of

these sets, it checks if there are more representa-
tives than the assumed percentage of all records.
If yes, it increases the probability for Others class
by 0.01 (independently in each set) until it reaches
the number of representatives lower than the as-
sumed percentage. The assumed percentage value
was defined as 5.5% taking into account the vali-
dation set.

4 Experiments and Results

4.1 Data

In our work on the system, we used only official
data sets made available by the organizers. Ho-
wever, we noticed that there are cases when co-
nversations occur twice, but with different labels.
We have removed these records and received sets
which are shown in Table 1.

Number of records
train 29977
validation 2755
test 5509

Table 1: Data sets statistics.

4.2 Word Embeddings

For our experiments, we chose five word embed-
dings: three semantic and two sentiment. Se-
mantic embeddings are GloVe (Pennington et al.,
2014) trained on Twitter data1, Word2Vec (Mi-

1https://nlp.stanford.edu/projects/
glove/

https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


177

Hyper-parameter name Possible values
SEM LSTM DIM [200, 230, 256, 280, 300, 320]
SEM FIRST BIDIRECTIONAL [False, True]
SEM SECOND BIDIRECTIONAL [False, True]
SEN LSTM DIM [200, 230, 256, 280, 300, 320]
SEN FIRST BIDIRECTIONAL [False, True]
SEN SECOND BIDIRECTIONAL [False, True]
HIDDEN DIM [100, 128, 150]
LSTM DIM [200, 230, 256, 280, 300, 320]
BATCH SIZE [32, 64, 80, 100, 128]
DROPOUT (0.1, 0.5)
RECURRENT DROPOUT (0.1, 0.5)
LEARNING RATE (0.001, 0.004)
OTHERS CLASS WEIGHT (1.0, 3.0)

Table 2: The names of hyper-parameters with possible values.

kolov et al., 2013) with ten affective dimensions
trained by NTUA-SLP team as part of their solu-
tion for SemEval2018 (Baziotis et al., 2018)2 (we
call it NTUA 310) and ELMo (Peters et al., 2018)
trained on 1 Billion Word Benchmark3. As sen-
timent embeddings we chose Sentiment-Specific
Word Embedding (SSWE) (Tang et al., 2014)4 and
Emo2Vec (Xu et al., 2018)5.

4.3 Hyper-parameters Search
In order to tune the hyper-parameters of our
model, we adopt a Bayesian optimization by
using Hyperopt library6. The names of hyper-
parameters with possible values (list or range)
are shown in Table 2. Parameters with SEM
prefix apply to the Semantic Recurrent Network,
and with SEN prefix to the Sentiment Recur-
rent Network. LSTM DIM parameter is for BiL-
STM baseline systems. In order to cope with the
differences in the distribution of classes in the
training set and the validation and test sets, as
well as the previously mentioned actual distribu-
tion of emotion classes in relation to the Others
class, apart from the use of Others Class Regula-
rizer we also used class weight for Others class
(OTHERS CLASS WEIGHT parameter).

4.4 Methodology
We train all models using the training set and tune
the hyper-parameters using the validation set. Due
to the time frame of the competition, we limited
the search of hyper-parameters to 10 iterations for

2https://github.com/cbaziotis/
ntua-slp-semeval2018

3https://tfhub.dev/google/elmo/2
4http://ir.hit.edu.cn/˜dytang/
5https://github.com/pxuab/emo2vec_

wassa_paper
6https://hyperopt.github.io/hyperopt/

each model. Then, for the best parameters (found
in a limited number of iterations), we once again
learned this model with a training and validation
set. The final model validation took place on the
test set. During all experiments, we used the pre-
processing described in section 3.1.

4.5 Experiments

The results of our experiments are shown in Table
3. We have divided them into two stages: valida-
tion of the baseline systems and our solution.

For the first stage, we used the 2-layer bidirec-
tional LSTM model (BiLSTM) with all the word
embedding presented in section 4.2 and compa-
red this approach to the baseline model prepared
by the organizers (Baseline). The model using
NTUA 310 embedding (73.34) performed best,
compared to the Baseline, we have an improve-
ment of about fifteen percent. The second best
model was a solution using ELMo embedding
(72.42). From sentiment embeddings the best was
Emo2Vec (71.18).

The second stage was focused on the validation
of the ConSSED model. In this experiment, we
trained six models to verify all possible pairs of
semantic embedding-sentiment embedding. The
results show that the use of the ConSSED model
allows better results than corresponding baseline
systems. As we could have guessed from the first
stage, the best was a combination of NTUA 310
and Emo2Vec (75.31), which was our official so-
lution during the competition. In parentheses, we
presented the results without the use of Others
Class Regularizer. As we can see, the use of this
component improves the results but only slightly.
In addition, after the competition, we have rerun
the search for hyper-parameters (this time incre-
asing the number of iterations) for the ConSSED-

https://github.com/cbaziotis/ntua-slp-semeval2018
https://github.com/cbaziotis/ntua-slp-semeval2018
https://tfhub.dev/google/elmo/2
http://ir.hit.edu.cn/~dytang/
https://github.com/pxuab/emo2vec_wassa_paper
https://github.com/pxuab/emo2vec_wassa_paper
https://hyperopt.github.io/hyperopt/


178

Happy F1 Sad F1 Angry F1 Avg. F1
Baseline 54.61 61.49 59.45 58.61
BiLSTM-GloVe 59.62 67.16 73.64 67.39
BiLSTM-ELMo 67.99 74.69 74.35 72.42
BiLSTM-NTUA 310 70.29 77.21 73.07 73.34
BiLSTM-SSWE 66.34 71.54 69.07 68.86
BiLSTM-Emo2Vec 69.48 73.27 70.93 71.18
ConSSED-GloVe-SSWE 68.48 (67.86) 74.91 (69.69) 76.54 (74.00) 73.30 (70.62)
ConSSED-GloVe-Emo2Vec 68.46 (68.46) 77.51 (77.51) 73.21 (71.39) 72.90 (72.18)
ConSSED-ELMo-SSWE 69.27 (69.16) 79.30 (79.30) 74.88 (73.32) 74.27 (73.60)
ConSSED-ELMo-Emo2Vec 71.30 (71.30) 76.05 (76.05) 76.67 (76.50) 74.69 (74.68)
ConSSED-NTUA 310-SSWE 70.69 (70.69) 78.13 (78.13) 75.54 (74.92) 74.66 (74.45)
ConSSED-NTUA 310-Emo2Vec 69.69 (69.69) 78.39 (78.39) 77.67 (76.95) 75.31 (75.10)
*ConSSED-NTUA 310-Emo2Vec 72.66 (72.66) 79.60 (79.60) 77.80 (76.83) 76.64 (76.31)

Table 3: Results of our experiments on the test set. The values without the use of Others Class Regularizer are
shown in parentheses. Bolded model indicate our official solution in the competition. Experiment with an asterisk
was carried out after the end of the competition.

Competition Model Best Model
Avg. F1 75.31 76.64
SEM LSTM DIM 320 320
SEM FIRST BIDIRECTIONAL True True
SEM SECOND BIDIRECTIONAL False False
SEN LSTM DIM 256 280
SEN FIRST BIDIRECTIONAL True True
SEN SECOND BIDIRECTIONAL True True
HIDDEN DIM 150 150
BATCH SIZE 100 100
DROPOUT 0.30328 0.34468
RECURRENT DROPOUT 0.31007 0.29362
LEARNING RATE 0.00338 0.00333
OTHERS CLASS WEIGHT 2.41235 2.63698

Table 4: Comparison between two ConSSED-NTUA 310-Emo2Vec models: official Competition Model and
Best Model trained after the end of the competition.

NTUA 310-Emo2Vec model, which give us a
better result than our official competition result
(76.64). Hyper-parameters found for ConSSED-
NTUA 310-Emo2Vec models and differences be-
tween them are shown in Table 4.

4.6 Competition Results
The best result we have obtained on official le-
aderboard is equal to 75.31 according to micro-
averaged F1 score. Our solution is ranked 16th
out of 165 participating systems.

5 Conclusion

In this paper, we present Configurable Semantic
and Sentiment Emotion Detector (ConSSED) - our
system participating in the SemEval-2019 Task 3.
ConSSED has achieved good results, and subse-
quent studies show that it can achieve even bet-
ter which results from a further search for hyper-
parameters. We think that the use of fine-tuned
ELMo model (e.g. by Twitter data) would improve
the result even more. In addition, we would like

to integrate our system with the BERT embedding
(Devlin et al., 2018).

For developing our system we used Keras7

with TensorFlow8 as backend. We make our
source code available at https://github.
com/rafalposwiata/conssed.

References
Christos Baziotis, Athanasiou Nikolaos, Alexandra

Chronopoulou, Athanasia Kolovou, Georgios Pa-
raskevopoulos, Nikolaos Ellinas, Shrikanth Naray-
anan, and Alexandros Potamianos. 2018. NTUA-
SLP at semeval-2018 task 1: Predicting affec-
tive content in tweets with deep attentive rnns and
transfer learning. In Proceedings of The 12th
International Workshop on Semantic Evaluation,
SemEval@NAACL-HLT, New Orleans, Louisiana,
June 5-6, 2018, pages 245–255.

Christos Baziotis, Nikos Pelekis, and Christos Doul-
keridis. 2017. Datastories at semeval-2017 task
7https://keras.io/
8https://www.tensorflow.org/

https://github.com/rafalposwiata/conssed
https://github.com/rafalposwiata/conssed
https://aclanthology.info/papers/S18-1037/s18-1037
https://aclanthology.info/papers/S18-1037/s18-1037
https://aclanthology.info/papers/S18-1037/s18-1037
https://aclanthology.info/papers/S18-1037/s18-1037
https://keras.io/
https://www.tensorflow.org/


179

4: Deep lstm with attention for message-level and
topic-based sentiment analysis. In Proceedings of
the 11th International Workshop on Semantic Evalu-
ation (SemEval-2017), pages 747–754, Vancouver,
Canada. Association for Computational Linguistics.

Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Ka-
zemzadeh, Emily Mower Provost, Samuel Kim, Je-
annette Chang, Sungbok Lee, and Shrikanth Naray-
anan. 2008. Iemocap: Interactive emotional dyadic
motion capture database. Language Resources and
Evaluation, 42:335–359.

Ankush Chatterjee, Umang Gupta, Manoj Kumar
Chinnakotla, Radhakrishnan Srikanth, Michel Gal-
ley, and Puneet Agrawal. 2019a. Understanding
emotions in text using deep learning and big data.
Computers in Human Behavior, 93:309–317.

Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019b. Semeval-2019
task 3: Emocontext: Contextual emotion detection
in text. In Proceedings of The 13th International
Workshop on Semantic Evaluation (SemEval-2019),
Minneapolis, Minnesota.

Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,
Ting-Hao K. Huang, and Lun-Wei Ku. 2018. Emo-
tionlines: An emotion corpus of multi-party conver-
sations. CoRR, abs/1802.08379.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR, abs/1406.1078.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kri-
stina Toutanova. 2018. Bert: Pre-training of deep bi-
directional transformers for language understanding.
arXiv preprint arXiv:1810.04805.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep co-
nvolutional neural networks. In Proceedings of the
25th International Conference on Neural Informa-
tion Processing Systems - Volume 1, NIPS’12, pages
1097–1105, USA. Curran Associates Inc.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed represen-
tations of words and phrases and their compositio-
nality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Na-
tural Language Processing (EMNLP), pages 1532–
1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word re-
presentations. In Proc. of NAACL.

Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Trans. Si-
gnal Processing, 45:2673–2681.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Me-
eting of the Association for Computational Lingu-
istics (Volume 1: Long Papers), pages 1555–1565,
Baltimore, Maryland. Association for Computatio-
nal Linguistics.

Peng Xu, Andrea Madotto, Chien-Sheng Wu, Ji Ho
Park, and Pascale Fung. 2018. Emo2vec: Lear-
ning generalized emotion representation by multi-
task training. In Proceedings of the 9th Workshop
on Computational Approaches to Subjectivity, Sen-
timent and Social Media Analysis, pages 292–298.
Association for Computational Linguistics.

https://doi.org/10.1007/s10579-008-9076-6
https://doi.org/10.1007/s10579-008-9076-6
http://arxiv.org/abs/1802.08379
http://arxiv.org/abs/1802.08379
http://arxiv.org/abs/1802.08379
http://arxiv.org/abs/1406.1078
http://arxiv.org/abs/1406.1078
http://arxiv.org/abs/1406.1078
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1162/neco.1997.9.8.1735
http://dl.acm.org/citation.cfm?id=2999134.2999257
http://dl.acm.org/citation.cfm?id=2999134.2999257
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/P14-1146
http://www.aclweb.org/anthology/P14-1146
http://www.aclweb.org/anthology/P14-1146
http://aclweb.org/anthology/W18-6243
http://aclweb.org/anthology/W18-6243
http://aclweb.org/anthology/W18-6243

