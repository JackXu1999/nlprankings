



















































Transforming Dependency Structures to LTAG Derivation Trees


Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 112–121,
Umeå, Sweden, September 4–6, 2017. c© 2017 Association for Computational Linguistics

Transforming Dependency Structures to LTAG Derivation Trees

Caio Corro Joseph Le Roux
Laboratoire d’Informatique de Paris Nord,

Université Paris 13 – SPC, CNRS UMR 7030,
F-93430, Villetaneuse, France
{corro,leroux}@lipn.fr

Abstract

We propose a new algorithm for pars-
ing Lexicalized Tree Adjoining Grammars
(LTAGs) which uses pre-assigned bilexi-
cal dependency relations as a filter. That
is, given a sentence and its correspond-
ing well-formed dependency structure, the
parser assigns elementary trees to words
of the sentence and return attachment sites
compatible with these elementary trees
and predefined dependencies. Moreover,
we prove that this algorithm has a linear-
time complexity in the input length. This
algorithm returns all compatible derivation
trees as a packed forest. This result is of
practical interest to the development of ef-
ficient weighted LTAG parsers based on
derivation tree decoding.

1 Introduction

Lexicalized Tree Adjoining Grammars (LTAGs),
that is TAGs where each elementary tree contains
exactly one lexical anchor, have been proposed
as an attractive formalism to model the phrase-
structure construction in natural languages (Sch-
abes et al., 1988; Abeille et al., 1990). An im-
portant property of lexicalized grammars is their
ability to directly encode semantic information in
combination operations. Borrowing the example
provided by Eisner and Satta (2000), in the sen-
tence “She deliberately walks the dog”, the
tree anchored with “dog” is combined to the tree
anchored with “walks”, see Figure 1a.1 Thus,
the object associated with a transitive realization
“walks” can be restricted to a subset of allowed
words, including “dog” but not “river”.

Unfortunately, parsing with a LTAG is hardly
tractable. Eisner and Satta (2000) proposed the

1We use a simplified grammar to ease the presentation.

best-known parsing strategy with a O(n7) worst
case time complexity and O(n5) space complex-
ity where n is the length of the input sentence.
This is a major drawback for downstream applica-
tions where speed and low memory use is impor-
tant. Moreover, by reducing boolean matrix multi-
plication to TAG parsing, Satta (1994) argued that
obtaining a lower complexity bound for the latter
problem is unlikely to be straightforward. Hence,
parsing with weighted TAGs (Resnik, 1992) has
received too little attention even though Chiang
(2000) experimentally demonstrated their useful-
ness in the Penn Treebank parsing task. In order to
bypass this major bottleneck, two main strategies
have been explored. On the one hand, splittable
grammars (Schabes and C. Waters, 1995; Carreras
et al., 2008) are interesting because they have a
lower asymptotic complexity than LTAGs. How-
ever, they cannot directly encode several proper-
ties that make TAGs linguistically plausible, such
as cross-serial dependencies. In fact, they are re-
stricted to context-free languages. On the other
hand, a popular approach to speed up LTAG pars-
ing is to include a preliminary step called su-
pertagging: only a subset of tree fragments per
word are retained as candidates, or exactly one in
the most aggressive form (Chen and Bangalore,
1999). However, this pruning does not improve
the asymptotic complexity of the parser.2 More-
over, it is usually performed via local methods that
can hardly take into account long distance rela-
tionships since lexical dependencies are unknown
at this step. For instance, in the sentence “She
deliberately walks, despite her hatred for

quadruped mammals, the dog”, capturing the
transitive nature of the first verb is difficult without
further analysis (Bonfante et al., 2009).

2Intuitively, even when lexicon assigns only one elemen-
tary tree per word, a sentence can have several derivations
which exhibit different tree structures.

112



walks

VBZ

VP

S

NP

NP
VP

VP*ADVP

RB

deliberately

NP

PRP

She NP

NN

dog

DET

DET

the

(a) Sentence construction with a LTAG

She

PRP

NP

deliberately

RB

ADVP

walks

VBZ

VP

VP

S

the

DET

dog

NN

NP

(b) Resulting phrase-structure

v1
τ1 v2

τ2

v3
τ3

v4
τ4

v5
τ5

1.1
1.2

1.2.2

1.1

She deliberately walks the dog

(c) Associated derivation tree

Figure 1: Phrase-structure of the sentence “She deliberately walks the dog”, its construction thanks
to a LTAG and the associated derivation tree. Dashed arrows in the left figure represent combination
operations. Note that the derivation tree is a a labelled dependency structure.

In this work, we explore a component for a new
alternative in fast and efficient LTAG parsing. De-
pendency structures can be interpreted as sets of
derivation trees which share lexical composition
operations. This property has already been investi-
gated in order to propose efficient phrase-structure
parsers under several formalisms (Section 2). The
derivation tree induced by an LTAG analysis, as
shown in Figure 1c, is a dependency structure ex-
posing bilexical dependencies and labelled with
elementary trees and operation sites (Section 3).
As such, LTAG parsing can be seen as dependency
parsing where both valid tree structures and valid
labellings are constrained. Following the pipeline
used in most labeled dependency parsers:

1. a parser starts by assigning a single head to
each lexical item, without taking into account
the grammar;

2. then, a parse labeler assigns elementary trees
and operation sites.

Regarding the first step, seminal work of
Bodirsky et al. (2005) showed that, ignoring la-
bels, the derivation tree is an arborescence3 that
can be characterized thanks to two structural
properties: 2-bounded block degree and well-
nestedness. Gómez-Rodrı́guez et al. (2009) intro-
duced an algorithm with a O(n7) time complex-
ity for decoding this type of structures. Still, this
is a dependency parsing problem hardly tractable
for sentences longer than 15 ∼ 20 words. More
recently, Corro et al. (2016) proposed an exper-
imentally fast alternative based on combinatorial

3In the rest of the paper, we use tree to denote the lin-
guistic object and arborescence to denote the graph-theoretic
object. The term tree may be confusing because in graph the-
ory it refers to an undirected type of graph.

optimization and the maximum spanning arbores-
cence problem. Either way, this means that it is
now possible to obtain dependency trees that are
compatible with LTAG parsing.

Still, we are left with the second step: the parse
labeler. Our contribution is a novel algorithm for
this second step that can infer elementary trees
and operation sites (Sections 4 and 5) as a post-
processing step to build all compatible derivation
trees, from which derived trees are completely
specified. The time complexity in the length of
the sentence is linear (Section 6).

2 Related work

Syntactic content must not be confused with the
type of representation, as clearly argued by Ram-
bow (2010). As such, phrase structures can be en-
coded into dependency-like structures as long as
the transformation is correctly formalized. One
example of this fact is the correspondence be-
tween derivation trees constructed from lexical-
ized grammar and bilexical dependency relations.
However, the equivalence with dependency-based
linguistic theories may not be as straightforward
as the similarity in the type of representation sug-
gests (Rambow and Joshi, 1997; Kallmeyer and
Kuhlmann, 2012). Following this line of thought,
we reduce LTAG parsing to dependency parsing
where unannotated bilexical dependencies repre-
sents abstract attachment operations and depen-
dency labels specify the type of attachment and the
site of the operation on the head elementary tree.

Historically, many syntactic dependency tree-
banks have been built by transforming phrase-
structures using head-percolation tables (Collins,
2003; Yamada and Matsumoto, 2003), and thus
one could learn a dependency parser from a

113



phrase-structure treebank. More Recently, fol-
lowing this observation, Fernández-González and
Martins (2015) proposed to encode constituents as
bilexical dependency labels without relying on a
grammar, solving the two problems jointly. In this
setting, constituent parse tree is recovered from a
labeled dependency parse, which makes it possi-
ble to use tools developed for dependency pars-
ing. This is appealing because dependency parsing
has received a considerable amount of attention
recently and is now well understood. Moreover,
dependency parsing is usually less complex than
phrase-structure parsing, at least in practice if not
asymptotically. This technique has been showed
to be experimentally efficient but it may be argued
that the lack of an explicit grammar may result in
non-interpretable structures. For instance, the va-
lency of a verbal predicate may be incorrect.

Another line of work used predicted bilexical
dependencies as a filter for a standard grammar
parser. Recently, Kong et al. (2015) applied this
technique to Context-Free Grammars with lexi-
cal projections, ie. head-projections are specified
in right-hand sides of production rules (Collins,
2003). The resulting algorithm is certified to have
quadratic worst time-complexity but, in practice,
they observed a linear running-time with respect to
the sentence length. Our method is very close but
with one major difference. In an LTAG derivation
each word is assigned a single elementary tree,
while in CFGs with lexical projections a word can
anchor several rules. This observation provides us
with way to improve the worst-case time complex-
ity of our algorithm from cubic to linear with re-
gards to the input length. We believe that this ob-
servation could also be adapted to the framework
of Kong et al. (2015). Also, we can interpret the
low-order dependency parsing model explored by
Carreras et al. (2008) as a similar filtering proce-
dure. However, they did not study the theoreti-
cal complexity benefit of their approach. Note that
both of these works only handle context-free lan-
guages whereas we focus on a mildly-context sen-
sitive formalism.

Guided parsing has been studied for TAGs and
related formalisms like RCGs (Barthélemy et al.,
2001). Our approach differs because the prede-
fined dependency tree cannot be considered as a
guide, nor an oracle, since it is not required to be
a superset of the set of possible derivations. Also,
we could see our method as an algorithm to com-

pute the intersection of the tree language defined
by an LTAG and the tree language consisting of a
single dependency tree. In this way our method
can be seen as an instance of the framework de-
fined in (Nederhof, 2009).

Finally, we follow the common idea that deriva-
tion trees should be built directly and that a derived
tree is a by-product of a derivation tree. See for in-
stance Debusmann et al. (2004) which introduced
LTAG parsing as a constraint satisfaction problem.

3 Lexicalized Tree Adjoining Grammar

In this section, we describe notions related to
LTAG parsing that we will use to expose the pars-
ing algorithm in Section 4. We assume the reader
is familiar with the TAG formalism (Joshi, 1987;
Joshi and Schabes, 1992). We start by defining
TAGs, then we introduce the structure of deriva-
tion trees and finally we give an overview of pars-
ing with LTAGs.

3.1 Definition
We define a LTAG as a tuple
〈N,T,ΓI ,ΓA,ΓS , fOA, fSA, fSS〉 where:
• N and T are disjoint finite sets of non-

terminals (constituents) and terminals
(words), respectively;

• ΓI and ΓA are the finite sets of initial and
auxiliary trees, respectively, built uponN and
T and we define the set of elementary trees as
Γ , ΓI ∪ ΓA;
• ΓS ⊆ ΓI is the set of start trees.
We use Gorn addresses4 to index nodes and p ∈

τ is true if and only if address p exists in tree τ . We
use the term site to refer to both the Gorn address
and the corresponding node.

Each node of the elementary tree τ ∈ Γ is la-
belled with a non-terminal except exactly one leaf
labelled with the terminal whose address is de-
noted lex (τ), called the lexical anchor. For any
auxiliary tree τ ∈ ΓA, the address of its manda-
tory foot node is written foot(τ). Without loss of
generality, we restrict Γ to binary trees only.

In order to control combination operations over
nodes in Γ, we use the following functions:

• fOA : Γ×Z+ → B specifies whether adjunc-
tion is obligatory at a site in a tree or not;5

4A Gorn address for a node is a sequence of integers from
Z+ indicating a path from the root to the addressed node.

5B is the set containing boolean values true and false.

114



• fSA : Γ × Z+ × ΓA → B specifies the trees
that can be adjoined at a site;

• fSS : Γ×Z+×ΓI → B is a similar function
for substitution.

To simply notation, we also use function fSA
and fSS in order to check if a site is an adjunction
or substitution site, and thus assume these func-
tions check if the given site is a leaf or not.

The derived tree is built by combining elemen-
tary trees thanks to combination operations: sub-
stitution and adjunction. In this paper, we have
to distinguish three different types of adjunction.
A wrapping adjunction is the attachment of a sub-
analysis which has lexical anchors on both sides of
the adjoined auxiliary tree’s foot node. Similarly,
left adjunction and right adjunction restrict lexi-
cal anchors to the left and right of the foot node,
respectively. 6

3.2 Derivation tree
Given a sentence s = s1 . . . sn and a LTAG, a
derivation tree is a dependency structure repre-
senting a valid parse of the sentence by means of
elementary trees and combination operations. It is
a directed graphG = (V,A) with V = {v1 . . . vn}
the set of vertices, vi corresponding to word si.
The set of arcs A ⊂ V × V describes a spanning
arborescence: A contains n−1 arcs with no circuit
and each vertex has at most one incoming arc. The
unique vertex vr without incoming arc is called the
root vertex. The set of children of a vertex is de-
fined as children(vh) = {vm|vh → vm ∈ A}.
Moreover, vertices are labelled with elementary
trees and arcs with Gorn addresses. Vertex la-
bels indicate supertag assignment, while arc la-
bels specify adjunction or substitution sites. The
derivation tree contains all necessary information
to construct the derived tree. See Figure 1b and
1c for an example. Different derivation trees can
induce the same derived tree.

We borrow notation and definitions from Corro
et al. (2016). The predecessor of a vertex vi ∈ V
is vi−1. We refer to the vertex with the smaller
(resp. larger) index in a set as the leftmost (resp.
rightmost) vertex. The yield of a vertex vk ∈ V is
the set of vertices reachable from vk with respect
to A, including itself. We note (vk)⇐ and (vk)⇒
the leftmost and rightmost vertices in the yield of

6These should not be confused with wrapping, left and
right auxiliary trees which only describe the structure of aux-
iliary trees.

v1

v2

v3

v4

v5

v6

Why, he asks, does she walk ?

Notation Value
(v4)⇐ v1
(v4)⇒ v6
(v4)← v2
(v4)→ v3
(v4)↑ v3

Figure 2: Example of a 2-bounded block degree
and well-nested dependency structure. The right
table exposes notation we use for information we
can extract about vertex v4.

vk respectively. The span of vk corresponds to the
set of vertices delimited by (vk)⇐ and (vk)⇒, pos-
sibly including vertices which are not in the yield
of vk. The block degree of a vertex set W ⊆ V is
the number of vertices of W without a predeces-
sor inside W . The block degree of a vertex is the
block degree of its yield and the block degree of
an arborescence is the maximum block degree of
its incident vertices.

Traditionally, dependency structures have been
characterized as either projective (all vertices have
a yield equal to their span) or non-projective (any
arborescence). Previous work of Bodirsky et al.
(2005) showed that the structure of G is highly
constrained and can be characterized thanks to two
finer-grained structural properties. First, G has a
2-bounded block degree, meaning the block de-
gree of G is less or equal to two. Given a vertex
vk ∈ V with a block degree of 2, its gap is the ver-
tex set of block degree 1 including a vertex with
its predecessor and one with its successor in the
yield of vk. We note (vk)← and (vk)→ the leftmost
and rightmost nodes in the gap of vk, respectively,
(vk)← = (vk)→ = − if there is none (ie. vk as a
block degree of 1, thus its yield is equal to its span
and contains no gap). An example is illustrated
in Figure 2. Second, G is well-nested, that is two
distinct sub-trees may not interleave. We will not
explicitly use this property in the following and re-
fer readers interested dependency structures char-
acterization to Kuhlmann and Nivre (2006). How-
ever, we assume dependency structures to be well-
nested.7

3.3 Parsing
So far we defined LTAGs and discussed the struc-
ture of derivation trees. We will now briefly focus

7 More precisely, the algorithm proposed in Section 4 can-
not parse ill-nested arborescences. Well-nestedness is a re-
quired property of the input.

115



on the parsing problem.
Given a sentence, this term may typically re-

fer to three different but strongly related tasks via
the notion of semi-ring parsing (Goodman, 1999).
First, recognition: can this sentence be generated
by a given grammar? Second, derivation forest
parsing: decoding the set of all possible derivation
trees. Third, weighted disambiguation: what is the
best parse in the derivation forest given a scoring
model? Our work was motivated by the weighted
framework, but can generalize to others with some
limitation.

Thus, a parser takes as input a grammar and a
sentence. Because the LTAG chart-based parser
has a high complexity, a standard approach is to
use a pipeline system. First, a labeler assigns a
single elementary tree to each lexical item. Then,
the chart-based parser is run. This is merely a
beam approach but it does not impact the assymp-
totic complexity of the second step. We propose
to reverse this standard pipeline. In the first stage
of the pipeline, which we consider already per-
formed, a dependency parser assigns bilexical re-
lations that we interpret as an abstract, ie. unla-
belled, derivation tree. Contrary to the standard
pipeline, this first step do not take into account the
grammar, which lead to the development of an ef-
ficient parser based on combinatorial optimization
(Corro et al., 2016). Next, the linear-time algo-
rithm exposed in Section 4 can be used to assign
elementary trees and operation sites.

4 Parsing with given bilexical relations

Not every labelled graph G describing an arbores-
cence is a valid dependency tree. Indeed, many
constraints must be satisfied in order to transform
the derivation tree to into a derived tree. Since an
arc vh → vm represents a combination operation
of the tree of vm into the tree of vh, non-terminals
at attachment sites must be equal and if the desti-
nation site is a leaf (resp. internal) node, the tree of
vm must be an initial (resp. auxiliary) tree, among
others.

In this section, we propose a new algorithm for
LTAG parsing with given bilexical relations in the
form of a deduction system (Pereira and Warren,
1983). We formalize the algorithm as a recog-
nizer: the Goal item can only be obtained if the
input can be generated by the grammar. As usual,
it can be implemented as a chart-based dynamic
program with back-pointers in order to retrieve

every allowed derivation tree. Moreover, rules
can be augmented with scores to build a weighted
parser (Goodman, 1999).

Intuitively, our algorithm is an adaptation of
the standard CYK variant for TAG parsing (Vijay-
Shankar and K. Joshi, 1986; Vijay-Shanker and
J. Weir, 1993), extended to handle constraints of
lexicalized grammars. It is bottom-up in two ways.
First, vertices of the dependency structure are tra-
versed from leaves to root: a vertex is considered
only after all its children. Second, given a vertex
of the dependency structure, its corresponding el-
ementary tree is visited in a similar fashion as in
the CYK-like algorithm. The resulting algorithm
has a linear time complexity. This is due to the
fact that the maximum number of operations on a
given elementary tree is bounded by its size. Thus,
given a word, its maximum number of modifiers
is not bounded by the sentence length but by the
grammar, which leads to a tighter asymptotic com-
plexity.

4.1 Item definition
Given a LTAG 〈N,T,ΓI ,ΓA,ΓS , fOA, fSA, fSS〉,
a sentence s = s1 . . . sn and the corresponding
dependency structure G = (V,A), items are 6-
tuples of the form [vh, τ, p, c, bl, br] with:

1. considered vertex vh ∈ V in the dependency
structure, corresponding to word sh;

2. elementary tree τ ∈ Γ, indicating the associ-
ation of the anchor of τ with word sh;

3. Gorn address p ∈ τ of a node in the elemen-
tary tree;

4. combination flag c ∈ {⊥,>} specifying if a
combination operation has already been in-
vestigated > or not ⊥ at node p;

5. left boundary bl ∈ V ∪{lh, gh,
←
gh,
→
gh} defines

the left boundary of the yield of the item,
which is discussed in more details below;

6. right boundary br ∈ V ∪ {lh, gh,
←
gh,
→
gh} de-

fines its right boundary.

In most approaches, boundaries of the yield are
given using integer indices. Instead, we use either
vertices V or special values lh, gh,

←
gh and

→
gh.8

8 In the literature, the yield is often defined as a pair 〈i, j〉
meaning words from si+1 to sj are covered. We do not fol-
low this convention: a yield 〈i, j〉 indicates that words from
si to sj are covered.

116



If the left boundary of an item is node vm ∈ V ,
the left (resp. right) boundary position is given by
(vm)⇐ (resp. (vm)⇒). Special value lh is used to
indicate that the lexical anchor is used as a bound-
ary, thus we define (lh)⇐ , h and (lh)⇒ , h.
The remaining values are used to indicate that the
boundary is determined by the foot node span. The
boundary is set to special value gh if and only if
the span of vh has a gap, ie. (vh)← 6= −. Sim-
ply, we set (gh)⇐ , (vh)← and (gh)⇒ , (vh)→.
Finally, note that vertices which do not have a gap
in their span, can still be combined through left
or right adjunction (see Figure 1c). Thus, we use
←
gh (resp.

→
gh) in order to qualify the boundary

of an sub-analysis which targets to be combined
through a left (resp. right) adjunction. We set
(
←
gh)⇐ , (vh)⇒ + 1 and (

→
gh)⇒ , (vh)⇐ − 1.

Note that (
←
gh)⇒ and (

→
gh)⇐ are undefined, mean-

ing that rules which use these values can not be
applied.

Tree τ is a candidate for word represented by
vertex vh and its dependants if we can go up at
its root node with boundaries equals to the span of
vh. In order to simplify notation, we use interme-
diate items to represent them. If the item has both
boundaries defined, then:

Full:
[vh, τ, 1,>, bl, br]

(br)⇒ = (vh)⇒

(bl)⇐ = (vh)⇐,

[vh, τ ]

If h 6= r, this item will be a candidate to com-
bination through substitution (resp. adjunction) if
τ ∈ T I (resp. τ ∈ TA), or similarly, if vh has
no gap (resp. has a gap). Two other intermedi-
ate items are used specifically to indicate that they
are meaning to be combined through left and right
adjunction:

Full left:
[vh, τ, 1,>, bl,

←
gh]

(bl)⇐ = (vh)⇐
[vh, τ,←]

Full right:

[vh, τ, 1,>,
→
gh, br]

(br)⇒ = (vh)⇒
[vh, τ,→]

A triplet item ending with the← (resp.→) symbol
is a candidate for left (resp. right) adjunction. Ob-
viously, τ in the antecedent of both rules must be
an auxiliary tree. This is constrained by the Foot
scan rule introduced in the following subsection.

4.2 Axioms and goal

The first axiom is:

Lex scan:
τ(p) = sh

lex(τ) = p,

[vh, τ, p,>, lh, lh]
meaning, for each vertex vh and elementary tree
τ , we instantiate items with compatible elemen-
tary trees, starting at the lexical anchor address.
Moreover, we create items at the foot position of
an auxiliary tree for vertices with a gap in their
span:

Foot scan:

(vh)← 6= −
foot(τ) = p,

τ ∈ ΓA,

[vh, τ, p,>, gh, gh]
Finally, the two last axioms are used to predict
possible trees, on vertices without gap, that will
be combined through left or right adjunction:

Foot scan left:

(vh)← = −
foot(τ) = p,

τ ∈ ΓA,

[vh, τ, p>,
←
g h,

←
g h]

Foot scan right:

(vh)← = −
foot(τ) = p,

τ ∈ ΓA,

[vh, τ, p>,
→
g h,

→
g h]

A proof completes if any tree τ ∈ S is a candi-
date for the root vertex vr of the dependency struc-
ture:

Goal:
[vr, τ ]

τ ∈ τS

In the rest of this section, we describe rules gov-
erning allowed deductions.

4.3 Traversal rules

We start with tree traversal.9 Obviously, the
premise of any move operation is that we already
checked any potential operation, marked by the >
flag. Given address p · 1 in tree τ , we consider two
cases. First, if node p · 1 do not have any sibling,
ie. p · 2 /∈ τ :

Move unary:
[vh, τ, p · 1,>, bl, br]

(p · 2) /∈ τ
[vh, τ, p,⊥, bl, br]

Secondly, if p ·2 exists, the siblings must share the
same frontier:

9We assume binary elementary trees in the following pre-
sentation, but this can be generalized to other tree structures.

117



Move binary:
[vh, τ, p · 1,>, bl1, br1]
[vh, τ, p · 2,>, bl2, br2]

(br1)⇒ + 1 = (bl2)⇐
[vh, τ, p,⊥, bl1, br2]

4.4 Combination rules

Finally, let us concentrate on combination oper-
ations. The simplest, substitution, can only hap-
pen at substitution nodes and both attachment sites
must be labelled with the same non-terminal. We
assume that these conditions are checked by the
fSS function:

Substitute:
[vm, τ

′]
fSS(τ, p, τ

′)

(vm)← = −,

[vh, τ, p,>, vm, vm]
The wrapping adjunction combines a modifier
with a gap:

Wrapping adjoin:
[vm, τ

′] [vh, τ, p,⊥, bl, br]
fSA(τ, p, τ

′)

(vm)→ = (br)⇒,

(vm)← = (bl)⇐,

[vh, τ, p,>, vm, vm]
Similarly, left and right adjunctions deal with ver-
tices without gap:

Left adjoin:
[vm, τ

′,←][vh, τ, p,⊥, bl, br]
fSA(τ, p, τ

′)

(vm)⇒ = (bl)⇐ − 1,

[vh, τ, p,>, vm, br]

Right adjoin:
[vm, τ

′,→][vh, τ, p,⊥, bl, br]
fSA(τ, p, τ

′)

(br)⇒ = (vm)⇐ − 1,

[vh, τ, p,>, bl, vm]

Finally, adjunction may be skipped if this is al-
lowed at the current site:

Null adjoin:
[vh, τ, p,⊥, bl, br] ¬fOA(τ, p)
[vh, τ, p,>, bl, br]

5 Correctness

Since we use the deductive parsing framework,
proving the correctness of the algorithm is
straightforward from the notion of item invariant.
Proving that every production rule maintains this
invariant gives us soundness. Conversely, com-
pleteness can be proven by induction on items. In
the following we explain our invariant.

An item [vh, τ, p, c, bl, br] can be deduced from
the axioms through the application of deduction

X

Y

X∗

s(bl)⇐ s(vh)←−1 s(vh)→+1 s(br)⇒s(vh)⇐ s(vh)⇒

Figure 3: Invariant of an item [vh, τ, p, c, bl, br]
when vertex vh has a block degree of 2. X is the
root node of τ , X∗ its foot node and Y the node at
address p. Only the gray area has been parsed.

rules if and only if, with respect to the input de-
pendency parse, τ(p) can be derived to generate
the subsequence of terminals and foot nodes:

• if address p in τ does not dominate a foot
node, the subsequence is s(bl)⇐ . . . s(bl)⇒ ;

• if p in τ dominates a foot node and
this is not a left nor a right adjunction
bl, br /∈ {

←
gh,
→
gh}, the subsequence is

s(bl)⇐ . . . s(vh)←−1X
∗s(vh)→+1 . . . s(br)⇒ ;

• if p in τ dominates a foot node and bl =
←
gh,

the subsequence is X∗s(vh)⇐ . . . s(br)⇒ ;

• if p in τ dominates a foot node and br =
→
gh,

the subsequence is s(bl)⇐ . . . s(vh)⇒X
∗;

withX∗ the foot node of τ . A visualization is pro-
vided on Figure 3.

6 Complexity

It is common practice to directly deduce space
and time complexities from item structures and de-
duction rules, respectively. However, improving
bounds in this setting may lead to algorithms diffi-
cult to understand. Thus, we decided to propose a
rule-based algorithm that is simple to understand
but which naively exposes a loose upper bound
on its underlying complexity. In this section, we
prove that the space and time complexities are lin-
ear.

In order to simplify the analysis, we suppose an
agenda-based implementation (Kay, 1986). Each
deduced item is placed in an agenda. While the
agenda is not empty, the main loop pops out an
item from it and add it to the chart. Then, the

118



popped out item is tested as an antecedent, and all
deduced consequents are pushed in the agenda if
not already present in the chart. See Algorithm 1
for an outline of the algorithm.

Algorithm 1 Outline of the parsing algorithm.
Lines 22-27 apply the Binary move rule.

1: A← [] . Empty Agenda
2: for 1 ≤ m ≤ n do . Init
3: for τ ∈ Γ do . Lex. scan
4: if τ(lex(τ)) = sm then
5: A.push([τ, lex(τ),>, lm, lm])
6: end if
7: end for
8: for τ ∈ ΓA do . Foot scan
9: if (vm)→ 6= − then

10: A.push([τ, foot(τ),>, gm, gm])
11: else
12: A.push([τ, foot(τ),>, ←gm,

←
gm])

13: A.push([τ, foot(τ),>, →gm,
→
gm])

14: end if
15: end for
16: end for
17: C ← [] . Empty Chart
18: while |A| > 0 do
19: [τ, p · 1,>, bl1, br1]← A.pop()
20: C.add([τ, p · 1,>, bl1, br1])
21: . apply Move binary
22: Let bl2 be the unique boundary with

(br1)⇒ + 1 = (bl2)⇐

23: for [τ, p · 2,>, bl2, br2] ∈ C do
24: if [τ, p,>, bl1, br2] /∈ C then
25: A.push([τ, p,>, bl1, br2])
26: end if
27: end for
28: ...Apply other rules...
29: end while

Before analysing the algorithm complexity, we
observe that the first value of an item, the cur-
rent vertex, is redundant. Indeed, given the value
of the left boundary (or right boundary), we can
always retrieve the considered vertex in constant
time. Obviously, when the boundary is given by a
vertex vm, we have vh = (vm)↑.10 For special val-
ues allowed for boundaries, all indexed by a word
position h, a similar operation is straightforward.
For example, if a boundary is given by lh then the
considered vertex is vh.

10We assume that the data structure storing the dependency
graph provides such function in constant time.

The algorithm has a maximum of two nested
loops: the main while loop and for loops match-
ing the second antecedent of binary rules. We
first consider the while loop. An item is added to
the agenda if only if it is not present in the chart.
Thus, each item is considered exactly once by this
loop. We note n the length of the input sentence,
t the maximum number of nodes in an elemen-
tary tree τ ∈ Γ and g , |Γ| the number of ele-
mentary trees.11 Naively, the number of items is
then bounded by O(n2tg). However, the number
of combination operations which can be applied
on an elementary tree is bounded by its number
of nodes, provided we dismiss multiple adjunction
sites. In this case, each node of an elementary tree
may be adjoined or substituted on at most once.
Thus, given an elementary tree and a left bound-
ary, the number of values allowed as a right bound-
ary is limited. This leads to a tighter bound on the
number of items: O(min(t, n)ntg).

We now investigate time complexity. Move bi-
nary is the only rule which has two free an-
tecedents. Indeed, it is easy to see that, in the other
binary rules, fixing the right antecedent always
fixes the left one. For the Move binary rule, given
the left antecedent, the number of candidates for
the right one is naively bounded by O(n). How-
ever, we previously argued that, given a fixed left
boundary, the maximum number of right boundary
alternatives cannot exceed the number of sites on
the current elementary tree. Thus, we can tighten
the bound to O(min(t, n)).

In conclusion, the time complexity of the pro-
posed algorithm is O(min(t, n)2ntg), that is
asymptotically linear with respect to the input sen-
tence length.

7 Conclusion

We proposed an algorithm to compute LTAG
derivations given a sentence and a dependency
structure describing lexical attachments. We
showed that under mild assumptions the worst-
case complexity is linear in the length of the input.

This work fits in the more general project to
interpret LTAG parsing as a dependency struc-
ture decoding problem. In this framework, it is
common to label the graph structure in a post-
processing step. Hence our approach relies on
the availability of valid dependency structures for

11Alternatively, g can be the number of elementary trees
associated with the most ambiguous word of the vocabulary.

119



LTAGs, more precisely well-nested arborescences
with 2-bounded block degree.

Experiments remain to be done in order to vali-
date the practical interest of this approach. More-
over, one limitation of the pipeline system, as in
all pipeline approaches, is the possibility of error
cascades: the first step may decode a dependency
structure that is not recognizable by the grammar
while the sentence is grammatical.

We hope that this work will open new perspec-
tives on parsing with lexicalized grammars. Ex-
ploring a similar technique, one may develop an
efficient parsing algorithm for weighted Lexical-
ized Linear Context Free Rewriting System.

Acknowledgements

We thank the anonymous reviewers for their in-
sightful comments. First author is supported by
a public grant overseen by the French National
Research Agency (ANR) as part of the Investisse-
ments d’Avenir program (ANR-10-LABX-0083).
Second author, supported by a public grant over-
seen by the French ANR (ANR-16-CE33-0021),
completed this work during a CNRS research
leave at LIMSI, CNRS / Université Paris Saclay.

References
Anne Abeille, Kathleen Bishop, Sharon Cote, and Yves

Schabes. 1990. A lexicalized tree adjoining gram-
mar for english. Technical report, University of Pen-
sylvania.

François Barthélemy, Pierre Boullier, Philippe De-
schamp, and Éric Villemonte de la Clergerie.
2001. Guided parsing of range concatena-
tion languages. In Proceedings of 39th An-
nual Meeting of the Association for Compu-
tational Linguistics. Association for Computa-
tional Linguistics, Toulouse, France, pages 42–49.
https://doi.org/10.3115/1073012.1073019.

Manuel Bodirsky, Marco Kuhlmann, and Mathias
Möohl. 2005. Well-nested drawings as models of
syntactic structure. In Proceedings of the 10th Con-
ference on Formal Grammar (FG) and Ninth Meet-
ing on Mathematics of Language (MOL). Edinburgh,
UK, pages 195–203.

Guillaume Bonfante, Bruno Guillaume, and Math-
ieu Morey. 2009. Proceedings of the 11th
International Conference on Parsing Technolo-
gies (IWPT’09), Association for Computational
Linguistics, chapter Dependency Constraints
for Lexical Disambiguation, pages 242–253.
http://aclweb.org/anthology/W09-3840.

Xavier Carreras, Michael Collins, and Terry Koo. 2008.
CoNLL 2008: Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learn-
ing, Coling 2008 Organizing Committee, chapter
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-Rich Parsing, pages 9–16.
http://aclweb.org/anthology/W08-2102.

John Chen and Srinivas Bangalore. 1999. New
models for improving supertag disambiguation.
In Ninth Conference of the European Chapter
of the Association for Computational Linguistics.
http://aclweb.org/anthology/E99-1025.

David Chiang. 2000. Statistical parsing with
an automatically-extracted tree adjoining gram-
mar. In Proceedings of the 38th Annual Meeting
of the Association for Computational Linguistics.
http://aclweb.org/anthology/P00-1058.

Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, Volume 29, Number 4, December 2003
http://aclweb.org/anthology/J03-4003.

Caio Corro, Joseph Le Roux, Mathieu Lacroix, An-
toine Rozenknop, and Roberto Wolfler Calvo. 2016.
Dependency parsing with bounded block degree
and well-nestedness via lagrangian relaxation and
branch-and-bound. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Associ-
ation for Computational Linguistics, pages 355–366.
https://doi.org/10.18653/v1/P16-1034.

Ralph Debusmann, Denys Duchier, Marco Kuhlmann,
and Stefan Thater. 2004. Tag parsing as model enu-
meration. In Proceedings of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms. Vancouver, Canada, pages 148–154.
http://www.aclweb.org/anthology/W04-3320.

Jason Eisner and Giorgio Satta. 2000. A faster parsing
algorithm for lexicalized tree-adjoining grammars.
In Proceedings of the Fifth International Workshop
on Tree Adjoining Grammar and Related Frame-
works (TAG+5). Université Paris 7, pages 79–84.
http://www.aclweb.org/anthology/W00-2011.

Daniel Fernández-González and André F. T. Mar-
tins. 2015. Parsing as reduction. In Proceed-
ings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Association
for Computational Linguistics, pages 1523–1533.
https://doi.org/10.3115/v1/P15-1147.

Carlos Gómez-Rodrı́guez, David Weir, and John Car-
roll. 2009. Parsing mildly non-projective depen-
dency structures. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009). Association for Computational Linguistics,
pages 291–299. http://aclweb.org/anthology/E09-
1034.

120



Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, Volume 25, Number 4, December
1999 http://aclweb.org/anthology/J99-4004.

Aravind K Joshi. 1987. An introduction to tree adjoin-
ing grammars. Mathematics of language 1:87–115.

Aravind K Joshi and Yves Schabes. 1992. Tree-
adjoining grammars and lexicalized grammars. Tree
Automata and Languages .

Laura Kallmeyer and Marco Kuhlmann. 2012. A
formal model for plausible dependencies in lex-
icalized tree adjoining grammar. In Pro-
ceedings of the 11th International Workshop
on Tree Adjoining Grammars and Related For-
malisms (TAG+11). Paris, France, pages 108–116.
http://www.aclweb.org/anthology/W12-4613.

Martin Kay. 1986. Readings in Natural Lan-
guage Processing, Morgan Kaufmann Pub-
lishers Inc., San Francisco, CA, USA, chap-
ter Algorithm Schemata and Data Struc-
tures in Syntactic Processing, pages 35–70.
http://dl.acm.org/citation.cfm?id=21922.24327.

Lingpeng Kong, M. Alexander Rush, and A. Noah
Smith. 2015. Transforming dependencies into
phrase structures. In Proceedings of the 2015
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies. Association
for Computational Linguistics, pages 788–798.
https://doi.org/10.3115/v1/N15-1080.

Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Pro-
ceedings of the COLING/ACL 2006 Main
Conference Poster Sessions. Association for
Computational Linguistics, pages 507–514.
http://aclweb.org/anthology/P06-2066.

Mark-Jan Nederhof. 2009. Weighted parsing of
trees. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies.
Association for Computational Linguistics,
Stroudsburg, PA, USA, IWPT ’09, pages 13–24.
http://dl.acm.org/citation.cfm?id=1697236.1697239.

Fernando C. N. Pereira and David H. D. Warren.
1983. Parsing as deduction. In 21st Annual Meet-
ing of the Association for Computational Linguis-
tics. http://aclweb.org/anthology/P83-1021.

Owen Rambow. 2010. The simple truth about de-
pendency and phrase structure representations: An
opinion piece. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 337–340. http://aclweb.org/anthology/N10-
1049.

Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order

phenomena. Recent trends in meaning-text theory
39:167–190.

Philip Resnik. 1992. Probabilistic tree-adjoining gram-
mar as a framework for statistical natural language
processing. In COLING 1992 Volume 2: The 15th
International Conference on Computational Lin-
guistics. http://aclweb.org/anthology/C92-2065.

Giorgio Satta. 1994. Tree-adjoining grammar pars-
ing and boolean matrix multiplication. Computa-
tional Linguistics, Volume 20, Number 2, June 1994
http://aclweb.org/anthology/J94-2002.

Yves Schabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with ’lexicalized’ gram-
mars: Application to tree adjoining grammars.
In Coling Budapest 1988 Volume 2: Interna-
tional Conference on Computational Linguistics.
http://aclweb.org/anthology/C88-2121.

Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: A cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, Volume 21, Number 4, December 1995
http://aclweb.org/anthology/J95-4002.

K. Vijay-Shankar and Aravind K. Joshi. 1986. Some
computational properties of tree adjoining gram-
mars. In Strategic Computing - Natural Lan-
guage Workshop: Proceedings of a Workshop Held
at Marina del Rey, California, May 1-2, 1986.
http://aclweb.org/anthology/H86-1020.

K. Vijay-Shanker and David J. Weir. 1993. Parsing
some constrained grammar formalisms. Computa-
tional Linguistics, Volume 19, Number 4, December
1993 http://aclweb.org/anthology/J93-4002.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the IWPT (Volume 3).

121


