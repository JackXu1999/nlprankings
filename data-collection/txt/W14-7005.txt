









































System Description: Dependency-based Pre-ordering for
Japanese-Chinese Machine Translation

Jingsheng Cai Yujie Zhang Hua Shan Jinan Xu
School of Computer and Information Technology, Beijing Jiaotong University

{12120397, yjzhang, 13120422, jaxu}@bjtu.edu.cn

Abstract

This paper describes the Beijing Jiao-
tong University Japanese-Chinese ma-
chine translation system which partici-
pated in the 1st Workshop on Asian Trans-
lation (WAT 2014). We propose a pre-
ordering approach based on dependency
parsing for Japanese-Chinese statistical
machine translation (SMT). Our system
achieves a BLEU of 24.12 and a RIBES of
79.48 on the Japanese-Chinese translation
task in the official evaluation.

1 Introduction

Difference in word order between source language
and target language will cause troubles in word
alignment and further affect the quality of statis-
tical machine translation (SMT). Therefore, it be-
comes an issue in SMT, especially in the language
pairs where there are great difference in word or-
der between source language and target language.

Syntax-based pre-ordering has demonstrated ef-
fectiveness in previous research. These kinds of
approaches first parse the sentences in the side of
source language. Then pre-ordering rules are ap-
plied to the created parse trees, in order to ob-
tain source language sentences which have simi-
lar word order with target language sentences. Fi-
nally, the reordered source language sentences are
used in the SMT system, not only in training, but
also in tuning and translating. In other words, all
of the sentences in the training set, the develop-
ment set and the test set should be reordered while
applying these kinds of approaches.

Since parsing is required in pre-ordering ap-
proaches, two popular parsing are often consid-
ered, i.e. constituent parsing and dependency
parsing. Constituent parsing has been employed
in pre-ordering for the translation of English-
French (Xia and McCord, 2004), Germany-
English (Collins et al., 2005), Chinese-English

(Wang et al., 2007), etc. and shown its effective-
ness. In recent years, more and more pre-ordering
research used dependency parsing for the transla-
tion of Arabic-English (Habash, 2007), English-
SOV languages (Xu et al., 2009) and Chinese-
English (Cai et al., 2014), because the accuracy
of dependency parsing is greatly improved.

This paper introduces a Japanese-Chinese SMT
system which employs a pre-ordering approach
based on dependency parsing. Since Japanese is
a language with a fairly free word order, depen-
dency parsing can describe the relation between
two Japanese cases in a sentence better than con-
stituent parsing. Therefore, we adopt dependency
parsing for pre-ordering. Experimental results
show that our approach can improve the BLEU
score on the test set by 0.18, compared with the
baseline system (without pre-ordering).

Section 2 describes some issues in Japanese-
Chinese translation and proposes our dependency-
based pre-ordering approach. Section 3 reports
on our experiment results on a Japanese-Chinese
phrase-based SMT (PBSMT) system. Section 4
concludes this paper.

2 Dependency-based Pre-ordering
Approach

Japanese is a kind of SOV language and Chinese
is a kind of SVO language. Therefore, great dif-
ference exists in their word order. Moreover, both
of them have very free word order, i.e. a sentence
may have several expressions by only changing its
word order. This fact causes troubles while trans-
lating Japanese into Chinese.

This section describes some issues in the trans-
lation of Japanese-Chinese and then proposes our
dependency-based pre-ordering approach to tackle
these problems.

39
Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 39‒43,

Tokyo, Japan, 4th October 2014.
2014 Copyright is held by the author(s).



Figure 1: An example Japanese sentence and its
Chinese counterpart, along with their word align-
ment.

Figure 2: An example with a cross word align-
ment, owing to the existence of a Japanese particle
“に”.

2.1 Issues in Japanese-Chinese Translation

As is known to all, Japanese is a kind of SOV lan-
guage, in which the verb (V) occurs after the ob-
ject (O), and Chinese is a kind of SVO language,
in which the verb (V) occurs before the object
(O). This is the most common difference between
Japanese and Chinese. Figure 1 shows an example
of a Japanese sentence and its corresponding Chi-
nese sentence with their word alignment. These
sentences means “I read book.”. As described
in above, the verb “read” occurs after the object
“book” in Japanese but before the object in Chi-
nese. This kind of phenomena is often observed in
Japanese-Chinese parallel corpus, which unavoid-
ably brings about wrong results word alignment
and further affect the quality of machine transla-
tion.

Japanese is a kind of agglutinative language
but Chinese is not. A Japanese sentence consists
of phrases which are usually ended with a parti-
cle, indicating the case of the phrase. Although
all Japanese particles are not content words, the
particles are often incorrectly aligned with Chi-
nese content words in conventional word align-
ment. To relieve this problem, we consider align-
ment to connect the particles with Chinese prepo-
sitions. Figure 2 shows such an example, of which
the Japanese sentence consisting of four phrases
- “wa” case, “no” case, “ni” case and “bunmatu”
case. It means “What company do you work for?”
in English. The tokens within one phrase are dis-
played by one underline. The Japanese particle

Figure 3: Dependency parse tree for the Japanese
sentence in Figure 1.

“に” is aligned to the Chinese preposition “在”,
which is a cross word alignment within the “ni”
case (“どこの会社に”). Erroneous word align-
ments often appear in such kinds of sentences.

2.2 Dependency-based Pre-ordering

After investigating the issues in Japanese-Chinese
translation, we attempt to tackle them by intro-
ducing a pre-ordering approach into the PBSMT
system. Considering that current Japanese de-
pendency parsing is of high accuracy and pro-
vides case information, we think these conditions
are suitable for both global and local pre-ordering
operations. We therefore propose a dependency-
based pre-ordering approach.

Figure 3 shows the dependency parse tree of the
Japanese sentence in Figure 1. There are three
cases in this sentence. Both of the “ga” case and
the “wo” case depend on the last phrase called
as “bunmatu” case, i.e. the ROOT. According to
the dependency tree, we can easily move the “wo”
case after the “bunmatu” to obtain an SVO word
order sentence. After operating this pre-ordering,
the Japanese sentence can be translated into “我/I
读/read书/book。/.” in Chinese, whose word or-
der is the same as its Chinese counterpart.

We investigate the phenomena of word order ex-
changing, such as “wo” case and “ni” case, on
the ASPEC Japanese-Chinese paper excerpt cor-
pus1. We take statistics of the various cases and
observe the samples of the cases with high fre-
quency. Based on the observed results, we build
pre-ordering rules, which consist of global pre-
ordering and local pre-ordering.

Inspired by the work of Xu et al. (2009), we ob-
tain the global pre-ordering rules in the following
way. We focus on verbs and classify the cases that
directly depend on verbs into several groups. We
then set sequence numbers to the groups by con-
sidering the word orders of their Chinese transla-
tions in a Chinese sentence, as we observe from

1http://lotus.kuee.kyoto-u.ac.jp/ASPEC/

40



(a) A Japanese sentence and its Chinese counterpart, along
with their word alingment.

(b) Dependency parse tree for the Japanese sentence.

Figure 4: An example of pre-ordering operation
on a Japanese sentence.

the corpus. Table 1 lists the groups and their se-
quence numbers.

We first define the subtree of a case as the phrase
ended with the particle and those phrases that di-
rectly or indirectly depend on this phrase. Then
we design pre-ordering operation. For a given
Japanese dependency tree, if its root node con-
tains a verb, conduct pre-ordering as the following
steps.

Step 1. Global Pre-ordering: Change the order
of the subtrees of the cases directly depend on the
“bunmatu” case and the “bunmatu” case itself, ac-
cording to the sequence number of the groups that
they belong to, as listed in Table 1. The cases of
Group 1 should occur before those of Group 2,
and so forth. Note that if any two cases belong
to the same group, we keep their relative order un-
changed.

Step 2. Local Pre-ordering: For “wo” case, “de”
case, “ni” case, “he” case, “kara” case and “made”
case, move the particles to the front of the whole
subtree.

Figure 4 gives an example of conducting pre-
ordering operation. Figure 4 (a) displays the
Japanese sentence to be reordered, the correspond-
ing Chinese translation and the word alignment
between them. The English meaning is “(We) dis-
cussed the number of the elements that are con-
tained in these categories.”. The dependency tree
of the Japanese sentence is shown in Figure 4 (b).
First, global pre-ordering is conducted. The sub-
tree of the “wo” case, i.e. all of the tokens occur-
ring before “を”, is moved to the position after the
“bunmatu” case. Then, local pre-ordering is con-
ducted. The particle “を” and “に” are moved to
the front of the subtree of their cases, respectively.
The reordered Japanese sentence can be translated

Group Case
1 ga/が wa/は
2 renyou/conjunction
3 de/で ni/に he/ヘ kara/から

made/まで yori/より to/と
4 bunmatu/文末
5 wo/を

Table 1: Groups of the cases and their sequence
numbers.

into “研讨了/discussed这些/these范畴/category
包含/conclude 要素/element 数/number 。/.” in
Chinese, whose word order is more similar to its
Chinese counterpart.

3 Experiments

Section 2 describes our dependency-based pre-
ordering approach for the translation of Japanese-
Chinese. This section reports on our experiments
and evaluation results.

We use MOSES PBSMT system (Koehn et al.,
2007) in our experiments and use BLEU scores
(Papineni et al., 2002) and RIBES score (Isozaki
et al., 2010) for evaluation.

The data sets are from the ASPEC Japanese-
Chinese paper excerpt corpus. The training data
contains 672,315 sentences, the development data
contains 2,090 sentences and the test data contains
2,107 sentences.

We use a bilingual-based approach proposed in
Su et al. (2013) for Chinese word segmentation,
in which n-gram feature from the raw Chinese
part and word alignment from the parallel corpus
are introduced to augment the conventional model
based on annotation data.

We employ the KNP parser2 for Japanese de-
pendency parsing. The KNP parser can create de-
pendency tree for a Japanese sentence and pro-
vide the part-of-speech (POS) for each word, both
of which are used in our dependency-based pre-
ordering approach.

After Japanese dependency trees are obtained,
we conduct pre-ordering on the training set, the
development set and the test set by applying the
pre-ordering rules described in Section 2.2. The
reordered data sets are then used in training, tuning
and test in the Japanese-Chinese PBSMT system.

On the other hand, data sets without pre-

2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP

41



System BLEU on Dev BLEU on Tst RIBES on Tst
Baseline 21.55 20.71 77.34

Pre-ordering 21.71 20.89 77.67

Table 2: Comparisons of the pre-ordering system and the baseline system.

Category Positive Unchanged Negative
Global 115 49 36
Local 63 108 29

Table 3: Human evaluation for 200 reordered sentences, respectively.

ordering are used in our baseline system for com-
parison.

Table 2 presents the evaluation results of the
pre-ordering system and the baseline system, in-
cluding the BLEU scores on the development set,
the BLEU scores and the RIBES scores on the test
set. The results show that the system employing
our dependency-based pre-ordering approach out-
performs the baseline system slightly, both on the
development set and the test set. Note that these
results are different with those in the official eval-
uation because we should keep the consistency
in Chinese word segmentation, while the official
evaluation uses other word segmenters (kytea and
Stanford).

To access the accuracy of our pre-ordering ap-
proach, we also conduct human evaluation for the
reordered sentences. 200 global reordered sen-
tences and 200 local reordered sentences are ex-
tracted from the training data, along with their
corresponding Chinese sentences. For global pre-
ordering, we align the Japanese cases with the
Chinese phrases. For local pre-ordering, we pick
the reordered Japanese cases and align the words
in them with the Chinese words. For both cate-
gories of pre-ordering, we count the number of
cross alignment and compare it with the one of
the original sentence pairs. Table 3 shows the re-
sults of our human evaluation. Here, “Positive”
means that the number of the sentences in which
the cross alignment decreases after applying the
pre-ordering, “Negative” means that the number
of the sentences in which the cross alignment in-
creases after applying the pre-ordering, and “Un-
changed” means that the number of the sentences
in which the cross alignment stays the same af-
ter applying the pre-ordering. As shown in Ta-
ble 3, global pre-ordering helps improve the word
alignment in 57.5% sentences and 24.5% stays the

same. Local pre-ordering helps improve the word
alignment in 31.5% sentences and 54% stays the
same. The results access the accuracy and ef-
fectiveness of our approach. Note that the “Un-
changed” local pre-ordering is 108, which is over
half of all. The reason for this is that in fact many
Japanese particles have no alignment relation with
any Chinese word, pre-ordering of these particles
will not change the number of cross alignment at
all.

4 Conclusion

This paper describes the Beijing Jiaotong Uni-
versity Japanese-Chinese PBSMT system partic-
ipated in WAT 2014. The system employs a
dependency-based pre-ordering approach. The ap-
proach conducts two categories of pre-ordering:
1) global pre-ordering reorders the cases directly
depending on the verb of “bunmatu” case; 2)
local pre-ordering moves the Japanese particles
to the front of the case they belong to. Ex-
perimental results show that our pre-ordering ap-
proach improves the BLEU score by 0.18 and the
RIBES score by 0.34 on the test set of the AS-
PEC Japanese-Chinese corpus. The accuracy and
effectiveness of this approach are also accessed by
human evaluation.

Note also that some cases which are not listed
in Table 1 may also be parsed to depend on a verb
of “bunmatu” case. We do not operate any pre-
ordering in such situation currently because their
frequency is very low and it is difficult to summa-
rize their reordering phenomena.

Our research builds up a framework for
Japanese-Chinese pre-ordering. However, the im-
provement for the PBSMT system is not as great
as our expectation. As shown in Table 3, some
negative results are brought into the word align-
ment after pre-ordering. This implies that the there

42



are some problems in current pre-ordering rules.
More features from dependency tree may be intro-
duced as constrains to the pre-ordering approach
in the future.

Acknowledgments

This work is supported by Key Lab of Intelli-
gent Information Processing of Chinese Academy
of Sciences (CAS), Institute of Computing Tech-
nology, CAS, Beijing 100190, China and Inter-
national Science & Technology Cooperation Pro-
gram of China (Grant No. 2014DFA11350).

References
Jingsheng Cai, Masao Utiyama, Eiichiro, Sumita,

and Yujie Zhang. 2014. Dependency-based Pre-
ordering for Chinese-English Machine Translation.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics, pages
155-160.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 531-540.

Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proceedings of the
11th Machine Translation Summit (MT-Summit).

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 944-952.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177-180.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311-318.

Chen Su, Yujie Zhang, Zhen Guo and Jinan Xu. 2013.
Exploring Multiple Chinese Word Segmentation Re-
sults Based on Linear Model. Natural Language
Processing and Chinese Computing, 400:5059.

Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 737-745.

Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508-514.

Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J.
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of HLT-NAACL, pages 245-253.

43




