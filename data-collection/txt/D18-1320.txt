































Multimodal neural pronunciation modeling for spoken languages with logographic origin


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2916–2922
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2916

Multimodal neural pronunciation modeling for spoken languages
with logographic origin

Minh Nguyen
National University of

Singapore
elenguy@nus.edu.sg

Gia H. Ngo
National University of

Singapore
ngohgia@u.nus.edu

Nancy F. Chen
Institute for Infocomm Research

Singapore
nancychen@alum.mit.edu

Abstract
Graphemes of most languages encode pro-
nunciation, though some are more ex-
plicit than others. Languages like Spanish
have a straightforward mapping between its
graphemes and phonemes, while this mapping
is more convoluted for languages like English.
Spoken languages such as Cantonese present
even more challenges in pronunciation mod-
eling: (1) they do not have a standard writ-
ten form, (2) the closest graphemic origins are
logographic Han characters, of which only a
subset of these logographic characters implic-
itly encodes pronunciation. In this work, we
propose a multimodal approach to predict the
pronunciation of Cantonese logographic char-
acters, using neural networks with a geomet-
ric representation of logographs and pronun-
ciation of cognates in historically related lan-
guages. The proposed framework improves
performance by 18.1% and 25.0% respective
to unimodal and multimodal baselines.

1 Introduction

In phonographic languages, there is a di-
rect correspondence between graphemes and
phonemes (Defrancis, 1996), though this corre-
spondence is not always one-to-one. For exam-
ple, in English, the word table corresponds to
the pronunciation [‘‘teI.bl], in which each alpha-
betic character corresponds to one phoneme, and
the character e is mapped to silence. However,
in logographic languages, the correspondence be-
tween graphemes and phonemes is more ambigu-
ous (Defrancis, 1996), as only some sub-units in a
grapheme are indicative of its phonemes. Korean1,
Vietnamese2 and Chinese languages (e.g. Can-
tonese) are examples of logographic languages, all

1A large portion of Korean vocabulary are Sino-Korean
written in Hanja (Korean logographs) (Sohn, 2001)

2Traditional Vietnamese vocabulary comprises of Sino-
Vietnamese words written by Chinese logographs and
locally-invented Nom logographs (Alves, 1999).

belonging to the Han logographic family. Sim-
ilar to pronunciation modeling in phonographic
languages, in which words are broken down into
characters and modeling is done at the character
level, pronunciation modeling in logographic lan-
guages requires decomposing logographs into sub-
units and extracting only sub-units carrying pro-
nunciation hints. As the correspondence of Han
logograph to phoneme is intricately complex with
many sub-rules or exceptions (Hashimoto, 1978),
it is challenging to computationally model these
correspondences using white box approaches (e.g.
graphical model). Instead, we exploit neural net-
works, as they (1) can flexibly model the im-
plicit similarity of grapheme-phoneme relation-
ships across languages with Han origin, (2) can au-
tomatically learn the most relevant knowledge rep-
resentation with minimal feature engineering (Le-
Cun et al., 2015), such as extracting pronunciation
hints from logographic representations.

Due to historical contact, there is much lexi-
cal overlap across Han logographic languages, as
they borrowed words from one another (Rokuro,
1969; Miyake, 1997; Loveday, 1996; Sohn, 2001;
Alves, 1999). As a result, cognates in different
languages are written using identical graphemes
but pronounced differently. For example, [she]
in Mandarin and [sip] in Cantonese are cog-
nates; their pronunciations are different yet they
are written using the same logograph (懾), which
represents “admire”. Though Han logographic
languages are mutually unintelligible (Tang and
Van Heuven, 2009; Handel, 2015), the correspon-
dence of Han logographic graphemes to phonemes
across languages is often similar in systematic
ways (Cai et al., 2011; Frellesvig and Whitman,
2008; Miyake, 1997). The shared characteristics
in pronunciation of cognates could be leveraged in
deciphering the pronunciation of Han logographs.
In this work, we proposed a neural pronuncia-



2917

tion model that exploits both embeddings of lo-
gographs and cognates’ phonemes. The proposed
model significantly improves pronunciation pre-
diction of logographs in Cantonese.

2 Related Work

The basic units in writing (graphemes) of Han lo-
gographic languages are logographs. A word con-
tains one or more logographs and a logograph con-
sists of one or more radicals. The pronunciation of
a logograph corresponds to a syllable which has
three phonemes: onset, nucleus and coda.

Grapheme-to-phoneme (G2P) approaches such
as (Xu et al., 2004; Chen et al., 2016) predicted a
Han logograph’s pronunciation from its local con-
text in a phrase. This was similar to predicting
a Latin word’s pronunciation from its surrounding
words, essentially treated individual logographs as
the basic units of the model and did not delve fur-
ther into the logographic sub-units (the radicals).

While we are unaware of any work that de-
rives features for pronunciation prediction from
logographs, there are recent work in deriving rep-
resentation of logographs for various semantic
tasks. Some methods (Shi et al., 2015; Ke and
Hagiwara, 2017; Nguyen et al., 2017; Zhuang
et al., 2017) decomposed logographs into sub-
units using expert-defined rules and then extracted
the relevant semantic features. Other methods use
convolutional neural network to extract features
from the images of logographs (Dai and Cai, 2017;
Liu et al., 2017; Toyama et al., 2017). Other works
combined multiple level of information for feature
extraction, using both logograph and sub-units ob-
tained from logograph decomposition (Dong et al.,
2016; Han et al., 2017; Peng et al., 2017; Yu et al.,
2017; Yin et al., 2016).

In this work, we explicitly looked at the rela-
tionship between a logograph’s constituent rad-
icals and its pronunciation. Among Han lo-
gographs, 81% of frequently used logographs
are semantic-phonetic compounds (Li and Kang,
1993) which consist of radicals that might contain
phonetic or semantic hints (Hsiao and Shillcock,
2006). The pronunciation of a logograph could
conceivably be predicted from the phonetic radi-
cals. Furthermore, the relative position of radicals
in the logograph might also offer clues about it
pronunciation. Table 1 shows an example of such
intricate relationships between a logograph’s pro-
nunciation and its constituent radicals. All Han

logographs in the table have a common phonetic
radical (in red), which offers an inkling of the pro-
nunciation of these logographs. For instance, lo-
gographs that have the phonetic radical on the left
(剖 and 部) share a similar pronunciation in Ko-
rean (in blue) while logographs that have the pho-
netic radical on the right (陪, 賠, and 蓓) share
a similar pronunciation in Mandarin, Cantonese
and Vietnamese. Note that for each logograph,
their pronunciations across the different languages
share similarities: when the phonetic radical is on
the left, the nucleus ends in a back vowel like u
or o, whereas when the phonetic radical is on the
right, the nucleus ends in a front vowel like i.

Mandarin pou bu pei pei bei
Cantonese fau bou pui pui bui
Korean pwu pwu pay pay pay
Vietnamese phau bo boi boi bui

Position of 咅

Logograph

Table 1: The position of radicals affects pronuncia-
tions. All logographs share a common radical in red.
Similar pronunciations for 剖 and 部 are bolded in
blue. Similar pronunciations for 陪, 賠, and 蓓 are
bolded in green. The pronunciation of a logograph in
Mandarin, Cantonese, Korean and Vietnamese are rep-
resented by Pinyin, Jyutping, Yale, and Vietnamese al-
phabet symbols respectively.

The example in Table 1 explains the motivation
for our proposed approach to predict a logograph’s
pronunciation by modelling both the constituent
radicals and their geometric positions. Further-
more, the proposed approach can generalize to un-
seen logographs if the co-occurrence patterns of
their constituent radicals have been learnt.

3 Model

We first describe a geometric decomposition of lo-
gographs and then different neural pronunciation
models for logographs. Finally, we present a mul-
timodal neural model that incorporates both logo-
graphic input and the cognates’ phonemes in pre-
dicting pronunciation of logographs.

Representation of Han logographs

The majority of logographs (characters) in Han lo-
gographic language family comprise of a radical
that indicates its nominal semantic category and a
phonetic radical that gives an inkling of the pro-
nunciation (Defrancis, 1996). Thus, patterns of
co-occurrence of radicals across logographs might



2918

忄

A
Tree 
forms

聶

懾

忄

B

聑

懾

耳

⿱

⿰ ⿰

忄

C

懾

耳

⿱

⿰

耳耳

⿰

Vector forms

忄⿰ ⿱ 耳 ⿰ 耳 耳忄⿰ ⿱ 耳 聑忄⿰ 聶

Figure 1: Geometric representation of the logograph
“admire”. A, B and C are equivalent decomposition of
the same logograph but with different levels of granu-
larity. The geometric representation comprises of both
the radicals and geometric operators, which can be
used to reconstruct the original logograph.

be exploited to find the phonetic radicals, which in
turn can suggest the corresponding pronunciation
of a logograph. Using this intuition, we model the
pronunciation of logographs at the radical level.

We investigated two representations of radicals
in a logograph. In the first approach, a logograph
is represented as a bag of its unordered constituent
radicals (BoR), encoded as a vector of radical
counts. The second approach is to use a decom-
position of radicals in the logograph that retains
the original geometric organization of the radi-
cals. The geometric decomposition (GeoD) ap-
proach preserves important cues about the word’s
pronunciation in the relative position of the rad-
icals. For example, differentiating the left radi-
cal from the right radical in a left-right semantic-
phonetic compound allows more effective extrac-
tion of pronunciation hints. In addition, radi-
cals that should be interpreted together are closer
spatially in the GeoD representation, making the
knowledge representation easier to learn. Note
that the GeoD representation is lossless as the
original logograph can be reconstructed perfectly
(details in Appendix A). Figure 1 shows the geo-
metric decomposition of the Han logograph “ad-
mire” at three levels of granularity.

Neural pronunciation prediction models

Figure 2 and Figure 3 show two neural pronuncia-
tion prediction models of logographs. In Figure 2,
each logograph is treated as an ordered “bag of
radicals” (BoR). For example, assume the vocab-
ulary of radicals in the whole dataset is [忄, 氵,
耳, 灬], the word 懾 (“admire” - see Figure 1) is
represented by a vector of counts [1, 0, 3, 0], cor-
responding to one radical忄 and three radicals耳.

The BoR is input to a multilayer perceptron (MLP)
with three layers of size 750, 500, 250. L2 regular-
ization of 1e-4 is applied to the hidden layers. The
three dropout layers have dropout probabilities of
0.5, 0.5, and 0.2, respectively. As the output vari-
ables are categorical, cross-entropy loss was used.

We investigated two structures for predicting
output phonemes (i.e. onset, nucleus, coda). In the
first structure, output phonemes were predicted in-
dependently using the last hidden layer. The sec-
ond structure made a sequential prediction (1) the
coda was first predicted using the last hidden layer
(2) the nucleus was predicted using both the final
hidden layer and the predicted coda, and (3) the
onset was predicted using the last hidden layer to-
gether with the predicted coda and nucleus. The
second structure was motivated by a stronger de-
pendency between the nuclues and coda. For ex-
ample, the nucleus and coda are often grouped to-
gether as a single unit (rime/final) in the syllabic
structure of most languages (Kessler and Treiman,
2002). In our experiments, the sequential structure
yielded lower error rates so it is used in all neural
network models.

Radicals
vocabulary

s (ON)忄

氵

耳

灬

750

FC

Drop
out

500 250

Drop
outFC

FC
Drop
out

i (NU)

p (CD)
Output

phonemes
Radicals 

count 
for 懾

1
0
3
0

Figure 2: Pronunciation model of logographs using
multilayer perceptron (MLP). FC: Fully connected.

In Figure 3, each logograph is represented by
its geometric decomposition (GeoD). For exam-
ple, the logograph懾 is represented by a sequence
of radicals and geometric operators shown in Fig-
ure 1C. The neural prediction model consists of
two LSTM layers with 256 memory cells each. In-
put and recurrent dropout (Gal and Ghahramani,
2016) of 0.2 and 0.5 are applied to the LSTM lay-
ers to prevent overfitting.

s (ON)

i (NU)

p (CD)
Output

phonemesGeometric decomposition (GeoD) of radicals

忄

LSTM

懾 ⿰ ⿱ 耳 ⿰ 耳 耳
Input

logograph

LSTM

Logographic radical Geometric operator

Figure 3: Neural pronunciation model with geometric
decomposition of logographs.



2919

Multimodal neural pronunciation model of
logographs

In this section, we want to model the pronuncia-
tions of a logograph in the target language Can-
tonese using multimodal information from both
the logograph and phonemes of the cognates,
as shown in Figure 4. Given a vocabulary of
phonemes in the source languages related to Can-
tonese (Mandarin, Korean, Vietnamese), the cog-
nates’ phonemes are encoded as an indicator vec-
tor, with an element equals 1 if the corresponding
phoneme in the vocabulary appears in a cognate’s
pronunciation, and 0 otherwise.

The geometric decomposition (GeoD) of the lo-
gograph is fed to two LSTM layers. The output at
the last time step is concatenated together with the
multilingual phonemic vector and used as input
for a multi-layer perceptron (MLP). The MLP and
LSTM setups are the same as those in Figure 2 and
Figure 3 respectively. Deep supervision (Szegedy
et al., 2015) was applied by using the output of
the LSTM to make auxiliary prediction of the out-
put phonemes. Note that the auxiliary prediction
should be identical to the main prediction. While
predicting the same target, the main prediction
used both cognate phonemes and the logograph
while the auxiliary prediction used only the logo-
graph. This was to ensure features extracted from
the logographs are useful for pronunciation pre-
diction and are complementary to the features ex-
tracted from the multilingual phonemes.

4 Experiments

We investigate whether Cantonese phonemes
could be predicted using Han logographs and the
cognates’ phonemes from Mandarin, Korean, and
Vietnamese. The prediction output are Cantonese
onsets, nuclei and codas. The experimental de-
sign is motivated by the nature of Han-logographic
languages. A Chinese logograph (character) is
phonologically equivalent to a syllable in English
while the constituent radicals are analogous to al-
phabet letters (with far less phonetic information).
While in most languages, a syllable’s pronuncia-
tion is influenced by neighboring syllables, most
Han-logographic languages are monosyllabic and
a logograph’s pronunciation is rarely affected by
neighboring logographs. Therefore, pronunciation
prediction at the logograph (character) level for
Han logographs is more appropriate. We use string
error rate (SER) and token error rate (TER) as

evaluation metrics. A wrongly predicted phoneme
(onset, nucleus or coda) is counted as one token er-
ror. A syllable containing token error(s) is counted
as one string error. All the neural networks were
trained using Adam (Kingma and Ba, 2014).

Data

The dataset is extracted from the UniHan
database,3 which is a pronunciation database of
logographs from Han logographic languages and
maintained by the Unicode consortium. For each
entry in the dataset, a logograph corresponds to
phonemes in Cantonese, Mandarin, Korean and
Vietnamese, represented by Jyutping,4 Pinyin,5

Yale,6 and Vietnamese alphabet symbols respec-
tively.7 We randomly partition the dataset into two
sets, with 80% for training and the other 20% for
testing. Overall, there are 16,011 entries in the
training set and 4,002 entries in the test set. 1000
entries of the training set are used as the develop-
ment set for hyper-parameters fine-tuning.

In the test set, only 16% of logographs have
pronunciations in all non-target languages, while
6% of logographs have no non-target language
pronunciation. The availability of pronunciations
in non-target languages differs from logograph to
logograph. For example, some logographs have
Mandarin and Korean pronunciations, while oth-
ers only have Mandarin pronunciations.

Predicting pronunciation using logograph
input

We compared the neural networks against a deci-
sion tree baseline. The decision tree baseline was
implemented using scikit-learn (Pedregosa et al.,
2011). The input of the decision tree (DT) model
is the BoR representation of the logograph, while
the input of neural networks can be either BoR or
GeoD. The MLP network in Figure 2 uses BoR,
while the LSTM in Figure 3 uses GeoD as input.
All models output phonemes in Cantonese.

From Table 2, the neural network (MLP) out-
performs decision tree when using BoR input.
Both the SER and TER of the MLP model are
lower than those of the decision tree. The LSTM
model using GeoD leads to the lowest SER and
TER, suggesting the benefits of relative positional

3https://www.unicode.org/charts/unihan.html
4https://en.wikipedia.org/wiki/Jyutping
5https://en.wikipedia.org/wiki/Pinyin
6https://en.wikipedia.org/wiki/Yale romanization of Korean
7https://en.wikipedia.org/wiki/Vietnamese alphabet



2920

s (ON)

750

FC

Drop
out

500 250

Drop
outFC

FC
Drop
out

i (NU)

p (CD)
Output

phonemes

0
1
…
1
0
..
0
1
…

Logographic radical

Geometric operator

Phonemic
vocabulary

Phonemic
indicator vector

Mandarin phonemes

Korean phonemes
Logographic embedding

Geometric decomposition (GeoD) of radicals

忄

LSTM

懾 ⿰ ⿱ 耳 ⿰ 耳 耳
Input

logograph

LSTM

s (ON)i (NU)

Auxiliary output phonemes

p (CD)

k
o
…
s
i

...
a
t

…

Vietnamese phonemes

Figure 4: Multimodal neural pronunciation prediction model using logographs’ geometric representation and
cognates’ phonemes.

information of radicals in predicting pronuncia-
tion. The trends of onset, nucleus and coda er-
ror rates are similar to those of TER and SER.
However, as the gap of of error rate between MLP
(BoR) and LSTM (GeoD) for TER and SER are
quite small, using BoR instead of GeoD can be a
good computation-accuracy trade-off.

Method SER TER On. Nu. Cd.

DT (BoR) 63.8 39.8 50.7 45.7 22.9

MLP (BoR) 59.2 33.6 44.5 38.6 17.8

LSTM (GeoD) 58.4 32.6 43.3 37.4 17.1

Table 2: Prediction error rates of Cantonese
phonemes by decision tree (DT), MLP and LSTM us-
ing only logographic input. Best results are in bold.

Predicting pronunciation using multimodal
input

The input of the models are logographs and cog-
nate phonemes from Mandarin, Korean and Viet-
namese. Table 3 shows that the proposed multi-
modal neural network exploits multimodal and ge-
ometric information effectively. The relative im-
provement reaches 18.2% and 33.3% for SER and
TER respectively. The last rows in Table 2 and
Table 3 show that by combining Korean, Man-
darin and Vietnamese phonemes input with GeoD,
the prediction performance improves by 54.1%
relative in TER and by 65.5% relative in SER.
Moreover, using solely logograph input resulted
in higher onset error (43.3%) than nucleus error
(37.4%) while using both logographs and multilin-
gual phonemes improves the onset error (23.5%)
to be lower than nucleus error (24.6%). This
suggests that logographs and phonemes of cog-
nates provide complementary information about
the pronunciation of a logograph, which in this

case, most notably at the onset position. While
logographs usually carry hints about phonemes at
the nucleus and coda position but not at the onset
position, multilingual phonemes input might carry
hints about pronunciation at all three positions.

Method SER TER On. Nu. Cd.

DT (BoR, ph) 44.0 24.8 29.8 29.9 14.7

MLP (BoR, ph) 38.5 19.6 23.4 24.8 10.5

LSTM (GeoD, ph) 37.2 18.6 22.6 23.4 9.8

Table 3: Prediction error rates of Cantonese
phonemes by multimodal models; BoR: Bag of Radi-
cals; GeoD: Geometric Decomposition; ph:phonemes.
Best results are in bold.

5 Discussion

We have empirically shown that the systematic yet
tenuous correspondence between pronunciations
of cognates in Han logographic languages can be
exploited for pronunciation modeling using neural
networks. Moreover, combining logograph with
cognate pronunciations further improves pronun-
ciation prediction. These results could be poten-
tially applied to speech processing tasks such as
speech synthesis, where the construction of pro-
nunciation dictionaries are expert labor-intensive,
especially for under-resourced spoken languages.

For future work, recursive neural network (Tai
et al., 2015) can be used as it is better suited for the
hierarchical logographic decomposition. Besides,
incorporating more detailed relationship between
radicals (e.g. (Zhuang et al., 2017)) can help im-
prove the model. The proposed approaches can
also be applied to other languages such as Min
Nan or Hakka, which are spoken languages that
are even less well-documented than Cantonese.



2921

References
Mark J Alves. 1999. What’s so Chinese about Viet-

namese. In Papers from the ninth annual meeting of
the Southeast Asian Linguistics Society, pages 221–
242.

Zhenguang G Cai, Martin J Pickering, Hao Yan, and
Holly P Branigan. 2011. Lexical and syntactic rep-
resentations in closely related languages: Evidence
from Cantonese–Mandarin bilinguals. Journal of
Memory and Language, 65(4):431–445.

Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
2016. Acoustic data-driven pronunciation lexicon
generation for logographic languages. In Acous-
tics, Speech and Signal Processing (ICASSP), 2016
IEEE International Conference on, pages 5350–
5354. IEEE.

Falcon Z Dai and Zheng Cai. 2017. Glyph-aware
Embedding of Chinese Characters. EMNLP 2017,
page 64.

John Defrancis. 1996. Graphemic indeterminacy in
writing systems. Word, 47(3):365–377.

Chuanhai Dong, Jiajun Zhang, Chengqing Zong,
Masanori Hattori, and Hui Di. 2016. Character-
based LSTM-CRF with radical-level features for
Chinese named entity recognition. In Natural Lan-
guage Understanding and Intelligent Applications,
pages 239–250. Springer.

Bjarke Frellesvig and John Whitman. 2008.
The Japanese-Korean vowel correspondences.
Japanese/Korean Linguistics, 13:15–28.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems, pages 1019–1027.

He Han, Yang Xiaokun, Wu Lei, Yan Hua, Gao
Zhimin, Feng Yi, and Townsend George. 2017.
Dual long short-term memory networks for sub-
character representation learning. arXiv preprint
arXiv:1712.08841.

Zev Handel. 2015. The classification of Chinese:
sinitic (the Chinese language family). In The Ox-
ford handbook of Chinese linguistics, pages 34–44.
Oxford University Press.

Mantaro J Hashimoto. 1978. Current developments in
Sino—Vietnamese studies. Journal of Chinese Lin-
guistics, pages 1–26.

Janet Hui-wen Hsiao and Richard Shillcock. 2006.
Analysis of a chinese phonetic compound database:
Implications for orthographic processing. Journal
of psycholinguistic research, 35(5):405–426.

Yuanzhi Ke and Masafumi Hagiwara. 2017. Radical-
level Ideograph Encoder for RNN-based Sentiment
Analysis of Chinese and Japanese. arXiv preprint
arXiv:1708.03312.

Brett Kessler and Rebecca Treiman. 2002. Syllable
structure and the distribution of phonemes in english
syllables.

Diederik P. Kingma and Jimmy Ba. 2014.
Adam: A Method for Stochastic Optimization.
arXiv:1412.6980 [cs]. ArXiv: 1412.6980.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature, 521(7553):436.

Y Li and JS Kang. 1993. Analysis of phonetics of the
ideophonetic characters in Modern Chinese. Infor-
mation analysis of usage of characters in modern
Chinese, pages 84–98.

Frederick Liu, Han Lu, Chieh Lo, and Graham
Neubig. 2017. Learning character-level compo-
sitionality with visual features. arXiv preprint
arXiv:1704.04859.

Leo J Loveday. 1996. Language contact in Japan: A
sociolinguistic history. Clarendon Press.

Marc Hideo Miyake. 1997. Pre-Sino-Korean and Pre-
Sino-Japanese: reexamining an old Problem from a
modern perspective. Japanese/Korean Linguistics,
6:179–211.

Viet Nguyen, Julian Brooke, and Timothy Baldwin.
2017. Sub-character Neural Language Modelling in
Japanese. In Proceedings of the First Workshop on
Subword and Character Level Models in NLP, pages
148–153.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Haiyun Peng, Erik Cambria, and Xiaomei Zou. 2017.
Radical-based hierarchical embeddings for Chinese
sentiment analysis at sentence level. In The 30th In-
ternational FLAIRS conference. Marco Island.

Kono Rokuro. 1969. The Chinese writing and its in-
fluence on the Scripts of the Neighbouring Peoples
with special reference to Korea and Japan. Memoirs
of the Research Department of the Toyo Bunko (The
Oriental Library) No, 27:117–123.

Xinlei Shi, Junjie Zhai, Xudong Yang, Zehua Xie,
and Chao Liu. 2015. Radical embedding: Delving
deeper to Chinese radicals. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), volume 2, pages 594–598.

Ho-Min Sohn. 2001. The Korean Language. Cam-
bridge University Press.



2922

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2015. Going deeper with convolutions. In
The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR).

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers).

Chaoju Tang and Vincent J Van Heuven. 2009. Mu-
tual intelligibility of Chinese dialects experimentally
tested. Lingua, 119(5):709–732.

Yota Toyama, Makoto Miwa, and Yutaka Sasaki. 2017.
Utilizing Visual Forms of Japanese Characters for
Neural Review Classification. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
volume 2, pages 378–382.

Jun Xu, Guohong Fu, and Haizhou Li. 2004.
Grapheme-to-phoneme conversion for chinese text-
to-speech. In Eighth International Conference on
Spoken Language Processing.

Rongchao Yin, Quan Wang, Peng Li, Rui Li, and Bin
Wang. 2016. Multi-granularity Chinese word em-
bedding. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 981–986.

Jinxing Yu, Xun Jian, Hao Xin, and Yangqiu Song.
2017. Joint Embeddings of Chinese Words, Charac-
ters, and Fine-grained Subcharacter Components. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 286–
291.

Hang Zhuang, Chao Wang, Changlong Li, Qingfeng
Wang, and Xuehai Zhou. 2017. Natural Language
Processing Service Based on Stroke-Level Convo-
lutional Networks for Chinese Text Classification.
In Web Services (ICWS), 2017 IEEE International
Conference on, pages 404–411. IEEE.


