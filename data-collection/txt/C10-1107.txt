Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 949–957,

Beijing, August 2010

949

Bringing Active Learning to Life

Ines Rehbein

Josef Ruppenhofer

Alexis Palmer

Computational Linguistics

Computational Linguistics

Computational Linguistics

Saarland University

Saarland University

Saarland University

{rehbein|josefr|apalmer}@coli.uni-sb.de

Abstract

Active learning has been applied to dif-
ferent NLP tasks, with the aim of limit-
ing the amount of time and cost for human
annotation. Most studies on active learn-
ing have only simulated the annotation
scenario, using prelabelled gold standard
data. We present the ﬁrst active learning
experiment for Word Sense Disambigua-
tion with human annotators in a realistic
environment, using ﬁne-grained sense dis-
tinctions, and investigate whether AL can
reduce annotation cost and boost classiﬁer
performance when applied to a real-world
task.

1 Introduction
Active learning has recently attracted attention as
having the potential to overcome the knowledge
acquisition bottleneck by limiting the amount of
human annotation needed to create training data
for statistical classiﬁers. Active learning has been
shown, for a number of different NLP tasks, to re-
duce the number of manually annotated instances
needed for obtaining a consistent classiﬁer perfor-
mance (Hwa, 2004; Chen et al., 2006; Tomanek et
al., 2007; Reichart et al., 2008).

The majority of such results have been achieved
by simulating the annotation scenario using prela-
belled gold standard annotations as a stand-in for
real-time human annotation. Simulating annota-
tion allows one to test different parameter set-
tings without incurring the cost of human anno-
tation. There is, however, a major drawback: we

do not know whether the results of experiments
performed using hand-corrected data carry over to
real-world scenarios in which individual human
annotators produce noisy annotations.
In addi-
tion, we do not know to what extent error-prone
annotations mislead the learning process. A sys-
tematic study of the impact of erroneous annota-
tion on classiﬁer performance in an active learn-
ing (AL) setting is overdue. We need to know a)
whether the AL approach can really improve clas-
siﬁer performance and save annotation time when
applied in a real-world scenario with noisy data,
and b) whether AL works for classiﬁcation tasks
with ﬁne-grained or complex annotation schemes
and a low inter-annotator agreement.

In this paper we bring active learning to life in
the context of frame semantic annotation of Ger-
man texts within the SALSA project (Burchardt
et al., 2006). Speciﬁcally, we apply AL methods
for learning to assign semantic frames to predi-
cates, following Erk (2005) in treating frame as-
signment as a Word Sense Disambiguation task.
Under our ﬁne-grained annotation scheme, anno-
tators have to deal with a high level of ambigu-
ity, resulting in low inter-annotator agreement for
some word senses. This fact, along with the po-
tential for wrong annotation decisions or possi-
ble biases from individual annotators, results in
an annotation environment in which we get noisy
data which might mislead the classiﬁer. A sec-
ond characteristic of our scenario is that there is no
gold standard for the newly annotated data, which
means that evaluation is not straightforward. Fi-
nally, we have multiple annotators whose deci-

950

sions on particular instances may diverge, raising
the question of which annotations should be used
to guide the AL process. This paper thus investi-
gates whether active learning can be successfully
applied in a real-world scenario with the particular
challenges described above.

Section 2 of the paper gives a short overview
of the AL paradigm and some related work, and
Section 3 discusses the multi-annotator scenario.
In Section 4 we present our experimental design
and describe the data we use. Section 5 presents
results, and Section 6 concludes.

2 Active Learning
The active learning approach aims to reduce the
amount of manual annotation needed to create
training data sufﬁcient for developing a classiﬁer
with a given performance. At each iteration of
the AL cycle, the actual knowledge state of the
learner guides the learning process by determin-
ing which instances are chosen next for annota-
tion. The main goal is to advance the learning
process by selecting instances which provide im-
portant information for the machine learner.

In a typical active learning scenario, a small set
of manually labelled seed data serves as the ini-
tial training set for the classiﬁer (learner). Based
on the predictions of the classiﬁer, a large pool
of unannotated instances is queried for the next
instance (or batch of instances) to be presented
to the human annotator (sometimes called the or-
acle). The underlying active learning algorithm
controlling the learning process tries to select the
most informative instances in order to get a strong
boost in classiﬁer performance. Different meth-
ods can be used for determining informativity of
instances. We use uncertainty sampling (Cohn et
al., 1995) in which “most informative” instances
are those for which the classiﬁer has the lowest
conﬁdence in its label predictions. The rough in-
tuition behind this selection method is that it iden-
tiﬁes instance types which have yet to be encoun-
tered by the classiﬁer. The learning process pro-
ceeds by presenting the selected instances to the
human annotator, who assigns the correct label.
The newly-annotated instances are added to the
seed data and the classiﬁer is re-trained on the new
data set. The newly trained classiﬁer now picks

the next instances, based on its updated knowl-
edge, and the process repeats. If the learning pro-
cess can provide precisely that information which
the classiﬁer still needs to learn, a smaller number
of instances should sufﬁce to achieve the same ac-
curacy as on a larger training set of randomly se-
lected training examples.

Active learning has been applied to a num-
ber of natural
language processing tasks like
POS tagging (Ringger et al., 2007), NER (Laws
and Sch¨utze, 2008; Tomanek and Hahn, 2009),
syntactic parsing (Osborne and Baldridge, 2004;
Hwa, 2004), Word Sense Disambiguation (Chen
et al., 2006; Chan and Ng, 2007; Zhu and Hovy,
2007; Zhu et al., 2008) and morpheme gloss-
ing for language documentation (Baldridge and
Palmer, 2009). While most of these studies suc-
cessfully show that the same classiﬁcation accu-
racy can be achieved with a substantially smaller
data set, these ﬁndings are mostly based on simu-
lations using gold standard data.

For our task of Word Sense Disambiguation
(WSD), mixed results have been achieved. AL
seems to improve results in a WSD task with
coarse-grained sense distinctions (Chan and Ng,
2007), but the results of (Dang, 2004) raise doubts
as to whether AL can successfully be applied to
a ﬁne-grained annotation scheme, where Inter-
Annotator Agreement (IAA) is low and thus the
consistency of the human annotations decreases.
In general, AL has been shown to reduce the cost
of annotation when applied to classiﬁcation tasks
where a single human annotator predicts labels for
new data points with a reasonable consistency and
accuracy. It is not clear whether the same settings
can be applied to a multi-annotator environment
where IAA is low.

3 Active Learning in a realistic task

including multiple annotators

Another possible difference between active learn-
ing simulations and real-world scenarios is the
multi-annotator environment.
In such a setting,
two or more annotators assign labels to the same
instances, which are then merged to check for con-
ﬂicting decisions from different annotators. This
is standard practise in many annotation projects
doing ﬁne-grained semantic annotation with a

951

high level of ambiguity, and it necessitates that all
annotators work on the same data set.

Replicating an active learning simulation on
hand-corrected data, starting with a ﬁxed set of
seed data and ﬁxed parameter settings, using the
same algorithm, will always result in the same
training set selected from the pool. Human anno-
tators, however, will assign different labels to the
same instances, thus inﬂuencing the selection of
the next instance from the pool. This means that
individual annotators might end up with very dif-
ferent sets of annotated data, depending on factors
like their interpretation of the annotation guide-
lines, an implicit bias towards a particular label,
or simply errors made during annotation.

There is not much work addressing this prob-
lem.
(Donmez and Carbonell, 2008) consider
modiﬁcations of active learning to accommodate
variability of annotators. (Baldridge and Palmer,
2009) present a real-world study with human an-
notators in the context of language documenta-
tion. The task consists of producing interlin-
ear glossed text,
including morphological and
grammatical analysis, and can be described as
a sequence labelling task. Annotation cost is
measured as the actual time needed for annota-
tion. Among other settings, the authors compare
the performance of two annotators with different
grades of expertise. The classiﬁer trained on the
data set created by the expert annotator in an ac-
tive learning setting does obtain a higher accuracy
on the gold standard. For the non-expert annota-
tor, however, the active learning setting resulted
in a lower accuracy than for a classiﬁer trained on
a randomly selected data set. This ﬁnding sug-
gests that the quality of annotation needs to be
high enough for active learning to actually work,
and that annotation noise is a problem for AL.

There are two problems arising from this:

1. It is not clear whether active learning will

work when applied to noisy data

2. It is not straightforward to apply active learn-
ing to a real-world scenario, where low IAA
asks for multiple annotators

In our experiment we address these questions
by systematically investigating the impact of an-
notation noise on classiﬁer performance and on

the composition of the training set. The next sec-
tion presents the experimental design and the data
used in our experiment.
4 Experimental Design
In the experiment we annotated 8 German cau-
sation nouns, namely Ausgang, Anlass, Ergeb-
nis, Resultat, Grund, Konsequenz, Motiv, Quelle
(outcome, occasion, effect, result, reason, con-
sequence, motive, source of experience). These
nouns were chosen because they exhibit a range
of difﬁculty in terms of the number of senses they
have in our annotation scheme. They all encode
subtle distinctions between different word senses,
but some of them are clearly easier to disam-
biguate than others. For instance, although Aus-
gang has 9 senses, they are easier to distinguish
for humans than the 4 senses of Konsequenz.

Six annotators participated in the experiment.
While all annotators were trained, having at least
one year experience in frame-semantic annota-
tion, one of the annotators is an expert with several
years of training and working experience in the
Berkeley FrameNet Project. This annotator also
deﬁned the frames (word senses) used in our ex-
periment.

Prior to the experiment, all annotators were
given 100 randomly chosen sentences. After
annotating the training data, problematic cases
were discussed to make sure that the annotators
were familiar with the ﬁne-grained distinctions
between word senses in the annotation scheme.
The data sets used for training were adjudicated
by two of the annotators (one of them being the
expert) and then used as a gold standard to test
classiﬁer performance in the active learning pro-
cess.

4.1 Data and Setup
For each lemma we extracted sentences from the
Wahrig corpus1 containing this particular lemma.
The annotators had to assign word senses to 300
instances for each target word, split into 6 pack-
ages of 50 sentences each. This resulted in 2,400
annotated instances per annotator (14,400 anno-
tated instances in total). The annotation was done

1The Wahrig corpus includes more than 113 mio. sen-
tences from German newspapers and magazines covering
topics such as politics, science, fashion, and others.

952

Anlass
Occasion (37) Motif (47)
Reason(53)
Reason

Motiv

(63)

Konsequenz
Causation (32)
Level of det.(6)
(61)
Response
MWE1
(1)

Ergebnis / Resultat
Quelle
(4/10)
Causation
Relational nat feat.(3)
Competitive score(12/36)
Source of getting (14)
(11/6)
Decision
(14)
Source of exp.
(2/3)
Efﬁcacy
(56)
Source of info.
(24/23)
Well
Finding out
(6)
(1/0)
Emissions source (7) Mathematics
(36/5)
(10/17)

Operating result
Outcome

Ausgang
(67)
Outcome
(4)
Have leave
Portal
(21)
Outgoing goods (4)
(0)
Ostomy
(5)
Origin
Tech output
(7)
(2)
Process end
Departing
(1)

Grund
(24)
Causation
(58)
Reason
Death
(1)
Part orientation. (0)
Locale by owner(3)
(0)
Surface earth
Bottom layer
(0)
(1)
Soil
(0)
CXN1
(0)
CXN2
(0)
MWE1
(10)
MWE2
(0)
MWE3
MWE4
(3)
(0)
MWE5
MWE6
(0)

0.67

0.79

0.55

0.77

0.63 / 0.59

0.82

0.43

Fleiss’ kappa for the 6 annotators for the 150 instances annotated in the random setting

Table 1: 8 causation nouns and their word senses (numbers in brackets give the distribution of word
senses in the gold standard (100 sentences); CXN: constructions, MWE: multi-word expressions; note
that Ergebnis and Resultat are synonyms and therefore share the same set of frames.)

using a Graphical User Interface where the sen-
tence was presented to the annotator, who could
choose between all possible word senses listed in
the GUI. The annotators could either select the
frame by mouse click or use keyboard shortcuts.
For each instance we recorded the time it took
the annotator to assign an appropriate label. To
ease the reading process the target word was high-
lighted.

As we want

to compare time requirements
needed for annotating random samples and sen-
tences selected by active learning, we had to con-
trol for training effects which might speed up the
annotation. Therefore we changed the annotation
setting after each package, meaning that the ﬁrst
annotator started with 50 sentences randomly se-
lected from the pool, then annotated 50 sentences
selected by AL, followed by another 50 randomly
chosen sentences, and so on. We divided the an-
notators into two groups of three annotators each.
The ﬁrst group started annotating in the random
setting, the second group in the AL setting. The
composition of the groups was changed for each
lemma, so that each annotator experienced all dif-
ferent settings during the annotation process. The
annotators were not aware of which setting they
were in.

Pool data For the random setting we randomly
selected three sets of sentences from the Wahrig
corpus which were presented for annotation to all
six annotators. This allows us to compare annota-
tion time and inter-annotator agreement between

the annotators. For the active learning setting we
randomly selected three sets of 2000 sentences
each, from which the classiﬁer could pick new in-
stances during the annotation process. This means
that for each trial the algorithm could select 50 in-
stances out of a pool of 2000 sentences. On any
given AL trial each annotator uses the same pool
as all the other annotators. In an AL simulation
with ﬁxed settings and gold standard labels this
would result in the same subset of sentences se-
lected by the classiﬁer. For our human annotators,
however, due to different annotation decisions the
resulting set of sentences is expected to differ.

Sampling method Uncertainty sampling is a
standard sampling method for AL where new in-
stances are selected based on the conﬁdence of the
classiﬁer for predicting the appropriate label. Dur-
ing early stages of the learning process when the
classiﬁer is trained on a very small seed data set,
it is not beneﬁcial to add the instances with the
lowest classiﬁer conﬁdence. Instead, we use a dy-
namic version of uncertainty sampling (Rehbein
and Ruppenhofer, 2010), based on the conﬁdence
of a maximum entropy classiﬁer2, taking into ac-
count how much the classiﬁer has learned so far.
In each iteration one new instance is selected from
the pool and presented to the oracle. After anno-
tation the classiﬁer is retrained on the new data
set. The modiﬁed uncertainty sampling results in
a more robust classiﬁer performance during early
stages of the learning process.

2http://maxent.sourceforge.net

953

Anlass
U
R
9.6
8.6
5.7
4.4
9.2
9.9
4.9
5.8
3.5
3.0
6.3
5.4
6.2
6.5
25.8 27.8

Motiv
U
R
6.6
5.9
5.9
4.8
6.7
6.8
3.6
3.6
2.6
3.0
4.7
5.3
4.9
5.0
27.8 26.0

Konsequenz
U
10.5
9.2
8.3
11.3
4.9
8.6
8.8
25.8

R
10.7
8.2
6.8
9.9
4.8
6.7
7.8
24.2

Quelle
U
R
4.8
6.0
4.9
4.9
6.1
7.4
3.5
4.8
3.0
3.8
4.6
5.4
5.4
4.5
24.9 26.5

Ergebnis
U
R
7.4
10.5
4.4
6.4
7.6
9.4
7.1
7.9
4.8
6.8
6.1
7.8
8.1
6.2
25.7 25.2

Resultat
U
R
9.6
10.1
11.7
8.5
9.0 12.3
9.7 11.1
6.7
6.1
9.0
8.7
9.3
9.4
29.0 35.9

R

Ausgang
U
6.4 10.0
7.7
5.1
8.5
7.5
4.1
3.6
3.1
3.5
6.6
6.9
5.4
6.7
25.5 27.9

Grund
R

U
10.2 11.1
9.0
9.3
11.7 10.2
9.4
9.9
6.3
6.0
8.5
9.3
9.4
9.1
26.8 29.7

A1
A2
A3
A4
A5
A6
ø
sl

Table 2: Annotation time (sec/instance) per target/annotator/setting and average sentence length (sl)

5 Results
The basic idea behind active learning is to se-
lect the most informative instances for annotation.
The intuition behind “more informative” is that
these instances support the learning process, so we
might need fewer annotated instances to achieve
a comparable classiﬁer performance, which could
decrease the cost of annotation. On the other
hand, “more informative” also means that these
instances might be more difﬁcult to annotate, so it
is only fair to assume that they might need more
time for annotation, which increases annotation
cost. To answer the question of whether AL re-
duces annotation cost or not we have to check a)
how long it took the annotators to assign labels
to the AL samples compared to the randomly se-
lected instances, and b) how many instances we
need to achieve the best (or a sufﬁcient) perfor-
mance in each setting. Furthermore, we want to
investigate the impact of active learning on the
distribution of the resulting training sets and study
the correlation between the performance of the
classiﬁer trained on the annotated data and these
factors: the difﬁculty of the annotation task (as-
sessed by IAA), expertise and individual proper-
ties of the annotators.

5.1 Does AL speed up the annotation process

when working with noisy data?

Table 2 reports annotation times for each annota-
tor and target for random sampling (R) and uncer-
tainty sampling (U). For 5 out of 8 targets the time
needed for annotating in the AL setting (averaged
over all annotators) was higher than for annotat-
ing the random samples. To investigate whether
this might be due to the length of the sentences
in the samples, Table 2 shows the average sen-
tence length for random samples and AL samples

for each target lemma. Overall, the sentences se-
lected by the classiﬁer during AL are longer (26.2
vs. 28.1 token per sentence), and thus may take
the annotators more time to read.3 However, we
could not ﬁnd a signiﬁcant correlation (Spearman
rank correlation test) between sentence length and
annotation time, nor between sentence length and
classiﬁer conﬁdence.

The three target lemmas which took longer to
annotate in the random setting are Ergebnis (re-
sult), Grund (reason) and Quelle (source of expe-
rience). This observation cannot be explained by
sentence length. While sentence length for Ergeb-
nis is nearly the same in both settings, for Grund
and Quelle the sentences picked by the classi-
ﬁer in the AL setting are signiﬁcantly longer and
therefore should have taken more time to anno-
tate. To understand the underlying reason for this
we have to take a closer look at the distribution of
word senses in the data.

5.2 Distribution of word senses in the data
In the literature it has been stated that AL implic-
itly alleviates the class imbalance problem by ex-
tracting more balanced data sets, while random
sampling tends to preserve the sense distribution
present in the data (Ertekin et al., 2007). We could
not replicate this ﬁnding when using noisy data
to guide the learning process. Table 3 shows the
distribution of word senses for the target lemma
Ergebnis a) in the gold standard, b) in the random
samples, and c) in the AL samples.

The variance in the distribution of word senses
in the random samples and the gold standard can

3The correlation between sentence length and annotation
time is not obvious, as the annotators only have to label one
target in each sentence. For ambiguous sentences, however,
reading time may be longer, while for the clear cases we do
not expect a strong effect.

954

Ergebnis
Frame
Causation
Outcome
Finding out
Efﬁcacy
Decision
Mathematics
Operating result
Competitive score

gold (%) R (%) U (%)
3.7
10.5
8.2
0.1
3.2
0.4
66.7
7.2

4.0
10.0
24.0
2.0
11.0
1.0
36.0
12.0

4.8
17.8
26.2
0.8
5.1
1.6
24.5
19.2

Anlass
Motiv
Konseq.
Quelle
Ergebnis
Resultat
Ausgang
Grund

50
0.85
0.57
0.55
0.56
0.39
0.31
0.67
0.48

Random

100
0.86
0.62
0.59
0.53
0.42
0.35
0.69
0.47

150
0.85
0.63
0.60
0.54
0.41
0.37
0.69
0.47

Uncertainty

50
0.84
0.64
0.61
0.52
0.39
0.32
0.68
0.47

100
0.85
0.67
0.62
0.52
0.37
0.34
0.72
0.44

150
0.84
0.70
0.62
0.57
0.38
0.34
0.74
0.48

Table 3: Distribution of frames (word senses) for
the lemma Ergebnis in the gold standard (100 sen-
tences), in the random samples (R) and AL sam-
ples (U) (150 sentences each)

Table 4: Avg. classiﬁer performance (acc.) over
all annotators for the 8 target lemmas when train-
ing on 50, 100 and 150 annotated instances for
random samples and uncertainty samples

be explained by low inter-annotator agreement
caused by the high level of ambiguity for the tar-
get lemmas. The frame distribution in the data
selected by uncertainty sampling, however, cru-
cially deviates from those of the gold standard
and the random samples. A disproportionately
high 66% of the instances selected by the classi-
ﬁer have been assigned the label Operating result
by the human annotators. This is the more sur-
prising as this frame is fairly easy for humans to
distinguish.

The classiﬁer, however, proved to have seri-
ous problems learning this particular word sense
and thus repeatedly selected more instances of this
frame for annotation. As a result, the distribution
of word senses in the training set for the uncer-
tainty samples is highly skewed, having a nega-
tive effect on the overall classiﬁer performance.
The high percentage of instances of the “easy-to-
decide” frame Operating result explains why the
instances for Ergebnis took less time to annotate
in the AL setting. Thus we can conclude that an-
notating the same number of instances on average
takes more time in the AL setting, and that this
effect is not due to sentence length.

5.3 What works, what doesn’t, and why
For half of the target lemmas (Motiv, Konsequenz,
Quelle, Ausgang), we did obtain best results in
the AL setting (Table 4). For Ausgang and Mo-
tiv AL gives a substantial boost in classiﬁer per-
formance of 5% and 7% accuracy, while the gains
for Konsequenz and Quelle are somewhat smaller
with 2% and 1%, and for Grund the highest accu-
racy was reached on both the AL and the random

sample.

Figure 1 (top row) shows the learning curves
for Resultat, our worst-performing lemma, for the
classiﬁer trained on the manually annotated sam-
ples for each individual annotator. The solid black
line represents the majority baseline, obtained by
assigning the most frequent word sense in the gold
standard to all instances. For both random and AL
settings, results are only slightly above the base-
line. The curves for the AL setting show how erro-
neous decisions can mislead the classiﬁer, result-
ing in classiﬁer accuracy below the baseline for
two of the annotators, while the learning curves
for these two annotators on the random samples
show the same trend as for the other 4 annotators.
For Konsequenz (Figure 1, middle), the classi-
ﬁer trained on the AL samples yields results over
the baseline after around 25 iterations, while in
the random sampling setting it takes at least 100
iterations to beat the baseline. For Motiv (Figure
1, bottom row), again we observe far higher re-
sults in the AL setting. A possible explanation for
why AL seems to work for Ausgang, Motiv and
Quelle might be the higher IAA4 (κ 0.825, 0.789,
0.768) as compared to the other target lemmas.
This, however, does not explain the good results
achieved on the AL samples for Konsequenz, for
which IAA was quite low with κ 0.554.

Also startling is the fact that AL seems to work
particularly well for one of the annotators (A6,
Figure 1) but not for others. Different possible ex-
planations come to mind: (a) the accuracy of the
annotations for this particular annotator, (b) the

4IAA was computed on the random samples, as the AL

samples do not include the same instances.

955

Resultat (Random Sampling)

Resultat (Uncertainty Sampling)

y
c
a
r
u
c
c
a

0
4
.
0

0
3
.
0

0
2
.
0

0
1
.
0

Annotator
A1
A2
A3
A4
A5
A6

50
100
no. of iterations

150

0

50
100
no. of iterations

150

Konsequenz (Random Sampling)

Konsequenz (Uncertainty Sampling)

y
c
a
r
u
c
c
a

7

.

0

6

.

0

5

.

0

4

.

0

3

.

0

100
50
no. of iterations

150

0

100
50
no. of iterations

150

Motiv (Random Sampling)

Motiv (Uncertainty Sampling)

y
c
a
r
u
c
c
a

0
7

.

0

0
6

.

0

0
5

.

0

50
100
no. of iterations

150

0

50
100
no. of iterations

150

y
c
a
r
u
c
c
a

y
c
a
r
u
c
c
a

y
c
a
r
u
c
c
a

0
4
.
0

0
3
.
0

0
2
.
0

0
1
.
0

7

.

0

6

.

0

5

.

0

4

.

0

3

.

0

0
7

.

0

0
6

.

0

0
5

.

0

0

0

0

Figure 1: Active learning curves for Resultat, Konsequenz and Motiv (random sampling versus uncer-
tainty sampling; the straight black line shows the majority baseline)

956

Konsequenz A1
human
0.80
0.60
maxent

A2
0.72
0.63

A3
0.89
0.67

A4
0.73
0.60

A5
0.89
0.63

A6
0.76
0.64

Table 5: Acc. for human annotators against the
adjudicated random samples and for the classiﬁer

instances selected by the classiﬁer based on the
annotation decisions of the individual annotators,
and (c) the distribution of frames in the annotated
training sets for the different annotators.

To test (a) we evaluated the annotated ran-
dom samples for Konsequenz for each annotator
against the adjudicated gold standard. Results
showed that there is no strong correlation between
the accuracy of the human annotations and the
performance of the classiﬁer trained on these an-
notations. The annotator for whom AL worked
best had a medium score of 0.76 only, while the
annotator whose annotations were least helpful
for the classiﬁer showed a good accuracy of 0.80
against the gold standard.

Next we tested (b) the impact of the particu-
lar instances in the AL samples for the individ-
ual annotators on classiﬁer performance. We took
all instances in the AL data set from A6, whose
annotations gave the greatest boost to the clas-
siﬁer, removed the frame labels and gave them
to the remaining annotators for re-annotation.
Then we trained the classiﬁer on each of the re-
annotated samples and compared classiﬁer perfor-
mance. Results for 3 of the remaining annotators
were in the same range or even higher than the
ones for A6 (Figure 2). For 2 annotators, however,
results remained far below the baseline.

This again shows that the AL effect is not di-
rectly dependent on the accuracy of the individual
annotators, but that particular instances are more
informative for the classiﬁer than others. Another
crucial point is (c) the distribution of frames in
the samples. In the annotated samples for A1 and
A2 the majority frame for Konsequenz is Causa-
tion, while in the samples for the other annotators
Response was more frequent. In our test set Re-
sponse also is the most frequent frame, therefore it
is not surprising that the classiﬁers trained on the
samples of A3 to A6 show a higher performance.
This means that high-quality annotations (identi-
ﬁed by IAA) do not necessarily provide the in-

Konsequenz: Re−annotated samples

7
0

.

6

.

0

y
c
a
r
u
c
c
a

.

5
0

4

.

0

3

.

0

0

Annotator
A1
A2
A3
A4
A5
A6

50
100
no. of iterations

150

Figure 2: Re-annotated instances for Konsequenz
(AL samples from annotator A6)

formation from which the classiﬁer beneﬁts most,
and that in a realistic annotation task address-
ing the class imbalance problem (Zhu and Hovy,
2007) is crucial.

6 Conclusions

We presented the ﬁrst experiment applying AL in
a real-world scenario by integrating the approach
in an ongoing annotation project. The task and
annotation environment pose speciﬁc challenges
to the AL paradigm. We showed that annotation
noise caused by biased annotators as well as erro-
neous annotations mislead the classiﬁer and result
in skewed data sets, and that for this particular task
no time savings are to be expected when applied
to a realistic scenario. Under certain conditions,
however, classiﬁer performance can improve over
the random sampling baseline even on noisy data
and thus yield higher accuracy in the active learn-
ing setting. Critical features which seem to inﬂu-
cence the outcome of AL are the amount of noise
in the data as well as the distribution of frames
in training- and test sets. Therefore, addressing
the class imbalance problem is crucial for apply-
ing AL to a real annotation task.

957

Osborne, Miles and Jason Baldridge.

2004.
Ensemble-based active learning for parse selection.
In Proceedings of HLT-NAACL 2004.

Rehbein, Ines and Josef Ruppenhofer. 2010. Theres
no data like more data? revisiting the impact of
data size on a classiﬁcation task.
In Proceedings
of LREC-07, 2010.

Reichart, Roi, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for
linguistic annotations. In Proceedings of ACL-08:
HLT.

Ringger, Eric, Peter Mcclanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, and
Deryle Lonsdale. 2007. Active learning for part-
of-speech tagging: Accelerating corpus annotation.
In Proceedings of ACL Linguistic Annotation Work-
shop.

Tomanek, Katrin and Udo Hahn. 2009. Reducing
class imbalance during active learning for named
entity annotation. In Proceedings of the 5th Interna-
tional Conference on Knowledge Capture, Redondo
Beach, CA.

Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data.
In Proceedings of
EMNLP-CoNLL 2007.

Zhu, Jingbo and Ed Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proceed-
ings of EMNLP-CoNLL 2007.

Zhu, Jingbo, Huizhen Wang, Tianshun Yao, and Ben-
jamin K. Tsou. 2008. Active learning with sam-
pling by uncertainty and density for word sense dis-
ambiguation and text classiﬁcation. In Proceedings
of Coling 2008.

Acknowledgments
This work was funded by the German Research
Foundation DFG (grant PI 154/9-3 and the MMCI
Cluster of Excellence).

References

Baldridge, Jason and Alexis Palmer. 2009. How well
does active learning actually work?: Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP 2009.

Burchardt, Aljoscha, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pad´o, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics.
In Proceedings of LREC-
2006.

Chan, Yee Seng and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of ACL-2007.

Chen, Jinying, Andrew Schein, Lyle Ungar, and
Martha Palmer. 2006. An empirical study of the
behavior of active learning for word sense disam-
biguation.
In Proceedings of NAACL-2006, New
York, NY.

Cohn, David A., Zoubin Ghahramani, and Michael I.
Jordan. 1995. Active learning with statistical mod-
els. In Tesauro, G., D. Touretzky, and T. Leen, ed-
itors, Advances in Neural Information Processing
Systems, volume 7, pages 705–712. The MIT Press.

Dang, Hoa Trang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
PhD dissertation, University of Pennsylvania, Penn-
sylvania, PA.

Donmez, Pinar and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles.
In Proceedings of
CIKM08.

Erk, Katrin. 2005. Frame assignment as word sense

disambiguation. In Proceedings of the IWCS-6.

Ertekin, S¸ eyda, Jian Huang, L’eon Bottou, and Lee
Giles. 2007. Learning on the border: active learn-
ing in imbalanced data classiﬁcation. In Proceed-
ings of CIKM ’07.

Hwa, Rebecca. 2004. Sample selection for statisti-
cal parsing. Computational Linguistics, 30(3):253–
276.

Laws, Florian and Heinrich Sch¨utze. 2008. Stopping
criteria for active learning of named entity recogni-
tion. In Proceedings of Coling 2008.

