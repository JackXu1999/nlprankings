



















































Text2Math: End-to-end Parsing Text into Math Expressions


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5327–5337,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5327

Text2Math: End-to-end Parsing Text into Math Expressions

Yanyan Zou and Wei Lu
StatNLP Research Group

Singapore University of Technology and Design
yanyan zou@mymail.sutd.edu.sg, luwei@sutd.edu.sg

Abstract

We propose Text2Math, a model for seman-
tically parsing text into math expressions. The
model can be used to solve different math
related problems including arithmetic word
problems (Roy and Roth, 2017; Liang et al.,
2018) and equation parsing problems (Roy
et al., 2016). Unlike previous approaches, we
tackle the problem from an end-to-end struc-
tured prediction perspective where our algo-
rithm aims to predict the complete math ex-
pression at once as a tree structure, where min-
imal manual efforts are involved in the pro-
cess. Empirical results on benchmark datasets
demonstrate the efficacy of our approach.

1 Introduction

Designing computer algorithms that can automat-
ically solve math word problems is a challenge
for the AI research community (Bobrow, 1964).
Two representative tasks have been proposed and
studied recently – solving arithmetic word prob-
lems (Wang et al., 2017; Roy and Roth, 2018; Zou
and Lu, 2019b) and equation parsing (Roy et al.,
2016), as illustrated in Figure 1. The former task
focuses on mapping the input paragraph (which
may involve multiple sentences) into a target math
expression, from which an answer can be calcu-
lated. The latter task focuses on mapping a de-
scription (usually a single sentence) into a math
equation that typically involves one or more un-
knowns. As we can observe from Figure 1, in both
cases, the output can be represented as a tree struc-
ture.

Earlier approaches to solving arithmetic word
problems focused on rule-based methods where
hand-crafted rules have been used (Mukherjee
and Garain, 2008; Hosseini et al., 2014). Re-
cently, learning-based approaches based on statis-
tical classifiers (Kushman et al., 2014; Roy and
Roth, 2015; Roy et al., 2016; Liang et al., 2018) or

Problem 1
Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6
apples at the farm. In total, how many apples were picked?
Expressionion (7 + (3 + 6))

+

7 +

3 6

Answerioniion 16
Problem 2
3 times one of the numbers is 11 less than 5 times the other.
Expressionionio (3×X1) = (5×X2)− 11

=

−

× 11

5 X2

×

3 X1

Figure 1: An example arithmetic word problem (top)
and an example equation parsing problem (bottom)
where the outputs can be represented as trees.

neural networks (Wang et al., 2017, 2018b) have
been used for making decisions in the expression1

construction process. However, these models do
not focus on predicting the target tree as a com-
plete structure at once, but locally trained classi-
fiers are often used and local decisions are then
combined. Such local classifiers often make pre-
dictions on the choice of the underlying operator
between two operands (e.g., numbers) appearing
in the text in a particular order. As a result, spe-
cial treatments of the non-commutative operators
such as subtraction (−) and division (÷) are often
involved, where the introduction of inverse opera-
tors is typically required2. Shi et al. (2015) tack-
led the problem from a structured prediction per-
spective, where a semantic parsing algorithm us-

1In this work, we use the term expression to refer to a math
expression (for arithmetic word problem) or an equation (for
equation parsing).

2For example, the inverse operator −i applied to two
operands a followed by b is used to denote b− a.



5328

ing context-free grammars (CFG) was used. How-
ever, their approach relies on semi-automatically
generated rules and involves a manual step for in-
terpreting the semantic representation they used.

While all these approaches focused on solving
arithmetic word problems only, separate models
have been developed for the task of equation pars-
ing (Roy et al., 2016). It is not clear how easy
each of these models specifically designed for one
task can be adapted for the other task. Moti-
vated by the observation that both problems in-
volve mapping a text sequence to a tree structured
representation, we propose Text2Math which
regards both tasks as a class of structured predic-
tion problems, and tackle them from a semantic
parsing perspective. We make use of an end-to-
end latent-variable approach to automatically pro-
duce the target math expression at once as a com-
plete structure, where no prior knowledge on the
operators (such as whether an operator is non-
commutative) is required. Our model outperforms
all baselines on two benchmark datasets. To the
best of our knowledge, this is the first approach
based on semantic parsing that tackles both arith-
metic word problems and equation parsing with
a single model. Our code is available at http:
//statnlp.org/research/ta.

2 Approach

2.1 Expression Tree

We first define tree representations for math ex-
pressions, which will then be regarded as the se-
mantic representations used in the standard se-
mantic parsing setup.

The nodes involved in the math expression trees
can be classified into two categories, namely, op-
erator and quantity nodes. Specifically, operator
nodes are the tree nodes that define the types of
operations involved in expressions. In this work
we consider ADD (addition, +), SUB (subtraction,
−), MUL (multiplication, ×) and DIV (division,
÷). We also regard the equation sign (=) as an op-
eration involved in math expressions and use EQU
to denote it. We consider two types of quantity
nodes: CON denoting constants, and VAR for un-
known variables. Table 1 lists the above nodes.
Each tree node comes with an arity which spec-
ifies the number of direct child nodes that should
appear below the given node. For example, the op-
erator node SUB with arity 2 is expecting two child
nodes below it in the expression tree, while CON

Category Node Interpretation Arity

Operator

EQU ni EQU nj ⇔ (ni = nj) 2
ADD ni ADD nj ⇔ (ni + nj) 2
SUB ni SUB nj ⇔ (ni − nj) 2
MUL ni MUL nj ⇔ (ni × nj) 2
DIV ni DIV nj ⇔ (ni ÷ nj) 2

Quantity
CON A constant 0
VAR A variable 0

Table 1: Expression tree nodes with interpretations,
where ni (nj) refers to the first (second) operand.

Math expression (7 + (3 + 6))
Order in text (7, 3, 6)

Expression tree

ADD(+)

CON(7) ADD(+)

CON(3) CON(6)

Math expression (3×X1) = (5×X2)− 11
Order in text (3, X1, 11, 5, X2)

Expression tree

EQU(=)

SUB(−)

MUL(×)

CON(5) VAR(X2)

CON(11)

MUL(×)

CON(3) VAR(X1)

Figure 2: Expression trees for the two math expressions
in Figure 1. “Order in text” refers to the order that the
textual expressions of operands appear in the problem
text. We use subscripts to indicate the actual semantic
interpretations.

with arity 0 is supposed to be a leaf node. The two
math expressions in Figure 1 can be equivalently
represented by expression trees consisting of such
nodes, as illustrated in Figure 2.

2.2 Latent Text-Math Tree

With the specifically designed expression trees for
representing the math expressions, we will now be
able to design a model for parsing the text into
the expression tree. This is essentially a seman-
tic parsing task. One of the key assumptions made
by the various semantic parsing algorithms is the
intermediate joint representation used for connect-
ing the words and semantics (Wong and Mooney,
2006; Zettlemoyer and Collins, 2007; Lu et al.,
2008; Artzi and Zettlemoyer, 2013b). In this work,
we adopt an approach that is inspired by (Lu et al.,
2008; Lu, 2014), which learns a latent joint rep-
resentation for words and semantics in the form
of hybrid trees where word-semantics correspon-
dence information is captured. Specifically, we in-
troduce a text-math tree representation that jointly
encodes both text and the math expression tree.

http://statnlp.org/research/ta
http://statnlp.org/research/ta


5329

Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6 apples at the farm. In total, how many apples were picked?

ADD(+)
(wAwBw)

{Mike picked}A {.} B {In total, how many apples were picked?}

ADD(+)
(wAwBw)

{Nancy picked}A {and Keith picked} B {at the farm.}

CON(6)
(w)

{6 apples}

CON(3)
(w)

{3 apples}

CON(7)
(w)

{7 apples}

ADD(+)
(wAwBw)

{Mike picked}A {. Nancy picked} B {apples were picked?}

ADD(+)
(ABw)

AB {at the farm. In total, how many}

CON(6)
(w)

{6 apples}

CON(3)
(w)

{3 apples and Keith picked}

CON(7)
(w)

{7 apples}

Figure 3: Example text-math trees for the arithmetic word problem example in Figure 1. The left tree captures the
semantic correspondence well, while the right tree fails to capture the correct correspondence.

Such joint representations can be understood as
a modified expression tree where each semantic
node is now augmented with additional word in-
formation from the corresponding text.

From the joint representations we would be able
to recover the semantic level correspondence in-
formation between words and math expressions.
Possible joint representations of the two examples
in Figure 1 are illustrated in Figure 3 and Figure
4 (left), respectively. Consider Problem 1 in Fig-
ure 1. We illustrate two possible text-math expres-
sion trees in Figure 3. Here each node in such
joint representations is essentially a node in the
original expression trees augmented with words
from the problem text. For example, consider
the left tree in Figure 3, the root node is an op-
erator (the root node of the original expression
tree) paired with discontiguous sequence of words
“{Mike picked}. . . {.}. . . {In total, how many ap-
ples were picked?}” that appear in the problem
text. This way, such a text-math tree is able to cap-
ture the semantic correspondence between words
and basic units involved in the math expressions
(i.e., operators, quantities). However, the text-
math trees are not explicitly given during the train-
ing phase. For example, the right side of Figure
3 gives an alternative text-math tree that can also
serve as a joint representation of both the text and
the expression tree. Comparing both trees we may
see the one on the left appears to be better at cap-
turing the true semantic level correspondence be-
tween words and math expressions. Since there is
no gold text-math tree explicitly given, we model
it with a latent-variable approach.

Formally, given a text x, paired with the expres-
sion y (or equivalently, the expression tree), we
assume there exists a latent joint text-math repre-
sentation in the form of text-math tree, that com-
prises exactly x and y, denoted as t. Each node

is a word-semantics association 〈x, y, p〉 where x
is a (possibly discontiguous) word sequence of x
and y is an individual expression tree node from y,
and p is the word association pattern that is used
to specify how words interact with the expression
tree (further details will be provided in Sec. 2.3).
Intuitively, such a joint text-math representation
should precisely contain the exact information as-
sociated with the text and its corresponding math
expression and nothing else. We will defer the dis-
cussion on how to exactly construct such joint rep-
resentations until Sec. 2.3.

The training corpus provides both the problem
text x and its math expression, which we represent
with an expression tree y. The joint representation
t is not available in the training data, which we
model as a latent variable. The conditional random
fields (CRF) (Lafferty et al., 2001) has been suc-
cessfully applied to many tasks in the NLP com-
munity (Lample et al., 2016; Zou and Lu, 2018,
2019a). In this work, we also apply CRF to model
the conditional probability of the latent variable t
and output expression y, conditioned on the input
x. The objective is defined as follows:

PΛ,Θ(y|x) =
∑

t∈T (x,y)

PΛ,Θ(y, t|x)

=

∑
t∈T (x,y) e

[Λ·Φ(x,y,t)+GΘ(x,y,t)]∑
y′,t′∈T (x,y′) e

[Λ·Φ(x,y′,t′)+GΘ(x,y′,t′)]
(1)

where Φ(x,y, t) returns a list of discrete features
defined over the tuple (x,y, t), Λ is the feature
weight vector, GΘ is a neural scoring function pa-
rameterized by Θ and T (x,y) is a set of possible
joint representations (i.e., text-math trees) for the
pair (x,y).



5330

3 times one of the numbers is 11 less than 5 times the other.

EQU(=)
(AwB)
A {is} B

SUB(−)
(BwA)

B {less than}A

CON(11)
(w)
{11}

MUL(×)
(AwB)

A {times} B

VAR(X2)
(w)

{the other.}

CON(5)
(w)
{5}

MUL(×)
(AwB)

A {times} B

VAR(X1)
(w)

{one of the numbers}

CON(3)
(w)
{3}

(3×X1) = (5×X2)− 11

3 times one of the numbers is 11 minus 5 times the other.

EQU(=)
(AwB)
A {is} B

SUB(−)
(AwB)

A {minus} B

MUL(×)
(AwB)

A {times} B

VAR(X2)
(w)

{the other.}

CON(5)
(w)
{5}

CON(11)
(w)
{11}

MUL(×)
(AwB)

A {times} B

VAR(X1)
(w)

{one of the numbers}

CON(3)
(w)
{3}

(3×X1) = 11− (5×X2)

Figure 4: Left: example text-math tree for the equation parsing example in Figure 1, where the word association
pattern BwA in the example is used for modeling reordering. Right: another example for a slightly different
instance where the reordering is not required.

2.3 Inference

One challenge associated with the inference pro-
cedure in both training and decoding is how to
handle the large space of latent structures defined
by T (x, y) and T (x). Without any constraints,
searching or calculation that involves all possible
structures within this space may be intractable. We
therefore introduce some assumptions on the set of
allowable structures, such that tractable inference
can be applied to such structures.

We first introduce three symbols A, B and w.
The symbol A refers to a placeholder for the left
sub-tree (rooted by the left child node), and simi-
larly B is a placeholder for the right sub-tree. The
symbol w refers to a contiguous sequence of (1 or
more) words. We will then use these three sym-
bols to define the set of word association patterns,
which are used to specify how the words interact
with the sub-trees of the current node. Specifi-
cally, for expression tree nodes with arity 0 (i.e.,
quantity nodes), only one pattern w is allowed to
be attached to them, indicating that a contiguous
word sequence from a given problem text is as-
sociated with such expression tree nodes. As for
expression tree nodes with arity 2 (i.e., operator
nodes), we define 16 allowable patterns denoted
as {[w]A[w]B[w], [w]B[w]A[w]}, where [] de-
notes optional. Based on such word association
patterns, we will be able to define a set of possible
text-math trees for a particular text-expression pair
that we regard as valid.

Before we formally define what is a valid text-
math tree, let us look at an example in Figure 3
which shows two valid trees. First of all, we can
verify that, if we exclude the words from both
trees, we arrive at the math expression that corre-

sponds to the text. Second, we can also recover the
text information from such a joint representation.
Let us look at the right tree in Figure 3. Consider
the right sub-tree of the complete tree rooted by
the node 〈x, y, p〉 = 〈{at the farm. In total, how
many}, ADD, ABw〉. If we replace the place-
holders A and B with the word sequences associ-
ated with its left and right sub-trees, respectively,
we will arrive at the word sequence “3 apples and
Keith picked 6 apples at the farm. In total, how
many”. Recursively performing such a rewriting
procedure in a bottom-up manner, we will end up
with a word sequence which is exactly the original
input text as illustrated at the top of Figure 3.

Based on the above discussion, we can define
T (x, y) as a set that consists of the valid trees:
Definition 2.1 For a given text x and an expres-
sion tree y, a valid text-math tree satisfies the fol-
lowing two properties: 1) the semantics portion
of the tree gives exactly y, and 2) the text ob-
tained through the recursive rewriting procedure
discussed above gives exactly x3.

Given the definition of the valid text-math trees,
we will be able to use a bottom-up procedure to
construct the set T (x,y). Similarly, we will be
able to construct the set T (x) by considering a
forest-structured semantic representation that en-
codes all possible expression trees following (Lu,
2015). One nice property associated with con-
sidering only such joint representations is that
there are known algorithms that can be used for
performing efficient inference. Indeed, the re-
sulting text-math trees are similar to the hybrid
tree representations used in (Lu et al., 2008; Lu,

3We regard words that appear at different positions in x
as distinct words, regardless of their string forms.



5331

2014)4, where dynamic programming based infer-
ence algorithms have been developed. Such algo-
rithms allow O(n3m) time complexity for infer-
ence where n is the text length and m is the num-
ber of grammar rules5 associated with the latent
text-math trees.

We note that some prior systems (Roy and Roth,
2017, 2018) require extra inverse operators – in-
verse subtraction “−r” and inverse division “÷r”
to handle the scenarios where the order of quanti-
ties appearing in the text is not consistent with the
order that they appear in the expression. Exem-
plified by the example in the left of Figure 4, by
introducing two operators −r and ÷r to take their
operands in a reverse order, the equation on the left
is represented as “(3 × X1) = 11 −r (5 × X2)”.
However, we do not need such two inverse oper-
ators. The two group patterns [w]A[w]B[w] and
[w]B[w]A[w] are capable to capture both orders.
A pattern from the first group handles the order
that is consistent with the problem text, while a
pattern from the second group is able to capture
reordering of operands below an operator. Ex-
emplified by Figure 4, reordering is required for
the first example, but not for the second, though
their texts only differ slightly. Unlike the second
example, instead of using the pattern AwB, the
first joint representation adopts the pattern BwA
for the SUB expression node. Thus, our model is
able to work without the underlying knowledge on
whether an operator is commutative or not.

2.4 Features

Discrete Features. The feature function
Φ(x,y, t) is defined over each node 〈x, y, p〉 in
the joint tree as well as the complete expression
tree y. For each node 〈x, y, p〉, we extract word n-
gram, the word association pattern, and POS tags
for words (Manning et al., 2014). The knowledge
that whether a number is relevant to the question
(if available in the annotated data) is also taken
as a binary feature. To assess the quality of the
structure associated with the expression tree (i.e.,
features defined over y), we extract parent-child
relational information (ya, yb) from y, where ya is

4They need to handle semantic nodes with arity 1 (which
requires special constraints for properly defining T (x) (Lu,
2015)), and their semantic nodes are also assumed to con-
vey semantic type information for guiding the expression tree
construction process, while we do not need to consider them.

5The grammars are related to the word association pat-
terns. The possible latent text-math trees are constructed
based on such grammar rules.

the parent of yb, as features. Following previous
works (Roy et al., 2016; Liang et al., 2018), we
also consider incorporating a lexicon in our model
so as to make a fair comparison with such works,
although we would like to stress that our model
does not strictly require such lexicons for learn-
ing. More details are in supplementary material.
Neural Features. We design neural features over
the pair of the L-sized window surrounding the
target word xi in x and an expression tree node
yj . The network takes as input the contiguous
word sequence (xi−L, . . . , xi, . . . , xi+L), whose
distributed representation is a simple concatena-
tion of embeddings of each word. The hidden
layer applies an affine transformation with an
element-wise nonlinear activation function, like
tanh and ReLU. The final output layer contains
as many nodes as there are expression tree nodes
in the training set. The output is a score vec-
tor that gives a score for the input word sequence
(xi−L, . . . , xi, . . . , xi+L) and an expression tree
node yj . The neural scoring function is defined
as follows:

GΘ(x,y, t) =∑
(x,y)∈W(x,y,t)

c(x, y,x,y, t)× ψ(x, y)

where W(x,y, t) is the set of (x, y) pairs ex-
tracted from (x,y, t), c returns the number of oc-
currences and ψ(x, y) is a score of the target word
x with L-sized windows and the expression tree
node y, returned by the neural network. We regard
L as a hyperparameter.

2.5 Algorithms
Given the complete training set, the log-likelihood
is calculated as:

L(Λ,Θ) =
∑
i

logPΛ,Θ(yi|xi)

=
∑
i

log
∑

t∈T (xi,yi)

PΛ,Θ(yi, t|xi) (2)

where (xi,yi) refers to i-th instance in the training
set. The additional L2 regularization term can be
introduced to avoid over-fitting. Here, we omit it
for brevity.

The goal is to find optimal model parameters,
i.e., Λ and Θ, which maximize the objective. We
first consider the computation of gradients for Λ.
Assuming Λ = 〈λ1, λ2, . . . , λN 〉, to learn the op-
timal feature weight values, we can calculate the



5332

gradient for each λk in Λ as:

∂L(Λ,Θ)
∂λk

=
∑
i

∑
t

EPΛ,Θ(t|xi,yi)[φk(xi,yi, t)]

−
∑
i

∑
y,t

EPΛ,Θ(y,t|xi)[φk(xi,y, t)] (3)

where φk(x,y, t) is the number of occurrences for
the k-th feature extracted from (x,y, t).

We then compute the gradient for the neural net-
work parameters Θ. For an input word window x
and a semantic unit y, the gradient is defined as:

∂L(Λ,Θ)
∂ψ(x, y)

=
∑
i

∑
t

EPΛ,Θ(t|xi,yi)[c(x, y,xi,yi, t)]

−
∑
i

∑
y,t

EPΛ,Θ(y,t|xi)[c(x, y,xi,y, t)] (4)

The gradients (3,4) can be efficiently calculated
by applying a generalized forward-backward al-
gorithm, which allows us to conduct exact infer-
ence using the dynamic programming algorithm
described in (Lu, 2014). Next, standard methods
like gradient descent, L-BFGS (Liu and Nocedal,
1989) can be used to find optimal values for model
parameters.

During decoding, the optimal equation tree y∗

for a new input x can be calculated by:

y∗ = arg max
y

P (y|x)

= arg max
y

∑
t∈T (x,y)

eFΛ,Θ(x,y,t)

≈ arg max
y,t∈T (x,y)

eFΛ,Θ(x,y,t) (5)

where T (x,y) refers to the set of all possible text-
math trees that contain x and y.

Instead of directly computing the summation
over all possible latent text-math structures, we es-
sentially replace the

∑
by the max operation in-

side the arg max. In other words, we first find the
latent text-math tree t∗ which yields the highest
score and contains the input text x. Then, the op-
timal expression tree y∗ can be automatically ex-
tracted from t∗.

An efficient dynamic programming based infer-
ence algorithm similar to the work of Lu (2014)
was leveraged to find the optimal latent structure
t∗. We then obtain the optimal expression tree y∗

from t∗, which is the output of our system for the
input problem text x.

2.6 Comparisons with Roy et al. (2016)

It is worth noting that Roy et al. (2016) also pro-
posed a system that maps text into an equation
tree. Unlike this work that maps math problem
texts into math expressions in an end-to-end fash-
ion, Roy et al. (2016) designed three classifiers
which sequentially make local decisions, namely
identifying relevant numbers, recognizing possi-
ble variables and producing equations. They also
require extra inverse operators to handle the non-
commutative operation issues, which is not neces-
sary for our model. Generating equations via a se-
quence of local classification decisions may prop-
agate errors and even limit the ability to wholis-
ticly understand the underlying semantics of prob-
lem texts which is important for predicting cor-
rect mathematical operations. In this work, we re-
gard equation parsing problem as a structure pre-
diction task that allows to parse the text to equa-
tions from a semantic parsing perspective. More-
over, Text2Math is capable to handle both tasks
of equation parsing and arithmetic word problems,
while the system of Roy et al. (2016) is specific to
equation parsing.

3 Experiments

Datasets. Following prior works (Roy and
Roth, 2015; Liang et al., 2018), we focus on
two commonly-used benchmark datasets for arith-
metic word problems, AI2 (Hosseini et al., 2014)
and IL (Roy and Roth, 2015). We consider math-
ematical relations among numbers and calculate
numerical values of the predicted expressions. For
equation parsing, we also evaluate our model on
the data released by (Roy et al., 2016). A pre-
dicted equation is regarded as a correct one if it
is mathematically equivalent to the gold equation.

3.1 Empirical Results

Arithmetic Word Problem. Following previ-
ous work (Liang et al., 2018), we conduct 3-fold
and 5-fold cross-validation on AI2 and IL, respec-
tively, and report the accuracy scores, as shown in
Table 2. Our method achieves competitive results
on AI2 and IL. Overall, it performs better than pre-
vious systems in terms of average scores.

Ablation tests have been done to investigate
the effectiveness of different components, such
as POS tags, the lexicon and number relevance,
as indicated by “-POS”, “-LEX”, “-ID” in Table
2. By eliminating POS tag features, we achieve



5333

System AI2 IL Average
∗Liang et al. (2018) (Statistical) 81.5 81.0 81.25
∗Liang et al. (2018) (DNN) 69.8 70.6 70.20
∗Roy and Roth (2017) 76.2 71.0 73.60
Roy and Roth (2015) 78.0 73.9 75.95
Koncel-Kedziorski et al. (2015) 52.4 72.9 62.65
Roy et al. (2015) - 52.7 -
Kushman et al. (2014) 64.0 73.7 68.85
Hosseini et al. (2014) 77.7 - -

NON-NEURAL

Text2Math 85.8 80.4 83.10
12-POS 86.0 81.0 83.50
12-LEX 76.8 69.4 73.10
12-ID 75.4 78.1 76.75

NEURAL

L = 0 84.8 79.7 82.25
L = 1 84.5 80.3 82.40
L = 2 85.5 80.3 82.90
L = 3 86.2 80.9 83.55
L = 4 85.5 80.0 82.75
L = 5 85.2 81.4 83.30
L = 6 86.5 81.0 83.75

Table 2: Arithmetic Word Problem: Accuracy (%)
on the two benchmark datasets. (-POS, -LEX, -ID mean
that the model excludes POS tags feature, lexicon, or
the number relevance feature.) ∗ indicates model uses
prior knowledge, such as lexcion and inference rules.

new state-of-the-art results on two datasets, which
shows POS tag features do not appear to be help-
ful in this case. Without using lexicon, the per-
formance drops a lot as expected, but the results
are still comparable with most previous systems.
These figures demonstrate the effectiveness of the
lexicon. It is worth noting that the work of Liang
et al. (2018) that achieve previous state-of-the-art
results leverage inference rules during the infer-
ence phase. Their approach can be regarded as
a different way of using the lexicon similar to
ours. We also consider the effects of neural fea-
tures (see Sec. 2.4) with different window sizes
L ∈ {0, 1, 2, 3, 4, 5, 6}. According to empirical
results, a larger window size tends to give better
results. One possible reason is that an arithmetic
word problem often consists of several sentences,
where a large word window is required to capture
mathematical semantics.

Equation Parsing. We compare our model with
previous work (Roy et al., 2016) on the equation
parsing dataset, as shown in Table 3. Our method
yields competitive results. Unlike the work of
Roy et al. (2016), annotations of unknown vari-
ables are not required in our model. As reported
in (Roy et al., 2016), they trained SPF (Artzi and
Zettlemoyer, 2013a), a publicly available seman-
tic parser, with sentence-equation pairs and a seed
lexicon for mathematical terms. But it only ob-
tained 3.1% accuracy. The result taken from (Roy
et al., 2016) shows that it might be difficult for
such a semantic parser in handling the equation

System Equation
∗Roy et al. (2016) (Pipeline) 71.3
∗Roy et al. (2016) (Joint) 60.9
∗ SPF (Artzi and Zettlemoyer, 2013a) 03.1

NON-NEURAL

Text2Math 71.4
12-POS 69.1
12-LEX 71.4
12+G 73.2
12+G-LEX 73.2
L = 0 71.4
L = 1 71.9
L = 2 73.8

NEURAL L = 3 73.5
L = 4 74.5
L = 5 74.0
L = 6 73.2

Table 3: Equation Parsing: Accuracy (%) on equation
parsing dataset. -POS: without POS tag features; -LEX:
without Lexicon; +G: with gold identification for num-
bers. ∗ indicates model uses lexicon. Result of SPF
(Artzi and Zettlemoyer, 2013a) is taken from Roy et al.
(2016).

parsing task even with a high precision lexicon.
One possible reason is that mapping text into a
math equation is essentially a structure prediction
problem. Our model is capable to make guaran-
teed decisions from a structure prediction perspec-
tive. Different from arithmetic word problems,
where numbers are explicitly given in the form
of digits, some texts from equation parsing cor-
pus describe numbers in string forms. Hence, a
structured predictor is used to identify the num-
bers in the sentence, which achieves 95.3% ac-
curacy. The identifications of numbers are taken
as features. We also consider the gold label of
numbers, indicated by (+G). The performance im-
proves a lot, which shows that the accurate identi-
fication of numbers is necessary to in order to ob-
tain a good performance. By removing POS tag
features, there is a slight drop in accuracy. On
the other hand, it is worth noting that even with-
out the high precision lexicon, our model can still
achieve new state-of-the-art accuracy in this task,
while the previous work (Roy et al., 2016) always
requires a high precision lexicon to boost perfor-
mance. Incorporating neural features leads to new
state-of-the-art accuracy of 74.5% when L = 4.
Expression Construction. In arithmetic word
problems, the expression consists of several num-
bers only, exemplified by Problem 1 in Figure 1.
In practice, an unknown variable X, representing
the goal that the problem aims to calculate, can
be appended to the expression to form an equa-
tion. We further investigate two constructions:
appending the unknown variable X to the begin-
ning or to the end of an expression. Results are
listed in the first block of Table 4. For instance,



5334

Variants AI2 IL Average
Text2Math 85.8 80.4 83.10
Prefix X 84.3 81.0 82.65
Suffix X 84.3 81.2 82.75
Text2Math + Inverse 86.3 80.1 83.20
Prefix X + Inverse 84.0 79.9 80.45
Suffix X + Inverse 84.0 80.5 82.25

Table 4: Performance of different constructions for ex-
pression of arithmetic word problems and the effects of
incorporating inverse operators.

two new constructions of the running example are
X = (29 + (16 + 20)) as indicated by “Prefix X”,
and (29 + (16 + 20)) = X reported as “Suffix X”.
It is interesting that including anX and its position
influences the performance. Overall, excluding X
works the best which is adopted in this work.
Inverse Operators. As we discussed in Sec.
2.3, one distinct advantage of our approach, as
compared to others, is that we do not need in-
verse operators, such as “−r” and “÷r”. Our
designed word association patterns are capable
to handle the reordering issue. Here, we con-
sider model variants by introducing two inverse
expression tree nodes, SUBr and DIVr, to repre-
sent “−r” and “÷r”, respectively. Empirical re-
sults, reported in the second block in Table 4, show
that Text2Math (without including inverse oper-
ators) can obtain comparative results compared to
the model variants with inverse operators. These
results confirm that our model does not require ad-
ditional knowledge of the semantics of operands,
which is a unique property of our approach.

3.2 Qualitative Analysis

Output Comparisons. Equation parsing is more
challenging than arithmetic word problems, since
it requires generating unknown variables mapped
to phrases residing in the text. We analyze output
of this task to investigate the source that leads to
better performance. Comparing predictions made
by Pipeline (Roy et al., 2016) and our approach,
we found that Text2Math can better capture the
meaning of the problem text. We illustrate two ex-
amples in Table 5. The Pipeline approach fails to
capture the meaning of “rises to 36% from 3.4%”
which implies subtraction of two numbers, while
our model is capable to capture such knowledge.
In the second example, Pipeline misunderstands
the meaning of “five more than three”, although
it seems correct in a local context. However, an
equation should be mapped from the complete
sentence that captures mathematical relations in a

Input: Japan January jobless rate rises to 3.6%
from 3.4%.

Gold: X1 = 0.036− 0.034
Pipeline: X1 × 0.036 = 0.034
Text2Math: X1 = 0.036− 0.034
Input: The number of baseball cards he has is five

more than three times the number of foot-
ball cards.

Gold: X1 = 5 + (3×X2)
Pipeline: X1 ×X2 = 5− 3
Text2Math: X1 = 5 + (3×X2)

Table 5: Comparison between predictions made by the
previous state-of-the-art system (Roy et al., 2016) (de-
noted as Pipeline) and Text2Math.

global perspective. Our model holds such a ca-
pability and makes more guaranteed predictions,
which proves the efficacy of solving math prob-
lems from a structure prediction perspective.
Robustness. To further investigate the property
of our model, we studied outputs. We found
that our method is able to conduct self-correction.
Exemplified by Example 3 in Table 6, consider-
ing the sentence “Germany’s DAX opens 0.7%
lower at 18,842.” with annotated equation X1 +
(0.007 × X1) = 18, 842, the prediction made by
our method isX1−(0.007×X1) = 18, 842. It can
be seen that the prediction made by our method is
supposed to be the correct one, while the annota-
tion is actually wrong. To make a fair compari-
son with previous works, we did not count such
cases as correct during evaluation, which implies
that accuracy reported in Table 3 is in fact higher.
Error Analysis. For arithmetic word prob-
lem, it is interesting that the operand of two
operands should be addition/subtraction (multipli-
cation/division), while the prediction is subtrac-
tion/addition (division/multiplication). Consider
Example 4 and 5 in Table 6. Descriptions of such
two problems share many words, such as each,
how many, there are, etc. Slight difference in
problem descriptions may lead to different results,
which makes it a challenge.

As for equation parsing, the work of Roy
et al. (2016) requires annotations on which phrases
should be mapped to unknowns during the train-
ing phase. However, such supervised knowledge
is not required for our method. In our setup, we
did not make hard constraint that each predic-
tion must contain one or two variables. There-
fore, missing or redundant variables appearing in
the predicted equations are one of the major er-
ror sources. Example 6 and 7 from Table 6 illus-
trate such cases. On the other hand, lack of profes-
sional background information also leads to miss-



5335

Example 3: Germany’s DAX opens 0.7% lower at 10,842.
Gold: X1 + (0.007×X1) = 10842
Text2Math: X1 − (0.007×X1) = 10842
Example 4: Each child has 5 bottle caps. If there are 9

children, how many bottle caps are there in
total?

Gold: 5× 9
Text2Math: 5÷ 9
Example 5: The school is planning a field trip. There are

14 students and 2 seats on each school bus.
How many buses are needed to take the trip?

Gold: 14÷ 2
Text2Math: 14× 2
Example 6: 530 pesos can buy 4 kilograms of fish and 2

kilograms of pork.
Gold: 530 = (4×X1) + (2×X2)
Text2Math: 530×X3 = (4×X1) + (2×X2)
Example 7: Flying with the wind , a bird was able to make

150 kilometers per hour.
Gold: X1 +X2 = 150
Text2Math: X1 = 150

Table 6: Examples with wrong predictions. Gold de-
notes the annotated correct equations and Text2Math
refers to output equations generated by our method.

ing variables. Consider Example 6. Without world
knowledge, it might be difficult for the algorithm
to recognize that “Flying with the wind” implies
the speed of the wind which should be considered
as a variable of the equation.

4 Related Work

Math Word Problems. Mukherjee and Garain
(2008) surveyed related approaches to this task in
literature. Hosseini et al. (2014); Mitra and Baral
(2016) solved the task by categorizing verbs or
problems. The first method that can handle gen-
eral arithmetic problems with multiple steps was
proposed by Roy and Roth (2015), which was fur-
ther extended by introducing (Roy and Roth, 2017,
2018). Zou and Lu (2019b,c) is the first work that
proposed a sequence labelling approach to solv-
ing arithmetic word problems, which focuses on
addition-subtraction word problems. Other sys-
tems include semantic parsing based approaches
(Liang et al., 2018) and neural methods (Wang
et al., 2017, 2018a,b). Unlike arithmetic word
problems, the goal of algebra word problems is to
map the text to an equation set (Kushman et al.,
2014; Shi et al., 2015). Other types of problems
have also been investigated, including probability
problems (Dries et al., 2017), logic puzzle prob-
lems (Mitra and Baral, 2015; Chesani et al., 2017)
and geometry problems (Seo et al., 2014, 2015).
Besides the benchmark datasets used in this work,

other popular datasets include Dolphin18K (Shi
et al., 2015) and AQuA (Ling et al., 2017) for alge-
bra word problems which are not the focus in this
work. Roy et al. (2016) first proposed the Equa-
tion Parsing task and designed a pipeline method
with three structured predictors.
Semantic Parsing. Another line of related works
is semantic parsing (Wong and Mooney, 2006;
Zettlemoyer and Collins, 2007; Kwiatkowksi
et al., 2010; Liang et al., 2011; Dong and La-
pata, 2018; Zou and Lu, 2018), which aims to
map sentences into logic forms, including CCG-
based lambda calculus expressions (Zettlemoyer
and Collins, 2007; Kwiatkowksi et al., 2010;
Artzi and Zettlemoyer, 2013b; Dong and Lap-
ata, 2016), FunQL (Kate et al., 2005; Wong and
Mooney, 2006; Jones et al., 2012), lambda-DCS
(Liang et al., 2011; Berant et al., 2013; Jia and
Liang, 2016), graph queries (Harris et al., 2013;
Holzschuher and Peinl, 2013) and SQL (Yin et al.,
2015; Sun et al., 2018). In this work, we adopt a
text-math semantic representation encoding words
and the expression tree.

5 Conclusion

In this work, we propose a unified structured pre-
diction approach, Text2Math, to solving both
arithmetic word problems and equation parsing
tasks. We leverage a novel joint representation
to automatically learn the correspondence between
words and math expressions which reflects seman-
tic closeness. Different from many existing mod-
els, Text2Math is agnostic of the semantics of
operands and learns to map from text to math
expressions in an end-to-end manner based on a
data-driven approach. Experiments demonstrate
the efficacy of our model. In the future, we would
like to investigate how such an approach can be
applied to more complicated math word problems,
like algebra word problems where a problem usu-
ally maps to an equation set. Another interesting
direction is to investigate how to incorporate world
knowledge into the graph-based approach to boost
the performance.

Acknowledgments

We would like to thank the three anonymous re-
viewers for their thoughtful and constructive com-
ments. This work is supported by Singapore
Ministry of Education Academic Research Fund
(AcRF) Tier 2 Project MOE2017-T2-1-156.



5336

References
Yoav Artzi and Luke Zettlemoyer. 2013a. Uw spf: The

university of washington semantic parsing frame-
work. arXiv preprint arXiv:1311.3011.

Yoav Artzi and Luke Zettlemoyer. 2013b. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of EMNLP.

Daniel G Bobrow. 1964. Natural language input for
a computer problem solving system. Technical re-
port, MASSACHUSETTS INST OF TECH CAM-
BRIDGE PROJECT MAC.

Federico Chesani, Paola Mello, and Michela Milano.
2017. Solving mathematical puzzles: A challenging
competition for ai. AI Magazine, 38.

Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. In Proceedings of ACL.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceedings
of ACL.

Anton Dries, Angelika Kimmig, Jesse Davis, Vaishak
Belle, and Luc De Raedt. 2017. Solving probabil-
ity problems in natural language. In Proceedings of
IJCAI.

Steve Harris, Andy Seaborne, and Eric
Prud’hommeaux. 2013. Sparql 1.1 query lan-
guage. W3C Recommendation, 21(10):778.

Florian Holzschuher and René Peinl. 2013. Perfor-
mance of graph query languages: comparison of
cypher, gremlin and native access in neo4j. In Pro-
ceedings of the Joint EDBT/ICDT 2013 Workshops.

Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In Proceedings of EMNLP.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of ACL.

Bevan Jones, Mark Johnson, and Sharon Goldwater.
2012. Semantic parsing with bayesian tree transduc-
ers. In Proceedings of ACL.

Rohit J Kate, Yuk Wah Wong, and Raymond J Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings of AAAI.

Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
Sabharwal, Oren Etzioni, and Siena Dumas Ang.
2015. Parsing algebraic word problems into equa-
tions. Transactions of the Association for Computa-
tional Linguistics, 3:585–597.

Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
ACL.

Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of EMNLP.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT.

Chao-Chun Liang, Yu-Shiang Wong, Yi-Chung Lin,
and Keh-Yih Su. 2018. A meaning-based statistical
english math word problem solver. In Proceedings
of NAACL.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of ACL.

Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45.

Wei Lu. 2014. Semantic parsing with relaxed hybrid
trees. In Proceedings of EMNLP.

Wei Lu. 2015. Constrained semantic forests for im-
proved discriminative semantic parsing. In Proceed-
ings of ACL.

Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of EMNLP.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of ACL.

Arindam Mitra and Chitta Baral. 2015. Learning to au-
tomatically solve logic grid puzzles. In Proceedings
of EMNLP.

Arindam Mitra and Chitta Baral. 2016. Learning to
use formulas to solve simple arithmetic problems.
In Proceedings of ACL.

Anirban Mukherjee and Utpal Garain. 2008. A review
of methods for automatic understanding of natural
language mathematical problems. Artificial Intelli-
gence Review, 29(2).



5337

Subhro Roy and Dan Roth. 2015. Solving gen-
eral arithmetic word problems. In Proceedings of
EMNLP.

Subhro Roy and Dan Roth. 2017. Unit dependency
graph and its application to arithmetic word problem
solving. In Proceedings of AAAI.

Subhro Roy and Dan Roth. 2018. Mapping to declara-
tive knowledge for word problem solving. Transac-
tions of the Association of Computational Linguis-
tics, 6:159–172.

Subhro Roy, Shyam Upadhyay, and Dan Roth. 2016.
Equation parsing: Mapping sentences to grounded
equations. In Proceedings of EMNLP.

Subhro Roy, Tim Vieira, and Dan Rote. 2015. Reason-
ing about quantities in natural language. Transac-
tions of the Association for Computational Linguis-
tics, 3(1):1–13.

Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, and
Oren Etzioni. 2014. Diagram understanding in ge-
ometry questions. In Proceedings of AAAI.

Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren
Etzioni, and Clint Malcolm. 2015. Solving geome-
try problems: Combining text and diagram interpre-
tation. In Proceedings of EMNLP.

Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang
Liu, and Yong Rui. 2015. Automatically solving
number word problems by semantic parsing and rea-
soning. In Proceedings of EMNLP.

Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Gui-
hong Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and
Ming Zhou. 2018. Semantic parsing with syntax-
and table-aware sql generation. In Proceedings of
ACL.

Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang,
and Xiaojiang Liu. 2018a. Translating a math word
problem to an expression tree. In Proceedings of
EMNLP.

Lei Wang, Dongxiang Zhang, Lianli Gao, Jingkuan
Song, Long Guo, and Heng Tao Shen. 2018b. Math-
dqn: Solving arithmetic word problems via deep re-
inforcement learning. In Proceedings of AAAI.

Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017.
Deep neural solver for math word problems. In Pro-
ceedings of EMNLP.

Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of NAACL.

Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben
Kao. 2015. Neural enquirer: Learning to query
tables with natural language. arXiv preprint
arXiv:1512.00965.

Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to log-
ical form. In Proceedings of EMNLP-CoNLL.

Yanyan Zou and Wei Lu. 2018. Learning cross-lingual
distributed logical representations for semantic pars-
ing. In Proceedings of ACL.

Yanyan Zou and Wei Lu. 2019a. Joint detection and
location of english puns. In Proceedings of ACL.

Yanyan Zou and Wei Lu. 2019b. Quantity tagger: A
latent-variable sequence labeling approach tosolving
addition-subtraction word problems. In Proceedings
of ACL.

Yanyan Zou and Wei Lu. 2019c. Supplementary mate-
rial for quantity tagger: A latent-variable sequence
labeling approach tosolving addition-subtraction
word problems. In Proceedings of ACL.

https://doi.org/10.18653/v1/P18-2107
https://doi.org/10.18653/v1/P18-2107
https://doi.org/10.18653/v1/P18-2107

