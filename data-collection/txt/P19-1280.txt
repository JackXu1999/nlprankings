

















































Fine-Grained Temporal Relation Extraction


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2906–2919
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2906

Fine-Grained Temporal Relation Extraction

Siddharth Vashishtha
University of Rochester

Benjamin Van Durme
Johns Hopkins University

Aaron Steven White
University of Rochester

Abstract

We present a novel semantic framework for
modeling temporal relations and event dura-
tions that maps pairs of events to real-valued
scales. We use this framework to construct
the largest temporal relations dataset to date,
covering the entirety of the Universal Depen-
dencies English Web Treebank. We use this
dataset to train models for jointly predicting
fine-grained temporal relations and event du-
rations. We report strong results on our data
and show the efficacy of a transfer-learning ap-
proach for predicting categorical relations.

1 Introduction

Natural languages provide a myriad of formal and
lexical devices for conveying the temporal struc-
ture of complex events—e.g. tense, aspect, auxil-
iaries, adverbials, coordinators, subordinators, etc.
Yet, these devices are generally insufficient for de-
termining the fine-grained temporal structure of
such events. Consider the narrative in (1).
(1) At 3pm, a boy broke his neighbor’s window.

He was running away, when the neighbor
rushed out to confront him. His parents were
called but couldn’t arrive for two hours be-
cause they were still at work.

Most native English speakers would have little dif-
ficulty drawing a timeline for these events, likely
producing something like that in Figure 1. But
how do we know that the breaking, the run-
ning away, the confrontation, and the calling were
short, while the parents being at work was not?
And why should the first four be in sequence, with
the last containing the others?

The answers to these questions likely involve a
complex interplay between linguistic information,
on the one hand, and common sense knowledge
about events and their relationships, on the other

run away

arrive

rush out
confront

call
be at work

break

5pm3pm 4pm

Figure 1: A typical timeline for the narrative in (1).

(Minsky, 1975; Schank and Abelson, 1975; Lam-
port, 1978; Allen and Hayes, 1985; Hobbs et al.,
1987; Hwang and Schubert, 1994). But it remains
a question how best to capture this interaction.

A promising line of attack lies in the task
of temporal relation extraction. Prior work
in this domain has approached this task as a
classification problem, labeling pairs of event-
referring expressions—e.g. broke or be at work
in (1)—and time-referring expressions—e.g. 3pm
or two hours—with categorical temporal rela-
tions (Pustejovsky et al., 2003; Styler IV et al.,
2014; Minard et al., 2016). The downside of this
approach is that time-referring expressions must
be relied upon to express duration information.
But as (1) highlights, nearly all temporal dura-
tion information can be left implicit without hin-
dering comprehension, meaning these approaches
only explicitly encode duration information when
that information is linguistically realized.

In this paper, we develop a novel framework
for temporal relation representation that puts event
duration front and center. Like standard ap-
proaches using the TimeML standard, we draw in-
spiration from Allen’s (1983) seminal work on in-
terval representations of time. But instead of an-
notating text for categorical temporal relations, we
map events to their likely durations and event pairs
directly to real-valued relative timelines. This
change not only supports the goal of giving a more
central role to event duration, it also allows us to
better reason about the temporal structure of com-



2907

plex events as described by entire documents.
We first discuss prior work on temporal rela-

tion extraction (§2) and then present our frame-
work and data collection methodology (§3). The
resulting dataset—Universal Decompositional Se-
mantics Time (UDS-T)—is the largest temporal
relation dataset to date, covering all of the Univer-
sal Dependencies (Silveira et al., 2014; De Marn-
effe et al., 2014; Nivre et al., 2015) English Web
Treebank (Bies et al., 2012). We use this dataset to
train a variety of neural models (§4) to jointly pre-
dict event durations and fine-grained (real-valued)
temporal relations (§5), yielding not only strong
results on our dataset, but also competitive perfor-
mance on TimeML-based datasets (§6).1

2 Background

We review prior work on temporal relations frame-
works and temporal relation extraction systems.

Corpora Most large temporal relation datasets
use the TimeML standard (Pustejovsky et al.,
2003; Styler IV et al., 2014; Minard et al., 2016).
TimeBank is one of the earliest large corpora built
using this standard, aimed at capturing ‘salient’
temporal relations between events (Pustejovsky
et al., 2003). The TempEval competitions build on
TimeBank by covering relations between all the
events and times in a sentence.

Inter-sentential relations, which are necessary
for document-level reasoning, have not been a fo-
cus of the TempEval tasks, though at least one
sub-task does address them (Verhagen et al., 2007,
2010; UzZaman et al., 2013, and see Chambers
et al. 2014). Part of this likely has to do with the
sparsity inherent in the TempEval event-graphs.
This sparsity has been addressed with corpora
such as the TimeBank-Dense, where annotators la-
bel all local-edges irrespective of ambiguity (Cas-
sidy et al., 2014). TimeBank-Dense does not cap-
ture the complete graph over event and time rela-
tions, instead attempting to achieve completeness
by capturing all relations both within a sentence
and between neighboring sentences. We take in-
spiration from this work for our own framework.

This line of work has been further improved
on by frameworks such as Richer Event De-
scription (RED), which uses a multi-stage an-
notation pipeline where various event-event phe-
nomena, including temporal relations and sub-

1Data and code are available at http://decomp.io/.

event relations are annotated together in the same
datasets (O’Gorman et al., 2016). Similarly, Hong
et al. (2016) build a cross-document event cor-
pus which covers fine-grained event-event rela-
tions and roles with more number of event types
and sub-types (see also Fokkens et al., 2013).

Models Early systems for temporal relation ex-
traction use hand-tagged features modeled with
multinomial logistic regression and support vector
machines (Mani et al., 2006; Bethard, 2013; Lin
et al., 2015). Other approaches use combined rule-
based and learning-based approaches (D’Souza
and Ng, 2013) and sieve-based architectures—
e.g. CAEVO (Chambers et al., 2014) and
CATENA (Mirza and Tonelli, 2016). Recently,
Ning et al. (2017) use a structured learning ap-
proach and show significant improvements on
both TempEval-3 (UzZaman et al., 2013) and
TimeBank-Dense (Cassidy et al., 2014). Ning
et al. (2018) show further improvements on
TimeBank-Dense by jointly modeling causal and
temporal relations using Constrained Conditional
Models and formulating the problem as an Interger
Linear Programming problem.

Neural network-based approaches have used
both recurrent (Tourille et al., 2017; Cheng and
Miyao, 2017; Leeuwenberg and Moens, 2018)
and convolutional architectures (Dligach et al.,
2017). Such models have furthermore been used
to construct document timelines from a set of
predicted temporal relations (Leeuwenberg and
Moens, 2018). Such use of pairwise annotations
can result in inconsistent temporal graphs, and ef-
forts have been made to avert this issue by employ-
ing temporal reasoning (Chambers and Jurafsky,
2008; Yoshikawa et al., 2009; Denis and Muller,
2011; Do et al., 2012; Laokulrat et al., 2016; Ning
et al., 2017; Leeuwenberg and Moens, 2017).

Other work has aimed at modeling event du-
rations from text (Pan et al., 2007; Gusev et al.,
2011; Williams and Katz, 2012), though this work
does not tie duration to temporal relations (see also
Filatova and Hovy, 2001). Our approach com-
bines duration and temporal relation information
within a unified framework, discussed below.

3 Data Collection

We collect the Universal Decompositional Seman-
tics Time (UDS-T) dataset, which is annotated on
top of the Universal Dependencies (Silveira et al.,
2014; De Marneffe et al., 2014; Nivre et al., 2015)



2908

Dataset #Events #Event-Event
Relations

TimeBank 7,935 3,481
TempEval 2010 5,688 3,308
TempEval 2013 11,145 5,272
TimeBank-Dense 1,729 8,130
Hong et al. (2016) 863 25,610
UDS-T 32,302 70,368

Table 1: Number of total events, and event-event tem-
poral relations captured in various corpora

English Web Treebank (Bies et al., 2012) (UD-
EWT). The main advantages of UD-EWT over
other similar corpora are: (i) it covers text from
a variety of genres; (ii) it contains gold standard
Universal Dependency parses; and (iii) it is com-
patible with various other semantic annotations
which use the same predicate extraction standard
(White et al., 2016; Zhang et al., 2017; Rudinger
et al., 2018; Govindarajan et al., 2019). Table 1
compares the size of UDS-T against other tempo-
ral relations datasets.

Protocol design Annotators are given two con-
tiguous sentences from a document with two high-
lighted event-referring expressions (predicates).
They are then asked (i) to provide relative time-
lines on a bounded scale for the pair of events re-
ferred to by the highlighted predicates; and (ii) to
give the likely duration of the event referred to
by the predicate from the following list: instan-
taneous, seconds, minutes, hours, days, weeks,
months, years, decades, centuries, forever. In ad-
dition, annotators were asked to give a confidence
ratings for their relation annotation and each of
their two duration annotation on the same five-
point scale - not at all confident (0), not very con-
fident (1), somewhat confident (2), very confident
(3), totally confident (4).

An example of the annotation instrument is
shown in Figure 2. Henceforth, we refer to the
situation referred to by the predicate that comes
first in linear order (feed in Figure 2) as e1 and the
situation referred to by the predicate that comes
second in linear order (sick in Figure 2) as e2.

Annotators We recruited 765 annotators from
Amazon Mechanical Turk to annotate predicate
pairs in groups of five. Each predicate pair con-
tained in the UD-EWT train set was annotated by
a single annotator, and each in the UD-EWT de-
velopment and test sets was annotated by three.

Predicate extraction We extract predicates
from UD-EWT using PredPatt (White et al., 2016;

Figure 2: An annotated example from our protocol

Zhang et al., 2017), which identifies 33,935 pred-
icates from 16,622 sentences. We concatenate all
pairs of adjacent sentences in the documents con-
tained in UD-EWT, allowing us to capture inter-
sentential temporal relations. Considering all pos-
sible pairs of predicates in adjacent sentences is in-
feasible, so we use a heuristic to capture the most
interesting pairs. (See Appendix A for details.)

1

A

B

C

0

0.12 0.48

0.48 0.66
e1

e2

0.02 0.38

0.38 0.56
e1

e2

0.30 0.48

0.48
0.57e1

e2

0 1

0 0.66

0.66 1
e1

e2

Figure 3: Normalization of slider values

Normalization We normalize the slider re-
sponses for each event pair by subtracting the min-
imum slider value from all values, then dividing
all such shifted values by the maximum value (af-
ter shifting). This ensures that the earliest begin-
ning point for every event pair lies at 0 and that the
right-most end-point lies at 1 while preserving the
ratio between the durations implied by the sliders.
Figure 3 illustrates this procedure for three hypo-
thetical annotators annotating the same two events
e1 and e2. Assuming that the duration classes for
e1 or e2 do not differ across annotators, the relative
chronology of the events is the same in each case.
This preservation of relative chronology, over ab-
solute slider position, is important because, for the
purposes of determining temporal relation, the ab-
solute positions that annotators give are meaning-
less, and we do not want our models to be forced
to fit to such irrelevant information.

Inter-annotator agreement We measure inter-
annotator agreement (IAA) for the temporal re-
lation sliders by calculating the rank (Spearman)



2909

0.00

0.05

0.10

0.15

0.20

0.25

ins
tan
t

se
co
nd
s

mi
nu
tes
ho
ursda

ys
we
ek
s

mo
nth
s
ye
ars

de
ca
de
s

ce
ntu
rie
s
for
eve
r

R
el

at
iv

e 
Fr

eq
ue

nc
y

Split
dev
train

Figure 4: Distribution of event durations.

correlation between the normalized slider posi-
tions for each pair of annotators that annotated the
same group of five predicate pairs in the develop-
ment set.2 The development set is annotated by
724 annotators. Rank correlation is a useful mea-
sure because it tells us how much different anno-
tators agree of the relative position of each slider.
The average rank correlation between annotators
was 0.665 (95% CI=[0.661, 0.669]).

For the duration responses, we compute the ab-
solute difference in duration rank between the du-
ration responses for each pair of annotators that
annotated the same group of five predicate pairs in
the development set. On average, annotators dis-
agree by 2.24 scale points (95% CI=[2.21, 2.25]),
though there is heavy positive skew (�1 = 1.16,
95% CI=[1.15, 1.18])—evidenced by the fact that
the modal rank difference is 1 (25.3% of the re-
sponse pairs), with rank difference 0 as the next
most likely (24.6%) and rank difference 2 as a dis-
tant third (15.4%).

Summary statistics Figure 4 shows the distri-
bution of duration responses in the training and de-
velopment sets. There is a relatively high density
of events lasting minutes, with a relatively even
distribution across durations of years or less and
few events lasting decades or more.

The raw slider positions themselves are some-
what difficult to directly interpret. To improve in-
terpretability, we rotate the slider position space
to construct four new dimensions: (i) PRIORITY,
which is positive when e1 starts and/or ends ear-
lier than e2 and negative otherwise; (ii) CONTAIN-
MENT,which is most positive when e1 contains
more of e2; (iii) EQUALITY, which is largest when

2Our protocol design also allows us to detect some bad
annotations internal to the annotation itself, as opposed to
comparing one annotator’s annotation of an item to another.
See Appendix B for further details on our deployment of such
annotation-internal validation techniques.

Figure 5: Distribution of event relations.

both e1 and e2 have the same temporal extents
and smallest when they are most unequal; and (iv)
SHIFT, which moves the events forward or back-
ward in time. We construct these dimensions by
solving for R in

R

2

664

�1 �1 1 1
�1 1 1 �1
�1 1 �1 1
1 1 1 1

3

775 = 2S� 1

where S 2 [0, 1]N⇥4 contains the slider posi-
tions for our N datapoints in the following order:
beg(e1), end(e1), beg(e2), end(e2).

Figure 5 shows the embedding of the event pairs
on the first three of these dimensions of R. The tri-
angular pattern near the top and bottom of the plot
arises because strict priority—i.e. extreme positiv-
ity or negativity on the y-axis—precludes any tem-
poral overlap between the two events, and as we
move toward the center of the plot, different prior-
ity relations mix with different overlap relations—
e.g. the upper-middle left corresponds to event
pairs where most of e1 comes toward the begin-
ning of e2, while the upper middle right of the plot
corresponds to event pairs where most of e2 comes
toward the end of e1.

4 Model

For each pair of events referred to in a sentence,
we aim to jointly predict the relative timelines of
those events as well as their durations. We then
use a separate model to induce document timelines
from the relative timelines.

Relative timelines The relative timeline model
consists of three components: an event model, a



2910

What to feed my been sickdog ….…. for

What to feed my been sickdog ….…. for

What to feed my been sickdog ….…. for

ELMo

Attention Attention

AttentionAttention Attention

gpred(i) gpred(j)

MLPrelMLPdur MLPdur

gdur(i) gdur(j)grel(i,j)

hours days

Tuner

Figure 6: Network diagram for model. Dashed arrows
are only included in some models.

duration model, and a relation model. These com-
ponents use multiple layers of dot product atten-
tion (Luong et al., 2015) on top of an embedding
H 2 RN⇥D for a sentence s = [w1, . . . , wN ]
tuned on the three M -dimensional contextual em-
beddings produced by ELMo (Peters et al., 2018)
for that sentence, concatenated together.

H = tanh (ELMo(s)WTUNE + bTUNE)

where D is the dimension for the tuned embed-
dings, WTUNE 2 R3M⇥D, and bTUNE 2 RN⇥D.

Event model We define the model’s represen-
tation for the event referred to by predicate k as
gpredk 2 RD, where D is the embedding size.
We build this representation using a variant of dot-
product attention, based on the predicate root.

a
SPAN
predk = tanh

�
A

SPAN
PREDhROOT(predk) + b

SPAN
PRED

�

↵predk = softmax
�
HSPAN(predk)a

SPAN
predk

�

gpredk = [hROOT(predk);↵predkHSPAN(predk)]

where ASPANPRED 2 RD⇥D,bSPANPRED 2 RD; hROOT(predk)
is the hidden representation of the kth predicate’s
root; and HSPAN(predk) is obtained by stacking the
hidden representations of the entire predicate.

As an example, the predicate been sick for
now in Figure 2 has sick as its root, and thus
we would take the hidden representation for sick
as hROOT(predk). Similarly, HSPAN(predk) would be
equal to taking the hidden-state representations

of been sick for now and stacking them together.
Then, if the model learns that tense information is
important, it may weight been using attention.

Duration model The temporal duration rep-
resentation gdurk for the event referred to by the
kth predicate is defined similarly to the event rep-
resentation, but instead of stacking the predicate’s
span, we stack the hidden representations of the
entire sentence H.

a
SENT
durk = tanh

�
A

SENT
DUR gpredk + b

SENT
DUR

�

↵durk = softmax(Ha
SENT
durk )

gdurk = [gpredk ;↵durkH]

where ASENTDUR 2 RD⇥size(gpredk ) and bSENTDUR 2 RD.
We consider two models of the categorical du-

rations: a softmax model and a binomial model.
The main difference is that the binomial model
enforces that the probabilities pdurk over the 11
duration values be concave in the duration rank,
whereas the softmax model has no such constraint.
We employ a cross-entropy loss for both models.

Ldur(dk;p) = � log pdk
In the softmax model, we pass the duration rep-
resentation gdurk for predicate k through a multi-
layer perceptron (MLP) with a single hidden layer
of ReLU activations, to yield probabilities pdurk
over the 11 durations.

vdurk = ReLU(W
(1)
DURgdurk + b

(1)
DUR)

p = softmax(W(2)DURvdurk + b
(2)
DUR)

In the binomial distribution model, we again pass
the duration representation through a MLP with
a single hidden layer of ReLU activations, but in
this case, we yield only a single value ⇡durk . With
vdurk as defined above:

⇡ = �
⇣
w

(2)
DURvdurk + b

(2)
DUR

⌘

pc =

✓
n

c

◆
⇡n(1� ⇡)(n�c)

where c 2 {0, 1, 2, ..., 10} represents the ranked
durations – instant (0), seconds (1), minutes (2),
..., centuries (9), forever (10) – and n is the maxi-
mum class rank (10).

Relation model To represent the temporal re-
lation representation between the events referred
to by the ith and jth predicate, we again use a sim-
ilar attention mechanism.



2911

a
SENT
relij = tanh

�
A

SENT
REL [gpredi ;gpredj ] + b

SENT
REL

�

↵relij = softmax
⇣
Ha

SENT
relij

⌘

grelij = [gpredi ;gpredj ;↵relijH]

where ASENTREL 2 RD⇥2size(gpredk ) and bSENTREL 2 RD.
The main idea behind our temporal model is to

map events and states directly to a timeline, which
we represent via a reference interval [0, 1]. For
situation k, we aim to predict the beginning point
bk and end-point ek � bk of k.

We predict these values by passing grelij
through an MLP with one hidden layer of
ReLU activations and four real-valued outputs
[�̂i, �̂i, �̂j , �̂j ], representing the estimated relative
beginning points (�̂i, �̂j) and durations (�̂i, �̂j) for
events i and j. We then calculate the predicted
slider values ŝij = [b̂i, êi, b̂j , êj ]

h
b̂k, êk

i
=

h
�
⇣
�̂k

⌘
,�

⇣
�̂k +

����̂k
���
⌘i

The predicted values ŝij are then normalized in the
same fashion as the true slider values prior to being
entered into the loss. We constrain this normalized
ŝij using four L1 losses.

Lrel(sij ; ŝij) =
���(bi � bj)� (b̂i � b̂j)

���+
���(ei � bj)� (êi � b̂j)

���+
���(ej � bi)� (êj � b̂i)

���+

|(ei � ej)� (êi � êj)|

The final loss function is then L = Ldur + 2Lrel.
Duration-relation connections We also ex-

periment with four architectures wherein the du-
ration and relation models are connected to each
other in the Dur! Rel or Dur Rel directions.

In the Dur ! Rel architectures, we modify
grelij in two ways: (i) additionally concatenating
the ith and jth predicate’s duration probabilities
from the binomial distribution model, and (ii) not
using the relation representation model at all.

grelij = [gpredi ;gpredj ;↵relijH;pi;pj ]

grelij = [pi;pj ]

In the Dur Rel architectures, we use two mod-
ifications: (i) we modify gdurk by concatenating
the b̂k and êk from the relation model, and (ii)
we do not use the duration representation model

at all, instead use the predicted relative duration
êk � b̂k obtained from the relation model, passing
it through the binomial distribution model.

gdurk = [gpredk ;↵durkH; b̂k; êk]

⇡durk = êk � b̂k

Document timelines We induce the hidden doc-
ument timelines for the documents in the UDS-
T development set using relative timelines from
(i) actual pairwise slider annotations; or (ii) slider
values predicted by the best performing model on
UDS-T development set. To do this, we assume a
hidden timeline T 2 Rnd⇥2+ , where nd is the to-
tal number of predicates in that document, the two
dimensions represent the beginning point and the
duration of the predicates. We connect these la-
tent timelines to the relative timelines, by anchor-
ing the beginning points of all predicates such that
there is always a predicate with 0 as the beginning
point in a document and defining auxiliar variables
⌧ij and ŝij for each events i and j.

⌧ij = [ti1, ti1 + ti2, tj1, tj1 + tj2]

ŝij =
⌧ij �min(⌧ij)

max(⌧ij �min(⌧ij))

We learn T for each document under the relation
loss Lrel(sij , ŝij). We further constrain T to pre-
dict the categorical durations using the binomial
distribution model on the durations tk2 implied by
T, assuming ⇡k = �(c log(tk2)).

5 Experiments

We implement all models in pytorch 1.0. For
all experiments, we use mini-batch gradient de-
scent with batch-size 64 to train the embedding
tuner (reducing ELMo to a dimension of 256), at-
tention, and MLP parameters. Both the relation
and duration MLP have a single hidden layer with
128 nodes and a dropout probability of 0.5 (see
Appendix D for further details).

To predict TimeML relations in TempE-
val3 (TE3; UzZaman et al., 2013, Task C-relation
only) and TimeBank-Dense (TD; Cassidy et al.,
2014), we use a transfer learning approach. We
first use the best-performing model on the UDS-T
development set to obtain the relation representa-
tion (grelij ) for each pair of annotated event-event
relations in TE3 and TD (see Appendix E for pre-
processing details). We then use this vector as in-
put features to a SVM classifier with a Gaussian



2912

Model Duration Relation
Duration Relation Connection ⇢ rank diff. R1 Absolute ⇢ Relative ⇢ R1

softmax X - 32.63 1.86 8.59 77.91 68.00 2.82
binomial X - 37.75 1.75 13.73 77.87 67.68 2.35
- X Dur Rel 22.65 3.08 -51.68 71.65 66.59 -6.09
binomial - Dur! Rel 36.52 1.76 13.17 77.58 66.36 0.85
binomial X Dur! Rel 38.38 1.75 13.85 77.82 67.73 2.58
binomial X Dur Rel 38.12 1.75 13.68 78.12 68.22 2.96

Table 2: Results on test data based on different model representations; ⇢ denotes the Spearman-correlation coef-
ficient; rank-diff is the duration rank difference. The model highlighted in blue performs best on durations and is
also close to the top performing model for relations on the development set. The numbers highlighted in bold are
the best-performing numbers on the test data in the respective columns.

kernel to train on the training sets of these datasets
using the feature vector obtained from our model.3

Following recent work using continuous la-
bels in event factuality prediction (Lee et al.,
2015; Stanovsky et al., 2017; Rudinger et al.,
2018; White et al., 2018) and genericity prediction
(Govindarajan et al., 2019), we report three met-
rics for the duration prediction: Spearman correla-
tion (⇢), mean rank difference (rank diff ), and pro-
portion rank difference explained (R1). We report
three metrics for the relation prediction: Spearman
correlation between the normalized values of ac-
tual beginning and end points and the predicted
ones (absolute ⇢), the Spearman correlation be-
tween the actual and predicted values in Lrel (rela-
tive ⇢), and the proportion of MAE explained (R1).

R1 = 1� MAEmodel
MAEbaseline

where MAEbaseline is always guessing the median.

6 Results

Table 2 shows the results of different model ar-
chitectures on the UDS-T test set, and Table 4
shows the results of our transfer-learning approach
on test set of TimeBank-Dense (TD-test).

UDS-T results Most of our models are able to
predict the relative position of the beginning and
ending of events very well (high relation ⇢) and
the relative duration of events somewhat well (rel-
atively low duration ⇢), but they have a lot more
trouble predicting relation exactly and relatively
less trouble predicting duration exactly.

3For training on TE3, we use TimeBank (TB; Pustejovsky
et al., 2003) + AQUAINT (AQ; Graff) datasets provided in
the TE3 workshop (UzZaman et al., 2013). For training on
TD, we use TD-train and TD-dev.

Duration model The binomial distribution
model outperforms the softmax model for dura-
tion prediction by a large margin, though it has
basically no effect on the accuracy of the relation
model, with the binomial and softmax models per-
forming comparably. This suggests that enforcing
concavity in duration rank on the duration proba-
bilities helps the model better predict durations.

Connections Connecting the duration and re-
lation model does not improve performance in
general. In fact, when the durations are directly
predicted from the temporal relation model—i.e.
without using the duration representation model—
the model’s performance drops by a large margin,
with the Spearman correlation down by roughly 15
percentage points. This indicates that constrain-
ing the relations model to predict the durations is
not enough and that the duration representation is
needed to predict durations well. On the other
hand, predicting temporal relations directly from
the duration probability distribution—i.e. without
using the relation representation model—results
in a similar score as that of the top-performing
model. This indicates that the duration representa-
tion is able to capture most of the relation charac-
teristics of the sentence. Using both duration rep-
resentation and relation representation separately
(model highlighted in blue) results in the best per-
formance overall on the UDS-T development set.

TimeBank-Dense and TempEval3 Table 4 re-
ports F1-micro scores on the test set of TimeBank-
Dense compared with some other systems as re-
ported by Cheng and Miyao (2017). We report
these scores only on Event-Event (E-E) relations
as our system captures only those. We also com-
pute the standard temporal awareness F1 score on
the test set of TempEval-3 (TE3-PT) considering



2913

Duration
Word Attention Rank Freq

soldiers 0.911 1.28 69
months 0.844 1.38 264
Nothing 0.777 5.07 114
minutes 0.768 1.33 81
astronauts 0.756 1.37 81
hour 0.749 1.41 84
Palestinians 0.735 1.72 288
month 0.721 2.03 186
cartoonists 0.714 1.35 63
years 0.708 1.94 588
days 0.635 1.39 84
thoughts 0.592 2.90 60
us 0.557 2.09 483
week 0.531 2.23 558
advocates 0.517 2.30 105

Relation
Word Attention Rank Freq

occupied 0.685 1.33 54
massive 0.522 2.71 66
social 0.510 1.68 57
general 0.410 3.52 168
few 0.394 3.07 474
mathematical 0.393 7.66 132
are 0.387 3.47 4415
comes 0.339 2.39 51
or 0.326 3.50 3137
and 0.307 4.86 17615
emerge 0.305 2.67 54
filed 0.303 7.14 66
s 0.298 4.03 1152
were 0.282 3.49 1308
gets 0.239 7.36 228

Table 3: Mean attention weight, mean attention rank, and frequency for 15 words in the development set with the
highest mean duration-attention (left) and relation-attention (right) weights. For duration, the words highlighted in
bold directly correspond to some duration class. For relation, the words in bold are either conjunctions or words
containing tense information.

only E-E relations and achieve a score of 0.498.4

Our system beats the TD F1-micro scores of all
other systems reported in Table 4. As a refer-
ence, the top performing system on TE3-PT (Ning
et al., 2017) reports an F1 score of 0.672 over
all relations, but is not directly comparable to our
system as we only evaluate on event-event rela-
tions. These results indicate that our model is able
to achieve competitive performance on other stan-
dard temporal classification problems.

Systems Evaluation Data F1
(E-E)

CAEVO TD-test 0.494
CATENA TD-test 0.519
Cheng and Miyao (2017) TD-test 0.529
This work TD-test 0.566

Table 4: F1-micro scores of event-event relations in
TD-test based on our transfer learning experiment.

7 Model Analysis and Timelines

We investigate two aspects of the best-performing
model on the development set (highlighted in Ta-
ble 2): (i) what our duration and relation rep-
resentations attend to; and (ii) how well docu-
ment timelines constructed from the model’s pre-

4We do not report the temporal awareness scores (F1)
of other systems on TE3-PT, since they report their metrics
on all relations, including timex-timex, and event-timex re-
lations, and thus they are not directly comparable. For TD,
only those systems are reported that report F1-micro scores.

dictions match those constructed from the annota-
tions. (See Appendix F for further analyses.)

Attention The advantage of using an attention
mechanism is that we can often interpret what lin-
guistic information the model is using by analyz-
ing the attention weights. We extract these at-
tention weights for both the duration representa-
tion and the relation representation from our best
model on the development set.

Duration We find that words that denote
some time period—e.g. month(s), minutes, hour,
years, days, week—are among the words with
highest mean attention weight in the duration
model, with seven of the top 15 words directly
denoting one of the duration classes (Table 3).
This is exactly what one might expect this model
to rely heavily on, since time expressions are
likely highly informative for making predictions
about duration. It also may suggest that we
do not need to directly encode relations between
event-referring and time-referring expressions in
our framework—as do annotation standards like
TimeML—since our models may discover them.

The remainder of the top words in the dura-
tion model are plurals or mass nouns (soldiers,
thoughts etc.). This may suggest that the plu-
rality of a predicate’s arguments is an indicator
of the likely duration of the event referred to



2914

by that predicate. To investigate this possibil-
ity, we compute a multinomial regression predict-
ing the attention weights ↵s for each sentence s
from the K morphological features of each word
in that sentence Fs 2 {0, 1}length(s)⇥K , which
are extracted from the UD-EWT features column
and binarized. To do this, we optimize coeffi-
cients c in argcmin

P
sD (↵s k softmax (Fsc)),

where D is the KL divergence. We find that
the five most strongly weighted positive features
in c are all features of nouns—NUMBER=plur,
CASE=acc, PRONTYPE=prs, NUMBER=sing, GEN-
DER=masc—suggesting that good portion of du-
ration information can be gleaned from the ar-
guments of a predicate. This may be because
nominal information can be useful in determin-
ing whether the clause is about particular events
or generic events (Govindarajan et al., 2019).

Relation A majority of the words with high-
est mean attention weight in the relation model
are either coordinators—such as or and and—or
bearers of tense information—i.e. lexical verbs
and auxiliaries. The first makes sense because, in
context, coordinators can carry information about
temporal sequencing (see Wilson and Sperber,
1998, i.a.). The second makes sense in that infor-
mation about the tense of predicates being com-
pared likely helps the model determine relative or-
dering of the events they refer to.

Similar to duration attention analysis, for rela-
tion attention, we find that the five most strongly
weighted positive features in c are all fea-
tures of verbs or auxiliaries—PERSON=1, PER-
SON=3, TENSE=pres, TENSE=past, MOOD=ind—
suggesting that a majority of the information rel-
evant to relation can be gleaned from the tense-
bearing units in a clause.

Document timelines We apply the document
timeline model described in §4 to both the an-
notations on the development set and the best-
performing model’s predictions to obtain timelines
for all documents in the development set. Figure
7 shows an example, comparing the two resulting
document timelines.

For these two timelines, we compare the in-
duced beginning points and durations, obtaining a
mean Spearman correlation of 0.28 for beginning
points and -0.097 for durations. This suggests that
the model agrees to some extent with the annota-
tions about the beginning points of events in most
documents but is struggling to find the correct du-

was lower than

got

rate

showed

was great

took

recommend

go

explain

Figure 7: Learned timeline for the following docu-
ment based on actual (black) and predicted (red) an-
notations: “A+. I would rate Fran pcs an A + because
the price was lower than everyone else , i got my com-
puter back the next day , and the professionalism he
showed was great . He took the time to explain things
to me about my computer , i would recommend you go
to him. David”

ration spans. One possible reason for poor pre-
diction of durations could be the lack of a direct
source of duration information. The model cur-
rently tries to identify the duration based only on
the slider values, which leads to poor performance
as already seen in one of the Dur Rel model.

8 Conclusion

We presented a novel semantic framework for
modeling fine-grained temporal relations and
event durations that maps pairs of events to real-
valued scales for the purpose of constructing
document-level event timelines. We used this
framework to construct the largest temporal rela-
tions dataset to date – UDS-T – covering the en-
tirety of the UD-EWT. We used this dataset to train
models for jointly predicting fine-grained tempo-
ral relations and event durations, reporting strong
results on our data and showing the efficacy of a
transfer-learning approach for predicting standard,
categorical TimeML relations.

Acknowledgments

We are grateful to the FACTS.lab at the University
of Rochester as well as three anonymous reviewers
for useful comments on this work. This research
was supported by the University of Rochester,
JHU HLTCOE, and DARPA AIDA. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.



2915

References
James F Allen. 1983. Maintaining knowledge about

temporal intervals. Communications of the ACM,
26(11):832–843.

James F Allen and Patrick J Hayes. 1985. A common-
sense theory of time. In Proceedings of the 9th Inter-
national Joint Conference on Artificial Intelligence-
Volume 1, pages 528–531. Morgan Kaufmann Pub-
lishers Inc.

Steven Bethard. 2013. Cleartk-timeml: A minimalist
approach to tempeval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), Volume 2: Proceedings of the Seventh In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), volume 2, pages 10–14.

Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English web treebank. Linguistic Data Con-
sortium, Philadelphia, PA.

Taylor Cassidy, Bill McDowell, Nathanael Chambers,
and Steven Bethard. 2014. An annotation frame-
work for dense event ordering. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 501–506.

Nathanael Chambers, Taylor Cassidy, Bill McDowell,
and Steven Bethard. 2014. Dense event ordering
with a multi-pass architecture. Transactions of the
Association for Computational Linguistics, 2:273–
284.

Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 698–706. Association for Computational Lin-
guistics.

Fei Cheng and Yusuke Miyao. 2017. Classifying tem-
poral relations by bidirectional lstm over depen-
dency paths. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), volume 2, pages
1–6.

Marie-Catherine De Marneffe, Timothy Dozat, Na-
talia Silveira, Katri Haverinen, Filip Ginter, Joakim
Nivre, and Christopher D. Manning. 2014. Uni-
versal Stanford dependencies: A cross-linguistic ty-
pology. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC-2014), volume 14, pages 4585–4592.

Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In
IJCAI-11-International Joint Conference on Artifi-
cial Intelligence.

Dmitriy Dligach, Timothy Miller, Chen Lin, Steven
Bethard, and Guergana Savova. 2017. Neural tem-
poral relation extraction. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers, volume 2, pages 746–751.

Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
inference for event timeline construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 677–
687. Association for Computational Linguistics.

Jennifer D’Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 918–927.

Elena Filatova and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Proceedings of
the Workshop on Temporal and Spatial Informa-
tion Processing-Volume 13, page 13. Association for
Computational Linguistics.

Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, Luciano Ser-
afini, Rachele Sprugnoli, and Jesper Hoeksema.
2013. GAF: A grounded annotation framework for
events. In Workshop on Events: Definition, Detec-
tion, Coreference, and Representation, pages 11–20.

Venkata Subrahmanyan Govindarajan, Benjamin
Van Durme, and Aaron Steven White. 2019.
Decomposing generalization: Models of generic,
habitual, and episodic statements. arXiv preprint
arXiv:1901.11429.

David Graff. The aquaint corpus of English news
text:[content copyright] Portions c� 1998-2000 New
York Times, Inc., c� 1998-2000 Associated Press,
Inc., c� 1996-2000 Xinhua News Service. Linguis-
tic Data Consortium.

Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Juraf-
sky. 2011. Using query patterns to learn the dura-
tion of events. In Proceedings of the Ninth Inter-
national Conference on Computational Semantics,
pages 145–154. Association for Computational Lin-
guistics.

Jerry R Hobbs, William Croft, Todd Davies, Douglas
Edwards, and Kenneth Laws. 1987. Commonsense
metaphysics and lexical semantics. Computational
Linguistics, 13(3-4):241–250.

Yu Hong, Tongtao Zhang, Tim O’Gorman, Sharone
Horowit-Hendler, Heng Ji, and Martha Palmer.
2016. Building a cross-document event-event rela-
tion corpus. In Proceedings of the 10th Linguistic
Annotation Workshop held in conjunction with Asso-
ciation for Computational Linguistics 2016 (LAW-X
2016), pages 1–6.



2916

Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.

Chung Hee Hwang and Lenhart K Schubert. 1994. In-
terpreting tense, aspect and time adverbials: A com-
positional, unified approach. In Temporal Logic,
pages 238–264. Springer.

Leslie Lamport. 1978. Time, clocks, and the ordering
of events in a distributed system. Communications
of the ACM, 21(7):558–565.

Natsuda Laokulrat, Makoto Miwa, and Yoshimasa Tsu-
ruoka. 2016. Stacking approach to temporal relation
classification with temporal inference. Information
and Media Technologies, 11:53–78.

Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-
moyer. 2015. Event detection and factuality assess-
ment with non-expert supervision. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing, pages 1643–1648.

Artuur Leeuwenberg and Marie-Francine Moens.
2018. Temporal information extraction by predict-
ing relative time-lines. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1237–1246.

Tuur Leeuwenberg and Marie-Francine Moens. 2017.
Structured learning for temporal relation extraction
from clinical records. In Proceedings of the 15th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 1150–
1158.

Chen Lin, Dmitriy Dligach, Timothy A Miller, Steven
Bethard, and Guergana K Savova. 2015. Multi-
layered temporal modeling for the clinical domain.
Journal of the American Medical Informatics Asso-
ciation, 23(2):387–395.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 753–760. Association for Computational
Linguistics.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60.

Anne-Lyse Myriam Minard, Manuela Speranza, Ruben
Urizar, Begona Altuna, Marieke van Erp, Anneleen
Schoen, and Chantal van Son. 2016. Meantime,

the newsreader multilingual event and time corpus.
In Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation (LREC
2016). European Language Resources Association
(ELRA).

Marvin Minsky. 1975. A framework for representing
knowledge. The Psychology of Computer Vision.

Paramita Mirza and Sara Tonelli. 2016. Catena: Causal
and temporal relation extraction from natural lan-
guage texts. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 64–75.

Qiang Ning, Zhili Feng, and Dan Roth. 2017. A struc-
tured learning approach to temporal relation extrac-
tion. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1027–1037.

Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018.
Joint reasoning for temporal and causal relations. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 2278–2288.

Joakim Nivre, Zeljko Agic, Maria Jesus Aranzabe,
Masayuki Asahara, Aitziber Atutxa, Miguel Balles-
teros, John Bauer, Kepa Bengoetxea, Riyaz Ah-
mad Bhat, Cristina Bosco, Sam Bowman, Giuseppe
G. A. Celano, Miriam Connor, Marie-Catherine
de Marneffe, Arantza Diaz de Ilarraza, Kaja Do-
brovoljc, Timothy Dozat, Toma Erjavec, Richrd
Farkas, Jennifer Foster, Daniel Galbraith, Filip Gin-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Gold-
berg, Berta Gonzales, Bruno Guillaume, Jan Haji,
Dag Haug, Radu Ion, Elena Irimia, Anders Jo-
hannsen, Hiroshi Kanayama, Jenna Kanerva, Si-
mon Krek, Veronika Laippala, Alessandro Lenci,
Nikola Ljubei, Teresa Lynn, Christopher Man-
ning, Ctlina Mrnduc, David Mareek, Hctor Mart-
nez Alonso, Jan Maek, Yuji Matsumoto, Ryan Mc-
Donald, Anna Missil, Verginica Mititelu, Yusuke
Miyao, Simonetta Montemagni, Shunsuke Mori,
Hanna Nurmi, Petya Osenova, Lilja vrelid, Elena
Pascual, Marco Passarotti, Cenel-Augusto Perez,
Slav Petrov, Jussi Piitulainen, Barbara Plank, Mar-
tin Popel, Prokopis Prokopidis, Sampo Pyysalo, Lo-
ganathan Ramasamy, Rudolf Rosa, Shadi Saleh, Se-
bastian Schuster, Wolfgang Seeker, Mojgan Ser-
aji, Natalia Silveira, Maria Simi, Radu Simionescu,
Katalin Simk, Kiril Simov, Aaron Smith, Jan tpnek,
Alane Suhr, Zsolt Sznt, Takaaki Tanaka, Reut Tsar-
faty, Sumire Uematsu, Larraitz Uria, Viktor Varga,
Veronika Vincze, Zdenk abokrtsk, Daniel Zeman,
and Hanzhi Zhu. 2015. Universal Dependencies 1.2.
http://universaldependencies.github.io/docs/.

Tim O’Gorman, Kristin Wright-Bettner, and Martha
Palmer. 2016. Richer event description: Integrating
event coreference with temporal, causal and bridg-
ing annotation. In Proceedings of the 2nd Workshop
on Computing News Storylines (CNS 2016), pages
47–56.



2917

Feng Pan, Rutu Mulkar-Mehta, and Jerry R Hobbs.
2007. Modeling and learning vague event dura-
tions for temporal reasoning. In Proceedings of the
22nd National Conference on Artificial Intelligence-
Volume 2, pages 1659–1662. AAAI Press.

Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The timebank corpus. In Corpus
linguistics, volume 2003, page 40. Lancaster, UK.

Rachel Rudinger, Aaron Steven White, and Benjamin
Van Durme. 2018. Neural models of factuality.
arXiv preprint arXiv:1804.02472.

Roger C Schank and Robert P Abelson. 1975. Scripts,
plans, and knowledge. In Proceedings of the
4th International Joint Conference on Artificial
Intelligence-Volume 1, pages 151–157. Morgan
Kaufmann Publishers Inc.

Natalia Silveira, Timothy Dozat, Marie-Catherine
De Marneffe, Samuel R Bowman, Miriam Connor,
John Bauer, and Christopher D Manning. 2014. A
gold standard dependency corpus for english. In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014), pages 2897–2904.

Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy
Puzikov, Ido Dagan, and Iryna Gurevych. 2017. In-
tegrating deep linguistic features in factuality pre-
diction over unified datasets. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 352–357.

William F Styler IV, Steven Bethard, Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, et al. 2014. Temporal annotation in the clin-
ical domain. Transactions of the Association for
Computational Linguistics, 2:143.

Julien Tourille, Olivier Ferret, Aurelie Neveol, and
Xavier Tannier. 2017. Neural architecture for tem-
poral relation extraction: A bi-lstm approach for de-
tecting narrative containers. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 224–230.

Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (* SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), volume 2,
pages 1–9.

Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal
relation identification. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 75–80. Association for Computational Lin-
guistics.

Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57–
62. Association for Computational Linguistics.

Aaron Steven White, Drew Reisinger, Keisuke Sak-
aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger,
Kyle Rawlins, and Benjamin Van Durme. 2016.
Universal decompositional semantics on universal
dependencies. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1713–1723, Austin, TX. Associ-
ation for Computational Linguistics.

Aaron Steven White, Rachel Rudinger, Kyle Rawl-
ins, and Benjamin Van Durme. 2018. Lexicosyn-
tactic inference in neural models. arXiv preprint
arXiv:1808.06232.

Jennifer Williams and Graham Katz. 2012. Extracting
and modeling durations for habits and events from
twitter. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Short Papers-Volume 2, pages 223–227. Asso-
ciation for Computational Linguistics.

Deirdre Wilson and Dan Sperber. 1998. Pragmatics
and time. Pragmatics and Beyond New Series, pages
1–22.

Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with markov logic. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1, pages 405–413. Asso-
ciation for Computational Linguistics.

Sheng Zhang, Rachel Rudinger, and Benjamin
Van Durme. 2017. An evaluation of predpatt and
open ie via stage 1 semantic role labeling. In IWCS
2017—12th International Conference on Computa-
tional Semantics (Short papers).



2918

A Data Collection

We concatenate two adjacent sentences to form
a combined sentence which allows us to capture
inter-sentential temporal relations. Considering all
possible pairs of events in the combined sentence
results into an exploding number of event-event
comparisons. Therefore, to reduce the total num-
ber of comparisons, we find the pivot-predicate of
the antecedent of the combined sentence as fol-
lows - find the root predicate of the antecedent and
if it governs a CCOMP, CSUBJ, or XCOMP, fol-
low that dependency to the next predicate until a
predicate is found that doesn’t govern a CCOMP,
CSUBJ, or XCOMP. We then take all pairs of the
antecedent predicates and pair every predicate of
the consequent only with the pivot-predicate. This
results into

�N
2

�
+M predicates instead of

�N+M
2

�

per sentence, where N and M are the number of
predicates in the antecedent and consequent re-
spectively. This heuristic allows us to find a pred-
icate that loosely denotes the topic being talked
about in the sentence. Figure 8 shows an example
of finding the pivot predicate.

Figure 8: Our heuristic finds fly as (the root of) the
pivot predicate in Has anyone considered that perhaps
George Bush just wanted to fly jets?

B Rejecting Annotations

We design multiple checks to detect potentially
bad annotations during our data collection. A sin-
gle assignment contains 5 annotations (predicate-
pairs). Once an annotation is flagged by any of
these checks, we may accept or reject the assign-
ment based on our subjective opinion about the
particular case. Annotations are flagged based on
the following conditions:

B.1 Time completion
Our pilot studies indicate a median time of roughly
4 minutes to complete a single assignment (5 an-
notations). We automatically reject any assign-

Figure 9: An example illustrating an inconsistency be-
tween the annotated slider positions and the durations

ment which is completed under a minute as we
believe that it is not plausible to finish the assign-
ment within a minute. We find that such annota-
tions mostly had default values annotated.

B.2 Same slider values

If all the beginning points and end-points in an as-
signment have the same values, we automatically
reject those assignments.

B.3 Same duration values

Sometimes we encounter cases where all duration
values in an assignment are annotated to have the
same value. This scenario , although unlikely,
could genuinely be an instance of correct anno-
tation. Hence we manually check for these cases
and reject only if the annotations look dubious in
nature based on our subjective opinion.

B.4 Inconsistency between the slider
positions and durations

Our protocol design allows us to detect poten-
tially bad annotations by detecting inconsistency
between the slider positions (beginning and end-
points) and the duration values of events in an an-
notated sentence. The annotator in Figure 9 as-
signs slider values for e1 (think) as [7,60] i.e. a
time-span of 53 and assigns its duration as min-
utes. But at the same time, the slider values for e2
(do) are annotated as [50,60] i.e. a time-span of
10, even though its duration is assigned as years.
This is an inconsistency as e2 has a smaller time-
span denoted by the sliders but has the longer du-
ration as denoted by years. We reject assignments
where more than 60% of annotations have this in-
consistency.



2919

C Inter-annotator agreement

Annotators were asked to approximate the relative
duration of the two events that they were annotat-
ing using the distance between the sliders. This
means that an annotation is coherent insofar as the
ratio of distances between the slider responses for
each event matches the ratio of the categorical du-
ration responses. We rejected annotations wherein
there was gross mismatch between the categori-
cal responses and the slider responses — i.e. one
event is annotated as having a longer duration but
is given a shorter slider response — but because
this does not guarantee that the exact ratios are
preserved, we assess that here using a canonical
correlation analysis (CCA; Hotelling 1936) be-
tween the categorical duration responses and the
slider responses.

Duration Relation

du
r(e
1)

du
r(e
2)

be
g(e
1)

en
d(e
1)

be
g(e
2)

en
d(e
2)

CC2

CC1
0.5

0.0

−0.5

Figure 10: Scores from canonical correlation analysis
comparing categorical duration annotations and slider
relation annotations.

Figure 10 shows the CCA scores. We find that
the first canonical correlation, which captures the
ratios between unequal events, is 0.765; and the
second, which captures the ratios between roughly
unequal events, is 0.427. This preservation of the
ratios is quite impressive in light of the fact that
our slider scales are bounded; though we hoped
for at least a non-linear relationship between the
categorical durations and the slider distances, we
did not expect such a strong linear relationship.

D Confidence Ratings

Annotators use the confidence scale in different
ways. Some always respond with totally confi-
dent whereas others use all five options. To cater
to these differences, we normalize the confidence
ratings for each event-pair using a standard ordi-
nal scale normalization technique known as ridit
scoring. In ridit scoring ordinal labels are mapped
to (0, 1) using the empirical cumulative distribu-
tion function of the ratings given by each annota-
tor. Ridit scoring re-weights the importance of a
scale label based on the frequency of its usage.

We weight both Ldur, and Lrel by the ridit-
scored confidence ratings of event durations and

event relations, respectively.

E Processing TempEval3 and
TimeBank-Dense

Since we require spans of predicates for our
model, we pre-process TB+AQ and TD by re-
moving all xml tags from the sentences and then
we pass it through Stanford CoreNLP 3.9.2
(Manning et al., 2014) to get the corresponding
conllu format. Roots and spans of predicates are
then extracted using PredPatt. To train the SVM
classifier, we use sklearn 0.20.0; Pedregosa
et al. 2011. We run a hyperparameter grid-search
over 4-fold CV with C: (0.1, 1, 10), and gamma:
(0.001, 0.01, 0.1, 1). The best performance on
cross-validation (C=1 and gamma=0.001) is then
evaluated on the test set of TE3 i.e. TE3-Platinum
(TE3-PT), and TD-test. For our purposes, the
identity and simultaneous relations in TB+AQ are
equivalent when comparing event-event relations.
Hence, they are collapsed into one single relation.

F Further analysis

We rotate the predicted slider positions in the re-
lation space defined in §3 and compare it with the
rotated space of actual slider positions. We see a
Spearman correlation of 0.19 for PRIORITY, 0.23
for CONTAINMENT, and 0.17 for EQUALITY. This
suggests that our model is best able to capture
CONTAINMENT relations and slightly less good
at capturing PRIORITY and EQUALITY relations,
though all the numbers are quite low compared
to the absolute ⇢ and relative ⇢ metrics reported
in Table 2. This may be indicative of the fact
that our models do somewhat poorly on predict-
ing more fine-grained aspects of an event relation,
and in the future it may be useful to jointly train
against the more interpretable PRIORITY, CON-
TAINMENT, and EQUALITY measures instead of
or in conjunction with the slider values.


