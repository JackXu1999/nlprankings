



















































SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 902–914
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

902

SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for
Evaluating Natural Language Inference

Martin Schmitt and Hinrich Schütze
Center for Information and Language Processing (CIS)

LMU Munich, Germany
martin@cis.lmu.de

Abstract

We present SherLIiC,1 a testbed for lexical in-
ference in context (LIiC), consisting of 3985
manually annotated inference rule candidates
(InfCands), accompanied by (i) ~960k un-
labeled InfCands, and (ii) ~190k typed tex-
tual relations between Freebase entities ex-
tracted from the large entity-linked corpus
ClueWeb09. Each InfCand consists of one of
these relations, expressed as a lemmatized de-
pendency path, and two argument placehold-
ers, each linked to one or more Freebase types.
Due to our candidate selection process based
on strong distributional evidence, SherLIiC is
much harder than existing testbeds because
distributional evidence is of little utility in the
classification of InfCands. We also show that,
due to its construction, many of SherLIiC’s
correct InfCands are novel and missing from
existing rule bases. We evaluate a number of
strong baselines on SherLIiC, ranging from se-
mantic vector space models to state of the art
neural models of natural language inference
(NLI). We show that SherLIiC poses a tough
challenge to existing NLI systems.

1 Introduction

Lexical inference (LI) can be seen as a focused
variant of natural language inference (NLI), also
called recognizing textual entailment (Dagan et al.,
2013). Recently, Gururangan et al. (2018) showed
that annotation artifacts in current NLI testbeds
distort our impression of the performance of state
of the art systems, giving rise to the need for new
evaluation methods for NLI. Glockner et al. (2018)
investigated LI as a way of evaluating NLI systems
and found that even simple cases are challenging to
current systems. In this paper, we release SherLIiC,
a testbed specifically designed for evaluating a sys-
tem’s ability to solve the hard problem of modeling
lexical entailment in context.

1https://github.com/mnschmit/SherLIiC

(1) troponymy
ORGF[A] is granting to EMPL[B]
⇒ ORGF[A] is giving to EMPL[B]

(2)
synonymy + ORGF[A] is supporter of ORGF[B]
derivation ⇒ ORGF[A] is backing ORGF[B]

(3)
typical AUTH[A] is president of LOC[B]
actions ⇒ AUTH[A] is representing LOC[B]

(4)
script PER[A] is interviewing AUTH[B]
knowledge ⇒ PER[A] is asking AUTH[B]

(5)
common sense ORGF[A] claims LOC[B]
knowledge ⇒ ORGF[A] is wanting LOC[B]

Table 1: Examples of SherLIiC InfCands and NLI
challenges they cover. ORGF=organization founder,
EMPL=employer, AUTH=book author, LOC=location,
POL=politician, PER=person.

Levy and Dagan (2016) identified context-
sensitive – as opposed to “context-free” – entail-
ment as an important evaluation criterion and cre-
ated a dataset for LI in context (LIiC). In their data,
WordNet (Miller, 1995; Fellbaum, 2005) synsets
serve as context for one side of a binary relation,
but the other side is still instantiated with a sin-
gle concrete expression. We aim to improve this
setting in two ways.

First, we type our relations on both sides, thus
making them more general. Types provide a con-
text that can help in disambiguation and at the same
time allow generalization over contexts because ar-
guments of the same type are represented abstractly.
An example for the need for disambiguation is the
verb “run”. “run” entails “lead” in the context of
PERSON / COMPANY (“Bezos runs Amazon”). But
in the context of COMPUTER / SOFTWARE, “run”
entails “execute”/“use” (“my mac runs macOS”).
Here, types help find the right interpretation.

Second, we only consider relations between
named entities (NEs). Inference mining based on
non-NE types such as WordNet synsets (e.g., ANI-
MAL, PLANT LIFE) primarily discovers facts like
“parrotfish feed on algae”. In contrast, the focus

https://github.com/mnschmit/SherLIiC


903

on NEs makes it more likely that we will capture
events like “Walmart closes gap with Amazon” and
thus knowledge about event entailment like [“A
is closing gap with B”⇒ “B is having lead over
A”] that is substantially different from knowledge
about general facts.

In more detail, we create SherLIiC as follows.
First, we extract verbal relations between Freebase
(Bollacker et al., 2008) entities from the entity-
linked web corpus ClueWeb09 (Gabrilovich et al.,
2013).2 We then divide these relations into typable
subrelations based on the most frequent Freebase
types found in their extensions. We then create a
large set of inference rule candidates (InfCands),
i.e., premise-hypothesis-pairs of verbally expressed
relations. Finally, we use Amazon Mechanical Turk
to classify each InfCand in a randomly sampled
subset as entailment or non-entailment.

In summary, our contributions are the follow-
ing: (1) We create SherLIiC, a new resource for
LIiC, consisting of 3985 manually annotated Inf-
Cands. Additionally, we provide ~960k unlabeled
InfCands (SherLIiC-InfCands), and the typed event
graph SherLIiC-TEG, containing ~190k typed tex-
tual binary relations between Freebase entities.
(2) SherLIiC is harder than existing testbeds be-
cause distributional evidence is of limited utility
in the classification of InfCands. Thus, SherLIiC
is a promising and challenging resource for devel-
oping NLI systems that go beyond shallow seman-
tics. (3) Human-interpretable knowledge graph
types serve as context for both sides of InfCands.
This makes InfCands more general and boosts
the number of event-like relations in SherLIiC.
(4) SherLIiC is complementary to existing collec-
tions of inference rules as evidenced by the low
recall these resources achieve (cf. Table 3). (5) We
evaluate a large number of baselines on SherLIiC.
The best-performing baseline makes use of typing.
(6) We demonstrate that existing NLI systems do
poorly on SherLIiC.

2 Generation of InfCands

This section describes creation (§ 2.1) and typing
(§ 2.2) of the typed event graph SherLIiC-TEG and
then the generation of SherLIiC-InfCands (§ 2.3).

2.1 Relation Extraction

For each sentence s in ClueWeb09 that contains
at least two entity mentions, we use MaltParser

2http://lemurproject.org/clueweb09

(Nivre et al., 2007) to generate a dependency graph,
where nodes are labeled with their lemmas and
edges with dependency types. We take all shortest
paths between all combinations of two entities in s
and represent them by alternating edge and node la-
bels. As we want to focus on relations that express
events, we only keep paths with a nominal subject
on one end. We also apply heuristics to filter out
erroneous parses. See Appendix A for heuristics
and Table 5 for examples of relations.
Notation. Let R denote the set of extracted re-
lations. A relation R ∈ R is represented as a
set of pairs of Freebase entities (its extension):
R ⊆ E × E , with E the set of Freebase entities.
Let π1, π2 be functions that map a pair to its first
or second entry, respectively. By abuse of notation,
we also apply them to sets of pairs. Finally, let T
be the set of Freebase types and τ : E → 2T the
function that maps an entity to the set of its types.

2.2 Typing

We define a typable subrelation of R ∈ R as a
subrelation whose entities in each argument slot
share at least one type, i.e., an S ⊆ R such that:

∀i ∈ {1, 2} : ∃t ∈ T : t ∈
⋂

e∈πi(S)

τ(e)

We compute the set Typek2(R) of the (up to) k
2

largest typable subrelations of R and use them in-
stead of R. First, for each argument slot i of the
binary relation R, the k types tij (with 1 ≤ j ≤ k)
are computed that occur most often in this slot:

tij := argmax
t

∣∣{ p ∈ R | t ∈ τ ij(πi(p))}∣∣
with

τ i1(e) = τ(e)

τ ij+1(e) = τ
i
j(e)−

{
tij
}

Then, for each pair

(s, u) ∈
{ (
t1j , t

2
l

)
| j, l ∈ {1, . . . , k}

}
of these types, we construct a subrelation

Rs,u := { (e1, e2) ∈ R | s ∈ τ(e1), u ∈ τ(e2) }

If |Rs,u| ≥ rmin, Rs,u is included in Typek2(R).
In our experiments, we set k = rmin = 5.

http://lemurproject.org/clueweb09


904

The type signature (tsg) of a typed relation T is
defined as the pair of sets of types that is common
to first (resp. second) entities in the extension:

tsg(T ) =

 ⋂
e∈π1(T )

τ(e),
⋂

e∈π2(T )

τ(e)


Incomplete type information. Like all large
knowledge bases, Freebase suffers from incom-
pleteness: Many entities have no type. To avoid
losing information about relations associated with
such entities, we introduce a special type > and
define argmaxt |∅| := >. We define the relations
Rs,>,R>,u andR>,> to have no type restriction on
entities in a > slot. This concerns approximately
17.6% of the relations in SherLIiC-TEG.

2.3 Entailment Discovery
Our discovery procedure is based on Sherlock
(Schoenmackers et al., 2010). For the InfCand
A⇒ B (A,B ∈ R), we define the relevance score
Relv, a metric expressing Sherlock’s stat. relevance
criterion P (B | A)� P (B) (cf. Salmon, 1971).

Relv(A,B) :=
P (B | A)
P (B)

=
|A ∩B| |E × E|
|A| |B|

Our significance score σ(A,B) is a scaled version
of the significance test lrs used by Sherlock:

P (B | A) lrs(A,B) = |A ∩B| lrs(A,B)
|A|

with lrs(A,B) (likelihood ratio statistic) defined as

2 |A|
∑

H∈{B,¬B}

P (H | A) log(Relv(A,H)).

Additionally, we introduce the entity support ratio:

esr(A,B) :=

∣∣∣⋃i∈{1,2} πi(A ∩B)∣∣∣
2 |A ∩B|

This score measures the diversity of entities in
A ∩ B. We found that many InfCands involve
a few frequent entities and so obtain high Relv and
σ scores even though the relations of the rule are se-
mantically unrelated. esr penalizes such InfCands.

We apply our three scores defined above to all
possible pairs of relations (A,B) ∈ R × R and
accept a rule iff all of the following criteria are met:

1. ∀i ∈ {1, 2} : πi(tsg(A⇒ B)) 6= ∅
2. |A ∩B| ≥ rmin

Fact: location[B] is annexing location[A] .
Examples for location[B]: Russia / USA / Indonesia
Examples for location[A]: Cuba / Algeria / Crimea

� fact incomprehensible

Please answer the following questions:
Is it certain that location[B] is taking control of location[A]?

yes no incomprehensible
Is it certain that location[B] is taking location[A]?

yes no incomprehensible
Is it certain that location[B] is bordered by location[A]?

yes no incomprehensible

Figure 1: Annotation Interface on Amazon MTurk

3. ∀i ∈ {1, 2} : |πi(A ∩B)| ≥ rmin
4. Relv(A,B) ≥ ϑrelv
5. σ(A,B) ≥ ϑσ
6. esr(A,B) ≥ ϑesr

where tsg(A ⇒ B) is component-wise intersec-
tion of tsg(A) and tsg(B) and ϑrelv = 1000,
ϑσ = 15, ϑesr = 0.6. We found these numbers by
randomly sampling InfCands and inspecting their
scores. Typing lets us set these thresholds higher,
benefitting the quality of SherLIiC-InfCands.

Lastly, we apply Schoenmackers et al. (2010)’s
heuristic to only accept the 100 best-scoring
premises for each hypothesis. For each hypothesis
B, we rank all possible premises A by the product
of the three scores and filter out cases where A and
B only differ in their types.

3 Crowdsourced Annotation

SherLIiC-InfCands contains ~960k InfCands. We
take a random sample of size 5267 and annotate it
using Amazon Mechanical Turk (MTurk).

3.1 Task Formulation

We are asking our workers the same kind of ques-
tions as Levy and Dagan (2016) did. We likewise
form batches of sentence pairs to reduce annota-
tion cost. Instead of framing the task as judging
the appropriateness of answers, however, we state
the premise as a fact and ask workers about its en-
tailed consequences, i.e., we ask for each candidate
hypothesis whether it is certain that it is also true.
Fig. 1 shows the annotation interface schematically.

We use a morphological lexicon (XTAG Re-
search Group, 2001) and form the present tense
of a dependency path’s predicate. If a sentence
is flagged incomprehensible (e.g., due to a parse
error), it is excluded from further evaluation.



905

Annotated Subset of SherLIiC-InfCands
Validated InfCands 3985
Balance yes/no 33% / 67%
Pairs with unanimous gold label 53.0%
Pairs with 1 disagreeing annotation 27.4%
Pairs with 2 disagreeing annotations 19.6%
Individual label = gold label 86.7%

Table 2: Statistics for crowd-annotated InfCands. The
gold label is the majority label.

We put premise and hypothesis in the present
(progressive if suitable) based on the intuition that
a pair is only to be considered an entailment if the
premise makes it necessary for the hypothesis to be
true at the same time. This condition of simultane-
ity follows the tradition of datasets such as SNLI
(Bowman et al., 2015) – in contrast to more lenient
evaluation schemes that consider a rule to be cor-
rect if the hypothesis is true at any time before or
after the reference time of the premise (cf. Lin and
Pantel, 2001; Lewis and Steedman, 2013).

3.2 Annotation Quality
We imposed several qualification criteria on crowd
workers: number of previous jobs on MTurk, over-
all acceptance rate and a test that each worker had
to pass. Some workers still had frequent low agree-
ment with the majority. However, in most cases
we obtained a clear majority annotation. These
annotations were then used to automatically detect
workers with low trust, where trust is defined as
the ratio of submissions agreeing with the majority
answer and a worker’s total number of submissions.
We excluded workers with a trust of less than 0.8
and collected replacement annotations until we had
at least five annotations per InfCand.

Table 2 shows that workers agreed unanimously
for 53% and that the maximal number of two dis-
agreements only occurs for 19.6%. The high num-
ber of times an individual agrees with the gold label
suggests that humans can do the task reliably. Inter-
estingly, the number of disagreements is not evenly
distributed among the two classes entailment/non-
entailment. If the majority agrees on entailment, it
is comparatively much more likely that at least one
of the workers disagrees (cf. Fig. 2). This suggests
that our workers were strict and keen on achieving
high precision in their annotations.

4 Baselines

We split our annotated data 25:75 into SherLIiC-
dev and SherLIiC-test, stratifying on annotated la-

no yes
Class label

0

200

400

600

800

1000

1200

1400

1600

Nu
m

be
r o

f a
nn

ot
at

io
ns

 d
isa

gr
ee

in
g 

wi
th

 th
e 

m
aj

or
ity 0

1
2

Figure 2: Number of disagreements per class label on
all annotations.

bel (yes/no) and number of disagreements with the
majority (0/1/2). If unanimity of annotations marks
particularly clear cases, we figure they should be
evenly distributed on dev and test.

To establish the state of the art for SherLIiC,
we evaluate a number of baselines. Input to these
baselines are either the dependency paths or the
sentences that were presented to the crowd workers.
Baselines that require a threshold to be used as
binary classifiers are tuned on SherLIiC-dev.
Lemma baseline. Following Levy and Dagan
(2016), this baseline classifies an InfCand as valid
if the following holds true for the premise p and
hypothesis h after lemmatization: (1) p contains
all of h’s content words,3 (2) p’s and h’s predicates
are identical, and (3) the relations’ active/passive
voice matches their arguments’ alignment.
Rule collection baselines. Berant I (Berant
et al., 2011) and Berant II (Berant, 2012)4

are entailment graphs. PPDB is the largest collec-
tion (XXXL) of PPDB 2.0 (Pavlick et al., 2015).5

Patty is a collection of relational patterns, con-
sisting of ontological types, POS placeholders
and words. We use the version extracted from
Wikipedia with Freebase types (Nakashole et al.,
2012). Schoenmackers is the rule collection
released by Schoenmackers et al. (2010). Chirps
is an ever-growing6 predicate paraphrase database
extracted via event coreference in news Tweets
(Shwartz et al., 2017b). All Rules denotes the
union of all of these rule bases. For rules with type
(or POS) constraints, we ignore these constraints

3We use the stop word list of nltk (Loper and Bird, 2002).
4We use default threshold 0.
5We ignore stop words and punctuation for phrases.
6We use the version downloaded on May 28, 2019.



906

to boost recall. We will see that even with these
recall-enhancing measures, the majority of our cor-
rect InfCands is not covered by existing rule bases.

Word2vec baselines. word2vec is based on
Mikolov et al. (2013b)’s pre-trained word em-
beddings of size 300. We average them to ob-
tain a vector representation of relations consist-
ing of multiple words and use cosine similar-
ity to judge a relation pair. typed rel emb
(resp. untyped rel emb) is obtained by train-
ing word2vec skip-gram (Mikolov et al., 2013a)
with vector size 300 and otherwise default param-
eters on a synthetic corpus representing the exten-
sions of typed (resp. untyped) relations. The corpus
is obtained by writing out one entity-relation-entity-
triple per line, where each entity is prefixed with
the argument slot it belongs to. w2v+typed rel
(resp. w2v+untyped rel) produces its score
by summing the scores of word2vec and
typed rel emb (resp. untyped rel emb).

Some type signatures (tsgs) benefit more
from type-informed methods than others. For
example, the correct inference [INFLUENCER
is explaining in WRITTEN WORK ⇒ INFLU-
ENCER is writing in WRITTEN WORK] is
detected by w2v+typed rel, but not by
w2v+untyped rel. We therefore combine
these two methods by using, for each tsg, the
method that works better for that tsg on dev. (For
tsgs not occurring in dev, we take the method that
works better for the individual types occurring
in the tsg. We use untyped embeddings if all
else fails.) We refer to this combination as
w2v+tsg rel emb.

Knowledge graph embedding baselines. As
SherLIiC-TEG has the structure of a knowledge
graph (KG), we also evaluate the two KG embed-
ding methods TransE (Bordes et al., 2013) and
ComplEx (Trouillon et al., 2016), as provided by
the OpenKE framework (Han et al., 2018).

Asymmetric baselines. Entailment models built
upon cosine similarity are symmetric whereas en-
tailment is not. Therefore many asymmmetric mea-
sures based on the distributional inclusion hypoth-
esis have been proposed (Kotlerman et al., 2010;
Santus et al., 2014; Shwartz et al., 2017a; Roller
et al., 2018). We consider WeedsPrec (Weeds
et al., 2004) and invCL (Lenci and Benotto, 2012),
which have strong empirical results on hypernym
detection. We use cooccurrence counts with entity
pairs as distributional representation of a relation.

Supervised NLI models. As LIiC is a special
case of NLI, our dataset can also be used to eval-
uate the generalization capabilities of supervised
models trained on large NLI datasets. We pick
ESIM (Chen et al., 2017), a state-of-the-art super-
vised NLI model, trained on MultiNLI (Williams
et al., 2018) as provided by the framework Jack
the Reader (Weissenborn et al., 2018). Input to
ESIM are the sentences from the annotation pro-
cess with placeholders instantiated by entities ran-
domly picked from the example lists that had also
been shown to the crowd workers (cf. Fig. 1). As
we want to measure ESIM’s capacity to detect en-
tailment, we map its prediction of both neutral and
contradiction to our non-entailment class.
Sherlock+ESR. We also evaluate the candidate
scoring method inspired by Schoenmackers et al.
(2010) that created the data in the first place.
We again combine the three scores described in
§ 2 by multiplication. The low performance of
Sherlock+ESR (cf. Table 3) is evidence that the
dataset is not strongly biased in its favor and thus
is promising as a general evaluation benchmark.

5 Experimental Results and Discussion

Quantitative observations. Table 3 summarizes
the performance of our baselines on predicting the
entailment class for SherLIiC-dev and -test.

Rule collections (lines 1–6) have recall between
0.119 and 0.308; the recall of their union (line 7)
is only 0.483 on dev and 0.493 on test, showing
that we found indeed new valid inferences missing
from existing rule bases.

The state-of-the-art neural model ESIM does not
generalize well from MultiNLI (its training set) to
LIiC. In fact, it hardly improves on the baseline that
always predicts entailment (Always yes). Our
dataset was specifically designed to only contain
good InfCands based on distributional features. So
it poses a challenge to models that cannot make the
fine semantic distinctions necessary for LIiC.

Turning to vector space models (lines 11–24),
dense relation representations (lines 12, 13) predict
entailment better than sparse models (lines 17–20)
although they cannot use asymmetric measures.

KG embeddings (lines 21–24) do not seem at
all appropriate for measuring the similarity of re-
lations. First, their performance is very close to
Always yes. Second, their F1-optimal thresh-
olds are very low – even negative. This suggests
that their relation vectors do not contain any helpful



907

dev test

Baseline θ∗ P R F1 P R F1

1 Berant I – 0.699 0.154 0.252 0.762 0.126 0.216
2 Berant II – 0.800 0.181 0.296 0.774 0.186 0.300
3 PPDB – 0.631 0.211 0.317 0.621 0.240 0.347
4 Patty – 0.795 0.187 0.303 0.779 0.153 0.256
5 Schoenmackers – 0.780 0.139 0.236 0.849 0.119 0.208
6 Chirps – 0.370 0.308 0.336 0.341 0.295 0.316
7 All Rules – 0.418 0.483 0.448 0.404 0.493 0.444

8 Lemma – 0.900 0.109 0.194 0.907 0.089 0.161
9 Always yes – 0.332 1.000 0.499 0.333 1.000 0.499

10 ESIM – 0.391 0.831 0.532 0.390 0.833 0.531

11 word2vec 0.321 0.556 0.625 0.589 0.520 0.606 0.559
12 typed rel emb 0.864 0.561 0.568 0.565 0.532 0.486 0.508
13 untyped rel emb 0.613 0.511 0.740 0.605 0.499 0.672 0.572

14 w2v+typed rel 1.106 0.549 0.710 0.619 0.523 0.688 0.594
15 w2v+untyped rel 0.884 0.565 0.740 0.641 0.528 0.695 0.600
16 w2v+tsg rel emb 0.884 0.566 0.776 0.655 0.518 0.727 0.605

17 WeedsPrec (typed) 0.073 0.335 0.994 0.501 0.333 0.988 0.498
18 WeedsPrec (untyped) 0.057 0.403 0.807 0.538 0.386 0.783 0.517
19 invCL (typed) 0.000 0.332 1.000 0.499 0.333 1.000 0.499
20 invCL (untyped) 0.148 0.362 0.876 0.512 0.357 0.863 0.505

21 TransE (typed) −0.922 0.336 1.000 0.503 0.333 0.991 0.498
22 TransE (untyped) −0.476 0.340 0.964 0.503 0.332 0.942 0.491
23 ComplEx (typed) −0.033 0.339 0.955 0.500 0.337 0.949 0.497
24 ComplEx (untyped) −0.030 0.340 0.952 0.501 0.334 0.939 0.493

25 Sherlock+ESR 9.460 · 105 0.504 0.592 0.544 0.491 0.526 0.508

Table 3: Precision, recall and F1 score on SherLIiC-dev and -test. All baselines run on top of Lemma. Thresholds
(θ∗) are F1-optimized on dev. Best result per column is set in bold.

information for the task. These methods were not
developed to compare relations; the lack of useful
information is still surprising and thus is a promis-
ing direction for future work on KG embeddings.

General purpose dense representations
(word2vec, line 11) perform comparatively
well, showing that, in principle, they cover the
information necessary for LIiC. Embeddings
trained on our relation extensions SherLIiC-TEG
(line 13), however, can already alone achieve better
performance than word2vec embeddings alone.

In general, type-informed relation embeddings
seem to have a disadvantage compared to unre-
stricted ones (e.g., cf. lines 12 and 13) – presumably
because type-informed baselines have training sets
that are smaller (due to filtering) and sparser (since
relations are split up according to type signatures).
The combination of general word2vec and special-
ized relation embeddings (lines 14–16), however,
consistently brings gains. This indicates that distri-
butional word properties are complementary to the
relation extensions our method extracts. So using
both sources of information is promising for future
research on modeling relational semantics.

w2v+tsg rel emb is the best-performing
method. It combines typed and untyped relation
embeddings as well as general-purpose word2vec
embeddings. Even though one cannot rely on typed
extensions only, this shows that incorporating type
information is beneficial for good performance.

We use w2v+tsg rel emb to provide a noisy
annotation for SherLIiC-InfCands. This is a useful
resource because learning from noisy labels has
been well studied (Frénay and Verleysen, 2014;
Hendrycks et al., 2018) and is often beneficial.
Qualitative observations. Although SherLIiC’s
creation is based on the same method that was used
to create Schoenmackers, SherLIiC is funda-
mentally different for several reasons: (1) The rule
sets are complementary (cf. the low recall of 0.139
and 0.119 in Table 3). (2) The majority of rules
in Schoenmackers has more than one premise,
leaving only ~13k InfCands in Schoenmackers
compared to ~960k in SherLIiC-InfCands that fit
the format of NLI. (3) Schoenmackers is fil-
tered more aggressively with the goal of maximiz-
ing the number of correct rules. This, however,
makes it inadequate as a challenging benchmark be-



908

(1) PERSON[A] is REGION[B]’s ruler⇒ PERSON[A] is dictator of REGION[B]

(2) LOCATION[A] is fighting with ORGF[B]⇒ LOCATION[A] is allied with ORGF[B]

(3) ORGF[A] is coming into LOCATION[B]⇒ ORGF[A] is remaining in LOCATION[B]

(4) ORGF[A] is seeking from ORGF[B]⇒ ORGF[B] is giving to ORGF[A]

(5) LOCATION[A] is winning war against LOCATION[B]⇒ LOCATION[A] is declaring war on LOCATION[B]

Table 4: False positives for each of the three
best-performing baselines taken from SherLIiC-dev.
ORGF=organization founder.

cause the performance of Always yes would be
close to 100%. (4) SherLIiC is focused on events.
When linking the relations from SherLIiC-TEG
back to their surface forms in the corpus, 80% of
them occur at least once in the progressive, which
suggests that the large majority of our relations
indeed represent events.

Taking a closer look at SherLIiC, we see that
the data require a large variety of lexical knowl-
edge even though their creation has been entirely
automatic. Table 1 shows five positively labeled
examples from SherLIiC-dev, each highlighting
a different challenge for statistical models that is
crucial for NLI. (1) is an instance of troponymy:
“granting” is a manner or kind of “giving”. This
is the verbal equivalent to nominal hyponymy. (2)
combines synonymy (“support” ⇔ “back”) with
morph. derivation. (3) can only be classified cor-
rectly if one knows that it is one of the typical
actions of a president to represent their country.
(4) requires knowledge about the typical course
of events when interviewing someone. A typical
interview involves asking questions. (5) can only
be detected with common sense knowledge that
goes even beyond that: you generally only claim
something if you want it.

An error analysis of the three best-performing
baselines (lines 14–16 in Table 3) reveals that none
of them was able to detect the five correct InfCands
from Table 1. Explicit modeling of one of the phe-
nomena described above seems a promising direc-
tion for future research to improve recall. Table 4
shows five cases where InfCands were incorrectly
labeled as entailment. (1) shows the importance
of modeling directionality: every “dictator” is a
“ruler” but not vice versa. (2) shows a well-known
problem in representation learning from cooccur-

nsubj–X–prep–of–obj ⇔ nsubj–X–poss
A is an ally of B A is B’s ally
nsubj–X–prep–in–obj ⇔ nsubj–X–poss
A is the capital in B A is B’s capital
nsubjpass–X–prep–by–obj ⇔ obj–X–nsubj
A is followed by B B follows A
nsubj–one–prep–of–obj–X–obj ⇔ nsubj–X–obj
A is one of the countries in B A is a country in B
nsubj–capital–conj–X–obj ⇒ nsubj–X–obj
A is the capital and biggest city in B A is a city in B

nsubj–Xer–prep–of–obj ⇔ nsubj–X–obj
A is a teacher of B A teaches B
nsubj–co-Xer–prep–of–obj ⇒ nsubj–X–obj
A is a co-founder of B A founds B
nsubj–reX–obj ⇒ nsubj–X–obj
A rewrites B A writes B
nsubj–overX–obj ⇒ nsubj–X–obj
A overtakes B A takes B

nsubj–agree–xcomp–X–obj ⇒ nsubj–X–obj
A agrees to buy B A buys B
nsubjpass–force–xcomp–X–obj ⇒ nsubj–X–obj
A is forced to leave B A leaves B
nsubjpass–elect–xcomp–X–obj ⇔ nsubj–X–obj
A is elected to be governor of B A is governor of B
nsubj–go–xcomp–X–obj ⇒ nsubj–X–obj
A is going to beat B A beats B
nsubj–try–xcomp–X–obj ⇒ nsubj–X–obj
A tries to compete with B A competes with B
nsubj–decide–xcomp–X–obj ⇒ nsubj–X–obj
A decides to move to B A moves to B
nsubjpass–expect–xcomp–X–obj ⇒ nsubj–X–obj
A is expected to visit B A visits B

Table 5: Most frequent meta rules (top), character level
meta rules (middle), and implicative verb meta rules
(bottom). Bold: Words corresponding to X.

rence: antonyms tend to be close in the embed-
ding space (Mohammad et al., 2008; Mrkšić et al.,
2016). The other examples show other types of
correlation that models relying entirely on distri-
butional information will fall for: the outcome of
events like “coming into a country” or “seeking
something from someone” are in general uncertain
although possible outcomes like “remaining in said
country” (3) or “being given the object of desire”
(4) will be highly correlated with them. Finally,
better models will also take into account the simul-
taneity constraint: “winning a war” and “declaring
a war” (5) rarely happen at the same time.

6 Meta Rules and Implicative Verbs

In addition to the annotated data, we also make
available all ~960k SherLIiC-InfCands found by
our unsupervised algorithm. SherLIiC-InfCands’s
distribution is similar enough to our labeled dataset



909

to be useful for domain adaptation, representation
learning and other techniques when working on
LIiC. It can also be investigated on its own in a
purely unsupervised fashion as we will show now.

We can find easily interpretable patterns by look-
ing for cases where premise and hypothesis of an
InfCand have common parts. By masking these
parts (X), we can abstract away from concrete in-
stances and interesting meta rules emerge (Table 5).
The most common patterns represent reasonable
equivalent formulations, e.g., active/passive voice
or “be Y ’sX⇔ beX of Y ” (the in-variant coming
from a lot of location-typed rule instances). The
fifth most frequent pattern could still be formulated
in an even more abstract way but shows already
that the general principle of a conjunction Y ∧X
implying one of its components X can be learned.

If we search for meta rules whose X is part of a
lemma (rather than a longer dependency path), we
discover cases of derivational morphology such as
agent nouns (e.g., ruler, leader) and sense preserv-
ing verb prefixes (e.g., re-write, over-react).

Finally, we observe several implicative verbs
(verbs that entail their complement clauses) in their
typical pattern V to X ⇒ X . A lot of these verbs
are not traditional implicatives, but are called de
facto implicatives by Pavlick and Callison-Burch
(2016) – who argue for the importance of data-
driven approaches to detecting de facto implica-
tives. The meta rule discovery method just de-
scribed is such a data-driven approach.

7 Related Work

NLI challenge datasets. A lot of work exists that
aims at uncovering weaknesses in state-of-the-art
NLI models. Several approaches are based on
modifications of popular datasets, such as SNLI
or MultiNLI. These modifications range from sim-
ple rule-based transformations (Naik et al., 2018) to
rewritings generated by genetic algorithms (Alzan-
tot et al., 2018) or adversarial neural networks
(Zhao et al., 2018). Lalor et al. (2016) constructed
an NLI test set by judging the difficulty of the sen-
tence pairs in a small SNLI subset based on crowd-
sourced human responses via Item Response The-
ory. These works are related as they, too, challenge
existing NLI models with new data but orthogonal
to ours as their goal is not to measure a model’s
knowledge about lexical inference in context.

Glockner et al. (2018) modified SNLI by replac-
ing one word from a given sentence by a synonym,

(co-)hyponym, hypernym or antonym to build a
test set that requires NLI systems to use lexical
knowledge. They rely on WordNet’s lexical taxon-
omy. This, however, is difficult for verbs because
their semantics depends more on context. Finally,
Glockner et al. (2018)’s dataset has a strong bias
for contradiction whereas our dataset is specifically
designed to contain cases of entailment.

Our work is more closely related to the dataset
by Levy and Dagan (2016), who frame relation en-
tailment as the task of judging the appropriateness
of candidate answers. Their hypothesis is that an
answer is only appropriate if it entails the predi-
cate of the question. This is often but by no means
always true; certain questions imply additional in-
formation. Consider: “Which country annexed
country[B]?” The answer candidate “country[A]
administers country[B]” might be considered valid,
given that it is unlikely that one country annexes B
and another country administers it. The inference
administer⇒ annex, however, does not hold. Be-
cause of these difficulties, we follow the more tra-
ditional approach (Zeichner et al., 2012) of asking
about consequences of a given fact (the premise).

Relation extraction. Some works (Schoenmack-
ers et al., 2010; Berant, 2012; Zeichner et al., 2012)
rely on the output from open information extrac-
tion systems (Banko et al., 2007; Fader et al., 2011).
A more flexible approach is to represent relations
as lexicalized paths in dependency graphs (Lin
and Pantel, 2001; Szpektor et al., 2004), some-
times with semantic postprocessing (Shwartz et al.,
2017b) and/or retransforming into textual patterns
(Nakashole et al., 2012). We, too, choose the latter.

Relation typing. Typing relations has become
standard in inference mining because of its use-
fulness for sense disambiguation (Schoenmackers
et al., 2010; Nakashole et al., 2012; Yao et al., 2012;
Lewis and Steedman, 2013). Still some resources
only provide types for one argument slot of their bi-
nary relations (Levy and Dagan, 2016) or no types
at all (Zeichner et al., 2012; Berant, 2012; Shwartz
et al., 2017b). Our InfCands are typed in both argu-
ment slots, which both facilitates disambiguation
and makes them more general.

Some works (Yao et al., 2012; Lewis and Steed-
man, 2013) learn distributions over latent type sig-
natures for their relations via topic modeling. A
large disadvantage of latent types is their lack of
intuitive interpretability. By design, our KG types
are meaningful and human-interpretable.



910

Schoenmackers et al. (2010) type common
nouns based on cooccurrence with class nouns
identified by Hearst patterns (Hearst, 1992) and
later try to filter out unreasonable typings by us-
ing frequency thresholds and PMI. As KG entities
are manually labeled with their correct types, we
do not need this kind of heuristics. Furthermore,
in contrast to this ad-hoc type system, KG types
are the result of a KG design process. Notably,
Freebase types function as interfaces, i.e., permit
type-specific properties to be added, and are thus
inherently motivated by relations between entities.

Lexical ontologies, such as WordNet (as used
by Levy and Dagan, 2016) likewise lack this con-
nection between relations and types. Moreover,
relations between real-world entities are more of-
ten events than relations between common nouns.
Thus, in contrast to existing resources that do not
restrict relations to KG entities, SherLIiC contains
more event-like relations.

Nakashole et al. (2012) also use KG types as con-
text for their textual patterns. They simply create
a new relation for each possible type combination
for each entity occurring with a pattern and each
possible type of this entity. It is unclear how the
combinatorial explosion and the resulting sparsity
affects pattern quality. Our approach of succes-
sively splitting a typewise heterogenous relation
into its k largest homogenous subrelations aims at
finding only the most typical types for an action and
our definition of type signature as intersection of
all common types avoids unnecessary redundancy.

Entailment candidate collection. Distributional
features are a common choice for paraphrase detec-
tion and relation clustering (Lin and Pantel, 2001;
Szpektor et al., 2004; Sekine, 2005; Yao et al.,
2012; Lewis and Steedman, 2013).

The two most important alternatives are bilin-
gual pivoting (Ganitkevitch et al., 2013) – which
identifies identically translated phrases in bilingual
corpora – and event coreference in the news (Xu
et al., 2014; Zhang et al., 2015; Shwartz et al.,
2017b) – which relies on lexical variability in two
articles or headlines referring to the same event.
We specifically focus on distributional information
for our InfCand collection because current models
of lexical semantics are also mainly based on that
(e.g., Grave et al., 2017). Our goal is not to build
a resource free of typical mistakes made by distri-
butional approaches but to provide a benchmark to
study the progress on overcoming them (cf. § 5).

Another difference to aforementioned works is
that we explicitly model unidirectional entailment
as opposed to bidirectional synonymy (cf. Table 4,
(1)). Here one can distinguish a learning-based
approach (Berant, 2012), where an SVM classi-
fier with various features is trained on lexical on-
tologies like WordNet, followed by the application
of global transitivity constrains to enhance consis-
tency, and probabilistic models of noisy set inclu-
sion in the tradition of the distributional inclusion
hypothesis (Schoenmackers et al., 2010; Nakashole
et al., 2012). We adapt Sherlock, an instance of the
latter, for its simplicity and effectiveness.

8 Conclusion

We presented SherLIiC, a new challenging testbed
for LIiC and NLI, based on typed textual relations
between named entities (NEs) from a KG. The
restriction to NEs (as opposed to common nouns)
allowed us to harness more event-like relations than
previous similar collections as these naturally occur
more often with NEs. The distributional similar-
ity of both positive and negative examples makes
SherLIiC a promising benchmark to track future
NLI models’ ability to go beyond shallow seman-
tics relying primarily on distributional evidence.
We showed that existing rule bases are complemen-
tary to SherLIiC and that current semantic vector
space models as well as SOTA neural NLI mod-
els cannot achieve at the same time high precision
and high recall on SherLIiC. Although SherLIiC’s
creation is entirely data-driven, it shows a large
variety of linguistic challenges for NLI, ranging
from lexical relations like troponymy, synonymy or
morph. derivation to typical actions and common
sense knowledge (cf. Table 1). The large unlabeled
resources, SherLIiC-InfCands and SherLIiC-TEG,
are potentially useful for further linguistic analysis
(as we showed in § 6), as well as for data-driven
models of lexical semantics, e.g., techniques such
as representation learning and domain adaptation.
We hope that SherLIiC will foster better modeling
of lexical inference in context as well as progress
in NLI in general.

Acknowledgments

We gratefully acknowledge a Ph.D. scholarship
awarded to the first author by the German Aca-
demic Scholarship Foundation (Studienstiftung des
deutschen Volkes). This work was supported by the
BMBF as part of the project MLWin (01IS18050).



911

References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,

Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018. Generating natural language adversarial ex-
amples. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 2890–2896, Brussels, Belgium. Association
for Computational Linguistics.

Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Procs.
of IJCAI, volume 7, pages 2670–2676.

Jonathan Berant. 2012. Global Learning of Textual
Entailment Graphs. Ph.D. thesis, The Blavatnik
School of Computer Science, Raymond and Beverly
Sackler Faculty of Exact Sciences, Tel Aviv Univer-
sity.

Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 610–
619, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ’08, pages 1247–1250,
New York, NY, USA.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 26, pages 2787–2795. Curran Associates,
Inc.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Association for Computational Linguistics.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced lstm for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1657–1668. Association for Computational Linguis-
tics.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Morgan & Clay-
pool Publishers.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1535–1545, Edinburgh, Scotland, UK. Associ-
ation for Computational Linguistics.

Christiane Fellbaum. 2005. Wordnet and wordnets.
In Keith Brown et al., editor, Encyclopedia of Lan-
guage and Linguistics, second edition, pages 665–
670. Elsevier, Oxford.

Benoı̂t Frénay and Michel Verleysen. 2014. Classifi-
cation in the Presence of Label Noise: A Survey.
IEEE TRANSACTIONS ON NEURAL NETWORKS
AND LEARNING SYSTEMS, 25(5):845–869.

Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. FACC1: Freebase annota-
tion of ClueWeb corpora, Version 1 (Release date
2013-06-26, Format version 1, Correction level 0).

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 758–764, Atlanta, Georgia. Associa-
tion for Computational Linguistics.

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking NLI systems with sentences that re-
quire simple lexical inferences. In The 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Melbourne, Australia.

Edouard Grave, Tomas Mikolov, Armand Joulin, and
Piotr Bojanowski. 2017. Bag of tricks for efficient
text classification. In EACL (2), pages 427–431. As-
sociation for Computational Linguistics.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel Bowman, and Noah A.
Smith. 2018. Annotation artifacts in natural lan-
guage inference data. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 107–112. Association for Computational Lin-
guistics.

Xu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu,
Maosong Sun, and Juanzi Li. 2018. Openke: An
open toolkit for knowledge embedding. In Proceed-
ings of EMNLP.

Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics
- Volume 2, COLING ’92, pages 539–545, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and
Kevin Gimpel. 2018. Using trusted data to train
deep networks on labels corrupted by severe noise.

https://www.aclweb.org/anthology/D18-1316
https://www.aclweb.org/anthology/D18-1316
https://www.aclweb.org/anthology/D11-1142
https://www.aclweb.org/anthology/D11-1142
https://www.aclweb.org/anthology/N13-1092
https://www.aclweb.org/anthology/N13-1092
http://aclweb.org/anthology/N18-2017
http://aclweb.org/anthology/N18-2017


912

In S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, and R. Garnett, editors, Ad-
vances in Neural Information Processing Systems
31, pages 10477–10486. Curran Associates, Inc.

Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Nat. Lang. Eng.,
16(4):359–389.

John Lalor, Hao Wu, and hong yu. 2016. Building an
evaluation scale using item response theory. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 648–
657, Austin, Texas. Association for Computational
Linguistics.

Alessandro Lenci and Giulia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In *SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (SemEval
2012), pages 75–79, Montréal, Canada. Association
for Computational Linguistics.

Omer Levy and Ido Dagan. 2016. Annotating relation
inference in context via question answering. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2016, Au-
gust 7-12, 2016, Berlin, Germany, Volume 2: Short
Papers.

Mike Lewis and Mark Steedman. 2013. Combining
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.

Dekang Lin and Patrick Pantel. 2001. DIRT: Discovery
of Inference Rules from Text. In Proceedings of the
Seventh ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD’01),
pages 323–328, New York, NY, USA. ACM Press.

Edward Loper and Steven Bird. 2002. Nltk: The natu-
ral language toolkit. In In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics. Philadelphia: Association for
Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41.

Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 982–991,
Honolulu, Hawaii. Association for Computational
Linguistics.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gašić, Lina M. Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Steve Young. 2016. Counter-fitting word vectors to
linguistic constraints. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 142–148, San Diego,
California. Association for Computational Linguis-
tics.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2340–2353,
Santa Fe, New Mexico, USA. Association for Com-
putational Linguistics.

Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1135–1145, Jeju Is-
land, Korea. Association for Computational Linguis-
tics.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülşen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering,
13(2):95–135.

Ellie Pavlick and Chris Callison-Burch. 2016. Tense
manages to predict implicative behavior in verbs.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2225–2229, Austin, Texas. Association for Compu-
tational Linguistics.

Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,
Benjamin Van Durme, and Chris Callison-Burch.
2015. Ppdb 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 425–430, Beijing, China. As-
sociation for Computational Linguistics.

Stephen Roller, Douwe Kiela, and Maximilian Nickel.
2018. Hearst patterns revisited: Automatic hyper-
nym detection from large text corpora. In Proceed-
ings of the 56th Annual Meeting of the Association

https://aclweb.org/anthology/D16-1062
https://aclweb.org/anthology/D16-1062
http://www.cs.ualberta.ca/~lindek/papers/kdd01-1.pdf
http://www.cs.ualberta.ca/~lindek/papers/kdd01-1.pdf
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1301.3781
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://www.aclweb.org/anthology/D08-1103
http://www.aclweb.org/anthology/N16-1018
http://www.aclweb.org/anthology/N16-1018
https://www.aclweb.org/anthology/C18-1198
http://www.aclweb.org/anthology/D12-1104
http://www.aclweb.org/anthology/D12-1104
https://doi.org/10.1017/S1351324906004505
https://doi.org/10.1017/S1351324906004505
https://doi.org/10.1017/S1351324906004505
https://aclweb.org/anthology/D16-1240
https://aclweb.org/anthology/D16-1240
http://www.aclweb.org/anthology/P15-2070
http://www.aclweb.org/anthology/P15-2070
http://www.aclweb.org/anthology/P15-2070


913

for Computational Linguistics (Volume 2: Short Pa-
pers), pages 358–363, Melbourne, Australia. Asso-
ciation for Computational Linguistics.

Wesley C Salmon. 1971. Statistical explanation and
statistical relevance, volume 69. University of Pitts-
burgh Pre.

Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine
Schulte im Walde. 2014. Chasing hypernyms in vec-
tor spaces with entropy. In Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, volume 2: Short
Papers, pages 38–42, Gothenburg, Sweden. Associ-
ation for Computational Linguistics.

Stefan Schoenmackers, Jesse Davis, Oren Etzioni,
and Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing: 2010, pages 1088–1098.

Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between NE pairs.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).

Vered Shwartz, Enrico Santus, and Dominik
Schlechtweg. 2017a. Hypernyms under siege:
Linguistically-motivated artillery for hypernymy
detection. In Proceedings of the 15th Conference
of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Papers,
pages 65–75, Valencia, Spain. Association for
Computational Linguistics.

Vered Shwartz, Gabriel Stanovsky, and Ido Dagan.
2017b. Acquiring predicate paraphrases from news
tweets. In Proceedings of the 6th Joint Conference
on Lexical and Computational Semantics (*SEM
2017), pages 155–160, Vancouver, Canada. Associa-
tion for Computational Linguistics.

Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP
2004, pages 41–48, Barcelona, Spain. Association
for Computational Linguistics.

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Complex
embeddings for simple link prediction. In Proceed-
ings of the 33rd International Conference on Inter-
national Conference on Machine Learning - Volume
48, ICML’16, pages 2071–2080.

Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of Coling 2004, pages
1015–1021, Geneva, Switzerland. COLING.

Dirk Weissenborn, Pasquale Minervini, Tim Dettmers,
Isabelle Augenstein, Johannes Welbl, Tim
Rocktäschel, Matko Bošnjak, Jeff Mitchell, Thomas
Demeester, Pontus Stenetorp, and Sebastian Riedel.

2018. Jack the Reader – A Machine Reading Frame-
work. In Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics
(ACL) System Demonstrations.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112–1122. Association for
Computational Linguistics.

XTAG Research Group. 2001. A lexicalized tree ad-
joining grammar for english. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.

Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. 2014. Extracting lexically
divergent paraphrases from twitter. Transactions
of the Association for Computational Linguistics,
2:435–448.

Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense
disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ’12, pages
712–720, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 156–160, Jeju Island, Korea.
Association for Computational Linguistics.

Congle Zhang, Stephen Soderland, and Daniel S. Weld.
2015. Exploiting parallel news streams for unsuper-
vised event extraction. Transactions of the Associa-
tion for Computational Linguistics, 3:117–129.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.
Generating natural adversarial examples. In Interna-
tional Conference on Learning Representations.

A Relation Filter Heuristics

In order to be kept as a relation, a dependency path
must fulfill all of the following criteria:

1. It starts or ends with nsubj or nsubjpass.

2. It starts or ends with one of the following
labels: nsubj, nsubjpass, iobj, dobj,
pobj, appos, poss, rcmod, infmod,
partmod.

3. It is not longer than 7 words and 8 dependency
labels.

http://www.aclweb.org/anthology/E14-4008
http://www.aclweb.org/anthology/E14-4008
https://www.aclweb.org/anthology/I05-5011
https://www.aclweb.org/anthology/I05-5011
https://www.aclweb.org/anthology/S17-1019
https://www.aclweb.org/anthology/S17-1019
https://www.aclweb.org/anthology/W04-3206
https://www.aclweb.org/anthology/W04-3206
https://arxiv.org/abs/1806.08727
https://arxiv.org/abs/1806.08727
http://aclweb.org/anthology/N18-1101
http://aclweb.org/anthology/N18-1101
https://www.aclweb.org/anthology/Q14-1034
https://www.aclweb.org/anthology/Q14-1034
https://www.aclweb.org/anthology/P12-2031
https://www.aclweb.org/anthology/Q15-1009
https://www.aclweb.org/anthology/Q15-1009
https://openreview.net/forum?id=H1BLjgZCb


914

4. At least one of the presumable lemmas con-
tains at least 3 letters.

5. It does not have the same dependency label at
both ends.

6. It does not contain any of the following la-
bels: parataxis, pcomp, csubj, advcl,
ccomp.

7. It does not contain immediate repetitions of
words or dependency labels.


