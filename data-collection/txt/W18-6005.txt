
























































Er ... well, it matters, right? On the role of data representations in spoken language dependency parsing


Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 37–46
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

37

Er ... well, it matters, right?
On the role of data representations in spoken language dependency

parsing

Kaja Dobrovoljc
Jožef Stefan Institute
Ljubljana, Slovenia

kaja.dobrovoljc@ijs.si

Matej Martinc
Jožef Stefan Institute
Ljubljana, Slovenia

matej.martinc@ijs.si

Abstract

Despite the significant improvement of data-
driven dependency parsing systems in recent
years, they still achieve a considerably lower
performance in parsing spoken language data
in comparison to written data. On the exam-
ple of Spoken Slovenian Treebank, the first
spoken data treebank using the UD annota-
tion scheme, we investigate which speech-
specific phenomena undermine parsing perfor-
mance, through a series of training data and
treebank modification experiments using two
distinct state-of-the-art parsing systems. Our
results show that utterance segmentation is the
most prominent cause of low parsing perfor-
mance, both in parsing raw and pre-segmented
transcriptions. In addition to shorter utter-
ances, both parsers perform better on nor-
malized transcriptions including basic mark-
ers of prosody and excluding disfluencies, dis-
course markers and fillers. On the other
hand, the effects of written training data addi-
tion and speech-specific dependency represen-
tations largely depend on the parsing system
selected.

1 Introduction

With an exponential growth of spoken language
data available online on the one hand and the
rapid development of systems and techniques for
language understanding on the other, spoken lan-
guage research is gaining increasing prominence.
Many syntactically annotated spoken language
corpora have been developed in the recent years to
benefit the data-driven parsing systems for speech
(Hinrichs et al., 2000; van der Wouden et al.,
2002; Lacheret et al., 2014; Nivre et al., 2006),
including two spoken language treebanks adopt-
ing the Universal Dependencies (UD) annotation
scheme, aimed at cross-linguistically consistent
dependency treebank annotation (Nivre, 2015).

However, in the recent CoNLL 2017 shared task
on multilingual parsing from raw text to UD (Ze-
man et al., 2017), the results achieved on the Spo-
ken Slovenian Treebank (Dobrovoljc and Nivre,
2016) - the only spoken treebank among the 81
participating treebanks - were substantially lower
than on other treebanks. This includes the writ-
ten Slovenian treebank (Dobrovoljc et al., 2017),
with a best labeled attachment score difference of
more than 30 percentage points between the two
treebanks by all of the 33 participating systems.

Given this significant gap in parsing perfor-
mance between the two modalities, spoken and
written language, this paper aims to investigate
which speech-specific phenomena influence the
poor parsing performance for speech, and to what
extent. Specifically, we focus on questions re-
lated to data representation in all aspects of the
dependency parsing pipeline, by introducing dif-
ferent types of modifications to spoken language
transcripts and speech-specific dependency anno-
tations, as well as to the type of data used for spo-
ken language modelling.

This paper is structured as follows. Section 2
addresses the related research on spoken language
parsing and Section 3 presents the structure and
annotation of the Spoken Slovenian Treebank on
which all the experiments were conducted. Sec-
tion 4 presents the parsing systems used in the ex-
periments (4.1) and the series of SST data modi-
fications to narrow the performance gap between
written and spoken treebanks for these systems,
involving the training data (4.3.1), speech tran-
scriptions (4.3.2) and UD dependency annotations
(4.3.3). Results are presented in Section 5, while
conclusions and some directions for further work
are addressed in Section 6.



38

eee aha še ena stvar to sem se tukaj s [gap] spomnil zdajle ko vidim ta komentar eem
er yes more one thing this I-have (PRON) here r- [gap] remembered now when I-see this comment uhm

discourse:filler
discourse

advmod
nummod

obj
aux

expl
advmod

reparandum
punct

parataxis

advmod mark

advcl

det
obj
discourse:filler

(oh yes one more thing I just r- [gap] remembered this here now that I see this comment)

Figure 1: An example utterance taken from the Spoken Slovenian Treebank.

2 Related work

In line with divergent approaches to syntactic an-
notation of transcribed spoken data that either aim
to capture the syntactic structure involving all ut-
tered lexical phenomena in an utterance, or dis-
card the (variously defined) noisy speech-specific
structural particularities on the other, research into
parsing spoken language can broadly be catego-
rized in two main groups. On the one side of the
spectrum, we find approaches that separate disflu-
ences from parsing. Charniak and Johnson (2001)
and Jørgensen (2007), for example, both report
a significant increase in parsing the Switchboard
section of the Penn Discourse Treebank (Godfrey
et al., 1992), if disfluencies are first removed from
the data. These two-pass pipeline approaches thus
involve a separate task of automatic disfluency de-
tection, one of the fundamental issues in automatic
speech recognition (Liu et al., 2006; Lease et al.,
2006).

Recently, however, several parsing systems
using non-monotonic transition-based algorithms
have emerged that enable joint parsing and dis-
fluency detection (Honnibal et al., 2013; Honnibal
and Johnson, 2015; Rasooli and Tetreault, 2013),
showing that joint treatment of both problems can
actually outperform state-of-the-art pipeline ap-
proaches (Honnibal and Johnson, 2014). These
findings open a promising line of future research
for the development of speech-specific parsing
systems (Yoshikawa et al., 2016), especially those
that also incorporate acoustic information (Kahn
et al., 2005; Tran et al., 2017).

Nevertheless, apart from research on speech-
specific parsing systems, very little research has
been dedicated to other, data-related aspects of
spoken language parsing. To our knowledge, with
expection of Caines et al. (2017) and Nasr et al.
(2014), who investigate the role of different types
of training data used for parsing transcripts of

speech, there have been no other systematic stud-
ies on the role of spoken data representations, such
as transcription or annotation conventions, in spo-
ken language parsing.

3 Spoken Slovenian Treebank

The Spoken Slovenian Treebank (Dobrovoljc and
Nivre, 2016), which was first released as part of
UD v1.3 (under the CC-BY-NC-SA 4.0 licence), is
the first syntactically annotated collection of spon-
taneous speech in Slovenian. It is a sample of the
Gos reference corpus of Spoken Slovenian (Zwit-
ter Vitez et al., 2013), a collection of transcribed
audio recordings of spontaneous speech in differ-
ent everyday situations, in both public (TV and ra-
dio shows, school lessons, academic lectures etc.)
and private settings (work meetings, services, con-
versations between friends and family etc.).

The SST treebank currently amounts to 29,488
tokens (3,188 utterances), which include both lex-
ical tokens (words) and tokens signalling other
types of verbal phenomena, such as filled pauses
(fillers) and unfinished words, as well as some ba-
sic markers of prosody and extralinguistic speech
events. The original segmentation, tokeniza-
tion and spelling principles described by Ver-
donik et al. (2013) have also been inherited
by SST. Among the two types of Gos tran-
scriptions (pronunciation-based and normalized
spelling, both in lowercase only), subsequent man-
ual annotations in SST have been performed on
top of normalized transcriptions.

For syntactic annotation of the transcripts, un-
available in Gos, the SST treebank adopted the
Universal Dependencies annotation scheme due to
its high degree of interoperability across different
grammatical frameworks, languages and modali-
ties. In this original application of the UD scheme
to spoken language transcripts, several modifica-
tions of the scheme were implemented to accom-



39

modate the syntactic particularities in speech, ei-
ther by extending the scope of application of ex-
isting universal labels (e.g. using punct for la-
beling markers of prosody) or introducing new
speech-specific sub-labels (e.g. discourse:filler
for annotation of hesitation sounds). In subsequent
comparison of the SST treebank with the writ-
ten SSJ Slovenian UD treebank (Dobrovoljc et al.,
2017), Dobrovoljc and Nivre (2016) observed sev-
eral syntactic differences between the two modal-
ities, as also illustrated in Figure 1.

4 Experiment setup

4.1 Parsing systems and evaluation

To enable system-independent generalizations,
two parsing systems were selected, UDPipe
1.2 (Straka and Straková, 2017) and Stan-
ford (Dozat et al., 2017), covering the two most
common parsing approaches, transition-based and
graph-based parsing (Aho and Ullman, 1972), re-
spectively. UDPipe 1.2 is a trainable pipeline
for sentence segmentation, tokenization, POS tag-
ging, lemmatization and dependency parsing. It
represents an improved version of the UDPipe 1.1
(used as a baseline system in the CONLL-2017
Shared Task (Zeman et al., 2017)) and finished as
the 8th best system out of 33 systems participating
in the task.

A single-layer bidirectional GRU network to-
gether with a case insensitive dictionary and a
set of automatically generated suffix rules are
used for sentence segmentation and tokenization.
The part of speech tagging module consists of a
guesser, which generates several universal part of
speech (XPOS), language-specific part of speech
(UPOS), and morphological feature list (FEATS)
tag triplets for each word according to its last four
characters. These are given as an input to an av-
eraged perceptron tagger (Straka et al., 2016) to
perform the final disambiguation on the generated
tags. Transition-based dependency parser is based
on a shallow neural network with one hidden layer
and without any recurrent connections, making
it one of the fastest parsers in the CONLL-2017
Shared Task. We used the default parameter con-
figuration of ten training iterations and a hidden
layer of size 200 for training all the models.

Stanford parser is a neural graph-based
parser (McDonald et al., 2005) capable of lever-
aging word and character based information in
order to produce part of speech tags and labeled

dependency parses from segmented and tokenized
sequences of words. Its architecture is based
on a deep biaffine neural dependency parser
presented by (Dozat and Manning, 2016), which
uses a multilayer bidirectional LSTM network
to produce vector representations for each word.
These representations are used as an input to a
stack of biaffine classifiers capable of producing
the most probable UD tree for every sentence and
the most probable part of speech tag for every
word. The system was ranked first according
to all five relevant criteria in the CONLL-2017
Shared Task. Same hyperparameter configuration
was used as reported in (Dozat et al., 2017) with
every model trained for 30,000 training steps.
For the parameters values that were not explicitly
mentioned in (Dozat et al., 2017), default values
were used.

For both parsers, no additional fine-tuning was
performed for any specific data set, in order to
minimize the influence of training procedure on
the parser’s performance for different data pre-
processing techniques, especially given that no de-
velopment data has been released for the small
SST treebank.

For evaluation, we used the official CoNLL-
ST-2017 evaluation script (Zeman et al., 2017) to
calculate the standard labeled attachments score
(LAS), i.e. the percentage of nodes with cor-
rectly assigned reference to parent node, includ-
ing the label (type) of relation. For baseline ex-
periments involving parsing of raw transcriptions
(see Section 4.2), for which the number of nodes
in gold-standard annotation and in the system out-
put might vary, the F1 LAS score, marking the
harmonic mean of precision an recall LAS scores,
was used instead.

4.2 Baseline

Prior to experiments involving different data mod-
ifications, both parsing systems were evaluated on
the written SSJ and spoken SST Slovenian tree-
banks, released as part of UD version 2.2 (Nivre
et al., 2018).1 The evaluation was performed
both for parsing raw text (i.e. automatic tok-
enization, segmentation, morphological annota-
tion and dependency tree generation) and parsing

1Note that the SST released as part of UD v2.2 involves a
different splitting of utterances into training and test tests as
in UD v2.0, which should be taken into account when com-
paring our results to the results reported in the CoNLL 2017
Shared Task.



40

UDPipe Stanford
Parsing raw text

Treebank Sents UPOS UAS LAS Sents UPOS UAS LAS
sst 20.35 88.32 52.49 45.47 20.35 93.21 60.35 54.00
ssj 76.49 94.59 79.90 76.32 76.49 96.32 87.50 85.02
ssj 20k 76.42 89.88 71.79 66.40 76.42 94.61 82.60 78.60

Dependency parsing only
Treebank Sents UPOS UAS LAS Sents UPOS UAS LAS
sst 100 100 74.66 69.13 100 100 77.58 72.52
ssj 100 100 90.16 88.41 100 100 95.63 94.52
ssj 20k 100 100 86.69 84.21 100 100 91.93 89.60

Table 1: UDPipe and Stanford sentence segmentation (Sents), part-of-speech tagging (UPOS), unlabelled (UAS)
and labelled attachment (LAS) F1 scores on the spoken SST and written SSJ Slovenian UD treebanks for parsing
raw text, and for parsing texts with gold-standard tokenization, segmentation and tagging information.

gold-standard annotations (i.e. dependency pars-
ing only). For Stanford parser, which only pro-
duces tags and dependency labels, the UDPipe to-
kenization and segmentation output was used as
input.

The results displayed in Table 1 (Parsing raw
text) confirm the difficulty of parsing spoken lan-
guage transcriptions, given that both UDPipe and
Stanford systems perform significantly worse on
the spoken SST treebank in comparison with the
written SSJ treebank, with the difference in LAS
F1 score amounting to 30.85 or 31.02 percent-
age points, respectively. These numbers decrease
if we neutralize the important difference in tree-
bank sizes - with 140.670 training set tokens for
the written SSJ and 29.488 tokens for the spoken
SST - by training the written model on a compa-
rable subset of SSJ training data (20.000 tokens),
however, the difference between the two modali-
ties remains evident.

A subsequent comparison of results in depen-
dency parsing only (Table 1, Dependency parsing
only) reveals that a large share of parsing mistakes
can be attributed to difficulties in lower-level pro-
cessing, in particular utterance segmentation (with
an F1 score of 20.35),2 as spoken language pars-
ing performance increases to the (baseline) LAS
score of 69.13 and 72.52 for the UDPipe and Stan-
ford parser, respectively. Consequently, the actual
difference between written and spoken language

2Note that the low segmentation score is not spe-
cific to UDPipe, but to state-of-the-art parsing sys-
tems in general, as none of the 33 systems com-
peting in the CoNLL 2017 Shared Task managed to
achieve a significantly better result in SST treebank
segmentation: http://universaldependencies.
org/conll17/results-sentences.html.

parsing reduces to approximately 15-17 percent-
age points, if based on the same amount of training
data.

In order to prevent the dependency parsing ex-
periments in this paper being influenced by the
performance of systems responsible for produc-
ing other levels of linguistic annotation, the ex-
periments set out in the continuation of this paper
focus on evaluation of gold-standard dependency
parsing only.

4.3 Data modifications

Given the observed difference in parsing spoken
and written language for both parsing systems,
several automated modifications of the data fea-
tured in the parsing pipeline have been introduced,
to investigate the influence of different factors on
spoken language parsing performance.

4.3.1 Modifications of training data type
Although the relationship between written and
spoken language has often been portrayed as a
domain-specific dichotomy, both modalities form
part of the same language continuum, encourag-
ing further investigations of cross-modal model
transfers. In the first line of experiments, we
thus conducted experiments on evaluation of spo-
ken language parsing by training on spoken (sst)
and written (ssj) data alone, as well as on the
combination of both (sst+ssj). Given that the
transcriptions in the SST treebank are written in
lowercase only and do not include any written-
like punctuation, two additional models excluding
these features were generated for the written tree-
bank (ssj lc and ssj no-punct) to neutral-
ize the differences in writing system conventions

http://universaldependencies.org/conll17/results-sentences.html
http://universaldependencies.org/conll17/results-sentences.html


41

for both modalities.

4.3.2 Modifications of speech transcription
The second line of experiments investigates the
role of spoken language transcription conventions
for the most common speech-specific phenomena,
by introducing various automatically converted
versions of the SST treebank (both training and
testing data).

Spelling: For word form spelling, the origi-
nal normalized spelling compliant with standard
orthography was replaced by pronunciation-based
spelling (sst pron-spell), reflecting the re-
gional and colloquial pronunciation variation (e.g.
the replacement of the standard pronominal word
form jaz “I” by pronunciation-based word forms
jz, jaz, jst, jez, jes, ja etc.).

Segmentation: Inheriting the manual segmen-
tation of the reference Gos corpus, sentences (ut-
terances) in SST correspond to ”semantically, syn-
tactically and acoustically delimited units” (Ver-
donik et al., 2013). As such, the utterance segmen-
tation heavily depends on subjective interpreta-
tions of what is the basic functional unit in speech,
in line with the multitude of existing segmentation
approaches, based on syntax, semantics, prosody,
or their various combinations (Degand and Simon,
2009). To evaluate parsing performance for alter-
native types of segmentation, based on a more ob-
jective set of criteria, two additional SST segmen-
tations were created. In the minimally segmented
version of the SST treebank (sst min-segm),
utterances involving two or more clauses joined by
a parataxis relation (denoting a loose inter-clausal
connections without explicit coordination, subor-
dination, or argument relation) have been split into
separate syntactic trees (clauses), as illustrated in
the example below (Figure 2).

glej jo še kar stoka
look at-her still (PART) she-moans

parataxis

(look at her she’s still moaning)

Figure 2: Splitting utterances by parataxis.

Vice versa, the maximally segmented SST ver-
sion (sst max-segm) includes utterances corre-
sponding to entire turns (i.e. units of speech by
one speaker), in which neighbouring utterances by

a speaker have been joined into a single syntactic
tree via the parataxis relation.

Disfluencies: Following the traditional ap-
proaches to spoken language processing, the
sst no-disfl SST treebank version marks the
removal of disfluencies, namely filled pauses, such
as eee, aaa, mmm (labeled as discourse:filler),
overridden disfluencies, such as repetitions, sub-
stitutions or reformulations (labeled as reparan-
dum), and [gap] markers, co-occurring with unfin-
ished or incomprehensible speech fragments (Fig-
ure 3).

mmm ne bom po [gap] prispeval podpisa
hmmm not I-will sig- [gap] give signature

discourse:filler

reparandum
punct

(uhm I will not sig- [gap] give my signature)

Figure 3: Removal of disfluencies.

Similar to structurally ’redundant’ phenom-
ena described above, the sst no-discourse
version of the SST treebank excludes syntacti-
cally peripheral speech-specific lexical phenom-
ena, annotated as discourse, discourse:filler or
parataxis:discourse, such as interjections (aha
“uh-huh”), response tokens (ja “yes”), expressions
of politeness (adijo “bye”), as well as clausal and
non-clausal discourse markers (no “well”, mislim
“I think”).

Prosody: Although the SST treebank lacks
phonetic transcription, some basic prosodic infor-
mation is provided through specific tokens denot-
ing exclamation or interrogation intonation, silent
pauses, non-turn taking speaker interruptions, vo-
cal sounds (e.g. laughing, sighing, yawning)
and non-vocal sounds (e.g. applauding, ring-
ing). In contrast to the original SST treebank,
in which these nodes were considered as regu-
lar nodes of dependency trees (labeled as punct),
prosodic markers have been excluded from the
sst no-pros version of the treebank.

4.3.3 Modifications of UD annotation
Given that the SST treebank was the first spo-
ken treebank to be annotated using the UD an-
notation scheme, the UD annotation principles
for speech-specific phenomena set out in Dobro-
voljc and Nivre (2016) have not yet been evaluated
within a wider community. To propose potential



42

future improvements of the UD annotation guide-
lines for spoken language phenomena, the third set
of SST modifications involved alternations of se-
lected speech-specific UD representations.

Extensions: The SST treebank introduced five
new subtypes of existing UD relations to an-
notate filled pauses (discourse:filler), clausal re-
pairs (parataxis:restart), clausal discourse mark-
ers (parataxis:discourse) and general extenders
(conj:extend). In the sst no-extensions
version of the treebank, these extensions have
been replaced by their universal counterparts (i.e.
discourse, parataxis and conj).

Head attachment: For syntactic relations,
such as discourse or punct, which are not di-
rectly linked to the predicate-driven structure of
the sentence, the choice of the head node to
which they attach to is not necessarily a straight-
forward task. The original SST treebank fol-
lowed the general UD principle of attaching such
nodes to the highest node preserving projectiv-
ity, typically the head of the most relevant nearby
clause or clause argument. To evaluate the im-
pact of such high attachment principle on pars-
ing performance, an alternative robust attachment
has been implemented for two categories with
the weakest semantic connection to the head,
filled pauses (sst discourse:filler) and
prosodic markers (sst punct), attaching these
nodes to the nearest preceding node instead, re-
gardless of its syntactic role, as illustrated in Fig-
ure 4.

mene je strah ker se snema [all : laughter]
I is afraid because (PRON) it-tapes [all : laughter]

punct

punct

(I am afraid because it’s being taped)

Figure 4: Change of head for prosody markers.

For the reparandum relation, which currently
denotes a relation between the edited unit (the
reparandum) and its repair, the opposite principle
was implemented in sst reparandum, by at-
taching the reparandum to the head of its repair,
i.e. to the node it would attach to had it not been
for the repair (Figure 5).

Following a similar higher-attachment prin-
ciple, the parataxis:restart relation, used for

da so te eee ti stroški čim manjši
that are these (F) er these (M) costs most low

mark
cop

nsubj
detdiscourse:filler

reparandum

reparandum

(so that these costs are as low as possible)

Figure 5: Change of head for reparandum.

annotation of sentences replacing an aban-
doned preceding clause, has been modified in
sst parataxis:restart so as to span from
the root node instead of the more or less randomly
positioned head of the unfinished clause.

Clausal discourse markers: In the original
SST treebank, clausal discourse markers (e.g.
ne vem “I don’t know”, (a) veš “you know”,
glej “listen”) have been labeled as parataxis
(specifically, the parataxis:discourse extension),
in line with other types of sentential parenthet-
icals. Given the distinct distributional charac-
teristics of these expressions (limited list, high
frequency) and similar syntactic behaviour to
non-clausal discourse markers (no dependents,
both peripheral and clause-medial positions), their
label has been changed to discourse in the
sst parataxis:discourse version of the
treebank. For multi-word clausal markers, the
fixed label was also introduced to annotate the
internal structure of this highly grammaticized
clauses (Figure 6.

kaj boš pa drugega počel a veš
what you-will (PART) else do you know

parataxis:discourse

discourse fixed

(what else can you do you know)

Figure 6: Change of annotation for clausal discourse
markers.

5 Results

Table 2 gives LAS evaluation of both parsing sys-
tems for each data modification described in Sec-
tion 4.3 above, including the baseline results for
training and parsing on the original SST treebank



43

Model UDPipe Stanford
Training data

1 sst (= baseline) 69.13 72.52
2 ssj+sst 68.53 77.38
3 ssj no-punct 57.40 62.57
4 ssj 55.76 62.08
5 ssj lc 55.61 61.99

Transcriptions
6 sst min-segm 74.89 78.31
7 sst no-disfl 71.47 74.77
8 sst no-discourse 70.73 75.47
9 sst no-pros 68.70 71.78
10 sst pron-spell 67.52 71.64
11 sst max-segm 63.93 68.13

Annotations
12 sst punct 71.32 73.65
13 sst discourse:filler 69.13 72.85
14 sst parataxis:restart 68.53 71.95
15 sst no-new-ext. 68.45 73.05
16 sst reparandum 68.41 72.81
17 sst parataxis:disc. 68.32 72.35

Best combination
18 sst 6-7-8-12 79.58 N/A
19 sst 6-7-8-12-15 N/A 87.35

Table 2: LAS on the Spoken Slovenian Treebank
(sst) for different types of training data, transcrip-
tion and annotation modifications. Improvements of
the baseline are marked in bold.

(see Section 4.2).
When evaluating the impact of different types

of training data on the original SST parsing, both
parsers give significantly poorer results than the
baseline sst model if trained on the written SSJ
treebank alone (ssj), which clearly demonstrates
the importance of (scarce) spoken language tree-
banks for spoken language processing. In addi-
tion, no significant improvement is gained if the
written data is modified so as to exclude punc-
tuation (ssj no-punct) or perform lowercas-
ing (ssj lc), which even worsens the results.
Somewhat surprisingly, no definite conclusion
can be drawn on the joint training model based
on both spoken and written data (sst+ssj),
as the parsers give significantly different results:
while Stanford parser substantially outperforms
the baseline result when adding written data to
the model (similar to the findings by Caines et al.
(2017)), this addition has a negative affect on UD-
Pipe. This could be explained by the fact that

global, exhaustive, graph-based parsing systems
are more capable of leveraging the richer con-
textual information gained with a larger train set
in comparison with local, greedy, transition-based
systems (McDonald and Nivre, 2007).

The results of the second set of experiments, in
which LAS was evaluated for different types of
spoken language transcriptions, confirm that pars-
ing performance varies with different approaches
to transcribing speech-specific phenomena. As ex-
pected, both systems achieve significantly better
results if parsing is performed on shorter utter-
ances (sst min-segm). On the other hand, a
similar LAS drop-off interval is identified for pars-
ing full speaker turns (sst max-segm). These
results confirm the initial observations in Section
4.2 that speech segmentation is the key bottle-
neck in the spoken language dependency parsing
pipeline. Nevertheless, it is encouraging to ob-
serve that even the absence of any internal seg-
mentation of (easily identifiable) speaker turns re-
turns moderate parsing results.

As has already been reported in related work,
parsing performance also increases if spoken
data is removed of its most prominent syntac-
tic structures, such as disfluencies, discourse
markers and fillers. Interestingly, for Stan-
ford parser, the removal of discourse mark-
ers (sst no-discourse) is even more ben-
eficial than the removal of seemingly less pre-
dictable false starts, repairs and other disfluencies
(sst no-disfl). On the contrary, the removal
of prosody markers (sst no-pros) damages the
baseline results for both parsers, suggesting that
the presence of these markers might even con-
tribute to parsing accuracy for certain types of con-
structions given their punctuation-like function in
speech.

As for spelling, the results on the tree-
bank based on pronunciation-based word spelling
(sst pron-spell) support our initial hypothe-
sis that the multiplication of token types damages
parser performance, yet not to a great extent. This
could be explained by the fact that token pronun-
ciation information can sometimes help with syn-
tactic disambiguation of the word form in context,
if a certain word form pronunciation is only asso-
ciated with a specific syntactic role (e.g. the col-
loquial pronunciation tko da of the discourse con-
nective tako da “so that” that does not occur with
other syntactic roles of this lexical string).



44

No definite conclusion can be drawn from
the parsing results for different alternations
of speech-specific UD annotations, as the re-
sults vary by parsing system and by the
types of UD modification. While both sys-
tems benefit from an alternative attachment of
prosodic markers to their nearest preceding to-
ken (sst punct),3 and prefer the current la-
beling and attachment principles for clausal re-
pairs (sst parataxis:restart) and clausal
discourse markers (parataxis:discourse),
the effect of other changes seems to be system-
dependent. What is more, none of the changes in
UD representations seem to affect the parsing per-
formance to a great extent, which suggests that the
original UD adaptations for speech-specific phe-
nomena, applied to the Spoken Slovenian Tree-
bank, represent a reasonable starting point for fu-
ture applications of the scheme to spoken language
data.

Finally, all transcription and annotation vari-
ables that were shown to improve spoken language
LAS for each of the parsing systems, have been
joined into a single representation, i.e. a treebank
with new, syntax-bound utterance segmentation,
excluding disfluencies and discourse elements,
and a change in prosody-marker-attachment (UD-
Pipe), as well as a change in filler-attachment
and addition of written parsing model (Stanford).4

Both UDPipe and Stanford achieved substantially
higher LAS scores for their best-fitting combina-
tion than the original SST baseline model (sst),
i.e. 79.58 and 87.35, respectively, moving the SST
parsing performance much closer to the perfor-
mance achieved on its same-size written counter-
part (ssj 20k, Table 1), with the gap narrowing
to 4.63 for UDPipe and 2.25 for Stanford. This
confirms that the speech-specific phenomena out-
lined in this paper are indeed the most important
phenomena affecting spoken language processing
scores. Nevertheless, the remaining gap between

3Note that the sst punct results should be interpreted
with caution, as a brief analysis into the punct-related pars-
ing errors on the original SST treebank revealed a substantial
amount of (incorrect) non-projective attachments of the [gap]
marker indicating speech fragments. This issue should be re-
solved in future releases of the SST treebank.

4Modifications set out in 13
(sst discourse:filler) and 16 (sst reparandum)
that have also increased Stanford parser performance, are not
applicable to the Stanford best-combination representation,
since discourse fillers and repairs have already been removed
by modifications set out in 7 (sst no-disfl) and 8
(sst no-discourse).

the two modalities encourages further data-based
investigations into the complexity of spoken lan-
guage syntax, which evidently reaches beyond the
prototypical structural and pragmatic phenomena
set forward in this paper and the literature in gen-
eral.

6 Conclusion and Future Work

In this paper, we have investigated which speech-
specific phenomena are responsible for below op-
timal parsing performance of state-of-the-art pars-
ing systems. Several experiments on Spoken
Slovenian Treebank involving training data and
treebank modifications were performed in order to
identify and narrow the gap between the perfor-
mances on spoken and written language data. The
results show that besides disfluencies, the most
common phenomena addressed in related work,
segmentation of clauses without explicit lexical
connection is also an important factor in low pars-
ing performance. In addition to that, our re-
sults suggest that for graph-based parsing systems,
such as Stanford parser, spoken language parsing
should be performed by joint modelling of both
spoken and written data excluding punctuation.

Other aspects of spoken data representation,
such as the choice of spelling, the presence of
basic prosodic markers and the syntactic anno-
tation principles seem less crucial for the over-
all parser performance. It has to be emphasized,
however, that the UD annotation modifications set
forward in this paper represent only a few se-
lected transformations involving labeling and at-
tachment, whereas many other are also possible,
in particular experiments involving enhanced rep-
resentations (Schuster and Manning, 2016).

These findings suggest several lines of future
work. For the SST treebank in particular and spo-
ken language treebanks in general, it is essential to
increase the size of annotated data and reconsider
the existing transcription and annotation princi-
ples to better address the difficulties in spoken lan-
guage segmentation and disfluency detection. Par-
ticularly in relation to the latter, our results should
be evaluated against recent speech-specific parsing
systems references in Section 2, as well as other
state-of-the-art dependency parsers. A promising
line of future work has also been suggested in re-
lated work on other types of noisy data (Blod-
gett et al., 2018), employing a variety of cross-
domain strategies for improving parsing with little



45

in-domain data.
Our primary direction of future work, however,

involves an in-depth evaluation of parsing perfor-
mance for individual dependency relations, to de-
termine how the modifications presented in this
paper affect specific constructions, and to over-
come the prevailing approaches to spoken lan-
guage parsing that tend to over-generalize the syn-
tax of speech.

References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The

Theory of Parsing, Translation, and Compiling.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.

Su Lin Blodgett, Johnny Wei, and Brendan O’Connor.
2018. Twitter Universal Dependency parsing for
African-American and Mainstream American En-
glish. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1415–1425. Associa-
tion for Computational Linguistics.

Andrew Caines, Michael McCarthy, and Paula But-
tery. 2017. Parsing transcripts of speech. In Pro-
ceedings of the Workshop on Speech-Centric Natu-
ral Language Processing, pages 27–36. Association
for Computational Linguistics.

Eugene Charniak and Mark Johnson. 2001. Edit de-
tection and parsing for transcribed speech. In Pro-
ceedings of the Second Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Language Technologies, NAACL ’01,
pages 1–9, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Liesbeth Degand and Anne Catherine Simon. 2009. On
identifying basic discourse units in speech: theoreti-
cal and empirical issues. Discours, 4.

Kaja Dobrovoljc, Tomaz Erjavec, and Simon Krek.
2017. The Universal Dependencies Treebank for
Slovenian. In Proceedings of the 6th Work-
shop on Balto-Slavic Natural Language Process-
ing, BSNLP@EACL 2017, Valencia, Spain, April 4,
2017, pages 33–38.

Kaja Dobrovoljc and Joakim Nivre. 2016. The Uni-
versal Dependencies Treebank of Spoken Slovenian.
In Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation (LREC
2016), Paris, France. European Language Resources
Association (ELRA).

Timothy Dozat and Christopher D. Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. CoRR, abs/1611.01734.

Timothy Dozat, Peng Qi, and Christopher D. Manning.
2017. Stanford’s graph-based neural dependency

parser at the CoNLL 2017 Shared Task. In Proceed-
ings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies,
pages 20–30, Vancouver, Canada. Association for
Computational Linguistics.

John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proceed-
ings of the 1992 IEEE International Conference on
Acoustics, Speech and Signal Processing - Volume
1, ICASSP’92, pages 517–520, Washington, DC,
USA. IEEE Computer Society.

Erhard W. Hinrichs, Julia Bartels, Yasuhiro Kawata,
Valia Kordoni, and Heike Telljohann. 2000. The
Tübingen treebanks for spoken German, English,
and Japanese. In Wolfgang Wahlster, edi-
tor, Verbmobil: Foundations of Speech-to-Speech
Translation, Artificial Intelligence, pages 550–574.
Springer Berlin Heidelberg.

Matthew Honnibal, Yoav Goldberg, and Mark John-
son. 2013. A non-monotonic arc-eager transition
system for dependency parsing. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning, pages 163–172, Sofia, Bul-
garia. Association for Computational Linguistics.

Matthew Honnibal and Mark Johnson. 2014. Joint
incremental disfluency detection and dependency
parsing. Transactions of the Association for Com-
putational Linguistics, 2(1):131–142.

Matthew Honnibal and Mark Johnson. 2015. An im-
proved non-monotonic transition system for depen-
dency parsing. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1373–1378. Association for Com-
putational Linguistics.

Fredrik Jørgensen. 2007. The effects of disfluency de-
tection in parsing spoken language. In Proceedings
of the 16th Nordic Conference of Computational
Linguistics NODALIDA-2007, pages 240–244.

Jeremy G Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
Proceedings of the conference on human language
technology and empirical methods in natural lan-
guage processing, pages 233–240. Association for
Computational Linguistics.

Anne Lacheret, Sylvain Kahane, Julie Beliao, Anne
Dister, Kim Gerdes, Jean-Philippe Goldman, Nico-
las Obin, Paola Pietrandrea, and Atanas Tchobanov.
2014. Rhapsodie: a prosodic-syntactic treebank for
spoken French. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14), pages 295–301, Reykjavik,
Iceland. European Language Resources Association
(ELRA).



46

Matthew Lease, Mark Johnson, and Eugene Charniak.
2006. Recognizing disfluencies in conversational
speech. IEEE Transactions on Audio, Speech, and
Language Processing, 14(5):1566–1573.

Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on audio, speech, and language pro-
cessing, 14(5):1526–1540.

Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 523–530. Association for Computa-
tional Linguistics.

Alexis Nasr, Frederic Bechet, Benoit Favre, Thierry
Bazillon, Jose Deulofeu, and andre Valli. 2014. Au-
tomatically enriching spoken corpora with syntactic
information for linguistic studies. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC’14), Reykjavik,
Iceland. European Language Resources Association
(ELRA).

Joakim Nivre. 2015. Towards a universal grammar
for natural language processing. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, volume 9041 of Lecture Notes
in Computer Science, pages 3–16. Springer Interna-
tional Publishing.

Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC), pages 1392–1395.

Joakim Nivre et al. 2018. Universal Dependencies 2.2.
LINDAT/CLARIN digital library at the Institute of
Formal and Applied Linguistics (UFAL), Faculty of
Mathematics and Physics, Charles University.

Mohammad Sadegh Rasooli and Joel Tetreault. 2013.
Joint parsing and disfluency detection in linear time.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
124–129, Seattle, Washington, USA. Association
for Computational Linguistics.

Sebastian Schuster and Christopher D. Manning. 2016.
Enhanced english universal dependencies: An im-
proved representation for natural language under-
standing tasks. In Proceedings of the Tenth Interna-

tional Conference on Language Resources and Eval-
uation (LREC 2016), Paris, France. European Lan-
guage Resources Association (ELRA).

Milan Straka, Jan Hajic, and Jana Straková. 2016. Ud-
pipe: Trainable pipeline for processing CoNLL-U
files performing tokenization, morphological anal-
ysis, POS tagging and parsing. In Proceedings
of the Tenth International Conference on Language
Resources and Evaluation (LREC 2016), Paris,
France. European Language Resources Association
(ELRA).

Milan Straka and Jana Straková. 2017. Tokenizing,
pos tagging, lemmatizing and parsing UD 2.0 with
UDPipe. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies, pages 88–99, Vancouver, Canada.
Association for Computational Linguistics.

Trang Tran, Shubham Toshniwal, Mohit Bansal, Kevin
Gimpel, Karen Livescu, and Mari Ostendorf. 2017.
Joint modeling of text and acoustic-prosodic cues for
neural parsing. CoRR, abs/1704.07287.

Darinka Verdonik, Iztok Kosem, Ana Zwitter Vitez, Si-
mon Krek, and Marko Stabej. 2013. Compilation,
transcription and usage of a reference speech cor-
pus: the case of the Slovene corpus GOS. Language
Resources and Evaluation, 47(4):1031–1048.

Ton van der Wouden, Heleen Hoekstra, Michael
Moortgat, Bram Renmans, and Ineke Schuurman.
2002. Syntactic analysis in the Spoken Dutch Cor-
pus (CGN). In Proceedings of the Third Interna-
tional Conference on Language Resources and Eval-
uation, LREC 2002, May 29-31, 2002, Las Palmas,
Canary Islands, Spain.

Masashi Yoshikawa, Hiroyuki Shindo, and Yuji Mat-
sumoto. 2016. Joint transition-based dependency
parsing and disfluency detection for automatic
speech recognition texts. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016, pages 1036–1041.

Daniel Zeman et al. 2017. CoNLL 2017 Shared Task:
Multilingual parsing from raw text to Universal De-
pendencies. In Proceedings of the CoNLL 2017
Shared Task: Multilingual Parsing from Raw Text
to Universal Dependencies, pages 1–19, Vancouver,
Canada. Association for Computational Linguistics.

Ana Zwitter Vitez, Jana Zemljarič Miklavčič, Simon
Krek, Marko Stabej, and Tomaž Erjavec. 2013. Spo-
ken corpus Gos 1.0. Slovenian language resource
repository CLARIN.SI.


