



















































Neural Gaussian Copula for Variational Autoencoder


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4333–4343,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4333

Neural Gaussian Copula for Variational Autoencoder

Prince Zizhuang Wang
Department of Computer Science

University of California Santa Barbara
zizhuang wang@ucsb.edu

William Yang Wang
Department of Computer Science

University of California Santa Barbara
william@cs.ucsb.edu

Abstract

Variational language models seek to estimate
the posterior of latent variables with an ap-
proximated variational posterior. The model
often assumes the variational posterior to be
factorized even when the true posterior is not.
The learned variational posterior under this
assumption does not capture the dependency
relationships over latent variables. We ar-
gue that this would cause a typical training
problem called posterior collapse observed in
all other variational language models. We
propose Gaussian Copula Variational Autoen-
coder (VAE) to avert this problem. Copula is
widely used to model correlation and depen-
dencies of high-dimensional random variables,
and therefore it is helpful to maintain the de-
pendency relationships that are lost in VAE.
The empirical results show that by modeling
the correlation of latent variables explicitly us-
ing a neural parametric copula, we can avert
this training difficulty while getting competi-
tive results among all other VAE approaches.
1

1 Introduction

Variational Inference (VI) (Wainwright et al.,
2008; Hoffman et al., 2013) methods are inspired
by calculus of variation (Gelfand et al., 2000). It
can be dated back to the 18th century when it was
mainly used to study the problem of the change
of functional, which is defined as the mapping
from functions to real space and can be understood
as the function of functions. VI takes a distri-
bution as a functional and then studies the prob-
lem of matching this distribution to a target dis-
tribution using calculus of variation. After the
rise of deep learning (Krizhevsky et al., 2012),
a deep generative model called Variational Au-
toencoder (Kingma and Welling, 2014; Hoffman

1Code will be released at https://github.com/
kingofspace0wzz/copula-vae-lm

Figure 1: Intuitive illustration of VI: The elliptic P is a
distribution family containing the true posterior p ∈ P ,
and the circle Q is a Mean-field variational family con-
taining a standard normal prior N . The optimal solu-
tion q∗ is the one in Q that has the smallest KL(q||p).
In reality these two families may not overlap.

et al., 2013) is proposed based on the theory of VI
and achieves great success over a huge number of
tasks, such as transfer learning (Shen et al., 2017),
unsupervised learning (Jang et al., 2017), image
generation (Gregor et al., 2015), semi-supervised
classification (Jang et al., 2017), and dialogue gen-
eration (Zhao et al., 2017). VAE is able to learn a
continuous space of latent random variables which
are useful for a lot of classification and generation
tasks.

Recent studies (Bowman et al., 2015; Yang
et al., 2017; Xiao et al., 2018; Xu and Durrett,
2018) show that when it comes to text genera-
tion and language modeling, VAE does not per-
form well and often generates random texts with-
out making good use of the learned latent codes.
This phenomenon is called Posterior Collapse,
where the Kullback-Leibler (KL) divergence be-
tween the posterior and the prior (often assumed
to be a standard Gaussian) vanishes. It makes the
latent codes completely useless because any text
input will be mapped to a standard Gaussian vari-
able. Many recent studies (Yang et al., 2017; Xu
and Durrett, 2018; Xiao et al., 2018; Miao et al.,

https://github.com/kingofspace0wzz/copula-vae-lm
https://github.com/kingofspace0wzz/copula-vae-lm


4334

2016; He et al., 2018) try to address this issue by
providing new model architectures or by changing
the objective functions. Our research lies in this
second direction. We review the theory of VAE,
and we argue that one of the most widely used
assumptions in VAE, the Mean-field assump-
tion, is problematic. It assumes that all approx-
imated solutions in a family of variational distri-
butions should be factorized or dimensional-wise
independent for tractability. We argue that it leads
to the posterior collapse problem since any varia-
tional posterior learned in this way does not main-
tain the correlation among latent codes and will
never match the true posterior which is unlikely
factorized.

We avert this problem by proposing a Neural
Gaussian Copula (Copula-VAE) model to train
VAE on text data. Copula (Nelsen, 2007) can
model dependencies of high-dimensional random
variables and is very successful in risk manage-
ment (Kole et al., 2007; McNeil et al., 2005),
financial management (Wang and Hua, 2014),
and other tasks that require the modeling of de-
pendencies. We provide a reparameterization
trick (Kingma and Welling, 2014) to incorporate
it with VAE for language modeling. We argue
that by maintaining the dependency relationships
over latent codes, we can dramatically improve the
performance of variational language modeling and
avoid posterior collapse. Our major contributions
can be summarized as the following:

• We propose Neural parameterized Gaussian
Copula to get a better estimation of the pos-
terior for latent codes.

• We provide a reparameterization technique
for Gaussian Copula VAE. The experiments
show that our method achieves competitive
results among all other variational language
modeling approaches.

• We perform a thorough analysis of the orig-
inal VAE and copula VAE. The results and
analysis reveal the salient drawbacks of VAE
and explain how introducing a copula model
could help avert the posterior collapse prob-
lem.

2 Related Work

Copula: Before the rise of Deep Learning
Copula (Nelsen, 2007) is a multivariate distribu-
tion whose marginals are all uniformly distributed.

Over the years, it is widely used to extract corre-
lation within high-dimensional random variables,
and achieves great success in many subjects such
as risk management (Kole et al., 2007; McNeil
et al., 2005), finance (Wang and Hua, 2014), civil
engineering(Chen et al., 2012; Zhang and Singh,
2006), and visual description generation (Wang
and Wen, 2015). In the past, copula is often es-
timated by Maximum Likelihood method (Choroś
et al., 2010; Jaworski et al., 2010) via parametric
or semi-parametric approaches (Tsukahara, 2005;
Choroś et al., 2010). One major difficulty when
estimating the copula and extracting dependen-
cies is the dimensionality of random variables. To
overcome the curse of dimensionality, a graphi-
cal model called vine copula (Joe and Kurowicka,
2011; Czado, 2010; Bedford et al., 2002) is pro-
posed to estimate a high-dimensional copula den-
sity by breaking it into a set of bivariate condi-
tional copula densities. However, this approach
is often hand-designed which requires human ex-
perts to define the form of each bivariate cop-
ula, and hence often results in overfitting. There-
fore, Gaussian copula (Xue-Kun Song, 2000; Frey
et al., 2001) is often used since its multivariate ex-
pression has a simple form and hence does not suf-
fer from the curse of dimensionality.

VAE for Text Bowman et al. (2015) proposed
to use VAE for text generation by using LSTM as
encoder-decoder. The encoder maps the hidden
states to a set of latent variables, which are fur-
ther used to generate sentences. While achieving
relatively low sample perplexity and being able to
generate easy-to-read texts, the LSTM VAE often
results in posterior collapse, where the learned la-
tent codes become useless for text generation.

Recently, many studies are focusing on how
to avert this training problem. They either pro-
pose a new model architecture or modify the
VAE objective. Yang et al. (2017) seeks to re-
place the LSTM (Hochreiter and Schmidhuber,
1997) decoder with a CNN decoder to control
model expressiveness, as they suspect that the
over-expressive LSTM is one reason that makes
KL vanish. Xiao et al. (2018) introduces a topic
variable and pre-trains a Latent Dirichlet Alloca-
tion (Blei et al., 2003) model to get a prior distri-
bution over the topic information. Xu and Dur-
rett (2018) believes the bubble soup effect of high-
dimensional Gaussian distribution is the main rea-
son that causes KL vanishing, and therefore learns



4335

a hyper-spherical posterior over the latent codes.

3 Variational Inference

The problem of inference in probabilistic mod-
eling is to estimate the posterior density p(z|x)
of latent variable z given input samples {xi}Di=1.
The direct computation of the posterior is in-
tractable in most cases since the normalizing con-
stant

∫
p(z,x)dz lacks an analytic form. To get an

approximation of the posterior, many approaches
use sampling methods such as Markov chain
Monte Carlo (MCMC) (Gilks et al., 1995) and
Gibbs sampling (George and McCulloch, 1993).
The downside of sampling methods is that they
are inefficient, and it is hard to tell how close
the approximation is from the true posterior. The
other popular inference approach, variational in-
ference (VI) (Wainwright et al., 2008; Hoffman
et al., 2013), does not have this shortcoming as it
provides a distance metric to measure the fitness
of an approximated solution.

In VI, we assume a variational family of distri-
butions Q to approximate the true posterior. The
Kullback-Leibler (KL) divergence is used to mea-
sure how close q ∈ Q is to the true p(z|x). The
optimal variational posterior q∗ ∈ Q is then the
one that minimizes the KL divergence

KL(q||p) =
∑

q(z|x)log q(z|x)
p(z|x)

Based on this, variational autoencoder
(VAE) (Kingma and Welling, 2014) is proposed
as a latent generative model that seeks to learn a
posterior of the latent codes by minimizing the
KL divergence between the true joint density
pθ(x, z) the variational joint density qφ(z,x).
This is equivalent to maximizing the following
evidence lower bound ELBO,

L(θ;φ;x) = −KL(qφ(z,x)||pθ(x, z))
= Eq(x)[Eqφ(z|x)[log pθ(x|z)]
−KL(qφ(z|x)||p(z))]

In this case, Mean-field (Kingma and Welling,
2014) assumption is often used for simplicity.
That is, we assume that the members of vari-
ational family Q are dimensional-wise indepen-
dent, meaning that the posterior q can be writ-
ten as q(z|x) =

∏D
i=1 q(zi|x). The simplicity

of this form makes the estimation of ELBO very
easy. However, it also leads to a particular training

difficulty called posterior collapse, where the KL
divergence term becomes zero and the factorized
variational posterior collapses to the prior. The la-
tent codes z would then become useless since the
generative model p(x|z) no longer depends on it.

We believe the problem comes from the nature
of variational family itself and hence we propose
our Copula-VAE which makes use of the depen-
dency modeling ability of copula model to guide
the variational posterior to match the true poste-
rior. We will provide more details in the following
sections.

We hypothesize that the Mean-field assumption
is problematic itself as the q under this assumption
can never recover the true structure of p. On the
other hand, Copula-VAE makes use of the depen-
dency relationships maintained by a copula model
to guide the variational posterior to match the true
posterior. Our approach differs from Gaussian
Copula-VAE (Suh and Choi, 2016) in that we use
copula to estimate the joint density p(z) rather
than the empirical data density p(x).

4 Our Approach: Neural Gaussian
Copula

4.1 Gaussian Copula
In this section, we review the basic concepts of
Gaussian copula. Copula is defined as a probabil-
ity distribution over a high-dimensional unit cube
[0, 1]d whose univariate marginal distributions are
uniform on [0, 1]. Formally, given a set of unifor-
maly distributed random variables U1, U2, ..., Un,
a copula is a joint distribution defined as

C(u1, u2, ..., un) = P (U1 ≤ u1, ..., Un ≤ un)

What makes a copula model above so useful is
the famous Sklar’s Theorem. It states that for any
joint cumulative distribution function (CDF) with
a set of random variables {xi}d1 and marginal CDF
Fi(xi) = P (Xi ≤ x), there exists one unique cop-
ula function such that the joint CDF is

F (x1, ..., xd) = C(F1(x1), ..., Fd(xd))

By probability integral transform, each marginal
CDF is a uniform random variable on [0, 1].
Hence, the above copula is a valid one. Since
for each joint CDF, there is one unique cop-
ula function associated with it given a set of
marginals, we can easily construct any joint distri-
bution whose marginal univariate distributions are



4336

the ones Fi(xi) that are given. And, for a given
joint distribution, we can also find the correspond-
ing copula which is the CDF function of the given
marginals.

A useful representation we can get by Sklar’s
Theorem for a continuous copula is,

C(u1, ...ud) = F (F
−
1 (u1), ..., F

−
d (ud))

If we further restrict the marginals to be Gaus-
sian, then we can get an expression for Gaussian
copula, that is,

CΣ(u1, ..., ud) = Φ(Φ
−
1 (u1), ...,Φ

−
d (ud); 0,Σ)

where Φ(·; Σ) is the CDF of a multivariate Gaus-
sian DistributionN (0,Σ), and {Φ−i } is the inverse
of a set of marginal Gaussian CDF.

To calculate the joint density of a copula func-
tion, we take the derivative with respect to random
variables u and get

cΣ(u1, ..., ud) =
∂CΣ(u1, ..., ud)

∂u1 · · ·ud

=
∂CΣ(u1, ..., ud)

∂q1 · · · qd

d∏
i=1

∂qi
∂ui

= (
d∏
i=1

σi)|Σ−1/2|exp(−
1

2
qTMq)

where M = Σ− − diag(Σ)−, and qi = Φ−i (ui).
Then, if the joint density p(x1, ..., xd) has a

Gaussian form, it can be expressed by a copula
density and its marginal densities, that is,

p(x1, ..., xd) =
∂F (·)

∂x1 · · ·xd

=
∂CΣ(·)
∂u1 · · ·ud

∏ ∂ui
∂xi

= cΣ(u1, ..., ud)
∏
i

p(xi)

Therefore, we can decompose the problem of es-
timating the joint density into two smaller sub-
problems: one is the estimation for the marginals;
the other is the estimation for the copula density
function cΣ. In many cases, we assume indepen-
dence over random variables due to the intractabil-
ity of the joint density. For example, in the case
of variational inference, we apply Mean-Field as-
sumption which requires the variational distribu-
tion family to have factorized form so that we can
get a closed-form KL divergence with respect to

the prior. This assumption, however, sacrifices
the useful dependency relationships over the la-
tent random variables and often leads to training
difficulties such as the posterior collapse problem.
If we assume the joint posterior of latent vari-
ables to be Gaussian, then the above Gaussian cop-
ula model can be used to recover the correlation
among latent variables which helps obtain a bet-
ter estimation of the joint posterior density. In the
VAE setting, we can already model the marginal
independent posterior of latent variables, so the
only problem left is how to efficiently estimate the
copula density function cΣ. In the next section, we
introduce a neural parameterized Gaussian copula
model, and we provide a way to incorporate it with
the reparameterization technique used in VAE.

4.2 Neural Gaussian Copula for VI
By Mean-field assumption, we construct a varia-
tional familyQ assuming that each member q ∈ Q
can be factorized,

q(z) =

d∏
i

q(zi)

This assumption, however, loses dependencies
over latent codes and hence does not consider the
non-factorized form of the true posterior. In this
case, as pictured by Figure 2, when we search the
optimal q*, it will never reach to the true poste-
rior p. If we relieve the assumption, the varia-
tional family may overlap with the posterior fam-
ily. However, this is intractable as the Monto Carlo
estimator with respect to the objective often has
very high variance (Kingma and Welling, 2014).
Hence, we need to find a way to make it possi-
ble to match the variational posterior with the true
posterior while having a simple and tractable ob-
jective function so that the gradient estimator of
the expectation is simple and precise.

This is where Gaussian Copula comes into the
story. Given a factorized posterior, we can con-
struct a Gaussian copula for the joint posterior,

qφ(z|x) = cΣ(Φ1(z1), ...Φ(zd))
d∏
i

qφ(zi|x)

where cΣ is the Gaussian copula density. If we
take the log on both sides, then we have,

log qφ(z|x) = log cΣ(u1, ..., ud) +
d∑
i

log qφ(zi|x)



4337

Figure 2: Training stage of VAE. Initially, the model tries to maximize ELBO by maximizing Eq(z|x)[p(x|z)]. Once
Eq(z|x)[p(x|z)] is maximized, the model maximizes ELBO by minimizing KL. During this stage, the posterior
starts to move closer to the prior. In the final stage, the posterior collapses to the prior. But, the ELBO and
Eq(z|x)[p(x|z)] are already maximized, which means the model keeps constraining KL and there are not enough
gradients to move the posterior away from the prior anymore.

Figure 3: Ideal final stage of Copula-VAE. The family
of distributions that contains the true posterior is now a
subset of the variational family.

Note that the second term on the right hand side
is just the factorized log posterior we have in
the original VAE model. By reparameterization
trick (Kingma and Welling, 2014), latent codes
sampled from the posterior are parameterized as
a deterministic function of µ and σ2, that is, z =
µ+σ · �, � ∼ N (0, I), where µ, σ2 are parameter-
ized by two neural networks whose inputs are the
final hidden states of the LSTM encoder. Since∏
i qφ(zi|x) = N (µ, σ2I), we can compute the

sum of log density of posterior by,

d∑
i

log qφ(zi|x) = −
d∑
i

log |σi| −
d∑
i

(zi − µi)2

2σ2i

− d
2

log 2π

Now, to estimate the log copula density
log cΣ(·), we provide a reparameterization method
for the copula samples q ∼ CΣ(Φ1(q1), ...Φ(qd)).
As suggested by Kingma and Welling (2014);
Hoffman et al. (2013), reparameterization is
needed as it gives a differentiable, low-variance

estimator of the objective function. Here, we pa-
rameterize the copula samples with a determinis-
tic function with respect to the Cholesky factor L
of its covariance matrix Σ. We use the fact that
for any multivariate Gaussian random variables,
a linear transformation of them is also a multi-
variate Gaussian random variable. Formally, if
X ∼ N (µ,Σ), and Y = AX , then we must have
Y ∼ N (Aµ,AΣAT ). Hence, for a Gaussian cop-
ula with the form cΣ = N (0,Σ), we can reparam-
eterize its samples q by,

� ∼ N (0, I)
q = L · �

It is easy to see that q = L · � ∼ N (0, LI−LT =
Σ) is indeed a sample from the Gaussian cop-
ula model. This is the standard way of sampling
from Gaussian distribution with covariance matrix
LLT . To ensure numerical stability of the above
reparameterization and to ensure that the covari-
ance Σ = LLT is positive definite, we provide the
following algorithm to parameterize L.

Algorithm 1: Neural reparameterization of
Copula: Cholesky approach

h = LSTM(x)
w = ReLU(W1 · h+ b1)
a = Tanh(W2 · h+ b2)
Σ = w · I + aaT
L = CholeskyFactorization(Σ)

In Algorithm 1, we first parameterize the covari-
ance matrix and then perform a Cholesky factor-



4338

the company said it will be sold to the company ’s promotional programs and UNK
the company also said it will sell $ n million of soap eggs turning millions of dollars
the company said it will be UNK by the company ’s UNK division n
the company said it would n’t comment on the suit and its reorganization plan
mr . UNK said the company ’s UNK group is considering a UNK standstill agreement with the company
traders said that the stock market plunge is a UNK of the market ’s rebound in the dow jones industrial average
one trader of UNK said the market is skeptical that the market is n’t UNK by the end of the session
the company said it expects to be fully operational by the company ’s latest recapitalization

i was excited to try this place out for the first time and i was disappointed .
the food was good and the food a few weeks ago , i was in the mood for a UNK of the UNK
i love this place . i ’ ve been here a few times and i ’ m not sure why i ’ ve been
this place is really good . i ’ ve been to the other location many times and it ’s very good .
i had a great time here . i was n’t sure what i was expecting . i had the UNK and the
i have been here a few times and have been here several times . the food is good , but the food is good
this place is a great place to go for lunch . i had the chicken and waffles . i had the chicken and the UNK
i really like this place . i love the atmosphere and the food is great . the food is always good .

Table 1: Qualitative comparison between VAE and our proposed approach. First row: PTB samples generated
from prior p(z) by VAE (upper half ) and copula-VAE (lower half ). Second row: Yelp samples generated from
prior p(z) by VAE (upper half ) and copula-VAE (lower half ).

ization (Chen et al., 2008) to get the Cholesky fac-
tor L. The covariance matrix Σ = w · I + aaT
formed in this way is guaranteed to be positive
definite. It is worth noting that we do not sam-
ple the latent codes from Gaussian copula. In fact,
z still comes from the independent Gaussian dis-
tribution. Rather, we get sample q from Gaussian
copula CΣ so that we can compute the log cop-
ula density term in the following, which will then
be used as a regularization term during training, in
order to force the learned z to respect the depen-
dencies among individual dimensions.

Now, to calculate the log copula density, we
only need to do,

log cΣ =

d∑
i

log σi − 1/2 log |Σ|+ 1/2qTMq

where M = diag(Σ−)− Σ−.
To make sure that our model maintains the de-

pendency structure of the latent codes, we seek to
maximize both the ELBO and the joint log poste-
rior likelihood log q(z|x) during the training. In
other words, we maximize the following modified
ELBO,

L′ = L+ λ(log cΣ(·) +
d∑
i

log qφ(zi|x))

where L is the original ELBO. λ is the weight of
log density of the joint posterior. It controls how
good the model is at maintaining the dependency
relationships of latent codes. The reparameteriza-
tion tricks both for z and q makes the above objec-
tive fully differentiable with respect to µ, σ2,Σ.

Maximizing L′ will then maximize the log in-
put likelihood log p(x) and the joint posterior log-
likelihood log q(z|x). If the posterior collapses to
the prior and has a factorized form, then the joint
posterior likelihood will not be maximized since
the joint posterior is unlikely factorized. There-
fore, maximizing the joint posterior log-likelihood
along with ELBO forces the model to generate
readable texts while also considering the depen-
dency structure of the true posterior distribution,
which is never factorized.

4.3 Evidence Lower Bound

Another interpretation can be seen by taking a
look at the prior. If we compose the copula density
with the prior, then, like the non-factorized poste-
rior, we can get the non-factorized prior,

log pθ(z) = log cΣ(u1, ..., ud) +
d∑
i

log pθ(zi)

And the corresponding ELBO is,

L(θ;φ;x) = Eq(x)[Eqφ(z|x)[log pθ(x|z)]
−KL(qφ(z|x)||pθ(z))]

= Eq(x)[Eqφ(z|x)[log pθ(x|z)]
− Eqφ(z|x)[log qφ(z|x)− log p(z)]

+ Eqφ(z|x)[log cΣ(u1, ..., ud)]]

Like Normalizing flow (Rezende and Mohamed,
2015), maximizing the log copula density will
then learns a more flexible prior other than a stan-
dard Gaussian. The dependency among each zi is



4339

then restored since the KL term will push the pos-
terior to this more complex prior.

We argue that relieving the Mean-field assump-
tion by maintaining the dependency structure can
avert the posterior collapse problem. As shown
in Figure 2, during the training stage of origi-
nal VAE, if kl-annealing (Bowman et al., 2015)
is used, the model first seeks to maximize the ex-
pectation Eq(z|x)[p(x|z)]. Then, since q(z|x) can
never reach to the true p(z|x), q will reach to a
boundary and then the expectation can no longer
increase. During this stage, the model starts to
maximize the ELBO by minimizing the KL diver-
gence. Since the expectation is maximized and can
no longer leverage KL, the posterior will collapse
to the prior and there is not sufficient gradient to
move it away since ELBO is already maximized.
On the other hand, if we introduce a copula model
to help maintain the dependency structure of the
true posterior by maximizing the joint posterior
likelihood, then, in the ideal case, the variational
family can approximate distributions of any forms
since it is now not restricted to be factorized, and
therefore it is more likely for q in Figure 3 to be
closer to the true posterior p. In this case, the
Eq(z|x)[p(x|z)] can be higher since now we have
latent codes sampled from a more accurate poste-
rior, and then this expectation will be able to lever-
age the decrease of KL even in the final training
stage.

5 Experimental Results

5.1 Datasets

Data Train Valid Test Vocab
Yelp13 62522 7773 8671 15K
PTB 42068 3370 3761 10K
Yahoo 100K 10K 10K 20K

Table 2: Size and vocabulary size for each dataset.

In the paper, we use Penn Tree (Marcus et al.,
1993), Yahoo Answers (Xu and Durrett, 2018;
Yang et al., 2017), and Yelp 13 reviews (Xu
et al., 2016) to test our model performance over
variational language modeling tasks. We use
these three large datasets as they are widely used
in all other variational language modeling ap-
proaches (Bowman et al., 2015; Yang et al., 2017;
Xu and Durrett, 2018; Xiao et al., 2018; He et al.,
2018; Kim et al., 2018). Table 2 shows the statis-
tics, vocabulary size, and number of samples in
Train/Validation/Test for each dataset.

Model NLL KL PPL
LSTM-LM (Yang et al., 2017) 116.2 - 104.2
VAE (Bowman et al., 2015) 105.2 1.74 121
vmf-VAE (Xu and Durrett, 2018) 96.0 5.7 79.6
VAE-NF 96.8 0.87 82.9
non-diag-VAE 105.1 4.9 121.0
copula-VAE(cho) λ = 0.4 92.2 7.3 67.2

Table 3: Variational language modeling on PTB
Model NLL KL PPL
VAE (Bowman et al., 2015) 197.1 0.03 58.2
vmf-VAE (Xu and Durrett, 2018) 198.0 6.4 59.3
VAE-NF 200.4 0.1 62.5
non-diag-VAE 198.3 4.7 59.7
copula-VAE(cho) λ = 0.5 187.7 10.0 48.0

Table 4: Variational language modeling on Yelp Re-
views 13

Model NLL KL PPL
VAE (Bowman et al., 2015) 351.6 0.3 81.6
vmf-VAE (Xu and Durrett, 2018) 359.3 17.9 89.9
VAE-NF 353.8 0.1 83.0
lagging-VAE (He et al., 2018) 326.6 6.7 64.9
non-diag-VAE 352.2 5.7 82.3
copula-VAE(cho) λ = 0.5 344.2 15.4 74.4

Table 5: Variational language modeling on Yahoo

5.2 Experimental Setup

We set up a similar experimental condition as in
(Bowman et al., 2015; Xiao et al., 2018; Yang
et al., 2017; Xu and Durrett, 2018). We use LSTM
as our encoder-decoder model, where the num-
ber of hidden units for each hidden state is set to
512. The word embedding size is 512. And, the
number of dimension for latent codes is set to 32.
For both encoder and decoder, we use a dropout
layer for the initial input, whose dropout rate is
α = 0.5. Then, for inference, we pass the final
hidden state to a linear layer following a Batch
Normalizing (Ioffe and Szegedy, 2015) layer to
get reparameterized samples from

∏
i q(zi|x) and

from Gaussian copula CΣ. For the training stage,
the maximum vocabulary size for all inputs are set
to 20000, and the maximum sequence length is set
to 200. Batch size is set to 32, and we train for 30
epochs for each dataset, where we use the Adam
stochastic optimization (Kingma and Ba, 2014)
whose learning rate is r = 10−3.

We use kl annealing (Bowman et al., 2015) dur-
ing training. We also observe that the weight of log
copula density is the most important factor which
determines whether our model avoids the posterior
collapse problem. We hyper tuned this parameter
in order to find the optimal one.



4340

(a) train loss (b) train kld

(c) validation loss (d) validation kld

Figure 4: Training and Validation KL divergence and sequential loss for PTB using Cholesky Neural Copula with
different copula density weights λ. It is obvious that the optimal λ plays a huge role in alleviating the posterior
collapse problem and does not result in overfitting as VAE does. Models are trained using 1-layer LSTM with 200
hidden units for Encoder/Decoder, in which the embedding size is 200 and the latent dimension is 32.

Figure 5: Reconstruction loss, KL divergence, and PPL
(sum of loss and KL) on PTB. When we gradually in-
crease λ, the KL divergence increases and the test re-
construction will decrease.

5.3 Language Modeling Comparison

5.3.1 Comparison with other Variational
models

We compare the variational language modeling re-
sults over three datasets. We show the results for
Negative log-likelihood (NLL), KL divergence,
and sample perplexity (PPL) for each model on

these datasets. NLL is approximated by the evi-
dence lower bound.

First, we observe that kl-annealing does not
help alleviate the posterior collapse problem when
it comes to larger datasets such as Yelp, but the
problem is solved if we can maintain the latent
code’s dependencies by maximizing the copula
likelihood when we maximize the ELBO. We also
observe that the weight λ of log copula density af-
fects results dramatically. All λ produce compet-
itive results compared with other methods. Here,
we provide the numbers for those weights λ that
produce the lowest PPL. For PTB, copula-VAE
achieves the lowest sample perplexity, best NLL
approximation, and do not result in posterior col-
lapse when λ = 0.4. For Yelp, the lowest sample
perplexity is achieved when λ = 0.5.

We also compare with VAE models trained
with normalizing flows (Rezende and Mohamed,
2015). We observed that our model is superior
to VAE based on flows. It is worth noting that
Wasserstein Autoencoder trained with Normaliz-
ing flow (Wang and Wang, 2019) achieves the low-
est PPL 66 on PTB, and 41 on Yelp. However, the



4341

problem of designing flexible normalizing flow is
orthogonal to our research.

5.3.2 Generation
Table 1 presents the results of text generation task.
We first randomly sample z from p(z), and then
feed it into the decoder p(x|z) to generate text us-
ing greedy decoding. We can tell whether a model
suffers from posterior collapse by examining the
diversity of the generated sentences. The original
VAE tends to generate the same type of sequences
for different z. This is very obvious in PTB where
the posterior of the original VAE collapse to the
prior completely. Copula-VAE, however, does not
have this kind of issue and can always generate a
diverse set of texts.

5.4 Hyperparameter-Tuning: Copula weights
play a huge role in the training of VAE

In this section, we investigate the influence of log
copula weight λ over training. From Figure 5, we
observe that our model performance is very sen-
sitive to the value of λ. We can see that when λ
is small, the log copula density contributes a small
part to the objective, and therefore does not help to
maintain dependencies over latent codes. In this
case, the model performs like the original VAE,
where the KL divergence becomes zero at the end.
When we increase λ, test KL becomes larger and
test reconstruction loss becomes smaller.

This phenomenon is also observed in validation
datasets, as shown in Figure 4. The training PPL
is monotonically decreasing in general. However,
when λ is small and the dependency relationships
over latent codes are lost, the model quickly over-
fits, as the KL divergence quickly becomes zero
and the validation loss starts to increase. This fur-
ther confirms what we showed in Figure 2. For
original VAE models, the model first maximizes
Eq(z|x)[p(x|z)] which results in the decrease of
both train and validation loss. Then, as q(z|x) can
never match to the true posterior, Eq(z|x)[p(x|z)]
reaches to its ceiling which then results in the
decrease of KL as it is needed to maximize the
ELBO. During this stage, the LSTM decoder starts
to learn how to generate texts with standard Gaus-
sian latent variables which then causes the in-
crease of validation loss. On the other hand, if we
gradually increase the contribution of copula den-
sity by increasing the λ, the model is able to main-
tain the dependencies of latent codes and hence
the structure of the true posterior. In this case,

Eq(z|x)[p(x|z)] will be much higher and will lever-
age the decrease of KL. In this case, the decoder is
forced to generate texts from non-standard Gaus-
sian latent codes. Therefore, the validation loss
also decreases monotonically in general.

One major drawback of our model is the amount
of training time, which is 5 times longer than
the original VAE method. In terms of perfor-
mance, copula-VAE achieves the lowest recon-
struction loss when λ = 0.6. It is clear that from
Figure 5 that increasing λ will result in larger KL
divergence.

6 Conclusion

In this paper, we introduce Copula-VAE with
Cholesky reparameterization method for Gaussian
Copula. This approach averts Posterior Collapse
by using Gaussian copula to maintain the depen-
dency structure of the true posterior. Our results
show that Copula-VAE significantly improves the
language modeling results of other VAEs.

References
Tim Bedford, Roger M Cooke, et al. 2002. Vines–

a new graphical model for dependent random vari-
ables. The Annals of Statistics, 30(4):1031–1068.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. CoNLL.

Lu Chen, Vijay P Singh, Shenglian Guo, Ashok K
Mishra, and Jing Guo. 2012. Drought analysis us-
ing copulas. Journal of Hydrologic Engineering,
18(7):797–808.

Yanqing Chen, Timothy A Davis, William W Hager,
and Sivasankaran Rajamanickam. 2008. Algorithm
887: Cholmod, supernodal sparse cholesky factor-
ization and update/downdate. ACM Transactions on
Mathematical Software (TOMS), 35(3):22.

Barbara Choroś, Rustam Ibragimov, and Elena Permi-
akova. 2010. Copula estimation. In Copula theory
and its applications, pages 77–91. Springer.

Claudia Czado. 2010. Pair-copula constructions of
multivariate copulas. In Copula theory and its ap-
plications, pages 93–109. Springer.

Rüdiger Frey, Alexander J McNeil, and Mark Nyfeler.
2001. Copulas and credit models. Risk,
10(111114.10).



4342

Izrail Moiseevitch Gelfand, Richard A Silverman, et al.
2000. Calculus of variations. Courier Corporation.

Edward I George and Robert E McCulloch. 1993. Vari-
able selection via gibbs sampling. Journal of the
American Statistical Association, 88(423):881–889.

Walter R Gilks, Sylvia Richardson, and David Spiegel-
halter. 1995. Markov chain Monte Carlo in practice.
Chapman and Hall/CRC.

Karol Gregor, Ivo Danihelka, Alex Graves,
Danilo Jimenez Rezende, and Daan Wierstra.
2015. Draw: A recurrent neural network for image
generation. ICML.

Junxian He, Daniel Spokoyny, Graham Neubig, and
Taylor Berg-Kirkpatrick. 2018. Lagging inference
networks and posterior collapse in variational au-
toencoders.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Matthew D Hoffman, David M Blei, Chong Wang,
and John Paisley. 2013. Stochastic variational infer-
ence. The Journal of Machine Learning Research,
14(1):1303–1347.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint
arXiv:1502.03167.

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cat-
egorical reparameterization with gumbel-softmax.
ICLR.

Piotr Jaworski, Fabrizio Durante, Wolfgang Karl Har-
dle, and Tomasz Rychlik. 2010. Copula theory and
its applications, volume 198. Springer.

Harry Joe and Dorota Kurowicka. 2011. Dependence
modeling: vine copula handbook. World Scientific.

Yoon Kim, Sam Wiseman, Andrew Miller, David Son-
tag, and Alexander Rush. 2018. Semi-amortized
variational autoencoders. In International Confer-
ence on Machine Learning, pages 2683–2692.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. stat, 1050:10.

Erik Kole, Kees Koedijk, and Marno Verbeek. 2007.
Selecting copulas for risk management. Journal of
Banking & Finance, 31(8):2405–2423.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Alexander J McNeil, Rüdiger Frey, Paul Embrechts,
et al. 2005. Quantitative risk management: Con-
cepts, techniques and tools, volume 3. Princeton
university press Princeton.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural
variational inference for text processing. In ICML.

Roger B Nelsen. 2007. An introduction to copulas.
Springer Science & Business Media.

Danilo Jimenez Rezende and Shakir Mohamed. 2015.
Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In NIPS.

Suwon Suh and Seungjin Choi. 2016. Gaussian cop-
ula variational autoencoders for mixed data. arXiv
preprint arXiv:1604.04960.

Hideatsu Tsukahara. 2005. Semiparametric estimation
in copula models. Canadian Journal of Statistics,
33(3):357–375.

Martin J Wainwright, Michael I Jordan, et al. 2008.
Graphical models, exponential families, and varia-
tional inference. Foundations and Trends® in Ma-
chine Learning, 1(1–2):1–305.

Prince Zizhuang Wang and William Yang Wang. 2019.
Riemannian normalizing flow on variational wasser-
stein autoencoder for text modeling. arXiv preprint
arXiv:1904.02399.

William Yang Wang and Zhenhao Hua. 2014. A semi-
parametric gaussian copula regression model for
predicting financial risks from earnings calls. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1155–1165.

William Yang Wang and Miaomiao Wen. 2015. I
can has cheezburger? a nonparanormal approach to
combining textual and visual information for pre-
dicting and generating popular meme descriptions.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Denver, CO, USA. ACL.

Yijun Xiao, Tiancheng Zhao, and William Yang Wang.
2018. Dirichlet variational autoencoder for text
modeling. arXiv preprint arXiv:1811.00135.

Jiacheng Xu, Danlu Chen, Xipeng Qiu, and Xuangjing
Huang. 2016. Cached long short-term memory neu-
ral networks for document-level sentiment classifi-
cation. EMNLP.



4343

Jiacheng Xu and Greg Durrett. 2018. Spherical latent
spaces for stable variational autoencoders. EMNLP.

Peter Xue-Kun Song. 2000. Multivariate dispersion
models generated from gaussian copula. Scandina-
vian Journal of Statistics, 27(2):305–320.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. ICML.

LSVP Zhang and VP Singh. 2006. Bivariate flood fre-
quency analysis using the copula method. Journal
of hydrologic engineering, 11(2):150–164.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. ACL.


