



















































SRHR at SemEval-2017 Task 6: Word Associations for Humour Recognition


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 401–406,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

SRHR at SemEval-2017 Task 6: Word Associations for Humour
Recognition

Andrew Cattle Xiaojuan Ma
Hong Kong University of Science and Technology
Department of Computer Science and Engineering

Clear Water Bay, Hong Kong
{acattle,mxj}@cse.ust.hk

Abstract

This paper explores the role of semantic
relatedness features, such as word asso-
ciations, in humour recognition. Specif-
ically, we examine the task of infer-
ring pairwise humour judgments in Twit-
ter hashtag wars. We examine a va-
riety of word association features de-
rived from the University of Southern
Florida Free Association Norms (USF)
(Nelson et al., 2004) and the Edinburgh
Associative Thesaurus (EAT) (Kiss et al.,
1973) and find that word association-
based features outperform Word2Vec sim-
ilarity, a popular semantic relatedness
measure. Our system achieves an ac-
curacy of 56.42% using a combination
of unigram perplexity, bigram perplex-
ity, EATtweet-avgdifference, USF

max
forward, EAT

word-avg
difference,

USFword-avgdifference, EAT
min
forward, USF

tweet-max
difference , and

EATminbackward.

1 Introduction

What makes something funny? Humour is very
personal; what is funny to one person may not be
funny to another. Yet, there are still certain works
which seem to have widespread appeal, from co-
median Louis C.K. to sitcom The Big Bang The-
ory. What makes these works more humorous to
the average person than similar ones? A good
place to start might be with the show @midnight
and their nightly Hashtag Wars segment. Each
night viewers are given a prompt in the form of
a Twitter hashtag and asked to tweet their funni-
est responses. Given two such tweets, how can we
decide which is funnier?

This paper largely focuses on semantic
relatedness-based features and their application in
humour recognition. It is reasonable to assume

that a punchline should be related to a setup; that
is to say, a tweet’s relevance to its hashtag prompt
should be apparent. Similarly, it is reasonable
to assume that punchlines should have a certain
amount of unexpectedness; in other words,
funnier tweets should be harder to guess. As
such, it follows that semantic relation strength in
general should serve as a barometer for humour
where weaker relations are less understandable
and stronger relations are more obvious (Cattle
and Ma, 2016). Moreover, we hypothesize that
the interplay between this understandability and
unexpectedness should provide an even more
powerful indication of humour.

2 Previous Work

Early work on computational humour focused
more on humour generation in specific contexts,
such as punning riddles (Binsted and Ritchie,
1994; Ritchie et al., 2007), humorous acronyms
(Stock and Strapparava, 2003), or jokes in the
form of “I like my X like I like my Y” (Petrovic
and Matthews, 2013). Labutov and Lipson (2012)
offered a slightly more generalized approach using
Semantic Script Theory of Humour.

Recently, humour recognition has gained in-
creasing attention. Taylor and Mazlack (2004)
presented a method for recognizing wordplay in
“Knock Knock” jokes. Mihalcea and Strapparava
(2005) used stylistic features, such as alliteration
and antonymy, to identify humorous one-liners.
Mihalcea and Pulman (2007) expanded on this ap-
proach, finding that human-centeredness and neg-
ative sentiment are both useful in not only identi-
fying humorous one-liners, but also distinguishing
satirical news articles from genuine ones. Related
to humor recognition, irony identification (Davi-
dov et al., 2010; Tsur et al., 2010; Reyes et al.,
2012) typically uses n-gram and sentiment fea-

401



tures to distinguish ironic from non-ironic tweets.
Shahaf et al. (2015) and Radev et al. (2016) ex-

amine humour recognition as a ranking task. Both
works aim to identify the funnier of a pair of car-
toon captions taken from submissions to The New
Yorker’s Cartoon Caption Contest1. Each week,
New Yorker readers are presented “a cartoon in
need of a caption” and encouraged to submit their
own humorous suggestions. Shahaf et al. (2015)
found that simpler grammatical structures, less re-
liance on proper nouns, and shorter joke phrases
all lead to funnier captions. Radev et al. (2016)
showed that in addition to human-centeredness
and sentiment, high LexRank score was a strong
indication of humour, where LexRank is a graph-
based text summarization technique introduced in
Erkan and Radev (2004).

Cattle and Ma (2016) noted that cartoon cap-
tion contests and hashtag wars are very similar in
that they both involve short, humorous texts writ-
ten as a response to an external stimulus. Further-
more, Cattle and Ma (2016) explored the role of
semantic relatedness between setups and punch-
lines in perceived humour and found USF Free
Association Norm- (Nelson et al., 2004) and Nor-
malized Google Distance-based features (Cilibrasi
and Vitanyi, 2007) to be useful in identifying fun-
nier tweets. However, the results of Cattle and Ma
(2016) were based on a small dataset of only four
hashtag prompts, inferred humour judgments from
Twitter likes and retweets, and relied on human
annotations to identify both setups and punchlines.

3 System Definition

3.1 Dataset

We performed all training and testing on the
dataset introduced in Potash et al. (2017) specifi-
cally for this task. The dataset consists of response
tweets to 112 hashtags created by @midnight. The
tweets are separated into files according to their
respective hashtags, each hashtag file containing
an average of 114 tweets. Each tweet includes a
label specifying whether it was deemed to be fun-
niest, in the top ten, or neither for that particular
hashtag according to the @midnight staff. Potash
et al. (2017) further divides the hashtags into three
sets: Trial, Training, and Evaluation containing
five, 101, and six hashtags, respectively.

1http://contest.newyorker.com/

3.2 Preprocessing and Baseline Features

Before feature extraction, tweets went through a
preprocessing procedure. Each tweet was low-
ercased and then tokenized and POS tagged us-
ing Tweet NLP (Gimpel et al., 2011; Owoputi
et al., 2013). English stop words were removed
along with any punctuation, discourse markers
(e.g. “RT”), interjections, emoticons, and URLs
according to Tweet NLP’s POS tags. Further-
more, prepositions, postpositions, subordinating
conjunctions, coordinating conjunctions, verb par-
ticles, and predeterminers were also removed as
these tended to be closed-class words (Gimpel
et al., 2011) which do not affect the word-level se-
mantic relationships this paper focuses on. Any
references to the @midnight Twitter account or
the relevant hashtag prompt were also removed.
Each hashtag prompt was tokenized according
to the hashtag segmentations included with the
dataset. English stop words were removed along
with any single digit numbers and the words “in”
and “words”. This was to omit the collocation
“in # words”, a common type of hashtag prompt,
which does not affect semantic meaning.

Following the model of Shahaf et al. (2015),
unigram and bigram bag-of-words features were
extracted for each tweet. Furthermore, both un-
igram and bigram perplexities were calculated
based on a simple language model created us-
ing n-gram counts from the Rovereto Twitter N-
Gram Corpus (Herdağdelen, 2013). For simplic-
ity, our language model uses add-one smooth-
ing, although we intend to explore more complex
smoothing techniques in future works. These fea-
tures were intended to serve as a baseline for se-
mantic relatedness-based features.

3.3 Semantic Relatedness Features

The results of Cattle and Ma (2016) suggest that
University of Southern Florida Free Association
Norm-based features are useful in humour recog-
nition. Given two words, A and B, the USF
Free Association Norms (USF) report the forward
strength, i.e. proportion of participants who, when
given word A, produce word B as their first reac-
tion (Nelson et al., 2004). The USF dataset was
represented as a graph where each node U referred
to a word in the vocabulary and each edge from
U to V had a weight proportional to the negative
log of the forward strength form word U to word
V. By representing the forward strengths as their

402



negative logs, finding the shortest path between
two nodes using Dijkstra’s Algorithm is equiva-
lent to finding the path with the maximal product
of forward strengths. Using this information we
can easily estimate the forward strength between
any two words in the USF vocabulary.

Word association is unidirectional; given the
word “beer” a participant might say “glass” but
given “glass” they might not say “beer” (Ma,
2013). Thus, we collect both USFforward, rep-
resenting how strongly the words in the hash-
tag prompt are associated with the words in the
tweet’s content, and USFbackward, representing
how strongly the tweet’s content is associated with
the hashtag. These can be roughly interpreted as
how easy a punchline is to guess given only the
setup and how easy a punchline is to understand
in context, respectively. Unlike Cattle and Ma
(2016), which used human annotations to limit
their scope to only punch words, we consider all
hashtag-word/tweet-word pairs. We record the
maximum, minimum, and average values for each
feature across all such pairs.

Since we expect tweets which are relatively un-
expected, i.e. low USFforward, but also relatively
easy to understand, i.e. high USFbackward, to be
deemed funnier, we also collect USFdifference, the
difference between the two values. USFdifference
is calculated both at word-level, e.g. USFword-maxdifference
refers to the maximal difference for a single word,
and tweet-level, e.g. USFtweet-maxdifference refers to the dif-
ference between USFmaxforward and USF

max
backward.

In addition to USF association-based features,
we also extract an identical set of features in the
same manner but using the association strengths
reported in the Edinburgh Associative Thesaurus
(EAT) (Kiss et al., 1973).

To test the effectiveness of association-based
features, we also collected the maximum, min-
imum, and average Word2Vec (Mikolov et al.,
2013) cosine similarities across all hashtag-
word/tweet-word pairs to serve as a semantic-
feature baseline. We used Google’s pre-trained
Word2Vec embeddings2.

3.4 Classifier

Features were extracted for each tweet following
the methodology presented in the previous sec-
tions. Next, for each hashtag, tweet pairs were
generated such that the two tweets had different

2https://code.google.com/archive/p/word2vec/

humour judgment labels, i.e. one of the tweets is
judged funnier according to the gold standard rat-
ings. Each tweet pair then became two ordered
training examples; one where the funnier of the
two tweets was on the left and one where the fun-
nier tweet was on the right, with appropriate train-
ing labels. For each training example, the left
tweet’s feature vector was concatenated with that
of the right tweet’s as well as the difference be-
tween the two. These training vectors were then
used to train a Random Forest Classifier using
scikit-learn3, a popular Python machine learning
library, using default settings and 100 estimators.

4 Results and Discussion

Feature selection experiments were performed us-
ing Training data for training and Trial data
as a validation set to identify the best per-
forming features. Using these features, we
trained a new classifier on a combination of
Training and Trial data and evaluated its per-
formance on Evaluation data. The results in
Table 1 show that the highest performing fea-
tures in the validation test were EATtweet-avgdifference,
USFmaxforward, EAT

word-avg
difference, USF

word-avg
difference, EAT

min
forward,

USFtweet-maxdifference , and EAT
min
backward. The results using

only these features are reported as Best Features.
We also evaluated the performance of two more
feature combinations: best features plus perplex-
ity and n-gram features, as Best Features+, and
best feature plus perplexity features only, as Best
Features+ (no n-gram).

Interestingly, although n-gram features on their
own performed no better than chance in both val-
idation and evaluation tests, their addition to the
Best Features resulted in a large 8% point gain
in validation tests compared to the same features
minus n-grams (p=0.02 for paired t-test on file-
level accuracies). However, their addition resulted
in a drop in performance in evaluation tests, al-
though this result was not statistically significant.
Considering the dataset contains under 13,000
unique tweets, this extreme variation in perfor-
mance might be due to n-gram features overfitting
on the small dataset. By comparison, Shahaf et al.
(2015) found n-grams alone offered a 55% accu-
racy on the similar task of selecting the funnier
of two cartoon captions but their dataset contained
over four times as many unique documents.

Another problem facing n-gram features is that
3http://scikit-learn.org/

403



Features Accuracy %
Trial Evaluation

ba
se

lin
e

n-grams 50.42 50.20
unigram perplexity 53.05 50.27
bigram perplexity 54.29 53.78
Word2Vec Sim 50.72 50.76

B
es

tF
ea

tu
re

s

EATtweet-avgdifference 57.40 46.54
USFmaxforward 55.15 48.33

EATword-avgdifference 54.80 46.18

USFword-avgdifference 54.80 51.63
EATminforward 53.84 47.29
USFtweet-maxdifference 52.44 50.58
EATminbackward 51.94 44.33

Best Features 53.42 52.40
Best Features+ 61.51 53.72
Best Features+ (no n-gram) 53.39 56.42

Table 1: Accuracy by feature on Trial and Evalu-
ation data

compared to cartoon captions, hashtag wars have
a higher incidence of novel word-forms, typically
in service of a pun, which occur only a few times
for a particular hashtag prompt and never again.
E.g. ”HELLMFAO. #SpookyBands @midnight”,
or ”Purrassic Park #CatBooks @Midnight”. Sim-
ple n-gram models, such as the one used in this
paper, are ill-equipped to deal with these types of
out-of-vocabulary words.

Compared to Word2Vec, association-based fea-
tures proved more discerning. One possible expla-
nation for this is that word association is a more
flexible relatedness measure than similarity. It is
hard to find examples of similar concepts which
are not also associated, but easy to find examples
of associated concepts which are not similar. E.g.
”red” and ”green” would be similar in that they
are both colours and associated in that they ap-
pear together at Christmas. However, ”green” and
”grass” are associated in that grass is green but the
two words are very different.

Another possible explanation is that word asso-
ciations are unidirectional while most similarity or
distance metrics are not. The fact that four of the
top seven best features are some variation of asso-
ciation difference seems to support our hypothesis
that the interplay between a joke’s unexpectedness
and its understandability serves as a useful indica-
tion of humour.

Cattle and Ma (2016) noted that their USF per-
formance was hurt by a lack of coverage. This
seems to be the case for us as well. Less than 65%

of tweets contained a valid USFforward strength,
with valid USFbackward and USFdifference strengths
appearing in only 70% and less than 60%, respec-
tively. By comparison, almost 95% of tweets con-
tained a valid Word2Vec similarity. EAT showed
slightly better coverage with valid EATforward and
EATbackward strengths each appearing in almost
75% of tweets and EATdifference appearing in just
under 70%. This may explain why EAT features
outperformed USF in the validation tests. This
was expected given that EAT contains twice as
many words as USF and four times as many edges.

In order to avoid using human annotations, such
as those in Cattle and Ma (2016), USF, EAT,
and Word2Vec features were calculated across all
hashtag-word/tweet-word pairs. Even though the
bag-of-words was heavily filtered by POS to leave
only words which carry more word-level seman-
tic meaning, this is a shotgun-like approach which
likely added noise to the data. This may explain
why only two of the top seven word association
features use min values. The punchline makes up
only a small part of the tweet and it is expected that
the remainder would not show any strong associ-
ations. Some kind of automatic punchline identi-
fication would help in this respect but may exac-
erbate the aforementioned coverage issue faced by
USF and EAT.

5 Conclusion and Future Work

Humour recognition in general is a very dif-
ficult task and humour recognition in hashtag
wars is no exception. The majority of features
tested performed only slightly better than chance
if better than chance at all. Our optimal result
was only a 56.42% accuracy on Evaluation data
and was obtained using only the features uni-
gram perplexity, bigram perplexity, EATtweet-avgdifference,
USFmaxforward, EAT

word-avg
difference, USF

word-avg
difference, EAT

min
forward,

USFtweet-maxdifference , and EAT
min
backward. Although our ac-

curacy is fairly low we believe semantic related-
ness features, and word association-based features
in particular, are worthy of further study.

Our system could be improved by using auto-
matic punchline detection to cut down on the noise
in our word association features. Furthermore, a
larger vocabulary or even the ability to automati-
cally infer association strength would increase the
usefulness of word association features. Finally, a
larger dataset may be needed to rule out the effi-
cacy of n-gram features.

404



References
Kim Binsted and Graeme Ritchie. 1994. An imple-

mented model of punning riddles. In Proceedings
of the Twelfth AAAI National Conference on Artifi-
cial Intelligence. AAAI Press, pages 633–638.

Andrew Cattle and Xiaojuan Ma. 2016. Effects of se-
mantic relatedness between setups and punchlines in
twitter hashtag games. PEOPLES 2016 page 70.

Rudi L Cilibrasi and Paul MB Vitanyi. 2007. The
google similarity distance. IEEE Transactions on
knowledge and data engineering 19(3).

Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the four-
teenth conference on computational natural lan-
guage learning. Association for Computational Lin-
guistics, pages 107–116.

Günes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research 22:457–479.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers-Volume 2. As-
sociation for Computational Linguistics, pages 42–
47.

Amaç Herdağdelen. 2013. Twitter n-gram corpus with
demographic metadata. Language resources and
evaluation 47(4):1127–1147.

George R Kiss, Christine Armstrong, Robert Milroy,
and James Piper. 1973. An associative thesaurus
of english and its computer analysis. The computer
and literary studies pages 153–165.

Igor Labutov and Hod Lipson. 2012. Humor as cir-
cuits in semantic networks. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers-Volume 2. Asso-
ciation for Computational Linguistics, pages 150–
155.

Xiaojuan Ma. 2013. Evocation: analyzing and prop-
agating a semantic link based on free word as-
sociation. Language resources and evaluation
47(3):819–837.

Rada Mihalcea and Stephen Pulman. 2007. Charac-
terizing humour: An exploration of features in hu-
morous texts. In International Conference on Intel-
ligent Text Processing and Computational Linguis-
tics. Springer, pages 337–347.

Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of the Conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 531–538.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The university of south florida free
association, rhyme, and word fragment norms. Be-
havior Research Methods, Instruments, & Comput-
ers 36(3):402–407.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. Asso-
ciation for Computational Linguistics.

Sasa Petrovic and David Matthews. 2013. Unsuper-
vised joke generation from big data. In ACL (2).
Citeseer, pages 228–232.

Peter Potash, Alexey Romanov, and Anna Rumshisky.
2017. Semeval-2017 task 6: #hashtagwars: Learn-
ing a sense of humor. In Proceedings of the
11th International Workshop on Semantic Evalu-
ation (SemEval-2017). Association for Computa-
tional Linguistics, Vancouver, Canada, pages 49–57.
http://www.aclweb.org/anthology/S17-2004.

Dragomir Radev, Amanda Stent, Joel Tetreault, Aa-
sish Pappu, Aikaterini Iliakopoulou, Agustin Chan-
freau, Paloma de Juan, Jordi Vallmitjana, Alejandro
Jaimes, Rahul Jha, and Robert Mankoff. 2016. Hu-
mor in collective discourse: Unsupervised funniness
detection in the new yorker cartoon caption contest.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Sara Goggi, Marko Gro-
belnik, Bente Maegaard, Joseph Mariani, Helene
Mazo, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016). European Language Resources
Association (ELRA), Paris, France.

Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &
Knowledge Engineering 74:1–12.

Graeme Ritchie, Ruli Manurung, Helen Pain, Annalu
Waller, Rolf Black, and Dave OMara. 2007. A prac-
tical application of computational humour. In Pro-
ceedings of the 4th International Joint Conference
on Computational Creativity. pages 91–98.

Dafna Shahaf, Eric Horvitz, and Robert Mankoff.
2015. Inside jokes: Identifying humorous car-
toon captions. In Proceedings of the 21th

405



ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. ACM, New
York, NY, USA, KDD ’15, pages 1065–1074.
https://doi.org/10.1145/2783258.2783388.

Oliviero Stock and Carlo Strapparava. 2003. Ha-
hacronym: Humorous agents for humorous
acronyms. Humor 16(3):297–314.

Julia M Taylor and Lawrence J Mazlack. 2004. Com-
putationally recognizing wordplay in jokes. In
Proceedings of the Cognitive Science Society. vol-
ume 26.

Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm-a great catchy name: Semi-supervised recog-
nition of sarcastic sentences in online product re-
views. In ICWSM. pages 162–169.

406


