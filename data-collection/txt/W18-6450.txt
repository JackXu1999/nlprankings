



















































Results of the WMT18 Metrics Shared Task: Both characters and embeddings achieve good performance


Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 671–688
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64077

Results of the WMT18 Metrics Shared Task

Qingsong Ma
Tencent-MIG

AI Evaluation & Test Lab
qingsong.mqs@gmail.com

Ondřej Bojar
Charles University

MFF ÚFAL
bojar@ufal.mff.cuni.cz

Yvette Graham
Dublin City University

ADAPT
graham.yvette@gmail.com

Abstract

This paper presents the results of the
WMT18 Metrics Shared Task. We
asked participants of this task to score
the outputs of the MT systems in-
volved in the WMT18 News Transla-
tion Task with automatic metrics. We
collected scores of 10 metrics and 8 re-
search groups. In addition to that, we
computed scores of 8 standard met-
rics (BLEU, SentBLEU, chrF, NIST,
WER, PER, TER and CDER) as base-
lines. The collected scores were eval-
uated in terms of system-level corre-
lation (how well each metric’s scores
correlate with WMT18 official man-
ual ranking of systems) and in terms
of segment-level correlation (how often
a metric agrees with humans in judging
the quality of a particular sentence rel-
ative to alternate outputs). This year,
we employ a single kind of manual eval-
uation: direct assessment (DA).

1 Introduction

Accurate machine translation (MT) evaluation
is important for measuring improvements in
system performance. Human evaluation can
be costly and time consuming, and it is not
always available for the language pair of in-
terest. Automatic metrics can be employed
as a substitute for human evaluation in such
cases, metrics that aim to measure improve-
ments to systems quickly and at no cost to
developers. In the usual set-up, an automatic
metric carries out a comparison of MT system
output translations and human-produced ref-
erence translations to produce a single overall

score for the system.1 Since there exists a large
number of possible approaches to producing
quality scores for translations, it is sensible to
carry out a meta-evaluation of metrics with
the aim to estimate their accuracy as a substi-
tute for human assessment of translation qual-
ity. The Metrics Shared Task2 of WMT annu-
ally evaluates the performance of automatic
machine translation metrics in their ability to
provide a substitute for human assessment of
translation quality.

Again, we keep the two main types of metric
evaluation unchanged from the previous years.
In system-level evaluation, each metric pro-
vides a quality score for the whole translated
test set (usually a set of documents, in fact).
In segment-level evaluation, a score is assigned
by a given metric to every individual sentence.

The underlying texts and MT systems come
from the News Translation Task (Bojar et al.,
2018, denoted as Findings 2018 in the follow-
ing). The texts were drawn from the news
domain and involve translations to/from Chi-
nese (zh), Czech (cs), German (de), Estonian
(et), Finnish (fi), Russian (ru), and Turkish
(tr), each paired with English, making a total
of 14 language pairs.

A single form of golden truth of translation
quality judgement is used this year:

• In Direct Assessment (DA) (Graham et
al., 2016), humans assess the quality of
a given MT output translation by com-
parison with a reference translation (as
opposed to the source and reference).
DA is the new standard used in WMT

1The availability of a reference translation is the key
difference between our task and MT quality estimation,
where no reference is assumed.

2http://www.statmt.org/wmt18/metrics-task.
html, starting with Koehn and Monz (2006) up to
Bojar et al. (2017)

671

https://doi.org/10.18653/v1/W18-64077


News Translation Task evaluation, requir-
ing only monolingual evaluators.

As in last year’s evaluation, the official
method of manual evaluation of MT outputs
is no longer “relative ranking” (RR, evaluat-
ing up to five system outputs on an annota-
tion screen relative to each other) as this was
changed in 2017 to DA. For system-level eval-
uation, we thus use the Pearson correlation
r of automatic metrics with DA scores. For
segment-level evaluation, we re-interpret DA
judgements as relative comparisons and use
Kendall’s τ as a substitute, see below for de-
tails and references.

Section 2 describes our datasets, i.e. the
sets of underlying sentences, system outputs,
human judgements of translation quality and
also participating metrics. Sections 3.1 and
3.2 then provide the results of system and
segment-level metric evaluation, respectively.
We discuss the results in Section 4.

2 Data
This year, we provided the task participants
with one test set along with reference trans-
lations and outputs of MT systems. Partici-
pants were free to choose which language pairs
they wanted to participate and whether they
reported system-level, segment-level scores or
both.

2.1 Test Sets
We use the following test set, i.e. a set of
source sentences and reference translations:

newstest2018 is the test set used in
WMT18 News Translation Task (see
Findings 2018), with approximately 3,000
sentences for each translation direction
(except Chinese and Estonian which have
3,981 and 2,000 sentences, resp.). new-
stest2018 includes a single reference trans-
lation for each direction.

2.2 Translation Systems
The results of the Metrics Task are likely af-
fected by the actual set of MT systems par-
ticipating in a given translation direction. For
instance, if all of the systems perform simi-
larly, it will be more difficult, even for hu-
mans, to distinguish between the quality of

translations. If the task includes a wide range
of systems of varying quality, however, or sys-
tems are quite different in nature, this could
in some way make the task easier for metrics,
with metrics that are more sensitive to certain
aspects of MT output performing better.

This year, the MT systems included in the
Metrics Task were:

News Task Systems are machine trans-
lation systems participating in the
WMT18 News Translation Task (see
Findings 2018).3

Hybrid Systems are created automatically
with the aim of providing a larger set
of systems against which to evaluate
metrics, as in Graham and Liu (2016).
Hybrid systems were created for new-
stest2018 by randomly selecting a pair of
MT systems from all systems taking part
in that language pair and producing a sin-
gle output document by randomly select-
ing sentences from either of the two sys-
tems. In short, we create 10K hybrid MT
systems for each language pair.

Excluding the hybrid systems, we ended up
with 149 systems across 14 language pairs.

2.3 Manual MT Quality Judgments
Direct Assessment (DA) was employed as the
“golden truth” to evaluate metrics again this
year. The details of this method of hu-
man evaluation is provided in two sections
for system-level evaluation (Section 2.3.1) and
segment-level evaluation (Section 2.3.2).

The DA manual judgements were provided
by MT researchers taking part in WMT tasks,
a number of in-house human evaluators at
Amazon and crowd-sourced workers on Ama-
zon Mechanical Turk.4 Only judgements
from workers who passed DA’s quality control
mechanism were included in the final datasets
used to compute system and segment-level
scores employed as a gold standard in the Met-
rics Task.

3One system for tr-en was unfortunately omitted
from the first run of human evaluation in the News
Translation Task and due to time constraints was sub-
sequently omitted from the Metrics Task evaluation,
Alibaba-Ensemble.

4https://www.mturk.com

672



2.3.1 System-level Manual Quality
Judgments

In the system-level evaluation, the goal is to
assess the quality of translation of an MT sys-
tem for the whole test set. Our manual scoring
method, DA, nevertheless proceeds sentence
by sentence, aggregating the final score as de-
scribed below.

Direct Assessment (DA) This year the
translation task employed monolingual di-
rect assessment (DA) of translation adequacy
(Graham et al., 2013; Graham et al., 2014a;
Graham et al., 2016). Since sufficient levels
of agreement in human assessment of trans-
lation quality are difficult to achieve, the DA
setup simplifies the task of translation assess-
ment (conventionally a bilingual task) into
a simpler monolingual assessment. In addi-
tion, DA avoids bias that has been problem-
atic in previous evaluations introduced by as-
sessment of several alternate translations on a
single screen, where scores for translations had
been unfairly penalized if often compared to
high quality translations (Bojar et al., 2011).
DA therefore employs assessment of individual
translations in isolation from other outputs.

Translation adequacy is structured as a
monolingual assessment of similarity of mean-
ing where the target language reference trans-
lation and the MT output are displayed to
the human assessor. Assessors rate a given
translation by how adequately it expresses the
meaning of the reference translation on an
analogue scale corresponding to an underlying
0-100 rating scale.5

Large numbers of DA human assessments
of translations for all 14 language pairs in-
cluded in the News Translation Task were col-
lected from researchers and from workers on
Amazon’s Mechanical Turk, via sets of 100-
translation hits to ensure sufficient repeat as-
sessments per worker, before application of
strict quality control measures to filter out as-
sessments from poor performers.

In order to iron out differences in scoring
strategies attributed to distinct human as-
sessors, human assessment scores for transla-
tions were standardized according to an indi-

5The only numbering displayed on the rating scale
are extreme points 0 and 100%, and three ticks indicate
the levels of 25, 50 and 75 %.

vidual judge’s overall mean and standard de-
viation score. Final scores for MT systems
were computed by firstly taking the average
of scores for individual translations in the test
set (since some were assessed more than once),
before combining all scores for translations at-
tributed to a given MT system into its overall
adequacy score. The gold standard for system-
level DA evaluation is thus what is denoted
“Ave z” in Findings 2018 (Bojar et al., 2018).

Finally, although it was necessary to apply
a sentence length restriction in WMT human
evaluation prior to the introduction of DA, the
simplified DA setup does not require restric-
tion of the evaluation in this respect and no
sentence length restriction was applied in DA
WMT18.

2.3.2 Segment-level Manual Quality
Judgments

Segment-level metrics have been evaluated
against DA annotations for the newstest2018
test set. This year, a standard segment-level
DA evaluation of metrics, where each trans-
lation is assessed a minimum of 15 times,
was unfortunately not possible due to insuf-
ficient number of judgements collected. DA
judgements were therefore converted to rela-
tive ranking judgements (daRR) to produce
results. This is the same strategy as carried
out for some out-of-English language pairs in
last year’s evaluation.

daRR When we have at least two DA scores
for translations of the same source input, it is
possible to convert those DA scores into a rel-
ative ranking judgement, if the difference in
DA scores allows conclusion that one transla-
tion is better than the other. In the following,
we will denote these re-interpreted DA judge-
ments as “daRR”, to distinguish it clearly
from the “RR” golden truth used in the past
years.

Since the analogue rating scale employed by
DA is marked at the 0-25-50-75-100 points,
the difference in DA scores we employ to
distinguish translations that are better/worse
than one another is 25 points. Note that
we rely on judgements collected from known-
reliable volunteers and crowd-sourced workers
who passed DA’s quality control mechanism.
Any inconsistency that could arise from re-

673



DA>1 Ave DA pairs daRR
cs-en 2,491 3.6 13,223 5,110
de-en 2,995 11.4 192,702 77,811
en-cs 1,586 4.9 15,311 5,413
en-de 2,150 5.3 47,041 19,711
en-et 1,035 13.6 90,755 32,202
en-fi 1,481 5.3 30,613 9,809
en-ru 2,954 6.2 54,260 22,181
en-tr 707 3.4 4,750 1,358
en-zh 3,915 6.5 86,286 28,602
et-en 2,000 11.2 118,066 56,721
fi-en 2,972 5.4 39,127 15,648
ru-en 2,916 4.9 31,361 10,404
tr-en 2,991 4.5 24,325 8,525
zh-en 3,952 7.2 97,474 33,357

Table 1: Number of judgements for DA con-
verted to daRR data; “DA>1” is the num-
ber of source input sentences in the manual
evaluation where at least two translations of
that same source input segment received a DA
judgement; “Ave” is the average number of
translations with at least one DA judgement
available for the same source input sentence;
“DA pairs” is the number of all possible pairs
of translations of the same source input result-
ing from “DA>1”; and “daRR” is the num-
ber of DA pairs with an absolute difference
in DA scores greater than the 25 percentage
point margin.

liance on DA judgements collected from low
quality crowd-sourcing, for example, is thus
prevented.

From the complete set of human assess-
ments collected for the News Translation Task,
all possible pairs of DA judgements attributed
to distinct translations of the same source were
converted into daRR better/worse judge-
ments. Distinct translations of the same
source input whose DA scores fell within 25
percentage points (which could have been
deemed equal quality) were omitted from the
evaluation of segment-level metrics. Conver-
sion of scores in this way produced a large set
of daRR judgements for all language pairs,
shown in Table 1 due to combinatorial ad-
vantage of extracting daRR judgements from
all possible pairs of translations of the same
source input.

Kendall’s Tau-like Formulation for
daRR We measure the quality of metrics’
segment-level scores against the daRR golden
truth using a Kendall’s Tau-like formulation,
which is an adaptation of the conventional
Kendall’s Tau coefficient. Since we do not
have a total order ranking of all translations
we use to evaluate metrics, it is not possible
to apply conventional Kendall’s Tau given
the current daRR human evaluation setup
(Graham et al., 2015). Vazquez-Alvarez and
Huckvale (2002) also note that a genuine
pairwise comparison is likely to lead to
more stable results for segment-level metric
evaluation.

Our Kendall’s Tau-like formulation, τ , is as
follows:

τ =
|Concordant| − |Discordant|
|Concordant|+ |Discordant| (1)

where Concordant is the set of all human com-
parisons for which a given metric suggests the
same order and Discordant is the set of all
human comparisons for which a given metric
disagrees. The formula is not specific with re-
spect to ties, i.e. cases where the annotation
says that the two outputs are equally good.

The way in which ties (both in human and
metric judgement) were incorporated in com-
puting Kendall τ has changed across the years
of WMT Metrics Tasks. Here we adopt the
version used in the last years’ WMT17 daRR
evaluation (but not earlier). For a detailed
discussion on other options, see also Macháček
and Bojar (2014).

Whether or not a given comparison of a pair
of distinct translations of the same source in-
put, s1 and s2, is counted as a concordant
(Conc) or disconcordant (Disc) pair is defined
by the following matrix:

Metric
s1 < s2 s1 = s2 s1 > s2

H
um

an s1 < s2 Conc Disc Disc
s1 = s2 − − −
s1 > s2 Disc Disc Conc

In the notation of Macháček and Bojar
(2014), this corresponds to the setup used in
WMT12 (with a different underlying method
of manual judgements, RR):

674



Metric
WMT12 < = >

H
um

an < 1 -1 -1
= X X X
> -1 -1 1

The key differences between the evaluation
used in WMT14–WMT16 and evaluation used
in WMT17 and WMT18 are (1) the move from
RR to daRR and (2) the treatment of ties.6 In
the years 2014-2016, ties in metrics scores were
not penalized. With the move to daRR, where
the quality of the two candidate translations
is deemed substantially different and no ties
in human judgements arise, it makes sense to
penalize ties in metrics’ predictions in order to
promote discerning metrics.

Note that the penalization of ties makes our
evaluation asymmetric, dependent on whether
the metric predicted the tie for a pair where
humans predicted < or >. It is now important
to interpret the meaning of the comparison
identically for humans and metrics. For error
metrics, we thus reverse the sign of the met-
ric score prior to the comparison with human
scores: higher scores have to indicate better
translation quality. In WMT18, we did this
for ITER and the original authors did this for
CharacTER.

To summarize, the WMT18 Metrics Task
for segment-level evaluation:

• excludes all human ties (this is already
implied by the construction of daRR
from DA judgements),

• counts metric’s ties as a Discordant pairs,

• ensures that error metrics are first con-
verted to the same orientation as the hu-
man judgements, i.e. higher score indi-
cating higher translation quality.

We employ bootstrap resampling (Koehn,
2004; Graham et al., 2014b) to estimate con-
fidence intervals for our Kendall’s Tau for-
mulation, and metrics with non-overlapping
95% confidence intervals are identified as hav-
ing statistically significant difference in perfor-
mance.

6Due to an error in the write-up for WMT17 (er-
rata to follow), this second change was not properly
reflected in the paper, only in the evaluation scripts.

2.4 Participants of the Metrics Shared
Task

Table 2 lists the participants of the WMT18
Shared Metrics Task, along with their metrics.
We have collected 10 metrics from a total of 8
research groups.

The following subsections provide a brief
summary of all the metrics that participated.
The list is concluded by our baseline metrics
in Section 2.4.9.

As in last year’s task, we asked participants
whose metrics are publicly available to pro-
vide links to where the code can be accessed.
Table 3 provides links for metrics that partic-
ipated in WMT18 that are publicly available
for download.

2.4.1 BEER
BEER (Stanojević and Sima’an, 2015) is a
trained evaluation metric with a linear model
that combines features sub-word feature indi-
cators (character n-grams) and global word or-
der features (skip bigrams) to get language ag-
nostic and fast to compute evaluation metric.
BEER has participated in previous years of
the evaluation task.

2.4.2 Blend
Blend incorporates existing metrics to form
an effective combined metric, employing SVM
regression for training and DA scores as the
gold standard. For to-English language pairs,
incorporated metrics include 25 lexical based
metrics and 4 other metrics. Since some lexi-
cal based metrics are simply different variants
of the same metric, there are only 9 kinds of
lexical based metrics, namely BLEU, NIST,
GTM, METEOR, ROUGE, Ol, WER, TER
and PER. 4 other metrics are CharacTER,
BEER, DPMF and ENTF.
Blend has participated in the Metrics Task

in WMT17. This year, Blend follows its
setup in WMT17, but enlarges the training
data since there are some data available in
WMT17. For to-English language pairs, there
are 9280 sentences as training data, while1620
sentences for English-Russian (en-ru). Experi-
ments show the performance of Blend can be
improved if the training data increases.
Blend is flexible to be applied to any lan-

guage pairs if incorporated metrics support the

675



Metric Seg-level Sys-level Hybrids Participant
BEER • ⊘ ⊘ ILLC – University of Amsterdam (Stanojević and Sima’an, 2015)

BLEND • ⊘ ⊘ Tencent-MIG-AI Evaluation & Test Lab (Ma et al., 2017)
CharacTer • • • RWTH Aachen University (Wang et al., 2016a)

ITER • • ⋆ Jadavpur University (Panja and Naskar, 2018)
meteor++ • ⊘ ⊘ Peking University (Guo et al., 2018)

RUSE • ⊘ ⊘ Tokyo Metropolitan University (Shimanaka et al., 2018)
UHH_TSKM • ⊘ ⊘ (Duma and Menzel, 2017)

YiSi-* • ⊘ ⊘ NRC (Lo, 2018)

Table 2: Participants of WMT18 Metrics Shared Task. “•” denotes that the metric took part
in (some of the language pairs) of the segment- and/or system-level evaluation and whether
hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied,
simply taking arithmetic average of segment-level scores. “⋆” indicates that the original ITER
system-level scores should be calculated as the micro-average of segment-level scores but we
calculate them as simple macro-averaged for the hybrid systems. See the ITER paper for more
details.

BEER https://github.com/stanojevic/beer
BLEND https://github.com/qingsongma/blend
CharacTer https://github.com/rwth-i6/CharacTER
RUSE https://github.com/Shi-ma/RUSE
YiSi-0, incl. -1 and -1_srl http://chikiu-jackie-lo.org/home/index.php/yisi
Baselines: http://github.com/moses-smt/mosesdecoder
BLEU, NIST scripts/generic/mteval-v13a.pl
CDER, PER, TER, WER mert/evaluator (“Moses scorer”)
sentBLEU mert/sentence-bleu

chrF, chrF+ https://github.com/m-popovic/chrF

Table 3: Metrics available for public download that participated in WMT18. Most of the
baseline metrics are available with Moses, relative paths are listed.

specific language pair and DA scores are avail-
able.

2.4.3 CharacTer
CharacTer (Wang et al., 2016b; Wang et
al., 2016a), identical to the 2016 setup, is
a character-level metric inspired by the com-
monly applied translation edit rate (TER). It
is defined as the minimum number of charac-
ter edits required to adjust a hypothesis, un-
til it completely matches the reference, nor-
malized by the length of the hypothesis sen-
tence. CharacTer calculates the character-
level edit distance while performing the shift
edit on word level. Unlike the strict match-
ing criterion in TER, a hypothesis word is
considered to match a reference word and
could be shifted, if the edit distance between
them is below a threshold value. The Lev-
enshtein distance between the reference and

the shifted hypothesis sequence is computed
on the character level. In addition, the lengths
of hypothesis sequences instead of reference
sequences are used for normalizing the edit
distance, which effectively counters the is-
sue that shorter translations normally achieve
lower TER.

Similarly to other character-level metrics,
CharacTer is applied to non-tokenized out-
puts and references, which also holds for this
year’s submission.

This year tokenization was carried out for
en-ru hypotheses and reference before calcu-
lating the scores, since this results in large im-
provements in terms of correlations. For other
language pairs a tokenizer was not used for
pre-processing. A python library was used for
calculating the Levenshtein distance, so that
the metric is now about 7 times faster than
before.

676



2.4.4 ITER
ITER (Panja and Naskar, 2018) is an im-
proved Translation Edit/Error Rate (TER)
metric. In addition to the basic edit operations
in TER (insertion, deletion, substitution and
shift), ITER also allows stem matching and
uses optimizable edit costs and better normal-
ization.

Note that for segment-level evaluation, we
reverse the sign of the score, so that better
translations get higher scores. For system-
level confidence, we calculate the system-level
scores for hybrids systems slightly differently
than the original ITER definition would re-
quire. We use the unweighted arithmetic av-
erage of segment-level scores (macro-average)
whereas ITER would use the micro-average.

2.4.5 meteor++
meteor++ (Guo et al., 2018) is metric
based on Meteor (Denkowski and Lavie, 2014),
adding explicing treatment of “copy-words”,
i.e. words that are likely to be preserved across
all paraphrases of a sentence in a given lan-
guage.

2.4.6 RUSE
RUSE (Shimanaka et al., 2018) is a percep-
tron regressor based on three types of sentence
embeddings: Infersent, Quick-Thought and
Universal Sentence Encoder, designed with the
aim to utilize global sentence information that
cannot be captured by local features based on
character or word n-grams. The sentence em-
beddings come from pre-trained models and
the regression itself is trained on past manual
judgements in WMT shared tasks.

2.4.7 UHH_TSKM
UHH_TSKM (Duma and Menzel, 2017) is a
non-trained metric utilizing kernel functions,
i.e. methods for efficient calculation of over-
lap of substructures between the candidate
and the reference translations. The metric
uses both sequence kernels, applied on the to-
kenized input data, together with tree ker-
nels, that exploit the syntactic structure of
the sentences. Optionally, the match can also
be performed for the candidate and a pseudo-
reference (i.e. a translation by another MT
system) or for the source sentence and the

candidate back-translated into the source lan-
guage.

2.4.8 YiSi-0, YiSi-1 and YiSi-1_srl
The YiSi metrics (Lo, 2018) are recently pro-
posed semantic MT evaluation metrics in-
spired by MEANT_2.0 (?). Specifically,
YiSi-1 is identical to MEANT_2.0-nosrl
which featured in the WMT17 Metrics Task.
YiSi-1 also successfully served in the par-

allel corpus filtering task. Some details are
provided in the system description paper (?).
YiSi-1 measures the relative lexical seman-

tic similarity (weighted word embeddings co-
sine similarity aggregated into n-grams simi-
larity) of the candidate and reference trans-
lations, optionally taking the shallow seman-
tic structure (“srl”) into account. YiSi-0 is
a degenerate resource-free version using the
longest common character substring, instead
of word embeddings cosine similarity, to mea-
sure the word similarity of the candidate and
reference translations.

2.4.9 Baseline Metrics
As mentioned by Bojar et al. (2016), Metrics
Task occasionally suffers from “loss of knowl-
edge” when successful metrics participate only
in one year.

We attempt to avoid this by regularly eval-
uating also a range of “baseline metrics”:

• Mteval. The metrics BLEU (Pap-
ineni et al., 2002) and NIST (Dod-
dington, 2002) were computed using
the script mteval-v13a.pl7 that is
used in the OpenMT Evaluation Cam-
paign and includes its own tokeniza-
tion. We run mteval with the flag
--international-tokenization since
it performs slightly better (Macháček and
Bojar, 2013).

• Moses Scorer. The metrics TER
(Snover et al., 2006), WER, PER and
CDER (Leusch et al., 2006) were pro-
duced by the Moses scorer, which is used
in Moses model optimization. To tokenize
the sentences, we used the standard tok-
enizer script as available in Moses toolkit.
When tokenizing, we also convert all out-
puts to lowercase.

7http://www.itl.nist.gov/iad/mig/tools/

677



Since Moses scorer is versioned on Github,
we strongly encourage authors of high-
performing metrics to add them to Moses
scorer, as this will ensure that their metric
can be easily included in future tasks.

• SentBLEU. The metric sentBLEU is
computed using the script sentence-bleu,
a part of the Moses toolkit. It is a
smoothed version of BLEU that corre-
lates better with human judgements for
segment-level. Standard Moses tokenizer
is used for tokenization.

• chrF The metrics chrF and chrF+
(Popović, 2015; Popović, 2017) are com-
puted using their original Python imple-
mentation.

We run chrF++.py with the parameters
-nw 0 -b 3 to obtain the chrF score and
with -nw 0 -b 1 to obtain the chrF+
score. Note that chrF intentionally re-
moves all spaces before matching the n-
grams, detokenizing the segments but also
concatenating words.

We originally planned to use the chrF
implementation which was recently made
available in Moses Scorer but it mishan-
dles Unicode characters for now.

The baselines serve in system and segment-
level evaluations as customary: BLEU, TER,
WER, PER and CDER for system-level only;
sentBLEU for segment-level only and chrF
for both.

Chinese word segmentation is unfortu-
nately not supported by the tokenization
scripts mentioned above. For scoring Chi-
nese with baseline metrics, we thus pre-
processed MT outputs and reference transla-
tions with the script tokenizeChinese.py8 by
Shujian Huang, which separates Chinese char-
acters from each other and also from non-
Chinese parts.

For computing system-level and segment-
level scores, the same scripts were employed
as in last year’s Metrics Task as well as for
generation of hybrid systems from the given
hybrid descriptions.

8http://hdl.handle.net/11346/WMT17-TVXH

3 Results

We discuss system-level results for news task
systems in Section 3.1. The segment-level re-
sults are in Section 3.2.

3.1 System-Level Results
As in previous years, we employ the Pearson
correlation (r) as the main evaluation measure
for system-level metrics. The Pearson correla-
tion is as follows:

r =

∑n
i=1(Hi −H)(Mi −M)√∑n

i=1(Hi −H)2
√∑n

i=1(Mi −M)2
(2)

where Hi are human assessment scores of all
systems in a given translation direction, Mi
are the corresponding scores as predicted by a
given metric. H and M are their means re-
spectively.

Since some metrics, such as BLEU, for ex-
ample, aim to achieve a strong positive cor-
relation with human assessment, while error
metrics, such as TER aim for a strong neg-
ative correlation, after computation of r for
metrics, we compare metrics via the absolute
value of a given metric’s correlation with hu-
man assessment.

Table 4 provides the system-level corre-
lations of metrics evaluating translation of
newstest2018 into English while Table 5 pro-
vides the same for out-of-English language
pairs. The underlying texts are part of
the WMT18 News Translation test set (new-
stest2018) and the underlying MT systems are
all MT systems participating in the WMT18
News Translation Task with the exception of a
single tr-en system not included in the initial
human evaluation run.

As recommended by Graham and Bald-
win (2014), we employ Williams significance
test (Williams, 1959) to identify differences
in correlation that are statistically significant.
Williams test is a test of significance of a dif-
ference in dependent correlations and there-
fore suitable for evaluation of metrics. Corre-
lations not significantly outperformed by any
other metric for the given language pair are
highlighted in bold in Tables 4 and 5.

Since pairwise comparisons of metrics may
be also of interest, e.g. to learn which metrics

678



cs-en de-en et-en fi-en ru-en tr-en zh-en
n 5 16 14 9 8 5 14
Correlation |r| |r| |r| |r| |r| |r| |r|

BEER 0.958 0.994 0.985 0.991 0.982 0.870 0.976
BLEND 0.973 0.991 0.985 0.994 0.993 0.801 0.976
BLEU 0.970 0.971 0.986 0.973 0.979 0.657 0.978
CDER 0.972 0.980 0.990 0.984 0.980 0.664 0.982
CharacTER 0.970 0.993 0.979 0.989 0.991 0.782 0.950
ITER 0.975 0.990 0.975 0.996 0.937 0.861 0.980
meteor++ 0.945 0.991 0.978 0.971 0.995 0.864 0.962
NIST 0.954 0.984 0.983 0.975 0.973 0.970 0.968
PER 0.970 0.985 0.983 0.993 0.967 0.159 0.931
RUSE 0.981 0.997 0.990 0.991 0.988 0.853 0.981
TER 0.950 0.970 0.990 0.968 0.970 0.533 0.975
UHH_TSKM 0.952 0.980 0.989 0.982 0.980 0.547 0.981
WER 0.951 0.961 0.991 0.961 0.968 0.041 0.975
YiSi-0 0.956 0.994 0.975 0.978 0.988 0.954 0.957
YiSi-1 0.950 0.992 0.979 0.973 0.991 0.958 0.951
YiSi-1_srl 0.965 0.995 0.981 0.977 0.992 0.869 0.962

newstest2018

Table 4: Absolute Pearson correlation of to-English system-level metrics with DA human as-
sessment in newstest2018; correlations of metrics not significantly outperformed by any other
for that language pair are highlighted in bold; ensemble metrics are highlighted in gray.

en-cs en-de en-et en-fi en-ru en-tr en-zh
n 5 16 14 12 9 8 14
Correlation |r| |r| |r| |r| |r| |r| |r|

BEER 0.992 0.991 0.980 0.961 0.988 0.965 0.928
BLEND − − − − 0.988 − −
BLEU 0.995 0.981 0.975 0.962 0.983 0.826 0.947
CDER 0.997 0.986 0.984 0.964 0.984 0.861 0.961
CharacTER 0.993 0.989 0.956 0.974 0.983 0.833 0.983
ITER 0.915 0.984 0.981 0.973 0.975 0.865 −
NIST 0.999 0.986 0.983 0.949 0.990 0.902 0.950
PER 0.991 0.981 0.958 0.906 0.988 0.859 0.964
TER 0.997 0.988 0.981 0.942 0.987 0.867 0.963
WER 0.997 0.986 0.981 0.945 0.985 0.853 0.957
YiSi-0 0.973 0.985 0.968 0.944 0.990 0.990 0.957
YiSi-1 0.987 0.985 0.979 0.940 0.992 0.976 0.963
YiSi-1_srl − 0.990 − − − − 0.952

newstest2018

Table 5: Absolute Pearson correlation of out-of-English system-level metrics with DA human
assessment in newstest2018; correlations of metrics not significantly outperformed by any other
for that language pair are highlighted in bold; ensemble metrics are highlighted in gray.

679



cs-en de-en et-en

R
U

S
E

IT
E

R
B

LE
N

D
C

D
E

R
C

ha
ra

cT
E

R
B

LE
U

P
E

R
Y

iS
i.1

_s
rl

B
E

E
R

Y
iS

i.0
N

IS
T

U
H

H
_T

S
K

M
W

E
R

Y
iS

i.1
T

E
R

m
et

eo
r..

meteor..
TER
YiSi.1
WER
UHH_TSKM
NIST
YiSi.0
BEER
YiSi.1_srl
PER
BLEU
CharacTER
CDER
BLEND
ITER
RUSE

R
U

S
E

Y
iS

i.1
_s

rl
Y

iS
i.0

B
E

E
R

C
ha

ra
cT

E
R

Y
iS

i.1
B

LE
N

D
m

et
eo

r..
IT

E
R

P
E

R
N

IS
T

C
D

E
R

U
H

H
_T

S
K

M
B

LE
U

T
E

R
W

E
R

WER
TER
BLEU
UHH_TSKM
CDER
NIST
PER
ITER
meteor..
BLEND
YiSi.1
CharacTER
BEER
YiSi.0
YiSi.1_srl
RUSE

W
E

R
T

E
R

R
U

S
E

C
D

E
R

U
H

H
_T

S
K

M
B

LE
U

B
E

E
R

B
LE

N
D

N
IS

T
P

E
R

Y
iS

i.1
_s

rl
C

ha
ra

cT
E

R
Y

iS
i.1

m
et

eo
r..

IT
E

R
Y

iS
i.0

YiSi.0
ITER
meteor..
YiSi.1
CharacTER
YiSi.1_srl
PER
NIST
BLEND
BEER
BLEU
UHH_TSKM
CDER
RUSE
TER
WER

fi-en ru-en tr-en

IT
E

R
B

LE
N

D
P

E
R

R
U

S
E

B
E

E
R

C
ha

ra
cT

E
R

C
D

E
R

U
H

H
_T

S
K

M
Y

iS
i.0

Y
iS

i.1
_s

rl
N

IS
T

Y
iS

i.1
B

LE
U

m
et

eo
r..

T
E

R
W

E
R

WER
TER
meteor..
BLEU
YiSi.1
NIST
YiSi.1_srl
YiSi.0
UHH_TSKM
CDER
CharacTER
BEER
RUSE
PER
BLEND
ITER

m
et

eo
r..

B
LE

N
D

Y
iS

i.1
_s

rl
C

ha
ra

cT
E

R
Y

iS
i.1

R
U

S
E

Y
iS

i.0
B

E
E

R
C

D
E

R
U

H
H

_T
S

K
M

B
LE

U
N

IS
T

T
E

R
W

E
R

P
E

R
IT

E
R

ITER
PER
WER
TER
NIST
BLEU
UHH_TSKM
CDER
BEER
YiSi.0
RUSE
YiSi.1
CharacTER
YiSi.1_srl
BLEND
meteor..

N
IS

T
Y

iS
i.1

Y
iS

i.0
B

E
E

R
Y

iS
i.1

_s
rl

m
et

eo
r..

IT
E

R
R

U
S

E
B

LE
N

D
C

ha
ra

cT
E

R
C

D
E

R
B

LE
U

U
H

H
_T

S
K

M
T

E
R

P
E

R
W

E
R

WER
PER
TER
UHH_TSKM
BLEU
CDER
CharacTER
BLEND
RUSE
ITER
meteor..
YiSi.1_srl
BEER
YiSi.0
YiSi.1
NIST

zh-en en-cs

C
D

E
R

R
U

S
E

U
H

H
_T

S
K

M
IT

E
R

B
LE

U
B

LE
N

D
B

E
E

R
T

E
R

W
E

R
N

IS
T

Y
iS

i.1
_s

rl
m

et
eo

r..
Y

iS
i.0

Y
iS

i.1
C

ha
ra

cT
E

R
P

E
R

PER
CharacTER
YiSi.1
YiSi.0
meteor..
YiSi.1_srl
NIST
WER
TER
BEER
BLEND
BLEU
ITER
UHH_TSKM
RUSE
CDER

N
IS

T

T
E

R

W
E

R

C
D

E
R

B
LE

U

C
ha

ra
cT

E
R

B
E

E
R

P
E

R

Y
iS

i.1

Y
iS

i.0

IT
E

R

ITER

YiSi.0

YiSi.1

PER

BEER

CharacTER

BLEU

CDER

WER

TER

NIST

en-de en-et en-fi

B
E

E
R

Y
iS

i.1
_s

rl

C
ha

ra
cT

E
R

T
E

R

C
D

E
R

N
IS

T

W
E

R

Y
iS

i.0

Y
iS

i.1

IT
E

R

B
LE

U

P
E

R

PER

BLEU

ITER

YiSi.1

YiSi.0

WER

NIST

CDER

TER

CharacTER

YiSi.1_srl

BEER

C
D

E
R

N
IS

T

IT
E

R

W
E

R

T
E

R

B
E

E
R

Y
iS

i.1

B
LE

U

Y
iS

i.0

P
E

R

C
ha

ra
cT

E
R

CharacTER

PER

YiSi.0

BLEU

YiSi.1

BEER

TER

WER

ITER

NIST

CDER

C
ha

ra
cT

E
R

IT
E

R

C
D

E
R

B
LE

U

B
E

E
R

N
IS

T

W
E

R

Y
iS

i.0

T
E

R

Y
iS

i.1

P
E

R

PER

YiSi.1

TER

YiSi.0

WER

NIST

BEER

BLEU

CDER

ITER

CharacTER

en-ru en-tr en-zh

Y
iS

i.1

N
IS

T

Y
iS

i.0

B
E

E
R

B
LE

N
D

P
E

R

T
E

R

W
E

R

C
D

E
R

C
ha

ra
cT

E
R

B
LE

U

IT
E

R

ITER

BLEU

CharacTER

CDER

WER

TER

PER

BLEND

BEER

YiSi.0

NIST

YiSi.1

Y
iS

i.0

Y
iS

i.1

B
E

E
R

N
IS

T

T
E

R

IT
E

R

C
D

E
R

P
E

R

W
E

R

C
ha

ra
cT

E
R

B
LE

U

BLEU

CharacTER

WER

PER

CDER

ITER

TER

NIST

BEER

YiSi.1

YiSi.0

C
ha

ra
cT

E
R

P
E

R

T
E

R

Y
iS

i.1

C
D

E
R

W
E

R

Y
iS

i.0

Y
iS

i.1
_s

rl

N
IS

T

B
LE

U

B
E

E
R

BEER

BLEU

NIST

YiSi.1_srl

YiSi.0

WER

CDER

YiSi.1

TER

PER

CharacTER

Figure 1: System-level metric significance test results for DA human assessment in newstest2018;
green cells denote a statistically significant increase in correlation with human assessment for
the metric in a given row over the metric in a given column according to Williams test.

680



cs-en de-en et-en fi-en ru-en tr-en zh-en
n 10K 10K 10K 10K 10K 10K 10K
Correlation |r| |r| |r| |r| |r| |r| |r|

BEER 0.9497 0.9927 0.9831 0.9824 0.9755 0.7234 0.9677
BLEND 0.9646 0.9904 0.9820 0.9853 0.9865 0.7243 0.9686
BLEU 0.9557 0.9690 0.9812 0.9618 0.9719 0.5862 0.9684
CDER 0.9642 0.9797 0.9876 0.9764 0.9739 0.5767 0.9733
CharacTER 0.9595 0.9919 0.9754 0.9791 0.9841 0.6798 0.9424
ITER 0.9656 0.9904 0.9746 0.9885 0.9429 0.7420 0.9780
meteor++ 0.9367 0.9898 0.9753 0.9621 0.9892 0.7871 0.9541
NIST 0.9419 0.9816 0.9804 0.9655 0.9650 0.8622 0.9589
PER 0.9369 0.9820 0.9782 0.9834 0.9550 0.0433 0.9233
RUSE 0.9736 0.9959 0.9879 0.9829 0.9820 0.7796 0.9734
TER 0.9419 0.9699 0.9882 0.9599 0.9635 0.4495 0.9670
UHH_TSKM 0.9429 0.9794 0.9869 0.9738 0.9734 0.4433 0.9717
WER 0.9420 0.9612 0.9892 0.9534 0.9618 0.0720 0.9667
YiSi-0 0.9465 0.9925 0.9719 0.9694 0.9817 0.8629 0.9495
YiSi-1 0.9425 0.9909 0.9758 0.9641 0.9846 0.8810 0.9429
YiSi-1_srl 0.9565 0.9940 0.9783 0.9682 0.9860 0.7850 0.9540

newstest2018 Hybrids

Table 6: Absolute Pearson correlation of to-English system-level metrics with DA human assess-
ment for 10K hybrid super-sampled systems in newstest2018; ensemble metrics are highlighted
in gray.

en-cs en-de en-et en-fi en-ru en-tr en-zh
n 10K 10K 10K 10K 10K 10K 10K
Correlation |r| |r| |r| |r| |r| |r| |r|

BEER 0.9903 0.9891 0.9775 0.9587 0.9864 0.9327 0.9251
BLEND − − − − 0.9861 − −
BLEU 0.9931 0.9774 0.9706 0.9582 0.9767 0.7963 0.9414
CDER 0.9949 0.9842 0.9809 0.9605 0.9821 0.8322 0.9564
CharacTER 0.9902 0.9862 0.9495 0.9627 0.9814 0.7752 0.9784
ITER 0.8649 0.9778 0.9817 0.9664 0.9650 0.8724 −
NIST 0.9967 0.9839 0.9797 0.9436 0.9877 0.8703 0.9442
PER 0.9865 0.9787 0.9545 0.9044 0.9862 0.8289 0.9500
TER 0.9948 0.9861 0.9770 0.9391 0.9845 0.8373 0.9591
WER 0.9944 0.9842 0.9772 0.9418 0.9829 0.8239 0.9537
YiSi-0 0.9713 0.9829 0.9648 0.9422 0.9879 0.9530 0.9513
YiSi-1 0.9851 0.9826 0.9761 0.9384 0.9893 0.9418 0.9572
YiSi-1_srl − 0.9881 − − − − 0.9479

newstest2018 Hybrids

Table 7: Absolute Pearson correlation of out-of-English system-level metrics with DA human
assessment for 10K hybrid super-sampled systems in newstest2018; ensemble metrics are high-
lighted in gray.

681



cs-en de-en et-en

R
U

S
E

IT
E

R
B

LE
N

D
C

D
E

R
C

ha
ra

cT
E

R
Y

iS
i.1

_s
rl

B
LE

U
B

E
E

R
Y

iS
i.0

U
H

H
_T

S
K

M
Y

iS
i.1

W
E

R
T

E
R

N
IS

T
P

E
R

m
et

eo
r..

meteor..
PER
NIST
TER
WER
YiSi.1
UHH_TSKM
YiSi.0
BEER
BLEU
YiSi.1_srl
CharacTER
CDER
BLEND
ITER
RUSE

R
U

S
E

Y
iS

i.1
_s

rl
B

E
E

R
Y

iS
i.0

C
ha

ra
cT

E
R

Y
iS

i.1
IT

E
R

B
LE

N
D

m
et

eo
r..

P
E

R
N

IS
T

C
D

E
R

U
H

H
_T

S
K

M
T

E
R

B
LE

U
W

E
R

WER
BLEU
TER
UHH_TSKM
CDER
NIST
PER
meteor..
BLEND
ITER
YiSi.1
CharacTER
YiSi.0
BEER
YiSi.1_srl
RUSE

W
E

R
T

E
R

R
U

S
E

C
D

E
R

U
H

H
_T

S
K

M
B

E
E

R
B

LE
N

D
B

LE
U

N
IS

T
Y

iS
i.1

_s
rl

P
E

R
Y

iS
i.1

C
ha

ra
cT

E
R

m
et

eo
r..

IT
E

R
Y

iS
i.0

YiSi.0
ITER
meteor..
CharacTER
YiSi.1
PER
YiSi.1_srl
NIST
BLEU
BLEND
BEER
UHH_TSKM
CDER
RUSE
TER
WER

fi-en ru-en tr-en

IT
E

R
B

LE
N

D
P

E
R

R
U

S
E

B
E

E
R

C
ha

ra
cT

E
R

C
D

E
R

U
H

H
_T

S
K

M
Y

iS
i.0

Y
iS

i.1
_s

rl
N

IS
T

Y
iS

i.1
m

et
eo

r..
B

LE
U

T
E

R
W

E
R

WER
TER
BLEU
meteor..
YiSi.1
NIST
YiSi.1_srl
YiSi.0
UHH_TSKM
CDER
CharacTER
BEER
RUSE
PER
BLEND
ITER

m
et

eo
r..

B
LE

N
D

Y
iS

i.1
_s

rl
Y

iS
i.1

C
ha

ra
cT

E
R

R
U

S
E

Y
iS

i.0
B

E
E

R
C

D
E

R
U

H
H

_T
S

K
M

B
LE

U
N

IS
T

T
E

R
W

E
R

P
E

R
IT

E
R

ITER
PER
WER
TER
NIST
BLEU
UHH_TSKM
CDER
BEER
YiSi.0
RUSE
CharacTER
YiSi.1
YiSi.1_srl
BLEND
meteor..

Y
iS

i.1
Y

iS
i.0

N
IS

T
m

et
eo

r..
Y

iS
i.1

_s
rl

R
U

S
E

IT
E

R
B

LE
N

D
B

E
E

R
C

ha
ra

cT
E

R
B

LE
U

C
D

E
R

T
E

R
U

H
H

_T
S

K
M

W
E

R
P

E
R

PER
WER
UHH_TSKM
TER
CDER
BLEU
CharacTER
BEER
BLEND
ITER
RUSE
YiSi.1_srl
meteor..
NIST
YiSi.0
YiSi.1

zh-en en-cs

IT
E

R
R

U
S

E
C

D
E

R
U

H
H

_T
S

K
M

B
LE

N
D

B
LE

U
B

E
E

R
T

E
R

W
E

R
N

IS
T

m
et

eo
r..

Y
iS

i.1
_s

rl
Y

iS
i.0

Y
iS

i.1
C

ha
ra

cT
E

R
P

E
R

PER
CharacTER
YiSi.1
YiSi.0
YiSi.1_srl
meteor..
NIST
WER
TER
BEER
BLEU
BLEND
UHH_TSKM
CDER
RUSE
ITER

N
IS

T

C
D

E
R

T
E

R

W
E

R

B
LE

U

B
E

E
R

C
ha

ra
cT

E
R

P
E

R

Y
iS

i.1

Y
iS

i.0

IT
E

R

ITER

YiSi.0

YiSi.1

PER

CharacTER

BEER

BLEU

WER

TER

CDER

NIST

en-de en-et en-fi

B
E

E
R

Y
iS

i.1
_s

rl

C
ha

ra
cT

E
R

T
E

R

W
E

R

C
D

E
R

N
IS

T

Y
iS

i.0

Y
iS

i.1

P
E

R

IT
E

R

B
LE

U

BLEU

ITER

PER

YiSi.1

YiSi.0

NIST

CDER

WER

TER

CharacTER

YiSi.1_srl

BEER

IT
E

R

C
D

E
R

N
IS

T

B
E

E
R

W
E

R

T
E

R

Y
iS

i.1

B
LE

U

Y
iS

i.0

P
E

R

C
ha

ra
cT

E
R

CharacTER

PER

YiSi.0

BLEU

YiSi.1

TER

WER

BEER

NIST

CDER

ITER

IT
E

R

C
ha

ra
cT

E
R

C
D

E
R

B
E

E
R

B
LE

U

N
IS

T

Y
iS

i.0

W
E

R

T
E

R

Y
iS

i.1

P
E

R

PER

YiSi.1

TER

WER

YiSi.0

NIST

BLEU

BEER

CDER

CharacTER

ITER

en-ru en-tr en-zh

Y
iS

i.1

Y
iS

i.0

N
IS

T

B
E

E
R

P
E

R

B
LE

N
D

T
E

R

W
E

R

C
D

E
R

C
ha

ra
cT

E
R

B
LE

U

IT
E

R

ITER

BLEU

CharacTER

CDER

WER

TER

BLEND

PER

BEER

NIST

YiSi.0

YiSi.1

Y
iS

i.0

Y
iS

i.1

B
E

E
R

IT
E

R

N
IS

T

T
E

R

C
D

E
R

P
E

R

W
E

R

B
LE

U

C
ha

ra
cT

E
R

CharacTER

BLEU

WER

PER

CDER

TER

NIST

ITER

BEER

YiSi.1

YiSi.0

C
ha

ra
cT

E
R

T
E

R

Y
iS

i.1

C
D

E
R

W
E

R

Y
iS

i.0

P
E

R

Y
iS

i.1
_s

rl

N
IS

T

B
LE

U

B
E

E
R

BEER

BLEU

NIST

YiSi.1_srl

PER

YiSi.0

WER

CDER

YiSi.1

TER

CharacTER

Figure 2: System-level metric significance test results for 10K hybrid systems (DA human eval-
uation) from newstest2018; green cells denote a statistically significant increase in correlation
with human assessment for the metric in a given row over the metric in a given column according
to Williams test.

682



significantly outperform the most widely em-
ployed metric BLEU, we include significance
test results for every competing pair of metrics
including our baseline metrics in Figure 1.

The sample of systems we employ to evalu-
ate metrics is often small, as few as five MT
systems for cs-en, for example. This can lead
to inconclusive results, as identification of sig-
nificant differences in correlations of metrics
is unlikely at such a small sample size. Fur-
thermore, Williams test takes into account the
correlation between each pair of metrics, in ad-
dition to the correlation between the metric
scores themselves, and this latter correlation
increases the likelihood of a significant differ-
ence being identified.

To strenghten the conclusions of our evalu-
ation, we include significance test results for
large hybrid-super-samples of systems (Gra-
ham and Liu, 2016). 10K hybrid systems were
created per language pair, with correspond-
ing DA human assessment scores by sam-
pling pairs of systems from WMT18 News
Translation Task, creating hybrid systems by
randomly selecting each candidate translation
from one of the two selected systems. Sim-
ilar to last year, not all metrics participat-
ing in the system-level evaluation submitted
metric scores for the large set of hybrid sys-
tems. Fortunately, taking a simple average
of segment-level scores is the proper aggrega-
tion method for almost all metrics this year, so
where needed, we provided scores for hybrids
ourselves, see Table 2.

Correlations of metric scores with human as-
sessment of the large set of hybrid systems are
shown in Tables 6 and 7, where again metrics
not significantly outperformed by any other
are highlighted in bold. Figure 2 then pro-
vides significance test results for hybrid super-
sampled correlations for all pairs of competing
metrics for a given language pair.

3.2 Segment-Level Results
Segment-level evaluation relies on the manual
judgements collected in the News Translation
Task evaluation. This year, we were unable to
follow the methodology outlined in Graham
et al. (2015) for evaluation of segment-level
metrics because the sampling of sentences did
not provide sufficient number of assessments
of the same segment. We therefore convert

pairs of DA scores for competing translations
to daRR better/worse preferences and employ
a Kendall’s Tau formulation as described in
Section 2.3.2.

Results of the segment-level human evalua-
tion for translations sampled from the News
Translation Task are shown in Tables 8 and
9, where metric correlations not significantly
outperformed by any other metric are high-
lighted in bold. Head-to-head significance test
results for differences in metric performance
are included in Figure 3.

4 Discussion

4.1 Obtaining Human Judgements

Human data was collected in the usual way,
a portion via crowd-sourcing and the remain-
ing from researchers who mainly committed
their time contribution to the manual evalua-
tion as they had submitted a system in that
language pair. Evaluation of translations em-
ployed the DA set-up and it again successfully
acquired sufficient judgments to evaluate sys-
tems. As in the previous years, hybrid super-
sampling proved very effective and allowed to
obtain conclusive results of system-level evalu-
ation even for language pairs where as few as 5
MT systems participated. We should however
note that hybrid systems are constructed by
randomly mixing sentences coming from dif-
ferent MT systems. As soon as document-
level evaluation becomes relevant (which we
anticipate in the next evaluation campaign al-
ready), this style of hybridization is suscep-
tible to breaking cross-sentence references in
MT outputs and may no longer be applicable.

In the case of segment-level evaluation, the
optimal human evaluation data was unfor-
tunately not available due to resource con-
straints. Conversion of document-level data
held as a substitute for segment-level DA
scores. These scores are however not opti-
mal for evaluation of segment-level metrics
and we would like to return to DA’s stan-
dard segment-level evaluation in future, where
a minimum of 15 human judgments of transla-
tion quality are collected per translation and
combined to get highly accurate scores for
translations.

683



cs-en de-en et-en fi-en ru-en tr-en zh-en
Human Evaluation daRR daRR daRR daRR daRR daRR daRR
n 5,110 77,811 56,721 15,648 10,404 8,525 33,357
Correlation τ τ τ τ τ τ τ

BEER 0.295 0.481 0.341 0.232 0.288 0.229 0.214
BLEND 0.322 0.492 0.354 0.226 0.290 0.232 0.217
CharacTER 0.256 0.450 0.286 0.185 0.244 0.172 0.202
ITER 0.198 0.396 0.235 0.128 0.139 −0.029 0.144
meteor++ 0.270 0.457 0.329 0.207 0.253 0.204 0.179
RUSE 0.347 0.498 0.368 0.273 0.311 0.259 0.218
sentBLEU 0.233 0.415 0.285 0.154 0.228 0.145 0.178
UHH_TSKM 0.274 0.436 0.300 0.168 0.235 0.154 0.151
YiSi-0 0.301 0.474 0.330 0.225 0.294 0.215 0.205
YiSi-1 0.319 0.488 0.351 0.231 0.300 0.234 0.211
YiSi-1_srl 0.317 0.483 0.345 0.237 0.306 0.233 0.209

newstest2018

Table 8: Segment-level metric results for to-English language pairs in newstest2018: absolute
Kendall’s Tau formulation of segment-level metric scores with DA scores; correlations of metrics
not significantly outperformed by any other for that language pair are highlighted in bold;
ensemble metrics are highlighted in gray.

en-cs en-de en-et en-fi en-ru en-tr en-zh
Human Evaluation daRR daRR daRR daRR daRR daRR daRR
n 5,413 19,711 32,202 9,809 22,181 1,358 28,602
Correlation τ τ τ τ τ τ τ

BEER 0.518 0.686 0.558 0.511 0.403 0.374 0.302
BLEND − − − − 0.394 − −
CharacTER 0.414 0.604 0.464 0.403 0.352 0.404 0.313
ITER 0.333 0.610 0.392 0.311 0.291 0.236 −
sentBLEU 0.389 0.620 0.414 0.355 0.330 0.261 0.311
YiSi-0 0.471 0.661 0.531 0.464 0.394 0.376 0.318
YiSi-1 0.496 0.691 0.546 0.504 0.407 0.418 0.323
YiSi-1_srl − 0.696 − − − − 0.310

newstest2018

Table 9: Segment-level metric results for out-of-English language pairs in newstest2018: absolute
Kendall’s Tau formulation of segment-level metric scores with DA scores; correlations of metrics
not significantly outperformed by any other for that language pair are highlighted in bold;
ensemble metrics are highlighted in gray.

684



cs-en de-en et-en

R
U

S
E

B
LE

N
D

Y
iS

i.1

Y
iS

i.1
_s

rl

Y
iS

i.0

B
E

E
R

U
H

H
_T

S
K

M

m
et

eo
r..

C
ha

ra
cT

E
R

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

CharacTER

meteor..

UHH_TSKM

BEER

YiSi.0

YiSi.1_srl

YiSi.1

BLEND

RUSE

R
U

S
E

B
LE

N
D

Y
iS

i.1

Y
iS

i.1
_s

rl

B
E

E
R

Y
iS

i.0

m
et

eo
r..

C
ha

ra
cT

E
R

U
H

H
_T

S
K

M

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

UHH_TSKM

CharacTER

meteor..

YiSi.0

BEER

YiSi.1_srl

YiSi.1

BLEND

RUSE

R
U

S
E

B
LE

N
D

Y
iS

i.1

Y
iS

i.1
_s

rl

B
E

E
R

Y
iS

i.0

m
et

eo
r..

U
H

H
_T

S
K

M

C
ha

ra
cT

E
R

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

CharacTER

UHH_TSKM

meteor..

YiSi.0

BEER

YiSi.1_srl

YiSi.1

BLEND

RUSE

fi-en ru-en tr-en

R
U

S
E

Y
iS

i.1
_s

rl

B
E

E
R

Y
iS

i.1

B
LE

N
D

Y
iS

i.0

m
et

eo
r..

C
ha

ra
cT

E
R

U
H

H
_T

S
K

M

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

UHH_TSKM

CharacTER

meteor..

YiSi.0

BLEND

YiSi.1

BEER

YiSi.1_srl

RUSE

R
U

S
E

Y
iS

i.1
_s

rl

Y
iS

i.1

Y
iS

i.0

B
LE

N
D

B
E

E
R

m
et

eo
r..

C
ha

ra
cT

E
R

U
H

H
_T

S
K

M

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

UHH_TSKM

CharacTER

meteor..

BEER

BLEND

YiSi.0

YiSi.1

YiSi.1_srl

RUSE

R
U

S
E

Y
iS

i.1

Y
iS

i.1
_s

rl

B
LE

N
D

B
E

E
R

Y
iS

i.0

m
et

eo
r..

C
ha

ra
cT

E
R

U
H

H
_T

S
K

M

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

UHH_TSKM

CharacTER

meteor..

YiSi.0

BEER

BLEND

YiSi.1_srl

YiSi.1

RUSE

zh-en en-cs

R
U

S
E

B
LE

N
D

B
E

E
R

Y
iS

i.1

Y
iS

i.1
_s

rl

Y
iS

i.0

C
ha

ra
cT

E
R

m
et

eo
r..

se
nt

B
LE

U

U
H

H
_T

S
K

M

IT
E

R

ITER

UHH_TSKM

sentBLEU

meteor..

CharacTER

YiSi.0

YiSi.1_srl

YiSi.1

BEER

BLEND

RUSE

B
E

E
R

Y
iS

i.1

Y
iS

i.0

C
ha

ra
cT

E
R

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

CharacTER

YiSi.0

YiSi.1

BEER

en-de en-et en-fi

Y
iS

i.1
_s

rl

Y
iS

i.1

B
E

E
R

Y
iS

i.0

se
nt

B
LE

U

IT
E

R

C
ha

ra
cT

E
R

CharacTER

ITER

sentBLEU

YiSi.0

BEER

YiSi.1

YiSi.1_srl

B
E

E
R

Y
iS

i.1

Y
iS

i.0

C
ha

ra
cT

E
R

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

CharacTER

YiSi.0

YiSi.1

BEER

B
E

E
R

Y
iS

i.1

Y
iS

i.0

C
ha

ra
cT

E
R

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

CharacTER

YiSi.0

YiSi.1

BEER

en-ru en-tr en-zh

Y
iS

i.1

B
E

E
R

Y
iS

i.0

B
LE

N
D

C
ha

ra
cT

E
R

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

CharacTER

BLEND

YiSi.0

BEER

YiSi.1

Y
iS

i.1

C
ha

ra
cT

E
R

Y
iS

i.0

B
E

E
R

se
nt

B
LE

U

IT
E

R

ITER

sentBLEU

BEER

YiSi.0

CharacTER

YiSi.1

Y
iS

i.1

Y
iS

i.0

C
ha

ra
cT

E
R

se
nt

B
LE

U

Y
iS

i.1
_s

rl

B
E

E
R

BEER

YiSi.1_srl

sentBLEU

CharacTER

YiSi.0

YiSi.1

Figure 3: daRR segment-level metric significance test results for all language pairs (new-
stest2018): Green cells denote a significant win for the metric in a given row over the metric in
a given column according bootstrap resampling.

685



4.2 Overall Metric Performance

As always, the observed performance of met-
rics depends on the underlying texts and sys-
tems that participate in the News Transla-
tion Task. Two new metrics, RUSE and YiSi
stand out as metrics that achieve highest cor-
relation in the system level evaluation in more
than one language pair according to the hybrid
evaluation, and perform great across all their
language pairs on average. ITER also per-
forms very well in en-et, en-fi, zh-en and sev-
eral other languages but fails for en-ru and en-
cs, which drags its overall performance down.

Both YiSi and RUSE are based on neural
networks (YiSi via word and phrase embed-
dings, RUSE via sentence embeddings). This
is a new trend compared to the last year evalu-
ation where the best performance was reached
by character-level (not deep) metrics BEER,
chrF (and its variants) and CharacTer.

It is however important to note that the re-
sults of performance agreggated over language
pairs are not particularly stable across years.
In the last year’s evaluation, NIST seemed
worse than TER. The overall results is the op-
posite this year and NIST even ranks slightly
better than RUSE in terms of average system-
level correlation across languages.

Overall, the reported figures confirm the ob-
servation from the past years that system-
level metrics can achieve correlations above
0.9 but even the best ones can fall to 0.7 or
0.8 for some language pairs. Kendall’s Tau
achieved by segment-level metrics are gener-
ally lower, in the range of 0.25–0.4. The
best metrics in their best language pairs can
reach up to 0.69 of segment-level correlations
with humans. This capping could be possibly
in part attributed to the sub-optimal human
evaluation data, DA judgements converted to
relative ranking.

Two metrics that stand out as performing
consistently well are RUSE for evaluation of
into-English translation and YiSi-1* for out-
of-English. Overall, YiSi*, BEER, Char-
acTER, RUSE, and BLEND (in this order)
outperform sentBLEU.

All of the “winners” in this years campaign
are publicly available, which is very good for
their prospective wider adoption. If partici-
pants could put the additional effort of adding

their code to Moses scorer, this would guar-
antee their long-term inclusion in the Metrics
Task.

5 Conclusion

This paper summarizes the results of WMT18
shared task in machine translation evaluation,
the Metrics Shared Task. Participating met-
rics were evaluated in terms of their correlation
with human judgment at the level of the whole
test set (system-level evaluation), as well as
at the level of individual sentences (segment-
level evaluation). For the former, best met-
rics reach over 0.95 Pearson correlation or bet-
ter across several language pairs. Correlations
varied more than usual between 0.2 and 0.7
in terms of segment-level metrics Kendall’s τ
results.

Acknowledgments

Results in this shared task would not be possi-
ble without tight collaboration with organizers
of the WMT News Translation Task.

This study was supported in parts by
the grants 18-24210S of the Czech Sci-
ence Foundation, ADAPT Centre for Digi-
tal Content Technology (www.adaptcentre.
ie) at Dublin City University funded un-
der the SFI Research Centres Programme
(Grant 13/RC/2106) co-funded under the
European Regional Development Fund, and
Charles University Research Programme “Pro-
gres” Q18+Q48.

References
Ondřej Bojar, Miloš Ercegovčević, Martin Popel,

and Omar Zaidan. 2011. A Grain of Salt for
the WMT Manual Evaluation. In Proceedings
of the Sixth Workshop on Statistical Machine
Translation, pages 1–11, Edinburgh, Scotland,
July. Association for Computational Linguistics.

Ondřej Bojar, Christian Federmann, Barry Had-
dow, Philipp Koehn, Matt Post, and Lucia Spe-
cia. 2016. Ten Years of WMT Evaluation Cam-
paigns: Lessons Learnt. In Proceedings of the
LREC 2016 Workshop “Translation Evaluation
– From Fragmented Tools and Data Sets to an
Integrated Ecosystem”, pages 27–34, Portorose,
Slovenia, 5.

Ondřej Bojar, Yvette Graham, and Amir Kamran.
2017. Results of the WMT17 metrics shared

686



task. In Proceedings of the Second Confer-
ence on Machine Translation, Volume 2: Shared
Tasks Papers, Copenhagen, Denmark, Septem-
ber. Association for Computational Linguistics.

Ondřej Bojar, Jiří Mírovský, Kateřina Rysová, and
Magdaléna Rysová. 2018. Evald reference-less
discourse evaluation for wmt18. In Proceedings
of the Third Conference on Machine Transla-
tion, Belgium, Brussels, October. Association
for Computational Linguistics.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evalu-
ation for any target language. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.

George Doddington. 2002. Automatic Evalua-
tion of Machine Translation Quality Using N-
gram Co-occurrence Statistics. In Proceedings
of the Second International Conference on Hu-
man Language Technology Research, HLT ’02,
pages 138–145, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

Melania Duma and Wolfgang Menzel. 2017. UHH
submission to the WMT17 metrics shared task.
In Proceedings of the Second Conference on Ma-
chine Translation, Volume 2: Shared Tasks Pa-
pers, Copenhagen, Denmark, September. Asso-
ciation for Computational Linguistics.

Yvette Graham and Timothy Baldwin. 2014. Test-
ing for Significance of Increased Correlation with
Human Judgment. In Proceedings of the 2014
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 172–176,
Doha, Qatar, October. Association for Compu-
tational Linguistics.

Yvette Graham and Qun Liu. 2016. Achieving Ac-
curate Conclusions in Evaluation of Automatic
Machine Translation Metrics. In Proceedings of
the 15th Annual Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, San Diego, CA. Association for Computa-
tional Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Mof-
fat, and Justin Zobel. 2013. Continuous Mea-
surement Scales in Human Evaluation of Ma-
chine Translation. In Proceedings of the 7th Lin-
guistic Annotation Workshop & Interoperability
with Discourse, pages 33–41, Sofia, Bulgaria. As-
sociation for Computational Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Mof-
fat, and Justin Zobel. 2014a. Is Machine Trans-
lation Getting Better over Time? In Proceed-
ings of the 14th Conference of the European
Chapter of the Association for Computational

Linguistics, pages 443–451, Gothenburg, Swe-
den, April. Association for Computational Lin-
guistics.

Yvette Graham, Nitika Mathur, and Timothy
Baldwin. 2014b. Randomized significance tests
in machine translation. In Proceedings of the
ACL 2014 Ninth Workshop on Statistical Ma-
chine Translation, pages 266–274. Association
for Computational Linguistics.

Yvette Graham, Nitika Mathur, and Timothy
Baldwin. 2015. Accurate Evaluation of
Segment-level Machine Translation Metrics. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Com-
putational Linguistics Human Language Tech-
nologies, Denver, Colorado.

Yvette Graham, Timothy Baldwin, Alistair Mof-
fat, and Justin Zobel. 2016. Can machine trans-
lation systems be evaluated by the crowd alone.
Natural Language Engineering, FirstView:1–28,
1.

Yinuo Guo, Chong Ruan, and Junfeng Hu. 2018.
Meteor++: Incorporating copy knowledge into
machine translation evaluation. In Proceedings
of the Third Conference on Machine Transla-
tion, Belgium, Brussels, October. Association
for Computational Linguistics.

Philipp Koehn and Christof Monz. 2006. Manual
and Automatic Evaluation of Machine Trans-
lation Between European Languages. In Pro-
ceedings of the Workshop on Statistical Ma-
chine Translation, StatMT ’06, pages 102–121,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proc. of
Empirical Methods in Natural Language Process-
ing, pages 388–395, Barcelona, Spain. Associa-
tion for Computational Linguistics.

Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using
Block Movements. In In Proceedings of EACL,
pages 241–248.

Chi-kiu Lo. 2018. The NRC metric submission to
the WMT18 metric and parallel corpus filtering
shared task. In Arxiv.

Qingsong Ma, Yvette Graham, Shugen Wang, and
Qun Liu. 2017. Blend: a novel combined MT
metric based on direct assessment — casict-dcu
submission to WMT17 metrics task. In Pro-
ceedings of the Second Conference on Machine
Translation, Volume 2: Shared Tasks Papers,
Copenhagen, Denmark, September. Association
for Computational Linguistics.

687



Matouš Macháček and Ondřej Bojar. 2014. Re-
sults of the WMT14 metrics shared task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, pages 293–301, Balti-
more, MD, USA. Association for Computational
Linguistics.

Matouš Macháček and Ondřej Bojar. 2013. Re-
sults of the WMT13 Metrics Shared Task. In
Proceedings of the Eighth Workshop on Statis-
tical Machine Translation, pages 45–51, Sofia,
Bulgaria, August. Association for Computa-
tional Linguistics.

Joybrata Panja and Sudip Kumar Naskar. 2018.
Iter: Improving translation edit rate through
optimizable edit costs. In Proceedings of the
Third Conference on Machine Translation, Bel-
gium, Brussels, October. Association for Com-
putational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Au-
tomatic Evaluation of Machine Translation. In
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ’02,
pages 311–318.

Maja Popović. 2015. chrF: character n-gram F-
score for automatic MT evaluation. In Proceed-
ings of the Tenth Workshop on Statistical Ma-
chine Translation, Lisboa, Portugal, September.
Association for Computational Linguistics.

Maja Popović. 2017. chrF++: words helping char-
acter n-grams. In Proceedings of the Second
Conference on Machine Translation, Volume 2:
Shared Tasks Papers, Copenhagen, Denmark,
September. Association for Computational Lin-
guistics.

Hiroki Shimanaka, Tomoyuki Kajiwara, and
Mamoru Komachi. 2018. Ruse: Regressor us-
ing sentence embeddings for automatic machine
translation evaluation. In Proceedings of the
Third Conference on Machine Translation, Bel-
gium, Brussels, October. Association for Com-
putational Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In In Proceedings of Associa-
tion for Machine Translation in the Americas,
pages 223–231.

Miloš Stanojević and Khalil Sima’an. 2015. BEER
1.1: ILLC UvA submission to metrics and tun-
ing task. In Proceedings of the Tenth Work-
shop on Statistical Machine Translation, Lisboa,
Portugal, September. Association for Computa-
tional Linguistics.

Yolanda Vazquez-Alvarez and Mark Huckvale.
2002. The reliability of the ITU-t p.85 standard

for the evaluation of text-to-speech systems. In
Proc. of ICSLP - INTERSPEECH.

Weiyue Wang, Jan-Thorsten Peter, Hendrik
Rosendahl, and Hermann Ney. 2016a. Charac-
ter: Translation edit rate on character level. In
ACL 2016 First Conference on Machine Trans-
lation, pages 505–510, Berlin, Germany, August.

Weiyue Wang, Jan-Thorsten Peter, Hendrik
Rosendahl, and Hermann Ney. 2016b. Charac-
Ter: Translation Edit Rate on Character Level.
In Proceedings of the First Conference on Ma-
chine Translation, Berlin, Germany, August.
Association for Computational Linguistics.

Evan James Williams. 1959. Regression analysis,
volume 14. Wiley New York.

688


