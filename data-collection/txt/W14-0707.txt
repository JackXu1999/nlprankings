



















































Recognizing Causality in Verb-Noun Pairs via Noun and Verb Semantics


Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 48–57,
Gothenburg, Sweden, April 26, 2014. c©2014 Association for Computational Linguistics

Recognizing Causality in Verb-Noun Pairs
via Noun and Verb Semantics

Mehwish Riaz and Roxana Girju
Department of Computer Science and Beckman Institute

University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA

{mriaz2,girju}@illinois.edu

Abstract

Several supervised approaches have been
proposed for causality identification by re-
lying on shallow linguistic features. How-
ever, such features do not lead to improved
performance. Therefore, novel sources
of knowledge are required to achieve
progress on this problem. In this paper,
we propose a model for the recognition of
causality in verb-noun pairs by employing
additional types of knowledge along with
linguistic features. In particular, we fo-
cus on identifying and employing seman-
tic classes of nouns and verbs with high
tendency to encode cause or non-cause re-
lations. Our model incorporates the in-
formation about these classes to minimize
errors in predictions made by a basic su-
pervised classifier relying merely on shal-
low linguistic features. As compared with
this basic classifier our model achieves
14.74% (29.57%) improvement in F-score
(accuracy), respectively.

1 Introduction

The automatic detection of causal relations is im-
portant for various natural language processing ap-
plications such as question answering, text sum-
marization, text understanding and event predic-
tion. Causality can be expressed using various nat-
ural language constructions (Girju and Moldovan,
2002; Chang and Choi, 2006). Consider the fol-
lowing examples where causal relations are en-
coded using (1) a verb-verb pair, (2) a noun-noun
pair and (3) a verb-noun pair.
1. Five shoppers were killed when a car blew up

at an outdoor market.
2. The attack on Kirkuk’s police intelligence

complex sees further deaths after violence
spilled over a nearby shopping mall.

3. At least 1,833 people died in hurricane.
Since, the task of automatic recognition of

causality is quite challenging, researchers have
addressed this problem by considering specific
constructions. For example, various models
have been proposed to identify causation between
verbs (Bethard and Martin, 2008; Beamer and
Girju, 2009; Riaz and Girju, 2010; Do et al., 2011;
Riaz and Girju, 2013) and between nouns (Girju
and Moldovan, 2002; Girju, 2003). Do et al.
(2011) have worked with verb-noun pairs for
causality detection but they focused only on a
small list of predefined nouns representing events.

In this paper, we focus on the task of identi-
fying causality encoded by verb-noun pairs (ex-
ample 3). We propose a novel model which first
predicts cause or non-cause relations using a su-
pervised classifier and then incorporates additional
types of knowledge to reduce errors in predictions.
Using a supervised classifier, our model identi-
fies causation by employing shallow linguistic fea-
tures (e.g., lemmas of verb and noun, words be-
tween verb and noun). Such features have been
used successfully for various NLP tasks (e.g., part-
of-speech tagging, named entity recognition, etc.)
but confinement to such features does not help
much to achieve performance for identifying cau-
sation (Riaz and Girju, 2013). Therefore, in our
model we plug in additional types of knowledge
to obtain better predictions for the current task.
For example, we identify the semantic classes of
nouns and verbs with high tendency to encode
cause or non-causal relations and use this knowl-
edge to achieve better performance. Specifically,
the contributions of this paper are as follows:
• In order to build a supervised classifier, we

use the annotations of FrameNet to generate a
training corpus of verb-noun instances encod-
ing cause and non-cause relations. We propose
a set of linguistic features to learn and identify
causal relations.

48



• In order to make intelligent predictions, it is
important for our model to have knowledge
about the semantic classes of nouns with high
tendency to encode causal or non-causal re-
lations. For example, a named entity such
as person, organization or location may have
high tendency to encode non-causality unless a
metonymic reading is associated with it. In our
approach, we identify such semantic classes of
nouns by exploiting a named entity recognizer,
the annotations of frame elements provided in
FrameNet and WordNet.
• Verbs are the important components of lan-

guage for expressing events of various types.
For example, Pustejovsky et al. (2003)
have classified events into eight semantic
classes: OCCURRENCE, PERCEPTION, RE-
PORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. We argue that there
are some semantic classes in this list with high
tendency to encode cause or non-cause rela-
tions. For example, reporting events repre-
sented by verbs say, tell, etc., have high ten-
dency to just report other events instead of en-
coding causality with them. In our model, we
use such information to reduce errors in predic-
tions.
• Each causal relation is characterized by two

roles i.e., cause and its effect. In example 3
above, the noun “hurricane” is cause and the
verb “died” is its effect. However, a verb-noun
pair may not encode causality when a verb
and a noun represent same event. For exam-
ple, in instance “Colin Powell presented fur-
ther evidence in his presentation.”, the verb
“presented” and the noun “presentation” rep-
resent same event of “presenting” and thus en-
coding non-cause relation with each other. In
our model, we determine the verb-noun pairs
representing same or distinct events to make
predictions accordingly.
• We adopt the framework of Integer Linear Pro-

gramming (ILP) (Roth and Yih, 2004; Do et al.,
2011) to combine all the above types of knowl-
edge for the current task.

This paper is organized as follows. In next sec-
tion, we briefly review the previous research done
for identifying causality. We introduce our model
and evaluation with discussion on results in sec-
tion 3 and 4, respectively. The section 5 of the
paper concludes our current research.

2 Related Work

In computational linguistics, researchers have al-
ways shown interest in the task of automatic
recognition of causal relations because success on
this task is critical for various natural language
applications (Girju, 2003; Chklovski and Pantel,
2004; Radinsky and Horvitz, 2013).

Following the successful employment of lin-
guistic features for various tasks (e.g., part-of-
speech tagging, named entity recognition, etc.),
initially NLP researchers proposed approaches re-
lying mainly on such features to identify causal-
ity (Girju, 2003; Bethard and Martin, 2008;
Sporleder and Lascarides, 2008; Pitler and
Nenkova, 2009; Pitler et al., 2009). However,
researchers have recently shifted their attention
from these features and tried to consider other
sources of knowledge for extracting causal rela-
tions (Beamer and Girju, 2009; Riaz and Girju,
2010; Do et al., 2011; Riaz and Girju, 2013).
For example, Riaz and Girju (2010) and Do et
al. (2011) have proposed unsupervised metrics for
learning causal dependencies between two events.
Do et al. (2011) have also incorporated minimal
supervision with unsupervised metrics. For a pair
of events (a, b), their model makes the decision of
cause or non-cause relation based on unsupervised
co-occurrence counts and then improves this deci-
sion by using minimal supervision from the causal
and non-causal discourse markers (e.g., because,
although, etc.).

In search of novel and effective types of knowl-
edge to identify causation between two verbal
events, Riaz and Girju (2013) have proposed a
model to learn a Knowledge Base (KBc) of verb-
verb pairs. In this knowledge base, the English
language verb-verb pairs are automatically clas-
sified into three categories: (1) Strongly Causal,
(2) Ambiguous and (2) Strongly Non-Causal. The
Strongly Causal and Strongly Non-Causal cate-
gories contain verb-verb pairs with highest and
least tendency to encode causality, respectively
and rest of the verb-verb pairs are considered am-
biguous with tendency to encode both types of
relations. They claim that this knowledge base
of verb-verb pairs is a rich source of causal as-
sociations. The incorporation of this resource
into a causality detection model can help identi-
fying causality with better performance. In this
research, we also try to go beyond the scope of
shallow linguistic features and identify additional

49



interesting types of knowledge for the current task.

3 Computational Model for Identifying
Causality

In this section, we introduce our model for iden-
tifying causality encoded by verb-noun pairs.
Specifically, we extract all main verbs and noun
phrases from a sentence and predict cause or non-
cause relation on verb-noun phrase (v-np) pairs.
In order to make task easier, we consider only
those v-np pairs where v (verb) is grammatically
connected to np (noun phrase). We assume that a v
and np are grammatically connected if there exists
a dependency relation between them in the depen-
dency tree. We apply a dependency parser (Marn-
effe et al., 2006) to identify such dependencies.
Our model first employs a supervised classifier re-
lying on linguistic features to make binary predic-
tions (i.e., does a verb-noun phrase pair encode a
cause or non-cause relation?). We then incorpo-
rate additional types of knowledge on top of these
binary predictions to improve performance.

3.1 Supervised Classifier
In this section, we propose a basic supervised
classifier to identify causation encoded by v-np
pairs. To set up this supervised classifier, we need
a training corpus of instances of v-np pairs en-
coding cause and non-cause relations. For this
purpose, we employ the annotations of FrameNet
project (Baker et al., 1998) provided for verbs.
For example, consider the following annotation
from FrameNet for the verb “dying” with ar-
gument “solvent abuse” where the pair “dying-
solvent abuse” encodes causality.

A campaign has started to try to cut the
rising number of children dying [cause
from solvent abuse].

To generate a training corpus, we collect anno-
tations of verbs from FrameNet s.t. the annotated
element (aka. frame element) is a noun phrase.
For example, we get a causal training instance of
“dying-solvent abuse” pair from the above anno-
tation. We assume that if a FrameNet’s annotated
element contains a verb in it then this may not rep-
resent a training instance of v-np pair. For exam-
ple, we do not consider the following annotation
in our training corpus where causality is encoded
between two verbs i.e., “died-fell”.

A fitness fanatic died [cause when 26
stone of weights fell on him as he ex-
ercised].

After extracting training instances from
FrameNet, we assign them cause (c) and non-
cause (¬ c) labels. We manually examined the
inventory of labels of FrameNet and use the
following scheme to assign the c or ¬c to each
training instance. All the annotations of FrameNet
with following labels are considered as causal
training instances and rest of the annotations are
considered as non-causal training instances.

Purpose, Internal cause, Result, Exter-
nal cause, Cause, Reason, Explanation,
Required situation, Purpose of Event,
Negative consequences, resulting ac-
tion, Effect, Cause of shine, Purpose of
Goods, Response action, Enabled situa-
tion, Grinding cause, Trigger

For this work, we have acquired 2, 158
(65, 777) cause (non-cause) training instances
from FrameNet. Since, the non-cause instances
are very large in number, our supervised model
tends to assign non-cause labels to almost all in-
stances. Therefore, we employ equal number of
cause and non-cause instances for training. In fu-
ture, we plan to extract more annotations from
the FrameNet and employ more than one human
annotators to assign the labels of cause and non-
cause relations to the full inventory of labels of
FrameNet.
• Lexical Features: verb, lemma of verb, noun

phrase, lemma of all words of noun phrase,
head noun of noun phrase, lemmas of all words
between verb and head noun of noun phrase.
• Semantic Features: We adopted this feature

from Girju (2003) to capture the semantics of
nouns. The 9 noun hierarchies of WordNet i.e.,
entity, psychological feature, abstraction, state,
event, act, group, possession, phenomenon are
used as this feature. Each of these hierarchies
is set to 1 if any sense of the head noun of noun
phrase lies in that hierarchy otherwise set to 0.
• Structural Features: This feature is applied

by considering both subject (i.e., sub in np)
and object (i.e., obj in np) of a verb. For ex-
ample, for a v-np pair the variable sub in np is
set to 1 if the subject of v is contained in np,
set to 0 if the subject of v is not contained in
np and set to -1 if the subject of v is not avail-
able in the instance. The subject and object of
a verb are its core arguments and may some-
time be part of an event represented by a verb.
Therefore, these argument may have high ten-
dency to encode non-cause relations.

50



We set up the following integer linear program
after acquiring predictions of c and ¬ c labels us-
ing our supervised classifier.

Z1 = max
∑

v-np∈I

∑
l∈L1

x1(v-np, l)P (v-np, l) (1)∑
l∈L1

x1(v-np, l) = 1 ∀ v-np ∈ I (2)

x1(v-np, l) ∈ {0, 1} ∀ v-np ∈ I ∀l ∈ L1 (3)

Here L1 = {c,¬c}, I is the set of all instances
of v-np pairs and x1(v-np, l) is the decision vari-
able set to 1 only if the label l ∈ L1 is assigned
to v-np. The Equation 2 constraints that only one
label out of |L1| choices can be assigned to a v-np
pair. The equation 3 requires x1(v-np, l) to be a
binary variable. Specifically, we try to maximize
the objective function Z1 (equation 1) which as-
signs the label cause or non-cause to all v-np pairs
(i.e., set the variables x1(v-np, l) to 1 or 0 for all
l ∈ L1 and for all v-np pairs in I) depending
on the probabilities of assignment of labels (i.e.,
P (v-np, l))1. These probabilities can be obtained
by running a supervised classification algorithm
(e.g., Naive Bayes and Maximum Entropy). In our
experiments, we provide results using the follow-
ing probabilities acquired with Naive Bayes.

P (v-np, c) = 1.0−
∑n

k=1 logP (fk | c)∑n
k=1

∑
l∈(c,¬c) logP (fk | l)

P (v-np,¬c) = 1.0− P ((v, np), c) (4)

where fk is a feature, n is total number of fea-
tures and P (fk | l) is the smoothed probability of
a feature fk given the training instances of label l.

3.2 Knowledge of Semantic classes of nouns
Philospher Jaegwon Kim (Kim, 1993) (as cited by
Girju and Moldovan (2002)) pointed out that the
entities which represent either causes or effects are
often events, but also conditions, states, phenom-
ena, processes, and sometimes even facts. There-
fore, according to this our model should have
knowledge of the semantic classes of noun phrases
with high tendency to encode cause or non-cause
relations. Considering this type of knowledge, we
can automatically review and correct the wrong
predictions made by our basic supervised classi-
fier.

1We use the integer linear program solver available at
http://sourceforge.net/projects/lpsolve/

We argue that if a noun phrase represents a
named entity then it can have least tendency to en-
code causal relations unless there is a metonymic
reading associated with it. For example, con-
sider the following cause and non-cause examples
where noun phrase is a named entity.
4. Sandy hit Cuba as a Category 3 hurricane.
5. Almost all the weapon sites in Iraq were de-

stroyed by the United States.
In example 4, Cuba is location and does not

encode causality. However, in example 5 the
pair “destroyed-the United States” encode causal-
ity where a metonymic reading is associated with
the location. We apply Named Entity Recog-
nizer (Finkel et al., 2005) and assume if a noun
phrase is identified as a named entity then its cor-
responding verb-noun phrase pair encodes non-
cause relation. This constraint can lead to a false
negative prediction when the metonymic reading
is associated with a noun phrase. In order to
avoid as much false negatives as possible, we im-
ply the following simple rule i.e., if one of the
following cue words appear between a verb and a
noun phrase then do not apply the constraint stated
above.

by, from, because of, through, for

In our experiments, the above simple rule helps
avoiding some false negatives but in future any
subsequent improvement with a better metonomy
resolver (Markert and Nissim, 2009) should im-
prove the performance of our model.

In addition to named entities, there can be var-
ious noun phrases with least tendency to encode
causation. Consider the following example, where
“city” is a location and does not encode cause-
effect relation with the verb “remained”.

Substantially fewer people remained in
the city during the Hurricane Ivan evac-
uation.

In this work, we identify the semantic classes
of noun phrases which do not normally represent
events, conditions, states, phenomena, processes
and thus have high tendency to encode non-cause
relations. For this purpose, we manually examine
the inventory of labels assigned to noun phrases
in FrameNet (see table 1) and classify these labels
into two classes (cnp and ¬cnp). Here, the class
cnp (¬cnp) represents the labels of noun phrases
with high (less) tendency to encode cause-effect
relations. For example, the label “Place” ∈ ¬cnp

51



(see table 1) represents a location and it may have
least tendency to encode causality if metonymy is
not associated with it. Using the classification of
frame elements in table 1, we obtain the annota-
tions of noun phrases from FrameNet and catego-
rize these annotations into cnp and ¬cnp classes.
On top of the annotations of these two semantic
classes, we build a supervised classifier for pre-
dicting cnp or ¬cnp label for the noun phrases.
After obtaining predictions, we select all noun
phrases lying in class ¬cnp and apply the same
constraint stated above for the named entities. We
use the following set of features to set up a super-
vised classifier for cnp and ¬cnp labels.
• Lexical Features: words of noun phrase, lem-

mas of all words of noun phrase, head word
of noun phrase, first two (three) (four) letters
of head noun of noun phrase, last two, (three)
(four) letters of head noun of noun phrase.
• Word Class Features: part-of-speech tags of

all words of noun phrase, part-of-speech tag of
head noun of noun phrase.
• Semantic Features: all (frequent) sense(s) of

head noun of noun phrase.

We have acquired 23,334 (81,279) training in-
stances of cnp (¬cnp) class, respectively for this
work. We also use WordNet to obtain more train-
ing instances of these classes. We follow the ap-
proach similar to Girju and Moldovan (2002) and
adopt some senses of WordNet (shown in table 1)
to acquire training instances of noun phrases. For
example, considering the table 1, we assign ¬cnp
label to any noun whose all senses in WordNet lie
in the semantic hierarchy originated by the sense
{time period, period of time, period}. Follow-
ing this scheme, we extract instances of nouns and
noun phrases from English GigaWord corpus and
assign the labels cnp and ¬cnp to them by em-
ploying WordNet senses given in table 1. Girju
and Moldovan (2002) have used similar scheme
to rank noun phrases according to their tendency
to encode causation. In comparison to them, we
use the WordNet senses to increase the size of
our training set of noun phrases obtained using
FrameNet above. In addition to this, we build a
automatic classifier on the training data obtained
using labels of FrameNet and WordNet senses to
classify noun phrases of test instances into two se-
mantics classes (i.e., cnp and ¬cnp). In our train-
ing corpus of there are 2, 214, 68 instances of noun
phrases (50% belongs to each of cnp and ¬cnp

classes).
We incorporate the knowledge of semantics of

nouns in our model by making the following ad-
ditions to the integer linear program introduced in
section 3.1.

Z2 = Z1 +
∑

np:v-np∈I

∑
l∈L2

x2(np, l)P (np, l) (5)∑
l∈L2

x2(np, l) = 1 ∀ np : v-np ∈ I −M (6)

x2(np, l) ∈ {0, 1} ∀ np : v-np ∈ I −M (7)
∀l ∈ L2

x1(v-np,¬c)− x2(np,¬cnp) ≥ 0 (8)
∀ np : v-np, ∀ v-np ∈ I −M

Here L2 = {cnp,¬cnp} and M is the set of
instances of those v-np pairs for which we con-
sider the possibility of attachment of metonymic
reading with np, x2(np, l) is the decision variable
set to 1 only if the label l ∈ L2 is assigned to
np. The Equation 6 constraints that only one la-
bel out of |L2| choices can be assigned to a np.
The equation 7 requires x2(np, l) to be a binary
variable. The constraint 8 assumes that if an np
belongs to the semantic class ¬cnp then its corre-
sponding pair v-np is assigned the label ¬c. We
maximize the objective function Z2 (equation 5)
of our integer linear program subject to the con-
straints introduced above. We predict the semantic
class of a noun phrase using the supervised classi-
fier for cnp and ¬cnp classes and set the probabil-
ities i.e., P (np, l) = 1, P (np, {L2} − {l}) = 0 if
the label l ∈ L2 is assigned to np. Again we use
Naive Bayes to predict the labels for noun phrases.
Also before running this supervised classifier, we
run the named entity recognizer and assign ¬cnp
labels to all noun phrases identified as named en-
tities. For our model, we apply named entity rec-
ognizer for seven classes i.e., LOCATION, PER-
SON, ORGANIZATION, DATE, TIME, MONEY,
PERCENT (Finkel et al., 2005).

3.3 Knowledge of Semantic classes of verbs
In this section, we introduce our method to in-
corporate the knowledge of semantic classes of
verbs to identify causation. Verbs are the com-
ponents of language for expressing events of var-
ious types. In TimeBank corpus, Pustejovsky et
al. (2003) have introduced eight semantic classes
of events i.e., OCCURRENCE, PERCEPTION,
REPORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. According to the defi-
nitions of these classes provided by Pustejovsky

52



Semantic
Class

FrameNet Labels WordNet Senses

cnp Event, Goal, Purpose, Cause, Internal
cause, External cause, Result, Means, Rea-
son, Phenomena

{act, deed, human action, human activity},
{phenomenon}, {state}, {psychological
feature}, {event}, {causal agent, cause,
causal agency}

¬cnp Artist, Performer, Duration, Time, Place,
Distributor, Area, Path, Direction, Sub-
region

{time period, period of time, period},
{measure, quantity, amount}, {group,
grouping}, {organization, organisation},
{time unit, unit of time}, {clock time, time}

Table 1: This table presents some examples of FrameNet labels in cnp and ¬cnp classes. The full set of
labels in both semantic classes are given in appendix A. It also presents the WordNet senses of nouns
lying in cnp and ¬cnp classes.

et al. (2003), the reporting events describe the
action of a person, declare something or narrate
an event e.g., the reporting events represented by
verbs say, tell, etc. Here, we argue that a reporting
event has the least tendency to encode causation
because such an event only describes or narrates
another event instead of encoding causality with
it. We assume that the verbs representing report-
ing events have least tendency to encode causa-
tion and thus their corresponding v-np pairs have
least tendency to encode causation. To add this
knowledge to our model, we consider two classes
of verbs i.e., cv and ¬cv where the class cv (¬cv)
contains the verbs with high (less) tendency to en-
code causation. Using above argument we claim
that all verbs representing reporting events belong
¬cv class and verbs representing rest of the types
of events belong to cv class. We build a supervised
classifier which automatically classifies verbs into
cv and ¬cv classes. We extract the instances of
verbal events (i.e., verbs or verbal phrases) from
TimeBank corpus and assign the labels cv and
¬cv to these instances. Using these labeled in-
stances, we build a supervised classifier by adopt-
ing the same set features as introduced in Bethard
and Martin (2006) to identify semantic classes of
verbs. Due to space constraint, we refer the reader
to Bethard and Martin (2006) for the details of fea-
tures. Again we use Naive Bayes to take predic-
tions of cv and ¬cv labels and their corresponding
probabilities using equation 4.

We incorporate the knowledge of semantics of
verbs in our model by making the following addi-
tions to the integer linear program.

Z3 = Z2 +
∑

v:v-np∈I

∑
l∈L3

x3(v, l)P (v, l) (9)

∑
l∈L3

x3(v, l) = 1 ∀ v : v-np ∈ I (10)

x3(v, l) ∈ {0, 1} ∀ v : v-np ∈ I ∀l ∈ L3 (11)

x1(v-np,¬c)− x3(v,¬cv) >= 0 (12)
∀ v : v-np, ∀ v-np ∈ I

x3(v, cv)− x1(v-np, c) >= 0 (13)
∀ v : v-np, ∀ v-np ∈ I

Here L3 = {cv,¬cv}, x3(v, l) is the decision
variable set to 1 only if the label l ∈ L3 is as-
signed to v. The Equation 10 constraints that only
one label out of |L3| choices can be assigned to
a v. The equation 11 requires x3(v, l) to be a bi-
nary variable. The constraint 12 assumes if a verb
v belongs to the class cv (i.e., has least potential
to encode causation) then its corresponding pair
v-np encodes non-causality. The constraint 12 en-
forces that if a verb v belongs to the class ¬cv then
its corresponding v-np pair is assigned the label
¬c. Similarly, the constraint 16 enforces that if
a v-np pair encodes causality then its verb v has
potential to encode causal relation. We maximize
the objective function Z3 subject to the constraints
introduced above.

3.4 Knowledge of Indistinguishable Verb and
Noun

As introduced earlier, each causal relation is char-
acterized by two roles i.e., cause and its effect. In
order to encode causal relation, two components
of an instance of verb-noun phrase pair need to
represent distinct events, processes or phenomena.
Employing simple lexical matching, we determine
if a verb and a noun phrase represent same event
or not as follows:

53



• We use NOMLEX (Macleod et al., 2009) to
transform a verb into its corresponding nomi-
nalization and use the following text segments
for lexical matching.

Tv = [Subject] verb [Object]2

Tn = Head noun of noun phrase
• We remove stopwords and duplicate words

from Tv and Tn and take lemmas of all words.
If the subject or object or both arguments are
contained in noun phrase then we remove these
arguments from Tv. We determine the proba-
bility of a verb (v) and a noun phrase (np) rep-
resenting same event as follows. If head noun
(i.e., Tn) lexically matches with any word of Tv
then set P(v ≡ np) to 1 and 0 otherwise.

We assign non-cause relation if P(v ≡ np) = 1.
Next, we incorporate the knowledge of indistin-
guishable verb and noun in our model using the
following additions to our integer linear program.

Z4 = Z3 +
∑

v-np∈I

∑
l∈L4

x4(v-np, l)P (v-np, l) (14)∑
l∈L4

x4(v-np, l) = 1 ∀ v-np ∈ I (15)

x4(v-np, l) ∈ {0, 1} ∀ v-np ∈ I, ∀l ∈ L4

x1(v-np,¬c)− x4(v-np,≡) ≥ 0 (16)
∀ v-np ∈ I

Here L4 = {≡, 6≡} where the label ≡ (6≡) rep-
resents same (distinct) events, x4(v-np, l) is the
decision variable set to 1 only if the label l ∈ L4
is assigned to v-np. The Equation 15 constraints
that only one label out of |L4| choices can be as-
signed to a v-np pair. The equation 16 requires
x4(v-np, l) to be a binary variable. The con-
straint 16 enforces that if a v-np pair belongs to
the class ≡ then this pair is assigned the label ¬c.
We maximize the objective function Z4 subject to
the constraints introduced above.

4 Evaluation and Discussion

In this section we present the experiments, evalu-
ation procedures, and a discussion on the results
achieved through our model for the current task.

In order to evaluate our model, we generated a
test set with instances of form verb-noun phrase
where the verb is grammatically connected to the
noun phrase in an instance. For this purpose, we

2Following Riaz ang Girju (2010), we assume that the
subject and object of a verb are parts of an event represented
by a verb. Therefore, we use these arguments along with a
verb for lexical matching with a noun phrase.

collected three wiki articles on the topics of Hurri-
cane Katrina, Iraq War and Egyptian Revolution
of 2011. We selected first 100 sentences from
these articles and applied part-of-speech tagger
(Toutanova et al., 2003) and dependency parser
(Marneffe et al., 2006) on these sentences. Using
each sentence, we extracted all verb-noun phrase
pairs where the verb has a dependency relation
with any word of noun phrase. We manually in-
spected all of the extracted instances and removed
those instances in which a word had been wrongly
classified as a verb by the part-of-speech tagger.
There are total 1106 instances in our test set. We
assigned the task of annotation of these instances
with cause and non-cause relations to a human
annotator. Using manipulation theory of causal-
ity (Woodward, 2008), we adopted the annotation
guidelines from Riaz and Girju (2010) which is as
follows: “Assign cause label to a pair (a, b), if the
following two conditions are satisfied: (1), a tem-
porally precedes/overlap b in time, (2) while keep-
ing as many state of affairs constant as possible,
modifying a must entail predictably modifying b.
Otherwise assign non-cause label. ”

We have 149 (957) cause (non-cause) instances
in our test set3, respectively. We evaluate the per-
formance of our model using F-score and accuracy
evaluation measures (see table 2 for results).

The results in table 2 reveal that the ba-
sic supervised classifier is a naive model and
achieves only 27.27% F-score and 46.47% ac-
curacy. The addition of novel types of
knowledge introduced in section 3 (i.e., the
model Basic+SCNM +SCV+IVN) brings 14.74%
(29.57%) improvements in F-score (accuracy), re-
spectively. These results show that the knowledge
of semantics of nouns and verbs and the knowl-
edge of indistinguishable verb and noun are crit-
ical to achieve performance. The maximum im-
provement in results is achieved with the addition
of semantic classes of nouns (i.e., Basic+SCNM ).
The consideration of association of metonymic
readings using model Basic+SCNM helps us to
maintain recall as compared with SCN!M and
therefore brings better F-score.

One can notice that almost all models suffer
from low precision which leads to lower F-scores.
Although, our model achieves 14.58% increase in
precision over basic supervised classifier, the lack
of high precision is still responsible for lower F-

3We will make the test set available

54



Model Basic +SCN!M +SCNM +SCNM +SCV +SCNM +SCV+IVN
Accuracy 46.47 75.76 74.41 75.31 76.04
Precision 16.69 28.14 29.53 30.47 31.27
Recall 74.49 50.66 64.00 64.00 64.00
F-score 27.27 39.19 40.42 41.29 42.01

Table 2: This table presents results of the basic supervised classifier (i.e., Basic) and the models after
incrementally adding the knowledge of semantic classes of nouns without consideration of metonymic
readings (i.e., + SCN!M ), the knowledge of semantic classes of nouns with consideration of metonymic
readings (i.e., + SCNM ), the knowledge of semantic classes of verbs (i.e., +SCNM +SCV) and the knowl-
edge of indistinguishable verb and noun (i.e., +SCNM +SCV+IVN).

score. The highly skewed distribution of test set
with only 13.47% causal instances results in lots
of false positives. We manually examined false
positives to determine the language features which
may help us reducing more false positives without
affecting F-score. We noticed that the direct ob-
jects of the verbs are mostly part of the event rep-
resented by the verbs and therefore encodes non-
causation with the verbs. For example, consider
following instances:
6. The hurricane surge protection failures

prompted a lawsuit.
7. They provided weather forecasts.

In example 6, “lawsuit” is the direct object of
the verb “prompted” and is part of the event rep-
resented by the verb “prompt”. However there
is a cause relation between “protection failures”
and “prompted”. Similarly in example 7, the di-
rect object “forecasts” is part of the “providing”
event and thus the noun phrase “weather fore-
casts” encode non-cause relation with the verb
“provide”. Therefore, following this observation
we employed the training corpus of cause and non-
cause relations (see section 3.1) and learned the
structure of verb-noun phrase pairs encoding non-
cause relations most of the time. We considered
only those training instances where the subject
and/or object of the verb was available. For the
current purpose, we picked up following four fea-
tures (1) sub in np, (2) !sub in np, (3) obj in np
and (4) !obj in np. Just to remind the reader, the
feature sub in np (!sub in np) is set to 1 if the
subject of the verb is (not) contained in the noun
phrase np, respectively. For each of the above four
features, the percentage of cause and entropy of
relations with that feature are as follows:
• sub in np (%c = 34.72, Entropy = 0.931)
• !sub in np (%c = 59.71, Entropy = 0.972)
• obj in np (%c = 28.89, Entropy = 0.867)

• !obj in np (%c = 55.30, Entropy = 0.991).
There are two important observations from

above scores: (1) verbs mostly encode non-cause
relations with their objects and subjects (i.e., high
%¬c with obj in np and sub in np), (2) among
obj in np and sub in np features, obj in np yields
least entropy i.e., there are least chances of encod-
ing causality of a verb with its object.

Considering the above statistics, we enforce the
constraint on each verb-noun phrase pair that if
the object of the verb is contained in the noun
phrase of the above pair then assigns non-cause
relation to that pair. Using this constraint, we ob-
tain 46.61% (80.74%) F-score (accuracy), respec-
tively. This confirms our observation that the ob-
ject of a verb is normally part of an event repre-
sented by the verb and thus it encodes non-cause
relation with the verb.

In this research, we have utilized novel types
of knowledge to improve the performance of our
model. In future, we need to consider more
additional information (e.g., predictions from
metonymy resolver) to achieve further progress.

5 Conclusion

In this paper, we have proposed a model for iden-
tifying causality in verb-noun pairs by employing
the knowledge of semantic classes of nouns and
verbs and the knowledge of indistinguishable noun
and verb of an instance along with shallow linguis-
tic features. Our empirical evaluation of model
has revealed that such novel types of knowledge
are critical to achieve a better performance on the
current task. Following the encouraging results
achieved by our model, we invite researchers to
investigate more interesting types of knowledge in
future to make further progress on the task of rec-
ognizing causality.

55



References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.

1998. The Berkeley FrameNet project. In proceed-
ings of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics (COLING-ACL).

Brandon Beamer and Roxana Girju. 2009. Using
a Bigram Event Model to Predict Causal Poten-
tial. In proceedings of the Conference on Compu-
tational Linguistics and intelligent Text Processing
(CICLING).

Steven Bethard and James H. Martin. 2006. Identifica-
tion of Event Mentions and their Semantic Class. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).

Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of the Associ-
ation for Computational Linguistics (ACL).

Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, volume 42 issue 3, 662678.

Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).

Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Association for
Computational Linguistics (ACL).

Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond.

Roxana Girju and Dan Moldovan. 2002. Mining An-
swers for Causation Questions. In American Asso-
ciations of Artificial Intelligence (AAAI), 2002 Sym-
posium.

Jaegwon Kim. 1993. Causes and Events. Mackie on
Causation. In Cansation, Oxford Readings in Phi-
losophy, ed. Ernest Sosa, and Michael Tooley, Ox-
ford University Press.

Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, Ruth Reeves. 1998. NOMLEX: A
Lexicon of Nominalizations. In proceedings of EU-
RALEX.

Katja Markert, Malvina Nissim 2009. Data and mod-
els for metonymy resolution. Language Resources
and Evaluation Volume 43 Issue 2, Pages 123−138.

Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).

Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP.

Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP.

James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics.

Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, (WSDM).

Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).

Mehwish Riaz and Roxana Girju 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
the annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL).

Dan Roth and Wen-tau Yih 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In Proceedings of the Annual Con-
ference on Computational Natural Language Learn-
ing (CoNLL).

Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3.

Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Technol-
ogy and North American Chapter of the Association
for Computational Linguistics (HLT-NAACL).

James Woodward. 2008. Causation and Manipulation.
Online Encyclopedia of Philosophy.

Appendix A. Semantic Classes of Nouns

This appendix presents the FrameNet labels we as-
sign to cnp and ¬cnp classes (see section 3.2).

56



Semantic Class FrameNet Labels
cnp Event, Goal, Purpose, Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Char-

acterization, Coordinated event, Final state, Information, Topic, Containing event, Mental content, Ac-
tion, Experience, Impactee, Impactor, Message, Question, Circumstances, Desired goal, Explanation,
Required situation, Complaint, Content, Activity, Intended goal, Phenomenon, State, Dependent state,
Forgery, Purpose of Event, Negative consequences, Inference, Appraisal, Noisy event, Function, Evi-
dence, Process, Paradigm, Standard, Old order, Focal occasion, Landmark occasion, resulting action,
Victim, Issue, Effect, State of affairs, Cause of shine, Qualification, Undesirable Event, Skill, Precept,
Outcome, Norm, Act, State of Affairs, Phenomenon 1, Phenomenon 2, Quality Eventuality, Expression,
Intended event, Cognate event, Epistemic stance, Goal conditions, Possession, Support Proposition,
Domain of Relevance, Charges, Idea, Initial subevent, Hypothetical event, Scene, Purpose of Goods,
Response action, Motivation, Executed, Affliction, Medication, Treatment, Stimulus, Last subevent,
Undesirable situation, Sleep state, Initial state, Enabled situation, Grinding cause, Finding, Case, Legal
Basis, Role of focal participant, Trigger, Authenticity, World state, Emotion, Emotional state, Evalua-
tion, New idea, Production, Performance, Undertaking, Destination event

¬cnp Artist, Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region, Creator, Copy,
Original, Iteration, Manner, Frequency, Agent, Body part, Depictive, Theme, Subregion, Area, De-
gree, Angle, Fixed location, Path shape, Addressee, Entity, Individual 1, Individual 2, Road, Distance,
Speaker, Medium, Clothing, Wearer, Bodypart of agent, Locus, Cognizer, Salient entity, Name, Inspec-
tor, Ground, Unwanted entity, Location of inspector, Researcher, Population, Searcher, Sought entity,
Instrument, Created entity, Components, Forgoer, Desirable, Bad entity, Dodger, Experiencer, Vehicle,
Self mover, Speed, Cotheme, Consecutive, Re encoding, Supplier, Individuals, Driver, Complainer,
Communicator, Protagonist, Attribute, Final value, Item, Initial value, Difference, Group, Value range,
Co participant, Perceiver agentive, Target symbol, Location of perceiver, Location, Expected entity,
Focal participant, Time of Event, Variable, Limits, Limit1, Limit2, Point of contact, Goods, Lessee,
Lessor, Money, Rate, Unit, Reversive, Perceiver passive, Sound, Sound source, Location of source,
Fidelity, Official, Selector, Role, Concessive, New leader, Body, Old leader, Leader, Governed, Result
size, Size change, Dimension, Initial size, Elapsed time, Interval, Category, Criteria, Text, Final cor-
relate, Correlate, Initial correlate, Manipulator, Side 1, Sides, Side 2, Perpetrator, Value 1, Value 2,
Actor, Partner 2, Partner 1, Partners, Figure, Resident, Co resident, Student, Subject, Institution, Level,
Teacher, Undergoer, Subregion bodypart, Course, Owner, Defendant, Judge, Co abductee, Location
of appearance, Material, Accused, Arraign authority, Hair, Configuration, Emitter, Beam, Amount of
progress, Evaluee, Patient, Buyer, Seller, Recipient, Relay, Relative location, Connector, Items, Part
1, Part 2, Parts, Whole, Name source, Payer, Fine, Executioner, Interlocutor 1, Interlocutor 2, Inter-
locutors, Healer, Food, Cook, Container, Heating instrument, Temperature setting, Resource controller,
Resource, Donor, Constant location, Carrier, Sender, Co theme, Transport means, Holding location,
Rope, Knot, Handle, Containing object, Fastener, Enclosed region, Container portal, Aggregate, Sus-
pect, Authorities, Offense, Source of legal authority, Ingestor, Ingestibles, Sleeper, Pieces, Goal area,
Period of iterations, Mode of transportation, Produced food, Ingredients, Cognizer agent, Excreter,
Excreta, Air, Perceptual source, Escapee, Undesirable location, Evader, Capture, Pursuer, Amount of
discussion, Means of communication, Periodicity, Author, Honoree, Reader, Child, Mother, Father,
Egg, Flammables, Flame, Kindler, Mass theme, Address, Intermediary, Communication, Location of
communicator, Firearm, Indicated entity, Hearer, Sub region, Member, Object, Organization, Guardian,
New Status, Arguer, Criterion, Liquid, Impactors, Force, Coparticipant, Holding Location, Legal basis,
Precipitation, Quantity, Voice, Duration of endstate, Period of Iterations, Employer, Employee, Task,
Position, Compensation, Field, Place of employment, Amount of work, Contract basis, Recipients, Hot
Cold source, Temperature goal, Temperature change, Hot/Cold source, Dryee, Temperature, Traveler,
Iterations, Baggage, Deformer, Resistant surface, Fluid, Injured Party, Avenger, Injury, Punishment,
Offender, Grinder, Profiled item, Standard item, Profiled attribute, Standard attribute, Extent, Source
emitter, Emission, Sub source, Item 1, Item 2, Parameter, Form, Chosen, Change agent, Injuring entity,
Severity, Substance, Delivery device, Entry path, Wrong, Amends, Grounds, Expressor, Basis, Signs,
Manufacturer, Product, Factory, Consumer, Interested party, Performer1, Performer2, Whole patient,
Destroyer, Exporting area, Importing area, Accuracy, Time of Eventuality, Indicator, Indicated, Au-
dience, Valued entity, Journey, Duration of end state, Killer, Beneficiary, Destination time, Landmark
time, Seat of emotion, Arguers, Arguer1, Arguer2, Company, Asset, Origin, Sound maker, Static object,
Themes, Heat source, Following distance, Perceiver, Intended perceiver, Location of expressor, Path of
gaze, Relatives, Final temperature, Particular iteration, Participant 1, Language

Table 3: This table presents the FrameNet labels we assign to cnp and ¬cnp classes.

57


