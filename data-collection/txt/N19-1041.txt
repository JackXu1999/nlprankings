



















































ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems


Proceedings of NAACL-HLT 2019, pages 430–436
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

430

ReWE: Regressing Word Embeddings
for Regularization of Neural Machine Translation Systems

Inigo Jauregi Unanue1,2, Ehsan Zare Borzeshi∗,2, Nazanin Esmaili2, Massimo Piccardi1
1 University of Technology Sydney, Sydney, Australia

2 Capital Markets Cooperative Research Centre, Sydney, Australia

{ijauregi,ezborzeshi,nesmaili}@cmcrc.com
massimo.piccardi@uts.edu.au

Abstract

Regularization of neural machine translation
is still a significant problem, especially in
low-resource settings. To mollify this prob-
lem, we propose regressing word embeddings
(ReWE) as a new regularization technique in
a system that is jointly trained to predict the
next word in the translation (categorical value)
and its word embedding (continuous value).
Such a joint training allows the proposed sys-
tem to learn the distributional properties rep-
resented by the word embeddings, empirically
improving the generalization to unseen sen-
tences. Experiments over three translation
datasets have showed a consistent improve-
ment over a strong baseline, ranging between
0.91 and 2.54 BLEU points, and also a marked
improvement over a state-of-the-art system.

1 Introduction

The last few years have witnessed remarkable
improvements in the performance of machine
translation (MT) systems. These improvements
are strongly linked to the development of neu-
ral machine translation (NMT): based on encoder-
decoder architectures (also known as seq2seq),
NMT can use recurrent neural networks (RNNs)
(Sutskever et al., 2014; Cho et al., 2014; Wu et al.,
2016), convolutional neural networks (CNNs)
(Gehring et al., 2017) or transformers (Vaswani
et al., 2017) to learn how to map a sentence from
the source language to an adequate translation in
the target language. In addition, attention mecha-
nisms (Bahdanau et al., 2015; Luong et al., 2015)
help soft-align the encoded source words with the
predictions, further improving the translation.

NMT systems are usually trained via maxi-
mum likelihood estimation (MLE). However, as

∗∗ The author has changed affiliation to Microsoft af-
ter the completion of this work. His new email is:
Ehsan.ZareBorzeshi@microsoft.com

LSTM

sj
Linear Softmax

ReWE

...

s_dim

vocab_dim

word_emb_dim

pj

ej

Decoder

Figure 1: The proposed regularizer: the hidden vec-
tor in the decoder, sj , transits through two paths: 1)
a linear and a softmax layers that output vector vj (vo-
cab dim) which is used for predicting the target word as
usual, and 2) a two-layer network (ReWE) that outputs
a vector, ej , of word embedding size (word emb dim).
During training, ej is used in a regressive loss with the
ground-truth embedding.

pointed out by (Elbayad et al., 2018), MLE suf-
fers from two obvious limitations: the first is that
it treats all the predictions other than the ground
truth as equally incorrect. As a consequence, syn-
onyms and semantically-similar words — which
are often regarded as highly interchangeable with
the ground truth — are completely ignored dur-
ing training. The second limitation is that MLE-
trained systems suffer from “exposure bias” (Ben-
gio et al., 2015; Ranzato et al., 2015) and do
not generalize well over the large output space
of translations. Owing to these limitations, NMT
systems still struggle to outperform other tradi-
tional MT approaches when the amount of super-
vised data is limited (Koehn and Knowles, 2017).

In this paper, we propose a novel regulariza-
tion technique for NMT aimed to influence model
learning with contextual properties. The technique
— nicknamed ReWE from “regressing word em-
bedding” — consists of modifying a conventional
seq2seq decoder to jointly learn to a) predict the
next word in the translation (categorical value), as



431

usual, and b) regress its word embedding (numer-
ical value). Figure 1 shows the modified decoder.
Both predictions are incorporated in the training
objective, combining standard MLE with a con-
tinuous loss function based on word embeddings.
The rationale is to encourage the system to learn to
co-predict the next word together with its context
(by means of the word embedding representation),
in the hope of achieving improved generalization.
At inference time, the system operates as a stan-
dard NMT system, retaining the categorical pre-
diction and ignoring the predicted embedding. We
qualify our proposal as a regularization technique
since, like any other regularizers, it only aims to
influence the model’s training, while leaving the
inference unchanged. We have evaluated the pro-
posed system over three translation datasets of dif-
ferent size, namely English-French (en-fr), Czech-
English (cs-en), and Basque-English (eu-en). In
each case, ReWE has significantly outperformed
its baseline, with a marked improvement of up to
2.54 BLEU points for eu-en, and consistently out-
performed a state-of-the-art system (Denkowski
and Neubig, 2017).

2 Related work

A substantial literature has been devoted to im-
proving the generalization of NMT systems.
Fadaee et al. (2017) have proposed a data augmen-
tation approach for low-resource settings that gen-
erates synthetic sentence pairs by replacing words
in the original training sentences with rare words.
Kudo (2018) has trained an NMT model with dif-
ferent subword segmentations to enhance its ro-
bustness, achieving consistent improvements over
low-resource and out-of-domain settings. Zhang
et al. (2018) have presented a novel regulariza-
tion method that encourages target-bidirectional
agreement. Other work has proposed improve-
ments over the use of a single ground truth for
training: Ma et al. (2018) have augmented the
conventional seq2seq model with a bag-of-words
loss under the assumption that the space of cor-
rect translations share similar bag-of-words vec-
tors, achieving promising results on a Chinese-
English translation dataset; Elbayad et al. (2018)
have used sentence-level and token-level reward
distributions to “smooth” the single ground truth.
Chousa et al. (2018) have similarly leveraged a
token-level smoother.

In a recent paper, Denkowski and Neubig

(2017) have achieved state-of-the-art translation
accuracy by leveraging a variety of techniques
which include: dropout (Srivastava et al., 2014),
lexicon bias (Arthur et al., 2016), pre-translation
(Niehues et al., 2016), data bootstrapping (Chen
et al., 2016), byte-pair encoding (Sennrich et al.,
2016) and ensembles of independent models
(Rokach, 2010).

However, to our knowledge none of the men-
tioned approaches have explicitly attempted to
leverage the embeddings of the ground-truth to-
kens as targets. For this reason, in this paper we
explore regressing toward pre-trained word em-
beddings as an attempt to capture contextual prop-
erties and achieve improved model regularization.

3 Model

3.1 Seq2seq baseline
The model is a standard NMT model with atten-
tion in which we use RNNs for the encoder and de-
coder. Following the notation of (Bahdanau et al.,
2015), the RNN in the decoder generates a se-
quence of hidden vectors, {s1, . . . , sm}, given the
context vector, the previous hidden state sj−1 and
the previous predicted word yj−1:

sj = decrnn(sj−1, yj−1, cj) j = 1, . . . ,m (1)

where y0 and s0 are initializations for the state and
label chains. Each hidden vector sj (of parameter
size S) is then linearly transformed into a vector of
vocabulary size, V , and a softmax layer converts
it into a vector of probabilities (Eq. 2), where W
(a matrix of size V × S) and b (a vector of size
V × 1) are learnable parameters. The predicted
conditional probability distribution over the words
in the target vocabulary, pj , is given as:

pj = softmax(Wsj + b) (2)

As usual, training attempts to minimize the neg-
ative log-likelihood (NLL), defined as:

NLLloss = −
m∑
j=1

log(pj(yj)) (3)

where pj(yj) notes the probability of ground-truth
word yj . The NLL loss is minimized when the
probability of the ground truth is one and that of
all other words is zero, treating all predictions dif-
ferent from the ground truth as equally incorrect.



432

3.2 ReWE
Pre-trained word embeddings (Pennington et al.,
2014; Bojanowski et al., 2017; Mikolov et al.,
2013) capture the contextual similarities of words,
typically by maximizing the probability of word
wt+k to occur in the context of center word wt.
This probability can be expressed as:

p(wt+k|wt), − c ≤ k ≤ c, k 6= 0
t = 1, . . . , T

(4)

where c is the size of the context and T is the total
number of words in the training set. Traditionally,
word embeddings have only been used as input
representations. In this paper, we instead propose
using them in output as part of the training objec-
tive, in the hope of achieving regularization and
improving prediction accuracy. Building upon the
baseline model presented in Section 3.1, we have
designed a new “joint learning” setting: our de-
coder still predicts the probability distribution over
the vocabulary, pj (Eq. 2), while simultaneously
regressing the same shared sj to the ground-truth
word embedding, e(yj). The ReWE module con-
sists of two linear layers with a Rectified Linear
Unit (ReLU) in between, outputting a vector ej of
word embedding size (Eq. 5). Please note that
adding this extra module adds negligible compu-
tational costs and training time. Full details of this
module are given in the supplementary material.

ej = ReWE(sj)
= W2(ReLU(W1sj + b1)) + b2

(5)

The training objective is a numerical loss, l (Eq.
6), computed between the output vector, ej , and
the ground-truth embedding, e(yj):

ReWEloss = l(ej , e(yj)) (6)

In the experiment, we have explored two cases
for the ReWEloss: the minimum square error
(MSE)1 and the cosine embedding loss (CEL)2.
Finally, the NLLloss and the ReWEloss are com-
bined to form the training objective using a posi-
tive trade-off coefficient, λ:

Loss = NLLloss + λReWEloss (7)

As mentioned in the Introduction, at inference
time we ignore the ReWE output, ej , and the
model operates as a standard NMT system.

1https://pytorch.org/docs/stable/nn.html#torch.nn.
MSELoss

2https://pytorch.org/docs/stable/nn.html#torch.nn.
CosineEmbeddingLoss

Dataset Size Sources
IWSLT16 en-fr 219, 777 TED talks
IWSLT16 cs-en 114, 243 TED talks
WMT16 eu-en 89, 413 IT-domain data

Dataset Validation set Test set
en-fr TED test 2013+2014 TED test 2015+2016
cs-en TED test 2012+2013 TED test 2015+2016
eu-en Sub-sample of PaCo IT-domain test

Table 1: Top: parallel training data. Bottom: validation
and test sets.

4 Experiments

We have developed our models building upon the
OpenNMT toolkit (Klein et al., 2017)3. For train-
ing, we have used the same settings as (Denkowski
and Neubig, 2017). We have also explored the use
of sub-word units learned with byte pair encoding
(BPE) (Sennrich et al., 2016). All the preprocess-
ing steps, hyperparameter values and training pa-
rameters are described in detail in the supplemen-
tary material to ease reproducibility of our results.

We have evaluated these systems over three
publicly-available datasets from the 2016 ACL
Conference on Machine Translation (WMT16)4

and the 2016 International Workshop on Spoken
Language Translation (IWSLT16)5. Table 1 lists
the datasets and their main features. Despite hav-
ing nearly 90,000 parallel sentences, the eu-en
dataset only contains 2,000 human-translated sen-
tences; the others are translations of Wikipedia
page titles and localization files. Therefore, we
regard the eu-en dataset as very low-resource.

In addition to the seq2seq baseline, we have
compared our results with those recently reported
by Denkowski and Neubig for non-ensemble mod-
els (2017). For all models, we report the BLEU
scores (Papineni et al., 2002), with the addition
of selected comparative examples. Two con-
trastive experiments are also added in supplemen-
tary notes.

4.1 Results

As a preliminary experiment, we have carried out a
sensitivity analysis to determine the optimal value
of the trade-off coefficient, λ (Eq. 6), using the
en-fr validation set. The results are shown in Fig-
ure 2, where each point is the average of three runs
trained with different seeds. The figure shows that

3Our code can be found at:
https://github.com/ijauregiCMCRC/ReWE NMT

4WMT16: http://www.statmt.org/wmt16/
5IWSLT16: https://workshop2016.iwslt.org/



433

Models en-fr cs-en eu-en
Word BPE Word BPE Word BPE

(Denkowski and Neubig, 2017) 33.60 34.50 21.00 22.60
(Denkowski and Neubig, 2017) + Dropout 34.5 34.70 21.4 23.60
(Denkowski and Neubig, 2017) + Lexicon 33.9 34.80 20.6 22.70
(Denkowski and Neubig, 2017) + Pre-translation N/A 34.90 N/A 23.80
(Denkowski and Neubig, 2017) + Bootstrapping 34.40 35.20 21.60 23.60
Our baseline 34.16 34.09 20.57 22.69 12.14 17.17
Our baseline + ReWE (CEL) (λ = 20) 35.52 35.22 21.83 23.60 13.73 19.71

Table 2: BLEU scores over the test sets. Average of 10 models independently trained with different seeds.

Figure 2: BLEU scores of three models over the en-
fr validation set for different λ values: baseline (red,
dashed), baseline + ReWE (MSE) (green, •), baseline
+ ReWE (CEL) (blue,×). Each point in the graph is an
average of 3 independently trained models.

the MSE loss has outperformed slightly the base-
line for small values of λ (< 1), but the BLEU
score has dropped drastically for larger values.
Conversely, the CEL loss has increased steadily
with λ, reaching 38.23 BLEU points for λ = 20,
with a marked improvement of 1.53 points over
the baseline. This result has been encouraging and
therefore for the rest of the experiments we have
used CEL as the ReWEloss and kept the value of
λ to 20. In Section 4.3, we further discuss the be-
havior of CEL and MSE.

Table 2 reports the results of the main experi-
ment for all datasets. The values of our experi-
ments are for blind runs over the test sets, averaged
over 10 independent runs with different seeds. The
results show that adding ReWE has significantly
improved the baseline in all cases, with an aver-
age of 1.46 BLEU points. In the case of the eu-en
dataset, the improvement has reached 2.54 BLEU
points. We have also run unpaired t-tests between
our baseline and ReWE, and the differences have
proved statistically significant (p-values < 0.05)
in all cases. Using BPE has proved beneficial for
the cs-en and eu-en pairs, but not for the en-fr

Src: Hautatu Kontrol panela → Programa
lehenetsiak , eta aldatu bertan .

Ref: Go to Control Panel → Default pro-
grams , and change it there .

Baseline: Select the Control Panel → program ,
and change .

Baseline + ReWE: Select the Control Panel→ Default Pro-
gram , and change it .

Table 3: Translation example from the eu-en test set.

pair. We speculate that English and French may
be closer to each other at word level and, there-
fore, less likely to benefit from the use of sub-word
units. Conversely, Czech and Basque are morpho-
logically very rich, justifying the improvements
with BPE.

Table 2 also shows that our model has outper-
formed almost all the state-of-the-art results re-
ported in (Denkowski and Neubig, 2017) (dropout,
lexicon bias, pre-translation, and bootstrapping),
with the only exception of the pre-translation case
for the cs-en pair with BPE. This shows that the
proposed model is competitive with contemporary
NMT techniques.

4.2 Qualitative comparison

To further explore the improvements obtained
with ReWE, we have qualitatively compared sev-
eral translations provided by the baseline and the
baseline + ReWE (CEL), trained with identical
seeds. Overall, we have noted a number of in-
stances where ReWE has provided translations
with more information from the source (higher ad-
equacy). For reasons of space, we report only one
example in Table 3, but more examples are avail-
able in the supplementary material. In the exam-
ple, the baseline has chosen a generic word, “pro-
gram”, while ReWE has been capable of correctly
predicting “Default Program” and being specific
about the object, “it”.



434

Figure 3: Plot of the values of various loss functions during training of our model over the en-fr training set: green,
•: training loss (NLL + (λ = 20) ReWE (CEL); Eq.7); red, +: NLL loss; blue, dashed: ReWE (CEL) loss;
magenta,×: ReWE (CEL) loss scaled by λ = 20. Each point in the graph is an average value of the corresponding
loss over 25,000 sentences.

4.3 Discussion
To further explore the behaviour of the ReWE loss,
Figure 3 plots the values of the NLL and ReWE
(CEL) losses during training of our model over
the en-fr training set. The natural values of the
ReWE (CEL) loss (blue, dashed) are much lower
than those of the NLL loss (red, +), and thus its
contribution to the gradient is likely to be limited.
However, when scaled up by a factor of λ = 20
(magenta, ×), its influence on the gradient be-
comes more marked. Empirically, both the NLL
and ReWE (CEL) losses decrease as the training
progresses and the total loss (green, •) decreases.
As shown in the results, this combined training ob-
jective has been able to lead to improved transla-
tion results.

Conversely, the MSE loss has not exhibited a
similarly smooth behaviour (supplementary mate-
rial). Even when brought to scale with the NLL
loss, it shows much larger fluctuations as the train-
ing progresses. In particular, it shows major in-
creases at the re-starts of the optimizer for the sim-
ulated annealing that are not compensated for by
the rest of the training. It is easy to speculate that
the MSE loss is much more sensitive than the co-
sine distance to the changes in the weights caused
by dropout and the re-starts. As such, it seems less
suited for use as training objective.

5 Conclusion

In this paper, we have proposed a new regulariza-
tion technique for NMT (ReWE) based on a joint

learning setting in which a seq2seq model simul-
taneously learns to a) predict the next word in the
translation and b) regress toward its word embed-
ding. The results over three parallel corpora have
shown that ReWE has consistently improved over
both its baseline and recent state-of-the-art results
from the literature. As future work, we plan to ex-
tend our experiments to better understand the po-
tential of the proposed regularizer, in particular for
unsupervised NMT (Artetxe et al., 2018; Lample
et al., 2018).

Acknowledgments

We would like to acknowledge the financial sup-
port received from the Capital Markets Coopera-
tive Research Centre (CMCRC), an industry-led
research initiative of the Australian Government.
We would also like to thank Ben Hachey, Michael
Nolan and Nadia Shnier for their careful reading
of our paper and their insightful comments. Fi-
nally, we are grateful to the anonymous reviewers
for all their comments and suggestions.

References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and

Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. In International Conference on
Learning Representations.

Philip Arthur, Graham Neubig, and Satoshi Nakamura.
2016. Incorporating discrete translation lexicons
into neural machine translation. In Empirical Meth-



435

ods in Natural Language Processing, pages 1557–
1567.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 1171–1179.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, pages 135–146.

Wenhu Chen, Evgeny Matusov, Shahram Khadivi,
and Jan-Thorsten Peter. 2016. Guided alignment
training for topic-aware neural machine translation.
arXiv preprint arXiv:1607.01628.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Empirical
Methods in Natural Language Processing, pages
1724–1734.

Katsuki Chousa, Katsuhito Sudoh, and Satoshi Naka-
mura. 2018. Training neural machine translation
using word embedding-based loss. arXiv preprint
arXiv:1807.11219.

Michael Denkowski and Graham Neubig. 2017.
Stronger baselines for trustable results in neural ma-
chine translation. In Proceedings of the First Work-
shop on Neural Machine Translation, pages 18–27.
Empirical Methods in Natural Language Processing.

Maha Elbayad, Laurent Besacier, and Jakob Verbeek.
2018. Token-level and sequence-level loss smooth-
ing for rnn language models. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics, pages 2094–2103.

Marzieh Fadaee, Arianna Bisazza, and Christof Monz.
2017. Data augmentation for low-resource neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, pages 567–573.

Jonas Gehring, Michael Auli, David Grangier, and
Yann N Dauphin. 2017. A convolutional encoder
model for neural machine translation. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, pages 123–135.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M Rush. 2017. Opennmt:
Open-source toolkit for neural machine translation.
arXiv preprint arXiv:1701.02810.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39. Association for Compu-
tational Linguistics.

Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiple
subword candidates. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics, pages 66–75.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018.
Phrase-based & neural unsupervised machine trans-
lation. Empirical Methods in Natural Language
Processing, pages 5039–5049.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Empirical
Methods in Natural Language Processing, pages
1412–1421.

Shuming Ma, Xu Sun, Yizhong Wang, and Junyang
Lin. 2018. Bag-of-words as target for neural ma-
chine translation. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics, pages 332–338.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex
Waibel. 2016. Pre-translation for neural machine
translation. arXiv preprint arXiv:1610.05243.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Empirical Methods in Natural
Language Processing, pages 1532–1543.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. In Advances in
Neural Information Processing Systems.

Lior Rokach. 2010. Ensemble-based classifiers. Artifi-
cial Intelligence Review, 33(1-2):1–39.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1715–1725.



436

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Zhirui Zhang, Shuangzhi Wu, Shujie Liu, Mu Li, Ming
Zhou, and Enhong Chen. 2018. Regularizing neu-
ral machine translation by target-bidirectional agree-
ment. arXiv preprint arXiv:1808.04064.


