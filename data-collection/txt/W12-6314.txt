










































Rules-based Chinese Word Segmentation on MicroBlog for CIPS-SIGHAN on CLP2012


Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 74–78,
Tianjin, China, 20-21 DEC. 2012

Rules-based Chinese Word Segmentation on MicroBlog for CIPS-
SIGHAN on CLP2012 

 
 

Jing Zhang 
Dalian University of Technology,  

DaLian, P. R. China. 
zhangjingqf@mail.dlut.edu.cn 

Degen Huang 
Dalian University of Technology,  

DaLian, P. R. China. 
huangdg@dlut.edu.cn 

 

Xia Han 
Dalian University of Technology,  

DaLian, P. R. China. 
hanxia@mail.dlut.edu.cn 

Wei Wang 
Dalian University of Technology,  

DaLian, P. R. China. 
wangwei.dl@263.net 

 
  

 

Abstract 

 

In this evaluation, we have taken part in the 
task of the Word Segmentation on Chinese 
MicroBlog. In this task, after analysing the 
feature of the MicroBlog and the result of our 
original Chinese word segmentation system, 
four Optimization Rules are proposed to opti-
mize the segmentation algorithm for Chinese 
word segmentation on MicroBlog corpora. 
The optimized segmentation system is based 
on character-based and word-based Condi-
tional Random Fields (CRFs). Experiments 
show that the optimized segmentation system 
can obviously improve the performance of 
CWS on MicroBlog corpora. 

1 Introduction 

Chinese word segmentation is a crucial funda-
mental task in Chinese language processing. Af-
ter years of intensive researches, Chinese word 
segmentation has achieved a quite high perform-
ance. However, it is not so satisfying when the 
Chinese word segmentation works on MicroBlog 
corpora. This CIPS-SIGHAN-2012 bake-off task 
of Chinese word segmentation focuses on the 
performance of Chinese word segmentation algo-
rithms on MicroBlog corpora. This evaluation is 
an opened evaluation on simplified Chinese word 
segmentation task. The task provides no training 
set, and we are free to use data learned or model 
trained from any resources. 

In this evaluation task, we propose some use-
ful optimization rules for Chinese Word Segmen-
tation (CWS) on MicroBlog corpora, after ana-
lysing the results of segmentation on MicroBlog 
corpora by our original CWS system, which 
combines character-based and word-based Con-
ditional Random Fields (CRFs).  

The rest of this paper is organized as follows. 
Section II outlines the new Chinese word seg-
mentation algorithm on MicroBlog corpora. Sec-
tion III reports the results of experiments and 
some discussions. Finally, some conclusions are 
presented in Section IV. 
 

74



2 Word Segmentation Algorithm 

2.1 Machine Learning Models 

Conditional random fields (CRFs), a statistical 
model for sequence labeling, was first introduced 
by Lafferty, McCallum and Pereira (2001). It is 
the undirected graph theory that CRFs mainly 
use to achieve global optimum sequence labeling. 
It is good enough to avoid label bias problem by 
using a global normalization. 
In previous labeling task of character-based 
CRFs, the number of the characters in the ob-
served sequence is as same as the one in the an-
notation sequence. However, for CWS task, the 
input of n-character will generate the output of 
m-word sequence on such a condition that m is 
not larger than n. But this problem can be well 
solved by word-lattice based CRFs, because the 
conditional probability of the output sequence 
depends no longer on the number of the observed 
sequence, but the words in the output path. For a 
given input sentence, its possible paths may be 
various and the word-lattice can well represent 
this phenomenon. A word-lattice can not only 
express all possible segmentation paths, but also 
reflect the different attributes of all possible 
words in the path. Zhang, Chen and Hu (2012) 
and Nakagawa (2004) have successfully used the 
word lattice in Japanese lexical analysis. 
  Our paper adopt the word-lattice based CRFs 
that combines the character-based CRFs and the 
word-based CRFs, and specifically, we put the 
candidate words selected by the character-based 
CRFs into a word-lattice, and then label all the 
candidate words in the word-lattice using word-
based CRFs model. When training the word-
lattice based CRFs model, the maximum likeli-
hood estimation is used in order to avoid over-
loading. And Viterbi algorithm is utilized in the 
decoding process which is similar with (Huang 
and Tong, 2012). 

2.2 Feature Templates 

The character-based CRFs in our method adopt a 
6-tag set in (Kudo, Yamamoto and Matsumoto, 
2004), and its feature template comes from 
(Huang and Tong, 2010), including C-1, C0, C1, 
C-1C0, C0C1, C-1C1 and T-1T0T1, in which C 
stands for a character and T stands for the type of 
characters, such as Number, String. Character 
and so on, and the subscripts -1, 0 and 1 stand for 
the previous, current and next character, respec-
tively. Four categories of character sets are pre-
defined as: Numbers, Letters, Punctuation and 
Chinese characters. The feature templates of the 

character-based CRFs are described in detail in 
Table 1. 
 

No. Feature Description of Feature 
1 C0 The current character 
2 C1 The later character 
3 C-1 The former character 

4 C-1C0 
The former and the current 

characters 

5 C0C1 
The current and the later 

characters 

6 C-1C1 
The former and the later 

characters 

7 C-1C0C1 
The former, current and the 

later characters 

8 T-1T0T1 
The type of the former, cur-
rent and the later characters 

 
Table 1: The feature templates of the character-based 

CRFs 
 
Two kinds of features are selected for the word-
based CRFs, like (Huang and Tong, 2012): uni-
gram features and bigram features. The unigram 
ones only consider the attributes information of 
current word, and bigram ones are also called 
compound features, which utilize contextual in-
formation of multiple words. Theoretically, the 
current word’s context sliding window can be 
infinitely large, but due to efficiency factors, we 
define the sliding window as 2. The specific fea-
tures are W0, T0, W0T0, W0T1, T0T1, W0W1, 
where W stands for the morphology of the word, 
T stands for the part-of-speech of the words, and 
subscript 0 and subscript 1, respectively, stand 
for the former and the latter of two adjacent 
words. Furthermore, the Accessor Variety (AV) 
in (Zhao, Huang and Li, 2006) is applied as 
global feature. The feature templates of the 
word-based CRFs are shown in Table 2. 
 

No. Feature Description of Feature 
1 W0 The current word 

2 T0 
The POS of the  current 

word 

3 T-1T0 
The POS of the former 

and the current words 

4 T0T1 
The POS of the current 
and the later words 

 
Table 2: The feature templates of the word-based 

CRFs 
 

 

75



2.3 Optimization Rules 

As we all know, there exist plenty of new words, 
a great variety of symbols, and a good deal of 
URLs in MicroBlog corpora. Those features 
bring a big challenge to Chinese word segmenta-
tion.  Considering the features of MicroBlog cor-
pora and the segmentation result of our original 
Chinese word segmentation system, we propose 
several rules to optimize the segmentation result 
on MicroBlog corpora.  
The features of MicroBlog corpora we summa-
rized is as follows: 
I. There are a lot of new words in MicroBlog, 

such as "团购" tuan-gou (online shopping), "
点评网" dian-ping-wang (HankowThames), 
"有木有" you-mu-you (yes or not) and so on.  

II. Many kinds of special symbols are used in 
MicroBlog, and what we deal with is mainly 
included in the following three cases: 
A. All kinds of combinations of the 

punctuation, especially, "！", " 。", 
"-", for example,  "其实应该很开心 
的呀 ！  ！ ！ ！" (Actually we are 
suposed to be very happy!!!!), "我 
要 虚 脱 了 。 。 。" (I am ex-
hausted。。。). 

B. The frequently use of "@", e.g. "@姚
晨" @-yao-chen. 

C. There also exist large number of 
emoticon icons, for instance, "^_^", 
"→_→" and so on.  

III. The expression forms of time or date are 
quite various. 

IV. The vast majority of the MicroBlog have 
URLs.  

Our original segmentation system does not solve 
those problems mentioned above very well. 
Therefore, considering these characteristics of 
the MicroBlog, we propose some optimization 
rules to optimize the original results, which fi-
nally improve the segmentation results.  
The rules are described in detail as follows: 

Optimization Rule 1: With regards to the first 
feature, we use the contextual information, which 
is described in detail in (Huang and Tong, 2012) 
to calculate the frequency of the new words, and 
then added the high-frequency words to the dic-
tionary. 

Optimization Rule 2: According to the second 
feature, we have collected some commonly used 
combinations of punctuations to the dictionary. 

Optimization Rule 3: Considering the third 
feature, the original system can not deal with the 

string of time very well, for instance,  "2012年
11 月 8 日" (November 8, 2012), the string of 
time is segmented as  "2012/年 /11/月 /8/日", 
while the correct segmentation is "2012年/11月
/8日". Under this circumstance, we have built a 
set of Time Templates. If the string matches any 
of the Time Templates, it will be segment as 
Time.  

Optimization Rule 4: As to the last point, first, 
we search for the key word "http", and then we 
look for the right boundary of the URLs. At last, 
we merge all the string between the "http" and 
the right boundary together.  

 

2.4 Word Segmentation Process 

The Process of the optimized segmentation sys-
tem is as follows: 

Step1. Collect the commonly used combinations 
of punctuations to the dictionary which is mentioned 
in Rule 2.  

 
Step2. Put all the candidate words in 3-Best 

paths selected by the character-based CRFs 
model into the word-lattice. 

 
Step3. To build the word-lattice, in other word, 

give properties and costs to each node, the can-
didate words selected by character-based CRFs 
in Step2, in the word-lattice, which is divided 
into four cases to deal with: 

 ①If the candidate words are in the system 
dictionary, then assign the properties and cost of 
the words in the system dictionary directly to the 
candidate words in the word-lattice. 

 ②If the candidate words are not in the system 
dictionary, then we use Optimization Rule 1, 
search the dictionary of contextual information, 
if it is in there, then the properties of the words in 
the contextual information dictionary will be as-
siged to the candidate words, and a weight value, 
calculated by Eq. (1), will be added to the cost of 
the candidate words. 
 

 

0

0

1.0
( )

1
( )

0.2
0.8 ( )

log( 2)

cost w rNum
rNum

cost w
cost w rNum

frequency

 × += 
  + ×  + 

’

　　　   >0

　 =0

 (1) 

Where w stands for the word, and t on behalf of 
the Part of Speech (POS), and Cost represents  
the difficulty of the emerging of a candidate 

76



word, and Frequency delegates the frequency of 
being a candidate word, and rNum is in the name 
of the frequency of being the node in the final 
segmentation path. Besides, cost0 (w) stands for 
the original cost of the words. 

 
③If the candidate words is not in the system 

dictionary, neither in the contextual information 
dictionary, then we will search the synonyms 
forest to find a synonym of the candidate words. 
If the synonym exits in the system dictionary, 
we’d like to replace the candidate word with it.  

④If the above cases are not suitable for the 
candidate words, then the candidate words will 
be classified according to the classification men-
tioned above. 

 
Step4. To find the optimal path, the least 

costly path of word segmentation, in the word-
lattice using the Viterbi algorithm according to 
Eq. (4), and the values of TransCost(ti,ti+1) and 
Cost(wi) can be calculated by Eq. (2) and Eq. (3), 
respectively. Since all feature functions are bi-
nary ones, the cost of the word is equal to the 
sum of all the weight of the unigram features 
about the word, and the transition cost is equal to 
the sum of all bigram features about the two 
parts of speech. 

( )

( )
k

k

f
f U w

Cost w factor λ
∈

= − ∗ ∑
 (2) 

1 2
( , )1 2

( , )
kf

f B t tk

TransCost t t factor λ
∈

=− ∗ ∑
 (3) 

Where U(w) is the unigram feature set of the cur-
rent word, B(t1, t2) is the bigram feature set of the 
adjacent words t1 and t2. λfk is the weight of the 
corresponding feature fk and factor is the amplifi-
cation coefficient.  

#

1
0

( ) Cos ( , ) ( )
Y

i i i
i

Score Y Trans t t t Cost w+
=

= +∑（ ）
(4) 

It can be seen from the above process that the 
factors of recognizing the territorial words are 
considered in Step3. Contextual information as 
well as synonym information is used to adjust the 
cost and the properties of the candidate words in 
the path, which can contribute to the follow-up 
Step4 to select the best path.  

Step5. To optimize the original segmentation 
results. Optimization Rule 1 and Optimization 
Rule 2 have been used in the previous steps, 

while Optimization Rule 3 and Optimization 
Rule 4 are utilized in the end. The purpose of 
these two rules is to revise the segmentation re-
sults. In another word, some errors in the seg-
mentation results can be corrected by Rule 3 and 
Rule 4. 
 

3 Experiment Results  

3.1 Data Sets 

Our method is tested on the simplified Chinese 
MicroBlog testing data and the training data 
from the CIPS-SIGHAN-2012 bake-off task. The 
test corpus consists of approximately 5,000 texts 
from MicroBlog, and the training data includes 
500 texts from MicroBlog with the gold standard 
result. The experiment results are evaluated by P 
(Precision), R (Recall) and F-measure. The sys-
tem dictionary we used is extracted from the 
People’s Daily from January to June, in 2000, 
containing 85000 words, with the POS. The 
word-based CRFs model is trained by the corpus 
with POS tag which is from the People's Daily of 
January, in 1998). 

3.2 Evaluation Metrics 

The metrics we used in this bake-off task is as 
follows: 

 

%100*
Num2
Num1

Precision =  

 

%100*
Num3
Num1

callRe =  

 

100%*
callRePrecision

callRe*Precision*2
-

+
=measureF

 
Num1 means the number of words correctly 

segmented. 
Num2 stands for the number of words seg-

mented.  
Num3 means the number of words in the ref-

erence.  

3.3 Experimental Results 

 
Test Track P R F 

Base500 78.76 88.59 83.39 
Final500 83.50 89.21 86.26 
Final5000 83.35 89.43 86.28 

 
Table 3: The result of the experiments 

77



In our experiments, at first, we use our original 
Chinese word segmentation system as the Base-
line, and the 500 MicroBlog corpora provided by 
the organization are used as the test corpora. The 
segmentation result is shown in the first row of 
Table 3.   
   After that, in order to compare with the Base-
line, we use the segmentation system added the 
optimization rules segments the 500 MicroBlog 
corpora, and the second row of Table 3 shows 
the result of this experiment. From the result we 
can see that our optimization works very well, 
and the F-measure is promoted obviously. 
   At last, we use the 5000 MicroBlog corpora to 
test our Final system, the segmentation system 
added the optimization rules, and we can see the 
result from the last row of Table 3, having the 
similar promotion with the second row. 
   From the above, we can clearly get that our 
optimized segmentation system can promote the 
segmentation performance significantly. 
 

3.4 Error Analysis 

Although the optimization rules improve the 
segmentation performance significantly, several 
typical errors are observed in the results of the 
experiment. 

First, those problems we mentioned above are 
not solved thoroughly, especially the variety of 
punctuation problems. Because the combination 
is so flexible to sum up, we just summarize some 
frequently used combinations of punctuations. 

Second, there still exist many new words 
which occur just a few times in the corpora, so 
they have not been added into the system dic-
tionary eventually. 

4 Conclusions 

In this evaluation task, according to the features 
of MicroBlog, we propose several optimization 
rules of Chinese word segmentation on MicroB-
log corpora. In the processing, experiments show 
that those optimization rules works very well on 
this task. While there still exit amount of prob-
lems need to be solved when Chinese word seg-
mentation works on MicroBlog, and we have a 
lot of works to do in the future. 

 
Acknowledgments 
This work has been supported by the National 
Natural Science Foundation of China 
(No.61173100, No.61173101, No.61272375), 
Fundamental Research Funds for the Central 

Universities (DUT10RW202). The authors wish 
to thank Wu Qiong, Wang Dandan and for their 
useful suggestions, comments and help during 
the design and editing of the manuscript. 

References  

Huang Degen and Tong Deqin. 2012. Context Infor-
mation and Fragments Based Cross-Domain Word 
Segmentation. J. China Communications, 9 (3): 49-
57 

Huang Degen, Tong Deqin, and Luo Yanyan. 2010. 
HMM Revises Low Marginal Probability by CRF 
for Chinese Word Segmentation. Proc of CIPS-
SIGHAN Joint Conference on Chinese Processing. 
216-220. ACL, Beijing 

Kudo T, Yamamoto K, and Matsumoto Y. 2004. Ap-
plying conditional random fields to Japanese mor-
phological analysis. Proc of EMNLP2004. 230-237. 
ACL, Barcelona 

Lafferty J, McCallum A, and Pereira F. 2001. Condi-
tional Random Fields: probabilistic models for 
segmenting and labeling sequence data. Proceed-
ings of ICML2001. 282-289. Morgan Kaufmann, 
San Francisco 

Nakagawa T. 2004. Chinese and Japanese word seg-
mentation using word-level and character-level in-
formation. Proc of COLING 2004. 466-472. ACL, 
Geneva 

Zhang Chongyang, Chen Zhigang, and Hu Guoping. 
2012. A Chinese Word Segmentation System 
Based on Structured Support Vector Machine 
Utilization of Unlabeled Text Corpus. Proc of 
CIPS-SIGHAN Joint Conference on Chinese Proc-
essing. 221-227. ACL, Beijing 

 

Zhao Hai, Huang Changning, and Li Mu, et al. 2006. 
Effective tag set selection in Chinese word seg-
mentation via Conditional Random Field modeling. 
In PACLIC-20. 87-94. ACL, Wuhan  

78


