



















































Using LSTMs to Assess the Obligatoriness of Phonological Distinctive Features for Phonotactic Learning


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1595–1605
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1595

Using LSTMs to Assess the Obligatoriness of Phonological Distinctive
Features for Phonotactic Learning

Nicole Mirea∗ and Klinton Bicknell‡, ∗

∗Northwestern University ‡Duolingo
nimirea@u.northwestern.edu

klinton@duolingo.com

Abstract

To ascertain the importance of phonetic in-
formation in the form of phonological dis-
tinctive features for the purpose of segment-
level phonotactic acquisition, we compare the
performance of two recurrent neural network
models of phonotactic learning: one that has
access to distinctive features at the start of
the learning process, and one that does not.
Though the predictions of both models are
significantly correlated with human judgments
of non-words, the feature-naive model signif-
icantly outperforms the feature-aware one in
terms of probability assigned to a held-out test
set of English words, suggesting that distinc-
tive features are not obligatory for learning
phonotactic patterns at the segment level.

1 Introduction

Knowing a language involves having systematic
expectations about the sequential sound patterns
within syllables and words in the language—
a sensitivity to the phonotactic generalizations
that exist in the language. This sensitivity helps
language users segment a continuous stream of
speech (Vitevitch et al., 1997), incorporate new
words into the lexicon (Storkel et al., 2006), and
reconstruct parts of an utterance that may have
been obscured by noise. However, the details of
how language learners infer these phonotactic gen-
eralizations from incoming acoustic data are still
unclear. The current project seeks to clarify the ex-
tent to which phonetic information (at the level of
phonological distinctive features) is useful for pre-
dicting upcoming phones within a word, by build-
ing computational models of phonotactic acquisi-
tion.

Phonotactic patterns are typically stated in
terms of generalizations over natural classes; for
example, voiced stops cannot follow voiceless
stops word-finally in English. These natural

classes are defined by a hierarchy or set of dis-
tinctive features that is either taken to be univer-
sal across languages (Chomsky and Halle, 1965;
Clements, 2009) or emergent from the process
of phonological acquisition—including phonotac-
tic acquisition—in a particular language (Mielke,
2008; Dresher, 2015). Nevertheless, most models
of phonotactic acquisition require that phonologi-
cal distinctive features be specified in advance of
learning. Our work interrogates this assumption
through the following questions:

1. Is external information regarding phonologi-
cal distinctive features a necessary prerequi-
site for learning word-level phonotactic gen-
eralizations?

2. Must models become sensitive to phonologi-
cal properties of incoming segments in order
to represent phonotactic generalizations?

To answer them, we use recurrent neural net-
works with long short-term memory (LSTM)
nodes, which have shown considerable success in
learning patterns at the word (Sundermeyer et al.,
2015) and character (Kim et al., 2016) levels.
These models encode each phonetic segment in
the inventory as a vector of numbers. With ex-
posure to more training data, these representations
adapt to the task at hand: incrementally predicting
each segment in a word, given all previous seg-
ments in the word.

If phonetic segments must be specified in terms
of distinctive features in advance of phonotactic
learning, we would expect a model that encodes
phonetic segments in this manner the outset of
training to ultimately represent phonotactic gen-
eralizations more accurately than one that initially
encodes each phonetic segment as a random vector
containing no phonetic information whatsoever.
If, on the other hand, all information required to



1596

learn phonotactic generalizations is already latent
in the sequence of segments, then the featurally-
informed model should have no advantage. Al-
ternatively, initializing the model with distinctive
features might constrain it to explore a suboptimal
area of the solution space, ultimately leading to a
less accurate representation of phonotactics.

To investigate our second question, we deter-
mine whether the resultant learned encodings of
each phonetic segment reflect phonetic informa-
tion by examining the state of the models after
training. If the post-training encodings do encode
phonetic information, it would support the central-
ity of a phonetic representation of incoming acous-
tic data for phonotactic learning.

Following previous work by Futrell et al.
(2017), in Experiment 1 we evaluate how well
our models capture phonotactic generalizations by
measuring the probability they assign to unseen
words from a test corpus. In accordance with ra-
tional analysis (Anderson and Milson, 1989), we
make claims about the mind by studying the en-
vironment in which it operates, under the assump-
tion that the mind adapts to the environment in or-
der to achieve its goals—here, the goal of learn-
ing what constitutes a “likely” word-form in a lan-
guage.1 If the optimal way of achieving this re-
lies on phonological distinctive features, then we
should expect that language users do draw upon
this resource in order to infer phonotactic regular-
ities.

To verify this expectation, in Experiment 2 we
evaluate our models using the more traditional
means of assessing phonotactic learners: com-
parison with human wordlikeness ratings of non-
words. If our models have indeed learned an eco-
logically valid representation of English phonotac-
tics, we expect the probabilities that they assign to
non-words to correlate with wordlikeness ratings
assigned by English speakers.

2 Related Work

To ask whether distinctive features are helpful for
phonotactic generalization, it is first essential to
establish what form these phonotactic generaliza-
tions should take. Experimental work supports a
characterization of phonotactics as gradient expec-
tations over sequences of sounds, instead of cate-

1This goal is subordinate to other goals: speech segmenta-
tion, word learning, perception of speech in noise, communi-
cation, survival, etc. We have chosen this as a tractable level
of analysis.

gorical restrictions designating certain sound se-
quences as marked. In the phonotactic learning
experiment conducted by Goldrick (2004), partic-
ipants were able to acquire feature-based phono-
tactic constraints of both gradient and categorical
forms. Gradient phonotactic sensitivity has also
been found in children’s productions (Coady and
Aslin, 2004) as well as adults’ wordlikeness judg-
ments (Frisch et al., 2000). Following this, our
model will represent gradient constraints, and its
task will be to assign gradient acceptability ratings
to sequences of phonetic segments.

Bernard (2017) demonstrated that humans are
capable of simultaneously tracking and learning
phonotactic generalizations defined at the level
of word boundaries, syllable positions, and co-
occurrences between adjacent phonetic segments.
Our LSTM networks are capable of capturing all
three types of constraints. Crucially, they are ca-
pable of representing dependencies between non-
adjacent units in a sequence (Sundermeyer et al.,
2015), which means that they can learn gradient
phonotactic constraints at both the word, syllable,
and segment level, without the need for explicit
syllable coding in the training data.

Many models have addressed the question of
how phonotactic generalizations are induced from
incoming data (Hayes and Wilson, 2008; Albright,
2009; Futrell et al., 2017)2. These vary in terms
of the algorithm that the learner uses to learn cor-
respondences between segments. Nevertheless,
most of these models of phonotactic acquisition
presuppose that incoming data is encoded in terms
of a set or hierarchy of distinctive features that
are predetermined by the researcher. Our research
questions this fundamental assumption, with po-
tential implications for these phonotactic learning
models if the assumption is unsubstantiated.

This assumption has already been challenged
by a baseline from Albright (2009), which com-
pared bigram models over distinctive features and
segments. The segmental bigram model yielded
slightly higher agreement with human wordlike-
ness judgments than the featural bigram model,
although the featural bigram model was closer to
human judgments for words containing unattested
sequences. However, these results may change for
models capable of learning generalizations across
longer units of structure; this possibility warrants
another test.

2See Daland et al. (2011) for a comprehensive review.



1597

Previous attempts to explicitly quantify the rel-
evance and “psychological accuracy” of a univer-
sal, innate set of distinctive features for phono-
tactic learning have also produced mixed results.
Mielke (2008) used a typological analysis to argue
for language-specific, learned distinctive features;
in 2012, Mielke devised another phonetic similar-
ity metric that corresponds to surface phonologi-
cal patterns roughly as well as distinctive features
do. Drawing upon this work, Dunbar et al. (2015)
compared how well featural representations de-
rived from acoustic, articulatory, and phonotac-
tic models capture phonemic distinctions in En-
glish. The phonotactic-derived feature representa-
tions performed markedly worse than the acous-
tic or articulatory representations at separating
this phonemic space, suggesting a weaker-than-
expected link between phonotactics and acous-
tic/articulatory phonetics—and indeed, between
phonotactics and the features required to distin-
guish phonemic space. Our work probes this link
in the opposite direction, questioning the extent to
which distinctive features are necessary to learn
phonotactic generalizations.

3 Model3

Our models are recurrent neural networks with
LSTM nodes. Each network’s task is to incre-
mentally predict the next phonetic segment in a se-
quence, given the beginning of the sequence as in-
put. Models were constructed using PyTorch 0.3.1
(Paszke et al., 2017).

The function and description of each layer in the
model is as follows:

3.1 Input Layer

The input layer reads in each phonetic segment is a
one-hot vector. The number of nodes in this layer
is equal to the size of the phonetic inventory—i.e.,
the number of unique phones in the corpus (with
vowels of different stress levels counted as sepa-
rate phones). For the present data, this number is
equal to 77, including start and end symbols that
delimited each word in the corpus.

3.2 Embedding Layer

The embedding layer projects each phonetic seg-
ment in the input into a continuous representation

3All source code for models, training/validation/test sets,
result files, and analysis scripts are included as supplementary
material and freely available on GitHub.

that is passed along to the recurrent layers. The
embedding layer has 68 nodes: twice the num-
ber of phonological features in the feature repre-
sentation that we chose (described in more detail
in Section 4.1). Since the input layer uses a one-
hot representation, this means that every phonetic
segment in the inventory is represented as a vec-
tor of 68 weights between the corresponding in-
put node and the embedding layer—i.e., an embed-
ding. These weights were initialized according to
the procedure described in Section 4.1. The acti-
vation function for nodes in this layer was linear,
with a bias term of 0.

3.3 Recurrent Layers

Each of the two recurrent layers of the network
consisted of 512 LSTM nodes. The number of re-
current layers, as well as the number of nodes in
each layer, were determined through extensive hy-
perparameter tuning (see Table A1 for details).

Each LSTM node receives input not only from
the embedding layer, but also from its previous
state. This allows the network to maintain a his-
tory, keeping track of the phones in the word up
to the current point. Compared to simple recur-
rent neural networks, LSTMs have proven better
at learning longer-distance dependencies, allow-
ing them to represent more complex dependen-
cies across non-adjacent timesteps (Hochreiter and
Schmidhuber, 1997).

3.4 Output Layer

The output layer is a linear decoder layer as large
as the segment inventory: 77 nodes. As in the in-
put layer, each node corresponds to a particular
phonetic segment. The output of the entire model,
then, corresponds to a probability distribution over
the next segment. This distribution is normalized
using a softmax function, and the cross-entropy
between this normalized distribution and the one-
hot vector of the actual next segment indexes the
accuracy of the model’s prediction.

4 Experiment 1: Evaluating on a
Held-Out Test Set

To investigate whether pre-specified distinctive
features are helpful for acquiring phonotactic gen-
eralizations, we created two versions of a phono-
tactic learner: one that initially represents incom-
ing phonetic segments as distinctive feature bun-
dles (a feature-aware condition), and one that ini-

https://github.com/nimirea/phonRNN


1598

tially represents phonetic segments as random vec-
tors (a feature-naive condition). Our experimen-
tal manipulation occurs in the initialization of the
weights between the input layer and the embed-
ding layer; all other parameters were held con-
stant. To compare these, we trained them on a
identical subsets of the CELEX2 corpus (Baayen
et al., 1995), and evaluated the likelihood that each
model assigned to a non-overlapping test subset
from the same corpus.

4.1 Method

Training Procedure
All models had the structure described in Section
3. Before training, the value of the weights be-
tween the input and embedding layers was deter-
mined in one of two ways, depending on the ex-
perimental condition to which the network was as-
signed:

1. Feature-aware condition: The weight vec-
tor of each phonetic segment was determined
according to its distinctive feature specifica-
tion, according to the scheme described later
in this section (see “Distinctive Features”).
Each weight was initialized as either -1, 0, or
1, depending on the phonetic segment’s value
for the feature in question.

2. Feature-naive condition: The weight vector
of each phonetic segment was populated ran-
domly from a distribution over the values -1,
0, and 1, with proportions identical to those
found in the feature-aware condition.

All other weights were initialized from a uni-
form distribution between ±h−1, where h was the
number of nodes in the subsequent layer.

All weights in the network were adjusted via
backpropagation during the course of training.
These included the weights between each layer, as
well as the weights between successive states of
the recurrent layers and those controlling each gate
of each LSTM node. The error function used for
this was cross-entropy loss, calculated over the 77
phonetic segment classes. Minimizing this cross-
entropy loss is equivalent to maximizing log like-
lihood.

Each word in the training corpus was treated
as a minibatch, with stored error backpropagated
through the network once per word using stochas-
tic gradient descent. Activations in each layer

were automatically reset after each backpropaga-
tion to random values that were generated at the
beginning of training.

Through hyperparameter tuning (detailed in Ta-
ble A1), we settled on 1.0 as a suitable value for
the initial learning rate, and annealed this by a fac-
tor of 0.25 every time there was no improvement
on the validation set. The aforementioned hyper-
parameter tuning also led us to employ a dropout
of 0.2, adjusting only 80% of the training weights
per minibatch. Each model was trained for a total
of 25 epochs (complete runs through the training
corpus), after which the iteration of the model that
assigned the highest log likelihood to the valida-
tion corpus was evaluated on the held-out test cor-
pus, and the phonetic segment embeddings were
stored for further analysis (see Section 6).

Twenty-five random initializations were trained
in both the feature-aware and feature-naive con-
ditions, for a total of 50 initializations. Within a
condition, each initialization varied with respect to
the initial weights except those between the input
layer and the embedding layer.

Data
Corpus We used a randomly selected 50,000-
lemma subset from the English part of the
phonetically-transcribed CELEX2 database
(Baayen et al., 1995) to train and test our model4.
30,000 of these lemma words were used to
train the model, and the remaining 20,000 were
randomly divided into validation and test sets
of 10,000 lemmas each. Lemmas were used
instead of inflected forms in order to minimize the
number of shared stems across the three sets.

The only preprocessing steps applied to these
data were the translation of each lemma from the
DISC notation used in CELEX2 into IPA (with
diphthongs split into separate phonetic segments,
in order to increase comparability with Futrell
et al., 2017) and the addition of start and end sym-
bols around each word. No syllabification was
added, because the models should infer the shape
of syllables from the data alone, due to their ability
to represent information across multiple timesteps.

Distinctive Features The precise distinctive
feature structure we used to initialize the phonetic
segment embeddings was based on Futrell et al.
(2017)’s hierarchical feature dependency graphs,

4The CELEX2 corpus was also the basis of Futrell et al.
(2017)’s data set.



1599

in order to compare our model to this prior work.
In these graphs, each node represents a feature,
and certain features are only defined if their an-
cestor nodes have a certain value. For example,
the “height” node is only defined if the manner
of segment at hand is “vowel”; this is because the
manner node is an ancestor of the height node.5

The first modification that we made to these fea-
ture dependency graphs is representing each mul-
tivalent feature as a binary one. This is because
the values of several features do not lie along a
straightforward unidimensional continuum. For
instance, the “manner” node specifies the manner
of a syllable, and has “trill” and “fricative” as two
of its values. These manner classes are equivalent
in terms of the size of the articulatory aperture:
their ordering along a unidimensional continuum
would be totally arbitrary. Instead, we split each
possible value of a multivalent feature into a set of
binary features, of which only one can be positive
(1) at a given time; the rest must be negative (-1),
if the feature is defined for the segment at hand.

In translating these dependency graphs into vec-
tors, we represent each feature as a pair of di-
mensions in each phonetic segment vector. The
first dimension in each pair expresses the value
of the node: positive (+1), negative (-1), or unset
(0). The second dimension in each pair denotes
whether the node is set (1) or unset (-1), allowing
for privative feature representation. This auxiliary
dimension may seem redundant, but we include it
because it is not the case that unset feature val-
ues are truly ‘intermediate’ between positive and
negative ones, as a representation without the aux-
iliary dimension would suggest. We also add an-
other two pairs of dimensions to represent start and
end symbols.

Dependent Variable
We used log likelihood on the held-out test cor-
pus of 10,000 lemmas to evaluate the quality
of our models’ phonotactic generalizations. The
more accurate a model’s representation of English
phonotactics is, the higher the likelihood it should
assign to extant English words that it has not seen.

4.2 Results

Performance of each model is plotted in Fig. 1.
Using a Wilcoxon rank sum test with a continu-

5These features are detailed further in Graff (2012),
though some have been omitted since they are not distinctive
in English.

-22

-20

-18

Feature-awareFeature-naive
Condition

Av
er

ag
e 

na
tu

ra
l l

og
 li

ke
lih

oo
d 

ov
er

 a
ll 

wo
rd

s 
in

 s
et

Word set: test (10000 words)
Average log likelihoods per model

Figure 1: Box-plot of log likelihoods per model in each
experimental condition. Each observation used to gen-
erate this plot (N = 50) is the average log likelihood
assigned to each word in the test set, for a single model.

ity correction, we find that models in the feature-
naive condition assigns a significantly higher log
likelihood to the test corpus than those in the
feature-aware condition (W = 2.43 × 1010; p <
.001). On average, the feature-aware models as-
signed a log likelihood of −20.98 to the words
in the test set, and the feature-naive models as-
signed an average log likelihood of −20.07. In
other words, the feature-naive models assigned
over twice the probability mass to the test set com-
pared to the feature-aware models, in terms of raw
(non-log) probability. The poorer performance of
the feature-aware condition suggests that distinc-
tive features need not be specified a priori of train-
ing, and that in fact they may bias the model to-
ward suboptimal solutions.

5 Experiment 2: Comparison to Human
Judgments

In an effort to validate our models externally
against evaluations that humans make, we ran
another experiment correlating our models’ log-
likelihood ratings of non-words to human word-
likeness judgments of the same non-words.

5.1 Method
Stimuli
Non-words were designed by Daland et al. (2011)
to vary in the level of sonority sequencing princi-
ple violation, and as such their form was quite con-
strained: 96 stress-initial CCVCVC non-words,
each starting with a consonant cluster that was ei-
ther unattested (18 clusters), marginally attested
(12 clusters), or frequently attested (18 clusters) as
an onset in English. No non-word had more than
one lexical neighbor, and non-words whose first



1600

or last 4 segments formed a existing word were
excluded.6

Procedure
All human data for this experiment was collected
by Daland et al. (2011). Forty-eight participants
were recruited through Amazon Mechanical Turk;
results were only retained from those reporting
high (N = 2) or native (N = 36) English profi-
ciency. Each participant performed a Likert word-
likeness rating task (1–6, where 6 was more word-
like) on all 96 stimuli, followed by a head-to-
head comparison rating task in which participants
were given two words and instructed to choose the
non-word that seemed more like a typical English
word. Each of the 4560 possible pairs was as-
signed to a single participant, and no participant
saw any non-word more than twice during this
task.

Daland et al. (2011) found that the comparison
average of each non-word (proportion of compar-
ison trials in which it was selected as better than
its competitor) correlated with its average Likert
rating across participants. However, the compar-
ison average was more sensitive in differentiating
non-words at the bottom of the Likert scale; there-
fore, we used the comparison average to evaluate
our models.

Our models were the same feature-aware and
feature-naive models from Experiment 1, trained
on the same data. After training, we calculated the
log-likelihood of each of the 96 non-word stim-
uli from Daland et al. (2011) for each of the 50
models from Experiment 1, and correlated these
log-likelihoods to the human-derived comparison
averages via the Spearman method.

5.2 Results
The correlations between the models’ log-
likelihood ratings and the human-derived com-
parison averages were moderate-to-strong, with
Spearman’s ρ ranging from 0.50 to 0.79, which
is in the range of the best-performing models from
Daland et al. (2011) that were trained on a com-
parable, but smaller, amount of unsyllabified data
(20,000 vs 30,000 words). However, a Wilcoxon
rank sum test on ρ yielded no significant dif-
ference between feature-naive and feature-aware
models in this regard (W = 282; p = 0.56).
This indicates that, although both feature-aware

6A full list of these words, as well as their wordlikeness
scores, is downloadable from the first author’s website.

and feature-naive models can predict human judg-
ments of non-words, the log-likelihoods assigned
to this particular set of non-words do not dis-
tinguish the feature-aware from the feature-naive
models.

6 Clustering of Learned Phone
Embeddings

To examine the representations that are most
helpful for characterizing word-level phonotactic
generalizations, we performed a qualitative clus-
ter analysis of the phonetic segment embeddings
learned by the randomly-initialized model within
each condition that assigned the highest average
log likelihood to the test corpus.

First, we used agglomerative nesting to cluster
the learned phonetic segment embeddings, which
were grouped according to the Euclidean distance
between them7. Position of each group was cal-
culated in the 68-dimensional space via the un-
weighted pair-group average method (Sokal and
Michener, 1958). The results are depicted in Fig.
2 for the feature-aware model and Fig. 3 for the
feature-naive model. Comparing them, we see that
the feature-aware model maintains manner-based
distinctions even at late stages of the clustering.
In contrast, these distinctions as not as clearly de-
picted in the feature-naive model, but it appears
this model still encodes some phonetic informa-
tion; namely: all stops are incorporated into the
structure early, most vowels are incorporated into
the structure after non-vowels, and several clusters
contain only vowels of the same quality, collaps-
ing over stress.

As clustering based on Euclidean distances is
only a simplification over the non-linear trans-
formations the network performs, this is a lower
bound on the amount of structure the network
can find. The feature-naive models’ better perfor-
mance on the test set suggests that these models
may be encoding phonotactic-relevant knowledge
in a more distributed representation that cannot
be visualized thus—for example, a representation
across several layers.

The phonetic information encoded by the mod-
els may be reflected in the heat map of feature
embeddings, plotted in Figs. 4 and 5. To gener-
ate these, agglomerative clustering was performed
in two dimensions: both on the phonetic seg-

7Clustering along Manhattan distance, as recommended
by Aggarwal et al. (2001), yielded similar results.

https://sites.google.com/site/rdaland/publications/Daland_etal_2011__AverageScores.csv?attredirects=0&d=1


1601

ʊ
ɪ

ɹ

ʃ

ɑ

ʌ

ɑː

ɒː

ɜː

ɪ1
ɑ1

ʌ1

ʊ1

ɑ1ː

ɒ1ː

ɜ1ː

ɪ2

ɑ2

ʌ2

ʊ2

ɑ2ː

ɒ2ː

ɜ2ː

aa1
a2æ

æ1

æ1ː

æ2

b

d

ð
d.ʒ

e

ə

ɛ

e1

ɛ1

e2

ɛ2

f

g

h

iː

i1ː

i2ː

j

k

l

m

n

ŋ

o

ɔ

ɔː

o1

ɔ1

ɔ1ː

o2

ɔ2

ɔ2ː

p
s

</s><s>

t

t.ʃ

uː

u1ː

u2ː

v

w

x

z

ʒ

θ

Manner class a a a a aapproximant nasal obs start/end vowel

Figure 2: Dendrogram created using agglomerative
clustering on trained embeddings from the feature-
aware model that achieved the highest log likelihood
on the test corpus. <s> and </s> signify start- and
end-of-word symbols, respectively, and numbers after
vowels indicate primary (1) and secondary (2) stress.

<s>n
ʊɪ
lt
ms
əp
kb
df
gv
ɹz
ʃŋ
d.ʒu1ː
i1ːɪ1
ɪ2w
uːa1
at.ʃ
ɛ1ɛ2
jæ
æ2æ1
ɑɑ1
ɑ2ɛ
i2ːɑ1ː
ɔ1ːx
ʌ1ʌ2
ʌɑː
iɒːː
e</s>
ɒ1ːθ
ðe1
ʒɑ2ː
ɔ2ːɒ2ː
æ1ːh
ʊ1ʊ2
o1o
o2ɔ2
ɜ2ːɔː
ɜːɜ1ː
u2ːa2
e2ɔ
ɔ1

Manner class a a a a aapproximant nasal obs start/end vowel

Figure 3: Dendrogram created using agglomerative
clustering on trained embeddings from the feature-
naive model that achieved the highest log likelihood on
the test corpus.



1602

</
s> <s
> ŋ p s k ə ɪ l n ʊ t m b f ɹ d ʃ u1
gː v z θ d͡ʒ t
͡ʃ x ʒ ð h j ɛ ɛ2 ɛ1 ɪ2 ɪ1 æ

1 ɑ1 ʌ ʌ2 ʌ1 ɑ æ u
ː

u2
ː i2
ː iː i1
ː ɔː ɔ2
ː

ɔ1
ː

æ
2 ɑ2 ɜ2
ː ɜ ɒːː ɒ2
ː

ɒ1
ː

ɑ2
ː

æ
1ː ɑː ɑ1
ː

ʊ2 ʊ1 ɜ1 aː a2 a1 e e1 e2 ɔ
2 ɔ ɔ1 o o2 o1 w

1712
2033
663
634
710
3127
930
2832
2623
2422
2925
462
6564
5958
6357
6160
568
1318
1115
2119
162
465
5054
4538
5349
5548
5147
5268
6736
4235
4043
4139
4437
114

Phonetic Segment

Em
be

dd
in

g 
D

im
en

si
on

-2 0 2
Value

0
20

0
40

0

Color Key
and Histogram

C
ou

nt

Figure 4: Heatmap of trained embeddings created from
the feature-aware model that achieved the best perfor-
mance on the test set. Clusterings along top axis are
based on trained embeddings of each segment.

ments and on the embedding dimensions. Col-
ored patches of activations in these heat maps cor-
respond to clusters of dimensions that all activate
in response to certain phonetic segments—that is,
clusters of dimensions that define a feature. Espe-
cially informative are patches with the same acti-
vation value below a cluster of phones: this means
that the cluster is based on the feature encoded
by those dimensions. For example, the two most
well-defined final clusters formed by the feature-
aware model are supported by multiple features,
and Fig. 4 reflects this through wide horizontal
bands that span the length of those clusters. Here,
the vertical width of each band indexes the number
of features that define the cluster.

The picture is much less clear for Fig. 5, which
represents the embeddings learned in the feature-
naive condition. The noisiness of the heat map in-
dicates the clusters are not as distinct from each
other: though every cluster is defined by at least
one embedding dimension, these dimensions do
not correlate in terms of their response to other
phones outside the cluster. Instead of creating
a straightforward clustering along embedding di-
mensions, the feature-naive model encodes any
information that may be relevant to phonotactic
probability in a more distributed representation.

a a1 ɪ1 ɪ2 i1
ː

u1
ː d͡ʒ ŋ
ʃ z ɹ v g f d b k p m t l ɪ ʊ n s ə w uː t͡ʃ ɛ2 ɛ1
j æ æ

1
æ

2 ɑ ɑ2 ɑ1 ɛ i2
ː

ɑ1
ː

ɔ1
xː ʌ ʌ2 ʌ1 i
ː ɒː ɑ eː

</
s> ɒ1
ː

<s
> θ ð ʒ e1 ɑ2
ː

ɔ2
ː

ɒ2
ː

æ
1 hː ʊ2 ʊ1 ɔ2 o2 o1 o ɜ2
ː ɜː ɔː ɜ1
ː

u2
ː

a2 e2 ɔ ɔ1

4961
3146
5835
532
5448
167
4417
625
479
6815
4142
6320
5122
3439
1062
3321
2745
443
5553
2918
2464
1350
1259
6640
6028
13
219
3756
865
3011
1467
2352
5736
3826

Phonetic Segment

Em
be

dd
in

g 
D

im
en

si
on

-2 -1 0 1 2
Value

0
10

0
20

0
30

0

Color Key
and Histogram

C
ou

nt

Figure 5: Heatmap of trained embeddings created from
the feature-naive model that achieved the best perfor-
mance on the test set.

7 Discussion

Returning to our initial questions, it seems pre-
specified phonological distinctive features are not
required for phonotactic learning. All else being
equal, representing phonetic segments as bundles
of phonological distinctive features does not ap-
pear to aid in forming segment-level phonotac-
tic generalizations, and, for this class of learn-
ing model, this specific distinctive feature set may
even be detrimental. The fact that the feature-
naive condition was able to encode phonotactic
patterns indicates that all data required to repre-
sent these patterns as probabilities between pho-
netic segments is present in the sequence of seg-
ments itself; the learner need not rely on external
information, such as distance between phones in
acoustic space.

This is not to say that phonetic information is ir-
relevant to phonotactic learning. From examining
the encodings that are learned during this process,
we observe that the best models do encode some
phonetic data.

This work is an example of how the initial-
ization of even a single layer of a deep learning
model can affect its ultimate performance on a
held-out test set, a fact already demonstrated and
discussed by, for instance, Sutskever et al. (2013).
This effect was not observed in the models’ cor-
relations with human judgments, but this may be
due to the limited number and form of non-words



1603

tested; with more statistical power, this measure
may gain enough precision to distinguish the two
conditions8.

Finally, most of our models do assign a higher
log likelihood to the test corpus than Futrell
et al. (2017), which achieved a log likelihood of
−21.73, suggesting that neural networks may be
just as good at capturing phonotactic regularities
as models that generate upcoming phonetic seg-
ments via stochastic memoization. However, our
initial training set was much larger; when trained
on only the 2,500 lemmas in Futrell et al.’s train-
ing set, our models yielded slightly lower log like-
lihoods than theirs (though we could not compare
directly because their test set was inaccessible).

8 Implications

Per our results, phonological distinctive features
do not appear to be mandatory for phonotactic ac-
quisition. At the segment level, phonotactic pat-
terns are learnable from distributional characteris-
tics of each segment alone. This signals a need for
revision of segmental phonotactic learning mod-
els that rely on a set of predetermined distinctive
features—or at least stronger justification for the
inclusion of any proposed distinctive feature set
over another.

There are still a few additional tests that must
be done before our conclusions can be general-
ized beyond these experiments. First, although the
feature set that we used is typical of those used
by other models of phonotactics, it is still possi-
ble that some other phonological feature set would
result in better performance. Second, distinctive
features may yet be helpful for models that train
on much smaller datasets than ours, since they can
provide hints to phonological structure that are not
inferrable from such limited data.

Beyond distinctive features, some other, more
detailed phonetic representation may yet prove
helpful for phonotactic acquisition, if phonotac-
tic expectations actually contain more detail about
token-level variability, instead of the discrete
segment-level representation assumed herein. Pre-
cise consequences for extant phonotactic learning
models will depend whether this is the case; the
determination is complicated by the fact that hu-
mans acquire both phonetic categories and phono-

8This homogeneity may not have been an issue for Daland
et al. (2011)’s comparison because the models tested therein
had very diverse structures, and may have become sensitive
to very different aspects of the training data as a result.

tactic patterns simultaneously (Jusczyk et al.,
1994; Werker and Tees, 1984).

One interesting avenue for future research is the
multi-language case—i.e., training the model on a
corpus in one language, and analyzing its perfor-
mance on a corpus in a different language. This
can help us make predictions about the types of
pronunciation difficulties that speakers are likely
to encounter in a second language, illuminating
phonological effects of cross-linguistic transfer.

We must nonetheless be wary of using these re-
sults to make claims about human language ac-
quisition. Human language is shaped by many
other factors that are extraneous to our models, in-
cluding articulatory restrictions, perceptual limita-
tions, and constraints of cognitive economy. At the
risk of overreaching, we must better specify these
factors and their consequences before drawing fur-
ther analogies.

9 Conclusion

Phonotactic acquisition can be accomplished with-
out external, prior knowledge of distinctive fea-
tures; indeed, according to our results, this knowl-
edge may be a slight hindrance rather than a help.
Though segment-level phonotactic inference may
still benefit from access to a finer-grained pho-
netic specification of the speech stream, a prede-
termined encoding of this input in terms of dis-
tinctive features does not appear to be required for
this purpose.

Acknowledgements

We thank Matthew Goldrick, members of the
Northwestern Language & Computation Lab and
Northwestern SoundLab, as well as the audience
at MidPhon 2018 and anonymous reviewers for
their helpful feedback. We also wish to thank
the developers of the PyTorch Word-Level lan-
guage modeling RNN example, which served as
a starting point for our RNN code. This research
was supported in part by NSF GRFP Award DGE-
1842165 (Mirea) and NSF 1734217 (Bicknell).

References
Charu C. Aggarwal, Alexander Hinneburg, and

Daniel A. Keim. 2001. On the Surprising Be-
havior of Distance Metrics in High Dimensional
Space. In Gerhard Goos, Juris Hartmanis, Jan van
Leeuwen, Jan Van den Bussche, and Victor Vianu,
editors, Database Theory — ICDT 2001, volume

https://doi.org/10.1007/3-540-44503-X_27
https://doi.org/10.1007/3-540-44503-X_27
https://doi.org/10.1007/3-540-44503-X_27


1604

1973, pages 420–434. Springer Berlin Heidelberg,
Berlin, Heidelberg.

Adam Albright. 2009. Feature-based generalisation
as a source of gradient acceptability. Phonology,
26(1):9–41.

John R. Anderson and Robert Milson. 1989. Human
memory: An adaptive perspective. Psychological
Review, 96(4):703–719.

R H Baayen, R Piepenbrock, and L Gulikers. 1995.
CELEX2.

Amélie Bernard. 2017. Novel phonotactic learning:
Tracking syllable-position and co-occurrence con-
straints. Journal of Memory and Language, 96:138–
154.

Noam Chomsky and Morris Halle. 1965. Some contro-
versial questions in phonological theory. Journal of
Linguistics, 1(2):97–138.

G. Nick Clements. 2009. The Role of Features
in Phonological Inventories. In Eric Raimy and
Charles E. Cairns, editors, Contemporary Views
on Architecture and Representations in Phonology,
pages 19–68. MIT Press.

Jeffry A. Coady and Richard N. Aslin. 2004. Young
children’s sensitivity to probabilistic phonotactics in
the developing lexicon. Journal of Experimental
Child Psychology, 89(3):183–213.

Robert Daland, Bruce Hayes, James White, Marc
Garellek, Andrea Davis, and Ingrid Norrmann.
2011. Explaining sonority projection effects.
Phonology, 28:197–234.

B. Elan Dresher. 2015. The arch not the stones:
Universal feature theory without universal features.
Nordlyd, 41(2):165–181.

Ewan Dunbar, Gabriel Synnaeve, and Emmanuel
Dupoux. 2015. Quantitative Methods for Compar-
ing Featural Representations. In Proceedings of the
International Congress of Phonetic Sciences.

Stefan A. Frisch, Nathan R. Large, and David B.
Pisoni. 2000. Perception of Wordlikeness: Effects
of Segment Probability and Length on the Process-
ing of Nonwords. Journal of memory and language,
42(4):481–496.

Richard Futrell, Adam Albright, Peter Graff, and Tim-
othy J. O’Donnell. 2017. A Generative Model of
Phonotactics. Transactions of the Association for
Computational Linguistics, 5(0):73–86.

Matthew Goldrick. 2004. Phonological features and
phonotactic constraints in speech production. Jour-
nal of Memory and Language, 51(4):586–603.

Peter Nepomuk Herwig Maria Graff. 2012. Commu-
nicative Efficiency in the Lexicon. Thesis, Mas-
sachusetts Institute of Technology.

Bruce Hayes and Colin Wilson. 2008. A Maxi-
mum Entropy Model of Phonotactics and Phonotac-
tic Learning. Linguistic Inquiry, 39(3):379–440.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Peter W. Jusczyk, Paul A. Luce, and Jan Charles-Luce.
1994. Infants′ Sensitivity to Phonotactic Patterns in
the Native Language. Journal of Memory and Lan-
guage, 33(5):630–645.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-Aware Neural Lan-
guage Models. In Proceedings of the Thirtieth
AAAI Conference on Artificial Intelligence (AAAI-
16), pages 2741–2749. Association for the Advance-
ment of Artificial Intelligence.

Jeff Mielke. 2008. The Emergence of Distinctive Fea-
tures. Oxford Studies in Typology and Linguis-
tic Theory. Oxford University Press, Oxford, New
York.

Jeff Mielke. 2012. A phonetically based metric of
sound similarity. Lingua, 122(2):145–163.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in PyTorch.

Robert R. Sokal and Charles D. Michener. 1958. A
statistical method for evaluating systematic rela-
tionships. University of Kansas Science Bulletin,
38(1):1409–1438.

Holly L Storkel, J Armbrüster, and Hogan, T P. 2006.
Differentiating phonotactic probability and neigh-
borhood density in adult word learning. Jour-
nal of Speech, Language & Hearing Research,
49(6):1175–1192.

Martin Sundermeyer, Hermann Ney, and Ralf Schlüter.
2015. From Feedforward to Recurrent LSTM Neu-
ral Networks for Language Modeling. IEEE/ACM
Trans. Audio, Speech and Lang. Proc., 23(3):517–
529.

Ilya Sutskever, James Martens, and George Dahl. 2013.
On the importance of initialization and momentum
in deep learning. In Proceedings of Machine Learn-
ing Research, volume 28, page 9, Atlanta, Georgia,
USA.

Michael S. Vitevitch, Paul A. Luce, Jan Charles-Luce,
and David Kemmerer. 1997. Phonotactics and Syl-
lable Stress: Implications for the Processing of
Spoken Nonsense Words. Language and Speech,
40(1):47–62.

Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49–63.

https://doi.org/10.1017/S0952675709001705
https://doi.org/10.1017/S0952675709001705
https://doi.org/10.1037/0033-295X.96.4.703
https://doi.org/10.1037/0033-295X.96.4.703
https://doi.org/10.1016/j.jml.2017.05.006
https://doi.org/10.1016/j.jml.2017.05.006
https://doi.org/10.1016/j.jml.2017.05.006
https://doi.org/10.1017/S0022226700001134
https://doi.org/10.1017/S0022226700001134
https://doi.org/10.1016/j.jecp.2004.07.004
https://doi.org/10.1016/j.jecp.2004.07.004
https://doi.org/10.1016/j.jecp.2004.07.004
https://doi.org/10.1017/S0952675711000145
https://doi.org/10.7557/12.3412
https://doi.org/10.7557/12.3412
https://doi.org/10.1006/jmla.1999.2692
https://doi.org/10.1006/jmla.1999.2692
https://doi.org/10.1006/jmla.1999.2692
https://doi.org/10.1016/j.jml.2004.07.004
https://doi.org/10.1016/j.jml.2004.07.004
https://doi.org/10.1162/ling.2008.39.3.379
https://doi.org/10.1162/ling.2008.39.3.379
https://doi.org/10.1162/ling.2008.39.3.379
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1006/jmla.1994.1030
https://doi.org/10.1006/jmla.1994.1030
https://doi.org/10.1016/j.lingua.2011.04.006
https://doi.org/10.1016/j.lingua.2011.04.006
https://doi.org/10.1109/TASLP.2015.2400218
https://doi.org/10.1109/TASLP.2015.2400218
https://doi.org/10.1177/002383099704000103
https://doi.org/10.1177/002383099704000103
https://doi.org/10.1177/002383099704000103
https://doi.org/10.1016/S0163-6383(84)80022-3
https://doi.org/10.1016/S0163-6383(84)80022-3
https://doi.org/10.1016/S0163-6383(84)80022-3


1605

A Appendix

Hyperparameter Name Description Values Tested

rand reset
whether activations in the model reset to a
random state (True), or to zero (False) after
each word

True, False

lr initial learning rate 0.1, 0.5, 1.0, 2.0

anneal factor
amount by which to anneal learning rate,
if no improvement found

0, 0.25, 0.5, 1.0

patience
number of training epochs to wait for
validation loss to improve before updating
weights

0, 2, 4

dropout proportion of weights to keep fixed 0, 0.2, 0.5

epochs
number of epochs (complete passes through the
data) to train for

25, 50, 100

nlayers number of recurrent layers 1, 2, 4
nhid number of nodes in each recurrent layer 128, 256, 512, 1250

Table A1: Particulars of hyperparameter testing. Hy-
perparameters were optimized for speed and likelihood
assigned to the validation set. Optimal parameters for
the validation set are bolded, and were used in the ex-
periments reported here.


