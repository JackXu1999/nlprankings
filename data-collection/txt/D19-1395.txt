



















































Improving Distantly-Supervised Relation Extraction with Joint Label Embedding


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3821–3829,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3821

Improving Distantly-Supervised Relation Extraction with
Joint Label Embedding

Linmei Hu1, Luhao Zhang1, Chuan Shi1 ∗, Liqiang Nie2, Weili Guan3, Cheng Yang1
1Beijing University of Posts and Telecommunications, China

2Shan Dong University, China
3Hewlett Packard Enterprise Singapore, Singapore

{hulinmei,zhangluhao,shichuan}@bupt.edu.cn
{nieliqiang, honeyguan, albertyang33}@gmail.com

Abstract

Distantly-supervised relation extraction has
proven to be effective to find relational facts
from texts. However, the existing approaches
treat labels as independent and meaningless
one-hot vectors, which cause a loss of poten-
tial label information for selecting valid in-
stances. In this paper, we propose a novel
multi-layer attention-based model to improve
relation extraction with joint label embedding.
The model makes full use of both structural
information from Knowledge Graphs and tex-
tual information from entity descriptions to
learn label embeddings through gating integra-
tion, while avoiding the imposed noise with
an attention mechanism. Then the learned
label embeddings are used as another atten-
tion over the instances (whose embeddings are
also enhanced with the entity descriptions) for
improving relation extraction. Extensive ex-
periments demonstrate that our model signifi-
cantly outperforms state-of-the-art methods.

1 Introduction

Knowledge Graphs (KGs) such as Freebase and
DBpedia have shown their strong power in many
natural language processing tasks including ques-
tion answering (Zhang et al., 2018) and dialog
generation (Zhou et al., 2018). However, these
KGs are far from complete. Relation extraction,
which aims to fill this gap by extracting semantic
relationships between entity pairs from plain texts,
is thus of great importance.

Most existing supervised relation extraction
methods require a large number of labeled train-
ing data, which is time-consuming and laborious.
Distant supervision has been proposed by (Mintz
et al., 2009) to address the challenge. It assumes
that if two entities have a relation in KGs, then
all sentences mentioning the two entities express

∗Corresponding author: Chuan Shi.

Romania … Its capital and 

largest city is Bucharest … 

include Cluj-Napoca…

Knowledge Graph

Entity Description

Entity Description

A bag of sentences

Method (2)

(a) Existing Methods

(b) Our Proposed Method

Label Embedding

Classifier

Att

Att

Classifier

Method (3)

Knowledge Graph
A bag of sentences

Att

Textual Relation

Embedding

Classifier

Method (1)

Att

Textual Relation

Embedding

Figure 1: Illustration of comparison of existing meth-
ods and our proposed method.

this relation. Thus, distant supervision can auto-
matically generate a large number of labeled data
without labor cost. Simultaneously, it often suf-
fers from wrong labeling problem (Surdeanu et al.,
2012; Zeng et al., 2015).

Recently, significant progress has been made in
applying deep neural networks for relation extrac-
tion under distant supervision (Zeng et al., 2014,
2015; Feng et al., 2017). To alleviate the wrong
labeling problem in distant supervision, attention
models have been proposed to select valid in-
stances (Ji et al., 2017). As shown in Figure 1 (a),
they can be divided into three categories: (1) typ-
ical attention models without external information
(Lin et al., 2016; Du et al., 2018), (2) attention
models using KGs (Han et al., 2018a), (3) atten-
tion models using side information such as entity
descriptions (Vashishth et al., 2018; Ji et al., 2017).
However, they all have flaws in selecting valid in-
stances due to failing to exploit potential label in-
formation. They treat labels as independent and
meaningless one-hot vectors, which cause a loss



3822

of potential label information. Label embeddings
aim to learn representations of labels based on re-
lated information. The label embeddings can be
used to attend over the bag of instances for relation
classification. Additionally, they don’t take advan-
tage of both structural information from KGs and
textual information from entity descriptions and
ignore the imposed noise.

In this paper, we propose a novel multi-layer
attention-based model RELE (Relation Extraction
with Joint Label Embedding) to improve rela-
tion extraction. Our model integrates both struc-
tural information from KGs and textual informa-
tion from entity descriptions with a gating mech-
anism to learn label embeddings, while avoiding
the imposed noise (highlighted in green) in entity
descriptions with an attention mechanism. Then
the label embeddings are used as another anten-
tion over the bag of instances to select valid ones
for improving relation extraction. Note that we
also enhance the instance embedding with entity
descriptions. The contributions of this paper can
be summarized as follows:

• We propose a novel multi-layer attention-
based model RELE to improve distantly su-
pervised relation extraction with joint label
embedding. The label embeddings can be
used to attend over the bag of instances for
relation classification.

• RELE makes full use of both structural infor-
mation from KGs and textual information of
entity descriptions to learn label embeddings
through gating integration, while avoiding
the imposed noise with attention.

• Extensive experiments on two benchmark
datasets have demonstrated that our model
significantly outperforms state-of-the-art
methods on distantly-supervised relation
extraction.

2 Related Work

Our work is mainly related to distant supervision,
neural relation extraction and label embedding.

Distant Supervision. Most supervised rela-
tion extraction methods require large-scale labeled
training data which are expensive. Distant Super-
vision proposed by (Mintz et al., 2009) is an ef-
fective method to automatically label large-scale
training data under the assumption that if two

entities have a relation in a KG, then all sen-
tences mentioning those entities express this rela-
tion. The assumption does not work in all cases
and causes the mislabeling problem.

MultiR (Hoffmann et al., 2011) and MIMLRE
(Surdeanu et al., 2012) introduce multi-instance
learning where the instances mentioning the same
entity pair are processed at a bag level. How-
ever, these methods rely heavily on handcrafted
features.

Neural Relation Extraction. With the devel-
opment of deep learning, neural networks have
proven to be efficient to automatically extract valid
features from sentences in recent years. Some re-
searches (Zeng et al., 2014, 2015) adopt Convo-
lution Neural Networks (CNN) to learn sentence
representations automatically. To alleviate the
mislabeling problem, attention mechanisms (Lin
et al., 2016; Du et al., 2018) have been employed.
Apart from that, some studies apply other relevant
information to improve relation extraction (Zeng
et al., 2017; Vashishth et al., 2018; Han et al.,
2018b,a; Ji et al., 2017). For example, RESIDE
(Vashishth et al., 2018) utilizes the available side
information from knowledge bases, including en-
tity types and relation alias information. Han et al.
(2018a) proposed a joint representation learning
framework of KGs and instances, which lever-
ages the KG embeddings to select valid instances.
APCNN+D (Ji et al., 2017) exploits the entity de-
scriptions as background knowledge for selection
of valid instances, and ignores the imposed noise.

Different from the existing works, we propose
a novel multi-layer attention-based model RELE
with joint label embedding. Our model makes
full use of both structural information from KGs
and textual information from entity descriptions to
learn label embeddings through gating integration,
while avoiding the imposed noise with an atten-
tion mechanism. The label embeddings are then
used as another attention over the bag of instances
to select valid instances for relation classification.

Label Embedding. Label embedding has been
widely exploited in computer vision including im-
age classification (Akata et al., 2016) and text
recognition (Rodriguez-Serrano et al., 2015). Re-
cently, LEAM (Wang et al., 2018) successfully ap-
plies label embedding in text classification, which
obtains each label’s embedding by its correspond-
ing text descriptions. In this work, we are the first
to apply it for relation extraction. We propose



3823

a novel multi-layer attention-based model to im-
prove relation extraction with joint label embed-
ding.

3 Preliminaries

In this section, we briefly introduce some nota-
tions and concepts used in this paper.

For convenience, we denote a KG as G =
{(h, r, t)}, which contains considerable triplets
(h, r, t) where h and t are respectively head entity
and tail entity, and r denotes the relation. Their
embeddings are denoted as (h, r, t).

Formally, given a pair of entities (h, t) in a
KG G and a bag of instances (sentences) B =
{s1, s2, · · · , sm}, where each instance si contains
(h, t), the task of relation extraction is to train a
classifier based on B to predict the relation label
y of (h, t) from a predefined relation set. If no
relation exists, we simply assign NA to it.

To improve relation extraction, we make full
use of both the KG G and entity descriptions
D = {d1, d2, · · · , dn} to learn label embeddings
which can benefit selection of valid instances. For
each entity ei, we take the first paragraph of its
corresponding Wikipedia page as its description
text di = {w1, w2, · · · , wl}, where wi ∈ V de-
notes the description word, l is the length and V is
the vocabulary.

4 Our Proposed Model

In this section, we will detail our proposed multi-
layer attention-based model RELE for relation
extraction with joint label embedding. Existing
methods for relation extraction take labels as inde-
pendent and meaningless one-hot vectors, which
cause a loss of potential label information for se-
lecting valid instances. Additionally, they don’t
take full advantage of both structural information
from KGs and textual information from entity de-
scriptions and ignore the imposed noisy informa-
tion.

As shown in Figure 2, our model is based on
a multi-layer attention, containing two parts: 1)
label embedding (shown in the right) and 2) neu-
ral relation classification (shown in the left) . The
former makes full use of structural information
from KGs and textual information from entity de-
scriptions to learn label embeddings through gat-
ing mechanism, while avoiding the imposed noise
with an attention mechanism. The latter leverages
the label embeddings as another attention over the

…

𝒅𝒆

𝒔𝒎
Entity

Descriptions

𝒔𝟏 𝒕 𝒉

CNNCNN

𝒓𝒉𝒕

𝒓𝒉𝒕
′

𝒓𝒔

𝒅 = (𝒘𝟏,⋯ ,𝒘𝒍 )

Knowledge

Graph
Sentences

…

…

…

𝒔𝟏
′ 𝒔𝒎

′

…

Conv Conv…

…

Attention 𝜶

Attention 𝝀

⨁ ⨁ G
Gating

Mechanism

Concatenation

Label

Embedding
ClassifierClassifier

Figure 2: Illustration of our multi-layer attention based
model RELE.

instances to select valid instances for improving
neural relation extraction. Note that we also use
the entity descriptions to enhance the representa-
tions of instances. We detail the two parts as fol-
lows.

4.1 Joint Label Embedding

Label information plays a vital role in selecting
valid instances for improving relation extraction.
We make full use of both structural information
from KGs (Han et al., 2018a) and textual infor-
mation from entity descriptions (Ji et al., 2017) to
learn label embeddings via gating integration. En-
tity descriptions provide rich background knowl-
edge for entities (Newman-Griffis et al., 2018) and
are supposed to benefit the label embedding and
relation extraction. Nevertheless, as shown in Fig-
ure 1, entity descriptions may also contain irrel-
evant and even misleading information. There-
fore, we propose to use KG embeddings to at-
tend over the entity descriptions, alleviating the
imposed noise. Then a gating mechanism is used
to integrate both the KGs and entity descriptions
for learning label embeddings.

KG Embedding. We use TransE (Bordes et al.,
2013) for KG embedding. Given a triplet (h, r, t),
the model aims to learn low-dimensional represen-
tations vector for entities h, t and the relationship
r into the same vector space, and regards a rela-
tionship r as a translation from the head entity h
to tail entity t, assuming the embedding t should



3824

be close to h+ r if (h, r, t) exists. The score func-
tion is defined as :

f(h, r, t) = −‖h + r− t‖22. (1)

Note that since the true relations in test set are un-
known, we simply represent the relation by:

rht = t− h. (2)

In this way, we can also get the relation embed-
dings given the entity pairs during testing.

Entity Description Embedding. Then we use
the representations of relations rht as attention
over the words of an entity description to reduce
the weights of noisy words. Formally, for each en-
tity e, we learn the representation of its description
d = (w1, w2, · · · , wl) as follows:

xi = CNN(wi− c−1
2
, · · · ,wi+ c−1

2
), (3)

x̂i = tanh(Wx · xi + bx), (4)

αi =
exp(x̂i · rht)∑
i=1 exp(x̂i · rht)

, (5)

de =

l∑
i=1

αixi, (6)

where CNN(·) denotes a convolution layer with
window size c over the word sequence. xi ∈ RDh
is the hidden representation of the word wi. Wx is
the weight matrix and bx is the bias vector. αi is
the attention weight of the word wi, which is com-
puted based on the relation embedding rht. Fi-
nally, the text description embedding de is com-
puted by the weighted average of words.

Gating Integration. We apply a gating mecha-
nism (Xu et al., 2016) to integrate the textual entity
description embedding de and the structural infor-
mation (entity embedding e) from KGs:

e′ = g � e + (1− g)� de, (7)

where g ∈ RDw is a gating vector for integration,
e′ ∈ RDw represents the final integrated entity
embedding and � represents Hadamard product.
Consequently, we compute the final label embed-
ding l:

l = t′ − h′. (8)

Label Classifier. Ideally, each label embedding
is supposed to act as an “anchor” for each relation
class. To achieve this goal, we consider to train

the learned label embeddings l to be easily clas-
sified as the correct relation class. Therefore, we
use softmax to get the predicted probabilities of
the relation classes:

P(y|D,G) = Softmax(Mkl + bk), (9)

where Mk is the transformation matrix, bk is the
bias.

4.2 Neural Relation Extraction
After obtaining the embeddings of labels and en-
tity descriptions, we leverage them to advance the
neural relation extraction. We first use the en-
tity descriptions to enhance instance embeddings.
Then we leverage the label embeddings to attend
over the instances to select valid instances for re-
lation classification.

Instance Embedding. We enrich the repre-
sentation of an instance with the pair of entity
descriptions. Firstly, for each word w ∈ s =
{w1, · · · , wn}, its embedding ŵi is initialized as
follows:

ŵi = wi ⊕ pi1 ⊕ pi2, (10)

where wi is the pre-trained word vector of wi, pi1
and pi2 are its position embeddings to incorporate
relative distances to two target entities into two
Dp-dimensional vectors respectively (Zeng et al.,
2014). The symbol ⊕ represents concatenation
operator.

Then, we choose CNN (Zeng et al., 2014) with
window size c as our encoder to learn the instance
embedding considering the text of the instance it-
self.

zi = CNN(ŵi− c−1
2
, · · · , ŵi+ c−1

2
), (11)

[s]j = max{[z1]j , · · · , [zn]j}, (12)

where s ∈ RDh is the sentence (instance) embed-
ding, [·]j is the j-th value of a vector and function
max denotes max-pooling.

Finally, we concatenate the original instance
embedding s with the entity descriptions (dh, dt),
obtaining the new instance representation s′. For-
mally,

s′ = s⊕ dh ⊕ dt. (13)

Attention over Instances. To alleviate the
wrong labeling problem of distant supervision, we
leverage the label embedding l as attention over



3825

instances to reduce the weights of noisy instances
in the sentence bag B = {s1, · · · , sm}. Then the
representation of textual relation feature s̄ from the
bagB can be calculated as weighted average of the
instance embeddings s′i:

ŝi = tanh(Wss
′
i + bs), (14)

λi =
exp(l · ŝi)∑m
j=1 exp(l · ŝi)

, (15)

s̄ =

m∑
i=1

λis
′
i, (16)

where Ws is the weight matrix and bs is the bias
vector. λi is the attention score of the instance si,
computed based on the label embedding l.

Relation Classifier. Finally, to compute the
confidence of each relation class, we feed the rep-
resentation of the textual relation s̄ into a softmax
classifier after being processed by a linear trans-
formation. Formally,

P(y|B) = Softmax(Mss̄ + bs), (17)

where Ms is the transformation matrix, and bs is
the bias.

4.3 Model Training
The objective function of our joint model RELE
includes two parts, the loss of label classifier L1
and the loss of relation classifier L2.

Assuming that there are N bags in training set
{B1, B2, · · · , BN}, and their corresponding la-
bels {y1, y2, · · · , yN}, we exploit cross entropy
for the loss function of label classifier L1:

L1 = −
N∑
i=1

log P(yi|D,G), (18)

The loss L1 aims to train the label embedding to
be classified to the correct relation class.

Similarly, for the loss of relation classifier L2,
we also exploit cross entropy and get:

L2 = −
N∑
i=1

log P(yi|Bi), (19)

Finally, we aim to minimize the loss function L
with L2-norm:

minL = L1 + L2 + η‖Θ‖2, (20)

where η is the regularization coefficient and Θ
denotes the parameters in our model. Stochastic
gradient descent (SGD) is used to optimize our
model.

5 Experiments

5.1 Datasets
In our experiments, we evaluate our model over
the NYT-FB60K dataset (Han et al., 2018a) and
GIDS-FB8K dataset (Jat et al., 2018). In the fol-
lowing, we detail each dataset.

• NYT-FB60K. The dataset NYT-FB60K (Han
et al., 2018a) includes three parts: knowl-
edge graphs (FB60K extended from Riedel
dataset (Riedel et al., 2010), containing
1,324 relations, 69,512 entities, and 335,350
facts), text corpus (whose sentences are from
Riedel dataset, containing 570,088 sentences,
63,696 entities, and 56 relations) and entity
descriptions (which are the first paragraphs
of the entities’ Wikipedia pages, containing
around 80 words on average).

• GIDS-FB8K. We construct the dataset GIDS-
FB8K based on GIDS dataset (Jat et al.,
2018). It also contains three parts: knowl-
edge graphs (FB8K extended from GIDS
dataset, containing 208 relations, 8,917 en-
tities, and 38,509 facts), text corpus (whose
sentences are from GIDS dataset, contain-
ing 16,960 sentences, 14,261 entities, and 5
relations) and entity descriptions (which are
the first paragraphs of the entities’ Wikipedia
pages, containing around 80 words on aver-
age).

5.2 Baselines
We compare our model with the state-of-the-art
baselines:

• Mintz (Mintz et al., 2009). A multi-class lo-
gistic regression model under distant supervi-
sion.

• MultiR (Hoffmann et al., 2011). A prob-
abilistic graphical model for multi-instance
learning.

• MIMLRE (Surdeanu et al., 2012). A graph-
ical model for multi-instance multi-label
learning.

• CNN+ATT (Lin et al., 2016). A CNN model
with instance-level attention.

• APCNN+D (Ji et al., 2017) A Piecewise
CNN model with instance-level attention us-
ing entity descriptions. As no code available,
we implemented it by ourselves.



3826

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Recall

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0
Pr

ec
isi

on
RELE
JointD+KATT
APCNN+D
MIMLRE

Mintz
MultiR
CNN+ATT
RESIDE

(a) NYT-FB60K Dataset

0.0 0.2 0.4 0.6 0.8 1.0
Recall

0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

Pr
ec

isi
on

RELE
RESIDE
JointD+KATT
CNN+ATT
APCNN+D

(b) GIDS-FB8K Dataset

Figure 3: Precision-recall curves for different methods. RELE achieves higher precision over the entire range of
recall compared to all the baselines on both datasets.

Parameter Value

Word/Entity/Relation Dimension Dw 50
Position Dimension Dp 5
Hidden Layer Dimension Dh 230
Kernel Size c 3
Learning Rate α 0.5
Regularization Coefficient η 0.0001
Dropout Probability p 0.5

Table 1: Parameter Settings.

• JointD+KATT (Han et al., 2018a). A joint
model for knowledge graph embedding and
relation extraction.

• Reside (Vashishth et al., 2018). A neural net-
work based model which makes use of rel-
evant side information and employs Graph
Convolution Networks for encoding syntac-
tic information of instances.

5.3 Evaluation Metrics

Following previous studies (Lin et al., 2016),
our model is evaluated held-out, which compares
the relations discovered from test corpus with
those in Freebase. We report the Precision-Recall
curve and top-N precision (P@N) metric for NYT-
FB60K dataset.

For GIDS-FB8K dataset, we report the
Precision-Recall curve, F1 score and Mean Av-
erage Precision (MAP). We do not use the top-N
precision (P@N) metric for the dataset which is
small containing only 5 relation classes.

5.4 Parameter Settings
For all the models, we use the pre-trained word
embeddings with word2vec tool∗ on NYT corpus
for initialization. The embeddings of entities men-
tioned in datasets are pre-trained through TransE
model. We select the learning rate α among
{0.1, 0.01, 0.005, 0.001} for minimizing the loss.
For other parameters, we simply follow the set-
tings used in (Lin et al., 2016; Han et al., 2018a)
so that it can be fairly compared with these base-
lines. Table 1 shows all the parameters used in our
experiment.

5.5 Experiment Results
5.5.1 Precision-Recall Curves on Both

Datasets
Figure 3 shows the comparison results in terms
of Precision-Recall Curves on NYT-FB60K and
GIDS-FB8K datasets. Overall, we observe that:
(1) As shown in Figure 3(a), the neural network
based approaches have more obvious advantages
than Mintz, MultiR, and MIMLRE, illustrating the
limitation of human-designed features and the ad-
vancement of neural networks in relation extrac-
tion. (2) APCNN+D using entity descriptions,
and JointD+KATT exploiting KGs both outper-
form CNN+ATT on both datasets, showing that
entity descriptions and KGs are both useful for im-
proving the performance of relation extraction un-
der distant supervision. RESIDE achieves better
performance than APCNN+D and JointD+KATT.
It is probably because that RESIDE utilizes more
available side information, including entity types
and relation alias information. (3) Our model

∗https://code.google.com/p/word2vec/



3827

Test Setttings One Two All

P@N(%) 100 300 500 Mean 100 300 500 Mean 100 300 500 Mean

CNN+ATT 67.3 61.1 56.5 61.6 73.3 62.1 53.5 63.0 74.2 62.4 55.9 64.1
APCNN+D 78.0 67.7 60.6 68.7 77.0 63.3 55.6 65.3 77.0 66.7 63.0 68.9
JointD+KATT 80.0 70.0 64.0 71.3 81.0 67.0 59.6 69.2 84.0 68.7 63.7 71.0
RESIDE 79.0 67.6 57.4 68.0 83.0 70.6 62.6 72.0 84.0 78.5 69.8 77.4

RELE 87.0 76.6 67.0 76.8 88.0 73.3 63.2 78.7 88.0 78.6 69.8 78.8

Table 2: Evaluation results P@N of different models using different number of sentences in bags on NYT-FB60K
dataset. Here, One, Two and All represent the number of sentences randomly selected from a bag.

JointD+KATT APCNN+D RESIDE RELE
73

76

79

82

85

88
F1(%)
MAP(%)

Figure 4: Comparison on GIDS-FB8K dataset.

RELE achieves the best performance compared to
all the baselines. We believe the reason is that we
make use of potential label information through
joint label embedding. The learned label embed-
dings are of high quality since we fully exploit
both the structural information from KGs and tex-
tual information from texts via gating integration
while avoiding the imposed noise by an attention
mechanism.

5.5.2 P@N Evaluation on NYT-FB60K
Dataset

As shown in Table 2, we report Precision@N
of different neural network based approaches on
NYT-FB60K dataset. To verify the performance
of our model on those entity pairs with few in-
stances, we randomly select one, two and all
instances for each entity pair, following previ-
ous studies (Du et al., 2018; Vashishth et al.,
2018). As we can observe: (1) APCNN+D and
JointD+KATT both outperform CNN+ATT in all
cases, demonstrating the effectiveness of entity de-
scriptions and KGs. RESIDE achieves better per-
formance than APCNN+D and JointD+KATT by
incorporating more side information (e.g., entity
types). (2) Our model RELE significantly out-
performs all the baselines. The reason is that

P@N(%) 100 300 500 Mean

RELE w/o LE 74.2 62.4 55.9 64.1
RELE w/o ATTe 82.0 75.6 69.6 75.7
RELE w/o LC 81.0 74.0 69.0 74.7
RELE 88.0 78.6 69.8 78.8

Table 3: Evaluation results P@N of variant models on
NYT-FB60K dataset.

our model learns high quality of label embeddings
which play a critical role in relation extraction.

5.5.3 Results on GIDS-FB8K Dataset

Based on the results on the NYT-FB60K dataset,
we choose APCNN+D, JointD+KATT and RE-
SIDE as representative baselines to compare
against our models on the GIDS-FB8K dataset in
terms of F1 and MAP. As shown in Figure 4, our
model consistently achieves better performance,
which verifies the effectiveness of our model with
joint label embedding. The overall results on
GIDS-FB8K dataset show that our model can be
well applied to smaller-scale datasets.

5.5.4 Comparison of Variant Models

In order to verify the effectiveness of different
modules of our model, we design three variant
models:

• RELE w/o LE removes the label embed-
ding from RELE, which degenerates to
CNN+ATT.

• RELE w/o ATTe removes the attention of
the KG over entity descriptions during la-
bel embedding. We use max-pooling instead,
which does not consider the noise in the en-
tity descriptions.



3828

𝑠1: In Bucharest , Romania , president Traian Basescu said Sunday night that the entry into …

𝑠2: Last year, it opened offices in Warsaw and Bucharest , the capital of Romania.

𝑠3: … by the presence of two films from Romania , the way I spent … east of Bucharest …

𝑠4:… Ervin a of New York City , on April 18th , age 98 , formerly of Bucharest , Romania .

Triplet(Romania, Bucharest, /location/country/capital)JointD+
KATT(×)

RELE

(√)
APCNN+D

(×)

0

1

Figure 5: A case study for predicting the relation between “Bucharest” and “Romania”. The baselines all predict
wrongly while our model gives the right result. The left shows the weights assigned to different sentences by
different models. Our model always gives higher weights to correct sentences (shown in red).

Romania is a country located at the crossroads of Central , Eastern ,  

and Southeastern Europe . It borders the Black Sea to the southeast ,  

… , Romania is the 12th largest country and also the 7th most  

populous ... Its capital and largest city is Bucharest , and other major  

urban areas include Cluj-Napoca ... 

(a) Description text of “Romania”

 

 

x 

 

Bucharest is the capital and largest city of Romania , as well as its 

cultural , industrial , and financial centre . It is located in the southeast 

of the country , … ,on the banks of the Dambovita River , less than 60 km 

north of the Danube River and the Bulgarian border. 

 

x (b) Description text of “Bucharest”

Figure 6: Visualization of attention values of words in the descriptions of the entities “Romania” and “Bucharest”.

• RELE w/o LC removes the label classifier.
It does not train the label embeddings to be
classified to the correct classes.

As shown in Table 3, without label embedding,
the performance of RELE w/o LE drops signifi-
cantly (more than 10%). It demonstrates the ef-
fectiveness of our joint label embedding. If we do
not consider the imposed noise in the entity de-
scriptions, the performance of RELE w/o ATTe
decreases by around 3% on mean P@N. It demon-
strates that the attention of KGs over entity de-
scriptions is important for learning high-quality la-
bel embeddings. We also explore the performance
of RELE w/o LC which does not train the label
embeddings to be classified to the correct classes
and find that label classifier plays a critical role in
label embedding. The performance decreased by
around 4% on mean P@N, without label classifier.

5.5.5 Case Study
Figure 5 illustrates an example from the test
set of NYT-FB60K. As we can observe, rep-
resentative attention-based baselines APCNN+D
and JointD+KATT both predict wrong rela-
tion labels for the entities, while our model
RELE correctly predicts the relation label “/loca-
tion/country/capital”. That is because the base-
lines assign relatively low weights to correct sen-
tences (s1 and s2) and achieve inferior representa-
tions of the textual relation for classification. Our
model RELE assigns higher weights to all the cor-
rect sentences s1, s2 and s4, demonstrating that

RELE learns high-quality label embeddings.
We also provide the insights of the attention of

KGs over entity descriptions about “Romania” and
“Bucharest”. As shown in Figure 6, the words
“capital”, “largest”. and “centre” which are re-
lated to the relation are given higher weights. It
demonstrates that the attention of KGs over entity
descriptions can help reduce the noise in entity de-
scriptions and thus improve the label embeddings.

6 Conclusion

In this work, we consider leveraging potential
label information to select valid instances for
distantly-supervised relation extraction. We pro-
pose a novel multi-layer attention-based model
RELE to improve relation extraction with joint la-
bel embedding. Our model takes full advantage of
both structural information from KGs and textual
information from entity descriptions to learn la-
bel embeddings, while avoiding the imposed noise
with an attention mechanism. The label embed-
dings are trained to be classified to the correct re-
lation classes. Then, the learned label embeddings
are used as another attention over the bag to select
valid instances for relation extraction. Extensive
experiments have demonstrated that our model
significantly outperforms state-of-the-art methods.

In the future, we will explore other useful in-
formation (e.g., correlations among the relations
from KGs) available to improve the label embed-
dings.



3829

Acknowledgement

This work is supported by the National Natural
Science Foundation of China (No. 61806020,
61772082, 61702296), the National Key Re-
search and Development Program of China
(2017YFB0803304), the Beijing Municipal Nat-
ural Science Foundation (4182043), the CCF-
Tencent Open Fund, and the Fundamental Re-
search Funds for the Central Universities.

References
Zeynep Akata, Florent Perronnin, Zaı̈d Harchaoui, and

Cordelia Schmid. 2016. Label-embedding for image
classification. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 38:1425–1438.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In NIPS, pages 2787–2795.

Jinhua Du, Jingguang Han, Andy Way, and Dadong
Wan. 2018. Multi-level structured self-attentions for
distantly supervised relation extraction. In EMNLP,
pages 2216–2225.

Xiaocheng Feng, Jiang Guo, Bing Qin, Ting Liu, and
Yongjie Liu. 2017. Effective deep memory networks
for distant supervised relation extraction. In IJCAI,
pages 4002–4008.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2018a. Neu-
ral knowledge acquisition via mutual attention be-
tween knowledge graph and text. In AAAI, pages
4832–4839.

Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and
Peng Li. 2018b. Hierarchical relation extraction
with coarse-to-fine grained attention. In EMNLP,
pages 2236–2245.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL, pages 541–550.

Sharmistha Jat, Siddhesh Khandelwal, and Partha
Talukdar. 2018. Improving distantly supervised re-
lation extraction using word and entity based atten-
tion. arXiv preprint arXiv:1804.06987.

Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao, et al.
2017. Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In AAAI, pages 3060–3066.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In ACL, vol-
ume 1, pages 2124–2133.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL/IJCNLP, pages
1003–1011.

Denis Newman-Griffis, Albert M Lai, and Eric Fosler-
Lussier. 2018. Jointly embedding entities and text
with distant supervision. ACL 2018, page 195.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In ECML/PKDD, pages 148–163.

Jose A Rodriguez-Serrano, Albert Gordo, and Florent
Perronnin. 2015. Label embedding: A frugal base-
line for text recognition. International Journal of
Computer Vision, 113(3):193–207.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In
EMNLP-CoNLL, pages 455–465.

Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga,
Chiranjib Bhattacharyya, and Partha Talukdar. 2018.
Reside: Improving distantly-supervised neural rela-
tion extraction using side information. In EMNLP,
pages 1257–1266.

Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe
Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo
Henao, and Lawrence Carin. 2018. Joint embedding
of words and labels for text classification. In ACL,
volume 1, pages 2321–2331.

Jiacheng Xu, Kan Chen, Xipeng Qiu, and Xuanjing
Huang. 2016. Knowledge graph representation with
jointly structural and textual encoding. In IJCAI,
pages 1318–1324.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction
via piecewise convolutional neural networks. In
EMNLP, pages 1753–1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
lutional deep neural network. In COLING, pages
2335–2344.

Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and
Maosong Sun. 2017. Incorporating relation paths in
neural relation extraction. In EMNLP, pages 1768–
1777.

Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-
der J Smola, and Le Song. 2018. Variational reason-
ing for question answering with knowledge graph.
In AAAI, pages 6069–6076.

Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,
Jingfang Xu, and Xiaoyan Zhu. 2018. Com-
monsense knowledge aware conversation generation
with graph attention. In IJCAI, pages 4623–4629.


