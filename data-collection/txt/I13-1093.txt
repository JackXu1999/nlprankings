










































Little by Little: Semi Supervised Stemming through Stem Set Minimization


International Joint Conference on Natural Language Processing, pages 774–780,
Nagoya, Japan, 14-18 October 2013.

Little by Little: Semi Supervised Stemming through Stem Set
Minimization

Vasudevan N, Pushpak Bhattacharyya
Dept. of Computer Science and Engg.

IIT Bombay, Mumbai
{vasudevan,pb}@cse.iitb.ac.in

Abstract

In this paper we take an important step to-
wards completely unsupervised stemming
by giving a scheme for semi supervised
stemming. The input to the system is
a list of word forms and suffixes. The
motivation of the work comes from the
need to create a root or stem identifier
for a language that has electronic corpora
and some elementary linguistic work in
the form of, say, suffix list. The scope
of our work is suffix based morphology,
(i.e., no prefix or infix morphology). We
give two greedy algorithms for stemming.
We have performed extensive experimen-
tation with four languages: English, Hindi,
Malayalam and Marathi. Accuracy figures
ranges from 80% to 88% are reported for
all languages.

1 Introduction

Stemming is critical for many NLP, IR and IE
problems (Hull, 1996). In the current paper, we
report construction of a semi supervised stemmer
that does stemming by minimizing the total num-
ber of distinct stems. The input to the system is the
word list along with the legal suffix list of the lan-
guage. Even if a language does not have an elab-
orate linguistic tradition and exhaustive body of
linguistic work, the language is expected to have
at least the legal suffix list for nouns and verbs.

To get the intuition behind our work, consider
the word list {boy, boys, moss, mosses}. The split-
ting (the split is formally defined later) that gener-
ate the minimum number of stems (viz., 2) from
the above word list is {boy+φ, boy+s, moss+φ,
moss+es }, where φ is the null suffix. The mini-
mum stem set is {boy, moss}. Any other splitting,
say mosse+s will increase the number of stems.

This work is applicable to languages with con-
catenative morphology where suffixes stack one
after another. However, problems arise when there
are phonemic changes in the boundaries of the
stems and suffixes (sandhi). In such situation, ex-
istence of fused, composite suffixes in the suffix
list is assumed.

The roadmap of the paper is as follows. Related
work in morphology learning is explained in sec-
tion 2. Notations and terminologies used in this
paper are defined in section 3. In section 4 we
defined the stemming problem addressed in this
work. Two models for this stemming are proposed
in section 5 and section 6. In section 7, we de-
scribed various experiments conducted. The con-
clusions and future works are presented in section
8.

2 Related Work

Morphology learning is one of the widely at-
tempted problems in NLP. A recent survey by Har-
ald Hammarström (2011) gives an overall view of
unsupervised morphology learning. The Linguis-
tica (Goldsmith, 2001) model based on minimum
description length (MDL) principle is one of the
benchmark works of unsupervised stemming. In
the Linguistica model, the authors defined a sig-
nature structure. The objective of their stemming
approach is the minimization of total description
length, i.e., the description length of stem list, suf-
fix list, signatures and corpus.

Maximum a posteriori model (Creutz and La-
gus, 2007) is a generalization of the Linguistica
model, in the sense of being a recursive MDL.
This probabilistic approach is more suitable for
languages with more than one suffix. Stochastic
transducer based model (Clark, 2001) and gener-
ative probabilistic model (Snover et al., 2002) are
other relevant probabilistic models for stemming.

774



A Markov Random Field by Dreyer (2009) is also
a useful probabilistic approach related to unsuper-
vised morphology.

Graph based model (Johnson and Martin,
2003), lazy learning based model (van den
Bosch and Daelemans, 1999), clustering based
same stem identification model (Hammarström,
2006a; Hammarström, 2006b), ParaMor system
for paradigm learning (Monson et al., 2008) and
full morpheme segmentation and automatic in-
duction of orthographic rules (Dasgupta and Ng,
2007; Dasgupta and Ng, 2006) are also a relevant.

3 Terminology and Notation

Let us define some terms and notations used
throughout this paper.

w: word

W : input word list

N : number of words in the input word list

X: input suffix list

x: suffix candidate of w (possible suffix)

φ: null suffix, i.e., the suffix with zero length.

t: stem candidate of w (possible stem)

T : the set of possible stems from the word list

| · |: overloaded for the cardinality of a set and
the length of a string

‘+’: splitting (breakage) of string

‘·’: concatenation of strings

Stem: the longest common prefix of the in-
flected words of a lexeme. The stem from the set
of inflections of the lexeme play ({plays, playing
and played}) is play. Note that while a lexeme has
to be a meaningful word in the language, the stem
need not to be so. E.g., stem of the words lady and
ladies is lad, but the lexeme is lady.

Suffix: the portion(s) of the word after removing
its stem. E.g., the suffix of boys is s. The suffix
can be a null string (φ) or chain of suffixes (Words
in agglutinative languages can have multiple suf-
fixes. In this case, chain of suffixes is taken as a
single suffix.).

Split: the outcome of the process of segmenta-
tion (null strings permitted). A word can be seg-
mented in multiple ways, giving rise to multiple
splits. E.g., a split of boys can be b+oys, bo+ys,
boy+s or boys+φ. The correct split is the split that

separates a word in to its correct stem and correct
suffix. E.g., the boy+s is the correct split of boys.

Splitset: a set of splits obtained from the whole
input word list. For every word in the word list,
exactly one split will be there in the splitset. In
other words, splitset = { t + x | t · x ∈ W and
for any t′ + x′ ∈ splitset, t · x = t′ · x′ → t =
t′ and x = x′}. E.g., {bo+ys, girl+φ, play+ing}
is a splitset of {boys, girl, playing}. The correct
splitset is defined as the set of correct splits of all
the words from the given word list. The correct
splitset of {boys, girl, playing} is {boy+s, girl+φ,
play+ing}.

Ts(splitset): the set of stems from the splitset.
E.g., Ts({bo+ys, girl+φ, play+ing}) = {bo, girl,
play}.

Xs(splitset): the set of suffixes from the
splitset. E.g., Xs({bo+ys, girl+φ, play+ing}) =
{ys, φ, ing}.

4 Problem Definition

The stemming problem addressed in this paper is
defined as follows. Given a list of word forms W
and a list of suffixes X, the problem is to find the
correct splitset.

The suffix list of a language plays a crucial role
in this problem. The suffix list considered in this
problem should contain all atomic suffixes (E.g.,
s, es, ing) and its orthographic variants (E.g., iness
in happiness). The suffix list also should contain
chain of suffixes in case of agglutination. E.g., the
concatenated form of Malayalam1 plural marker
I³(kal) and genitive case marker qtS(ude) is I-
jqtS(kalude). This concatenated form should be
in the suffix list since it is the suffix (as per our
definition) of the word Iq½oIjqtS(kuttikalude)(of
children).

The suffix list X should be as large as possible
and X should be a superset of all suffixes in the
word list, i.e., Xs(correct splitset) ⊆ X. E.g.,
for the sample word list W = {boy, boys, moss,
mosses}, the set Xs(correct splitset) is { φ, s,
es}, where φ is the null suffix. So the suffix list
should contain at least φ, s and es.

The desired output of the above input is its
correct splitset, i.e., {boy+φ, boy+s, moss+φ,

1A morphologically rich language of India belonging to
the Dravidian family.

775



moss+es}. Two computational models for this
stemming problem is proposed in the section 5 and
section 6.

5 Minimum Stem Set Model for
Stemming

Consider the sample word list {boy, boys, moss,
mosses} and suffix list {φ, s, es}. Out of all pos-
sible splitsets of this input, the correct splitset,
{boy+φ, boy+s, moss+φ, moss+es} produces the
minimum number of distinct stems. This intuition
leads to the Minimum Stem Set model for stem-
ming. The Minimum Stem Set model (MSS) iden-
tifies the correct splitset by minimizing the num-
ber of distinct stems. In other words, this model
identifies the splitset with the minimum number
of distinct stems as the correct split.

Core of the MSS model is an optimization prob-
lem (MSS problem). The MSS problem is for-
mally stated as follows,

Input: A list of word forms (W ) and a set of
suffixes (X) such that Xs(correct splitset) ⊆ X

Output: argmin
splitset:Xs(splitset)⊆X

{

|Ts(splitset)|

}

5.1 Greedy Algorithm for MSS

Since the complexity of computing the MSS prob-
lem is NP-Hard (Vasudevan and Bhattacharyya,
2012), we designed an approximation algorithm
by utilizing similarity between our problem and
the set cover problem. Set cover problem is a
well known NP-Hard problem, which has a sim-
ple greedy approximation algorithm with approx-
imation factor of log(N) (Chvatal, 1979). This
approximation factor is the best attainable fac-
tor for the set cover problem (Feige, 1998). The
corresponding greedy algorithm for MSS problem
is the best polynomial time approximation algo-
rithm. This greedy approximation algorithm for
the MSS problem (Approx-MSS) is described be-
low.

Input of the Approx-MSS algorithm is a word
list W and a suffix list X. The algorithm first ini-
tializes a set of all possible stems T . This can be
done by stripping suffixes in X from end of each
word in W . Then it initialize sets of all possible in-
flections of each t in T , let’s call Infl(t). Infl(t)
can be initialized by appending suffixes from X to
t. If a word created by appending a suffix x in X to

t is in W , then add that word into Infl(t). Set of
all possible stems (T ) and Infl(t) of the running
example is shown in Table 1.

After the initialization, the algorithm start the
iterations with an empty splitset. In the first
step, it chooses a stem t from T that has max-
imum |Infl(t) ∩ W |. I.e., it finds a t∗ =
argmax

t∈T
{|Infl(t) ∩ W |}. In the next step, for

all words (w) in Infl(t∗) ∩ W , the split t∗ + x
is added to splitset, where x is the suffix of word
w after the stem t∗. Then it removes all words
from W whose splits are added to splitset. This
process is repeated until the splitset is complete,
i.e., for all words there is a split in the splitset.
The complexity of this approximation algorithm is
O(|W ||X|) and approximation factor is log(|W |).

Consider the example shown in Table 1. Ini-
tially both boy and moss have highest |Infl(t) ∩
W |. So the greedy algorithm chooses either one
of them in the first step as t∗. In the next step
it chooses the other one. By these two steps, the
greedy algorithm identifies the correct split of all
four words.

T mosses mosse moss mos boys boy

Infl(t) {mosses} {mosses} {mosses,
moss}

{moss} {boys} {boys,
boy}

Table 1: Possible Stems, their Infl()

6 Weighted Minimum Stem Set (WMSS)
Model

MSS problem uses the information from other
words to identify the stem of each word. If the
word list doesn’t have any other inflections of a
word, then MSS cannot choose the stem properly.
In this case MSS randomly selects one of the pos-
sible stems. This is one of the main drawbacks
of MSS. Languages with poor morphology have a
lesser number of inflections than that of language
with rich morphology. So in a word list with fixed
number of words, the above problem is more seri-
ous for morphologically poor languages.

We extended the MSS model to a Weighted
Minimum Stem Set (WMSS) model, which re-
duces the number of distinct stems and the number
of splits with non empty suffixes. Output of this
model is also a splitset. Consider a small word list
{boy, boys, moss, mosses} and a suffix list {φ, s,
es, ses}. In this case both {boy+φ, boy+s, moss+φ,

776



moss+es} and {boy+φ, boy+s, mos+s, mos+ses}
are optimum solutions for MSS problem. In such
a tie situation, the WMSS model prefer the splitset
with more number of null suffix (φ), i.e., the first
one. From our knowledge about English language,
we can see that the first one is the correct splitset.

In the WMSS model, a weight function wg(t)
is defined for each and every possible stem t as
wg(t) = 1 + [t/∈W ]|W | . Where [t /∈ W ] is the Iverson
bracket (Weisstein, Online 30 04 2010), i.e., it is
1 if t /∈ W , 0 otherwise. WMSS will find out a
splitset such that the total weight of all stems in
Ts(splitset) is minimum. Let’s define the prob-
lem in WMSS model formally.

Input: A list of word forms (W ) and a set of
suffixes (X) such that Xs(correct splitset) ⊆ X

Output: argmin
splitset:Xs(splitset)⊆X

{

∑

t∈Ts(splitset)

wg(t)

}

In this extended problem formulation, wg(t)
contain two terms. The first term, the constant 1
is for reducing the number of distinct stems and
the second term, [t/∈W ]|W | is for reducing the number
of splits with non empty suffixes. If there is no
second term then wg(t) = 1 and it is exactly the
same as MSS problem.

Since the maximum value of the second
term in WMSS is 1|W | and maximum num-
ber of stems in any Ts(splitset) is less than
|W |, |Ts(splitset)| <

∑

t∈Ts(splitset)
wg(t) <

|Ts(splitset)| + 1. Therefore any solution of
WMSS should be a solution of MSS, but the re-
verse is false. Relevance of this WMSS problem
comes only if there are multiple solutions for MSS
problem.

Since the solution of WMSS problem is a so-
lution of MSS problem, the reduction from MSS
to WMSS is trivial. Suppose WMSS have a poly-
nomial time algorithm, then we can use that algo-
rithm for MSS problem also. Since MSS is NP-
Hard we can say that, WMSS is also NP-Hard.

6.1 Greedy Algorithm for WMSS

The WMSS problem can be solved effectively
by utilizing its similarity with weighted set cover
problem. Weighted set cover problem is also an
NP-Hard problem, and its greedy approximation
have a bound of log(N). The greedy algorithm for
weighted set cover problem is adapted for WMSS

problem. The corresponding greedy algorithm for
WMSS (Approx-WMSS) is explained below.

The Approx-WMSS is similar to Approx-MSS.
The only difference is in the first step. While
Approx-MSS algorithm selects a stem with max-
imum |Infl(t) ∩ W | in the first step, Approx-
WMSS algorithm selects a stem with maximum
|Infl(t)∩W |

wg(t) . Note that, when wg(t) is 1 then both
terms are the same. All remaining steps are the
same for both algorithms. Similar to Approx-MSS
algorithm, the complexity of this approximation
algorithm is O(|W ||X|) and approximation factor
is log(|W |).

Consider the W = {boy, boys, moss, mosses}
and X = {φ, s, es, ses}. The set of all possi-
ble stems and its corresponding Infl() and wg()
are shown in Table 2. Initially boy has the highest
|Infl(t)∩W |

wg(t) . So this greedy algorithm chooses the
stem boy and add boy+φ and boy+s to splitset in
the first step. In the next step it chooses moss and
add moss+es and moss+φ to splitset. By these two
steps, this greedy algorithm terminate by identify-
ing correct splitset.

T mosses mosse moss mos boys boy

Infl(t) {mosses} {mosses} {mosses,
moss}

{moss} {boys} {boys,
boy}

wg(t) 1 1 + 14 1 1 +
1
4 1 1

Table 2: Possible Stems, their Infl() and wg()

7 Experimentation

Two new stemming systems based on the greedy
algorithms for MSS problem and WMSS prob-
lem are implemented. Performances of these sys-
tems are evaluated for four languages from Indo-
European family and Dravidian family. The se-
lected languages are English, Hindi, Marathi and
Malayalam, in the increasing order of morpho-
logical complexity. First three languages are
from Indo-European family while the fourth lan-
guage, Malayalam is a highly agglutinative lan-
guage from Dravidian family. These spectrum of
languages from different families with different
morphological richness is necessary for the eval-
uation of the suitability of proposed models.

Performance of proposed models are compared
with different baselines. The first baseline is a ran-
dom stem selection, which randomly selects a split
for each word such that the suffix in this split is in
the input suffix list. The length of the suffix (or

777



stem) is another information that can provide sec-
ond and third baselines. The second one selects a
split for each word form that has the smallest stem,
albeit with the suffix in the input suffix list. Simi-
larly the third one selects the split with the largest
stem.

Linguistica is an MDL based system that iden-
tifies stem of each word in a word list without us-
ing any other input. One of the heuristics used in
Linguistica model is modified to make the fourth
baseline. In the Linguistica heuristics, a probabil-
ity is assigned to every split for every word. Then
iteratively it learns the best probability distribu-
tion by optimizing a figure of merit, which is a
function of length and frequency of morphemes.
Since there is no need to consider any split with
a suffix which is not in the input suffix list, the
sample space can be minimized. Probability distri-
bution after this modification is learned using the
same iterative procedure as in Linguistica. We im-
plemented this modified Linguistica algorithm and
considered it as fourth baseline.

7.1 Data Analysis

Word list of size 10,000 distinct words in Unicode
format were selected for English, Hindi, Marathi
and Malayalam. English words are taken from
Brown and BNC corpora (Francis and Kucera,
1964; Edition, 2007). Selected Hindi words are
from tourism and news corpus. The source of
Marathi words for experimentation is the cor-
pora from the Indian Language Corpora Initia-
tive (ILCI) project, which is a Government of In-
dia effort (http://www.tdil.mit.gov.in). Malayalam
words are obtained from IIITMK2 and from var-
ious blogs and newspapers. For each words the
correct stem as per the definition, i.e., the largest
prefix of all inflected forms of the lexeme, is iden-
tified for the evaluation. Suffix lists are mainly
created from the words in the word list. By adding
available suffixes from web, the suffix lists are ex-
panded as big as possible.

Counts and frequencies of stems and suffixes
are relevant statistics to reflect the nature of word
list for stemming. So the number of distinct stems
(StCount) and suffixes (SfCount) are counted from
each word list. The average stem frequencies
(StFreq) and average suffix frequencies (SfFreq)

2Indian Institute of Information Technology and
Management-Kerala

are also measured from word lists of all four lan-
guages. These measured values are shown in Table
3.

Language StCount StFreq SfCount SfFreq |X|

English 4974 2.01 43 232.58 436
Hindi 4792 2.09 134 74.63 726
Marathi 4086 2.45 604 16.56 1958
Malayalam 1077 9.29 762 13.12 26248

Table 3: Statistics of Word List and Suffix List (X)

Number of distinct stems in the word form list
decreases and average stem frequencies increases
along with morphological complexity of language.
Similarly the number of distinct suffixes in the
word list increases and average suffix frequencies
decreases along with morphological complexity.
We can also see that the number of suffixes in a
language also increases with morphological com-
plexity. Since these patterns are quite intuitive,
the data taken for experiments seems to be proper
samples that represents the languages.

7.2 Results and Discussion

The accuracy of four baselines and two newly pro-
posed systems for four languages are tabulated in
Table 4. The results indicates the effectiveness of
the new systems over baseline systems across var-
ious languages. Improvement in the performance
of WMSS over MSS is also clearly visible in the
table. Above 80% accuracies for all languages
are obtained by using the WMSS model. English,
Hindi, Marathi and Malayalam are the languages
in the increasing order of morphological complex-
ity. We can observe that the accuracies are de-
creasing along with the morphological complexity
of language. This indicates stemming is difficult
for morphologically complex languages.

Language Random
Stem

Largest
Stem

Smallest
Stem

Modified
Lin-
guistica

MSS WMSS

English 44.98 47.39 49.36 53.82 84.44 88.86
Hindi 50.68 43.04 57.44 62.74 80.71 83.98
Marathi 41.66 30.44 69.766 59.33 78.28 80.19
Malayalam 19.31 3.51 57.58 65.86 78.32 80.06

Table 4: Stemming Accuracies in Percentage

For all four languages, the baseline which se-
lects stems with maximum length have a lesser
score than the baseline which selects stems with
minimum length. This shows, if there are more

778



than one stem candidate, then smaller stems is pre-
ferred. Since null suffix is present in the suffix
list, maximum stem length baseline always selects
the word itself as its stem. So the low score for
maximum stem length baseline for Malayalam in-
dicates, most of the Malayalam words are in the
inflected form. To get better insight about the re-
maining issues an error analysis of the output sam-
ple is required.

7.3 Error Analysis

To get better insight about the remaining issues,
erroneous samples generated by the best perform-
ing system, i.e., WMSS, are categorized in to un-
der stemming 3, over stemming 4 and weight error.
The weight error is the case where the correct stem
is in the word list but the identified incorrect stem
is not. Such errors can be corrected by modify-
ing the weight function in the WMSS formulation.
Percentage of errors in various categories are tab-
ulated in Table 5.

If the number of suffixes in the word list is very
small compared to the total number of suffixes in
the suffix list, then there is a high chance for over-
stemming. So the ratio between total number of
suffixes and the number of suffixes in the word list
(suffix ratio), for all four languages are also in-
cluded in Table 5.

Language Under-
Stemming

Over-
Stemming

Weight-
Error

Suffix
ratio

English 2.76 8.38 3.74 10.0
Hindi 3.90 12.12 5.94 5.42
Marathi 8.36 11.45 5.57 3.24
Malayalam 0.93 19.01 0.24 34.5

Table 5: Percentage of Errors and Suffix ratios

Suffix ratio of English is high (10). It decreases
in Hindi, and further decreases in Marathi. Ac-
cording to this pattern, the under stemming errors
are very few (only 3%) in English and it increases
in Hindi and Marathi. The suffix ratio of Malay-
alam is higher than English so the under stemming
errors are negligibly small (less than 1%). The re-
lation between suffix ratio and under stemming er-
rors are clearly visible from these numbers. So to
reduce the under stemming errors, we need to in-
crease the number of input suffixes.

3identified stem is longer than correct stem, e.g., mosse in
mosses

4identified stem is shorter than correct stem, e.g., s in sing

The over stemming errors increases from En-
glish to Malayalam. This indicates that the over
stemming errors are more sensitive to morphologi-
cal complexity than the suffix ratio. From the table
we can see that, weight errors are significant ex-
cept in Malayalam. This indicates the requirement
of weight modification. Also, we can see that the
weight errors are high in Hindi and Marathi, and
hence the weight modification is crucial for these
languages.

After the analysis, the main observation is about
the importance of weight modification. Some
sample words from all four languages are shown
in Figure 1.

Figure 1: Output Samples (WMSS)

8 Conclusion

Two algorithms for stemming, that produces a
mapping from words to stems by minimizing the
number of stems upto a limit, given a word list
and a suffix list are proposed and implemented.
Stemming systems that use these algorithms are
evaluated using languages from Indo European
and Dravidian families. Moderate to high accu-
racies of stemming are obtained in case of for all
four languages: English, Hindi, Malayalam and
Marathi.

Collecting a word list is relatively an easy task
for a new language. But, collecting a complete
list of suffixes is a much more involved task since
detailed linguistic work is required. So completely
unsupervised stemming is our future work. Stems
will be produced from only the word form list.

779



References

V. Chvatal. 1979. A greedy heuristic for the set-
covering problem. Mathematics of Operations Re-
search, 4(3):pp. 233–235.

Alexander Clark. 2001. Partially supervised learning
of morphology with stochastic transducers. In NL-
PRS, pages 341–348.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. TSLP, 4(1).

Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, pages 311–330.

Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In HLT-NAACL, pages 155–163.

Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proc. of EMNLP-
09, pages 101–110.

The British National Corpus, Version 3 BNC XML
Edition. 2007. Distributed by Oxford University
Computing Services on behalf of the BNC Consor-
tium.

Uriel Feige. 1998. A threshold of ln n for approximat-
ing set cover. JOURNAL OF THE ACM, 45:314–
318.

Withrop N. Francis and Henry Kucera. 1964. Manual
of Information to accompany A standard corpus of
present-day edited American English, for use with
digital computers with Digital Computers. Brown
University Press.

John A. Goldsmith. 2001. Unsupervised learning of
the morphology of a natural language. CL, (2):153–
198.

Harald Hammarström and Lars Borin. 2011. Unsuper-
vised learning of morphology. CL, (2):309–350.

Harald Hammarström. 2006a. A naive theory of af-
fixation and an algorithm for extraction. In Proc. of
HLT-NAACL-06, pages 79–88, June.

Harald Hammarström. 2006b. Poor man’s stemming:
Unsupervised recognition of same-stem words. In
AIRS, pages 323–337.

David A. Hull. 1996. Stemming algorithms: A case
study for detailed evaluation. JASIS, 47(1):70–84.

Howard Johnson and Joel Martin. 2003. Unsupervised
learning of morphology for english and inuktitut. In
Proc. of NAACL-HLT-03, pages 43–45.

Christian Monson, Jaime G. Carbonell, Alon Lavie,
and Lori S. Levin. 2008. Paramor and morpho chal-
lenge 2008. In CLEF, pages 967–974.

Matthew G. Snover, Gaja E. Jarosz, and Michael R.
Brent. 2002. Unsupervised learning of morphology
using a novel directed search algorithm: taking the
first step. In Proc. of ACL-WMPL-02, pages 11–20.

Antal van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proc. of
ACL-99.

N. Vasudevan and Pushpak Bhattacharyya. 2012. Op-
timal stem identification in presence of suffix list. In
CICLing (1), pages 92–103.

Eric W Weisstein. [Online; 30-04-2010]. Iverson
bracket. MathWorldA Wolfram Web Resources.
http://mathworld.wolfram.com/IversonBracket.html.

780


