



















































Unsupervised Bilingual Lexicon Induction via Latent Variable Models


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 621–626
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

621

Unsupervised Bilingual Lexicon Induction via Latent Variable Models

Zi-Yi Dou Zhi-Hao Zhou
State Key Laboratory for Novel Software Technology, Nanjing University, China
{douzy@smail, zhouzh@nlp, huangsj@nlp}.nju.edu.cn

Shujian Huang

Abstract

Bilingual lexicon extraction has been studied
for decades and most previous methods have
relied on parallel corpora or bilingual dictio-
naries. Recent studies have shown that it
is possible to build a bilingual dictionary by
aligning monolingual word embedding spaces
in an unsupervised way. With the recent ad-
vances in generative models, we propose a
novel approach which builds cross-lingual dic-
tionaries via latent variable models and adver-
sarial training with no parallel corpora. To
demonstrate the effectiveness of our approach,
we evaluate our approach on several language
pairs and the experimental results show that
our model could achieve competitive and even
superior performance compared with several
state-of-the-art models.

1 Introduction

Learning the representations of languages is a fun-
damental problem in natural language processing
and most existing methods exploit the hypothesis
that words occurring in similar contexts tend to
have similar meanings (Pennington et al., 2014;
Bojanowski et al., 2017), which could lead word
vectors to capture semantic information. Mikolov
et al. (2013) first point out that word embeddings
learned on separate monolingual corpora exhibit
similar structures. Based on this finding, they sug-
gest it is possible to learn a linear mapping from a
source to a target embedding space and then gener-
ate bilingual dictionaries. This simple yet effective
approach has led researchers to investigate on im-
proving cross-lingual word embeddings with the
help of bilingual word lexicons (Faruqui and Dyer,
2014; Xing et al., 2015).

For low-resource languages and domains, cross-
lingual signal would be hard and expensive to ob-
tain, and thus it is necessary to reduce the need for
bilingual supervision. Artetxe et al. (2017) suc-
cessfully learn bilingual word embeddings with

only a parallel vocabulary of aligned digits. Zhang
et al. (2017) utilize adversarial training to obtain
cross-lingual word embeddings without any paral-
lel data. However, their performance is still signif-
icantly worse than supervised methods. By com-
bining the merits of several previous works, Con-
neau et al. (2018) introduce a model that reaches
and even outperforms supervised state-of-the-art
methods with no parallel data.

In recent years, generative models have be-
come more and more powerful. Both Genera-
tive Adversarial Networks (GANs) (Goodfellow
et al., 2014) and Variational Autoencoders (VAEs)
(Kingma and Welling, 2014) are prominent ones.
In this work, we borrow the ideas from both GANs
and VAEs to tackle the problem of bilingual lex-
icon induction. The basic idea is to learn latent
variables that could capture semantic meaning of
words, which would be helpful for bilingual lex-
icon induction. We also utilize adversarial train-
ing for our model and require no form of super-
vision. We evaluate our approach on several lan-
guage pairs and experimental results demonstrate
that our model could achieve promising perfor-
mance. We further combine our model with sev-
eral helpful techniques and show our model could
perform competitively and even superiorly com-
pared with several state-of-the-art methods.

2 Related Work

2.1 Bilingual Lexicon Induction

Extracting bilingual lexica has been studied by re-
searchers for a long time. Mikolov et al. (2013)
first observe there is isomorphic structure among
word embeddings trained separately on monolin-
gual corpora and they learn the linear transforma-
tion between languages. Zhang et al. (2016b) im-
prove the method by constraining the transforma-
tion matrix to be orthogonal. Xing et al. (2015) in-
corporate length normalization during training and



622

D

source target

qфs

pθs pθt

qфt

Figure 1: Illustration of our model. φs and φt map
the source and target word embeddings into latent vari-
ables. Discriminator D guides the two latent distribu-
tions to be the same.

maximize the cosine similarity instead. They point
out that adding an orthogonality constraint can im-
prove performance and has a closed-form solution,
which was referred to as Procrustes approach in
Smith et al. (2017). Canonical correlation analy-
sis has also been used to map both languages to a
shared vector space (Faruqui and Dyer, 2014; Lu
et al., 2015).

To reduce the need for supervision signals,
Artetxe et al. (2017) use identical digits and num-
bers to form an initial seed dictionary and then
iteratively refine their results until convergence.
Zhang et al. (2017) apply adversarial training to
align monolingual word vector spaces with no
supervision. Conneau et al. (2018) improve the
model by combining adversarial training and Pro-
crustes approach, and their unsupervised approach
could reach and even outperform state-of-the-art
supervised approaches. In this work, we make
further improvements and enhance the model pro-
posed in (Conneau et al., 2018) with latent variable
model and iterative training procedure.

2.2 Generative Models
VAEs (Kingma and Welling, 2014) represent one
of the most successful deep generative models.
Standard VAEs assume observed variables are
generated from latent variables and the latent vari-
ables are sampled from a simple Gaussian distri-
bution. Typically, VAEs utilize an neural infer-
ence model to approximate the intractable poste-
rior, and optimize model parameters jointly with

a reparameterized variational lower bound. VAEs
have been successfully applied in several natural
language processing tasks before (Zhang et al.,
2016a; Bowman et al., 2016).

GANs (Goodfellow et al., 2014) are another
framework for estimating generative models via
an adversarial process and have attracted huge at-
tention. The basic strategy is to train a generative
model and a discriminative model simultaneously
via an adversarial process. Adversarial training
technique for matching distribution has proven to
be powerful in a variety of tasks (Bowman et al.,
2016). Adversarial Autoencoder (Makhzani et al.,
2015) is a probabilistic autoencoder that uses the
GANs to perform variational inference. By com-
bining a VAE with a GAN, Larsen et al. (2016) use
learned feature representations in the GAN dis-
criminator as the basis for the VAE reconstruction
objective. GANs have been applied in machine
translation before (Yang et al., 2018; Lample et al.,
2018).

3 Proposed Approach

In this section, we first briefly introduce VAEs,
and then we illustrate the details and training tech-
niques of our proposed model.

3.1 Variational Autoencoder
Variational Autoencoders (VAEs) are deep genera-
tive model which are capable of learning complex
density models for data via latent variables. Given
a nonlinear generative model pθ(x|z) with input
x ∈ RD and associated latent variable z ∈ RL
drawn from a prior distribution p0(z), the goal of
VAEs is to use a recognition model, qφ(z|x) to ap-
proximate the posterior distribution of the latent
variables by maximizing the following variational
lower bound

Lθ,φ = Eqφ(z|x)[log pθ(x|z)]−KL(qφ(z|x)||p0(z)),
(1)

where KL refers to Kullback-Leibler divergence.

3.2 Our Model
Basically, our model assumes that the source word
embedding {xn} and the target word embedding
{yn} could be drawn from a same latent variable
space {zn}, where {zn} is capable of capturing
semantic meaning of words.

In contrast to the standard VAE prior that as-
sumes each latent embedding zn to be drawn from
the same latent Gaussian, our model just requires



623

the distribution of latent variables for source and
target word embeddings to be equal. To achieve
such a goal, we utilize adversarial training to guide
the two latent distributions to match with each
other.

As in adversarial training, we have networks φs
and φt for both source and target space, striving
to map words into the same latent space, while
the discriminator D is a binary classifier which
tries to distinguish between the two languages. We
also have reconstruction networks θs and θt as in
VAEs.

The objective function for the discriminator D
could be formulated as

LD = Ezy∼qφt (z|y)[logD(zy)]

+ Ezx∼qφs (z|x)[log(1−D(zx))].
(2)

For the source side, the objective is to minimize

Lφs,θs = Ezx∼qφs (z|x)[log pθs(x|zx)]
− Ezx∼qφs (z|x)[logD(zx)].

(3)

Here we define qφs(z|x) = N (µs(x),Σs(x)),
where µs(x) = Wµsx and Σs(x) = exp(Wσsx);
Wµs and Wσs are learned parameters. We also de-
fine the mean of pθs(x|z) to be WTµsz. The objec-
tive function and structure for φt are similar.

The basic framework of our model is shown in
Figure 1. As we could see from the figure, our
model tries to map the source and target word em-
bedding into the same latent space which could
capture the semantic meaning of words.

Theoretical analysis has revealed that adversar-
ial training tries to minimize the Jensen-Shannon
(JS) divergence between the real and fake distribu-
tion. Therefore, one can view our model as replace
KL divergence in Equation 1 with JS divergence
and change the Gaussian prior to the target distri-
bution.

3.3 Training Strategy

Our model has two generators φs and φt, and we
have found that training them jointly would be ex-
tremely unstable. In this paper, we propose an
iterative method to train our models. Basically,
we first initialize Wµt to be identity matrix and
train φs and θs on the source side. After conver-
gence, we freeze Wµs , and then train φt and θt in
the target side. The pseudo-code for this process
is shown in Algorithm 1. It should be noted that
there is no variance once completing training.

Algorithm 1 Training Strategy
1: Wµt = I
2: for i = 1, · · · , niter do
3: while φs and θs have not converged do
4: Update discriminator D
5: Update φs and θs
6: end while
7: while φt and θt have not converged do
8: Update discriminator D
9: Update φt and θt

10: end while
11: end for

4 Experiment

Our experiments could be divided into two parts.
In the first part, we conduct experiments on small-
scale datasets and our main baseline is Zhang et al.
(2017). In the second part, we combine our model
with several advanced techniques and we compare
our model with Conneau et al. (2018) on large-
scale datasets.

#tokens vocab. size

zh-en
zh 21m 3,349
en 53m 5,154

es-en
es 61m 4,774
en 95m 6,637

it-en
it 73m 8,490
en 93m 6,597

Table 1: Statistics of the non-parallel corpora. Lan-
guage codes: zh = Chinese, en = English, es = Spanish,
it = Italian.

4.1 Small-scale Datasets

In this section, our experiments focus on small-
scale datasets and our main baseline model is ad-
versarial autoencoder (Zhang et al., 2017). For
justice, we use the same model selection strategy
with Zhang et al. (2017), i.e. we choose the model
whose sum of reconstruction loss and classifica-
tion accuracy is the least. The source and target
word embeddings would be first mapped into the
latent space. For each source word embedding x,
it would be first transformed into zx. The the its k
nearest target embeddings would be retrieved and
be compared against the entry in a ground truth
bilingual lexicon. Performance is measured by
top-1 accuracy.



624

4.1.1 Experiments on Chinese-English
Dataset

For this set of experiments, we use the same data
as Zhang et al. (2017). The statistics of the fi-
nal training data is given in Table 1. We use
Chinese-English Translation Lexicon Version 3.0
(LDC2002L27) as our ground truth bilingual lexi-
con for evaluation.

The baseline models are MonoGiza system
(Dou et al., 2015), translation matrix (TM)
(Mikolov et al., 2013), isometric alignment (IA)
(Zhang et al., 2016b) and adversarial training ap-
proach (Zhang et al., 2017).

Table 2 summarizes the performance of baseline
models and our approach. The results of baseline
models are cited from Zhang et al. (2017). As we
can see from the table, our model could achieve
superior performance compared with other base-
line models. Table 3 lists some word translation
examples given by our model.

Model #seeds Accuracy (%)
MonoGiza w/o emb. 0 0.05
MonoGiza w/ emb. 0 0.09

TM 50 0.29
IA 100 21.79

Zhang et al. (2017) 0 43.31
Ours 0 51.37

Table 2: Experimental results on Chinese-English
dataset. The results of baseline models are cited from
Zhang et al. (2017).

航空 铁路 时代 学校

airline rail antiquity school
aviation railway era education
airliner railroad century college
service freight medieval student
flight metro historian teacher

Table 3: Word translation examples for Chinese-
English dataset. Ground truths are marked in bold.

4.1.2 Experiments on Other Language Pairs
Datasets

We also conduct experiments on Spanish-English
and Italian-English language pairs. Again, we use
the same dataset with Zhang et al. (2017). and the
statistics are shown in Table 1. The ground truth
bilingual lexica for Spanish-English and Italian-

English are obtained from Multilingual Unsuper-
vised and Supervised Embeddings (MUSE) 1.

Model Accuracy (%)

es-en
Zhang et al. (2017) 69.22

Ours 75.21

it-en
Zhang et al. (2017) 55.31

Ours 61.08

Table 4: Experimental results on Spanish-English and
Italian-English datasets.

The experimental results are shown in Table 4.
Because Spanish, Italian and English are closely
related languages, the accuracy would be higher
than the Chinese-English dataset. Our model is
able to outperform baseline model in this setting.

Model
Accuracy (%)

en-es en-ru en-zh
Methods without refinement

Adv-NN 69.8 29.1 18.5
Adv-CSLS 75.7 37.2 23.4
Ours-NN 71.8 32.8 22.9

Ours-CSLS 76.6 39.3 26.0
Methods with refinement

Adv-Refine-NN 79.1 37.3 30.9
Adv-Refine-CSLS 81.7 44.0 32.5
Ours-Refine-NN 79.1 42.7 32.5

Ours-Refine-CSLS 82.1 48.7 33.3

Table 5: Experimental results on large-scale datasets.
Language codes: en=English, es = Spanish, ru = Rus-
sian, zh = Chinese.

4.2 Large-scale Datasets

In this section, we integrate our method with Con-
neau et al. (2018), whose method improves Zhang
et al. (2017) by more sophiscated refinement pro-
cedure and validation criterion. We replace their
first step, namely the adversarial training step,
with our model. Basically, we first map the source
and target embeddings into the latent space us-
ing our algorithm, and then fine-tune the identity
mapping in the latent space with the closed-form
Procrustes solution. We use their similarity mea-
sure, namely cross-domain similarity local scaling
(CSLS), to produce reliable matching pairs and
validation criterion for unsupervised model selec-
tion.

1https://github.com/facebookresearch/MUSE



625

We conduct experiments on English-Spanish,
English-Russian and English-Chinese datasets,
which are the same as Conneau et al. (2018).
The results are shown in Table 5. As seen, our
model could consistently achieve better perfor-
mance compared with adversarial training. After
refinement, our model could further achieve com-
petitive and even superior results compared with
state-of-the-art unsupervised methods. This fur-
ther demonstrates the capacity of our model.

5 Conclusion

Based on the assumption that word vectors in dif-
ferent languages could be drawn from a same la-
tent variable space, we propose a novel approach
which builds cross-lingual dictionaries via latent
variable models and adversarial training with no
parallel corpora. Experimental results on several
language pairs have demonstrated the effective-
ness and universality of our model. We hope our
method could be beneficial to other areas such as
unsupervised machine translation (Lample et al.,
2018).

Future directions include validate our model on
more realistic scenarios (Dinu et al., 2015) as well
as combine our algorithms with more sophiscated
adversarial networks (Arjovsky et al., 2017; Gul-
rajani et al., 2017).

Acknowledgments

We thank all the anonymous reviewers for their in-
sightful comments.

References
Martin Arjovsky, Soumith Chintala, and Léon Bottou.

2017. Wasserstein generative adversarial networks.
In International Conference on Machine Learning
(ICML).

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics (TACL).

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew Dai, Rafal Jozefowicz, and Samy Bengio.
2016. Generating sentences from a continuous
space. In SIGNLL Conference on Computational
Natural Language Learning (CoNLL).

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Inter-
national Conference on Learning Representations
(ICLR).

Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2015. Improving zero-shot learning by mit-
igating the hubness problem. International Con-
ference on Learning Representations (ICLR), Work-
shop Track.

Qing Dou, Ashish Vaswani, Kevin Knight, and Chris
Dyer. 2015. Unifying bayesian inference and vec-
tor space models for improved decipherment. In In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Annual Meeting of the Association
for Computational Linguistics (ACL).

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Conference on Neural Information
Processing Systems (NIPS).

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-
cent Dumoulin, and Aaron C Courville. 2017. Im-
proved training of wasserstein gans. In Advances in
Neural Information Processing Systems (NIPS).

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In International Con-
ference on Learning Representations (ICLR).

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2018. Unsupervised
machine translation using monolingual corpora
only. International Conference on Learning
Representations (ICLR).

Anders Boesen Lindbo Larsen, Søren Kaae Sønderby,
Hugo Larochelle, and Ole Winther. 2016. Autoen-
coding beyond pixels using a learned similarity met-
ric. In International Conference on Machine Learn-
ing (ICML).

Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel,
and Karen Livescu. 2015. Deep multilingual cor-
relation for improved word embeddings. In An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics
(NAACL).

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly,
Ian Goodfellow, and Brendan Frey. 2015. Adversar-
ial autoencoders. arXiv preprint arXiv:1511.05644.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.



626

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. In International Conference on Learning
Representations (ICLR).

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2018.
Improving neural machine translation with condi-
tional sequence generative adversarial nets. In An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics
(NAACL).

Biao Zhang, Deyi Xiong, Hong Duan, Min Zhang,
et al. 2016a. Variational neural machine transla-
tion. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Adversarial training for unsupervised
bilingual lexicon induction. In Annual Meeting
of the Association for Computational Linguistics
(ACL).

Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi Jaakkola. 2016b. Ten pairs to tag–
multilingual pos tagging via coarse mapping be-
tween embeddings. In Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL).


