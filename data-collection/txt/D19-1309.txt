



















































An End-to-End Generative Architecture for Paraphrase Generation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3132–3142,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3132

An End-to-End Generative Architecture for Paraphrase Generation

Qian Yang1∗ , Zhouyuan Huo2, Dinghan Shen1

Yong Cheng3, Wenlin Wang1, Guoyin Wang1, Lawrence Carin1

1 Duke University 2 University of Pittsburgh 3 Google AI
qian.yang@duke.edu

Abstract

Generating high-quality paraphrases is a fun-
damental yet challenging natural language
processing task. Despite the effectiveness of
previous work based on generative models,
there remain problems with exposure bias in
recurrent neural networks, and often a failure
to generate realistic sentences. To overcome
these challenges, we propose the first end-
to-end conditional generative architecture for
generating paraphrases via adversarial train-
ing, which does not depend on extra linguis-
tic information. Extensive experiments on
four public datasets demonstrate the proposed
method achieves state-of-the-art results, out-
performing previous generative architectures
on both automatic metrics (BLEU, METEOR,
and TER) and human evaluations.

1 Introduction

Paraphrases convey the same meaning as the orig-
inal sentences or text, but with different expres-
sions in the same language. Paraphrase genera-
tion aims to synthesize paraphrases of a given sen-
tence automatically. This is a fundamental natu-
ral language processing (NLP) task, and it is im-
portant for many downstream applications (Mad-
nani and Dorr, 2010; Yang et al., 2016, 2017; Pas-
sonneau et al., 2018). For example, paraphrases
can help diversify the response of chatbot engines
(Yan et al., 2016), strengthen question answering
(Harabagiu and Hickl, 2006; Duboue and Chu-
Carroll, 2006; Fader et al., 2014), augment relation
extraction (Romano et al., 2006), and extend the
coverage of semantic parsers (Berant and Liang,
2014).

Generating accurate and diverse paraphrases
automatically is still very challenging. Traditional
methods (McKeown, 1983; Bolshakov and Gel-
bukh, 2004; Zhao et al., 2008) exploit how linguis-

∗ Corresponding author

tic knowledge can improve the quality of gener-
ated paraphrases, including shallow linguistic fea-
tures (Zhao et al., 2009), and syntactic and seman-
tic information (Kozlowski et al., 2003; Ellsworth
and Janin, 2007). However, they are often domain-
specific and hard to scale, or yield inferior results.

With the help of growing large data and neu-
ral network models, recent studies have shown
promising results. Three families of deep learn-
ing architectures have been investigated with the
goal of generating high-quality paraphrases. The
first is to formulate paraphrase generation as a
sequence-to-sequence problem, following experi-
ence from machine translation (Bahdanau et al.,
2014; Cheng et al., 2016). Prakash et al. (2016)
proposes stacked residual long short-term mem-
ory networks (LSTMs), while Cao et al. (2017)
makes use of the gated recurrent unit (GRU) as
the recurrent unit. Shortly afterwards, several
works have focused on providing extra informa-
tion to enhance the encoder-decoder model. Ma
et al. (2018) adds distributed word representations,
Huang et al. (2018) considers a paraphrased dic-
tionary and

Iyyer et al. (2018) utilizes a syntactic parser.
Recently Wang et al. (2018) utilizes a semantic
augmented Transformer (Vaswani et al., 2017);
however, its accuracy largely depends on extra se-
mantic information. The second family of models
employs reinforcement learning (Li et al., 2017b).
The third family is the generative architecture that
we focus on. Specifically, Gupta et al. (2018)
applies the conditional variational autoencoders
(CVAEs) (Sohn et al., 2015) with LSTM encoder-
decoders in generating paraphrases, expecting
RNN’s advantage of modeling local dependencies
to be a good complement to the CVAE’s power at
learning global representations. This simple com-
bination, however, has two major challenges.

First, CVAE may fail to generate realistic sen-



3133

original How do I improve my English speaking ?
paraphrased How do I speak English fluently ?
generated How do I I to ?

original What is the easiest way to lose weight faster?
paraphrased What is the best way to loose weight quickly ?
generated How can I learn lose weight ?

Table 1: Examples for generating paraphrases by
CVAE, from which it fails to generate realistic sen-
tences. Original: source sentence. Paraphrased: target
sentence. Generated: the sentence generated by CVAE.

tences. When generating synthetic sentences by
decoding random samples from the latent space,
most regions of the latent space do not necessarily
decode to realistic sentences; the example in Table
1 reflects this challenge. In (Bowman et al., 2016),
the authors attempted to utilize RNN-based VAE
to generate more diverse sentences. However, this
ignores the fundamental problem of the posterior
distribution over latent variables not covering the
latent space appropriately. Second, when learning,
the ground-truth words are used to decode, while
during inference, the RNN generates words in se-
quence from previously generated words. Bengio
et al. (2015) called this phenomenon “Exposure
Bias” and tried to solve it by a scheduled sam-
pling method. However, in practice, it produced
unstable results, because it is fundamentally an in-
consistent training strategy (Huszár, 2015).

We propose the first paraphrase generative
model via adversarial training to tackle the above
problems, which we believe is a natural answer to
the aforementioned challenges. Specifically, we
formalize CVAE as a generator of the Generative
Adversarial Network (GAN) (Goodfellow et al.,
2014) and tailor a discriminator to CVAE. By in-
troducing an adversarial game between a generator
and a discriminator, GAN matches the distribution
of synthetic data with that of real data. The gen-
erator of GAN seeks to map samples from a given
prior distribution to realistic synthetic data. The
discriminator of GAN compares the entire real and
synthetic sentences, instead of individual words,
which should in principle alleviates the exposure-
bias issue (Yu et al., 2017; Li et al., 2017a; Zhang
et al., 2017; Guo et al., 2018). The intuition behind
our model can be interpreted as using CVAE to
generate similar sentences and enhancing CVAE
with an extended discriminator, so that the gener-
ator and discriminator work effectively together;
they provide feedback signals to each other, result-

ing in a mutual adversarial framework. Overall,
our contributions are as follows.

• To the best of our knowledge, this work rep-
resents the first to propose an end-to-end
paraphrase generation architecture via adver-
sarial training, which does not require extra
linguistic information.

• We take advantage of GAN to help choose
a better latent-variable distribution. By this,
we not only utilize the better latent variables
but also strengthen the expressiveness of the
generative model.

• Our experiments show that the proposed
model is capable of generating plausible
paraphrased sentences and outperforms com-
petitive baseline models, with state-of-the-art
results.

2 Preliminaries

We first provide preliminaries of variational
auto-encoders and conditional variational auto-
encoders.

VAE. The variational auto-encoder (VAE)
(Kingma and Welling, 2013; Sohn et al., 2015) is
a directed graphical model with latent variables.
The generative process of VAE employs an

• (Encoder) generating a set of latent variable
z from the prior distribution pθ(z), where θ
is the generative parameters;

• (Decoder) then generating the data x from
the generative distribution pθ(x|z) condi-
tioned on z.

Although due to intractable posterior inference,
parameter estimation of directed graphical mod-
els is generally challenging, VAE parameters can
be estimated efficiently by a stochastic gradient
variational bayes (SGVB) estimator (Kingma and
Welling, 2013), and can be optimized straight-
forwardly using standard stochastic gradient tech-
niques.

SGVB treats the variational lower bound of the
log-likelihood as a surrogate objective function,



3134

which can be written as:

log pθ(x) = −KL(qφ(z|x) ‖ pθ(z))
+KL(qφ(z|x) ‖ pθ(z|x))
+Eqφ(z|x)[log pθ(x|z)]

≥ −KL(qφ(z|x) ‖ pθ(z))
+Eqφ(z|x)[log pθ(x|z)], (1)

where the inequality follows because the second
term KL(qφ(z|x) ‖ pθ(z|x)) in (1) is nonnega-
tive.

In practice, we can approximate the sec-
ond term by drawing latent variable samples
{z1, z2, ...,zL} following qφ(z|x), where φ is the
variational parameters, and the empirical objective
of VAE with Gaussian latent variables can be rep-
resented as:

L̃V AE(x; θ, φ) = −KL(qφ(z|x) ‖ pθ(z))

+
1

L

L∑
l=1

log pθ(x|zl), (2)

where zl = gφ(x, �l), and �l ∼ N (0, I). That
means qφ(z|x) is reparameterized with a differ-
entiable unbiased function gφ(x, �l), where x is
the data and � is the noise variable. VAE can be
trained efficiently by stochastic gradient descent
(SGD) because the reparameterization trick allows
error backpropagation through the Gaussian latent
variables.

CVAE. Sohn et al. (2015) develops a deep con-
ditional generative model (CVAE) for structured
output prediction using Gaussian latent variables.
Different from the normal VAE, CVAE consists of
three variables: input variables x, output variables
y, and latent variables z, and its prior distribution
is pθ(z|x). CVAE is trained to maximize the con-
ditional log-likelihood log pθ(y|x), and again it is
formulated in the framework of SGVB. The varia-
tional lower bound of the model is:

log pθ(y|x) ≥ −KL(qφ(z|x,y) ‖ pθ(z|x))
+Eqφ(z|x,y)[log pθ(y|x, z)](3)

and the empirical lower bound is represented as:

L̃CVAE(x,y; θ, φ) = −KL(qφ(z|x,y) ‖ pθ(z|x))

+
1

L

L∑
l=1

log pθ(y|x,zl), (4)

where zl = gφ(x,y, �l), � ∼ N (0, I), and L is
the number of samples.

s ̃ 
p

E1 E2

G1 G2 D1

Reparameterization
Trick

z ∼  (μ, σ)

s
p

s
o

s
o

D+rec1 rec2

KL

DG

Vector
Transfer

Copy

s
p

Figure 1: Our generative model. The red arrows de-
note the process for generating sentences and the green
one denotes the transmission of target paraphrased sen-
tences.

3 Model

Our new model, which we call GAP for Gen-
erative Adversarial Paraphrase model, targets the
goal of generating plausible paraphrased sentences
conditioned on the original sentences, without any
extra linguistic information. We first propose
our end-to-end conditional generative architecture
for generating paraphrase via adversarial training.
Two training techniques are then described for the
proposed model.

3.1 GAP

We consider a dataset with pairs of the orig-
inal sentence and the paraphrased sentence
{(sok, s

p
k)}

K
k=1. For a given sentence s, let wt de-

note the t-th word. Each word wt is embedded
into a d-dimensional vector xt = We[wt], where
We ∈ Rd×V is a learned word embedding matrix,
V is the vocabulary size, and the υ-th column of
We is We[υ]. All columns of We are normalized
to have unit `2-norm, i.e., ‖We[υ]‖2 = 1,∀υ,
by dividing each column with its `2-norm. After
embedding, a sentence of length T (padded when
necessary) is represented as X ∈ Rd×T , by simply
concatenating its word embeddings, i.e., xt is the
t-th column of X.

The training set is S = {(sok, s
p
k)}

K
k=1 of pair-

wise data, where sok ∈ So denotes the original
sentence sequence, and spk ∈ S

p denotes the refer-
ence paraphrased sentence sequence. The goal of
our model is to learn a function f : So 7→ Sp. The



3135

overall model structure of the proposed method is
shown in Figure 1. The components can be di-
vided into three modules: encoder in yellow, de-
coder in blue (which is also the generator), and the
discriminator in purple. The connections between
them are drawn in arrows.

EncoderE. The encoderE consists of two LSTM
networks E1 and E2, and E generates latent vari-
able z from the input sentences. There are two
different paths generating latent variable z. In the
first path, we input sampled so and sp into E1 and
E2, respectively, and generate sufficient statistics
of a Gaussian distribution, mean µ and standard
deviation σ. Therefore, the distribution of the la-
tent variable z can be represented as q(z|so, sp).

However, we cannot know the paraphrased sen-
tence sp when we evaluate or apply the trained
model in real applications. The training frame-
work is not fit for the circumstance at testing time,
and onlyE1 is used during the inference. Thus, we
also compute the other set of sufficient statistics,
mean µ′ and standard deviation σ′ by inputting so

into encoder E1. The sampled latent variable z is
only dependent on so, and we can represent the
distribution of z as p(z|so).
Decoder/Generator G. We again combine two
LSTM decodersG1 andG2 as the generatorG. At
first, we feed the original sentence so into decoder
G1 and obtain final state cG1T , h

G1
T . The para-

phrased sentence s̃p is predicted word by word. At
each step, we concatenate the latent variable z and
the previously predicted (ground truth) word, and
then input it into LSTM decoder G2 with hidden
state from the last step. At timestep 0, the input
word is BOS (“begin of sentence” symbol), and
the hidden state is the output from G1. Therefore,
the probability of the predicted paraphrased sen-
tence s̃p, given the encoded feature vector z, is
defined as:

p(s̃p|z, so) =
T∏
t=1

p(w̃pt |w̃
p
<t, z, s

o), (5)

where w̃pt is the t-th generated token, < t ,
{0, · · · , t − 1}. wp0 denotes BOS. All the words
in s̃p are generated sequentially using the LSTM,
based on previously generated words, until the
end-of-sentence symbol is generated. The t-th
word w̃pt is generated as w̃

p
t = argmax(Vht).

Note that the hidden units ht are updated through

E(x̃pt−1,ht−1, z), where E denotes the transition
function in the second LSTM cell related to the up-
dates. The transition function E(·) is implemented
with an LSTM. V is a weight matrix used to com-
pute a distribution over words. x̃pt−1 is the em-
bedding vector of the previously generated word
w̃pt−1, i.e.,

x̃pt−1 = We[w̃
p
t−1], (6)

and is the input for t-th step. Consequently, the
generated sentence s̃p = [w̃p1, · · · , w̃

p
T ] is obtained

given z, by simply concatenating the generated
words.

Two-Path Loss. Thus far, we can compute the
loss from two paths of the encoder. In the first
path, we minimize the KL loss and MLE loss:

min
E

KL(q(z|so, sp)||p(z|so))

−Eq(z|so,sp) [log p(s̃p|z, so)] . (7)

In the second path, the loss is presented as follows:

min
E

−Eq(z|so) [log p(s̃p|z, so)] . (8)

The proposed two-path reconstruction loss dimin-
ishes the gap between prediction pipelines at train-
ing and testing, and helps to generate diverse but
realistic predictions.

The objective function of optimizing E and G
can be written as follows:

min
E,G

λ1KL(q(z|so, sp)||p(z|so))

−λ1Eq(z|so,sp) [log p(s̃p|z, so)]
−λ2Eq(z|so) [log p(s̃p|z, so)]
−λ3Es̃p∼p(s̃p|z′,so) log(D(s̃p)), (9)

where D denotes the discriminator (described be-
low) and z′ ∼ q(z|so). We use λ1, λ2 and λ3 to
balance the weights of the different losses.

Discriminator D. We use one LSTM as the dis-
criminator. The goal of the discriminator is to
distinguish real paraphrased sentences from gen-
erated sentences. Given the embeddings of true
paraphrased sentence sp and the fake generated
sentence s̃p, the discriminator loss is defined as:

min
D

−Es̃p∼p(s̃p|z′,so) log(1−D(s̃p))

−Esp∼p(sp)[logD(sp)], (10)

where z′ ∼ q(z|so).



3136

Algorithm 1 Training Pipeline
Initialize: Gradients of corresponding parame-

ters: Encoder E parameters gE = [gE1, gE2],
Decoder/Generator G parameters gG =
[gG1, gG2], Discriminator D parameters gD;

1: while G has not converged do
2: Sample a batch of {so, sp} from dataset;
3: z1 = E(s

o, sp);
4: LKL = KL(q(z1|so, sp)||p(z|so));
5: z2 = E1(s

o);
6: Predict s̃p1 following: s̃

p
1 = G(s

o, z1);
7: Predict s̃p2 following: s̃

p
2 = G(s

o, z2);
8: Lrec1 = − log p(s̃p1|z1, so);
9: Lrec2 = − log p(s̃p2|z2, so);

10: Input sp and s̃p2 into Discriminator D;
11: LD = − log(1−D(s̃p2))− logD(sp);
12: LDG = − log(D(s̃p2));
13: gE1 = ∇E1(λ1LKL+λ1Lrec1+λ2Lrec2+

λ3LDG);
14: gE2 = ∇E2(LKL + Lrec1);
15: gG = ∇G(λ1Lrec1 + λ2Lrec2 + λ3LDG);
16: gD = ∇LD;
17: Update model parameters using Adam op-

timizer with gradients gE1, gE2, gG and gD.
18: end while

3.2 Training Techniques

We summarize the training pipeline in Algorithm
1. At each iteration, a pair of original and para-
phrased sentences are fed into two paths for sen-
tence reconstruction. A discriminator is also used
to distinguish between real and generated sen-
tences, which is helpful to the exposure bias prob-
lem.

Policy Gradient. Backpropagation of gradients
from the discriminative model to the generative
model is difficult for sequence generative mod-
els. Following Yu et al. (2017), we use the RE-
INFORCE algorithm (Williams, 1992) to approx-
imate gradients with respect to the generator and
encoder. In the experiments, we regard the gener-
ator G as the stochastic policy and the output of
discriminator D as its reward. In this way, we can
propagate the gradients from the discriminator to
both the generator models and encoder models.

Warm-up Training. It is difficult to train GAN
using gradient-based methods (Goodfellow et al.,
2014). Previous generative models (Zhang et al.,
2017) often pre-train the generator using a super-
vised model, like an auto-encoder. In this paper,

we propose a warm-up technique to train our gen-
erative model. Following the notation in Algo-
rithm 1, we suppose a warm-up step twp. In the
training process, we gradually increase the weight
of LDG to a fixed value λ3. From step t = 0 to
twp, the update of λt3 can be represented as

λt3 =
t

twp
· λ3. (11)

When t ≥ twp, we fix λt3 = λ3 as a constant.
We also need to balance the losses between (7)

and (8). The second loss (8) represents that during
inference, z is generated only depending on xo.
We let λ1 = 1 − λt2 and increase the value of λt2
gradually until twp. Similarly, λt2 is updated from
t = 0 to twp through:

λt2 =
t

twp
· λ2. (12)

If t ≥ twp, we fix λt2 = λ2 .

4 Experiments

We assess the performance of our GAP model and
compare it with previous methods. We first de-
scribe the datasets we use, then present the details
of experimental setup, and finally analyze both
quantitative and qualitative results.

4.1 Datasets
Following previous work (Prakash et al., 2016),
we conduct experiments on the same four standard
datasets that are used widely for paraphrase gener-
ation. Their content is specified below.

MSCOCO. MSCOCO was originally an image-
captions dataset, containing over 120K images,
with five different captions from five different
annotators per image. All the annotations to-
ward one image describe the most prominent ob-
ject or action in this image, which is suitable
for the paraphrase generation task. Specifically,
the dataset has two divisions: training dataset
(“Train2014”, over 82K images) and validation
dataset (“Val2014”, over 40K images). We fol-
low the operations of previous papers, randomly
choosing four captions out of five as two source-
reference pairs and limit the length of sentence to
be 15 words (removing the words beyond the first
15), so that we can compare our results with pub-
lished work.

Quora. Quora consists of over 400K lines of po-
tential question pairs which are duplicate to each



3137

other if a particular position of this line is anno-
tated with 1. Again, we follow the operations of
previous work and filter out those pairs annotated
with 1. There are 155K such question pairs in to-
tal, among which three sub-datasets are created,
i.e., training dataset 50K, 100K, 150K and test
dataset 4K. The goal of using three sub-datasets
is to show how the size of dataset can affect the
results of paraphrase generation.

4.2 Experimental Setup

Encoders E1, E2 and generators G1, G2 are con-
structed using 2-layer LSTMs (Hochreiter and
Schmidhuber, 1997). For discriminator D, it is
also 2-layer LSTM. At the beginning, we map
each word to a 300-dimensional feature vector and
it is initialized with 300-dimensional GloVe vec-
tors (Pennington et al., 2014). Therefore, a sen-
tence of length N can be represented by a matrix
of size N × 300. Before inputting the embedding
vector into LSTM models, we pre-process these
vectors using a two-layer highway network (Sri-
vastava et al., 2015). We set the dimension of la-
tent variables z to be 300. To balance these losses,
we set λ2 = 0.5 and λ3 = 0.01. The warm-up
step is 1× 104 in all experiments.

We use Adam optimizer (Kingma and Ba, 2014)
with learning rate 1 × 10−4 and other parameters
as default, for all models. Following (Sutskever
et al., 2014), we clip gradients of the encoder and
generator parameters if the norm of the parameter
vector exceeds 10, and we clip the gradients of the
discriminator if the norm of the parameter vector
exceeds 5. Greedy search is employed to generate
paraphrases in the experiment.

4.3 Automatic Evaluation

We follow previous papers to choose the same
three well-known automatic evaluation metrics:
BLEU (Papineni et al., 2002), METEOR (Lavie
and Agarwal, 2007), and TER (Snover et al.,
2006). As studied by (Wubben et al., 2010), hu-
man judgments on generated paraphrases correlate
well with these metrics. We then compare our re-
sults on these metrics with previous baselines be-
low.

Baselines. Our GAP model contains LSTM and
VAE components, so we compare it with (i)
the basic attentive sequence-to-sequence model
from Machine Translation (seq2seq) (Bahdanau
et al., 2014), (ii) the stacked residual LSTM

Model Measure
MSCOCO

BLEU↑ METEOR↑ TER↓
Seq2Seq + Att - 18.60 16.80 63.00
Residual LSTM - 37.00 27.00 51.60
Transformer - 41.00 32.80 40.50
Transformer-PB - 44.00 34.70 37.10
VAE-SVG best-B 41.70 30.80 41.70
VAE-SVG best-M 41.30 31.00 41.60
VAE-SVG best-T 41.30 30.90 40.80
VAE∗ avg 42.53 32.77 41.38
Ours best-B 45.60 35.72 39.47
Ours best-M 41.32 36.17 40.52
Ours best-T 42.46 34.79 36.83

Table 2: Test accuracy on MSCOCO dataset, in per-
centage. VAE∗ is our implementation of VAE. “best-
B”, “best-M” and “best-T” represent the scores with
the best BLEU, METEOR and TER respectively. “avg”
denotes an average over “best-B”, “best-M”, and “best-
T”. All other results are directly cited from the respec-
tive papers. For different model variants exist in one
paper, we only show the one with highest scores here.
For the arrows ↑ of BLEU and TER, a higher score is
better. ↓ of TER represents a lower score is better. The
best results are in boldface.

model doing paraphrase generation (Residual
LSTM) (Prakash et al., 2016), and (iii) a VAE
model based on LSTM together with its variants
(VAE-S, VAE-SVG, VAE-SVG-eq) (Gupta et al.,
2018). Beyond these, we also evaluate other
baselines including Transformer (Vaswani et al.,
2017), semantic augmented Transformer which re-
quires extra linguistic information (Transformer-
PB) (Wang et al., 2018), and a deep reinforcement
learning approach (RbM-SL) (Li et al., 2017b).

Results and Analysis. The results on MSCOCO
are shown in Table 2. We find that our GAP model
outperforms the baseline models on all metrics.
For example, comparing our scores on MSCOCO
dataset when we achieve the best BLEU with
the ones from VAE-SVG, we improve the results
about 4 BLEU, 5 METEOR, and 4 TER, respec-
tively.

Table 3 shows the performance on Quora
dataset. We notice that our accuracy increases
with the increasing size of data. Meanwhile,
our model is more robust for a relatively smaller
dataset such as Quora-50K, leveraging its advan-
tage in learning with fewer data. For example, we
achieve much better results than VAE and its vari-
ants on the smaller Quora-50k.

Ablation Study. VAE∗ in Table 3 extends the pre-



3138

Model Measure
Quora-50K Quora-100K Quora-150K

BLEU↑ METEOR↑ TER↓ BLEU↑ METEOR↑ TER↓ BLEU↑ METEOR↑ TER↓
Seq2Seq + Att - 26.06 20.35 - 36.55 26.28 - - - -
Residual LSTM - 27.32 22.37 - 37.38 28.17 - - - -
RbM-SL - 35.81 28.12 - 43.54 32.84 - - - -
VAE-S best-B 15.80 20.10 69.40 17.50 21.60 67.10 19.80 22.60 63.90
VAE-S best-M 15.60 21.10 71.50 17.50 22.70 69.50 19.70 23.80 66.90
VAE-SVG best-B 17.10 21.30 63.10 22.50 24.60 55.70 30.30 28.50 47.30
VAE-SVG best-M 17.10 22.20 63.80 22.40 25.50 55.60 30.30 29.20 47.10
VAE-SVG-eq best-B 17.40 21.40 61.90 22.90 24.70 55.00 31.40 29.00 46.80
VAE-SVG-eq best-M 17.30 22.20 62.60 22.90 25.50 54.90 32.00 30.00 46.10
VAE∗ avg 33.54 22.36 60.45 41.33 28.46 58.24 43.31 28.25 42.36
Ours best-B 38.23 22.51 56.66 45.05 31.49 55.54 47.06 30.94 41.32
Ours best-M 37.14 24.09 61.43 45.05 31.49 55.54 45.51 34.31 40.49
Ours best-T 36.19 22.22 50.50 44.41 27.73 46.41 42.30 29.60 39.47

Table 3: Test accuracy on Quora dataset, in percentage. VAE∗ is our implementation of VAE. “best-B”, “best-M”
and “best-T” represent the scores with the best BLEU, METEOR and TER respectively. “avg” denotes an average
over “best-B”, “best-M”, and “best-T”. All other results are directly cited from the respective papers, and “-” means
no such results reported. For the arrows ↑ of BLEU and TER, a higher score is better. ↓ of TER represents a lower
score is better. The best results are in boldface.

vious VAE-SVG in (Gupta et al., 2018) by using
our proposed two-path loss. We observe that our
VAE∗ usually outperforms previous VAE-based
models. It demonstrates the proposed two-path re-
construction loss can improve the quality of gener-
ated paraphrases. Moreover, our proposed method
via adversary training, denoted as “Ours”, also has
superior performance than VAE∗. Therefore, our
proposed adversarial training and two-path loss
take an apparently positive effect on alleviating the
exposure bias problem and generating diverse but
realistic predictions.

4.4 Human Evaluation

Data Preparation. The accurate evaluation of
paraphrases is an open problem. We believe that
automatic evaluation is not enough for evaluat-
ing paraphrases from a fine-grained perspective,
in terms of three attributes: grammar correct-
ness (plausibility), equivalence to the original sen-
tence (equivalence), diversity expression (diver-
sity). For both MSCOCO and Quora datasets,
we randomly sample 100 sentence pairs (original
sentence, paraphrased sentence) from the test cor-
pus, and apply the generative architectures, includ-
ing both the VAE∗ model and our GAP model,
to generating paraphrases. Thus, we obtain three
different sentence pairs: (original sentence, “gen-
erated” sentence) by the reference, (original sen-
tence, generated paraphrase) by VAE∗, and by our
GAP model. To make the analysis fair, we ran-

domly shuffled all of them. We then partitioned
them into ten buckets.

Process. We set up an Amazon Mechanical Turk
experiment; ten human judges are asked to eval-
uate the quality of paraphrases. We hope our
judges play similar roles to the discriminator in
our model, to make a true/fake judgment. It is eas-
ier for them to make a binary choice than score
how good the paraphrased sentences are from a
wide score range. These ten human judges are
finally involved in evaluating the total 600 sen-
tence pairs. Each pair is judged by two different
judges, and the average score is the final judg-
ment. The agreement between judges is moder-
ate (kappa=0.42). These ten human judges con-
firmed that they were proficient in English and
they had understood the goals of the annotation
process very well. They were trained by means
of instructions and examples. If the meaning of
a generated sentence contains a grammatical error
or does not express the same meaning as the orig-
inal sentence, or lacks diversity, we asked the an-
notators to score 0 for the corresponding attributes,
and 1 otherwise.

Results and Analysis. We report the results in
Table 4. Our model generates paraphrases with
higher scores in terms of plausibility, equivalence,
diversity than the previous VAE model, and their
differences are statistically significant (paired t-
test, p-value < 0.01). The failure cases we ob-
served include implausible sentences, inequiva-



3139

Model
MSCOCO Quora

Plausibility Equivalence Diversity Plausibility Equivalence Diversity
Reference 0.79 0.68 0.59 0.87 0.75 0.66
VAE∗ 0.30 0.26 0.22 0.35 0.31 0.26
Ours 0.43 0.38 0.35 0.46 0.45 0.37

Table 4: Human judgments for paraphrase generation on different models.

original a group of kids are eating pizza and
(MSCOCO) drinking soda

paraphrased a person sits at a table eating pizza
generated-VAE a man sitting at a table with a pizza
generated-Ours a man sitting at a table eating

a plate of pizza and soda

original What is the fastest possible way to
(Quora-50K) lose weight?

paraphrased What are some ways to lose weight fast?
generated-VAE How can I lose weight weight?
generate-Ours How can I lose weight in a month?

original What were the immediate and most
(Quora-100K) important causes that led to World War 1?

paraphrased What were the causes of World War I?
generated-VAE What were the main causes of World War I?
generated-Ours What were the direct and main causes

of World War 1?

original What could be the reason behind Arnab
(Quora-150K) Goswami quitting Times Now?

paraphrased Is Arnab Goswami quitting from Times now?
generated-VAE Why did Arnab Goswami quit?
generated-Ours Why did Arnab Goswami resign from Times

Now?

Table 5: Samples of generated paraphrases from
MSCOCO and Quora. Note that for the last two exam-
ples, our generated result is even better than the ground
truth where our model paraphrasing “immediate” using
“direct”, and paraphrasing “quit” using “resign”.

lent, and inaccurate expressions. It is believed that
the reason for this is related to the input training
data. It contains noise caused by the length limita-
tion of ≤ 15 words. But note that even for the
reference paraphrase, the accuracy cannot be as
high as 100%. Here is an example from the real
data: for the original sentence “children are play-
ing soccer on a field with several adults observ-
ing nearby”, the reference paraphrase is “soccer
player hits another player in the face”. Consider-
ing this, the results show that our GAP generates
relatively more plausible and diverse paraphrase
sentence, compared to the baseline model.

Case Study. In Table 5, we show examples sam-
pled from both MSCOCO and Quora. We observe
that our model generates paraphrases with higher
diversity than VAE, with no loss of information.

What’s more, we discover some results where our
model is even better than the ground truth, which
are shown in the last two examples in the Table 5.

5 Related Work

Generative models or conditional generative mod-
els have experienced remarkable progress in the
visual domain, such as VAEs in (Kingma et al.,
2014; Sohn et al., 2015) and InfoGAN (Chen et al.,
2016). Recent work (Larsen et al., 2015; Chen
et al., 2017) also considers combining autoen-
coders or variational autoencoders with GAN to
demonstrate superior performance on image gen-
eration. In the area of NLP, generation can be sum-
marized from two perspectives: text generation
with the goal of yielding diverse and plausible sen-
tences given an original sentence, and conditional
text generation aiming to generate new sentences
conditioned on an original sentence. Considering
the latter, examples include generating a new sen-
tence similar to an original sentence (paraphrase
generation), in a different style from the original
sentence (style transfer), or dependent on dialogue
history (dialogue generation). Attempts at using
VAEs (Bowman et al., 2016; Wang et al., 2019),
GANs (Yu et al., 2017; Zhang et al., 2016), and
both (Hu et al., 2017) have been made to address
generic text generation. However, all of them are
not suitable for conditional text generation. Re-
cent work in (Gupta et al., 2018) tries to handle
paraphrase generation using VAEs. However, it
suffers from challenges common to VAEs. Our
method addresses these challenges with the help
of GAN. To the best of our knowledge, this is
the first work on using GAN with VAEs for para-
phrase generation task.

6 Conclusions

We propose the first deep conditional generative
architecture for generating paraphrases via adver-
sarial training, in the hope of combining advan-
tages of CVAE to generate similar distributions
with the advantage of GAN to generate plausible



3140

sentences. Experimental results evaluated on au-
tomatic metrics demonstrate the advantages of our
model, with human evaluations also verifying ef-
fectiveness. In future work, we intend to acceler-
ate the training of our encoders and decoders with
the techniques in (Huo et al., 2018) and apply our
architecture and training techniques to other NLP
tasks. Overall, we believe that our research makes
an important step for using generative models in
NLP, especially in conditional text generation.

Acknowledgments

This research was supported in part by DARPA,
DOE, NIH, ONR and NSF.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 1171–1179.

Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1415–1425.

Igor A Bolshakov and Alexander Gelbukh. 2004. Syn-
onymous paraphrasing using wordnet and internet.
In International Conference on Application of Nat-
ural Language to Information Systems, pages 312–
323. Springer.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2016. Generating sentences from a continuous
space. CoNLL 2016, page 10.

Ziqiang Cao, Chuwei Luo, Wenjie Li, and Sujian Li.
2017. Joint copying and restricted generation for
paraphrase. In Thirty-First AAAI Conference on Ar-
tificial Intelligence.

Liqun Chen, Shuyang Dai, Yunchen Pu, Chunyuan Li,
Qinliang Su, and Lawrence Carin. 2017. Symmetric
variational autoencoder and connections to adversar-
ial learning. arXiv preprint arXiv:1709.01846.

Xi Chen, Yan Duan, Rein Houthooft, John Schul-
man, Ilya Sutskever, and Pieter Abbeel. 2016. Info-
gan: Interpretable representation learning by infor-
mation maximizing generative adversarial nets. In
Advances in neural information processing systems,
pages 2172–2180.

Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and
Wei Xu. 2016. Neural machine translation with
pivot languages. arXiv preprint arXiv:1611.04928.

Pablo Duboue and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked:
The impact of paraphrasing for question answering.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers.

Michael Ellsworth and Adam Janin. 2007. Mu-
taphrase: Paraphrasing with framenet. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 143–150. Associ-
ation for Computational Linguistics.

Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In Proceedings of the
20th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1156–
1165. ACM.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems, pages 2672–2680.

Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong
Yu, and Jun Wang. 2018. Long text generation
via adversarial training with leaked information. In
Thirty-Second AAAI Conference on Artificial Intelli-
gence.

Ankush Gupta, Arvind Agarwal, Prawaan Singh, and
Piyush Rai. 2018. A deep generative framework for
paraphrase generation. In Thirty-Second AAAI Con-
ference on Artificial Intelligence.

Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, pages 905–912. Association
for Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Toward con-
trolled generation of text. In Proceedings of the 34th
International Conference on Machine Learning-
Volume 70, pages 1587–1596. JMLR. org.

Shaohan Huang, Yu Wu, Furu Wei, and Ming
Zhou. 2018. Dictionary-guided editing net-
works for paraphrase generation. arXiv preprint
arXiv:1806.08077.



3141

Zhouyuan Huo, Bin Gu, Heng Huang, et al. 2018. De-
coupled parallel backpropagation with convergence
guarantee. In International Conference on Machine
Learning, pages 2103–2111.

Ferenc Huszár. 2015. How (not) to train your genera-
tive model: Scheduled sampling, likelihood, adver-
sary? stat, 1050:16.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
arXiv preprint arXiv:1804.06059.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114.

Durk P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in neural information processing systems, pages
3581–3589.

Raymond Kozlowski, Kathleen F McCoy, and K Vijay-
Shanker. 2003. Generation of single-sentence para-
phrases from predicate/argument structure using
lexico-grammatical resources. In Proceedings of the
second international workshop on Paraphrasing-
Volume 16, pages 1–8. Association for Computa-
tional Linguistics.

Anders Boesen Lindbo Larsen, Søren Kaae Sønderby,
Hugo Larochelle, and Ole Winther. 2015. Autoen-
coding beyond pixels using a learned similarity met-
ric. arXiv preprint arXiv:1512.09300.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228–231. Association for Com-
putational Linguistics.

Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean,
Alan Ritter, and Dan Jurafsky. 2017a. Adversar-
ial learning for neural dialogue generation. arXiv
preprint arXiv:1701.06547.

Zichao Li, Xin Jiang, Lifeng Shang, and Hang Li.
2017b. Paraphrase generation with deep reinforce-
ment learning. arXiv preprint arXiv:1711.00279.

Shuming Ma, Xu Sun, Wei Li, Sujian Li, Wenjie Li,
and Xuancheng Ren. 2018. Query and output: Gen-
erating words by querying distributed word repre-
sentations for paraphrase generation. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 196–206.

Nitin Madnani and Bonnie J Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.

Kathleen R McKeown. 1983. Paraphrasing questions
using given and new information. Computational
Linguistics, 9(1):1–10.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Rebecca J Passonneau, Ananya Poddar, Gaurav Gite,
Alisa Krivokapic, Qian Yang, and Dolores Perin.
2018. Wise crowd content assessment and educa-
tional rubrics. International Journal of Artificial In-
telligence in Education, 28(1):29–55.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Aaditya Prakash, Sadid A Hasan, Kathy Lee, Vivek
Datla, Ashequl Qadir, Joey Liu, and Oladimeji
Farri. 2016. Neural paraphrase generation with
stacked residual lstm networks. arXiv preprint
arXiv:1610.03098.

Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investigating
a generic paraphrase-based approach for relation ex-
traction. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, volume 200.

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In Advances in
neural information processing systems, pages 3483–
3491.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. arXiv
preprint arXiv:1505.00387.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.



3142

Su Wang, Rahul Gupta, Nancy Chang, and Jason
Baldridge. 2018. A task in a suit and a tie:
paraphrase generation with semantic augmentation.
arXiv preprint arXiv:1811.00119.

Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang,
Guoyin Wang, Dinghan Shen, Changyou Chen, and
Lawrence Carin. 2019. Topic-guided variational
autoencoders for text generation. arXiv preprint
arXiv:1903.07137.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Sander Wubben, Antal Van Den Bosch, and Emiel
Krahmer. 2010. Paraphrase generation as monolin-
gual translation: Data and evaluation. In Proceed-
ings of the 6th International Natural Language Gen-
eration Conference, pages 203–207. Association for
Computational Linguistics.

Zhao Yan, Nan Duan, Junwei Bao, Peng Chen, Ming
Zhou, Zhoujun Li, and Jianshe Zhou. 2016. Doc-
chat: An information retrieval approach for chatbot
engines using unstructured documents. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 516–525.

Qian Yang, Gerard de Melo, Yong Cheng, and Sen
Wang. 2017. Hitext: text reading with dynamic
salience marking. In Proceedings of the 26th Inter-
national Conference on World Wide Web Compan-
ion, pages 311–319. International World Wide Web
Conferences Steering Committee.

Qian Yang, Rebecca J Passonneau, and Gerard
De Melo. 2016. Peak: Pyramid evaluation via au-
tomated knowledge extraction. In Thirtieth AAAI
Conference on Artificial Intelligence.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In Thirty-First AAAI Confer-
ence on Artificial Intelligence.

Yizhe Zhang, Zhe Gan, and Lawrence Carin. 2016.
Generating text via adversarial training. In NIPS
workshop on Adversarial Training, volume 21.

Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo
Henao, Dinghan Shen, and Lawrence Carin. 2017.
Adversarial feature matching for text generation. In
Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 4006–4015.
JMLR. org.

Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 2-Volume 2, pages 834–842.
Association for Computational Linguistics.

Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008. Combining multiple resources to
improve smt-based paraphrasing model. Proceed-
ings of ACL-08: HLT, pages 1021–1029.


