



















































TR9856: A Multi-word Term Relatedness Benchmark


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 419–424,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

TR9856: A Multi-word Term Relatedness Benchmark

Ran Levy and Liat Ein-Dor and Shay Hummel and Ruty Rinott and Noam Slonim
IBM Haifa Research Lab, Mount Carmel, Haifa, 31905, Israel
{ranl,liate,shayh,rutyr,noams}@il.ibm.com

Abstract

Measuring word relatedness is an impor-
tant ingredient of many NLP applications.
Several datasets have been developed in
order to evaluate such measures. The main
drawback of existing datasets is the fo-
cus on single words, although natural lan-
guage contains a large proportion of multi-
word terms. We propose the new TR9856
dataset which focuses on multi-word terms
and is significantly larger than existing
datasets. The new dataset includes many
real world terms such as acronyms and
named entities, and further handles term
ambiguity by providing topical context
for all term pairs. We report baseline
results for common relatedness methods
over the new data, and exploit its magni-
tude to demonstrate that a combination of
these methods outperforms each individ-
ual method.

1 Introduction

Many NLP applications share the need to deter-
mine whether two terms are semantically related,
or to quantify their degree of “relatedness”. De-
veloping methods to automatically quantify term
relatedness naturally requires benchmark data of
term pairs with corresponding human relatedness
scores. Here, we propose a novel benchmark data
for term relatedness, that addresses several chal-
lenges which have not been addressed by previ-
ously available data. The new benchmark data
is the first to consider relatedness between multi–
word terms, allowing to gain better insights re-
garding the performance of relatedness assessment
methods when considering such terms. Second, in
contrast to most previous data, the new data pro-
vides a context for each pair of terms, allowing to
disambiguate terms as needed. Third, we use a

simple systematic process to ensure that the con-
structed data is enriched with “related” pairs, be-
yond what one would expect to obtain by random
sampling. In contrast to previous work, our en-
richment process does not rely on a particular re-
latedness algorithm or resource such as Wordnet
(Fellbaum, 1998), hence the constructed data is
less biased in favor of a specific method. Finally,
the new data triples the size of the largest previ-
ously available data, consisting of 9, 856 pairs of
terms. Correspondingly, it is denoted henceforth
as TR9856. Each term pair was annotated by 10
human annotators, answering a binary question –
related/unrelated. The relatedness score is given as
the mean answer of annotators where related = 1
and unrelated = 0.

We report various consistency measures that
indicate the validity of TR9856. In addition,
we report baseline results over TR9856 for sev-
eral methods, commonly used to assess term–
relatedness. Furthermore, we demonstrate how the
new data can be exploited to train an ensemble–
based method, that relies on these methods as un-
derlying features. We believe that the new TR9856
benchmark, which is freely available for research
purposes, 1 along with the reported results, will
contribute to the development of novel term relat-
edness methods.

2 Related work

Assessing the relatedness between single words
is a well known task which received substantial
attention from the scientific community. Corre-
spondingly, several benchmark datasets exist. Pre-
sumably the most popular among these is the
WordSimilarity-353 collection (Finkelstein et al.,
2002), covering 353 word pairs, each labeled by
13−16 human annotators, that selected a continu-
ous relatedness score in the range 0-10. These hu-

1https://www.research.ibm.com/haifa/
dept/vst/mlta_data.shtml

419



man results were averaged, to obtain a relatedness
score for each pair. Other relatively small datasets
include (Radinsky et al., 2011; Halawi et al., 2012;
Hill et al., 2014).

A larger dataset is Stanford’s Contextual Word
Similarities dataset, denoted SCWS (Huang et al.,
2012) with 2,003 word pairs, where each word ap-
pears in the context of a specific sentence. The au-
thors rely on Wordnet (Fellbaum, 1998) for choos-
ing a diverse set of words as well as to enrich
the dataset with related pairs. A more recent
dataset, denoted MEN (Bruni et al., 2014) con-
sists of 3,000 word pairs, where a specific relat-
edness measure was used to enrich the data with
related pairs. Thus, these two larger datasets are
potentially biased in favor of the relatedness al-
gorithm or lexical resource used in their devel-
opment. TR9856 is much larger and potentially
less biased than all these previously available data.
Hence, it allows to draw more reliable conclu-
sions regarding the quality and characteristics of
examined methods. Moreover, it opens the door
for developing term relatedness methods within
the supervised machine learning paradigm as we
demonstrate in Section 5.2.

It is also worth mentioning the existence of re-
lated datasets, constructed with more specific NLP
tasks in mind. For examples, datasets constructed
to assess lexical entailment (Mirkin et al., 2009)
and lexical substitution (McCarthy and Navigli,
2009; Kremer et al., 2014; Biemann, 2013) meth-
ods. However, the focus of the current work is
on the more general notion of term–relatedness,
which seems to go beyond these more concrete re-
lations. For example, the words whale and ocean
are related, but are not similar, do not entail one
another, and can not properly substitute one an-
other in a given text.

3 Dataset generation methodology

In constructing the TR9856 data we aimed to ad-
dress the following issues: (i) include terms that
involve more than a single word; (ii) disambiguate
terms, as needed; (iii) have a relatively high frac-
tion of “related” term pairs; (iv) focus on terms
that are relatively common as opposed to eso-
teric terms; (v) generate a relatively large bench-
mark data. To achieve these goals we defined and
followed a systematic and reproducible protocol,
which is described next. The complete details are
included in the data release notes.

3.1 Defining topics and articles of interest

We start by observing that framing the related-
ness question within a pre-specified context may
simplify the task for humans and machines alike,
in particular since the correct sense of ambigu-
ous terms can be identified. Correspondingly,
we focus on 47 topics selected from Debatabase
2. For each topic, 5 human annotators searched
Wikipedia for relevant articles as done in (Aharoni
et al., 2014). All articles returned by the annota-
tors – an average of 21 articles per topic – were
considered in the following steps. The expectation
was that articles associated with a particular topic
will be enriched with terms related to that topic,
hence with terms related to one another.

3.2 Identifying dominant terms per topic

In order to create a set of terms related to a topic of
interest, we used the Hyper-geometric (HG) test.
Specifically, given the number of sentences in the
union of articles identified for all topics; the num-
ber of sentences in the articles identified for a spe-
cific topic, i.e., in the topic articles; the total num-
ber of sentences that include a particular term, t;
and the number of sentences within the topic ar-
ticles, that include t, denoted x; we use the HG
test to assess the probability p, to observe ≥ x
occurrences of t within sentences selected at ran-
dom out of the total population of sentences. The
smaller p is, the higher our confidence that t is re-
lated to the examined topic. Using this approach,
for each topic we identify all n–gram terms, with
n = 1, 2, 3 , with a p-value ≤ 0.05, after applying
Bonfferroni correction. We refer to this collection
of n–gram terms as the topic lexicon and refer to
n–gram terms as n–terms.

3.3 Selecting pairs for annotation

For each topic, we define Sdef as the set of manu-
ally identified terms mentioned in the topic def-
inition. E.g., for the topic “The use of per-
formance enhancing drugs in professional sports
should be permitted”, Sdef = {“performance en-
hancing drugs”,“professional sports”}. Given the
topic lexicon, we anticipate that terms with a small
p–value will be highly related to terms in Sdef .
Hence, we define Stop,n to include the top 10 n–
terms in the topic lexicon, and add to the dataset
all pairs in Sdef×Stop,n for n = 1, 2, 3. Similarly,
we define Smisc,n to include an additional set of 10

2http://idebate.org/debatabase

420



n–terms, selected at random from the remaining
terms in the topic lexicon, and add to the dataset
all pairs in Sdef × Smisc,n. We expect that the av-
erage relatedness observed for these pairs will be
somewhat lower. Finally, we add to the dataset
60 · |Sdef | pairs – i.e., the same number of pairs
selected in the two previous steps – selected at ran-
dom from ∪n,mStop,n × Smisc,m. We expect that
the average relatedness observed for this last set of
pairs will be even lower.

3.4 Relatedness labeling guidelines

Each annotator was asked to mark a pair of terms
as “related”, if she/he believes there is an imme-
diate associative connection between them, and as
“unrelated” otherwise. Although “relatedness” is
clearly a vague notion, in accord with previous
work – e.g., (Finkelstein et al., 2002), we assumed
that human judgments relying on simple intuition
will nevertheless provide reliable and reproducible
estimates. As discussed in section 4, our results
confirm this assumption.

The annotators were further instructed to con-
sider antonyms as related, and to use resources
such as Wikipedia to confirm their understanding
regarding terms they are less familiar with. Fi-
nally, the annotators were asked to disambiguate
terms as needed, based on the pair’s associated
topic. The complete labeling guidelines are avail-
able as part of the data release.

We note that in previous work, given a pair of
words, the annotators were typically asked to de-
termine a relatedness score within the range of 0
to 10. Here, we took a simpler approach, asking
the annotators to answer a binary related/unrelated
question. To confirm that this approach yields sim-
ilar results to previous work we asked 10 annota-
tors to re-label the WS353 data using our guide-
lines – except for the context part. Comparing the
mean binary score obtained via this re-labeling to
the original scores provided for these data we ob-
serve a Spearman correlation of 0.87, suggesting
that both approaches yield fairly similar results.

4 The TR9856 data – details and
validation

The procedure described above led to a collec-
tion of 9, 856 pairs of terms, each associated with
one out of the 47 examined topics. Out of these
pairs, 1, 489 were comprised of single word terms
(SWT) and 8, 367 were comprised of at least one

multi-word term (MWT). Each pair was labeled
by 10 annotators that worked independently. The
binary answers of the annotators were averaged,
yielding a relatedness score between 0 to 1 – de-
noted henceforth as the data score.

Using the notations above, pairs from Sdef ×
Stop,n had an average data score of 0.66; pairs
from Sdef × Smisc,n had an average data score
of 0.51; and pairs from Stop,n × Smisc,m had an
average relatedness score of 0.41. These results
suggest that the intuition behind the pair selection
procedure described in Section 3.3 is correct. We
further notice that 31% of the labeled pairs had
a relatedness score ≥ 0.8, and 33% of the pairs
had a relatedness score ≤ 0.2, suggesting the con-
structed data indeed includes a relatively high frac-
tion of pairs with related terms, as planned.

To evaluate annotator agreement we followed
(Halawi et al., 2012; Snow et al., 2008) and di-
vided the annotators into two equally sized groups
and measured the correlation between the results
of each group. The largest subset of pairs for
which the same 10 annotators labeled all pairs
contained roughly 2,900 pairs. On this subset, we
considered all possible splits of the annotators to
groups of size 5, and for each split measured the
correlation of the relatedness scores obtained by
the two groups. The average Pearson correlation
was 0.80. These results indicate that in spite of the
admitted vagueness of the task, the average anno-
tation score obtained by different sets of annota-
tors is relatively stable and consistent.

Several examples of term pairs and their corre-
sponding dataset scores are given in Table 1. Note
that the first pair includes an acronym – wipo –
which the annotators are expected to resolve to
World Intellectual Property Organization.

4.1 Transitivity analysis

Another way to evaluate the quality and consis-
tency of a term relatedness dataset is by measur-
ing the transitivity of its relatedness scores. Given
a triplet of term pairs (a, b) , (b, c) and (a, c), the
transitivity rule implies that if a is related to b,
and b is related to c then a is related to c. Using
this rule, transitivity can be measured by comput-
ing the relative fraction of pair triplets fulfilling it.
Note that this analysis can be applied only if all
the three pairs exist in the data. Here, we used the
following intuitive transitivity measure: let (a, b),
(b, c), and (a, c), be a triplet of term pairs in the

421



Term 1 Term 2 Score
copyright wipo 1.0
grand theft
auto

violent video
games

1.0

video games
sales

violent video
games

0.7

civil rights affirmative
action

0.6

rights public prop-
erty

0.5

nation of is-
lam

affirmative
action

0.1

racial sex discrimi-
nation

0.1

Table 1: Examples of pairs of terms and their as-
sociated dataset scores.

dataset, and let R1, R2, and R3 be their related-
ness scores, respectively. Then, for high values
of R2, R1 is expected to be close to R3. More
specifically, on average, |R3 − R1| is expected to
decrease with R2. Figure 1 shows that this behav-
ior indeed takes place in our dataset. The p-value
of the correlation between mean(|R3 − R1|) and
R2 is ≈ 1e − 10. Nevertheless, the curves of the
WS353 data (both with the original labeling and
with our labeling) do not show this behavior, prob-
ably due to the very few triplet term pairs existing
in these data, resulting with a very poor statistics.
Besides validating the transitivity behavior, these
results emphasize the advantage of the relatively
dense TR9856 data, in providing sufficient statis-
tics for performing this type of analysis.

Figure 1: mean(|R3 −R1|) vs. R2.

5 Results for existing techniques

To demonstrate the usability of the new TR9856
data, we present baseline results of commonly
used methods that can be exploited to predict
term relatedness, including ESA (Gabrilovich and
Markovitch, 2007), Word2Vec (W2V) (Mikolov
et al., 2013) and first–order positive PMI (PMI)
(Church and Hanks, 1990). To handle MWTs, we
used summation on the vector representations of
W2V and ESA. For PMI, we tokenized each MWT
and averaged the PMI of all possible single–word
pairs. For all these methods we used the March
2015 Wikipedia dump and a relatively standard
configuration of the relevant parameters. In ad-
dition, we report results for an ensemble of these
methods using 10-fold cross validation.

5.1 Evaluation measures

Previous experiments on WS353 and other
datasets reported Spearman Correlation (ρ) be-
tween the algorithm predicted scores and the
ground–truth relatedness scores. Here, we also
report Pearson Correlation (r) results and demon-
strate that the top performing algorithm becomes
the worst performing algorithm when switching
between these two correlation measures. In ad-
dition, we note that a correlation measure gives
equal weight to all pairs in the dataset. How-
ever, in some NLP applications it is more impor-
tant to properly distinguish related pairs from un-
related ones. Correspondingly, we also report re-
sults when considering the problem as a binary
classification problem, aiming to distinguish pairs
with a relatedness score ≥ 0.8 from pairs with a
relatedness score ≤ 0.2.

5.2 Correlation results

The results of the examined methods are summa-
rized in Table 2. Note that these methods are not
designed for multi-word terms, and further do not
exploit the topic associated with each pair for dis-
ambiguation. The results show that all methods
are comparable except for ESA in terms of Pear-
son correlation, which is much lower. This suggest
that ESA scores are not well scaled, a property that
might affect applications using ESA as a feature.

Next, we exploit the relatively large size of
TR9856 to demonstrate the potential for using su-
pervised machine learning methods. Specifically,
we trained a simple linear regression using the
baseline methods as features, along with a token

422



Method r ρ
ESA 0.43 0.59
W2V 0.57 0.56
PMI 0.55 0.58

Table 2: Baseline results for common methods.

length feature, that counts the combined number
of tokens per pair, in a 10-fold cross validation
setup. The resulting model outperforms all indi-
vidual methods, as depicted in Table 3.

Method r ρ
ESA 0.43 0.59
W2V 0.57 0.56
PMI 0.55 0.58

Lin. Reg. 0.62 0.63

Table 3: Mean results over 10-fold cross valida-
tion.

5.3 Single words vs. multi-words
To better understand the impact of MWTs, we di-
vided the data into two subsets. If both terms
are SWTs the pair was assigned to the SWP sub-
set; otherwise it was assigned to the MWP sub-
set. The SWP subset included 1, 489 pairs and the
MWP subset comprised of 8, 367 pairs. The ex-
periment in subsection 5.2 was repeated for each
subset. The results are summarized in Table 4. Ex-
cept for the Pearson correlation results of ESA, for
all methods we observe lower performance over
the MWP subset, suggesting that assessing term–
relatedness is indeed more difficult when MWTs
are involved.

Method r ρ
SWP MWP SWP MWP

ESA 0.41 0.43 0.63 0.58
W2V 0.62 0.55 0.58 0.55
PMI 0.63 0.55 0.63 0.59

Table 4: Baseline results for SWP vs. MWP.

5.4 Binary classification results
We turn the task into binary classification task
by considering the 3, 090 pairs with a data score
≥ 0.8 as positive examples, and the 3, 245 pairs
with a data score ≤ 0.2 as negative examples. We
use a 10-fold cross validation to choose an opti-
mal threshold for the baseline methods as well as

to learn a Logistic Regression (LR) classifier, that
further used the token length feature. Again, the
resulting model outperforms all individual meth-
ods, as indicated in Table 5.

Method Mean Error
ESA 0.19
W2V 0.22
PMI 0.21

Log. Reg. 0.18

Table 5: Binary classification results.

6 Discussion

The new TR9856 dataset has several important ad-
vantages compared to previous datasets. Most im-
portantly – it is the first dataset to consider the re-
latedness between multi–word terms; ambiguous
terms can be resolved using a pre–specified con-
text; and the data itself is much larger than previ-
ously available data, enabling to draw more reli-
able conclusions, and to develop supervised ma-
chine learning methods that exploit parts of the
data for training and tuning.

The baseline results reported here for com-
monly used techniques provide initial intrigu-
ing insights. Table 4 suggests that the perfor-
mance of specific methods may change substan-
tially when considering pairs composed of uni-
grams vs. pairs in which at least one term is a
MWT. Finally, our results demonstrate the poten-
tial of supervised–learning techniques to outper-
form individual methods, by using these methods
as underlying features.

In future work we intend to further investigate
the notion of term relatedness by manually label-
ing the type of the relation identified for highly re-
lated pairs. In addition, we intend to develop tech-
niques that aim to exploit the context provided for
each pair, and to consider the potential of more ad-
vanced – and in particular non–linear – supervised
learning methods.

Acknowledgments

The authors thank Ido Dagan and Mitesh Khapra
for many helpful discussions.

References
Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel

Hershcovich, Ran Levy, Ruty Rinott, Dan Gutfre-

423



und, and Noam Slonim. 2014. A benchmark dataset
for automatic detection of claims and evidence in the
context of controversial topics. ACL 2014, page 64.

Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Language Resources and Evaluation, 47(1):97–122.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. J. Ar-
tif. Intell. Res.(JAIR), 49:1–47.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.

Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI, vol-
ume 7, pages 1606–1611.

Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of
word relatedness with constraints. In Proceedings of
the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1406–
1414. ACM.

Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with
(genuine) similarity estimation. arXiv preprint
arXiv:1408.3456.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Annual Meeting of the Association
for Computational Linguistics (ACL).

Gerhard Kremer, Katrin Erk, Sebastian Padó, and Ste-
fan Thater. 2014. What substitutes tell us - anal-
ysis of an ”all-words” lexical substitution corpus.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 540–549, Gothenburg, Sweden,
April. Association for Computational Linguistics.

Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language resources
and evaluation, 43(2):139–159.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 558–566. Association
for Computational Linguistics.

Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
pages 337–346. ACM.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254–263. Association for Computational
Linguistics.

424


