



















































Learning Text Similarity with Siamese Recurrent Networks


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 148–157,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Learning Text Similarity with Siamese Recurrent Networks

Paul Neculoiu, Maarten Versteegh and Mihai Rotaru
Textkernel B.V. Amsterdam

{neculoiu,versteegh,rotaru}@textkernel.nl

Abstract

This paper presents a deep architecture for
learning a similarity metric on variable-
length character sequences. The model
combines a stack of character-level bidi-
rectional LSTM’s with a Siamese archi-
tecture. It learns to project variable-
length strings into a fixed-dimensional em-
bedding space by using only informa-
tion about the similarity between pairs of
strings. This model is applied to the task
of job title normalization based on a manu-
ally annotated taxonomy. A small data set
is incrementally expanded and augmented
with new sources of variance. The model
learns a representation that is selective to
differences in the input that reflect seman-
tic differences (e.g., “Java developer” vs.
“HR manager”) but also invariant to non-
semantic string differences (e.g., “Java de-
veloper” vs. “Java programmer”).

1 Introduction

Text representation plays an important role in nat-
ural language processing (NLP). Tasks in this field
rely on representations that can express the seman-
tic similarity and dissimilarity between textual el-
ements, be they viewed as sequences of words or
characters. Such representations and their asso-
ciated similarity metrics have many applications.
For example, word similarity models based on
dense embeddings (Mikolov et al., 2013) have re-
cently been applied in diverse settings, such as
sentiment analysis (dos Santos and Gatti, 2014)
and recommender systems (Barkan and Koenig-
stein, 2016). Semantic textual similarity measures
have been applied to tasks such as automatic sum-
marization (Ponzanelli et al., 2015), debate anal-
ysis (Boltuzic and Šnajder, 2015) and paraphrase
detection (Socher et al., 2011).

Measuring the semantic similarity between
texts is also fundamental problem in Information
Extraction (IE) (Martin and Jurafsky, 2000). An
important step in many applications is normaliza-
tion, which puts pieces of information in a stan-
dard format, so that they can be compared to other
pieces of information. Normalization relies cru-
cially on semantic similarity. An example of nor-
malization is formatting dates and times in a stan-
dard way, so that “12pm”, “noon” and “12.00h” all
map to the same representation. Normalization is
also important for string values. Person names, for
example, may be written in different orderings or
character encodings depending on their country of
origin. A sophisticated search system may need to
understand that the strings “李小龙”, “Lee, Jun-
fan” and “Bruce Lee” all refer to the same person
and so need to be represented in a way that in-
dicates their semantic similarity. Normalization
is essential for retrieving actionable information
from free, unstructured text.

In this paper, we present a system for job title
normalization, a common task in information ex-
traction for recruitment and social network anal-
ysis (Javed et al., 2014; Malherbe et al., 2014).
The task is to receive an input string and map it to
one of a finite set of job codes, which are prede-
fined externally. For example, the string “software
architectural technician Java/J2EE” might need to
be mapped to “Java developer”. This task can be
approached as a highly multi-class classification
problem, but in this study, the approach we take
focuses on learning a representation of the strings
such that synonymous job titles are close together.
This approach has the advantage that it is flexi-
ble, i.e., the representation can function as the in-
put space to a subsequent classifier, but can also
be used to find closely related job titles or explore
job title clusters. In addition, the architecture of
the learning model allows us to learn useful repre-
sentations with limited supervision.

148



2 Related Work

The use of (deep) neural networks for NLP has
recently received much attention, starting from
the seminal papers employing convolutional net-
works on traditional NLP tasks (Collobert et al.,
2011) and the availability of high quality seman-
tic word representations (Mikolov et al., 2013).
In the last few years, neural network models have
been applied to tasks ranging from machine trans-
lation (Zou et al., 2013; Cho et al., 2014) to
question answering (Weston et al., 2015). Cen-
tral to these models, which are usually trained on
large amounts of labeled data, is feature repre-
sentation. Word embedding techniques such as
word2vec (Mikolov et al., 2013) and Glove (Pen-
nington et al., 2014) have seen much use in such
models, but some go beyond the word level and
represent text as a sequence of characters (Kim et
al., 2015; Ling et al., 2015). In this paper we take
the latter approach for the flexibility it affords us
in dealing with out-of-vocabulary words.

Representation learning through neural net-
works has received interest since autoencoders
(Hinton and Salakhutdinov, 2006) have been
shown to produce features that satisfy the two
desiderata of representations; that they are invari-
ant to differences in the input that do not matter
for that task and selective to differences that do
(Anselmi et al., 2015).

The Siamese network (Bromley et al., 1993)
is an architecture for non-linear metric learning
with similarity information. The Siamese network
naturally learns representations that embody the
invariance and selectivity desiderata through ex-
plicit information about similarity between pairs
of objects. In contrast, an autoencoder learns in-
variance through added noise and dimensionality
reduction in the bottleneck layer and selectivity
solely through the condition that the input should
be reproduced by the decoding part of the network.
In contrast, a Siamese network learns an invariant
and selective representation directly through the
use of similarity and dissimilarity information.

Originally applied to signature verification
(Bromley et al., 1993), the Siamese architecture
has since been widely used in vision applica-
tions. Siamese convolutional networks were used
to learn complex similarity metrics for face veri-
fication (Chopra et al., 2005) and dimensionality
reduction on image features (Hadsell et al., 2006).
A variant of the Siamese network, the triplet net-

work (Hoffer and Ailon, 2015), was used to learn
an image similarity measure based on ranking data
(Wang et al., 2014).

In other areas, Siamese networks have been ap-
plied to such diverse tasks as unsupervised acous-
tic modelling (Synnaeve et al., 2014; Thiolliere
et al., 2015; Kamper et al., 2016; Zeghidour et
al., 2015), learning food preferences (Yang et al.,
2015) and scene detection (Baraldi et al., 2015). In
NLP applications, Siamese networks with convo-
lutional layers have been applied to matching sen-
tences (Hu et al., 2014). More recently, (Mueller
and Thyagarajan, 2016) applied Siamese recurrent
networks to learning semantic entailment.

The task of job title normalization is often
framed as a classification task (Javed et al., 2014;
Malherbe et al., 2014). Given the large number of
classes (often in the thousands), multi-stage clas-
sifiers have shown good results, especially if in-
formation outside the string can be used (Javed et
al., 2015). There are several disadvantage to this
approach. The first is the expense of data acquisi-
tion for training. With many thousands of groups
of job titles, often not too dissimilar from one an-
other, manually classifying large amounts of job
title data becomes prohibitively expensive. A sec-
ond disadvantage of this approach is its lack of
corrigibility. Once a classification error has been
discovered or a new example has been added to
a class, the only option to improve the system is
to retrain the entire classifier with the new sam-
ple added to the correct class in the training set.
The last disadvantage is that using a traditional
classifier does not allow for transfer learning, i.e.,
reusing the learned model’s representations for a
different task.

A different approach is the use of string similar-
ity measures to classify input strings by proximity
to an element of a class (Spitters et al., 2010). The
advantage of this approach is that there is no need
to train the system, so that improvements can be
made by adding job title strings to the data. The
disadvantages are that data acquisition still needs
to be performed by manually classifying strings
and that the bulk of the problem is now shifted to
constructing a good similarity metric.

By modeling similarity directly based on pairs
of inputs, Siamese networks lend themselves well
to the semantic invariance phenomena present in
job title normalization: typos (e.g. “Java de-
velopeur”), near-synonymy (e.g., “developer” and

149



“programmer”) and extra words (e.g., “experi-
enced Java developer”). This is the approach we
take in this study.

3 Siamese Recurrent Neural Network

Recurrent Neural Networks (RNN) are neural net-
works adapted for sequence data (x1, . . . , xT ).
At each time step t ∈ {1, . . . , T}, the hidden-
state vector ht is updated by the equation ht =
σ(Wxt + Uht−1), in which xt is the input at
time t, W is the weight matrix from inputs to the
hidden-state vector and U is the weight matrix on
the hidden-state vector from the previous time step
ht−1. In this equation and below the logistic func-
tion is denoted by σ(x) = (1 + e−x)−1.

The Long Short-Term Memory (Hochreiter and
Schmidhuber, 1997) variant of RNNs in particular
has had success in tasks related to natural language
processing, such as text classification (Graves,
2012) and language translation (Sutskever et al.,
2014). Standard RNNs suffer from the vanish-
ing gradient problem in which the backpropagated
gradients become vanishingly small over long se-
quences (Pascanu et al., 2013). The LSTM model
was proposed as a solution to this problem. Like
the standard RNN, the LSTM sequentially updates
a hidden-state representation, but it introduces a
memory state ct and three gates that control the
flow of information through the time steps. An
output gate ot determines how much of ct should
be exposed to the next node. An input gate it con-
trols how much the input xt matters at this time
step. A forget gate ft determines whether the pre-
vious time step’s memory should be forgotten. An
LSTM is parametrized by weight matrices from
the input and the previous state for each of the
gates, in addition to the memory cell. We use the
standard formulation of LSTMs with the logistic
function (σ) on the gates and the hyperbolic tan-
gent (tanh) on the activations. In the equations
(1) below, ◦ denotes the Hadamard (elementwise)
product.

it = σ(Wixt + Uiht−1) (1)
ft = σ(Wfxt + Ufht−1) (2)
ot = σ(Woxt + Uoht−1) (3)
c̃t = tanh(Wcxt + Ucht−1) (4)
ct = it ◦ c̃t + ft ◦ ct−1 (5)
ht = ot ◦ tanh(ct) (6)

Bidirectional RNNs (Schuster and Paliwal,
1997) incorporate both future and past context by
running the reverse of the input through a sep-
arate RNN. The output of the combined model
at each time step is simply the concatenation of
the outputs from the forward and backward net-
works. Bidirectional LSTM models in particular
have recently shown good results on standard NLP
tasks like Named Entity Recognition (Huang et al.,
2015; Wang et al., 2015) and so we adopt this tech-
nique for this study.

Siamese networks (Chopra et al., 2005) are
dual-branch networks with tied weights, i.e., they
consist of the same network copied and merged
with an energy function. Figure 1 shows an
overview of the network architecture in this study.
The training set for a Siamese network consists of
triplets (x1, x2, y), where x1 and x2 are charac-
ter sequences and y ∈ {0, 1} indicates whether x1
and x2 are similar (y = 1) or dissimilar (y = 0).
The aim of training is to minimize the distance
in an embedding space between similar pairs and
maximize the distance between dissimilar pairs.

3.1 Contrastive loss function

The proposed network contains four layers of
Bidirectional LSTM nodes. The activations at
each timestep of the final BLSTM layer are aver-
aged to produce a fixed-dimensional output. This
output is projected through a single densely con-
nected feedforward layer.

Let fW(x1) and fW(x2) be the projections of
x1 and x2 in the embedding space computed by
the network function fW. We define the energy of
the model EW to be the cosine similarity between
the embeddings of x1 and x2:

EW(x1, x2) =
〈fW(x1), fW(x2)〉
‖fW(x1)‖‖fW(x2)‖ (7)

For brevity of notation, we will denote
EW(x1, x2) by EW. The total loss function over a
data set X =

{
〈x(i)1 , x(i)2 , y(i)〉

}
is given by:

LW(X) =
N∑

i=1

L
(i)
W(x

(i)
1 , x

(i)
2 , y

(i)) (8)

The instance loss function L(i)W is a contrastive loss
function, composed of terms for the similar (y =

150



Figure 1: Overview of the Siamese Recurrent Network architecture used in this paper. The weights of
all the layers are shared between the right and the left branch of the network.

1) case (L+), and the dissimilar (y = 0) case (L−):

L
(i)
W = y

(i)L+(x
(i)
1 , x

(i)
2 )+ (9)

(1− y(i))L−(x(i)1 , x(i)2 ) (10)
(11)

The loss functions for the similar and dissimilar
cases are given by:

L+(x1, x2) =
1
4
(1− EW)2 (12)

L−(x1, x2) =

{
E2W if EW < m
0 otherwise

(13)

Figure 2 gives a geometric perspective on the
loss function, showing the positive and negative
components separately. Note that the positive loss
is scaled down to compensate for the sampling ra-
tios of positive and negative pairs (see below).

The network used in this study contains four
BLSTM layers with 64-dimensional hidden vec-
tors ht and memory ct. There are connections at
each time step between the layers. The outputs
of the last layer are averaged over time and this
128-dimensional vector is used as input to a dense
feedforward layer. The input strings are padded to

Figure 2: Positive and negative components of the
loss function.

produce a sequence of 100 characters, with the in-
put string randomly placed in this sequence. The
parameters of the model are optimized using the
Adam method (Kingma and Ba, 2014) and each
model is trained until convergence. We use the
dropout technique (Srivastava et al., 2014) on the
recurrent units (with probability 0.2) and between
layers (with probability 0.4) to prevent overfitting.

151



4 Experiments

We conduct a set of experiments to test the model’s
capabilities. We start from a small data set based
on a hand made taxonomy of job titles. In each
subsequent experiment the data set is augmented
by adding new sources of variance. We test the
model’s behavior in a set of unit tests, reflecting
desired capabilities of the model, taking our cue
from (Weston et al., 2015). This section discusses
the data augmentation strategies, the composition
of the unit tests, and the results of the experiments.

4.1 Baseline
Below we compare the performance of our model
against a baseline n-gram matcher (Daelemans et
al., 2004). Given an input string, this matcher
looks up the closest neighbor from the base tax-
onomy by maximizing a similarity scoring func-
tion. The matcher subsequently labels the input
string with that neighbor’s group label. The sim-
ilarity scoring function is defined as follows. Let
Q = 〈q1, . . . , qM 〉 be the query as a sequence of
characters and C = 〈c1, . . . , cN 〉 be the candidate
match from the taxonomy. The similarity function
is defined as:

sim(Q,C) = M −match(Q,C)
match(Q,C) = |TQ 	 TC | − |TQ ∩ TC |

where
A	B = (A \B) ∪ (B \A)

TQ =
M−2⋃
i=1

{〈qi, qi+1, qi+2〉}

TC =
N−2⋃
i=1

{〈ci, ci+1, ci+2〉}

This (non-calibrated) similarity function has the
properties that it is easy to compute, doesn’t re-
quire any learning and is particularly insensitive
to appending extra words in the input string, one
of the desiderata listed below.

In the experiments listed below, the test sets
consist of pairs of strings, the first of which is
the input string and the second a target group la-
bel from the base taxonomy. The network model
projects the input string into the embedding space
and searches for its nearest neighbor under co-
sine distance from the base taxonomy. The test

records a hit if and only if the neighbor’s group
label matches the target.

4.2 Data and Data Augmentation

The starting point for our data is a hand made pro-
prietary job title taxonomy. This taxonomy parti-
tions a set of 19,927 job titles into 4,431 groups.
Table 1 gives some examples of the groups in the
taxonomy. The job titles were manually and semi-
automatically collected from résumés and vacancy
postings. Each was manually assigned a group,
such that the job titles in a group are close to-
gether in meaning. In some cases this closeness
is an expression of a (near-)synonymy relation be-
tween the job titles, as in “developer” and “devel-
oper/programmer” in the “Software Engineer” cat-
egory. In other cases a job title in a group is a spe-
cialization of another, for example “general opera-
tor” and “buzz saw operator” in the “Machine Op-
erator” category. In yet other cases two job titles
differ only in their expression of seniority, as in
“developer” and “senior developer” in the “Soft-
ware Engineer” category. In all cases, the relation
between the job titles is one of semantic similar-
ity and not necessarily surface form similarity. So
while, “Java developer” and “J2EE programmer”
are in the same group, “Java developer” and “real
estate developer” should not be.

Note that some groups are close together in
meaning, like the “Production Employee” and
“Machine Operator” groups. Some groups could
conceivably be split into two groups, depending
on the level of granularity that is desired. We
make no claim to completeness or consistency of
these groupings, but instead regard the wide va-
riety of different semantic relations between and
within groups as an asset that should be exploited
by our model.

The groups are not equal in size; the sizes fol-
low a broken power-law distribution. The largest
group contains 130 job titles, the groups at the
other end of the distribution have only one. This
affects the amount of information we can give to
the system with regards to the semantic similar-
ity between job titles in a group. The long tail of
the distribution may impact the model’s ability to
accurately learn to represent the smallest groups.
Figure 3 shows the distribution of the group sizes
of the original taxonomy.

We proceed from the base taxonomy of job titles
in four stages. At each stage we introduce (1) an

152



Customer Service Agent Production Employee Software Engineer Machine Operator Software Tester

support specialist assembler developer operator punch press tester sip
service desk agent manufacturing assistant application programmer machinist test consultant
support staff production engineer software architect buzz saw operator stress tester
customer care agent III factory employee cloud engineer operator turret punch press kit tester
customer service agent casting machine operator lead software engineer blueprint machine operator agile java tester
customer interaction helper production senior developer general operator test engineer
customer care officer production laborer developer/programmer operator nibbler QTP tester

Table 1: Example job title groups from the taxonomy. The total taxonomy consists of 19,927 job titles
in 4,431 groups.

Figure 3: The distributions of group sizes in the
original taxonomy (blue) and the taxonomy aug-
mented with synonym substitutions (green) follow
broken power-law distributions. Note that both
axes are on a logarithmic scale. The figure shows
the long tail of the distribution, in which groups
contain one or only a few job titles.

augmentation of the data which focuses on a par-
ticular property and (2) a test that probes the model
for behavior related to that property. Each stage
builds on the next, so the augmentations from the
previous stage are always included. Initially, the
data set consists of pairs of strings sampled from
the taxonomy in a 4:1 ratio of between-class (neg-
ative) pairs to within-class (positive) pairs. This
ratio was empirically determined but other studies
have found a similar optimal ratio of negative to
positive pairs in Siamese networks (Synnaeve and
Dupoux, 2016). In the subsequent augmentations,
we keep this ratio constant.

1. Typo and spelling invariance. Users of the
system may supply job titles that differ in spelling
from what is present in the taxonomy (e.g., “la-

borer” vs “labourer”) or they may make a typo and
insert, delete or substitute a character. To induce
invariance to these we augment the base taxonomy
by extending it with positive sample pairs consist-
ing of job title strings and the same string but with
20% of characters randomly substituted and 5%
randomly deleted. Of the resulting training set,
10% consists of these typo pairs. The correspond-
ing test set (Typos) consists of all the 19,928 job
title strings in the taxonomy with 5% of their char-
acters randomly substituted or deleted. This corre-
sponds to an approximate upper bound on the pro-
portion of spelling errors (Salthouse, 1986).

2. Synonyms. Furthermore, the model must
be invariant to synonym substitution. To continue
on the example given above, the similarity be-
tween “Java developer” and “Java programmer”
show that in the context of computer science “de-
veloper” and “programmer” are synonyms. This
entails that, given the same context, “developer”
can be substituted for “programmer” in any string
in which it occurs without altering the meaning of
that string. So “C++ developer” can be changed
into “C++ programmer” and still refer to the same
job. Together with the selectivity constraint, the
invariance to synonym substitution constitutes a
form of compositionality on the component parts
of job titles. A model with this compositionality
property will be able to generalize over the mean-
ings of parts of job titles to form useful represen-
tations of unobserved inputs. We augment the data
set by substituting words in job titles by synonyms
from two sources. The first source is a manually
constructed job title synonym set, consisting of
around 1100 job titles, each with between one and
ten synonyms for a total of 7000 synonyms. The
second source of synonyms is by induction. As
in the example above, we look through the taxon-
omy for groups in which two job titles share one
or two words, e.g., “C++”. The complements of
the matching strings form a synonym candidate,
e.g., “developer” and “programmer”. If the can-

153



didate meets certain requirements (neither part oc-
curs in isolation, the parts do not contain special
characters like ‘&’, the parts consist of at most
two words), then the candidate is accepted as a
synonym and is substituted throughout the group.
The effect of this augmentation on the group sizes
is shown in figure 3. The corresponding test set
(Composition) consists of a held out set of 7909
pairs constructed in the same way.

3. Extra words. To be useful in real-world
applications, the model must also be invariant to
the presence of superfluous words. Due to pars-
ing errors or user mistakes the input to the nor-
malization system may contain strings like “look-
ing for C++ developers (urgent!)”, or references
to technologies, certifications or locations that are
not present in the taxonomy. Table 2 shows some
examples of real input. We augment the data set
by extracting examples of superfluous words from
real world data. We construct a set by selecting
those input strings for which there is a job title
in the base taxonomy which is the complete and
strict substring of the input and which the base-
line n-gram matcher selects as the normalization.
As an example, in table 2, the input string “public
relations consultant business business b2c” con-
tains the taxonomy job title “public relations con-
sultant”. Part of this set (N = 1949) is held out
from training and forms the corresponding test set
(Extra Words).

Input string

supervisor dedicated services share plans
part II architectural assistant or architect at
geography teacher 0.4 contract now
customer relationship management developer super user â
forgot password
public relations consultant business business b2c
teaching assistant degree holders only contract

Table 2: Example input strings to the system.

4. Feedback. Lastly, and importantly for indus-
trial applications, we would like our model to be
corrigible, i.e., when the model displays undesired
behavior or our knowledge about the domain in-
creases, we want the model to facilitate manual in-
tervention. As an example, if the trained model as-
signs a high similarity score to the string “Java de-
veloper” and “Coffee expert (Java, Yemen)” based
on the corresponding substrings, we would like to
be able to signal to the model that these particular
instances do not belong together. To test this be-
havior, we manually scored a set of 11929 predic-

tions. This set was subsequently used for training.
The corresponding test set (Annotations) consists
of a different set of 1000 manually annotated held-
out input strings.

4.3 Results

Table 3 shows the results of the experiments. It
compares the baseline n-gram system and pro-
posed neural network models on the four tests out-
lined above. Each of the neural network models
(1)-(4) was trained on augmentations of the data
set that the previous model was trained on.

The first thing to note is that both the n-gram
matching system and the proposed models have
near-complete invariance to simple typos. This is
of course expected behavior, but this test functions
as a good sanity check on the surface form map-
ping to the representations that the models learn.

In the performance of all tests except for the An-
notations test, we see a strong effect of the asso-
ciated augmentation. Model (1) shows 0.04 im-
provement over model (0) on the typo test. This
indicates that the proposed architecture is suitable
for learning invariance to typos, but that the addi-
tion of typos and spelling variants to the training
input only produces marginal improvements over
the already high accuracy on this test.

Model (2) shows 0.29 improvement over model
(1) on the Composition test. This indicates that
model (2) has successfully learned to combine the
meanings of individual words in the job titles into
new meanings. This is an important property for
a system that aims to learn semantic similarity be-
tween text data. Compositionality is arguably the
most important property of human language and it
is a defining characteristic of the way we construct
compound terms such as job titles. Note also that
the model learned this behavior based largely on
observations of combinations of words, while hav-
ing little evidence on the individual meanings.

Model (3) shows 0.45 improvement over model
(2) on the Extra Words test, jumping from 0.29 ac-
curacy to 0.76. This indicates firstly that the pro-
posed model can successfully learn to ignore large
portions of the input sequence and secondly that
the evidence of extra words around the job title is
crucial for the system to do so. Being able to ig-
nore subsequences of an input sequence is an im-
portant ability for information extraction systems.

The improvements on the Annotations test is
also greatest when the extra words are added to the

154



Typos Composition Extra Words Annotations
(N = 19928) (N = 7909) (N = 1949) (N = 1000)

n-gram 0.99 0.61 1.00* 0.83

(0) RNN base taxonomy 0.95 0.55 0.40 0.69
(1) + typos 0.99 0.54 0.36 0.77
(2) + synonyms 1.00 0.83 0.29 0.76
(3) + extra words 1.00 0.84 0.76 0.87
(4) + feedback 1.00 0.79 0.82 0.84

Table 3: Accuracy of the baseline and models on each of the four test cases. The best performing neural
network in each column is indicated in bold. Note that the performance of the n-gram match system (*)
on the Extra Words test is 1.00 by construction.

training set. Model (4) actually shows a decrease
in performance with respect to model (3) on this
test. The cause for this is likely the fact that the
Extra Words test and the held out Annotations tests
show a lot of similarity in the structure of their in-
puts. Real production inputs often consist of ad-
ditional characters, words and phrases before or
after the actual job title. It is unclear why model
(4) shows an improvement on the Extra Words
test while simultaneously showing a decrease in
performance on the Composition and Annotations
tests. This matter is left to future investigation.

5 Discussion

In this paper, we presented a model architec-
ture for learning text similarity based on Siamese
recurrent neural networks. With this architec-
ture, we learned a series of embedding spaces,
each based on a specific augmentation of the
data set used to train the model. The experi-
ments demonstrated that these embedding spaces
captured important invariances of the input; the
models showed themselves invariant to spelling
variation, synonym replacements and superfluous
words. The proposed architecture made no as-
sumptions on the input distribution and naturally
scales to a large number of classes.

The ability of the system to learn these in-
variances stems from the contrastive loss function
combined with the stack of recurrent layers. Using
separate loss functions for similar and dissimilar
samples helps the model maintain selectivity while
learning invariance over different sources of vari-
ability. The experiment shows that the explicit use
of prior knowledge to add these sources of invari-
ance to the system was crucial in learning. With-
out this knowledge extra words and synonyms will

negatively affect the performance of the system.
We would like to explore several directions in

future work. The possibility space around the
proposed network architecture could be explored
more fully, for example by incorporating convolu-
tional layers in addition to the recurrent layers, or
by investigating a triplet loss function instead of
the contrastive loss used in this study.

The application used here is a good use case for
the proposed system, but in future work we would
also like to explore the behavior of the Siamese
recurrent network on standard textual similarity
and semantic entailment data sets. In addition, the
baseline used in this paper is relatively weak. A
comparison to a stronger baseline would serve the
further development of the proposed models.

Currently negative samples are selected ran-
domly from the data set. Given the similarity be-
tween some groups and the large differences in
group sizes, a more advanced selection strategy is
likely to yield good results. For example, nega-
tive samples could be chosen such that they always
emphasize minimal distances between groups. In
addition, new sources of variation as well as the
sampling ratios between them can be explored.

Systems like the job title taxonomy used in the
current study often exhibit a hierarchical structure
that we did not exploit or attempt to model in the
current study. Future research could attempt to
learn a single embedding which would preserve
the separations between groups at different lev-
els in the hierarchy. This would enable sophisti-
cated transfer learning based on a rich embedding
space that can represent multiple levels of similar-
ities and contrasts simultaneously.

155



References
Fabio Anselmi, Lorenzo Rosasco, and Tomaso Poggio.

2015. On invariance and selectivity in representa-
tion learning. arXiv preprint arXiv:1503.05938.

Lorenzo Baraldi, Costantino Grana, and Rita Cuc-
chiara. 2015. A deep siamese network for scene
detection in broadcast videos. In Proceedings of the
23rd Annual ACM Conference on Multimedia Con-
ference, pages 1199–1202. ACM.

Oren Barkan and Noam Koenigstein. 2016. Item2vec:
Neural item embedding for collaborative filtering.
arXiv preprint arXiv:1603.04259.

Filip Boltuzic and Jan Šnajder. 2015. Identifying
prominent arguments in online debates using seman-
tic textual similarity. In Proceedings of the 2nd
Workshop on Argumentation Mining, pages 110–
115.

Jane Bromley, James W Bentz, Léon Bottou, Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
Säckinger, and Roopak Shah. 1993. Signature ver-
ification using a “siamese” time delay neural net-
work. International Journal of Pattern Recognition
and Artificial Intelligence, 7(04):669–688.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with
application to face verification. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
539–546. IEEE.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2004. Timbl: Tilburg
memory-based learner. Tilburg University.

Cı́cero Nogueira dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In COLING, pages 69–78.

Alex Graves. 2012. Supervised sequence labelling.
Springer.

Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006.
Dimensionality reduction by learning an invariant
mapping. In Computer vision and pattern recog-
nition, 2006 IEEE computer society conference on,
volume 2, pages 1735–1742. IEEE.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504–507.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Elad Hoffer and Nir Ailon. 2015. Deep metric learning
using triplet network. In Similarity-Based Pattern
Recognition, pages 84–92. Springer.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems, pages 2042–2050.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Faizan Javed, Matt McNair, Ferosh Jacob, and Meng
Zhao. 2014. Towards a job title classification sys-
tem. In WSCBD 2014: Webscale Classification:
Classifying Big Data from the Web, WSDM Work-
shop.

Faizan Javed, Qinlong Luo, Matt McNair, Ferosh Ja-
cob, Meng Zhao, and Tae Seung Kang. 2015.
Carotene: A job title classification system for the
online recruitment domain. In Big Data Computing
Service and Applications (BigDataService), 2015
IEEE First International Conference on, pages 286–
293. IEEE.

H. Kamper, W. Wang, and K. Livescu. 2016.
Deep convolutional acoustic word embeddings us-
ing word-pair side information. In Proceedings
ICASSP.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2015. Character-aware neural lan-
guage models. arXiv preprint arXiv:1508.06615.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wang Ling, Tiago Luı́s, Luı́s Marujo, Ramón Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015. Finding function
in form: Compositional character models for open
vocabulary word representation. arXiv preprint
arXiv:1508.02096.

Emmanuel Malherbe, Mamadou Diaby, Mario Cataldi,
Emmanuel Viennet, and Marie-Aude Aufaure.
2014. Field selection for job categorization and
recommendation to social network users. In Ad-
vances in Social Networks Analysis and Mining
(ASONAM), 2014 IEEE/ACM International Confer-
ence on, pages 588–595. IEEE.

James H. Martin and Daniel Jurafsky. 2000. Speech
and language processing. International Edition.

156



Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In Thirtieth AAAI Conference on Artificial Intel-
ligence.

R. Pascanu, T. Mikolov, and Y. Bengio. 2013. On
the difficulty of training recurrent neural networks.
Journal of Machine Learning Research.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP, volume 14, pages
1532–1543.

Luca Ponzanelli, Andrea Mocci, and Michele Lanza.
2015. Summarizing complex development artifacts
by mining heterogeneous data. In Proceedings of
the 12th Working Conference on Mining Software
Repositories, pages 401–405. IEEE Press.

Timothy A. Salthouse. 1986. Perceptual, cognitive,
and motoric aspects of transcription typing. Psycho-
logical bulletin, 99(3):303.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.

Richard Socher, Eric H. Huang, Jeffrey Pennin,
Christopher D Manning, and Andrew Y. Ng. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems, pages 801–
809.

Martijn Spitters, Remko Bonnema, Mihai Rotaru, and
Jakub Zavrel. 2010. Bootstrapping information ex-
traction mappings by similarity-based reuse of tax-
onomies. In CEUR Workshop Proceedings, volume
673. Citeseer.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Gabriel Synnaeve and Emmanuel Dupoux. 2016. A
temporal coherence loss function for learning unsu-
pervised acoustic embeddings. Procedia Computer
Science, 81:95–100.

Gabriel Synnaeve, Thomas Schatz, and Emmanuel
Dupoux. 2014. Phonetics embedding learning with
side information. In Spoken Language Technology
Workshop (SLT), 2014 IEEE, pages 106–111. IEEE.

Roland Thiolliere, Ewan Dunbar, Gabriel Synnaeve,
Maarten Versteegh, and Emmanuel Dupoux. 2015.
A hybrid dynamic time warping-deep neural net-
work architecture for unsupervised acoustic model-
ing. In Proc. Interspeech.

Jiang Wang, Yang Song, Thomas Leung, Chuck Rosen-
berg, Jingbin Wang, James Philbin, Bo Chen, and
Ying Wu. 2014. Learning fine-grained image simi-
larity with deep ranking. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1386–1393.

Peilu Wang, Yao Qian, Frank K Soong, Lei He, and
Hai Zhao. 2015. A unified tagging solution: Bidi-
rectional lstm recurrent neural network with word
embedding. arXiv preprint arXiv:1511.00215.

Jason Weston, Antoninipe Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards AI-complete ques-
tion answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698.

Longqi Yang, Yin Cui, Fan Zhang, John P Pollak,
Serge Belongie, and Deborah Estrin. 2015. Plate-
click: Bootstrapping food preferences through an
adaptive visual interface. In Proceedings of the
24th ACM International on Conference on Informa-
tion and Knowledge Management, pages 183–192.
ACM.

Neil Zeghidour, Gabriel Synnaeve, Maarten Versteegh,
and Emmanuel Dupoux. 2015. A deep scattering
spectrum - deep siamese network pipeline for unsu-
pervised acoustic modeling. In IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing.

Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP, pages 1393–1398.

157


