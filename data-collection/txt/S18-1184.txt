



















































ECNU at SemEval-2018 Task 12: An End-to-End Attention-based Neural Network for the Argument Reasoning Comprehension Task


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1094–1098
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

ECNU at SemEval-2018 Task 12: An End-to-End Attention-based Neural
Network for the Argument Reasoning Comprehension Task

Junfeng Tian1, Man Lan1,2* and Yuanbin Wu1,2
1 School of Computer Science and Software Engineering,

East China Normal University, Shanghai, P.R.China
2 Shanghai Key Laboratory of Multidimensional Information Processing

51151201048@stu.ecnu.edu.cn, {mlan, ybwu}@cs.ecnu.edu.cn

Abstract

This paper presents our submissions to Se-
mEval 2018 Task 12: the Argument Reasoning
Comprehension Task. We investigate an end-
to-end attention-based neural network to rep-
resent the two lexically close candidate war-
rants. On the one hand, we extract their differ-
ent parts as attention vectors to obtain distin-
guishable representations. On the other hand,
we use their surrounds (i.e., claim, reason, de-
bate context) as another attention vectors to get
contextual representations, which work as fi-
nal clues to select the correct warrant. Our
model achieves 60.4% accuracy and ranks 3rd

among 22 participating systems.

1 Introduction

Reasoning is a crucial part of natural language ar-
gumentation. In order to comprehend an argumen-
t, one must analyze its warrant, which explains
why its claim follows form its premises (aka rea-
sons) (Habernal et al., 2018a).

SemEval-2018 Task 12 provides the argumen-
t reasoning comprehension task (Habernal et al.,
2018b). Given a reason and a claim along with the
title and a short description of the debate they oc-
cur in, the goal is to identify the correct warrant
from two candidates. Figure 1 gives an example.
The abstract structure of an argument is: reason
→ (since) warrant→ (therefore) claim.

The challenging factor is that both candidate
warrants are plausible and lexically very close
while leading to contradicting claims (Habernal
et al., 2018a). Here we give three examples of the
two candidate warrants:

Ex1: A huge pandemic would (not) be a great news story.

Ex2: The role of a citizen and a supreme court justice are
inseparable /separable.

Ex3: The rest of the comments can be skipped easily
/make the section unreadable.

Title: Have Comment Section Failed?
Description: In recent years, many media companies have
disabled them because of widespread abuse and obscenity.
Reason: Many comments are thoughtful and insightful. And
since {Warrant0 |Warrant1},
Claim: Comment sections have not failed.
3 Warrant0: The rest of the comments can be skipped eas-
ily.
7 Warrant1: The rest of the comments make the section un-
readable.

Figure 1: An example of a debate in the argument rea-
soning comprehension task.

The differences are either negative words, or
antonyms, or opposite phrases. Therefore, it is
important to emphasize the different parts to ob-
tain distinguish representations of the two war-
rants, which express the opposite meanings.

To address this factor, we proposed an end-to-
end attention-based neural network. On the one
hand, we extract the different parts of the two war-
rants and use them as attention vectors to obtain
warrants’ distinguishable representations. On the
other hand, we represent their surrounds (i.e., rea-
son, claim, debate context) as another attention
vector to get the contextual representations.

2 System Architecture

Formally, given an instance containing two candi-
date warrants (W0,W1) and the context around the
warrants (i.e., R,C, T, I), the goal is to choose the
correct warrant y ∈ {0, 1}, where y = 0 means
W0 is the correct answer, and y = 1 otherwise.

2.1 Overview

The network is inspired by Siamese network
(Mueller and Thyagarajan, 2016). The two can-
didate warrants are modeled in the same structure.
Figure 2 illustrates our system architecture.

First, we first extract the different parts of war-

1094



Figure 2: The system architecture

rant0, warrant1 and claim to get Diff W0, Diff W1
and Diff Claim (see in Sec. 2.2).

Then, we stack a CNN and a RNN to represent
each component to obtain representation of each
component, because the combination of CNN and
RNN can make use of the spatial and temporal
context information (in Sec. 2.3).

Next, the intra-temporal attention is adopted to
obtain distinguishable representations of warrants.
Similarly, we apply the same strategy to the claim.
The intra-temporal attention is introduced in Sec.
2.4).

After that, we concatenate the representation of
surrounds (i.e., reason, claim, debate context and
warrant) as another attention vector to obtain the
contextual representations of the warrants.

Last, we adopt a dense layer to obtain the prob-
ability of the two candidate warrants (in Sec. 2.5).
The contextual representations work as hidden
clues to select the correct warrant.

2.2 Extract the Different Part

The two candidate warrants are lexically close (s-
ince they often mean the opposite), thus we extract
the different part between them to serve as atten-
tion vector to guide the neural network to generate
distinguishable representation for the warrants.

To do this, we remove the longest common pre-
fix and suffix, and let the remain part as the differ-
ent part, denoted as Diff W0, Diff W1. Take cases

mentioned in Sec. 1 as examples, it would extract
“not be ” as Diff W0 and “be” as Diff W1 in Ex1;
“inseparable” as Diff W0 and “separable” as Dif-
f W1 in Ex2; “can be skipped easily” as Diff W0
and “make the section unreadable” as Diff W1 in
Ex3. Note that if the remain part is empty, we use
the word after the prefix as the different part.

Similarly, we also get the different part between
the claim and its opposite, denoted as Diff Claim.
There always exists the opposite claims in de-
bates, since the reason chains R → W → C
and R →qW →qC both exists. We collected the
claims and warrants under the same debate. If the
warrants express the opposite meaning, then the t-
wo claims are opposite. Besides, the organizers
also provide similar dataset in “data/train-w-swap-
doubled.tsv”.

2.3 Context Representation

To incorporate contextual information of each
components in a debate, we combine Convolu-
tional Neural Network (CNN) and Recurrent neu-
ral network (RNN) to encode the input word vec-
tors. CNN is good at dealing with spatially related
data, such as “sometimes warranted” and “rarely
warranted”, while RNN is good at temporal sig-
nals. Instead of using a typical vanilla RNN, we
use Long Short-Term Memory Network (Hochre-
iter and Schmidhuber, 1997) for eliminating the
issue of long term dependencies.

1095



Given a sentence S = {wi}n1 , we first map each
word wi into its vector representation xi ∈ Rd via
a look-up table of word embeddings (d is the di-
mension of the word embeddings).

Then, we adopt CNN on the input sequence
{xi}n1 to obtain the spatial representation {x′i}n1 :

eji = ReLU(w
j [xi, . . . , xi+k−1]) (1)

x′i = [e
1
i , . . . , e

m
i ](1 ≤ j ≤ m) (2)

where k is the window size, wj is the parameter of
a filter, m is the number of the filters. We also
adopt padding before the convolution operation.
As a result, we obtain the spatial representations
x′i ∈ Rm, which has the same length as the input
sequence.

After that, we utilize a bi-directional LSTM (Bi-
LSTM) to obtain the temporal information. For
each time step t, the LSTM unit computation cor-
responds to :

it = σ(Wix
′
t + Uiht−1 + bi) (3)

ft = σ(Wfx
′
t + Ufht−1 + bf ) (4)

ot = σ(Wox
′
t + Uoht−1 + bo) (5)

c̃t = tanh(Wcx
′
t + Ucht−1 + bc) (6)

ct = it � c̃t + ft � ct−1 (7)
ht = ot � tanh(ct) (8)

where σ is the element-wise sigmoid function, �
is the element-wise product and it, ft, ot ,ct de-
mote the input gate, forget gate, output gate and
memory cell respectively.

2.4 Intra-Temporal Attention
Inspired from Habernal et al. (2018a), we use an
intra-temporal attention function to attend over
specific parts of the input sequence. This kind of
attention encourages the model to generate differ-
ent representation according to the attention vec-
tor. Habernal et al. (2018a) have shown that such
intra-temporal attention outperforms standard at-
tention.

We define va as the attention vector, and ht as
the hidden states at time step t:

mt = tanh(Umht � va + bm) (9)
at = σ(Wamt + ba) (10)

ht = ht � at (11)

where at is the attention weights over the hidden
states ht, � is element-wise multiplication.

We first apply the intra-temporal attention over
W0 and W1, in order to obtain different warran-
t representations from Diff W0 and Diff W1. As
a result, the model can easily distinguish the two
candidate warrants. Similarly, we apply the atten-
tion over claim to make the claim representation
distinguishable.

Moreover, we adopt another intra-temporal at-
tention over W0 and W1, with the concatenation
of {claim, reason, debate context} representation-
s as attention vector. The candidate warrants re-
ceive the information from the claim, reason and
debate context, and the model would select the
correct warrant which satisfies the reasoning chain
R→W → C.

Finally, we obtain two attended warrant vectors
attW0 , attW1 .

2.5 Output
To evaluate the probability distribution of the t-
wo candidate warrants, we employ a feed-forward
neural network with one dense layer, and apply the
Softmax function to predict the probability.

ho = ReLU(Wo[attW0 , attW1 ]) (12)

p̂ = Softmax(Wpho) (13)

As for the optimization, cross-entropy loss is used
as the loss function since we are handling a binary
classification problem:

L = −
(
yi log p̂i + (1− yi) log(1− p̂i)

)
(14)

where yi is the gold label.

3 Experiments

3.1 Datasets
SemEval 2018 provided 1,970 instances for the ar-
gument reasoning comprehension task (Habernal
et al., 2018b). The instances are divided into three
sets based on the year when the debates are taken
from. Table 1 lists the statistics of the datasets. We
also include the number of debate topics of each
set.

Dataset # Pairs # Topics Source Year
Train 1,210 111 2011-2015
Dev 316 31 2016
Test 444 30 2017

Table 1: The statistics of the datasets

Being a binary task, accuracy (Acc) is adopted
as the evaluation metric.

1096



Approach Dev
Intra-warrant attention 0.638 (±0.024)
Intra-warrant attention w/ context 0.637 (±0.040)
Our basic model 0.666 (±0.019)
· + Diff {W0, W1} 0.678 (±0.001)
· + Diff {W0, W1} + CNN 0.675 (±0.010)
· + Diff {W0, W1, Claim} + CNN 0.676 (±0.010)
Ensemble (Vote) 0.708

Table 2: Accuracy of each approach on the developing
dataset.

3.2 Parameters Setting

The word embeddings are initialized with the 300d
pre-trained word2vec (Mikolov et al., 2013), and
do not fine-tune during training. The window
sizes of CNN is (1,2,3) and the kernel size is
50. The dimensions of the hidden size in Bi-
LSTM and Att-LSTM are set to 50. The dense
layer in Output is 25. We train the model us-
ing Adam (Kingma and Ba, 2014) with gradient
clipping (the max norm is set to 30, batch size is
32), The networks are regularized by dropout (the
dropout ratio equals 0.8). We ran each model three
times with random initializations. Our code is
available at https://github.com/rgtjf/
SemEval2018-Task12.

3.3 Results on Training Data

Table 2 shows the results of each components of
our attention-based neural network. We have the
following findings:

(1) Comparing with the Intra-warrant attention
(w/ context) provided by the organizer, our basic
model obtains 2.8% improvement through sharing
parameters in Bi-LSTM. It indicates that the neu-
ral network need sufficient training data and pa-
rameters sharing could alleviate the demand.

(2) All of the three improvements achieves im-
proved accuracy. It suggests that utilizing the d-
ifferent part as attention vector can obtain dis-
criminative representation, which is beneficial for
choosing the correct answer.

(3) The introduction of CNN does not seem to
improve the performance of the model. The pos-
sible reason may be that RNN actually learn any
computational function and capture the spatial in-
formation.

(4) The ensemble of the three networks can fur-
ther improve the performance. Therefore, we con-
figure the ensemble model as our final submission.

Approach Test
Human average 0.798 (±0.162)
Human w/ training in reasoning 0.909 (±0.114)
Random baseline 0.508 (±0.015)
Intra-warrant attention w/ context 0.584 (±0.015)
Rank 1: GIST 0.712
Rank 2: blcu nlp 0.606
Rank 3: ECNU 0.604

Table 3: Accuracy of each approach (humans and sys-
tems) on the test set.

3.4 Results on Test Data

Table 3 lists the results of three top systems and
several baselines provided by the organizer. We
find that: (1) Comparing with Intra-warrant at-
tention w/ context model, our model outperform-
s it by 2% in terms of accuracy, which demon-
strates the efficiency of the proposed attention-
based model. (2) Comparing with GIST and
blcu nlp, our result is comparable to blcu nlp but
worse than GIST. Both of them use the pre-trained
ESIM model (Chen et al., 2017) trained on SNLI
(Bowman et al., 2015) and MultiNLI (Nangia
et al., 2017) dataset. Our model only uses the
training dataset and does not require any extra re-
sources. However, this is also the limitation of
our model because this small-size dataset is insuf-
ficient to learn parameters in our model.

4 Conclusion

In this work, we propose an end-to-end neural net-
work for the reading comprehension task. We s-
tack a CNN and a RNN to represent each com-
ponent in a debate and extract the warrants’ and
claim’s different part as attention vector to obtain
their distinguish representation. Moreover, we use
another attention network to incorporate the infor-
mation of reason, claim, debate context into the
contextual representation of the warrants for final
decisions. Our model achieves 60.4% accuracy
and ranks 3rd among 22 participating systems.

Acknowledgments

The authors would like to thank Changzhi Sun
for his valuable suggestions and the anonymous
reviewers for their helpful comments. This
work is supported by grants from Science and
Technology Commission of Shanghai Municipal-
ity (15ZR1410700), the Open Project of Shang-
hai Key Laboratory of Trustworthy Computing
(No.07dz22304201604).

1097



References
Samuel R. Bowman, Gabor Angeli, Christopher Potts,

and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced lstm for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).

Ivan Habernal, Henning Wachsmuth, Iryna Gurevych,
and Benno Stein. 2018a. The argument reasoning
comprehension task: Identification and reconstruc-
tion of implicit warrants. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.

Ivan Habernal, Henning Wachsmuth, Iryna Gurevych,
and Benno Stein. 2018b. Semeval-2018 task 12:
The argument reasoning comprehension task. In
Proceedings of the 12th International Workshop on
Semantic Evaluation (SemEval-2018), New Orleans,
LA, USA. Association for Computational Linguis-
tics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR, ab-
s/1412.6980.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In Thirtieth AAAI Conference on Artificial Intel-
ligence.

Nikita Nangia, Adina Williams, Angeliki Lazaridou,
and Samuel Bowman. 2017. The repeval 2017
shared task: Multi-genre natural language inference
with sentence representations. In Proceedings of the
2nd Workshop on Evaluating Vector Space Repre-
sentations for NLP, pages 1–10.

1098


