



















































Document Context Neural Machine Translation with Memory Networks


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1275–1284
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1275

Document Context Neural Machine Translation with Memory Networks

Sameen Maruf and Gholamreza Haffari
Faculty of Information Technology, Monash University, Australia

{firstname.lastname}@monash.edu

Abstract

We present a document-level neural ma-
chine translation model which takes both
source and target document context into
account using memory networks. We
model the problem as a structured pre-
diction problem with interdependencies
among the observed and hidden variables,
i.e., the source sentences and their unob-
served target translations in the document.
The resulting structured prediction prob-
lem is tackled with a neural translation
model equipped with two memory com-
ponents, one each for the source and tar-
get side, to capture the documental inter-
dependencies. We train the model end-
to-end, and propose an iterative decod-
ing algorithm based on block coordinate
descent. Experimental results of English
translations from French, German, and Es-
tonian documents show that our model is
effective in exploiting both source and tar-
get document context, and statistically sig-
nificantly outperforms the previous work
in terms of BLEU and METEOR.

1 Introduction

Neural machine translation (NMT) has proven to
be powerful (Sutskever et al., 2014; Bahdanau
et al., 2015). It is on-par, and in some cases,
even surpasses the traditional statistical MT (Lu-
ong et al., 2015) while enjoying more flexibil-
ity and significantly less manual effort for fea-
ture engineering. Despite their flexibility, most
neural MT models translate sentences indepen-
dently. Discourse phenomenon such as pronomi-
nal anaphora and lexical consistency, may depend
on long-range dependency going farther than a

few previous sentences, are neglected in sentence-
based translation (Bawden et al., 2017).

There are only a handful of attempts to
document-wide machine translation in statistical
and neural MT camps. Hardmeier and Federico
(2010); Gong et al. (2011); Garcia et al. (2014)
propose document translation models based on
statistical MT but are restrictive in the way they
incorporate the document-level information and
fail to gain significant improvements. More re-
cently, there have been a few attempts to incorpo-
rate source side context into neural MT (Jean et al.,
2017; Wang et al., 2017; Bawden et al., 2017);
however, these works only consider a very local
context including a few previous source/target sen-
tences, ignoring the global source and target docu-
mental contexts. The latter two report deteriorated
performance when using the target-side context.

In this paper, we present a document-level ma-
chine translation model which combines sentence-
based NMT (Bahdanau et al., 2015) with mem-
ory networks (Sukhbaatar et al., 2015). We cap-
ture the global source and target document con-
text with two memory components, one each for
the source and target side, and incorporate it into
the sentence-based NMT by changing the decoder
to condition on it as the sentence translation is
generated. We conduct experiments on three lan-
guage pairs: French-English, German-English and
Estonian-English. The experimental results and
analysis demonstrate that our model is effective in
exploiting both source and target document con-
text, and statistically significantly outperforms the
previous work in terms of BLEU and METEOR.

2 Background

2.1 Neural Machine Translation (NMT)

Our document NMT model is grounded on
sentence-based NMT model (Bahdanau et al.,



1276

2015) which contains an encoder to read the
source sentence as well as an attentional decoder
to generate the target translation.

Encoder It is a bidirectional RNN consisting of
two RNNs running in opposite directions over the
source sentence:
−→
hi =

−−→
RNN(

−→
h i−1,ES [xi]),

←−
h i =

←−−
RNN(

←−
h i+1,ES [xi])

where ES [xi] is embedding of the word xi from
the embedding table ES of the source language,
and
−→
h i and

←−
h i are the hidden states of the for-

ward and backward RNNs which can be based on
the LSTM (Hochreiter and Schmidhuber, 1997) or
GRU (Cho et al., 2014) units. Each word in the
source sentence is then represented by the concate-
nation of the corresponding bidirectional hidden
states, hi = [

−→
h i;
←−
h i].

Decoder The generation of each word yj is con-
ditioned on all of the previously generated words
y<j via the state of the RNN decoder sj , and the
source sentence via a dynamic context vector cj :

yj ∼ softmax(Wy · rj + br)
rj = tanh(sj +Wrc · cj +Wrj ·ET [yj−1])
sj = tanh(Ws · sj−1 +Wsj ·ET [yj−1] +Wsc · cj)

where ET [yj ] is embedding of the word yj from
the embedding table ET of the target language,
and W matrices and br vector are the parame-
ters. The dynamic context vector cj is computed
via cj =

∑
i αjihi, where

αj = softmax(aj)

aji = v · tanh(Wae · hi +Wat · sj−1)

This is known as the attention mechanism which
dynamically attends to relevant parts of the source
necessary for generating the next target word.

2.2 Memory Networks (MemNets)
Memory Networks (Weston et al., 2015) are a
class of neural models that use external memo-
ries to perform inference based on long-range de-
pendencies. A memory is a collection of vec-
tors M = {m1, ..,mK} constituting the mem-
ory cells, where each cell mk may potentially
correspond to a discrete object xk. The mem-
ory is equipped with a read and optionally a
write operation. Given a query vector q, the out-
put vector generated by reading from the mem-
ory is

∑|M |
i=1 pimi, where pi represents the rele-

vance of the query to the i-th memory cell p =

Figure 1: Factor graph for document-level MT

softmax(qT ·M). For the rest of the paper, we
denote the read operation by MemNet(M , q).

3 Document NMT as Structured
Prediction

We formulate document-wide machine translation
as a structured prediction problem. Given a set
of sentences {x1, . . . ,x|d|} in a source document
d, we are interested in generating the collection
of their translations {y1, . . . ,y|d|} taking into ac-
count interdependencies among them imposed by
the document. We achieve this by the factor graph
in Figure 1 to model the probability of the target
document given the source document. Our model
has two types of factors:

• fθ(yt;xt,x−t) to capture the interdependen-
cies between the translation yt, the corre-
sponding source sentence xt and all the other
sentences in the source document x−t, and

• gθ(yt;y−t) to capture the interdependencies
between the translation yt and all the other
translations in the document y−t.

Hence, the probability of a document translation
given the source document is

P (y1, . . . ,y|d||x1, . . . ,x|d|) ∝

exp
(∑

t

fθ(yt;xt,x−t) + gθ(yt;y−t)
)
.

The factors fθ and gθ are realised by neural ar-
chitectures whose parameters are collectively de-
noted by θ.

Training It is challenging to train the model
parameters by maximising the (regularised) like-
lihood since computing the partition function is
hard. This is due to the enormity of factors



1277

gθ(yt;y−t) over a large number of translation
variables yt’s (i.e., the number of sentences in
the document) as well as their unbounded domain
(i.e., all sentences in the target language). Thus,
we resort to maximising the pseudo-likelihood
(Besag, 1975) for training the parameters:

argmax
θ

∏
d∈D

|d|∏
t=1

Pθ(yt|xt,y−t,x−t) (1)

whereD is the set of bilingual training documents,
and |d| denotes the number of (bilingual) sen-
tences in the document d = {(xt,yt)}|d|t=1. We
directly model the document-conditioned NMT
model Pθ(yt|xt,y−t,x−t) using a neural archi-
tecture which subsumes both the fθ and gθ factors
(covered in the next section).

Decoding To generate the best translation for
a document according to our model, we need to
solve the following optimisation problem:

arg max
y1,...,y|d|

|d|∏
t=1

Pθ(yt|xt,y−t,x−t)

which is hard (due to similar reasons as mentioned
earlier). We hence resort to a block coordinate de-
scent optimisation algorithm. More specifically,
we initialise the translation of each sentence using
the base neural MT model P (yt|xt). We then re-
peatedly visit each sentence in the document, and
update its translation using our document-context
dependent NMT model P (yt|xt,y−t,x−t) while
the translations of other sentences are kept fixed.

4 Context Dependent NMT with
MemNets

We augment the sentence-level attentional NMT
model by incorporating the document context
(both source and target) using memory networks
when generating the translation of a sentence, as
shown in Figure 2.

Our model generates the target translation
word-by-word from left to right, similar to the
vanilla attentional neural translation model. How-
ever, it conditions the generation of a target word
not only on the previously generated words and
the current source sentence (as in the vanilla NMT
model), but also on all the other source sentences
of the document and their translations. That is, the

generation process is as follows:

Pθ(yt|xt,y−t,x−t) =
|yt|∏
j=1

Pθ(yt,j |yt,<j ,xt,y−t,x−t)

(2)

where yt,j is the j-th word of the t-th target sen-
tence, yt,<j are the previously generated words,
and x−t and y−t are as introduced previously.

Our model represents the source and target doc-
ument contexts as external memories, and at-
tends to relevant parts of these external memo-
ries when generating the translation of a sentence.
Let M [x−t] and M [y−t] denote external memo-
ries representing the source and target document
context, respectively. These contain memory cells
corresponding to all sentences in the document ex-
cept the t-th sentence (described shortly). Let ht
and st be representations of the t-th source sen-
tence and its current translation, from the encoder
and decoder respectively. We make use of ht
as the query to get the relevant context from the
source external memory:

csrct = MemNet(M [x−t],ht)

Furthermore, for the t-th sentence, we get the rel-
evant information from the target context:

ctrgt = MemNet(M [y−t], st +Wat · ht)

where the query consists of the representation of
the translation st from the decoder endowed with
that of the source sentence ht from the encoder
to make the query robust to potential noises in the
current translation and circumvent error propaga-
tion, and Wat projects the source representation
into the hidden state space.

Now that we have representations of the rele-
vant source and target document contexts, Eq. 2
can be re-written as:

Pθ(yt|xt,y−t,x−t) =
|yt|∏
j=1

Pθ(yt,j |yt,<j ,xt, ctrgt , c
src
t )

(3)

More specifically, the memory contexts csrct and
ctrgt are incorporated into the NMT decoder as:

• Memory-to-Context in which the memory
contexts are incorporated when computing
the next decoder hidden state:

st,j = tanh(Ws · st,j−1 +Wsj ·ET [yt,j ] +
Wsc · ct,j +Wsm · csrct +Wst · c

trg
t )



1278

Figure 2: Our Memory-to-Context document-
NMT model consisting of sentence-based NMT
model with source and target external memories.

• Memory-to-Output in which the memory
contexts are incorporated in the output layer:

yt,j ∼ softmax(Wy · rt,j +Wym · csrct +
Wyt · ctrgt + br)

where Wsm, Wst, Wym, and Wyt are the new
parameter matrices. We use only the source, only
the target, or both external memories as the ad-
ditional conditioning contexts. Furthermore, we
use either the Memory-to-Context or Memory-to-
Output architectures for incorporating the docu-
ment contexts. In the experiments, we will explore
these different options to investigate the most ef-
fective combination. We now turn our attention to
the construction of the external memories for the
source and target sides of a document.

The Source Memory We make use of a hierar-
chical 2-level RNN architecture to construct the
external memory of the source document. More
specifically, we pass each sentence of the docu-
ment through a sentence-level bidirectional RNN
to get the representation of the sentence (by con-
catenating the last hidden states of the forward
and backward RNNs). We then pass the sentence
representations through a document-level bidirec-
tional RNN to propagate sentences’ information
across the document. We take the hidden states

of the document-level bidirectional RNNs as the
memory cells of the source external memory.

The source external memory is built once for
each minibatch, and does not change through-
out the document translation. To be able to fit
the computational graph of the document NMT
model within GPU memory limits, we pre-train
the sentence-level bidirectional RNN using the
language modelling training objective. However,
the document-level bidirectional RNN is trained
together with other parameters of the document
NMT model by back-propagating the document
translation training objective.

The Target Memory The memory cells of the
target external memory represent the current trans-
lations of the document. Recall from the previous
section that we use coordinate descent iteratively
to update these translations. Let {y1, . . . ,y|d|} be
the current translations, and let {s|y1|, . . . , s|y|d||}
be the last states of the decoder when these trans-
lations were generated. We use these last de-
coder states as the cells of the external target mem-
ory. We could make use of hierarchical sentence-
document RNNs to transform the document trans-
lations into memory cells (similar to what we do
for the source memory); however, it would have
been computationally expensive and may have re-
sulted in error propagation. We will show in the
experiments that our efficient target memory con-
struction is indeed effective.

5 Experiments and Analysis

Datasets. We conducted experiments on three
language pairs: French-English, German-English
and Estonian-English. Table 1 shows the statis-
tics of the datasets used in our experiments.
The French-English dataset is based on the TED
Talks corpus1 (Cettolo et al., 2012) where each
talk is considered a document. The Estonian-
English data comes from the Europarl v7 corpus2

(Koehn, 2005). Following Smith et al. (2013),
we split the speeches based on the SPEAKER
tag and treat them as documents. The French-
English and Estonian-English corpora were ran-
domly split into train/dev/test sets. For German-
English, we use the News Commentary v9 corpus3

for training, news-dev2009 for development,
1https://wit3.fbk.eu/
2http://www.statmt.org/europarl/
3http://statmt.org/wmt14/news-commentary-v9-by-

document.tgz



1279

# docs # sents doc len src/tgt vocab
Fr-En 10/1.2/1.5 123/15/19 123/128/124 25.1/21
Et-En 150/10/18 209/14/25 14/14/14 48.6/24.9
De-En 49/.9/1.1/1.6 191/2/3/3 39/23/27/19 45.1/34.7

Table 1: Training/dev/test corpora statistics: num-
ber of documents (×100) and sentences (×1000),
average document length (in sentences) and
source/target vocabulary size (×1000). For De-
En, we report statistics of the two test sets
news-test2011 and news-test2016.

and news-test2011 and news-test2016
as the test sets. The news-commentary corpus has
document boundaries already provided.

We pre-processed all corpora to remove very
short documents and those with missing trans-
lations. Out-of-vocabulary and rare words (fre-
quency less than 5) are replaced by the <UNK>
token, following Cohn et al. (2016).4

Evaluation Measures We use BLEU (Papineni
et al., 2002) and METEOR (Lavie and Agarwal,
2007) scores to measure the quality of the gen-
erated translations. We use bootstrap resampling
(Clark et al., 2011) to measure statistical signifi-
cance, p < 0.05, comparing to the baselines.

Implementation and Hyperparameters We
implement our document-level neural machine
translation model in C++ using the DyNet li-
brary (Neubig et al., 2017), on top of the basic
sentence-level NMT implementation in mantis
(Cohn et al., 2016). For the source memory, the
sentence and document-level bidirectional RNNs
use LSTM and GRU units, respectively. The
translation model uses GRU units for the bidi-
rectional RNN encoder and the 2-layer RNN de-
coder. GRUs are used instead of LSTMs to re-
duce the number of parameters in the main model.
The RNN hidden dimensions and word embed-
ding sizes are set to 512 in the translation and
memory components, and the alignment dimen-
sion is set to 256 in the translation model.

Training We use a stage-wise method to train
the variants of our document context NMT
model. Firstly, we pre-train the Memory-to-
Context/Memory-to-Output models, setting their
readings from the source and target memories to

4We do not split words into subwords using BPE (Sen-
nrich et al., 2016) as that increases sentence lengths resulting
in removing long documents due to GPU memory limitations,
which would heavily reduce the amount of data that we have.

the zero vector. This effectively learns parame-
ters associated with the underlying sentence-based
NMT model, which is then used as initialisation
when training all parameters in the second stage
(including the ones from the first stage). For the
first stage, we make use of stochastic gradient de-
scent (SGD)5 with initial learning rate of 0.1 and
a decay factor of 0.5 after the fourth epoch for a
total of ten epochs. The convergence occurs in
6-8 epochs. For the second stage, we use SGD
with an initial learning rate of 0.08 and a decay
factor of 0.9 after the first epoch for a total of 15
epochs6. The best model is picked based on the
dev-set perplexity. To avoid overfitting, we em-
ploy dropout with the rate 0.2 for the single mem-
ory model. For the dual memory model, we set
dropout for Document RNN to 0.2 and for the en-
coder and decoder to 0.5. Mini-batching is used
in both stages to speed up training. For the largest
dataset, the document NMT model takes about 4.5
hours per epoch to train on a single P100 GPU,
while the sentence-level model takes about 3 hours
per epoch for the same settings.

When training the document NMT model in the
second stage, we need the target memory. One op-
tion would be to use the ground truth translations
for building the memory. However, this may re-
sult in inferior training, since at the test time, the
decoder iteratively updates the translation of sen-
tences based on the noisy translations of other sen-
tences (accessed via the target memory). Hence,
while training the document NMT model, we con-
struct the target memory from the translations gen-
erated by the pre-trained sentence-level model7.
This effectively exposes the model to its potential
test-time mistakes during the training time, result-
ing in more robust learned parameters.

5.1 Main Results

We have three variants of our model, using: (i)
only the source memory (S-NMT+src mem), (ii)
only the target memory (S-NMT+trg mem), or

5In our initial experiments, we found SGD to be more ef-
fective than Adam/Adagrad; an observation also made by Ba-
har et al. (2017).

6For the document NMT model training, we did some pre-
liminary experiments using different learning rates and used
the scheme which converged to the best perplexity in the least
number of epochs while for sentence-level training we follow
Cohn et al. (2016).

7We report results for two-pass decoding, i.e., we only
update the translations once using the initial translations gen-
erated from the base model. We tried multiple passes of de-
coding at test-time but it was not helpful.



1280

Memory-to-Context Memory-to-Output
BLEU METEOR BLEU METEOR

Fr→En De→En Et→En Fr→En De→En Et→En Fr→En De→En Et→En Fr→En De→En Et→En
NC-11 NC-16 NC-11 NC-16 NC-11 NC-16 NC-11 NC-16

S-NMT 20.85 5.24 9.18 20.42 23.27 10.90 14.35 24.65 20.85 5.24 9.18 20.42 23.27 10.90 14.35 24.65
+src 21.91† 6.26† 10.20† 22.10† 24.04† 11.52† 15.45† 25.92† 21.80† 6.10† 9.98† 21.50† 23.99† 11.53† 15.29† 25.44†

+trg 21.74† 6.24† 9.97† 21.94† 23.98† 11.58† 15.32† 25.89† 21.76† 6.31† 10.04† 21.82† 24.06† 12.10† 15.75† 25.93†

+both 22.00† 6.57† 10.54† 22.32† 24.40† 12.24† 16.18† 26.34† 21.77† 6.20† 10.23† 22.20† 24.27† 11.84† 15.82† 26.10†

Table 2: BLEU and METEOR scores for the sentence-level baseline (S-NMT) vs. variants of our Docu-
ment NMT model. bold: Best performance, †: Statistically significantly better than the baseline.

Memory-to-Context Memory-to-Output
Lang. Pair Fr→En De→En Et→En Fr→En De→En Et→En
S-NMT 42.5 66.8 58.4 42.5 66.8 58.5
+src mem 48.8 73.1 64.8 68.7 107.1 88.7
+trg mem 43.8 68.1 59.8 53.8 85.1 71.8
+both mems 50.1 74.4 66.1 80 125.4 102

Table 3: Number of model parameters (millions).

(iii) both the source and target memories (S-
NMT+both mems). We compare these variants
against the standard sentence-level NMT model
(S-NMT). We also compare the source memory
variants of our model to the local context-NMT
models8 of Jean et al. (2017) and Wang et al.
(2017), which use a few previous source sentences
as context, added to the decoder hidden state (sim-
ilar to our Memory-to-Context model).

Memory-to-Context We consistently observe
+1.15/+1.13 BLEU/METEOR score improve-
ments across the three language pairs upon com-
paring our best model to S-NMT (see Table 2).
Overall, our document NMT model with both
memories has been the most effective variant for
all of the three language pairs.

We further experiment to train the target mem-
ory variants using gold translations instead of
the generated ones for German-English. This
led to −0.16 and −0.25 decrease9 in the BLEU
scores for the target-only and both-memory vari-
ants, which confirms the intuition of constructing
the target memory by exposing the model to its
noises during training time.

Memory-to-Output From Table 2, we consis-
tently see +.95/+1.00 BLEU/METEOR improve-
ments between the best variants of our model and
the sentence-level baseline across the three lan-

8We implemented and trained the baseline local context
models using the same hyperparameters and training proce-
dure that we used for training our memory models.

9Latter is statistically significant decrease w.r.t. the both
memory model trained on generated target translations.

Smaller Corpus Larger Corpus
10

12

14

10.9

12.12

11.52

12.94

11.58

12.55
12.24

13.56

M
E

T
E

O
R

S-NMT S-NMT+src S-NMT+trg S-NMT+both

(a) Memory-to-Context model

Smaller Corpus Larger Corpus
10

12

14

10.9

12.12

11.53

12.48

12.1

13.21

11.84

12.99

M
E

T
E

O
R

S-NMT S-NMT+src S-NMT+trg S-NMT+both

(b) Memory-to-Output model

Figure 3: METEOR scores on De→En (NC-11)
while training S-NMT with smaller vs. larger cor-
pus.

guage pairs. For French→English, all variants of
document NMT model show comparable perfor-
mance when using BLEU; however, when eval-
uated using METEOR, the dual memory model
is the best. For German→English, the target
memory variants give comparable results, whereas
for Estonian→English, the dual memory variant
proves to be the best. Overall, the Memory-to-
Context model variants perform better than their
Memory-to-Output counterparts. We attribute this
to the large number of parameters in the latter ar-
chitecture (Table 3) and limited amount of data.

We further experiment with more data for train-



1281

BLEU METEOR
Fr→En De→En Et→EnFr→En De→En Et→En

NC-11 NC-16 NC-11 NC-16

Jean et al. (2017) 21.95 6.04 10.26 21.67 24.10 11.61 15.56 25.77
Wang et al. (2017) 21.87 5.49 10.14 22.06 24.13 11.05 15.20 26.00
S-NMT 20.85 5.24 9.18 20.42 23.27 10.90 14.35 24.65
+src mem 21.91† 6.26♣ 10.20 22.10♠ 24.04† 11.52♣15.45♣ 25.92♠

+both mems 22.00† 6.57♦10.54♣ 22.32♦ 24.40♦ 12.24♦16.18♦ 26.34♦

Table 4: Our Memory-to-Context Source Memory NMT
variants vs. S-NMT and Source context NMT baselines.
bold: Best performance, †, ♠, ♣, ♦: Statistically signifi-
cantly better than only S-NMT, S-NMT & Jean et al. (2017),
S-NMT & Wang et al. (2017), all baselines, respectively.

BLEU-1
Fr→En De→En Et→En

NC-11NC-16

Jean et al. (2017) 52.8 30.6 39.2 51.9
Wang et al. (2017) 52.6 28.2 38.3 52.3
S-NMT 51.4 28.7 36.9 50.4

+src mem 53.0 30.5 39.1 52.6
+both mems 53.5 33.1 41.3 53.2

Table 5: Unigram BLEU for our
Memory-to-Context Document NMT
models vs. S-NMT and Source con-
text NMT baselines. bold: Best per-
formance.

ing the sentence-based NMT to investigate the ex-
tent to which document context is useful in this
setting. We randomly choose an additional 300K
German-English sentence pairs from WMT’14
data to train the base NMT model in stage 1. In
stage 2, we use the same document corpus as be-
fore to train the document-level models. As seen
from Figure 3, the document MT variants still
benefit from the document context even when the
base model is trained on a larger bilingual corpus.
For the Memory-to-Context model, we see mas-
sive improvements of +0.72 and +1.44 METEOR
scores for the source memory and dual memory
model respectively, when compared to the base-
line. On the other hand, for the Memory-to-Output
model, the target memory model’s METEOR
score increases significantly by +1.09 compared
to the baseline, slightly differing from the corre-
sponding model using the smaller corpus (+1.2).

Local Source Context Models Table 4 shows
comparison of our Memory-to-Context model
variants to local source context-NMT mod-
els (Jean et al., 2017; Wang et al., 2017).
For French→English, our source memory
model is comparable to both baselines. For
German→English, our S-NMT+src mem model is
comparable to Jean et al. (2017) but outperforms
Wang et al. (2017) for one test set according
to BLEU, and for both test sets according to
METEOR. For Estonian→English, our model
outperforms Jean et al. (2017). Our global source
context model has only surface-level sentence in-
formation, and is oblivious to the individual words
in the context since we do an offline training to
get the sentence representations (as previously
mentioned). However, the other two context
baselines have access to that information, yet our

model’s performance is either better or quite close
to those models. We also look into the unigram
BLEU scores to see how much our global source
memory variants lead to improvement at the
word-level. From Table 5, it can be seen that our
model’s performance is better than the baselines
for majority of the cases. The S-NMT+both mems
model gives the best results for all three language
pairs, showing that leveraging both source and
target document context is indeed beneficial for
improving MT performance.

5.2 Analysis

Using Global/Local Target Context We first
investigate whether using a local target context
would have been equally sufficient in comparison
to our global target memory model for the three
datasets. We condition the decoder on the previ-
ous target sentence representation (obtained from
the last hidden state of the decoder) by adding it as
an additional input to all decoder states (PrevTrg)
similar to our Memory-to-Context model. From
Table 6, we observe that for French→English and
Estonian→English, using all sentences in the tar-
get context or just the previous target sentence
gives comparable results. We may attribute this
to these specific datasets, that is documents from
TED talks or European Parliament Proceedings
may depend more on the local than on the global
context. However, for German→English (NC-11),
the target memory model performs the best show-

BLEU METEOR
Lang. Pair Fr→En De→En Et→En Fr→En De→En Et→En
S-NMT 20.85 5.24 20.42 23.27 10.90 24.65

+prev trg 21.75 5.93 22.08 24.03 11.40 25.94
+trg mem 21.74 6.24 21.94 23.98 11.58 25.89

Table 6: Analysis of target context model.



1282

ing that for documents with richer context (e.g.
news articles) we do need the global target doc-
ument context to improve MT performance.

Output Analysis To better understand the dual
memory model, we look at the first sentence exam-
ple in Table 7. It can be seen that the source sen-
tence has the noun “Qimonda” but the sentence-
level NMT model fails to attend to it when gener-
ating the translation. On the other hand, the single
memory models are better in delivering some, if
not all, of the underlying information in the source
sentence but the dual memory model’s transla-
tion quality surpasses them. This is because the
word “Qimonda” was being repeated in this spe-
cific document, providing a strong contextual sig-
nal to our global document context model while
the local context model by Wang et al. (2017) is
still unable to correctly translate the noun even
when it has access to the word-level information
of previous sentences.

We resort to manual evaluation as there is no
standard metric which evaluates document-level
discourse information like consistency or pronom-
inal anaphora. By manual inspection, we observe
that our models can identify nouns in the source
sentence to resolve coreferent pronouns, as shown
in the second example of Table 7. Here the topic
of the sentence is “the country under the dictator-
ship of Lukashenko” and our target and dual mem-
ory models are able to generate the appropriate
pronoun/determiner as well as accurately translate
the word ‘diktatuur’, hence producing much better
translation as compared to both baselines. Apart
from these improvements, our models are better in
improving the readability of sentences by gener-
ating more context appropriate grammatical struc-
tures such as verbs and adverbs.

Furthermore, to validate that our model im-
proves the consistency of translations, we look
at five documents (roughly 70 sentences) from
the test set of Estonian-English, each of which
had a word being repeated in the gold translation.
Our model is able to resolve the consistency in
22 out of 32 cases as compared to the sentence-
based model which only accurately translates 16
of those. Following Wang et al. (2017), we also
investigate the extent to which our model can cor-
rect errors made by the baseline system. We ran-
domly choose five documents from the test set.
Out of the 20 words/phrases which were incor-
rectly translated by the sentence-based model, our

model corrects 85% of them while also generating
10% new errors.

Source qimonda täidab lissaboni strateegia eesmärke.
Target qimonda meets the objectives of the lisbon strategy.
S-NMT <UNK> is the objectives of the lisbon strategy.
+Src Mem the millennium development goals are fulfilling the

millennium goals of the lisbon strategy.
+Trg Mem in writing. - (ro) the lisbon strategy is fulfilling the

objectives of the lisbon strategy.
+Both Mems qimonda fulfils the aims of the lisbon strategy.
Wang et al. (2017) <UNK> fulfils the objectives of the lisbon strategy.
Source ... et riigis kehtib endiselt lukašenka diktatuur,

mis rikub inim- ning etnilise vähemuse õigusi.
Target ... this country is still under the dictatorship of

lukashenko, breaching human rights and the rights
of ethnic minorities.

S-NMT ... the country still remains in a position of lukashenko
to violate human rights and ethnic minorities.

+Src Mem ... the country still applies to the brutal dictatorship of
human and ethnic minority rights.

+Trg Mem ... the country still keeps the <UNK> dictatorship that
violates human rights and ethnic rights.

+Both Mems ... the country still persists in lukashenko’s dictatorship
that violate human rights and ethnic minority rights.

Wang et al. (2017) ... there is still a regime in the country that is
violating the rights of human and ethnic minority
in the country.

Table 7: Example Et→En sentence translations
(Memory-to-Context) from two test documents.

6 Related Work

Document-level Statistical MT There have
been a few SMT-based attempts to document MT,
but they are either restrictive or do not lead to sig-
nificant improvements. Hardmeier and Federico
(2010) identify links among words in the source
document using a word-dependency model to im-
prove translation of anaphoric pronouns. Gong
et al. (2011) make use of a cache-based sys-
tem to save relevant information from the previ-
ously generated translations and use that to en-
hance document-level translation. Garcia et al.
(2014) propose a two-pass approach to improve
the translations already obtained by a sentence-
level model.

Docent is an SMT-based document-level de-
coder (Hardmeier et al., 2012, 2013), which tries
to modify the initial translation generated by
the Moses decoder (Koehn et al., 2007) through
stochastic local search and hill-climbing. Garcia
et al. (2015) make use of neural-based continuous
word representations to incorporate distributional
semantics into Docent. In another work, Garcia
et al. (2017) incorporate new word embedding fea-
tures into Docent to improve the lexical consis-
tency of translations. The proposed methods fail
to yield improvements upon automatic evaluation.

Larger Context Neural MT Jean et al. (2017)



1283

extend the vanilla attention-based neural MT
model (Bahdanau et al., 2015) by conditioning
the decoder on the previous sentence via atten-
tion over its words. Extending their model to con-
sider the global source document context would
be challenging due to the large size of computa-
tion graph over all the words in the source docu-
ment. Wang et al. (2017) employ a 2-level hier-
arichal RNN to summarise three previous source
sentences, which is then used as an additional in-
put to the decoder hidden state. Bawden et al.
(2017) use multi-encoder NMT models to exploit
context from the previous source and target sen-
tence. They highlight the importance of target-
side context but report deteriorated BLEU scores
when using it. All these works consider a very
local source/target context and completely ignore
the global source and target document contexts.

7 Conclusion

We have proposed a document-level neural MT
model that captures global source and target doc-
ument context. Our model augments the vanilla
sentence-based NMT model with external memo-
ries to incorporate documental interdependencies
on both source and target sides. We show statis-
tically significant improvements of the translation
quality on three language pairs. For future work,
we intend to investigate models which incorporate
specific discourse-level phenomena.

Acknowledgments

The authors are grateful to André Martins and the
anonymous reviewers for their helpful comments
and corrections. This work was supported by
the Multi-modal Australian ScienceS Imaging and
Visualisation Environment (MASSIVE) (www.
massive.org.au), and partially supported by
a Google Faculty Award to GH and the Australian
Research Council through DP160102686.

References
Parnia Bahar, Tamer Alkhouli, Jan-Thorsten Peter,

Christopher Jan-Steffen Brix, and Hermann Ney.
2017. Empirical investigation of optimization algo-
rithms in neural machine translation. In Conference
of the European Association for Machine Transla-
tion, pages 13–26, Prague, Czech Republic.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of

the International Conference on Learning Represen-
tations.

Rachel Bawden, Rico Sennrich, Alexandra Birch,
and Barry Haddow. 2017. Evaluating discourse
phenomena in neural machine translation. In
arXiv:1711.00513.

Julian Besag. 1975. Statistical analysis of non-lattice
data. Journal of the Royal Statistical Society. Series
D (The Statistician), 24(3):179–195.

Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation, pages 261–268.

Kyunghyun Cho, B van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation (SSST-8).

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (Short
Papers), pages 176–181. Association for Computa-
tional Linguistics.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016. Incorporating structural alignment
biases into an attentional neural translation model.
In Proceedings of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 876–885. Asso-
ciation for Computational Linguistics.

Eva Martı́nez Garcia, Carles Creus, Cristina España-
Bonet, and Lluı́s Màrquez. 2017. Using word em-
beddings to enforce document-level lexical consis-
tency in machine translation. The Prague Bulletin
of Mathematical Linguistics, 108:85–96.

Eva Martı́nez Garcia, Cristina España-Bonet, and Lluı́s
Màrquez. 2014. Document-level machine transla-
tion as a re-translation process. Procesamiento del
Lenguaje Natural, 53:103–110.

Eva Martı́nez Garcia, Cristina España-Bonet, and Lluı́s
Màrquez. 2015. Document-level machine transla-
tion with word vector models. In Proceedings of
the18th Conference of the European Association for
Machine Translation, pages 59–66.

Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 909–919. Association for Computa-
tional Linguistics.

www.massive.org.au
www.massive.org.au
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/N16-1102
http://www.aclweb.org/anthology/N16-1102
http://dl.acm.org/citation.cfm?id=2145432.2145532
http://dl.acm.org/citation.cfm?id=2145432.2145532


1284

Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In International Workshop on
Spoken Language Translation, pages 283–289.

Christian Hardmeier, Joakim Nivre, and Jörg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179–1190. Association for Computational Linguis-
tics.

Christian Hardmeier, Sara Stymne, Jörg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine trans-
lation. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstrations, pages 193–198. Asso-
ciation for Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Sebastien Jean, Stanislas Lauly, Orhan Firat, and
Kyunghyun Cho. 2017. Does neural machine
translation benefit from larger context? In
arXiv:1704.05135.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the 10th Machine Translation Summit,
pages 79–86. AAMT.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ’07, pages 228–231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421. Asso-
ciation for Computational Linguistics.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel

Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311–318. Association
for Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1715–1725.

Jason R. Smith, Herve Saint-Amand, Chris Callison-
Burch, Magdalena Plamada, and Adam Lopez.
2013. Dirt cheap web-scale parallel text from the
common crawl. In Proceedings of the Conference of
the Association for Computational Linguistics.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems, pages 2440–2448. MIT Press.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems, pages 3104–3112. MIT Press.

Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2017. Exploiting cross-sentence context for
neural machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2816–2821. Association
for Computational Linguistics.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2015. Memory networks. In Proceedings of the
International Conference on Learning Representa-
tions.

http://www.aclweb.org/anthology/D12-1108
http://www.aclweb.org/anthology/D12-1108
http://www.aclweb.org/anthology/P13-4033
http://www.aclweb.org/anthology/P13-4033
http://www.aclweb.org/anthology/P13-4033
http://www.aclweb.org/anthology/P07-2045
http://www.aclweb.org/anthology/P07-2045
http://dl.acm.org/citation.cfm?id=1626355.1626389
http://dl.acm.org/citation.cfm?id=1626355.1626389
http://dl.acm.org/citation.cfm?id=1626355.1626389
http://aclweb.org/anthology/D15-1166
http://aclweb.org/anthology/D15-1166
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
http://www.aclweb.org/anthology/P16-1162
http://www.aclweb.org/anthology/P16-1162
http://aclweb.org/anthology//P/P13/P13-1135.pdf
http://aclweb.org/anthology//P/P13/P13-1135.pdf
http://aclweb.org/anthology/D17-1300
http://aclweb.org/anthology/D17-1300

