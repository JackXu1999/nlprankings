




































Vector of Locally Aggregated Embeddings for Text Representation


Proceedings of NAACL-HLT 2019, pages 1408–1414
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1408

Vector of Locally Aggregated Embeddings for Text Representation

Hadi Amiria and Mitra Mohtaramib
aHarvard, Department of Biomedical Informatics, Boston, MA, USA

bMIT, Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA
hadi@hms.harvard.edu, mitra@csail.mit.edu

Abstract
We present Vector of Locally Aggregated
Embeddings (VLAE) for effective and, ulti-
mately, lossless representation of textual con-
tent. Our model encodes each input text by
effectively identifying and integrating the rep-
resentations of its semantically-relevant parts.
The proposed model generates high quality
representation of textual content and improves
the classification performance of current state-
of-the-art deep averaging networks across sev-
eral text classification tasks.

1 Introduction

Representation learning algorithms can reveal in-
trinsic low-dimensional structure in data (Rumel-
hart et al., 1986; Bengio et al., 2013; LeCun et al.,
2015). In particular, deep averaging networks
(DANs) are effective for text classification (Shen
et al., 2018; Arora et al., 2017; Wieting et al.,
2016; Iyyer et al., 2015). They achieve their
improvement through use of word embeddings,
weighted averaging, and deepening networks. The
above works show that DANs can outperform
RNNs and CNNs in text classification while tak-
ing only a fraction of their training time.

In this work, with a special focus on DANs,
we study the effect of information loss associated
with average word embeddings and develop al-
gorithms that are robust against information loss
for text representation. We show that divergence
of word embeddings from their average can be
considered as a good proxy to quantify informa-
tion loss; in particular, longer documents suf-
fer from significant information loss when repre-
sented by average word embeddings. These results
inspire our work to develop a novel representa-
tion learning approach based on Vector of Locally
Aggregated Descriptors (VLAD) (Jégou et al.,
2010; Arandjelovic and Zisserman, 2013)–an ef-
fective approach to integrate image descriptors for

large scale image datasets. Our model identifies
semantically-relevant parts of documents and lo-
cally integrates their representations through clus-
tering and autoencoding. In contrast to averaging,
our model prevents larger semantically-relevant
parts of inputs to dominate final representations. It
improves DANs by 5.30 macro-F1 points in clas-
sifying longer texts and show comparable perfor-
mance to them on shorter text.

2 Preliminary Analysis

How can information loss be quantified when
word embeddings are averaged? How important
it is to address information loss when represent-
ing textual content? Are representation learn-
ing algorithms robust against information loss?
We conduct experiments to answer these ques-
tions with respect to deep averaging network
(DANs). Our study can inspire works in more
complex averaging approaches such as those re-
ported in (Torabi Asr et al., 2018; Kiela et al.,
2015) as well as recent works on unsupervised se-
mantic similarity (Pagliardini et al., 2018). We use
the DAN developed in (Joulin et al., 2017) and
several datasets containing short and long docu-
ments to answer these questions.

2.1 Quantifying Information Loss
Let’s assume a d-dimensional word embedding
space. We quantify the amount of information loss
in the average word embedding vector of a given
document S ∈ Rn×d by computing the average
divergence (or distance) between its word embed-
dings, wi ∈ Rd ∀i ∈ {1 . . . n}, and their average
vector, s = 1/n

∑
iwi, s ∈ Rd, as follows:

divergence =
1

n

∑
i

(1− cosine(s,wi)). (1)

Figures 1(a)–1(c) show strong positive corre-
lation between divergence and document length



1409

IMDb Reddit Twitter

0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70
divergence from average embedding

0

50

100

150

200

250

av
er

ag
e 

do
cu

m
en

t 
le

ng
th

document_length trend_line

(a) divergence vs. length

0.48 0.50 0.52 0.54 0.56 0.58 0.60
divergence from average embedding

0

25

50

75

100

125

av
er

ag
e 

do
cu

m
en

t 
le

ng
th

document_length trend_line

(b) divergence vs. length

0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55
divergence from average embedding

0

2

4

6

8

10

av
er

ag
e 

do
cu

m
en

t 
le

ng
th

document_length trend_line

(c) divergence vs. length

0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70
divergence from average embedding

0.75

0.80

0.85

0.90

F1
-m

ac
ro

F1-macro trend_line

(d) divergence vs. F1

0.48 0.50 0.52 0.54 0.56 0.58 0.60
divergence from average embedding

0.4
0.5
0.6
0.7
0.8
0.9
1.0

F1
-m

ac
ro

F1-macro trend_line

(e) divergence vs. F1

0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55
divergence from average embedding

0.6

0.7

0.8

0.9

F1
-m

ac
ro

F1-macro trend_line

(f) divergence vs. F1

Figure 1: Quantification of information loss associated with average word embeddings. Divergence indicates
the average distance between individual word embeddings (of size d = 300) and their average embedding, see
Equation (1). (a-c): show strong positive correlation between divergence and average document length (number
of words in a document) across datasets. (d-f): show macro F1 performance of the deep averaging network
developed in (Joulin et al., 2017) across datasets: performance considerably drops for higher values of information
loss/divergence, e.g. divergence values above 0.55. Note that, we sort and bin instances based on their divergence
values and report average length (a-c) and macro-F1 (d-f) for each bin.

across long and short text datasets. Given these
results and if we assume longer documents should
suffer from greater information loss if represented
by average word embeddings, divergence from
mean can be a good proxy to quantify information
loss associated with average embeddings.

2.2 Effect of Information Loss

As Figures 1(d) and 1(e) show, DAN’s macro-F1
classification performance considerably decreases
as divergence (or text length) increases for IMDb
and Reddit datasets; note that we sort and bin in-
stances based on their divergence values and re-
port average macro-F1 for each bin. In partic-
ular, as the trend lines in Figures 1(d) and 1(e)
show, the average macro-F1 performance drops
from 0.86 and 0.82 on shorter IMDb and Red-
dit posts to 0.82 and 0.71 on their longer posts
respectively. In addition, the result on Twitter
dataset, Figure 1(f), shows that DANs are robust
against small information loss, i.e. small diver-
gence values below 0.55 do not inversely affect
macro-F1 performance. This result is also ob-
served on the other two datasets, see macro-F1
performance for small divergence values (≤ 0.55)
in Figures 1(d) and 1(e).

The above experiments show that (a): signifi-
cant information loss can occur when word em-
beddings are averaged, in particular, when repre-
senting longer documents, and (b) such informa-
tion loss can inversely affect the performance of
downstream classifiers like DANs on longer texts.
In this paper, we develop an effective representa-
tion learning model to tackle this problem.

3 Method

We propose to utilize semantically-relevant parts
of inputs to tackle information loss associated
with average word embeddings. Assuming that
semantically-relevant words are closer to each
other in semantic space (constructed over a global
vocabulary), we expect divergence between words
in semantically-relevant parts of inputs (i.e. infor-
mation loss associated to their average word em-
bedding) to be very small. Thus, as Figure 2 il-
lustrates, we propose to cluster the semantic space
to first identify semantically-relevant parts of in-
puts over a global vocabulary; we then effectively
integrate these parts to represent documents.

Let’s assume a global vocabulary V in which
words are represented in a d-dimensional space,
w ∈ Rd. As Figure 2 shows, we first cluster this



1410

semantic space into k clusters through the follow-
ing objective function over V:

min
∑
w∈V
||f(C,w)−w||2, (2)

where C is the set of k cluster centers, |C| = k, and
f(C,w) returns the nearest cluster center c ∈ C to
the embedding vector w based on cosine simi-
larity among embeddings or Euclidean distance in
case of K-Means.1

Given a document S ∈ Rn×d with an arbitrary
number of n ≥ 1 words, and the above k cluster
centers, we compute the representation of the doc-
ument in each cluster ci, i = 1 . . . k as follows:

ai =
1

zi

∑
j:f(C,wj)=ci

wj (3)

zi = |j : f(C,wj) = ci|,

where ai ∈ Rd indicates the representation of
the document at cluster ci and is obtained by tak-
ing the average embedding of words of the doc-
ument that have been assigned to cluster ci ac-
cording to Equation (2), and zi is the number of
such words in cluster ci. To this end, each doc-
ument can be represented by A ∈ Rd×k which
is obtained by concatenating its cluster-level rep-
resentations. Note that we didn’t observe any
performance difference between the above aver-
aging process versus computing residuals (differ-
ences between word embeddings and correspond-
ing cluster centroids) which is commonly used
to represent cluster-level image descriptors (Jégou
et al., 2010; Arandjelovic and Zisserman, 2013) in
image processing.

Since As are of fixed length, they can be read-
ily used as features in traditional classification and
clustering algorithms. However, they can cause ef-
ficiency issues because of their large size (d× k);
note that the typical value for embedding dimen-
sion d is 300 (Pennington et al., 2014; Mikolov
et al., 2013). To tackle this issue, we further in-
tegrate cluster-level representations, at the cost of
some further information loss, to create represen-
tations of lower dimension for inputs.

In particular, given all input documents with k
cluster-level representations A ∈ Rd×k for each
document, we develop an autoencoder with one

1This problem can be solved through gradient descent
seeded with an initial set of k examples drawn uniformly at
random from V (Bottou and Bengio, 1995; Sculley, 2010).

Figure 2: Illustration of VLAEs: squares show word
embeddings of a hypothetical document and crosses
show their corresponding average. (a): shows average
word embedding in center - the case of high informa-
tion loss or divergence from mean, (b): shows aver-
age word embeddings in corresponding word clusters;
these embeddings are used to produce VLAEs.

hidden layer that integrates these cluster-level rep-
resentations to create a final representation for
each document, vector a ∈ Rd×m where m is the
dimensionality reduction parameter and m × d is
length of the representation (final layer of the en-
coder) and is smaller than d×k for m < k. Train-
ing a single-layer autoencoder corresponds to op-
timizing the learning parameters to minimize the
overall loss between inputs and their reconstruc-
tions. For real-valued A, squared loss is often
used (Vincent et al., 2010), i.e. l(A) = ||A−Â||2
where Â ∈ Rd×k is reconstruction of A and gen-
erated by the decoder from a. Our intuition is that
if a leads to a good reconstruction of A, it has re-
tained all information available in the input.

We refer to a ∈ Rd×m as the Vector of Locally
Aggregated Embeddings (VLAE). We expect this
final representation to be robust against informa-
tion loss due to its cluster-level local aggregation
which prevents larger portions of semantically-
similar words to dominate the representation.

4 Experiments

Data: We investigate VLAEs in three binary
classification tasks: sentiment classification on
IMDb (Maas et al., 2011), disease-text classifi-
cation on Reddit, where the task is to classify
reddit posts as relevant or irrelevant to specific
diseases, and churn prediction on Twitter (Amiri
and Daumé III, 2015), where the task is to clas-
sify/predict if given tweets indicate user intention
about leaving brands, e.g. the tweet “my days with
BRAND are numbered” is a churny tweet. See de-
tails in Table 1. For pre-processing, we change all
texts to lowercase, and remove stop words, user
names, and URLs from texts.



1411

Train Val Test Unlabeled
IMDb 40K 5K 5K 50K
Reddit 2K 1K 1K 100K
Twitter 3K 1K 1K 100K

Table 1: Statistics of dataset used in experiments.

Settings: We use validation data for hyperpa-
rameter tuning and model selection. We use 300-
dimensional word embeddings (d = 300) pro-
vided by Google (Mikolov et al., 2013), and for
greater number of ds, we train word2vec on un-
labeled data, see Table 1. In addition, we set
the dimensionality reduction parameter m from
{1 . . . 4} using validation data. The best value
of m is the same across tasks/datasets, m = 2.
Furthermore, we determine the number of clus-
ters k for VLAEs by choosing the optimal k from
{2i, i = {1 . . . 7}} using validation data of each
dataset. We learn optimal k with respect to task,
but not embedding space, due to significant den-
sity of the semantic space of word embeddings,
see Note on Clustering Word Embeddings.

Baselines: We consider two versions of DANs
as baselines: Avg small and Avg large
which represent documents by average word em-
bedding of size d = 300 and d = m× 300 respec-
tively. Note that, for fair comparison, Avg large
has the exact same size as our model (VLAE);
however, depending on m, their network size is
1.3-1.6 times greater than that of Avg small due
to difference in input dimensions. We use 3 hid-
den layers of size 300 for above networks. Also, to
directly evaluate the effect of averaging, we do not
adjust initial word embeddings during training.

Experimental Results: Table 2 shows the per-
formance of different models across datasets. The
results show that VLAE significantly outperforms
Avg small and Avg large by 2.6 and 7.2
points in Macro-F1 on IMDb. The correspond-
ing values on Reddit dataset are 6.7 and 3.4 points
respectively. We believe these improvements are
due to more effective and lossless representation
of inputs. We note that Avg large performs
worse than Avg small on IMDb. This could be
attributed to the size of training data which may
not be enough to train Avg large, or to lower
quality of input representations in Avg large
compared to Avg small in case of IMDb. Note
that although VLAE has the same number of pa-
rameters as Avg large, it uses autoencoding to
effectively filter redundant information. Verify-

Avg small Avg large VLAE
IMDb 83.11 78.52 85.72*
Reddit 59.42 62.72 66.10*
Twitter 61.42 73.08* 72.62
AVG 67.98 71.44 74.81

Table 2: Macro-F1 performance of different models
across datasets. Asterisk mark (*) indicates signifi-
cant difference between top two systems. VLAE out-
performs other models on longer texts, and show com-
parable performance to Avg large on shorter text.

ing these hypotheses will be the subject of fu-
ture work. In addition, VLAE show lower perfor-
mance than Avg large on Twitter dataset, F1 of
72.62 versus 73.08. We attribute this result to the
shorter length of tweets for which, as we experi-
mentally showed before, averaging does not cause
major divergence in representations. On average,
VLAE improves Avg small and Avg large by
4.7 and 5.3 F1 points on IMDb and Reddit (longer
texts) respectively. It also shows comparable per-
formance to best performing model on Twitter
(shorter texts).

We also compare models in terms of the quality
of their representations. For this comparison, we
ignore input preparation time and assume a model
that generates better representations should con-
verge faster than other models; note that the over-
all turnaround time of VLAE is greater than that of
Avg small or Avg large because of its input
preparation time which we ignore for the purpose
of this experiment. The result show that VLAE
leads to 7.5, 1.3, and 1.3 times faster convergence
than Avg small and 14.9, 2.6, and 1.8 times
faster convergence than Avg large on IMDb,
Reddit, and Twitter datasets respectively. Consid-
ering the size of these networks, these results indi-
cate that representations obtained from VLAE are
much better than those of its counterparts.

Note on Clustering Word Embeddings: In ex-
periments, we observe clusters obtained from
word embeddings are often very dense. This is a
challenge for our model because with small num-
ber of clusters (ks) potentially dissimilar words
can appear in the same cluster, while with large
ks semantically-similar words may appear in dif-
ferent clusters. Neither of these are desired.

To illustrate the above challenge, we report Sil-
houette Coefficient (SC) (Rousseeuw, 1987) of k-
means with different number of clusters obtained
from words embeddings across datasets. SC in-
dicates how well cluster boundaries are detected



1412

2 4 8 16 32 64 128
#cluster

0.000

0.025

0.050

0.075

0.100

0.125

0.150

Si
lh

ou
et

te
 C

oe
ff

ic
ie

nt

IMDB Reddit

Figure 3: Mean Silhouette Coefficient computed for
different number of clusters; a higher Silhouette Co-
efficient score indicates better defined clusters.

by a clustering model. It is calculated using the
mean intra-cluster distance and the mean nearest-
cluster distance for each sample. Specifically, the
mean distance between each embedding and all
other embeddings in the same cluster (mc), and
the mean distance between the embedding and all
other embeddings in the next nearest cluster (the
nearest cluster that the embedding is not part of)
(mn) are used to measure SC for the embedding:

mn−mc
max(mn,mc)

.

The best and worst SC scores are 1 and −1 which
indicate ideal and worst clustering respectively.
Also, values near 0 indicate overlapping clusters.

Figure 3 shows the mean SCs computed over all
word embeddings for IMDb and Reddit datasets.2

The results show that (a): the best number of clus-
ters is k = 2 on both datasets, and (b): Silhou-
ette Coefficient scores generally home in on val-
ues close to zero as the number of clusters in-
creases. These results show significant density of
embeddings in semantic space. Therefore, we op-
timize the number of clusters for creating VLAEs
by resorting to validation data and measuring task-
specific performance. From these results, we
conclude that a hierarchical clustering approach
that recursively combines pairs of semantically-
similar clusters could help better defining these
clusters and perhaps improve the performance of
our model.

5 Related Work

Deep averaging networks (DANs) (Joulin et al.,
2017; Iyyer et al., 2015; Arora et al., 2017; Shen

2The corresponding values for Twitter data are [0.52,
0.48, 0.42, 0.31, 0.21, 0.13, 0.09] respectively. We have not
reported these values in Figure 3 to better illustrate the scores
for other datasets.

et al., 2018) were developed based on the suc-
cesses of vector operations in embedding space.
In contrast to their simplicity, DANs showed high
performance in text classification tasks.

Arora et al. (2017) showed that sentences can
be effectively represented by the weighted average
of their word embeddings modified by PCA/SVD.
In addition, the DANs developed in (Iyyer et al.,
2015), (Joulin et al., 2017), and (Shen et al., 2018)
were feed-forward networks that used average
word embeddings to represent inputs; they were
effective for several NLP tasks such as document
categorization, text pair similarity, and short sen-
tence classification. Furthermore, feed-forward
architectures like DANs have been used for lan-
guage modeling (Bengio et al., 2003) and greedy
transition-based dependency parsing (Chen and
Manning, 2014) with fast turnaround time.

In addition, previous research investigated a va-
riety of vector operations that could replace the
averaging operation used in the DANs. Many of
these operations have been studied in (Mitchell
and Lapata, 2008) for modeling the composition-
ality of short phrases, or showing the utility of sim-
ple vector computations(Banea et al., 2014). The
operations in (Mitchell and Lapata, 2008) were
also extended to use syntactic relation between
words and grammar (Erk and Padó, 2008; Col-
lobert and Weston, 2008). Also, clustering seman-
tic space was studied in (Mekala et al., 2017) to
learn context information for words and for tasks
like topic coherence and information retrieval.

In this work, we built on previous work on
DANs and investigated and tackled information
loss associated with average word embeddings.

6 Conclusion and Future Work

We investigate information loss associated with
average word embeddings. We show that averag-
ing lead to significant information loss and pro-
pose to tackle the issue by identify semantically-
similar parts of documents through clustering
of semantic space at word-level and integrating
cluster-level representations through autoencod-
ing. A promising future direction is to use hierar-
chical clustering to create better cluster-level rep-
resentations.

Acknowledgments

We thank anonymous reviewers for their insightful
comments and constructive feedback.



1413

References
Hadi Amiri and Hal Daumé III. 2015. Target-

dependent churn classification in microblogs. In
Proceedings of the Twenty-Ninth AAAI Conference
on Artificial Intelligence, pages 2361–2367. AAAI
Press.

Relja Arandjelovic and Andrew Zisserman. 2013. All
about vlad. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition, pages
1578–1585.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In International conference on learning
representations (ICLR).

Carmen Banea, Di Chen, Rada Mihalcea, Claire
Cardie, and Janyce Wiebe. 2014. Simcompass: Us-
ing deep learning word embeddings to assess cross-
level similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval
2014), pages 560–565.

Yoshua Bengio, Aaron Courville, and Pierre Vincent.
2013. Representation learning: A review and new
perspectives. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 35(8).

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Leon Bottou and Yoshua Bengio. 1995. Convergence
properties of the k-means algorithms. In Advances
in neural information processing systems, pages
585–592.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 conference on
empirical methods in natural language processing
(EMNLP), pages 740–750.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Katrin Erk and Sebastian Padó. 2008. A structured vec-
tor space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897–
906. Association for Computational Linguistics.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of ACL-IJCNLP.

Hervé Jégou, Matthijs Douze, Cordelia Schmid, and
Patrick Pérez. 2010. Aggregating local descrip-
tors into a compact image representation. In CVPR

2010-23rd IEEE Conference on Computer Vision &
Pattern Recognition, pages 3304–3311. IEEE Com-
puter Society.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Pa-
pers, pages 427–431.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2044–2048. Association for Compu-
tational Linguistics.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. nature, 521(7553):436.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th annual meeting of the as-
sociation for computational linguistics: Human lan-
guage technologies-volume 1, pages 142–150. Asso-
ciation for Computational Linguistics.

Dheeraj Mekala, Vivek Gupta, Bhargavi Paranjape, and
Harish Karnick. 2017. Scdv : Sparse composite
document vectors using soft clustering over distri-
butional representations. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 659–669. Association for
Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. Proceedings of
ACL-08: HLT, pages 236–244.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
2018. Unsupervised learning of sentence embed-
dings using compositional n-gram features. In Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 528–540.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Peter J Rousseeuw. 1987. Silhouettes: a graphical aid
to the interpretation and validation of cluster anal-
ysis. Journal of computational and applied mathe-
matics, 20:53–65.



1414

DE Rumelhart, G Eoo Hinton, and RJ Williams. 1986.
Learning internal representations by error propaga-
tion. Parallel distributed processing: explorations
in the microstructure of cognition, 1.

David Sculley. 2010. Web-scale k-means clustering.
In Proceedings of the 19th international conference
on World wide web, pages 1177–1178. ACM.

Dinghan Shen, Guoyin Wang, Wenlin Wang, Mar-
tin Renqiang Min, Qinliang Su, Yizhe Zhang, Chun-
yuan Li, Ricardo Henao, and Lawrence Carin.
2018. Baseline needs more love: On simple word-
embedding-based models and associated pooling
mechanisms. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics, volume 1.

Fatemeh Torabi Asr, Robert Zinkov, and Michael
Jones. 2018. Querying word embeddings for sim-
ilarity and relatedness. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
675–684.

Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research, 11:3371–3408.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding words and
sentences via character n-grams. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1504–1515. Asso-
ciation for Computational Linguistics.


