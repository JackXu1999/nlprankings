



















































Improving Abstractive Document Summarization with Salient Information Modeling


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2132–2141
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2132

Improving Abstractive Document Summarization with
Salient Information Modeling

Yongjian You1,2, Weijia Jia2, 1*, Tianyi Liu1,2, Wenmian Yang1,2
1Department of Computer Science and Engineering, Shanghai Jiao Tong University

2State Key Lab of IoT for Smart City, CIS, University of Macau, Macao, SAR China
{youyongjian, jiawj, liutianyi, sdq11111}@sjtu.edu.cn

Abstract

Comprehensive document encoding and
salient information selection are two major
difficulties for generating summaries with
adequate salient information. To tackle the
above difficulties, we propose a Transformer-
based encoder-decoder framework with two
novel extensions for abstractive document
summarization. Specifically, (1) to encode
the documents comprehensively, we design a
focus-attention mechanism and incorporate
it into the encoder. This mechanism models
a Gaussian focal bias on attention scores
to enhance the perception of local context,
which contributes to producing salient and
informative summaries. (2) To distinguish
salient information precisely, we design an
independent saliency-selection network which
manages the information flow from encoder
to decoder. This network effectively reduces
the influences of secondary information on the
generated summaries. Experimental results
on the popular CNN/Daily Mail benchmark
demonstrate that our model outperforms other
state-of-the-art baselines on the ROUGE
metrics.

1 Introduction

Document summarization is a fundamental task
of natural language generation which con-
denses the given documents and generates flu-
ent summaries with salient information automat-
ically. Recent successes of neural sequence-to-
sequence (seq2seq) models (Luong et al., 2015;
Wu et al., 2016; Tu et al., 2016) enable the end-
to-end framework for natural language genera-
tion, which inspires the research on abstractive
summarization. Abstractive document summa-
rization employs an end-to-end language model
to encode a document into high-dimensional rep-
resentations and then decode the representations
into an abstractive summary. Though promis-

Documents:
a [duke student] has [admitted to hanging a noose made
of rope] from a tree near a student union , [university of-
ficials] said thursday . the prestigious private school did
n’t identify the student , citing federal privacy laws . in a
news release , it said the student was [no longer] on cam-
pus and [will face] student conduct [review] . the [student
was identified during an investigation] by campus police
and the office of student affairs and admitted to placing
the noose on the tree early wednesday , the university said
. ... at a forum held on the steps of duke chapel , close to
where [the noose was discovered at 2 a.m]. , hundreds of
people gathered . “ you came here for the reason that you
want to say with me , ‘ this is no duke we will accept . ...

Reference summary:
student is no longer on duke university campus and will
face disciplinary review .
school officials identified student during investigation
and the person admitted to hanging the noose , duke says
.
the noose , made of rope , was discovered on campus
about 2 a.m.

Table 1: Example of a document and its correspond-
ing reference summary. We consider the reference
summary contains all salient information and mark the
words or phrases appearing in the document in [red].

ing improvements have been achieved recently (Li
et al., 2018c; Kryściński et al., 2018), there are still
many problems are not studied well, such as the
incompetence of salient information modeling.

Modeling salient information contains the pro-
cedure of information representation and discrim-
ination. Generally, the most essential prerequisite
for a practical document summarization model is
that the generated summaries should contain ad-
equate salient information of the original docu-
ments. However, previous seq2seq models are still
incapable of achieving convincing performance,
which are restricted by the following two difficul-
ties.

The first difficulty lies in the procedure of en-
coding. Considering a document is a long se-
quence of multiple sentences, the semantics of



2133

each token in document contain the dependen-
cies with other distant tokens and its local con-
text information. They both contribute to produc-
ing high-quality summaries with adequate salient
information. The lack of long-term dependen-
cies among tokens often leads to generating in-
complete summaries (Li et al., 2018c). Unfortu-
nately, traditional seq2seq encoders (recurrent or
convolution based) are deficient in modeling de-
pendencies among distant segments (Bengio et al.,
1994; Li et al., 2018c). In recent years, the Trans-
former model (Vaswani et al., 2017) reveals re-
markable performance in many similar tasks (De-
vlin et al., 2018) due to exploiting long-term de-
pendencies, but recent studies point out this model
may overlook local context occasionally (Yang
et al., 2018). The absence of local context infor-
mation accounts for inadequate details of salient
information. Therefore, it is challenging to en-
code global information and local context com-
prehensively for each token in documents, which
requires the capability of capturing long-term de-
pendencies and local semantics at the same time.
The second difficulty is to distinguish salient in-
formation from long documents precisely. In the
example shown in Table 1, salient segments ac-
count for only a small part of the whole document,
which is laborious for naive seq2seq models to
distinguish important information from much sec-
ondary information. The summaries generated by
these models usually lose salient information of
original documents or even contain repetitions (Li
et al., 2018c).

In this paper, we propose the Extended
Transformer model for Abstractive Document
Summarization (ETADS) to tackle the above is-
sues. Specifically, we design a novel focus-
attention mechanism and saliency-selection net-
work equipped in the encoder and decoder respec-
tively: (1) To comprehensively encode the doc-
uments, we design a focus-attention mechanism,
where a learnable Gaussian focal bias is employed
as a regularization term on attention scores. This
focal bias implicitly aggregates attention on local
continuous scopes to emphasize the corresponding
part of document. (2) To distinguish salient infor-
mation in documents, we design an independent
saliency-selection network to manage the informa-
tion flow from encoder to decoder explicitly. The
saliency-selection network employs a gate mech-
anism to assign a salient score for each token in

source documents according to their encoded rep-
resentations. We consider the lower-score tokens
are relatively insignificant and reduce their likeli-
hood of appearing in final summaries. Finally, we
conduct extensive experiments on the CNN/Daily
Mail dataset which is prevailing and widely used
for document summarization task. The experi-
mental results show that ETADS achieves state-
of-the-art ROUGE scores and outperforms many
strong baselines.

2 Related Work

With the development of seq2seq model on neu-
ral translation task, more and more researchers
take note of its great potential in text summariza-
tion area (Fan et al., 2017; Ling and Rush, 2017;
Cheng and Lapata, 2016), especially for abstrac-
tive methods. Rush et al. (2015) is the first to
apply seq2seq model with attention mechanism
to abstractive summarization and achieve promis-
ing improvement. Nallapati et al. (2016) modify
the basic model with RNN-based encoder and de-
coder and propose several techniques. Chen et al.
(2016) further propose to improve the novelty
of generated summaries and design a distraction-
based attentional model. Li et al. (2017) creatively
incorporate the variational auto-encoder into the
seq2seq model to learn the latent structure in-
formation. However, these models are nearly
designed for abstractive sentence summarization,
which focus on encoding and mining salient infor-
mation on sentence-level and lead to unsatisfac-
tory performances for document summarization.

Some recent work improves the performance of
neural abstractive models on document summa-
rization task from various aspects. To better grasp
the essential meaning for summarization, Chen
et al. (2016) propose not only to pay attention to
specific regions and content of input documents
with attention models, but also distract them to tra-
verse between different content. Tan et al. (2017)
propose a graph-based attention mechanism in a
hierarchical encoder-decoder framework to gen-
erate multi-sentence summary. Gehrmann et al.
(2018) presents a content selection model for sum-
marization that identifies phrases within a docu-
ment that are likely included in its summary. To
produce more informative summaries, (Gu et al.,
2016) is the first to show that the copy mecha-
nism(Vinyals et al., 2015) can alleviate the Out-
Of-Vocabulary problem by copying words from



2134

the source documents. See et al. (2017) re-
build this pointer-generator network and incorpo-
rate an additional coverage mechanism into the de-
coder. Li et al. (2018b) notice the necessity of ex-
plicit information selection and they build a gated
global information filter and local sentence selec-
tion mechanism. Moreover, reinforcement learn-
ing (RL) approaches have been shown to further
improve performance on these tasks(Celikyilmaz
et al., 2018; Li et al., 2018a). Pasunuru and Bansal
(2018) develop a loss-function based on whether
salient segments are included in a summary. How-
ever, the optimization of RL-based models can be
difficult to tune and slow to train.

3 Model

In this section, we describe our approach from
three aspects: (i) the Transformer-based encoder-
decoder framework, (ii) the focus-attention mech-
anism for the encoder to emphasize the local con-
text, and (iii) the saliency-selection network for
the decoder to select salient information.

3.1 Encoder-Decoder Framework
Given a document X = (x1, x2, ..., xm), the en-
coder maps its corresponding symbol represen-
tations E = (e1, e2, ..., em) to a sequence of
continuous representations Z = (z1, z2, ..., zm),
where m is the length of document. The de-
coder then decode Z into continuous representa-
tions S = (s1, s2, ..., sn) and generates abstrac-
tive summary Y = (y1, y2, ..., yn) one token a
time, where n is the length of summary. Vs and
Vt are the source/target vocabularies and xi ∈ Vs,
yj ∈ Vt. E is the sum of word embedding rep-
resentations and position embedding representa-
tions, where ei ∈ Rde . Both embedding repre-
sentations are initialized as (Vaswani et al., 2017)
and learned during the process of optimization.

3.1.1 Encoder
The encoder is composed of a stack of N identi-
cal layers, and each layer has two sub-layers. The
first is the self-attention sub-layer and the second
is the feed-forward sub-layer. The residual con-
nection is employed around each of the two sub-
layers, followed by layer normalization. Given the
example input t, the output of each sub-layer can
be formalized as LayerNorm(t + SubLayer(t)).
For encoder, the SubLayer(t) can be replaced
with ATT(t) or FFN(t), which represents the pre-
output of self-attention sub-layer or feed-forward

sub-layer respectively. The details of each sub-
layer are presented as follows.

The self-attention sub-layer takes the output of
previous layer as the input. Formally, the input
for the self-attention sub-layer of the l-th layer
is Zl−1 ∈ Rm×dm , where dm is the dimension
of output. Specially, Z0 = E and the output
of encoder Z = ZN . In the process of compu-
tation, three matrices query Ql ∈ Rm×dm , key
Kl ∈ Rm×dm and value Vl ∈ Rm×dm are obtained
firstly by the linear projections from Zl−1 with
three different metrics WQl ∈ R

dm×dm , WKl ∈
Rdm×dm and W Vl ∈ Rdm×dm . Then the pre-
output of self-attention sub-layer can be computed
with the scaled dot-product attention mechanism:

ATT(Zl−1) = att(Ql,Kl, Vl)

= softmax(
QlK

T
l√

dm
)Vl

(1)

and the final outputAl of this sub-layer is obtained
with residual connection and layer normalization.
Moreover, the self-attention sub-layer can be fur-
ther extended into multi-head manner. Namely,

ATTM (Zl−1) = concat(H1, ...,Hh)W
C
l

where Hi = att(QlW
Q
l,i,KlW

K
l,i , VlW

V
l,i)

(2)
where h is the number of heads, WQl,i ∈ R

dm×dh ,
WKl,i ∈ Rdm×dh , W Vl,i ∈ Rdm×dh and WCl ∈
Rh∗dh×dm are four learnable weight matrices, dh
is the dimension for each head, we set dh = dm/h.

The feed-forward sub-layer takes the output of
self-attention sub-layer Al as the input and the
computation of pre-output FFN(Al) is straight-
forward with a position-wise fully connected feed-
forward network:

FFN(Al) = relu(AlW
1
l + b

1
l )W

2
l + b

2
l (3)

where W 1l ∈ Rdm×df and W 2l ∈ Rdf×dm are two
learnable weight matrices, df is the dimension of
intermediate output. b1l ∈ Rdf and b2l ∈ Rdm
are two learnable biases. The final output of feed-
forward sub-layer Zl is also the output for the l-th
layer which is obtained after residual connection
and layer normalization.

3.1.2 Decoder
The decoder in our framework has a similar
stacked structure with N identical layers. In ad-
dition to the two sub-layers introduced above,



2135

the decoder inserts another self-attention sub-layer
in between, which performs multi-head attention
over the output of the encoder. For clarity, we use
the “bridge sub-layer” to refer to this additional
self-attention sub-layer and BATT(Z, t) to rep-
resent the pre-output of this sub-layer, where Z
is the encoder output and t is a example of en-
coded partial generated summary. The calculation
of BATT(Z, t) is similar to the Eq.(1). Specifi-
cally, for the l-th bridge sub-layer in the decoder,
key Kl and value Vl are obtained by linear projec-
tions from Z. Apart from the additional sub-layer,
the rest of computation process is the same as the
encoder, and the output of last layerHN is consid-
ered as the final decoder output H .

Finally, for the i-th decoding step, we compute
a distribution over the Vt for target elements yi
by projecting the output of decoder stack Si via
a linear layer with weights W o ∈ Rdm×T and bias
bo ∈ RT ,

p(yi|y1, ..., yi−1;X) = softmax(W oSi + bo)
(4)

where T is the size of vocabulary Vt.

3.2 Focus-Attention Mechanism
To take full advantage of documents informa-
tion during the process of encoding, we design a
focus-attention mechanism and build it in the self-
attention sub-layers of the encoder, which is de-
picted as Figure 1. The “dotted boxes” indicate
that the corresponding modules can be adapted
into the multi-head manner.

The focus-attention mechanism models a focal
bias as a regularization term on attention scores
which is determined by the position of center and
effective coverage scope. In the l-th self-attention
sub-layer, since the query Ql, key Kl and value Vl
are obtained by linear projections from the input
Zl−1, so that they contain similar information in
different semantic space. To reduce the amount of
calculation, we only utilize the query matrices Ql
to compute the position vector and coverage scope
vector. Specifically, for the i-th encoding step in
l-th layer, the center position scalar µil ∈ R and
the coverage scope scalar σil ∈ R are calculated
by two linear projections, namely:

µil = U
T
c tanh(WpQ

i
l +WgGl)

σil = U
T
d tanh(WpQ

i
l +WgGl)

(5)

whereWp ∈ Rdm×dm , andWg ∈ Rdm×dm are two
shared weight matrices. Uc ∈ Rdm and Ud ∈ Rdm

Focal Bias

Softmax

Document

Attention Energy

Attention Scores

Query Key

Scaled Dot-
Product
Attention

Focus-
Attention
Mechanism

Figure 1: The focus-attention mechanism.

are two different linear projection weight vectors,
m is the length of input document and Gl =
1
m

∑m
i=1Q

i
l is the mean vector to provide comple-

mentary information. Furthermore, we regulate µil
and σil to the closed interval [0,m],

µ̃il = m ∗ sigmoid(µil)
σ̃il = m ∗ sigmoid(σil)

(6)

According to the definition of Gaussian distri-
bution, the focal bias for the i-th step f il ∈ Rm
can be easily obtained with µ̃il and σ̃

i
l as follows:

f i,jl = −
(P j − µ̃il)2

(σ̃il)
2/2

(7)

where P j is the absolute position of word xj in the
document. f i,jl ∈ [−∞, 0] measures the distance
between word xj and the center position µ̃il .

Eventually, this focal bias is added to the atten-
tion energy of encoder layers before softmax nor-
malization.

ATT(Zl−1) = att(Ql,Kl, Vl)

= softmax(
QlK

T
l√
d
⊕ fl)Vl

(8)

where ⊕ denotes the addition.
Moreover, we further adapt the focus-attention

mechanism into the multi-head manner as Eq.2.
Accordingly, the distinct focal biases are assigned
for each head and different weight matrices are uti-
lized in the process of computation.



2136

3.3 Saliency-Selection Network

Abstractive document summarization is a special
NLP generation task which requires to reduce the
influence of secondary information and integrate
salient segments to produce a condensed sum-
mary. Traditional seq2seq models often have lim-
ited performance on distinguishing salient seg-
ments (Tan et al., 2017), which emphasizes the ne-
cessity of customized selection network. In this
work, we design the saliency-selection network
for information selection, which is depicted as
Figure 2.

Concretely, we measure the saliency of each
word in the document by assigning a saliency
score and make a soft selection. For the i-th de-
coding step in l-th layer, the saliency-selection
network takes query matrices Qil ∈ Rdm and key
matrices Kl ∈ Rm×dm as the input, where m is
the length of the input document. Then, the net-
work computes saliency score gil ∈ Rm as:

gi,jl = sigmoid((WhQ
i
l)(WsK

j
l )
T ) (9)

where Wh ∈ Rdm×dm and Ws ∈ Rdm×dm are two
learnable weight matrices. gi,jl ∈ [0, 1] measures
the saliency of the j-th token in document for the
i-th position in summary. Furthermore, we incor-
porate the computed saliency score gl into the at-
tention network of bridge sub-layer by:

BATT(Z, Sl−1) = att(Ql,Kl, Vl)

= gl ⊗ softmax(
QlK

T
l√
d

)Vl

(10)
where ⊗ denotes element-wise multiplication.

Moreover, we also adopt the saliency-selection
network into the multi-head manner, which allows
to model saliency from different perspectives at
different positions.

3.4 Objective Function

Our goal is to maximize the output summary prob-
ability given the input document. Therefore, we
optimize the negative log-likelihood loss function:

L = − 1
|τ |

∑
(X,Y )∈τ

log p(Y |X; θ) (11)

DocumentPartial
Summary

Scaled Dot-
Product
Attention

Saliency ScoresAttention Scores

Final Scores

Key

Saliency-
Selection
Network

Query

Figure 2: The saliency-selection netowrk.

where θ is the model parameter, and (X,Y ) is a
document-summary pair in training set τ , then

log(Y |X; θ) =
n∑
i=1

log p(yi|y1, ..., yi−1, X; θ)

(12)
where p(yi|y1, ..., yi−1, X; θ) is calculated by the
decoder.

4 Experiments

In this section, we introduce the experiment set-
up, the implementation details, the baseline mod-
els and the experimental results.

4.1 Setup

We conduct the experiments on a large-scale cor-
pus of CNN/Daliy Mail, which has been widely
used for the explorations on document summariza-
tion. The corpus is originally constructed by col-
lecting human generated highlights for new stories
in CNN and Daily Mail website (Hermann et al.,
2015). We use the scripts supplied by Nallap-
ati et al. (2016) to further obtain the CNN/Daily
Mail dataset. This dataset contains 287,226 train-
ing pairs, 13,368 validation pairs and 11,490 test
pairs. We use the same non-anonymized version
of dataset as See et al. (2017) which requires no
pre-processing1. The average number of sentences

1https://github.com/abisee/cnn-dailymail



2137

in documents and summaries are 42.1 and 3.8, re-
spectively. We assume the length of all documents
should not exceed 400 tokens and all summaries
should not exceed 100 tokens. The word dictio-
nary shared by documents and summaries contains
50,000 most popular tokens in documents.

In our model, we set the number of en-
coder/decoder layers N = 4 and the number of
heads h = 8. The dimensions of the signal repre-
sentation de and output dm are set to 512, and the
dimension of intermediate output df is set to 2048.
Besides, the dropout rate is set to 0.8 in the pro-
cess of training. We implement our model in Py-
Torch2 1.0. In all experiment, the batch size is set
to 4096. We use the Adam optimizer (Kingma and
Ba, 2014) to train our model with β1 = 0.9, β2 =
0.998 and � = 10−8. The learning rate varies ev-
ery step with the Noam decay strategy (Vaswani
et al., 2017) and the warmup threshold is 8000.
The maximum norm of gradient-clipping is set
to 2. In the end, we conduct our experiment on
one machine with 4 NVIDIA Titan Xp GPUs and
the training process lasts 200,000 steps for each
model.

We use the beam search algorithm (Sutskever
et al., 2014) with coverage technique (Tu et al.,
2016) to generate multiple summary candidates
in parallel to obtain better results, the coverage
weight is set to 1. For fear of favoring shorter gen-
erated summaries, we utilize length penalty (Wu
et al., 2016) during the process of inference. We
set the beam size to 10, the length penalty param-
eter α to 0.9 and β to 5. The minimum length of
the generated summary is set to 35 and the batch
size for inference is set to 1.

Following the previous studies, we use the
ROUGE scores (Lin and Hovy, 2003) to evalu-
ate the performance of our model with Python
implementation3 and standard options. ROUGE
scores measure the quality of summary by com-
puting overlapping lexical units with references,
such as uni-gram, bi-gram and longest common
subsequence (LCS). F-measures ROUGE-1 (uni-
gram), ROUGE-2 (bi-gram) and ROUGE-L (LCS)
are reported as the evaluation metrics.

4.2 Baselines

In this work, we compare our approach with these
following state-of-the-art baselines:

2https://pytorch.org/
3https://github.com/falcondai/pyrouge

Models RG-1 RG-2 RG-L
words-1vt2k-temp-att 36.64 15.66 33.42
PG+cov 39.53 17.28 36.38
ConvS2S 39.75 17.29 36.54
Explicit-Selection 41.54 18.18 36.47
ROUGEEsal+Ent 40.43 18.00 37.10
Bottom-Up 41.22 18.68 38.34
Basic model 39.45 17.20 36.49
+Focus-Attention 40.29 18.63 38.11
+Saliency-Selection 40.76 18.40 37.67

ETADS 41.75 19.01 38.89

Table 2: ROUGE scores on the CNN/Daliy Mail
test set. All ROUGE scores have 95% confidence
interval of at most ±0.24 computed by the official
ROUGE script. To save space, we use “PG+cov”
and “Bottom-Up” to denote the baseline “Pointer-
Generator+coverage” and “Bottom-Up Summariza-
tion”. The symbol “+” stands for the corresponding
module is added on the “Basic model” which is a
vanilla Transformer with 4 identical layers.

words-1vt2k-temp-att: Nallapati et al. (2016)
build this model with the basic seq2seq encoder-
decoder architecture and attention mechanism,
which is a pioneering effort for much other work.

Pointer-generator+coverage: To deal with
Out-Of-Vocabulary words (OOV words) and re-
peating problem, See et al. (2017) combine the
pointer network into the RNN-based seq2seq
model and design a coverage mechanism.

ConvS2S: Gehring et al. (2017) creatively uti-
lize convolution neural networks to build seq2seq
model and achieve high performance on many
tasks, including abstractive summarization.

Explicit-Selection: Li et al. (2018b) propose to
extend the basic seq2seq model with an informa-
tion selection layer to explicitly control informa-
tion flow.

ROUGESal+Ent(RL): Pasunuru and Bansal
(2018) address main difficulties via a reinforce-
ment learning approach with two novel reward
functions.

Bottom-Up Summarization: This work com-
bines extractive and abstractive summarization by
firstly using a data-efficient content selector to
over-determine phrase related (Gehrmann et al.,
2018).

4.3 Results
The experimental results are given in Table 2.
Overall, ETADS achieves advantages of ROUGE



2138

F1 scores over all of the other baselines (reported
in their own articles) and two extensions we pro-
posed both improve the performances based on
the basic model. Concretely, we design the focus-
attention mechanism to improve the capability of
capturing the local context information and further
encode the document comprehensively. There-
fore, the basic model with focus-attention mech-
anism is expected to achieve improvement in pro-
ducing summaries with continuous salient seg-
ments. The significant improvement on ROUGE-
L verifies our hypothesis. Besides, we notice that
the improvements provided by the basic model
with saliency-selection network particularly lie
in ROUGE-1 F1 scores. We consider the reason
may lie in the saliency-selection network is more
sensitive to the short segments due to the separate
saliency measuring process.

Comparing with the two classical RNN-based
baselines words-1vt2k-temp-att and Pointer-
generator+coverage and one CNN-based base-
line ConvS2S, our basic model is capable of
achieving equivalent performance. We believe
it should give credit to the capability of mod-
eling long-term dependencies. When com-
pared with more recent work, Explicit-Selection
equips a selection layer similar to our saliency-
selection network to mine salient information. De-
spite being aware of this problem, our saliency-
selection network achieves better performance
with the help of stacked architecture. The per-
formance of reinforcement learning based model
ROUGEEsal+Ent is worse than our model obvi-
ously. The strongest baseline Bottom-Up Sum-
marization combines the advantages of CNN-
based model and RNN-based model but is also
slightly inferior to our model.

4.4 Case Study

To further illustrate the effectiveness of our pro-
posed ETADS vividly and analyze the reasons
of improving the performance, we compare the
generated summaries by baselines words-1vt2k-
temp-att, Bottom-Up Summarization and our
ETADS approach. For the case in Table 3, the
input document focuses on analyzing the latest
financial report of the Apple company and fur-
ther discusses the impact of the new Apple Watch
on retail revenue. The performance of words-
1vt2k-temp-att is unsatisfactory, three generated
sentences are irrelevant to the main concepts and

Reference summary:
apple sold more than 61 million iphones in the quarter .
apple did n’t report any results for the new apple watch .
believed around 2 million watches have been sold , ac-
cording to estimates .

words-1vt2k-temp-att:
the iphone is still the engine behind apple ’s phenomenal
success .
apple has vied with south korea ’s samsung for the no. 1
position in the global smartphone market .
apple ceo tim cook has said he ’s optimistic about new
markets such as [china china china china china ...]

Bottom-Up Summarization:
[apple sold more than 61 million iphones in the quarter]
, accounting for more than two-thirds of its $ 58 billion
in revenue for the quarter and the lion ’s share of $ 13.6
billion in profit - and up 40 % from a year ago .
$ 200 billion in cash , up from around $ 150 billion for
one year .
revenue from mac computers rose 2 % to $ 5.6 billion .

ETADS:
[apple sold more than 61 million iphones in the quarter .]
it was a 40 percent increase over the number of iphones
sold in the first three months of 2014 .
[apple did n’t report any results for the new apple watch] ,
which it began selling this month , after the quarter ended
.

Table 3: Example of generated summaries. We high-
light the words or sentences in [red] which are consis-
tent with partial reference summary. Repetition seg-
ments are marked in [blue] .

even contains repetitions at the end of the sum-
mary. Abstractive summary generated by base-
line Bottom-Up Summarization is much more
better, which indicates the effectiveness of modi-
fications. However, the generated summary only
contains partial salient information of the docu-
ment. ETADS achieves the best performance in
this case due to two of the generated sentences
containing salient information and without repeti-
tions. The above results verify that the extensions
in our model improve the capability of document
summarization from not only quantitative but also
qualitative perspectives.

4.5 Discussion
In this section, we first validate the robustness
of our model with different encoder/decoder ar-
chitectures and then discuss the different deploy
strategies for our extensions.

4.5.1 Architecture Robustness
We conduct experiments to see how the model’s
performance is affected by the stacked architec-
ture. We perform a set of experiments which ad-
just the structures of the encoder and decoder to



2139

Encoder RG-1 RG-2 RG-L # of paras
2 layers 35.12 14.05 32.41 3190K *

4 layers 39.45 17.20 36.49 3821K
6 layers 39.67 17.47 35.71 4451K
Decoder RG-1 RG-2 RG-L # of paras
2 layers 31.10 12.93 27.04 3406K
4 layers 39.45 17.20 36.49 4246K
6 layers 39.35 18.01 36. 21 5087K
* 1K equals to 1000

Table 4: ROUGE scores on the CNN/Daily Mail test
set. “# of paras” denotes the number of training pa-
rameters. We fix the decoder to 4 layers when adjust
structure of the encoder and vice versa.

Layers RG-1 RG-2 RG-L
- 40.87 17.78 37.73

[1-2] 42.81 20.12 39.68
[3-4] 41.91 19.65 39.32
[1-4] 43.06 20.85 40.12

Table 5: ROUGE precision scores on the CNN/Daliy
Mail test set. We use the token “-” to indicate the basic
model which does not contain saliency-selection net-
work. “[1-2]” indicates we deploy saliency-selection
network on the first and second layer of basic model,
“[3-4]” and “[1-4]” are similar.

2, 4 and 6 layers respectively. Experimental re-
sults on the test set in Table 4 show that there is
no notable difference between 4 layers or 6 lay-
ers for encoder or decoder. However, the number
of parameters is significantly increased nearly 1/4
for 6 layers, which means more time is needed
for convergence. Employing 2 layers for either
the encoder or decoder leads to rapid performance
degradation. From the aspect of efficiency and ef-
fectiveness, we decide to equip 4 layers for the en-
coder and decoder eventually.

4.5.2 Deployment Strategies
In this section, we discuss the different deploy-
ment strategies for our extensions on the encoder-
decoder framework.

Firstly, we deploy the saliency-selection net-
work on different layers to discuss strategies of
saliency-selection deployment. As we mentioned
before, the major difficulty of this salient infor-
mation selection procedure is to comprehend the
relative semantic meanings and make the correct
selection, which significantly affects the precision
scores. Therefore, it is proper to use precision

Layers RG-1 RG-2 RG-L
- 41.10 17.82 37.91

[1-2] 40.92 18.61 38.22
[3-4] 40.57 18.20 38.19
[1-4] 41.31 18.72 38.93

Table 6: ROUGE recall scores on the CNN/Daliy Mail
test set. “-” to indicate the basic model which does
not contain focus-attention mechanism. Other symbols
express same meaning with Table 5

scores to measure effectiveness. From Table 5,
it can be observed that the improvements brought
by our saliency-selection network do not increase
with layers linearly. In the shallow layers, the
saliency-selection network contributes to notable
improvement which is close to the best results we
achieved. However, for the deeper layers, the im-
provement brought by the saliency-selection net-
work is limited. We believe it can be attributed to
the characteristics of our encoder-decoder frame-
work. Self-attention sub-layer effectively reduces
the cost of long-term information fusion, which
leads to difficult to comprehend the original se-
mantic information. The saliency-selection net-
work we proposed is not competent to distinguish
noise information when the original semantic in-
formation becoming confusing.

Furthermore, we discuss the strategies for
focus-attention mechanism with ROUGE recall
scores. The results of Table 6 demonstrate a sim-
ilar phenomenon to Table 5 where improvements
mainly come from shallow layers. We believe it is
a trade-off between local context and global infor-
mation. Focus-attention mechanism aims to gather
attention to the local context around a center which
deviates from the original goal. (Vaswani et al.,
2017; Shi et al., 2016) indicate that there exists a
consensus in the NLP community that shallow lay-
ers of a stacked model are sensitive to local con-
text and deeper layers modeling global semantics.
Therefore, as the module designed to capture lo-
cal context, we believe it is reasonable to obtain
more promotion where it is equipped on shallower
layers which is also a side proof of effectiveness.

5 Conclusion

In this paper, we propose a novel framework
for abstractive document summarization with ex-
tended Transformer model. The proposed model
consists of a concise pipeline. First, the stacked



2140

encoder with focus-attention mechanism captures
long-term dependencies and local context of in-
put document comprehensively. Then the decoder
with saliency-selection network distinguishes and
condenses the salient information into the output.
Finally, an inference algorithm produces the ab-
stractive summaries. Our experiments show that
the proposed model achieves a significant im-
provement for abstractive document summariza-
tion over previous state-of-the-art baselines.

Acknowledgments

This work is supported by Chinese National Re-
search Fund (NSFC) Key Project No. 61532013
and No. 61872239. FDCT/0007/2018/A1, DCT-
MoST Joint-project No. (025/2015/AMJ), Uni-
versity of Macau Grant Nos: MYRG2018-00237-
RTO, CPG2018-00032-FST and SRG2018-
00111-FST of SAR Macau, China.

References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.

1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE transactions on neural
networks, 5(2):157–166.

Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep communicating agents for
abstractive summarization. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 1662–1675.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei,
and Hui Jiang. 2016. Distraction-based neural net-
works for document summarization. arXiv preprint
arXiv:1610.08462.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 484–494.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Angela Fan, David Grangier, and Michael Auli. 2017.
Controllable abstractive summarization. arXiv
preprint arXiv:1711.05217.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning. In Proceedings
of the 34th International Conference on Machine
Learning-Volume 70, pages 1243–1252. JMLR. org.

Sebastian Gehrmann, Yuntian Deng, and Alexander
Rush. 2018. Bottom-up abstractive summarization.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4098–4109.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1631–1640.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wojciech Kryściński, Romain Paulus, Caiming Xiong,
and Richard Socher. 2018. Improving abstraction
in text summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1808–1817.

Piji Li, Lidong Bing, and Wai Lam. 2018a. Actor-
critic based training framework for abstractive sum-
marization. arXiv preprint arXiv:1803.11070.

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.
Deep recurrent generative decoder for abstractive
text summarization. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2091–2100.

Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo
Wang. 2018b. Improving neural abstractive doc-
ument summarization with explicit information se-
lection modeling. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1787–1796.

Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo
Wang. 2018c. Improving neural abstractive docu-
ment summarization with structural regularization.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4078–4087.

Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics.

Jeffrey Ling and Alexander Rush. 2017. Coarse-to-fine
attention models for document summarization. In
Proceedings of the Workshop on New Frontiers in
Summarization, pages 33–42.



2141

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al. 2016. Abstractive text summa-
rization using sequence-to-sequence rnns and be-
yond. arXiv preprint arXiv:1602.06023.

Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-
reward reinforced summarization with saliency and
entailment. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers), volume 2,
pages 646–653.

Alexander M Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1526–
1534.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017.
Abstractive document summarization with a graph-
based attentional neural model. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1171–1181.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 76–85.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Baosong Yang, Zhaopeng Tu, Derek F Wong, Fandong
Meng, Lidia S Chao, and Tong Zhang. 2018. Mod-
eling localness for self-attention networks. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 4449–
4458.


