



















































Cross-Sentence Grammatical Error Correction


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 435–445
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

435

Cross-Sentence Grammatical Error Correction

Shamil Chollampatt, Weiqi Wang, and Hwee Tou Ng
Department of Computer Science, National University of Singapore
{shamil,weiqi}@u.nus.edu, nght@comp.nus.edu.sg

Abstract

Automatic grammatical error correction
(GEC) research has made remarkable progress
in the past decade. However, all existing ap-
proaches to GEC correct errors by considering
a single sentence alone and ignoring crucial
cross-sentence context. Some errors can only
be corrected reliably using cross-sentence
context and models can also benefit from the
additional contextual information in correct-
ing other errors. In this paper, we address
this serious limitation of existing approaches
and improve strong neural encoder-decoder
models by appropriately modeling wider
contexts. We employ an auxiliary encoder that
encodes previous sentences and incorporate
the encoding in the decoder via attention and
gating mechanisms. Our approach results
in statistically significant improvements
in overall GEC performance over strong
baselines across multiple test sets. Analysis of
our cross-sentence GEC model on a synthetic
dataset shows high performance in verb
tense corrections that require cross-sentence
context.

1 Introduction

Grammatical error correction (GEC) is the task
of correcting errors in input text and produc-
ing well-formed output text. GEC models are
essential components of writing assistance and
proof-reading tools that help both native and
non-native speakers. Several adaptations of so-
phisticated sequence-to-sequence learning models
with specialized techniques have been shown to
achieve impressive performance (Ge et al., 2018;
Lichtarge et al., 2018; Chollampatt and Ng, 2018b;
Junczys-Dowmunt et al., 2018).

All prior approaches to GEC consider one sen-
tence at a time and ignore useful contextual infor-
mation from the document in which it appears, un-
like a human proofreader. Cross-sentence context

OUT OF CONTEXT:
As a result, they are not convenient enough.
IN CONTEXT:
Electric cars have a very obvious shortage in
their technique design. The electric cars in-
vented in 1990 did not have a powerful bat-
tery. Due to the limitation of its weigh, size
and the battery technology, the battery used in
the electric cars at that time was limited to a
range of 100 miles (Rogers,2003). As a re-
sult, they are were not convenient enough.
Instead, Hydrogen fuel cell was brought up to
substitute the electric battery.

Figure 1: A sentence from NUCLE appears correct out
of context, but erroneous in context.

is essential to identify and correct certain errors,
mostly involving tense choice, use of the definite
article ‘the’, and use of connectives. The example
in Figure 1, from the NUS Corpus of Learner En-
glish or NUCLE (Dahlmeier et al., 2013), shows a
learner-written sentence that seems correct in iso-
lation, but actually involves a verb tense error in
context. Sentence-level GEC systems fail to reli-
ably correct such errors. Moreover, models may
also benefit from the additional context by being
able to disambiguate error corrections better.

In this paper, we present the first approach to
cross-sentence GEC1. We build on a state-of-the-
art convolutional neural encoder-decoder model
and incorporate cross-sentence context from pre-
vious sentences using an auxiliary encoder. The
decoder attends to the representations generated
by the auxiliary encoder via separate attention
mechanisms and incorporates them via gating that
controls the information that goes into the de-

1Our source code is publicly available at https://
github.com/nusnlp/crosentgec.

https://github.com/nusnlp/crosentgec
https://github.com/nusnlp/crosentgec


436

coder. Auxiliary encoders have also been used in
other sequence generation tasks such as automatic
post editing (Libovický and Helcl, 2017), multilin-
gual machine translation (Firat et al., 2016), and
document-level neural machine translation (Jean
et al., 2017; Wang et al., 2017).

Our cross-sentence GEC model shows statis-
tically significant improvements over a compet-
itive sentence-level baseline across multiple test
sets. We further improve the baseline and cross-
sentence model by ensembling multiple models
and rescoring. We also incorporate probabilities
computed by BERT (Devlin et al., 2018) as a fea-
ture. The gains in performance with the cross-
sentence component over the sentence-level base-
line still remain even with the improvements. Our
final cross-sentence model also outperforms state-
of-the-art models trained on the same datasets on
the CoNLL-2014 test set. They show notable
improvements in correcting determiner and verb
tense errors. Our analysis demonstrates that our
cross-sentence model is able to accurately correct
many cross-sentence verb tense errors when eval-
uated on a synthetic test set.

2 Incorporating Cross-Sentence Context

Consider a source document S = S1, . . . SN made
up of N source sentences, where Sk represents
the kth source sentence. Sk comprises of |Sk| to-
kens sk,1 . . . sk,|Sk|. In a standard sentence-level
encoder-decoder model, the probability of a cor-
rected target hypothesis Tk = tk,1 . . . tk,|Tk|, given
the source sentence Sk and the set of model pa-
rameters Θ, is computed by

P (Tk|Sk,Θ) =
|Tk|∏
i=1

P (tk,i|Tk,<i, Sk,Θ) (1)

where Tk,<i denotes previous target words
tk,1, . . . , tk,i−1. The above model assumes that
the correction of Sk is independent of other sen-
tences in the source document S. This assumption
may not always hold since cross-sentence context
is required or helpful for correcting many errors.
In our proposed cross-sentence model, the above
conditional probability also depends on other sen-
tences in the source document, denoted by Sdoc.
Equation 1 is rewritten as

P (Tk|S,Θ) =
|Tk|∏
i=1

P (tk,i|Tk,<i, Sk,Sdoc,Θ) (2)

We make an assumption to simplify modeling and
consider only two previous source sentences as
the cross-sentence context (Sdoc = Sk−1, Sk−2).
We also do not explicitly consider previously cor-
rected target sentences to avoid error propaga-
tion (Wang et al., 2017). We extend a sentence-
level encoder-decoder baseline model to build our
cross-sentence GEC model. We describe our
baseline encoder-decoder model and the proposed
cross-sentence model below.

2.1 Baseline Encoder-Decoder Framework

We use a deep hierarchical convolutional encoder-
decoder model (Gehring et al., 2017) as our
encoder-decoder framework. Ensembles of convo-
lutional models have achieved high performance
for GEC and are used in two recent state-of-the-
art GEC models (Chollampatt and Ng, 2018b; Ge
et al., 2018).

A source sentence S is embedded as S =
EMBtok(S) + EMBpos(S) where S ∈ R|S|×d.
EMBtok(·) and EMBpos(·) are token embeddings
and learned position embeddings, respectively.
The source embeddings S are then passed through
a linear layer (denoted by LIN) before they are
fed to the initial encoder layer as H0 = LIN(S),
where H0 ∈ R|S|×h. The output of the lth en-
coder layer Hl ∈ R|S|×h, where l = 1, . . . , L,
is computed by passing the output of the pre-
vious layer Hl−1 through a convolutional neu-
ral network (CONV) followed by gated linear unit
(GLU) activation function (Dauphin et al., 2017),
and residual connections (He et al., 2016):

Hl = GLU(CONV(Hl−1)) + Hl−1 (3)

Output of the encoder is a transformation of the
final encoder layer output HL to E ∈ R|S|×d.

The decoder network also consists of multiple
layers. During the prediction of the (n+1)th word,
the decoder has access to previously predicted n
words t1, . . . , tn and their embeddings T ∈ Rn×d.
The embeddings are obtained in the same way as
the encoder via separate embedding layers in the
decoder. T passes through a linear layer to obtain
the input to the initial decoder layer G0 ∈ Rn×h.
The lth decoder layer computes an intermediate
representation Yl ∈ Rn×h by passing the out-
puts of the previous decoder layer Gl−1 through
a CONV layer followed by a GLU activation:

Yl = GLU(CONV(Gl−1)) (4)



437

previous sentences

Positional
Embedding

Token
Embedding

L × GLU

Conv

source sentence

GLU

Conv

previous target words

Source
Attention

Positional
Embedding

Token
Embedding

+

Auxiliary
Attention

Positional
Embedding

Token
Embedding

L' ×
GLU

Conv

+

L ×

Linear

Softmax

output probabilities

×

�

+

+

++

Figure 2: Our cross-sentence convolutional encoder-
decoder model with auxiliary encoder and gating.

Additionally, each decoder layer has an atten-
tion mechanism that utilizes Yl to compute sum-
marized representations Cl ∈ Rn×h of the en-
coder states E ∈ R|S|×d for n decoder states. At-
tention at layer l is computed as follows

Zl = LIN(Yl) + T (5)

Xl = SOFTMAX(Zl ·E>) · (E + S) (6)
Cl = LIN(Xl) (7)

Output of the lth decoder layer is computed as

Gl = Yl + Cl + Gl−1 (8)

The decoder outputs a linear transformation of
the final decoder layer output GL to D ∈ Rn×d.
For predicting the (n+1)th target word, the output
softmax is computed after a linear transformation
of the last decoder state to the output vocabulary
size. The last decoder state corresponds to the last
row of D.

2.2 Cross-Sentence GEC Model
Our proposed model (Figure 2) incorporates cross-
sentence context using an auxiliary encoder that is
similar in structure to our source sentence encoder
(Section 2.1). For encoding the context consist-
ing of two previous sentences, we simply concate-
nate these two sentences and pass it to the auxil-
iary encoder. Let Ŝ represent the cross-sentence

context consisting of |Ŝ| tokens ŝ1, ..., ŝ|Ŝ| from
the previous source sentences. They are embedded
as Ŝ ∈ R|Ŝ|×d using separate token and position
embedding layers and fed into the auxiliary multi-
layer encoder (consisting of L′ identical layers) to
obtain the auxiliary encoder output Ê ∈ R|Ŝ|×d.

For integrating the auxiliary encoder output dur-
ing decoding, each decoder layer employs a sepa-
rate attention mechanism similar to that described
previously. We use another LIN layer to compute
Ẑl (Equation 5) and use Ê and Ŝ in place of E and
S, respectively, to compute summarized auxiliary
encoder representations Ĉl (Equations 6 and 7).

The summarized auxiliary encoder representa-
tion at each layer Ĉl is added to the output of
the layer along with other terms in Equation 8.
All corrections do not depend equally on cross-
sentence context. So, instead of adding Ĉl directly
with an equal weight as the other terms, we add
a gating Λl ∈ Rn×h at each layer to control the
cross-sentence information that gets passed:

Gl = Yl + Cl + Λl ◦ Ĉl + Gl−1 (9)

where ◦ denotes the Hadamard product. The gate
Λl is determined based on Yl and the summarized
context representations of the source sentence Cl.

Λl = σ(LIN(Yl) + LIN(Cl)) (10)

where σ represents element-wise sigmoid activa-
tion that restricts values to [0, 1]. Λl can be re-
garded as probabilities of retaining values in Ĉl.

We also analyze our proposed approach by
comparing against two other ways for modeling
cross-sentence context (Section 5.1). One way
is to integrate representations from the auxiliary
encoder directly in the decoder layers via the
attention mechanism, without gating. Another
more straightforward way of incorporating cross-
sentence context is by simply passing the concate-
nation of the previous source sentences and the
current source sentence (Tiedemann and Scherrer,
2017) to the main encoder itself, without employ-
ing an auxiliary encoder.

2.3 Other Techniques and BERT Rescoring
We further improve our models (both sentence-
level baseline and cross-sentence model) with sev-
eral techniques from prior work that have been
shown to be useful for GEC. They include ini-
tializing the word embedding vectors with pre-
trained embeddings, pretraining the decoder on



438

large English corpora using a language modeling
objective (Junczys-Dowmunt et al., 2018), label
smoothing (Szegedy et al., 2016), and dropping
out entire word embeddings of source words dur-
ing training. We found that using the technique of
training with an edit-weighted objective (Junczys-
Dowmunt et al., 2018) results in a higher recall
but hurts the overall performance of our baseline
model, and hence we do not use it in our models.

We also rescore the final candidates using
feature-based rescoring (with edit operation and
language model features) for both our sentence-
level baseline and our cross-sentence model fol-
lowing Chollampatt and Ng (2018a). We in-
vestigate the effectiveness of BERT (Devlin
et al., 2018) for GEC. We use masked language
model probabilities computed by pretrained BERT
model2 as an additional feature during rescoring
to further improve our sentence-level baseline and
our cross-sentence GEC model. Specifically, we
replace tokens in a hypothesis T with the [MASK]
token, one at a time, predicting the log probability
of the masked token using BERT. The final BERT
feature is computed as

fBERT(T ) =

|T |∑
i=1

logPBERT(ti|T−i) (11)

where T−i is the target sentence where the ith tar-
get word ti is masked.

3 Experiments

3.1 Data and Evaluation
Similar to most published GEC models (Chol-
lampatt and Ng, 2018b; Junczys-Dowmunt et al.,
2018; Grundkiewicz and Junczys-Dowmunt,
2018), we rely on two datasets for training:
Lang-8 Learner Corpora3 v2.0 (Mizumoto et al.,
2011) and NUCLE (Dahlmeier et al., 2013). Both
datasets have document context available which
make them suitable for cross-sentence GEC. We
split training and development datasets based
on essay boundaries. We extract development
set from a subset of NUCLE. To ensure the
development set has a high number of error
annotations, we sort the essays in decreasing
order of the ratio of corrected sentences per essay.
We select 25% of essays from the top (sampling

2We use pretrained cased BERT base model from
https://github.com/huggingface/pytorch-pretrained-BERT

3https://sites.google.com/site/naistlang8corpora

Dataset No. of No. of No. of
essays sentences src tokens

Train 178,972 1,306,108 18,080,501
NUCLE 1,125 16,284 423,503
Lang-8 177,847 1,289,824 17,656,998

Dev 272 5,006 128,116

Table 1: Statistics of training and development
datasets.

one essay from every four) until we get over
5,000 annotated sentences. The remaining essays
from NUCLE are used for training. From Lang-8,
we extract essays written by learners whose
native language is English and contain at least
two English sentences with a minimum of one
annotation. We identify the language of a sentence
using langid.py (Lui and Baldwin, 2012). From
the extracted essays, we select annotated English
sentences (with at most 80 tokens) as our source
sentences and their corresponding corrections as
our target sentences. Statistics of the datasets are
given in Table 1. For training our cross-sentence
GEC models, we select two previous English
sentences for each source sentence from its essay
as the cross-sentence context. We found that
using two previous sentences performed better
than using only one previous sentence as the
context. Still, in our dataset, the first English
sentence of an essay has an empty context and the
second English sentence of an essay has only one
previous sentence as its context.

We evaluate on three test sets which have
document-level contexts: CoNLL-2013 (Ng et al.,
2013) and CoNLL-2014 (Ng et al., 2014) shared
task test sets, and Cambridge Learner Corpus-First
Certificate Exam or FCE test set (Yannakoudakis
et al., 2011). Another recent dataset, JFLEG
(Napoles et al., 2017), does not have document-
level contexts available and hence we do not use
it for evaluation. We use the M2scorer (Dahlmeier
and Ng, 2012) for evaluation and perform signif-
icance tests using one-tailed sign test with boot-
strap resampling on 100 samples.

3.2 Model

We extend a convolutional encoder-decoder model
following recent state-of-the-art sentence-level
GEC models (Chollampatt and Ng, 2018b; Ge
et al., 2018) with identical architecture and hy-
perparameters to build our sentence-level baseline.



439

CoNLL-2013 CoNLL-2014 FCE
P R F0.5 P R F0.5 P R F0.5

BASELINE (avg) 54.51 15.16 35.88 65.16 27.13 50.88 52.89 26.85 44.29
CROSENT (avg) 55.65 16.93 38.17 65.59 30.07 53.06 52.17 28.25 44.61
BASELINE (ens) 55.67 15.02 36.12 66.45 27.12 51.51 53.71 26.79 44.72
CROSENT (ens) 58.72 16.64 38.99 68.06 29.94 54.25 54.49 28.63 46.15
BASELINE (ens+rs) 51.88 19.15 38.66 62.53 33.62 53.35 52.01 31.19 45.89
CROSENT (ens+rs) 54.43 20.22 40.67 64.15 35.26 55.12 52.93 32.81 47.15
BASELINE (ens+rsBERT) 52.77 19.01 38.93 64.08 34.21 54.55 53.47 30.91 46.65
CROSENT (ens+rsBERT) 55.24 20.71 41.43 64.32 35.98 55.57 53.91 32.81 47.77

Table 2: Results of our proposed cross-sentence GEC model (CROSENT). ‘avg’ row reports average precision (P),
recall (R), and F0.5 score of 4 independently trained single models. ‘ens’ denotes results of the 4-model ensemble.
‘rs’ denotes results of feature-based rescoring, and ‘rsBERT’ additionally uses BERT feature for rescoring. All
CROSENT results are statistically significant compared to the corresponding BASELINE results (p < 0.001).

The final system of Chollampatt and Ng (2018b) is
an ensemble of three variants of this architecture
initialized and trained differently. We replicate the
best-performing variant using Fairseq4 v0.5 on our
training data. We incorporate the techniques men-
tioned in Section 2.3 such as source word dropout,
pretraining word embedding, decoder pretraining,
and label smoothing. We reuse the the pretrained
word embeddings and vocabularies from (Chol-
lampatt and Ng, 2018a) and the pretrained decoder
from (Chollampatt and Ng, 2018b). Word em-
beddings had been pretrained on Wikipedia cor-
pus consisting of 1.78 billion words. The decoder
had been pretrained using 100 million sentences
(1.42 billion words) from Common Crawl. To fit
training of a single convolutional encoder-decoder
model efficiently in a single Titan X GPU with
12 GB memory, we restrict each batch to a maxi-
mum of 6,000 source or target tokens per batch,
apart from setting a maximum batch size of 96
instances. All other hyperparameters, pretrained
models, and vocabularies are from (Chollampatt
and Ng, 2018b), with source token dropout of 0.2
and label smoothing parameter of 0.1. We refer to
this model as BASELINE in our results. The best
model is chosen based on the development set per-
plexity.

We extend Fairseq and implement an auxiliary
encoder for modeling previous sentences. We use
the source vocabulary itself as the vocabulary for
the auxiliary encoder and initialize its embeddings
with the same pretrained word embeddings used
to initialize the source encoder. We use embed-
dings of size 500. We use three layers in the auxil-

4https://github.com/pytorch/fairseq

iary encoder with the output size set to 1024. The
cross-sentence context is encoded by the auxiliary
encoder. In the case of the first sentence in an es-
say, an empty context consisting of only padding
tokens and the end-of-sentence marker token is
passed. We denote our cross-sentence GEC model
as CROSENT.

4 Results

We evaluate the performance of our cross-sentence
GEC model, CROSENT, and compare it to the
sentence-level BASELINE on three test sets in
Table 2. Average single model results (‘avg’)
show a notable improvement on CoNLL-2013
and CoNLL-2014 for our CROSENT model. On
FCE, a significant improvement of 0.32 F0.5 is ob-
served (p < 0.001) for a single model. CoNLL-
2013 and FCE have only a single set of annota-
tions for each sentence and hence, reference cov-
erage could potentially underrate model perfor-
mance on these test sets. Interestingly, the per-
formance gap widens for an ensemble of models.
CROSENT achieves significant improvements of
2.87, 2.74, and 1.43, respectively, on the three test
sets. Feature-based rescoring (‘rs’) shows further
improvements. Adding BERT (‘rsBERT’) improves
our BASELINE and CROSENT model further. The
improvements due to the integration of the cross-
sentence component still remain notable and sig-
nificant.

4.1 Comparison to the State of the Art

We compare our CROSENT model to the state
of the art. Our best result of 55.57 for
CROSENT (ens+rsBERT) is competitive to the



440

F0.5
BASELINE (ens+rsBERT) 54.55
CROSENT (ens+rsBERT) 55.57
?NUS1+2+3 (ens) 52.49
?NUS2+3+BASELINE (ens) 52.77
?NUS2+3+CROSENT (ens) 54.87
?NUS2+3+BASELINE (ens+rsBERT) 55.47
?NUS2+3+CROSENT (ens+rsBERT) 57.16

+ spell 57.30
Best published results (same datasets)
NUS1+2+3 (2018b) (ens+rs+spell) 56.43
G&J (2018) (w/ spell) 56.25
JGGH (2018) 55.8
NUS1 (2018a) (w/ spell) 54.79
Best published results (larger training datasets)
Google (2018) 58.3
MSR (2018) 60.0

Table 3: Comparison to the best published results on
the CoNLL-2014 test set. ?indicates controlled repli-
cation under identical setup as ours. All CROSENT re-
sults are statistically significant compared to the corre-
sponding baselines (p < 0.001).

best published models trained using the same
training datasets: NUS1+2+3 (Chollampatt and
Ng, 2018b), G&J (Grundkiewicz and Junczys-
Dowmunt, 2018), JGGH (Junczys-Dowmunt
et al., 2018), and NUS1 (Chollampatt and Ng,
2018a). NUS1+2+3 is a 12-model ensemble of
three sets of models (4 each), with one set reused
from NUS1. We re-implement this ensemble
model (?NUS1+2+3) using identical preprocess-
ing and data used by our BASELINE, without
rescoring and spell checking. Our replicated re-
sults reach 52.49. We replace the weakest set of 4
models in NUS1+2+3, i.e., NUS1, in the ensemble
with our 4 BASELINE models instead. We reach a
result of 52.77. We then use our CROSENT mod-
els in place of BASELINE models and observe no-
table improvements of 2.1 F0.5 score. When we
use BERT feature and rescore, our improved base-
line ?NUS2+3+BASELINE (ens+rsBERT) achieves
55.47 F0.5 score, and using CROSENT models in-
stead of our BASELINE models achieves a result
of 57.16 (+1.69 F0.5). This result is better than
that of all prior competing models trained on the
same datasets5, of which three systems use a spell
checker from (Chollampatt and Ng, 2018a). When

5After the submission of this paper, improved results
on the CoNLL-2014 benchmark have been published (Zhao
et al., 2019; Lichtarge et al., 2019; Stahlberg et al., 2019).

Dev CoNLL-2013
F0.5 P R F0.5

BASELINE 33.21 54.51 15.16 35.88
concat 33.41 55.14 15.28 36.23
aux (no gate) 32.99 55.10 14.83 35.69
aux (+gate) 35.68 55.65 16.93 38.17

Table 4: Average single model results comparing dif-
ferent strategies to model cross-sentence context. ‘aux
(+gate)’ is used in our CROSENT model.

we add this spell checker in a similar way, our final
result reaches 57.30 F0.5.

With much larger high-quality annotated train-
ing datasets, a better result of 60.0 is achieved
by MSR (Ge et al., 2018). Recently, Google
(Lichtarge et al., 2018) published a higher result of
58.3 by an ensemble of big Transformers (Vaswani
et al., 2017) additionally trained on Wikipedia ed-
its (4.1 billion words). Such large-scale training is
very likely to further improve our model as well.
We leave it to future work to explore large-scale
cross-sentence GEC.

5 Analysis

We analyze our model choices on our development
data (5,006 sentences from NUCLE) and held-out
CoNLL-2013 test set. We have not used CoNLL-
2013 test set directly during training and hence,
it can be used to evaluate the generalization per-
formance of our models. We also analyze model
outputs on the CoNLL-2013 test set.

5.1 Modeling Cross-Sentence Context

We investigate different mechanisms of integrat-
ing cross-sentence context. Table 4 shows the av-
erage single model results of our sentence-level
BASELINE compared to two different strategies
of integrating cross-sentence context. ‘concat’
refers to simply prepending the previous source
sentences to the current source sentence. The con-
text and the current source sentence is separated
by a special token (<CONCAT>). This model does
not have an auxiliary encoder. ‘aux (no gate)’
uses an auxiliary encoder similar to our CROSENT
model except for gating. ‘aux (+gate)’ is our
CROSENT model (Section 2.2) which employs
the auxiliary encoder with the gating mechanism.
The first two variants perform comparably to our
sentence-level BASELINE and shows no notable
gains from using cross-sentence context. When



441

I agree that RFID technology shall not be made
available to the public for easy abuse and dis-
torted usage. First, our privacy is at threat.
Though tracking devices such as the applica-
tions in our smartphones these days can add fun
and entertainment to our fast-living paced liv-
ings, this can be a double-edged sword.We re-
vealed our locations, for our friends to catch
up with us, however we can never know who
is watching us out there secretly.
BASELINE: We revealed our locations, for our

friends to catch up with us, ...
CROSENT: We reveal our locations, for our

friends to catch up with us, however ...
REF: We reveal our locations, for our

friends to catch up with us, ...

Figure 3: Output showing a cross-sentence correction.

the gating mechanism is added, results improve
substantially. Using the gating mechanism is cru-
cial in our CROSENT model, as it has the ability to
selectively pass information through. This shows
that properly modeling cross-sentence context is
essential to improve overall performance.

5.2 Ability to Correct Cross-Sentence Errors

We show an example from the CoNLL-2013 test
set which involves a verb tense error that requires
cross-sentence context for correction. The origi-
nal sentence (with its context) and the corrections
made by our BASELINE and CROSENT ensemble
models are shown in Figure 3. The verb ‘revealed’
is to be corrected to its present tense form ‘reveal’
according to the annotated reference (REF). Our
CROSENT model corrects this verb tense error ac-
curately. However, our sentence-level BASELINE
is unable to make this correction as it only has ac-
cess to the sentence-level context.

To investigate if cross-sentence context is ade-
quately captured by our model, we adopt a larger
scale controlled evaluation. To do this, 795 well-
formed sentences are obtained from multiple doc-
uments in simple English Wikipedia along with
their previous sentences. We create a synthetic
dataset of verb tense errors, by corrupting all verbs
in these 795 sentences and replacing them by
their present tense form6, producing 1,090 syn-
thetic verb tense errors. These errors require cross-

6Adapted from https://github.com/bendichter/tenseflow

P / R / F0.5 nc / np
BASELINE 22.86 / 8.35 / 16.96 91 / 398
CROSENT 44.60 / 20.83 / 36.31 227 / 509

Table 5: Performance on contextual verb tense errors
on a synthetic test set. nc denotes the no. of correct
changes and np denotes the no. of proposed changes.

sentence context for correction7. An example is
shown below:

CONTEXT:
He got a flat tyre, and had to drive slowly back
to the pits. He finished the race in fifth place.
ORIGINAL:
His championship lead was reduced .
CORRUPTED:
His championship lead is reduced.

We analyze the performance of our cross-
sentence model on this dataset by passing the cor-
rupted sentences and their contexts as input. We
evaluate the ability of our model to correct the verb
tense errors and recover the original sentences.
The result of our BASELINE and CROSENT en-
semble models on this dataset is shown in Table 5.
In this dataset, we find a sharp increase in both pre-
cision and recall for our CROSENT model show-
ing their ability to capture cross-sentence context.
While the BASELINE accurately corrects 91 er-
rors, our CROSENT model accurately corrects 227
errors. The number of proposed corrections (np)
is also significantly higher for CROSENT. The re-
sults indicate that our cross-sentence model can
identify and correct a significant number of cross-
sentence errors.

5.3 Overall Performance across Error Types

We evaluate the overall performance on com-
mon error types (Figure 4) on the CoNLL-2013
dataset using ERRANT (Bryant et al., 2017).
They include determiner (DET), noun number
(NOUN:NUM), preposition (PREP), verb tense
(VERB:TENSE), and errors that are not catego-
rized (OTHERS). We find that the largest margins
of improvements are observed on verb tense errors
and determiner errors. This aligns with our expec-
tation of cross-sentence models.

7We keep the previous sentences in their actual form to
analyze the model’s ability in a controlled way. However, in
reality, previous sentences may also contain errors.



442

DET OTHER NOUN:NUM PREP VERB:TENSE
0.0

0.1

0.2

0.3

0.4

0.5
F 0

.5
BASELINE
CROSENT

Figure 4: Performance on common error types on the
CoNLL-2013 test set.

5.4 Attention on Auxiliary Context

We also analyze the output words that produce at-
tention distributions on the auxiliary context that
deviate the most from a uniform distribution, indi-
cating their dependence on cross-sentence context.
For each output word, we find the KL divergence
between its auxiliary attention distribution and a
uniform distribution on words in the auxiliary con-
text. We average the KL divergences for all in-
stances of this output word in the CoNLL-2013
test set. We then identify the top words for our
CROSENT ensemble model. The top words mostly
include nouns (such as criminal, chip, economy,
and individual). Manual analysis shows that many
of these nouns appear in consecutive sentences,
which results in higher attention placed on the
same words on the source side.

We conduct a more coarse-grained analysis to
find the top 5 part-of-speech tags of output words
(based on Universal tags) that produce the high-
est average KL divergences (Table 6). Nouns
and proper nouns (PROPN) attending to the same
word in previous source sentences cause them
to rank higher. Verbs and determiners (DET)
are also among the top tags since they often re-
quire cross-sentence context for disambiguation.
According to ERRANT (Section 5.3), compared
to our BASELINE ensemble, CROSENT ensemble
achieves notable improvements in performance on
verb tense errors (+7.8% F0.5) and on determiner
errors (+4.7%).

6 Related Work

Sophisticated sequence-to-sequence architectures
(Gehring et al., 2017; Vaswani et al., 2017) have
contributed to the progress of GEC research. Em-
ploying diverse ensembles (Chollampatt and Ng,

POS Avg.
KL Div.

NOUN 0.55
VERB 0.51
ADJ 0.50
DET 0.50

PROPN 0.47

Table 6: Top 5 part-of-speech (POS) tags based on av-
erage KL-divergence between auxiliary attention and
uniform distribution.

2018b), rescoring (Chollampatt and Ng, 2018a),
iterative decoding strategies (Ge et al., 2018;
Lichtarge et al., 2018), synthetic (Xie et al.,
2018) and semi-supervised corpora (Lichtarge
et al., 2018), and other task-specific techniques
(Junczys-Dowmunt et al., 2018) has achieved im-
pressive results on this task. However, all prior
work ignores document-wide context for GEC,
and uses sentence-level models. For spell check-
ing, Flor and Futagi (2012) used document-level
context to check if a candidate correction for a
misspelled word had been used earlier in the doc-
ument. Zheng et al. (2018) proposed splitting run-
on sentences into separate sentences. However,
they did not use cross-sentence context.

On the other hand, there are a number of studies
recently on integrating cross-sentence context for
neural machine translation (NMT). There are three
major approaches for document-level NMT: (1)
translating an extended source (context concate-
nated with the source) to a single or extended tar-
get (Tiedemann and Scherrer, 2017; Bawden et al.,
2018); (2) using an additional encoder to capture
document-wide context (Jean et al., 2017; Wang
et al., 2017; Bawden et al., 2018; Voita et al., 2018;
Miculicich et al., 2018; Zhang et al., 2018); and
(3) using discrete (Kuang et al., 2018) or contin-
uous cache (Tu et al., 2018; Maruf and Haffari,
2018) mechanisms during translation for storing
and retrieving document-level information. We
investigated the first two approaches in this pa-
per as we believe that most of the ambiguities
can be resolved by considering a few previous
sentences. Since GEC is a monolingual rewrit-
ing task, most of the disambiguating information
is in the source sentence itself, unlike bilingual
NMT. All approaches for document-level NMT
extended recurrent models or Transformer models.
There is no prior work that extends convolutional



443

sequence-to-sequence models for document-level
NMT.

The aim of this paper is to demonstrate the
necessity of modeling cross-sentence context for
GEC. It is beyond the scope of this paper to com-
prehensively evaluate all approaches to exploit
document-level context, and we leave it to future
work to evaluate more sophisticated models such
as memory networks to capture entire document-
level context or incorporate external knowledge
sources.

7 Conclusion

We present the first approach to cross-sentence
GEC, building on a convolutional encoder-
decoder architecture. Our cross-sentence mod-
els show significant gains over strong sentence-
level baselines. On the CoNLL-2014 benchmark
test set, when using larger ensembles and inte-
grating BERT during rescoring, our final cross-
sentence model achieves 57.30 F0.5 score which
is higher than all prior published F0.5 scores at the
time of paper submission, when trained using the
same datasets. We also demonstrate the ability of
our models to exploit wider contexts adequately
and correct errors on a synthetic test set of cross-
sentence verb tense errors.

References
Rachel Bawden, Rico Sennrich, Alexandra Birch, and

Barry Haddow. 2018. Evaluating discourse phe-
nomena in neural machine translation. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.

Christopher Bryant, Mariano Felice, and Ted Briscoe.
2017. Automatic annotation and evaluation of error
types for grammatical error correction. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics.

Shamil Chollampatt and Hwee Tou Ng. 2018a. A mul-
tilayer convolutional encoder-decoder neural net-
work for grammatical error correction. In Proceed-
ings of the 32nd AAAI Conference on Artificial In-
telligence.

Shamil Chollampatt and Hwee Tou Ng. 2018b. Neural
quality estimation of grammatical error correction.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing.

Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-

ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.

Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS corpus of learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications.

Yann N. Dauphin, Angela Fan, Michael Auli, and
David Grangier. 2017. Language modeling with
gated convolutional networks. In Proceedings of the
34th International Conference on Machine Learn-
ing.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Michael Flor and Yoko Futagi. 2012. On using context
for automatic correction of non-word misspellings
in student essays. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP.

Tao Ge, Furu Wei, and Ming Zhou. 2018. Reaching
human-level performance in automatic grammatical
error correction: An empirical study. arXiv preprint
arXiv:1807.01270.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convolu-
tional sequence to sequence learning. In Proceed-
ings of the 34th International Conference on Ma-
chine Learning.

Roman Grundkiewicz and Marcin Junczys-Dowmunt.
2018. Near human-level performance in grammati-
cal error correction with hybrid machine translation.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(Short Papers).

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.

Sebastien Jean, Stanislas Lauly, Orhan Firat, and
Kyunghyun Cho. 2017. Does neural machine trans-
lation benefit from larger context? arXiv preprint
arXiv:1704.05135.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Shubha Guha, and Kenneth Heafield. 2018. Ap-
proaching neural grammatical error correction as a



444

low-resource machine translation task. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.

Shaohui Kuang, Deyi Xiong, Weihua Luo, and
Guodong Zhou. 2018. Modeling coherence for
neural machine translation with dynamic and topic
caches. In Proceedings of the 27th International
Conference on Computational Linguistics.

Jindřich Libovický and Jindřich Helcl. 2017. Attention
strategies for multi-source sequence-to-sequence
learning. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Short Papers).

Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam
Shazeer, Niki Parmar, and Simon Tong. 2019. Cor-
pora generation for grammatical error correction. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Jared Lichtarge, Christopher Alberti, Shankar Kumar,
Noam Shazeer, and Niki Parmar. 2018. Weakly su-
pervised grammatical error correction using iterative
decoding. arXiv preprint arXiv:1811.01710.

Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations.

Sameen Maruf and Gholamreza Haffari. 2018. Docu-
ment context neural machine translation with mem-
ory networks. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics.

Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas,
and James Henderson. 2018. Document-level neural
machine translation with hierarchical attention net-
works. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing.

Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revi-
sion log of language learning SNS for automated
Japanese error correction of second language learn-
ers. In Proceedings of the Fifth International Joint
Conference on Natural Language Processing.

Courtney Napoles, Keisuke Sakaguchi, and Joel
Tetreault. 2017. JFLEG: A fluency corpus and
benchmark for grammatical error correction. In Pro-
ceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics (Short Papers).

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In Proceedings of
the 18th Conference on Computational Natural Lan-
guage Learning: Shared Task.

Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proceedings of the 17th Conference on Computa-
tional Natural Language Learning: Shared Task.

Felix Stahlberg, Christopher Bryant, and Bill Byrne.
2019. Neural grammatical error correction with fi-
nite state transducers. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the Inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition.

Jörg Tiedemann and Yves Scherrer. 2017. Neural ma-
chine translation with extended context. In Proceed-
ings of the Third Workshop on Discourse in Machine
Translation.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2018. Learning to remember translation his-
tory with a continuous cache. Transactions of the
Association for Computational Linguistics, 6:407–
420.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30.

Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan
Titov. 2018. Context-aware neural machine trans-
lation learns anaphora resolution. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics.

Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2017. Exploiting cross-sentence context for
neural machine translation. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew
Ng, and Dan Jurafsky. 2018. Noising and denoising
natural language: Diverse backtranslation for gram-
mar correction. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies.

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies.

Jiacheng Zhang, Huanbo Luan, Maosong Sun, and
Feifei Zhai. 2018. Improving the Transformer trans-
lation model with document-level context. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing.



445

Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and
Jingming Liu. 2019. Improving grammatical error
correction via pre-training a copy-augmented archi-
tecture with unlabeled data. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies.

Junchao Zheng, Courtney Napoles, Joel Tetreault, and
Kostiantyn Omelianchuk. 2018. How do you cor-
rect run-on sentences its not as easy as it seems. In
Proceedings of the 2018 EMNLP Workshop W-NUT:
The 4th Workshop on Noisy User-generated Text.


