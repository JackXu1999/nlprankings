



















































Higher-Order Syntactic Attention Network for Longer Sentence Compression


Proceedings of NAACL-HLT 2018, pages 1716–1726
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Higher-order Syntactic Attention Network for Long Sentence
Compression

Hidetaka Kamigaito♡, Katsuhiko Hayashi♢, Tsutomu Hirao♠ and Masaaki Nagata♠

♡ Institute of Innovative Research, Tokyo Institute of Technology
♢ Institute of Scientific and Industrial Research, Osaka University
♠ NTT Communication Science Laboratories, NTT Corporation

kamigaito@lr.pi.titech.ac.jp, katsuhiko-h@sanken.osaka-u.ac.jp,

{hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp

Abstract

Sentence compression methods based on
LSTM can generate fluent compressed
sentences. However, the performance of
these methods is significantly degraded
when compressing long sentences since it
does not explicitly handle syntactic fea-
tures. To solve this problem, we propose
a higher-order syntactic attention network
(HiSAN) that can handle higher-order de-
pendency features as an attention distri-
bution on LSTM hidden states. Further-
more, to avoid the influence of incorrect
parse results, we train HiSAN by max-
imizing the probability of a correct out-
put together with the attention distribu-
tion. Experiments on the Google sentence
compression dataset show that our method
achieved the best performance in terms of
F1 as well as ROUGE-1,2 and L scores,
83.2, 82.9, 75.8 and 82.7, respectively.
In subjective evaluations, HiSAN outper-
formed baseline methods in both readabil-
ity and informativeness.

1 Introduction

Sentence compression is the task of compress-
ing long sentences into short and concise ones by
deleting words. To generate compressed sentences
that are grammatical, many researchers (Jing,
2000; Knight and Marcu, 2000; Berg-Kirkpatrick
et al., 2011; Filippova and Altun, 2013) have
adopted tree trimming methods. Even though Fil-
ippova and Altun (2013) reported the best results
on this task, automatic parse errors greatly degrade
the performances of these tree trimming methods.

1We used an LSTM-based sentence compression method
(Filippova et al., 2015) in the evaluation setting as described
in Section 4.1.

11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 51-55
Sentence Length

60
65
70
75
80
85
90

F1
Figure 1: F1 scores of LSTM-based sentence com-
pression method for each sentence length.1

11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 51-55
Sentence Length

4
5
6
7
8
9

10
11
12

Av
er
ag

e 
De

pt
h

Figure 2: Average tree depths for each sentence
length.2

Recently, Filippova et al. (2015) proposed
an LSTM sequence-to-sequence (Seq2Seq) based
sentence compression method that can generate
fluent sentences without utilizing any syntactic
features. Therefore, Seq2Seq based sentence com-
pression is a promising alternative to tree trim-
ming.

However, as reported for a machine transla-
tion task (Cho et al., 2014; Pouget-Abadie et al.,
2014; Koehn and Knowles, 2017), the longer the
input sentences are, the worse the Seq2Seq per-
formances become. We also observed this prob-
lem in the sentence compression task. As shown
in Figure 1, the performance of Seq2Seq is de-
graded when compressing long sentences. In par-
ticular, the performance significantly falls if sen-
tence length exceeds 26 words. This is an impor-
tant problem, because sentences longer than the
average sentence length (=28 words) accounts for
42% of the Google sentence compression dataset.

As shown in Figure 2, long sentences have deep
2We treat the maximum distance from root node to the

leaf node as dependency tree depth.

1716



Pakistan signed a resolution on Monday to import 1,300 MW of electricity from Kyrgyz Republic and Tajikistan to overcome power shortage in summer season ...

Figure 3: An example compressed sentence and its dependency tree. The words colored by gray represent
deleted words.

dependency trees, which have long distances from
root node to words at leaf nodes. Therefore, im-
proving compression performance for sentences
with such deep dependency trees can help to com-
press longer sentences.

To deal with sentences that have deep depen-
dency trees, we focus on the chains of depen-
dency relationships. Figure 3 shows an exam-
ple of a compressed sentence with its dependency
tree. The topic of this sentence is import agree-
ment related to electricity. Thus, to generate in-
formative compression, the compressed sentence
must retain the country name. In this example,
the compressed sentence should keep the phrase
“from Kyrgyz Republic and Tajikistan”. Thus, the
compressed sentence must also keep the depen-
dency chain “import”, “resolution” and “signed”
because the phrase is a child of this chain. By
considering such higher-order dependency chains,
the system can implement informative compres-
sion. As can be seen from the example in Figure 3,
tracking a higher-order dependency chain for each
word would help to compress long sentences. This
paper refers to such dependency relationships by
the expression “d-length dependency chains”.

To handle a d-length dependency chain for sen-
tence compression with LSTM, we propose the
higher-order syntactic attention network (HiSAN).
HiSAN computes the deletion probability for a
given word based on the d-length dependency
chain starting from the word. The d-length de-
pendency chain is represented as an attention dis-
tribution, learned using automatic parse trees. To
alleviate the influence of parse errors in automatic
parse trees, we learn the attention distribution to-
gether with deletion probability.

Evaluation results on the Google sentence com-
pression dataset (Filippova and Altun, 2013) show
that HiSAN achieved the best F1, ROUGE-1,2 and
L scores 83.2, 82.9, 75.8 and 82.7, respectively. In
particular, HiSAN attained remarkable compres-
sion performance with long sentences. In human
evaluations, HiSAN also outperformed the base-
line methods.

2 Baseline Sequence-to-Sequence
Method

Sentence compression can be regarded as a tag-
ging task, where given a sequence of input tokens
x = (x0, ..., xn), a system assigns output label
yt, which is one of three types of specific labels
(“keep,”“delete,” or“end of sentence”) to each in-
put token xt (1 ≤ t ≤ n).

The LSTM-based approaches for sentence com-
pression are mostly based on the bi-LSTM based
tagging method (Tagger) (Klerke et al., 2016;
Wang et al., 2017; Chen and Pan, 2017) or
Seq2Seq (Filippova et al., 2015; Tran et al., 2016).
Tagger independently predicts labels in a point es-
timation manner, whereas Seq2Seq predicts labels
by considering previously predicted labels. Since
Seq2Seq is more expressive than Tagger, we built
HiSAN on the baseline Seq2Seq model.

Our baseline Seq2Seq is a version of Filip-
pova et al. (2015) extended through the addition
of bi-LSTM, an input feeding approach (Vinyals
et al., 2015; Luong et al., 2015), and a monotonic
hard attention method (Yao and Zweig, 2015; Tran
et al., 2016). As described in the evaluations sec-
tion, this baseline achieved comparable or even
better scores than the state-of-the-art scores re-
ported in Filippova et al. (2015). The baseline
Seq2Seq model consists of embedding, encoder,
decoder, and output layers.

In the embedding layer, the input tokens x are
converted to the embeddings e. As reported in
Wang et al. (2017), syntactic features are impor-
tant for learning a generalizable embedding for
sentence compression. Following their results, we
also introduce syntactic features into the embed-
ding layer. Specifically, we combine the surface
token embedding wi, POS embedding pi, and de-
pendency relation label embedding ri into a single
vector as follows:

ei = [wi, pi, ri] , (1)

where [] represents vector concatenation, and ei is
an embedding of token xi.

The encoder layer converts the embedding e
into a sequence of hidden states h = (h0, ..., hn)

1717



using a stacked bidirectional-LSTM (bi-LSTM) as
follows:

hi =
[−→
hi ,
←−
hi

]
(2)

−→
h i = LSTM−→θ (

−→
h i−1, ei) (3)

←−
h i = LSTM←−θ (

←−
h i−1, ei), (4)

where LSTM−→
θ

and LSTM←−
θ

represent forward
and backward LSTM, respectively. The final state
of the backward LSTM

←−
h 0 is inherited by the de-

coder as its initial state.
In the decoder layer, the concatenation of a 3-

bit one-hot vector which is determined by previ-
ously predicted label yt−1, previous final hidden
state dt−1 (explained later), and the input embed-
ding of xt, is encoded into the decoder hidden state−→s t using stacked forward LSTMs.

Contrary to the original softmax attention
method, we can deterministically focus on one en-
coder hidden state ht (Yao and Zweig, 2015) to
predict yt in the sentence compression task (Tran
et al., 2016).3

In the output layer, label probability is calcu-
lated as follows:

P (yt | y<t,x) = softmax(Wo · dt) · δyt , (5)
dt = [ht,

−→s t] (6)

where Wo is a weight matrix of the softmax layer
and δyt is a binary vector where the yt-th element
is set to 1 and the other elements to 0.

3 Higher-order Syntactic Attention
Network

The key component of HiSAN is its attention mod-
ule. Unlike the baseline Seq2Seq, HiSAN em-
ploys a packed d-length dependency chain as dis-
tributions in the attention module. Section 3.1 ex-
plains the packed d-length dependency chain. Sec-
tion 3.2 describes the network structure of our at-
tention module, and Section 3.3 explains the learn-
ing method of HiSAN.

3.1 Packed d-length Dependency Chain
The probability for a packed d-length depen-
dency chain is obtained from a dependency graph,
which is an edge-factored dependency score ma-
trix (Hashimoto and Tsuruoka, 2017; Zhang et al.,

3This is because the output length is the same as the input
length, and each xt can be assigned to each yt in a one-to-one
correspondence.

A B C D

Automatic parse tree

A B C D1.0
0.6

0.40.6
0.4

(a) Dependency
graph

A B C D1.0
0.6

0.40.6
0.4

A B C D

A B C D

(b) Packed 2-length
dependency chain
(for node D)

1.0
0.6

0.4
0.4

0.6 0.4
Dependency graph.

0.76 = 1.0 · 0.6 + 0.4 · 0.4

0.24 = 0.6 · 0.4

Parent Attention Module

Input sequence

Output sequence

A, B, C, D

1, 1, 0, 0

Output layer

1. .0 .0 .0
.4 .0 .3 .3
.3 .4 .0 .3
.3 .3 .4 .0

A
B
C
D

A B C D

1. .0 .0 .0
1. .0 .0 .0
.4 .6 .0 .0
.0 .6 .4 .0 =

Recursive Attention Module

Training data Training

A B C D

0.76

0.24

epoch:1 epoch:30

(A is a root node.)Attention distribution

Selective Attention Module

Figure 4: Dependency structures used in our
higher-order syntactic attention network.

2017). First, we explain the dependency graph.
Figure 4 (a) shows an example of the dependency
graph. HiSAN represents a dependency graph as
an attention distribution generated by the attention
module. A probability for each dependency edge
is obtained from the attention distribution.

Figure 4 (b) shows an example of the packed
d-length dependency chain. With our recursive
attention module, the probability for a packed d-
length dependency chain is computed as the sum
of probabilities for each path yielded by recur-
sively tracking from a word to its d-th ancestor.
The probability for each path is calculated as the
product of the probabilities of tracked edges. The
probability for the chain can represent several d-
length dependency chains compactly, and so alle-
viates the influence of incorrect parse results. This
is the advantage of using dependency graphs.

3.2 Network Architecture

Figure 5 shows the prediction process of HiSAN.
In this figure, HiSAN predicts output label y7
from the input sentence. The prediction process
of HiSAN is as follows.

1. Parent Attention module calculates
Pparent(xj |xt,x), the probability of xj being
the parent of xt, by using hj and ht. This
probability is calculated for all pairs of
xj , xt. The arc in Figure 5 shows the most
probable dependency parent for each child
token.

1718



x0 x1 x2 x3 x4 x5 x6 x7

h0 h1 h2 h3 h4 h5 h6 h7

Parent Attention

h1 h2 h3 h4 h5 h6 h7

Σ

Σ

Σ

Recursive Attention (d = {1, 2, 3})

d = 1

d = 2

d = 3

Σ

Selective
A

ttention

−→
h 7
←−
h 0
−→s 7

Ω7

y7

γ3,7

γ2,7

γ1,7

α1,2,7 · h2

α1,5,7 · h5

α1,6,7 · h6

β3,7 · γ3,7

β2,7 · γ2,7

β1,7 · γ1,7

Pparent

ROOT

h7

−→s 7
x7y6

softmax

y6

h6 Ω6

−→s 6
x6y5

softmax

y1

h1 Ω1

−→s 1
x1y0

softmax

←−
h 0

Figure 5: Prediction process of our higher-order
syntactic attention network.

2. Recursive Attention module calculates
αd,t,j , the probability of xj being the d-th
order parent (d denotes the chain length) of
xt, by recursively using Pparent(xj |xt,x).
αd,t,j is also treated as an attention distribu-
tion, and used to calculate γd,t, the weighted
sum of h for each length d. For example,
a 3-length dependency chain of word x7
with highest probability is x6-x5-x2. The
encoder hidden states h6, h5 and h2, which
correspond to the dependency chain, are
weighted by calculated parent probabilities
α1,7,6, α2,7,5 and α3,7,2, respectively, and
then fed to the selective attention module.

3. Selective Attention module calculates
weight βd,t from its length, d ∈ d, for each
γd,t. d represents a group of chain lengths.
βd,t is calculated by encoder and decoder
hidden states. Each βd,t · γd,t is summed
to Ωt, the output of the selective attention
module.

4. Finally, the calculated Ωt is concatenated and
input to the output layer.

Details of each module are explained in the fol-
lowing subsection.

3.2.1 Parent Attention Module
Zhang et al. (2017) formalized dependency pars-
ing as the problem of independently selecting the
parent of each word in a sentence. They produced
a distribution over possible parents for each child
word by using the attention layer on bi-LSTM hid-
den layers.

In a dependency tree, a parent has more than
one child. Under this constraint, dependency pars-
ing is represented as follows. Given sentence
S = (x0, x1, ..., xn), the parent of xj is selected
from S \ xi for each token S \ x0. Note that x0
denotes the root node. The probability of token
xj being the parent of token xt in sentence x is
calculated as follows:

Pparent(xj |xt,x)=softmax(g(hj′ , ht)) · δxj ,(7)
g(hj′ , ht)=v

T
a · tanh(Ua · hj′ + Wa · ht), (8)

where va, Ua and Wa are weight matrices of g.
Different from the attention based dependency

parser, Pparent(xj |xt,x) is jointly learned with
output label probability P (y | x) in the training
phase. Training details are given in Section 3.3.

3.2.2 Recursive Attention Module
The recursive attention module recursively calcu-
lates αd,t,j , the probability of xj being the d-th or-
der parent of xt, as follows:

αd,t,j =





n∑
k=1

αd−1,t,k · α1,k,j (d>1)
Pparent(xj |xt,x) (d=1)

. (9)

Furthermore, in a dependency parse tree, root
should not have any parent, and a token should not
depend on itself. In order to satisfy these rules, we
impose the following constraints on α1,t,j :

α1,t,j =





1 (t = 0 ∧ j = 0)
0 (t = 0 ∧ j > 0)
0 (t ̸= 0 ∧ t = j)

(10)

The 1st and 2nd lines of Eq. (10) represent the
case that the parent of root is also root. These con-
straints imply that root does not have any parent.
The 3rd line of Eq. (10) prevents a token from de-
pending on itself. Because the 1st line of Eq. (9)
is similar to the definition of matrix multiplication,
Eq. (9) can be efficiently computed on a CPU and
GPU4.

4In training, HiSAN with 1- and 3-length dependency
chains took 25 and 26 minutes, respectively, per epoch on
an Intel Xeon E5-2697 v3 2.60 GHz.

1719



By recursively using the single attention distri-
bution, it is no longer necessary to prepare addi-
tional attention distributions for each order when
computing the probability of higher order par-
ents. Furthermore, since it is not necessary to
learn multiple attention distributions, it becomes
unnecessary to use hyper parameters for adjust-
ing the weight of each distribution in training. Fi-
nally, this method can avoid the problem of sparse
higher-order dependency relations in the training
dataset.

The above calculated αd,t,j is used to weight the
bi-LSTM hidden layer h as follows:

γd,t =

n∑

k=j

αd,t,j · hj . (11)

Note that γd,t is inherited by the selective attention
module, as explained in the next section.

3.2.3 Selective Attention Module
To select suitable dependency orders of the input
sentence, the selective attention module weights
and sums the hidden states γd,t to Ωt by using
weighting parameter βd,t, according to the current
context as follows:

βd,t = softmax(Wc · ct) · δd, (12)
Ωt =

∑

d∈{0,d}
βd,t · γd,t, (13)

where Wc is the weight matrix of the softmax
layer, d is a group of chain lengths, ct is a
vector representing the current context, γ0,t is a
zero-vector, and β0,t indicates the weight when
the method does not use the dependency fea-
tures. Context vector ct is calculated as ct =
[
←−
h 0,
−→
h n,
−→s t] using the current decoder hidden

state −→s t.
The calculated Ωt is concatenated and input to

the output layer. In detail, dt in Eq. (5) is replaced
by concatenated vector d′t = [ht, Ωt,

−→s t]; further-
more, instead of dt, d′t is also fed to the input of
the decoder LSTM at t + 1.

3.3 Objective Function

To alleviate the influence of parse errors, we
jointly update the 1st-order attention distribution
α1,t,k and label probability P (y|x) (Kamigaito
et al., 2017). The 1st-order attention distribution
is learned by dependency parse trees. If at,j = 1
is an edge between parent word wj and child wt

on a dependency tree (at,j = 0 denotes that wj is
not a parent of wt.), the objective function of our
method can be defined as:

−logP (y|x)− λ ·
n∑

j=1

n∑

t=1

at,j · logα1,t,j , (14)

where λ is a hyper-parameter that controls the im-
portance of the output labels and parse trees in the
training dataset.

4 Evaluations

4.1 Evaluation Settings

4.1.1 Dataset
This evaluation used the Google sentence com-
pression dataset (Filippova and Altun, 2013)5.
This dataset contains information of compression
labels, part-of-speech (POS) tags, dependency
parents and dependency relation labels for each
sentence.

We used the first and last 1,000 sentences of
comp-data.eval.json as our test and devel-
opment datasets, respectively. Note that our test
dataset is compatible wth that used in previous
studies (Filippova et al., 2015; Tran et al., 2016;
Klerke et al., 2016; Wang et al., 2017).

In this paper, we trained the following
baselines and HiSAN on all sentences of
sent-comp.train*.json (total 200,000
sentences)6,7,8.

In our experiments, we replaced rare words that
appear fewer than 10 times in our training dataset
with a special token ⟨UNK⟩. After this filtering,
the input vocabulary size was 23, 168.

4.1.2 Baseline Methods
For a fair comparison of HiSAN, we used the in-
put features described in Eq. (1) for the following
baseline methods:

5https://github.com/google-research-datasets/sentence-
compression

6Note that Filippova et al. (2015) used 2,000,000 sen-
tences for training their method, but these datasets are not
publicly available.

7We also demonstrate an experimental evaluation on a
small training set (total 8,000 sentences), that was used in
previous research. The results of this setting are listed in our
supplemental material.

8Note that the large training dataset lacks periods at the
end of compressed sentences. To unify the form of com-
pressed sentences in small and large settings, we added pe-
riods to the end of compressed sentences in the large training
dataset.

1720



Tagger: A method that regards sentence compres-
sion as a tagging task based on bi-LSTM (Klerke
et al., 2016; Wang et al., 2017).

Tagger+ILP: An extension of Tagger that inte-
grates ILP (Integer Linear Programming)-based
dependency tree trimming (Wang et al., 2017). We
set their positive parameter λ to 0.2.

Bi-LSTM: A method that regards sentence com-
pression as a sequence-to-sequence translation
task proposed by (Filippova et al., 2015). For a
fair comparison, we replaced their one-directional
LSTM with the more expressive bi-LSTM in the
encoder part. The initial state of the decoder is set
to the sum of the final states of the forward and
backward LSTMs.

Bi-LSTM-Dep: An extension of Bi-LSTM that
exploits features obtained from a dependency
tree (named LSTM-PAR-PRES in Filippova et al.
(2015)). Following their work, we fed the word
embedding and the predicted label of a depen-
dency parent word to the current decoder input of
Bi-LSTM.

Base: Our baseline Seq2Seq method described in
Section 2.

Attn: An extension of the softmax based atten-
tion method (Luong et al., 2015). We replaced ht
in Eq. (6) with the weighted sum calculated by
the commonly used concat attention (Luong et al.,
2015).

HiSAN-Dep: A variant of HiSAN that utilizes the
pipeline approach. We fix α1,j,t to 1.0 if xj is a
parent of xt in the input dependency parse tree,
0.0 otherwise. In this baseline, d = {1} was used.

4.1.3 Training Details
Following the previous work (Wang et al., 2017),
the dimensions of the word embeddings, LSTM
layers, and attention layer were set to 100. For
the Tagger-style methods, the depth of the LSTM
layer was set to 3, and for the Seq2Seq-style meth-
ods, the depth of the LSTM layer was set to 2.
In this setting, all methods have a total of six
LSTM-layers. The dimensions of POS and the
dependency-relation label embeddings were set to
40. All parameters were initialized by Glorot and
Bengio (2010)’s method. For all methods, we ap-
plied Dropout (Srivastava et al., 2014) to the input
of the LSTM layers. All dropout rates were set to
0.3.

During training, the learning rate was tuned
with Adam (Kingma and Ba, 2014). The initial
learning rate was set to 0.001. The maximum
number of training epochs was set to 30. The
hyper-parameter λ was set to 1.0 in the supervised
attention setting. All gradients were averaged in
each mini-batch. The maximum mini-batch size
was set to 16. The order of mini-batches was shuf-
fled at the end of each training epoch. The clip-
ping threshold of the gradient was set to 5.0. We
selected trained models with early stopping based
on maximizing per-sentence accuracy (i.e., how
many compressions could be fully reproduced) of
the development data set.

To obtain a compressed sentence, we used
greedy decoding, rather than beam decoding, as
the latter attained no gain in the development
dataset. All methods were written in C++ on
Dynet (Neubig et al., 2017).

4.2 Automatic Evaluation
In the automatic evaluation, we used token
level F1-measure (F1) as well as recall of
ROUGE-1, ROUGE-2 and ROUGE-L (Lin and
Och, 2004)9 as evaluation measures. We
used ∆C = system compression ratio −
gold compression ratio to evaluate how close
the compression ratio of system outputs was to
that of gold compressed sentences. The average
compression ratio of the gold compression for in-
put sentence was 39.8. We used micro-average for
F1-measure and compression ratio10, and macro-
average for ROUGE scores, respectively.

To verify the benefits of our methods on long
sentences, we additionally report scores on sen-
tences longer than the average sentence length
(= 28) in the test set. The average compression
ratio of the gold compression for longer input sen-
tences was 31.4.

All results are reported as the average scores of
five trials. In each trial, different random choices
were used to generate the initial values of the em-
beddings and the order of mini-batch processing.

Table 1 shows the results. HiSANs outper-
formed the other methods. In particular, HiSAN
(d = {1, 2, 4}) achieved the best score on F1,

9We used the ROUGE-1.5.5 script with option “-n 2 -m -d
-a”.

10We also report the macro-average of F1-measure and
compression ratio in our supplemental material.

11Note that we used average of all metrics to decide the
best score of the development dataset. The results are listed
in our supplemental material.

1721



ALL LONG
F1 ROUGE ∆C F1 ROUGE ∆C

1 2 L 1 2 L

Tagger 82.8 81.1 72.4 80.9 -3.0 80.4 78.7 69.7 78.4 -2.8
Tagger+ILP 79.0 76.1 64.6 75.8 -4.1 74.3 73.7 62.1 73.2 -3.6

Bi-LSTM 81.9 81.1 73.7 80.9 -2.2 78.9 78.4 70.4 78.0 -2.1
Bi-LSTM-Dep 82.3 81.5 74.1 81.3 -2.1 79.6 78.9 71.0 78.5 -1.9
Attn 82.4 81.6 74.3 81.4 -2.3 79.8 79.4 71.4 78.7 -2.2
Base 82.7 81.9 74.7 81.7 -2.4 80.1 79.1 71.7 79.0 -2.3
HiSAN-Dep (d = {1}) 82.7 82.1 74.9 81.9 -2.2 80.1 79.7 72.0 79.3 -1.9

(d = {1, 2}) 82.7 81.9 74.6 81.7 -2.4 80.5 79.9 72.3 79.5 -2.2
HiSAN-Dep (d = {1, 2, 3}) 83.1 82.0 74.9 81.8 -2.5 80.5 79.5 71.9 79.2 -2.3

(d = {1, 2, 4})∗ 82.9 82.1 74.9 81.9 -2.4 80.4 79.7 72.1 79.4 -2.1
(d = {1, 2, 3, 4}) 82.7 82.1 74.8 81.9 -2.2 80.1 79.6 71.9 79.3 -2.0
(d = {1}) 83.0 81.7 74.5 81.5 -2.9 80.6 79.6 72.1 79.3 -2.5
(d = {1, 2}) 83.1 82.3 75.1 82.2 -2.1 80.9 80.5 72.8 80.2 -1.9

HiSAN (d = {1, 2, 3}) 82.9 82.5 75.2 82.1 -2.1 80.7 80.2 72.6 79.9 -2.1
(d = {1, 2, 4})∗ 83.2 82.9 75.8 82.7 −1.7 80.9 80.6 73.2 80.3 −1.8
(d = {1, 2, 3, 4}) 82.7 82.0 74.7 81.8 -2.3 80.6 79.8 72.6 79.5 -2.3

Table 1: Results of automatic evaluation. ALL and LONG represent, respectively, the results in all
sentences and long sentences (longer than average length 28) in the test dataset. d represents the groups
of d-length dependency chains. ∗ indicates the model that achieved the best score among the same
methods with different d in the development dataset11. Bold values indicate the best scores.

ROUGE, and ∆C in all settings. The F1 scores of
HiSAN (ALL) were higher than the current state-
of-the-art score of .82, reported by Filippova et al.
(2015). The improvements in F1 and ROUGE
scores from the baselines methods in the LONG
setting are larger than those in the ALL setting.
¿From these results, we can conclude that d-length
dependency chains are effective for sentence com-
pression, especially in the case of longer than av-
erage sentences. HiSAN (d = {1}) outperformed
HiSAN-Dep in F1 scores in ALL and LONG set-
tings. This result shows the effectiveness of joint
learning the dependency parse tree and the output
labels.

4.3 Human Evaluation

In the human evaluation, we compared the base-
lines with our method, which achieved the highest
F1 score in the automatic evaluations. We used the
first 100 sentences that were longer than the aver-
age sentence length (= 28) in the test set for hu-
man evaluation. Similar to Filippova et al. (2015),
the compressed sentence was rated by five raters
who were asked to select a rating on a five-point
Likert scale, ranging from one to five for read-
ability (Read) and for informativeness (Info). We
report the average of these scores from the five
raters. To investigate the differences between the
methods, we also compared the baseline meth-

Read Info CR

All Tagger 4.54 3.41 30.9
(100) Base 4.64 3.45 31.1

HiSAN (d={1, 2, 4}) 4.68 3.52 31.6
Diff Base 4.79 3.46 29.4
(41) HiSAN (d={1, 2, 4}) 4.89 3.64 30.6

Table 2: Results of human evaluations. All de-
notes results for all sentences in the test set, and
Diff denotes results for the sentences for which the
methods yielded different compressed sentences.
Parentheses ( ) denote sentence size. CR denotes
the compression ratio. The average gold compres-
sion ratio for input sentence in All and Diff were
32.1 and 31.5, respectively. Other notations are
similar to those in Table 1.

ods and HiSAN using the sentences for which the
methods yielded different compressed sentences.

Table 2 shows the results. HiSAN (d =
{1, 2, 4}) achieved better results than the baselines
in terms of both readability and informativeness.
The results agree with those obtained from the au-
tomatic evaluations. ¿From the results on the sen-
tences whose compressed sentences were differ-
ent between Base and HiSAN (d = {1, 2, 4}), we
can clearly observe the improvement attained by
HiSAN (d={1, 2, 4}) in informativeness.

1722



Input Pakistan signed a resolution on Monday to import 1,300 MW of electricity from Kyrgyz Republic
and Tajikistan to overcome power shortage in summer season, said an official press release .

Gold Pakistan signed a resolution to import 1,300 MW of electricity from Kyrgyz Republic and Tajik-
istan .

Tagger Pakistan signed a resolution to import 1,300 MW of electricity Tajikistan to overcome shortage .
Tagger-ILP Pakistan signed resolution to import MW said .

Base Pakistan signed a resolution to import 1,300 MW of electricity .
HiSAN-Dep (d={1}) Pakistan signed a resolution to import 1,300 MW of electricity .
HiSAN (d={1, 2, 4}) Pakistan signed a resolution to import 1,300 MW of electricity from Kyrgyz Republic and Tajik-

istan .
Input US whistleblower Bradley Manning , charged with releasing over 700,000 battlefield reports from

Iraq and Afghanistan to Wikileaks , received a sentence of 35 years in prison from a military court
Wednesday .

Gold Bradley Manning received a sentence of 35 years in prison .

Tagger Bradley Manning received a sentence of 35 years .
Tagger-ILP Bradley Manning received a sentence of years .

Base Bradley Manning received a sentence of 35 years .
HiSAN-Dep (d={1}) Bradley Manning charged with releasing over 700,000 battlefield reports to Wikileaks received .
HiSAN (d={1, 2, 4}) Bradley Manning received a sentence of 35 years in prison .

Table 3: Example sentences and compressions.

5 Analysis

Table 3 shows examples of source sentences and
their compressed variants output by baseline and
HiSAN (d = {1, 2, 4}).

For both examples, the compressed sentence
output by Base is grammatically correct. How-
ever, the informativeness is inferior to that attained
by HiSAN (d = {1, 2, 4}). The compressed sen-
tence output by HiSAN-Dep in the second exam-
ple lacks both readability and informativeness. We
believe that this compression failure is caused by
incorrect parse results, because HiSAN-Dep em-
ploys the features obtained from the dependency
tree in the pipeline procedure.

As reported in recent papers (Klerke et al.,
2016; Wang et al., 2017), the F1 scores of Tagger
match or exceed those of the Seq2Seq-based meth-
ods. The compressed sentence of the first example
in Table 3 output by Tagger is ungrammatical. We
believe that this is mainly because Tagger can-
not consider the predicted labels of the previous
words. Tagger-ILP outputs grammatically incor-
rect compressed sentences in both examples. This
result indicates that THE ILP constraint based on
the parent-child relationships between words is in-
sufficient to generate fluent sentences.

Compared with these baselines, HiSAN (d =
{1, 2, 4}) output compressed sentences that were
fluent and had higher informativeness. This obser-
vation, which confirmed our expectations, is sup-

ported by the automatic and human evaluation re-
sults.

F1 ROUGE ∆C

1 2 L

Tagger 82.8 80.6 72.2 80.3 -3.2
Tagger+ILP 77.5 74.7 64.1 74.3 -4.6

Bi-LSTM 81.3 80.4 73.3 80.1 -2.2
Bi-LSTM-Dep 81.5 80.7 73.5 80.3 -2.1
Attn 81.9 81.0 73.9 80.6 -2.3
Base 82.1 81.0 73.9 80.7 -2.5

HiSAN-Dep
d = {1} 82.3 81.2 74.3 80.9 -2.4
d = {1, 2} 81.9 80.8 73.8 80.4 -2.6
d = {1, 2, 3} 82.6 81.2 74.3 80.9 -2.6
d = {1, 2, 4} 82.0 80.7 73.7 80.4 -2.7
d = {1, 2, 3, 4} 82.1 81.0 74.1 80.7 -2.5
HiSAN
d = {1} 82.7 81.4 74.5 81.1 -2.8
d = {1, 2} 82.6 81.8 74.9 81.5 -2.1
d = {1, 2, 3} 82.6 81.8 74.9 81.5 -2.3
d = {1, 2, 4} 82.8 82.2 75.5 82.0 −2.0
d = {1, 2, 3, 4} 82.4 81.3 74.4 81.0 -2.4

Table 4: Results of automatic evaluation using
sentences with deep dependency trees (deeper than
average depth 8). Bold results indicate the best
scores.

We confirm that the compression performance
of HiSAN actually improves if the sentences have
deep dependency trees. Table 4 shows the auto-
matic evaluation results for sentences with deep
dependency trees. We can observe that HiSAN

1723



Pakistan signed a resolution on Monday to import 1,300 MW of electricity from Kyrgyz Republic and Tajikistan to overcome power shortage in summer season ...

1.0 1.0

1.0

0.1

0.9 1.0 1.0

0.08

0.91

0.01

1.0

1.0

1.0

0.02

0.47

0.16

0.33

1.0 1.0

1.0

1.0

1.0

1.0

0.30

0.01

0.69

1.0

1.0

1.0 1.0

1.0

Figure 6: An example compressed sentence and its dependency graph of HiSAN d = {1, 2, 4}. The
words colored by gray represent deleted words. The numbers for each arc represent a probabilistic
weight of a relationship between a parent and child words. The arcs contained in the parsed dependency
tree are located on the top side. The arcs not contained in the parsed dependency tree are located on the
bottom side.

with higher-order dependency chains has better
compression performance if the sentences have
deep dependency trees.

Figure 6 shows a compressed sentence and its
dependency graph as determined by HiSAN d =
{1, 2, 4}. Almost all arcs with large probabilis-
tic weights are contained in the parsed dependency
trees. Interestingly, some arcs not contained in the
parsed dependency trees connecting words which
are connected by the dependency chains in the
parsed dependency tree (colored by red). Consid-
ering the training dataset does not contain such
dependency relationships, we can estimate that
these arcs are learned in support of compressing
sentences. This result meets our expectation that
the dependency chain information is necessary for
compressing sentences accurately.

6 Related Work

Several neural network based methods for sen-
tence compression use syntactic features. Filip-
pova et al. (2015) employs the features obtained
from automatic parse trees in the LSTM-based
encoder-decoder in a pipeline manner. Wang et al.
(2017) trims dependency trees based on the scores
predicted by an LSTM-based tagger. Although
these methods can consider dependency relation-
ships between words, the pipeline approach and
the 1st-order dependency relationship fail to com-
press longer than average sentences.

Several recent machine translation studies also
utilize syntactic features in Seq2Seq models.
Eriguchi et al. (2017); Aharoni and Goldberg
(2017) incorporate syntactic features of the target
language in the decoder part of Seq2Seq. Both
methods outperformed Seq2Seq without syntac-
tic features in terms of translation quality. How-
ever, both methods fail to provide an entire parse
tree until the decoding phase is finished. Thus,

these methods cannot track all possible parents for
each word within the decoding process. Similar
to HiSAN, Hashimoto and Tsuruoka (2017) use
dependency features as attention distributions, but
different from HiSAN, they use pre-trained depen-
dency relations, and do not take into account the
chains of dependencies. Marcheggiani and Titov
(2017); Bastings et al. (2017) consider higher-
order dependency relationships in Seq2Seq by in-
corporating a graph convolution technique (Kipf
and Welling, 2016) into the encoder. However, the
dependency information of the graph convolution
technique is still given in pipeline manner.

Unlike the above methods, HiSAN can capture
higher-order dependency features using d-length
dependency chains without relying on pipeline
processing.

7 Conclusion

In this paper, we incorporated higher-order de-
pendency features into Seq2Seq to compress sen-
tences of all lengths.

Experiments on the Google sentence compres-
sion test data showed that our higher-order syn-
tactic attention network (HiSAN) achieved the
better performance than baseline methods on F1
as well as ROUGE-1,2 and L scores 83.2, 82.9,
75.8 and 82.7, respectively. Of particular impor-
tance, challenged with longer than average sen-
tences, HiSAN outperformed the baseline meth-
ods in terms of F1, ROUGE-1,2 and L scores. Fur-
thermore, HiSAN also outperformed the previous
methods for both readability and informativeness
in human evaluations.

From the evaluation results, we conclude that
HiSAN is an effective tool for the sentence com-
pression task.

1724



References
Roee Aharoni and Yoav Goldberg. 2017. Towards

string-to-tree neural machine translation. In Pro-
ceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
2: Short Papers). Association for Computational
Linguistics, Vancouver, Canada, pages 132–140.
http://aclweb.org/anthology/P17-2021.

Joost Bastings, Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Simaan. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Lin-
guistics, Copenhagen, Denmark, pages 1957–1967.
https://www.aclweb.org/anthology/D17-1209.

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, Portland, Oregon, USA, pages 481–
490. http://www.aclweb.org/anthology/P11-1049.

Yanju Chen and Rong Pan. 2017. Automatic emphatic
information extraction from aligned acoustic data
and its application on sentence compression. In Pro-
ceedings of the Thirty-First AAAI Conference on Ar-
tificial Intelligence, February 4-9, 2017, San Fran-
cisco, California, USA.. pages 3422–3428.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the proper-
ties of neural machine translation: Encoder–decoder
approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in
Statistical Translation. Association for Computa-
tional Linguistics, Doha, Qatar, pages 103–111.
http://www.aclweb.org/anthology/W14-4012.

Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun
Cho. 2017. Learning to parse and translate
improves neural machine translation. In Pro-
ceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
2: Short Papers). Association for Computational
Linguistics, Vancouver, Canada, pages 72–78.
http://aclweb.org/anthology/P17-2012.

Katja Filippova, Enrique Alfonseca, Carlos A.
Colmenares, Lukasz Kaiser, and Oriol Vinyals.
2015. Sentence compression by deletion with
lstms. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 360–368.
http://aclweb.org/anthology/D15-1042.

Katja Filippova and Yasemin Altun. 2013. Over-
coming the lack of parallel data in sentence com-
pression. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language

Processing. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 1481–1491.
http://www.aclweb.org/anthology/D13-1155.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the Thirteenth In-
ternational Conference on Artificial Intelligence and
Statistics. pages 249–256.

Kazuma Hashimoto and Yoshimasa Tsuruoka. 2017.
Neural machine translation with source-side latent
graph parsing. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Copenhagen, Denmark, pages 125–135.
https://www.aclweb.org/anthology/D17-1012.

Hongyan Jing. 2000. Sentence reduction for au-
tomatic text summarization. In Proceedings of
the Sixth Conference on Applied Natural Language
Processing. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 310–315.
https://doi.org/10.3115/974147.974190.

Hidetaka Kamigaito, Katsuhiko Hayashi, Tsutomu
Hirao, Hiroya Takamura, Manabu Okumura, and
Masaaki Nagata. 2017. Supervised attention for
sequence-to-sequence constituency parsing. In Pro-
ceedings of the Eighth International Joint Con-
ference on Natural Language Processing (Volume
2: Short Papers). Asian Federation of Natural
Language Processing, Taipei, Taiwan, pages 7–12.
http://www.aclweb.org/anthology/I17-2002.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.

Thomas N. Kipf and Max Welling. 2016. Semi-
supervised classification with graph convo-
lutional networks. CoRR abs/1609.02907.
http://arxiv.org/abs/1609.02907.

Sigrid Klerke, Yoav Goldberg, and Anders Søgaard.
2016. Improving sentence compression by learning
to predict gaze. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 1528–
1533. http://www.aclweb.org/anthology/N16-1179.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. AAAI/IAAI 2000:703–710.

Philipp Koehn and Rebecca Knowles. 2017. Six
challenges for neural machine translation. In
Proceedings of the First Workshop on Neu-
ral Machine Translation. Association for Com-
putational Linguistics, Vancouver, pages 28–39.
http://www.aclweb.org/anthology/W17-3204.

1725



Chin-Yew Lin and Franz Josef Och. 2004. Au-
tomatic evaluation of machine translation
quality using longest common subsequence
and skip-bigram statistics. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main
Volume. Barcelona, Spain, pages 605–612.
https://doi.org/10.3115/1218955.1219032.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1412–
1421. http://aclweb.org/anthology/D15-1166.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for
semantic role labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Copenhagen, Denmark, pages 1506–
1515. https://www.aclweb.org/anthology/D17-
1159.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, et al. 2017. Dynet: The
dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .

J. Pouget-Abadie, D. Bahdanau, B. van Merrien-
boer, K. Cho, and Y. Bengio. 2014. Overcoming
the Curse of Sentence Length for Neural Machine
Translation using Automatic Segmentation. ArXiv
e-prints .

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research 15(1):1929–1958.

Nhi-Thao Tran, Viet-Thang Luong, Ngan Luu-Thuy
Nguyen, and Minh-Quoc Nghiem. 2016. Effec-
tive attention-based neural architectures for sentence
compression with bidirectional long short-term
memory. In Proceedings of the Seventh Symposium
on Information and Communication Technology.
ACM, New York, NY, USA, SoICT ’16, pages 123–
130. https://doi.org/10.1145/3011077.3011111.

Oriol Vinyals, Ł ukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, Curran Associates, Inc., pages 2773–
2781. http://papers.nips.cc/paper/5635-grammar-
as-a-foreign-language.pdf.

Liangguo Wang, Jing Jiang, Hai Leong Chieu,
Chen Hui Ong, Dandan Song, and Lejian Liao.

2017. Can syntax help? improving an lstm-based
sentence compression model for new domains. In
Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Vancouver, Canada, pages 1385–1393.
http://aclweb.org/anthology/P17-1127.

Kaisheng Yao and Geoffrey Zweig. 2015. Sequence-
to-sequence neural net models for grapheme-
to-phoneme conversion. In INTERSPEECH
2015, 16th Annual Conference of the In-
ternational Speech Communication Associ-
ation, Dresden, Germany, September 6-10,
2015. pages 3330–3334. http://www.isca-
speech.org/archive/interspeech 2015/i15 3330.html.

Xingxing Zhang, Jianpeng Cheng, and Mirella Lapata.
2017. Dependency parsing as head selection. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 1, Long Papers. Association for
Computational Linguistics, Valencia, Spain, pages
665–676. http://www.aclweb.org/anthology/E17-
1063.

1726


