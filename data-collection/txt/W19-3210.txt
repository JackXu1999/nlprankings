



















































Correlating Twitter Language with Community-Level Health Outcomes


Proceedings of the 4th Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task, pages 71–78
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

71

Correlating Twitter Language with Community-Level Health Outcomes

Arno Schneuwly
EPFL

arno.schneuwly@epfl.ch

Ralf Grubenmann
SpinningBytes

rg@spinningbytes.com

Séverine Rion Logean
Swiss Re

severine rion@swissre.com

Mark Cieliebak
ZHAW

ciel@zhaw.ch

Martin Jaggi
EPFL

martin.jaggi@epfl.ch

Abstract

We study how language on social media is
linked to diseases such as atherosclerotic heart
disease (AHD), diabetes and various types of
cancer. Our proposed model leverages state-
of-the-art sentence embeddings, followed by a
regression model and clustering, without the
need of additional labelled data. It allows
to predict community-level medical outcomes
from language, and thereby potentially trans-
late these to the individual level. The method
is applicable to a wide range of target vari-
ables and allows us to discover known and
potentially novel correlations of medical out-
comes with life-style aspects and other socioe-
conomic risk factors.

1 Introduction

Surveys and empirical studies have long been
a cornerstone of psychological, sociological and
medical research, but each of these traditional
methods pose challenges for researchers. They are
time-consuming, costly, may introduce a bias or
suffer from bad experiment design.

With the advent of big data and the increasing
popularity of the internet and social media, larger
amounts of data are now available to researchers
than ever before. This offers strong promise new
avenues of research using analytic procedures, ob-
taining a more fine-grained and at the same time
broader picture of communities and populations as
a whole (Salathé, 2018). Such methods allow for
faster and more automated investigation of demo-
graphic variables. It has been shown that Twitter
data can predict atherosclerotic heart-disease risk
at the community level more accurately than tradi-
tional demographic data (Eichstaedt et al., 2015).
The same method has also been used to capture
and accurately predict patterns of excessive alco-
hol consumption (Curtis et al., 2018).

In this study, we utilize Twitter data to pre-
dict various health target variables (AHD, dia-
betes, various types of cancers) to see how well
language patterns on social media reflect the geo-
graphic variations of those targets. Furthermore,
we propose a new method to study social media
content by characterizing disease-related correla-
tions of language, by leveraging available demo-
graphic and disease information on the commu-
nity level. In contrast to (Eichstaedt et al., 2015),
our method is not relying on word-based topic
models, but instead leverages modern state-of-the-
art text representation methods, in particular sen-
tence embeddings, which have been in increas-
ing use in the Natural Language Processing, In-
formation Retrieval and Text Analytics fields in
the past years. We demonstrate that our approach
helps capturing the semantic meaning of tweets
as opposed to features merely based on word fre-
quencies, which come with robustness problems
(Brown and Coyne, 2018; Schwartz et al., 2018).
We examine the effectiveness of sentence embed-
dings in modeling language correlates of the med-
ical target variables (disease outcome).

Section 2 gives a generalized description of
our method. We apply the previously described
method to the tweets and health data in Section 3
The system’s performance is evaluated in Sec-
tion 4 followed by the discussion in Section 5. Our
code is available on github.com/epfml/correlating-
tweets.

2 Method

We are given a large quantity of text (sentences or
tweets) in the form of social media messages by
individuals. Each individual—and therefore each
sentence—is assigned to a predefined category, for
example a geographic region or a population sub-
set. We assume the number of sentences to be sig-

https://github.com/epfml/correlating-tweets
https://github.com/epfml/correlating-tweets


72

nificantly larger than the number of communities.
Furthermore, we assume that the target variable of
interest, for example disease mortality or preva-
lence rate, is available for each community (but
not for each individual). Our system consists of
two subsystems:

1. (Prediction) The predictive subsystem makes
predictions of target variables (e.g. AHD
mortality rate) based on aggregated language
features. The resulting linear predictions
are applicable on the community level (e.g.
counties) or on the individual level, and are
trained using k-fold cross-validated Ridge re-
gression.

2. (Interpretability) The averaged regression
weights from the prediction system allow for
interpretation of the system: We use a fixed
clustering (which was obtained from all sen-
tences without any target information), and
then rank each topic cluster with respect to a
prediction weight vector from point 1). The
top and bottom ranked topic clusters for each
target variable give insights into known and
potentially novel correlations of topics with
the target medical outcome.

In summary, the community association is used
as a proxy or weak labelling to correlate individual
language with community-level target variables.
The following subsections give a more detailed de-
scription of the two subsystems.

2.1 System Description
Let S be the set of sentences (e.g. tweets), with
their total number denoted as |S| = S. Each sen-
tence is associated to exactly one of the A com-
munities A = {a1, . . . , aA} (e.g. geographic re-
gions). The function δ : S → A defines this map-
ping. Let y ∈ RA be the target vector for an ar-
bitrary target variable, so that each community aj
has a corresponding target value yaj ∈ R.

Preprocessing and Embeddings. The complete
linguistic preprocessing pipeline of a sentence
is incorporated by the function ρ(si), ∀ i ∈
{1, . . . , S}, which represents an arbitrary sen-
tence si as a sequence of tokens. Each sentence si
then is represented by a D-dimensional embed-
ding vector providing a numerical representation
of the semantics for the given short text:

xi = Sent2Vec(ρ(si)) ∈ RD. (1)

Sentences

Sent2Vec

Learning

Subsampling

ClusteringTarget

Ranking

Category 
Aggregation

Figure 1: System Description.

While our method is generic for any text represen-
tation method, here Sent2Vec (Pagliardini et al.,
2018) was chosen for its computational efficiency
and scalability to large datasets.

2.2 Feature Aggregation
We use averaging of the sentence embedding vec-
tors over each community to obtain the language
features for each community. Formally, the com-
plete feature matrix of all sentences is denoted
as X ∈ RS×D. For our approach, the sentence
embedding features are averaged over each com-
munity aj . Formally, an individual feature xaj ,d of
the averaged embedding xaj ∈ R1×D for a given
community aj is defined as

xaj ,d =
1

Naj

∑
xi:si∈S ∧ δ(si)=aj

xi,d, (2)

where Naj = |{si : si ∈ S ∧ δ(si) = aj}|
is the number of sentences belonging to commu-
nity aj . Consequently, the aggregated community-
level embedding matrix is given by

X =

x
>
a1
...

x>aA

 ∈ RA×D. (3)
2.3 Train-Test Split
Leveraging the targets available for each commu-
nity, our regression method is applied to the aggre-
gated features X and the target y. We employ K-
fold cross-validation: the previously defined setA
is split into K as equally sized pairwise disjoint
subsets Ak as possible such that: A =

⋃K
k=1Ak,

Ai ∩ Aj = ∅ ∀i, j ∈ 1, . . . ,K, i 6= j and
|A1| ≈ · · · ≈ |AK |. The training set for a fold k
is TRk =

(⋃K
i=1Ai

)
\Ak with the corresponding

test set TEk = Ak, where N θk = |TRk| and NΛk =



73

|TEk|. The operators θk : {1, . . . , N θk} → TRk
and Λk : {1, . . . , NΛk } → TEk uniquely map the
indexes to the corresponding communities aj for
the kth train-test split. For each split k the train
and test embedding matrices respectively are de-
fined as

Xθk =
[
xθk(1), . . . ,xθk(Nθk )

]>
, (4)

XΛk =
[
xΛk(1), . . . ,xΛk(NΛk )

]>
. (5)

Accordingly, we define the target vectors

yθk =
[
yθk(1), . . . , yθk(Nθk )

]>
, (6)

yΛk =
[
yΛk(1), . . . , yΛk(NΛk )

]>
. (7)

2.4 Ridge Regression
For each train-test split k we perform linear re-
gression from the community-level textual fea-
tures Xθk to the health target variable yθk . We em-
ploy Ridge regression (Hoerl and Kennard, 1970).
In our context, the Ridge regression is defined as
the following optimization problem:

min
ωk∈RD

1

2A

Nθk∑
i=1

[
yθk(i)−x

>
θk
ωk
]2

+λ‖ωk‖22, (8)

where the optimal solution is

ω?k =
(
X
>
θk
Xθk + 2N

θ
kλI
)−1

X
>
θk

∈ RD. (9)

Within each each fold we tune the regularization
parameter λ.

2.5 Prediction Subsystem
Let yΛk = XΛkω

?
k = [yΛk(1), . . . , yΛk(NΛk )

]> be
the predicted values for the test set of the split k.
The concatenated prediction vector for all splits is

yΛ =

y
>
Λ1
...

y>ΛK

 ∈ RA (10)
Accordingly, we define the concatenated true

target vector as

yΛ =

y
>
Λ1
...

y>ΛK

 ∈ RA, (11)
i.e., the set of individual scalars is identical to the
entries in the original target vector y. The pre-
dictive performance of the system can be assessed
through the following metrics:

• Pearson Correlation Coefficient

• Mean Average Error of prediction (MAE)

• Classification Accuracy for Quantile Predic-
tion

The first two metrics are evaluated with the vec-
tors yΛ and yΛ from all folds. In the quantile-
based assessment we independently bin the true
values yΛ and the predicted values yΛ into C
different quantiles. Each individual true and
predicted value is assigned to a quantile cj ∈
{c1, . . . , cC}. These assignments can be used to
visually compare results on a heat-map or as regu-
lar evaluation scores in terms of accuracy.

2.5.1 Ridge-Weight Aggregation
For the final prediction model, the regression
weights ω?k from Ridge regression are averaged
over the K folds, i.e. ω = 1K

∑K
k=1ω

?
k.

For every sentence embedding xq, the predic-
tion is computed as yq = x

>
q ω ∈ R.

2.6 Interpretation Subsystem: Cluster
Ranking

We employ predefined textual topic clusters—
which are independent of any target values—in
order to enable interpretation of the textual cor-
relates. Each cluster is a collection of sentences
and should, intuitively, be interpretable as a topic,
e.g. separate topics about indoor and outdoor ac-
tivities as shown in Fig. 4. For each cluster m a
ranking score can be computed with respect to a
linear prediction model ω such as defined above.
Let Qm = {q : ζ(q) = m ∧ q ∈ Q} be the set
of sentences assigned to cluster m. The score ιm
for the cluster m is the average of all predictions
yq = x

>
q ω within the cluster m:

ιm =
1

|Qm|
∑

yq : q∈Qm

yq (12)

By ordering the scores ιm of all clusters, we
obtain the final ranking sequence of all clusters,
with respect to the target-specific model ω.

Clustering Preprocessing. For obtaining the
fixed clustering, as X is a very large matrix, clus-
tering might require subsampling to reduce com-
putational complexity. Hence, Q out of the S em-
beddings in S are randomly subsampled into the
set Q. The mapping Φ(Q) = [φ(1), . . . , φ(Q)]>



74

is a uniformly random selection of row indexes in
X out of

(
N
Q

)
. We define the subsampled data ma-

trix as XQ =
[
xφ(1), . . . ,xφ(Q)

]> ∈ RQ×D.
The subset XQ is clustered with the Yinyang

K-Means algorithm (Ding et al., 2015). We
use M centroids and the cosine similarity as a
distance function. The cluster assignment vec-
tor M ∈ [1, . . . ,M ]> assigns one cluster for
each embedding in XQ. Accordingly, the opera-
tor ζ : {1, . . . , Q} → {1, . . . ,M} indicates the
assigned clusterm for a given sentence s inQ (see
cluster ranking above). The cluster centers are de-
fined in MQ ∈ RQ×D.

3 Data sources

We apply the method described in Section 2 to the
following setting: The pool of sentences S con-
sists of geotagged Tweets. The assigned locations
are in the United States. The geotags are catego-
rized into US-counties which represent the set of
communities A. The target variables y are health-
related variables, for example normalized mortal-
ity or prevalence rates. We focus on cancer and
AHD mortality as well as on diabetes prevalence.
Hence, the quantile-based predictions give a cate-
gorization of the Ridge regression predictions on
a US-county level. The ranked topics assess what
language might relate to higher or lower rates of
the corresponding disease. Table 1 provides an
overview of the size of the data sources, the year
the data was collected in and the mean µ and
standard deviation σ of the target variables. Not
all counties are covered in the publicly available
datasets, usually being limited to more populous
counties. The collected Tweets are from 2014 and
2015. The target variables are the union-averaged
values from 2014 and 2015: if the target variable
is available for both years the two values are av-
eraged. Conversely, if a county data point is only
available for one, but not both years, we use this
standalone value.

3.1 Datorium Tweets

Tweets are short messages of no more than 140
characters1 published by users of the Twitter plat-
form. They reflect discussions, thoughts and ac-
tivities of its users. We use a dataset of approxi-
mately 144 million tweets collected from first of
June 2014 to first of June 2015 (Datorium, 2017).

1Twitter increased the limit to 280 characters in 2017,
which doesn’t affect our data.

Name # tweets Year
Datorium 147M 14/15
Name # counties Year µ, σ
AHD 803 14/15 43.0, 16.1
Diabetes 3129 13 9.7, 2.2
Breast 487 13/14 12.4, 2.8
Colon 490 13/14 12.1, 3.0
Liver 293 13/14 7.5, 2.4
Lung 1612 13/14 52.4, 16.2
Melanoma 162 13/14 3.8, 1.2
Prostate 351 13/14 8.5, 2.0
Stomach 136 13/14 3.6, 0.9

Table 1: Overview of data sources.

Each tweet was geotagged by the submitting user
with exact GPS coordinates and all tweets are
from within the US, allowing accurate county-
level mapping of individual tweets.

3.2 AHD & Cancer Mortality

Our source of the statistical county-level target
variables is the CDC WONDER2 database (CDC,
2018) for AHD and cancer. Values are given as
deaths per capita (100’000).

3.3 Diabetes Prevalence

We use county-wise age-adjusted diabetes preva-
lence data from the year 2013 (CDC, 2016), pro-
vided as percent of the population afflicted with
type II diabetes. The data is available for almost
all the 3144 US counties, making it a valuable tar-
get to use.

4 Results

The results of our method for the various target
variables are listed in Table 2 along with the per-
formance of the baseline model outlined in Sec-
tion 4.1. We provide the Pearson correlation (ρ)
and the mean absolute error (MAE) of our system
along with the baseline model’s Pearson correla-
tion.

4.1 LDA Baseline Model

We reimplemented the approach proposed by
Eichstaedt et al. (2015) as a baseline for compar-
ison, and were able to reproduce their findings
about AHD with recent data: similar results were

2US Centers for Disease Control and Prevention - Wide-
ranging Online Data for Epidemiologic Research.



75

found with the Datorium Twitter dataset (Dato-
rium, 2017) and CDC AHD data from 2014 and
2015. Their approach averages topics generated
with Latent Dirichlet Allocation (LDA) of tweets
per county as features for Ridge regression. We do
not use any hand-curated emotion-specific dictio-
naries, as these did not impact performance in our
experiments. We used the predefined Facebook
LDA coefficients of Eichstaedt et al. (2015), up-
dated them with the word frequencies of our col-
lected Twitter data (Datorium, 2017). Our results
are computed with a 10-fold cross-validation and
without any feature selection.

Type ρ ρ LDA MAE
AHD 0.46 0.31 13.4
Diabetes 0.73 0.72 1.1
Breast 0.44 0.42 1.80
Colon 0.55 0.51 1.87
Liver 0.29 0.40 1.59
Lung 0.68 0.63 8.44
Melanoma 0.72 0.61 0.68
Prostate 0.39 0.38 1.34
Stomach 0.44 0.51 0.72

Table 2: Results of predictions on different health tar-
gets. ρ: our system (Section 2.5), ρ LDA: topic model
baseline (Eichstaedt et al. (2015), Section 4.1), MAE:
mean absolute error of our system (Section 2.5).

4.2 Detailed Results

In this section we discuss a selection of our re-
sults in detail, with additional information avail-
able in Appendix A.1.

Diabetes has a strong demographic bias, with a
higher prevalence in the south-east of the US, the
so called diabetes belt. Compared to the national
average, the african-american population in the di-
abetes belt has a higher risk of diabetes by a factor
of more than 2 (Barker et al., 2011) and the south-
east of the US has a large african-american popula-
tion. Therefore, linguistic features (Green, 2002)
common in african-american are a strong predic-
tor of diabetes rates. The model learns these lin-
guistic features, as seen in Figure 3, and its predic-
tions closely match the actual geographic distribu-
tion, as seen in Figure 2. A moderate alcohol con-
sumption is linked to a low risk of type II diabetes
compared to no or excessive consumption (Koppes
et al., 2005). The strongest negatively correlated
word clouds in Figure 3 support this finding.

The most positively related word clouds for
melanoma in Figure 4 are related to outdoor ac-
tivities (Elwood et al., 1985). Conversely, the
strongest negatively correlated word clouds sug-
gest indoor activity related language.

5 Discussion

In this paper, we introduced a novel approach
for language-based predictions and correlation
of community-level health variables. For vari-
ous health-related demographic variables, our ap-
proach outperforms in most cases (Table 2) simi-
lar models based on traditional demographic data
by using only geolocated tweets. Our approach
provides a method for discovering novel correla-
tions between open-vocabulary topics and health
variables, allowing researchers to discover yet un-
known contributing factors based on large collec-
tions of data with minimal effort.

Our findings, when applying our method to
AHD risk, diabetes prevalence and the risk of vari-
ous types of cancers, using geolocated tweets from
the US only, show that a large variety of health-
related variables can be predicted with surpris-
ingly high precision based solely on social media
data. Furthermore, we show that our model iden-
tifies known and novel risk or protective factors in
the form of topics. Both aspects are of interest to
researchers and policy makers. Our model proved
to be robust for the majority of targets it was ap-
plied to.

For AHD risk, we show that our approach sig-
nificantly outperforms previous models based on
topic models such as LDA or traditional statis-
tical models (Eichstaedt et al., 2015), achieving
a ρ-value of 0.46, an increase of 0.09 over pre-
vious approaches. For diabetes prevalence our
model correctly predicts its geographic distribu-
tion by identifying linguistic features common in
high-prevalence areas among other features, with
a ρ-value of 0.73. For melanoma risk, it finds a
high-correlation with the popularity of outdoor ac-
tivities, corresponding to exposure to sunlight be-
ing one of the main risk factors in skin cancer, with
an overall ρ-value of 0.72.

One of the main limitations of our approach
is the need for a large collection of sentences
for each community as well as a large number
of communities with target variables, leading to
potentially unreliable results when this is not the
case, such as for social media posts by individuals



76

(a) (b)

Figure 2: Quantiles of the prevalence of diabetes. (a) Target values (b) Predicted values from tweets

(a) (b)

(c) (d)

Figure 3: Word clouds of topics correlating with di-
abetes: (a) (b) strongest positive correlation (c) (d)
strongest negative correlation among M = 2000 clus-
ters.

(a) (b)

(c) (d)

Figure 4: Word clouds of topics correlating with
melanoma: (a) (b) strongest positive correlation (c) (d)
strongest negative correlation among M = 2000 clus-
ters.

or when modeling target values which are only
available in e.g. few counties. Further research
is needed to ascertain whether significant results
can also be achieved in such scenarios, and if
robustness of our approach is improved compared
to bag-of-words-based baselines (Eichstaedt et al.,

2015; Brown and Coyne, 2018; Schwartz et al.,
2018). Furthermore, all mentioned approaches
rely on correlation, and thus do not provide a way
to determine any causation, or ruling out of poten-
tial underlying factors not captured by the model.
Even though using social media data introduces a
non-negligible bias towards users of social media,
our approach was able to predict target variables
tied to very different age-groups, which is encour-
aging and supports the robustness of our approach.

Our method captures language features on a
community scale. This raises the question of how
these findings can be translated to the individual
person. Theoretically, a community-based model
as described above could be used to rank social
media posts or messages of an individual user,
with respect to specific health risks. However, as
we currently do not have ground truth values on
the individual level, and since user’s social media
history has very high variance, this is left for fu-
ture investigation.

Future research should also address the applica-
bility of our model to textual data other than Twit-
ter and potentially from non-social media sources,
to communities that are not geography based, to
the time evolution of topics and health/lifestyle
statistics, as well as to targets that are not health
related. The general methodology offers promise
for new avenues for data-driven discovery in fields
such as medicine, sociology and psychology.

Acknowledgements. We would like to thank
Ahmed Kulovic and Maxime Delisle for valuable
input and discussions.



77

References
Lawrence E. Barker, Karen A. Kirtland, Edward W.

Gregg, Linda S. Geiss, and Theodore J. Thompson.
2011. Geographic distribution of diagnosed diabetes
in the us: a diabetes belt. American journal of pre-
ventive medicine, 40(4):434–439.

Nicholas JL. Brown and James C. Coyne. 2018. Does
Twitter language reliably predict heart disease? a
commentary on eichstaedt et al.(2015a). PeerJ,
6:e5656.

CDC. 2016. County data. National Center for Chronic
Disease Prevention and Health Promotion, Division
of Diabetes Translation.

CDC. 2018. CDC WONDER. WONDER – Wide-
ranging Online Data for Epidemiologic Research.

Brenda Curtis, Salvatore Giorgi, Anneke EK. Buffone,
Lyle H. Ungar, Robert D. Ashford, Jessie Hem-
mons, Dan Summers, Casey Hamilton, and H. An-
drew Schwartz. 2018. Can Twitter be used to predict
county excessive alcohol consumption rates? PloS
one, 13(4):e0194290.

Datorium. Geotagged Twitter posts from the united
states: A tweet collection to investigate representa-
tiveness [online]. 2017.

Yufei Ding, Yue Zhao, Xipeng Shen, Madanlal Musu-
vathi, and Todd Mytkowicz. 2015. Yinyang K-
means: A drop-in replacement of the classic K-
means with consistent speedup. In ICML’15 - Pro-
ceedings of the 32nd International Conference on
International Conference on Machine Learning.

Johannes C. Eichstaedt, Hansen Andrew Schwartz,
Margaret L. Kern, Gregory Park, Darwin R.
Labarthe, Raina M. Merchant, Sneha Jha, Megha
Agrawal, Lukasz A. Dziurzynski, Maarten Sap, et al.
2015. Psychological language on Twitter predicts
county-level heart disease mortality. Psychological
science, 26(2):159–169.

J. Mark Elwood, Richard P. Gallagher, GB. Hill, and
JCG. Pearson. 1985. Cutaneous melanoma in re-
lation to intermittent and constant sun exposurethe
western canada melanoma study. International jour-
nal of cancer, 35(4):427–433.

Lisa J. Green. 2002. African American English: a lin-
guistic introduction. Cambridge University Press.

Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67.

Lando LJ. Koppes, Jacqueline M. Dekker, Henk FJ.
Hendriks, Lex M. Bouter, and Robert J. Heine. 2005.
Moderate alcohol consumption lowers the risk of
type 2 diabetes: a meta-analysis of prospective ob-
servational studies. Diabetes care, 28(3):719–725.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
2018. Unsupervised Learning of Sentence Embed-
dings using Compositional n-Gram Features. In
NAACL 2018 - Conference of the North American
Chapter of the Association for Computational Lin-
guistics.

Marcel Salathé. 2018. Digital epidemiology: what is
it, and where is it going? Life sciences, society and
policy, 14(1):1.

H. Andrew Schwartz, Salvatore Giorgi, Margaret L.
Kern, Gregory Park, Maarten Sap, Darwin R.
Labarthe, Emily E. Larson, Martin Seligman,
Lyle H. Ungar, et al. 2018. More evidence that Twit-
ter language predicts heart disease: a response and
replication.

https://www.cdc.gov/diabetes/data/countydata/countydataindicators.html
https://wonder.cdc.gov/
https://datorium.gesis.org/xmlui/handle/10.7802/1166
https://datorium.gesis.org/xmlui/handle/10.7802/1166
https://datorium.gesis.org/xmlui/handle/10.7802/1166
http://proceedings.mlr.press/v37/ding15.pdf
http://proceedings.mlr.press/v37/ding15.pdf
http://proceedings.mlr.press/v37/ding15.pdf


78

A Appendices

A.1 Additional Figures

(a)

(b)

(c)

(d)

Figure 5: Word clouds of topics correlating with col-
orectal cancer: (a) (b)strongest positively correlated
topics (c) (d) strongest negatively correlated topics
among M = 2000 clusters.

Figure 6: Confusion matrix for decile-based prediction
of diabetes prevalence.

A.2 Implementation Details
Tweets were collected according to the provided
datorium IDs using the Tweepy3 library. The
tweets were then imported into Google BigQuery4

and processed using Apache Beam5. The sen-
tence embeddings were computed using the offi-
cial Sent2Vec source code and the provided 700-
dimensional pre-trained model for tweets (using
bigrams)6. Clustering was performed by libKM-
CUDA7. Scikit-learn8 was used for 10-fold cross
validation, Ridge regression, calculating the corre-
lation and hyperparameter search.

3https://www.tweepy.org/
4https://cloud.google.com/bigquery/
5https://beam.apache.org/
6https://github.com/epfml/sent2vec
7https://github.com/src-d/kmcuda
8https://scikit-learn.org/stable/


