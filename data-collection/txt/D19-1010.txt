



















































Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 100–110,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

100

Guided Dialog Policy Learning:
Reward Estimation for Multi-Domain Task-Oriented Dialog

Ryuichi Takanobu1, Hanlin Zhu2, Minlie Huang1∗
Institute for AI, BNRist, 1DCST, 2IIIS, Tsinghua University, Beijing, China

gxly19@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn

Abstract

Dialog policy decides what and how a task-
oriented dialog system will respond, and plays
a vital role in delivering effective conversa-
tions. Many studies apply Reinforcement
Learning to learn a dialog policy with the re-
ward function which requires elaborate de-
sign and pre-specified user goals. With the
growing needs to handle complex goals across
multiple domains, such manually designed re-
ward functions are not affordable to deal with
the complexity of real-world tasks. To this
end, we propose Guided Dialog Policy Learn-
ing, a novel algorithm based on Adversarial
Inverse Reinforcement Learning for joint re-
ward estimation and policy optimization in
multi-domain task-oriented dialog. The pro-
posed approach estimates the reward signal
and infers the user goal in the dialog sessions.
The reward estimator evaluates the state-action
pairs so that it can guide the dialog policy at
each dialog turn. Extensive experiments on a
multi-domain dialog dataset show that the dia-
log policy guided by the learned reward func-
tion achieves remarkably higher task success
than state-of-the-art baselines.

1 Introduction

Dialog policy, deciding the next action that the
dialog agent should take at each turn, is a cru-
cial component of a task-oriented dialog system.
Among many models, Reinforcement Learning
(RL) is commonly used to learn dialog policy
(Fatemi et al., 2016; Peng et al., 2017; Chen et al.,
2017; Yarats and Lewis, 2018; Lei et al., 2018; He
et al., 2018; Su et al., 2018), where users are mod-
eled as a part of the environment and the policy is
learned through interactions with users.

While it is too expensive to learn directly from
real users since RL requires a large number of

∗Corresponding author

U: I’m looking for a hotel to stay that has 5 stars and
cheap price range.

S: I am sorry that there is no such hotel, would you like
to reserve a 3-star hotel as an alternative?

U: I’d prefer a 4-star hotel even if it’s a bit expensive.
Oh, and I need parking.

S: OK, I find a moderately priced 4-star hotel that
includes parking and free wifi.

U: Are there any places to eat around it?
S: Many. Japanese, Indian, French, etc. What kind of

food would you like?

Table 1: An example of the multi-domain task-oriented
dialog between the user (U) and the system (S). The di-
alog proceeds successfully because the system informs
the user that no matching hotel exists (the first turn),
identifies the new user goal about parking (the second
turn), and shifts the topic to the restaurant domain (the
third turn), which well understands the user’s demand.

samples to train, most existing studies use data-
driven approaches to build a dialog system from
conversational corpora (Zhao and Eskenazi, 2016;
Dhingra et al., 2017; Shah et al., 2018; Shi and Yu,
2018), where a common strategy is to build a user
simulator, and then to learn dialog policy through
making simulated interactions between an agent
and the simulator. A typical reward function on
policy learning consists of a small negative penalty
at each turn to encourage a shorter session, and a
large positive reward when the session ends suc-
cessfully if the agent completes the user goal.

However, specifying an effective reward func-
tion is challenging in task-oriented dialog. On
one hand, the short dialogs resulted from the nega-
tive constant rewards are not always efficient. The
agent may end a session too quickly to complete
the task properly. For example, it is inappropriate
to book a 3-star hotel without confirming with the
user at the first turn in Table 1. On the other hand,
an explicit user goal is essential to evaluate the
task success in the reward design, but user goals
are hardly available in real situations (Su et al.,



101

2016). In addition, the user goal may change as
the conversation proceeds. For instance, the user
introduces a new requirement for the parking in-
formation at the second turn in Table 1.

Unlike a handcrafted reward function that only
evaluates the task success at the end of a session,
a good reward function should be able to guide
the policy dynamically to complete the task during
the conversation. We refer to this as the reward
sparsity issue. Furthermore, the reward function
is often manually tweaked until the dialog pol-
icy performs desired behaviors. With the grow-
ing needs for the system to handle complex tasks
across multiple domains, a more sophisticated re-
ward function would be designed, which poses a
serious challenge to manually trade off those dif-
ferent factors.

In this paper, we propose a novel model for
learning task-oriented dialog policy. The model
includes a robust dialog reward estimator based on
Inverse Reinforcement Learning (IRL). The main
idea is to automatically infer the reward and goal
that motivates human behaviors and interactions
from the real human-human dialog sessions. Dif-
ferent from conventional IRL that learns a reward
function first and then trains the policy, we inte-
grate Adversarial Learning (AL) into the method
so that the policy and reward estimator can be
learned simultaneously in an alternate way, thus
improving each other during training. To deal with
reward sparsity, the reward estimator evaluates the
generated dialog session using state-action pairs
instead of the entire session, which provides re-
ward signals at each dialog turn and guides dialog
policy learning better.

To evaluate the proposed approach, we conduct
our experiments on a multi-domain, multi-intent
task-oriented dialog corpus. The corpus involves
large state and action spaces, multiple decision
making in one turn, which makes it more chal-
lenging for the reward estimator to infer the user
goal. Furthermore, we experiment with two dif-
ferent user simulators. The contributions of our
work are in three folds:

• We build a reward estimator via Inverse Rein-
forcement Learning (IRL) to infer an appro-
priate reward from multi-domain dialog ses-
sions, in order to avoid manual design of re-
ward function.

• We integrate Adversarial Learning (AL) to
train the policy and estimator simultaneously,

and evaluate the policy using state-action
pairs to better guide dialog policy learning.

• We conduct experiments on the multi-
domain, multi-intent task-oriented dialog
corpus, with different types of user simu-
lators. Results show the superiority of our
model to the state-of-the-art baselines.

2 Related Work

2.1 Multi-Domain Dialog Policy Learning

Some recent efforts have been paid to multi-
domain task-oriented dialog systems where users
converse with the agent across multiple domains.
A natural way to handle multi-domain dialog
systems is to learn multiple independent single-
domain sub-policies (Wang et al., 2014; Gašić
et al., 2015; Cuayáhuitl et al., 2016). Multi-
domain dialog completion was also addressed by
hierarchical RL which decomposes the task into
several sub-tasks in terms of temporal order (Peng
et al., 2017) or space abstraction (Casanueva et al.,
2018), but the hierarchical structure can be very
complex and constraints between different do-
mains should be considered if an agent conveys
multiple intents.

2.2 Reward Learning in Dialog Systems

Handcrafted reward functions for dialog policy
learning require elaborate design. Several reward
learning algorithms have been proposed to find
better rewards, including supervised learning on
expert dialogs (Li et al., 2014), online active learn-
ing from user feedback (Su et al., 2016), multi-
object RL to aggregate measurements of various
aspects of user satisfaction (Ultes et al., 2017), etc.
However, these methods still require some knowl-
edge about user goals or annotations of dialog rat-
ings from real users. Boularias et al. (2010) and
Barahona and Cerisara (2014) learn the reward
from dialogs using linear programming based on
IRL, but do not scale well in real applications.
Recently, Liu and Lane (2018) use adversarial re-
wards as the only source of reward signal. It trains
a Bi-LSTM as a discriminator that works on the
entire session to predict the task success.

2.3 Adversarial Inverse Reinforcement
Learning

IRL aims to infer the reward function R underly-
ing expert demonstrations sampled from humans



102

Dialog
Policy

Reward
Estimator

User
Simulator Data

𝑠

𝑎

𝑎𝑢𝑠𝑢

𝑓𝜔(𝑠, 𝑎)

𝜋𝜃 𝑎 𝑠)

𝑟𝜔(𝑠, 𝑎)

State
Tracker

Figure 1: Architecture of GDPL. The dialog policy π
decides the dialog act a according to the dialog state s
provided by the state tracker, and the reward estimator
f evaluates the dialog policy by comparing the gener-
ated state-action pair (s, a) with the human dialog.

or the optimal policy π∗. This is similar to the
discriminator network in AL that evaluates how
realistic the sample looks. Finn et al. (2016)
draw a strong connection between GAN and max-
imum entropy causal IRL (Ziebart et al., 2010) by
replacing the estimated data density in AL with
the Boltzmann distribution in IRL, i.e. p(x) ∝
exp(−E(x)). Several approaches (Ho and Er-
mon, 2016; Fu et al., 2018) obtain a promising re-
sult on automatic reward estimation in large, high-
dimensional environments by combining AL with
IRL. Inspired by this, we apply AIRL to complex,
multi-domain task-oriented dialog, which faces
new issues such as discrete action space and lan-
guage understanding.

3 Guided Dialog Policy Learning

We propose Guided Dialog Policy Learning
(GDPL), a flexible and practical method on joint
reward learning and policy optimization for multi-
domain task-oriented dialog systems.

3.1 Overview

The overview of the full model is depicted in Fig.
1. The framework consists of three modules: a
multi-domain Dialog State Tracker (DST) at the
dialog act level, a dialog policy module for decid-
ing the next dialog act, and a reward estimator for
policy evaluation.

Specifically, given a set of collected human
dialog sessions D = {τ1, τ2, . . . }, each dia-
log session τ is a trajectory of state-action pairs
{su0 , au0 , s0, a0, su1 , au1 , s1, a1, . . . }. The user sim-
ulator µ(au, tu|su) posts a response au according
to the user dialog state su where tu denotes a bi-
nary terminal signal indicating whether the user
wants to end the dialog session. The dialog policy
πθ(a|s) decides the action a according to the cur-

rent state s and interacts with the simulator µ. Dur-
ing the conversation, DST records the action from
one dialog party and returns the state to the other
party for deciding what action to take in the next
step. Then, the reward estimator fω(s, a) evalu-
ates the quality of the response from the dialog
policy, by comparing it with sampled human di-
alog sessions from the corpus. The dialog policy
π and the reward estimator f are MLPs parameter-
ized by θ, ω respectively. Note that our approach
does not need any human supervision during train-
ing, and modeling a user simulator is beyond the
scope of this paper.

In the subsequent subsections, we will first ex-
plain the state, action, and DST used in our algo-
rithm. Then, the algorithm is introduced in a ses-
sion level, and last followed by a decomposition
of state-action pair level.

3.2 Multi-Domain Dialog State Tracker
A dialog state tracker keeps track of the dia-
log session to update the dialog state (Williams
et al., 2016; Zhang et al., 2019). It records in-
formable slots about the constraints from users and
requestable slots that indicates what users want to
inquiry. DST maintains a separate belief state for
each slot. Given a user action, the belief state of its
slot type is updated according to its slot value (Roy
et al., 2000). Action and state in our algorithm are
defined as follows:

Action : Each system action a or user action au
is a subset of dialog act setA as there may be mul-
tiple intents in one dialog turn. A dialog act is
an abstract representation of an intention (Stolcke
et al., 2000), which can be represented in a quadru-
ple composed of domain, intent, slot type and slot
value in the multi-domain setting (e.g. [restau-
rant, inform, food, Italian]). In practice, dialog
acts are delexicalized in the dialog policy. We re-
place the slot value with a count placeholder and
refill it with the true value according to the entity
selected from the external database, which allows
the system to operate on unseen values.

State : At dialog turn t1, the system state st =
[aut ; at−1; bt; qt] consists of (I) user action at cur-
rent turn aut ; (II) system action at the last turn
at−1; (III) all belief state bt from DST; and (IV)
embedding vectors of the number of query results
qt from the external database.

1We regard a user turn and a system turn as one dialog
turn throughout the paper.



103

As our model works at the dialog act level, DST
can be simply implemented by extracting the slots
from actions.

3.3 Session Level Reward Estimation
Based on maximum entropy IRL (Ziebart et al.,
2008), the reward estimator maximizes the log
likelihood of observed human dialog sessions to
infer the underlying goal,

ω∗ = argmax
ω

Eτ∼D[fω(τ)],

fω(τ) = log pω(τ) = log
eRω(τ)

Zω
,

Rω(τ) =

T∑
t=0

γtrω(st, at),

Zω =
∑
τ

eRω(τ).

where f models human dialogs as a Boltzmann
distribution (Ziebart et al., 2008), R stands for
the return of a session, i.e. γ-discounted cumula-
tive rewards, and Z is the corresponding partition
function.

The dialog policy is encouraged to mimic hu-
man dialog behaviors. It maximizes the expected
entropy-regularized return Eπ[R]+H(π) (Ziebart
et al., 2010) based on the principle of maximum
entropy through minimizing the KL-divergence
between the policy distribution and Boltzmann
distribution,

Jπ(θ) = −KL(πθ(τ)||pω(τ))
= Eτ∼π[fω(τ)− log πθ(τ)]
= Eτ∼π[Rω(τ)]− logZω +H(πθ),

where the term logZω is independent to θ, and
H(·) denotes the entropy of a model. Intuitively,
maximizing entropy is to resolve the ambiguity of
language that many optimal policies can explain a
set of natural dialog sessions. With the aid of the
likelihood ratio trick, the gradient for the dialog
policy is

∇θJπ = Eτ∼π[(fω(τ)− log πθ(τ))∇θ log πθ(τ)].

In the fashion of AL, the reward estimator aims
to distinguish real human sessions and generated
sessions from the dialog policy. Therefore, it min-
imizes KL-divergence with the empirical distribu-
tion, while maximizing the KL-divergence with
the policy distribution,

Jf (ω)=−KL(pD(τ)||pω(τ))+KL(πθ(τ)||pω(τ))
=Eτ∼D[fω(τ)]+H(p)−Eτ∼π[fω(τ)]−H(πθ).

Algorithm 1: Guided Dialog Policy Learning
Require: Dialog corpus D, User simulator µ

1 foreach training iteration do
2 Sample human dialog sessions DH from

D randomly
3 Collect the dialog sessions DΠ by

executing the dialog policy π and
interacting with µ, au ∼ µ(·|su),
a ∼ π(·|s), where s is maintained by
DST

4 Update the reward estimator f by
maximizing Jf w.r.t. ω adversarially
(Eq. 2)

5 Compute the estimated reward of each
state-action pair in DΠ,
r̂ = fω(s, a)− log πθ(a|s)

6 Update π, V using the estimated reward r̂
by maximizing Jπ, JV w.r.t. θ (Eq. 3
and Eq. 4)

7 end

Similarly, H(p) and H(πθ) is independent to ω,
so the gradient for the reward estimator yields

∇ωJf = Eτ∼D[∇ωfω(τ)]− Eτ∼π[∇ωfω(τ)].

3.4 State-Action Level Reward Estimation

So far, the reward estimation uses the entire ses-
sion τ , which can be very inefficient because of
reward sparsity and may be of high variance due to
the different lengths of sessions. Here we decom-
pose a session τ into state-action pairs (s, a) in the
reward estimator to address the issues. Therefore,
the loss functions for the dialog policy and the re-
ward estimator become respectively as follows:

Jπ(θ) = Es,a∼π[
T∑
k=t

γk−t(fω(sk, ak)

− log πθ(ak|sk))], (1)
Jf (ω) = Es,a∼D[fω(s, a)]− Es,a∼π[fω(s, a)],

(2)

where T is the number of dialog turns. Since the
reward estimator evaluates a state-action pair, it
can guide the dialog policy at each dialog turn
with the predicted reward r̂ω(s, a) = fω(s, a) −
log πθ(a|s).

Moreover, the reward estimator fω can be trans-
formed to a reward approximator gω and a shaping
term hω according to (Fu et al., 2018) to recover



104

an interpretable and robust reward from real hu-
man sessions. Formally,

fω(st, at, st+1) = gω(st, at)+γhω(st+1)−hω(st),

where we replace the state-action pair (st, at) with
the state-action-state triple (st, at, st+1) as the in-
put of the reward estimator. Note that, differ-
ent from the objective in (Fu et al., 2018) that
learns a discriminator in the form Dω(s, a) =

pω(s,a)
pω(s,a)+π(a|s) , GDPL directly optimizes fω, which
avoids unstable or vanishing gradient issue in
vanilla GAN (Arjovsky et al., 2017).

In practice, we apply Proximal Policy Opti-
mization (PPO) (Schulman et al., 2017), a simple
and stable policy based RL algorithm using a con-
stant clipping mechanism as the soft constraint for
dialog policy optimization,

Jπ(θ)=Es,a∼π[min{βtÂt, clip(βt,1−�,1+�)Ât}],

(3)

Ât=δt + γλÂt+1,

δt= r̂t + γVθ(st+1)− Vθ(st),

JV (θ)=−(Vθ(st)−
T∑
k=t

γk−tr̂k)
2, (4)

where Vθ is the approximate value function, βt =
πθ(at|st)
πθold (at|st)

is the ratio of the probability under the

new and old policies, Â is the estimated advantage,
δ is TD residual, λ and � are hyper-parameters.

In summary, a brief script for GPDL algorithm
is shown in Algorithm 1.

4 Experimental Setting

4.1 Data and Simulators

We use MultiWOZ (Budzianowski et al., 2018),
a multi-domain, multi-intent task-oriented dialog
corpus that contains 7 domains, 13 intents, 25 slot
types, 10,483 dialog sessions, and 71,544 dialog
turns in our experiments. Among all the sessions,
1,000 each are used for validation and test. Dur-
ing the data collection process, a user is asked to
follow a pre-specified user goal, but it encourages
the user to change its goal during the session and
the changed goal is also stored, so the collected di-
alogs are much closer to reality. The corpus also
provides the ontology that defines all the entity at-
tributes for the external database.

We apply two user simulators as the interaction
environment for the agent. One is the agenda-
based user simulator (Schatzmann et al., 2007)
which uses heuristics, and the other is a data-
driven neural model, namely, Variational Hierar-
chical User Simulator (VHUS) derived from (Gür
et al., 2018). Both simulators initialize a user goal
when the dialog starts2, provide the agent with a
simulated user response at each dialog turn, and
work at the dialog act level. Since the original
corpus only annotates the dialog acts at the system
side, we use the annotation at the user side from
ConvLab (Lee et al., 2019) to implement the two
simulators.

4.2 Evaluation Metrics
Evaluation of a task-oriented dialog mainly con-
sists of the cost (dialog turns) and task success (in-
form F1 & match rate). The definition of inform
F1 and match rate is explained as follows.

Inform F1 : This evaluates whether all the re-
quested information (e.g. address, phone number
of a hotel) has been informed. Here we compute
the F1 score so that a policy which greedily an-
swers all the attributes of an entity will only get a
high recall but a low precision.

Match rate : This evaluates whether the booked
entities match all the indicated constraints (e.g.
Japanese food in the center of the city) for all do-
mains. If the agent fails to book an entity in one
domain, it will obtain 0 score on that domain. This
metric ranges from 0 to 1 for each domain, and the
average on all domains stands for the score of a
session.

Finally, a dialog is considered successful only if
all the information is provided (i.e. inform recall =
1) and the entities are correctly booked (i.e. match
rate = 1) as well3. Dialog success is either 0 or 1
for each session.

4.3 Implementation Details
Both the dialog policy π(a|s) and the value func-
tion V (s) are implemented with two hidden layer
MLPs. For the reward estimator f(s, a), it is split
into two networks g(s, a) and h(s) according to
the proposed algorithm, where each is a one hid-
den layer MLP. The activation function is all Relu

2Refer to the appendix for user goal generation.
3If the user does not request any information in the ses-

sion, this will just compute match rate, and similarly for in-
form recall.



105

Hyper-parameter Value

Learning rate 1e-4
Mini-batch size 32

Discount factor γ 0.99
Clipping factor � in PPO 0.2

GAE factor λ in PPO 0.95

Table 2: Hyper-parameter settings.

for MLPs. We use Adam as the optimization algo-
rithm. The hyper-parameters of GDPL used in our
experiments are shown in Table 2.

4.4 Baselines
First of all, we introduce three baselines that use
handcrafted reward functions. Following (Peng
et al., 2017), the agent receives a positive reward
of 2L for success at the end of each dialog, or
a negative reward of −L for failure, where L is
the maximum number of turns in each dialog and
is set to 40 in our experiments. Furthermore, the
agent receives a reward of −1 at each turn so that
a shorter dialog is encouraged.

GP-MBCM (Gašić et al., 2015): Multi-domain
Bayesian Committee Machine for dialog man-
agement based on Gaussian process, which de-
composes the dialog policy into several domain-
specific policies.

ACER (Wang et al., 2017): Actor-Critic RL pol-
icy with Experience Replay, a sample efficient
learning algorithm that has low variance and scales
well with large discrete action spaces.

PPO (Schulman et al., 2017): The same as the
dialog policy in GDPL.

Then, we also compare with another strong
baseline that involves reward learning.

ALDM (Liu and Lane, 2018): Adversarial
Learning Dialog Model that learns dialog rewards
with a Bi-LSTM encoding the dialog sequence as
the discriminator to predict the task success. The
reward is only estimated at the end of the session
and is further used to optimize the dialog policy.

For a fair comparison, each method is pre-
trained for 5 epoches by simple imitation learning
on the state-action pairs.

5 Result Analysis

5.1 Main Results
The performance of each approach that interacts
with the agenda-based user simulator is shown in

Method
Agenda

Turns Inform Match Success

GP-MBCM 2.99 19.04 44.29 28.9
ACER 10.49 77.98 62.83 50.8
PPO 9.83 83.34 69.09 59.1
ALDM 12.47 81.20 62.60 61.2

GDPL-sess 7.49 88.39 77.56 76.4
GDPL-discr 7.86 93.21 80.43 80.5
GDPL 7.64 94.97 83.90 86.5

Human 7.37 66.89 95.29 75.0

Table 3: Performance of different dialog agents on
the multi-domain dialog corpus by interacting with the
agenda-based user simulator. All the results except
“dialog turns” are shown in percentage terms. Real
human-human performance computed from the test set
(i.e. the last row) serves as the upper bounds.

Table 3. GDPL achieves extremely high perfor-
mance in the task success on account of the sub-
stantial improvement in inform F1 and match rate
over the baselines. Since the reward estimator of
GDPL evaluates state-action pairs, it can always
guide the dialog policy during the conversation
thus leading the dialog policy to a successful strat-
egy, which also indirectly demonstrates that the re-
ward estimator has learned a reasonable reward at
each dialog turn. Surprisingly, GDPL even outper-
forms human in completing the task, and its av-
erage dialog turns are close to those of humans,
though GDPL is inferior in terms of match rate.
Humans almost manage to make a reservation in
each session, which contributes to high task suc-
cess. However, it is also interesting to find that
human have low inform F1, and that may explain
why the task is not always completed successfully.
Actually, there have high recall (86.75%) but low
precision (54.43%) in human dialogs when an-
swering the requested information. This is pos-
sibly because during data collection human users
forget to ask for all required information of the
task, as reported in (Su et al., 2016).

ACER and PPO obtain high performance in in-
form F1 and match rate as well. However, they ob-
tain poor performance on the overall task success,
even when they are provided with the designed re-
ward that already knows the real user goals. This
is because they only receive the reward about the
success at the last turn and fail to understand what
the user needs or detect the change of user goals.

Though ALDM obtains a lower inform F1 and
match rate than PPO, it gets a slight improvement



106

97.48%

93.55%
94.70%

91.05%

68.71%

86.01%

93.25%

80.49%

69.04%

91.25%

74.26%

59.92%

50%

60%

70%

80%

90%

100%

1 2 3

Inform F1

GDPL ALDM PPO ACER

89.34%
85.67%

75.94%
78.69%

55.56%

73.49%

87.09%

71.53%

50.47%

83.61%

64.52%

40.57%40%

50%

60%

70%

80%

90%

1 2 3

Match Rate

GDPL ALDM PPO ACER

93.87%

85.98%

74.07%
84.19%

43.98%

73.33%

89.35%

50.95%

27.78%

85.48%

40.53%

17.90%

10%

25%

40%

55%

70%

85%

100%

1 2 3

Dialog Success

GDPL ALDM PPO ACER

Figure 2: Performance of dialog agents according to the different number of domains in the dialog session. The
ratio of the sessions with 1:2:3 domains is 310:528:162 respectively.

GP-MBCM ACER PPO ALDM GDPL

1.666 0.775 0.639 1.069 0.238

Table 4: KL-divergence between different dialog pol-
icy and the human dialog KL(πturns||pturns), where
πturns denotes the discrete distribution over the num-
ber of dialog turns of simulated sessions between the
policy π and the agenda-based user simulator, and
pturns for the real human-human dialog.

on task success by encoding the entire session in
its reward estimator. This demonstrates that learn-
ing effective rewards can help the policy to capture
user intent shift, but the reward sparsity issue re-
mains unsolved. This may explain why the gain is
limited, and ALDM even has longer dialog turns
than others. In conclusion, the dialog policy bene-
fits from the guidance of the reward estimator per
dialog turn.

Moreover, GDPL can establish an efficient di-
alog thanks to the learned rewards that infer hu-
man behaviors. Table 4 shows that GDPL has the
smallest KL-divergence to the human on the num-
ber of dialog turns over the baselines, which im-
plies that GDPL behaves more like the human. It
seems that all the approaches generate many more
short dialogs (dialog turns less than 3) than hu-
man, but GDPL generates far less long dialogs (di-
alog turns larger than 11) than other baselines ex-
cept GP-MBCM. Most of the long dialog sessions
fail to reach a task success.

We also observe that GP-MBCM tries to pro-
vide many dialog acts to avoid the negative penalty
at each turn, which results in a very low inform
F1 and short dialog turns. However, as explained
in the introduction, a shorter dialog is not always
the best. The dialog generated by GP-MBCM is

Method
VHUS

Turns Inform Match Success

ACER 22.35 55.13 33.08 18.6
PPO 19.23 56.31 33.08 18.3
ALDM 26.90 54.37 24.15 16.4

GDPL 22.43 52.58 36.21 19.7

Table 5: Performance of different agents on the neural
user simulator.

too short to complete the task successfully. GP-
MBCM is a typical case that focuses too much on
the cost of the dialog due to the handcrafted re-
ward function and fails to realize the true target
that helps the users to accomplish their goals.

5.2 Ablation Study

Ablation test is investigated in Table 3. GDPL-
sess sums up all the rewards at each turn to the
last turn and does not give any other reward be-
fore the dialog terminates, while GDPL-discr is to
use the discriminator form as (Fu et al., 2018) in
the reward estimator. It is perceptible that GDPL
has better performance than GDPL-sess on the
task success and is comparable regarding the di-
alog turns, so it can be concluded that GDPL does
benefit from the guidance of the reward estima-
tor at each dialog turn, and well addresses the
reward sparsity issue. GDPL also outperforms
GDPL-discr which means directly optimizing fω
improves the stability of AL.

5.3 Interaction with Neural Simulator

The performance that the agent interacts with
VHUS is presented in Table 5. VHUS has poor
performance on multi-domain dialog. It some-
times becomes insensible about the dialog act so



107

VS.
Efficiency Quality Success

W D L W D L W D L

ACER 55 25 20 44 32 24 52 30 18
PPO 74 13 13 56 26 18 59 31 10
ALDM 69 19 12 49 25 26 61 24 15

Table 6: The count of human preference on dialog ses-
sion pairs that GDPL wins (W), draws with (D) or loses
to (L) other methods based on different criteria. One
method wins the other if the majority prefer the former
one.

it often gives unreasonable responses. Therefore,
it is more laborious for the dialog policy to learn
a proper strategy with the neural user simulator.
All the methods cause a significant drop in perfor-
mance when interacting with VHUS. ALDM even
gets worse performance than ACER and PPO. In
comparison, GDPL is still comparable with ACER
and PPO, obtains a better match rate, and even
achieves higher task success. This indicates that
GDPL has learned a more robust reward function
than ALDM.

5.4 Goal across Multiple Domains

Fig. 2 shows the performance with the different
number of domains in the user goal. In compar-
ison with other approaches, GDPL is more scal-
able to the number of domains and achieves the
best performance in all metrics. PPO suffers from
the increasing number of the domain and has re-
markable drops in all metrics. This demonstrates
the limited capability for the handcrafted reward
function to handle complex tasks across multiple
domains in the dialog.

ALDM also has a serious performance degrada-
tion with 2 domains, but it is interesting to find that
ALDM performs much better with 3 domains than
with 2 domains. We further observe that ALDM
performs well on the taxi domain, most of which
appear in the dialogs with 3 domains. Taxi domain
has the least slots for constraints and requests,
which makes it easier to learn a reward about that
domain, thus leading ALDM to a local optimal. In
general, our reward estimator has higher effective-
ness and scalability.

5.5 Human Evaluation

For human evaluation, we hire Amazon Mechan-
ical Turkers to state their preferences between
GDPL and other methods. Because all the poli-
cies work at dialog act level, we generate the texts

Type
Inform Match Success

Mean Num Mean Num Mean Num

Full 8.413 903 10.59 450 11.18 865
Other -99.95 76 -48.15 99 -71.62 135

Table 7: Return distribution of GDPL on each metric.
The first row counts the dialog sessions that get the full
score of the corresponding metric, and the results of the
rest sessions are included in the second row.

from dialog acts using hand-crafted templates to
make the dialog readable. Given a certain user
goal, Turkers first read two simulated dialog ses-
sions, one from the interaction between GDPL and
the agenda-based user simulator, the other from
another baseline with the same simulator. Then,
they are asked to judge which dialog is better (win,
draw or lose) according to different subjective as-
sessments. In addition to Task Success, we exam-
ine another two measures concerning Dialog Cost
in the human evaluation: Efficiency such as dia-
log turn cost or response delay, and Quality such
as redundant information or inappropriate reply
(Walker et al., 1997). Since the text is generated
by templates for all policies, we do not evaluate
language generation here (including grammar, di-
versity, etc.). We randomly sample 300 user goals
from the test set, 100 each for one baseline, and
each session pair is evaluated by 3 Turkers.

Table 6 presents the results of human evalua-
tion. GDPL outperforms three baselines signifi-
cantly in all aspects (sign test, p-value < 0.01) ex-
cept for the quality compared with ACER. Among
all the baselines, GDPL obtains the most pref-
erence against PPO. Note that the difference be-
tween PPO and GDPL is only in the reward sig-
nal. This again demonstrates the advantage of re-
ward learning in GDPL over the handcrafted re-
ward function. The agreement on the superiority
of GDPL between objective rating in Table 3 and
human preference here also indicates that the au-
tomatic metrics used in our experiments is reliable
to reflect user satisfaction to some extent.

5.6 Reward Evaluation
To provide an insight into the learned reward func-
tion itself, Table 7 provides a quantitative evalua-
tion on the learned rewards by showing the dis-
tribution of the return R =

∑
t γ

trt according to
each metric. It should be noted that some sessions
do not have inform F1 because the user does not
request any information, and similarly for match



108

Reward

U: I'm looking for a cheap Italian restaurant in the center of the city.
S: There are 3 restaurants available for you.
U: Can I get the address of one?
S: The first result is Pizza Hut City Centre, address is Regent Street City Centre. 
U: Sounds good. Can you book me a table for 2 at 11:45 on Wednesday?
S: You're all set. Your reference number is 7GAWK763.
U: I also need a train on Thursday.
S: Where are you departing from and heading for?
U: From Bishops Stortford to Cambridge.
S: Is there a specific time you would like to travel at?
U: I would perfer to leave after 21:00.
S: There is a train leaving at 21:29. Would you like me to make a reservation?
U: Yes, I need the train ID, please.
S: The ID is TR6336. Is there any other help you need?
U: That will be all. Have a good day! Bye.
S: Glad to be of help. Good day to you too.

-0.94

-0.91

-0.39

-0.33

-0.18

0.25

1.1

1.4

0.5

0.0

0.5

1.0

Figure 3: Reward visualization of a dialog session simulated between GDPL and the agenda-based user simulator
that contains restaurant and train domains.4

rate. It can be observed that the learned reward
function has good interpretability in that the re-
ward is positive when the dialog gets a full score
on each metric, and negative otherwise.

Fig. 3 gives an illustration of the learned re-
ward at each dialog turn as a qualitative evaluation.
In the beginning, the agent is unaware of the user
goal thus it starts with a low reward. As the dialog
proceeds, the agent has collected enough informa-
tion from the user, then books the restaurant suc-
cessfully and the reward remarkably increases at
the third turn. The reward continues to grow stably
after the topic shifts to the train domain. Again,
the agent offers the correct train ID given sufficient
information. Since the user has been informed all
the requested information and the restaurant and
train are both booked successfully, the user leaves
the session with satisfaction at last, and the reward
rises to the top as well. In brief, the learned reward
can well reflect the current state of the dialog. It is
also noticeable that the dialog policy manages to
express multiple intents during the session.

6 Discussion

In this paper, we propose a guided policy learn-
ing method for joint reward estimation and pol-
icy optimization in multi-domain task-oriented di-
alog. The method is based on Adversarial Inverse
Reinforcement Learning. Extensive experiments
demonstrate the effectiveness of our proposed ap-

4Refer to the appendix for the dialog acts.

proach and that it can achieve higher task suc-
cess and better user satisfaction than state-of-the-
art baselines.

Though the action space A of the dialog policy
is defined as the set of all dialog acts, it should
be noted that GDPL can be equipped with NLU
modules that identify the dialog acts expressed in
utterance, and with NLG modules that generate ut-
terances from dialog acts. In this way, we can con-
struct the framework in an end-to-end scenario.

The agenda-based user simulator is powerful to
provide a simulated interaction for the dialog pol-
icy learning, however, it needs careful design and
is lack of generalization. While training a neu-
ral user simulator is quite challenging due to the
high diversity of user modeling and the difficulty
of defining a proper reward function, GDPL may
offer some solutions for multi-agent dialog pol-
icy learning where the user is regarded as another
agent and trained with the system agent simulta-
neously. We leave this as the future work.

Acknowledgement

This work was supported by the National Sci-
ence Foundation of China (Grant No. 61936010 /
61876096) and the National Key R&D Program of
China (Grant No. 2018YFC0830200). We would
like to thank THUNUS NExT Joint-Lab for the
support, anonymous reviewers for their valuable
suggestions, and our lab mate Qi Zhu for help-
ful discussions. The code is available at https:
//github.com/truthless11/GDPL.

https://github.com/truthless11/GDPL
https://github.com/truthless11/GDPL


109

References

Martin Arjovsky, Soumith Chintala, and Léon Bottou.
2017. Wasserstein generative adversarial networks.
In 34th International Conference on Machine Learn-
ing, pages 214–223.

Lina M Rojas Barahona and Christophe Cerisara. 2014.
Bayesian inverse reinforcement learning for model-
ing conversational agents in a virtual environment.
In 15th International Conference on Computational
Linguistics and Intelligent Text Processing, pages
503–514.

Abdeslam Boularias, Hamid R Chinaei, and Brahim
Chaib-draa. 2010. Learning the reward model of
dialogue pomdps from data. In 24th Annual Con-
ference on Neural Information Processing Systems,
Workshop on Machine Learning for Assistive Tech-
nologies.

Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-
madan, and Milica Gašić. 2018. Multiwoz: A large-
scale multi-domain wizard-of-oz dataset for task-
oriented dialogue modelling. In 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 5016–5026.

Iñigo Casanueva, Paweł Budzianowski, Pei-Hao Su,
Stefan Ultes, Lina M Rojas Barahona, Bo-Hsiang
Tseng, and Milica Gašić. 2018. Feudal reinforce-
ment learning for dialogue management in large do-
mains. In 2018 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
714–719.

Lu Chen, Xiang Zhou, Cheng Chang, Runzhe Yang,
and Kai Yu. 2017. Agent-aware dropout dqn for
safe and efficient on-line dialogue policy learning.
In 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2454–2464.

Heriberto Cuayáhuitl, Seunghak Yu, Ashley
Williamson, and Jacob Carse. 2016. Deep re-
inforcement learning for multi-domain dialogue
systems. In 30th Annual Conference on Neural In-
formation Processing Systems, Deep Reinforcement
Learning Workshop.

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
logue agents for information access. In 55th Annual
Meeting of the Association for Computational Lin-
guistics, pages 484–495.

Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He,
and Kaheer Suleman. 2016. Policy networks with
two-stage training for dialogue systems. In 17th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 101–110.

Chelsea Finn, Paul Christiano, Pieter Abbeel, and
Sergey Levine. 2016. A connection between gen-
erative adversarial networks, inverse reinforcement
learning, and energy-based models. In 30th Annual
Conference on Neural Information Processing Sys-
tems, Workshop on Adversarial Training.

Justin Fu, Katie Luo, and Sergey Levine. 2018. Learn-
ing robust rewards with adversarial inverse rein-
forcement learning. In 6th International Conference
on Learning Representations.

Milica Gašić, Nikola Mrkšić, Pei-Hao Su, David
Vandyke, Tsung-Hsien Wen, and Steve Young.
2015. Policy committee for adaptation in multi-
domain spoken dialogue systems. In 2015 IEEE
Workshop on Automatic Speech Recognition and
Understanding, pages 806–812.

Izzeddin Gür, Dilek Hakkani-Tür, Gokhan Tür, and
Pararth Shah. 2018. User modeling for task oriented
dialogues. In 2018 IEEE Spoken Language Technol-
ogy Workshop, pages 900–906.

He He, Derek Chen, Anusha Balakrishnan, and Percy
Liang. 2018. Decoupling strategy and generation in
negotiation dialogues. In 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2333–2343.

Jonathan Ho and Stefano Ermon. 2016. Generative ad-
versarial imitation learning. In 30th Annual Con-
ference on Neural Information Processing Systems,
pages 4565–4573.

Sungjin Lee, Qi Zhu, Ryuichi Takanobu, Zheng Zhang,
Yaoqin Zhang, Xiang Li, Jinchao Li, Baolin Peng,
Xiujun Li, Minlie Huang, and Jianfeng Gao. 2019.
Convlab: Multi-domain end-to-end dialog system
platform. In 57th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 64–69.

Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun
Ren, Xiangnan He, and Dawei Yin. 2018. Sequic-
ity: Simplifying task-oriented dialogue systems with
single sequence-to-sequence architectures. In 56th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1437–1447.

Lihong Li, He He, and Jason D Williams. 2014. Tem-
poral supervised learning for inferring a dialog pol-
icy from example conversations. In 2014 IEEE Spo-
ken Language Technology Workshop, pages 312–
317.

Bing Liu and Ian Lane. 2018. Adversarial learning of
task-oriented neural dialog models. In 19th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 350–359.

Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,
Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.
2017. Composite task-completion dialogue policy
learning via hierarchical deep reinforcement learn-
ing. In 2017 Conference on Empirical Methods in
Natural Language Processing, pages 2231–2240.



110

Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 93–
100.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a pomdp dialogue
system. In 2007 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
149–152.

John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347.

Pararth Shah, Dilek Hakkani-Tür, Bing Liu, and
Gokhan Tür. 2018. Bootstrapping a neural conver-
sational agent with dialogue self-play, crowdsourc-
ing and on-line reinforcement learning. In 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 41–51.

Weiyan Shi and Zhou Yu. 2018. Sentiment adaptive
end-to-end dialog systems. In 56th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1509–1519.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339–373.

Pei-Hao Su, Milica Gašić, Nikola Mrkšić, Lina M Ro-
jas Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016. On-line active
reward learning for policy optimisation in spoken di-
alogue systems. In 54th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2431–
2441.

Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Yun-Nung Chen. 2018. Discriminative deep
dyna-q: Robust planning for dialogue policy learn-
ing. In 2018 Conference on Empirical Methods in
Natural Language Processing, pages 3813–3823.

Stefan Ultes, Paweł Budzianowski, Iñigo Casanueva,
Nikola Mrkšić, Lina M Rojas Barahona, Pei-Hao
Su, Tsung-Hsien Wen, Milica Gašić, and Steve
Young. 2017. Reward-balancing for statistical spo-
ken dialogue systems using multi-objective rein-
forcement learning. In 18th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
pages 65–70.

Marilyn A Walker, Diane J Litman, Candace A Kamm,
and Alicia Abella. 1997. Paradise: A framework for
evaluating spoken dialogue agents. In 35th Annual

Meeting of the Association for Computational Lin-
guistics, pages 271–280.

Zhuoran Wang, Hongliang Chen, Guanchun Wang,
Hao Tian, Hua Wu, and Haifeng Wang. 2014. Policy
learning for domain selection in an extensible multi-
domain spoken dialogue system. In 2014 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 57–67.

Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr
Mnih, Remi Munos, Koray Kavukcuoglu, and
Nando de Freitas. 2017. Sample efficient actor-critic
with experience replay. In 5th International Confer-
ence on Learning Representations.

Jason D Williams, Antoine Raux, and Matthew Hen-
derson. 2016. The dialog state tracking challenge
series: A review. Dialogue & Discourse, 7(3):4–33.

Denis Yarats and Mike Lewis. 2018. Hierarchical text
generation and planning for strategic dialogue. In
35th International Conference on Machine Learn-
ing, pages 5587–5595.

Zheng Zhang, Minlie Huang, Zhongzhou Zhao, Feng
Ji, Haiqing Chen, and Xiaoyan Zhu. 2019. Memory-
augmented dialogue management for task-oriented
dialogue systems. ACM Transactions on Informa-
tion Systems, 37(3):34.

Tiancheng Zhao and Maxine Eskenazi. 2016. Towards
end-to-end learning for dialog state tracking and
management using deep reinforcement learning. In
17th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 1–10.

Brian D Ziebart, J Andrew Bagnell, and Anind K Dey.
2010. Modeling interaction via the principle of max-
imum causal entropy. In 27th International Confer-
ence on Machine Learning, pages 1255–1262.

Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and
Anind K Dey. 2008. Maximum entropy inverse re-
inforcement learning. In 23rd AAAI Conference on
Artificial Intelligence, pages 1433–1438.


