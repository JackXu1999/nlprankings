9

Coling 2010: Poster Volume, pages 9–17,

Beijing, August 2010

Document Expansion Based on WordNet

for Robust IR

Eneko Agirre
IXA NLP Group

Xabier Arregi
IXA NLP Group

Arantxa Otegi
IXA NLP Group

Univ. of the Basque Country

e.agirre@ehu.es

Univ. of the Basque Country
xabier.arregi@ehu.es

Univ. of the Basque Country
arantza.otegi@ehu.es

Abstract

The use of semantic information to im-
prove IR is a long-standing goal. This pa-
per presents a novel Document Expansion
method based on a WordNet-based system
to ﬁnd related concepts and words. Ex-
pansion words are indexed separately, and
when combined with the regular index,
they improve the results in three datasets
over a state-of-the-art IR engine. Consid-
ering that many IR systems are not robust
in the sense that they need careful ﬁne-
tuning and optimization of their parame-
ters, we explored some parameter settings.
The results show that our method is spe-
cially effective for realistic, non-optimal
settings, adding robustness to the IR en-
gine. We also explored the effect of doc-
ument length, and show that our method
is specially successful with shorter docu-
ments.

1

Introduction

Since the earliest days of IR, researchers noted
the potential pitfalls of keyword retrieval, such
as synonymy, polysemy, hyponymy or anaphora.
Although in principle these linguistic phenom-
ena should be taken into account in order to ob-
tain high retrieval relevance, the lack of algo-
rithmic models prohibited any systematic study
of the effect of this phenomena in retrieval. In-
stead, researchers resorted to distributional se-
mantic models to try to improve retrieval rele-
vance, and overcome the brittleness of keyword
matches. Most research concentrated on Query

Expansion (QE) methods, which typically ana-
lyze term co-occurrence statistics in the corpus
and in the highest scored documents for the orig-
inal query in order to select terms for expanding
the query terms (Manning et al., 2009). Docu-
ment expansion (DE) is a natural alternative to
QE, but surprisingly it was not investigated un-
til very recently. Several researchers have used
distributional methods from similar documents in
the collection in order to expand the documents
with related terms that do not actually occur in the
document (Liu and Croft, 2004; Kurland and Lee,
2004; Tao et al., 2006; Mei et al., 2008; Huang
et al., 2009). The work presented here is com-
plementary, in that we also explore DE, but use
WordNet instead of distributional methods.

Lexical semantic resources such as WordNet
(Fellbaum, 1998) might provide a principled and
explicit remedy for the brittleness of keyword
matches. WordNet has been used with success
in psycholinguistic datasets of word similarity and
relatedness, where it often surpasses distributional
methods based on keyword matches (Agirre et al.,
2009b). WordNet has been applied to IR before.
Some authors extended the query with related
terms (Voorhees, 1994; Liu et al., 2005), while
others have explicitly represented and indexed
word senses after performing word sense disam-
biguation (WSD) (Gonzalo et al., 1998; Stokoe
et al., 2003; Kim et al., 2004). More recently,
a CLEF task was organized (Agirre et al., 2008;
Agirre et al., 2009a) where queries and docu-
ments were semantically disambiguated, and par-
ticipants reported mixed results.

This paper proposes to use WordNet for docu-
ment expansion, proposing a new method: given

10

a full document, a random walk algorithm over
the WordNet graph ranks concepts closely related
to the words in the document. This is in con-
trast to previous WordNet-based work which fo-
cused on WSD to replace or supplement words
with their senses. Our method discovers impor-
tant concepts, even if they are not explicitly men-
tioned in the document. For instance, given a doc-
ument mentioning virus, software and DSL, our
method suggests related concepts and associated
words such us digital subscriber line, phone com-
pany and computer. Those expansion words are
indexed separately, and when combined with the
regular index, we show that they improve the re-
sults in three datasets over a state-of-the-art IR en-
gine (Boldi and Vigna, 2005). The three datasets
used in this study are ResPubliQA (Pe˜nas et al.,
2009), Yahoo! Answers (Surdeanu et al., 2008)
and CLEF-Robust (Agirre et al., 2009a).

Considering that many IR systems are not ro-
bust in the sense that they need careful ﬁne-tuning
and optimization of their parameters, we decided
to study the robustness of our method, explor-
ing some alternative settings, including default pa-
rameters, parameters optimized in development
data, and parameters optimized in other datasets.
The study reveals that the additional semantic ex-
pansion terms provide robustness in most cases.

We also hypothesized that semantic document
expansion could be most proﬁtable when docu-
ments are shorter, and our algorithm would be
most effective for collections of short documents.
We artiﬁcially trimmed documents in the Robust
dataset. The results, together with the analysis of
document lengths of the three datasets, show that
document expansion is specially effective for very
short documents, but other factors could also play
a role.

The paper is structured as follows. We ﬁrst in-
troduce the document expansion technique. Sec-
tion 3 introduces the method to include the expan-
sions in a retrieval system. Section 4 presents the
experimental setup. Section 5 shows our main re-
sults. Sections 6 and 7 analyze the robustness and
relation to document length. Section 8 compares
to related work. Finally, the conclusions and fu-
ture work are mentioned.

2 Document Expansion Using WordNet

Our key insight is to expand the document with
related words according to the background infor-
mation in WordNet (Fellbaum, 1998), which pro-
vides generic information about general vocabu-
lary terms. WordNet groups nouns, verbs, adjec-
tives and adverbs into sets of synonyms (synsets),
each expressing a distinct concept. Synsets are in-
terlinked with conceptual-semantic and lexical re-
lations, including hypernymy, meronymy, causal-
ity, etc.

In contrast with previous work, we select those
concepts that are most closely related to the doc-
ument as a whole. For that, we use a technique
based on random walks over the graph represen-
tation of WordNet concepts and relations.

We represent WordNet as a graph as fol-
lows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets asso-
ciated to them by directed edges. We used ver-
sion 3.0, with all relations provided, including the
gloss relations. This was the setting obtaining the
best results in a word similarity dataset as reported
by Agirre et al. (2009b).

Given a document and the graph-based repre-
sentation of WordNet, we obtain a ranked list of
WordNet concepts as follows:

1. We ﬁrst pre-process the document to obtain
the lemmas and parts of speech of the open
category words.

2. We then assign a uniform probability distri-
bution to the terms found in the document.
The rest of nodes are initialized to zero.

3. We

compute

personalized
PageR-
ank (Haveliwala, 2002) over
the graph,
using the previous distribution as the reset
distribution, and producing a probability
distribution over WordNet concepts The
higher the probability for a concept,
the
more related it is to the given document.

Basically, personalized PageRank is computed
by modifying the random jump distribution vec-
tor in the traditional PageRank equation. In our
case, we concentrate all probability mass in the
concepts corresponding to the words in the docu-

11

ment.

Let G be a graph with N vertices v1, . . . , vN
and di be the outdegree of node i; let M be a N ×
N transition probability matrix, where Mji = 1
di
if a link from i to j exists, and zero otherwise.
Then, the calculation of the PageRank vector Pr
over G is equivalent to resolving Equation (1).

Pr = cM Pr + (1 − c)v

(1)

In the equation, v is a N × 1 vector and c is the
so called damping factor, a scalar value between
0 and 1. The ﬁrst term of the sum on the equa-
tion models the voting scheme described in the
beginning of the section. The second term repre-
sents, loosely speaking, the probability of a surfer
randomly jumping to any node, e.g. without fol-
lowing any paths on the graph. The damping fac-
tor, usually set in the [0.85..0.95] range, models
the way in which these two terms are combined at
each step.

The second term on Eq. (1) can also be seen as a
smoothing factor that makes any graph fulﬁll the
property of being aperiodic and irreducible, and
thus guarantees that PageRank calculation con-
verges to a unique stationary distribution.

In the traditional PageRank formulation the
vector v is a stochastic normalized vector whose
element values are all 1
N , thus assigning equal
probabilities to all nodes in the graph in case of
random jumps. In the case of personalized PageR-
ank as used here, v is initialized with uniform
probabilities for the terms in the document, and
0 for the rest of terms.

PageRank is actually calculated by applying an
iterative algorithm which computes Eq. (1) suc-
cessively until a ﬁxed number of iterations are
executed. In our case, we used a publicly avail-
able implementation1, with default values for the
damping value (0.85) and the number of iterations
(30). In order to select the expansion terms, we
chose the 100 highest scoring concepts, and get
all the words that lexicalize the given concept.

Figure 1 exempliﬁes the expansion. Given the
short document from Yahoo! Answers (cf. Sec-
tion 4) shown in the top, our algorithm produces
the set of related concepts and words shown in the

1http://ixa2.si.ehu.es/ukb/

bottom. Note that the expansion produces syn-
onyms, but also other words related to concepts
that are not mentioned in the document.

3

Including Expansions in a Retrieval
System

Once we have the list of words for document ex-
pansion, we create one index for the words in the
original documents and another index with the ex-
pansion terms. This way, we are able to use the
original words only, or to also include the expan-
sion words during the retrieval.

The retrieval system was implemented using
MG4J (Boldi and Vigna, 2005), as it provides
state-of-the-art results and allows to combine sev-
eral indices over the same document collection.
We conducted different runs, by using only the in-
dex made of original words (baseline) and also by
using the index with the expansion terms of the
related concepts.

BM25 was the scoring function of choice. It is
one of the most relevant and robust scoring func-
tions available (Robertson and Zaragoza, 2009).

wBM 25

Dt

:=

tfDt

k1(cid:16)(1 − b) + b dlD

avdlD(cid:17) + tfDt

(2)

idft

where tfDt is the term frequency of term t in doc-
ument D, dlD is the document length, idft is the
inverted document frequency (or more speciﬁcally
the RSJ weight, (Robertson and Zaragoza, 2009)),
and k1 and b are free parameters.

The two indices were combined linearly, as fol-

lows (Robertson and Zaragoza, 2009):

score(d, e, q) :=

wBM 25

Dt

Xt∈q∩d

(3)

+ λ Xt∈q∩e

wBM 25

Et

where D and E are the original and expanded in-
dices, d, e and q are the original document, the
expansion of the document and the query respec-
tively, t is a term, and λ is a free parameter for the
relative weight of the expanded index.

12

You should only need to turn off virus and anti-spy not uninstall.

And that’s

Then turn them back on later after

done within each of the softwares themselves.
installing any DSL softwares.
06566077-n → computer software, package, software, software package, software program, software system
03196990-n → digital subscriber line, dsl
01569566-v → instal, install, put in, set up
04402057-n → line, phone line, suscriber line, telephone circuit, telephone line
08186221-n → phone company, phone service, telco, telephone company, telephone service
03082979-n → computer, computing device, computing machine, data processor, electronic computer
Figure 1: Example of a document expansion, with original document on top, and some of the relevant
WordNet concepts identiﬁed by our algorithm, together with the words that lexicalize them. Words in
the original document are shown in bold, synonyms in italics, and other related words underlined.

4 Experimental Setup

We chose three data collections. The ﬁrst is based
on a traditional news collection. DE could be
specially interesting for datasets with short docu-
ments, which lead our choice of the other datasets:
the second was chosen because it contains shorter
documents, and the third is a passage retrieval task
which works on even shorter paragraphs. Table 1
shows some statistics about the datasets.

One of the collections is the English dataset
of the Robust task at CLEF 2009 (Agirre et al.,
2009a). The documents are news collections from
LA Times 94 and Glasgow Herald 95. The top-
ics are statements representing information needs,
consisting of three parts: a brief title statement; a
one-sentence description; a more complex narra-
tive describing the relevance assessment criteria.
We use only the title and the description parts of
the topics in our experiments.

The Yahoo! Answers corpus is a subset of a
dump of the Yahoo! Answers web site2 (Surdeanu
et al., 2008), where people post questions and
answers, all of which are public to any web user
willing to browse them. The dataset is a small
subset of the questions, selected for their linguis-
tic properties (for example they all start with ”how
{tokdokdidkdoeskcankwouldkcouldkshould}”).
Additionally, questions and answers of obvious
low quality were removed. The document set was
created with the best answer of each question
(only one for each question).

docs
Robust
166,754
Yahoo!
89610
ResPubliQA 1,379,011

length
532
104
20

q. train
150
1000
100

q. test
160
88610
500

Table 1: Number of documents, average docu-
ment length, number of queries for train and test
in each collection.

The other collection is the English dataset of
ResPubliQA exercise at the Multilingual Ques-
tion Answering Track at CLEF 2009 (Pe˜nas et al.,
2009). The exercise is aimed at retrieving para-
graphs that contain answers to a set of 500 natu-
ral language questions. The document collection
is a subset of the JRC-Acquis Multilingual Paral-
lel Corpus, and consists of 21,426 documents for
English which are aligned to a similar number of
documents in other languages3. For evaluation,
we used the gold standard released by the orga-
nizers, which contains a single correct passage for
each query. As the retrieval unit is the passage,
we split the document collection into paragraphs.
We applied the expansion strategy only to pas-
sages which had more than 10 words (half of the
passages), for two reasons: the ﬁrst one was that
most of these passages were found not to contain
relevant information for the task (e.g. “Article 2”
or “Having regard to the proposal from the Com-
mission”), and the second was that we thus saved
some computation time.

In order to evaluate the quality of our expansion
in practical retrieval settings, the next Section re-

2Yahoo! Webscope dataset “ydata-yanswers-manner-

questions-v1 0” http://webscope.sandbox.yahoo.com/

3Note that Table 1 shows the number of paragraphs,

which conform the units we indexed.

13

base.
MAP
.3781
Robust
MRR .2900
Yahoo!
P@1
.2142
ResPubliQA MRR .3931
.2860

P@1

expa.
.3835***
.2950***
.2183***
.4077***
.3000**

∆
1.43%
1.72%
1.91%
3.72%
4.90%

base.
MAP
.3740
Robust
MRR .3070
Yahoo!
P@1
.2293
ResPubliQA MRR .4970
.3980

P@1

expa.
.3823**
.3100***
.2317*
.4942
.3940

∆
2.20%
0.98%
1.05%
-0.56%
-1.01%

Table 2: Results using default parameters.

Table 3: Results using optimized parameters.

port results with respect to several parameter set-
tings. Parameter optimization is often neglected
in retrieval with linguistic features, but we think it
is crucial since it can have a large effect on rele-
vance performance and therefore invalidate claims
of improvements over the baseline. In each setting
we assign different values to the free parameters in
the previous section, k1, b and λ.

5 Results

The main evaluation measure for Robust is mean
Average Precision (MAP), as customary. In two of
the datasets (Yahoo! and ResPubliQA) there is a
single correct answer per query, and therefore we
use Mean Reciprocal Rank (MRR) and Mean Pre-
cision at rank 1 (P@1) for evaluation. Note that in
this setting MAP is identical to MRR. Statistical
signiﬁcance was computed using Paired Random-
ization Test (Smucker et al., 2007). In the tables
throughout the paper, we use * to indicate statis-
tical signiﬁcance at 90% conﬁdence level, ** for
95% and *** for 99%. Unless noted otherwise,
base. refers to MG4J with the standard index, and
expa.
refers to MG4J using both indices. Best
results per row are in bold when signiﬁcant. ∆ re-
ports relative improvement respect to the baseline.

5.1 Default Parameter Setting
The values for k1 and b are the default values as
provided in the wBM 25 implementation of MG4J,
1.2 and 0.5 respectively. We could not think of a
straightforward value for λ. A value of 1 would
mean that we are assigning equal importance to
original and expanded terms, which seemed an
overestimation, so we used 0.1. Table 2 shows
the results when using the default setting of pa-
rameters. The use of expansion is beneﬁcial in all
datasets, with relative improvements ranging from
1.43% to 4.90%.

Robust

Setting
Default

System
base.
expa.
base.
expa.
basel.
Yahoo!
expa.
ResPubliQA base.
expa.

k1
1.20
1.20
1.80
1.66
0.99
0.84
0.09
0.13

b

0.50
0.50
0.64
0.55
0.82
0.87
0.56
0.65

λ
-

-

-

-

0.100

0.075

0.146

0.090

Table 4: Parameters as in the default setting or as
optimized in each dataset. The λ parameter is not
used in the baseline systems.

5.2 Optimized Parameter Setting
We next optimized all three parameters using the
train part of each collection. The optimization of
the parameters followed a greedy method called
“promising directions” (Robertson and Zaragoza,
2009). The comparison between the baseline and
expansion systems in Table 3 shows that expan-
sion helps in Yahoo! and Robust, with statistical
signiﬁcance. The differences in ResPubliQA are
not signiﬁcant, and indicate that expansion terms
were not helpful in this setting.

Note that the optimization of the parameters
yields interesting effects in the baseline for each
of the datasets. If we compare the results of the
baseline with default settings (Table 2) and with
optimized setting (Table 3), the baseline improves
MRR dramatically in ResPubliQA (26% relative
improvement), signiﬁcantly in Yahoo! (5.8%) and
decreases MAP in Robust (-0.01%). This dis-
parity of effects could be explained by the fact
that the default values are often approximated us-
ing TREC-style news collections, which is exactly
the genre of the Robust documents, while Yahoo
uses shorter documents, and ResPubliQA has the
shortest documents.

Table 4 summarizes the values of the parame-
ters in both default and optimized settings. For k1,
the optimization yields very different values. In
Robust the value is similar to the default value, but

14

base.
MAP
.3781
Rob
MRR .2900
Y!
P@1
.2142
ResP. MRR .3931
.2860

P@1

expa.
.3881***
.2980***
.2212***
.4221***
.3180**

∆

λ
2.64% 0.18
2.76% 0.27
3.27%
7.39% 0.61
11.19%

Table 5: Results obtained using the λ optimized
setting, including actual values of λ.

in ResPubliQA the optimization pushes it down
below the typical values cited in the literature
(Robertson and Zaragoza, 2009), which might ex-
plain the boost in performance for the baseline in
the case of ResPubliQA. When all three param-
eters are optimized together, the values λ in the
table range from 0.075 to 0.146. The values of the
optimized λ can be seem as an indication of the
usefulness of the expanded terms, so we explored
this farther.

5.3 Exploring λ
As an additional analysis experiment, we wanted
to know the effect of varying λ keeping k1 and b
constant at their default values. Table 5 shows the
best values in each dataset, which that the weight
of the expanded terms and the relative improve-
ment are highly correlated.

5.4 Exploring Number of Expansion

Concepts

One of the free parameters of our system is the
number of concepts to be included in the docu-
ment expansion. We have performed a limited
study with the default parameter setting on the
Robust setting, using 100, 500 and 750 concepts,
but the variations were not statistically signiﬁcant.
Note that with 100 concepts we were actually ex-
panding with 268 words, with 500 concepts we
add 1247 words and with 750 concepts we add
1831 words.

6 Robustness

The results in the previous section indicate that
optimization is very important, but unfortunately
real applications usually lack training data. In this
Section we wanted to study whether the param-
eters can be carried over from one dataset to the
other, and if not, whether the extra terms found by

Rob.

Y!

ResP.

train
base.
.3781
def. MAP
.3740
Rob. MAP
.3786
Y!
MAP
Res. MAP
.3146
def. MRR .2900
Rob. MRR .2920
Y!
MRR .3070
Res. MRR .2600
def. MRR .3931
Rob. MRR .3066
Y!
MRR .3010
Res. MRR .4970

expa.
.3835***
.3823**
.3759
.3346***
.2950***
.2920
.3100**
.2750***
.4077***
.3655***
.3459***
.4942

∆
1.43%
2.20%
-0.72%
6.35%
1.72%
0.0%
0.98%
5.77%
3.72%
19.22%
14.93%
-0.56%

Table 6: Results optimizing parameters with train-
ing from other datasets. We also include default
and optimization on the same dataset for compar-
ison. Only MRR and MAP results are given.

DE would make the system more robust to those
sub-optimal parameters.

Table 6 includes a range of parameter set-
tings, including defaults, and optimized parame-
ters coming from the same and different datasets.
The values of the parameters are those in Table
4. The results show that when the parameters are
optimized in other datasets, DE provides improve-
ment with statistical signiﬁcance in all cases, ex-
cept for the Robust dataset when using parameters
optimized from Yahoo! and vice-versa.

Overall, the table shows that our DE method ei-
ther improves the results signiﬁcantly or does not
affect performance, and that it provides robustness
across different parameter settings, even with sub-
optimal values.

7 Exploring Document Length

The results in Table 6 show that
the perfor-
mance improvements are best in the collection
with shortest documents (ResPubliQA). But the
results for Robust and Yahoo! do not show any re-
lation to document length. We thus decided to do
an additional experiment artiﬁcially shrinking the
document in Robust to a certain percentage of its
original length. We create new pseudo-collection
with the shrinkage factors of 2.5%, 10%, 20% and
50%, keeping the ﬁrst N% words in the document
and discarding the rest. In all cases we used the
same parameters, as optimized for Robust.

Table 7 shows the results (MAP), with some
clear indication that the best improvements are ob-

15

tained for the shortest documents.

2.5%
10%
20%
50%
100%

length
13
53
107
266
531

base.
.0794
.1757
.2292
.3063
.3740

expa.
.0851
.1833
.2329
.3098
.3823

∆
7.18%
4.33%
1.61%
1.14%
2.22%

Table 7: Results (MAP) on Robust when arti-
ﬁcially shrinking documents to a percentage of
their length. In addition to the shrinking rate we
show the average lengths of documents.

8 Related Work
Given the brittleness of keyword matches, most
research has concentrated on Query Expansion
(QE) methods. These methods analyze the user
query terms and select automatically new related
query terms. Most QE methods use statistical
(or distributional) techniques to select terms for
expansion. They do this by analyzing term co-
occurrence statistics in the corpus and in the high-
est scored documents of the original query (Man-
ning et al., 2009). These methods seemed to im-
prove slightly retrieval relevance on average, but
at the cost of greatly decreasing the relevance of
difﬁcult queries. But more recent studies seem
to overcome some of these problems (Collins-
Thompson, 2009).

An alternative to QE is to perform the expan-
sion in the document. Document Expansion (DE)
was ﬁrst proposed in the speech retrieval commu-
nity (Singhal and Pereira, 1999), where the task
is to retrieve speech transcriptions which are quite
noisy. Singhal and Pereira propose to enhance the
representation of a noisy document by adding to
the document vector a linearly weighted mixture
of related documents.
In order to determine re-
lated documents, the original document is used as
a query into the collection, and the ten most rele-
vant documents are selected.

Two related papers (Liu and Croft, 2004; Kur-
land and Lee, 2004) followed a similar approach
on the TREC ad-hoc document retrieval task.
They use document clustering to determine simi-
lar documents, and document expansion is carried
out with respect to these. Both papers report sig-
niﬁcant improvements over non-expanded base-

lines. Instead of clustering, more recent work (Tao
et al., 2006; Mei et al., 2008; Huang et al., 2009)
use language models and graph representations of
the similarity between documents in the collec-
tion to smooth language models with some suc-
cess. The work presented here is complementary,
in that we also explore DE, but use WordNet in-
stead of distributional methods. They use a tighter
integration of their expansion model (compared to
our simple two-index model), which coupled with
our expansion method could help improve results
further. We plan to explore this in the future.

An alternative to statistical expansion methods
is to use lexical semantic knowledge bases such as
WordNet. Most of the work has focused on query
expansion and the use of synonyms from Word-
Net after performing word sense disambiguation
(WSD) with some success (Voorhees, 1994; Liu
et al., 2005). The short context available in
the query when performing WSD is an impor-
tant problems of these techniques.
In contrast,
we use full document context, and related words
beyond synonyms. Another strand of WordNet
based work has explicitly represented and indexed
word senses after performing WSD (Gonzalo et
al., 1998; Stokoe et al., 2003; Kim et al., 2004).
The word senses conform a different space for
document representation, but contrary to us, these
works incorporate concepts for all words in the
documents, and are not able to incorporate con-
cepts that are not explicitly mentioned in the doc-
ument. More recently, a CLEF task was orga-
nized (Agirre et al., 2009a) where terms were se-
mantically disambiguated to see the improvement
that this would have on retrieval; the conclusions
were mixed, with some participants slightly im-
proving results with information from WordNet.
To the best of our knowledge our paper is the ﬁrst
on the topic of document expansion using lexical-
semantic resources.

We would like to also compare our performance
to those of other systems as tested on the same
datasets. The systems which performed best in
the Robust evaluation campaign (Agirre et al.,
2009a) report 0.4509 MAP, but note that they de-
ployed a complex system combining probabilis-
tic and monolingual translation-based models. In
ResPubliQA (Pe˜nas et al., 2009), the ofﬁcial eval-

16

uation included manual assessment, and we can-
not therefore reproduce those results. Fortunately,
the organizers released all runs, but only the ﬁrst
ranked document for each query was included, so
we could only compute P@1. The P@1 of best
run was 0.40. Finally (Surdeanu et al., 2008) re-
port MRR ﬁgure around 0.68, but they evaluate
only in the questions where the correct answer
is retrieved by answer retrieval in the top 50 an-
swers, and is thus not comparable to our setting.
Regarding the WordNet expansion technique
we use here, it is implemented on top of publicly
available software4, which has been successfully
used in word similarity (Agirre et al., 2009b) and
word sense disambiguation (Agirre and Soroa,
2009).
In the ﬁrst work, a single word was in-
put to the random walk algorithm, obtaining the
probability distribution over all WordNet synsets.
The similarity of two words was computed as the
similarity of the distribution of each word, obtain-
ing the best results for WordNet-based systems on
the word similarity dataset, and comparable to the
results of a distributional similarity method which
used a crawl of the entire web. Agirre et al. (2009)
used the context of occurrence of a target word to
start the random walk, and obtained very good re-
sults for WordNet WSD methods.

9 Conclusions and Future Work

This paper presents a novel Document Expan-
sion method based on a WordNet-based system
to ﬁnd related concepts and words. The docu-
ments in three datasets were thus expanded with
related words, which were fed into a separate in-
dex. When combined with the regular index we
report improvements over MG4J using wBM 25 for
those three datasets across several parameter set-
tings, including default values, optimized param-
eters and parameters optimized in other datasets.
In most of the cases the improvements are sta-
tistically signiﬁcant, indicating that the informa-
tion in the document expansion is useful. Similar
to other expansion methods, parameter optimiza-
tion has a stronger effect than our expansion strat-
egy. The problem with parameter optimization is
that in most real cases there is no tuning dataset

4http://ixa2.si.ehu.es/ukb

available. Our analysis shows that our expansion
method is more effective for sub-optimal param-
eter settings, which is the case for most real-live
IR applications. A comparison across the three
datasets and using artiﬁcially trimmed documents
indicates that our method is particularly effective
for short documents.

As document expansion is done at indexing
time, it avoids any overhead at query time.
It
also has the advantage of leveraging full document
context, in contrast to query expansion methods,
which use the scarce information present in the
much shorter queries. Compared to WSD-based
methods, our method has the advantage of not
having to disambiguate all words in the document.
Besides, our algorithm picks the most relevant
concepts, and thus is able to expand to concepts
which are not explicitly mentioned in the docu-
ment. The successful use of background informa-
tion such as the one in WordNet could help close
the gap between semantic web technologies and
IR, and opens the possibility to include other re-
sources like Wikipedia or domain ontologies like
those in the Uniﬁed Medical Language System.

Our method to integrate expanded terms using
an additional index is simple and straightforward,
and there is still ample room for improvement.
A tighter integration of the document expansion
technique in the retrieval model should yield bet-
ter results, and the smoothed language models of
(Mei et al., 2008; Huang et al., 2009) seem a
natural choice. We would also like to compare
with other existing query and document expan-
sion techniques and study whether our technique
is complementary to query expansion approaches.

Acknowledgments
This work has been supported by KNOW2
(TIN2009-14715-C04-01) and KYOTO (ICT-
2007-211423) projects. Arantxa Otegi’s work is
funded by a PhD grant from the Basque Govern-
ment. Part of this work was done while Arantxa
Otegi was visiting Yahoo! Research Barcelona.

References
Agirre, E. and A. Soroa. 2009. Personalizing PageR-
In Proc. of

ank for Word Sense Disambiguation.

17

Manning, C. D., P. Raghavan, and H. Sch¨utze. 2009.
An introduction to information retrieval. Cam-
bridge University Press, UK.

Mei, Qiaozhu, Duo Zhang, and ChengXiang Zhai.
2008.
A general optimization framework for
smoothing language models on graph structures. In
Proceedings of SIGIR ’08, pages 611–618.

Pe˜nas, A., P. Forner, R. Sutcliffe, A. Rodrigo,
C. For˘ascu, I. Alegria, D. Giampiccolo, N. Moreau,
and P. Osenova. 2009. Overview of ResPubliQA
2009: Question Answering Evaluation over Euro-
pean Legislation.
In Working Notes of the Cross-
Lingual Evaluation Forum.

Robertson, S. and H. Zaragoza. 2009. The Proba-
bilistic Relevance Framework: BM25 and Beyond.
Foundations and Trends in Information Retrieval,
3(4):333–389.

Singhal, A. and F. Pereira. 1999. Document expansion
for speech retrieval. In Proceedings of SIGIR ’99,
pages 34–41, New York, NY, USA. ACM.

Smucker, M. D., J. Allan, and B. Carterette. 2007. A
comparison of statistical signiﬁcance tests for infor-
mation retrieval evaluation. In Proc. of CIKM 2007,
Lisboa, Portugal.

Stokoe, C., M. P. Oakes, and J. Tait. 2003. Word sense
disambiguation in information retrieval revisited. In
Proceedings of SIGIR ’03, page 166.

Surdeanu, M., M. Ciaramita, and H. Zaragoza. 2008.
Learning to Rank Answers on Large Online QA
Collections. In Proceedings of ACL 2008.

Tao, T., X. Wang, Q. Mei, and C. Zhai. 2006. Lan-
guage model information retrieval with document
expansion.
In Proceedings of HLT/NAACL, pages
407–414, June.

Voorhees, E. M. 1994. Query expansion using lexical-
In Proceedings of SIGIR ’94,

semantic relations.
page 69.

EACL 2009, Athens, Greece.

Agirre, E., G. M. Di Nunzio, N. Ferro, T. Mandl,
and C. Peters. 2008. CLEF 2008: Ad-Hoc Track
Overview. In Working Notes of the Cross-Lingual
Evaluation Forum.

Agirre, E., G. M. Di Nunzio, T. Mandl, and A. Otegi.
2009a. CLEF 2009 Ad Hoc Track Overview: Ro-
bust - WSD Task. In Working Notes of the Cross-
Lingual Evaluation Forum.

Agirre, E., A. Soroa, E. Alfonseca, K. Hall, J. Kraval-
ova, and M. Pasca. 2009b. A Study on Similarity
and Relatedness Using Distributional and WordNet-
based Approaches.
In Proc. of NAACL, Boulder,
USA.

Boldi, P. and S. Vigna. 2005. MG4J at TREC 2005.
In The Fourteenth Text REtrieval Conference (TREC
2005) Proceedings, number SP 500-266 in Special
Publications. NIST.

Collins-Thompson, Kevyn. 2009. Reducing the risk
of query expansion via robust constrained optimiza-
tion. In Proceedings of CIKM ’09, pages 837–846.

Fellbaum, C., editor.

1998. WordNet: An Elec-
tronic Lexical Database and Some of its Applica-
tions. MIT Press, Cambridge, Mass.

Gonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with WordNet synsets can improve
text retrieval. In Proceedings ACL/COLING Work-
shop on Usage of WordNet for Natural Language
Processing.

Haveliwala, T. H. 2002. Topic-sensitive PageRank. In

Proceedings of WWW ’02, pages 517–526.

Huang, Yunping, Le Sun, and Jian-Yun Nie. 2009.
Smoothing document language model with local
word graph.
In Proceedings of CIKM ’09, pages
1943–1946.

Kim, S. B., H. C. Seo, and H. C. Rim. 2004. Informa-
tion retrieval using word senses: root sense tagging
approach. In Proceedings of SIGIR ’04, pages 258–
265.

Kurland, O. and L. Lee. 2004. Corpus structure, lan-
guage models, and ad hoc information retrieval. In
Proceedings of SIGIR ’04, pages 194–201.

Liu, X. and W. B. Croft. 2004. Cluster-based retrieval
In Proceedings of SIGIR

using language models.
’04, pages 186–193.

Liu, S., C. Yu, and W. Meng. 2005. Word sense dis-
In Proceedings of CIKM

ambiguation in queries.
’05, pages 525–532.

