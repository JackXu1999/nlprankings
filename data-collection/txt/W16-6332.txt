



















































Proceedings of the...


D S Sharma, R Sangal and A K Singh. Proc. of the 13th Intl. Conference on Natural Language Processing, pages 259–266,
Varanasi, India. December 2016. c©2016 NLP Association of India (NLPAI)

Use of Semantic Knowledge Base for Enhancement of Coherence of
Code-mixed Topic-Based Aspect Clusters

Kavita Asnani
Computer Engineering Dept
Goa College of Engineering

Goa
India

kavita@gec.ac.in

Jyoti D Pawar
Computer Science and Technology Dept

Goa University
Goa
India

jyotidpawar@gmail.com

Abstract

In social media code-mixing is getting
very popular due to which there is enor-
mous generation of noisy and sparse mul-
tilingual text which exhibits high disper-
sion of useful topics which people dis-
cuss. Also, the semantics is expressed
across random occurrence of code-mixed
words. In this paper, we propose
code-mixed knowledge based LDA (cmk-
LDA), which infers latent topic based as-
pects from code-mixed social media data.
We experimented on FIRE 2014, a code-
mixed corpus and showed that with the
help of semantic knowledge from mul-
tilingual external knowledge base, cmk-
LDA learns coherent topic-based aspects
across languages and improves topic inter-
pretibility and topic distinctiveness better
than the baseline models . The same is
shown to have agreed with human judg-
ment.

1 Introduction

The huge amount of social media text available
online is becoming increasingly popular thereby
providing an additional opportunity of mining use-
ful information from it. Therefore, most of the re-
search on social media text has concentrated on
English chat data or on multilingual data where
each message as a component is monolingual. In
social media, people often switch between two or
more languages, both at conversation level and at
message level (Ling et al., 2013). However, ma-
jority of conversational data on social network-
ing forums is informal and occurs in random mix
of languages (Das and Gambäck, 2014). When
this code alternation occurs at or above the utter-
ance level, the phenomenon is referred to as code-
switching; when the alternation is utterance inter-
nal, the term code-mixing is common (Gambäck

and Das, 2016). Thus, code-mixing while chatting
has become prevalent in current times. However,
exponentially increasing large volumes of short
and long code-mixed messages contain lot of noise
and has useful information highly dispersed. Un-
fortunately, it is not an easy task to retrieve useful
knowledge from such data as code-mixing occurs
at different levels of code-complexity and imposes
fundamental challenges namely:

1. Code-mixed social media data is multilin-
gual, usually bilingual (San, 2009). There-
fore, semantics is spread across languages.

2. Social media data do not have specific termi-
nology (Eisenstein, 2013).

Therefore, using training data from parallel or
comparable corpora will not be useful in this con-
text. Also, availability of pre annotated corpora for
all language pairs used in social media may prac-
tically be very difficult to obtain. Our objective is
to model unsupervised aspect extraction using top-
ics, from the code-mixed context to obtain useful
knowledge. Probabilistic Latent Semantic Analy-
sis (pLSA) (Hofmann, 1999) and Latent Dirichlet
Allocation (LDA)(Blei et al., 2003) are popularly
recommended unsupervised topic modeling meth-
ods for this purpose; but a shortcoming with them
is that they result in extracting some incoherent
topics (Chang et al., 2009).This is due to the oc-
currence of some irrelevant and polysemous terms
in extraction (Chang et al., 2009), which is likely
to get aggregated in multilingual content. There-
fore, in code-mixed context extraction of incoher-
ent topics are highly likely to occur. In order to
cross over the word level language barrier and aug-
ment each word with its semantic description we
propose to leverage knowledge from external mul-
tilingual semantic knowledge base such as Babel-
Net v3.0 1 (Navigli et al., 2012) which is created

1http://babelnet.org259



from integration of both Wikipedia and WordNet
(Miller, 1995).

Our approach of incorporating semantic knowl-
edge in LDA topic model has resemblance with
General Knowledge based LDA (GK-LDA)(Chen
et al., 2013) model. However, their sets were
monolingual and were constructed using Word-
Net. They called their semantic sets as Lexical
Relation Sets(LR-sets) and were comprising of
synonym, antonym and adjective relations. They
addressed the problem of multiple senses using
synonyms and antonyms in LR-Sets. However,
in their work since semantic knowledge is aug-
mented at word level not all LR-sets resulted in
correct context knowledge. They handled this
problem by using explicit word-correlation ma-
trix and also fixed the wrong knowledge explic-
itly. But we take a different approach. In a sin-
gle step, we obtain disambiguated synsets for a
code-mixed message, retrieving correct multilin-
gual synsets appropriate to the context across lan-
guages. This also resolves language related multi-
ple senses issue. As per our knowledge, our pro-
posed model is the first model to exploit semantic
knowledge from BabelNet in topic models for pro-
ducing coherent topics from code-mixed data.

The remaining part of the paper is organized as
follows: Section 2 presents related work, Section
3 describes our proposed work, Section 4 gives
implementation details, Section 5 presents experi-
mental results and Section 6 states the conclusion.

2 Related Work

Code-switching and code-mixing are popularly
been observed on social networking forums, es-
pecially in highly multilingual societies. In
this paper, we will use the term code-mixing
to refer to both of these situations. In gen-
eral, multilingual users communicate in two or
more languages. In (Barman et al., 2014) study
was performed on code-mixed content by col-
lecting data from Facebook posts, code-mixed
in three languages Bengali(BN)-English(EN)-
Hindi(HI). However, in (Bali et al., 2014), the
analysis of code-mixed bilingual English-Hindi
data showed significant amount of code-mixing
and proved that most of the active users on face-
book are bilingual. Out of 43,012 words on face-
book chat, 38,144 were written in Roman script
and 2661 in Devanagari script. They claimed
that the deeper analysis of such data requires dis-

course linguists. We first briefly describe survey
studies addressing characteristics of code-mixed
text content which are affected by linguistic chal-
lenges. Linguistic processing of code-switched
data was done by (Vyas et al., 2014) (Sequiera et
al., 2015)(Solorio and Liu, 2008)(San, 2015) and
they concluded that natural language processing
tools are not equipped to resolve all issues related
to pre-processing of code-mixed text. (Vyas et
al., 2014) created multi-level annotated corpus of
Hindi-English code-mixed text from Facebook fo-
rums. The annotations were attributed across three
levels representing POS tags, language identifica-
tion and transliteration respectively. They proved
that POS tagging of code-mixed text cannot be
done by placing two or more monolingual POS
taggers together. They identified that it is complex
to deal with code-mixed data due to spelling vari-
ation. (Sequiera et al., 2015) experimented with
joint modeling of language identification and POS
tagging. (San, 2015) used Random Forest based
method on 400 code-mixed utterances from Face-
book and Twitter and reported 63.5% word level
tagging accuracy. Their work also illustrates chal-
lenge in POS tagging and need for transliteration.

Thus, code-mixed social media data suffer from
its associated linguistic complexities which make
the semantic interpretation of such high dimen-
sional content very challenging. Also, content
analysis in social media containing Twitter posts
where training and evaluation is concerned, is
done using supervised models in machine learn-
ing and NLP (Ramage et al., 2010). They claimed
that this requires good prior knowledge about data.
Such supervised approaches are not feasible in the
context of code-mixed chat data as such data is
generated randomly in any language and availabil-
ity of parallel or comparable corpora for training
in certain language pairs is difficult. Also, the use
of machine translators is very challenging and not
feasible in social media context as the volume of
the data is large, inconsistent and translation is re-
quired to be done at the word-level. Using a shal-
low parser (Sharma et al., 2016), our proposed ap-
proach first addresses noise elimination and need
for normalization. Then, we were motivated to
model appropriate unsupervised topic based as-
pect extraction to discover useful knowledge of-
fering significant information to the administrator
or end user. Therefore, we turned to unsupervised
topic models, as our goal is to use large collection260



of code-mixed documents and convert it into as-
pects of the text in the form of clusters of similar
themes together called as topics.

(Peng et al., 2014) proposed code-switched
LDA (cs-LDA) which is used for topic alignment
and to find correlations across languages from
code-switched social media text. They used two
code-switched corpora (English- Spanish Twitter
data and English-Chinese Weibo data) and per-
formed per topic language-specific word distribu-
tion and per word language identification. They
showed that cs-LDA improves perplexity over
LDA, and learns semantically coherent aligned
topics as judged by the human annotators. In ad-
dition, as code-mixed social media data involve
words occurring in random mix of languages, very
few languages have word-level language identifi-
cation systems in place.

In this paper, we propose utilizing information
from large multilingual semantic knowledge base
called BabelNet (Navigli et al., 2012) as it pro-
vides the same concept expressed in many dif-
ferent languages; which can be used to augment
information to code-mixed words, thereby drop-
ping the language barrier. BabelNet (Navigli et
al., 2012) offers wide coverage of lexicographic
and encyclopedic terms. BabelNet provides multi-
lingual synsets where each synset is represented
by corresponding synonymous concepts in dif-
ferent languages. BabelNet v3.0 offers coverage
for 271 languages with 14 million entries com-
prising of 6M concepts, 745M word senses and
380M semantic relations.The knowledge incorpo-
rated from BabelNet is used to guide our proposed
cmk-LDA topic model.

In order to handle wrong knowledge (injected
due to multiple senses), Babelfy (Moro et al.,
2014) is used to obtain disambiguated code-mixed
words across the message. Babelfy leverages in-
formation from BabelNet for its joint approach to
multilingual word sense disambiguation and en-
tity linking. It performs semantic interpretation
of an ambiguous sentence using a graph and then
extracts the densest subgraph as the most coher-
ent interpretation. In order to discover knowl-
edge from social media (Manchanda, 2015) inves-
tigated to find new entities and disambiguation as
a joint task on short microblog text. They aimed
to improve the disambiguation of entities using
linked datasets and also discovery of new entities
from tweets, thus improving the overall accuracy

Figure 1: An Example

of the system.

3 Our Proposed Work

In this section we introduce our proposed work.
We first present how we addressed random oc-
currence of words in different languages in a
code-mixed message. For this purpose, we
utilize knowledge from BabelNet which helps
augment semantic interpretations of code-mixed
words across languages in the form of multilingual
knowledge Sets (mulkSets). We propose a
new knowledge based LDA for code-mixed data,
which we have called code-mixed knowledge
based LDA (cmk-LDA). In order to automatically
deal with random words occurring in different lan-
guages we add a new latent variable k in LDA,
which denotes the mulkSet assignment to each
word. We initially tried to construct mulkSets by
directly obtaining synsets from BabelNet. But for
each code-mixed word it resulted in retrieval of
large number of synsets. This is due to all the
possible multilingual senses assigned at the word
level. Therefore, for correct semantic interpreta-
tion we had to ensure that code-mixed words shar-
ing the same context should share similar sense in
their mulkSets. Hence, we constructed mulkSet
with disambiguated synsets using BabelFy 2. Such
disambiguated synsets address the shared context
across languages. An example is presented in
Figure 1. Our disambiguated set therefore con-
tains the revised vocabulary having three words.
We found this knowledge to be beneficial to our
proposed cmk-LDA topic model as each topic is
a multinomial distribution over mulkSets. Thus,
cmk-LDA model finds co-occuring words auto-
matically in a language independent manner. For
the purpose of demonstration at the sentence level
we illustrate two instances in the Figure 3. The

2http://babelfy.org261



first sentence in Figure 3 is an input code-mixed
sentence to Babelfy and the second sentence is
the disambiguated output further augmented with
synsets across languages from BabelNet. Based
on this knowledge, cmk-LDA model further gen-
erates topic-based aspects across sentences by pro-
cessing probability distribution over mulkSets.

3.1 The cmk-LDA Model
Given a collection of code-mixed messages
M= {mL1 ,m

L
2 ,m

L
3 , ....,m

L
n }

where n denotes number of code-mixed messages
in L=l1, l2, l3,..., ll languages where l denotes num-
ber of languages in which code-mixing has oc-
curred.

The code-mixed message is represented as

m
L
i = {wLi1, wLi2wLi3 ..., wLiNi}

where Ni denotes number of words in the i
th

mes-
sage and wij denotes j

th
word of the i

th
message.

Figure 2 shows graphical representation for our
proposed cmk-LDA model. Each circle node in-
dicates a random variable and the shaded node
indicates w, which is the only observed variable.
We introduce new latent variable k, which assigns
mulkSet to each word. Assume that there are K
mulksets in total. Therefore, language indepen-
dent code-mixed topics across the chat collection
are given as: Z= { z1, z2, z3,..., zk }

Each code-mixed message is thus considered as
a mixture of K latent code-mixed topics from the
set Z. These topic distributions are modeled by
probability scores P ( zk m

i
) Thus, M is repre-

sented as set Z of latent concepts present in M.
We have presented the generative process in Al-

gorithm 1.
We performed approximate inference in cmk-

LDA model using the block Gibbs sampler as
followed typically in LDA. Gibbs sampling con-
structs Markov chain over latent variables, by
computing the conditional distribution to assign a
topic z and the mulkSet k to the word. The condi-
tional distribution for sampling posterior is given
in Equation 1.

P (zi, ki‖z−i, k−i, w, α, β, γ) ∝
nz,m−i + α

nm−i + zα
X

(nk,z)−i + β
nk−i +Kβ

X
(nz,k,wi)−i + γ

nz,k−i +Wγ
(1)

1. foreach topic z ∈ Z do
Draw mulkSet distribution
ϕ ∼ Dir(β)

foreach mulkSet k ∈ { 1, ...,K } do
Draw mulkSet distribution over words
ψz x k ∼ Dir(γ)
end

end

2. foreach code-mixed message m∈ M do
Draw topic distribution θm ∼ Dir(α)

foreach code-mix word
w

l
m,nwhere language l∈ L and L ={

l1, l2, l3,..., ll } and n∈ { 1...Nm } do
Draw a topic zm,n ∼ θm
Draw a k-mulkSet km,n ∼ ϕzm,n
Draw a topic wm,n ∼ ψ zm,n,km,n
end

end

Algorithm 1: cmk-LDA Generative Process

Figure 2: cmk-LDA Plate Notation

Figure 3: Example Sentences262



Figure 6 shows the sample clusters generated
by cmk-LDA.

4 Implementation Details

We have evaluated the proposed cmk-LDA model
and compare it with the two baselines pLSA (Hof-
mann, 1999) and LDA(Blei et al., 2003). In our
experiments, our proposed cmk-LDA model can
deal with words randomly sharing context in ei-
ther of the two languages Hindi or English. We
compared the models by measuring coherence of
aspect clusters. For evaluating semantic coherence
of topics we used two evaluation metrics; topic co-
herence(UMass) and KL-Divergence to measure
topic interpretability and topic distinctiveness re-
spectively. We performed experiments on four
models based on use of external semantic knowl-
edge. The two baselines are addressed as wek-
PLSA and wek-LDA where wek indicates models
without external knowledge. We perform testing
with different number of topics k and we made
sure that we compare topic aspect clusters of the
same size.

4.1 Dataset Used

We performed experiments on FIRE 20143(Forum
for IR Evaluation) for shared task on transliter-
ated search. This dataset comprises of social
media posts in English mixed with six other In-
dian languages.The English-Hindi corpora from
FIRE 2014 was introduced by (Das and Gambäck,
2014). It consists of 700 messages with the total
of 23,967 words which were taken from Facebook
chat group for Indian University students. The
data contained 63.33% of tokens in Hindi. The
overall code-mixing percentage for English-Hindi
corpus was as high as 80% due to the frequent
slang used in two languages randomly during the
chat (Das and Gambäck, 2014).

4.2 Code-mixed data pre-processing

In our proposed cmk-LDA topic model we believe
that a topic is semantically coherent if it assigns
high probability scores to words that are semanti-
cally related irrespective of the language in which
they are written. In the pre-processing phase, we
used Shallow parser (Sharma et al., 2016) for our
purpose to obtain normalized output. (Sharma et
al., 2016) experimented on the same code-mixed
English-Hindi FIRE 2014 dataset as we did and

3http://www.isical.ac.in/ fire/

Table 1: Cohens Kappa for inter-rater agreement
Index 1 2 3 4 5
k 3 6 9 12 15
Precision@k 0.899 0.798 0.876 0.712 0.766

they reported the accuracy on parsing as 74.48%
and 75.07% respectively. Basically noise was
eliminated by removal of stop-words 4 for Hindi
and English.

4.3 Code-Mixed Message as a Document

Topic models are applied to documents to produce
topics from them (Titov and McDonald, 2008).
The key step in our method is to determine context
and for that we address the code-mixed words co-
occurring in same context. Such words with sim-
ilar probabilities belong to the same topic and re-
jects words that have different probabilities across
topics. Therefore, we treat each code-mixed mes-
sage independently. Although, relationship be-
tween messages is lost, the code-mixed words
across the language vocabulary within the code-
mix message contribute to the message context.
This representation is fair enough as it is suitable
to obtain relevant disambiguated sets from Ba-
belfy as it resolves context at the message level.

5 Experimental Results

5.1 Measuring Topic Quality by Human
Judgement

We followed (Mimno et al., 2011)(Chuang et
al., 2013) to evaluate quality of each topic as
(good, intermediate, or bad). The topics were
annotated as good if they contained more than
half of its words that could be grouped together
thematically, otherwise bad. Each topic was
presented as a list in the steps of 5, 10, 15 and
20 code- mixed aspects generated by cmk-LDA
model and sorted in the descending order of
probabilities under that topic. For each topic, the
judges annotated the topics at word level and then
we aggregated their results to annotate the cluster.
Table 1 reports the Cohens Kappa score for topic
annotation, which is above 0.5, indicating good
agreement. We observed a high score at k=3 due
to few aspect topics with context highly dispersed
resulting in strong agreement on low quality
clusters. According to the scale the Kappa score

4https://sites.google.com/site/kevinbouge/stopwords-lists263



Figure 4: Topic Interpretibility Comparison

increases with more number of topic-based aspect
clusters as the topics get semantically stronger.
Relatively high agreement at k=9 points to the
likely generation of good quality aspect topics.

5.2 Measuring Topic Interpretability

The UMass measure uses a pairwise score func-
tion introduced by (Mimno et al., 2011). The score
function is not symmetric as it is an increasing
function of the empirical probability of common
words. Figure 4 shows testing for topic inter-
pretability of topic based aspect clusters for com-
parison of all models with and without external
knowledge. We see from the trend generated by
cmk-LDA relative to the other models, generates
rise indicating enhancement in coherence of topic
distributions. Since most of the topics across lan-
guages indicate common context, such high prob-
ability topics in a cluster seem to be contributing
to high UMass coherence. Both pLSA and cmk-
LDA offer better topic interpretibility of clusters.
These results confirm that incorporating multilin-
gual external semantic knowledge in code-mixed
data find higher quality topics offering better in-
terpretation.

5.3 Measuring Topic Distinctiveness

The Kullback Leibler (KL) divergence measure
(Johnson and Sinanovic, 2001) is a standard mea-
sure for comparing distributions. We apply sym-
metrical version of KL Divergence and average it
across the topics. Higher KL divergence score in-
dicates that the words across the topics are dis-
tinct and are considered to generate higher topic

Figure 5: Topic Distinctivity Comparison

distinctiveness. From the Figure 5, we can see
that initially at k=3, cmk-LDA is lower because
the context is wide and therefore words are com-
mon across the clusters. While KL scores of
pLSA model closely follow cmk-LDA, topics at
k=9 have generated higher KL score for cmk-
LDA. This suggests that though the interpretibility
of topics gains is as high in cmk-LDA, the topics
are more distinct as well.

6 Conclusion and Future Work

In order to enhance coherence of topic-based as-
pects discovered from code -mixed social media
data, we proposed a novel unsupervised cmk-LDA
topic model which utilizes semantic knowledge
from external resources. Such knowledge resolves
semantic interpretations across random mix of lan-
guages. Our evaluation results show that cmk-
LDA outperforms the baselines. We state that our
proposed framework supports the utility of lexical
and semantic knowledge freely available in exter-
nal multilingual resources which can drop the lan-
guage barrier and can help discover useful aspects.

We have not addressed mixed-script in our ex-
periments. In our future experiments we will
explore methods to perform term matching and
spelling variation modelling the terms across the
scripts.

References
David M Blei, Andrew Y Ng, and Michael I Jor-

dan. Latent dirichlet allocation. Journal of machine
Learning research, 3(Jan):993–1022, 2003.

Jonathan Chang, Sean Gerrish, Chong Wang, Jor-264



Figure 6: Sample Topics with Probability (Top n probability aspects comprise topics)

dan L Boyd-Graber, and David M Blei. Reading
tea leaves: How humans interpret topic models. In
Advances in neural information processing systems,
pages 288–296, 2009.

Amitava Das and Björn Gambäck. Identifying lan-
guages at the word level in code-mixed indian social
media text. 2014.

Jacob Eisenstein. What to do about bad language on
the internet. In HLT-NAACL, pages 359–369, 2013.

Thomas Hofmann. Probabilistic latent semantic in-
dexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 50–57.
ACM, 1999.

Wang Ling, Guang Xiang, Chris Dyer, Alan W Black,
and Isabel Trancoso. Microblogs as parallel cor-
pora. In ACL (1), pages 176–186, 2013.

Roberto Navigli and Simone Paolo Ponzetto. Babelnet:
The automatic construction, evaluation and applica-
tion of a wide-coverage multilingual semantic net-
work. Artificial Intelligence, 193:217–250, 2012.

Hong Ka San. Chinese-english code-switching in
blogs by macao young people. Master’s thesis, The
University of Edinburgh, UK, 2009.

Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. Code mixing: A challenge for lan-
guage identification in the language of social media.
EMNLP 2014, 13, 2014.

Anupam Jamatia, Björn Gambäck, and Amitava Das.
Part-of-speech tagging for code-mixed english-hindi
twitter and facebook chat messages. Recent Ad-
vances in Natural Language Processing, Bulgaria,
page 239–248, 2015.

Nanyun Peng, Yiming Wang, and Mark Dredze. Learn-
ing polylingual topic models from code-switched so-
cial media documents. In ACL (2), pages 674–679,
2014.

Daniel Ramage, Susan T Dumais, and Daniel J
Liebling. Characterizing microblogs with topic
models. ICWSM, 10:1–1, 2010.

Royal Sequiera, Monojit Choudhury, and Kalika Bali.
Pos tagging of hindi-english code mixed text from
social media: Some machine learning experiments.
ICON 2015

Kalika Bali Jatin Sharma, Monojit Choudhury, and Yo-
garshi Vyas. i am borrowing ya mixing? an analysis
of english-hindi code mixing in facebook. EMNLP
2014, page 116, 2014.

Thamar Solorio and Yang Liu. Part-of-speech tagging
for english-spanish code-switched text. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1051–1060. Asso-
ciation for Computational Linguistics, 2008.

Yogarshi Vyas, Spandana Gella, Jatin Sharma, Ka-
lika Bali, and Monojit Choudhury. Pos tagging of
english-hindi code-mixed social media content. In
EMNLP, volume 14, pages 974–979, 2014.

Arnav Sharma, Sakshi Gupta, Raveesh Motlani, Piyush
Bansal, Manish Srivastava, Radhika Mamidi, and
Dipti M Sharma. Shallow parsing pipeline for
hindi-english code-mixed social media text. arXiv
preprint arXiv:1604.03136, 2016.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. Entity linking meets word sense disambigua-
tion: a unified approach. Transactions of the As-
sociation for Computational Linguistics, 2:231–244,
2014.

Björn Gambäck, and Amitava Das. Comparing the
Level of Code-Switching in Corpora. LREC, 2016.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh. Dis-
covering coherent topics using general knowledge.
In Proceedings of the 22nd ACM international con-
ference on Information & Knowledge Management,
pages 209–218. ACM, 2013.

Jason Chuang, Sonal Gupta, Christopher D Manning,
and Jeffrey Heer. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In
ICML (3), pages 612–620, 2013.

Don Johnson and Sinan Sinanovic. Symmetrizing the
kullback-leibler distance. IEEE Transactions on In-
formation Theory, 2001.265



George A Miller. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38(11):39–41,
1995.

Pikakshi Manchanda. Entity linking and knowledge
discovery in microblogs. ISWC-DC 2015 The ISWC
2015 Doctoral Consortium, page 25, 2015.

Ivan Titov and Ryan McDonald. Modeling online re-
views with multi-grain topic models. In Proceed-
ings of the 17th international conference on World
Wide Web, pages 111–120. ACM, 2008.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. Opti-
mizing semantic coherence in topic models. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 262–272. As-
sociation for Computational Linguistics, 2011.

266


