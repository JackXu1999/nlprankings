



















































Learning Contextually Informed Representations for Linear-Time Discourse Parsing


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Learning Contextually Informed Representations for Linear-Time
Discourse Parsing

Yang Liu and Mirella Lapata
Institute for Language, Cognition and Computation

School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB

Yang.Liu2@ed.ac.uk,mlap@inf.ed.ac.uk

Abstract

Recent advances in RST discourse parsing
have focused on two modeling paradigm-
s: (a) high order parsers which jointly
predict the tree structure of the discourse
and the relations it encodes; or (b) linear-
time parsers which are efficient but mostly
based on local features. In this work, we
propose a linear-time parser with a novel
way of representing discourse constituents
based on neural networks which takes into
account global contextual information and
is able to capture long-distance dependen-
cies. Experimental results show that our
parser obtains state-of-the art performance
on benchmark datasets, while being effi-
cient (with time complexity linear in the
number of sentences in the document) and
requiring minimal feature engineering.

1 Introduction

The computational treatment of discourse phe-
nomena has recently attracted much attention, due
to their increasing importance for potential ap-
plications. Knowing how text units can be com-
posed into a coherent document and how they re-
late to each other e.g., whether they express con-
trast, cause, or elaboration, can usefully aid down-
stream tasks such summarization (Yoshida et al.,
2014), question answering (Chai and Jin, 2004),
and sentiment analysis (Somasundaran, 2010).

Rhetorical Structure Theory (RST, Mann and
Thompson 1988), one of the most influential
frameworks in discourse processing, represents
texts by trees whose leaves correspond to Elemen-
tary Discourse Units (EDUs) and whose nodes
specify how these and larger units (e.g., multi-
sentence segments) are linked to each other by
rhetorical relations. Discourse units are further

The projections are in the neighborhood 
of 50 cents a share to 75 cent,

compared with a restated 
$1.65 a share a year earlier,

when profit was $107.8 million 
on sales of $435.5 million.

nucleus satellite 

COMPARISON

[Cray Research Inc. said]e1 [it sold one of its newest and largest computer systems, 
the Cray Y-MP/832, to the United Kingdom Meteorological Office.]e2
[The system is the first]e3 [to be sold through the joint marketing agreement between 
Cray and Control Data Corp.]e4

[Only a few months ago, the 124-year-old securities 
firm seemed to be on the verge of a meltdown,]e1 
[racked by internal squabbles and defections.]e2 
[Its relationship with parent General Electric Co. 
had been frayed since a big Kidder insider-trading 
scandal two years ago.]e3

e1 e2

consequence

e1:2 e3

list

e1:3

Figure 1: Example text (bottom) composed of two
sentences (three EDUs) and its RST discourse tree
representation (top).

characterized in terms of their importance in tex-
t: nuclei denote central segments, whereas satel-
lites denote peripheral ones. Figure 1 shows an
example of a discourse tree representing two sen-
tences with three EDUs (e1,e2, and e3). EDUs e1
and e2 are connected with a mononuclear relation
(i.e., Consequence), where e1 is the nucleus and
e2 the satellite (indicated by the left pointing ar-
row in the figure). Span e1:2 is related to e3 via
List, a multi-nuclear relation, expressing the fact
that both spans are equally important and there-
fore both nucleus.

Given such tree-based representations of dis-
course structure, it is not surprising that RST-style
document analysis is often viewed as a parsing
task. State-of-the-art performance on RST parsing
is achieved by cubic-time parsers (Li, Li, and Hov-
y, 2014; Li, Li, and Chang, 2016), with O(n3) time
complexity (where n denotes the number of sen-

1289



tences in the document). These systems model the
relations between all possible adjacent discourse
segments and use a CKY-style algorithm to gen-
erate a global optimal tree. The high order com-
plexity renders such parsers inefficient in prac-
tice, especially when processing large documents.
As a result, more efficient linear-time discourse
parsers have been proposed (Feng and Hirst, 2014;
Ji and Eisenstein, 2014) which make local de-
cisions and model the structure of the discourse
and its relations separately. In this case, features
are extracted from a local context (i.e., a smal-
l window of discourse constituents) without con-
sidering document-level information, which has
been previously found useful in discourse analy-
sis (Feng and Hirst, 2012).

In this paper, we propose a simple and efficient
linear-time discourse parser with a novel way of
learning contextual representations for discourse
constituents. To guarantee linear-time complexity,
we use a two-stage approach: we first parse each
sentence in a document into a tree whose leaves
correspond to EDUs, and then parse the document
into a tree whose leaves correspond to already pre-
processed sentences. The feature learning process
for both stages is based on neural network model-
s. At the sentence level, Long-Short Term Mem-
ory Networks (LSTMs; Hochreiter and Schmid-
huber 1997) learn representations for EDUs and
larger constituents, whereas at the document level,
LSTMs learn representations for entire sentences.
Treating a sentence as a sequence of EDUs and
a document as a sequence of sentences allows to
incorporate important contextual information on
both levels capturing long-distance dependencies.

Recurrent neural networks excel at modeling
sequences, but cannot capture hierarchical struc-
ture which is important when analyzing multi-
sentential discourse. We therefore adopt a more
structure-aware representation at the documen-
t level which we argue is complementary to the flat
representations obtained from the LSTM. We rep-
resent documents as trees using recursive neural
networks (Socher et al., 2012). Experimental eval-
uation on the RST Treebank shows that our parser
yields comparable performance to previous linear-
time systems, without requiring extensive manu-
al feature engineering and improves upon related
neural models (Li et al., 2014, 2016) on discourse
relation classification, while being more efficient.

The rest of this paper is organized as follows.

We overview related work in the following sec-
tion. We describe the general flow of our pars-
er in Section 3 and provide details on our pars-
ing algorithm and feature learning method in Sec-
tion 4. Experimental results are reported in Sec-
tion 5. Section 7 concludes the paper.

2 Related Work

Recent advances in discourse modeling have
greatly benefited from the availability of resources
annotated with discourse-level information such
as the RST Discourse Treebank (RST-DT; Carl-
son et al. 2003) and the Penn Discourse Treebank
(PDTB, Prasad et al. 2008). In this work, we fo-
cus on RST-style discourse parsing, where a tree
representation is derived for an entire document.
In PDTB, discourse relations are annotated most-
ly between adjacent sentences and no global tree
structure is provided.

Early approaches to discourse parsing (Marcu,
2000; LeThanh et al., 2004) have primarily fo-
cused on overt discourse markers (or cue words)
and used a series of rules to derive the discourse
tree structure. Soricut and Marcu (2003) employed
a standard bottom-up chart parsing algorithm with
syntactic and lexical features to conduct sentence-
level parsing. Baldridge and Lascarides (2005)
and Sagae (2009) used probabilistic head-driven
parsing techniques. Subba and Di Eugenio (2009)
were the first to incorporate rich compositional
semantics into sentence- and document-level dis-
course parsing.

HILDA (Hernault et al., 2010) has been
one of the most influential document-level dis-
course parsers paving the way for many machine
learning-based models. HILDA parses a document
pre-segmented into EDUs with two support vec-
tor machine classifiers working iteratively in a
pipeline. At each iteration, a binary SVM predicts
which adjacent units should be merged and then
a multi-class SVM predicts their discourse rela-
tion. Subsequent work (Feng and Hirst, 2014; Joty
et al., 2013) has shown that two-stage systems are
not only efficient but can also achieve competitive
performance. CKY-based parsers which guarantee
globally optimal results have also been developed
(Joty et al., 2013; Li et al., 2014).

Ji and Eisenstein (2014) were the first to ap-
ply neural network models to RST discourse pars-
ing; their shift-reduce parser uses a feedforward
neural network to learn the representations of the

1290



transition stack and queue. Li et al. (2014) pro-
posed a CKY-based parser which uses recursive
neural networks to learn representations for EDUs
and their composition during the tree-building pro-
cess. More recently, Li et al. (2016) designed a
CKY-based parser which uses LSTMs to model
text spans and a tensor-based transformation to
compose adjacent spans.

Our own work joins others (Feng and Hirst,
2014; Joty et al., 2013) in adopting a two-stage
architecture for our discourse parser. However,
rather than considering each discourse constituen-
t independently, we learn contextually-aware rep-
resentations capturing long-range dependencies
across sentences and documents. Discourse con-
stituents at all levels are modeled with recurrent
neural networks adopting a relatively simple, yet
efficient, architecture compared to previously pro-
posed neural systems (Li et al., 2016). Finally,
we experimentally assess whether sequence-based
representations are expressive enough by compar-
ing them to those obtained (with recursive neural
networks) from structured inputs.

3 Discourse Parser Overview

In RST discourse parsing, a document is first seg-
mented into EDUs and a parser then builds a dis-
course tree with the EDUs as leaves. The first sub-
task is considered relatively easy with state-of-art
accuracy at above 90% (Hernault et al., 2010). As
a result, recent research focuses on the second sub-
task and often uses manual EDU segmentation.

Joty et al. (2013) found that a two-stage pars-
ing strategy, which separates intra-sentential from
multi-sentential parsing, has some advantages for
document-level discourse parsing, since the distri-
bution of discourse relations and useful features
are different in the two stages. Based on their
findings, our model also follows a two-stage ap-
proach and is composed by two components: an
intra-sentential parser and a multi-sentential pars-
er. Given a document pre-segmented into EDUs,
our intra-sentential parser first builds a sentence-
level discourse tree for individual sentences. Then,
our multi-sentential parser creates a discourse tree
for the entire document.

To guarantee linear-time complexity, both
parsers adopt a greedy bottom-up tree-building
process (Hernault et al., 2010) and are based on
two conditional random field (CRF) models, one
for creating discourse structure and another one

for assigning relations. The intra-sentential pars-
er considers adjacent EDUs and decides whether
they should be connected (based on the scores pre-
dicted by the first CRF) and their relation (based
on predictions of the second CRF). The multi-
sentential parser follows the same procedure while
operating over sentences.

The CRFs employ feature representations
which we obtain using neural networks. Specifi-
cally, discourse constituents at all levels are mod-
eled with recurrent neural networks (see Sec-
tion 4.2 for details). In addition, for inter-sentential
constituents, we complement the flat text span rep-
resentation with recursive neural networks (see
Section 4.4). We argue that the combination is ad-
vantageous; a sequential text span representation
is in principle unsuitable for capturing hierarchi-
cal discourse structure, whereas a tree-based rep-
resentation can be more precise, albeit less robust
(due to the accumulation of errors from the recur-
sive tree-building process).

4 Parsing Model

4.1 Intra-sentential Parser

To parse a sentence, we start with EDUs,
which can be viewed as discourse constituents
at the first level. As mentioned earlier, our intra-
sentential parser is based on two linear-chain
CRFs, the structure CRF decides which pair of
constituents should be merged at the current lev-
el and the relation CRF assigns discourse rela-
tions to non-leaf constituents. For example, let
C1 = {e1,e2, · · · ,em} denote a sentence with a se-
quence of EDUs, where ei is the ith EDU in the
sentence. Suppose the structure CRF decides to
merge together e2 and e3, then the next-level se-
quence is C2 = {e1,e2:3,e4, · · · ,em} and the rela-
tion CRF will assign a discourse relation to e2:3,
the only non-leaf constituent so far. This process
iterates until all EDUs are merged and a discourse
subtree is generated for the entire sentence.

A linear-chain CRF for intra-sentential dis-
course parsing is shown in Figure 2. Here,
C = {c0, · · · ,ct , · · · ,cn} are observed discourse
constituents and L = {l1, · · · , lt , · · · , ln} are hidden
structure nodes; label lt ∈ {1,0} denotes whether
constituents ct and ct+1 should be connected. Our
model differs from standard linear-chain CRFs in
that the score between adjacent hidden nodes is
not calculated based on a transition matrix, but
is learned from observations instead. Specifically,

1291



ct-1 ct

lt

ct+1

lt+1

c0 c1

l1

cn-1 cn

ln

Figure 2: Intra-sentential structure CRF with pair-
wise modeling.

given a constituent sequence C, the probability of
forming the structure sequence Y str is written as:

p(Y str|C) = 1
Z

n

∏
t=1

exp(ustrt +b
str
t,t+1) (1)

ustrt = θ(ct ,ct+1,yt) (2)
bstrt,t+1 = γ(ct−1,ct ,ct+1,yt ,yt+1) (3)

where the ustrt represents the unary potential s-
core of structure node lt = yt , depending on con-
stituents ct and ct+1. The binary potential score
bstrt,t+1 for st = yt ,st+1 = yt+1 is calculated based
on ct−1,ct and ct+1. The binary potential pro-
vides information for discriminating between st =
0,st+1 = 1 and st = 1,st+1 = 0. Also, we impose
the constraint that one constituent can be merged
with at most one other adjacent constituent.

Figure 3 depicts the relation CRF for intra-
sentential parsing. Similar to the structure CRF,
C = {c0, · · · ,ct , · · · ,cn} are observed constituents
and R = {r1, · · · ,rt , · · · ,rn} hidden nodes, corre-
sponding to discourse relations. The CRF will as-
sign a relation to a non-leaf constituent ct based
on its right and left children, ct,L and ct,R, respec-
tively. If a constituent is a single EDU, we force its
hidden node to be the special label LEAF , whereas
hidden nodes for constituents which are the prod-
uct of merging cannot be LEAF . Given a sequence
of constituents C, the probability of the relation la-
bel sequence Y rel can be written as:

p(Y rel|C) = 1
Z

n

∏
t=1

exp(urelt +b
rel
t,t+1) (4)

urelt = θ(ct ,yt) (5)

brelt,t+1 = γ(ct ,ct+1,yt ,yt+1) (6)

4.2 Intra-sentential Feature Learning
Instead of adopting a high order parsing model,
we use neural networks to capture contextual in-
formation and recover the meaning of discourse
constituents. Aside from modeling long distance
dependencies, our representation learning process

rt rt+1

c0,L c0,R

rnr0

c0

ct,L ct,R

ct ct+1 cn

Figure 3: Intra-sentential relation CRF with pair-
wise modeling.

alleviates the need for elaborate feature engineer-
ing and selection. Our approach is based on L-
STMs (Hochreiter and Schmidhuber, 1997) which
have recently emerged as a popular architecture
for modeling sequences and have been success-
fully applied to a variety of tasks ranging from
machine translation (Sutskever et al., 2014), to
speech recognition (Graves et al., 2013), and im-
age description generation (Vinyals et al., 2015b).
LSTMs have also been incorporated into syntactic
parsing in a variety of ways (Vinyals et al. 2015a;
Kiperwasser and Goldberg 2016; Dyer et al. 2015,
inter alia). Of particular relevance to this work
is LSTM-minus, a method for learning embed-
dings of text spans, which has achieved compet-
itive performance in both dependency and con-
stituency parsing (Wang and Chang, 2016; Cross
and Huang, 2016). We describe below how we ex-
tend this method which is based on subtraction be-
tween LSTM hidden vectors to discourse parsing.

We represent each sentence as a sequence of
word embeddings [wwwsos,www1, · · · ,wwwi, · · · ,wwwn,wwweos]
and insert a special embedding wE to indicate the
boundaries of EDUs. We run a bidirectional LST-
M over the sentence and obtain the output vector
sequence [hhh0, · · · ,hhhi, · · · ,hhht ], where hhhi = [~hhhi, ~hhhi] is
the output vector for the ith word, and ~hhhi and ~hhhi
are the output vectors from the forward and back-
ward directions, respectively. We represent a con-
stituent c from position a to b with a span vec-
tor sssppp which is the concatenation of the vector d-
ifferences~hhhb+1−~hhha and ~hhha−1− ~hhhb:

sssppp = [~hhhb+1−~hhha, ~hhha−1− ~hhhb] (7)

As illustrated in Figure 4, spans are represented
using output from both backward and forward L-
STM components. Intuitively, this allows to obtain
representations for EDUs and larger constituents
in context, as embeddings are learned based on in-

1292



EDUk EDUk+1EDUk-1

Figure 4: Modeling discourse constituents by LSTM-minus features. The feature vector sssppp j for a con-
stituent covering EDUk and EDUk+1 is [~hhhk+1−~hhhi+3, ~hhhk−1− ~hhhi+6]. Blue nodes indicate word embeddings
and LSTM outputs for words, while gray nodes represent EDU separators. Black nodes are learned
LSTM-minus features for constituents.

ct-1 ct

lt

ct+1

lt+1

ct-3 ct-2

lt-1

ct+2

lt+2lt

H1
H2

H3

Figure 5: Multi-sentential structure CRF with
sliding-window. The window size is 3, H1,H2, and
H3 denote the windows for predicting st , which is
highlighted by the shaded rectangle.

formation from the span itself and the words sur-
rounding it.

The structure CRF model calculates the unary
potential score ustrt and the binary potential s-
core bstrt,t+1 based on the span vector as follows:

uuustrt = WWW
str
u [sssppp j,sssppp j+1] (8)

bbbstrt,t+1 = WWW
str
b [sssppp j−1,sssppp j,sssppp j+1] (9)

where sssppp j is the span vector for the j
th constituent

in the current sequence; and WWW stru ∈ Rd×2,WWW strb ∈
Rd×4 are weight matrices. bbbstrt is reshaped into a
2× 2 matrix, where the (i, j)th entry indicates the
score of the transition from label i to label j be-
tween constituent ct and ct+1.

Analogously, unary and binary potential scores
for the relation CRF are calculated as:

uuurelt = WWW
rel
u [sssppp j,L,sssppp j,R] (10)

bbbrelt,t+1 = WWW
rel
b [sssppp j,L,sssppp j,R,sssppp j+1,L,sssppp j+1,R] (11)

where sssppp j,L and sssppp j,R are span vector represen-
tations for the left and right child of the jth con-
stituent; WWW relu ∈ Rd×nr ,WWW relb ∈ Rd×n

2
r , and nr is the

ct-1 ct

rt

ct+1

rt+1

ct-3 ct-2

rt-1 rt+2rt

c0,L c0,R c0,L c0,R c0,L c0,R

H1
H2

H3

Figure 6: Multi-sentential relation CRF with
sliding-window. The window size is 3, H1,H2 and
H3 denote the windows for predicting rt , which
highlighted by the shaded rectangle.

number of discourse relations; bbbrelt is also reshaped
into a nr× nr matrix. For a constituent that is a s-
ingle EDU, sssppp j,R and sssppp j+1,R are special vectors
wwwLEAFL and wwwLEAFR .

4.3 Multi-sentential Parser

The multi-sentential discourse parser treats sen-
tences as the smallest possible discourse units,
following a process similar to the intra-sentential
model. Unfortunately, it is practically unfeasible
to model all constituents with one CRF when pro-
cessing entire documents. The forward-backward
algorithm for calculating the CRF normalization
factor on a sequence with T units has time com-
plexity O(T M2), where M is the number of labels,
leading to O(T 2M2) time for parsing a document.

We therefore modify the two CRFs into sliding-
window versions. Specifically, at each level, when
decoding a constituent sequence, for each hidden
structure node li or relation node ri, we find al-
l windows of constituents that contain the hidden
node, and set the hidden node’s label according to
the window with the maximum joint probability.

1293



We present the modified sliding-window CRFs in
Figure 5 (structure) and Figure 6 (relation).

4.4 Multi-sentential Feature Learning
For multi-sentential parsing, we also use the
LSTM-minus method to model constituents, how-
ever, the minimum units here are sentences rather
than EDUs. We represent individual sentences
with the same bidirectional LSTM used for intra-
sentential parsing. For sentence si, the LSTM out-
put vector of the first token hhh0 and the last token hhhn
are taken to form the sentence representation vvvi:

vvvi = [hhh0,hhhn] (12)

In order to capture additional contextual infor-
mation, we then build another bidirectional LSTM
over a sequence of sentence vectors. We learn rep-
resentations for constituents (aka text spans with-
in a document) with the LSTM-minus method and
use fff lll to denote the resulting vectors. Although
this flat representation is relatively straightforward
to obtain, it is perhaps overly simplistic for mod-
eling documents where the number of sentences
can be relative large. Moreover, it is not clear that
it is discriminating enough for modeling relations
between constituents. Such relations follow struc-
tural regularities (e.g., they can be asymmetric or
symmetric, left- or right-branching) which cannot
be captured when adopting a sequence-based doc-
ument view.

To inject structural knowledge in our represen-
tation, we also model constituents as subtrees with
recursive neural networks (Socher et al., 2012).
The latter operate over tree structures which we
obtain during training from the RST Discourse
Treebank (Carlson et al., 2003). The representa-
tion for each parent is computed based on its chil-
dren iteratively, in a bottom-up fashion. More for-
mally, let vectors hhhL and hhhR denote the left and
right children of constituent c and dis their rhetor-
ical relation. The vector for parent c is:

hhhc = tanh(WWW dis[hhhL,hhhR]+bbbdis) (13)

where [hhhL,hhhR] is the concatenation of the chil-
dren representations hhhL ∈Rd and hhhR ∈Rd , WWW dis ∈
R2d×d , and bbbdis ∈Rd is the bias vector. We hence-
forth use the term tree vector tttrrr to refer to the rep-
resentation in Equation (13) since constituents are
now subtrees in a document-wide discourse tree.

In multi-sentential parsing, the span vector sssppp
now becomes the concatenation of the flat vec-

vi

words in a sentence

vi-1 vi vi+1

hi-1 hi hi+1

sentence 
representation

vi vi+1vi-1

tr

tree 
representation

flat
representation

Figure 7: Document-level constituents with
LSTM-minus features. Blue nodes are LSTM
outputs for words, light green nodes represent
vectors for sentences, dark green nodes are LSTM
outputs for sentences, and black nodes are learned
constituent representations.

tor fff lll and the tree vector tttrrr:

sssppp = [ fff lll, tttrrr] (14)

The representation above, attempts to capture rich-
er semantic features during the tree building pro-
cess while benefiting from the robustness afforded
by the flat LSTM-based text spans. An example of
this representation is given in Figure 7.

Unary and binary potential scores for the struc-
ture and relation CRFs are calculated as in intra-
sentential parsing (see Section 4.2).

4.5 Training

We first train the intra-sentential parser and use
the learned LSTM as a component of the multi-
sentential parser. During training, we maximize
the log-likelihood of the correct label sequence Y
given a constituent sequence C, which is p(Y str|C)
for the structure CRFs and p(Y rel|C) for the rela-
tion CRFs. Stochastic gradient descent with mo-
mentum is used to update the parameters of the
network. In our experiments, the momentum is set
to 0.9 and the learning rate is 0.001. The LSTMs
in our paper have one hidden layer.

5 Experimental Setup

In this section we present our experimental set-
up for assessing the performance of the discourse
parser described above. We give details on the

1294



datasets we used, evaluation protocol, and model
training.

Evaluation We evaluated our model on the
RST Discourse Treebank (RST-DT; Carlson et al.
2003), which is partitioned into 347 documents for
training and 38 documents for testing. Following
previous work (Joty et al., 2013; Li et al., 2016),
we converted non-binary relations into a cascade
of right-branching binary relations.

Predicted RST-trees are typically evaluated by
computing F1 against gold standard trees (Mar-
cu, 2000). Evaluation metrics for RST-style dis-
course parsing include: (a) span (S) which mea-
sures whether the predicted subtrees match the
goldstandard; (b) nucleus (N) which measures
whether subtrees have the same nucleus as in the
goldstandard and (c) relation (R) which measures
whether discourse relations have been identified
correctly. The three metrics are interdependent, er-
rors on the span metric propagate to the nuclearity
metric, and in turn to the relation metric. Follow-
ing other RST-style discourse parsing systems (H-
ernault et al., 2010), we evaluate the relation met-
ric using 18 coarse-grained relation classes, and
with nuclearity attached, we have a total of 41 dis-
tinct relations.1 Since EDU segmentation falls out-
side the scope of this work, we evaluate our system
on gold-standard EDUs. Comparison systems are
also assessed in the same setting.

Training Details Word embeddings were pre-
trained with the Gensim2 implementation of
word2vec (Mikolov et al., 2013) on the English
GigaWord corpus (with case left intact). The di-
mensionality of the word embeddings was set
to 50. Following Li et al. (2016), the embed-
dings were fine-tuned using a mapping matrix WWW ∈
R50×50 trained with the following criterion:

min
WWW ,bbb
‖LLLTTT tuned−LLLTTT preWWW +bbb‖ (15)

where LLLTTT tuned , and LLLTTT pre are lookup tables for
fine-tuned and pre-trained word embeddings in the
training set. Matrix W can be subsequently used to
to estimate fine-tuned embeddings for words in the
test set.

Tokenization, POS tagging and sentence s-
plitting were performed using the Stanford

1For calculating the binary potential scores, 41 relations
will lead to a large number of parameters; to avoid this, we
only use 18 relations without nuclearity.

2https://radimrehurek.com/gensim/

CIDER S N R
Tree Span 79.5 68.1 56.6
Flat Span (−minus) 82.7 69.3 55.6
Flat Span (+minus) 83.6 70.1 55.4
Tree + Flat Span (+minus) 83.6 71.1 57.3

Table 1: CIDER performance using different con-
stituent representations (RST-DT test set).

CoreNLP toolkit (Manning et al., 2014). All neu-
ral network parameters were initialized random-
ly with Xavier’s initialization (Glorot and Bengio,
2010). The hyper-parameters are tuned by cross-
validation on the training set.

Additional Features Most existing state-of-the-
art systems rely heavily on handcrafted features
(Hernault et al., 2010; Feng and Hirst, 2014; Joty
et al., 2013) some of which have been also proved
helpful in neural network models (Li et al., 2014,
2016). In our experiments, we use the following
basic features which have been widely adopted
in various discourse parsing models: (1) the first
three words and the last two words of each con-
stituent; (2) the POS tags of the first three words
and the last two words of each constituent; (3) the
number of EDUs; and (4) the number of tokens
of each constituent. We concatenate these features
with the constituent vectors learned by our neural
networks, and train new CRF models.

6 Results
In this paper we have presented two views for
modeling discourse constituents, namely as trees
or sequences. We experimentally assessed whether
these two views are overlapping or complemen-
tary. Table 1 reports the performance of our pars-
er which we call CIDER (as a shorthand for
Contextually Informed Discourse Parser) without
the additional features introduced in Section 5.
The first row presents a version of CIDER based
solely on tree span representations. In the second
row, CIDER uses flat representations without LST-
M minus features. Specifically, each constituent is
represented as the average of the LSTM output-
s within it. In the third row, CIDER’s representa-
tions are computed with the LSTM minus method,
while the fourth row shows results for the full sys-
tem.

As can be seen, on the span (S) metric, CIDER
with flat span representations is much better than
CIDER with tree span representations; on the nu-
clearity (N) metric tree representations are still in-
ferior to flat representations but the performance

1295



Discourse Parsers S N R Speed
Ji and Eisenstein (2014) 82.1 71.1 61.6 0.21
Feng and Hirst (2014) 85.7 71.0 58.2 9.88
Heilman and Sagae (2015) 83.5 68.1 55.1 0.40
CIDER (−AF) 83.6 71.1 57.3 3.80
CIDER (+AF) 85.0 71.1 59.0 3.80
Li et al. (2014) 84.0 70.8 58.6 26.00
Li et al. (2016) 85.8 71.7 58.9 –
Human 88.7 77.7 65.8 –

Table 2: Comparison with state-of-the-art system-
s (RST-DT test set). Speed indicates the average
number seconds taken to parse a document.

gap is narrower, whereas on the the relation (R)
metric, tree span representations are superior. We
believe this can be explained by the fact that re-
lation identification relies on more semantic in-
formation while span identification relies on more
shallow features, like the beginning and the end of
the span (Ji and Eisenstein, 2014). Span features
based on the LSTM minus method bring improve-
ments over vanilla LSTM representations on the s-
pan and nuclearity metrics. Perhaps unsurprising-
ly, the combination of span and tree representa-
tions achieves the best results overall.

In Table 2, we compare our system with sev-
eral state-of-the-art discourse parsers, which can
be classified in two groups depending on their
time complexity. Linear-time systems (first block
in the table) include two transition-based parser-
s (Ji and Eisenstein, 2014; Heilman and Sagae,
2015) and one CRF-based parser (Feng and Hirst,
2014), whereas cubic-time parsers (second block)
include two neural network models (Li, Li, and
Hovy, 2014; Li, Li, and Chang, 2016). CIDER
falls in the first group as it is a liner-time parser,
while it shares with parsers in the second group
the use of neural architectures for automated fea-
ture extraction. We report CIDER scores with and
without the additional features (AF) discussed in
the previous section. As an upper bound, we also
report inter-annotator agreement on the discourse
parsing task (last row in the table).

Amongst linear-time systems, our parser
achieves comparable results on the span and
relation metric, and best performance on the
nuclearity metric. Note that the three metrics
evaluate different aspects of a discourse parser,
and CIDER achieves the most balanced results
across all metrics. As far as other comparison
systems are concerned, Ji and Eisenstein (2014)
employ a shift-reduce discourse parser. They

represent EDUs with word-count vectors and use
a projection matrix to combine them into text
spans. A support vector machine classifier is used
to decide the actions of the parser. Heilman and
Sagae (2015) also adopt a shift-reduce approach
and use multi-class logistic regression to select
the best parsing action. Their classifier considers
a variety of lexical, syntactic, and positional
features. Feng and Hirst’s (2014) system is closest
to ours in their use of linear-chain CRFs, but
their features are mainly extracted from local con-
stituents. Furthermore, they adopt a post-editing
method which modifies the discourse trees their
parser creates with height features.

With regard to previously proposed cubic-time
systems, CIDER outperforms Li et al. (2014)
across all metrics. Their CYK-based parser adopts
a recursive deep model for composing EDUs hi-
erarchically together with several additional fea-
tures to boost performance. CIDER performs s-
lightly worse on span and nuclearity compared to
Li et al. (2016), but is better at identifying relation-
s. Their system uses an attention-based hierarchi-
cal neural network for modeling text spans and a
tensor-based transformation for combining two s-
pans. A CKY-like algorithm is used to generate the
discourse tree structure. In comparison, CIDER is
conceptually simpler, and more efficient.

We used paired bootstrap re-sampling (Efron
and Tibshirani, 1993) to assess whether dif-
ferences in performance are statistically signif-
icant. CIDER is significantly better than Feng
and Hirst’s 2014 system on the relation metric
(p < 0.05); it is also significantly better (p < 0.05)
than Heilman and Sagae (2015) on all three met-
rics and better than Ji and Eisenstein (2014) on the
span metric. Compared to Li et al. (2014), CIDER
is significantly better on the span and relation met-
rics (p < 0.05). Unfortunately, we cannot perform
significance tests against Li et al. (2016) as we do
not have access to the output of their system.

We also evaluated the speed of CIDER and
comparison discourse parsers on a platform with
Intel Core-i5-7200U CPU at 2.50GHz. We report
the average number of seconds taken to parse a
document in the RST-DT test set. The times shown
in Table 2 do not include pre-processing, which for
CIDER is only part-of-speech tagging, whereas all
other linear-time systems rely on a syntactic pars-
er. As can be seen CIDER is quite efficient com-
pared to related systems (Feng and Hirst, 2014;

1296



Heilman and Sagae, 2015) whilst requiring less
feature engineering.

7 Conclusions
In this paper we described CIDER, a simple and
efficient discourse parser which adopts a two-
stage parsing strategy, whilst exploiting a more
global feature space. We proposed a novel way
to learn contextually informed representations of
constituents with the LSTM minus method, at the
sentence and document level. We also demonstrat-
ed that flat representations of text spans can be
usefully complemented with tree-based ones lead-
ing to a more accurate characterization of dis-
course relations. Experimental results showed that
CIDER performs on par with the state of the art (Li
et al., 2016), despite the greedy parsing algorith-
m and relatively simple neural architecture. In the
future, we would like to improve parsing accura-
cy by leveraging unlabeled text rather than relying
exclusively on human annotated training data.

Acknowledgments The authors gratefully ac-
knowledge the support of the European Research
Council (award number 681760).

References
Jason Baldridge and Alex Lascarides. 2005. Proba-

bilistic head-driven parsing for discourse structure.
In Proceedings of the CoNLL conference. pages 96–
103.

Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Current and new directions in discourse and dia-
logue, Springer, pages 85–112.

Joyce Y. Chai and Rong Jin. 2004. Discourse struc-
ture for context question answering. In HLT-NAACL
2004: Workshop on Pragmatics of Question Answer-
ing. pages 23–30.

James Cross and Liang Huang. 2016. Span-based con-
stituency parsing with a structure-label system and
provably optimal dynamic oracles. In Proceedings
of the EMNLP conference. pages 1–11.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the ACL confer-
ence. pages 334–343.

Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction ot the Bootstrap. Chapman & Hall, New
York, NY.

Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In
Proceedings of the ACL conference. pages 60–68.

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of the ACL con-
ference. pages 511–521.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of the 13th International
Conference on Artificial Intelligence and Statistics.
volume 9, pages 249–256.

Alex Graves, Jaitly Navdeep, and A-R Mohamed.
2013. Hybrid speech recognition with deep bidirec-
tional lstm. In IEEE Workshop on Automatic Speech
Recognition and Understanding. pages 293–278.

Michael Heilman and Kenji Sagae. 2015. Fast rhetor-
ical structure theory discourse parsing. arXiv
preprint arXiv:1505.02425 .

Hugo Hernault, Helmut Prendinger, David A DuVer-
le, Mitsuru Ishizuka, and Tim Paek. 2010. Hilda: a
discourse parser using support vector machine clas-
sification. Dialogue and Discourse 1(3):1–33.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
Proceedings of the ACL conference. pages 13–24.

Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the ACL confer-
ence. pages 486–496.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transaction-
s of the Association for Computational Linguistics
4:313–327.

Huong LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Generating discourse structures for
written texts. In Proceedings of the 20th internation-
al conference on Computational Linguistics. page
329.

Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recur-
sive deep models for discourse parsing. In Proceed-
ings of the EMNLP conference. pages 2061–2069.

Qi Li, Tianshi Li, and Baobao Chang. 2016. Discourse
parsing with attention-based hierarchical neural net-
works. In Proceedings of the EMNLP conference.
pages 362–371.

1297



William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse 8(3):243–281.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of the ACL confer-
ence: System Demonstrations. pages 55–60.

Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional linguistics 26(3):395–448.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781 .

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The Penn discourse treebank
2.0. In Proceedings of LREC.

Kenji Sagae. 2009. Analysis of discourse structure
with syntactic dependencies and data-driven shift-
reduce parsing. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies. pages
81–84.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the EMNLP conference. pages 1201–
1211.

Swapna Somasundaran. 2010. Discourse-level Rela-
tion for Opinion Analysis. Ph.D. thesis, University
of Pittsburgh.

Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the NAACL conference.
pages 149–156.

Rajen Subba and Barbara Di Eugenio. 2009. An ef-
fective discourse parser that uses rich linguistic in-
formation. In Proceedings of the ACL conference.
pages 566–574.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Proceedings of the NIPS conference. pages
3104–3112.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015a. Gram-
mar as a foreign language. In Proceedings of the
NIPS conference. pages 2773–2781.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015b. Show and tell: A neural im-
age caption generator. In Proceedings of the CVPR
conference. pages 3156–3164.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Pro-
ceedings of the ACL conference. pages 2306–2315.

Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, and
Masaaki Nagata. 2014. Dependency-based dis-
course parser for single-document summarization.
In Proceedings of the EMNLP conference. pages
1834–1839.

1298


