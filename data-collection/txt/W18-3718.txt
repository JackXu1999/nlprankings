






















































%


Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 125–129
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

125

   

A Short Answer Grading System in Chinese by Support Vector 

Approach 

 
Shih-Hung Wu, Wen-Feng Shih 

Dept. of CSIE, Chaoyang University of Technology 

168, Jifeng E.Rd. Wufeng District, 

Taichung, 41349, Taiwan (R.O.C) 

shwu@cyut.edu.tw, wu0fu491@gmail.com 

Abstract 

In this paper, we report a short answer 

grading system in Chinese. We build a 

system based on standard machine 

learning approaches and test it with 

translated corpus from two publicly 

available corpus in English. The experi-

ment results show similar results on two 

different corpus as in English. 

1 Introduction 

To assess the learning outcomes of students 

with tests in various question types and grading 

methods, short answer question is one type of 

test that can test the level of students’ under-

standing of specific concepts in a subject do-

main. Since grading short answer question re-

quires natural language understanding, the test 

was manually graded by teachers.  

Although technically similar to automatic 

essay grading, automatic short answer grading 

is not as mature as automatic essay grading. 

(Burrows et al., 2015) gives a survey on how the 

automatic short answer grading is dealt by var-

ious researchers. The traditional approach is 

string matching, which could be very efficient 

but not very effective. 

Early work relied on regular expression pat-

terns which were manually extracted from ref-

erence answers (Mitchell et al., 2002). The pat-

terns included keywords in the reference an-

swers. Patterns could also be learnt from the ref-

erence answers (Ramachandran et al., 2015). 

(Sultan et al., 2016) adopted the simpler notion 

of semantic alignment to avoid explicitly gener-

ating complicated patterns. 

Semantic matching had also been proposed 

in early work (Leacock and Chodorow, 2003). 

This approach was also used by many research-

ers (Mohler et al., 2009; Mohler et al., 2011; 

Heilman and Madnani, 2013) in supervised 

learning machine learning. A large set of simi-

larity measures is defined as features for a su-

pervised learning model. Features range from 

word level n-gram overlap to deeper semantic 

similarity measures based on dictionary and dis-

tributional methods.  

The short-text grading in SemEval Semantic 

Textual Similarity (STS) task (Agirre et al., 

2012; Agirre et al., 2013; Agirre et al., 2014; 

Agirre et al., 2015) drew the attention of many 

researchers and provided an evaluation plat-

form. Since then, several systems have been 

proposed for short answer grading based on the 

semantic similarity with given reference an-

swers (Mohler and Mihalcea, 2009; Mohler et 

al., 2011; Heilman and Madnani, 2013; Rama-

chandran et al., 2015).  (Sultan et al., 2016) pre-

sented a simple short answer grading system for 

short answer in English. Given a question and 

its reference answers, a system measures the 

correctness of a student answer by calculating 

the similarity with the correct answers.  

Comparing to the field in English, there are 

very little research projects on short answer 

grading in Chinese, and there is no publicly 

available corpus for short answering grading in 

Chinese. 

In this paper we report how we build a sys-

tem and how to test it with a translated corpus 

from two publicly available English corpus. 

The system first extracts the text similarity 

features, and the features are used in a support 

vector model. In the first corpus, answers are 

graded from 0 to 5; we use support vector re-

gression (SVR) model to learn the grading. In 

the second corpus, answers are graded as cor-

rect/incorrect; we use a support vector machine 

(SVM) classifier approach to deal with it. In the 

following sections, we will show the system ar-

chitecture and experimental results.  



126

   

2 System Architecture 

We adopt the previous works on the textual en-

tailment (TE) as our prototype to tackle the short 

answer grading problem in Chinese. TE can be 

briefly defined as: ”Given a pair of sentences 

(Student Answer, Reference answer), a program 

has to decide whether the information in Refer-

ence answer can be inferred by the Student an-

swer”. TE can be used in various applications, 

such as question answering system, information 

extraction, information retrieval, and machine 

translation. Once a system is able to decide 

whether T1 entails T2 or not, it can be regarded 

as an information filter to help users find useful 

information. Traditional approaches to TE are 

based on the semantic and syntactic similarities 

of the words in the sentences. 

2.1 Support Vector Machines 

Support vector machines (SVM) is a supervised 

machine learning classification algorithm, which 

can be used for classifying problem in n-dimen-

sion space. It is used widely in various natural 

language processing research projects and gener-

ally generates good results. Comparing to other 

classification algorithms, SVM algorithm usu-

ally has better result when the number of features 

is quite large and the data is sparse.  

SVM uses 𝑔(𝑥) = 𝑤𝑇∅(𝑥) + 𝑏 as the linear 
separation hyperplane, where w is the weight 

vector, b is the bias, ∅(∙) is a set of high dimen-
sional non-linear transformation function, where 

w and b is determined by training data that opti-

mizes the following formulas: 

 

min
1

2
 WtW + C ∑ ξi

N
i=1                            (1) 

s. t. {
yig(xi) ≥ 1 − ξi
ξi ≥ 0, i = 1 ⋯ N

 

 

where ξI  is the slack variables, and C is the pen-
alty coefficient for all the training samples 

(𝑥𝑖, yi). 

2.2 Support Vector Regression 

 Support Vector Regression (SVR) is using the 

SVM algorithm on regression problem. The goal 

of SVM is to find the separation hyperplane, and 

the goal of SVR is to find the regression hyper-

plane. For the given training set:  
                                                      
1 https://www.csie.ntu.edu.tw/~cjlin/libsvm/ 

{(𝑥1, 𝑧1), … , (𝑥1, 𝑧1)} 
where 𝑥𝑖 ∈ 𝑅

𝑛 is a feature vector, and 𝑧𝑖 ∈ 𝑅
1 is 

the target output. In order to find the hyperplane, 

two parameters C > 0, and ε > 0 must be given 
and the support vector regression can be defined: 

  
1

2
𝑤𝑇𝑤𝑤,𝑏,𝜉,𝜉∗

𝑚𝑖𝑛 + 𝐶 ∑ 𝜉𝑖
𝑙
𝑖=1 + ∑ 𝜉𝑖

∗𝑙
𝑖=1            (2) 

Subject to    𝑤𝑇𝜙(𝑥𝑖) + 𝑏 − 𝑧𝑖 ≤ 𝜖 + 𝜉𝑖  , 

𝑧𝑖 −  𝑤
𝑇𝜙(𝑥𝑖) − 𝑏 ≤ 𝜖 + 𝜉𝑖

∗ , 

𝜉𝑖 , 𝜉𝑖
∗ ≥ 0, 𝑖 = 1, … , 𝑙 

 

    In our experiment, we use a free SVM toolkit, 

LIBSVM, to train the SVR model.1 (Chang and 

Lin, 2011) 

2.3 Feature extraction  

In this section, we briefly introduce the features 

used in SVM, which are the same as those used 

in previous work. Table 1 shows the ten features 

used in the experiments. The first three features 

are the numbers of common terms both in T1 and 

T2. The next three features are the BLEU scores. 

The rest four features are the numbers and differ-

ences of sentence length of T1 and T2. 

3 Data Sets 

3.1 Data Sets in English 

SciEntBank: 

This data set was used in SemEval-2013 and 

available via github2. The data set assigns one of 

five labels to a student response: correct, partially 

2 https://github.com/leocomelli/score-freetext-answer/ar-

chive/master.zip 

No  Feature  

1  unigram_recall  

2  unigram_precision  

3  unigram_F_measure  

4  log_bleu_recall  

5  log_bleu_precision  

6  log_bleu_F_measure  

7  difference in sentence length (charac-

ter)  

8  absolute difference in sentence length 

(character)  

9  difference in sentence length (term)  

10  absolute difference in sentence length 

(term)  

Table 1:  Features used in the system 

 

 



127

   

correct/incomplete, contradictory, irrelevant, and 

non-domain.  

SciEntBank corpus in English contains 9,804 

answers to 197 questions in 15 scientific do-

mains.  There is one reference answer for each 

question. 

Data Structure Data Set:3 

The data set is provided by (Mohler and Mihal-

cea, 2009), which is Data Structure questions and 

student responses graded by two judges. The data 

set assigns one of two labels to a student response: 

correct or incorrect. The questions are collected 

from ten assignments and two tests, and each one 

has a topic such as programming basics or sort-

ing algorithms. A reference answer is also pro-

vided for each question. The interannotator 

agreement is 0.586 (Pearson’s r) and .659 

(RMSE on a 5-point scale). Average score of the 

two judges is used as the final gold score for each 

student answer. 

3.2 Chinese Corpus Translation 

Since there is no publicly available data set in 

Chinese, our experiments are conducted on the 

translated corpus. With the help of machine 

translation, we translate the two data set into Chi-

nese and use them in our experiments. The sen-

tences are then segmented into words by the 

Jieba4 word segmentation toolkit. The quality of 

machine translation is not perfect, 12% of the 

sentences have to be corrected manually. The 

major error types are synonyms with improper 

usage in the context for both nouns and adjec-

tives. There are also sentences with bad grammar.  

4 Experiments 

Since the SciEntBank data set has 5 way label-

ling, we use regression model to predict the 

scores of the student responses. And the Data 

Structure Data Set has 2 way labelling, we use 

the classification model to predict the scores of 

the student responses. 

4.1 Metrics 

For a regression result evaluation, we adopt the 

squared correlation coefficient and mean 

squared error. For a classification result evalua-

tion, we adopt the accuracy.  

Squared correlation coefficient, R2 

                                                      
3 http://web.eecs.umich.edu/mmihalcea/down-

loads/ShortAnswerGrading_v1.0.tar.gz 

R2 is the square of the Pearson correlation coeffi-

cient between the observed x and modeled (pre-

dicted) y data values of the score. Pearson's cor-

relation coefficient is commonly represented by 

the letter r. So if we have one dataset {x1,...,xn} 

containing n values and the prediction of the da-

taset {y1,...,yn} containing n values, then that for-

mula for r is: 

r = 
∑ (𝑥𝑖−�̅�)(𝑦𝑖−�̅�)

𝑛
𝑖=1

√∑ (𝑥𝑖−�̅�)
2𝑛

𝑖=1 √∑ (𝑦𝑖−�̅�)
2𝑛

𝑖=1

 

where n is the sample size, xi is the sample in-

dexed with i, yi is the correspondent system pre-

diction, and �̅�, �̅� are the means of xi, and yi, re-
spectively. 

Root mean squared error (RMSE) 

RMSE is defined as 

RMSE = √
1

𝑛
∑ (𝑥𝑖 − 𝑦𝑖)

2𝑛
𝑖=1  

4.2 Results 

 

Features 𝑹𝟐 RMSE 
all features 0.083041 1.173427 

only bleu 0.127850 1.102370 

Table 2 shows the regression results on the 

Chinese version of the Mohler et al. (2011) da-

taset. Where all features means the system uses 

all the features listed in Table 1, and only bleu 

means the system uses only the bleu features. 

The experiment result shows that more features 

can improve the performance. 

Table 3 shows the classification result on the 

Chinese version of the SemEval-2013 dataset,  

where all features means the system uses all the 

features listed in Table 1, and only bleu means 

 
4 https://github.com/fxsjy/jieba 

Features 𝑹𝟐 RMSE 
all features 0.083041 1.173427 

only bleu 0.127850 1.102370 

Table 2:  Performance on the Chinse version 

of the Mohler et al. (2011) dataset with in-do-

main training. 

 

 

Features Accuracy(%) 

all features 59.569 

only bleu 59.568 

Table 3:  Performance on the Chinse ver-

sion of the SemEval-2013 datasets. 

http://web.eecs.umich.edu/~mihalcea/downloads/ShortAnswerGrading_v1.0.tar.gz
http://web.eecs.umich.edu/~mihalcea/downloads/ShortAnswerGrading_v1.0.tar.gz


128

   

the system uses only the bleu features.  In this 

experiment, the accuracy is almost the same. The 

result shows that more features do not improve 

the performance. 

4.3 Discussions 

Since the data sets are translated ones, it is not 

suitable to compare the results to the original 

ones. However, comparing to the result in Eng-

lish (Sultan et al., 2016), we find that the perfor-

mance is similar. 

5 Conclusion and Future Works 

In this paper, we report a short answer grading 

system in Chinese based on a machine learning 

approach. We test it with translated corpus from 

two publicly available corpus in English. The ex-

periment result shows that the results on the two 

different corpus is promising. 

In the future, we will further develop the sys-

tem with deep learning models. First at all, we 

will use distributed word embedding technique, 

such as word2vec, to improve the representation 

of the text. Then a recurrent neural network with 

long short term memory neuron is desired to re-

place the SVM model. Also curate corpus from 

native Chinese students is also important. Word 

segmentation is also important; instead of Jieba, 

we might use CKIP word segmentation service 

(Ma and Chen, 2003). 

Most research projects require reference an-

swers, and unsupervised automatic short answer 

grading is an interesting way to bypass the re-

quirement (Adams et al., 2016) 

6 Acknowledgment 

This study is supported by the Ministry of Sci-

ence under the grant numbers MOST106-2221-

E-324-021-MY2. 

References  

Oliver Adams, Shourya Roy, Raghuram Krishnapu-

ram. 2016. Distributed Vector Representations for 

Unsupervised Automatic Short Answer Grading, 

in Proceedings of The 3rd Workshop on Natural 

Language Processing Techniques for Educational 

Applications, Osaka, Japan. 

Eneko Agirre, Daniel Cer, Mona Diab, and Aitor 

Gonzalez-Agirre. 2012. SemEval-2012 task 6: A 

Pilot on Semantic Textual Similarity. In SemEval. 

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonza-

lez-Agirre, and Weiwei Guo. 2013. *SEM 2013 

Shared Task: Semantic Textual Similarity. In Sec-

ond Joint Conference on Lexical and Computa-

tional Semantics (*SEM).  

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel 

Cer, Mona Diab, Aitor Gonzalez-Agirre,Weiwei 

Guo, Rada Mihalcea, German Rigau, and Janyce 

Wiebe. 2014. SemEval-2014 Task 10: Multilin-

gual Semantic Textual Similarity. In SemEval. 

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel 

Cer, Mona Diab, Aitor Gonzalez-Agirre,Weiwei 

Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, 

Rada Mihalcea, German Rigau, Larraitz Uria, and 

Janyce Wiebe. 2015. SemEval-2015 Task 2: Se-

mantic Textual Similarity, English, Spanish and 

Pilot on Interpretability. In SemEval. 

Steven Burrows, Iryna Gurevych, and Benno Stein. 

2015. The Eras and Trends of Automatic Short 

Answer Grading. International Journal of Artifi-

cial Intelligence in Education 25: 60. 

https://doi.org/10.1007/s40593-014-0026-8 

Chih-Chung Chang and Chih-Jen Lin. 2011. 

LIBSVM : a library for support vector machines. 

ACM Transactions on Intelligent Systems and 

Technology, 2:27:1--27:27. 

Michael Heilman and Nitin Madnani. 2013. ETS: 

Domain Adaptation and Stacking for Short An-

swer Scoring. In SemEval. 

Claudia Leacock and Martin Chodorow. 2003. C-

rater: Automated Scoring of Short-Answer Ques-

tions. Computers and the Humanities, 37(04). 

Ma, Wei-Yun and Keh-Jiann Chen, 2003, "Introduc-

tion to CKIP Chinese Word Segmentation System 

for the First International Chinese Word Segmen-

tation Bakeoff", Proceedings of ACL, Second 

SIGHAN Workshop on Chinese Language Pro-

cessing, pp168-171. 

Tom Mitchell, Terry Russell, Peter Broomhead, and 

Nicola Aldridge. 2002. Towards Robust Comput-

erised Marking of Free-Text Responses. In Pro-

ceedings of the 6th International Computer As-

sisted Assessment (CAA) Conference. 

Michael Mohler and Rada Mihalcea. 2009. Text-to-

text Semantic Similarity for Automatic Short An-

swer Grading, in Proceedings of the European 

Chapter of the Association for Computational 

Linguistics (EACL 2009), Athens, Greece. 

Michael Mohler, Razvan Bunescu, and Rada Mihal-

cea. 2011. Learning to Grade Short Answer Ques-

tions Using Semantic Similarity Measures and 

Dependency Graph Alignments. In ACL. 

Lakshmi Ramachandran, Jian Cheng, and Peter 

Foltz. 2015. Identifying Patterns For Short An-

https://www.uni-weimar.de/medien/webis/publications/papers/stein_2015a.pdf
https://www.uni-weimar.de/medien/webis/publications/papers/stein_2015a.pdf
https://www.uni-weimar.de/medien/webis/publications/papers/stein_2015a.pdf


129

   

swer Scoring using Graph-based Lexico-Seman-

tic Text Matching. In SemEval. 

Md Arafat Sultan, Cristobal Salazar, and Tamara 

Sumner. 2016. Fast and Easy Short Answer Grad-

ing with High Accuracy. Proceedings of NAACL-

HLT 2016, pages 1070–1075, San Diego, Califor-

nia, June 12-17, 2016.  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

http://www.aclweb.org/anthology/N16-1123
http://www.aclweb.org/anthology/N16-1123

