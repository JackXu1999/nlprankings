



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623–1633
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1149

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623–1633
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1149

Bridging Text and Knowledge by Learning Multi-Prototype Entity
Mention Embedding

Yixin Cao1, Lifu Huang2, Heng Ji2, Xu Chen1, Juanzi Li1∗
1 Tsinghua National Laboratory for Information Science and Technology

Dept. of Computer Science and Technology, Tsinghua University, China 100084
{caoyixin2011,successcx,lijuanzi2008}@gmail.com

2 Dept. of Computer Science, Rensselaer Polytechnic Institute, USA 12180
{huangl7,jih}@rpi.edu

Abstract

Integrating text and knowledge into a uni-
fied semantic space has attracted signifi-
cant research interests recently. However,
the ambiguity in the common space re-
mains a challenge, namely that the same
mention phrase usually refers to various
entities. In this paper, to deal with the
ambiguity of entity mentions, we propose
a novel Multi-Prototype Mention Embed-
ding model, which learns multiple sense
embeddings for each mention by jointly
modeling words from textual contexts and
entities derived from a knowledge base.
In addition, we further design an efficient
language model based approach to disam-
biguate each mention to a specific sense.
In experiments, both qualitative and quan-
titative analysis demonstrate the high qual-
ity of the word, entity and multi-prototype
mention embeddings. Using entity linking
as a study case, we apply our disambigua-
tion method as well as the multi-prototype
mention embeddings on the benchmark
dataset, and achieve state-of-the-art per-
formance.

1 Introduction

Jointly learning text and knowledge representa-
tions in a unified vector space greatly benefits
many Natural Language Processing (NLP) tasks,
such as knowledge graph completion (Han et al.,
2016; Wang and Li, 2016), relation extraction
(Weston et al., 2013), word sense disambiguation
(Mancini et al., 2016), entity classification (Huang
et al., 2017) and linking (Huang et al., 2015).

Existing work can be roughly divided into two
categories. One is encoding words and entities
into a unified vector space using Deep Neural

∗Corresponding author.

Networks (DNN). These methods suffer from the
problems of expensive training and great limita-
tions on the size of word and entity vocabulary
(Han et al., 2016; Toutanova et al., 2015; Wu et al.,
2016). The other is to learn word and entity em-
beddings separately, and then align similar words
and entities into a common space with the help of
Wikipedia hyperlinks, so that they share similar
representations (Wang et al., 2014; Yamada et al.,
2016).

m1

m1

m2

… action film ''Independence 
Day'', the United States military 

uses alien technology…
… holds annual Independence 

Day celebrations and other 
festivals …

… bands played it during 
public events, such as July 4th 

celebrations.
Text

d1

d2

d3

Independence Day (film)

Independence Day (US)

Entity

e2

e1

Knowledge Base

Independence Day
July 4th

Mention

m2
m1

Figure 1: Examples.

However, there are two major problems arising
from directly integrating word and entity embed-
dings into a unified semantic space. First, men-
tion phrases are highly ambiguous and can refer to
multiple entities in the common space. As shown
in Figure 1, the same mention independence day
(m1) can either refer to a holiday: Independence
Day (US) or a film: Independence Day (film). Sec-
ond, an entity often has various aliases when men-
tioned in various contexts, which implies a much
larger size of mention vocabulary compared with
entities. For example, in Figure 1, the documents
d2 and d3 describes the same entity Independence
Day (US) (e2) with distinct mentions: indepen-
dence day and July 4th. We observe tens of mil-
lions of mentions referring to 5 millions of entities
in Wikipedia.

To address these issues, we propose to learn
multiple embeddings for mentions inspired by
the Word Sense Disambiguation (WSD) task
(Reisinger and Mooney, 2010; Huang et al., 2012;

1623

https://doi.org/10.18653/v1/P17-1149
https://doi.org/10.18653/v1/P17-1149


Tian et al., 2014; Neelakantan et al., 2014; Li and
Jurafsky, 2015). The basic idea behind it is to con-
sider entities in KBs that can provide a meaning
repository of mentions (i.e. words or phrases) in
texts. That is, each mention has one or multiple
meanings, namely mention senses, and each sense
corresponds to an entity. Furthermore, we assume
that different mentions referring to the same en-
tity express the same meaning and share a com-
mon mention sense embedding, which largely re-
duces the size of mention vocabulary to be learned.
For example, the mentions Independence Day in
d2 and July 4th in d3 have a common mention
sense embedding during training since they refer
to the same holiday. Thus, text and knowledge are
bridged via mention sense.

In this paper, we propose a novel Multi-
Prototype Mention Embedding (MPME) model,
which jointly learns the representations of words,
entities, and mentions at sense level. Different
mention senses are distinguished by taking ad-
vantage of both textual context information and
knowledge of reference entities. Following the
frameworks in (Wang et al., 2014; Yamada et al.,
2016), we use separate models to learn the rep-
resentations for words, entities and mentions, and
further align them by a unified optimization ob-
jective. Extending from skip-gram model and
CBOW model, our model can be trained effi-
ciently (Mikolov et al., 2013a,b) from a large
scale corpus. In addition, we also design a lan-
guage model based approach to determine the
sense for each mention in a document based on
multi-prototype mention embeddings.

For evaluation, we first provide qualitative anal-
ysis to verify the effectiveness of MPME to bridge
text and knowledge representations at the sense
level. Then, separate tasks for words and enti-
ties show improvements by using our word, en-
tity and mention representations. Finally, using
entity linking as a case study, experimental results
on the benchmark dataset demonstrate the effec-
tiveness of our embedding model as well as the
disambiguation method.

2 Preliminaries

In this section, we formally define the input and
output of multi-prototype mention embedding.

A knowledge baseKB contains a set of entities
E = {ej}, and their relations. We use Wikipedia
as the given knowledge base, and organize it as a

directed knowledge network: nodes denote enti-
ties, and edges are outlinks from Wikipedia pages.
In the directed network, we define the entities that
point to ej as its neighborsN (ej), but ignore those
entities that ej points to, so that the repeated com-
putations on the same edge would be avoided if
edges were undirected.

A text corpus D is a set of sequential words
D = {w1, · · · , wi, · · · , w|D|}, where wi is the ith
word and |D| is the length of the word sequence.
Since an entity mention ml may consist of mul-
tiple words, we define an annotated text corpus1

as D′ = {x1, · · · , xi, · · · , x|D′|}, where xi cor-
responds to either a word wi or a mention ml.
We define the words around xi within a predefined
window as its context words C(xi).

An Anchor is a Wikipedia hyperlink from a
mention ml linking to its entity ej , and is repre-
sented as a pair< mh, ej >∈ A. The anchors pro-
vide mention boundaries as well as their reference
entities from Wikipedia articles. These Wikipedia
articles are used as an annotated text corpus D′ in
this paper.

Multi-Prototype Mention Embedding . Given
a KB, an annotated text corpus D′ and a set of
anchors A, we aim to learn multi-prototype men-
tion embedding, namely multiple sense embed-
dings sjl ∈ Rk for each mention ml as well as
word embeddings w and entity embeddings e. We
useM∗l = {slj} to denote the sense set of mention
ml, where each slj refers to an entity ej . Thus,
the vocabulary size is reduced to a fixed number
|{s∗j}| = |E|. We use s∗j to denote the shared sense
of mentions referring to entity ej .
Example As shown in Figure 1, Independence
Day (m1) has two mention senses s11, s

1
2, and July

4th (m2) has one mention sense s22. Based on the
assumption in Section 1, we have s∗2 = s

1
2 = s

2
2

referring to entity Independence Day (US) (e2).

3 An Overview of Our Method

Given a knowledge base KB, an annotated text
corpusD′ and a set of anchorsA, we aim to jointly
learn word, entity and mention sense representa-
tions: w, e, s.

As shown in Figure 2, our framework contains
two key components:

1Generally, the mention boundary can be obtained by
using NER tools like Standford NER (Finkel et al., 2005).
In this paper, we use Wikipedia anchors as annotations of
Wikipedia text corpus for the concentration of our main pur-
pose.

1624



Representation Learning

Entity Representation Learning

Text Representation Learning

bands played it 
during public events, 

such as 
[[Independence Day 

(US)|July 4th]] 
celebrations

… In the 1996 action film [[Independence Day 
(film)|Independence Day]], the United States 

military uses alien technology captured …

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

computing similarity between word and mention
embeddings referring to that entity.

3 Method

In this section, we present three main components
in MPME: text model, knowledge model and joint
model, and then introduce the detailed information
on training process. Finally, we briefly introduce
the framework for entity linking.

3.1 Skip-gram model
capable of iterative learning; capable of learning
more mention names; capable of tuning mention
sense via text model; capable of NIL sense; 1. take
pre-trained word and entity embeddings as input;
2. collect mention name to entity title mapping;
use anchor to annotate each mention. each men-
tion corresponds multiple sense; each sense relates
to one entity title. 3. given the context and the
mention’s sense, predict the entity; got entity title
embedding. 4. each title has multiple vector, each
corresponds to a different entity. maintain the con-
text cluster; the cluster role. 5. text model again,
use context to predict mention sense, to predict the
context; also can predict a new sense, called NIL
in EL tasks, future work.

3.2 Text model

Lw =
TX

t=1

logP (wt+j |wmt , si)P (si|wcontext)

+

TX

t=1

X

�cjc,j 6=0
logP (wt+j |wt)

(1)

DX CX
P (wt+j |wmt , si)P (si|wmt , wcontext)

3.3 Knowledge model
KBX NX

P (eneighbor|ei)

3.4 Joint model
AX

P (ej |wmt , si) + P (ej |wcontext)

eIndependence Day (film)

eIndependence Day (US)

wm1Independence Day

wm2Independence Day

3.5 Training

3.6 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation

4.2 Baseline Methods

1. directly align words with entity.
2. align mention with entity using single proto-

type model.

4.3 Parameter Setting

4.4 Text Evaluation

4.5 Entity Evaluation

4.6 EL evaluation

5 Related Work

6 Conclusion

References
Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.

Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging deep neural networks and knowl-
edge graphs for entity disambiguation. CoRR,
abs/1504.07678.

Massimiliano Mancini, José Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Embed-
ding words and senses together via joint knowledge-
enhanced training. CoRR, abs/1612.02703.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Christopher J. C. Burges, Léon Bot-
tou, Zoubin Ghahramani, and Kilian Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 26: 27th Annual Conference on Neural In-
formation Processing Systems 2013. Proceedings of
a meeting held December 5-8, 2013, Lake Tahoe,
Nevada, United States., pages 3111–3119.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi
Choudhury, and Michael Gamon. 2015. Represent-
ing text for joint embedding of text and knowledge
bases. ACL Association for Computational Linguis-
tics.

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

computing similarity between word and mention
embeddings referring to that entity.

3 Method

In this section, we present three main components
in MPME: text model, knowledge model and joint
model, and then introduce the detailed information
on training process. Finally, we briefly introduce
the framework for entity linking.

3.1 Skip-gram model
capable of iterative learning; capable of learning
more mention names; capable of tuning mention
sense via text model; capable of NIL sense; 1. take
pre-trained word and entity embeddings as input;
2. collect mention name to entity title mapping;
use anchor to annotate each mention. each men-
tion corresponds multiple sense; each sense relates
to one entity title. 3. given the context and the
mention’s sense, predict the entity; got entity title
embedding. 4. each title has multiple vector, each
corresponds to a different entity. maintain the con-
text cluster; the cluster role. 5. text model again,
use context to predict mention sense, to predict the
context; also can predict a new sense, called NIL
in EL tasks, future work.

3.2 Text model

Lw =
TX

t=1

logP (wt+j |wmt , si)P (si|wcontext)

+

TX

t=1

X

�cjc,j 6=0
logP (wt+j |wt)

(1)

DX CX
P (wt+j |wmt , si)P (si|wmt , wcontext)

3.3 Knowledge model
KBX NX

P (eneighbor|ei)

3.4 Joint model
AX

P (ej |wmt , si) + P (ej |wcontext)

eIndependence Day (film)

eIndependence Day (US)

wm1Independence Day

wm2Independence Day

3.5 Training

3.6 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation

4.2 Baseline Methods

1. directly align words with entity.
2. align mention with entity using single proto-

type model.

4.3 Parameter Setting

4.4 Text Evaluation

4.5 Entity Evaluation

4.6 EL evaluation

5 Related Work

6 Conclusion

References
Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.

Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging deep neural networks and knowl-
edge graphs for entity disambiguation. CoRR,
abs/1504.07678.

Massimiliano Mancini, José Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Embed-
ding words and senses together via joint knowledge-
enhanced training. CoRR, abs/1612.02703.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Christopher J. C. Burges, Léon Bot-
tou, Zoubin Ghahramani, and Kilian Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 26: 27th Annual Conference on Neural In-
formation Processing Systems 2013. Proceedings of
a meeting held December 5-8, 2013, Lake Tahoe,
Nevada, United States., pages 3111–3119.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi
Choudhury, and Michael Gamon. 2015. Represent-
ing text for joint embedding of text and knowledge
bases. ACL Association for Computational Linguis-
tics.

Anchor

Text 4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

tities by modeling semantic network constructed
from the given knowledge base.

Joint model learns multiple mention embeddings
by maximizing the probability of the mention in
the context referring to target entity.

Text model .

Kg model .

Joint model .
As shown in Figure 3, for each anchor ai =

(mj , ek), we firstly replace the mention name
with entity title m⇤t via pre-defined mapping rules.
Given KB, D and the mapped anchors A, we it-
eratively train the three models until convergence
using a joint optimization objective, which will be
introduced later.

Though following the basic components of
three models in (Wang et al., 2014; Yamada et al.,
2016), MPME designs different structure in text
model and joint model to combine text and knowl-
edge in phrase level via multi-prototype mention
embedding, rather than aligning between single-
prototype word embeddings and entity embed-
dings. Actually, MPME is flexible to utilize pre-
trained entity embeddings from arbitrary knowl-
edge representation model, and enjoys their ad-
vantages of different aspects in knowledge bases2.
This is reasonable because we output two sepa-
rately semantic vector spaces for text and knowl-
edge respectively, while we can still obtain the re-
latedness between word and entity indirectly by
computing similarity between word and mention
embeddings referring to that entity.

3 Method

In this section, we present three main components
in MPME: text model, knowledge model and joint
model, and then introduce the detailed information
on training process. Finally, we briefly introduce
the framework for entity linking.

3.1 Skip-gram model
capable of iterative learning; capable of learning
more mention names; capable of tuning mention
sense via text model; capable of NIL sense; 1. take
pre-trained word and entity embeddings as input;
2. collect mention name to entity title mapping;
use anchor to annotate each mention. each men-
tion corresponds multiple sense; each sense relates

2Thus, MPME only trains text model and joint model.

to one entity title. 3. given the context and the
mention’s sense, predict the entity; got entity title
embedding. 4. each title has multiple vector, each
corresponds to a different entity. maintain the con-
text cluster; the cluster role. 5. text model again,
use context to predict mention sense, to predict the
context; also can predict a new sense, called NIL
in EL tasks, future work.

3.2 Text model

Lw =
TX

t=1

logP (wt+j |wmt , si)P (si|wcontext)

+
TX

t=1

X

�cjc,j 6=0
logP (wt+j |wt)

(1)

DX CX
P (wt+j |wmt , si)P (si|wmt , wcontext)

3.3 Knowledge model

KBX NX
P (eneighbor|ei)

3.4 Joint model

AX
P (ej |wmt , si) + P (ej |wcontext)

eIndependence Day (film)

eIndependence Day (US)

wm1Independence Day

wm2Independence Day

wfilm

wcelebrations

wmMemorial Day

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

tities by modeling semantic network constructed
from the given knowledge base.

Joint model learns multiple mention embeddings
by maximizing the probability of the mention in
the context referring to target entity.

Text model .

Kg model .

Joint model .
As shown in Figure 3, for each anchor ai =

(mj , ek), we firstly replace the mention name
with entity title m⇤t via pre-defined mapping rules.
Given KB, D and the mapped anchors A, we it-
eratively train the three models until convergence
using a joint optimization objective, which will be
introduced later.

Though following the basic components of
three models in (Wang et al., 2014; Yamada et al.,
2016), MPME designs different structure in text
model and joint model to combine text and knowl-
edge in phrase level via multi-prototype mention
embedding, rather than aligning between single-
prototype word embeddings and entity embed-
dings. Actually, MPME is flexible to utilize pre-
trained entity embeddings from arbitrary knowl-
edge representation model, and enjoys their ad-
vantages of different aspects in knowledge bases2.
This is reasonable because we output two sepa-
rately semantic vector spaces for text and knowl-
edge respectively, while we can still obtain the re-
latedness between word and entity indirectly by
computing similarity between word and mention
embeddings referring to that entity.

3 Method

In this section, we present three main components
in MPME: text model, knowledge model and joint
model, and then introduce the detailed information
on training process. Finally, we briefly introduce
the framework for entity linking.

3.1 Skip-gram model
capable of iterative learning; capable of learning
more mention names; capable of tuning mention
sense via text model; capable of NIL sense; 1. take
pre-trained word and entity embeddings as input;
2. collect mention name to entity title mapping;
use anchor to annotate each mention. each men-
tion corresponds multiple sense; each sense relates

2Thus, MPME only trains text model and joint model.

to one entity title. 3. given the context and the
mention’s sense, predict the entity; got entity title
embedding. 4. each title has multiple vector, each
corresponds to a different entity. maintain the con-
text cluster; the cluster role. 5. text model again,
use context to predict mention sense, to predict the
context; also can predict a new sense, called NIL
in EL tasks, future work.

3.2 Text model

Lw =
TX

t=1

logP (wt+j |wmt , si)P (si|wcontext)

+
TX

t=1

X

�cjc,j 6=0
logP (wt+j |wt)

(1)

DX CX
P (wt+j |wmt , si)P (si|wmt , wcontext)

3.3 Knowledge model

KBX NX
P (eneighbor|ei)

3.4 Joint model

AX
P (ej |wmt , si) + P (ej |wcontext)

eIndependence Day (film)

eIndependence Day (US)

wm1Independence Day

wm2Independence Day

wfilm

wcelebrations

wmMemorial Day

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2 Skip-gram model

g(Independence Day, )

P (N (ej)|ej)
P (ej |C(mh), tsl )
P (C(wi)|wi)P (C(mh)|tsl ,mh)

3.3 Text model

Lw =
TX

t=1

logP (wt+j |wmt , si)P (si|wcontext)

+
TX

t=1

X

�cjc,j 6=0
logP (wt+j |wt)

(1)

DX CX
P (wt+j |wmt , si)P (si|wmt , wcontext)

3.4 Knowledge model
KBX NX

P (eneighbor|ei)

3.5 Joint model
AX

P (ej |wmt , si) + P (ej |wcontext)

3.6 Training

3.7 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation

4.2 Baseline Methods

1. directly align words with entity.
2. align mention with entity using single proto-

type model.

4.3 Parameter Setting

4.4 Qualitative Analysis

4.5 Entity Relatedness

4.6 Word Analogy

4.7 EL evaluation

5 Related Work

6 Conclusion

References
Antoine Bordes, Nicolas Usunier, Alberto Garcı́a-

Durán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Burges et al. (Burges et al., 2013),
pages 2787–2795.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging deep neural networks and knowl-
edge graphs for entity disambiguation. CoRR,
abs/1504.07678.

Massimiliano Mancini, José Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Embed-
ding words and senses together via joint knowledge-
enhanced training. CoRR, abs/1612.02703.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Burges et al. (Burges et al., 2013),
pages 3111–3119.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi
Choudhury, and Michael Gamon. 2015. Represent-
ing text for joint embedding of text and knowledge
bases. ACL Association for Computational Linguis-
tics.

Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced
representation learning for knowledge graph. In
Subbarao Kambhampati, editor, Proceedings of the
Twenty-Fifth International Joint Conference on Arti-
ficial Intelligence, IJCAI 2016, New York, NY, USA,
9-15 July 2016, pages 1293–1299. IJCAI/AAAI
Press.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In Alessandro Moschitti, Bo Pang, and
Walter Daelemans, editors, Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1591–1601. ACL.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2013, 18-21 October 2013,
Grand Hyatt Seattle, Seattle, Washington, USA, A
meeting of SIGDAT, a Special Interest Group of the
ACL, pages 1366–1371. ACL.

Independence  
Day (US)

United 
States

Fireworks

Independence  
Day (film)

Memorial 
Day

Celebrations

Ob
se

rv
ed

 b
y

Public holidays in 
the United States

cat
ego

ry

Will 
Smith

st
ar

rin
g

Philadelphia

bo
rn country

inlink

out
link inlink

Knowledge Base

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2 Skip-gram model

g(Independence Day, )

P (N (ej)|ej)
P (ej |C(mh), tsl )
e1
e2

P (C(wi)|wi)
· P (C(mh)|tsl ,mh)

(1)

t1Independence Day
t2Independence Day
t1Memorial Day

g(Independence Day,

Independence Day (US)) (2)

g(Independence Day)

g(July 4th)
(3)

3.3 Text model

Lw =
TX

t=1

logP (wt+j |wmt , si)P (si|wcontext)

+
TX

t=1

X

�cjc,j 6=0
logP (wt+j |wt)

(4)

DX CX
P (wt+j |wmt , si)P (si|wmt , wcontext)

3.4 Knowledge model
KBX NX

P (eneighbor|ei)

3.5 Joint model
AX

P (ej |wmt , si) + P (ej |wcontext)

3.6 Training

3.7 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation

4.2 Baseline Methods

1. directly align words with entity.
2. align mention with entity using single proto-

type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
4.5 Entity Relatedness
4.6 Word Analogy
4.7 EL evaluation

5 Related Work

6 Conclusion

References
Antoine Bordes, Nicolas Usunier, Alberto Garcı́a-

Durán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Burges et al. (Burges et al., 2013),
pages 2787–2795.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging deep neural networks and knowl-
edge graphs for entity disambiguation. CoRR,
abs/1504.07678.

Massimiliano Mancini, José Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Embed-
ding words and senses together via joint knowledge-
enhanced training. CoRR, abs/1612.02703.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Burges et al. (Burges et al., 2013),
pages 3111–3119.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi
Choudhury, and Michael Gamon. 2015. Represent-
ing text for joint embedding of text and knowledge
bases. ACL Association for Computational Linguis-
tics.

Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced
representation learning for knowledge graph. In
Subbarao Kambhampati, editor, Proceedings of the
Twenty-Fifth International Joint Conference on Arti-
ficial Intelligence, IJCAI 2016, New York, NY, USA,
9-15 July 2016, pages 1293–1299. IJCAI/AAAI
Press.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2 Skip-gram model

g(Independence Day, )

P (N (ej)|ej)
P (ej |C(mh), tsl )
e1
e2

P (C(wi)|wi)
· P (C(mh)|tsl ,mh)

(1)

t1Independence Day
t2Independence Day
t1Memorial Day

g(Independence Day,

Independence Day (US)) (2)

g(Independence Day)

g(July 4th)
(3)

3.3 Text model

Lw =
TX

t=1

logP (wt+j |wmt , si)P (si|wcontext)

+
TX

t=1

X

�cjc,j 6=0
logP (wt+j |wt)

(4)

DX CX
P (wt+j |wmt , si)P (si|wmt , wcontext)

3.4 Knowledge model
KBX NX

P (eneighbor|ei)

3.5 Joint model
AX

P (ej |wmt , si) + P (ej |wcontext)

3.6 Training

3.7 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation

4.2 Baseline Methods

1. directly align words with entity.
2. align mention with entity using single proto-

type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
4.5 Entity Relatedness
4.6 Word Analogy
4.7 EL evaluation

5 Related Work

6 Conclusion

References
Antoine Bordes, Nicolas Usunier, Alberto Garcı́a-

Durán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Burges et al. (Burges et al., 2013),
pages 2787–2795.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging deep neural networks and knowl-
edge graphs for entity disambiguation. CoRR,
abs/1504.07678.

Massimiliano Mancini, José Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Embed-
ding words and senses together via joint knowledge-
enhanced training. CoRR, abs/1612.02703.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Burges et al. (Burges et al., 2013),
pages 3111–3119.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi
Choudhury, and Michael Gamon. 2015. Represent-
ing text for joint embedding of text and knowledge
bases. ACL Association for Computational Linguis-
tics.

Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced
representation learning for knowledge graph. In
Subbarao Kambhampati, editor, Proceedings of the
Twenty-Fifth International Joint Conference on Arti-
ficial Intelligence, IJCAI 2016, New York, NY, USA,
9-15 July 2016, pages 1293–1299. IJCAI/AAAI
Press.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

wi/t
s
l , , , w, , ej , e (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

C(·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

C(·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

C(·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

Mention Representation Learning

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

wi/t
s
l , , , w, , ej , e (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

C(·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

C(·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

C(·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

N (·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

N (·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

N (·) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

played it during public 
events, such as 

[[                                 ]] 
celebrations

Mention Sense 
Mapping

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

3.2.3 Text Representation Learning
Given the annotated text corpus, we learn word
and mention representations simultaneously by us-
ing a multi-prototype embedding model. Partic-
ularly, each word has a unique vector, and each
mention has multiple sense vectors including two
kinds of mention senses: entity-centric sense and
out-of-KB sense.

Based on the fixed number of entity-centric
senses (Section 3.1), we further learn a varying
number of out-of-KB senses for each entity title.
When encounter an mention of entity title tl, in-
spired by the idea of word sense disambiguation
(WSD) task, we use the context information to
distinguish existing mention senses, or create a
new out-of-KB sense. To be concrete, each men-
tion sense has an embedding (sense vector) tsl and
a context cluster with center µ(tsl ). The repre-
sentation of the context is defined as the aver-
age of the word vectors in the context: C(wi) =

1
|C(wi)|

P
wj2C(wi)wj.

We predict tsl , the sense of entity title tl in the
mention < tl, C(tl) >, when observed with con-
text C(tl) as the context cluster membership. For-
mally, we have:

tsl =

⇢
ts+1l t

max
l < �

tmaxl otherwise
(5)

where � is a hyper-parameter and tmaxl =
argmaxtsl

sim(µ(tsl ), C(tl)). We adopt an online
non-parametric clustering procedure to learn out-
of-KB mention senses, which means that if the
nearest distance of the context vector to sense clus-
ter center is larger than a threshold, we create a
new context cluster and a new sense vector that
doesn’t belong to any entity-centric senses. The
cluster center is the average of all the context vec-
tors belonging to that cluster. For the similarity
metric, we use cosine in our experiments.

Here, we extend Skip-gram model to learn word
embeddings as well as mention sense embeddings
by the following objective to maximize the proba-
bility of observing the context words given either
a word wi or a mention sense of entity title tsl :

Lw =
X

wi,tl2D
P (C(wi)|wi) + P (C(tl)|tl, tsl )

(6)

g(July 4th, e1) (7)

3.2.4 Entity-centric Sense Representation
Learning
Lm =

X

(mh,ej)2A
P (ej |C(mh), tsl ) (8)

3.2.5 Jointly Training
3.3 Integrating into GBDT for EL

4 Experiment

4.1 Data Preparation
4.2 Baseline Methods
1. directly align words with entity.

2. align mention with entity using single proto-
type model.

4.3 Parameter Setting
4.4 Qualitative Analysis
before conducting the experiments on the tasks,
we first give qualitative analysis of words, men-
tions and entities.

firstly, we give the phrase embedding by its
nearest words and entities.

next, we give quantitative analysis on several
tasks.

4.5 Entity Relatedness
4.6 Word Similarity
4.7 EL evaluation
4.7.1 gbdt
4.7.2 unsupervised
5 Related Work

6 Conclusion

References
Alfred V Aho and Margaret J Corasick. 1975. Effi-

cient string matching: an aid to bibliographic search.
Communications of the ACM, 18(6):333–340.

J-I Aoe. 1989. An efficient digital search algorithm by
using a double-array structure. IEEE Transactions
on Software Engineering, 15(9):1066–1077.

Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-
mani, and Kilian Q. Weinberger, editors. 2013. Ad-
vances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR,
abs/1611.04125.

3

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

as well as word embeddings w and entity embed-
dings e. Note that slj 2 m⇤l denotes that mention
sense of ml refers to entity ej , where m⇤l repre-
sents the sense set of ml. Different mentions may
share the same mention sense, denoted as s⇤j .
Example As shown in Figure 1, there are two
different mentions “Independence Day” m1 and
“July 4th” m2 in the documents. MPME is to
learn two mention senses s11, s

1
2 for m1, and one

mention sense s22 for m2. Clearly, these two men-
tions share a common sense in the last two docu-
ments: the United States holiday e2, so we have
s⇤2 = s

1
2 = s

2
2. Note that w,m, s are naturally em-

bedded into the same semantic space since they
are basic units in texts, and e modeling the graph
structure in KB is actually in another semantic
space.

3 Method

In this section, we firstly describe the framework
of MPME, followed by the detailed information of
each key component. Then, we introduce a well
designed mention sense disambiguation method,
which can also be used for entity linking in a un-
supervised way.

eNational Day

s⇤Independence Day (film), s
⇤
Independence Day (US)

3.1 Framework
Given KB, D and A, we are to jointly learn
word, entity and mention representations: w, e,
m. Serving as basic units in texts, Word {wi}
and entity title {tl} are naturally embedded into
a unified semantic space, meanwhile entities {ej}
are mapped to one of mention senses of its ti-
tle: tsl . Thus, text and knowledge are com-
bined via the bridge of mentions. We can eas-
ily obtain the similarity between word and en-
tity Similarity(wi, ej) by computing the similar-
ity between word and its corresponding mention
sense: Similarity(wi, f(ej)).

As shown in Figure 2, our proposed MPME
contains four key components: (1) Mention Sense
Mapping: we map the anchor < mh, ej >2 A to
the corresponding mention sense tsl to reduce the
vocabulary to learn. (2) Entity Representation
Learning given a knowledge base KB, we con-
struct a knowledge network among entities, and

learn their embeddings so that similar entities on
the graph have similar representations. (3) Text
Representation Learning given text corpus D as
well as the annotated anchors, we learn word and
entity title embeddings by maximizing the prob-
ability of co-occurring words/entity titles so that
similar words/entity titles have similar represen-
tations. (4) Mention Representation Learning
given annotated anchors tsl :< mh, ej >2 A,
we learn entity title embeddings by incorporating
both contextual words embeddings and entity em-
beddings in order to distinguish different mention
senses that has similar representations to its corre-
sponding entity embeddings.

Representation learning of (2), (3) and (4) uses
an iterative update procedure following a unified
optimization objective. The outputs of word em-
beddings wi and entity embeddings ej keep their
own semantic space and are naturally bridged via
the new learned entity title embeddings tl, which
inspires us to globally optimize the probability of
choosing mention senses of all the phrases of men-
tion names in the given document. Since each
mention sense corresponds to an entity, the men-
tion sense disambiguation process can also be re-
garded as linking entities to knowledge base in a
unsupervised way, which will be detailed in Sec-
tion ??.

3.2 Mention Sense Mapping

There are two kinds of mappings: from entities to
mention senses, and from mention names to men-
tion senses. The former is pre-defined at the very
beginning. Given the knowledge Base KB, we ex-
tract entity titles {tl} and initialize with multiple
mention senses, where the sense number depends
on how many entities share a common title. The
latter is to find possible mention senses for the
given mention name, which is similar to candidate
mention generation in entity linking task.

Conventional candidate mention generation
generally maintains a list of pairs of mention name
and entity that denotes a candidate reference in
knowledge base for the mention name, and recog-
nizes the mention name in text by accurate string
matching. Or it firstly recognizes possible mention
names in texts using NER (Named Entity Recog-
nition) tool, and then approximately retrieves can-
didate entities via an information retrieval method.

Since this component is not key point in this
paper, we adopt the first method to collect a3

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

as well as word embeddings w and entity embed-
dings e. Note that slj 2 m⇤l denotes that mention
sense of ml refers to entity ej , where m⇤l repre-
sents the sense set of ml. Different mentions may
share the same mention sense, denoted as s⇤j .
Example As shown in Figure 1, there are two
different mentions “Independence Day” m1 and
“July 4th” m2 in the documents. MPME is to
learn two mention senses s11, s

1
2 for m1, and one

mention sense s22 for m2. Clearly, these two men-
tions share a common sense in the last two docu-
ments: the United States holiday e2, so we have
s⇤2 = s

1
2 = s

2
2. Note that w,m, s are naturally em-

bedded into the same semantic space since they
are basic units in texts, and e modeling the graph
structure in KB is actually in another semantic
space.

3 Method

In this section, we firstly describe the framework
of MPME, followed by the detailed information of
each key component. Then, we introduce a well
designed mention sense disambiguation method,
which can also be used for entity linking in a un-
supervised way.

eNational Day

s⇤Independence Day (film), s
⇤
Independence Day (US)

3.1 Framework
Given KB, D and A, we are to jointly learn
word, entity and mention representations: w, e,
m. Serving as basic units in texts, Word {wi}
and entity title {tl} are naturally embedded into
a unified semantic space, meanwhile entities {ej}
are mapped to one of mention senses of its ti-
tle: tsl . Thus, text and knowledge are com-
bined via the bridge of mentions. We can eas-
ily obtain the similarity between word and en-
tity Similarity(wi, ej) by computing the similar-
ity between word and its corresponding mention
sense: Similarity(wi, f(ej)).

As shown in Figure 2, our proposed MPME
contains four key components: (1) Mention Sense
Mapping: we map the anchor < mh, ej >2 A to
the corresponding mention sense tsl to reduce the
vocabulary to learn. (2) Entity Representation
Learning given a knowledge base KB, we con-
struct a knowledge network among entities, and

learn their embeddings so that similar entities on
the graph have similar representations. (3) Text
Representation Learning given text corpus D as
well as the annotated anchors, we learn word and
entity title embeddings by maximizing the prob-
ability of co-occurring words/entity titles so that
similar words/entity titles have similar represen-
tations. (4) Mention Representation Learning
given annotated anchors tsl :< mh, ej >2 A,
we learn entity title embeddings by incorporating
both contextual words embeddings and entity em-
beddings in order to distinguish different mention
senses that has similar representations to its corre-
sponding entity embeddings.

Representation learning of (2), (3) and (4) uses
an iterative update procedure following a unified
optimization objective. The outputs of word em-
beddings wi and entity embeddings ej keep their
own semantic space and are naturally bridged via
the new learned entity title embeddings tl, which
inspires us to globally optimize the probability of
choosing mention senses of all the phrases of men-
tion names in the given document. Since each
mention sense corresponds to an entity, the men-
tion sense disambiguation process can also be re-
garded as linking entities to knowledge base in a
unsupervised way, which will be detailed in Sec-
tion ??.

3.2 Mention Sense Mapping

There are two kinds of mappings: from entities to
mention senses, and from mention names to men-
tion senses. The former is pre-defined at the very
beginning. Given the knowledge Base KB, we ex-
tract entity titles {tl} and initialize with multiple
mention senses, where the sense number depends
on how many entities share a common title. The
latter is to find possible mention senses for the
given mention name, which is similar to candidate
mention generation in entity linking task.

Conventional candidate mention generation
generally maintains a list of pairs of mention name
and entity that denotes a candidate reference in
knowledge base for the mention name, and recog-
nizes the mention name in text by accurate string
matching. Or it firstly recognizes possible mention
names in texts using NER (Named Entity Recog-
nition) tool, and then approximately retrieves can-
didate entities via an information retrieval method.

Since this component is not key point in this
paper, we adopt the first method to collect a

outlink

Observed by

category

3

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

KB, a text corpus D and a set of anchors A, multi-
prototype mention embedding is to learn multiple
sense embeddings sjl 2 Rk for each mention ml
as well as word embeddings w and entity embed-
dings e. Note that slj 2 m⇤l denotes that mention
sense of ml refers to entity ej , where m⇤l repre-
sents the sense set of ml. Different mentions may
share the same mention sense, denoted as s⇤j .
Example As shown in Figure 1, there are two
different mentions “Independence Day” m1 and
“July 4th” m2 in the documents. MPME is to
learn two mention senses s11, s

1
2 for m1, and one

mention sense s22 for m2. Clearly, these two men-
tions share a common sense in the last two docu-
ments: the United States holiday e2, so we have
s⇤2 = s

1
2 = s

2
2. Note that w,m, s are naturally em-

bedded into the same semantic space since they
are basic units in texts, and e modeling the graph
structure in KB is actually in another semantic
space.

3 Method

In this section, we firstly describe the framework
of MPME, followed by the detailed information of
each key component. Then, we introduce a well
designed mention sense disambiguation method,
which can also be used for entity linking in a un-
supervised way.

3.1 Framework

Given knowledge base KB, text corpus D and a set
of anchors A, we are to jointly learn word, entity
and mention representations: w, e, m. As shown
in Figure 2, our proposed MPME contains four
key components: (1) Mention Sense Mapping:
given an anchor < ml, ej >, we map it to the cor-
responding mention sense to reduce the mention
vocabulary to learn embeddings. If only a men-
tion is given, we map it to several mention senses
that requires disambiguation (Section 3.4). (2)
Entity Representation Learning based on out-
links in Wikipedia pages, we construct a knowl-
edge network to represent the semantic relatedness
among entities. And then learn entity embeddings
so that similar entities on the graph have simi-
lar representations. (3) Mention Representation
Learning given mapped anchors in contexts, we
learn mention sense embeddings by incorporating
both textual context embeddings and entity em-
beddings. (4) Text Representation Learning we
extend skip-gram model to simultaneously learn

word and mention sense embeddings on annotated
text corpus D0. Following (Yamada et al., 2016),
we use wikipedia articles as text corpus, and the
anchors provide annotated mentions1.

We jointly train (2), (3) and (4) by using a uni-
fied optimization objective. The outputs embed-
dings of word and mention are naturally in the
same semantic space since they are different units
in annotated text corpus D0 for text representation
learning. Entity embeddings keep their own se-
mantics in another vector space, because we only
use them as answers to predict in mention repre-
sentation learning by extending Continuous BOW
model, which will be further discussed in Section
??.

s⇤Memorial Day

word embeddings wi and entity embeddings ej
keep their own semantic space and are naturally
bridged via the new learned entity title embed-
dings tl, which inspires us to globally optimize
the probability of choosing mention senses of all
the phrases of mention names in the given docu-
ment. Since each mention sense corresponds to an
entity, the mention sense disambiguation process
can also be regarded as linking entities to knowl-
edge base in a unsupervised way, which will be
detailed in Section ??.

3.2 Mention Sense Mapping

There are two kinds of mappings: from entities to
mention senses, and from mention names to men-
tion senses. The former is pre-defined at the very
beginning. Given the knowledge Base KB, we ex-
tract entity titles {tl} and initialize with multiple
mention senses, where the sense number depends
on how many entities share a common title. The
latter is to find possible mention senses for the
given mention name, which is similar to candidate
mention generation in entity linking task.

Conventional candidate mention generation
generally maintains a list of pairs of mention name
and entity that denotes a candidate reference in
knowledge base for the mention name, and recog-
nizes the mention name in text by accurate string

1We can also annotate text corpus by using NER tool like
python nltk to recognize mentions, and disambiguating its
mapped mention senses as described in Section 3.4. This is
an ongoing work with the goal of learning additional out-of-
KB senses by self-training. In this paper, we will focus on
the effectiveness of our model and the quality of three kinds
of learned embeddings.

3

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.

KB, a text corpus D and a set of anchors A, multi-
prototype mention embedding is to learn multiple
sense embeddings sjl 2 Rk for each mention ml
as well as word embeddings w and entity embed-
dings e. Note that slj 2 m⇤l denotes that mention
sense of ml refers to entity ej , where m⇤l repre-
sents the sense set of ml. Different mentions may
share the same mention sense, denoted as s⇤j .
Example As shown in Figure 1, there are two
different mentions “Independence Day” m1 and
“July 4th” m2 in the documents. MPME is to
learn two mention senses s11, s

1
2 for m1, and one

mention sense s22 for m2. Clearly, these two men-
tions share a common sense in the last two docu-
ments: the United States holiday e2, so we have
s⇤2 = s

1
2 = s

2
2. Note that w,m, s are naturally em-

bedded into the same semantic space since they
are basic units in texts, and e modeling the graph
structure in KB is actually in another semantic
space.

3 Method

In this section, we firstly describe the framework
of MPME, followed by the detailed information of
each key component. Then, we introduce a well
designed mention sense disambiguation method,
which can also be used for entity linking in a un-
supervised way.

3.1 Framework

Given knowledge base KB, text corpus D and a set
of anchors A, we are to jointly learn word, entity
and mention representations: w, e, m. As shown
in Figure 2, our proposed MPME contains four
key components: (1) Mention Sense Mapping:
given an anchor < ml, ej >, we map it to the cor-
responding mention sense to reduce the mention
vocabulary to learn embeddings. If only a men-
tion is given, we map it to several mention senses
that requires disambiguation (Section 3.4). (2)
Entity Representation Learning based on out-
links in Wikipedia pages, we construct a knowl-
edge network to represent the semantic relatedness
among entities. And then learn entity embeddings
so that similar entities on the graph have simi-
lar representations. (3) Mention Representation
Learning given mapped anchors in contexts, we
learn mention sense embeddings by incorporating
both textual context embeddings and entity em-
beddings. (4) Text Representation Learning we
extend skip-gram model to simultaneously learn

word and mention sense embeddings on annotated
text corpus D0. Following (Yamada et al., 2016),
we use wikipedia articles as text corpus, and the
anchors provide annotated mentions1.

We jointly train (2), (3) and (4) by using a uni-
fied optimization objective. The outputs embed-
dings of word and mention are naturally in the
same semantic space since they are different units
in annotated text corpus D0 for text representation
learning. Entity embeddings keep their own se-
mantics in another vector space, because we only
use them as answers to predict in mention repre-
sentation learning by extending Continuous BOW
model, which will be further discussed in Section
3.3.4.

Figure 2 shows a real example of “”
eMemorial Day
word embeddings wi and entity embeddings ej

keep their own semantic space and are naturally
bridged via the new learned entity title embed-
dings tl, which inspires us to globally optimize
the probability of choosing mention senses of all
the phrases of mention names in the given docu-
ment. Since each mention sense corresponds to an
entity, the mention sense disambiguation process
can also be regarded as linking entities to knowl-
edge base in a unsupervised way, which will be
detailed in Section ??.

3.2 Mention Sense Mapping
There are two kinds of mappings: from entities to
mention senses, and from mention names to men-
tion senses. The former is pre-defined at the very
beginning. Given the knowledge Base KB, we ex-
tract entity titles {tl} and initialize with multiple
mention senses, where the sense number depends
on how many entities share a common title. The
latter is to find possible mention senses for the
given mention name, which is similar to candidate
mention generation in entity linking task.

Conventional candidate mention generation
generally maintains a list of pairs of mention name
and entity that denotes a candidate reference in
knowledge base for the mention name, and recog-
nizes the mention name in text by accurate string
matching. Or it firstly recognizes possible mention

1We can also annotate text corpus by using NER tool like
python nltk to recognize mentions, and disambiguating its
mapped mention senses as described in Section 3.4. This is
an ongoing work with the goal of learning additional out-of-
KB senses by self-training. In this paper, we will focus on
the effectiveness of our model and the quality of three kinds
of learned embeddings.

…  holds annual [[Independence Day (US)|
Independence Day]] celebrations and other 

festivals …

… early Confederate [[Memorial Day]] 
celebrations were simple, somber occasions for 
veterans and their families to honor the dead …

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.

N̂ (ml) denotes neighbor mentions of ml co-
occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.

P (slj) denotes prior probability of a mention
sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.

N̂ (ml) denotes neighbor mentions of ml co-
occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.

P (slj) denotes prior probability of a mention
sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.

N̂ (ml) denotes neighbor mentions of ml co-
occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.

P (slj) denotes prior probability of a mention
sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.

N̂ (ml) denotes neighbor mentions of ml co-
occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.

P (slj) denotes prior probability of a mention
sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.
N̂ (ml) denotes neighbor mentions of ml co-

occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.
P (slj) denotes prior probability of a mention

sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.

N̂ (ml) denotes neighbor mentions of ml co-
occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.

P (slj) denotes prior probability of a mention
sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.
N̂ (ml) denotes neighbor mentions of ml co-

occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.
P (slj) denotes prior probability of a mention

sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.
N̂ (ml) denotes neighbor mentions of ml co-

occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.
P (slj) denotes prior probability of a mention

sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

predict the context words by maximizing the fol-
lowing objective function:

Lw =
X

wi,ml2D0
logP (C(wi)|wi)

+ logP (C(ml)|s⇤j )
(6)

where s⇤j = g(< ml, ej >) is obtained from an-
chors in wikipedia articles.

Thus, similar words and mention senses
will be closed in text space, such as wfilm
and s⇤Independence Day (film), or wcelebrations and
s⇤Independence Day (US) because they frequently oc-
cur in the same contexts.

Similar to WDS, we maintain a context cluster
for each mention sense, which can be used for dis-
ambiguation given the contexts (Section 5). For
example, in d1 of Figure 2, the context cluster of
s⇤ consists of all context vectors When encounter-
ing a mention, the context vector

we also maintain a context cluster center µ⇤j
for each mention sense s⇤j , which is computed
by averaging all the context vectors belonging
to the cluster. We define context vector as
the average sum of context word embeddings

1
|C(wi)|

P
wj2C(wi)wj. The cluster center is help-

ful for inducing mention sense in contexts. When
encounter a mention, we map it to a set of mention
senses, and then find the nearest one according to
the distance from its context vector to each men-
tion sense cluster center, which will be discussed
in Section 5.

d1, d2, d3, s
⇤
j , wi/s

⇤
j , e3

s⇤Independence Day (US)

P (ej |C(ml), s⇤j )

P (C(wi)|wi) · P (C(ml)|s⇤j ) (7)

4.5 Joint Training

Considering all the above representation learning
components, we define the overall objective func-
tion as linear combinations:

L = Lw + Le + Lm (8)

The training of MPME is to maximize the above
function, and iteratively update three types of em-
beddings. Also, we use negative sampling tech-
nique for efficiency (Mikolov et al., 2013a).

5 Mention Sense Disambiguation

MPME learns each mention with multiple sense
embeddings, and each sense corresponds to a con-
text cluster. Given an annotated document D0 in-
cluding M mentions, and their sense sets accord-
ing to Section ??: M⇤l = {slj |slj 2 g(ml),ml 2
M}. In this section, we describe how to determine
the mention sense for each mention ml in the doc-
ument.

Based on language model, identifying mention
senses in a document can be regarded as maximiz-
ing their joint probability. However, the global op-
timum is expensive, in which each mention gets
an optimum sense, to search over the space of all
mention senses of all mentions in the document.
Thus, we approximately assign each mention in-
dependently:

P (D0, . . . , slj , . . . , )
⇡
Y

P (D0|slj) · P (slj)

⇡
Y

P (C(ml)|slj) · P (N̂ (ml)|slj) · P (slj)
(9)

where P (C(ml)|slj) is proportional to cosine sim-
ilarity between context vector and mention sense
cluster center µlj to measure the mention’s local
similarity, namely local probability.
N̂ (ml) denotes neighbor mentions of ml co-

occurring in a piece of text (e.g. a document),
and P (N̂ (ml)|slj) is defined as global probabil-
ity since it measures global coherence of neighbor
mentions. The underlying idea is to achieve con-
sistent semantics in a piece of text assuming that
all mentions inside it are talking about the same
topic. In this paper, we regard the mention senses
identified first as neighbors of the rest mentions.
P (slj) denotes prior probability of a mention

sense occurring in texts proportional to the fre-
quency of corresponding entity in Wikipedia an-
chors:

P (slj) = (
|Aej |
|A| )

� � 2 [0, 1]

where � is a hyper-parameter to smooth the
gaps between different entity frequencies, namely
smoothing parameter. It controls the importance

Knowledge Space

Text Space

Figure 2: Framework of Multi-Prototype Mention Embedding model.

Mention Sense Mapping To reduce the size of
the mention vocabulary, each mention is mapped
to a set of shared mention senses according to
a predefined dictionary. We build the dictionary
by collecting entity-mention pairs < ml, ej >
from Wikipedia anchors and page titles, then cre-
ate mention senses if there is a different entity. The
sense number of a mention depends on how many
different entity-mention pairs it is involved.

Formally, we have: M∗l = g(ml) =
⋃
g(<

ml, ej >) = {s∗j}, where g(·) denotes the map-
ping function from an entity mention to its men-
tion sense given an anchor. We directly use
the anchors contained in the annotated text cor-
pus D

′
for training. As Figure 2 shows, we re-

place the anchor <July 4th, Independence Day
(US)> with the corresponding mention sense:
s∗Independence Day (US).

Representation Learning Using KB, A and D′
as input, we design three separate models and a
unified optimization objective to jointly learn en-
tity, word and mention sense representations into
two semantic spaces. As shown in the knowledge
space in Figure 2, entity embeddings can reflect
their relatedness in the network. For example,
Independence Day (US) (e1) and Memorial Day
(e3) are close to each other because they share
some common neighbors, such as United States
and Public holidays in the United States.

Word and mention embeddings are learned in

the same semantic space. As two basic units in
D′, their embeddings represent their distributed
semantics in texts. For example, mention Inde-
pendence Day and word celebrations co-occur fre-
quently when it refers to the holiday: Indepen-
dence Day (US), thus they have similar representa-
tions. Without disambiguating the mention senses,
some words, such as film will also share similar
representations as Independence Day.

Besides, by introducing entity embeddings into
our MPME framework, the knowledge informa-
tion will also be distilled into mention sense em-
beddings, so that the mention sense Memorial Day
will be similar as Independence Day (US).

Mention Sense Disambiguation According to
our predefined dictionary, each mention has been
mapped to more than one senses, and learned with
multiple embedding vectors. Consequently, to in-
duce the correct sense for a mention within a con-
text is critical in the usage of the multiprototype
embeddings, especially in an unsupervised way.
Formally, given an annotated document D′, we
determine one sense ŝ∗j ∈ M∗l for each mention
ml ∈ D′, where ŝ∗j is the correct sense.

Based on language model, we design a mention
sense disambiguation method without using any
supervision that takes into account three aspects:
1) sense prior denotes how dominant the sense is,
2) local context information reflects how seman-
tically appropriate the sense is in the context, and

1625



3) global mention information denotes how se-
mantically consistent the sense is with the neigh-
bor mentions. To better utilize the context infor-
mation, we maintain a context cluster for each
mention sense during training, which will be de-
tailed in Section 4.4.

Since each mention sense corresponds to an en-
tity in the given KB, the disambiguation method is
equivalent to entity linking. Thus, text and knowl-
edge base is bridged via the multiprototype men-
tion embeddings. We will give more analysis in
Section 6.4.

4 Representation Learning

Distributional representation learning plays an in-
creasing important role in many fileds (Bengio
et al., 2013; Zhang et al., 2017, 2016) due to its
effectiveness for dimensionality reduction and ad-
dressing sparseness issue. For NLP tasks, this
trends has been accelerated by the Skip-gram and
CBOW models (Mikolov et al., 2013a,b) due to its
efficiency and remarkable semantic composition-
ality of embedding vectors. In this section, we first
briefly introduce the Skip-gram and CBOW mod-
els, and then extend them to three variants for the
word, mention and entity representation learning.

4.1 Skip-Gram and CBOW model

The basic idea of the Skip-gram and CBOW mod-
els is to model the predictive relations among se-
quential words. Given a sequence of words D, the
optimization objective of Skip-gram model is to
use the current word to predict its context words
by maximizing the average log probability:

L =
∑

wi∈D

∑

wo∈C(wi)
logP (wo|wi) (1)

In contrast, CBOW model aims to predict the
current word given its context words:

L =
∑

wi∈D
logP (wi|C(wi)) (2)

Formally, the conditional probability P (wo|wi)
is defined using a softmax function:

P (wo|wi) =
exp(wi ·wo)∑

wo∈D exp(wi ·wo)
(3)

where wi,wo denote the input and output word
vectors during training. Furthermore, these two

models can be accelerated by using hierarchi-
cal softmax or negative sampling (Mikolov et al.,
2013a,b).

4.2 Entity Representation Learning
Given a knowledge base KB, we aim to learn
entity embeddings by modeling “contextual” en-
tities, so that the entities sharing more common
neighbors tend to have similar representations.
Therefore, we extend Skip-gram model to a net-
work by maximizing the log probability of being a
neighbor entity.

Le =
∑

ej∈E
logP (N (ej)|ej) (4)

Clearly, the neighbor entities serve a similar
role as the context words in Skip-gram model. As
shown in Figure 2, entity Memorial Day (e3) also
share two common neighbors of United States and
Public holidays in the United States with entity In-
dependence Day (US), thus their embeddings are
close in the Knowledge Space. These entity em-
beddings will be later used to learn mention repre-
sentations.

4.3 Mention Representation Learning
As mentioned above, the textual context informa-
tion and reference entities are helpful to distin-
guish different senses for a mention. Thus, given
an anchor < ml, ej > and its context words
C(ml), we combine mention sense embeddings
with its context word embeddings to predict the
reference entity by extending CBOW model. The
objective function is as follows:

Lm =
∑

<ml,ej>∈A
logP (ej |C(ml), s∗j ) (5)

where s∗j = g(< ml, ej >). Thus, if two mentions
refer to similar entities and share similar contexts,
they tend to be close in semantic vector space.
Take Figure 1 as an example again, mentions Inde-
pendence Day and Memorial Day refer to similar
entities Independence Day (US) (e1) and Memo-
rial Day (e2), they also share some similar context
words, such as celebrations in documents d2, d3,
so their sense embeddings are close to each other
in the text space.

4.4 Text Representation Learning
Instead of directly using a word or a mention to
predict the context words, we incorporate mention

1626



sense to joint optimize word and sense represen-
tations, which can avoid some noise introduced
by ambiguous mentions. For example, in Fig-
ure 2, without identifying the mention Indepen-
dence Day as the holiday or the film, various dis-
similar context words such as the words celebra-
tions and film in documents d1, d2 will share simi-
lar semantics, which will further affect the perfor-
mance of entity representations during joint train-
ing.

Given the annotated corpus D′, we use a word
wi or a mention sense s∗j to predict the con-
text words by maximizing the following objective
function:

Lw =
∑

wi,ml∈D′
logP (C(wi)|wi)

+ logP (C(ml)|s∗j )
(6)

where s∗j = g(< ml, ej >) is obtained from an-
chors in Wikipedia articles.

Thus, words and mention senses will share the
same vector space, where similar words and men-
tion senses are close to each other, such as cele-
brations and Independence Day (US) because they
frequently occur in the same contexts.

Similar to WDS, we maintain a context clus-
ter for each mention sense, which can be used for
mention sense disambiguation (Section 5). The
context cluster of a mention sense s∗j contains all
the context vectors of its mentionml. We compute
context vector of ml by averaging the sum of its
context word embeddings: 1|C(ml)|

∑
wj∈C(ml)wj.

Further, the center of a context cluster µ∗j is de-
fined as the average of context vectors of all men-
tions which refer to the sense. These context clus-
ters will be later used to disambiguate the sense of
a given mention with its contexts.

4.5 Joint Training
Considering all of the above representation learn-
ing components, we define the overall objective
function as linear combinations:

L = Lw + Le + Lm (7)
The goal of training MPME is to maximize the

above function, and iteratively update three types
of embeddings. Also, we use negative sampling
technique for efficiency (Mikolov et al., 2013a).

MPME shares the same entity representation
learning method with (Yamada et al., 2016), but

the role of entities in the entire framework as well
as mention representation learning is different in
three aspects. First, we focus on learning embed-
dings for mentions, not merely words as in (Ya-
mada et al., 2016). Clearly, MPME is more natu-
ral to integrate text and knowledge base. Second,
we propose to learn multiple embeddings for each
mention denoting its different meanings. Third,
we prefer to use both mentions and context words
to predict entities, so that the distribution of en-
tities will help improve word embeddings, mean-
while, avoid being hurt if we force entity embed-
dings to satisfy word embeddings during train-
ing (Wang et al., 2014). We will give more analy-
sis in experiments.

5 Mention Sense Disambiguation

As mentioned in Section 3, we induce a correct
sense ŝ∗j ∈ M∗l for each mention ml in an an-
notated document D′. We regard this problem
from the perspective of language model that max-
imizes a joint probability of all mention senses
contained in the document. However, the global
optimum is expensive with a time complexity of
O(|M||M∗l |). Thus, we approximately identify
each mention sense independently:

P (D′, . . . , s∗j , . . . , )
≈
∏

P (D′|s∗j ) · P (s∗j )

≈
∏

P (C(ml)|s∗j ) · P (N̂ (ml)|s∗j ) · P (s∗j )
(8)

where P (C(ml)|s∗j ), local context information
(Section 3), denotes the probability of the local
contexts of ml given its mention sense s∗j . we
define it proportional to the cosine similarity be-
tween the current context vector and the sense con-
text cluster center µ∗j as described in Section 4.4.
It measures how likely a mention sense occurring
together with current context words. For example,
given the mention sense Independence Day (film),
word film is more likely to appear within the con-
text than the word celebrations.
P (N̂ (ml)|slj), global mention information, de-

notes the probability of the contextual mentions of
ml given its sense slj , where N̂ (ml) is the collec-
tion of the neighbor mentions occurring together
with ml in a predefined context window. We de-
fine it proportional to the cosine similarity be-
tween mention sense embeddings and the neigh-
bor mention vector, which is computed similar to

1627



context vector:
∑ 1
|N̂ (ml)|

ŝlj, where ŝ
l
j is the cor-

rect sense for ml.
Considering there are usually multiple mentions

in a document to be disambiguated. The men-
tions disambiguated first will be helpful for induc-
ing the senses of the rest mentions. That is, how
to choose the mentions disambiguated first will in-
fluence the performance. Intuitively, we adopt two
orders similar to (Chen et al., 2014): 1) L2R (left
to right) induces senses for all the mentions in the
document following natural order that varies ac-
cording to language, normally from left to right in
the sequence. 2) S2C (simple to complex) denotes
that we determine the correct sense for those men-
tions with fewer senses, which makes the problem
easier.

Global mention information assumes that there
should be consistent semantics in a context win-
dow, and measures whether all neighbor mentions
are related. For instance, two mentions Memorial
Day and Independence Day occur in the same doc-
ument. If we already know that Memorial Day de-
notes a holiday, then obviously Independence Day
has higher probability of being a holiday than a
film.
P (s∗j ), sense prior, is a prior probability of

sense s∗j indicating how possible it occurs with-
out considering any additional information. We
define it proportional to the frequency of sense s∗j
in Wikipedia anchors:

P (s∗j ) = (
|As∗j |
|A| )

γ γ ∈ [0, 1]

where As∗j is the set of anchors annotated with
s∗j , and γ is a smoothing hyper-parameter to con-
trol the impact of prior on the overall probability,
which is set by experiments (Section 6.4).

6 Experiment

Setup We choose Wikipedia, the March 2016
dump, as training corpus, which contains nearly
75 millions of anchors, 180 millions of edges
among entities and 1.8 billions of tokens after pre-
processing. We then train MPME2 for 1.5 millions
of words, 5 millions of entities and 1.7 millions of
mentions. The entire training process in 10 iter-
ations costs nearly 8 hours on the server with 64
core CPU and 188GB memory.

2Our main code for MPME can be found in
https://github.com/TaoMiner/bridgeGap.

We use the default settings in word2vec3, and
set our embedding dimension as 200 and context
window size as 5. For each positive example, we
sample 5 negative examples4.

Baseline Methods As far as we know, this is the
first work to deal with mention ambiguity in the
integration of text and knowledge representations,
so there is no exact baselines for comparison. We
use the method in (Yamada et al., 2016) as a base-
line, marked as ALIGN5, because (1) this is the
most similar work that directly aligns word and en-
tity embeddings. (2) it achieves the state-of-the-art
performance in entity linking task.

To investigate the effect of multi-prototype, we
degrade our method to single-prototype as another
baseline, which means to use one sense to repre-
sent all mentions with the same phrase, namely
Single-Prototype Mention Embedding (SPME).
For example, SPME only learns one unique sense
vector for Independence Day whatever it denotes
a holiday or a film.

6.1 Qualitative Analysis

We use cosine similarity to measure the similar-
ity of two vectors, and present the top 5 nearest
words and entities for two most popular senses of
the mention Independence Day. Because ALIGN
is incapable of dealing with multiple words, we
only present the results of SPME and MPME.

As shown in Figure 1, without considering men-
tion sense, the mention Independence Day can
only show a dominant holiday sense based on
SPME and ignore all other senses. Instead, MPME
successfully learns two clear and distinct senses.
For the sense Independence Day (US), all of its
nearest words and entities, such as parades, cele-
brations, and Memorial Day, are holiday related,
while for another sense Independence Day (film),
its nearest words and entities, like robocop and
The Terminator, are all science fiction films. The
results verify the effectiveness of our framework
in learning mention embeddings at the sense level.

3https://code.google.com/archive/p/word2vec/
4We tested different parameters (e.g. window size of 10

and dimension of 500) which achieve similar results, and re-
port the current settings considering program runtime effi-
ciency.

5We carefully re-implemented ALIGN and used the same
shared parameters in our model for fairly comparison. How-
ever, we failed to fully reproduce the positive result in the
original paper, meanwhile the authors are unable to release
their code.

1628



Mention Sense Nearest words Nearest entities
SPME Independence

Day
lee-jackson, thanksgiving, di-
wali, strassenfest, chiraghan

National Aboriginal and Torres Strait Islander Educa-
tion Policy, E. Chandrasekharan Nair, Jean Aileen Lit-
tle, Thessalian barbel, 1825 in birding and ornithology

MPME IndependenceDay (US)
thanksgiving, parades, lee-
jackson, festivities, celebrations

Memorial Day, Labor Day, Thanksgiving, Thanksgiv-
ing (United States), Saint Patrick’s Day

Independence
Day (film)

robocop, clockstoppers, mind-
hunters, tarantino, terminator

The Terminator, True Lies, Total Recall (1990 film),
RoboCop 2, Die Hard

Table 1: The nearest neighbors of mention Independence Day.

6.2 Entity Relatedness

To evaluate the quality of entity embeddings, we
conduct experiments using the dataset which is de-
signed for measuring entity relatedness (Ceccarelli
et al., 2013; Huang et al., 2015; Yamada et al.,
2016). The dataset contains 3,314 entities, and
each mention has 91 candidate entities on average
with gold-standard labels indicating whether they
are semantically related.

We compute cosine similarity between entity
embeddings to measure their relatedness, and rank
them in a descending order. To evaluate the
ranking quality, we use two standard metrics:
normalized discounted cumulative gain (NDCG)
(Järvelin and Kekäläinen, 2002) and mean average
precision (MAP) (Schütze, 2008).

We design another baseline method: En-
tity2vec, which learns entity embeddings using
the method described in Section 4.2, without joint
training with word and mention sense embed-
dings.

Table 2: Entity Relatedness.
NDCG MAP

@1 @5 @10
ALIGN 0.416 0.432 0.472 0.410
Entity2vec 0.593 0.595 0.636 0.566
SPME 0.593 0.594 0.636 0.566
MPME 0.613 0.613 0.654 0.582

As shown in Table 2, ALIGN achieves lower
performance than Entity2vec, because it doesn’t
consider the mention phrase ambiguity and yields
lots of noise when forcing entity embeddings to
satisfy word embeddings and aligning them into
the unified space. For example, the entity Gente
(magazine) should be more relevant to the en-
tity France, the place where its company lo-
cates. However, ALIGN mixed various meanings
of mention Gente (e.g., the song) and ranked some
bands higher (e.g., entity Poolside (band)).

SPME also doesn’t consider the ambiguity of

mentions but achieves comparative results with
Entity2vec. We analyze the reasons and find that,
it can avoid some noise by using word embed-
dings to predict entities. MPME outperforms all
the other methods, which demonstrates that the
unambiguous textual information is helpful to re-
fine the entity embeddings.

6.3 Word Analogical Reasoning

Following (Mikolov et al., 2013a; Wang et al.,
2014), we use the word analogical reasoning
task to evaluate the quality of word embeddings.
The dataset consists of 8,869 semantic questions
(“Paris”:“France”::“Rome”:?), and 10,675 syn-
tactic questions (e.g., “sit”:“sitting”::“walk”:?).
We solve it by finding the closest word vector w?
to wFrance−wParis+wRome according to cosine
similarity. We compute accuracy for top 1 nearest
word to measure the performance.

Table 3: Word Analogical Reasoning.
Word2vec ALIGN SPME MPME

Semantic 66.78 68.34 71.65 71.65
Syntactic 61.58 59.73 55.28 54.75

We also adopt Word2vec6 as an additional base-
line method, which provides a standard to measure
the impact from other components on word em-
beddings.

Table 3 shows the results. We can see that
ALIGN, SPME and MPME, achieve higher
performance in dealing with semantic ques-
tions, because relations among entities (e.g.,
country-capital relation for entity France and
Paris) enhance the semantics in word embeddings
through jointly training. On the other hand, their
performance for syntactic questions is weakened
because more accurate semantics yields a bias
to predict semantic relations even though given
a syntactic query. For example, given the query
“pleasant”:“unpleasant”::“possibly”:?, our

6https://code.google.com/archive/p/word2vec/

1629



model tends to return the word (e.g., probably)
highly semantical related to query words, such
as possibly, instead of the syntactical similar
word impossibly. In this scenario, we are more
concerned about semantic task to incorporate
knowledge of reference entities into word embed-
dings, and this issue could be tackled, to some
extent, by using syntactic tool like stemming.

The word embeddings of MPME achieve the
best performance for semantic questions mainly
because (1) text representation learning has bet-
ter generalization ability due to the larger size
of training examples than entities (e.g., 1.8b v.s.
0.18b) as well as relatively smaller size of vocab-
ulary (e.g., 1.5m v.s. 5m). (2) unambiguous men-
tion embeddings capture both textual context in-
formation and knowledge, and thus enhance word
and entity embeddings.

6.4 A Case Study: Entity Linking

Entity linking is a core NLP task of identifying
the reference entity for mentions in texts. The
main difficulty lies in the ambiguity of various en-
tities sharing the same mention phrase. Previous
work addressed this issue by taking advantage of
the similarity between words and entities (Francis-
Landau et al., 2016; Sun et al., 2015), and/or
the relations among entities (Thien Huu Nguyen,
2016; Cao et al., 2015). Therefore, we use en-
tity linking as a case study for a comprehensive
measurement of the multi-prototype mention em-
beddings. Given mentions in a text, entity linking
aims to link them to a predefined knowledge base.
One of the main challenges in this task is the am-
biguity of entity mentions.

We use the public dataset AIDA created by
(Hoffart et al., 2011), which includes 1,393 docu-
ments and 27,816 mentions referring to Wikipedia
entries. The dataset has been divided into 946,
216 and 231 documents for the purpose of train-
ing, developing and testing. Following (Pershina
et al., 2015; Yamada et al., 2016), we use a pub-
licly available dictionary to generate candidate en-
tities and mention senses. For evaluation, we rank
the candidate entities for each mention and report
both standard micro (aggregates over all mentions)
and macro (aggregates over all documents) preci-
sion over top-ranked entities.

Supervised Entity Linking
Yamada et al. (2016) designed a list of fea-

tures for each mention and candidate entity pair.

By incorporating these features into a supervised
learning-to-rank algorithm, Gradient Boosting Re-
gression Tree (GBRT), each pair is assigned a
relevance score indicating whether they should
be linked to each other. Following their recom-
mended parameters, we set the number of trees as
10,000, the learning rate as 0.02 and the maximum
depth of the decision tree as 4.

Based on word and entity embeddings learned
by ALIGN, the key features in (Yamada et al.,
2016) are from two aspects: (1) the cosine simi-
larity between context words and candidate entity,
and (2) the coherence among “contextual” entities
in the same document.

To evaluate the performance of multi-prototype
mention embeddings, we incorporate the follow-
ing features into GBDT for comparison: (1) the
cosine similarity between the current context vec-
tor and the sense context cluster center µ∗j , which
denotes how likely the mention sense refers to the
candidate entity, (2) the cosine similarity between
the current context vector and the mention sense
embeddings.

Table 4: Performance of Supervised Method
ALIGN SPME MPME

Micro P@1 0.828 0.820 0.851
Macro P@1 0.862 0.844 0.881

As shown in Table 4, we can see that ALIGN
performs better than SPME. This is because
SPME learns word embeddings and entity em-
beddings in separate semantic spaces, and fails to
measure the similarity between context words and
candidate entities. However, MPME computes
the similarity between context words with mention
sense instead of entities, thus achieves the best per-
formance, which also demonstrates the high qual-
ity of the mention sense embeddings.

Unsupervised Entity Linking
Linking a mention to a specific entity equals to

disambiguating mention senses since each candi-
date entity corresponds to a mention sense. As de-
scribed in Section 5, we disambiguate senses in
two orders: (1) L2R (from left to right), and (2)
S2C (from simple to complex).

We evaluate our unsupervised disambiguation
methods on the entire AIDA dataset. To be fair, we
choose the state-of-the-art unsupervised methods,
which are proposed in (Hoffart et al., 2011; Al-
helbawy and Gaizauskas, 2014; Cucerzan, 2007;

1630



Table 5: Performance of Unsupervised Methods
Cucerzan Kulkarni Hoffart Shirakawa Alhelbawy MPME (L2R) MPME (S2C)

Micro P@1 0.510 0.729 0.818 0.823 0.842 0.882 0.885
Macro P@1 0.437 0.767 0.819 0.830 0.875 0.875 0.890

Kulkarni et al., 2009; Masumi Shirakawa and
Nishio, 2011) using the same dataset.

Table 5 shows the results. We can see that
our two methods outperform all other methods.
MPME (L2R) is more efficient and easy to ap-
ply, while MPME (S2C) slightly outperforms it
because the additional step of ranking mentions
according to their candidates number guarantees
a higher disambiguation performance for those
simple mentions, which consequently help disam-
biguate those complex mentions through global
mention information in Equation 8.

We analyze the results and observe a disam-
biguation bias to popular senses. For example,
there are three mentions in the sentence “Japan
began the defence of their Asian Cup I title with a
lucky 2-1 win against Syria in a Group C cham-
pionship match on Friday”, where the country
name Japan and Syria actually denote their na-
tional football teams, while the football match
name Asian Cup I has little ambiguity. Compared
to the team, the sense of country occurs more fre-
quently and has a dominant prior, which greatly
affects the disambiguation. By incorporating lo-
cal context information and global mention infor-
mation, both the context words (e.g., defence or
match) and the neighbor mentions (e.g., Asian Cup
I) provide us enough clues to identify a soccer re-
lated mention sense instead of the country.

Influence of Smoothing Parameter As men-
tioned above, a mention sense may possess a dom-
inant prior and greatly affect the disambiguation.
So we introduce a smoothing parameter γ to con-
trol its importance to the overall probability. Fig-
ure 3 shows the linking accuracy under different
values of γ on the dataset of AIDA. γ = 0 indi-
cates we don’t use any prior knowledge, and γ = 1
indicates the case without smoothing parameter.

We can see that both micro and macro accu-
racy decrease a lot if we don’t use the parameter
(γ = 1). Only using local and global probabilities
for disambiguation (γ = 0) achieves a comparable
performance when γ = 0.05, both accuracy reach
their peaks, which is optimal and default value in
our experiments.

Figure 3: Impact of Smoothing Parameter γ.

7 Conclusions and Future Work

In this paper, we propose a novel Multi-Prototype
Mention Embedding model that jointly learns
word, entity and mention sense embeddings.
These mention senses capture both textual con-
text information and knowledge from reference
entities, and provide an efficient approach to dis-
ambiguate mention sense in text. We conduct a
series of experiments to demonstrate that multi-
prototype mention embedding improves the qual-
ity of both word and entity representations. Using
entity linking as a study case, we apply our disam-
biguation method as well as the multi-prototype
mention embeddings on the benchmark dataset,
and achieve the state-of-the-art.

In the future, we will improve the scalability of
our model and learn multi-prototype embeddings
for the mentions without reference entities in a
knowledge base, and introduce compositional ap-
proaches to model the internal structures of multi-
word mentions.

8 Acknowledgement

This work is supported by NSFC Key Pro-
gram (No. 61533018), 973 Program (No.
2014CB340504), Fund of Online Educa-
tion Research Center, Ministry of Education
(No. 2016ZD102), Key Technologies Re-
search and Development Program of China
(No. 2014BAK04B03), NSFC-NRF (No.
61661146007) and the U.S. DARPA LORELEI
Program No. HR0011-15-C-0115.

1631



References
Ayman Alhelbawy and Robert J Gaizauskas. 2014.

Graph ranking for collective named entity disam-
biguation. In ACL (2). pages 75–80.

Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis
and machine intelligence 35(8):1798–1828.

Yixin Cao, Juanzi Li, Xiaofei Guo, Shuanhu Bai, Heng
Ji, and Jie Tang. 2015. Name list only? target en-
tity disambiguation in short texts. In EMNLP. pages
654–664. https://doi.org/10.18653/v1/D15-1077.

Diego Ceccarelli, Claudio Lucchese, Salvatore Or-
lando, Raffaele Perego, and Salvatore Trani. 2013.
Learning relatedness measures for entity linking. In
Proceedings of the 22nd ACM international con-
ference on Information & Knowledge Management.
ACM, pages 139–148.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In EMNLP. Citeseer, pages 1025–
1035. https://doi.org/10.3115/v1/D14-1110.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data .

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd annual meet-
ing on association for computational linguistics. As-
sociation for Computational Linguistics, pages 363–
370.

Matthew Francis-Landau, Greg Durrett, and Dan
Klein. 2016. Capturing semantic similarity for
entity linking with convolutional neural networks.
In Proceedings of NAACL-HLT . pages 1256–1261.
https://doi.org/10.18653/v1/N16-1150.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2016.
Joint representation learning of text and knowl-
edge for knowledge graph completion. CoRR
abs/1611.04125.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 782–792.

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. ACL.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging deep neural networks and knowledge

graphs for entity disambiguation. arXiv preprint
arXiv:1504.07678 .

Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji,
Xiang Ren, Jiawei Han, Lin Zhao, and James A
Hendler. 2017. Liberal entity extraction: Rapid con-
struction of fine-grained entity typing systems. Big
Data 5(1):19–31.

Kalervo Järvelin and Jaana Kekäläinen. 2002. Cu-
mulated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems (TOIS)
20(4):422–446.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD international con-
ference on Knowledge discovery and data mining.
ACM, pages 457–466.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proc. EMNLP. https://doi.org/10.18653/v1/D15-
1200.

Massimiliano Mancini, José Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Embed-
ding words and senses together via joint knowledge-
enhanced training. CoRR abs/1612.02703.

Haixun Wang Yangqiu Song Zhongyuan Wang Kotaro
Nakayama Takahiro Hara Masumi Shirakawa and
Shojiro Nishio. 2011. Entity disambiguation based
on a. technical report. In Technical Report MSR-TR-
2011-125. Microsoft Research.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS. pages 3111–3119.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient
non-parametric estimation of multiple embeddings
per word in vector space. In Proc. EMNLP.
https://doi.org/10.3115/v1/D14-1113.

Maria Pershina, Yifan He, and Ralph Grishman. 2015.
Personalized page rank for named entity disam-
biguation. In HLT-NAACL. pages 238–243.

Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proc. NAACL.

Hinrich Schütze. 2008. Introduction to information
retrieval. In Proceedings of the international com-
munication of association for computing machinery
conference.

1632



Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2015. Modeling mention,
context and entity with neural networks for entity
disambiguation. In IJCAI. pages 1333–1339.

Nicolas Fauceglia Mariano Rodriguez-Muro Oktie
Hassanzadeh Alfio Massimiliano Gliozzo Moham-
mad Sadoghi Thien Huu Nguyen. 2016. Joint learn-
ing of local and global features for entity linking
via neural networks. In COLING 2016, 26th Inter-
national Conference on Computational Linguistics,
Proceedings of the Conference: Technical Papers,
December 11-16, 2016, Osaka, Japan. pages 2310–
2320.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In COLING.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi
Choudhury, and Michael Gamon. 2015. Represent-
ing text for joint embedding of text and knowledge
bases. ACL Association for Computational Linguis-
tics https://doi.org/10.18653/v1/D15-1174.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and
Zheng Chen. 2014. Knowledge graph and
text jointly embedding. In Proc. EMNLP.
https://doi.org/10.3115/v1/D14-1167.

Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced
representation learning for knowledge graph. In
Proceedings of the Twenty-Fifth International Joint
Conference on Artificial Intelligence.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proc. ACL.

Jiawei Wu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2016. Knowledge representation via joint
learning of sequential text and knowledge graphs.
CoRR .

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda,
and Yoshiyasu Takefuji. 2016. Joint learn-
ing of the embedding of words and entities for
named entity disambiguation. In Proc. CoNLL.
https://doi.org/10.18653/v1/K16-1025.

Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and
Tat-Seng Chua. 2017. Visual translation embedding
network for visual relation detection. arXiv preprint
arXiv:1702.08319 .

Hanwang Zhang, Xindi Shang, Wenzhuo Yang, Huan
Xu, Huanbo Luan, and Tat-Seng Chua. 2016. On-
line collaborative learning for open-vocabulary vi-
sual classifiers. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition.
pages 2809–2817.

1633


	Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding

