



















































Neural News Recommendation with Long- and Short-term User Representations


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 336‚Äì345
Florence, Italy, July 28 - August 2, 2019. c¬©2019 Association for Computational Linguistics

336

Neural News Recommendation with Long- and Short-term
User Representations

Mingxiao An1,*, Fangzhao Wu2, Chuhan Wu3, Kun Zhang1, Zheng Liu2, Xing Xie2
1University of Science and Technology of China, Hefei 230026, China

2Microsoft Research Asia, Beijing 100080, China
3Department of Electronic Engineering, Tsinghua University, Beijing 100084, China

{anmx,zhkun}@mail.ustc.edu.cn, wufangzhao@gmail.com
wuch15@mails.tsinghua.edu.cn, {zhengliu,xingx}@microsoft.com

Abstract

Personalized news recommendation is impor-
tant to help users find their interested news and
improve reading experience. A key problem
in news recommendation is learning accurate
user representations to capture their interests.
Users usually have both long-term preferences
and short-term interests. However, existing
news recommendation methods usually learn
single representations of users, which may be
insufficient. In this paper, we propose a neu-
ral news recommendation approach which can
learn both long- and short-term user represen-
tations. The core of our approach is a news
encoder and a user encoder. In the news en-
coder, we learn representations of news from
their titles and topic categories, and use atten-
tion network to select important words. In the
user encoder, we propose to learn long-term
user representations from the embeddings of
their IDs. In addition, we propose to learn
short-term user representations from their re-
cently browsed news via GRU network. Be-
sides, we propose two methods to combine
long-term and short-term user representations.
The first one is using the long-term user repre-
sentation to initialize the hidden state of the
GRU network in short-term user representa-
tion. The second one is concatenating both
long- and short-term user representations as
a unified user vector. Extensive experiments
on a real-world dataset show our approach can
effectively improve the performance of neural
news recommendation.

1 Introduction

Online news platforms such as MSN News1 and
Google News2 which aggregate news from various
sources and distribute them to users have gained

*This work was done when the first author was an intern
in Microsoft Research Asia.

1https://www.msn.com/news
2https://news.google.com/

2017 NBA Championship 
Celebration From Warriors

Rami Malek Wins 
the 2019 Oscar

Oklahoma City Thunder vs. 
Golden State Warriors 

Bohemian Rhapsody Is Highest-
Grossing Musician Biopic Ever 

ùë°ùë°1
ùë°ùë°ùëñùëñ

ùë°ùë°ùëñùëñ+1
ùë°ùë°ùëóùëó

‚Ä¶

‚Ä¶

Figure 1: An illustrative example of long-term and
short-term interests in news reading.

huge popularity and attracted hundreds of millions
of users (Das et al., 2007; Wang et al., 2018).
However, massive news are generated everyday,
making it impossible for users to read through
all news (Lian et al., 2018). Thus, personalized
news recommendation is very important for online
news platforms to help users find their interested
contents and alleviate information overload (Lavie
et al., 2010; Zheng et al., 2018).

Learning accurate user representations is criti-
cal for news recommendation (Okura et al., 2017).
Existing news recommendation methods usually
learn a single representation for each user (Okura
et al., 2017; Lian et al., 2018; Wu et al., 2019).
For example, Okura et al. (2017) proposed to learn
representations of news using denoising autoen-
coder and learn representations of users from their
browsed news using GRU network (Cho et al.,
2014). However, it is very difficult for RNN net-
works such as GRU to capture the entire informa-
tion of very long news browsing history. Wang
et al. (2018) proposed to learn the representa-
tions of news using knowledge-aware convolu-
tional neural network (CNN), and learn the repre-
sentations of users from their browsed news based
on the similarities between the candidate news and
the browsed news. However, this method needs
to store the entire browsing history of each user
in the online news recommendation stage, which
may bring huge challenge to the storage and may
cause heavy latency.



337

Our work is motivated by the observation that
the interests of online users in news are very di-
verse. Some user interests may last for a long
time and are consistent for the same user (Li et al.,
2014). For example, as shown in Fig. 1, if a
user is a fan of ‚ÄúGolden State Warriors‚Äù, this user
may tend to read many basketball news about this
NBA team for several years. We call this kind of
user preferences as long-term interest. In addition,
many user interests may evolve with time and may
be triggered by specific contexts or temporal de-
mands. For example, in Fig. 1, the browsing of the
news on movie ‚ÄúBohemian Rhapsody‚Äù causes the
user reading several related news such as ‚ÄúRami
Malek Wins the 2019 Oscar‚Äù since ‚ÄúRami Malek‚Äù
is an important actor in this movie, although this
user may never read news about ‚ÄúRami Malek‚Äù be-
fore. We call this kind of user interests as short-
term interest. Thus, both long-term and short-
term user interests are important for personalized
news recommendation, and distinguishing long-
term user interests from short-term ones may help
learn more accurate user representations.

In this paper, we propose a neural news rec-
ommendation approach with both long- and short-
term user representations (LSTUR). Our approach
contains two major components, i.e., a news en-
coder and a user encoder. The news encoder is
used to learn representations of news articles from
their titles and topic categories. We apply attention
mechanism to the news encoder to learn informa-
tive news representations by selecting important
words. The user encoder consists of two modules,
i.e., a long-term user representation (LTUR) mod-
ule and a short-term user representation (STUR)
module. In STUR, we use a GRU network to learn
short-term representations of users from their re-
cently browsing news. In LTUR, we learn the
long-term representations of users from the em-
beddings of their IDs. In addition, we propose
two methods to combine the short-term and long-
term user representations. The first one is using
the long-term user representations to initialize the
hidden state of GRU network in the STUR model.
The second one is concatenating the long-tern and
short-term user representations as a unified user
vector. We conducted extensive experiments on a
real-world dataset. The experimental results show
our approach can effectively improve the perfor-
mance of news recommendation and consistently
outperform many baseline methods.

2 Related Works

Personalized news recommendation is an impor-
tant task in natural language processing field and
has wide applications (Zheng et al., 2018). It is
critical for news recommendation methods to learn
accurate news and user representations (Wang
et al., 2018). Many conventional news recommen-
dation methods rely on manual feature engineer-
ing to build news and user representations (Phelan
et al., 2009; Liu et al., 2010; Li et al., 2010; Son
et al., 2013; Li et al., 2014; Bansal et al., 2015;
Lian et al., 2018). For example, Liu et al. (2010)
proposed to use the topic categories and interests
features predicted by a Bayesian model to repre-
sent news, and use the click distribution features of
news categories to represent users. Li et al. (2014)
used a Latent Dirichlet Allocation (LDA) (Blei
et al., 2003) model to generate topic distribution
features as the news representations. They repre-
sented a session by using the topic distribution of
browsed news in this session, and the representa-
tions of users were built from their session repre-
sentations weighted by the time. However, these
methods heavily rely on manual feature engineer-
ing, which needs massive domain knowledge to
craft. In addition, the contexts and orders of words
in news are not incorporated, which are important
for understanding the semantic meanings of news
and learning representations of news and users.

In recent years, several deep learning meth-
ods were proposed for personalized news rec-
ommendation (Wang et al., 2018; Okura et al.,
2017; Zheng et al., 2018). For example, Okura
et al. (2017) proposed to learn representations of
news from news bodies using denoising autoen-
coder, and learn representations of users from
the representations of their browsed news using
a GRU network. Wang et al. (2018) proposed to
learn representations of news from their titles via
a knowledge-aware CNN network, and learn rep-
resentations of users from the representations of
their browsed news articles weighted by their sim-
ilarities with the candidate news. Wu et al. (2019)
proposed to learn news and user representations
with personalized word- and news-level attention
networks, which exploits the embedding of user
ID to generate the query vector for the attentions.
However, these methods usually learn a single rep-
resentation vector for each user, and cannot dis-
tinguish the long-term preferences and short-term
interests of users in reading news. Thus, the user



338

representations learned in these methods may be
insufficient for news recommendation. Different
from these methods, our approach can learn both
long-term and short-term user representations in
a unified framework to capture the diverse inter-
ests of users for personalized neural new commen-
dation. Extensive experiments on the real-world
dataset validate the effectiveness of our approach
and the advantage over many baseline methods.

3 Our Approach

In this section, we present our neural news rec-
ommendation approach with long- and short-term
user representations (LSTUR). Our approach con-
tains two major components, i.e., a news encoder
to learn representations of news and a user encoder
to learn representations of users. Next, we intro-
duce each component in detail.

3.1 News Encoder
The news encoder is used to learn representations
of news from their titles, topic and subtopic cat-
egories. The architecture of the news encoder in
our approach is illustrated in Fig. 2. There are two
sub-modules in the news encoder, i.e., a title en-
coder and a topic encoder.

The title encoder is used to learn news repre-
sentations from titles. There are three layers in the
title encoder. The first layer is word embedding,
which is used to convert a news title from a word
sequence into a sequence of dense semantic vec-
tors. Denote the word sequence in a news title t
as t = [w1, w2, . . . , wN ], where N is the length of
this title. It is transformed into [w1,w2, . . . ,wN ]
via a word embedding matrix.

The second layer in title encoder is a convolu-
tional neural network (CNN) (LeCun et al., 2015).
Local contexts are very useful for understanding
the semantic meaning of news titles. For exam-
ple, in the news title ‚ÄúNext season of super bowl
games‚Äù, the local contexts of ‚Äúbowl‚Äù such as ‚Äúsu-
per‚Äù and ‚Äúgames‚Äù are very important for inferring
that it belongs to a sports event name. Thus, we
apply a CNN network to learn contextual word
representations by capturing the local context in-
formation. Denote the contextual representation
of wi as ci, which is computed as follows:

ci = ReLU(C √ów[i‚àíM :i+M ] + b), (1)

where w[i‚àíM :i+M ] is the concatenation of the em-
beddings of words between position i ‚àí M and

ùíó

ùëé ùëé

Padding Padding

Word Embedding

ùíòùüè ùíòùüê ùíòùëµ ùüè ùíòùëµ

ùíÑùüè ùíÑùüê ùíÑùëµ ùüè ùíÑùëµ

ùë§ ùë§ ùë§ ùë§

ùíÜùíï

ùëé ùëé
Topic 

Embedding
Subtopic

Embedding

News 
Topic

News 
Subtopic

News Title

ùíÜùíóùíÜùíîùíó

‚®Å ‚®Å ùíÜ

Figure 2: The framework of the news encoder.

i +M . C and b are the parameters of the convo-
lutional filters in CNN, and M is the window size.

The third layer is an attention network (Bah-
danau et al., 2015). Different words in the same
news title may have different informativeness for
representing news. For instance, in the news ti-
tle ‚ÄúThe best NBA moments in 2018‚Äù, the word
‚ÄúNBA‚Äù is very informative for representing this
news since it is an important indication of sports
news, while the word ‚Äú2018‚Äù is less informative.
Thus, we employ a word-level attention network
to select important words in news titles to learn
more informative news representations. The at-
tention weight Œ±i of the i-th word is formulated
as follows:

ai = tanh(v √ó ci + vb),

Œ±i =
exp(ai)‚àëN
j=1 exp(aj)

,
(2)

where v and vb are the trainable parameters.
The final representation of a news title t is the
summation of its contextual word representations
weighted by their attention weights as follows:

et =
N‚àë
i=1

Œ±ici. (3)

The topic encoder module is used to learn news
representations from its topics and subtopics. On
many online news platforms such as MSN news,
news articles are usually labeled with a topic cate-
gory (e.g., ‚ÄúSports‚Äù) and a subtopic category (e.g.,
‚ÄúFootball NFL‚Äù) to help target user interests. The
topic and subtopic categories of news are also in-
formative for learning representations of news and
users. They can reveal the general and detailed
topics of the news, and reflect the preferences of
users. For example, if a user browsed many news
articles with the ‚ÄúSports‚Äù topic category, then we



339

User Click History

ùíÜ

GRU

News
Encoder

GRU GRU

‚Ä¶

‚Ä¶

News
Encoder

News
Encoder

ùíñ

ùíÜ ùíÜ

ùíÜùíñ

ùëê ùëê ùëê

ùëê

Dot Product

User 
Embedding

News
Encoder

Candidate 
News

Score

(a) LSTUR-ini.

ùíÜ

GRU

News
Encoder

GRU GRU

‚Ä¶

‚Ä¶

News
Encoder

News
Encoder

ùíñ

ùíÜ ùíÜ

ùíÜùíñ

ùëê ùëê ùëê

ùëê

User Click History

Dot Product

User 
Embedding

News
Encoder

Candidate 
News

Concatenation

Score

‚®Å

(b) LSTUR-con.

Figure 3: The two frameworks of our LSTUR approach.

can infer this user is probably interested in sports,
and it may be effective to recommend candidate
news in the ‚ÄúSports‚Äù topic category to this user.
To incorporate the topic and subtopic information
into news representation, we propose to learn the
representations of topics and subtopics from the
embeddings of their IDs, as shown in Fig. 2. De-
note ev and esv as the representations of topic and
subtopic. The final representation of a news arti-
cle is the concatenation of the representations of
its title, topic and subtopic, i.e., e = [et, ev, esv].

3.2 User Encoder

The user encoder is used to learn representations
of users from the history of their browsed news. It
contains two modules, i.e., a short-term user repre-
sentation model (STUR) to capture user‚Äôs tempo-
ral interests, and a long-term user representation
model (LTUR) to capture user‚Äôs consistent prefer-
ences. Next, we introduce them in detail.

3.2.1 Short-Term User Representation
Online users may have dynamic short-term inter-
ests in reading news articles, which may be influ-
enced by specific contexts or temporal information
demands. For example, if a user just reads a news
article about ‚ÄúMission: Impossible 6 ‚Äì Fallout‚Äù,
and she may want to know more about the actor
‚ÄúTom Cruise‚Äù in this movie and click news arti-
cles related to ‚ÄúTom Cruise‚Äù, although she is not
his fan and may never read his news before. We
propose to learn the short-term representations of
users from their recent browsing history to capture
their temporal interests, and use gated recurrent
networks (GRU) (Cho et al., 2014) network to cap-
ture the sequential news reading patterns (Okura
et al., 2017). Denote news browsing sequence
from a user sorted by timestamp in ascending or-

der as C = {c1, c2, . . . , ck}, where k is the length
of this sequence. We apply the news encoder to
obtain the representations of these browsed arti-
cles, denoted as {e1, e2, . . . , ek}. The short-term
user representation is computed as follows:

rt = œÉ(W r[ht‚àí1, et]),

zt = œÉ(W z[ht‚àí1, et]),

hÃÉt = tanh(W hÃÉ[rt ÔøΩ ht‚àí1, et]),
ht = zt ÔøΩ ht + (1‚àí zt)ÔøΩ hÃÉt,

(4)

where œÉ is the sigmoid function, ÔøΩ is the item-
wise product, W r, W z and W hÃÉ are the param-
eters of the GRU network. The short-term user
representation is the last hidden state of the GRU
network, i.e., us = hk.

3.2.2 Long-Term User Representations
Besides the temporal interests, online users may
also have long-term interests in reading news. For
example, a basketball fan may tend to browse
many sports news related to NBA in several years.
Thus, we propose to learn long-term representa-
tions of users to capture their consistent prefer-
ences. In our approach the long-term user repre-
sentations are learned from the embeddings of the
user IDs, which are randomly initialized and fine-
tuned during model training. Denote u as the ID of
a user and W u as the look-up table for long-term
user representation, the long-term user representa-
tion of this user is ul = W u[u].

3.2.3 Long- and Short-Term User
Representation

In this section, we introduce two methods to com-
bine the long-term and short-term user presenta-
tions for unified user representation, which are
shown in Fig. 3.



340

The first method is using the long-term user
representation to initialize the hidden state of the
GRU network in the short-term user representation
model, as shown in Fig. 3a. We denote this method
as LSTUR-ini. We use the last hidden state of the
GRU network as the final user representation. The
second method is concatenating the long-term user
representation with the short-term user represen-
tation as the final user representation, as shown in
Fig. 3b. We denote this method as LSTUR-con.

3.3 Model Training

For online news recommendation services where
user and news representations can be computed in
advance, the scoring function should be as simple
as possible to reduce latency. Motivated by (Okura
et al., 2017), we use the simple dot production to
compute the news click probability score. Denote
the representation of a user u as u and the repre-
sentation of a candidate news article ex as ex, the
probability score s(u, cx) of this user clicking this
news is computed as s(u, cx) = u>ex.

Motivated by (Huang et al., 2013) and (Zhai
et al., 2016), we propose to use the negative sam-
pling technique for model training. For each news
browsed by a user (regarded as a positive sam-
ple), we randomly sample K news articles from
the same impression which are not clicked by this
user as negative samples. Our model will jointly
predict the click probability scores of the positive
news and the K negative news. In this way, the
news click prediction problem is reformulated as
a pseudo K + 1-way classification task. We mini-
mize the summation of the negative log-likelihood
of all positive samples during training, which can
be formulated as follows:

‚àí
P‚àë
i=1

log
exp(s(u, cpi ))

exp(s(u, cpi )) +
‚àëK

k=1 exp(s(u, c
n
i,k))

,

(5)

where P is the number of positive training sam-
ples, and cni,k is the k-th negative sample in the
same session with the i-th positive sample.

Since not all users can be incorporated in news
recommendation model training (e.g., the new
coming users), it is not appropriate to assume all
users have long-term representations in our mod-
els in the prediction stage. In order to handle this
problem, in the model training stage, we randomly
mask the long-term representations of users with

a certain probability p. When we mask the long-
term representations, all the dimensions are set to
zero. Thus, the long-term user representation in
our LSTUR approach can be reformulated as:

ul =M ¬∑W u[u],M ‚àº B(1, 1‚àí p), (6)

where B is Bernoulli distribution, and M is a ran-
dom variable that subject toB(1, 1‚àíp). We find in
experiments that this trick for model training can
improve the performance of our approach.

4 Experiments

4.1 Dataset and Experimental Settings

Since there is no off-the-shelf dataset for news rec-
ommendation, we built one by ourselves through
collecting logs from MSN News3 in four weeks
from December 23rd, 2018 to January 19th, 2019.
We used the logs in the first three weeks for model
training, and those in the last week for test. We
also randomly sampled 10% of logs from the train-
ing set as the validation data. For each sample,
we collected the browsing history in last 7 days to
learn short-term user representations. The detailed
dataset statistics are summarized in Table 1.

# of users 25,000 # of users in training set 22,938
# of news 38,501 Avg. # of words per title 9.98
# of imprs 393,191 # of positive samples 492,185
NP ratio4 18.74 # of negative samples 9,224,537

Table 1: Statistics of the dataset in our experiments.

In our experiments, we used the pretrained
GloVe embedding5 (Pennington et al., 2014) as the
initialization of word embeddings. The word em-
bedding dimension is 200. The number of filters
in CNN network is 300, and the window size of
the filters in CNN network is set to 3. We applied
dropout (Srivastava et al., 2014) to each layer in
our approach to mitigate overfitting. The dropout
rate is 0.2. The default value of long-term user rep-
resentation masking probability p for model train-
ing is 0.5. We used Adam (Kingma and Ba, 2014)
to optimize the model, and the learning rate was
0.01. The batch size is set to 400. The number
of negative samples for each positive sample is
4. These hyper-parameters were all selected ac-
cording to the results on validation set. We used

3https://www.msn.com/en-us/news
4The ratio of the negative sample number to the positive

sample number.
5http://nlp.stanford.edu/data/glove.840B.300d.zip



341

impression-based ranking metrics to evaluate the
performance, including area under the ROC curve
(AUC), mean reciprocal rank (MRR), and nor-
malized discounted cumulative gain (nDCG). We
repeated each experiment for 10 times indepen-
dently, and reported the average results with 0.95
confidence probability.

4.2 Performance Evaluation
We evaluate the performance of our approach by
comparing it with several baseline methods, in-
cluding:

‚Ä¢ LibFM (Rendle, 2012), a state-of-the-art ma-
trix factorization method which is widely
used in recommendation. In our experiments,
the user features are the concatenation of
TF-IDF features extracted from the browsed
news titles, and the normalized count features
from the topics and subtopics of the browsed
news. The features for news consists of TF-
IDF features from its title, and one-hot vec-
tors of its topic and subtopic. The input to
LibFM is the concatenation of user features
and features of candidate news.

‚Ä¢ DeepFM (Guo et al., 2017), a widely used
method that combines factorization machines
and deep neural networks. We use the same
features as LibFM.

‚Ä¢ Wide & Deep (Cheng et al., 2016), another
deep learning based recommendation method
that combines a wide channel and a deep
channel. Again, the same features with
LibFM are used for both channels.

‚Ä¢ DSSM (Huang et al., 2013), deep structured
semantic model. The inputs are hashed words
via character trigram, where all the browsed
news titles are merged as query document.

‚Ä¢ CNN (Kim, 2014), using CNN with max
pooling to learn news representations from
the titles of browsed news by keeping the
most salient features.

‚Ä¢ DKN (Wang et al., 2018), a deep news rec-
ommendation model which contains CNN
and candidate-aware attention on the news
browsing histories.

‚Ä¢ GRU (Okura et al., 2017), learning news rep-
resentations by a denoising autoencoder and
user representations by a GRU network.

The results of comparing different methods are
summarized in Table 2.

We have obtained observations from Table 2.
First, the news recommendation methods (e.g.
CNN, DKN and LSTUR) which use neural net-
works to learn news and user representations can
significantly outperform the methods using man-
ual feature engineering (e.g. LibFM, DeepFM,
Wide & Deep, and DSSM). This is probably be-
cause handcrafted features are usually not optimal,
and neural networks can capture both global and
local semantic contexts in news, which are useful
for learning more accurate news and user repre-
sentations for news recommendation.

Second, our LSTUR approach outperforms all
baseline methods compared here, including deep
learning models such as CNN, GRU and DKN. Our
LSTUR approach can capture both the long-term
preferences and short-term interests to capture the
complex and diverse user interests in news read-
ing, while the baseline methods only learn a single
representation for each user, which is insufficient.
In addition, our LSTUR approach uses attention
mechanism in the news encoder to select impor-
tant words, which can help learn more informative
news representations.

Third, our proposed two methods to learn long-
and short-term user representations, i.e., LSTUR-
ini and LSTUR-con, can achieve comparable per-
formance and both outperform baseline methods,
which validate the effectiveness of these meth-
ods. In addtion, the performance of LSTUR-con
is more stable than LSTUR-ini, which indicates
that using the concatenation of both short-term
and long-term user representations is capable of
retaining all the information. We also conducted
experiments to explore the performance of com-
bining both LSTUR-con and LSTUR-ini in the
same model, but the performance improvement is
very limited, implying that each of them can fully
capture the long- and short-term user interests for
news recommendation.

4.3 Effectiveness of Long- and Short-Term
User Representation

In this section, we conducted several experiments
to explore the effectiveness of our approach in
learning both long-term and short-term user rep-
resentations. We compare the performance of our
LSTUR methods with the long-term user represen-
tation model LTUR and the short-term user rep-



342

Methods AUC MRR nDCG@5 nDCG@10
LibFM 56.52 ¬± 1.31 25.53 ¬± 0.81 26.66 ¬± 1.04 34.72 ¬± 0.95

DeepFM 58.13 ¬± 1.69 27.01 ¬± 0.20 28.37 ¬± 0.57 36.78 ¬± 0.62
Wide & Deep 58.07 ¬± 0.55 27.07 ¬± 0.37 28.51 ¬± 0.45 36.93 ¬± 0.43

DSSM 58.43 ¬± 0.58 27.25 ¬± 0.49 28.31 ¬± 0.60 36.91 ¬± 0.54
CNN 61.13 ¬± 0.77 29.44 ¬± 0.73 31.44 ¬± 0.87 39.51 ¬± 0.74
DKN 61.25 ¬± 0.78 29.47 ¬± 0.64 31.54 ¬± 0.79 39.59 ¬± 0.67
GRU 62.69 ¬± 0.16 30.24 ¬± 0.13 32.56 ¬± 0.17 40.55 ¬± 0.13

LSTUR-con 63.47 ¬± 0.10 30.94 ¬± 0.14 33.43 ¬± 0.13 41.34 ¬± 0.13
LSTUR-ini 63.56 ¬± 0.42 30.98 ¬± 0.32 33.45 ¬± 0.39 41.37 ¬± 0.36

Table 2: The performance of different methods on news recommendation.

Figure 4: The effectiveness of incorporating long-tern
user representations (LTUR) and short-term user rep-
resentations (STUR).

Figure 5: The comparisons of different methods in
learning short-term user representations from recently
browsed news articles.

resentation model STUR. The results are summa-
rized in Fig. 4.

From the results we find both LTUR and STUR
are useful for news recommendation, and the
STUR model can outperform the LTUR model.
According to the statistics in Table 1, the long-
term representations of many users in test data
are unavailable, which leads to relative weak per-
formance of LTUR on these users. In addition,
combining STUR and LTUR using our two long-
and short-term user representation methods, i.e.,
LSTUR-ini and LSTUR-con, can effectively im-
prove the performance. This result validates that
incorporating both long-term and short-term user
representations is useful to capture the diverse
user interests more accurately and is beneficial for
news recommendation.

4.4 Effectiveness of News Encoders in STUR

In our STUR model, GRU is used to learn short-
term user representations from the recent browsing
news. We explore the effectiveness of GRU in en-
coding news by replacing it with several other en-
coders, including: 1) Average: using the average
of all the news representations in recent browsing

history; 2) Attention: the summation of news rep-
resentations weighted by their attention weights;
3) LSTM (Hochreiter and Schmidhuber, 1997), re-
placing GRU with LSTM. The results are summa-
rized in Fig. 5.

According to Fig. 5, the sequence-based en-
coders (e.g., GRU, LSTM) outperform the Aver-
age and Attention based encoders. This is proba-
bly because the sequence-based encoders can cap-
ture the sequential new reading patterns to learn
short-term representations of users, which is diffi-
cult for Average and Attention based encoders. In
addition, GRU achieves better performance than
LSTM. This may be because GRU contains fewer
parameters and has lower risk of overfitting . Thus,
we select GRU as the news encoder in STUR.

4.5 Effectiveness of News Title Encoders

In this section, we conduct experiments to com-
pare different news title encoders. In our ap-
proach, the news encoder is a combination of
CNN network and an attention network (denoted
as CNN+Att). We compare it with several vari-
ants, i.e., CNN, LSTM, and LSTM with attention
(LSTM+Att), to validate the effectiveness of our



343

--
(a) AUC

--
(b) nDCG@10

Figure 6: The comparisons of different methods in learning news title representations and the effectiveness of
attention machenism in selecting important words.

--
(a) AUC

--
(b) nDCG@10

Figure 7: The effectiveness of incorporating news topic and subtopic information for news recommendation.

approach. The results are summarized in Fig. 6.
According to Fig. 6, using attention mechanism

in both encoders based on CNN and LSTM can
achieve better performance. This is probably be-
cause the attention network can select important
words, which can learn more informative news
representations. In addition, encoders using CNN
outperform those using LSTM. This may be be-
cause local contexts in news titles are more impor-
tant for learning news representations.

4.5.1 Effectiveness of News Topic
In this section, we conduct experiments to vali-
date the effectiveness of incorporating topic and
subtopic of news in the news encoder. We com-
pare the performance of our approach with its vari-
ants without topic and/or subtopics. The results
are shown in Fig. 7.

According to Fig. 7, incorporating either top-
ics or subtopics can effectively improve the per-
formance of our approach. In addition, the news
encoder with subtopics outperforms the news en-
coder with topics. This is probably because
subtopics can provide more fine-grained topic in-

formation which is more helpful for news rec-
ommendation. Thus, the model with subtopics
can achieve better news recommendation perfor-
mance. Moreover, combining topics and subtopics
can further improve the performance of our ap-
proach. These results validate the effectiveness of
our approach in exploiting topic information for
news recommendation.

4.5.2 Influence of Masking Probability
In this section, we explore the influence of the
probability p in Eq. (6) for randomly masking
long-term user representation in model training.
We vary the value of p from 0.0 to 0.9 with a step
of 0.1 for both LSTUR-ini and LSTUR-con. The
results are summarized in Fig. 8.

According to Fig. 8, the results of LSTUR-ini
and LSTUR-con have similar patterns. The perfor-
mance of both methods improves when p increases
from 0. When p is too small, the model will tend
to overfit on the LTUR, since LTUR has many pa-
rameters. Thus, the performance is not optimal.
However, when p is too large, the performance of
both methods starts to decline. This may be be-



344

0.0 0.3 0.6 0.9

63.0

64.0

AUC MRR nDCG@5 nDCG@10

40.5

41.5

0.0 0.3 0.6 0.9

30.0

31.0

32.0

33.0

34.0

Mask probability p

(a) LSTUR-ini.

0.0 0.3 0.6 0.9

63.0

64.0

AUC MRR nDCG@5 nDCG@10

40.5

41.5

0.0 0.3 0.6 0.9

30.0

31.0

32.0

33.0

34.0

Mask probability p

(b) LSTUR-con.

Figure 8: The influence of mask probability p on the performance of our approach.

2019 CES Highlights : Innovations in Enviro-Sensing for Robocars
California dries off after storm batter state for days
15 Recipes Inspired By Vintage Movies
Texas State Rep . Dennis Bonnen Elected As House Speaker
Should You Buy American Express Stock After Earnings ?
How Meghan Markle Has Changed Prince Harry Considerably

Figure 9: Visualization of the word-level attentions.

cause the useful information in LTUR cannot be
effectively incorporated. Thus, the performance is
also not optimal. A moderate choice on p (e.g.,
0.5) is most appropriate for both LSTUR-ini and
LSTUR-con methods, which can properly balance
the learning of LTUR and STUR.

5 Visualization of Attention Weights

In this section, we visually explore the effective-
ness of the word-level attention network in the
news encoder. The attention weights in several
example news titles are shown in Fig. 9. From
the results, we find our approach can effectively
recognize important words to learn more infor-
mative news representations. For example, the
words ‚ÄúEnviro-Sensing‚Äù and ‚ÄúRobocars‚Äù in the
first news title are assigned high attention weights
because these words are indications of news on
technologies, while the words ‚Äú2019‚Äù and ‚Äúfor‚Äù
are assigned low attention weights by our ap-
proach since they are less informative. These re-
sults validate the effectiveness of the attention net-
work in the news encoder.

6 Conclusion

In this paper, we propose a neural news recom-
mendation approach which can learn both long-
and short-term user representations. The core of
our model is a news encoder and a user encoder.
In the news encoder, we learn representations of
news from their titles and topic categories, and use
an attention network to highlight important words
for informative representation learning. In the user
encoder, we propose to learn long-term represen-
tations of users from the embeddings of their IDs.
In addition, we learn short-term representations of
users from their recently browsed news via a GRU
network. Besides, we propose two methods to fuse
long- and short-term user representations, i.e., us-
ing long-term user representation to initialize the
hidden state of the GRU network in short-term
user representation, or concatenating both long-
and short-term user representations as a unified
user vector. Extensive experiments on a real-world
dataset collected from MSN news show our ap-
proach can effecitively improve the performance
of news recommendation.

Acknowledgement

The authors would like to thank Microsoft News
for providing technical support and data in the ex-
periments, and Jiun-Hung Chen (Microsoft News)
and Ying Qiao (Microsoft News) for their support
and discussions. We also want to thank Jianqiang
Huang for his help in the experiments.



345

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Trapit Bansal, Mrinal Das, and Chiranjib Bhat-
tacharyya. 2015. Content driven user profiling
for comment-worthy recommendations of news and
blog articles. In RecSys, pages 195‚Äì202.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3(Jan):993‚Äì1022.

Heng-Tze Cheng, Mustafa Ispir, Rohan Anil, Zakaria
Haque, Lichan Hong, Vihan Jain, Xiaobing Liu,
Hemal Shah, Levent Koc, Jeremiah Harmsen, Tal
Shaked, Tushar Chandra, Hrishi Aradhye, Glen An-
derson, Greg Corrado, and Wei Chai. 2016. Wide &
deep learning for recommender systems. In DLRS,
pages 7‚Äì10.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder‚Äìdecoder
for statistical machine translation. In EMNLP, pages
1724‚Äì1734.

Abhinandan S. Das, Mayur Datar, Ashutosh Garg,
and Shyam Rajaram. 2007. Google news person-
alization: scalable online collaborative filtering. In
WWW, pages 271‚Äì280.

Huifeng Guo, Ruiming TANG, Yunming Ye, Zhen-
guo Li, and Xiuqiang He. 2017. DeepFM:
A factorization-machine based neural network for
CTR prediction. In IJCAI, pages 1725‚Äì1731.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735‚Äì1780.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In CIKM, pages 2333‚Äì2338.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP, pages 1746‚Äì
1751.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Talia Lavie, Michal Sela, Ilit Oppenheim, Ohad Inbar,
and Joachim Meyer. 2010. User attitudes towards
news content personalization. International Journal
of Human-Computer Studies, 68(8):483‚Äì495.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature, 521(7553):436‚Äì444.

Lei Li, Li Zheng, Fan Yang, and Tao Li. 2014. Model-
ing and broadening temporal user interest in person-
alized news recommendation. Expert Systems with
Applications, 41(7):3168‚Äì3177.

Lihong Li, Wei Chu, John Langford, and Robert E.
Schapire. 2010. A contextual-bandit approach to
personalized news article recommendation. In
WWW, pages 661‚Äì670.

Jianxun Lian, Fuzheng Zhang, Xing Xie, and
Guangzhong Sun. 2018. Towards better represen-
tation learning for personalized news recommenda-
tion: a multi-channel deep fusion approach. In IJ-
CAI, pages 3805‚Äì3811.

Jiahui Liu, Peter Dolan, and Elin R√∏nby Pedersen.
2010. Personalized news recommendation based on
click behavior. In IUI, pages 31‚Äì40.

Shumpei Okura, Yukihiro Tagami, Shingo Ono, and
Akira Tajima. 2017. Embedding-based news rec-
ommendation for millions of users. In KDD, pages
1933‚Äì1942.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532‚Äì1543.

Owen Phelan, Kevin McCarthy, and Barry Smyth.
2009. Using twitter to recommend real-time topical
news. In RecSys, pages 385‚Äì388.

Steffen Rendle. 2012. Factorization machines with
libFM. ACM Transactions on Intelligent Systems
and Technology, 3(3):1‚Äì22.

Jeong-Woo Son, A-Yeong Kim, and Seong-Bae Park.
2013. A location-based news article recommenda-
tion with explicit localized semantic analysis. In SI-
GIR, pages 293‚Äì302.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research, 15(1):1929‚Äì1958.

Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi
Guo. 2018. DKN: Deep knowledge-aware network
for news recommendation. In WWW, pages 1835‚Äì
1844.

Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang
Huang, Yongfeng Huang, and Xing Xie. 2019.
NPA: Neural news recommendation with personal-
ized attention. In KDD.

Shuangfei Zhai, Keng hao Chang, Ruofei Zhang, and
Zhongfei Mark Zhang. 2016. Deepintent: Learning
attentions for online advertising with recurrent neu-
ral networks. In KDD, pages 1295‚Äì1304.

Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang
Xiang, Nicholas Jing Yuan, Xing Xie, and Zhen-
hui Li. 2018. DRN: A deep reinforcement learn-
ing framework for news recommendation. In WWW,
pages 167‚Äì176.

https://arxiv.org/pdf/1409.0473.pdf
https://arxiv.org/pdf/1409.0473.pdf
https://doi.org/10.1145/2792838.2800186
https://doi.org/10.1145/2792838.2800186
https://doi.org/10.1145/2792838.2800186
http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf
https://doi.org/10.1145/2988450.2988454
https://doi.org/10.1145/2988450.2988454
https://doi.org/10.3115/v1/d14-1179
https://doi.org/10.3115/v1/d14-1179
https://doi.org/10.3115/v1/d14-1179
https://doi.org/10.1145/1242572.1242610
https://doi.org/10.1145/1242572.1242610
https://doi.org/10.24963/ijcai.2017/239
https://doi.org/10.24963/ijcai.2017/239
https://doi.org/10.24963/ijcai.2017/239
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1145/2505515.2505665
https://doi.org/10.1145/2505515.2505665
https://doi.org/10.1145/2505515.2505665
https://doi.org/10.3115/v1/d14-1181
https://doi.org/10.3115/v1/d14-1181
https://arxiv.org/abs/1412.6980v9
https://arxiv.org/abs/1412.6980v9
https://doi.org/10.1016/j.ijhcs.2009.09.011
https://doi.org/10.1016/j.ijhcs.2009.09.011
https://doi.org/10.1038/nature14539
https://doi.org/10.1016/j.eswa.2013.11.020
https://doi.org/10.1016/j.eswa.2013.11.020
https://doi.org/10.1016/j.eswa.2013.11.020
https://doi.org/10.1145/1772690.1772758
https://doi.org/10.1145/1772690.1772758
https://doi.org/10.24963/ijcai.2018/529
https://doi.org/10.24963/ijcai.2018/529
https://doi.org/10.24963/ijcai.2018/529
https://doi.org/10.1145/1719970.1719976
https://doi.org/10.1145/1719970.1719976
https://doi.org/10.1145/3097983.3098108
https://doi.org/10.1145/3097983.3098108
https://doi.org/10.3115/v1/d14-1162
https://doi.org/10.3115/v1/d14-1162
https://doi.org/10.1145/1639714.1639794
https://doi.org/10.1145/1639714.1639794
https://doi.org/10.1145/2168752.2168771
https://doi.org/10.1145/2168752.2168771
https://doi.org/10.1145/2484028.2484064
https://doi.org/10.1145/2484028.2484064
http://dl.acm.org/citation.cfm?id=2670313
http://dl.acm.org/citation.cfm?id=2670313
https://doi.org/10.1145/3178876.3186175
https://doi.org/10.1145/3178876.3186175
https://doi.org/10.1145/2939672.2939759
https://doi.org/10.1145/2939672.2939759
https://doi.org/10.1145/2939672.2939759
https://doi.org/10.1145/3178876.3185994
https://doi.org/10.1145/3178876.3185994

