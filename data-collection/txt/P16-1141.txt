



















































Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1489–1501,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Diachronic Word Embeddings Reveal Statistical Laws of
Semantic Change

William L. Hamilton, Jure Leskovec, Dan Jurafsky
Department of Computer Science, Stanford University, Stanford CA, 94305

wleif,jure,jurafsky@stanford.edu

Abstract

Understanding how words change their
meanings over time is key to models of
language and cultural evolution, but his-
torical data on meaning is scarce, mak-
ing theories hard to develop and test.
Word embeddings show promise as a di-
achronic tool, but have not been carefully
evaluated. We develop a robust method-
ology for quantifying semantic change
by evaluating word embeddings (PPMI,
SVD, word2vec) against known historical
changes. We then use this methodology
to reveal statistical laws of semantic evo-
lution. Using six historical corpora span-
ning four languages and two centuries, we
propose two quantitative laws of seman-
tic change: (i) the law of conformity—the
rate of semantic change scales with an in-
verse power-law of word frequency; (ii)
the law of innovation—independent of fre-
quency, words that are more polysemous
have higher rates of semantic change.

1 Introduction

Shifts in word meaning exhibit systematic regu-
larities (Bréal, 1897; Ullmann, 1962). The rate
of semantic change, for example, is higher in
some words than others (Blank, 1999) — com-
pare the stable semantic history of cat (from Proto-
Germanic kattuz, “cat”) to the varied meanings of
English cast: “to mould”, “a collection of actors’,
“a hardened bandage”, etc. (all from Old Norse
kasta, “to throw”, Simpson et al., 1989).

Various hypotheses have been offered about
such regularities in semantic change, such as an in-
creasing subjectification of meaning, or the gram-
maticalization of inferences (e.g., Geeraerts, 1997;
Blank, 1999; Traugott and Dasher, 2001).

But many core questions about semantic change
remain unanswered. One is the role of fre-
quency. Frequency plays a key role in other lin-
guistic changes, associated sometimes with faster
change—sound changes like lenition occur in
more frequent words—and sometimes with slower
change—high frequency words are more resistant
to morphological regularization (Bybee, 2007;
Pagel et al., 2007; Lieberman et al., 2007). What
is the role of word frequency in meaning change?

Another unanswered question is the relationship
between semantic change and polysemy. Words
gain senses over time as they semantically drift
(Bréal, 1897; Wilkins, 1993; Hopper and Trau-
gott, 2003), and polysemous words1 occur in
more diverse contexts, affecting lexical access
speed (Adelman et al., 2006) and rates of L2
learning (Crossley et al., 2010). But we don’t
know whether the diverse contextual use of pol-
ysemous words makes them more or less likely
to undergo change (Geeraerts, 1997; Winter et
al., 2014; Xu et al., 2015). Furthermore, poly-
semy is strongly correlated with frequency—high
frequency words have more senses (Zipf, 1945;
İlgen and Karaoglan, 2007)—so understanding
how polysemy relates to semantic change requires
controling for word frequency.

Answering these questions requires new meth-
ods that can go beyond the case-studies of a few
words (often followed over widely different time-
periods) that are our most common diachronic
data (Bréal, 1897; Ullmann, 1962; Blank, 1999;
Hopper and Traugott, 2003; Traugott and Dasher,
2001). One promising avenue is the use of distri-
butional semantics, in which words are embedded
in vector spaces according to their co-occurrence
relationships (Bullinaria and Levy, 2007; Turney
and Pantel, 2010), and the embeddings of words

1We use ‘polysemy’ here to refer to related senses as well
as rarer cases of accidental homonymy.

1489



Figure 1: Two-dimensional visualization of semantic change in English using SGNS vectors.2 a, The word gay shifted from
meaning “cheerful” or “frolicsome” to referring to homosexuality. b, In the early 20th century broadcast referred to “casting
out seeds”; with the rise of television and radio its meaning shifted to “transmitting signals”. c, Awful underwent a process of
pejoration, as it shifted from meaning “full of awe” to meaning “terrible or appalling” (Simpson et al., 1989).

are then compared across time-periods. This new
direction has been effectively demonstrated in a
number of case-studies (Sagi et al., 2011; Wijaya
and Yeniterzi, 2011; Gulordava and Baroni, 2011;
Jatowt and Duh, 2014) and used to perform large-
scale linguistic change-point detection (Kulkarni
et al., 2014) as well as to test a few specific hy-
potheses, such as whether English synonyms tend
to change meaning in similar ways (Xu and Kemp,
2015). However, these works employ widely dif-
ferent embedding approaches and test their ap-
proaches only on English.

In this work, we develop a robust methodol-
ogy for quantifying semantic change using embed-
dings by comparing state-of-the-art approaches
(PPMI, SVD, word2vec) on novel benchmarks.

We then apply this methodology in a large-scale
cross-linguistic analysis using 6 corpora spanning
200 years and 4 languages (English, German,
French, and Chinese). Based on this analysis, we
propose two statistical laws relating frequency and
polysemy to semantic change:
• The law of conformity: Rates of semantic

change scale with a negative power of word
frequency.
• The law of innovation: After controlling for

frequency, polysemous words have signifi-
cantly higher rates of semantic change.

2 Diachronic embedding methods

The following sections outline how we construct
diachronic (historical) word embeddings, by first
constructing embeddings in each time-period and
then aligning them over time, and the metrics that

2Appendix B details the visualization method.

we use to quantify semantic change. All of the
learned embeddings and the code we used to ana-
lyze them are made publicly available.3

2.1 Embedding algorithms

We use three methods to construct word em-
beddings within each time-period: PPMI, SVD,
and SGNS (i.e., word2vec).4 These distributional
methods represent each word wi by a vector wi
that captures information about its co-occurrence
statistics. These methods operationalize the ‘dis-
tributional hypothesis’ that word semantics are im-
plicit in co-occurrence relationships (Harris, 1954;
Firth, 1957). The semantic similarity/distance be-
tween two words is approximated by the cosine
similarity/distance between their vectors (Turney
and Pantel, 2010).

2.1.1 PPMI
In the PPMI representations, the vector embedding
for word wi ∈ V contains the positive point-wise
mutual information (PPMI) values betweenwi and
a large set of pre-specified ‘context’ words. The
word vectors correspond to the rows of the matrix
MPPMI ∈ R|V|×|VC | with entries given by

MPPMIi,j = max
{

log
(
p̂(wi, cj)
p̂(w)p̂(cj)

)
− α, 0

}
,

(1)
where cj ∈ VC is a context word and α > 0
is a negative prior, which provides a smooth-
ing bias (Levy et al., 2015). The p̂ correspond
to the smoothed empirical probabilities of word

3
http://nlp.stanford.edu/projects/histwords

4Synchronic applications of these three methods are re-
viewed in detail in Levy et al. (2015).

1490



Name Language Description Tokens Years POS Source

ENGALL English Google books (all genres) 8.5× 1011 1800-1999 (Davies, 2010)
ENGFIC English Fiction from Google books 7.5× 1010 1800-1999 (Davies, 2010)
COHA English Genre-balanced sample 4.1× 108 1810-2009 (Davies, 2010)
FREALL French Google books (all genres) 1.9× 1011 1800-1999 (Sagot et al., 2006)
GERALL German Google books (all genres) 4.3× 1010 1800-1999 (Schneider and Volk, 1998)
CHIALL Chinese Google books (all genres) 6.0× 1010 1950-1999 (Xue et al., 2005)

Table 1: Six large historical datasets from various languages and sources are used.

(co-)occurrences within fixed-size sliding win-
dows of text. Clipping the PPMI values above zero
ensures they remain finite and has been shown to
dramatically improve results (Bullinaria and Levy,
2007; Levy et al., 2015); intuitively, this clipping
ensures that the representations emphasize posi-
tive word-word correlations over negative ones.

2.1.2 SVD
SVD embeddings correspond to low-dimensional
approximations of the PPMI embeddings learned
via singular value decomposition (Levy et al.,
2015). The vector embedding for word wi is given
by

wSVDi = (UΣ
γ)i , (2)

where MPPMI = UΣV> is the truncated singular
value decomposition of MPPMI and γ ∈ [0, 1] is
an eigenvalue weighting parameter. Setting γ < 1
has been shown to dramatically improve embed-
ding qualities (Turney and Pantel, 2010; Bulli-
naria and Levy, 2012). This SVD approach can
be viewed as a generalization of Latent Seman-
tic Analysis (Landauer and Dumais, 1997), where
the term-document matrix is replaced with MPPMI.
Compared to PPMI, SVD representations can be
more robust, as the dimensionality reduction acts
as a form of regularization.

2.1.3 Skip-gram with negative sampling
SGNS ‘neural’ embeddings are optimized to pre-
dict co-occurrence relationships using an approx-
imate objective known as ‘skip-gram with nega-
tive sampling’ (Mikolov et al., 2013). In SGNS,
each word wi is represented by two dense, low-
dimensional vectors: a word vector (wSGNSi ) and
context vector (cSGNSi ). These embeddings are op-
timized via stochastic gradient descent so that

p̂(ci|wi) ∝ exp(wSGNSi · cSGNSj ), (3)
where p(ci|wi) is the empirical probability of see-
ing context word ci within a fixed-length window
of text, given that this window contains wi. The

SGNS optimization avoids computing the normal-
izing constant in (3) by randomly drawing ‘neg-
ative’ context words, cn, for each target word and
ensuring that exp(wSGNSi ·cSGNSn ) is small for these
examples.

SGNS has the benefit of allowing incremental
initialization during learning, where the embed-
dings for time t are initialized with the embed-
dings from time t − ∆ (Kim et al., 2014). We
employ this trick here, though we found that it had
a negligible impact on our results.

2.2 Datasets, pre-processing, and
hyperparameters

We trained models on the 6 datasets described
in Table 1, taken from Google N-Grams (Lin et
al., 2012) and the COHA corpus (Davies, 2010).
The Google N-Gram datasets are extremely large
(comprising≈6% of all books ever published), but
they also contain many corpus artifacts due, e.g.,
to shifting sampling biases over time (Pechenick
et al., 2015). In contrast, the COHA corpus was
carefully selected to be genre-balanced and rep-
resentative of American English over the last 200
years, though as a result it is two orders of mag-
nitude smaller. The COHA corpus also contains
pre-extracted word lemmas, which we used to val-
idate that our results hold at both the lemma and
raw token levels. All the datasets were aggregated
to the granularity of decades.5

We follow the recommendations of Levy et al.
(2015) in setting the hyperparameters for the em-
bedding methods, though preliminary experiments
were used to tune key settings. For all methods,
we used symmetric context windows of size 4 (on
each side). For SGNS and SVD, we use embed-
dings of size 300. See Appendix A for further im-
plementation and pre-processing details.

5The 2000s decade of the Google data was discarded due
to shifts in the sampling methodology (Michel et al., 2011).

1491



2.3 Aligning historical embeddings
In order to compare word vectors from differ-
ent time-periods we must ensure that the vectors
are aligned to the same coordinate axes. Ex-
plicit PPMI vectors are naturally aligned, as each
column simply corresponds to a context word.
Low-dimensional embeddings will not be natu-
rally aligned due to the non-unique nature of the
SVD and the stochastic nature of SGNS. In par-
ticular, both these methods may result in arbi-
trary orthogonal transformations, which do not af-
fect pairwise cosine-similarities within-years but
will preclude comparison of the same word across
time. Previous work circumvented this problem
by either avoiding low-dimensional embeddings
(e.g., Gulordava and Baroni, 2011; Jatowt and
Duh, 2014) or by performing heuristic local align-
ments per word (Kulkarni et al., 2014).

We use orthogonal Procrustes to align the
learned low-dimensional embeddings. Defining
W(t) ∈ Rd×|V| as the matrix of word embeddings
learned at year t, we align across time-periods
while preserving cosine similarities by optimizing:

R(t) = arg min
Q>Q=I

‖W(t)Q−W(t+1)‖F , (4)

with R(t) ∈ Rd×d. The solution corresponds
to the best rotational alignment and can be ob-
tained efficiently using an application of SVD
(Schönemann, 1966).

2.4 Time-series from historical embeddings
Diachronic word embeddings can be used in two
ways to quantify semantic change: (i) we can mea-
sure changes in pair-wise word similarities over
time, or (ii) we can measure how an individual
word’s embedding shifts over time.

Pair-wise similarity time-series Measuring
how the cosine-similarity between pairs of words
changes over time allows us to test hypotheses
about specific linguistic or cultural shifts in a con-
trolled manner. We quantify shifts by computing
the similarity time-series

s(t)(wi, wj) = cos-sim(w
(t)
i ,w

(t)
j ) (5)

between two words wi and wj over a time-period
(t, ..., t + ∆). We then measure the Spearman
correlation (ρ) of this series against time, which
allows us to assess the magnitude and signifi-
cance of pairwise similarity shifts; since the Spear-
man correlation is non-parametric, this measure

essentially detects whether the similarity series in-
creased/decreased over time in a significant man-
ner, regardless of the ‘shape’ of this curve.6

Measuring semantic displacement After
aligning the embeddings for individual time-
periods, we can use the aligned word vectors to
compute the semantic displacement that a word
has undergone during a certain time-period. In
particular, we can directly compute the cosine-
distance between a word’s representation for
different time-periods, i.e. cos-dist(wt,wt+∆),
as a measure of semantic change. We can also
use this measure to quantify ‘rates’ of semantic
change for different words by looking at the
displacement between consecutive time-points.

3 Comparison of different approaches

We compare the different distributional ap-
proaches on a set of benchmarks designed to test
their scientific utility. We evaluate both their syn-
chronic accuracy (i.e., ability to capture word sim-
ilarity within individual time-periods) and their di-
achronic validity (i.e., ability to quantify semantic
changes over time).

3.1 Synchronic Accuracy

We evaluated the synchronic (within-time-period)
accuracy of the methods using a standard modern
benchmark and the 1990s portion of the ENGALL
data. On Bruni et al. (2012)’s MEN similarity task
of matching human judgments of word similari-
ties, SVD performed best (ρ = 0.739), followed
by PPMI (ρ = 0.687) and SGNS (ρ = 0.649).
These results echo the findings of Levy et al.
(2015), who found SVD to perform best on sim-
ilarity tasks while SGNS performed best on anal-
ogy tasks (which are not the focus of this work).

3.2 Diachronic Validity

We evaluate the diachronic validity of the methods
on two historical semantic tasks: detecting known
shifts and discovering shifts from data. For both
these tasks, we performed detailed evaluations on
a small set of examples (28 known shifts and the
top-10 “discovered” shifts by each method). Us-
ing these reasonably-sized evaluation sets allowed
the authors to evaluate each case rigorously using
existing literature and historical corpora.

6Other metrics or change-point detection approaches, e.g.
mean shifts (Kulkarni et al., 2014) could also be used.

1492



Word Moving towards Moving away Shift start Source

gay homosexual, lesbian happy, showy ca 1920 (Kulkarni et al., 2014)
fatal illness, lethal fate, inevitable <1800 (Jatowt and Duh, 2014)
awful disgusting, mess impressive, majestic <1800 (Simpson et al., 1989)
nice pleasant, lovely refined, dainty ca 1900 (Wijaya and Yeniterzi, 2011)
broadcast transmit, radio scatter, seed ca 1920 (Jeffers and Lehiste, 1979)
monitor display, screen — ca 1930 (Simpson et al., 1989)
record tape, album — ca 1920 (Kulkarni et al., 2014)
guy fellow, man — ca 1850 (Wijaya and Yeniterzi, 2011)
call phone, message — ca 1890 (Simpson et al., 1989)

Table 2: Set of attested historical shifts used to evaluate the methods. The examples are taken from previous works on semantic
change and from the Oxford English Dictionary (OED), e.g. using ‘obsolete’ tags. The shift start points were estimated using
attestation dates in the OED. The first six examples are words that shifted dramatically in meaning while the remaining four are
words that acquired new meanings (while potentially also keeping their old ones).

Method Corpus % Correct %Sig.

PPMI ENGALL 96.9 84.4COHA 100.0 88.0
SVD ENGALL 100.0 90.6COHA 100.0 96.0
SGNS ENGALL 100.0 93.8COHA 100.0 72.0

Table 3: Performance on detection task, i.e. ability to cap-
ture the attested shifts from Table 2. SGNS and SVD capture
the correct directionality of the shifts in all cases (%Correct),
e.g., gay becomes more similar to homosexual, but there are
differences in whether the methods deem the shifts to be sta-
tistically significant at the p < 0.05 level (%Sig).

Detecting known shifts. First, we tested
whether the methods capture known historical
shifts in meaning. The goal in this task is for
the methods to correctly capture whether pairs of
words moved closer or further apart in semantic
space during a pre-determined time-period. We
use a set of independently attested shifts as an
evaluation set (Table 2). For comparison, we eval-
uated the methods on both the large (but messy)
ENGALL data and the smaller (but clean) COHA
data. On this task, all the methods performed
almost perfectly in terms of capturing the correct
directionality of the shifts (i.e., the pairwise
similarity series have the correct sign on their
Spearman correlation with time), but there were
some differences in whether the methods deemed
the shifts statistically significant at the p < 0.05
level.7 Overall, SGNS performed the best on the
full English data, but its performance dropped
significantly on the smaller COHA dataset, where
SVD performed best. PPMI was noticeably worse
than the other two approaches (Table 3).

Discovering shifts from data. We tested
whether the methods discover reasonable shifts

7All subsequent significance tests are at p < 0.05.

by examining the top-10 words that changed the
most from the 1900s to the 1990s according to
the semantic displacement metric introduced in
Section 2.4 (limiting our analysis to words with
relative frequencies above 10−5 in both decades).
We used the ENGFIC data as the most-changed
list for ENGALL was dominated by scientific
terms due to changes in the corpus sample.

Table 4 shows the top-10 words discovered by
each method. These shifts were judged by the au-
thors as being either clearly genuine, borderline,
or clearly corpus artifacts. SGNS performed by
far the best on this task, with 70% of its top-10
list corresponding to genuine semantic shifts, fol-
lowed by 40% for SVD, and 10% for PPMI. How-
ever, a large portion of the discovered words for
PPMI (and less so SVD) correspond to borderline
cases, e.g. know, that have not necessarily shifted
significantly in meaning but that occur in differ-
ent contexts due to global genre/discourse shifts.
The poor quality of the nearest neighbors gener-
ated by the PPMI algorithm—which are skewed
by PPMI’s sensitivity to rare events—also made
it difficult to assess the quality of its discovered
shifts. SVD was the most sensitive to corpus arti-
facts (e.g., co-occurrences due to cover pages and
advertisements), but it still captured a number of
genuine semantic shifts.

We opted for this small evaluation set and re-
lied on detailed expert judgments to minimize am-
biguity; each potential shift was analyzed in detail
by consulting consulting existing literature (espe-
cially the OED; Simpson et al., 1989) and all dis-
agreements were discussed.

Table 5 details representative example shifts in
English, French, and German. Chinese lacks suf-
ficient historical data for this task, as only years
1950-1999 are usable; however, we do still see

1493



Method Top-10 words that changed from 1900s to 1990s

PPMI know, got, would, decided, think, stop, remember, started, must, wanted
SVD harry, headed, calls, gay, wherever, male, actually, special, cover, naturally
SGNS wanting, gay, check, starting, major, actually, touching, harry, headed, romance

Table 4: Top-10 English words with the highest semantic displacement values between the 1900s and 1990s. Bolded entries
correspond to real semantic shifts, as deemed by examining the literature and their nearest neighbors; for example, headed
shifted from primarily referring to the “top of a body/entity” to referring to “a direction of travel.” Underlined entries are
borderline cases that are largely due to global genre/discourse shifts; for example, male has not changed in meaning, but its
usage in discussions of “gender equality” is relatively new. Finally, unmarked entries are clear corpus artifacts; for example,
special, cover, and romance are artifacts from the covers of fiction books occasionally including advertisements etc.

Word Language Nearest-neighbors in 1900s Nearest-neighbors in 1990s

wanting English lacking, deficient, lacked, lack, needed wanted, something, wishing, anything,
anybody

asile French refuge, asiles, hospice, vieillards, in-
firmerie

demandeurs, refuge, hospice, visas, ad-
mission

widerstand German scheiterte, volt, stromstärke, leisten,
brechen

opposition, verfolgung, nationalsozialis-
tische, nationalsozialismus, kollaboration

Table 5: Example words that changed dramatically in meaning in three languages, discovered using SGNS embeddings. The
examples were selected from the top-10 most-changed lists between 1900s and 1990s as in Table 4. In English, wanting
underwent subjectification and shifted from meaning “lacking” to referring to subjective ”desire”, as in “the education system
is wanting” (1900s) vs. ”I’ve been wanting to tell you” (1990s). In French asile (“asylum”) shifted from primarily referring
to “hospitals, or infirmaries” to also referring to “asylum seekers, or refugees”. Finally, in German Widerstand (“resistance”)
gained a formal meaning as referring to the local German resistance to Nazism during World War II.

some significant changes for Chinese in this short
time-period, such as 病毒 (“virus”) moving closer
to电脑 (“computer”, ρ = 0.89).

3.3 Methodological recommendations

PPMI is clearly worse than the other two meth-
ods; it performs poorly on all the benchmark tasks,
is extremely sensitive to rare events, and is prone
to false discoveries from global genre shifts. Be-
tween SVD and SGNS the results are somewhat
equivocal, as both perform best on two out of the
four tasks (synchronic accuracy, ENGALL detec-
tion, COHA detection, discovery). Overall, SVD
performs best on the synchronic accuracy task and
has higher average accuracy on the ‘detection’
task, while SGNS performs best on the ‘discov-
ery’ task. These results suggest that both these
methods are reasonable choices for studies of se-
mantic change but that they each have their own
tradeoffs: SVD is more sensitive, as it performs
well on detection tasks even when using a small
dataset, but this sensitivity also results in false dis-
coveries due to corpus artifacts. In contrast, SGNS
is robust to corpus artifacts in the discovery task,
but it is not sensitive enough to perform well on the
detection task with a small dataset. Qualitatively,
we found SGNS to be most useful for discovering
new shifts and visualizing changes (e.g., Figure 1),

while SVD was most effective for detecting subtle
shifts in usage.

4 Statistical laws of semantic change

We now show how diachronic embeddings can be
used in a large-scale cross-linguistic analysis to re-
veal statistical laws that relate frequency and pol-
ysemy to semantic change. In particular, we ana-
lyze how a word’s rate of semantic change,

∆(t)(wi) = cos-dist(w
(t)
i ,w

(t+1)
i ) (6)

depends on its frequency, f (t)(wi) and a measure
of its polysemy, d(t)(wi) (defined in Section 4.4).

4.1 Setup
We present results using SVD embeddings
(though analogous results were found to hold with
SGNS). Using all four languages and all four
conditions for English (ENGALL, ENGFIC, and
COHA with and without lemmatization), we per-
formed regression analysis on rates of seman-
tic change, ∆(t)(wi); thus, we examined one
data-point per word for each pair of consecutive
decades and analyzed how a word’s frequency
and polysemy at time t correlate with its degree
of semantic displacement over the next decade.
To ensure the robustness of our results, we ana-
lyzed only the top-10000 non–stop words by aver-

1494



Top-10 most polysemous yet, always, even, little, called, also, sometimes, great, still, quite
Top-10 least polysemous photocopying, retrieval, thirties, mom, sweater, forties, seventeenth,

fifteenth, holster, postage

Table 6: The top-10 most and least polysemous words in the ENGFIC data. Words like yet, even, and still are used in many
diverse ways and are highly polysemous. In contrast, words like photocopying, postage, and holster tend to be used in very
specific well-clustered contexts, corresponding to a single sense; for example, mail and letter are both very likely to occur in
the context of postage and are also likely to co-occur with each other, independent of postage.

a b

Figure 2: Higher frequency words have lower rates of change (a), while polysemous words have higher rates of change (b).
The negative curvature for polysemy—which is significant only at high d(wi)—varies across datasets and was not present with
SGNS, so it is not as robust as the clear linear trend that was seen with all methods and across all datasets. The trendlines show
95% CIs from bootstrapped kernel regressions on the ENGALL data (Li and Racine, 2007).

age historical frequency (lower-frequency words
tend to lack sufficient co-occurrence data across
years) and we discarded proper nouns (changes in
proper noun usage are primarily driven by non-
linguistic factors, e.g. historical events, Traugott
and Dasher, 2001). We also log-transformed the
semantic displacement scores and normalized the
scores to have zero mean and unit variance; we
denote these normalized scores by ∆̃(t)(wi).

We performed our analysis using a linear mixed
model with random intercepts per word and fixed
effects per decade; i.e., we fit βf , βd, and βt s.t.

∆̃(t)(wi) = βf log
(
f (t)(wi)

)
+βd log

(
d(t)(wi)

)
+ βt + zwi + �

(t)
wi ∀wi ∈ V, t ∈ {t0, ..., tn}, (7)

where zwi ∼ N (0, σwi) is the random intercept
for word wi and �

(t)
wi ∈ N (0, σ) is an error term.

βf , βd and βt correspond to the fixed effects for
frequency, polysemy and the decade t, respec-
tively8. Intuitively, this model estimates the effects
of frequency and polysemy on semantic change,
while controlling for temporal trends and correct-
ing for the fact that measurements on same word
will be correlated across time. We fit (7) using the
standard restricted maximum likelihood algorithm
(McCulloch and Neuhaus, 2001; Appendix C).

8Note that time is treated as a categorical variable, as each
decade has its own fixed effect.

4.2 Overview of results

We find that, across languages, rates of semantic
change obey a scaling relation of the form

∆(wi) ∝ f(wi)βf × d(wi)βd , (8)

with βf < 0 and βd > 0. This finding implies that
frequent words change at slower rates while pol-
ysemous words change faster, and that both these
relations scale as power laws.

4.3 Law of conformity: Frequently used
words change at slower rates

Using the model in equation (7), we found that
the logarithm of a word’s frequency, log(f(wi)),
has a significant and substantial negative effect on
rates of semantic change in all settings (Figures 2a
and 3a). Given the use of log-transforms in pre-
processing the data this implies rates of semantic
change are proportional to a negative power (βf )
of frequency, i.e.

∆(wi) ∝ f(wi)βf , (9)

with βf ∈ [−1.26,−0.27] across lan-
guages/datasets. The relatively large range
of values for βf is due to the fact that the COHA
datasets are outliers due to their substantially
smaller sample sizes (Figure 3; the range is
βf ∈ [−0.66,−0.27] with COHA excluded).

1495



Figure 3: a, The estimated linear effect of log-frequency (β̂f ) is significantly negative across all languages. The effect is
significantly stronger in the COHA data, but this is likely due to its small sample size (∼100× smaller than the other datasets);
the small sample size introduces random variance that may artificially inflate the effect of frequency. From the COHA data,
we also see that the result holds regardless of whether lemmatization is used. b, Analogous trends hold for the linear effect of
the polysemy score (β̂d), which is strong and significantly positive across all conditions. Again, we see that the smaller COHA
datasets are mild outliers.9 95% CIs are shown.

4.4 Law of innovation: Polysemous words
change at faster rates

There is a common hypothesis in the linguistic lit-
erature that “words become semantically extended
by being used in diverse contexts” (Winter et al.,
2014), an idea that dates back to the writings of
Bréal (1897). We tested this notion by examining
the relationship between polysemy and semantic
change in our data.

Quantifying polysemy

Measuring word polysemy is a difficult and
fraught task, as even “ground truth” dictionaries
differ in the number of senses they assign to words
(Simpson et al., 1989; Fellbaum, 1998). We cir-
cumvent this issue by measuring a word’s contex-
tual diversity as a proxy for its polysemousness.
The intuition behind our measure is that words
that occur in many distinct, unrelated contexts will
tend to be highly polysemous. This view of pol-
ysemy also fits with previous work on semantic
change, which emphasizes the role of contextual
diversity (Bréal, 1897; Winter et al., 2014).

We measure a word’s contextual diversity, and
thus polysemy, by examining its neighborhood in
an empirical co-occurrence network. We con-
struct empirical co-occurrence networks using the
PPMI measure defined in Section 2. In these net-
works words are connected to each other if they
co-occur more than one would expect by chance
(after smoothing). The polysemy of a word is then
measured as its local clustering coefficient within

9The COHA data is ∼100× smaller, which has a global
effect on the construction of the co-occurrence network (e.g.,
lower average degree) used to compute polysemy scores.

this network (Watts and Strogatz, 1998):

d(wi) = −
∑

ci,cj∈NPPMI(wi) I {PPMI(ci, cj) > 0}
|NPPMI(wi)|(|NPPMI(wi)| − 1) ,

(10)
where NPPMI(wi) = {wj : PPMI(wi, wj) > 0}.
This measure counts the proportion of wi’s neigh-
bors that are also neighbors of each other. Accord-
ing to this measure, a word will have a high clus-
tering coefficient (and thus a low polysemy score)
if the words that it co-occurs with also tend to co-
occur with each other. Polysemous words that are
contextually diverse will have low clustering co-
efficients, since they appear in disjointed or unre-
lated contexts.

Variants of this measure are often used in word-
sense discrimination and correlate with, e.g., num-
ber of senses in WordNet (Dorow and Widdows,
2003; Ferret, 2004). However, we found that
it was slightly biased towards rating contextually
diverse discourse function words (e.g., also) as
highly polysemous, which needs to be taken into
account when interpreting our results. We opted to
use this measure, despite this bias, because it has
the strong benefit of being clearly interpretable: it
simply measures the extent to which a word ap-
pears in diverse textual contexts. Table 6 gives ex-
amples of the least and most polysemous words in
the ENGFIC data, according to this score.

As expected, this measure has significant intrin-
sic positive correlation with frequency. Across
datasets, we found Pearson correlations in the
range 0.45 < r < 0.8 (all p < 0.05), confirm-
ing frequent words tend to be used in a greater di-
versity of contexts. As a consequence of this high
correlation, we interpret the effect of this measure
only after controlling for frequency (this control is
naturally captured in equation (7)).

1496



Polysemy and semantic change
After fitting the model in equation (7), we found
that the logarithm of the polysemy score exhibits a
strong positive effect on rates of semantic change,
throughout all four languages (Figure 3b). As with
frequency, the relation takes the form of a power
law

∆(wi) ∝ d(wi)βd , (11)
with a language/corpus dependent scaling constant
in βd ∈ [0.37, 0.77]. Note that this relation-
ship is a complete reversal from what one would
expect according to d(wi)’s positive correlation
with frequency; i.e., since frequency and poly-
semy are highly positively correlated, one would
expect them to have similar effects on seman-
tic change, but we found that the effect of poly-
semy completely reversed after controlling for fre-
quency. Figure 2b shows the relationship of pol-
ysemy with rates of semantic change in the EN-
GALL data after regressing out effect of frequency
(using the method of Graham, 2003).

5 Discussion

We show how distributional methods can reveal
statistical laws of semantic change and offer a ro-
bust methodology for future work in this area.

Our work builds upon a wealth of previous
research on quantitative approaches to semantic
change, including prior work with distributional
methods (Sagi et al., 2011; Wijaya and Yeniterzi,
2011; Gulordava and Baroni, 2011; Jatowt and
Duh, 2014; Kulkarni et al., 2014; Xu and Kemp,
2015), as well as recent work on detecting the
emergence of novel word senses (Lau et al., 2012;
Mitra et al., 2014; Cook et al., 2014; Mitra et al.,
2015; Frermann and Lapata, 2016). We extend
these lines of work by rigorously comparing dif-
ferent approaches to quantifying semantic change
and by using these methods to propose new statis-
tical laws of semantic change.

The two statistical laws we propose have strong
implications for future work in historical seman-
tics. The law of conformity—frequent words
change more slowly—clarifies frequency’s role
in semantic change. Future studies of semantic
change must account for frequency’s conforming
effect: when examining the interaction between
some linguistic process and semantic change, the
law of conformity should serve as a null model in
which the interaction is driven primarily by under-
lying frequency effects.

The law of innovation—polysemous words
change more quickly—quantifies the central role
polysemy plays in semantic change, an issue that
has concerned linguists for more than 100 years
(Bréal, 1897). Previous works argued that seman-
tic change leads to polysemy (Wilkins, 1993; Hop-
per and Traugott, 2003). However, our results
show that polysemous words change faster, which
suggests that polysemy may actually lead to se-
mantic change.

Overall, these two factors—frequency and
polysemy—explain between 48% and 88% of the
variance10 in rates of semantic change (across con-
ditions). This remarkable degree of explanatory
power indicates that frequency and polysemy are
perhaps the two most crucial linguistic factors that
explain rates of semantic change over time.

These empirical statistical laws also lend them-
selves to various causal mechanisms. The law
of conformity might be a consequence of learn-
ing: perhaps people are more likely to use rare
words mistakenly in novel ways, a mechanism for-
malizable by Bayesian models of word learning
and corresponding to the biological notion of ge-
netic drift (Reali and Griffiths, 2010). Or per-
haps a sociocultural conformity bias makes people
less likely to accept novel innovations of common
words, a mechanism analogous to the biological
process of purifying selection (Boyd and Richer-
son, 1988; Pagel et al., 2007). Moreover, such
mechanisms may also be partially responsible for
the law of innovation. Highly polysemous words
tend to have more rare senses (Kilgarriff, 2004),
and rare senses may be unstable by the law of con-
formity. While our results cannot confirm such
causal links, they nonetheless highlight a new role
for frequency and polysemy in language change
and the importance of distributional models in his-
torical research.

Acknowledgments

The authors thank D. Friedman, R. Sosic, C. Man-
ning, V. Prabhakaran, and S. Todd for their helpful
comments and discussions. We are also indebted
to our anonymous reviewers. W.H. was supported
by an NSERC PGS-D grant and the SAP Stanford
Graduate Fellowship. W.H., D.J., and J.L. were
supported by the Stanford Data Science Initiative,
and NSF Awards IIS-1514268, IIS-1149837, and
IIS-1159679.

10Marginal R2 (Nakagawa and Schielzeth, 2013).

1497



References
James S. Adelman, Gordon D. A. Brown, and José F.

Quesada. 2006. Contextual diversity, not word fre-
quency, determines word-naming and lexical deci-
sion times. Psychol. Sci., 17(9):814–823.

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python. O’Reilly
Media, Inc.

Andreas Blank. 1999. Why do new meanings occur?
A cognitive typology of the motivations for lexical
semantic change. In Peter Koch and Andreas Blank,
editors, Historical Semantics and Cognition. Walter
de Gruyter, Berlin, Germany.

Robert Boyd and Peter J Richerson. 1988. Culture and
the Evolutionary Process. University of Chicago
Press, Chicago, IL.

Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proc. ACL, pages 136–145.

Michel Bréal. 1897. Essai de Sémantique: Science des
significations. Hachette, Paris, France.

John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Behav.
Res. Methods, 39(3):510–526.

John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting semantic representations from word co-
occurrence statistics: stop-lists, stemming, and
SVD. Behav. Res. Methods, 44(3):890–907.

J.L. Bybee. 2007. Frequency of Use And the Organi-
zation of Language. Oxford University Press, New
York City, NY.

Paul Cook, Jey Han Lau, Diana McCarthy, and Timo-
thy Baldwin. 2014. Novel Word-sense Identifica-
tion. In Proc. COLING, pages 1624–1635.

Scott Crossley, Tom Salsbury, and Danielle McNa-
mara. 2010. The development of polysemy and
frequency use in english second language speakers.
Language Learning, 60(3):573–605.

Mark Davies. 2010. The Corpus of Historical
American English: 400 million words, 1810-2009.
http://corpus.byu.edu/coha/.

Beate Dorow and Dominic Widdows. 2003. Discov-
ering corpus-specific word senses. In Proc. EACL,
pages 79–82.

Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.

Olivier Ferret. 2004. Discovering word senses from
a network of lexical cooccurrences. In Proc. COL-
ING, page 1326.

J.R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. In Studies in Linguistic Analysis. Spe-
cial volume of the Philological Society. Basil Black-
well, Oxford, UK.

Lea Frermann and Mirella Lapata. 2016. A Bayesian
Model of Diachronic Meaning Change. Trans. ACL,
4:31–45.

Dirk Geeraerts. 1997. Diachronic Prototype Se-
mantics: A Contribution to Historical Lexicology.
Clarendon Press, Oxford, UK.

Michael H. Graham. 2003. Confronting multi-
collinearity in ecological multiple regression. Ecol-
ogy, 84(11):2809–2815.

Kristina Gulordava and Marco Baroni. 2011. A dis-
tributional similarity approach to the detection of
semantic change in the Google Books Ngram cor-
pus. In Proc. GEMS 2011 Workshop on Geometri-
cal Models of Natural Language Semantics, pages
67–71. Association for Computational Linguistics.

Zellig S. Harris. 1954. Distributional structure. Word,
10:146–162.

Paul J. Hopper and Elizabeth Closs Traugott. 2003.
Grammaticalization. Cambridge University Press,
Cambridge, UK.

Adam Jatowt and Kevin Duh. 2014. A framework
for analyzing semantic change of words across time.
In Proc. ACM/IEEE-CS Conf. on Digital Libraries,
pages 229–238. IEEE Press.

R. Jeffers and Ilse Lehiste. 1979. Principles and Meth-
ods for Historical Linguistics. MIT Press, Cam-
bridge, MA.

Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? In Text, Speech and Dialogue,
pages 103–111. Springer.

Yoon Kim, Yi-I. Chiu, Kentaro Hanaki, Darshan
Hegde, and Slav Petrov. 2014. Temporal analysis
of language through neural language models. arXiv
preprint arXiv:1405.3515.

Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and
Steven Skiena. 2014. Statistically significant de-
tection of linguistic change. In Proc. WWW, pages
625–635.

Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato’s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psychol. Rev., 104(2):211.

Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proc. EACL,
pages 591–601.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Trans. ACL, 3.

1498



Qi Li and Jeffrey Scott Racine. 2007. Nonparametric
econometrics: theory and practice. Princeton Uni-
versity Press, Princeton, NJ.

Erez Lieberman, Jean-Baptiste Michel, Joe Jackson,
Tina Tang, and Martin A. Nowak. 2007. Quantify-
ing the evolutionary dynamics of language. Nature,
449(7163):713–716.

Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram
corpus. In Proc. ACL, System Demonstrations,
pages 169–174.

Charles E McCulloch and John M Neuhaus. 2001.
Generalized linear mixed models. Wiley-
Interscience, Hoboken, NJ.

Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, Joseph P.
Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig,
Jon Orwant, and others. 2011. Quantitative analysis
of culture using millions of digitized books. Sci-
ence, 331(6014):176–182.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Bie-
mann, Animesh Mukherjee, and Pawan Goyal.
2014. That’s sick dude!: Automatic identification
of word sense change across different timescales. In
Proc. ACL.

Sunny Mitra, Ritwik Mitra, Suman Kalyan Maity,
Martin Riedl, Chris Biemann, Pawan Goyal, and
Animesh Mukherjee. 2015. An automatic ap-
proach to identify word sense changes in text media
across timescales. Natural Language Engineering,
21(05):773–798.

Shinichi Nakagawa and Holger Schielzeth. 2013. A
general and simple method for obtaining R 2 from
generalized linear mixed-effects models. Methods
Ecol. Evol., 4(2):133–142.

Mark Pagel, Quentin D. Atkinson, and Andrew Meade.
2007. Frequency of word-use predicts rates of
lexical evolution throughout Indo-European history.
Nature, 449(7163):717–720.

Eitan Adam Pechenick, Christopher M. Danforth, and
Peter Sheridan Dodds. 2015. Characterizing the
Google Books corpus: Strong limits to inferences of
socio-cultural and linguistic evolution. PLoS ONE,
10(10).

F. Reali and T. L. Griffiths. 2010. Words as alle-
les: connecting language evolution with Bayesian
learners to models of genetic drift. Proc. R. Soc.
B, 277(1680):429–436.

Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2011.
Tracing semantic change with latent semantic analy-
sis. In Kathryn Allan and Justyna A. Robinson, edi-
tors, Current Methods in Historical Semantics, page
161. De Gruyter Mouton, Berlin, Germany.

Benôit Sagot, Lionel Clément, Eric de La Clergerie,
and Pierre Boullier. 2006. The Lefff 2 syntactic
lexicon for French: architecture, acquisition, use. In
Proc. LREC, pages 1–4.

Gerold Schneider and Martin Volk. 1998. Adding
manual constraints and lexical look-up to a Brill-
tagger for German. In Proceedings of the ESSLLI-
98 Workshop on Recent Advances in Corpus Anno-
tation, Saarbrücken.

Peter H Schönemann. 1966. A generalized solution of
the orthogonal Procrustes problem. Psychometrika,
31(1):1–10.

J.S. Seabold and J. Perktold. 2010. Statsmodels:
Econometric and statistical modeling with python.
In Proc. 9th Python in Science Conference.

John Andrew Simpson, Edmund SC Weiner, et al.
1989. The Oxford English Dictionary, volume 2.
Clarendon Press Oxford, Oxford, UK.

Elizabeth Closs Traugott and Richard B Dasher. 2001.
Regularity in Semantic Change. Cambridge Univer-
sity Press, Cambridge, UK.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. J. Artif. Intell. Res., 37(1):141–188.

S. Ullmann. 1962. Semantics: An Introduction to the
Science of Meaning. Barnes & Noble, New York
City, NY.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(2579-2605):85.

Duncan J Watts and Steven H Strogatz. 1998. Col-
lective dynamics of ‘small-world’networks. Nature,
393(6684):440–442.

Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-
derstanding semantic change of words over cen-
turies. In Proc. Workshop on Detecting and Exploit-
ing Cultural Diversity on the Social Web, pages 35–
40. ACM.

David P Wilkins. 1993. From part to person: Natu-
ral tendencies of semantic change and the search for
cognates. Cognitive Anthropology Research Group
at the Max Planck Institute for Psycholinguistics.

B. Winter, Graham Thompson, and Matthias Urban.
2014. Cognitive Factors Motivating The Evolution
Of Word Meanings: Evidence From Corpora, Be-
havioral Data And Encyclopedic Network Structure.
In Proc. EVOLANG, pages 353–360.

1499



Yang Xu and Charles Kemp. 2015. A computational
evaluation of two laws of semantic change. In Proc.
Annual Conf. of the Cognitive Science Society.

Yang Xu, Terry Regier, and Barbara C. Malt. 2015.
Historical Semantic Chaining and Efficient Commu-
nication: The Case of Container Names. Cognitive
Science.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(02):207–238.

George Kingsley Zipf. 1945. The meaning-frequency
relationship of words. J. Gen. Psychol., 33(2):251–
256.

Bahar İlgen and Bahar Karaoglan. 2007. Investiga-
tion of Zipf’s ‘law-of-meaning’on Turkish corpora.
In International Symposium on Computer and Infor-
mation Sciences, pages 1–6. IEEE.

A Hyperparameter and pre-processing
details

For all datasets, words were lowercased and
stripped of punctuation. For the Google datasets
we built models using the top-100000 words by
their average frequency over the entire histori-
cal time-periods, and we used the top-50000 for
COHA. During model learning we also discarded
all words within a year that occurred below a cer-
tain threshold (500 for the Google data, 100 for the
COHA data).

For all methods, we used the hyperparameters
recommended in Levy et al. (2015). For the con-
text word distributions in all methods, we used
context distribution smoothing with a smoothing
parameter of 0.75. Note that for SGNS this cor-
responds to smoothing the unigram negative sam-
pling distribution. For both, SGNS and PPMI, we
set the negative sample prior α = log(5), while we
set this value to α = 0 for SVD, as this improved
results. When using SGNS on the Google data,
we also subsampled, with words being random re-
moved with probability pr(wi) = 1 −

√
10−5
f(wi)

, as
recommended by Levy et al. (2015) and Mikolov
et al. (2013). Furthermore, to improve the com-
putational efficiency of SGNS (which works with
text streams and not co-occurrence counts), we
downsampled the larger years in the Google N-
Gram data to have at most 109 tokens. No such
subsampling was performed on the COHA data.

For all methods, we defined the context set to
simply be the same vocabulary as the target words,
as is standard in most word vector applications

(Levy et al., 2015). However, we found that the
PPMI method benefited substantially from larger
contexts (similar results were found in Bullinaria
and Levy, 2007), so we did not remove any low-
frequency words per year from the context for that
method. The other embedding approaches did not
appear to benefit from the inclusion of these low-
frequency terms, so they were dropped for compu-
tational efficiency.

For SGNS, we used the implementation pro-
vided in Levy et al. (2015). The implementations
for PPMI and SVD are released with the code
package associated with this work.

B Visualization algorithm

To visualize semantic change for a word wi in two
dimensions we employed the following procedure,
which relies on the t-SNE embedding method
(Van der Maaten and Hinton, 2008) as a subrou-
tine:

1. Find the union of the word wi’s k nearest
neighbors over all necessary time-points.

2. Compute the t-SNE embedding of these
words on the most recent (i.e., the modern)
time-point.

3. For each of the previous time-points, hold
all embeddings fixed, except for the target
word’s (i.e., the embedding for wi), and op-
timize a new t-SNE embedding only for the
target word. We found that initializing the
embedding for the target word to be the cen-
troid of its k′-nearest neighbors in a time-
point was highly effective.

Thus, in this procedure the background words are
always shown in their “modern” positions, which
makes sense given that these are the current mean-
ings of these words. This approximation is neces-
sary, since in reality all words are moving.

C Regression analysis details

In addition to the pre-processing mentioned in the
main text, we also normalized the contextual di-
versity scores d(wi) within years by subtracting
the yearly median. This was necessary because
there was substantial changes in the median con-
textual diversity scores over years due to changes
in corpus sample sizes etc. Data points corre-
sponding to words that occurred less than 500
times during a time-period were also discarded, as

1500



these points lack sufficient data to robustly esti-
mate change rates (this threshold only came into
effect on the COHA data, however). We removed
stop words and proper nouns by (i) removing all
stop-words from the available lists in Python’s
NLTK package (Bird et al., 2009) and (ii) re-
stricting our analysis to words with part-of-speech
(POS) tags corresponding to four main linguistic
categories (common nouns, verbs, adverbs, and
adjectives), using the POS sources in Table 1.

When analyzing the effects of frequency and
contextual diversity, the model contained fixed ef-
fects for these features and for time along with
random effects for word identity. We opted not
to control for POS tags in the presented results,
as contextual diversity is co-linear with these tags
(e.g., adverbs are more contextual diverse than
nouns), and the goal was to demonstrate the main
effect of contextual diversity across all word types.
That said, the effect of contextual diversity re-
mained strong and significantly positive in all
datasets even after controlling for POS tags.

To fit the linear mixed models, we used
the Python statsmodels package with re-
stricted maximum likelihood estimation (REML)
(Seabold and Perktold, 2010). All mentioned
significance scores were computed according to
Wald’s z-tests, though these results agreed with
Bonferroni corrected likelihood ratio tests on the
eng-all data.

The visualizations in Figure 2 were computed
on the eng-all data and correspond to boot-
strapped locally-linear kernel regressions with
bandwidths selected via the AIC Hurvitch criteria
(Li and Racine, 2007).

1501


