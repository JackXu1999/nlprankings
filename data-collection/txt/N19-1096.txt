




































The problem with probabilistic DAG automata for semantic graphs


Proceedings of NAACL-HLT 2019, pages 902–911
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

902

The Problem with Probabilistic DAG Automata for Semantic Graphs

Ieva Vasiljeva∗ and Sorcha Gilroy∗ and Adam Lopez
Institute for Language, Cognition, and Computation

School of Informatics
University of Edinburgh

{vasiljeva.ieva, gilroysorcha}@gmail.com, alopez@inf.ed.ac.uk

Abstract

Semantic representations in the form of di-
rected acyclic graphs (DAGs) have been in-
troduced in recent years, and to model them,
we need probabilistic models of DAGs. One
model that has attracted some attention is the
DAG automaton, but it has not been stud-
ied as a probabilistic model. We show that
some DAG automata cannot be made into use-
ful probabilistic models by the nearly uni-
versal strategy of assigning weights to tran-
sitions. The problem affects single-rooted,
multi-rooted, and unbounded-degree variants
of DAG automata, and appears to be pervasive.
It does not affect planar variants, but these are
problematic for other reasons.

1 Introduction

Abstract Meaning Representation (AMR; Ba-
narescu et al. 2013) has prompted a flurry of in-
terest in probabilistic models for semantic pars-
ing. AMR annotations are directed acyclic graphs
(DAGs), but most probabilistic models view them
as strings (e.g. van Noord and Bos, 2017) or trees
(e.g. Flanigan et al., 2016), removing their ability
to represent coreference—one of the very aspects
of meaning that motivates AMR. Could we we in-
stead use probabilistic models of DAGs?

To answer this question, we must define prob-
ability distributions over sets of DAGs. For in-
spiration, consider probability distributions over
sets of strings or trees, which can be defined by
weighted finite automata (e.g. Mohri et al., 2008;
May et al., 2010): a finite automaton generates a
set of strings or trees—called a language—and if
we assume that probabilities factor over its transi-
tions, then any finite automaton can be weighted
to define a probability distribution over this lan-
guage. This assumption underlies powerful dy-

∗ Equal contribution. Work while Ieva Vasiljeva was at
the University of Edinburgh

namic programming algorithms like the Viterbi,
forward-backward, and inside-outside algorithms.

What is the equivalent of weighted finite au-
tomata for DAGs? There are several candidates
(Chiang et al., 2013; Björklund et al., 2016; Gilroy
et al., 2017), but one appealing contender is the
DAG automaton (Quernheim and Knight, 2012)
which generalises finite tree automata to DAGs ex-
plicitly for modeling semantic graphs. These DAG
automata generalise an older formalism called pla-
nar DAG automata (Kamimura and Slutzki, 1981)
by adding weights and removing the planarity con-
straint, and have attracted further study (Blum and
Drewes, 2016; Drewes, 2017), in particular by
Chiang et al. (2018), who generalised classic dy-
namic programming algorithms to DAG automata.
But while Quernheim and Knight (2012) clearly
intend for their weights to define probabilities,
they stop short of claiming that they do, instead
ending their paper with an open problem: “Inves-
tigate a reasonable probabilistic model.”

We investigate probabilistic DAG automata and
prove a surprising result: For some DAG au-
tomata, it is impossible to assign weights that
define non-trivial probability distributions. We
exhibit a very simple DAG automaton that gener-
ates an infinite language of graphs, and for which
the only valid probability distribution that can be
defined by weighting transitions is one in which
the support is a single DAG, with all other graphs
receiving a probability of zero.

Our proof relies on the fact that a non-planar
DAG automaton generates DAGs so prolifically
that their number grows factorially in their size,
rather than exponentially as in other automata. It
holds for DAG automata that allow multiple roots
or nodes of unbounded degree. But it breaks down
when applied to the planar DAGs of Kamimura
and Slutzki (1981), which are nevertheless too re-
strictive to model semantic graphs. Our result does



903

not mean that it is impossible to define a prob-
ability distribution for the language that a DAG
automaton generates. But it does mean that this
distribution does not factor over the automaton’s
transitions, so crucial dynamic programming algo-
rithms do not generalise to DAG automata that are
expressive enough to model semantic graphs.

2 DAGs, DAG Automata, and Probability

We are interested in AMR graphs like the one be-
low for “Rahul bakes his cake” (Figure 1, left),
which represents entities and events as nodes, and
relationships between them as edges. Both nodes
and edges have labels, representing the type of an
entity, event, or relationship. But the graphs we
model will only have labels on nodes. These node-
labeled graphs can simulate edge labels using a
node with one incoming and one outgoing edge,
as in the graph on the right of Figure 1.

bake

Rahul cake

ARG1ARG0

POSS

bake

Rahul cake

ARG0 ARG1

POSS

Figure 1: A graph with both node and edge labels (left)
and an equivalent graph with only node labels (right).

Definition 1. A node-labeled directed graph over
a label set Σ is a tuple G = (V,E, lab, src, tar)
where V is a finite set of nodes, E is a finite set of
edges, lab: V → Σ is a function assigning labels
to nodes, src: E → V is a function assigning a
source node to every edge, and tar: E → V is a
function assigning a target node to every edge.

Sometimes we will discuss the set of edges
coming into or going out of a node, so we define
functions IN: V → E∗ and OUT: V → E∗.

IN(v) = {e | tar(e) = v}
OUT(v) = {e | src(e) = v}

A node with no incoming edges is called a root,
and a node with no outgoing edges is called a leaf.
The degree of a node is the number of edges con-
nected to it, so the degree of v is |IN(v)∪OUT(v)|.

A path in a directed graph from node v to
node v′ is a sequence of edges (e1, . . . , en) where
src(e1) = v, tar(en) = v′ and src(ei+1) = tar(ei)
for all i from 1 to n−1. A cycle in a directed graph
is any path in which the first and last nodes are the

same (i.e., v = v′). A directed graph without any
cycles is a directed acyclic graph (DAG).

A DAG is connected if every pair of its nodes
is connected by a sequence of edges, not necessar-
ily directed. Because DAGs do not contain cycles,
they must always have at least one root and one
leaf, but they can have multiple roots and multi-
ple leaves. However, our results apply in different
ways to single-rooted and multi-rooted DAG lan-
guages, so, given a label set Σ, we distinguish be-
tween the set of all connected DAGs with a single
root, G1Σ; and those with one or more roots, G∗Σ.

2.1 DAG Automata
Finite automata generate strings by transitioning
from state to state. Top-down tree automata gen-
eralise string finite automata by transitioning from
a state to an ordered sequence of states, generating
trees top-down from root to leaves; while bottom-
up tree automata transition from an ordered se-
quence of states to a single state, generating trees
bottom-up from leaves to root. The planar DAG
automata of Kamimura and Slutzki (1981) gener-
alise tree automata, transitioning from one ordered
sequence of states to another ordered sequence
of states (Section 4). Finally, the DAG automata
of Quernheim and Knight (2012) transition from
multisets of states to multisets of states, rather than
from sequences to sequences, and this allows them
to generate non-planar DAGs. We summarise the
differences in Table 1 below.

Automaton Transitions Example

string one-to-one p→ p′
top-down tree one-to-many p→ (p′, q′)
bottom-up tree many-to-one (p′, q′)→ p

planar DAG many-to-many (p, q)→ (p′, q′)
non-planar DAG many-to-many {p, q} → {p′, q′}

Table 1: The forms of transitions in different automata.

For the remainder of this section and the next,
we will focus only on non-planar DAG automata,
and when we refer to DAG automata, we mean this
type. To formally define them, we need a notation
for multisets—sets that can contain repeated ele-
ments. A multiset is a pair (S,m) where S is a
finite set andm : S → N is a count function—that
is, m(x) counts the number of times x appears in
the multiset. The set of all finite multisets over
S is M(S). When we write multisets, we will of-
ten simply enumerate their elements. For example,
{p, q, q} is the multiset containing one p and two



904

b

a

b

c

b

aa

(iii)(ii)(i)

q

q

p′

t2, t3

q

pt2t1

p

t4
b

a

b

c

d

(iv)

q

p′
t4, t5

b

a

b

c

d

d

e

(v)

p

p

q

p

q

p′

p′

p′
a

b

b

c

d

d

e

(vii)

b

a

b

c

d

(vi)

p

p
q

p

q

p′

p′

p′

t4, t5t4
from
(iii)

q
p′

Figure 2: Two derivations using the automaton of Example 1. Parts (i), (ii), and (iii) are common to both deriva-
tions. Parts (iv) and (v) represent one completion, while (vi) and (vii) represent an alternative completion. Grey
double edges denote derivation steps, labeled with the corresponding transition(s); red edge labels on partial graphs
(i–iv) and (vi) denote frontier states; blue edge labels on complete graphs (v) and (vii) denote an accepting run.

q’s, and since multisets are unordered, it can also
be written {q, p, q} or {q, q, p}. We write ∅ for a
multiset containing no elements.

Definition 2. A DAG automaton is a triple A =
(Q,Σ, T ) where Q is a finite set of states; Σ is a
finite set of node labels; and T is a finite set of
transitions of the form α σ−→ β where σ ∈ Σ is a
node label, α ∈ M(Q) is the left-hand side, and
β ∈M(Q) is the right-hand side.

Example 1. Let A = (Q,Σ, T ) be a DAG au-
tomaton where Q = {p, p′, q}, Σ = {a, b, c, d, e}
and the transitions in T are as follows:

∅ a−→ {p} (t1)

{p} b−→ {p, q} (t2)

{p} c−→ {p′} (t3)

{p′, q} d−→ {p′} (t4)

{p′} e−→ ∅ (t5)

2.1.1 Generating Single-rooted DAGs
A DAG automaton generates a graph from root to
leaves. To illustrate this, we’ll focus on the case
where a DAG is allowed to have only a single
root, and return to the multi-rooted case in Section
3.1. To generate the root, the DAG automaton can
choose any transition with ∅ on its left-hand side—
these transitions behave like transitions from the
start state in a finite automaton on strings, and
always generate roots. In our example, the only
available transition is t1, which generates a node
labeled a with a dangling outgoing edge in state
p, as in Figure 2(i). The set of all such dangling
edges is the frontier of a partially-generated DAG.

While there are edges on the frontier, the DAG
automation must choose and apply a transition
whose left-hand side matches some subset of
them. In our example, the automaton can choose
either t2 or t3, each matching the available p edge.
The edges associated with the matched states are
attached to a new node with new outgoing frontier

edges specified by the transition, and the matched
states are removed from the frontier. If our au-
tomaton chooses t2, it arrives at the configuration
in Figure 2(ii), with a new node labeled b, new
edges on the frontier labeled p and q, and the in-
coming p state forgotten. Once again, it must
choose between t2 and t3—it cannot use the q state
because that state can only be used by t4, which
also requires a p′ on the frontier. So each time it
applies t2, the choice between t2 and t3 repeats.

If the automaton applies t2 again and then t3,
as it has done in Figure 2(iii), it will face a new
set of choices, between t4 and t5. But notice that
choosing t5 will leave the q states stranded, leav-
ing a partially derived DAG. We consider a run of
the automaton successful only when the frontier is
empty, so this choice leads to a dead end.

If the automaton chooses t4, it has an additional
choice: it can combine p′ with either of the avail-
able q states. If it combines with the lowermost
q, it arrives at the graph in Figure 2(iv), and it
can then apply t4 to consume the remaining q, fol-
lowed by t5, which has ∅ on its right-hand side.
Transitions to ∅ behave like transitions to a fi-
nal state in a finite automaton, and generate leaf
nodes, so we arrive at the complete graph in Fig-
ure 2(v). If the p′ state in Figure 2(iii) had instead
combined with the upper q, a different DAG would
result, as shown in Figure 2(vi-vii).

The DAGs in Figure 2(v) and Figure 2(vii) are
planar, which means they can be drawn without
crossing edges.1 But this DAG automaton can
also produce non-planar DAGs like the one in Fig-
ure 3. To see that it is non-planar, we first contract
each dotted edge by removing it and fusing its end-
points into a single node. This gives us the minor

1While the graph in Figure 2(vii) is drawn with crossing
b − d edges, one of these edges can be redrawn so that they
do not cross.



905

subgraph K3,3, and any graph with a K3,3 minor
is non-planar (Wagner, 1937).

a b b b b c

dddde

Figure 3: A non-planar graph that can be generated by
the automaton of Example 1. When the dotted edges
are contracted, we obtain K3,3, the complete (undi-
rected) bipartite graph over two sets of three nodes.
One set is denoted by hollow blue nodes ( ), the other
by dotted red nodes ( ).

2.1.2 Recognising DAGs and DAG Languages
We define the language generated by a DAG au-
tomaton in terms of recognition, which asks if an
input DAG could have been generated by an input
automaton. We recognise a DAG by finding a run
of the automaton that could have generated it. To
guess a run on a DAG, we guess a state for each of
its edges, and then ask whether those states simu-
late a valid sequence of transitions.

A run of a DAG automaton A = (Q,Σ, T )
on a DAG G = (V,E, lab, src, tar) is a map-
ping ρ : E → Q from edges of G to automa-
ton states Q. We extend ρ to multisets by saying
ρ({e1, . . . , en}) = {ρ(e1), . . . , ρ(en)}, and we
call a run accepting if for all v ∈ V there is a cor-
responding transition ρ(IN(v))

lab(v)−−−→ ρ(OUT(v))
in T . DAG G is recognised by automaton A if
there is an accepting run of A on G.

Example 2. The DAGs in Figure 2(v) and 2(vii)
are recognised by the automaton in Example 1.
The only accepting run for each DAG is denoted
by the blue edge labels.

The single-rooted language Ls(A) of a DAG
automaton A is {G ∈ G1Σ | A recognizes G}.

2.2 Probability and Weighted DAG Automata

Definition 3. Given a language L of DAGs, a
probability distribution over L is any function
p : L→ R meeting two requirements:

(R1) Every DAG must have a probability between
0 and 1, inclusive. Formally, we require that
for all G ∈ L, p(G) ∈ [0, 1].

(R2) The probabilities of all DAGs must sum to
one. Formally, we require

∑
G∈L p(G) = 1.

R1 and R2 suffice to define a probability distri-
bution, but in practice we need something stronger
than R1: all DAGs must receive a non-zero weight,
since in practical applications, objects with proba-
bility zero are effectively not in the language.

Definition 4. A probability distribution p has full
support of L if and only if it meets condition R1’.

(R1’) Every DAG must have a probability greater
than 0 and less than or equal to 1. Formally,
we require that for all G ∈ L, p(G) ∈ (0, 1].

While there are many ways to define a func-
tion that meets requirements R1’ and R2, proba-
bility distributions in natural language processing
are widely defined in terms of weighted automata
or grammars, so we adapt a common definition of
weighted grammars (Booth and Thompson, 1973)
to DAG automata.

Definition 5. A weighted DAG automaton is a
pair (A,w) where A = (Q,Σ, T ) is a DAG au-
tomaton and w : T → R is a function that assigns
real-valued weights to the transitions of A.

Since weights are functions of transitions, we
will write them on transitions following the node
label and a slash (/). For example, if p a−→ q is a
transition and 2 is its weight, we write p

a/2−−→ q.
Example 3. Let (A,w) be a weighted DAG au-
tomaton with A = (Q,Σ, T ), where Q = {p, q},
Σ = {a, b, c}, and the weighted transitions of T
are as follows:

∅ a/0.5−−−→ {p, q} (t′1)

{p} b/0.5−−−→ {p} (t′2)
{p, q} c/1−−→ ∅ (t′3)

We use the weights on transitions to weight runs.

Definition 6. Given a weighted DAG automaton
(A,w) and a DAG G = (V,E, lab, src, tar) with
an accepting run ρ, we extend w to compute the
weight of the run w(ρ) by multiplying the weights
of all of its transitions:

w(ρ) =
∏
v∈V

w(ρ(IN(v))
lab(v)−−−→ ρ(OUT(v)))

Example 4. The DAG automaton of Example 3
generates the DAG in Figure 4, shown with its
only accepting run in blue and the weighted tran-
sitions that generated it in grey. The weight of the
accepting run is 0.5× 0.5× 0.5× 1 = 0.125.



906

t′1/0.5
a

t′2/0.5
b

t′2/0.5
b

t′3/1
b

p

q

p p

Figure 4: A DAG generated by the automaton in Exam-
ple 3. Blue edge labels denote an accepting run; grey
node labels denote weighted transitions used in the run.

Let RA(G) be the set of all accepting runs of a
DAG G using the automaton A. We extend w to
calculate the weight of a DAG G as the sum of the
weights of all the runs that produce it:

w(G) =
∑

ρ∈RA(G)

w(ρ).

While all weighted DAG automata assign real
values to DAGs, not all weighted DAG automata
define probability distributions. To do so, they
must also satisfy requirements R1 and R2.

Definition 7. A weighted automaton (A,w) over
language L(A) is probabilistic if and only if func-
tion w : L(A)→ R is a probability distribution.
Example 5. Consider the weighted automaton in
Example 3. Every DAG generated by this automa-
ton must use t′1 and t

′
3 exactly once, and can use t

′
2

any number of times. If we let Gn be the DAG
that uses t′2 exactly n times, then the language
L defined by this automaton is

⋃
n∈NGn. Since

w(Gn) = w(t
′
1)w(t

′
2)
nw(t′3) and w(t

′
1), w(t

′
2)

and w(t′3) are positive, w satisfies R1 and:∑
G∈L

w(G) =
∞∑
n=0

w(Gn) =
∞∑
n=0

w(t′1)w(t
′
2)
nw(t′3)

=

∞∑
n=0

0.5n+1 = 1

Thusw also satisfies R2 and the weighted automa-
ton in Example 3 is probabilistic.

Definition 8. A probabilistic automaton (A,w)
over language L(A) is probabilistic with full sup-
port if and only if w has full support of L(A).

For every finite automaton over strings or trees,
there is a weighting of its transitions that makes it
probabilistic (Booth and Thompson, 1973), and it
is easy to show that it can be made probabilistic
with full support. For example, string finite au-
tomata have full support if for every state the sum
of weights on its outgoing transitions is 1 and each

weight is greater than 0.2 But as we will show, this
is not always possible for DAG automata.

3 Non-probabilistic DAG Automata

We will exhibit a DAG automaton that generates
factorially many DAGs for a given number of
nodes, and we will show that for any nontrivial
assignment of weights, this factorial growth rate
causes the weight of all DAGs to sum to infinity.

Theorem 1. Let A be the automaton defined in
Example 1. There is no w that makes (A, w) prob-
abilistic with full support over Ls(A).

Proof. In any run of the automaton, transition t1
is applied exactly once to generate the single root,
placing a p on the frontier. This gives a choice be-
tween t2 and t3. If the automaton chooses t2, it
keeps one p on the frontier and adds a q, and must
then repeat the same choice. Suppose it chooses
t2 exactly n times in succession, and then chooses
t3. Then the frontier will contain n edges in state
q and one in state p′. The only way to consume
all of the frontier states is to apply transition t4 ex-
actly n times, consuming a q at each step, and then
apply t5 to consume p′ and complete the deriva-
tion. Hence in any accepting run, t1, t3 and t5 are
each applied once, and t2 and t4 are each applied
n times, for some n ≥ 0. Since transitions map
uniquely to node labels, it follows that every DAG
in Ls(A) will have exactly one node each labeled
a, c, and e; and n nodes each labeled b and d.

When the automaton applies t4 for the first time,
it has n choices of q states to consume, each dis-
tinguished by its unique path from the root. The
second application of t4 has n−1 choices of q, and
the ith application of t4 has n − (i − 1) choices.
Therefore, there are n! different ways to consume
the q states, each producing a unique DAG.

Let f(n) be the weight of a run where t2 has
been applied n times, and to simplify our notation,
let B = w(t1)w(t3)w(t5), and C = w(t2)w(t4).
Let c(n) be the number of unique runs where t2
has been applied n times. By the above:

f(n) = w(t1)w(t2)
nw(t3)w(t4)

nw(t5) = BC
n

c(n) = n!

Now we claim that any DAG in Ls(A) has ex-
actly one accepting run, because the mapping of

2Assuming no epsilon transitions, in our notation for DAG
automata restricted to strings this would include transitions
to ∅, which correspond to states with a final probability of 1
(Mohri et al., 2008).



907

node labels to transitions also uniquely determines
the state of each edge in an accepting run. For ex-
ample, a b node must result from a t2 transition
and a d node from a t4 transition, and since the
output states of t2 and input states of t4 share only
a q, any edge from a b node to a d node must be la-
beled q in any accepting run. Now let G ∈ Ls(A)
be a DAG with n nodes labeled b. Since G has
only one accepting run, we have:

w(G) = f(n)

Let Ln be the set of all DAGs in Ls(A) with n
nodes labeled b. Then Ls(A) =

⋃∞
n=0 Ln and:∑

G∈Ls(A)

w(G) =
∞∑
n=0

∑
G∈Ln

w(G) =
∞∑
n=0

c(n)f(n)

=
∞∑
n=0

(n!)
(
BCn

)
Hence for (A, w) to be probabilistic with full

support, R1’ and R2 require us to choose B and
C so that, respectively, BCn ∈ (0, 1] for all n
and

∑∞
n=0 n!BC

n = 1. Note that this does not
constrain the component weights of B or C to be
in (0, 1]—they can be any real numbers. But since
R1’ requires BCn to be positive for all n, both
B and C must also be positive. If either were 0,
then BCn would be 0 for n > 0; if either were
negative, then BCn would be negative for some
or all values of n.

Now we show that any choice of positive C
causes

∑
G∈Ls(A)w(G) to diverge. Given an in-

finite series of the form
∑∞

n=0 an, the ratio test
(D’Alembert, 1768) considers the ratio between
adjacent terms in the limit, limn→∞

|an+1|
|an| . If this

ratio is greater than 1, the series diverges; if less
than 1 the series converges; if exactly 1 the test is
inconclusive. In our case:

lim
n→∞

|(n+ 1)!BCn+1|
|n!BCn|

= lim
n→∞

(n+ 1)|C| =∞.

Hence
∑

G∈Ls(A) diverges for any choice of C,
equivalently for any choice of weights. So there
is no w for which (A, w) is probabilistic with full
support over Ls(A).

Note that any automaton recognising Ls(A)
must accept factorially many DAGs in the number
of nodes. Our proof implies that there is no proba-
bilistic DAG automaton for languageLs(A), since

no matter how we design its transitions—each of
which must be isomorphic to one in A apart from
the identities of the states—the factorial will even-
tually overwhelm the constant factor correspond-
ing to C in our proof, no matter how small it is.

Theorem 1 does not rule out all probabilistic
variants of A. It requires R1’—if we only require
the weaker R1, then a solution of B=1 and C=0
makes the automaton probabilistic. But this trivial
distribution is not very useful: it assigns all of its
mass to the singleton language { a c e }.

Theorem 1 also does not mean that it is impossi-
ble to define a probability distribution over Ls(A)
with full support. If, for every DAG G with n
nodes labeled b, we let p(G) = 1

2n+1n!
, then:

∑
G∈Ls(A)

w(G) =
∞∑
n=0

1

2n+1n!
n! =

∞∑
n=0

1

2n+1
= 1

But this distribution does not factor over transi-
tions, so it cannot be used with the dynamic pro-
gramming algorithms of Chiang et al. (2018).

A natural way to define distributions using a
DAG automaton is to define two conditional prob-
abilities: one over the choice of nodes to rewrite,
given a frontier; and one over the choice of tran-
sition, given the chosen nodes. The latter factors
over transitions, but the former does not, so it also
cannot use the algorithms of Chiang et al. (2018).3

Theorem 1 only applies to single-rooted, non-
planar DAG automata of bounded degree. Next
we ask whether it extends to other DAG automata,
including those that recognise multi-rooted DAGs,
DAGs of unbounded degree, and planar DAGs.

3.1 Multi-rooted DAGs

What happens when we consider DAG languages
that allow multiple roots? In one reasonable inter-
pretation of AMRbank, over three quarters of the
DAGs have multiple roots (Kuhlmann and Oepen,
2016), so we want a model that permits this.4

Section 2.1.1 explained how a DAG automaton
can be constrained to generate single-rooted lan-
guages, by restricting start transitions (i.e. those

3In this model, the subproblems of a natural dynamic pro-
gram depend on the set of possible frontiers, rather than sub-
sets of nodes as in the algorithms of Chiang et al. (2018). We
do not know whether this could be made efficient.

4AMR annotations are single-rooted, but they achieve this
by duplicating edges: every edge type, like ARG0, has an
inverse type, like ARG0-OF. The number cited here assumes
edges of the second type are converted to the first type by
reversing their direction.



908

with ∅ on the left-hand side) to a single use at the
start of a derivation. To generate DAGs with mul-
tiple roots, we simply allow start transitions to be
applied at any time. We still require the result-
ing DAGs to be connected. For an automaton A,
we define its multi-rooted language Lm(A) as
{G ∈ G∗Σ|A recognises G}.

Although one automaton can define both single-
and multi-rooted DAG languages, these languages
are incomparable. Drewes (2017) uses a construc-
tion very similar to the one in Theorem 1 to show
that single-rooted languages have very expressive
path languages, which he argues are too expressive
for modeling semantics.5 Since the constructions
are so similar, it natural to wonder if the problem
that single-rooted automata have with probabili-
ties is related to their problem with expressivity,
and whether it likewise disappears when we allow
multiple roots. We now show that multi-rooted
languages have the same problem with probabil-
ity, because any multi-rooted language contains
the single-rooted language as a sublanguage.

Corollary 1. Let A be the automaton defined in
Example 1. There is no w that makes (A, w) prob-
abilistic with full support over Lm(A).

Proof. By their definitions, Ls(A) ⊂ Lm(A), so:∑
G∈Lm(A)

w(G) =

∑
G∈Ls(A)

w(G) +
∑

G∈Lm(A)\Ls(A)

w(G)

The first term is∞ by Theorem 1 and the second is
positive by R1’, so the sum diverges. Hence there
is no w for which (A, w) is probabilistic with full
support over Lm(A).

3.2 DAGs of Unbounded Degree
The maximum degree of any node in any DAG
recognised by a DAG automaton is bounded by
the maximum number of states in any transition,
because any transition α σ−→ β generates a node
with |α| incoming edges and |β| outgoing edges.
So, the families of DAG languages we have con-
sidered all have bounded degree.

5The path language of a DAG is the set of strings that
label a path from a root to a leaf, and the path language of
a DAG language is the set of all such strings over all DAGs.
For example, the path language of the DAG in Figure 2(v) is
{abde, abbdde, abbcdde}. Berglund et al. (2017) show that
path languages of multi-rooted DAG automata are regular,
while those of single-rooted DAG automata characterised by
a partially blind multi-counter automaton.

DAG languages with unbounded degree could
be useful to model phenomena like coreference in
meaning representations, and they have been stud-
ied by Quernheim and Knight (2012) and Chiang
et al. (2018). These families generalise and strictly
contain the family of bounded-degree DAG lan-
guages, so they too, include DAG automata that
cannot be made probabilistic.

3.3 Implications for semantic DAGs
We introduced DAG automata as a tool for model-
ing the meaning of natural language, but the DAG
automaton in Theorem 1 is very artificial, so it’s
natural to ask whether it has any real relevance
to natural language. We will argue informally
that this example illustrates a pervasive problem
with DAG automata—specifically, we conjecture
that the factorial growth we observe in Theorem 1
arises under very mild conditions that arise natu-
rally in models of AMR.

Consider object control in a sentence like “I
help Ruby help you” and its AMR in Figure 5.

help

I help

Ruby you

ARG1ARG0

ARG2

ARG0
ARG2

Figure 5: The AMR for “I help Ruby help you”.

We can extend the control structure unbound-
edly with additional helpers, as in “I help Briony
help Kim-Joy help Ruby help you”, and this leads
to unboundedly long repetitive graphs like the one
in Figure 6. These graphs can be cut to separate
the sequence of “help” predicates from their argu-
ments, as illustrated by the dashed blue line.

I

help

Briony

help

Kim-Joy

help

Ruby

help

you

�

ARG1

ARG0
ARG2

ARG1

ARG0
ARG2

ARG1

ARG0
ARG2

ARG0
ARG2

Figure 6: The AMR for “I help Briony help Kim-Joy
help Ruby help you” shown with a cut.

Let a cut be a set of edges such that remov-
ing them splits the graph into two connected sub-
graphs: one containing the root, and the other con-
taining all the leaves. Any cut in a complete graph



909

could have been the frontier of a partially-derived
graph. What if the number of edges in a cut—or
cut-width—can be unbounded, as in the language
of AMR graphs that model object control?

Since a DAG automaton can have only a finite
number of states, there is some state that can occur
unboundedly many times in a graph cut. All edges
in a cut with this state can be rewired by permuting
their target nodes, and the resulting graph will still
be recognised by the automaton, since the rewiring
would not change the multiset of states into or
out of any node. If each possible rewiring results
in a unique graph then the number of recognised
graphs will be factorial in the number of source
nodes for these edges, and the argument of Theo-
rem 1 can be generalised to show that no weight-
ing of any DAG automaton over the graph lan-
guage makes it probabilistic with full support. For
example, in the graph above, all possible rewirings
of the ARG2 edges result in a unique graph.6 Al-
though edge labels are not states, their translation
into node labels implies that they can only be asso-
ciated to a finite number of transitions, hence to a
finite number of states in any corresponding DAG
automaton. A full investigation of conditions un-
der which Theorem 1 generalises is beyond the
scope of this paper.

Conjecture 1. Under mild conditions, if language
L(A) of a DAG automaton A has unbounded cut-
width, there is no w that makes (A,w) probabilis-
tic with full support.

4 Planar DAG Automata

The fundamental problem with trying to assign
probabilities to non-planar DAG automata is the
factorial growth in the number of DAGs with re-
spect to the number of nodes. Does this problem
occur in planar DAG automata?

Planar DAG automata are similar to the DAG
automata of Section 2 but with an important differ-
ence: they transition between ordered sequences
of states rather than unordered multisets of states.
We write these sequences in parentheses, and their
order matters: (p, q) differs from (q, p). We write
� for the empty sequence. When a planar DAG
automaton generates DAGs, it keeps a strict order
over the set of frontier states at all times. A transi-
tion whose left-hand side is (p, q) can only be ap-
plied to adjacent states p and q in the frontier, with

6This is also a problem linguistically, since many of the
rewired graphs no longer model object control.

p preceding q. The matched states are replaced in
the frontier by the sequence of states in the transi-
tion’s right-hand side, maintaining order.

Example 6. Consider a planar DAG automaton
with the following transitions:

�
a−→ (p) (t′′1)

(p)
b−→ (p, q) (t′′2)

(p)
c−→ (p′) (t′′3)

(p′, q)
d−→ (p′) (t′′4)

(p′)
e−→ � (t′′5)

In the non-planar case, n applications of t2 can
generate n! unique DAGs, but n applications of
the corresponding transition t′′2 in this automaton
can only generate one DAG. To see this, consider
the partially derived DAG on the left of Figure 7,
with its frontier drawn in order from left to right.
The p′ state can only combine with the q state im-
mediately to its right, and since dead-ends are not
allowed, the only possible choice is to apply t′′4
twice followed by t′′5 , so the DAG on the right is
the only possible completion of the derivation.

a

b

b

c
d d e

b

b

a

c

p

p

p

p′ p′ p′

q
qt′′4 , t

′′
4 , t
′′
5

p′

q
q

Figure 7: A partial derivation using the planar DAG
automaton of Example 6 (left; red edge labels denote
frontier states) and its only possible completion (right;
blue edge labels denote an accepting run).

This automaton is probabilistic when w(t′′1) =
w(t′′2) = 1/2, w(t

′′
3) = w(t

′′
4) = w(t

′′
5) = 1, and

indeed the argument in Theorem 1 does not apply
to planar automata since the number of applicable
transitions is linear in the size of the frontier. But
planar DAG automata have other problems that
make them unsuitable for modeling AMR.

The first problem is that there are natural lan-
guage constructions that naturally produce non-
planar DAGs in AMR. For example, consider the
sentence “Four contestants mixed, baked and ate
cake.” Its AMR, shown in Figure 8, is not pla-
nar because it has a K3,3 minor, and it is easy
to see from this example that any coordination of
three predicates sharing two arguments produces
this structure. In the first release of AMR, 117 out
of 12844 DAGs are non-planar.

The second problem is that planar DAG au-



910

and

bakemix eat

contestant
4 cake

OP1 OP2 OP3

ARG0

ARG1

ARG0 ARG1

ARG0
ARG1QUANTITY

Figure 8: An AMR for the sentence “Four contestants
mixed, baked and ate a cake”. As in Figure 3, contract-
ing the dotted edge yields a K3,3 minor, with one set
denoted by hollow blue nodes ( ), the other by dotted
red nodes ( ).

tomata model Type-0 string derivations by design
(Kamimura and Slutzki, 1981). This seems more
expressive than needed to model natural language
and means that many important decision problems
are undecidable—for example, emptiness, which
is decidable in polynomial time for non-planar
DAG automata (Chiang et al., 2018).

5 Conclusions

Table 2 summarises the properties of several dif-
ferent variants of DAG automata. It has been ar-
gued that all of these properties are desirable for
probabilistic models of meaning representations
(Drewes, 2017). Since none of the variants sup-
ports all properties, this suggests that no variant of
the DAG automaton is a good candidate for mod-
eling meaning representations. We believe other
formalisms may be more suitable, including sev-
eral subfamilies of hyperedge replacement gram-
mars (Drewes et al., 1997) that have recently been
proposed (Björklund et al., 2016; Matheja et al.,
2015; Gilroy et al., 2017).

non-planar planar
bounded degree yes no yes

roots 1 1+ 1 1+ 1
probabilistic no no no no ?

decidable yes yes yes yes no
regular paths no yes no yes no

Table 2: DAG automata variants and their properties.

Acknowledgements

This work was supported in part by the EP-
SRC Centre for Doctoral Training in Data Sci-
ence, funded by the UK Engineering and Physical

Sciences Research Council (grant EP/L016427/1)
and the University of Edinburgh. We thank
Esma Balkir, Nikolay Bogoychev, Shay Co-
hen, Marco Damonte, Federico Fancellu, Joana
Ribeiro, Nathan Schneider, Miloš Stanojević, Ida
Szubert, Clara Vania, and the anonymous review-
ers for helpful discussion of this work and com-
ments on previous drafts of the paper.

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186, Sofia, Bulgaria.

Martin Berglund, Henrik Björklund, and Frank
Drewes. 2017. Single-rooted dags in regular dag
languages: Parikh image and path languages. In
Proceedings of the 13th International Workshop on
Tree Adjoining Grammars and Related Formalisms,
pages 94–101, Umeå, Sweden.

Henrik Björklund, Frank Drewes, and Petter Ericson.
2016. Between a rock and a hard place - uniform
parsing for hyperedge replacement DAG grammars.
In Language and Automata Theory and Applica-
tions - 10th International Conference, LATA 2016,
Prague, Czech Republic, March 14-18, 2016, Pro-
ceedings, pages 521–532.

Johannes Blum and Frank Drewes. 2016. Properties
of regular DAG languages. In Language and Au-
tomata Theory and Applications - 10th International
Conference, LATA 2016, Prague, Czech Republic,
March 14-18, 2016, Proceedings, pages 427–438.

T.L. Booth and R.A. Thompson. 1973. Applying prob-
ability measures to abstract languages. IEEE Trans-
actions on Computers, 22(5):442–450.

David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
924–932, Sofia, Bulgaria.

David Chiang, Frank Drewes, Daniel Gildea, Adam
Lopez, and Giorgio Satta. 2018. Weighted DAG au-
tomata for semantic graphs. Computational linguis-
tics, 44(1).

Jean D’Alembert. 1768. Opuscules, volume V.

Frank Drewes. 2017. Dag automata for meaning rep-
resentation. In Proceedings of the 15th Meeting on
the Mathematics of Language, pages 88–99, Lon-
don, UK.



911

Frank Drewes, Hans-Jörg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph grammars.
In Grzegorz Rozenberg, editor, Handbook of Graph
Grammars and Computing by Graph Transforma-
tion, pages 95–162. World Scientific.

Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and
Jaime Carbonell. 2016. Generation from abstract
meaning representation using tree transducers. In
Proc. of NAACL-HLT, pages 731–739.

Sorcha Gilroy, Adam Lopez, Sebastian Maneth, and Pi-
jus Simonaitis. 2017. (Re)introducing regular graph
languages. In Proceedings of the 15th Meeting on
the Mathematics of Language (MoL 15), pages 100–
113.

Tsutomu Kamimura and Giora Slutzki. 1981. Parallel
and two-way automata on directed ordered acyclic
graphs. Information and Control, 49(1):10–51.

Marco Kuhlmann and Stephan Oepen. 2016. Squibs:
Towards a catalogue of linguistic graph banks. Com-
putational Linguistics, 42(4):819–827.

Christoph Matheja, Christina Jansen, and Thomas
Noll. 2015. Tree-Like Grammars and Separation
Logic, pages 90–108. Springer International Pub-
lishing, Cham.

Jonathan May, Kevin Knight, and Heiko Vogler. 2010.
Efficient inference through cascades of weighted
tree transducers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’10, pages 1058–1066.

Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2008. Speech recognition with weighted
finite-state transducers. In Larry Rabiner and Fred
Juang, editors, Handbook on Speech Processing and
Speech Communication, Part E: Speech recognition,
pages 69–88. Springer.

Rik van Noord and Johan Bos. 2017. Neural semantic
parsing by character-based translation: Experiments
with abstract meaning representations. Computa-
tional Linguistics in the Netherlands Journal, 7:93–
108.

Daniel Quernheim and Kevin Knight. 2012. Towards
probabilistic acceptors and transducers for feature
structures. In Proceedings of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, SSST-6 ’12, pages 76–85.

Klaus Wagner. 1937. Über eine eigenschaft der ebenen
komplexe. Mathematische Annalen, 114(1):570–
590.


