Robust Unsupervised and Semi-Supervised Methods in Natural Language Processing, pages 33–40,

Hissar, Bulgaria, 15 September 2011.

33

Topic Models with Logical Constraints on Words

Hayato Kobayashi

Hiromi Wakaki

Tomohiro Yamasaki

Masaru Suzuki

Research and Development Center,

Toshiba Corporation, Japan

{hayato.kobayashi, hiromi.wakaki,

tomohiro2.yamasaki, masaru1.suzuki}@toshiba.co.jp

Abstract

This paper describes a simple method
to achieve logical constraints on words
for topic models based on a recently de-
veloped topic modeling framework with
Dirichlet forest priors (LDA-DF). Log-
ical constraints mean logical expressions
of pairwise constraints, Must-links and
Cannot-Links, used in the literature of
constrained clustering. Our method can
not only cover the original constraints of
the existing work, but also allow us eas-
ily to add new customized constraints.
We discuss the validity of our method by
deﬁning its asymptotic behaviors. We ver-
ify the effectiveness of our method with
comparative studies on a synthetic corpus
and interactive topic analysis on a real cor-
pus.

1 Introduction

Topic models such as Latent Dirichlet Allocation
or LDA (Blei et al., 2003) are widely used to cap-
ture hidden topics in a corpus. When we have do-
main knowledge of a target corpus, incorporating
the knowledge into topic models would be useful
in a practical sense. Thus there have been many
studies of semi-supervised extensions of topic
models (Andrzejewski et al., 2007; Toutanova and
Johnson, 2008; Andrzejewski et al., 2009; An-
drzejewski and Zhu, 2009), although topic mod-
els are often regarded as unsupervised learning.
Recently, (Andrzejewski et al., 2009) developed
a novel topic modeling framework, LDA with
Dirichlet Forest priors (LDA-DF), which achieves
two links Must-Link (ML) and Cannot-Link (CL)
in the constrained clustering literature (Basu et al.,
2008). For given words A and B, ML(A, B) and
CL(A, B) are soft constraints that A and B must
appear in the same topic, and that A and B cannot
appear in the same topic, respectively.

Let us consider

topic analysis of a cor-
illustrative pur-
pus with movie reviews for
poses. We know that
two words ‘jackie’
(means Jackie Chan) and ‘kung-fu’ should ap-
pear in the same topic, while ‘dicaprio’ (means
Leonardo DiCaprio) and ‘kung-fu’ should not
appear in the same topic.
In this case, we
can add constraints ML(‘jackie’, ‘kung-fu’) and
CL(‘dicaprio’, ‘kung-fu’) to smoothly conduct
analysis. However, what
if there is a word
‘bruce’ (means Bruce Lee) in the corpus, and we
want to distinguish between ‘jackie’ and ‘bruce’?
Our full knowledge among ‘kung-fu’, ‘jackie’,
and ‘bruce’ should be (ML(‘kung-fu’, ‘jackie’) ∨
ML(‘kung-fu’, ‘bruce’)) ∧ CL(‘bruce’, ‘jackie’),
although the original framework does not allow
a disjunction (∨) of links. In this paper, we ad-
dress such logical expressions of links on LDA-DF
framework.

Combination between a probabilistic model and
logical knowledge expressions such as Markov
Logic Network (MLN) is recently getting a lot of
attention (Riedel and Meza-Ruiz, 2008; Yu et al.,
2008; Meza-Ruiz and Riedel, 2009; Yoshikawa
et al., 2009; Poon and Domingos, 2009), and our
work can be regarded as on this research line. At
least, to our knowledge, our method is the ﬁrst
one that can directly incorporate logical knowl-
edge into a prior for topic models without MLN.
This means the complexity of the inference in our
method is essentially the same as in the original
LDA-DF, despite that our method can broaden
knowledge expressions.

2 LDA with Dirichlet Forest Priors

We brieﬂy review LDA-DF. Let w := w1 . . . wn
be a corpus consisting of D documents, where n
is the total number of words in the documents. Let
di and zi be the document that includes the i-th
word wi and the hidden topic that is assigned to
wi, respectively. Let T be the number of topics.

34

As in LDA, we assume a probabilistic language
model that generates a corpus as a mixture of hid-
den topics and infer two parameters: a document-
topic probability θ that represents a mixture rate of
topics in each document, and a topic-word proba-
bility ϕ that represents an occurrence rate of words
in each topic. The model is deﬁned as

θdi
zi|θdi
q ∼ DirichletForest(β, η),
ϕzi
wi|zi, ϕzi

∼ Dirichlet(α),
∼ Multinomial(θdi),
∼ DirichletTree(q),
∼ Multinomial(ϕzi),

where α and (β, η) are hyper parameters for θ
and ϕ, respectively. The only difference between
LDA and LDA-DF is that ϕ is chosen not from the
Dirichlet distribution, but from the Dirichlet tree
distribution (Dennis III, 1991), which is a gener-
alization of the Dirichlet distribution. The Dirich-
let forest distribution assigns one tree to each topic
from a set of Dirichlet trees, into which we encode
domain knowledge. The trees assigned to topics z
are denoted as q.

In the framework, ML(A, B) is achieved by the
Dirichlet tree in Fig. 1(a), which equalizes the oc-
currence probabilities of A and B in a topic when
η is large. This tree generates probabilities with
Dirichlet(2β, β) and redistributes the probability
for “2β” with Dirichlet(ηβ, ηβ).

In the case of CLs, we use the following algo-

rithm.

1. Consider a undirected graph regarding words
as vertices and links CL(A, B) as edges be-
tween A and B.

2. Divide the graph into connected components.

3. Extract the maximal independent sets of each

component.

4. Create Dirichlet trees to raise the occurrence
probabilities of words corresponding to each
maximal independent set.

For examples,
the algorithm creates the two
trees in Fig. 1(b) for the constraint CL(A, B) ∧
CL(A, C). The constraint is achieved when η is
large, since words in each topic are chosen from
the distribution of either the left tree that zeros the
occurrence probability of A, or the right tree that
zeros those of B and C.

(a) ML(A, B)

(b) CL(A, B) ∧ CL(A, C)

Figure 1: Dirichlet trees for two constraints of (a)
ML(A, B) and (b) CL(A, B) ∧ CL(A, C).

Inference of ϕ and θ is achieved by alternately
sampling topic zi for each word wi and Dirichlet
tree qz for each topic z. Since the Dirichlet tree
distribution is conjugate to the multinomial distri-
bution, the sampling equation of zi is easily de-
rived like LDA as follows:

Iz(↑i)∏
p(zi = z | z−i, q, w) ∝
−i

(
γ(Cz(s↓i))

∑Cz(s)

z

+ n(Cz(s↓i))
γ(k)
z + n(k)−i,z

s

k

) ,

(n(di)−i,z + α)

where n(d)−i,z represents the number of words (ex-
cluding wi) assigning topic z in document d. n(k)−i,z
represents the number of words (excluding wi) as-
signing topic z in the subtree rooted at node k in
tree qz. Iz(↑ i) and Cz(s ↓ i) represents the set of
internal nodes and the immediate child of node s,
respectively, on the path from the root to leaf wi
in tree qz. Cz(s) represents the set of children of
node s in tree qz. γ(k)
represents a weight of the
edge to node k in tree qz. Additionally, we deﬁne

s∈S.

S
s :=
Sampling of tree qz is achieved by sequen-
tially sampling subtree q(r)
corresponding to the
z
r-th connected component by using the following
equation:

∑

∑

z

′ | z, q−z, q(−r)

p(q(r)

z = q

(∑Cz(s)
 Γ
z,r∏
(∑Cz(s)

I (q

k

′

)

s

Γ

z

)∏Cz(s)

(
)∏Cz(s)

)
,
, w) ∝ |Mr,q′|×
)
(
γ(k)
z + n(k)
z
γ(k)
z

γ(k)
z
z + n(k)
z )

Γ

Γ

k

(γ(k)

k

k

′

)

′

where I (q
z,r represents the set of internal nodes in
the subtree q
corresponding to the r-th connected
component for tree qz. |Mr,q′| represents the size
of the maximal independent set corresponding to
the subtree q

for r-th connected component.

′

After sufﬁciently sampling zi and qz, we can in-
fer posterior probabilities ˆϕ and ˆθ using the last

2β

β

ηβ

ηβ

A

B

C

β

2ηβ

β

β

A

B

C

2β

β

β

ηβ

C

B

A

35

sampled z and q, in a similar manner to the stan-
dard LDA as follows.

∑
Iz(↑w)∏

T
z′=1

(
n(d)
z + α
n(d)
z′ + α
(
∑Cz(s)
γ(Cz(s↓w))

)
)
+ n(Cz(s↓w))
γ(k)
z + n(k)
z

z

z

s

k

ˆθ(d)
z

=

ˆϕ(w)
z

=

3 Logical Constraints on Words

In this section, we address logical expressions of
two links using disjunctions (∨) and negations (¬),
as well as conjunctions (∧), e.g., ¬ML(A, B) ∨
ML(A, C). We denote it as (∧,∨,¬)-expressions.
Since each negation can be removed in a prepro-
cessing stage, we focus only on (∧,∨)-expressions.
Interpretation of negations is discussed in Sec. 3.4.
3.1 (∧,∨)-expressions of Links
We propose a simple method that simultaneously
achieves conjunctions and disjunctions of links,
where the existing method can only treat conjunc-
tions of links. The key observation is that any
Dirichlet trees constructed by MLs and CLs are
essentially based only on two primitives. One
is Ep(A, B) that equalizes the occurrence prob-
abilities of A and B in a topic as in Fig. 1(a),
and the other is Np(A) that zeros the occurrence
probability of A in a topic as in the left tree of
Fig. 1(b). The right tree of Fig. 1(b) is created by
Np(B) ∧ Np(C). Thus, we can substitute ML and
CL with Ep and Np as follows:

ML(A, B)
CL(A, B)

=

=

Ep(A, B)
Np(A) ∨ Np(B)

Using this substitution, we can compile a (∧, ∨)-
expression of links to the corresponding Dirichlet
trees with the following algorithm.

1. Substitute all links (ML and CL) with the cor-

responding primitives (Ep and Np).

2. Calculate the minimum DNF of the primi-

tives.

3. Construct Dirichlet trees corresponding to the

(monotone) monomials of the DNF.

Let us consider three words A = ‘kung-fu’, B =
‘jackie’, and C = ‘bruce’ in Sec. 1. We want to
constrain them with (ML(A, B) ∨ ML(A, C)) ∧

CL(B, C). In this case, the algorithm calculates
the minimum DNF of primitives as
(ML(A, B) ∨ ML(A, C)) ∧ CL(B, C)
= (Ep(A, B) ∨ Ep(A, C)) ∧ (Np(B) ∨ N p(C))
= (Ep(A, B) ∧ Np(B)) ∨ (Ep(A, B) ∧ Np(C))
∨ (Ep(A, C) ∧ Np(B)) ∨ (Ep(A, C) ∧ Np(C))

∗

and constructs four Dirichlet trees correspond-
ing to the four monomials Ep(A, B) ∧ Np(B),
Ep(A, B) ∧ Np(C), Ep(A, C) ∧ Np(B), and
Ep(A, C) ∧ Np(C) in the last equation.
Considering only (∧)-expressions of links, our
method is equivalent to the existing method in the
original framework in terms of an asymptotic be-
havior of Dirichlet trees. We deﬁne asymptotic
behavior as Asymptotic Topic Family (ATF) as fol-
lows.
Deﬁnition 1 (Asymptotic Topic Family). For any
(∧, ∨)-expression f of primitives and any set W of
words, we deﬁne the asymptotic topic family of f
with respect to W as a family f
calculated by the
following rules: Given (∧, ∨)-expressions f1 and
f2 of primitives and words A, B ∈ W,
(i) (f1 ∨ f2)
(ii) (f1 ∧ f2)
(iii) Ep

(A, B) := {∅,{A, B}} ⊗ 2
∗
(iv) Np
Here, notation ⊗ is deﬁned as X ⊗ Y := {x ∪
y | x ∈ X, y ∈ Y } for given two sets X and
Y . ATF expresses all combinations of words that
can occur in a topic when η is large. In the above
example, the ATF of its expression with respect to
W = {A, B, C} is calculated as
((ML(A, B) ∨ ML(A, C)) ∧ CL(B, C))
∗
(
= (Ep(A, B) ∨ Ep(A, C)) ∧ (Np(B) ∨ Np(C))

∗
:= f
1
∗
:= f
1

∗
2 ，
∗
2 ，

∪ f
∩ f

W−{A,B}

W−{A}

)

:= 2

(A)

．

∗

∗

∗

,

∗

{∅,{A, B}} ⊗ 2
W−{A,B}
(
)
∪{∅,{A, C}} ⊗ 2W−{A,C}

W−{B} ∪ 2
2

W−{C}

=

∩

= {∅,{B},{C},{A, B},{A, C}}.

As we expected, the ATF of the last equation in-
dicates such a constraint that either A and B or A
and C must appear in the same topic, and B and
C cannot appear in the same topic. Note that the

36

part of {B} satisﬁes ML(A, C) ∧ CL(B, C).
If
you want to remove {B} and {C}, you can use
exclusive disjunctions. For the sake of simplicity,
we omit descriptions about W when its instance is
arbitrary or obvious from now on.

X∈X :X⊆2W (

′} | w, w

∧
∨

∧
∪

′ ∈ W, w ̸= w

The next theorem gives the guarantee of asymp-
totic equivalency between our method and the ex-
isting method. Let MIS(G) be the set of max-
imal independent sets of graph G. We deﬁne
′}. We con-
L := {{w, w
sider CLs only, since the asymptotic equivalency
including MLs is obvious by identifying all ver-
tices connected by MLs.
Theorem 2. For any (∧)-expression of CLs rep-
{x,y}∈ℓ:ℓ⊆L CL(x, y), the ATF of the
resented by
corresponding minimum DNF of primitives repre-
x∈X Np(x)) is equiva-
sented by
)
lent to the union of the power sets of every max-
∪
imal independent set S ∈ MIS(G) of a graph
G := (W, ℓ), that is,
=
S∈MIS(G) 2S.
Proof. For any (∧)-expressions of links character-
∪
ized by ℓ ⊆ L, we denote fℓ and Gℓ as the corre-
sponding minimum DNF and graph, respectively.
S∈MIS(Gℓ) 2S. When |ℓ| = 1,
We deﬁne Uℓ :=
ℓ = Uℓ is trivial. Assuming f
ℓ = Uℓ when
∗
∗
f
|ℓ| > 1, for any set ℓ
:= ℓ ∪ {{A, B}} with an
′
additional link characterized by {A, B} ∈ L, we
obtain
ℓ′ = ((Np(A) ∨ Np(B)) ∧ fℓ)
∗
∗
) ∩ Uℓ

(
W−{A} ∪ 2

∗
x∈X Np

(∩

W−{B}

)

= (2

X∈X

(x)

f

∪
W−{A} ∩ 2S)
∪
(2
W−{B} ∩ 2S)
∪(2
S∈MIS(Gℓ)
∪
S∈MIS(Gℓ) (2S−{A} ∪ 2S−{B}
)
S∈MIS(Gℓ′ ) 2S = Uℓ′
∪

This proves the theorem by induction.
last

∪
In the
∪
line of the above deformation, we used
S∈IS(G) 2S and MIS(Gℓ′) ⊆
S∈MIS(G) 2S =
S∈MIS(Gℓ)((S − {A}) ∪ (S − {B})) ⊆ IS(Gℓ′),
)

where IS(G) represents the set of all independent
sets on graph G.

(∩

∪

In the above theorem,

(x)
represents asymptotic behaviors of our method,
S∈MIS(G) 2S represents those of the exist-
while
ing method. By using a similar argument to the
proof, we can prove the elements of the two sets
are completely the same, i.e.,
(x) =

∩

x∈X Np

∪

∗
x∈X Np

X∈X

∗

=

=

=

{2S | S ∈ MIS(G)}. This interestingly means
that for any logical expression characterized by
CLs, calculating its minimum DNF is the same
as calculating the maximal independent sets of the
corresponding graph, or the maximal cliques of its
complement graph.

∗
1 = f

3.2 Shrinking Dirichlet Forests
Focusing on asymptotic behaviors, we can reduce
the number of Dirichlet trees, which means the
performance improvement of Gibbs sampling for
Dirichlet trees. This is achieved just by minimiz-
ing DNF on asymptotic equivalence relation de-
ﬁned as follows.
Deﬁnition 3 (Asymptotic Equivalence Relation).
Given two (∧, ∨)-expressions f1, f2, we say that
f1 is asymptotically equivalent to f2, if and only
2 . We denote the relation as notation ≍,
∗
if f
that is, f1 ≍ f2 ⇔ f
The next proposition gives an intuitive under-
standing of why asymptotic equivalence relation
can shrink Dirichlet forests.
Proposition 4. For any two words A, B ∈ W,
(a) Ep(A, B) ∨ (Np(A) ∧ Np(B)) ≍ Ep(A, B)，
(b) Ep(A, B) ∧ Np(A) ≍ Np(A) ∧ Np(B)．
Proof. We prove (a) only.
(A) ∩ Np
∗

∗
1 = f

∗
2 .

Ep

(A, B) ∪ (Np
∗
∗
= {∅,{A, B}} ⊗ 2
W−{A} ∩ 2

∪ (2

(B))
W−{A,B}
W−{B}

)

= ({∅,{A, B}} ∪ ({∅,{B}} ∩ {∅,{A}}))

⊗ 2

W−{A,B}

= {∅,{A, B}} ⊗ 2

W−{A,B}

∗

= Ep

(A, B)

In the above proposition, Eq. (a) directly re-
duces the number of Dirichlet trees since a dis-
junction (∨) disappears, while Eq. (b) indirectly
reduces since (Np(A) ∧ Np(B)) ∨ Np(B) =
Np(B).

We conduct an experiment to clarify how many
trees can be reduced by asymptotic equivalency. In
the experiment, we prepare conjunctions of ran-
dom links of MLs and CLs when |W| = 10,
and compare the average numbers of Dirichlet
trees compiled by minimum DNF (M-DNF) and
asymptotic minimum DNF (AM-DNF) in 100 tri-
als. The experimental result shown in Tab. 1

37

Table 1: The average numbers of Dirichlet
trees compiled by minimum DNF (M-DNF) and
asymptotic minimum DNF (AM-DNF) in terms of
the number of random links. Each value is the av-
erage of 100 trials.

# of links
M-DNF
AM-DNF

1
1
1

2

2.08
2.08

4

3.43
3.23

8

6.18
4.24

16

10.35
4.07

indicates that asymptotic equivalency effectively
reduces the number of Dirichlet trees especially
when the number of links is large.

3.3 Customizing New Links
Two primitives Ep and Np allow us to easily cus-
tomize new links without changing the algorithm.
Let us consider Imply-Link(A, B) or IL(A, B),
which is a constraint that B must appear if A ap-
pears in a topic (informally, A → B). In this case,
the setting

IL(A, B) = Ep(A, B) ∨ Np(A)

is acceptable, since the ATF of IL(A, B) with
respect to W = {A, B} is {∅,{A, B},{B}}.
IL(A, B) is effective when B has multiple mean-
ings as mentioned later in Sec. 4.
Informally regarding IL(A, B) as A → B and
ML(A, B) as A ⇔ B, ML(A, B) seems to be the
same meaning of IL(A, B) ∧ IL(B, A). However,
this anticipation is wrong on the normal equiv-
alency, i.e., ML(A, B) ̸= IL(A, B) ∧ IL(B, A).
The asymptotic equivalency can fulﬁll the antici-
pation with the next proposition. This simultane-
ously suggests that our deﬁnition is semantically
valid.
Proposition 5. For any two words A, B ∈ W,
IL(A, B) ∧ IL(B, A) ≍ ML(A, B)

Proof. From Proposition 4,
IL(A, B) ∧ IL(B, A)
= (Ep(A, B) ∨ Np(A)) ∧ (Ep(B, A) ∨ Np(B))
= Ep(A, B) ∨ (Ep(A, B) ∧ Np(A))

∨ (Ep(A, B) ∧ Np(B)) ∨ (Np(A) ∧ Np(B))

≍ Ep(A, B) ∨ (Np(A) ∧ Np(B))
≍ Ep(A, B) = ML(A, B)

Further, we can construct XIL(X1,··· , Xn, Y )
∧
as an extended version of IL(A, B), which allows
us to use multiple conditions like Horn clauses.
i=1 Xi → Y as an ex-
This informally means
n∧
tension of A → B. In this case, we set
XIL(X1,··· , Xn, Y ) =

Ep(Xi, Y )∨ n∨

n

Np(Xi).

i=1

i=1

When we want to isolate unnecessary words
(i.e., stop words), we can use Isolate-Link (ISL)
deﬁned as

ISL(X1,··· , Xn) =

n∧

Np(Xi).

i=1

This is easier than considering CLs between high-
frequency words and unnecessary words as de-
scribed in (Andrzejewski et al., 2009).

∗

3.4 Negation of Links
There are two types of interpretation for negation
of links. One is strong negation, which regards
¬ML(A, B) as “A and B must not appear in the
same topic”, and the other is weak negation, which
regards it as “A and B need not appear in the same
topic”. We set ¬ML(A, B) ≍ CL(A, B) for strong
negation, while we just remove ¬ML(A, B) for
weak negation. We consider the strong negation
in this study.
According to Def. 1, the ATF of the negation ¬f
of primitive f seems to be deﬁned as (¬f )
:=
W − f
. However, this deﬁnition is not ﬁt in
2
strong negation, since ¬ML(A, B) ̸≍ CL(A, B)
on the deﬁnition. Thus we deﬁne it to be ﬁt in
strong negation as follows.
Deﬁnition 6 (ATF of strong negation of links).
Given a link L with arguments X1,··· , Xn, let-
ting fL be the primitives of L, we deﬁne the ATF
of the negation of L as (¬L(X1,··· , Xn))
∗
:=
W − f
W−{X1,··· ,Xn}
.
(2
Note that the deﬁnition is used not for primi-
tives but for links. Actually, the similar deﬁnition
for primitives is not ﬁt in strong negation, and so
we must remove all negations in a preprocessing
stage.

L(X1,··· , Xn)) ∪ 2
∗

∗

The next proposition gives the way to remove
the negation of each link treated in this study. We
deﬁne no constraint condition as ϵ for the result of
ISL.
Proposition 7. For any words A, B, X1,··· , Xn,
Y ∈ W,

38

(a) ¬ML(A, B) ≍ CL(A, B)，
(b) ¬CL(A, B) ≍ ML(A, B)，
(c) ¬IL(A, B) ≍ Np(B)，
(d) ¬XIL(X1,··· , Xn, Y )

≍∧

i=1 Ep(Xi, Xn) ∧ Np(Y )，
n−1

(e) ¬ISL(X1,··· , Xn) ≍ ϵ．
Proof. We prove (a) only.
(¬ML(A, B))
∗
W − Ep
= (2
{A,B} − {∅,{A, B}}) ⊗ 2

(A, B)) ∪ 2

= (2

∗

W−{A,B}

W−{A,B}

∪ 2

W−{A,B}
= {∅,{A},{B}} ⊗ 2
W−{A} ∪ 2
W−{B}
= 2
(A) ∪ Np
∗
∗
= Np

W−{A,B}

(B) = (CL(A, B))

∗

4 Comparison on a Synthetic Corpus

a

using

synthetic

experiment

We
corpus
{ABAB, ACAC} × 2 with
vocabulary
W = {A, B, C} to clarify the property of
our method in the same way as in the existing
work (Andrzejewski et al., 2009). We set topic
size as T = 2. The goal of this experiment is
to obtain two topics: a topic where A and B
frequently occur and a topic where A and C
frequently occur. We abbreviate the grouping type
as AB|AC.
In preliminary experiments, LDA
yielded almost four grouping types: AB|AC,
AB|C, AC|B, and A|BC. Thus, we naively
classify a grouping type of each result into the
four types. Concretely speaking, for any two
′
topic-word probabilities ˆϕ and ˆϕ
, we calculate
the average of Euclidian distances between each
vector component of ˆϕ and the corresponding one
′
of ˆϕ
, ignoring the difference of topic labels, and
regard them as the same type if the average is less
than 0.1.

Fig. 2 shows the occurrence rates of grouping
types on 1,000 results after 1,000 iterations by
LDA-DF with six constraints (1) no constraint,
(2) ML(A, B), (3) CL(B, C), (4) ML(A, B) ∧
CL(B, C), (5) IL(B, A), and (6) ML(A, B) ∨
ML(A, C).
In the experiment, we set α = 1,
β = 0.01, and η = 100. In the ﬁgure, the higher
rate of the objective type AB|AC (open bar) is

Figure 2: Rates of Grouping types in the 1,000
results on synthetic corpus {ABAB, ACAC} ×
2 with six constraints:
(1) no constraint, (2)
(4) ML(A, B) ∧
ML(A, B),
CL(B, C), (5) IL(B, A), and (6) ML(A, B) ∨
ML(A, C).

(3) CL(B, C),

better. The results of (1-4) can be achieved even
by the existing method, and those of (5-6) can be
achieved only by our method. Roughly speaking,
the ﬁgure shows that our method is clearly better
than the existing method, since our method can ob-
tain almost 100% as the rate of AB|AC, which is
the best of all results, while the existing methods
can only obtain about 60%, which is the best of
the results of (1-4).

In the result,

The result of (1) is the same result as LDA,
the
because of no constraints.
rate of AB|AC is only about 50%, since each
of AB|C, AC|B, and A|BC remains at a high
15%. As we expected, the result of (2) shows that
ML(A, B) cannot remove AB|C although it can
remove AC|B and A|BC, while the result of (3)
shows that CL(B, C) cannot remove AB|C and
AC|B although it can remove A|BC. The re-
sult of (4) indicates that ML(A, B) ∧ CL(B, C)
is the best of knowledge expressions in the exist-
ing method. Note that ML(A, B) ∧ ML(A, C) im-
plies ML(B, C) by transitive law and is inconsis-
tent with all of the four types. The result (80%)
of (5) IL(B, A) is interestingly better than that
(60%) of (4), despite that (5) has less primitives
than (4). The reason is that (5) allows A to ap-
pear with C, while (4) does not. In the result of
(6) ML(A, B)∨ML(A, C), the constraint achieves
almost 100%, which is the best of knowledge ex-
pressions in our method. Of course, the constraint
of (ML(A, B) ∨ ML(A, C)) ∧ CL(B, C) can also
achieve almost 100%.

5 Interactive Topic Analysis

We demonstrate advantages of our method via in-
teractive topic analysis on a real corpus, which

39

consists of stemmed, down-cased 1,000 (positive)
movie reviews used in (Pang and Lee, 2004). In
this experiment, the parameters are set as α = 1,
β = 0.01, η = 1000, and T = 20.

We ﬁrst

ran LDA-DF with 1,000 itera-
tions without any constraints and noticed that
most topics have stop words (e.g., ‘have’ and
‘not’) and corpus-speciﬁc, unnecessary words
‘ﬁlm’,
(e.g.,
‘movie’), as in the ﬁrst block
in Tab. 2.
To remove them, we added
ISL(‘ﬁlm’, ‘movie’, ‘have’, ‘not’, ‘n’t’) to the con-
straint of LDA-DF, which is compiled to one
Dirichlet tree. After the second run of LDA-DF
with the isolate-link, we speciﬁed most topics such
as Comedy, Disney, and Family, since cumber-
some words are isolated, and so we noticed that
two topics about Star Wars and Star Trek are
merged, as in the second block. Each topic la-
bel is determined by looking carefully at high-
frequency words in the topic. To split the merged
two topics, we added CL(‘jedi’, ‘trek’) to the con-
straint, which is compiled to two Dirichlet trees.
However, after the third run of LDA-DF, we no-
ticed that there is no topic only about Star Trek,
since ‘star’ appears only in the Star Wars topic,
as in the third block. Note that the topic includ-
ing ‘trek’ had other topics such as a topic about
comedy ﬁlm Big Lebowski. We ﬁnally added
ML(‘star’, ‘jedi’) ∨ ML(‘star’, ‘trek’) to the con-
straint, which is compiled to four Dirichlet trees,
to split the two topics considering polysemy of
‘star’. After the fourth run of LDA-DF, we appro-
priately obtained two topics about Star Wars and
Star Trek as in the fourth block. Note that our so-
lution is not ad-hoc, and we can easily apply it to
similar problems.

6 Conclusions

We proposed a simple method to achieve topic
models with logical constraints on words. Our
method compiles a given constraint to the prior
of LDA-DF, which is a recently developed semi-
supervised extension of LDA with Dirichlet forest
priors. As well as covering the constraints in the
original LDA-DF, our method allows us to con-
struct new customized constraints without chang-
ing the algorithm. We proved that our method is
asymptotically the same as the existing method for
any constraints with conjunctive expressions, and
showed that asymptotic equivalency can shrink a
constructed Dirichlet forest.
In the comparative

star war trek planet effect special

Table 2: Characteristic topics obtained in the ex-
periment on the real corpus. Four blocks in the
table corresponds to the results of the four con-
straints ϵ, ISL(··· ), CL(‘jedi’, ‘trek’) ∧ ISL(··· ),
and (ML(‘jedi’, ‘trek’) ∨ ML(‘star’, ‘trek’)) ∧
CL(‘jedi’, ‘trek’) ∧ ISL(··· ), respectively.
High frequency words in each topic
Topic
have give night ﬁlm turn performance
?
not life have own ﬁrst only family tell
?
movie have n’t get good not see
?
have black scene tom death die joe
?
ﬁlm have n’t not make out well see
?
Isolated have ﬁlm movie not good make n’t
?
Comedy comedy funny laugh school hilarious
disney voice mulan animated song
Disney
Family
life love family mother woman father
Isolated have ﬁlm movie not make good n’t
StarWars star war lucas effect jedi special
?
Comedy funny comedy laugh get hilarious
Disney
disney truman voice toy show
Family
family father mother boy child son
Isolated have ﬁlm movie not make good n’t
StarWars star war toy jedi menace phantom
StarTrek alien effect star science special trek
Comedy comedy funny laugh hilarious joke
Disney
Family

disney voice animated mulan
life love family man story child

science world trek ﬁction lebowski

study on a synthetic corpus, we clariﬁed the prop-
erty of our method, and in the interactive topic
analysis on a movie review corpus, we demon-
strated its effectiveness. In the future, we intend to
address detail comparative studies on real corpora
and consider a simple method integrating nega-
tions into a whole, although we removed them in
a preprocessing stage in this study.

References
David Andrzejewski and Xiaojin Zhu. 2009. Latent
Dirichlet Allocation with Topic-in-Set Knowledge.
In Proceedings of the NAACL HLT 2009 Workshop
on Semi-Supervised Learning for Natural Language
Processing, pages 43–48.

David Andrzejewski, Anne Mulhern, Ben Liblit, and
Xiaojin Zhu. 2007. Statistical Debugging Using
Latent Topic Models.
In Proceedings of the 18th
European Conference on Machine Learning (ECML
2007), pages 6–17. Springer-Verlag.

40

Xiaofeng Yu, Wai Lam, and Shing-Kit Chan. 2008. A
Framework Based on Graphical Models with Logic
for Chinese Named Entity Recognition.
In Pro-
ceedings of the 3rd International Joint Conference
on Natural Language Processing (IJCNLP 2008),
pages 335–342.

David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009.
Incorporating domain knowledge into topic
modeling via Dirichlet Forest priors. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning (ICML 2009), pages 25–32. ACM.

Sugato Basu, Ian Davidson, and Kiri Wagstaff. 2008.
Constrained Clustering: Advances in Algorithms,
Theory, and Applications. Chapman & Hall/CRC,
1 edition.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Samuel Y. Dennis III. 1991. On the hyper-Dirichlet
type 1 and hyper-Liouville distributions. Com-
munications in Statistics — Theory and Methods,
20(12):4069–4081.

Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
Identifying Predicates, Arguments and Senses us-
ing Markov Logic. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2009),
pages 155–163. Association for Computational Lin-
guistics.

Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Sum-
marization Based on Minimum Cuts.
In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics (ACL 2004), pages 271–
278.

Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised Semantic Parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2009), pages 1–10. As-
sociation for Computational Linguistics.

Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collec-
tive Semantic Role Labelling with Markov Logic.
In Proceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL 2008),
pages 193–197. Association for Computational Lin-
guistics.

Kristina Toutanova and Mark Johnson.

2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging.
In Advances in Neural In-
formation Processing Systems 20, pages 1521–1528.
MIT Press.

Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly Identi-
fying Temporal Relations with Markov Logic.
In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing
(ACL-IJCNLP 2009), pages 405–413. Association
for Computational Linguistics.

