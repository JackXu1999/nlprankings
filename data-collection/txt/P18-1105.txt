




































Taylor's law for Human Linguistic Sequences


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1138–1148
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1138

Taylor’s Law for Human Linguistic Sequences

Tatsuru Kobayashi∗
Graduate School of

Information Science and Technology,
University of Tokyo

7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656

Japan

Kumiko Tanaka-Ishii†
Research Center for

Advanced Science and Technology,
University of Tokyo

4-6-1 Komaba, Meguro-ku
Tokyo 153-8904

Japan

Abstract

Taylor’s law describes the fluctuation char-
acteristics underlying a system in which
the variance of an event within a time
span grows by a power law with respect
to the mean. Although Taylor’s law has
been applied in many natural and social
systems, its application for language has
been scarce. This article describes a new
quantification of Taylor’s law in natural
language and reports an analysis of over
1100 texts across 14 languages. The Tay-
lor exponents of written natural language
texts were found to exhibit almost the
same value. The exponent was also com-
pared for other language-related data, such
as the child-directed speech, music, and
programming language code. The results
show how the Taylor exponent serves to
quantify the fundamental structural com-
plexity underlying linguistic time series.
The article also shows the applicability of
these findings in evaluating language mod-
els.

1 Introduction

Taylor’s law characterizes how the variance of the
number of events for a given time and space grows
with respect to the mean, forming a power law. It
is a quantification method for the clustering behav-
ior of a system. Since the pioneering studies of this
concept (Smith, 1938; Taylor, 1961), a substan-
tial number of studies have been conducted across
various domains, including ecology, life science,
physics, finance, and human dynamics, as well
summarized in (Eisler, Bartos, and Kertész, 2007).

∗kobayashi@cl.rcast.u-tokyo.ac.jp
†kumiko@cl.rcast.u-tokyo.ac.jp

More recently, Cohen and Xu (2015) reported Tay-
lor exponents for random sampling from various
distributions, and Calif and Schmitt (2015) re-
ported Taylor’s law in wind energy data using a
non-parametric regression. Those two papers also
refer to research about Taylor’s law in a wide range
of fields.

Despite such diverse application across do-
mains, there has been little analysis based on Tay-
lor’s law in studying natural language. The only
such report, to the best of our knowledge, is Ger-
lach and Altmann (2014), but they measured the
mean and variance by means of the vocabulary
size within a document. This approach essen-
tially differs from the original concept of Taylor
analysis, which fundamentally counts the number
of events, and thus the theoretical background of
Taylor’s law as presented in Eisler, Bartos, and
Kertész (2007) cannot be applied to interpret the
results.

For the work described in this article, we ap-
plied Taylor’s law for texts, in a manner close
to the original concept. We considered lexical
fluctuation within texts, which involves the co-
occurrence and burstiness of word alignment. The
results can thus be interpreted according to the an-
alytical results of Taylor’s law, as described later.
We found that the Taylor exponent is indeed a
characteristic of texts and is universal across vari-
ous kinds of texts and languages. These results are
shown here for data including over 1100 single-
author texts across 14 languages and large-scale
newspaper data.

Moreover, we found that the Taylor expo-
nents for other symbolic sequential data, includ-
ing child-directed speech, programming language
code, and music, differ from those for written nat-
ural language texts, thus distinguishing different
kinds of data sources. The Taylor exponent in this
sense could categorize and quantify the structural



1139

complexity of language. The Chomsky hierarchy
(Chomsky, 1956) is, of course, the most important
framework for such categorization. The Taylor ex-
ponent is another way to quantify the complexity
of natural language: it allows for continuous quan-
tification based on lexical fluctuation.

Since the Taylor exponent can quantify and
characterize one aspect of natural language, our
findings are applicable in computational linguis-
tics to assess language models. At the end of
this article, in §5, we report how the most basic
character-based long short-term memory (LSTM)
unit produces texts with a Taylor exponent of 0.50,
equal to that of a sequence of independent and
identically distributed random variables (an i.i.d.
sequence). This shows how such models are lim-
ited in producing consistent co-occurrence among
words, as compared with a real text. Taylor analy-
sis thus provides a possible direction to reconsider
the limitations of language models.

2 Related Work

This work can be situated as a study to quan-
tify the complexity underlying texts. As sum-
marized in (Tanaka-Ishii and Aihara, 2015), mea-
sures for this purpose include the entropy rate
(Takahira, Tanaka-Ishii, and Lukasz, 2016; Bentz
et al., 2017) and those related to the scaling behav-
iors of natural language. Regarding the latter, cer-
tain power laws are known to hold universally in
linguistic data. The most famous among these are
Zipf’s law (Zipf, 1965) and Heaps’ law (Heaps,
1978). Other, different kinds of power laws from
Zipf’s law are obtained through various methods
of fluctuation analysis, but the question of how to
quantify the fluctuation existing in language data
has been controversial. Our work is situated as
one such case of fluctuation analysis.

In real data, the occurrence timing of a particu-
lar event is often biased in a bursty, clustered man-
ner, and fluctuation analysis quantifies the degree
of this bias. Originally, this was motivated by a
study of how floods of the Nile River occur in
clusters (i.e., many floods coming after an initial
flood) (Hurst, 1951). Such clustering phenomena
have been widely reported in both natural and so-
cial domains (Eisler, Bartos, and Kertész, 2007).

Fluctuation analysis for language originates in
(Ebeling and Pöeschel, 1994), which applied the
approach to characters. That work corresponds
to observing the average of the variances of each

character’s number of occurrences within a time
span. Their method is strongly related to ours but
different from two viewpoints: (1) Taylor analysis
considers the variance with respect to the mean,
rather than time; and (2) Taylor analysis does not
average results over all elements. Because of these
differences, the method in (Ebeling and Pöeschel,
1994) cannot distinguish real texts from an i.i.d.
process when applied to word sequences (Taka-
hashi and Tanaka-Ishii, 2018).

Event clustering phenomena cause a sequence
to resemble itself in a self-similar manner. There-
fore, studies of the fluctuation underlying a se-
quence can take another form of long-range corre-
lation analysis, to consider the similarity between
two subsequences underlying a time series. This
approach requires a function to calculate the sim-
ilarity of two sequences, and the autocorrelation
function (ACF) is the main function considered.
Since the ACF only applies to numerical data, both
Altmann, Pierrehumbert, and Motter (2009) and
Tanaka-Ishii and Bunde (2016) applied long-range
correlation analysis by transforming text into in-
tervals and showed how natural language texts are
long-range correlated. Another recent work (Lin
and Tegmark, 2016) proposed using mutual infor-
mation instead of the ACF. Mutual information,
however, cannot detect the long-range correlation
underlying texts. All these works studied correla-
tion phenomena via only a few texts and did not
show any underlying universality with respect to
data and language types. One reason is that anal-
ysis methods for long-range correlation are non-
trivial to apply to texts.

Overall, the analysis based on Taylor’s law in
the present work belongs to the former approach
of fluctuation analysis and shows the law’s vast ap-
plicability and stability for written texts and even
beyond, quantifying universal complexity under-
lying human linguistic sequences.

3 Measuring the Taylor Exponent

3.1 Proposed method

Given a set of elements W (words), let X =
X1, X2, . . . , XN be a discrete time series of length
N , where Xi ∈ W for all i = 1, 2, . . . , N , i.e.,
each Xi represents a word. For a given segment
length ∆t ∈ N (a positive integer), a data sample
X is segmented by the length ∆t. The number of
occurrences of a specific word wk ∈ W is counted
for every segment, and the mean µk and standard



1140

deviation σk across segments are obtained. Doing
this for all word kinds w1, . . . , w|W | ∈ W gives
the distribution of σ with respect to µ. Following a
previous work (Eisler, Bartos, and Kertész, 2007),
in this article Taylor’s law is defined to hold when
µ and σ are correlated by a power law in the fol-
lowing way:

σ ∝ µα. (1)

Experimentally, the Taylor exponent α is known
to take a value within the range of 0.5 ≤ α ≤
1.0 across a wide variety of domains as reported
in (Eisler, Bartos, and Kertész, 2007), includ-
ing finance, meteorology, agriculture, and biol-
ogy. Mathematically, it is analytically proven that
α = 0.5 for an i.i.d process, and the proof is in-
cluded as Supplementary Material.

On the other hand, α = 1.0 when all segments
always contain the same proportion of the ele-
ments of W . For example, suppose that W =
{a, b}. If b always occurs twice as often as a in all
segments (e.g., three a and six b in one segment,
two a and four b in another, etc.), then both the
mean and standard deviation for b are twice those
for a, so the exponent is 1.0.

In a real text, this cannot occur for all W ,
so α < 1.0 for natural language text. Never-
theless, for a subset of words in W , this could
happen, especially for a template-like sequence.
For instance, consider a programming statement:
while (i < 1000) do i-. Here, the words
while and do always occur once, whereas i al-
ways occurs twice. This example shows that the
exponent indicates how consistently words depend
on each other in W , i.e., how words co-occur sys-
tematically in a coherent manner, thus indicating
that the Taylor exponent is partly related to gram-
maticality.

To measure the Taylor exponent α, the mean
and standard deviation are computed for every
word kind1 and then plotted in log-log coordi-
nates. The number of points in this work was the
number of different words. We fitted the points
to a linear function in log-log coordinates by the
least-squares method. We naturally took the loga-
rithm of both cµα and σ to estimate the exponent,
because Taylor’s law is a power law. The coeffi-
cient ĉ, and exponent α̂ are then estimated as the

1 In this work, words are not lemmatized, e.g. “say,”
“said,” and “says” are all considered different words. This
was chosen so in this work because the Taylor exponent
considers systematic co-occurrence of words, and idiomatic
phrases should thus be considered in their original forms.

following:

ĉ, α̂ = arg min
c,α

ϵ(c, α),

ϵ(c, α) =

√√√√ 1
|W |

|W |∑
k=1

(log σk − log cµαk )2.

This fit function could be a problem depending on
the distribution of errors between the data points
and the regression line. As seen later, the er-
ror distribution seems to differ with the kind of
data: for a random source the error seems Gaus-
sian, and so the above formula is relevant, whereas
for real data, the distribution is biased. Chang-
ing the fit function according to the data source,
however, would cause other essential problems for
fair comparison. Here, because Cohen and Xu
(2015) reported that most empirical works on Tay-
lor’s law used least-squares regression (including
their own), this work also uses the above scheme2,
with the error defined as ϵ(ĉ, α̂).

3.2 Data
Table 1 lists all the data used for this article. The
data consisted of natural language texts, language-
related sequences, and randomized data, listed as
different blocks in the table. The natural lan-
guage texts consisted of 1142 single-author long
texts (first block, extracted from Project Guten-
berg and Aozora Bunko across 14 languages3,
with the second block listing individual sam-
ples taken from Project Gutenberg together with
the complete works of Shakespeare), and news-
papers (third block, from the Gigaword corpus,
available from the Linguistic Data Consortium
in English, Chinese, and other major languages).
Other sequences appear in the fourth block: the
enwiki8 100-MB dump dataset (consisting of
tag-annotated text from English Wikipedia), the
10 longest child-directed speech utterances in
CHILDES data4 (preprocessed by extracting only
children’s utterances), four program sources (in
Lisp, Haskell, C++, and Python, crawled from

2The code for estimating the exponent is available from
https://github.com/Group-TanakaIshii/
word_taylor.

3All texts above a size threshold (1 megabyte) were ex-
tracted from the two archives, resulting in 1142 texts.

4Child Language Data Exchange System (MacWhinney,
2000; Bol, 1995; Lieven, Salomo, and Tomasello, 2009; Ron-
dal, 1985; Behrens, 2006; Gil and Tadmor, 2007; Oshima-
Takane et al., 1995; Smoczynska, 1985; And̄elković, Ševa,
and Moskovljević, 2001; Benedet et al., 2004; Plunkett and
Strömqvist, 1992)

https://github.com/Group-TanakaIshii/word_taylor
https://github.com/Group-TanakaIshii/word_taylor


1141

Table 1: Data we used in this article. For each dataset, length is the number of words, vocabulary is the
number of different words. For detail of the data kind, see §3.2.

Texts Language α̂ Number Length Vocabulary

mean of samples Mean Min Max Mean Min Max

English 0.58 910 313127.4 185939 2488933 17237.7 7321 69812

French 0.57 66 197519.3 169415 1528177 22098.3 14106 57193

Finnish 0.55 33 197519.3 149488 396920 33597.1 26275 47263

Chinese 0.61 32 629916.8 315099 4145117 15352.9 9153 60950

Dutch 0.57 27 256859.2 198924 435683 19159.1 13880 31595

German 0.59 20 236175.0 184321 331322 24242.3 11079 37228

Gutenberg Italian 0.57 14 266809.0 196961 369326 29103.5 18641 45032

Spanish 0.58 12 363837.2 219787 903051 26111.1 18111 36507

Greek 0.58 10 159969.2 119196 243953 22805.7 15877 31386

Latin 0.57 2 505743.5 205228 806259 59667.5 28739 90596

Portuguese 0.56 1 261382.0 261382 261382 24719.0 24719 24719

Hungarian 0.57 1 198303.0 198303 198303 38384.0 38384 38384

Tagalog 0.59 1 208455.0 208455 208455 26335.0 26335 26335

Aozora Japanese 0.59 13 616677.2 105343 2951320 19760.0 6620 49100

Moby Dick English 0.58 1 254655.0 254655 254655 20473.0 20473 20473

Hong Lou Meng Chinese 0.59 1 701256.0 701256 701256 18451.0 18451 18451

Les Miserables French 0.57 1 691407.0 690417 690417 31956.0 31956 31956

Shakespeare (All) English 0.59 1 1000238.0 1000238 1000238 40840.0 40840 40840

WSJ English 0.56 1 22679513.0 22679513 22679513 137467.0 137467 137467

NYT English 0.58 1 1528137194.0 1528137194 1528137194 3155495.0 3155495 3155495

People’s Daily Chinese 0.58 1 19420853.0 19420853 19420853 172140.0 172140 172140

Mainichi Japanese 0.56 24 (yrs) 31321594.3 24483331 40270706 145534.5 127290 169270

enwiki8 tag-annotated 0.63 1 14647848.0 14647848 14647848 1430791.0 1430791 1430791

CHILDES various 0.68 10 193434.0 48952 448772 9908.0 5619 17893

Programs - 0.79 4 34161018.8 3697199 68622162 838907.8 127653 1545127

Music - 0.79 12 135993.4 76629 215480 9187.9 907 27043

Moby Dick (shuffled) - 0.50 10 254655.0 254655 254655 20473.0 20473 20473

Moby Dick (bigram) - 0.50 10 300001.0 300001 300001 16963.8 16893 17056

3-layer stacked LSTM
(English) 0.50 1 256425.0 256425 256425 50115.0 50115 50115

(character-based)

Neural MT (English) 0.57 1 623235.0 623235 623235 27370.0 27370 27370

large representative archives, parsed, and stripped
of natural language comments), and 12 pieces of
musical data (long symphonies and so forth, trans-
formed from MIDI into text with the software
SMF2MML5, with annotations removed).

As for the randomized data listed in the last
block, we took the text of Moby Dick and gen-
erated 10 different shuffled samples and bigram-
generated sequences. We also introduced LSTM-
generated texts to consider the utility of our find-
ings, as explained in §5.

4 Taylor Exponents for Real Data

Figure 1 shows typical distributions for natural
language texts, with two single-author texts ((a)

5http://shaw.la.coocan.jp/smf2mml/

and (b)) and two multiple-author texts (newspa-
pers, (c) and (d)), in English and Chinese, respec-
tively. The segment size was ∆t = 5620 words6,
i.e., each segment had 5620 words and the hori-
zontal axis indicates the averaged frequency of a
specific word within a segment of 5620 words.

The points at the upper right represent the most
frequent words, whereas those at the lower left
represent the least frequent. Although the plots
exhibited different distributions, they could glob-
ally be considered roughly aligned in a power-law

6 In comparison, Figure 6 shows the effect on the expo-
nent of varying ∆t. As seen in that figure, larger ∆t increased
the differences in exponent among different data sets, making
the differences more distinguishable. Thus, ∆t had better be
as large as possible while keeping µ and σ computable. For
this article, we chose ∆t = 5620, which was one of the ∆t
values used in Figure 6.



1142

(a) Moby Dick (b) Hong Lou Meng

(c) Wall Street Journal (d) People’s Daily

Figure 1: Examples of Taylor’s law for natu-
ral language texts. Moby Dick and Hong Lou
Meng are representative of single-author texts, and
the two newspapers are representative of multiple-
author texts, in English and Chinese, respectively.
Each point represents a kind of word. The values
of σ and µ for each word kind are plotted across
texts within segments of size ∆t = 5620. The
Taylor exponents obtained by the least-squares
method were all around 0.58.

manner. This finding is non-trivial, as seen in other
analyses based on Taylor’s law (Eisler, Bartos, and
Kertész, 2007). The exponent α was almost the
same even though English and Chinese are differ-
ent languages using different kinds of script.

As explained in §3.1, the Taylor exponent in-
dicates the degree of consistent co-occurrence
among words. The value of 0.58 obtained here
suggests that the words of natural language texts
are not strongly or consistently coherent with re-
spect to each other. Nevertheless, the value is well
above 0.5, and for the real data listed in Table 1
(first to third blocks), not a single sample gave an
exponent as low as 0.5.

Although the overall global tendencies in Fig-
ure 1 followed power laws, many points deviated
significantly from the regression lines. The words
with the greatest fluctuation were often keywords.
For example, among words in Moby Dick with
large µ, those with the largest σ included whale,
captain, and sailor, whereas those with the small-
est σ included functional words such as to, that,
and with.

The Taylor exponent depended only slightly on
the data size. Figure 2 shows this dependency

Figure 2: Taylor exponent α̂ (vertical axis) cal-
culated for the two largest texts: The New York
Times and The Mainichi newspapers. To evaluate
the exponent’s dependence on the text size, parts
of each text were taken and the exponents were
calculated for those parts, with points taken log-
arithmically. The window size was ∆t = 5620.
As the text size grew, the Taylor exponent slightly
decreased.

for the two largest data sets used, The New York
Times (NYT, 1.5 billion words) and The Mainichi
(24 years) newspapers. When the data size was in-
creased, the exponent exhibited a slight tendency
to decrease. For the NYT, the decrease seemed to
have a lower limit, as the figure shows that the ex-
ponent stabilized at around 107 words.

The reason for this decrease can be explained
as follows. The Taylor exponent becomes larger
when some words occur in a clustered manner.
Making the text size larger increases the number of
segments (since ∆t was fixed in this experiment).
If the number of clusters does not increase as fast
as the increase in the number of segments, then the
number of clusters per segment becomes smaller,
leading to a smaller exponent. In other words, the
influence of each consecutive co-occurrence of a
particular word decays slightly as the overall text
size grows.

Analysis of different kinds of data showed how
the Taylor exponent differed according to the data
source. Figure 3 shows plots for samples from
enwiki8 (tagged Wikipedia), the child-directed
speech of Thomas (taken from CHILDES), pro-
gramming language data sets, and music. The dis-
tributions appear different from those for the natu-
ral language texts, and the exponents were signifi-
cantly larger. This means that these data sets con-
tained expressions with fixed forms much more
frequently than did the natural language texts.



1143

(a) enwiki8 (Wikipedia,
tagged)

(b) Thomas (CHILDES)

(c) Lisp (d) Bach’s St Matthew
Passion

Figure 3: Examples of Taylor’s law for alternative
data sets listed in Table 1: enwiki8 (tag-annotated
Wikipedia), Thomas (longest in CHILDES), Lisp
source code, and the music of Bach’s St Matthew
Passion. These examples exhibited larger Taylor
exponents than did typical natural language texts.

Figure 4 summarizes the overall picture among
the different data sources. The median and quan-
tiles of the Taylor exponent were calculated for the
different kinds of data listed in Table 1. The first
two boxes show results with an exponent of 0.50.
These results were each obtained from 10 random
samples of the randomized sequences. We will re-
turn to these results in the next section.

The remaining boxes show results for real data.
The exponents for texts from Project Gutenberg
ranged from 0.53 to 0.68. Figure 5 shows a his-
togram of these texts with respect to the value of
α̂. The number of texts decreased significantly at
a value of 0.63, showing that the distribution of
the Taylor exponent was rather tight. The kinds
of texts at the upper limit of exponents for Project
Gutenberg included structured texts of fixed style,
such as dictionaries, lists of histories, and Bibles.

The majority of texts were in English, followed
by French and then other languages, as listed in
Table 1. Whether α distinguishes languages is
a difficult question. The histogram suggests that
Chinese texts exhibited larger values than did texts
in Indo-European languages. We conducted a
statistical test to evaluate whether this difference
was significant as compared to English. Since
the numbers of texts were very different, we used
the non-parametric statistical test of the Brunner-

Munzel method, among various possible methods,
to test a null hypothesis of whether α was equal for
the two distributions (Brunner and Munzel, 2000).
The p-value for Chinese was p = 1.24 × 10−16,
thus rejecting the null hypothesis at the signifi-
cance level of 0.01. This confirms that α was
generally larger for Chinese texts than for En-
glish texts. Similarly, the null hypothesis was re-
jected for Finnish and French, but it was accepted
for German and Japanese at the 0.01 significance
level. Since Japanese was accepted despite its
large difference from English, we could not con-
clude whether the Taylor exponent distinguishes
languages.

Turning to the last four columns of Figure 4,
representing the enwiki8, child-directed speech
(CHILDES), programming language, and music
data, the Taylor exponents clearly differed from
those of the natural language texts. Given the
template-like nature of these four data sources,
the results were somewhat expected. The kind of
data thus might be distinguishable using the Tay-
lor exponent. To confirm this, however, would re-
quire assembling a larger data set. Applying this
approach with Twitter data and adult utterances
would produce interesting results and remains for
our future work.

The Taylor exponent also differed according to
∆t, and Figure 6 shows the dependence of α̂ on
∆t. For each kind of data shown in Figure 4,
the mean exponent is plotted for various ∆t. As
reported in (Eisler, Bartos, and Kertész, 2007),
the exponent is known to grow when the segment
size gets larger. The reason is that words occur
in a bursty, clustered manner at all length scales:
no matter how large the segment size becomes,
a segment will include either many or few in-
stances of a given word, leading to larger variance
growth. This phenomenon suggests how word co-
occurrences in natural language are self-similar.
The Taylor exponent is initially 0.5 when the seg-
ment size is very small. This can be analytically
explained as follows (Eisler, Bartos, and Kertész,
2007). Consider the case of ∆t=1. Let n be the
frequency of a particular word in a segment. We
have ⟨n⟩ ≪ 1.0, because the possibility of a spe-
cific word appearing in a segment becomes very
small. Because ⟨n⟩2 ≈ 0, σ2 = ⟨n2⟩ − ⟨n⟩2 ≈
⟨n2⟩. Because n = 1 or 0 (with ∆t=1), ⟨n2⟩ =
⟨n⟩ = µ. Thus, σ2 ≈ µ.

Overall, the results show the possibility of ap-



1144

Figure 4: Box plots of the Taylor exponents for different kinds of data. Each point represents one sample,
and samples from the same kind of data are contained in each box plot. The first two boxes are for the
randomized data, while the remaining boxes are for real data, including both the natural language texts
and language-related sequences. Each box ranges between the quantiles, with the middle line indicating
the median, the whiskers showing the maximum and minimum, and some extreme values lying beyond.

Figure 5: Histogram of Taylor exponents for long
texts in Project Gutenberg (1129 texts). The leg-
end indicates the languages, in frequency order.
Each bar shows the number of texts with that value
of α̂. Because of the skew of languages in the orig-
inal conception of Project Gutenberg, the majority
of the texts are in English, shown in blue, whereas
texts in other languages are shown in other col-
ors. The histogram shows how the Taylor expo-
nent ranged fairly tightly around the mean, and
natural language texts with an exponent larger than
0.63 were rare.

plying Taylor’s exponent to quantify the complex-
ity underlying coherence among words. Grammat-
ical complexity was formalized by Chomsky via
the Chomsky hierarchy (Chomsky, 1956), which
describes grammar via rewriting rules. The con-
straints placed on the rules distinguish four dif-
ferent levels of grammar: regular, context-free,
context-sensitive, and phrase structure. As indi-
cated in (Badii and Politi, 1997), however, this
does not quantify the complexity on a continuous
scale. For example, we might want to quantify
the complexity of child-directed speech as com-
pared to that of adults, and this could be addressed
in only a limited way through the Chomsky hi-
erarchy. Another point is that the hierarchy is
sentence-based and does not consider fluctuation
in the kinds of words appearing.

5 Evaluation of Machine-Generated Text
by the Taylor Exponent

The main contribution of this paper is the findings
of Taylor’s law behavior for real texts as presented
thus far. This section explains the applicability of
these findings, through results obtained with base-
line language models.

As mentioned previously, i.i.d. mathematical
processes have a Taylor exponent of 0.50. We
show here that, even if a process is not trivially
i.i.d., the exponent often takes a value of 0.50



1145

Figure 6: Growth of α̂ with respect to ∆t, aver-
aged across data sets within each data kind. The
plot labeled “random” shows the average for the
two datasets of randomized text from Moby Dick
(shuffled and bigrams, as explained in §5). Since
this analysis required a large amount of compu-
tation, for the large data sets (such as newspa-
per and programming language data), 4 million
words were taken from each kind of data and used
here. When ∆t was small, the Taylor exponent
was close to 0.5, as theoretically described in the
main text. As ∆t was increased, the value of α̂
grew. The maximum ∆t was about 10,000, or
about one-tenth of the length of one long literary
text. For the kinds of data investigated here, α̂
grew almost linearly. The results show that, at a
given ∆t, the Taylor exponent has some capabil-
ity to distinguish different kinds of text data.

(a) Moby Dick (shuffled) (b) Moby Dick (bigram)

Figure 7: Taylor analysis of a shuffled text of
Moby Dick and a randomized text generated by
a bigram model. Both exhibited an exponent of
0.50.

for random processes, including texts produced by
standard language models such as n-gram based
models. A more complete work in this direction is
reported in (Takahashi and Tanaka-Ishii, 2018).

Figure 7 shows samples from each of two sim-
ple random processes. Figure 7a shows the behav-
ior of a shuffled text of Moby Dick. Obviously,

(a) Text produced by
LSTM (3-layer stacked
character-based)

(b) Machine-translated
text using neural language
model

Figure 8: Taylor analysis for two texts produced
by standard neural language models: (a) a stacked
LSTM model that learned the complete works of
Shakespeare; and (b) a machine translation of Les
Misérables (originally in French, translated into
English), from a neural language model.

since the sequence was almost i.i.d. following
Zipf distribution, the Taylor exponent was 0.50.
Given that the Taylor exponent becomes larger for
a sequence with words dependent on each other,
as explained in §3, we would expect that a se-
quence generated by an n-gram model would ex-
hibit an exponent larger than 0.50. The simplest
such model is the bigram model, so a sequence
of 300,000 words was probabilistically generated
using a bigram model of Moby Dick. Figure 7b
shows the Taylor analysis, revealing that the expo-
nent remained 0.50.

This result does not depend much on the qual-
ity of the individual samples. The first and second
box plots in Figure 4 show the distribution of ex-
ponents for 10 different samples for the shuffled
and bigram-generated texts, respectively. The ex-
ponents were all around 0.50, with small variance.

State-of-the-art language models are based on
neural models, and they are mainly evaluated by
perplexity and in terms of the performance of in-
dividual applications. Since their architecture is
complex, quality evaluation has become an is-
sue. One possible improvement would be to
use an evaluation method that qualitatively dif-
fers from judging application performance. One
such method is to verify whether the properties un-
derlying natural language hold for texts generated
by language models. The Taylor exponent is one
such possibility, among various properties of nat-
ural language texts.

As a step toward this approach, Figure 8 shows
two results produced by neural language mod-
els. Figure 8a shows the result for a sam-
ple of 2 million characters produced by a stan-



1146

dard (three-layer) stacked character-based LSTM
unit that learned the complete works of Shake-
speare. The model was optimized to minimize
the cross-entropy with a stochastic gradient algo-
rithm to predict the next character from the previ-
ous 128 characters. See (Takahashi and Tanaka-
Ishii, 2017) for the details of the experimental set-
tings. The Taylor exponent of the generated text
was 0.50. This indicates that the character-level
language model could not capture or reproduce
the word-level clustering behavior in text. This
analysis sheds light on the quality of the language
model, separate from the prediction accuracy.

The application of Taylor’s law for a wider
range of language models appears in (Takahashi
and Tanaka-Ishii, 2018). Briefly, state-of-the-
art word-level language models can generate text
whose Taylor exponent is larger than 0.50 but
smaller than that of the dataset used for train-
ing. This indicates both the capability of modeling
burstiness in text and the room for improvement.
Also, the perplexity values correlate well with
the Taylor exponents. Therefore, Taylor expo-
nent can reasonably serve for evaluating machine-
generated text.

In contrast to character-level neural language
models, neural-network-based machine transla-
tion (NMT) models are, in fact, capable of main-
taining the burstiness of the original text. Fig-
ure 8b shows the Taylor analysis for a machine-
translated text of Les Misérables (from French to
English), obtained from Google NMT (Wu et al.,
2016). We split the text into 5000-character por-
tions because of the API’s limitation (See (Taka-
hashi and Tanaka-Ishii, 2017) for the details). As
is expected and desirable, the translated text re-
tains the clustering behavior of the original text,
as the Taylor exponent of 0.57 is equivalent to that
of the original text.

6 Conclusion

We have proposed a method to analyze whether a
natural language text follows Taylor’s law, a scal-
ing property quantifying the degree of consistent
co-occurrence among words. In our method, a se-
quence of words is divided into given segments,
and the mean and standard deviation of the fre-
quency of every kind of word are measured. The
law is considered to hold when the standard devi-
ation varies with the mean according to a power
law, thus giving the Taylor exponent.

Theoretically, an i.i.d. process has a Taylor
exponent of 0.5, whereas larger exponents indi-
cate sequences in which words co-occur systemat-
ically. Using over 1100 texts across 14 languages,
we showed that written natural language texts fol-
low Taylor’s law, with the exponent distributed
around 0.58. This value differed greatly from
the exponents for other data sources: enwiki8
(tagged Wikipedia, 0.63), child-directed speech
(CHILDES, around 0.68), and programming lan-
guage and music data (around 0.79). These Taylor
exponents imply that a written text is more com-
plex than programming source code or music with
regard to fluctuation of its components. None of
the real data exhibited an exponent equal to 0.5.
We conducted more detailed analysis varying the
data size and the segment size.

Taylor’s law and its exponent can also be ap-
plied to evaluate machine-generated text. We
showed that a character-based LSTM language
model generated text with a Taylor exponent of
0.5. This indicates one limitation of that model.

Our future work will include an analysis using
other kinds of data, such as Twitter data and adult
utterances, and a study of how Taylor’s law re-
lates to grammatical complexity for different se-
quences. Another direction will be to apply fluc-
tuation analysis in formulating a statistical test to
evaluate the structural complexity underlying a se-
quence.

Acknowledgments

This work was supported by JST Presto Grant
Number JPMJPR14E5 and HITE funding. We
thank Shuntaro Takahashi for offering his com-
ments and providing the machine-generated data
reported in §5.

References
Altmann, Eduardo G., Janet B. Pierrehumbert, and

Adilson E. Motter. 2009. Beyond word frequency:
Bursts, lulls, and scaling in the temporal distribu-
tions of words. PLOS ONE, 4(11):1–7.

And̄elković, Darinka, Nada Ševa, and Jasmina
Moskovljević. 2001. Serbian Corpus of Early Child
Language. Laboratory for Experimental Psychol-
ogy, Faculty of Philosophy, and Department of Gen-
eral Linguistics, Faculty of Philology, University of
Belgrade.

Badii, Remo and Antonio Politi. 1997. Complex-
ity: Hierarchical structures and scaling in physics.
Cambridge University Press.



1147

Behrens, Heike. 2006. The input-output relationship in
first language acquisition. Language and Cognitive
Processes, 21:2–24.

Benedet, Maria, Celis Cruz, Maria Carrasco, and
Catherine Snow. 2004. Spanish BecaCESNo Cor-
pus. TalkBank.

Bentz, Christian, Dimitrios Alikaniotis, Michael
Cysouw, and Ramon Ferrer i Cancho. 2017. The
entropy of words—learnability and edxpressibvity
across more than 1000 langauges. Entropy, (6).

Bol, Gerard W. 1995. Implicational scaling in child
language acquisition: the order of production of
Dutch verb constructions, Amsterdam Series in
Child Language Development, chapter 3. Amster-
dam: Institute for General Linguistics.

Brunner, Edgar and Ullrich Munzel. 2000. The non-
parametric behrens-fisher problem: Asymtotic the-
ory and a small-sample approximation. Biometrical
Journal, 42:17–25.

Calif, Rudy and François G. Schmitt. 2015. Taylor law
in wind energy data. Resources, 4(4):787–795.

Chomsky, Noam. 1956. Three models for the descrip-
tion of language. IRE Transactions on Information
Theory, 2:113–124.

Cohen, Joel E. and Meng Xu. 2015. Random sampling
of skewed distributions implies taylor’s power law
of fluctuation scaling. Proceedings of the National
Academy of Sciences, 112(25):7749–7754.

Ebeling, Werner and Thorsten Pöeschel. 1994. Entropy
and long-range correlations in literary english. Eu-
rophys. Letters, 26:241–246.

Eisler, Zoltán, Imre Bartos, and János Kertész. 2007.
Fluctuation scaling in complex systems: Taylor’s
law and beyond. Advances in Physics, pages 89–
142.

Gerlach, Martin and Eduardo G. Altmann. 2014. Scal-
ing laws and fluctuations in the statistics of word fre-
quencies. New Journal of Physics, 16(11):113010.

Gil, David and Uri Tadmor. 2007. The MPI-EVA
Jakarta Child Language Database. A joint project
of the Department of Linguistics, Max Planck In-
stitute for Evolutionary Anthropology and the Cen-
ter for Language and Culture Studies, Atma Jaya
Catholic University.

Heaps, Harold S. 1978. Information Retrieval: Com-
putational and Theoretical Aspects. Academic
Press, Inc., Orlando, FL, USA.

Hurst, Harold E. 1951. Long-term storage capacity of
reservoirs. Transactions of the American Society of
Civil Engineers, 116:770–808.

Lieven, Elena, Dorothé Salomo, and Michael
Tomasello. 2009. Two-year-old children’s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481–508.

Lin, Henry W. and Max Tegmark. 2016. Critical be-
havior from deep dynamics: A hidden dimension in
natural language. arXiv preprint, abs/1606.06737.

MacWhinney, Brian. 2000. The Childes Project. New
York: Psychology Press.

Montemurro, Marcelo A. and Pedro A. Pury. 2002.
Long-range fractal correlations in literary corpora.
Fractals, 10:451–461.

Oshima-Takane, Yuriko, Brian MacWhinney, Hidetosi
Sirai, Susanne Miyata, and Norio Naka. 1995.
CHILDES manual for Japanese. Montreal: McGill
University.

Plunkett, Kim and Sven Strömqvist. 1992. The acqui-
sition of scandinavian languages. In D. I. Slobin,
editor, The crosslinguistic study of language ac-
quisition, volume 3. Lawrence Erlbaum Associates,
pages 457–556.

Rondal, Jean A. 1985. Adult-child interaction and the
process of language acquisition. Praeger Publishers.

Smith, H. Fairfield. 1938. An empirical law describ-
ing hetero-geneity in the yields of agricultural crops.
Journal of Agriculture Science, 28(1).

Smoczynska, Magdalena. 1985. The acquisition of
polish. In D. I. Slobin, editor, The crosslinguistic
study of language acquisition. Lawrence Erlbaum
Associates, pages 595–686.

Takahashi, Shuntaro and Kumiko Tanaka-Ishii. 2017.
Do neural nets learn statistical laws behind natural
langauge? PLoS One. In press.

Takahashi, Shuntaro and Kumiko Tanaka-Ishii. 2018.
Assesing language models with scaling properties.
arXiv preprint, arXiv:1804.08881.

Takahira, Ryosuke, Kumiko Tanaka-Ishii, and De-
bowski Lukasz. 2016. Entropy rate estimates for
natural language : A new extrapolation of com-
pressed large-scale corpora. Entropy, 18(10).

Tanaka-Ishii, Kumiko and Shunsuke Aihara. 2015.
Text constancy measures. Computational Linguis-
tics, 41:481–502.

Tanaka-Ishii, Kumiko and Armin Bunde. 2016. Long-
range memory in literary texts: On the universal
clustering of the rare words. PLOS One. Online
journal.

Taylor, L. Roy. 1961. Aggregation, variance and the
mean. Nature, 732:189–190.

Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc Le,
Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, and
et al. 2016. Google ’s neural machine translation
system: Bridging the gap between human and ma-
chine translation. arXiv.



1148

Zipf, George K. 1965. Human behavior and the prin-
ciple of least effort: An introduction to human ecol-
ogy. Hafner.


