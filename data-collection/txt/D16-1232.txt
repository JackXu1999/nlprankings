



















































Nonparametric Bayesian Models for Spoken Language Understanding


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2144–2152,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Nonparametric Bayesian Models for Spoken Language Understanding

Kei Wakabayashi
Tsukuba University, 1-2 Kasuga,
Tsukuba, Ibaraki 305-8550, Japan

kwakaba@slis.tsukuba.ac.jp

Johane Takeuchi, Kotaro Funakoshi
and Mikio Nakano

Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama 351-0188, Japan

{johane.takeuchi,funakoshi,nakano}
@jp.honda-ri.com

Abstract

In this paper, we propose a new generative ap-
proach for semantic slot filling task in spoken
language understanding using a nonparamet-
ric Bayesian formalism. Slot filling is typi-
cally formulated as a sequential labeling prob-
lem, which does not directly deal with the pos-
terior distribution of possible slot values. We
present a nonparametric Bayesian model in-
volving the generation of arbitrary natural lan-
guage phrases, which allows an explicit cal-
culation of the distribution over an infinite set
of slot values. We demonstrate that this ap-
proach significantly improves slot estimation
accuracy compared to the existing sequential
labeling algorithm.

1 Introduction

Spoken language understanding (SLU) refers to the
challenge of recognizing a speaker’s intent from a
natural language utterance, which is typically de-
fined as a slot filling task. For example, in the ut-
terance “Remind me to call John at 9am tomorrow”,
the specified information {“time”: “9am tomor-
row”} and {“subject”: “to call John”} should be
extracted. The term slot refers to a variable such as
the time or subject that is expected to be filled with
a value provided through the user’s utterance.

The slot filling task is typically formulated as a
sequential labeling problem as shown in Figure 1.
This labeling scheme naturally represents the recog-
nition of arbitrary phrases that appear in the tran-
scription of an utterance. Formally speaking, when
we assume a given set of slots {s1, ..., sM} and de-
note the corresponding slot values by {vs1 , ..., vsM }

where vsi ∈ Vsi , the domain of each slot value Vsi is
an infinite set of word sequences. In this paper, we
use the term arbitrary slot filling task to refer to this
implicit problem statement, which inherently under-
lies the sequential labeling formulation.

In contrast, a different line of work has explored
the case where Vsi is provided as a finite set of possi-
ble values that can be handled by a backend system
(Henderson, 2015). We refer to this type of task as
a categorical slot filling task. In this case, the slot
filling task is regarded as a classification problem
that explicitly considers a value-based prediction, as
shown in Figure 2. From this point of view, we can
say that a distribution of slot values is actually con-
centrated in a small set of typical phrases, even in
the arbitrary slot filling task, because users basically
know what kind of function is offered by the system.

To reflect this observation, in this paper we ex-
plore the value-based formulation approach for arbi-
trary slot filling tasks. Unlike the sequential labeling
formulation, which is basically position-based label
prediction, our method directly estimates the poste-
rior distribution over an infinite set of possible val-
ues for each slot Vsi . The distribution is represented
by using a Dirichlet process (Gershman and Blei,
2012), which is a nonparametric Bayesian formal-
ism that generates a categorical distribution for any
space. We demonstrate that this approach improves
estimation accuracy in the arbitrary slot filling task
compared with conventional sequential labeling ap-
proach.

The rest of this paper is organized as follows. In
Section 2, we review the existing approaches for
categorical and arbitrary slot filling tasks and intro-

2144



!! !! !! !! "#$%&'! !! !! "#()'(! *#()'(! !!

+,-! .//0+12! 3/)! (! )'4$(5)(1$! +1! $6'! 3'1! 7+8/1! ()'(!

Figure 1: sequential labeling formulation for slot filling tasks.

!"#!$%%&!'(!)%*!+!*,-.+/*+'.!!'!.0,!),'!1!2%'!+*,+!

P(restaurant | u) = 0.96

P(pub | u) = 0.03

P(None | u) = 0.01

...

P( fen_ ditton | u) = 0.98

P(girton | u) = 0.005

P(None | u) = 0.01

...

P(italian | u) = 0.005

P(mexican | u) = 0.01

P(None | u) = 0.85

...

!"#"!

34.53,6! 34+*,+6! 34)%%16!

Figure 2: Value-based formulation. The posterior probabilities
of values for each slot are explicitly computed.

duce related work. In Section 3, we present our
nonparametric Bayesian formulation, the hierarchi-
cal Dirichlet process slot model (HDPSM), which
directly models an infinite set of slot values. On the
basis of the HDPSM, we develop a generative utter-
ance model that allows us to compute the posterior
probability of slot values in Section 4. In Section 5,
we introduce a two-stage slot filling algorithm that
consists of a candidate generation step and a candi-
date ranking step using the proposed model. In Sec-
tion 6, we show the experimental results for multiple
datasets in different domains to demonstrate that the
proposed algorithm performs better than the base-
line sequential labeling method. We conclude in
Section 7 with a brief summary.

2 Related Work

The difference between the categorical and arbitrary
slot filling approaches has not been explicitly dis-
cussed in a comparative manner to date. In this sec-
tion, we review existing work for both approaches.

For the categorical slot filling approach, various
algorithms that directly model the distribution of
slot values have been proposed, including generative
models (Williams, 2010), maximum entropy linear
classifiers (Metallinou et al., 2013), and neural net-
works (Ren et al., 2014). However, none of these
models are applicable for predicting a variable that
ranges over an infinite set, and it is not straightfor-
ward to extend them suitably. In particular, a dis-
criminative approach is not applicable for arbitrary
slot filling tasks because it requires a fixed finite set
of slot values to take statistics.

The arbitrary slot filling approach is a natural
application of shallow semantic parsing (Gildea,
2002), which is naturally formulated as a sequen-
tial labeling problem. Various sequential labeling
algorithms have been applied to this task, including
support vector machines, conditional random fields
(CRF) (Lafferty et al., 2001; Hahn et al., 2011), and
deep neural networks (Mesnil et al., 2015; Xu and
Sarikaya, 2013). Vukotic et al. (2015) reported that
the CRF is still the most accurate, rapid, and stable
method among them. Because the focus of this pa-
per is arbitrary slot filling tasks, we use CRFs as our
baseline method.

In this paper, we apply nonparametric Bayesian
models (Gershman and Blei, 2012) to represent
the distribution over arbitrary phrases for each slot.
The effectiveness of this phrase modeling approach
has been examined in various applications including
morphological analysis (Goldwater et al., 2011) and
infinite vocabulary topic models (Zhai and Boyd-
graber, 2013). Our method can be regarded as an
application of this idea, although it is not straight-
forward to integrate it with the utterance generation
process, as we explain later.

Consequently, our proposed method is catego-
rized as a generative approach. There are many ad-
vantages inherent in generative approaches that have
been examined, including unsupervised SLU (Chen
et al., 2015), automatic feature extraction (Tur et
al., 2013), and integration with syntactic modeling
(Lorenzo et al., 2013). Another convenient prop-
erty of generative models is that prior knowledge
can be integrated in an intuitive way (Raymond et
al., 2006). This often leads to better performance
with less training data compared with discriminative
models trained completely from scratch (Komatani
et al., 2010).

3 Hierarchical Dirichlet Process Slot
Model

In this section, we present a nonparametric Bayesian
formulation that directly models the distribution
over an infinite set of possible values for each slot.
Let S = {s1, ..., sMS} be a given set of slots and
MS be the number of slots. We define each slot si
as a random variable ranging over an infinite set of

2145



letter sequences V , which is represented as follows:

V = {b1, ..., bL|bι ∈ C,L ≥ 0}

where C is a set of characters including the blank
character and any other character that potentially ap-
pears in the transcription of an utterance. Conse-
quently, we regard the set of slots S as also being a
random variable that ranges over VMS . The objec-
tive of this section is to develop the formulation of
the probabilistic distribution p(S).

3.1 Dirichlet Process

We apply the Dirichlet process (DP) to model both
the distribution for an individual slot pi(si) and the
joint distribution p(S). In this subsection, we review
the definition and key properties of DP with general
notation for the target distribution G over the do-
main X . In the DP for the prior of pi(si) that is
described in Section 3.2, the domain X corresponds
to a set of slot values V , e.g., “fen ditton”, “new
chesterton”, and None. In the DP for p(S) presented
in Section 3.3, X indicates a set of tuples of slot val-
ues VMS , e.g., (“restaurant”, “new chesterton”, “fast
food”) and (“restaurant”, “fen ditton”, None).

The DP is a probabilistic distribution over the dis-
tribution G. DP is parameterized by α0 and G0,
where α0 > 0 is a concentration parameter and G0

is a base distribution over X . If G is drawn from
DP (α0, G0) (i.e., G ∼ DP (α0, G0)), then the fol-
lowing Dirichlet distributed property holds for any
partition of X denoted by {A1, ..., AL}:

(G(A1), ..., G(AL)) ∼ Dir(α(A1), ..., α(AL))

where α(A) = α0G0(A), which is known as the
base measure of DP.

Ferguson (1973) proved an important property
of a posterior distribution of repeated i.i.d. sam-
ples x1:N = {x1, ..., xN} drawn from G ∼
DP (α0, G0). Consider a countably infinite set of
atoms φ = {φ1, φ2, ...} that are independently
drawn from G0. Let ci ∈ N be the assignment of an
atom for sample xi, which is generated by a sequen-
tial draw with the following conditional probability:

p(cN+1 = k|c1:N ) =
{

nk
N+α0

k ≤ K
α0

N+α0
k = K + 1

where nk is the number of times that the kth atom
appears in c1:N and K is the number of different
atoms in c1:N . Given the assignment c1:N , the pre-
dictive distribution of xN+1 ∈ X is represented in
the following form:

P (xN+1 = θ|c1:N ,φ1:K , α0, G0)

=
K∑

k=1

nk
N + α0

δ(φk, θ) +
α0

N + α0
G0(θ)

The base distribution possibly generates an iden-
tical value for different atoms, such as (φ1 = “fen
ditton”, φ2 = “new chesterton”, φ3 = “fen ditton”).
The assignment ci is an auxiliary variable to indi-
cate which of these atoms is assigned to the ith data
point xi; when xi = “fen ditton”, ci can be 1 or 3.
The posterior distribution above depends on the fre-
quency of atom nk, not on the frequency of θ itself.
The atoms φ and the assignment c are latent vari-
ables that should be determined at runtime.

3.2 Individual Slot Model
First we formulate the distribution for an individ-
ual slot as pi(si) ∼ DP (α0i , G0i ) where G0i is a
base distribution over the set of phrases V . 1 We
define G0i as a generative model that consists of
two-step generation: generation of the phrase length
0 ≤ Li ≤ Lmax using a categorical distribution and
generation of a letter sequence s1:Li using an n-gram
model, as follows:

Li ∼ Categorical(λi)
sιi ∼ p(sιi|sι−n+1:ι−1i ,ηi)

where λi and ηi are parameters for the categorical
distribution and the n-gram model for slot si, respec-
tively. This explicit modeling of the length helps
avoid the bias toward shorter phrases and leads to
a better distribution, as reported by Zhai and Boyd-
graber (2013). We define G0i as a joint distribution
of these models:

G0i (s
1:Li
i ) = p(Li|λi)

Li∏

ι=1

p(sιi|sι−n+1:ι−1i ,ηi) (1)

G0i potentially generates an empty phrase of Li = 0
to express the case that the slot value vsi is not

1Note that the subscript i for s, p, α0 and G0 indicates the
slot type such as “type”, “area” and “food” in Figure 2.

2146



provided by an utterance. Therefore, the distribu-
tion pi(si) can naturally represent the probability of
None , which is shown in Figure 2.

We consider prior distributions of the parame-
ters λi and ηi to treat the n-gram characteristics of
each slot in a fully Bayesian manner. p(λ) is given
as a Lmax-dimensional symmetric Dirichlet distri-
bution with parameter a. We also define the |C|-
dimensional symmetric Dirichlet distributions with
parameter b for each n-gram context, since given the
context p(sιi|sι−n+1:ι−1i ,ηi) is just a categorical dis-
tribution that ranges over C. Consider we observe
N phrases si for slot i. Let nLiι be the number of
phrases that have length ι and nγih be the number
of times that letter sι = h appears after context
sι−n+1:ι−1 = γ. The predictive probability of a
phrase is represented as follows:

G0i (s
1:Li
i |si) =

nLiι + b

N + bC

Li∏

ι=1

nγisιi
+ a

∑
c n

γ
ic + a

∑Lmax
l=1 n

L
il

3.3 Generative Model for a Set of Slot Values
A naive definition of the joint distribution p(S) is
a product of all slot probabilities

∏MS
i=1 pi(si) for

making an independence assumption. However, the
slot values are generally correlated with each other
(Chen et al., 2015). To obtain more accurate dis-
tribution, we formulate p(S) using another DP that
recognizes a frequent combination of slot values, as
p(S) ∼ DP (α1, G2) where G2 is a base distribu-
tion over VMS . We apply the naive independence
assumption to G2 as follows:

G2(S) =

MS∏

i=1

pi(si)

The whole generation process of S involves two-
layered DPs that share atoms among them. In this
sense, this generative model is regarded as a hierar-
chical Dirichlet process (Teh et al., 2005).

Let G1i (si) = pi(si) and G
3(S) = p(S) for con-

sistent notations. In summary, we define the hierar-
chical Dirichlet process slot model (HDPSM) as a
generative model that has the following generation
process.

G1i ∼ DP (α0i , G0i )
G3 ∼ DP (α1, G2)
S ∼ G3

3.4 Inference of HDPSM
In a slot filling task, observations of S1:T =
{S1, ..., ST } are available as training data. The in-
ference of HDPSM refers to the estimation of λ, η
and the atom assignments for each DP.

We formulate the HDPSM in a form of the Chi-
nese restaurant franchise process, which is one of the
explicit representations of hierarchical DPs obtained
by marginalizing out the base distributions. Teh et
al. (2005) presents a Gibbs sampler for this repre-
sentation, which involves a repetitive resampling of
atoms and assignment. In our method, we prefer to
adopt a single pass inference, which samples the as-
signment for each observation only once. Our pre-
liminary experiments showed that the quality of in-
ference is not affected because S is observed unlike
the settings in Teh et al. (2005).

We denote the atoms and the atom assignment in
the first level DP DP (α1, G2) by φ1 and c11:N , re-
spectively. The posterior probability of atom assign-
ment for a new observation SN+1 is represented as
follows:

p(c1N+1 = k|c11:N ,φ1, SN+1)

∝
{
n1kδ(φ

1
k, SN+1) k ≤ K

α1G2(SN+1) k = K + 1

where n1k is the number of times that the kth atom
appears in c11:N and K is the number of different
atoms in c11:N .
φ0i and c

0
i1:K denote the atoms and the assignment

in the second level DPs DP (α0i , G
0
i ). The second

level DPs assign atoms to each first level atom φ1k,
i.e. the second level atom φ0it is generated only when
a new atom is assigned for SN+1 at the first level.
The posterior probability of atom assignment at the
second level is:

p(c0iK+1 = t|c0i1:K ,φ0i , sN+1i)

∝
{
n0itδ(φ

0
it, SN+1) t ≤ Ti

α0iG
0(SN+1) t = Ti + 1

where n0it is the number of times that the tth atom
appears in c0i1:K and Ti is the number of different
atoms in c0i1:K .

The single pass inference procedure is presented
in Algorithm 1. Given the atoms φ and the as-
signments c, the predictive distribution of SN+1 =

2147



Algorithm 1 Single pass inference of HDPSM
Input: A set of observations S1:N

1: Set empty list to c1 and c0i
2: for d = 1 to N do
3: k ∼ p(c1d = k|c11:d−1,φ1, Sd)
4: if k = K + 1 then
5: for i = 1 to MS do
6: ti ∼ p(c0iK+1 = ti|c0i1:K ,φ0i , sdi)
7: if ti = Ti + 1 then
8: Update nLi and n

γ
i with sdi

9: end if
10: c0K+1 ← ti and φ0iti ← sdi
11: end for
12: end if
13: c1d ← k and φ1k ← S
14: end for

{sN+11, ..., sN+1MS} is calculated as follows:

P (SN+1|c,φ) =
K∑

k=1

n1k
N + α1

δ(φ1k, SN+1) (2)

+
α1

N + α1

MS∏

i=1

P (sN+1i|c0i ,φ0i )

P (sN+1i|c0i ,φ0i ) =
Ti∑

t=1

n0it
K + α0i

δ(φ0it, sN+1i)

+
α0i

K + α0i
G0i (sN+1i|φ0i )

4 Generative Model for an Utterance

We present a generative utterance model to derive a
slot estimation algorithm given utterance u. Figure 3
presents the basic concept of our generative model.
In the proposed model, we formulate the distribution
of slot values as well as the distribution of non-slot
parts. In Figure 3, the phrases “hi we’re in um” and
“and we need a” should be removed to identify the
slot information. We call these non-slot phrases as
functional fillers because they more or less have a
function to convey information. Identifying the set
of non-slot phrases is equivalent to identifying the
set of slot phrases. Therefore, we define a generative
model of functional fillers in the same way as the slot
values.

!"#$%&'%#"(#)*#+%(#,"-.(#/(,#$%#(%%,#/#'%01/)'/(1!

!"#$2#'%01/)'/(1#

%&$%2#+%(#,"-.(#

'(()2#!"#$%

3-%'/(4%2!

5.(1%(1#67.12!

*$+,--,-+2#!"#$%&'%#"(#)*#

.,))/$2#/(,#$%#(%%,#/#

$-),-+2#!"#$!

8)(49.(/7#8"77%'2!

Figure 3: The proposed generative utterance model. We at-
tempt to find the best combination of the slot parts and the non-

slot parts (i.e., functional filler parts) by using this model.

4.1 Functional Filler

We assume an utterance u is a concatenation of slot
values S and functional fillers F . A functional filler
is represented as a phrase that ranges over V . To
derive the utterance model, we first formulate a gen-
erative model for functional fillers.

In our observation, the distribution of the func-
tional filler depends on its position in an utterance.
For example, utterances often begin with typical
phrases such as “Hello I’m looking for ...” or “Hi
please find ...”, which can hardly ever appear at other
positions. To reflect this observation, we introduce
a filler slot to separately model the functional fillers
based on a position feature. Specifically, we define
three filler slots: beginning filler f1, which precedes
any slot value, ending filler f3, which appears at the
end of an utterance, and middle filler f2, which is
inserted between slot values. We use the term con-
tent slot to refer to S when we intend to explicitly
distinguish it from a filler slot.

Let F = {f1, f2, f3} be a set of filler slots and
MF = 3 be the number of filler slots. Each slot fi
is a random variable ranging over V and F is a ran-
dom variable over VMF . These notations for filler
slots indicate compatibility to a content slot, which
suggests that we can formulate F using HDPSMs,
as follows:

H1i ∼ DP (β0i , H0i )
H3 ∼ DP (β1, H2)
F ∼ H3

where H0i is an n-gram-based distribution over
V that is defined in an identical way to (1) and
H2(F ) =

∏MF
i=1 H

1
i (F ).

2148



G
0

G
1

S

u

H
0

H
1

F

M
S

M
F

G
3

H
3

D

Figure 4: Graphical model of the utterance model.

4.2 Utterance Model
Figure 4 presents the graphical model of our utter-
ance model. We assume that an utterance u is built
with phrases provided by S and F . Therefore, the
conditional distribution p(u|S, F ) basically involves
a distribution over the permutation of these slot val-
ues with two constraints: f1 is placed first and f3
has to be placed last. In our formulation, we simply
adopt a uniform distribution over all possible permu-
tations.

For training the utterance model, we assume that a
set of annotated utterances is available. Each train-
ing instance consists of utterance u and annotated
slot values S. Given u and S, we assume that the
functional fillers F can be uniquely identified. For
the example in Figure 3, we can identify the sub-
sequence in u that corresponds to each content slot
value of “restaurant” and “fen ditton”. This match-
ing result leads to the identification of filler slot val-
ues. Consequently, a triple (u, S, F ) is regarded as
an observation. Because the HDPSMs of the content
slot and of the filler slot are conditionally indepen-
dent given S and F , we can separately apply Algo-
rithm 1 to train each HDPSM.

For slot filling, we examine the posterior proba-
bility of content slot values S given u, which can be
reformed as follows:

P (S|u) ∝
∑

F

P (u|S, F )P (S)P (F )

In this equation, we can remove the summation of F
because filler slot values F are uniquely identified
regarding u and S in our assumption. Additionally,
we approximately regard P (u|S, F ) as a constant if
u can be built with S and F . By using these assump-
tions, the posterior probability is reduced to the fol-
lowing formula:

P (S|u) ∝ P (S)P (F ) (3)

!"# ! ! ! "#$%&$ '#$%&$ '#$%&$ ! ! ! ! "#()*&

$%& ! ! ! ! "#$%&$ '#$%&$ ! ! ! ! "#()*&

'(& ! ! ! ! "#$%&$ '#$%&$ ! ! "#+,,- '#+,,- !

./ 0&1%& /2 34 +&2 -/((,2 $2- 0& 2&&- $ %&5($3%$2(

!"#

$%&$ 346+&26-/((,2

()*& %&5($3%$2(

+7 ./60&1%&6/2

+8 $2-60&62&&-6$

$%&

$%&$ +&26-/((,2

()*& %&5($3%$2(

+7 ./60&1%&6/2634

+8 $2-60&62&&-6$

'(&

$%&$ +&26-/((,2

+,,- 2&&-6$

+7 ./60&1%&6/2634

+8 $2-60&

+9 %&5($3%$2(

Figure 5: Candidate generation using sequential labeling algo-
rithm. The figure shows the case of N = 3.

where F in this formula is fillers identified given u
and S. Consequently, the proposed method attempts
to find the most likely combination of the slot val-
ues and the non-slot phrases, since all words in an
utterance have to belong to either of them. By using
trained HDPSM (i.e., the posterior given all training
data), P (S) and P (F ) can be computed by (2).

5 Candidate Generation

For estimating slot values given u, we adopt a can-
didate generation approach (Williams, 2014) that
leverages another slot filling algorithm to enumer-
ate likely candidates. 2 Specifically, we assume a
candidate generation function g(u) that generates N
candidates {S1, ..., SN} regarding u. Our slot fill-
ing algorithm computes the posterior probability by
(3) for each candidate slot Sj and takes the candi-
date that has the highest posterior probability. In this
estimation process, our utterance model works as a
secondary filter that covers the error of the primary
analysis.

Figure 5 provides an example of candidate gener-
ation by using a sequential labeling algorithm with
IOB tags. The subsequences to which the O tag is
assigned can be regarded as functional fillers. The
values for each filler slot are identified depending on
the position of the subsequence, as the figure shows.

6 Experiments

We evaluate the performance of the proposed gener-
ative model with an experiment using the algorithm

2The direct inference of the generative utterance model is a
topic for near future work. The MCMC method will circumvent
the difficulty of searching the entire candidate space.

2149



name #utterances #slots max. diversity
DSTC 1,441 6 55

Weather 1,442 3 191
Table 1: Datasets in the experiment. Max. diversity refers to
the maximum number of value types that are taken by a slot.

described in Section 5. We adopt a conditional ran-
dom field (CRF) as a candidate generation algorithm
that generates N -best estimation as candidates. For
the CRF, we apply commonly used features includ-
ing unigram and bigram of the surface form and part
of speech of the word. We used CRF++3 as the CRF
implementation.

6.1 Dataset

The performance of our method is evaluated using
two datasets from different languages, as summa-
rized in Table 1. The first dataset is provided by
the third Dialog State Tracking Challenge (Hender-
son, 2015), hereafter referred to as the DSTC corpus.
The DSTC corpus consists of dialogs in the tourist
information domain. In our experiment, we use the
user’s first utterance in each dialog, which typically
describes the user’s query to the system. Utterances
without any slot information are excluded. We man-
ually modified the annotated slot values into “as-
is form” to allow a sequential labeling method to
extract the ground-truth values. This identification
process can be done in a semi-automatic manner that
involves no expert knowledge. We apply the part of
speech tagger in NLTK4 for the CRF application.

The second dataset is a weather corpus consisting
of user utterances in an in-house corpus of human-
machine dialogues in the weather domain. It con-
tains 1,442 questions spoken in Japanese. In this
corpus, the number of value types for each slot is
higher than DSTC, which indicates a more challeng-
ing task. We applied the Japanese morphological
analyzer MeCab (Kudo et al., 2004) to segment the
Japanese text into words before applying CRF.

For both datasets, we examine the effect
of the amount of available annotated utterances
by varying the number of training data in
25, 50, 75, 100, 200, 400, 800, all.

3https://taku910.github.io/crfpp/
4http://www.nltk.org/

#train CRF best HDP N = 5 HDP N = 300
25 0.560 0.706* 0.684*
50 0.709 0.791* 0.765*
75 0.748 0.824* 0.817*
100 0.791 0.845* 0.837*
200 0.839 0.901* 0.876*
400 0.904 0.938* 0.936*
800 0.926 0.953* 0.947*
1296 0.938 0.960* 0.951

Table 2: Slot estimation accuracy for the DSTC corpus. The
asterisk (*) indicates that the accuracy is statistically significant

compared against CRF best (p < 0.005).

#train CRF best HDP N = 5 HDP N = 300
25 0.327 0.452* 0.480*
50 0.379 0.488* 0.499*
75 0.397 0.504* 0.522*
100 0.418 0.501* 0.512*
200 0.493 0.526* 0.531*
400 0.512 0.551* 0.549*
800 0.533 0.555* 0.554*
1297 0.546 0.560* 0.554

Table 3: Slot estimation accuracy for the Japanese weather cor-
pus. An asterisk (*) indicates statistical significance against

CRF best (p < 0.01).

6.2 Evaluation Metrics

The methods are compared in terms of slot estima-
tion accuracy. Let nc be the number of utterances
for which the estimated slot S and the ground-truth
slot Ŝ are perfectly matched, and let ne be the num-
ber of the utterances including an estimation error.
The slot estimation accuracy is simply calculated as
nc

nc+ne
. All evaluation scores are calculated as the

average of 10-fold cross validation. We also conduct
a binomial test to examine the statistical significance
of the improvement in the proposed algorithm com-
pared to the CRF baseline.

6.3 Results

Tables 2 and 3 present the slot estimation accu-
racy for the DSTC corpus and the Japanese weather
corpus, respectively. The baseline (CRF best) is a
method that takes only one best output of CRF for
slot estimation. HDP with N = 5 and N = 300
is the proposed method, where N is the number of
candidates generated by the CRF candidate genera-

2150



utterance estimation by CRF best estimation by HDP N = 5
im looking for a restaurant that type:restaurant (*) type:restaurant,food:fast food
serves fast food
i want a moderate restaurant in area:new chesterton, area:new chesterton,
the new chesterton area type:restaurant, type:restaurant,

food:moderate (*) pricerange:moderate
im looking for a cheap chine pricerange:cheap,type:restaurant, pricerange:cheap,type:restaurant,
chinese takeaway restaurant food:chinese takeaway food:chine chinese takeaway (*)

Table 4: Examples of estimated slot values for the condition of #train is 800. An asterisk (*) indicates misrecognition.

tor. The asterisks (*) beside the HDP accuracy in-
dicate the statistical significance against CRF best,
which is tested using the binomial test.

Results show that our proposed method performs
significantly better than CRF. Especially when the
amount of training data is limited, the proposed
method outperforms the baseline. This property is
attractive for practical speech recognition systems
that offer many different functions. Accurate recog-
nition at an early stage of development allows a
practitioner to launch a service that results in quickly
collecting hundreds of speech examples.

Since we use the CRF as a candidate generator,
we expect that the CRF N-best can rank the correct
answer higher in the candidate list. In fact, the top
five candidates cover almost all of the correct an-
swers. Therefore, the result in the comparison of
N = 5 and N = 300 suggests the stability of the
proposed method against the mostly noisy 295 can-
didates. Because the proposed algorithm makes no
use of the original ranking order, N = 300 is a
harder condition in which to identify the correct an-
swer. Nevertheless, the result shows that the drop in
the performance is limited; the accuracy is still sig-
nificantly better than the baseline. This result sug-
gests that the proposed method is less dependent on
the performance of the candidate generator.

Table 4 presents some examples of the slot val-
ues estimated by CRF best and HDP with N = 5
for the condition where the number of training utter-
ances is 800. The first two are samples where CRF
best failed to predict the correct values. These er-
rors are attributed to infrequent sequential patterns
caused by the less trained expressions “that serves
fast food” and “moderate restaurant” because CRF
is a position-based classifier. The value-based for-
mulation allows the model to learn that the phrase

“fast food” is more likely to be a food name than to
be a functional filler and to reject the candidate.

The third example in Table 4 shows an error
using HDP, which extracted “chine chinese take-
away” which includes a reparandum of disfluency
(Georgila et al., 2010). This error can be attributed
to the fact that this kind of disfluency resembles the
true slot value, which leads to a higher probability
of “chine” in the food slot model compared to in
the functional filler model. Regarding this type of
error, preliminary application of a disfluency detec-
tion method (Zayats et al., 2016) is promising for
improving accuracy.

The execution time for training the proposed
HDP utterance model with 1297 training data in
the Japanese weather corpus was about 0.3 seconds.
This is a good performance since the CRF training
takes about 5.5 seconds. Moreover, the training of
the proposed HDP model is scalable and works in an
online manner because it is a single pass algorithm.
When we have a very large number of training ex-
amples, the bottleneck is the CRF training, which
requires scanning the whole dataset repeatedly.

7 Conclusion

In this paper, we proposed an arbitrary slot fill-
ing method that directly deals with the posterior
probability of slot values by using nonparametric
Bayesian models. We presented a two-stage method
that involves an N-best candidate generation step,
which is typically done using a CRF. Experimental
results show that our method significantly improves
recognition accuracy. This empirical evidence sug-
gests that the value-based formulation is a promis-
ing approach for arbitrary slot filling tasks, which is
worth exploring further in future work.

2151



References
Yun-Nung Chen, William Yang Wang, Anatole Gersh-

man, and Alexander Rudnicky. 2015. Matrix Fac-
torization with Knowledge Graph Propagation for Un-
supervised Spoken Language Understanding. In Proc.
Annual Meeting of the Association for Computational
Linguistics.

Thomas S. Ferguson. 1973. A Bayesian Analysis of
Some Nonparametric Problems. The Annual of Statis-
tics, 1(2):209–230.

Kallirroi Georgila, Ning Wang, and Jonathan Gratch.
2010. Cross-Domain Speech Disfluency Detection. In
Proc. Annual SIGDIAL Meeting on Discourse and Di-
alogue.

Samuel J. Gershman and David M. Blei. 2012. A tu-
torial on Bayesian nonparametric models. Journal of
Mathematical Psychology, 56(1):1–12.

Daniel Gildea. 2002. Automatic labeling of semantic
roles. Computational Linguistics, 28(3):245–288.

Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2011. Producing Power-Law Distributions
and Damping Word Frequencies with Two-Stage Lan-
guage Models. Journal of Machine Learning Re-
search, 12:2335–2382.

Stefan Hahn, Marco Dinarelli, Christian Raymond,
Fabrice Lefevre, Patrick Lehnen, Renato De Mori,
Alessandro Moschitti, Hermann Ney, and Giuseppe
Riccardi. 2011. Comparing stochastic approaches to
spoken language understanding in multiple languages.
IEEE Transactions on Audio, Speech and Language
Processing, 19(6):1569–1583.

Matthew Henderson. 2015. Machine Learning for Dia-
log State Tracking: A Review. In Proc. Workshop on
Machine Learning in Spoken Language Processing.

Kazunori Komatani, Masaki Katsumaru, Mikio Nakano,
Kotaro Funakoshi, Tetsuya Ogata, and Hiroshi G.
Okuno. 2010. Automatic Allocation of Training Data
for Rapid Prototyping. In Proc. International Confer-
ence on Computational Linguistics.

Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proc. Empirical
Methods in Natural Language Processing.

John Lafferty, Andrew McCallum, and Fernando C N
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. International Conference on Machine Learn-
ing.

Alejandra Lorenzo, Lina M Rojas-barahona, and
Christophe Cerisara. 2013. Unsupervised structured
semantic inference for spoken dialog reservation tasks.
In Proc. Annual SIGDIAL Meeting on Discourse and
Dialogue.

Gregoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua
Bengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He,
Larry Heck, Gokhan Tur, Dong Yu, and Geoffrey
Zweig. 2015. Using Recurrent Neural Networks
for Slot Filling in Spoken Language Understanding.
IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing, 23(3):530–539.

Angeliki Metallinou, Dan Bohus, and Jason Williams.
2013. Discriminative state tracking for spoken dialog
systems. Proc. Annual Meeting of the Association for
Computational Linguistics.

Christian Raymond, Frédéric Béchet, Renato De Mori,
and Géraldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48(3-4):288–304.

Hang Ren, Weiqun Xu, and Yonghong Yan. 2014.
Markovian discriminative modeling for dialog state
tracking. In Proc. Annual SIGDIAL Meeting on Dis-
course and Dialogue.

Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2005. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566–1581.

Gokhan Tur, Asli Celikyilmaz, and Dilek Hakkani-Tur.
2013. Latent Semantic Modeling for Slot Filling in
Conversational Understanding. In Proc. International
Conference on Acoustics, Speech and Signal Process-
ing.

Vedran Vukotic, Christian Raymond, and Guillaume
Gravier. 2015. Is it Time to Switch to Word Em-
bedding and Recurrent Neural Networks for Spoken
Language Understanding? In Proc. Interspeech.

Jason D. Williams. 2010. Incremental partition re-
combination for efficient tracking of multiple dialog
states. In Proc. International Conference on Acous-
tics, Speech and Signal Processing.

Jason D Williams. 2014. Web-style ranking and SLU
combination for dialog state tracking. In Proc. Annual
SIGDIAL Meeting on Discourse and Dialogue.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neu-
ral network based triangular CRF for joint intent de-
tection and slot filling. In Proc. IEEE Workshop on
Automatic Speech Recognition and Understanding.

Vicky Zayats, Mari Ostendorf, and Hannaneh Hajishirzi.
2016. Disfluency Detection using a Bidirectional
LSTM. arXiv preprint arXiv:1604.03209.

Ke Zhai and Jordan Boyd-graber. 2013. Online Latent
Dirichlet Allocation with Infinite Vocabulary. In Proc.
International Conference on Machine Learning.

2152


