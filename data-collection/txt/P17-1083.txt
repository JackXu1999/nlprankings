



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 896–905
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1083

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 896–905
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1083

Tandem Anchoring:
a Multiword Anchor Approach for Interactive Topic Modeling

Jeffrey Lund, Connor Cook, Kevin Seppi
Computer Science Department

Brigham Young University
{jefflund,cojoco,kseppi}@byu.edu

Jordan Boyd-Graber
Computer Science Department
University of Colorado Boulder

jordan.boyd.graber@colorado.edu

Abstract

Interactive topic models are powerful tools
for understanding large collections of text.
However, existing sampling-based inter-
active topic modeling approaches scale
poorly to large data sets. Anchor meth-
ods, which use a single word to uniquely
identify a topic, offer the speed needed for
interactive work but lack both a mecha-
nism to inject prior knowledge and lack
the intuitive semantics needed for user-
facing applications. We propose combina-
tions of words as anchors, going beyond
existing single word anchor algorithms—
an approach we call “Tandem Anchors”.
We begin with a synthetic investigation of
this approach then apply the approach to
interactive topic modeling in a user study
and compare it to interactive and non-
interactive approaches. Tandem anchors
are faster and more intuitive than existing
interactive approaches.

Topic models distill large collections of text into
topics, giving a high-level summary of the the-
matic structure of the data without manual anno-
tation. In addition to facilitating discovery of top-
ical trends (Gardner et al., 2010), topic modeling
is used for a wide variety of problems including
document classification (Rubin et al., 2012), in-
formation retrieval (Wei and Croft, 2006), author
identification (Rosen-Zvi et al., 2004), and senti-
ment analysis (Titov and McDonald, 2008). How-
ever, the most compelling use of topic models is
to help users understand large datasets (Chuang
et al., 2012).

Interactive topic modeling (Hu et al., 2014) al-
lows non-experts to refine automatically generated

topics, making topic models less of a “take it or
leave it” proposition. Including humans input dur-
ing training improves the quality of the model and
allows users to guide topics in a specific way, cus-
tom tailoring the model for a specific downstream
task or analysis.

The downside is that interactive topic model-
ing is slow—algorithms typically scale with the
size of the corpus—and requires non-intuitive in-
formation from the user in the form of must-link
and cannot-link constraints (Andrzejewski et al.,
2009). We address these shortcomings of interac-
tive topic modeling by using an interactive version
of the anchor words algorithm for topic models.

The anchor algorithm (Arora et al., 2013) is an
alternative topic modeling algorithm which scales
with the number of unique word types in the data
rather than the number of documents or tokens
(Section 1). This makes the anchor algorithm fast
enough for interactive use, even in web-scale doc-
ument collections.

A drawback of the anchor method is that anchor
words—words that have high probability of being
in a single topic—are not intuitive. We extend the
anchor algorithm to use multiple anchor words in
tandem (Section 2). Tandem anchors not only im-
prove interactive refinement, but also make the un-
derlying anchor-based method more intuitive.

For interactive topic modeling, tandem anchors
produce higher quality topics than single word
anchors (Section 3). Tandem anchors provide
a framework for fast interactive topic model-
ing: users improve and refine an existing model
through multiword anchors (Section 4). Com-
pared to existing methods such as Interactive
Topic Models (Hu et al., 2014), our method is
much faster.

896

https://doi.org/10.18653/v1/P17-1083
https://doi.org/10.18653/v1/P17-1083


1 Vanilla Anchor Algorithm

The anchor algorithm computes the topic matrix
A, where Av,k is the conditional probability of ob-
serving word v given topic k, e.g., the probability
of seeing the word “lens” given the camera topic
in a corpus of Amazon product reviews. Arora
et al. (2012a) find these probabilities by assum-
ing that every topic contains at least one ‘anchor’
word which has a non-zero probability only in that
topic. Anchor words make computing the topic
matrix A tractable because the occurrence pattern
of the anchor word mirrors the occurrence pattern
of the topic itself.

To recover the topic matrix A using anchor
words, we first compute a V × V cooccurrence
matrix Q, where Qi,j is the conditional probabil-
ity p(wj |wi) of seeing word type wj after hav-
ing seen wi in the same document. A form of the
Gram-Schmidt process on Q finds anchor words
{g1 . . . gk} (Arora et al., 2013).

Once we have the set of anchor words, we can
compute the probability of a topic given a word
(the inverse of the conditioning in A). This coeffi-
cient matrix C is defined row-wise for each word i

C∗i,· = argmin
Ci,·

DKL

(
Qi,·

∥∥∥∥
K∑

k=1

Ci,kQgk,·

)
,

(1)
which gives the best reconstruction (based on
Kullback-Leibler divergence DKL) of non-anchor
words given anchor words’ conditional probabil-
ities. For example, in our product review data, a
word such as “battery” is a convex combination of
the anchor words’ contexts (Qgk,·) such as “cam-
era”, “phone”, and “car”. Solving each row of C
is fast and is embarrassingly parallel. Finally, we
apply Bayes’ rule to recover the topic matrix A
from the coefficient matrix C.

The anchor algorithm can be orders of mag-
nitude faster than probabilistic inference (Arora
et al., 2013). The construction of Q has a run-
time of O(DN2) where D is the number of docu-
ments and N is the average number of tokens per
document. This computation requires only a sin-
gle pass over the data and can be pre-computed
for interactive use-cases. Once Q is constructed,
topic recovery requires O(KV 2 +K2V I), where
K is the number of topics, V is the vocabulary
size, and I is the average number of iterations
(typically 100-1000). In contrast, traditional topic

Anchor Top Words in Topics
backpack backpack camera lens bag room carry fit

cameras equipment comfortable
camera camera lens pictures canon digital lenses

batteries filter mm photos
bag bag camera diaper lens bags genie smell

room diapers odor

Table 1: Three separate attempts to construct a
topic concerning camera bags in Amazon product
reviews with single word anchors. This example
is drawn from preliminary experiments with an au-
thor as the user. The term “backpack” is a good an-
chor because it uniquely identifies the topic. How-
ever, both “camera” and “bag” are poor anchors
for this topic.

model inference typically requires multiple passes
over the entire data. Techniques such as Online
LDA (Hoffman et al., 2010) or Stochastic Vari-
ation Inference (Hoffman et al., 2013) improves
this to a single pass over the entire data. How-
ever, from Heaps’ law (Heaps, 1978) it follows
that V 2 � DN for large datasets, leading to
much faster inference times for anchor methods
compared to probabilistic topic modeling. Further,
even if online were to be adapted to incorporate
human guidance, a single pass is not tractable for
interactive use.

2 Tandem Anchor Extension

Single word anchors can be opaque to users. For
an example of bewildering anchor words, con-
sider a camera bag topic from a collection of
Amazon product reviews (Table 1). The anchor
word “backpack” may seem strange. However,
this dataset contains nothing about regular back-
packs; thus, “backpack” is unique to camera bags.
Bizarre, low-to-mid frequency words are often an-
chors because anchor words must be unique to a
topic; intuitive or high-frequency words cannot be
anchors if they have probability in any other topic.

The anchor selection strategy can mitigate this
problem to some degree. For example, rather
than selecting anchors using an approximate con-
vex hull in high-dimensional space, we can find
an exact convex hull in a low-dimensional embed-
ding (Lee and Mimno, 2014). This strategy will
produce more salient topics but still makes it dif-
ficult for users to manually choose unique anchor
words for interactive topic modeling.

If we instead ask users to give us representative

897



words for this topic, we would expect combina-
tions of words like “camera” and “bag.” However,
with single word anchors we must choose a single
word to anchor each topic. Unfortunately, because
these words might appear in multiple topics, indi-
vidually they are not suitable as anchor words. The
anchor word “camera” generates a general cam-
era topic instead of camera bags, and the topic
anchored by “bag” includes bags for diaper pails
(Table 1).

Instead, we need to use sets of representative
terms as an interpretable, parsimonious descrip-
tion of a topic. This section discusses strategies
to build anchors from multiple words and the im-
plications of using multiword anchors to recover
topics. This extension not only makes anchors
more interpretable but also enables users to manu-
ally construct effective anchors in interactive topic
modeling settings.

2.1 Anchor Facets

We first need to turn words into an anchor. If
we interpret the anchor algorithm geometrically,
each row of Q represents a word as a point in
V -dimensional space. We then model each point
as a convex combination of anchor words to re-
construct the topic matrix A (Equation 1). In-
stead of individual anchor words (one anchor word
per topic), we use anchor facets, or sets of words
that describe a topic. The facets for each anchor
form a new pseudoword, or an invented point in
V -dimensional space (described in more detail in
Section 2.2).

While these new points do not correspond to
words in the vocabulary, we can express non-
anchor words as convex combinations of pseu-
dowords. To construct these pseudowords from
their facets, we combine the co-occurrence pro-
files of the facets. These pseudowords then aug-
ment the original cooccurrence matrix Q with K
additional rows corresponding to synthetic pseu-
dowords forming each of K multiword anchors.
We refer to this augmented matrix as S. The rest
of the anchor algorithm proceeds unmodified.

Our augmented matrix S is therefore a (V +
K) × V matrix. As before, V is the number of
token types in the data and K is the number of
topics. The first V rows of S correspond to the V
token types observed in the data, while the addi-
tionalK rows correspond to the pseudowords con-
structed from anchor facets. Each entry of S en-

codes conditional probabilities so that Si,j is equal
to p(wi |wj). For the additionalK rows, we invent
a cooccurrence pattern that can effectively explain
the other words’ conditional probabilities.

This modification is similar in spirit to super-
vised anchor words (Nguyen et al., 2015). This
supervised extension of the anchor words algo-
rithm adds columns corresponding to conditional
probabilities of metadata values after having seen
a particular word. By extending the vector-space
representation of each word, anchor words corre-
sponding to metadata values can be found. In con-
trast, our extension does not add dimensions to the
representation, but simply places additional points
corresponding to pseudoword words in the vector-
space representation.

2.2 Combining Facets into Pseudowords
We now describe more concretely how to combine
an anchor facets to describe the cooccurrence pat-
tern of our new pseudoword anchor. In tandem an-
chors, we create vector representations that com-
bine the information from anchor facets. Our an-
chor facets are G1 . . .GK , where Gk is a set of an-
chor facets which will form the kth pseudoword
anchor. The pseudowords are g1 . . . gK , where gk
is the pseudoword from Gk. These pseudowords
form the new rows of S. We give several candi-
dates for combining anchors facets into a single
multiword anchor; we compare their performance
in Section 3.

Vector Average An obvious function for com-
puting the central tendency is the vector average.
For each anchor facet,

Sgk,j =
∑

i∈Gk

Si,j
|Gk|

, (2)

where |Gk| is the cardinality of Gk. Vector average
makes the pseudoword Sgk,j more central, which
is intuitive but inconsistent with the interpretation
from Arora et al. (2013) that anchors should be
extreme points whose linear combinations explain
more central words.

Or-operator An alternative approach is to con-
sider a cooccurrence with any anchor facet in Gk.
For word j, we use De Morgan’s laws to set

Sgk,j = 1−
∏

i∈Gk
(1− Si,j). (3)

Unlike the average, which pulls the pseudoword
inward, this or-operator pushes the word outward,

898



increasing each of the dimensions. Increasing the
volume of the simplex spanned by the anchors ex-
plains more words.

Element-wise Min Vector average and or-
operator are both sensitive to outliers and cannot
account for polysemous anchor facets. Return-
ing to our previous example, both “camera” and
“bag” are bad anchors for camera bags because
they appear in documents discussing other prod-
ucts. However, if both “camera” and “bag” are
anchor facets, we can look at an intersection of
their contexts: words that appear with both. Us-
ing the intersection, the cooccurrence pattern of
our anchor facet will only include terms relevant
to camera bags.

Mathematically, this is an element-wise min op-
erator,

Sgk,j = min
i∈Gk

Si,j . (4)

This construction, while perhaps not as simple as
the previous two, is robust to words which have
cooccurrences which are not unique to a single
topic.

Harmonic Mean Leveraging the intuition that
we should use a combination function which is
both centralizing (like vector average) and ig-
nores large outliers (like element-wise min), the fi-
nal combination function is the element-wise har-
monic mean. Thus, for each anchor facet

Sgk,j =
∑

i∈Gk

(
S−1i,j
|Gk|

)−1
. (5)

Since the harmonic mean tends towards the lowest
values in the set, it is not sensitive to large outliers,
giving us robustness to polysemous words.

2.3 Finding Topics
After constructing the pseudowords of S we then
need to find the coefficients Ci,k which describe
each word in our vocabulary as a convex combi-
nation of the multiword anchors. Like standard
anchor methods, we solve the following for each
token type:

C∗i,· = argmin
Ci,·

DKL

(
Si,·

∥∥∥∥
K∑

k=1

Ci,kSgk,·

)
.

(6)
Finally, we appeal to Bayes’ rule, we recover the
topic-word matrix A from the coefficients of C.

The correctness of the topic recovery algorithm
hinges upon the assumption of separability. Sepa-
rability means that the occurrence pattern across

documents of the anchor words across the data
mirrors that of the topics themselves. For single
word anchors, this has been observed to hold for
a wide variety of data (Arora et al., 2012b). With
our tandem anchor extension, we make similar as-
sumptions as the vanilla algorithm, except with
pseudowords constructed from anchor facets. So
long as the occurrence pattern of our tandem an-
chors mirrors that of the underlying topics, we can
use the same reasoning as Arora et al. (2012a) to
assert that we can provably recover the topic-word
matrix A with all of the same theoretical guaran-
tees of complexity and robustness. Furthermore,
we runtime analysis given by Arora et al. (2013)
applies to tandem anchors.

If desired, we can also add further robustness
and extensibility to tandem anchors by adding reg-
ularization to Equation 6. Regularization allows
us to add something which is mathematically sim-
ilar to priors, and has been shown to improve
the vanilla anchor word algorithm (Nguyen et al.,
2014). We leave the question of the best regular-
ization for tandem anchors as future work, and fo-
cus our efforts on solving the problem of interac-
tive topic modeling.

3 High Water Mark for Tandem Anchors

Before addressing interactivity, we apply tandem
anchors to real world data, but with anchors
gleaned from metadata. Our purpose is twofold.
First, we determine which combiner from Sec-
tion 2.2 to use in our interactive experiments in
Section 4 and second, we confirm that well-chosen
tandem anchors can improve topics. In addi-
tion, we examine the runtime of tandem anchors
and compare to traditional model-based interac-
tive topic modeling techniques. We cannot assume
that we will have metadata available to build tan-
dem anchors, but we use them here because they
provide a high water mark without the variance in-
troduced by study participants.

3.1 Experimental Setup

We use the well-known 20 Newsgroups dataset
(20NEWS) used in previous interactive topic mod-
eling work: 18,846 Usenet postings from 20 dif-
ferent newgroups in the early 1990s.1 We remove
the newsgroup headers from each message, which
contain the newsgroup names, but otherwise left
messages intact with any footers or quotes. We

1http://qwone.com/˜jason/20Newsgroups/

899



then remove stopwords and words which appear
in fewer than 100 documents or more than 1,500
documents.

To seed the tandem anchors, we use the ti-
tles of newsgroups. To build each multiword
anchor facet, we split the title on word bound-
aries and expand any abbreviations or acronyms.
For example, the newsgroup title ‘comp.os.ms-
windows.misc’ becomes {“computer”, “operat-
ing”, “system”, “microsoft”, “windows”, “miscel-
laneous”}. We do not fully specify the topic;
the title gives some intuition, but the topic mod-
eling algorithm must still recover the complete
topic-word distributions. This is akin to know-
ing the names of the categories used but nothing
else. Critically, the topic modeling algorithm has
no knowledge of document-label relationships.

3.2 Experimental Results
Our first evaluation is a classification task to pre-
dict documents’ newsgroup membership. Thus,
we do not aim for state-of-the-art accuracy,2 but
the experiment shows title-based tandem anchors
yield topics closer to the underlying classes than
Gram-Schmidt anchors. After randomly splitting
the data into test and training sets we learn topics
from the test data using both the title-based tan-
dem anchors and the Gram-Schmidt single word
anchors.3 For multiword anchors, we use each
of the combiner functions from Section 2.2. The
anchor algorithm only gives the topic-word dis-
tributions and not word-level topic assignments,
so we infer token-level topic assignments using
LDA Latent Dirichlet Allocation (Blei et al., 2003)
with fixed topics discovered by the anchor method.
We use our own implementation of Gibbs sam-
pling with fixed topics and a symmetric document-
topic Dirichlet prior with concentration α = .01.
Since the topics are fixed, this inference is very
fast and can be parallelized on a per-document ba-
sis. We then train a hinge-loss linear classifier
on the newsgroup labels using Vowpal Wabbit4

with topic-word pairs as features. Finally, we infer
topic assignments in the test data and evaluate the
classification using those topic-word features. For
both training and test, we exclude words outside

2The best system would incorporate topic features with
other features, making it harder to study and understand the
topical trends in isolation.

3With fixed anchors and data the anchor algorithm is de-
terministic, so we use random splits instead of the standard
train/test splits so that we can compute variance.

4http://hunch.net/˜vw/

the LDA vocabulary.
The topics created from multiword anchor

facets are more accurate than Gram-Schmidt top-
ics (Figure 1). This is true regardless of the com-
biner function. However, harmonic mean is more
accurate than the other functions.5

Since 20NEWS has twenty classes, accuracy
alone does not capture confusion between closely
related newsgroups. For example, accuracy
penalizes a classifier just as much for label-
ing a document from ‘rec.sport.baseball’ with
‘rec.sport.hockey’ as with ‘alt.atheism’ despite the
similarity between sports newsgroups. Conse-
quently, after building a confusion matrix between
the predicted and true classes, external clustering
metrics reveal confusion between classes.

The first clustering metric is the adjusted Rand
index (Yeung and Ruzzo, 2001), which is akin to
accuracy for clustering, as it gives the percentage
of correct pairing decisions from a reference clus-
tering. Adjusted Rand index (ARI) also accounts
for chance groupings of documents. Next we use
F-measure, which also considers pairwise groups,
balancing the contribution of false negatives, but
without the true negatives. Finally, we use varia-
tion of information (VI). This metric measures the
amount of information lost by switching from the
gold standard labels to the predicted labels (Meilă,
2003). Since we are measuring the amount of in-
formation lost, lower variation of information is
better.

Based on these clustering metrics, tandem an-
chors can yield superior topics to those created us-
ing single word anchors (Figure 1). As with accu-
racy, this is true regardless of which combination
function we use. Furthermore, harmonic mean
produces the least confusion between classes.5

The final evaluation is topic coherence by New-
man et al. (2010), which measures whether the
topics make sense, and correlates with human
judgments of topic quality. Given V , the set of
the n most probable words of a topic, coherence is

∑

v1,v2∈V
log

D(v1, v2) + �

D(v2)
(7)

where D(v1, v2) is the co-document frequency of

5Significant at p < 0.01/4 when using two-tailed t-tests
with a Bonferroni correction. For each of our evaluations, we
verify the normality of our data (D’Agostino and Pearson,
1973) and use two-tailed t-tests with Bonferroni correction
to determine whether the differences between the different
methods are significant.

900



●●

● ●● ●

● ●

●

● ● ●

●●

● ●

●

● ●● ●

● ●●

Accuracy ARI F−Measure VI Coherence

Gram−Schmidt

Title+Average

Title+Or

Title+Min

Title+HMean

0.
55

0.
60

0.
65

0.
70

0.
30

0.
35

0.
40

0.
45

0.
50

0.
56

0.
60

0.
64

0.
68

0.
72 2.

4

2.
7

3.
0

3.
3

3.
6

−
23

0

−
22

5

−
22

0

−
21

5

−
21

0

Figure 1: Using metadata can improve anchor-based topic models. For all metrics, the unsupervised
Gram-Schmidt anchors do worse than creating anchors based on Newsgroup titles (for all metrics except
VI, higher is better). For coherence, Gram-Schmidt does better than two functions for combining anchor
words, but not the element-wise min or harmonic mean.

word types v1 and v2, and D(v2) is the document
frequency of word type v2. A smoothing parame-
ter � prevents zero logarithms.

Figure 1 also shows topic coherence. Although
title-based anchor facets produce better classifi-
cation features, topics from Gram-Schmidt an-
chors have better coherence than title-based an-
chors with the vector average or the or-operator.
However, when using the harmonic mean com-
biner, title-based anchors produce the most human
interpretable topics.6

Harmonic mean beats other combiner functions
because it is robust to ambiguous or irrelevant term
cooccurrences an anchor facet. Both the vector av-
erage and the or-operator are swayed by large out-
liers, making them sensitive to ambiguous terms
in an anchor facet. Element-wise min also has this
robustness, but harmonic mean is also able to bet-
ter characterize anchor facets as it has more cen-
tralizing tendency than the min.

3.3 Runtime Considerations

Tandem anchors will enable users to direct topic
inference to improve topic quality. However, for
the algorithm to be interactive we must also con-
sider runtime. Cook and Thomas (2005) argue that
for interactive applications with user-initiated ac-
tions like ours the response time should be less
than ten seconds. Longer waits can increase the
cognitive load on the user and harm the user inter-
action.

6Significant at p < 0.01/4 when using two-tailed t-tests
with a Bonferroni correction. For each of our evaluations, we
verify the normality of our data (D’Agostino and Pearson,
1973) and use two-tailed t-tests with Bonferroni correction
to determine whether the differences between the different
methods are significant.

Fortunately, the runtime of tandem anchors
is amenable to interactive topic modeling. On
20NEWS, interactive updates take a median time
of 2.13 seconds. This result was obtained using a
single core of an AMD Phemon II X6 1090T pro-
cessor. Furthermore, larger datasets typically have
a sublinear increase in distinct word types, so we
can expect to see similar run times, even on much
larger datasets.

Compared to other interactive topic modeling
algorithms, tandem anchors has a very attractive
run time. For example, using an optimized version
of the sampler for the Interactive Topic Model de-
scribed by Hu and Boyd-Graber (2012), and the
recommended 30 iterations of sampling, the Inter-
active Topic Model updates with a median time of
24.8 seconds (Hu and Boyd-Graber, 2012), which
is well beyond our desired update time for inter-
active use and an order of magnitude slower than
tandem anchors.

Another promising interactive topic modeling
approach is Utopian (Choo et al., 2013), which
uses non-negative factorization, albeit without the
benefit of anchor words. Utopian is much slower
than tandem anchors. Even on the small InfoVis-
VAST dataset which contains only 515 docu-
ments, Utopian takes 48 seconds to converge.
While the times are not strictly comparable due to
differing datasets, Utopian scales linearly with the
size of the data, we can intuit that even for mod-
erately sized datasets such as 20NEWS, Utopian is
infeasible for interactive topic modeling due to run
time.

While each of these interactive topic modeling
algorithms do achieve reasonable topics, only our
algorithm fits the run time requirements for inter-

901



Figure 2: Interface for user study with multiword
anchors applied to interactive topic modeling.

activity. Furthermore, since tandem anchors scales
with the size of the vocabulary rather than the size
of the data, this trend will only become more pro-
nounced as we increase the amount of data.

4 Interactive Anchor Words

Given high quality anchor facets, the tandem an-
chor algorithm can produce high quality topic
models (particularly when the harmonic mean
combiner is used). Moreover, the tandem anchor
algorithm is fast enough to be interactive (as op-
posed to model-based approaches such as the In-
teractive Topic Model). We now turn our attention
to our main experiment: tandem anchors applied
to the problem of interactive topic modeling. We
compare both single word and tandem anchors in
our study. We do not include the Interactive Topic
Model or Utopian, as their run times are too slow
for our users.

4.1 Interface and User Study

To show that interactive tandem anchor words are
fast, effective, and intuitive, we ask users to under-
stand a dataset using the anchor word algorithm.
For this user study, we recruit twenty participants
drawn from a university student body. The stu-
dent median age is twenty-two. Seven are female,
and thirteen are male. None of the students had
any prior familiarity with topic modeling or the
20NEWS dataset.

Each participant sees a simple user interface
(Figure 2) with topic given as a row with two
columns. The left column allows users to view and
edit topics’ anchor words; the right column lists
the most probable words in each topic.7 The user
can remove an anchor word or drag words from

7While we use topics generated using harmonic mean for
our final analysis, users were shown topics generated using
the min combiner. However, this does not change our result.

the topic word lists (right column) to become an
anchor word. Users can also add additional top-
ics by clicking the “Add Anchor” to create addi-
tional anchors. If the user wants to add a word to a
tandem anchor set that does not appear in the inter-
face, they manually type the word (restricted to the
model’s vocabulary). When the user wants to see
the updated topics for their newly refined anchors,
they click “Update Topics”.

We give each a participant a high level overview
of topic modeling. We also describe common
problems with topic models including intruding
topic words, duplicate topics, and ambiguous top-
ics. Users are instructed to use their best judge-
ment to determine if topics are useful. The task is
to edit the anchor words to improve the topics. We
asked that users spend at least twenty minutes, but
no more than thirty minutes. We repeat the task
twice: once with tandem anchors, and once with
single word anchors.8

4.2 Quantitative Results

We now validate our main result that for interac-
tive topic modeling, tandem anchors yields better
topics than single word anchors. Like our title-
based experiments in Section 3, topics generated
from users become features to train and test a clas-
sifier for the 20NEWS dataset. We choose this
dataset for easier comparison with the Interactive
Topic Modeling result of Hu et al. (2014). Based-
sie on our results with title-based anchors, we use
the harmonic mean combiner in our analysis. As
before, we report not only accuracy, but also mul-
tiple clustering metrics using the confusion ma-
trix from the classification task. Finally, we report
topic coherence.

Figure 3 summarizes the results of our quantita-
tive evaluation. While we only compare user gen-
erated anchors in our analysis, we include the un-
supervised Gram-Schmidt anchors as a baseline.
Some of the data violate assumptions of normal-
ity. Therefore, we use Wilcoxon’s signed-rank
test (Wilcoxon, 1945) to determine if the differ-
ences between multiword anchors and single word
anchors are significant.

Topics from user generated multiword anchors
yield higher classification accuracy (Figure 3).
Not only is our approach more scalable than
the Interactive Topic Model, but we also achieve

8The order in which users complete these tasks is counter-
balanced.

902



● ●● ● ●●

●

Accuracy ARI F−Measure VI Coherence

Tandem

Singleword

Gram−Schmidt

0.
55

0.
60

0.
65

0.
70

0.
30

0.
35

0.
40

0.
45

0.
50

0.
55

0.
60

0.
65

0.
70 2.

7

3.
0

3.
3

3.
6

−
24

0

−
23

0

−
22

0

−
21

0

−
20

0

−
19

0

Figure 3: Classification accuracy and coherence using topic features gleaned from user provided mul-
tiword and single word anchors. Grahm-Schmidt anchors are provided as a baseline. For all metrics
except VI, higher is better. Except for coherence, multiword anchors are best.

higher classification accuracy than Hu et al.
(2014).9 Tandem anchors also improve clustering
metrics.10

While user selected tandem anchors produce
better classification features than single word an-
chors, users selected single word anchors produce
topics with similar topic coherence scores.11

To understand this phenomenon, we use quality
metrics (AlSumait et al., 2009) for ranking topics
by their correspondence to genuine themes in the
data. Significant topics are likely skewed towards
a few related words, so we measure the distance
of each topic-word distribution from the uniform
distribution over words. Topics which are close
to the underlying word distribution of the entire
data are likely to be vacuous, so we also measure
the distance of each topic-word distribution from
the underlying word distribution. Finally, back-
ground topics are likely to appear in a wide range
of documents, while meaningful topics will appear
in a smaller subset of the data.

Figure 4 reports our topic significance findings.
For all three significance metrics, multiword an-
chors produce more significant topics than single
word anchors.10 Topic coherence is based solely
on the top n words of a topic, while both accuracy
and topic significance depend on the entire topic-
word distributions. With single word anchors, top-
ics with good coherence may still be too general.
Tandem anchors enables users to produce topics
with more specific word distributions which are
better features for classification.

Anchor Top Words in Topic
Automatic Gram Schmidt
love love god evolution romans heard car
game game games team hockey baseball

heard
Interactive Single-word
evolution evolution theory science faith quote

facts
religion religion god government state jesus is-

rael
baseball baseball games players word teams

car
hockey hockey team play games season play-

ers
Interactive Tandem
atheism god
exists prove

god science evidence reason faith ob-
jective

christian je-
sus

jesus christian christ church bible
christians

jew israel israel jews jewish israeli state religion
baseball bat
ball

hit baseball ball player games call

hockey nhl team hockey player nhl win play

Table 2: Comparison of topics generated for
20NEWS using various types of anchor words.
Users are able to combine words to create more
specific topics with tandem anchors.

4.3 Qualitative Results
We examine the qualitative differences between
how users select multiword anchor facets versus
single word anchors. Table 2 gives examples of
topics generated using different anchor strategies.
In a follow-up survey with our users, 75% find
it easier to affect individual changes in the top-
ics using tandem anchors compared to single word
anchors. Users who prefer editing multiword an-
chors over single word anchors often report that

9However, the values are not strictly comparable, as Hu
et al. (2014) use the standard chronological test/train fold,
and we use random splits.

10Significant at p < 0.01 when using Wilcoxon’s signed-
rank test.

11The difference between coherence scores was not statis-
tically significant using Wilcoxon’s signed-rank test.

903



●

●●●

●

●●●

●

●●●

uniform vacuous background

Tandem

Singleword

Gram−Schmidt

1 2 3

0.
5

1.
0

1.
5

2.
0

0.
5

1.
0

1.
5

2.
0

Figure 4: Topic significance for both single word and multiword anchors. In all cases higher is better.
Multiword anchors produce topics which are more significant than single word anchors.

multiword anchors make it easier to merge simi-
lar topics into a single focused topic by combin-
ing anchors. For example, by combining multi-
ple words related to Christianity, users were able
to create a topic which is highly specific, and dif-
ferentiated from general religion themes which in-
cluded terms about Atheism and Judaism.

While users find that use tandem anchors is eas-
ier, only 55% of our users say that they prefer
the final topics produced by tandem anchors com-
pared to single word anchors. This is in harmony
with our quantitative measurements of topic co-
herence, and may be the result of our stopping cri-
teria: when users judged the topics to be useful.

However, 100% of our users feel that the topics
created through interaction were better than those
generated from Gram-Schmidt anchors. This was
true regardless of whether we used tandem an-
chors or single word anchors.

Our participants also produce fewer topics when
using multiword anchors. The mean difference be-
tween topics under single word anchors and multi-
ple word anchors is 9.35. In follow up interviews,
participants indicate that the easiest way to resolve
an ambiguous topic with single word anchors was
to create a new anchor for each of the ambiguous
terms, thus explaining the proliferation of topics
for single word anchors. In contrast, fixing an am-
biguous tandem anchor is simple: users just add
more terms to the anchor facet.

5 Conclusion

Tandem anchors extend the anchor words algo-
rithm to allow multiple words to be combined into
anchor facets. For interactive topic modeling, us-
ing anchor facets in place of single word anchors
produces higher quality topic models and are more
intuitive to use. Furthermore, our approach scales
much better than existing interactive topic mod-
eling techniques, allowing interactivity on large

datasets for which interactivity was previous im-
possible.

Acknowledgements

This work was supported by the collaborative
NSF Grant IIS-1409287 (UMD) and IIS- 1409739
(BYU). Boyd-Graber is also supported by NSF
grants IIS-1320538 and NCSE-1422492.

References
Loulwah AlSumait, Daniel Barbará, James Gentle,

and Carlotta Domeniconi. 2009. Topic significance
ranking of LDA generative models. In Proceedings
of European Conference of Machine Learning.

David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings
of the International Conference of Machine Learn-
ing.

Sanjeev Arora, Rong Ge, Yonatan Halpern, David
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In Pro-
ceedings of the International Conference of Machine
Learning.

Sanjeev Arora, Rong Ge, Ravindran Kannan, and
Ankur Moitra. 2012a. Computing a nonnegative
matrix factorization–provably. In Proceedings of the
forty-fourth annual ACM symposium on Theory of
computing.

Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012b.
Learning topic models–going beyond svd. In Fifty-
Third IEEE Annual Symposium on Foundations of
Computer Science.

David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. Journal of Machine
Learning Research 3:993–1022.

Jaegul Choo, Changhyun Lee, Chandan K Reddy, and
Heejung Park. 2013. Utopian: User-driven topic
modeling based on interactive nonnegative matrix
factorization. Visualization and Computer Graph-
ics, IEEE Transactions on 19(12):1992–2001.

904



Jason Chuang, Christopher D Manning, and Jeffrey
Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of
the International Working Conference on Advanced
Visual Interfaces.

Kristin A. Cook and James J. Thomas. 2005. Illuminat-
ing the path: The research and development agenda
for visual analytics. Technical report, Pacific North-
west National Laboratory (PNNL), Richland, WA
(US).

Ralph D’Agostino and Egon S Pearson. 1973. Tests for
departure from normality. empirical results for the
distributions of b2 and b1. Biometrika 60(3):613–
622.

Matthew J Gardner, Joshua Lutes, Jeff Lund, Josh
Hansen, Dan Walker, Eric Ringger, and Kevin Seppi.
2010. The topic browser: An interactive tool for
browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Harold Stanley Heaps. 1978. Information retrieval:
Computational and theoretical aspects, Academic
Press, Inc., pages 206–208.

Matthew Hoffman, Francis R Bach, and David M Blei.
2010. Online learning for latent dirichlet allocation.
In advances in neural information processing sys-
tems.

Matthew D Hoffman, David M Blei, Chong Wang, and
John William Paisley. 2013. Stochastic variational
inference. Journal of Machine Learning Research
14(1):1303–1347.

Yuening Hu and Jordan Boyd-Graber. 2012. Efficient
tree-based topic modeling. In Proceedings of the As-
sociation for Computational Linguistics.

Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2014. Interactive topic modeling.
Machine Learning 95(3):423–469.

Moontae Lee and David Mimno. 2014. Low-
dimensional embeddings for interpretable anchor-
based topic inference. In Proceedings of Empirical
Methods in Natural Language Processing.

Marina Meilă. 2003. Comparing clusterings by the
variation of information. In Learning theory and
kernel machines.

David Newman, Jey Han Lau, Karl Grieser, and Timo-
thy Baldwin. 2010. Automatic evaluation of topic
coherence. In Proceedings of the Association for
Computational Linguistics.

Thang Nguyen, Jordan Boyd-Graber, Jeffrey Lund,
Kevin Seppi, and Eric Ringger. 2015. Is your anchor
going up or down? Fast and accurate supervised
topic models. In Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.

Thang Nguyen, Yuening Hu, and Jordan L Boyd-
Graber. 2014. Anchors regularized: Adding robust-
ness and extensibility to scalable topic-modeling al-
gorithms. In Proceedings of the Association for
Computational Linguistics.

Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of Un-
certainty in Artificial Intelligence.

Timothy Rubin, America Chambers, Padhraic Smyth,
and Mark Steyvers. 2012. Statistical topic mod-
els for multi-label document classification. Machine
Learning 1(88):157–208.

Ivan Titov and Ryan T McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of the Association for Compu-
tational Linguistics.

Xing Wei and W Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In Proceedings of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval.

Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics bulletin 1(6):80–83.

Ka Yee Yeung and Walter L Ruzzo. 2001. Details of
the adjusted rand index and clustering algorithms,
supplement to the paper an empirical study on prin-
cipal component analysis for clustering gene expres-
sion data. Bioinformatics 17(9):763–774.

905


	Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling

