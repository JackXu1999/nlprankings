



















































Top-Down Structurally-Constrained Neural Response Generation with Lexicalized Probabilistic Context-Free Grammar


Proceedings of NAACL-HLT 2019, pages 3762–3771
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3762

Top-Down Structurally-Constrained Neural Response Generation with
Lexicalized Probabilistic Context-Free Grammar

Wenchao Du
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213

wenchaod@cs.cmu.edu

Alan W Black
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213
awb@cs.cmu.edu

Abstract

We consider neural language generation un-
der a novel problem setting: generating the
words of a sentence according to the order of
their first appearance in its lexicalized PCFG
parse tree, in a depth-first, left-to-right man-
ner. Unlike previous tree-based language gen-
eration methods, our approach is both (i) top-
down and (ii) explicitly generating syntactic
structure at the same time. In addition, our
method combines neural model with symbolic
approach: word choice at each step is con-
strained by its predicted syntactic function.
We applied our model to the task of dialog re-
sponse generation, and found it significantly
improves over sequence-to-sequence baseline,
in terms of diversity and relevance. We also
investigated the effect of lexicalization on lan-
guage generation, and found that lexicalization
schemes that give priority to content words
have certain advantages over those focusing on
dependency relations.

1 Introduction

Neural encoder-decoder architectures have shown
promise and become very popular for natural
language generation. Over the past few years,
there has seen a surging interest in sequence-to-
sequence learning for dialog response generation
using neural encoder-decoder models (Vinyals and
Le, 2015; Serban et al., 2017). Typically, an
encoder encodes conversational context (source
side) information into vector representations, and
a decoder auto-regressively generates word tokens
conditioned on the source vectors and previously
generated words.

Two problems arise with the standard left-to-
right decoding mechanism. First, no future in-
formation is available at any step of the decoding
process, while the study of linguistic dependency
structure shows that certain words depend on the

others that come right to them. Second, preced-
ing words define the context for following words
in left-to-right, auto-regressive language models,
while linguistic theories may prefer other hierar-
chies (e.g., adjectives modifying nouns, adverbs
modifying verbs). Psycho-linguistics studies also
suggest that human may first generate the abstract
representation of the things to say, and then lin-
earize them into sentences (Dell et al., 1999).

Therefore, it is appealing to consider language
generation in alternative orders. This poses a
greater challenge because a mechanism in ex-
tra to word generation is needed for deciding
the position of each word. Some recent works
adopt a syntax-free approach to address this prob-
lem. (Mehri and Sigal, 2018) proposed a middle-
out decoder that starts from the middle of sen-
tences and finishes the rest in forward and back-
ward directions. (Mou et al., 2016) and (Li and
Sun, 2018) start with one or two predicted key-
words and generate the rest of sentences in a sim-
ilar fashion. Others incorporate tree structures
without syntactic relations and categories. (Zhou
et al., 2018) canonicalizes the dependency struc-
tures of sentences into ternary trees, and gener-
ate only the words top-down. Yet another line
of work aim to model the full syntactic trees.
(Gū et al., 2018) generates phrase structures and
part-of-speech tags along with words for machine
translation. (Dyer et al., 2016) generates shift-
reduce action sequences of context-free grammars
in addition to words for language model and pars-
ing. But words are still generated in left-to-right
order in their approaches.

In the domain of dialog, we believe language
generation can benefit from alternative orders, for
the same reasons argued earlier. On the other
hand, in human conversations, the structure of
utterances usually correspond with dialog states
(e.g., wh-noun or wh-adverb phrases are more



3763

likely to be used in a request state), so modelling
phrase structures can potentially help capturing
discourse level information. In order to be able to
generate complete syntactic trees, while be flexi-
ble about word generation order at the same time,
the use of lexicalized grammar becomes a natural
choice.

2 Related Work

Recent years has seen works in language model
and generation through alternative orders. (Zhang
et al., 2016) developed a top-down neural archi-
tecture for language model that alternates between
four LSTM decoders according to given depen-
dency relations. (Ford et al., 2018) proposed a
two-stage language model, of which the first stage
is a language model that generates templates, and
the second stage is a translation model that fills
in the blanks. Word generation order varies with
the choice of words that are generated at different
stages.

Language generation with tree structures has
been explored more thoroughly for neural ma-
chine translation. (Eriguchi et al., 2017) and (Aha-
roni and Goldberg, 2017) generate CFG trees in
bracketed form. (Wu et al., 2017) generates the
sequence of transitions to form dependency trees.
More recent works have focused on explicitly gen-
erating tree structures (Wang et al., 2018; Gū et al.,
2018).

Regarding neural architectures for tree gener-
ation in the field of natural language process-
ing, (Dong and Lapata, 2016) and (Yin and Neu-
big, 2017) use a single decoder with parent-
feeding mechanism to generate logical forms and
programming codes. (Gū et al., 2018) applied
the doubly-recurrent neural networks of (Alvarez-
Melis and Jaakkola, 2016) with attention mech-
anism to machine translation. Their model uses
two decoders, of which one memorizes the ances-
tors, and the other remembers the siblings. (Wang
et al., 2018) also uses two decoders, but one for
generating words and the other for generating syn-
tax trees.

In the domain of dialog response generation, the
use of syntactic structures is under-studied. (Mou
et al., 2016) and (Li and Sun, 2018) considered
starting with keywords, and finish the sentences in
forward and backward directions. Their models in
principle are not tree-structured. The closest thing
to our knowledge is by (Zhou et al., 2018). They

proposed to convert dependency trees to ternary
trees, but ignore the type of dependency relations.
In other words, they modelled on trees of which
the nodes and edges have no labels. The key dif-
ference between their approach and ours is that
we generate syntax trees with labels, and word
choices are also constrained by the labels.

3 Design Choices

We first consider the following three requirements
when generating an L-PCFG syntax tree:

Deciding the structure of children. Several
mechanisms have been proposed for deciding the
structure of children of each node in the context of
tree generation. One of them decide tree topology
by using a sequence model to generate children
one by one and predict stopping tokens (Alvarez-
Melis and Jaakkola, 2016). Then there is a simpler
approach that treats each combination of the la-
bels of the children as one token, and predict such
tokens when generating the parent node (Yin and
Neubig, 2017). For language generation, we adopt
the second approach and predict the combination
of the labels of children, i.e. the rules, for two rea-
sons: (i) the space of grammar rules is generally
sparse even when its dimensionality is exponen-
tial of the number of labels, and (ii) with sequen-
tial generation of labels, as in the first approach, it
is hard to enforce the labels of the children to form
a valid grammar rule.

Deciding the heir of a node. Recall the defini-
tion of lexicalized PCFG: let W , N , R be the sets
of lexicons, labels, and rules, where each rule is
one of the following forms:

• X(h)→ h

• X(h) → Y1(h1) . . . Yk(hk) such that there
exists i, hi = h.

where X,Y1, . . . , Yk ∈ N , h, h1, . . . , hk ∈ W .
We do not restrict ourselves to Chomsky Normal
Form, and rules can have any number of children.
The ith children in the second case is called the
heir. The key difficulty to top-down generation
of lexicalized PCFG parse tree is deciding which
child would be the heir. One way is to make
explicit decision to select the child by adding a
switch variable, at the cost of increasing the com-
plexity of the problem. Instead, we make a change
to the second case above, and simplify the prob-
lem by restricting the rules to be of the following
form:



3764

Figure 1: The first tree is the result of Stanford parser. The second one is obtained by performing content-based
lexicalization on the first tree. All labels are replaced by part-of-speech tags of heirs. Unary rules NP-me→ PRP
and VP-started→ VBN are removed.

• X(h) → Y1(h1) . . . Yk(hk) such that there
exists i, hi = h and Yi = X .

In other words, the heir would inherit both the lex-
icalization and the label of the parent (with the
possible exception that the root node may produce
children that are not labeled with “root”). When
generating a parent node and its children, we re-
strict the choice of rules to those containing the la-
bel of the parent, so the heir can be inferred from
the chosen rules by looking for the child that has
the same label as its parent (in case there are mul-
tiple children that have the same label, we choose
the rightmost one; other heuristics are possible).
Note that under such restriction we end up with
parse trees in which all labels are part-of-speech
tags.

Sequentialization of a tree. Previous works
adopt various construction orders of trees. (Zhang
et al., 2016), (Zhou et al., 2018), (Alvarez-Melis
and Jaakkola, 2016), and (Gū et al., 2018) generate
trees through level-order traversal (breadth-first,
left-to-right), whereas (Yin and Neubig, 2017) and
(Wang et al., 2018) generate trees through pre-
order traversal (depth-first, left-to-right). (Kun-
coro et al., 2018) also experimented with bottom-
up and left-corner construction orders for lan-
guage model. While finding the optimal order of
generating trees is beyond the scope of this work,
we follow (Yin and Neubig, 2017) and (Wang
et al., 2018), and generate lexicalized PCFG syn-
tax trees through pre-order traversal.

4 Methods

4.1 Definitions

In this paper, we give the following graph-
theoretic definition of L-PCFG syntax trees. Let
W and N be the sets of lexicons and labels. Let
R =

⋃∞
k=1N

k be the set of production rules.
Then an L-PCFG syntax tree T is an ordered tree
defined by the triple of vertices v ∈ V ⊂ W ×
N × R × N, edges e ∈ E ⊂ V × V , and bijec-
tion f : V × N+ → V such that (v, f(v, j)) ∈ E,
where j range from 1 to the number of children of
v. The fourth coordinate of v is the index of its
heir, that is,

(w, n, r, i) = f((w0, n0, r0, i0), i0)

=⇒ w = w0, n0 = n

We say a node v = (w, n, r, i) is a leaf if r is
unary:

v is a leaf ⇐⇒ r ∈ N

The parent of vk is denoted by vp(k) =
(wp(k),mp(k), np(k), ip(k)).

4.2 Generation Procedure

Following previous work, we sequentialize L-
PCFG parse trees and generate its content
in an auto-regressive manner. When gen-
erating kth node, we predict the lexicaliza-
tion wk and the rule rk. The pre-order his-
tory available when generating kth node is
n1· · ·nk−1, w1· · ·wk−1, r1· · · rk−1, denoted by



3765

Hk. The label “ROOT” is given at the start of gen-
eration. The label of the kth node is inferred from
the production rule of its parent and the order of
kth node among its siblings, and is used as input
together with Hk. When a leaf node is reached,
the program backtraces until it finds an ancestor
that has unfinished child, and proceeds to the first
such child.

We factor the joint probability of wk and rk
into two component: a word model and a syntax
model, as follow:

P (wk, rk |nk, Hk) =
P (wk | nk, Hk) · P (rk | nk, Hk)

The details of both models are given in the follow-
ing sections.

4.3 Lexicalization Schemes
We parse the responses in training corpus using the
lexicalized parser by (Klein and Manning, 2003)
(which we call Stanford parser for the rest of
this paper). We then replace the label of each
node with that of their heir in a bottom-up man-
ner. Unary rules at non-leaf nodes are removed
as they become redundant given our definition of
lexiclaized PCFG.

Stanford parser lexicalizes PCFG phrase struc-
tures by looking for the most likely combination of
phrase structure and dependency structure. While
their approach is optimized for parsing, the syntax
trees lexicalized this way has a drawback for the
purpose of generation. Empirically, their parser
tends to lexicalize the first few nodes with auxil-
iary verbs or common verbs (e.g. be, must), and
in some cases prefer function words over content
words (e.g. in preposition phrases). We hypoth-
esize that choosing content words over functions,
or infrequent words over frequent words as lex-
icalization heads will help making the generation
more specific and meaningful. Hence, we consider
two alternative lexicalization schemes:

Content-based lexicalization. We rank words
according to their part-of-speech in the sentence
by the following order: nouns > verbs = adjec-
tives > adverbs > everything else. If two words
have the same rank, we give priority to the right-
most one. See Figure 1 for an example.

Frequency-based lexicalization. We ignore
part-of-speech information and rank all words by
their frequencies. We regard less frequent words
as more important.

4.4 Encoding Tree Histories

To represent the state of a tree node by encoding
its pre-order historyHk, we use 3 LSTMs to mem-
orize the lexical and grammatical contents in Hk.

Encoding lexicalization history. We use 2
LSTMs to encode the lexicalization history, i.e.
w1· · ·wk−1: a surface decoder, Ls, which takes
the lexicalization of the leaves in the history as in-
puts; and an ancestor decoder, La, which is given
the lexicalization of the ancestors of the current
node. This is another form of doubly-recurrent
neural networks. Different from (Alvarez-Melis
and Jaakkola, 2016), we chose to encode leaves
instead of siblings. Denote the lexicalizaiton
of leaves and ancestors in Hk by {wl(k)i} and
{wa(k)i}. We show that for an L-PCFG syntax
tree, {wl(k)i} and {wa(k)i} sufficiently cover the
lexical content of Hk:

Proposition. For any index set Ik ⊂
{1, 2, . . . , k− 1}, if wn ∈ {wj}j∈Ik for all n < k,
then {l(k)i}

⋃
{a(k)i} ⊂ Ik, i.e. {l(k)i} and

{a(k)i} together is the minimal index set to cover
w1· · ·wk−1.

Encoding syntactic history. We encode the
previous labels and rules using the full history with
a single LSTM,Lg. At step k, the input toLg is the
concatenation of the embeddings of nk, the rule of
parent node rp(k), and depth of the node d. The
depths of nodes deeper than 10 are rounded down
to 10.

4.5 Encoding Source and Attention
Mechanism

We adopt attention mechanism into our architec-
ture for response generation. We use a one-layered
LSTM to encode the dialog history, which is the
concatenation of the last few utterances. The ini-
tial hidden states of Ls, La, and Lg is computed
from the last hidden state of source encoder using
2 fully-connected layers with rectified linear ac-
tivation. At time step t, the concatenation of the
hidden states of Ls and La at step t − 1 is used
as the key for querying the source. The attention
weights are the inner products of the key and the
hidden states at source side, normalized by soft-
max function. The weighted sum of source hidden
states results in the attention context, c(k)

4.6 Decoding

Denote the hidden states of Ls, La, Lg at node k
as hs(k), ha(k), hg(k). Denote the softmax func-



3766

Figure 2: Demonstration of inputs and outputs at node DT. The sequence of inputs to each encoder are shown in
the graph. The inputs to Lg is a sequence of labels, rules of parents, and tree depths (only labels are shown). Ls
and La are used for predicting the word for DT. Ls and Lg are used for predicting the rule for DT. “RULE: DT”
indicates DT will be a leaf node since the number of symbols is 1. In this tree, words are generated in the order:
daughter - I - have - a.

tion by σ. Ew ∈ R|W |×dw and Er ∈ R|P |×dr are
embedding matrices for words, labels, and rules.
Aw ∈ Rnw×dw and Ar ∈ Rnp×dr are weight
matrices (nw, np are the dimensions of input neu-
rons). We use weight tying (Press and Wolf, 2017)
to limit the search space for parameters.

Word prediction. To decode forwk, we use the
the hidden states of surface decoder Ls and ances-
tor decoder La. If vk is a heir, then

P (wk | nk, Hk) =

{
1 wk = wp(k)

0 wk 6= wp(k)

Otherwise, the probability of wk is given by:

P (wk |nk, Hk) =
σ(tanh([hs(k);ha(k); c(k)]Aw)E

T
w)

At decoding time, we impose an additional con-
straint that wk be a valid word for label nk, to en-
force grammaticality. This is estimated from the
co-occurrence of wk and nk in the tagged training
corpus. We only use those words whose frequency
of co-occurrence with the given label is above a
certain threshold.

Rule prediction. The probability of rk is given
by

P (rk |nk, Hk) =
σ(tanh([hs(k);hg(k); c(k)]Ar)E

T
r )

Given the definition of L-PCFG syntax tree, we
only consider rules that contain nk at decoding
time. There is one exception: at “ROOT” node,
only unary rules are considered, and they do not
have to contain the label “ROOT”.

Hence, we train our architecture by minimizing
the negative log-likelihood of words and rules:

− logP (T ) =

− 1
|W (T )|

∑
k

vk 6=f(vp(k),ip(k))

logP (wk | nk, Hk)

− 1
|T |

∑
k

logP (rk | nk, Hk)

where |W (T )| is the number of non-heir nodes (or
the number of words in the original sentence), and
|T | is the number of nodes in T . Note that the log
probability of each word in the sentence appears
exactly once in the above equation. At test time,
we conduct beam search and use the same equa-
tion to score each generation for selecting words
and rules.

In our experiments, we use unlexicalized PCFG
as an additional baseline. We still replace the la-
bels of each node with their heirs’ in the parse tree
returned from Stanford parser, but words are gen-
erated only at leaf nodes. This baseline has syn-
tactic structure while generating words from left
to right. We use it as a test against top-down gen-
eration of words with syntax.

4.7 Training Details
All models are implemented using PyTorch. The
hidden size of all LSTM encoders and decoders
are 512. The size of embeddings of words, labels,
rules, and tree depth are 300. We trained our mod-
els using stochastic gradient descent (SGD) with
momentum and exponential learning rate decay.
Dropout is applied to the input and output layer
of LSTMs.



3767

Stand. Depend. Content
Nouns 8.21 7.70 6.25
Verbs 9.56 7.07 7.03

Adjectives 7.38 7.82 7.77
Adverbs 6.38 6.99 7.29

Other 5.64 6.23 6.62

Table 1: Average absolute positions of different type of
words.

Stand. Depend. Freq.
1 .0262 .0095 .0002
2 .0149 .0274 .0238
3 .0147 .0116 .0156
4 .0133 .0145 .0147
5 .0134 .0131 .0146

Table 2: Average frequency of first five words in differ-
ent generation orders.

5 Experiments and Analysis

5.1 Data

We evaluate our model for dialog response gen-
eration on Persona dataset ((Zhang et al., 2018)).
Each person is give a list of persona descriptions
in simple sentences, and they are required to con-
verse according to the given persona. We use
last 3 utterances for each response as source. We
prepend persona descriptions to source. We use
global attention over persona descriptions to com-
pute context vectors. During pre-processing, we
truncate all trailing punctuations.

5.2 Positional Statistics

We measure how early do each type of words ap-
pear in different generation orders – standard left-
to-right order, dependency-based lexicalization (as
in Stanford parser), and content-based lexicaliza-
tion. The earlier a word appear, the less context
there is for predicting it. As shown in Table 1,
content-based lexicalization can make nouns and
verbs appear much earlier, while delaying function
words.

To verify frequency-based lexicalization is
making infrequent words appear earlier, we show
the average frequencies of the first five words.
The first few words are more important since
they decide the context for generating the follow-
ing words. In Table 2, the first two words of
each parse tree under frequency-based lexicaliza-

Seq2seq Ours
Standard 3.682 N/A
Dependency 4.015 3.964
Content 4.115 3.865
Frequency 4.088 3.827

Table 3: Perplexities.

tion are much less frequent.

5.3 Perplexity

We compare per word likelihood given different
generation orders and architectures. For left-to-
right sequence decoder, the non-standard gener-
ation orders are obtained by linearizing L-PCFG
parse trees in pre-order traversal, and words of
heirs are not repeated in the linearization. Note
that our word model generates word without us-
ing rules and labels as inputs to its networks. As
can be seen from Table 3, alternative word gen-
eration orders all make it harder for standard left-
to-right sequence decoder to learn to predict the
next word. On the other hand, using a doubly-
recurrent architecture, specifically the surface de-
coder and the ancestor decoder, can improve per-
plexity scores for top-down word generation over
the left-to-right decoder. While our word model
with top-down word generation orders has higher
perplexity scores than simple model with standard
generation order, we emphasize that perplexity is
not an appropriate measure for generation tasks.

5.4 Evaluation Metrics

Word Overlap Based Metrics. We use BLEU
(Papineni et al., 2002), ROUGE (Lin, 2004), and
METEOR (Banerjee and Lavie, 2005) scores as
automatic evaluation metrics. While the reliabil-
ity of such metrics has been criticized (Liu et al.,
2016), there is also evidence that for task-oriented
domains, these metrics correlate with human judg-
ment to a certain extent (Sharma et al., 2017).

Word Embedding Based Metrics. We evalu-
ate the semantic similarity between generated re-
sponses and human responses/persona by the co-
sine distance of their sentence embeddings. We
use the word averaging approach by (Arora et al.,
2016) to embed the responses, which has been
demonstrated to be very good at capturing lexical
level semantic similarity. The normalizing singu-
lar vector is obtained from the responses in train-
ing set.



3768

Human Baseline PCFG
L-PCFG

(Dependency)
L-PCFG
(Content)

L-PCFG
(Frequency)

Length 10.35 7.21 9.39 8.96 11.04 9.74
BLEU N/A 0.1926 0.2041 0.2038 0.2093 0.2028

ROUGE-L N/A 0.1639 0.1448 0.1571 0.1565 0.1624
METEOR N/A 0.0718 0.0704 0.0721 0.0777 0.0739

Cos. Sim. to targets N/A 0.0913 0.0385 0.0824 0.0964 0.0883
Cos. Sim. to last utterance 0.1108 0.0880 0.0714 0.0921 0.1113 0.0971

Cos. Sim. to persona 0.1489 0.0206 0.0455 0.0796 0.0691 0.0605
Distinct uni-gram 6327 678 725 891 813 874
Distinct bi-gram 44376 2802 3368 5306 5427 3680
Distinct tri-gram 77884 4844 6061 10348 10787 6441

Inertia 11771 4385 5027 6319 3756 3959
BLEU to training set 0.4334 0.8471 0.5320 0.5874 0.5402 0.6331

ROUGE to training set 0.4728 0.8970 0.5837 0.6701 0.6226 0.7175

Table 4: Evaluation results on Persona dataset.

Context Human Seq2seq L-PCFG
i am great . i just got
back from the club

this is my favorite time
of the year season wise

that is cool what do
you do for a living

awesome ! i am getting
ready to go to the club

sure i like tv ,
what do you watch ?

really anything ,
what about you ?

i watch a lot of tv
i watch lot of tv movies .

i like to watch tv

oh . tell me something
about yourself .

well i do not like heights
very much and i love

animals . what about you ?

i am an accountant .
what do you do

i am trying to learn how
to work with animals

i live in texas . i love
riding my bike here .

are you a christian ?
i am jewish

i have never been to
the west coast . do

you have any hobbies

i do too ! i wish i
was there so i

can do that for school

Table 5: Examples of generated responses.

Novelty and diversity. We measure word over-
lapping between generated responses and the re-
sponses in training set using BLEU and ROUGE
as a proxy for novelty. The responses in training
set with most common words with generated re-
sponses are used as references. For diversity, we
count the number of distinct n-grams. In addi-
tion, we perform a k-means clustering on the sen-
tence embeddings of responses into 10 clusters,
and measure average squared Euclidean distance
between members of each cluster (Inertia). The
larger the number, the harder it is to separate em-
beddings into 10 clusters, thus the greater the di-
versity.

5.5 Main Results

5.5.1 Quantitative Analysis

Evaluation results are shown in Table 4. For met-
rics that are not measured using ground truth re-

sponse as reference, we consider the closer to the
number for human responses the better. We first
look at measures for overall generation quality.
We can see modelling syntactic structures is capa-
ble of generating longer responses. BLEU scores
are positively correlated with lengths. While syn-
tactic models do better on BLEU, and slightly bet-
ter on METEOR than sequence-to-sequence base-
line, they are generally not on par with the baseline
in terms of ROUGE-L, except for frequency-based
lexicalization. Among grammar models, lexical-
ized grammars out-performed unlexicalized gram-
mar.

Relevance is measured using cosine similarities
with the previous utterance and persona. Syntac-
tic models with lexicalized grammar beat the base-
line in terms of relevance. Furthermore, content-
based lexicalization is much more on topic with
the last source utterance than dependency-based



3769

Lg + Ls Lg + Ls + La Lg
Lengths 11.04 7.95 8.46

Distinct uni-gram 813 383 376
Distinct bi-gram 5427 1763 1775
Distinct tri-gram 10787 3061 3522

Inertia 3756 2049 2346
BLEU on training 0.5402 0.8056 0.6612

ROUGE on training 0.6226 0.8543 0.7417

Table 6: Ablation studies.

and frequence-based lexicalization. Dependency-
based lexicalization is best at being adherent
to personas than the other two lexicalization
schemes.

All syntactic models generate more novel re-
sponses than sequence-to-sequence baseline, as
reflected in the last two rows in 4. This is
consistent with the observation that sequence-to-
sequence model exhibits retrieval-like behaviour,
selecting what is most common in the training cor-
pus. Syntactic models also have larger vocabular-
ies. As for cluster analysis, unlexicalized grammar
model and dependency-based lexicalized gram-
mar model have better diversity than sequence-
to-sequence model; content-based and frequency-
based lexicalization have slightly smaller inertia
than the baseline.

5.5.2 Qualitative Analysis

We present a few examples generated by
sequence-to-sequence baseline and L-PCFG
model. There is a clear difference of how left-
to-right decoder and L-PCFG tree decoder do
conjunctions. Most of the time, standard LSTM
decoder combine sentences with periods, while
tree decoders learn to use conjunction words, or
even clauses.

We also performed an error analysis on the gen-
erated responses by L-PCFG, and in Table 7 we se-
lected the most peculiar and representative. These
examples are all syntactically fine, but they do not
follow the convention of the language or common
sense. The first example contains the most com-
mon errors in the responses generated by L-PCFG:
misuse of prepositions and determiners. It can be
fixed by replacing “as a” with “for”. The error of
other two examples have even less to do with syn-
tax. The second one misuses the verb “be”, which
is probably caused by the high frequency of the
word in the corpus. The error of the third exam-

ple is beyond surface level. Note that phrases such
as “cooking as a dinner” and “be a dog” never ap-
pear in the corpus. It is clear that L-PCFG models
are learning to make compositions of words and
phrases, unlike standard LSTM decoder, which
seems to only memorize word combinations.

i am doing well . just finished cooking as a dinner
i am sure it is nice . i am going to be a dog
i like to ride my horses on my bike

Table 7: Some peculiar examples generated by L-
PCFG models.

5.6 Ablation Studies

We perform ablation studies on our architecture,
with content-based lexicalization. Specifically,
we consider two alternative ways of making rule
prediciton. The first one takes only the hidden
state of Lg for predicting rules, hence making
the prediction of rules entirely independent from
words. The second one takes both the hidden
states of Ls and La, together with Lg, for predict-
ing rules, in which way future lexical information
is used for the construction of syntax trees.

For rule prediction using the hidden states of
both surface decoder and ancestor decoder, we no-
ticed a significant drop in diversity in generated
responses. The model over-predicts “what do you
do for a living” for than 50% of the time, and the
lengths of responses tend to be shorter.

For the other choice in which rule prediction
is independent from words, the results are closer
to the original model, but there are still some de-
creases in lengths and lexical diversity. Upon man-
ual inspection, we found that this model behaved
more like sequence-to-sequence model. There are
less compound sentences, and more conjunctions
of simple sentences by end punctuation marks.



3770

The proportion of simple sentences are also larger.

6 Conclusion

We consider the problem of generating natural lan-
guage in alternative orders and with syntactic tree
structures, with the use of lexicalized grammar.
By incorporating syntactic structures, our models
are capable of generating longer sentences. By
changing lexicalizaion schemes and making con-
tent words appear earlier in generation process,
our models are able to make word choices that are
more relevant to source. Furthermore, incorpo-
rating syntax facilitates response generation with
richer vocabularies and more complex structures.
On the other hand, as shown in our error analysis,
there is still room for improvement on discourse
and pragmatics level.

Acknowledgments

This material is based upon work partially sup-
ported by the National Science Foundation (Award
# 1722822). Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the author(s) and do not neces-
sarily reflect the views of National Science Foun-
dation, and no official endorsement should be in-
ferred.

References
Roee Aharoni and Yoav Goldberg. 2017. Towards

string-to-tree neural machine translation. In Pro-
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), volume 2, pages 132–140.

David Alvarez-Melis and Tommi S Jaakkola. 2016.
Tree-structured decoding with doubly-recurrent
neural networks.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016.
A simple but tough-to-beat baseline for sentence em-
beddings.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Gary S Dell, Franklin Chang, and Zenzi M Grif-
fin. 1999. Connectionist models of language pro-
duction: Lexical access and grammatical encoding.
Cognitive Science, 23(4):517–542.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 33–43.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A Smith. 2016. Recurrent neural network
grammars. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 199–209.

Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun
Cho. 2017. Learning to parse and translate improves
neural machine translation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 72–78.

Nicolas Ford, Daniel Duckworth, Mohammad
Norouzi, and George Dahl. 2018. The importance
of generation order in language modeling. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
2942–2946.

Jetic Gū, Hassan S Shavarani, and Anoop Sarkar. 2018.
Top-down tree structured decoding with syntactic
connections for neural machine translation and pars-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 401–413.

Dan Klein and Christopher D Manning. 2003. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in neural information
processing systems, pages 3–10.

Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-
gatama, Stephen Clark, and Phil Blunsom. 2018.
Lstms can learn syntax-sensitive dependencies well,
but modeling structure makes them better. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1426–1436.

Jingyuan Li and Xiao Sun. 2018. A syntactically con-
strained bidirectional-asynchronous approach for
emotional conversation generation. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 678–683.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An em-
pirical study of unsupervised evaluation metrics for
dialogue response generation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, pages 2122–2132. Associa-
tion for Computational Linguistics.

http://www.aclweb.org/anthology/W04-1013
http://www.aclweb.org/anthology/W04-1013
https://doi.org/10.18653/v1/D16-1230
https://doi.org/10.18653/v1/D16-1230
https://doi.org/10.18653/v1/D16-1230


3771

Shikib Mehri and Leonid Sigal. 2018. Middle-out de-
coding. In Advances in Neural Information Process-
ing Systems, pages 5519–5530.

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,
and Zhi Jin. 2016. Sequence to backward and for-
ward sequences: A content-introducing approach to
generative short-text conversation. In Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 3349–3358.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 157–163. Association
for Computational Linguistics.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In AAAI, pages 3295–3301.

Shikhar Sharma, Layla El Asri, Hannes Schulz, and
Jeremie Zumer. 2017. Relevance of unsupervised
metrics in task-oriented dialogue for evaluating nat-
ural language generation. CoRR, abs/1706.09799.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Xinyi Wang, Hieu Pham, Pengcheng Yin, and Graham
Neubig. 2018. A tree-based decoder for neural ma-
chine translation. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 4772–4777.

Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li,
and Ming Zhou. 2017. Sequence-to-dependency
neural machine translation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 698–707.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 440–450.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you
have pets too? In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 2204–
2213. Association for Computational Linguistics.

Xingxing Zhang, Liang Lu, and Mirella Lapata. 2016.
Top-down tree long short-term memory networks.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 310–320.

Ganbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao,
Fen Lin, Bo Chen, and Qing He. 2018. Tree-
structured neural machine for linguistics-aware sen-
tence generation. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ficial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artificial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pages 5722–5729. AAAI Press.

A Appendices

A.1 Proof sketch for Proposition in Section
4.4

We prove by contradiction. Suppose there is a
node nk, whose lexicalization is not in the leaves
before node k, nor in the ancestors of node nk.
There must be a leaf l inheriting the lexicalization
of node nk. Since nk is not an ancestor of node k,
l must be a leaf before k, so we have a contradic-
tion.

http://www.aclweb.org/anthology/P02-1040
http://www.aclweb.org/anthology/P02-1040
http://aclweb.org/anthology/E17-2025
http://aclweb.org/anthology/E17-2025
http://aclweb.org/anthology/P18-1205
http://aclweb.org/anthology/P18-1205
http://aclweb.org/anthology/P18-1205
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16567
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16567
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16567

