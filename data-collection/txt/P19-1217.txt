



















































Inferential Machine Comprehension: Answering Questions by Recursively Deducing the Evidence Chain from Text


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2241–2251
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2241

Inferential Machine Comprehension: Answering Questions by
Recursively Deducing the Evidence Chain from Text

Jianxing Yu1,2, Zheng-Jun Zha3, Jian Yin1,2 ∗
1School of Data and Computer Science, Sun Yat-sen University

2Guangdong Key Laboratory of Big Data Analysis and Processing, China
3School of Information Science and Technology, University of Science and Technology of China

{yujx26, issjyin}@mail.sysu.edu.cn
zhazj@ustc.edu.cn

Abstract

This paper focuses on the topic of inferential
machine comprehension, which aims to ful-
ly understand the meanings of given text to
answer generic questions, especially the ones
needed reasoning skills. In particular, we first
encode the given document, question and op-
tions in a context aware way. We then propose
a new network to solve the inference problem
by decomposing it into a series of attention-
based reasoning steps. The result of the pre-
vious step acts as the context of next step. To
make each step can be directly inferred from
the text, we design an operational cell with pri-
or structure. By recursively linking the cells,
the inferred results are synthesized together to
form the evidence chain for reasoning, where
the reasoning direction can be guided by im-
posing structural constraints to regulate inter-
actions on the cells. Moreover, a termination
mechanism is introduced to dynamically de-
termine the uncertain reasoning depth, and the
network is trained by reinforcement learning.
Experimental results on 3 popular data sets, in-
cluding MCTest, RACE and MultiRC, demon-
strate the effectiveness of our approach.

1 Introduction

Machine comprehension is one of the hot research
topics in natural language processing. It mea-
sures the machine’s ability to understand the se-
mantics of a given document via answering ques-
tions related to the document. Towards this task,
many datasets and corresponding methods have
been proposed. In most of these datasets, such
as CNN/Daily Mail (Hermann et al., 2015), and
SQuAD (Rajpurkar et al., 2016), the answer is
often a single entity or a text span in the docu-
ment. That leads to the fact that lots of ques-
tions can be solved trivially via word and con-
text matching (Trischler et al., 2016a) instead of

∗Corresponding author.

Figure 1: Sample of question needed reasoning skill.
Correct answer is marked with an asterisk

genuine comprehension on text. To alleviate this
issue, some datasets are released, such as M-
CTest (Richardson et al., 2013), RACE (Lai et al.,
2017) and MultiRC (Khashabi et al., 2018), where
the answers are not restricted to be the text span-
s in the document; instead, they can be described
in any words. Specially, a significant proportion
of questions require reasoning which is a sophis-
ticated comprehension ability to choose the right
answers. As shown in Figure 1, the question asks
the reason for the phenomenon on sentence S5.
The answer has to be deduced over the logical re-
lations among sentence S3, S4 and S5, and then
entailed from S3 to the correct option B. Diffi-
cultly, such deduced chain is not explicitly given
but expressed on text semantics. Existing methods
primarily focus on document-question interaction
to capture the context similarity for answer span
matching (Wang et al., 2017). They have mini-
mal capability to synthesize supported facts scat-
tered across multiple sentences to form the evi-
dence chain which is crucial for reasoning.

To support inference, mainstream methods can
be summarized into three folds. One is convert-
ing the unstructured document to formal predicate
expressions, on which to perform mathematical d-



2242

eduction via Bayesian network or first-order log-
ic. The conversion lacks of adequate robustness
to be applicable. Another direction is to explicitly
parse the document into a relation tree, on which
to generate answers via hand-crafted rules (Sun
et al., 2018b). However, the parser often has to
cascade to the model, which is difficult to train
globally and would suffer from the error propaga-
tion problem. The third method exploits memory
network to imitate reasoning by multi-layer archi-
tecture and iterative attention mechanism (Weston
et al., 2014). Nevertheless, the reasoning ability
is insufficient due to the lack of prior structural
knowledge to lead the inference direction.

We observe that when humans answer the infer-
ential question, they often finely analyze the ques-
tion details and comprehend contextual relations
to derive an evidence chain step by step. Using
the sample in Figure 1 for illustration, humans first
investigate the question to find the useful details,
such as the question type “why”, and the aspect
asked, i.e. “some newspapers refused delivery to
distant suburbs”. Such details often play a criti-
cal role for answering. For example, why ques-
tion usually expects the causal relation that could
indicate the reasoning direction. Based on ques-
tion details, they then carefully read the document
to identify the content on which the question as-
pect mentions, that is, the sentence S5. Based on
the content, they would deduce new supported ev-
idences step by step guided by question type and
contextual relations, such as explainable relation
between S5 and S4, and casual relation among S3
and S4. By considering the options, they would
decide to stop when the observed information is
adequate already to answer the question. For in-
stance, by relevant paraphrase, S3 can entail op-
tion B that may be the answer. In this process, con-
textual relations and multi-step deduction are effi-
cient mechanisms for deriving the evidence chain.

Based on above observations, we here propose
an end-to-end approach to mimic human process
for deducing the evidence chain. In particular, we
first encode the given document, question and op-
tions by considering contextual information. We
then tackle the inference problem by proposing a
novel network that consists of a set of operational
cells. Each cell is designed with structural prior to
capture the inner working procedure of an elemen-
tary reasoning step, where the step can be direct-
ly inferred from the text without strong supervi-

sion. The cell includes the memory and three op-
erating units that work in tandem. That is, master
unit derives a series of attention-based operations
based on the question; reader unit extracts relevant
document content on the operation; and writer u-
nit performs the operation to deduce a result and
update the memory. The cells are recursively con-
nected, where the result of the previous step acts as
the context of next step. The interactions of cell-
s are restricted by structural constraints, so as to
regulate the reasoning direction. With such struc-
tural multi-step design, the network can integrate
the supported facts by contextual relations to build
the evidence chain in arbitrarily complex acyclic
form. Since the reasoning depth is uncertain, a
termination mechanism is exploited to adaptive-
ly determine the ending. Moreover, a reinforce-
ment approach is employed for effective training.
Experiments are conducted on 3 popular data set-
s that contain questions required reasoning skills,
including MCTest, RACE and MultiRC. The re-
sults show the effectiveness of our approach.

The main contributions of this paper include,

• We design a new network that can answer in-
ferential question by recursively deducing the
evidence chain from the text.
• We propose an effective termination mech-

anism which can dynamically determine the
uncertain reasoning depth.
• We employ a reinforcement training ap-

proach and conduct extensive experiments.

The rest of this paper is organized as follows.
Section 2 elaborates our approach on the infer-
ential framework. Section 3 presents the experi-
mental results. Section 4 reviews related work and
Section 5 concludes this paper with future works.

2 Approach

As shown in Figure 2, our approach consists of
three components, including input representation,
inferential network composed out of multiple cell-
s, and output. Next, we define some notations, and
then elaborate the details on each component.

2.1 Notations and Problem Formulation

Given a document D in unstructured text, the task
of machine comprehension is to answer the ques-
tions according to the semantics of the document.
In this paper, multi-choice questions are our ma-
jor focus. Thus, a set of plausible answer option-



2243

Figure 2: Overview of the our approach

s are assumed to be provided, and the task is re-
duced to select a correct option from the given set.
Formally, let q represent the question, of length
S, where {w1, · · · , wS} are the question words;
O = {o1, · · · , oL} denotes an option set. For a
given document and question x = (D, q), a s-
core h(x, y) ∈ R is assigned for each candidate
in the option set y = o ∈ O, so as to measure its
probability of being the correct answer. The op-
tion with highest score is outputted as the answer
ŷ = argmaxy∈Oh(x, y).

2.2 Input Representation

We first encode the input text into distributed vec-
tor representations by taking account the context.

Question: Two stages are conducted on the en-
coding. (1) We convert the question into a se-
quence of learned word embeddings by looking up
the pre-trained vectors, such as GloVe (Penning-
ton et al., 2014). By considering the question type
would help inference, we customize an embedding
to indicate such type via linguistic prior knowl-
edge, such as the positions of interrogative word-
s are often relatively fixed, and the correspond-
ing parts of speech (POS) are mainly adverbs or
conjunctions, etc. Practically, we utilize position
and POS embedding (Li et al., 2018b) generated
by word embedding tool. That is, the embedding
layer is a W ∈ Rd×v, where d is the dimension
and v denotes the number of instances. (2) We
concatenate the embeddings of word, position and
POS, and feed them into a bi-directional GRU (Bi-
GRU) (Cho et al., 2014) to incorporate sequential
context. Then we can yield two kinds of represen-
tations, including (a) contextual words: a series of
output states cws|Ss=1 that represent each word in
the context of the question, where cws = [

←−
hs,
−→
hs],

←−
hs and

−→
hs are the sth hidden states in the back-

ward and forward GRU passes respectively; and
(b) overall encoding: q = [←−−cw1,−−→cwS ], the con-
catenation of the final hidden states.

Options: Each option word is embedded by
pre-trained vectors and then option is contextually
encoded by BiGRU to generate an overall vector.

Document: Three steps are performed on the
encoding. (1) We encode each document sentence
by considering context via BiGRU as aforemen-
tioned. The sentence is transformed into an ni× d
matrix, where ni is the size of words in the sen-
tence i, and d is the dimension. (2) We conduct
attention to compress the sentence encoding into a
fixed size vector, and focus on the important com-
ponents. Intuitively, long sentence may contain
multiple significant parts, where each would help
inference. For example, two clauses are linked
by “or” with the causal relation in the sentence
“The oil spill must be stopped or it will spread
for miles.” The clauses and the contextual rela-
tion can assist answer the question “Why must
the oil spill be stopped?” To model such situa-
tion, structured self attention technique proposed
by Lin et al. (2017) is utilized. It can convert the
sentence into a J×dmatrix, attending at J signif-
icant parts of the sentence in a context aware way.
(3) All sentence matrices are fed into another Bi-
GRU, so as to capture the context between the sen-
tences. That is DH×J×d = {dsdh,j |

H,J
h,j=1,1}, where

H is the sentences size, ds is the sentence vector.

2.3 Micro-Infer Cell
Micro-infer is a recurrent cell designed to model
the mechanism of an atomic reasoning step. The
cell consists of one memory unit and three opera-
tional units, including the master unit, reader u-
nit and writer unit. The memory independently



2244

Figure 3: Flow chart of the master unit

stores the intermediate results obtained from the
reasoning process up to the tth step. Based on the
memory state, three operational units work togeth-
er in series to accomplish the reasoning process.
In particular, master unit analyzes the question de-
tails to focus on certain aspect via self-attention;
reader unit then extracts related content, guided by
the question aspect and text context; and the writ-
er unit iteratively integrates the content with pre-
ceding results from the memory to produce a new
intermediate result. The interactions between the
cell’s units are regulated by structured constrain-
s. Specially, the master outcome can only indi-
rectly guide the integration of relevant content into
the memory state by soft-attention maps and gat-
ing mechanisms. Moreover, a termination gate is
introduced to adaptively determine ending of the
inference. In the following, we detail the formal
specifications of three operational units in the cell.

2.3.1 Master Unit
As presented in Figure 3, this unit consists of two
components, involving the termination mechanis-
m and question analysis.

Termination Mechanism
A maximum step is set to guarantee termina-

tion. Since the complexity of the questions is dif-
ferent, the reasoning depths are uncertain. To dy-
namically adjust to such depth, a terminated gate
is designed by considering two conditions. That
is, the correlation between the intermediate result
mt−1 and the reasoning operation at−1 in previ-
ous step, as well as mt−1 and candidate answer
options ol|Ll=1. When both conditions are met,
an acceptable answer is highly probable to ob-
tain. Technically, the correlations are calculated
by Eq.(1), i.e. mt−1 � at−1, mt−1 � ol, respec-
tively. We then combine these two factors to get
tat,l, and utilize a sigmoid layer to estimate the
ending probability for a certain option. By maxi-
mizing over all the options, a termination function

fts(mt−1, at−1, ol|Ll=1; θts) is generated, where θts
is a parameter set, namely (W d×2dta , b

d
ta). Based on

the function, a binary random variable tt is prob-
abilistically drawn as tt ∼ p(·|fts(·; θts)). If tt is
True, stop and execute the answer module accord-
ingly; otherwise, continue the tth reasoning step.

tat,l =W
d×2d
ta [mt−1 � at−1,mt−1 � ol] + bdta

fts(·; θts) = max{sigmoid(tat,l)|Ll=1}
(1)

Question Analysis
We design a soft-attention based mechanism to

analyze the question and determine the basic op-
eration performed at each step. Instead of grasp-
ing the complex meaning on the whole question at
once, the model is encouraged to focus on certain
question aspect at a time, making the reasoning
operation can be directly inferred from the text.
Three stages are performed as follows.

Firstly, we project the question q through a
learned linear transformation to derive the aspect
related to tth reasoning step, as qt =W d×dqt q+b

d
qt.

Secondly, we use the previously performed op-
eration at−1 and memory result mt−1 as decision
base to lead tth reasoning operation. In detail-
s, we validate previous reasoning result by lever-
aging the terminated conditions in Eq.(1), that is,
pat =W

d×Ld
pa [tat,1, · · · , tat,L] + bdpa. We then in-

tegrate qt with preceding operation at−1 and val-
idation pat through a linear transformation into
aqt, as W d×3daq [qt, at−1, pat] + b

d
aq.

Thirdly, aqt is regulated by casting it back to o-
riginal question words cws|Ss=1 based on attention
in Eq.(2), so as to restrict the space of the valid rea-
soning operations and boost the convergence rate.
In particular, we calculate the correlation act,s and
pass it through a softmax layer to yield a distri-
bution avt,s over the question words. By aggre-
gation, a new reasoning operation at is generated,
represented in terms of the question words.

act,s =W
1×d
ac [aqt � cws] + b1ac

avt,s = softmax(act,s)

at =
∑S

s=1 avt,s · cws;
(2)

Briefly, the new reasoning operation at is
modeled by a function fna(q, at−1, cws; θna),
where θna is a set of parameters, including
(W d×dqt , b

d
qt,W

d×3d
aq , b

d
aq,W

d×Ld
pa , b

d
pa,W

1×d
ac , b

1
ac).

2.3.2 Reader Unit
As shown in Figure 4, reader unit retrieves rel-
evant document content that is required for per-



2245

Figure 4: Flow chart of the reader unit

forming the tth reasoning operation. The rele-
vance is measured by the content context in a soft-
attention manner, taking account of the current
reasoning operation and prior memory. We do not
rely on external tools to facilitate globally training.

To support transitive reasoning , we first extract
the document content relevant to the preceding re-
sult mt−1, resulting in dmt,h,j = [W d×dm mt−1 +
bdm]�[W d×dds dsh,j+b

d
ds]. The relevance often indi-

cates a contextual relation in the distributed space.
For instance, given a question aspect why, the con-
tents with causal relation are highly expected and
their relevant score is likely to be large.

Then, dmt,h,j is independently incorporated
with the document content dsh,j to produce
dnt,h,j , i.e. W d×2ddn [dmt,h,j , dsh,j ] + b

d
dn. This al-

lows us to also consider new information which is
not directly related to the prior intermediate result,
so as to assist parallel and inductive reasoning.

Lastly, we use soft attention to select content
that is relevant to the reasoning operation at and
candidate options ol|Ll=1. Precisely, we unify the
at and ol|Ll=1 by a linear transformation to obtain
oat, i.e. W d×Ldoa [at, ol|Ll=1] + bdoa, where the op-
tions size L is fixed and predefined. We then mea-
sure the correlation between oat and the extracted
content dnt,h,j , passing the result through softmax
layer to produce an attention distribution. By tak-
ing weighted average over the distribution, we can
retrieve related content rit by Eq.(3).

adt,h,j =W
d×d
ad [oat � dnt,h,j ] + b

d
ad

rvt,h,j = softmax(adt,h,j)

rit =
∑H;J

h=1;j=1 rvt,h,j · dsh,j ;
(3)

In short, the retrieved content rit is formulat-
ed by a function fri(mt−1, dsh,j , at, ol|Ll=1; θri),
where θri is a parameter set, involving (W d×dm , b

d
m,

W d×dds , b
d
ds,W

d×2d
dn , b

d
dn,W

d×Ld
oa , b

d
oa,W

d×d
ad , b

d
ad).

2.3.3 Writer Unit
As illustrated in Figure 5, writer unit is responsi-
ble to compute the intermediate result on the tth

Figure 5: Flow chart of the writer unit

reasoning process and update the memory state.
It integrates the retrieved content from the reader
unit with the preceding intermediate result in the
memory, guided by the tth reasoning operation in
the master unit. Details are presented as follows.

(1) Motivated by the work on relational reason-
ing (Santoro et al., 2017), we linearly incorporate
the retrieved content rit, prior result mt−1, and
question q to get mct = W d×3dmc [rit,mt−1, q] +
bdmc, so as to measure their correlations.

(2) By considering non-sequential reasoning,
such as tree or graph style, we refer to all pre-
vious memorized results instead of just the pro-
ceeding one mt−1. Motivated by the work on
scalable memory network (Miller et al., 2016),
we compute the attention of the current opera-
tion at against all previous ones ai|t−1i=1, yielding
sati = softmax(W

1×d
sa [at � ai] + b1sa). And

then we average over the previous results mi|t−1i=1
to get preceding relevant support as mpt, that is∑t−1

i=1 sati ·mi. By combining mpt with correlat-
ed result mct above, we can obtain a plausible re-
sult mut, namely W d×dmp mpt +W

d×d
mc mct + b

d
mu.

(3) The operations on some question aspects
such as why need multi-step reasoning and updat-
ing while others no need. In order to regulate the
valid reasoning space, an update gate is introduced
to determine whether to refresh the previous result
mt−1 in the memory by the new plausible result
mut. The gate αt is conditioned on the operation
at by using a learned linear transformation and a
sigmoid function. If the gate is open, the unit up-
dates the new result to the memory, otherwise, it
skips this operation and performs the next one.

αt = sigmoid(W
1×d
a at + b

1
a)

mt = αt ·mt−1 + (1− αt) ·mut;
(4)

In brief, the new reasoning resultmt is modeled
by a function fnm(mt−1, rit, q, at; θnm), where
θnm is a parameter set, including (W d×3dmc , b

d
mc,

W 1×dsa , b
1
sa,W

d×d
mp ,W

d×d
mc , b

d
mu,W

1×d
a , b

1
a).



2246

2.4 Output and Training
After the terminated condition is met, we can ob-
tain the memory state mt−1, which indicates the
final intermediate result of the reasoning process.
For the multi-choice questions focused in the pa-
per, there is a fixed set of possible answers. We
then leverage a classifier to predict an answer by
referring to the question q and options ol|Ll=1. Pre-
cisely, we first measure the correlation of mt−1 a-
gainst q and ol|Ll=1, to get mt−1 � q, mt−1 � ol.
By concatenation, we pass the outcome through a
2-layer fully-connected softmax network to derive
an answer option by Eq.(5), with ReLU activation
function to alleviate over-fitting. In summary, the
parameter set θans is (W d×2du , b

d
u, W

1×Ld
an , b

1
an).

ul = ReLU(W
d×2d
u [mt−1 � q,mt−1 � ol] + bdu)

Anst = softmax(W
1×Ld
an [u1, · · · , uL] + b1an)

(5)
Reinforcement Learning
Due to the discrete of the termination steps, the

proposed network could not be directly optimized
by back-propagation. To facilitate training, a re-
inforcement approach is used by viewing the in-
ference operations as policies, including the rea-
soning operation flow G1:T , termination decision
flow t1:T and answer prediction AT , where T is
the reasoning depth. Given ith training instance
〈qi;Di; oi〉, the expected reward r is defined to
be 1 if the predicted answer is correct, otherwise
0. The rewards on intermediate steps are 0, i.e.
{rt = 0}|T−1t=1 . Each probable value pair of (G; t;
A) corresponds to an episode, where all possible
episodes denote as A†. Let J(θ) = Eπ

[∑T
t=1 rt

]
be the total expected reward, where π(G, t,A; θ)
is a policy parameterized by the network parame-
ter θ, involving the encoding matrices θW , ques-
tion network θna, termination gate θts, reader net-
work θri, writer network θnm, and answer network
θans. To maximize the reward J , we explore gra-
dient descent optimization, with Monte-Carlo RE-
INFORCE (Williams, 1992) estimation by Eq.(6).

∇θJ(θ) = Eπ(G,t,A;θ) [∇θ log π(G, t, A; θ)(r − b)]
=

∑
(G,t,A)∈A†

π(G, t, A; θ)[∇θ log π(G, t, A; θ)(r − b)]

(6)

where b is a critic value function. It is usually
set as

∑
(G,t,A) π(G, t,A; θ)r (Shen et al., 2016)

and (r/b − 1) is often used instead of (r − b) to
achieve stability and boost the convergence speed.

3 Evaluations

In this section, we extensively evaluate the effec-
tiveness of our approach, including comparisons
with state-of-the-arts, and components analysis.

3.1 Data and Experimental Setting
As shown in Table 1, experiments were conduct-
ed on 3 popular data sets in 9 domains, including
MCTest, RACE and MultiRC. Different from da-
ta sets such as bAbI (Weston et al., 2015) that are
synthetic, the questions in the evaluated data sets
are high-quality to reflect real-world applications.

Data set #doc #q #domain ratio
MCTest 660 2,640 1 54.2%

MC160 160 640 1 53.3%
MC500 500 2,000 1 54.6%

RACE 27,933 97,687 1 25.8%
RACE-M 7,139 20,794 1 22.6%
RACE-H 28,293 69,394 1 26.9%

MultiRC 871 9,872 7 59.0%

Table 1: Statistics of the data sets. #doc, #q denote the
size of the documents and questions accordingly; ra-
tio means the proportion of the questions that require
reasoning on multiple sentences; MC160 is a human
double-check subset of MCTest, while MC500 is an
unchecked one; RACE-M and RACE-H are the subsets
of RACE on middle/ high school exams, respectively

Hyper-parameters were set as follows. For
question encoding, the POS tags were obtained by
using OpenNLP toolkit. Multiple cells were con-
nected to form the network, where the cells were
weight sharing. The maximum size of connected
cells length was 16. The network was optimized
via Adam (Kingma and Ba, 2014) with a learning
rate of 10−4 and a batch size of 64. We used gradi-
ent clipping with clipnorm of 8, and employed ear-
ly stopping based on the validation accuracy. For
word embedding, we leveraged 300-dimension
pre-trained word vectors from GloVe, where the
word embeddings were initialized randomly us-
ing a standard uniform distribution and not updat-
ed during training. The out-of-vocabulary word-
s were initialized with zero vectors. The num-
ber of hidden units in GRU was set to 256, and
the recurrent weights were initialized by random
orthogonal matrices. The other weights in GRU
were initialized from a uniform distribution be-
tween −0.01 and 0.01. We maintained the ex-
ponential moving averages on the model weight-
s with a decay rate of 0.999, and used them at
test time instead of the raw weights. Variational
dropout of 0.15 was used across the network and



2247

maximum reasoning step was set to 5. Training
usually converged within 30 epochs.

3.2 Comparisons with the State-of-the-Arts

We compared our approach with all published
baselines at the time of submission on the e-
valuated data sets. The baselines were summa-
rized as follows. (1) On RACE data set, six
baselines were employed, including three intro-
duced in the release of the data set, that is S-
liding Window (Richardson et al., 2013), Stan-
ford AR (Chen et al., 2016), and GA (Dhingra
et al., 2016); another three methods proposed re-
cently, namely DFN (Xu et al., 2017), BiAttention
250d MRU(Tay et al., 2018), and OFT (Radford
et al., 2018). (2) For MCTest data set, nine base-
lines were investigated, involving four on lexical
matching, i.e. RTE, SWD, RTE+SWD Richard-
son et al. (2013), Linguistic (Smith et al., 2015);
two methods used hidden alignment, that is Dis-
course (Narasimhan and Barzilay, 2015), Syn-
tax (Wang et al., 2015); three approaches based
on deep learning, i.e. EK (Wang et al., 2016),
PH (Trischler et al., 2016b), and HV (Li et al.,
2018a). (3) Regarding multi-choices questions in
MultiRC data set, we replace softmax to sigmoid
at the answer generation layer, so as to make pre-
diction on each option. Accordingly, five base-
lines were exploited, including three used in the
release of the data set, that is IR, SurfaceLR, and
LR (Khashabi et al., 2018); two methods currently
composed, namely OFT (Radford et al., 2018) and
Strategies (Sun et al., 2018a).

As elaborated in Figure 6, our approach outper-
formed the individual baselines on all three data
sets 1. Specifically, for RACE data set, our ap-
proach achieved the best performance and outper-
formed the second one (i.e. OFT) in terms of aver-
age accuracy by over 4.12%, 5.00% on RACE-M
and RACE-H, respectively. On MCTest data set,
the outperformance was 5.55%, 7.14% over PH
baseline which was the second best on MC160-
multi and MC500-multi, respectively, where mul-
ti is a subset of the data set that is more diffi-
cult and needs understanding multiple sentences
to answer. For MultiRC data set, our approach
led to a performance boost against the second best
one (i.e. Strategies) in terms of macro-average F1
by over 4.06%, while in terms of micro-average

1The leaderboard rankings were quickly refreshed, but
our performance is still competitive at the camera-ready time.

Figure 6: Comparisons of our approach against state-
of-the-arts on the RACE, MCTest, and MultiRC da-
ta sets respectively. Statistical significant with p-
values<0.01 using two-tailed paired test

F1 and exact match accuracy by over 5.20% and
6.64%, respectively. Such results showed that
our approach with structural multi-step design and
context aware inference can correctly answer the
questions, especially the non-trivial ones required
reasoning, thus boost the overall performance.

3.3 Ablations Studies

To gain better insight into the relative contribu-
tions of various components in our approach, em-
pirical ablation studies were performed on sev-
en aspects, including (1) position and POS aware
embedding on the question; (2) structural self-
attention in document encoding; (3) two in the
master unit, that is, guiding the reasoning op-
eration by previous memory result, and cast-
ing back to original question words; (4) extract-



2248

ing relevant content based on preceding mem-
ory result in reader unit; (5) two in the writ-
er unit, namely, non-sequential reasoning and
updating gate mechanisms. They were denot-
ed as pos aware, doc self att, rsn prior mem,
que w reg, prior mem res, non seq rsn, and
udt gate, respectively.

Figure 7: Ablation studies on various components of
our approach for affecting the performance

As displayed in Figure 7, the ablation on all e-
valuated components in our approach led to the
performance drop. The drop was more than 10%
on four components, including (1) rsn prior mem;
Lack of the memory guidance, the inferred result
from previous step could not be served as contex-
t for the next. Losing such valuable context may
lead to the misalignment of the reasoning chain.
(2) prior mem res; Discard of the preceding mem-
ory result, the relevant content with contextual re-
lations would not be identified. Such relations are
the key for transitive reasoning. (3) que w reg;
Without casting back to original question words,

Figure 8: Evaluation on the termination mechanism

it is equivalent to processing the complex ques-
tion at one step without identifying the details.
Such coarse-grained processing fails to effective-
ly regulate the space of the valid reasoning opera-
tions, and may confuse the reasoning direction. (4)
udt gate; The gate could help balance the complex
and simple questions, and reduce long-range de-
pendencies in the reasoning process by skipping,
which would improve performance. These results
further convinced us on the significant value of im-
posing strong structural priors to help the network
derive the evidence chain from text.

Furthermore, we evaluated the efficiency of the
termination mechanism by replacing it with fixed
steps from 1 up to 5. The results on the RACE and
MCTest data sets showed the replacement would
lead to drop on average accuracy and slowdown on
the convergence rate. As demonstrated in Figure
8, for fixed size reasoning, more steps performed
well at first, but deteriorated soon, while dynamic
strategy can adaptively determine the optimal ter-
mination, that may help boost the accuracy.

3.4 Case Study
Due to the use of soft attention, the proposed net-
work offers a traceable reasoning path which can
interpret the generation of the answer based on the
attended words. To better understand the reason-
ing behavior, we plotted the attention map over the
document, question and options in Figure 9 with
respect to the sample on Figure 1. From the se-
quence of the maps, we observed that the network
adaptively decided which part of an input question
should be analyzed at each hop. For example, it
first focused on the question aspect “some news-
papers refused delivery to distant suburbs.” Then
it generated evidence attended at S5 regarding to
the focused aspect by similarity. Subsequently, the
aspect “why” was focused and evidence attend-
ed at S4 was identified. We may infer that since



2249

Figure 9: Visualized attention map on figure 1 sample

S4 and previous intermediate result S5 contain the
explainable relation, they would most likely be
correlated in the distributed space with sentence-
level context aware encoding. Later, “why” was
re-focused, the evidence attended at S3 was de-
rived. Finally, option B was attended and the pro-
cess ended due to termination unit may be trig-
gered to work. Such results showed the network
can derive the answer by capturing underlying se-
mantics of the question and sequentially traversing
the relations on document based on the context.

4 Related Work

Earlier studies on machine comprehension main-
ly focused on the text span selection question.
It is often transformed into a similarity match-
ing problem and solved by feature engineering-
based methods (Smith et al., 2015) or deep neu-
ral networks. The classical features include lex-
ical features (e.g. overlapping of words, N-
gram, POS tagging) (Richardson et al., 2013),
syntactic features (Wang et al., 2015), discourse
features (Narasimhan and Barzilay, 2015), etc.
Besides, the typical networks involve Stanford
AR (Chen et al., 2016), AS Reader (Kadlec
et al., 2016), BiDAF (Seo et al., 2016), Match-
LSTM (Wang and Jiang, 2017), etc, which used
distributed vectors rather than discrete features to
better compute the contextual similarity.

To support inference, existing models can be
classified into three categories, including predi-
cate based methods (Richardson and Domingos,
2006), rule-based methods relied on external pars-
er (Sun et al., 2018b) or pre-built tree (Yu et al.,
2012), and multi-layer memory networks (Hill
et al., 2015), such as gated attended net (Dhingra
et al., 2016), double-sided attended net (Cui et al.,
2016), etc. These models either lack end-to-end
design for global training, or no prior structure to
subtly guide the reasoning direction. On the topic
of multi-hop reasoning, current models often have
to rely on the predefined graph constructed by ex-
ternal tools, such as interpretable network (Zhou
et al., 2018) on knowledge graph. The graph plain-
ly links the facts, from which the intermediate re-

sult in the next hop can be directly derived. How-
ever, in this paper, the evidence graph is not ex-
plicitly given by embodied in the text semantics.

Another related works are on Visual QA, aim-
ing to answer the compositional questions with re-
gards to a given image, such as “What color is the
matte thing to the right of the sphere in front of
the tiny blue block?” In particular, Santoro et al.
(2017) proposed a relation net, yet the net was re-
stricted to relational question, such as comparison.
Later, Hudson and Manning (2018) introduced an
iterative network. The network separated mem-
ory and control to improve interpretability. Our
work leverages such separated design. Different
from previous researches, we dedicate to inferen-
tial machine comprehension, where the question
may not be compositional, such as why question,
but requires reasoning on an unknown evidence
chain with uncertain depth. The chain has to be
inferred from the text semantics. To the best of our
knowledge, no previous studies have investigated
an end-to-end approach to address this problem.

5 Conclusions and Future Works

We have proposed a network to answer generic
questions, especially the ones needed reasoning.
We decomposed the inference problem into a se-
ries of atomic steps, where each was executed by
the operation cell designed with prior structure.
Multiple cells were recursively linked to produce
an evidence chain in a multi-hop manner. Besides,
a terminated gate was presented to dynamically
determine the uncertain reasoning depth and a re-
inforcement method was used to train the network.
Experiments on 3 popular data sets demonstrated
the efficiency of the approach. Such approach is
mainly applied to multiple-choice questions now.
In the future, we will expand it to support the ques-
tions on text span selection by using the relation
type rather than the option as the terminated con-
dition. For example, given the why question, rea-
soning process should be stopped when unrelated
relation is met, such as transitional relation.

Acknowledgments

This work is supported by the National Key
R&D Program of China (2018YFB1004404),
Key R&D Program of Guangdong Province
(2018B010107005, 2019B010120001), National
Natural Science Foundation of China (U1711262,
U1401256, U1501252, U1611264, U1711261).



2250

References
D. Chen, J. Bolton, and C.D. Manning. 2016. A

thorough examination of the cnn/daily mail reading
comprehension task. In Proceedings of the 54th A-
CL, volume abs/1606.02858.

K. Cho, B.V. Merrienboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. 2014.
Learning phrase representations using rnn encoder-
decoder for statistical machine translation. In Pro-
ceedings of EMNLP, volume abs/1704.04683.

Y. Cui, Z. Chen, S. Wei, S. Wang, T. Liu, and G. Hu.
2016. Attention-over-attention neural networks for
reading comprehension. In Proceedings of the 55th
ACL, volume abs/1607.04423.

B. Dhingra, H. Liu, W. W.Cohen, and R. Salakhutdi-
nov. 2016. Gated-attention readers for text compre-
hension. In Proceedings of the 55th ACL, volume
abs/1606.01549.

K.M. Hermann, T. Kociský, E. Grefenstette, L. Espe-
holt, W. Kay, M. Suleyman, and P. Blunsom. 2015.
Teaching machines to read and comprehend. In Pro-
ceedings of NIPS, volume abs/1506.03340.

F. Hill, A. Bordes, S. Chopra, and J. Weston. 2015.
The goldilocks principle: Reading children’s books
with explicit memory representations. In Journal of
Computer Science, abs/1511.02301.

D.A. Hudson and C.D. Manning. 2018. Compositional
attention networks for machine reasoning. In Pro-
ceedings of ICLR, volume abs/1803.03067.

R. Kadlec, M. Schmid, O. Bajgar, and J. Kleindienst.
2016. Text understanding with the attention sum
reader network. In Proceedings of the 54th ACL,
volume abs/1603.01547.

D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay,
and D. Roth. 2018. Looking beyond the surface: A
challenge set for reading comprehension over mul-
tiple sentences. In Proceedings of NAACL-HLT,
pages 252–262.

D.P. Kingma and J. Ba. 2014. Adam: A method for s-
tochastic optimization. In Proceedings of ICLR, vol-
ume abs/1412.6980.

G. Lai, Q. Xie, H. Liu, Y. Yang, and E.H. Hovy. 2017.
RACE: large-scale reading comprehension dataset
from examinations. In Proceedings of EMNLP, vol-
ume abs/1704.04683.

C. Li, Y. Wu, and M. Lan. 2018a. Inference on syntac-
tic and semantic structures for machine comprehen-
sion. In Proceedings of AAAI, pages 5844–5851.

L. Li, Y. Liu, and A. Zhou. 2018b. Hierarchical atten-
tion based position-aware network for aspect-level
sentiment analysis. In Proceedings of CoNLL, vol-
ume abs/1704.04683, page 181189.

Z. Lin, M. Feng, C.N. Santos, M. Yu, B. Xiang,
B. Zhou, and Y. Bengio. 2017. A structured self-
attentive sentence embedding. In Proceedings of I-
CLR, volume abs/1703.03130.

A.H. Miller, A. Fisch, J. Dodge, A. Karimi, A. Bordes,
and J. Weston. 2016. Key-value memory networks
for directly reading documents. In Proceedings of
the 54th ACL, volume abs/1606.03126.

K. Narasimhan and R. Barzilay. 2015. Machine com-
prehension with discourse relations. In Proceedings
of the 53rd ACL, pages 1253–1262.

J. Pennington, R. Socher, and C.D. Manning. 2014.
Glove: Global vectors for word representation. In
Proceedings of EMNLP, pages 1532–1543.

A. Radford, K. Narasimhan, T. Salimans, and
I. Sutskever. 2018. Improving language understand-
ing by generative pre-training. In eprint.

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016.
Squad: 100, 000+ questions for machine compre-
hension of text. volume abs/1606.05250.

M. Richardson, C. Burges, and E. Renshaw. 2013. M-
CTest: A challenge dataset for the open-domain ma-
chine comprehension of text. In Proceedings of
EMNLP, pages 193–203.

M. Richardson and P. Domingos. 2006. Markov logic
networks. In Journal of Machine Learning, 62(1-
2):107–136.

A. Santoro, D. Raposo, D.G.T. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T.P. Lillicrap. 2017. A
simple neural network module for relational reason-
ing. In eprint arXiv:1706.01427, abs/1706.01427.

M.J. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi.
2016. Bidirectional attention flow for machine com-
prehension. In Proceedings of ICLR, volume ab-
s/1611.01603.

Y. Shen, P. Huang, J. Gao, and W. Chen. 2016. Rea-
sonet: Learning to stop reading in machine compre-
hension. In Proceedings of the 23rd ACM SIGKDD,
volume abs/1609.05284.

E. Smith, N. Greco, M. Bosnjak, and A. Vlachos. 2015.
A strong lexical matching method for the machine
comprehension test. In Proceedings of EMNLP,
pages 1693–1698.

K. Sun, D. Yu, D. Yu, and C. Cardie. 2018a. Im-
proving machine reading comprehension with gen-
eral reading strategies. In eprint arXiv:1810-13441,
abs/1810.13441.

Y. Sun, G. Cheng, and Y. Qu. 2018b. Reading compre-
hension with graph-based temporal-casual reason-
ing. In Proceedings of COLING, pages 806–817.

Y. Tay, L.A. Tuan, and S.C. Hui. 2018. Multi-range
reasoning for machine comprehension. In eprint
arXiv:1803.09074, abs/1803.09074.

https://doi.org/10.18653/v1/P16-1223
https://doi.org/10.18653/v1/P16-1223
https://doi.org/10.18653/v1/P16-1223
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/P17-1168
https://doi.org/10.18653/v1/P17-1168
https://doi.org/arXiv:1506.03340
https://doi.org/arXiv:1511.02301
https://doi.org/arXiv:1511.02301
https://doi.org/arXiv:1803.03067
https://doi.org/arXiv:1803.03067
https://doi.org/10.18653/v1/P16-1086
https://doi.org/10.18653/v1/P16-1086
https://doi.org/10.18653/v1/N18-1023
https://doi.org/10.18653/v1/N18-1023
https://doi.org/10.18653/v1/N18-1023
https://doi.org/arXiv:1412.6980
https://doi.org/arXiv:1412.6980
https://doi.org/10.18653/v1/D17-1082
https://doi.org/10.18653/v1/D17-1082
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/K18-1018
https://doi.org/10.18653/v1/K18-1018
https://doi.org/10.18653/v1/K18-1018
https://doi.org/arXiv:1703.03130
https://doi.org/arXiv:1703.03130
https://doi.org/10.18653/v1/D16-1147
https://doi.org/10.18653/v1/D16-1147
https://doi.org/10.3115/v1/P15-1121
https://doi.org/10.3115/v1/P15-1121
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/D16-1264
https://doi.org/10.18653/v1/D16-1264
https://doi.org/10.18653/v1/D13-1020
https://doi.org/10.18653/v1/D13-1020
https://doi.org/10.18653/v1/D13-1020
https://doi.org/10.1007/s10994-006-8633-8
https://doi.org/10.1007/s10994-006-8633-8
https://doi.org/arXiv:1706.01427
https://doi.org/arXiv:1706.01427
https://doi.org/arXiv:1706.01427
https://doi.org/arXiv:1611.01603
https://doi.org/arXiv:1611.01603
https://doi.org/arXiv:1609.05284
https://doi.org/arXiv:1609.05284
https://doi.org/arXiv:1609.05284
https://doi.org/10.18653/v1/D15-1197
https://doi.org/10.18653/v1/D15-1197
https://doi.org/arXiv:1810.13441
https://doi.org/arXiv:1810.13441
https://doi.org/arXiv:1810.13441
https://doi.org/10.18653/v1/C18-1069
https://doi.org/10.18653/v1/C18-1069
https://doi.org/10.18653/v1/C18-1069
https://doi.org/arXiv:1803.09074
https://doi.org/arXiv:1803.09074


2251

A. Trischler, Z. Ye, X. Yuan, and K. Suleman. 2016a.
Natural language comprehension with the epireader.
In Proceedings of EMNLP, volume abs/1606.02270.

A. Trischler, Y. Zheng, X. Yuan, H. Jing, and P. Bach-
man. 2016b. A parallel-hierarchical model for ma-
chine comprehension on sparse data. In Proceedings
of the 54th ACL, pages 432–441.

B. Wang, S. Guo, L.L. Kang, S. He, and J. Zhao.
2016. Employing external rich knowledge for ma-
chine comprehension. In Proceedings of IJCAI.

H. Wang, M. Bansal, K. Gimpel, and D. McAllester.
2015. Machine comprehension with syntax, frames,
and semantics. In Proceedings of the 53rd ACL,
pages 700–706.

S. Wang and J. Jiang. 2017. Machine comprehension
using match-lstm and answer pointer. In Proceed-
ings ICLR, volume abs/1608.07905.

W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou.
2017. Gated self-matching networks for reading
comprehension and question answering. In Pro-
ceedings of the 55th ACL, pages 189–198.

J. Weston, A. Bordes, S. Chopra, and T. Mikolov. 2015.
Towards ai-complete question answering: A set of
prerequisite toy tasks. In Proceedings ICLR, volume
abs/1502.05698.

J. Weston, S. Chopra, and A. Bordes. 2014. Mem-
ory networks. In eprint arXiv:1503.08895, ab-
s/1410.3916.

R.J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. In Journal Machine Learning, 8(3-
4):229–256.

Y. Xu, J. Liu, J. Gao, Y. Shen, and X. Liu. 2017.
Dynamic fusion networks for machine reading
comprehension. In eprint arXiv:1711.04964, ab-
s/1711.04964.

J. Yu, Z.J. Zha, and T.S. Chua. 2012. Answering opin-
ion questions on products by exploiting hierarchical
organization of consumer reviews. In Proceedings
of EMNLP, volume abs/1704.04683.

M. Zhou, M. Huang, and X. Zhu. 2018. An inter-
pretable reasoning network for multi-relation ques-
tion answering. In Proceedings COLING, volume
abs/1801.04726.

https://doi.org/10.18653/v1/D16-1013
https://doi.org/10.18653/v1/P16-1041
https://doi.org/10.18653/v1/P16-1041
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/P16-1001
https://doi.org/10.18653/v1/P15-2115
https://doi.org/10.18653/v1/P15-2115
https://doi.org/arXiv:1608.07905
https://doi.org/arXiv:1608.07905
https://doi.org/10.18653/v1/P17-1018
https://doi.org/10.18653/v1/P17-1018
https://doi.org/arXiv:1502.05698
https://doi.org/arXiv:1502.05698
https://doi.org/arXiv:1503.08895
https://doi.org/arXiv:1503.08895
https://doi.org/10.1007/BF00992696
https://doi.org/10.1007/BF00992696
https://doi.org/10.1007/BF00992696
https://doi.org/arXiv:1711.04964
https://doi.org/arXiv:1711.04964
https://doi.org/10.3115/v1/D12-1036
https://doi.org/10.3115/v1/D12-1036
https://doi.org/10.3115/v1/D12-1036
https://doi.org/10.18653/v1/C18-1171
https://doi.org/10.18653/v1/C18-1171
https://doi.org/10.18653/v1/C18-1171

