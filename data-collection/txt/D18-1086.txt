



















































Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 768–773
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

768

Guided Neural Language Generation for Abstractive Summarization
using Abstract Meaning Representation

Hardy
The University of Sheffield

hhardy2@sheffield.ac.uk

Andreas Vlachos
The University of Sheffield

a.vlachos@sheffield.ac.uk

Abstract

Recent work on abstractive summarization has
made progress with neural encoder-decoder
architectures. However, such models are of-
ten challenged due to their lack of explicit
semantic modeling of the source document
and its summary. In this paper, we ex-
tend previous work on abstractive summa-
rization using Abstract Meaning Representa-
tion (AMR) with a neural language genera-
tion stage which we guide using the source
document. We demonstrate that this guid-
ance improves summarization results by 7.4
and 10.5 points in ROUGE-2 using gold stan-
dard AMR parses and parses obtained from
an off-the-shelf parser respectively. We also
find that the summarization performance us-
ing the latter is 2 ROUGE-2 points higher
than that of a well-established neural encoder-
decoder approach trained on a larger dataset.
Code is available at https://github.
com/sheffieldnlp/AMR2Text-summ

1 Introduction

Abstractive summarization is the task of automat-
ically producing the summary of a source doc-
ument through the process of paraphrasing, ag-
gregating and/or compressing information. Re-
cent work in abstractive summarization has made
progress with neural encoder-decoder architec-
tures (See et al., 2017; Chopra et al., 2016; Rush
et al., 2015). However, these models are of-
ten challenged when they are required to com-
bine semantic information in order to generate a
longer summary (Wiseman et al., 2017). To ad-
dress this shortcoming, several works have ex-
plored the use of Abstract Meaning Representa-
tion (Banarescu et al., 2013, AMR). These were
motivated by AMR’s capability to capture the
predicate-argument structure which can be utilized
in information aggregation during summarization.

However, the use of AMR also has its own
shortcomings. While AMR is suitable for infor-
mation aggregation, it ignores aspects of language
such as tense, grammatical number, etc., which
are important for the natural language generation
(NLG) stage that normally occurs in the end of the
summarization process. Due to the lack of such
information, approaches for NLG from AMR typ-
ically infer it from regularities in the training data
(Pourdamghani et al., 2016; Konstas et al., 2017;
Song et al., 2016; Flanigan et al., 2016), which
however is not suitable in the context of summa-
rization. Consequently, the main previous work on
AMR-based abstractive summarization (Liu et al.,
2015) only generated bag-of-words from the sum-
mary AMR graph.

In this paper, we propose an approach to guide
the NLG stage in AMR-based abstractive summa-
rization using information from the source docu-
ment. Our objective is twofold: (1) to retrieve
the information missing from AMR but needed for
NLG and (2) improve the quality of the summary.
We achieve this in a two-stages process: (1) esti-
mating the probability distribution of the side in-
formation, and (2) using it to guide a Luong et al.
(2015)’s seq2seq model for NLG.

Our approach is evaluated using the Proxy Re-
port section from the AMR dataset (Knight et al.,
2017, LDC2017T10) which contains manually an-
notated document and summary AMR graphs. Us-
ing our proposed guided AMR-to-text NLG, we
improve summarization results using both gold
standard AMR parses and parses obtained using
the RIGA (Barzdins and Gosko, 2016) parser by
7.4 and 10.5 ROUGE-2 points respectively. Our
model also outperforms a strong baseline seq2seq
model (See et al., 2017) for summarization by 2
ROUGE-2 points.

https://github.com/sheffieldnlp/AMR2Text-summ
https://github.com/sheffieldnlp/AMR2Text-summ


769

2 Related Work

Abstractive Summarization using AMR: In
Liu et al. (2015) work, the source document’s sen-
tences were parsed into AMR graphs, which were
then combined through merging, collapsing and
graph expansion into a single AMR graph repre-
senting the source document. Following this, a
summary AMR graph was extracted, from which
a bag of concept words was obtained without at-
tempting to form fluent text. Vilca and Cabezudo
(2017) performed a summary AMR graph extrac-
tion augmented with discourse-level information
and the PageRank (Page et al., 1998) algorithm.
For text generation, Vilca and Cabezudo (2017)
used a rule-based syntactic realizer (Gatt and Re-
iter, 2009) which requires substantial human input
to perform adequately.

Seq2seq using Side Information: In Neural
Machine Translation (NMT) field, recent work
(Zhang et al., 2018) explored modifications to the
decoder of seq2seq models to improve translation
results. They used a search engine to retrieve sen-
tences and their translation (referred to as trans-
lation pieces) that have high similarity with the
source sentence. When similar n-grams from a
source document were found in the translation
pieces, they rewarded the presence of those n-
grams during the decoding process through a scor-
ing mechanism calculating the similarity between
source sentence and the source side of the transla-
tion pieces. Zhang et al. (2018) reported improve-
ments in translation results up to 6 BLEU points
over their seq2seq NMT baseline. In this paper we
use the same principle and reward n-grams that are
found in the source document during the AMR-
to-Text generation process. However we use a
simpler approach using a probabilistic language
model in the scoring mechanism.

3 Guiding NLG for AMR-based
summarization

We first briefly describe the AMR-based summa-
rization method of Liu et al. (2015) and then our
guided NLG approach.

3.1 AMR-based summarization

In Liu et al. (2015)’s work, each of the sentence
of the source document was parsed into an AMR
graph, and combined into a source graph, G =
(V,E) where v ∈ V and e ∈ E are the unique

concepts and the relations between pairs of con-
cepts. They then extracted a summary graph, G′

using the following sub-graph prediction:

G′ = arg max
Ĝ=(V̂ ,Ê)

∑
v∈V̂

θᵀf(v) +
∑
e∈Ê

ψᵀf(e) (1)

where f(v) and f(e) are the feature representations
of node v and edge e respectively. The final sum-
mary produced was a bag of concept words ex-
tracted from G′. This output we will be replacing
with our proposed guided NLG.

3.2 Unguided NLG from AMR

Our baseline is a standard (unguided) seq2seq
model with attention (Luong et al., 2015) which
consists of an encoder and a decoder. The encoder
computes the hidden representation of the input,
{z1, z2, . . . , zk}, which is the linearized summary
AMR graph, G′ from Liu et al. (2015), follow-
ing Van Noord and Bos (2017)’s preprocessing
steps. Following this, the decoder generates the
target words, {y1, y2, . . . , ym}, using the condi-
tional probability Ps2s(yj |y<j , z), which is calcu-
lated using the equation

Ps2s(yj |y<j , z) = softmax(Wsh̃t) (2)

, where the attentional hidden state, h̃t is calcu-
lated using the equation

h̃t = tanh(Wc[ct; ht]) (3)

, where ct is the source context vector, and ht is
the target RNN hidden state. The source context
vector is defined as the weighted average over all
the source RNN hidden states, h̄s, given the align-
ment vector, at where at is defined as

at(s) =
exp(score(ht, h̄s))∑
s′ exp(score(ht, h̄s′))

(4)

3.3 Guided NLG from AMR

Our goal is to improve the text generated from the
summary AMR graph by the probability distribu-
tion of the seq2seq model, Ps2s using the source
document. Since not all sentences in the source
document will be used in generating the summary,
we prune the source document to a set of k sen-
tences which have the highest similarity with the
summary AMR graph. For graph-to-graph sim-
ilarity comparison, we use the source document
AMR parses and calculate the Longest Common



770

Subsequence (LCS) between the linearized AMR
parses and the summary AMR graph. We keep the
top-k sentences sorted by LCS length. To distin-
guish this pruned document from the source docu-
ment, we refer to the former as side information.

Our aim is is to combine Ps2s with the probabil-
ity distribution estimated using words in the side
information, Pside, in order to score each word
given its context during decoding. We estimate
Pside as the linear interpolation of 2-gram to 4-
gram probabilities in the form of

Pside(xj |xj−1j−3) = λ3PLM (xj |x
j−1
j−3)

+ λ2PLM (xj |xj−1j−2)
+ λ1PLM (xj |xj−1)

(5)

, where xj is a word occurring in side information
document, PLM is anN -gram LM estimated using
Maximum Likelihood:

PLM (xj |xj−1j−N−1) =
count(xj−N−1 . . . xj)

count(xj−N−1 . . . xj−1)
(6)

and λi is defined as

λi = θλi−1 where θ ∈ R, λi > 0 and
∑
i

λi = 1

(7)
where θ is a hyper-parameter that we tune using
the dev dataset during the experiments.

Lastly, we combine the probability distribution
of the decoder, Ps2s with that provided by the side
information, Pside, as follows:

s(yj |y<j , z) = log a+ ψ ∗ log(
b

a
+ 1) (8)

where ψ is a hyper-parameter determining the
influence of the side information on the de-
coding process, a is Ps2s(yj |y<j , z) and b is
Pside(yj |yj−1j−3). s(yj |y<j , z) is used during beam
search replacing Ps2s(yj |y<j , z) for all words that
occur in the side information. The intuition behind
Eq. 8 is that we are rewarding word yj when it ap-
pears in similar context in the side information, i.e.
the source document being summarized.

4 Experiments

We conduct experiments in order to answer the
following questions about our proposed approach:
(1) Is our baseline model comparable with the
state-of-the-art AMR-to-text approaches? (2)
Does the guidance from the source document im-
prove the result of AMR-to-Text in the context

Model BLEU
Our model (unguided NLG) 21.1
NeuralAMR (Konstas et al., 2017) 22.0
TSP (Song et al., 2016) 22.4
TreeToStr (Flanigan et al., 2016) 23.0

Table 1: Results for AMR-to-text

of summarization? (3) Does the improvement in
AMR-to-Text hold when we use the generator for
abstractive summarization using AMR? We an-
swer each of these in the following paragraphs.

AMR-to-Text baseline comparison We com-
pare our baseline model (described in §3.2) against
previous works in AMR-to-text using the data
from the recent SemEval-2016 Task 8 (May, 2016,
LDC2015E86). Table 1 reports BLEU scores
comparing our model against previous works.
Here, we see that our model achieves a BLEU
score comparable with the state-of-the-art, and
thus we argue that it is sufficient to be used in our
subsequent experiments with guidance.

Guided NLG for AMR-to-Text In this exper-
iment we apply our guided NLG mechanism de-
scribed in §3.3 to our baseline seq2seq model. To
isolate the effects of guidance we skip the actual
summarization process and proceed to directly
generating the summary text from the gold stan-
dard summary AMR graphs from the Proxy Re-
port section. To determine the hyper-parameters,
we perform a grid search using the dev dataset,
where we found the best combination of ψ, θ and
k are 0.95, 2.5 and 15 respectively. We have two
different settings for this experiment: the oracle
and non-oracle settings. In the oracle setting, we
directly use the gold standard summary text as the
guidance for our model. The intuition is that in this
setting, our model knows precisely which words
should appear in the summary text, thus providing
an upper bound for the performance of our guided
NLG approach. In the non-oracle setting, we use
the mechanism described in §3.3. We also com-
pare them against the baseline (unguided) model
from §3.2. Table 2 reports performance for all
models. The difference between the guided and
the unguided model is 16.2 points in BLEU and
9.9 points in ROUGE-2, while there is room for
improvement as evidenced by the difference be-
tween the oracle and non-oracle result.

Guided NLG for full summarization In this
experiment we combine our guided NLG model



771

Model BLEU F1 ROUGER-1 R-2 R-L
Guided NLG (Oracle) 61.3 79.4 63.7 76.4
Guided NLG 45.8 70.7 49.5 64.9
Unguided NLG 29.6 68.6 39.6 61.3

Table 2: BLEU and ROUGE results for guided and un-
guided models using test dataset.

with Liu et al. (2015)’s work in order to gener-
ate fluent texts from their summary AMR graphs
using the hyper-parameters tuned in the previous
paragraph. Liu et al. (2015) used parses from both
the manual annotation of the Proxy dataset as well
as those obtained using the JAMR parser (Flanigan
et al., 2014). Instead of JAMR we use the RIGA
parser (Barzdins and Gosko, 2016) which had the
highest accuracy in the SemEval 2016 Task 8
(May, 2016). We compare our result against Liu
et al. (2015)’s bag of words1, the unguided AMR-
to-text model from §3.2, and a seq2seq summa-
rization model (OpenNMT BRNN)2,3 which sum-
marizes directly from the source document to sum-
mary sentence without using AMR as an interlin-
gua and is trained on CNN/DM corpus (Hermann
et al., 2015) using the same settings as See et al.
(2017).

AMR NLG Model F1 ROUGEparses R-1 R-2 R-L

Gold
Guided 40.4 20.3 31.4
Unguided 38.9 12.9 27.0
Liu et al. (2015) 39.6 6.2 22.1

RIGA
Guided 42.3 21.2 33.6
Unguided 37.8 10.7 26.9
Liu et al. (2015) 40.9 5.5 21.4

Directly
from
Text

OpenNMT
BRNN 2 layer,
emb 256, hidden
1024

36.1 19.2 31.1

Table 3: The F1 ROUGE scores for guided, unguided,
Liu et al. (2015) (BoW) results in Gold and RIGA
parses, and seq2seq summarization. All models are run
using test dataset.

In Table 3, we can see that our approach results

1We were able to obtain comparable AMR summariza-
tion subgraph prediction to their reported results using their
published software but not to match their bag-of-word gener-
ation results.

2We use the OpenNMT-pytorch implementation
https://github.com/OpenNMT/OpenNMT-py and
a pre-trained model downloaded from http://opennmt.
net/OpenNMT-py/Summarization.html which has
higher result than See et al. (2017)’s summarizer.

3The pre-trained model generates multiple sentences
summary, but we use only the first sentence summary for
evaluation in accordance with the AMR dataset.

in improvements over both the unguided AMR-to-
text and the standard seq2seq summarization. One
interesting note is that using the RIGA parses re-
sult in higher ROUGE scores than the gold parses
for the guided model in our experiment. This phe-
nomenon was also observed in Liu et al. (2015)’s
experiment where the summary graphs extracted
from automatic parses had higher accuracy than
those extracted from manual parses. We hypothe-
size this can be attributed to how the AMR dataset
is annotated as there might be discrepancies in dif-
ferent annotator’s choices of AMR concepts and
relations for sentences with similar wording. In
contrast, the AMR parsers introduce errors, but
they are consistent in their choices of AMR con-
cepts and relations. The discrepancies in the man-
ual annotation could have impacted the perfor-
mance of the AMR summarizer that we use more
negatively than the noise introduced due to the
AMR parsing errors.

NLG Model Generated Summary
Gold on 8 august 2008 russia conducted

airstrikes on georgian targets .
Guided on 8 august 2008 russia conducted

airstrikes on georgian separatist tar-
gets .

Unguided on 8 august 2008 russia conducted a
softening of the georgia ’s separatist
target .

Seq2seq the russian laboratory complex is a 90
- building campus and served as the
location for russia ’s secret biological
weapons program in the soviet era of
a moscow regional depository threaten
moscow .

Table 4: Result summaries of guided, unguided and
seq2seq models compared with gold summary.

In Table 4, we show sample summaries from
the different models, where we can see that our
guided model improves the unguided model by
correcting a wrong word (a softening) into a cor-
rect one (airstrikes) and introducing a better suited
word from the source document (georgian instead
of georgia ’s).

NLG Model Fluency
Guided 2.66
Unguided 2.16

Table 5: Fluency scores on test dataset.

We also evaluated manually by asking human
evaluators to judge sentences’ fluency (grammat-
ical and naturalness) on a scale of 1 (worst) to 6
(best) for the guided and unguided model (see Ta-

https://github.com/OpenNMT/OpenNMT-py
http://opennmt.net/OpenNMT-py/Summarization.html
http://opennmt.net/OpenNMT-py/Summarization.html


772

ble 5). While the manual evaluation shows im-
provement over the unguided model, on the other
hand, grammatical mistakes and redundant repeti-
tion in the generated text are still major problems
(see Table 6) in our AMR generation.

Guided NLG Model Problems
the soldiers were injured
when a attempt to defuse the
bombs .

grammatical mistake

on 20 october 2002 the state
- run radio nepal reported on
20 october 2002 that at the
evening - run radio nepal re-
ported on 20 october 2002
that the guerrillas were killed
and killed .

redundant repetition

Table 6: Problems in guided model’s summaries.

5 Conclusion and Future Works

In this paper we proposed a guided NLG approach
that substantially improves the output of AMR-
based summarization. Our approach uses a simple
guiding process based on a probabilistic language
model. In future work we aim to improve summa-
rization performance by jointly training the guid-
ing process with the AMR-based summarization
process.

Acknowledgments

We are thankful for Gerasimos Lampouras for
his help with the manual evaluation process and
all volunteers who participated in it. We would
also like to thank the Indonesian government that
has sponsored the first author’s studies through
the Indonesia Endowment Fund for Education
(LPDP). The second author is supported by the
EU H2020 SUMMA project (grant agreement
number 688139) and the EPSRC grant eNeMILP
(EP/R021643/1).

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186.

Guntis Barzdins and Didzis Gosko. 2016. RIGA at
SemEval-2016 Task 8: Impact of Smatch Extensions
and Character-Level Neural Translation on AMR

Parsing Accuracy. Proceedings of the 10th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2016), pages 1143–1147.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive Sentence Summarization with At-
tentive Recurrent Neural Networks. In Proceedings
of the 15th Annual Conference of the NAACL HLT,
pages 93–98.

Jeffrey Flanigan, Chris Dyer, Noah A Smith, and Jaime
Carbonell. 2016. Generation from Abstract Mean-
ing Representation using Tree Transducers. In Pro-
ceedings of the 2016 Conference of the NAACL,
pages 731–739.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A Discrim-
inative Graph-Based Parser for the Abstract Mean-
ing Representation. Proceedings of the 52th Annual
Meeting of the ACL, pages 1426–1436.

Albert Gatt and Ehud Reiter. 2009. SimpleNLG : A
realisation engine for practical applications. Pro-
ceedings of the 12th European Workshop on NLG,
(March):90–93.

Karl Moritz Hermann, Tomáš Kočiský, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching Ma-
chines to Read and Comprehend. In Neural Infor-
mation Processing Systems, pages 1–14.

Kevin Knight, Bianca Badarau, Laura Baranescu,
Claire Bonial, Madalina Bardocz, Kira Griffitt, Ulf
Hermjakob, Daniel Marcu, Martha Palmer, Tim
O’Gorman, and Nathan Schneider. 2017. Ab-
stract Meaning Representation (AMR) Annotation
Release 2.0 LDC2017T10.

Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin
Choi, and Luke Zettlemoyer. 2017. Neural AMR:
Sequence-to-Sequence Models for Parsing and Gen-
eration. In Proceedings of the 2015 Conference of
the NAACL HLT, pages 1077 – 1086.

Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman
Sadeh, and Noah A Smith. 2015. Toward Ab-
stractive Summarization Using Semantic Represen-
tations. In Proceedings of the 2015 Conference of
the NAACL HLT, pages 1077–1086.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 1412–1421.
The Association for Computational Linguistics.

Jonathan May. 2016. SemEval-2016 Task 8: Mean-
ing Representation Parsing. Proceedings of the
10th International Workshop on Semantic Evalua-
tion (SemEval-2016), pages 1063–1073.



773

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The PageRank Citation
Ranking: Bringing Order to the Web. World
Wide Web Internet And Web Information Systems,
54(1999-66):1–17.

Nima Pourdamghani, Kevin Knight, and Ulf Herm-
jakob. 2016. Generating English from Abstract
Meaning Representations. In Proceedings of the 9th
International Natural Language Generation, vol-
ume 0, pages 21–25.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A Neural Attention Model for Abstractive
Sentence Summarization. In Proceedings of the
Conference on EMNLP, (September):379–389.

Abigail See, Peter J. Liu, and Christopher D. Man-
ning. 2017. Get To The Point: Summarization with
Pointer-Generator Networks. In Proceedings of the
55th Annual Meeting of the ACL.

Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo
Wang, and Daniel Gildea. 2016. AMR-to-text gen-
eration as a Traveling Salesman Problem. In Pro-
ceedings of the 2016 Conference on EMNLP, pages
2084–2089.

Rik Van Noord and Johan Bos. 2017. Neural seman-
tic parsing by character-based translation: Experi-
ments with abstract meaning representations. Com-
putational Linguistics in the Netherlands Journal,
7(2016):93–108.

Gregory Cesar Valderrama Vilca and Marco Anto-
nio Sobrevilla Cabezudo. 2017. A Study of Ab-
stractive Summarization Using Semantic Represen-
tations and Discourse Level Information. In Inter-
national Conference on Text, Speech, and Dialogue,
pages 482–490. Springer.

Sam Wiseman, Stuart M Shieber, and Alexander M
Rush. 2017. Challenges in Data-to-Document Gen-
eration. In Proceedings of the 2017 Conference on
EMNLP, pages 2253–2263, Copenhagen, Denmark.

Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra-
ham Neubig, and Satoshi Nakamura. 2018. Guiding
Neural Machine Translation with Retrieved Transla-
tion Pieces. In Proceedings of the 16th Annual Con-
ference of the NAACL HLT, New Orleans, Louisiana.
The Association for Computational Linguistics.


