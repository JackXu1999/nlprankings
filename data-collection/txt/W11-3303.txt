



















































Service Quality Improvement in Web Service Based Machine Translation


Proceedings of Workshop on Language Resources, Technology and Services in the Sharing Paradigm, pages 16–23,
Chiang Mai, Thailand, November 12, 2011.

Service Quality Improvement in Web Service Based Machine Transaltion

Sapa Chanyachatchawan and Virach Sornlertlamvanich and Thatsanee Charoenporn
National Electronics and Computer Technology Center (NECTEC)

112 Phahon Yothin Rd., Klong 1, Klong Luang, Pathumthani 12120,Thailand
{sapa.cha,virach.sor,thatsanee.cha}@nectec.or.th

Abstract

There are many approaches to increase
web service based machine translation re-
sult. However, perfect result alone does
not guarantee the quality of translation ser-
vice or user satisfaction. This paper pro-
poses framework to improve translation
service by using non functional attributes
information. In this paper, we present
methodology to measure quality of com-
posite translation service using existing
services information and also the guide-
line for selecting the composition web ser-
vice which has highest quality of service.

1 Introduction

The advantage of web service based machine
translation is the ability to create new language
pairs from existing language pairs. This pro-
cess is based on web service composition in SOA
(Service Oriented Architecture). Langrid Project
(NICT, 2011) is an example of machine translate
service based on web service composition tech-
nique. Langrid user can create multihop trans-
lation from existing language pairs. The auto-
matic composition process increases accessibility
for end users transparently. The most challenging
task among the automatic composition processes
is the discovery process. Based on W3C, web
service description standard (WSDL1.1) defines
only input and output name and basic data type
for web service with a few descriptions. OWL-S
is used to embed semantic into service input and
output which enable software agent to discover
service. However, translation accuracy does not
relate to quality of composite service or user satis-
faction. By embedding QoS (Quality of Service)
attributes as nonfunctional attributes into web ser-
vice description, we can improve quality of com-
posite service result. This paper proposes machine

translation service framework that can automati-
cally create new language pair from existing re-
sources. The objective of this paper is to provide
framework with model to managed nonfunctional
attributes and semantic of service. Framework is
presented in section 2, where component and over-
all process are explained in brief. In section 3 dis-
covery process is presented in general idea along
with model for evaluate quality of web service and
selection method.

2 Framework

In this section, we describe framework for manag-
ing web service composition process. The frame-
work is illustrated in Figure 1. From user aspect,
our framework is service provider. Users do not
need to search and compose web service by them-
selves. In this paper, existing translation service
and composite translation service are called ser-
vice and composite service respectively.

Component of the framework consists of

• Proxy Agent is responsible to interact with
users and manage user session.

• Discovery Agent is responsible to search and
interact with external web service registies.

• Service Agent is responsible to invoke exter-
nal web services.

• Repository is responsible for record compos-
ite web services and non functional attributes
information that do not included in the origi-
nal WSDL.

• Compose Agent is responsible for 1) inter-
act with other component in framework 2)
compose web service 3) evaluate web service
quality.

The process flow of framework is describe as
follow:

16



Figure 1: Framework

1. User send request to Proxy Agent. User re-
quest consists of input, output, and also se-
mantic information.

2. User send request to Proxy Agent. User re-
quest consists of input, output, and semantic
information.

3. Proxy Agent forwards request to Compose
Agent.

4. Compose Agent forwards request parameters
to Discovery Agent.

5. Discovery Agent searches services from web
service registry.

6. Discovery Agent returns candidate results to
Compose Agent

7. Compose Agent consults Repository if there
is additional information about candidate re-
sults.

8. Repository returns related information to
Composer Agent.

9. Compose Agent evaluates, selects web ser-
vice, creates workflow, creates instances of
Service Agent.

10. Service Agents binds web service.

11. After finish execution, Service Agents returns
result to Compose Agent.

12. Compose Agent updates service information
in Repository

13. Compose Agent forwards result to Proxy
Agent.

14. Proxy Agent returns output to user

15. Compose Agent creates WDSL for composite
service and forward to Discovery Agent and
stores in Repository

16. Discovery Agent publishes composite service
to web service registry

3 Evaluate quality of sevices

After discovery process, there are many compos-
ite services that satisfy user request. In order to
select the highest quality service, quality of ser-
vice is calculated using non functional attributes
of service.

3.1 Non functional attributes of web services
Non function attributes can represent QoS(Quality
of Service) which use to differentiate good service
from others. QoS values in interval or ratio scale
can be concerned as criterias for selecting optimal
services. QoS values in nominal scale value use
to reduce number of services. Some fundamental
attributes will be used in this paper as an example

For any web service S, the QoS attributes list
below:

Cost of service: denotes as QoSCost(S), this is
the cost to pay for service provider in or-
der to run service. The attributes consist of
two parts; first part is taken directly from ser-
vice provider called direct cost. Second part

17



is cost for set up and maintenance services
called indirect cost which assume that the
value is constant for every process through-
out the whole system.

Time of service: denotes as QoST ime(S), this is
the time measure from invoke service to re-
ceive respond from service in case of service
process successfully. Time of service con-
sists of process time and delay time. process
time is time needed to run instance of service,
delay time is overhead time. This value is
kept in Repository and be updated every time
that process finish as following equation

QoST ime(S) =

((QoSTime(S)N−1)∗(N−1))+T imeN (S)
N

whereas QoST ime(S)N is the average time
of process after be invoked for N time. This
information is kept in Repository.

Failure ratio: denotes as QoSFailure(S), is ratio
between number of failure and total number
of execution. Failure ratio initial value is set
to 0 and be updated by following formula

In case of service terminate normally;

QoSFailure(S) =
((QoSFailure(S)N )∗(N−1))

N

In case of service terminate abnormally;

QoSFailure(S) =

((QoSFailure(S)N )∗(N−1))+1
N

whereas QoSFailure(S)N is the failure ratio
of process after be invoked for N time.This
information is kept in Repository.

Unavailability: denotes as QoSUnavl(S), is
value to represent the unavailability of
services. QoSUnavl(S) is obtained using this
formula:

QoSUnavl(S) =
T (S)
C

Whereas, T (S) is total amount of time (sec-
onds) which service S is not available in last
C seconds, C is set by framework.

User satisfaction: denotes as QoSSat, is cardi-
nal scale value represent satisfaction level of
user, this value is variance depend on each
users. This information is kept in Repository.

Security level: denotes as QoSSec, is cardinal
scale value represent security level, this value
is taken directly from service or trusted third
party providers. This value is taken directly
from service providers.

Bandwidth: denotes as QoSBand, is bandwidth
required for running process. This value is
taken directly from service providers.

There are number of basic attributes used for
measure QoS in streaming application which al-
low some errors and lossy information.

Error: denotes as QoSError, is represent total
number of noise and error (in bytes) occur
while execute services.

Delay: denotes as QoSDelay, is total delay and jit-
ter time (in seconds) while execute services.

Some nominal scale non functional attributes
that can not be convert to ratio scale, such as user
context. These attributes are used to prune web
services. Examples of these attributes are:

Context: denotes as QoSContext, is set of context
attributes represent context of users and their
environment, such as location, demography
information, or user browser environment.

Summarization of non functional attributes is
presented in Table 1

3.2 Normalize QoS value

In order to compare or measure different at-
tributes, QoS need to be normalized. Each at-
tributes is assigned preference of its value (mini-
mum, maximum). Each attributes are normalized
as following:

Cost of service: is normalized by using transform
table because of cost of service should not be
linear function. Table 2 shows the example of
normalization of QoSCost(S).

Time of service: is normalized using formula:

QoSTime(S)
CMaxTime

18



Table 1: Non functional attributes summarization
Attribute Scale Source Description
QoSCost(S) Ratio Service Provider Service cost
QoST ime(S) Ratio Repository Average execute time
QoSFailure(S) Ratio Repository Failure ratio
QoSUnavl(S) Ratio Repository Unavailability ratio
QoSSat(S) Cardinal Repository User satisfaction
QoSSec(S) Cardinal Service Provider Security level
QoSBand(S) Ratio Service Provider Reqiured Bandwidth
QoSError(S) Ratio Repository Number of error information
QoSDelay(S) Ratio Repository Service delay time
QoSContext(S) Nominal User User context

Table 2: Cost of service transform table example

Cost(Dollars) Value
0-0.5 0.0
0.5-1 0.3
1-5 0.5

5-10 0.8
≥ 5 1.0

Whereas CMaxTime is the maximum execute
time assigned by framework.

Failure ratio: does not need to be normalized,
because QoSFailure(S) ∈ [0, 1]

Unavailability: does not need to be normalized,
because QoSUnavl(S) ∈ [0, 1]

User satisfaction: is not normalized and will be
used as constraint.

Security level: is not normalized and will be used
as constraint.

Bandwidth: is normalized using formula:

QoSBand(S)
CMaxBand

Whereas CMaxBand is the maximum band-
width of framework.

Error: is normalized using formula:

QoSError−CMinError
CMaxError−CMinError

Whereas CMinError and CMaxError are min-
imum and maximum error that framework al-
low to happen.

Delay: is normalized using formula:

QoSDelay−CMinDelay
CMaxDelay−CMinDelay

Whereas CMinDelay and CMaxDelay are min-
imum and maximum error that framework al-
low to happen.

Context: can not be normalized and will be used
as constraint.

Table 3 is the summarization of normalized at-
tributes.

3.3 Quality of composite service
Once the service has been composed, the QoS of
composite service will be calculated. Workflow of
composite service determines how QoS be com-
puted. Workflow of composite service is divided
into four types 1)sequential, 2)parallel, 3)condi-
tional, 4)loop, and 5)complex. Parallel, condi-
tional, loop and complex workflow are reduced
into one atomic task.As the result of reduction,
new workflow will consist of sequential workflow
only, and then sequential workflow is reduced to
one atomic workflow. Only ratio scale and interval
scale attributes will computed here. Hence, QoS
of composite service is calculated.

3.3.1 Sequential workflow
Sequential workflow CS (Figure 2)consists of n
sequential process denote as Si; 1 ≤ i ≤ n. The
work flow start at service S1 and finish at service
Sn. Process Si must finish before process Si+1
can start.

The QoS of CS can be obtained as follows:

QoST ime(CS) =
∑n

i=1QoST ime(Si)

19



Table 3: Normalized functional attributes
Attribute Range Preferred value Description
QoSCost(S) [0, 1] Minimum Service cost
QoST ime(S) [0, 1] Minimum Average execute time
QoSFailure(S) [0, 1] Minimum Failure ratio
QoSUnavl(S) [0, 1] Minimum Unavailability ratio
QoSSat(S) - - User satisfaction
QoSSec(S) - - Security level
QoSBand(S) [0, 1] Minimum Required Bandwidth
QoSError(S) [0, 1] Minimum Number of error information
QoSDelay(S) [0, 1] Minimum Service delay time
QoSContext(S) - - User context

S1 SN

Figure 2: Linear workflow

QoSCost(CS) =
∑n

i=1QoSCost(Si)

QoSFailure(CS) =
1−∏ni=1(1−QoSFailure(Si))

QoSUnavl(CS) = 1−
∏n

i=1(1−QoSUnavl(Si))
QoSBand(CS) = MAX1≤i≤n(QoSBand(Si))

QoSError(CS) =
∑n

i=1QoSError(Si)

QoSDelay(CS) =
∑n

i=1QoSDelay(Si)

3.3.2 Parallel workflow
Parallel workflow (Figure 3) CS consists of n par-
allel process denote as Si; 1 ≤ i ≤ n, each process
work independently and can start at same time.

S1

Si

Sn

Figure 3: Parallel workflow

The QoS of CS can be obtained as follows:

QoST ime(CS) = MAX1≤i≤n(QoST ime(Si))

QoSCost(CS) =
∑n

i=1QoSCost(Si)

QoSFailure(CS) =
1−∏ni=1(1−QoSFailure(Si))

QoSUnavl(CS) = 1−
∏n

i=1(1−QoSUnavl(Si))
QoSBand(CS) =

∑n
i=1QoSBand(Si)

QoSError(CS) =
∑n

i=1QoSError(Si)

QoSDelay(CS) = MAX1≤i≤n(QoSDelay(Si))

3.3.3 Conditional workflow
Conditional workflow CS (Figure 4)consists of n
process denote as Si; 1 ≤ i ≤ n, only one process
will be execute. pi is the probability of process
Si to be execute and

∑n
i pi = 1, these value store

from Repository.

S1

p1

pi

pn

Si

Sn

Figure 4: Conditional workflow

The QoS of CS can be obtained as follows:

QoST ime(CS) =
∑n

i=1(pi ∗QoST ime(Si))
QoSCost(CS) =

∑n
i=1(pi ∗QoSCost(Si))

QoSFailure(CS) =
∑n

i=1(pi ∗QoSFailure(Si))
QoSUnavl(CS) =

∑n
i=1(pi ∗QoSUnavl(Si))

QoSBand(CS) =
∑n

i=1(pi ∗QoSBand(Si))
QoSError(CS) =

∑n
i=1(pi ∗QoSError(Si))

QoSDelay(CS) =
∑n

i=1(pi ∗QoSDelay(Si))
3.3.4 Loop workflow
For loop workflow (Figure 5), there is condition
of loop to simplify the calculation. Give CS is
composite service that created by repeat execution
of service S with p is the chance that service will
be repeat and loop must be execute service S at
least one time.

The QoS of CS can be obtained as follows:

QoSCost(CS) =
QoSCost(S)

(1−p)

QoST ime(CS) =
QoSTime(S)

(1−p)

20



S

p

1−p

Figure 5: Loop workflow

QoSFailure(CS) = 1− (1−p)∗(1−QoSFailure(S))(1−p∗(1−QoSFailure))
QoSUnavl(CS) = 1− (1−p)∗(1−QoSUnavl(S))(1−p∗(1−QoSUnavl))

QoSBand(CS) = QoSBand(Si)

QoSError(CS) =
QoSError(S)

(1−p)

QoSDelay(CS) =
QoSDelay(S)

(1−p)

3.3.5 Complex workflow
Complex workflow CS (Figure 6)consists of N
process denote as Si; 1 ≤ i ≤ n, it is acyclic di-
rected graph.

S1 S3

S4

S5

S6

S2

Figure 6: Complex workflow

The QoS of CS can be obtained as follows:

QoSCost(CS) =
∑n

i=1QoSCost(Si)

QoSFailure(CS) =
1−∏ni=1(1−QoSFailure(Si))

QoSUnavl(CS) = 1−
∏n

i=1(1−QoSUnavl(Si))
QoSError(CS) =

∑n
i=1QoSError(Si)

For the calculation of QoST ime and QoSDelay,
concept of finding critical path in work flow is ap-
plied, method such as Finding the critical path in
a time-constrained workflow(Son and Kim, 2000)
, Finding Multiple Possible Critical Paths Using
Fuzzy PERT (Chen and Chang, 2001) or Critical
Path Method (CPM)(Samuel, 2004) can be used
to critical path. Given set A that member of ser-
vice A are services in critical path. QoST ime and
QoSDelay will be:

QoST ime(CS) =
∑

Si∈AQoST ime(Si)

QoSDelay(CS) =
∑

Si∈AQoSDelay(Si)

Due to complexity of workflow, the composite
bandwidth will be use the maximum bandwidth re-
quired by services.
QoSBand(CS) = MAX1≤i≤n(QoSBand(Si))

3.4 Objective function
Objective function is used to evaluate the fitness of
composite service. As many attributes are consid-
ered, the single unique value is needed for com-
parison between each possible combination. In
framework, QoS of composite service is defined
by formula:

QoS(X) =
∑N

i=1 (wi ∗QoSi(X))
whereas N=number of attributes; wi=weight of

attribute ith

Our objective is to find composite service that
have minimum QoS value, thus objective function
will be

minQoS(X) = min (
∑N

i=1 (wi ∗QoSi(X)))
some constraints are defined for composite ser-

vice to represent real life constraints.

QoSi(CS) ≤ Ci for each i ∈ 1, ..., N
whereas N=number of attributes; Ci=constraint

of attribute ith

3.5 Selecting web service
QoS function from previous section consists of
non linear parameter which make calculation com-
plex. To simplify problem, some assumptions are
given 1) suppose that only one possible workflow
returned from discovery process 2) composite ser-
vice is not streaming application. 3) user context
is irrelevant. 4) workflow consist only sequential
processes. Figure 7 show an example of such a
workflow.

Cardinal and nominal scale QoS and non-
linear composite QoS (QoSFailure, QoSUnavl,
QoSBand, QoSSat and QoSSec) are excluded
from objective function and used to prune to dis-
covered services. Hence, workflow objective func-
tion and constraints are solely linear function, and
then 0-1 linear programming model is applied.

S11
S12
.

S1m1

S21
S22
.

S2m2

Sn1
Sn2
.

Snmn

Figure 7: Output from discovery process

After discovery process, set of discov-
ered service is pruned with set of constraints

21



{QoSFailure,QoSUnavl,QoSBand,QoSSat,
QoSSec}.

The workflow consists of n process, each pro-
cesses there is set or service with size mi; 1 ≤ i ≤
n that can fulfill process requirements.

Then we introduce set of variable xij to repre-
sent decision variable.




x11
...

x1k

. . .
xn1

...
xnmi




The variable xij is correspond to Sij ; 1 ≤ i ≤ n
and 1 ≤ j ≤ mi. Whereas n is number of process.
xij = 1 iff service xij has been selected, otherwise
xij = 0

Hence, problem is transformed to linear pro-
gramming problem:

minimize:
∑n

i=1

∑mi
j=1QoS(Sij) ∗ xij

whereas QoS(Sij) = (wT ime ∗
QoST ime(Sij)) + (wCost ∗QoSCost(Sij))

subject to

mi∑

j=1

xij = 1

xij ∈ {0, 1}
QoST ime(Sij) ≤ CT ime
QoSCost(Sij) ≤ CCost

4 Update repository information

After composite that have the best QoS has been
selected and executed, there are processes after
finish. There are two cases 1) composite service
terminate normally 2) composite service terminate
abnormally. In later case, we update service infor-
mation (QoSFailure) in Repository. Process will
not repeat because of services that makes compos-
ite service fail tends to have better QoS value than
others. As the result, other combination of this
service must be excluded and rediscover web ser-
vices again. In case of composite service termi-
nate normally, service information (QoSFailure,
and QoSTime) is updated in to Repository, and
publish composite service to Web service Registry
with QoS information. QoS information can be
added to WSDL as extension (Unreaveling the
web services: an introduction to SOAP, WSDL,
and UDDI)(Curbera, 2002),using Semantic Anno-
tations for WSDL ,or using OWL-S.

5 Related works

There are many related studies about quality of
machine translation notably ones include Auto-
matic Evaluation of Machine Translation Quality
Using Longest Common Subsequence and Skip-
Bigram Statistics (Lin and Och, 2004) and ME-
TEOR: An Automatic Metric for MT Evaluation
with Improved Correlation with Human Judg-
ments (Banerjee and Lavie, 2005). There are re-
searches about quality of service in service com-
position process such as QoS-Aware Middleware
for Web Services Composition - A Qualitative Ap-
proach (Yeom, Yun and Min, 2006). There are
two important processes that we do not focus in
our framework. The first process is discovery pro-
cess which are managed by Discovery Agent in
our framework. Algorithms for discovery services
are not included in this paper. There are many re-
lated studies in searching non perfect match web
service such as Automate Composition and Reli-
able Execution of Ad-hoc Processes (Binder, Con-
statinescu, Faltings, Haller, and Turker, 2004) and
A software Framework For Matchmaking Based
on Semantic Web Technology (Li and Horrocks,
2003). Other research work as in Ontology as-
sisted Web services discover (Zhang and Li, 2005)
and Web Service Discovery via Semantic Associ-
ation Ranking and Hyperclique Pattern Discovery
(Paliwal, Adam, Xiong and Bornhovd, 2006) use
semantic information to discover web services.

The second process is how to search for the op-
timal quality composite service from all possible
combination of services. We can apply linear pro-
gramming technique as described cutting method
in A lift-and-project cutting plane algorithm for
mixed 01 programs (Balas, Ceria and Cornuejols,
1993) to perform this search task.

6 Conclusion and future work

The main contribution of this paper is to propose a
web service based machine translation framework
that enhances quality of translation. We present
the concept of embedding quality of service infor-
mation, method to measurement QoS, and calcu-
lation of composite translation QoS.

For future work, we plan to work on simplif-
ing the search space, discovery techniques using
semantic, mathemetic model for solving integer
programming problem, fault tolerance, and imple-
mentation of ontology to describe QoS attributes
in services.

22



References
Language Grid. 2011. Language Grid

http://http://langrid.nict.go.jp/en/index.html

Samuel L. Baker. 2004 Critical Path Method (CPM)
http://hadm.sph.sc.edu/COURSES/J716/CPM/CPM.html

Jin Hyun Son and Myoung Ho Kim. 2000. Finding the criti-
cal path in a time-constrained workflow Seventh Interna-
tional Conference on Real-Time Computing Systems and
Applications (RTCSA’00) 2000 p. 102

Shyi-Ming Chen and Tao-Hsing Chang. 2001. Find-
ing Multiple Possible Critical Paths Using Fuzzy PERT
IEEE Transactions on Systems, Man and Cybernetics, Part
B,2001

Walter Binder, Ion Constatinescu, Boi Faltings, Klaus Haller,
and Can Turker. Barcelona, 2004 2004. Automate Com-
position and Reliable Execution of Ad-hoc Processes Sec-
ond European Workshop on Multi-Agent Systems (EU-
MAS)

I. Li and Horrocks. 2003 A software Framework For Match-
making Based on Semantic Web Technology In Proc, 12th
Int Conf on the World Wide Web, 2003

Curbera F. 2002. Unreaveling the web services: an introduc-
tion to SOAP, WSDL, and UDDI IEEE Internet Comput-
ing, Vol. 6. No.2,pp. 86-93,2002.

Egon Balas ,Sebastian Ceria and Gerard Cornuejols. 1993 A
lift-and-project cutting plane algorithm for mixed 01 pro-
grams Mathematical Programming, Volume 58, Numbers
1-3 / January, 1993,pp. 295-324

Po Zhang, Juanzi Li. Ontology assisted Web services dis-
cover. Service-Oriented System Engineering, 2005. IEEE
International Workshop, 20-21 October 2005 Page(s):45 -
50

Paliwal, A.V.; Adam, N.R.; Hui Xiong; Bornhovd, C. 2006
Web Service Discovery via Semantic Association Rank-
ing and Hyperclique Pattern Discovery Web Intelligence,
2006. WI 2006. IEEE/WIC/ACM International Confer-
ence on 18-22 Dec. 2006 Page(s):649 - 652

Issa, H.; Assi, C.; Debbabi, M.; 2006. QoS-Aware Mid-
dleware for Web Services Composition - A Qualitative Ap-
proach Computers and Communications, 2006. ISCC ’06.
Proceedings. 11th IEEE Symposium on 26-29 June 2006
Page(s):359 - 364

Chin-Yew Lin and Franz Josef Och 2004. Automatic Evalua-
tion of Machine Translation Quality Using Longest Com-
mon Subsequence and Skip-Bigram Statistics. 42nd An-
nual Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, July 21- 26, 2004.

Banerjee, S. and Lavie, A. 2005. METEOR: An Automatic
Metric for MT Evaluation with Improved Correlation with
Human Judgments Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization at the
43rd Annual Meeting of the Association of Computational
Linguistics (ACL-2005), Ann Arbor, Michigan, June 2005

23


