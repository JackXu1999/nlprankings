



















































Discriminating between Lexico-Semantic Relations with the Specialization Tensor Model


Proceedings of NAACL-HLT 2018, pages 181–187
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Discriminating between Lexico-Semantic Relations
with the Specialization Tensor Model

Goran Glavaš
Data and Web Science Group

University of Mannheim
B6, 29, DE-68161 Mannheim

goran@informatik.uni-mannheim.de

Ivan Vulić
Language Technology Lab
University of Cambridge

9 West Road, Cambridge CB3 9DA
iv250@cam.ac.uk

Abstract

We present a simple and effective feed-
forward neural architecture for discriminating
between lexico-semantic relations (synonymy,
antonymy, hypernymy, and meronymy). Our
Specialization Tensor Model (STM) simulta-
neously produces multiple different special-
izations of input distributional word vectors,
tailored for predicting lexico-semantic rela-
tions for word pairs. STM outperforms more
complex state-of-the-art architectures on two
benchmark datasets and exhibits stable perfor-
mance across languages. We also show that,
if coupled with a lingual distributional space,
the proposed model can transfer the prediction
of lexico-semantic relations to a resource-lean
target language without any training data.

1 Introduction

Distributional vector spaces (i.e., word embed-
dings) (Mikolov et al., 2013; Pennington et al.,
2014; Bojanowski et al., 2017) are ubiquitous in
modern natural language processing (NLP). While
such vector spaces capture general semantic relat-
edness, their well-known limitation is the inability
to indicate the exact nature of the semantic relation
that holds between words. Yet, the ability to rec-
ognize the exact semantic relation between words
is crucial for many NLP applications: taxonomy
induction (Fu et al., 2014; Ristoski et al., 2017), nat-
ural language inference (Tatu and Moldovan, 2005;
Chen et al., 2017), text simplification (Glavaš and
Štajner, 2015), and paraphrase generation (Mad-
nani and Dorr, 2010), to name a few.

This is why numerous methods have been pro-
posed that either (1) specialize distributional vec-
tors to better reflect a particular relation (most
commonly synonymy) (Faruqui et al., 2015; Kiela
et al., 2015; Mrkšić et al., 2017; Vulić et al., 2017)
or (2) train supervised relation classifiers using
lexico-semantic relations (i.e., labeled word pairs)

from external resources such as WordNet (Fell-
baum, 1998) as training data (Baroni et al., 2012;
Roller et al., 2014; Shwartz et al., 2016; Glavaš and
Ponzetto, 2017).

Contributions. We present the Specialization Ten-
sor Model (STM), a simple and effective feed-
forward neural model for discriminating between
(arguably) most prominent lexico-semantic rela-
tions – synonymy, antonymy, hypernymy, and
meronymy. The STM architecture is based on the
hypothesis that different specializations of input
distributional vectors are needed for predicting dif-
ferent lexico-semantic relations. Our results show
that, despite its simplicity, STM outperforms more
complex models on the benchmarking CogALex-V
dataset (Santus et al., 2016). Further, it exhibits
stable performance across languages. Finally, we
show that, when coupled with a method for in-
ducing a multilingual distributional space (Artetxe
et al., 2017; Smith et al., 2017, inter alia), STM
can predict lexico-semantic relations also for lan-
guages with no training data available from exter-
nal linguistic resources. While in this work we
use STM to discriminate between four prominent
lexico-semantic relations, it can, at least conceptu-
ally, be trained to predict over an arbitrary set of
lexico-semantic relations, provided the availability
of respective training data.

2 Related Work

Specializing distributional vectors. Given a pair
of words, we cannot reliably determine the nature
of the lexico-semantic association between them
(if any), purely based on their distributional word
vectors (Mikolov et al., 2013; Pennington et al.,
2014, inter alia). It is a well-known property of
distributional methods to conflate different types of
semantic associations between words. This is why
methods for specializing word embeddings for par-

181



fS
(1)

fS
(2)

fS
(K)

...

...

... ...

fP
(1)

fP
(2)

...

fP
(K)

fclass...
Figure 1: Architecture of the Specialization Tensor Model (STM).

ticular relations use external linguistic constraints
(e.g., from WordNet) to either (1) modify the origi-
nal objective of general embedding algorithms and
directly train relation-specific embeddings from
corpora (Yu and Dredze, 2014; Kiela et al., 2015)
or (2) post-process the pre-trained distributional
space by moving closer together (or further apart)
words that stand in a particular relation (Wieting
et al., 2015; Mrkšić et al., 2017; Vulić and Mrkšić,
2018). While these methods specialize the distribu-
tional space to better reflect properties of a particu-
lar relation, e.g., synonymy (Wieting et al., 2015;
Mrkšić et al., 2017) or hypernymy (Vendrov et al.,
2016; Vulić and Mrkšić, 2018), they are not able to
discriminate between multiple lexico-semantic re-
lations at the same time, i.e., the embedding space
gets post-specialized for one particular relation.

Classifying lexico-semantic relations. Super-
vised relation classifiers learn to either identify one
particular relation of interest (Baroni et al., 2012;
Roller et al., 2014; Shwartz et al., 2016; Glavaš
and Ponzetto, 2017) or to discriminate between
multiple relations (Attia et al., 2016; Shwartz and
Dagan, 2016), using labeled word pairs from ex-
ternal resources like WordNet. The LexNet model
(Shwartz and Dagan, 2016) combines distributional
vectors with recurrent encodings of syntactic paths
taken from word co-occurrences in text corpora.
While adding the syntactic information boosts per-
formance, it limits the model’s portability to other
languages. Attia et al. (2016) train a convolutional
model in a multi-task setting, coupling multi-class
relation classification with binary classification of
word relatedness. Unlike LexNet, this model re-
quires only distributional vectors as input. Our
specialization tensor model also requires only dis-
tributional vectors as input, but compared to the
model of Attia et al. (2016), it has a simpler and
more intuitive feed-forward architecture.

Glavaš and Ponzetto (2017) recently showed that
asymmetric specialization of distributional vectors
helps to detect asymmetric relations (hypernymy,
meronymy). Following these findings, we hypoth-
esize that detection of different relations requires
different specializations of distributional vectors,
so we design STM accordingly.

3 Specialization Tensor Model

The high-level architecture of the Specialization
Tensor Model is depicted in Figure 1. The input to
the model is a pair of unspecialized distributional
word vectors (x1,x2). Both input vectors are first
transformed in K different ways with functions
f
(1)
S , . . . , f

(K)
S . Each pair of corresponding special-

izations f (i)S (x1) and f
(i)
S (x2) is then forwarded to

the respective scoring function f (i)P . Finally, we
feed the K scores obtained from K pairs of differ-
ently specialized distributional vectors as features
to the multi-class relation classifier fclass .

3.1 Specialization Tensor
STM assumes that different word vector special-
izations emphasize different subsets of semantic
properties of words that are more informative for
predicting some lexico-semantic relations than oth-
ers. In other words, we assume that a particular
specialization function f (i)S can be trained to trans-
form the input vectors x1 and x2 into vectors that
encode properties suitable for predicting a particu-
lar relation, e.g., hypernymy. We set the specializa-
tion function f (i)S : R

m → Rn to be a non-linear
feed-forward network with a single hidden layer: it
transforms the input vector x ∈ Rm into a special-
ized vector x(i) ∈ Rn:1

f
(i)
S (x) = tanh

(
W

(i)
S x + b

(i)
S

)

1We have also experimented with more hidden layers but
f
(i)
S with a single hidden layer yielded best performance.

182



with W(i)S ∈ Rn×m and b
(i)
S ∈ Rn parameteriz-

ing the specialization function. Transformation
matrices W(i)S of different specialization functions
f
(i)
S can be seen as slices of a specialization tensor

W
[1..K]
S (hence the model name), coupled with the

specialization bias matrix BS = b
[1..K]
S . The num-

ber of specialization functions K (i.e., the number
of slices of the specialization tensor) is the hyper-
parameter of the model.

3.2 Bilinear Product Scores

Following the assumption that specialization ten-
sor slices generate relation-specific representations,
we assume that an interaction between the corre-
sponding specialized vectors x(i)1 = f

(i)
S (x1) and

x
(i)
2 = f

(i)
S (x2), produced by the i-th specializa-

tion tensor slice, generates an informative score
(i.e., a feature) for classifying the lexico-semantic
relation for a word pair. We produce a single fea-
ture for each pair of specialized vectors (x(i)1 ,x

(i)
2 )

by non-linearly squashing their bilinear product:

f
(i)
P

(
x
(i)
1 ,x

(i)
2

)
= tanh

(
x
(i)
1

T
W

(i)
P x

(i)
2 + b

(i)
P

)

with the bilinear product matrices W(i)P ∈ Rn×n
and bias terms b(i)P ∈ R being trainable model
parameters. Bilinear product matrices W(i)P may
be seen as slices of the bilinear product tensor,
W

[1..K]
P , coupled with the bias vector bP =

[b
(1)
P , . . . , b

(K)
P ]

T . The final K-dimensional feature
vector is then simply the concatenation of bilinear
product scores, that is, s = [f (1)P , . . . , f

(K)
P ]

T .

3.3 Classification Objective

As the final step, we feed the feature vector s to the
relation classifier fclass , a feed-forward network
with a single hidden layer:

fclass(s) = tanh (Wcls + bcl )

with parameters Wcl ∈ RC×K and bcl ∈ RC ,
where C is the number of lexico-semantic rela-
tions between which we are discriminating. We
obtain the final prediction vector h by applying the
softmax function on the output of the relation clas-
sification component: h = softmax (fclass(s)).

STM is parametrized by (1) the specialization
tensor and bias matrix, (2) product tensor and
bias vector, and (3) classifier parameters, i.e.,

Ω = {W[1:K]S ,BS ,W
[1:K]
P ,bP ,Wcl ,bcl}. As-

sume the training set of N triples, each consisting
of distributional vectors of two words and one-hot
encoding of the relation that holds between these
words, {(x1k ,x2k ,yk)}Nk=1. We optimize STM’s
parameters by minimizing the regularized cross-
entropy loss (i.e., negative log-likelihood):

J(Ω) = λ‖Ω‖2 −
N∑

k=1

C∑

j=1

yjk ln(h
j
k)

where hjk is the probability that the j-th relation
holds in the k-th training example (as predicted by
the model), and λ is the regularization factor.

4 Evaluation

We first describe the evaluation setup (datasets,
baselines, and model optimization) and then show
STM’s performance on a benchmarking relation
classification dataset (Santus et al., 2016). Finally,
we report how STM performs for different lan-
guages and in the language transfer setting.

4.1 Experimental Setup

Datasets. We use the CogALex-V dataset from
the shared task on corpus-based identification of
semantic relations (Santus et al., 2016). Its train
and test portions contain 3,054 and 4,260 word
pairs, respectively, covering four relations (syn-
onymy: 5.4%; antonymy: 8.8%; hypernymy: 8.6%;
and meronymy: 6.1%) and randomly paired words
(71.1%). CogALex-V is severely skewed in favor
of random word pairs and its training portion is
very limited in size. Nonetheless to the best of our
knowledge, it is the only publicly available dataset
for multi-class classification of lexico-semantic re-
lations on which other models have been compar-
atively evaluated (Attia et al., 2016; Shwartz and
Dagan, 2016).

Besides the skewed class distribution and the
limited size, CogALex-V also suffers from lexi-
cal repetitiveness.2 We have thus created an ad-
ditional larger and more balanced dataset by ran-
domly sampling triples from WordNet (Fellbaum,
1998). This dataset, termed WN-LS, contains
10,000 word pairs (approximately 2,000 pairs for
each of the four lexico-semantic relations and 2,000

2A single word can be present in up to ten pairs (although
there is no lexical overlap between the train and test data).

183



randomly created pairs), split by 8:2 train-to-test ra-
tio. To support the multilingual analysis, we semi-
automatically translated the whole English (EN)
WN-LS dataset into German (DE) and Spanish
(ES).3 We additionally translated the test portion
of WN-LS to Croatian (HR), as an example of a
resource-lean language.4

Baselines. We compare STM against two baseline
models. The first baseline (CONCAT) feeds the
concatenation of the distributional embeddings to a
feed-forward classifier with a single hidden layer:

h(x1,x2)=softmax (tanh (Wcl[x1;x2] + bcl)) .

The second baseline, named BILIN-TENS is an
STM reduction in which we directly forward the in-
put vectors into the bilinear product tensor W[1..K]P ,
without being specialized. It can be seen as STM
with tensor specialization slices W(i)S fixed to iden-
tity matrices and biases b(i)S to zero vectors. Com-
paring STM with BILIN-TENS directly quantifies
the effect the specialization tensor has on relation
classification performance.

Optimization. We learn the STM’s parameters us-
ing the Adam algorithm (Kingma and Ba, 2015),
with initial learning rate set to 0.0001. We train in
mini-batches of size Nb = 50 and apply dropout
with the retaining probability of 0.5 to all model
layers. In all experiments, we find the optimal hy-
perparameters (the number of specialization tensor
slices K, the size of the specialized vectors n, and
the regularization factor λ) via grid search within
the 5-fold cross-validation on the training set.

4.2 Results and Discussion

Evaluation on CogALex-V. We show perfor-
mance (F1 score for all relations and micro-
averaged F1) on the CogALex-V dataset in Table
1.5 For a more direct comparison with the best-
performing shared task models, LexNet (Shwartz
et al., 2016) and the model of Attia et al. (2016),
we used 300-dimensional GloVe (Pennington et al.,
2014) distributional vectors as input.

Although not by a wide margin, STM out-
performs both best-performing models from the

3We first translated the dataset automatically with Google
Translate and then manually fixed the translation errors.

4We make WN-LS dataset publicly available, together
with the implementation of the specialization tensor model, at
https://github.com/codogogo/stm.

5Optimal STM config.: K = 5, n = 300, and λ = 0.001.

Model SYN ANT HYP MER All

Attia et al. (2016) 20.4 44.8 49.1 49.7 42.3
LexNet (2016) 29.7 42.5 52.6 49.3 44.5

CONCAT 10.9 28.5 34.8 32.9 27.4
BILIN-TENS 15.7 40.3 47.9 43.3 38.9

STM 22.1 50.4 49.8 50.4 45.3

Table 1: Performance on the CogALex-V dataset.

Model Lang. SYN ANT HYP MER All

LexNet EN 57.6 77.8 65.9 83.3 70.9
STM EN 58.6 86.6 63.5 79.5 72.5

STM DE 48.0 79.6 55.9 78.6 66.0
STM ES 52.3 80.5 62.6 78.8 68.6

Table 2: STM performance for three languages on (re-
spective translations of) the WN-LS dataset.

CogaLex-V shared task (Attia et al., 2016; Shwartz
et al., 2016), which is encouraging, given that STM
is simpler than both of these neural architectures.
STM outscores the model of Attia et al. (2016),
which uses the same input signal (i.e., only distri-
butional vectors) across the board. Overall, STM
also slightly outperforms LexNet (Shwartz and Da-
gan, 2016), despite the fact that LexNet addition-
ally employs rich syntactic signal. STM’s 6-point
edge over BILIN-TENS demonstrates the effective-
ness of multiple vector specializations, since this
performance gain can only be credited to the spe-
cialization tensor W[1..K]S . Antonymy is the relation
for which STM yields largest gain with respect to
other models.

Multilingual comparison. Table 2 displays clas-
sification performance for English, German, and
Spanish on respective variants of the WN-LS
dataset. On the EN WN-LS version we com-
pare STM’s performance against the LexNet model
(Shwartz and Dagan, 2016). To allow for a more
transparent comparison of STM’s performance
across languages, we employed 300-dimensional
English, German, and Spanish fastText em-
beddings (Bojanowski et al., 2017), pre-trained
on Wikipedia.6 STM slightly outperforms the
more complex LexNet model (Shwartz and Da-
gan, 2016) on the WN-LS dataset as well. We
believe STM’s (not drastically) lower scores for
German and Spanish are due to (1) distributional

6https://github.com/facebookresearch/
fastText/blob/master/pretrained-vectors.
md

184



Train Test SYN ANT HYP MER All

EN DE 39.1 66.8 49.3 67.6 55.1
EN ES 41.7 73.0 52.6 69.0 58.6
EN HR 30.5 64.7 49.1 60.5 51.5

DE EN 34.0 68.6 47.2 62.4 54.2
DE ES 39.1 61.9 44.4 60.5 50.6
DE HR 30.3 59.8 37.7 51.7 45.2

ES EN 47.9 74.9 46.4 68.2 59.9
ES DE 37.8 66.7 47.9 62.7 53.3
ES HR 36.1 62.2 44.6 61.5 51.4

Table 3: Zero-shot cross-lingual transfer. Best perfor-
mance for each test set is shown in bold.

vectors built from smaller corpora (ES and DE
Wikipedia being smaller than EN Wikipedia) and
(2) language-specific phenomena (e.g., a large num-
ber of compounds in German).

Zero-shot language transfer. Finally, we inves-
tigate whether a pre-trained STM model can be
leveraged to predict lexico-semantic relations for a
new language, from which it has observed no train-
ing instances. We are particularly interested in such
zero-shot language transfer for resource-lean lan-
guages, for which resources like WordNet do not
exist. To enable transfer experiments, we needed to
induce a shared bilingual (or multilingual) vector
space. In all experiments, we induced the shared
distributional spaces using the mapping approach
and translation matrices from Smith et al. (2017).

In the first set of transfer experiments, we trained
STM on the WN-LS train portion in one language
(EN, ES, or DE) and evaluated it on the test WN-LS
portions of all other languages, including Croatian
as a resource-lean language. We show the results of
these experiments in Table 3. Performance drops,
compared to respective monolingual settings (i.e.,
performance of models trained on the WN-LS train
set of the same language, see Table 2), range be-
tween 10% (EN→ES compared to monolingual ES
results) and 18% (DE→EN performance compared
to monolingual EN performance). These drops
in zero-shot language transfer are due to imper-
fect bilingual embedding spaces. In fact, language
transfer results seem to be very correlated with the
quality of corresponding embedding translation ma-
trices (highest for transfers between EN and ES and
lowest for DE→HR transfer).7 It is encouraging
that we can build a reasonable relation classifier

7For example, Smith et al. (2017) report P@1 bilingual
lexicon extraction performance of 73% for ES-EN, 61% for
DE-EN, and 55% for HR-EN.

Train Test SYN ANT HYP MER All

EN+ES HR 31.7 59.8 52.3 68.3 54.4
EN+DE HR 29.0 61.7 46.5 65.3 51.5
DE+ES HR 36.6 61.4 47.5 65.7 53.1

EN+ES+DE HR 36.5 64.6 51.2 64.7 54.1

Table 4: Language transfer results on the HR WN-LS.
Training on combinations of EN, ES, and DE data.

even for a resource-lean language, without a single
training instance for that language.

Finally, we examine whether we can improve
prediction performance for a resource-lean lan-
guage (i.e., Croatian) by combining the training
data from multiple resource-rich languages (i.e.,
English, German, and Spanish). We show the
results for this experiment in Table 4. By com-
bining training data from different resource-rich
languages, we further improve prediction perfor-
mance for a resource-lean language. Compared to
the EN→HR transfer, we observe 3% overall per-
formance gain when training on merged (EN+ES
and EN+ES+DE) datasets. ES and DE training
instances are, however, merely translations of the
original EN instances, i.e., there is no additional
external knowledge being introduced. We thus be-
lieve that the observed gains are due to additional
regularization provided by the multilingual train-
ing provides, which allows us to learn a model that
better generalizes across languages.

5 Conclusion

We have presented a novel neural architecture
for predicting lexico-semantic relations between
words. The proposed tensor-based specialization
model specializes distributional vectors in multi-
ple ways and then uses these specializations to
compute features for relation classification. We
have demonstrated that our model outperforms
more complex and resource-heavier models on two
benchmarking datasets. We have further shown that
our model is by design portable across languages
and that it supports zero-shot knowledge transfer to
resource-lean languages. As future work, we plan
to experiment with more advanced neural architec-
tures and finer-grained relations. We also intend to
port the model to more languages.

Acknowledgments

Ivan Vulić is supported by the ERC Consolidator
Grant LEXICAL (no. 648909).

185



References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.

Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 451–462.

Mohammed Attia, Suraj Maharjan, Younes Samih,
Laura Kallmeyer, and Thamar Solorio. 2016.
CogALex-V shared task: GHHH-detecting seman-
tic relations via word embeddings. In Proceedings
of the 5th Workshop on Cognitive Aspects of the Lex-
icon (CogALex-V), pages 86–91.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 23–32.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics (TACL), 5:135–
146.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, and
Diana Inkpen. 2017. Natural language infer-
ence with external knowledge. arXiv preprint
arXiv:1711.04289.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A. Smith.
2015. Retrofitting word vectors to semantic lexi-
cons. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 1606–1615.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.

Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2014. Learning semantic hi-
erarchies via word embeddings. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1199–1209.

Goran Glavaš and Sanja Štajner. 2015. Simplifying lex-
ical simplification: Do we need simplified corpora?
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 63–68.

Goran Glavaš and Simone Paolo Ponzetto. 2017.
Dual tensor model for detecting asymmetric lexico-
semantic relations. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 1757–1767.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 2044–2048.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations (ICLR).

Nitin Madnani and Bonnie J Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of the Conference on Neural In-
formation Processing Systems (NIPS), pages 3111–
3119.

Nikola Mrkšić, Ivan Vulić, Diarmuid O’Seaghdha, Ira
Leviant, Roi Reichart, Milica Gašić, Anna Korho-
nen, and Steve Young. 2017. Semantic special-
ization of distributional word vector spaces using
monolingual and cross-lingual constraints. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 5:309–324.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1532–1543.

Petar Ristoski, Stefano Faralli, Simone Paolo Ponzetto,
and Heiko Paulheim. 2017. Large-scale taxonomy
induction using entity and word embeddings. In Pro-
ceedings of the International Conference on Web In-
telligence, pages 81–87.

Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), pages 1025–1036.

Enrico Santus, Anna Gladkova, Stefan Evert, and
Alessandro Lenci. 2016. The CogALex-V shared
task on the corpus-based identification of seman-
tic relations. In Proceedings of the 5th Workshop
on Cognitive Aspects of the Lexicon (CogALex-V),
pages 69–79.

Vered Shwartz and Ido Dagan. 2016. CogALex-V
Shared Task: LexNET-integrated path-based and
distributional method for the identification of seman-
tic relations. In Proceedings of the 5th Workshop
on Cognitive Aspects of the Lexicon (CogALex-V),
pages 80–85.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 2389–
2398.

186



Samuel L. Smith, David H.P. Turban, Steven Hamblin,
and Nils Y. Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. In Proceedings of the International Con-
ference on Learning Representations (ICLR).

Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 371–
378.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Ur-
tasun. 2016. Order-embeddings of images and lan-
guage. In Proceedings of the International Confer-
ence on Learning Representations (ICLR).

Ivan Vulić and Nikola Mrkšić. 2018. Specialising word
vectors for lexical entailment. In Proceedings of the
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT).

Ivan Vulić, Nikola Mrkšić, Roi Reichart, Diarmuid
Ó Séaghdha, Steve Young, and Anna Korhonen.
2017. Morph-fitting: Fine-tuning word vector
spaces with simple language-specific rules. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 56–68.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to com-
positional paraphrase model and back. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 3:345–358.

Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 545–550.

187


