



















































SoNLP-DP System for ConLL-2016 Chinese Shallow Discourse Parsing


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 78–84,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

SoNLP-DP System for ConLL-2016 Chinese Shallow Discourse Parsing

Junhui Li1 Fang Kong1 Sheng Li2 Muhua Zhu2 Guodong Zhou1
1Natural Language Processing Lab, Soochow University, China
{lijunhui, kongfang, gdzhou}@suda.edu.cn

2 Alibaba Inc., Hangzhou, China
{lisheng.ls, muhua.zmh}@alibaba-inc.com

Abstract

This paper describes our submission to
the CoNLL-2016 shared task (Xue et al.,
2016) on end-to-end Chinese shallow dis-
course parsing. We decompose the end-
to-end process into four steps. Firstly,
we define a syntactically heuristic algo-
rithm to identify elementary discourse
units (EDUs) and further to recognize
valid EDU pairs. Secondly, we recognize
explicit discourse connectives. Thirdly,
we link each explicit connective to valid
EDU pairs to obtain explicit discourse re-
lations. For those valid EDU pairs not
linked to any explicit connective, they
become non-explicit discourse relations.1

Finally, we assign each discourse rela-
tion, either explicit or non-explicit with
a discourse sense. Our system is evalu-
ated on the closed track of the CoNLL-
2016 shared task and achieves 35.54% and
23.46% in F1-measure on the official test
set and blind test set, respectively.

1 Introduction

Shallow discourse parsing maps a piece of text
into a set of discourse relations, each of which
is composed of a discourse connective, two argu-
ments, and the sense of the discourse connective.
Shallow discourse parsing has been drawing more
and more attention in recent years due to its im-
portance in deep NLP applications, such as coher-
ence modeling (Barzilay and Lapata, 2005; Lin et
al., 2011), event extraction (Li et al., 2012), and
statistical machine translation (Tu et al., 2014).

During the past few years, English shallow dis-
course parsing has dominated the research on dis-

1In this paper, non-explicit discourse relations include dis-
course relations with type implicit, entrel, and altlex.

course parsing, thanks to the availability of Penn
Discourse TreeBank (PDTB) (Prasad et al., 2008).
As a representative, Lin et al. (2014) decompose
the end-to-end PDTB-styled discourse parser into
a few components, including a connective classi-
fier, an argument labeler, an explicit sense classi-
fier, and a non-explicit sense classifier. The popu-
larity of English shallow discourse parsing is fur-
ther fueled by the CoNLL-2015 shared task (Xue
et al., 2015). Meanwhile research on Chinese dis-
course parsing is also carried out smoothly (Zhou
and Xue, 2012; Li et al., 2014). As a comple-
ment to PDTB annotated on English TreeBank,
Chinese Discourse TreeBank (CDTB) (Zhou and
Xue, 2012) annotates shallow discourse relations
on Chinese TreeBank by using similar framework
of PDTB. However, the two languages have many
different properties. For example, the non-explicit
discourse relations in the training data of CoNLL-
2016 shared task dataset account for 54.75% in
English while they account for 78.27% in Chinese,
indicating the difficulties in Chinese shallow dis-
course parsing. Second, the two arguments of a
Chinese non-explicit discourse relation are more
apt to locate in the same sentence. This is veri-
fied by the statistics that 56.57% of Chinese non-
explicit discourse relations are within one sen-
tence while only 2.55% of English non-explicit
discourse relations are. In particular, the English
non-explicit discourse relations are usually com-
posed of two consecutive sentences.

This paper describes our submission to the
CoNLL-2016 shared task on end-to-end Chinese
shallow discourse parsing. A participant system
needs to (1) identify all explicit discourse connec-
tives in the text (e.g., continuous connectives “尽
管”, “另 一 方面”, discontinuous one “由于 ...
因此”), (2) identify the spans of text that func-
tion as the two arguments (i.e., Arg1 and Arg2)
for each discourse connective, and (3) predict the

78



sense of the discourse relations (e.g., Cause, Con-
dition, Contrast). Due to the differences between
Chinese and English, our approach to Chinese dis-
course parsing is very different from the one to
English discourse parsing (Lin et al., 2014; Kong
et al., 2014). For example, Lin et al. (2014) con-
struct non-explicit discourse relations in English
by looking for two consecutive sentences that are
not connected to any explicit connective. How-
ever, it fails to discover non-explicit discourse re-
lations in which the two arguments locate in one
sentence. Alternatively, we decompose the whole
process of our Chinese discourse parser into four
steps. Firstly, we define a syntactically heuristic
algorithm to identify elementary discourse units
(EDUs) and further to recognize valid EDU pairs.
Secondly, we recognize explicit discourse connec-
tives. Thirdly, we link each explicit connective to
valid EDU pairs to obtain explicit discourse re-
lations. For those valid EDU pairs not linked to
any explicit connective, they become non-explicit
discourse relations. Finally, we assign each dis-
course relation, either explicit or non-explicit with
a discourse sense. Our system is evaluated on the
closed track of the CoNLL-2016 shared task and
achieves 35.54% and 23.46% in F1-measure on
the official test set and blind test set, respectively.

The rest of this paper is organized as follows.
Section 2 describes the details of our Chinese shal-
low discourse parser. In Section 3, we present our
experimental results, followed by the conclusion
in Section 4.

2 System Architecture

In this section, we first present an overview of
our system. Then we describe the details of our
components in the end-to-end Chinese discourse
parser.

2.1 System Overview

A typical text consists of sentences glued together
in a systematic way to form a coherent discourse.
In PDTB and CDTB, shallow discourse parsing
focuses on shallow discourse relations either lex-
ically grounded in explicit discourse connectives
or associated with sentential adjacency. Different
from deep discourse parsing, shallow discourse
parsing transforms a piece of text into a set of
discourse relations between two adjacent or non-
adjacent discourse units, instead of connecting the
relations hierarchically to one another to form a

connected structure in the form of tree or graph.
Specifically, given a piece of text, the end-to-

end shallow discourse parser returns a set of dis-
course relations in the form of a discourse con-
nective (explicit or non-explicit) taking two argu-
ments with a discourse sense. Figure 1 shows the
framework of our end-to-end system which con-
sists of six components (i.e., from A to F). Next,
we decompose the process into four steps:

• Firstly, we define a heuristic algorithm to
identify elementary discourse units (EDUs)
and further to recognize valid EDU pairs.
This step includes components of A and B in
Figure 1.

• Secondly, we recognize explicit discourse
connectives. This is task of component C in
Figure 1.

• Thirdly, we link each explicit connective to
valid EDU pairs to obtain explicit discourse
relations. For those valid EDU pairs not
linked to any explicit connective, they be-
come non-explicit discourse relations. This
is what component D does in Figure 1.

• Finally, we assign each discourse relation, ei-
ther explicit or non-explicit with a discourse
sense. Specifically, we use component E to
assign sense for explicit discourse relations
while using component F for non-explicit
discourse relations.

2.2 EDU Identification
An EDU is a sequence of words that represents
an event, which is usually driven by a VP (a.k.a.
verbal phrase) node in parse tree. Given a parse
tree, we collect all basic VPs in it. In contrast to
a nested VP that is composed of either multiple
sub-VPs or a VP and its modifiers, a basic VP is a
VP that headed by a non-VP. For example, in Fig-
ure 2, VP2 and VP4 are basic VPs since VP2 is
headed by VE/无 while VP4 is headed by VV/通
过. In contrast, VP1 and VP3 are not basic VPs
since they are both headed by basic VPs, i.e., VP2
and VP4. For each basic VP, we use the heuris-
tic Algorithm 1 to find its left and right boundary
nodes, and thus obtain the word sequence repre-
senting the corresponding EDU.

It is easy to find the right boundary node since
we always set it as the basic VP node (line1). The
algorithm initializes the left boundary node as the

79



a	  piece	  of	  text	  

EDUs	   valid	  EDU	  pairs	  

explicit	  discourse	  connec5ves	  

explicit	  discourse	  
structure	  

non-­‐explicit	  
discourse	  
structure	  

explicit	  discourse	  
structure	  with	  

sense	  

non-­‐explicit	  
discourse	  

structure	  with	  
sense	  

A	  
B	  

C	  

D	  

E	  

F	  

A:	  EDU	  iden5fier;	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  D:	  linker	  that	  links	  connec5ve	  with	  EUD	  pairs;	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
B:	  valid	  EDU	  pair	  recognizer;	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  E:	  explicit	  discourse	  structure	  sense	  classifier;	  	  
C:	  explicit	  discourse	  connec5ve	  recognizer;	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  F:	  non-­‐explicit	  discourse	  structure	  sense	  classifier;	  

Figure 1: Framework of our end-to-end Chinese shallow discourse parser.

Algorithm 1: Obtaining EDU from a basic VP

Input: parse tree tree
basic VP node vp

Output: its corresponding EDU
1. define right boundary node rbn = vp;
2. define left boundary node lbn = vp;
3. set current node c as vp;
4. while (true)
5. set node p as c’s parent;
6. if (p == null) break;
7. get p’s production rule, say as lm .. l1 c r1 ..rn,

indicating c has m left hand siblings and
n right siblings;

8. for i from 1 to m
9. if li is dominated by c
10. lbn = li;
11. else
12. break;
13. if i <= m break;
14. c = p;
15. return word sequence from position leftmost of lbn

to rightmost of rbn;

basic VP node as well (line2). Then it repeat-
edly update the left boundary node until it finds a
proper one. To this end, the algorithm starts by set-
ting the current node c as the basic VP node (line
3), and first examine the left siblings from right to
left and see if they are dominated by c. It then it-
eratively moves one level up to the parent of c till
it reaches the root of the tree (line 14). At each
level, it repeatedly updates the left boundary node
(line 10). Specifically, if there exists a left sibling
which is not dominated by c, the algorithm stops
(line 12 & 13). Once both the left and right bound-
ary nodes are found. It uses the leftmost position
of the left boundary node and the rightmost posi-
tion of the right boundary node to obtain the word
sequence of the corresponding EDU. For VP2 and
VP4 in Figure 2, the algorithm will return “第二

IP	  

IP	  

VP1	  

QP	   VP2	  

第二 天	   无	   大 新闻	  

IP	  

种子 选手	  

VP4	  

NP	  

ADVP	  

VV	   QP	  

通过	   第一 轮	  ，	   顺利	  均	  

NP	  VE	  

ADVP	  

VP3	  

PU	  

e1	   e2	  

Figure 2: An example of recognizing EDUs.

天无大新闻” and “种子选手均顺利通过第
一轮” as their EDUs, respectively.

Note that for two EDUs that occur in one sen-
tence, they satisfy that either their spans have no
overlapping at all (e.g., e1 and e2 in Figure 2), or
one EDU fully covers the other.

2.3 Valid EDU Pair Recognition

A valid EDU pair is two EDUs that have discourse
relation, either explicit or non-explicit. We first
collect all potential EDU pairs as candidate, and
then identify valid ones. In an EDU pair, we pre-
sume the first EDU locates on the left side of the
second one.

Intra-EDU pair candidates. Intra-EDU pair
candidates indicate that the two focusing EDUs lo-
cate in one sentence. If a sentence contains two or
more EDUs, we enumerate all possible EDU pairs
as candidates as long as the pair have no overlap-
ping in position.

Inter-EDU pair candidates. The two EDUs
in an inter-EDU pair candidate locate in two sen-
tences. To make the task simple, we only consider

80



such candidates if the two EDUs are in two con-
secutive sentences. For two consecutive sentences
s1 and s2, we obtain their corresponding set (es1
and es2) of EDUs that are at top level (i.e., an EDU
is at top level if it is not covered by another EDU).
Then we enumerate all possible EDU pairs by se-
lecting one from es1 and the other from es2.

To identify an EDU pair candidate is valid or
not, we use tree kernel approach to explore im-
plicitly structured features by directly computing
the similarity between two subtrees. Given a parse
tree and an EDU pair candidate in it,2 we first
find the lowest ancestor node that fully covers the
two EDUs. Then we collect left and right siblings
along the path from the lowest ancestor node to
each basic VP node. For example, the dash circle
in Figure 2 represents the subtree for the EDU pair
of e1 and e2.

2.4 Explicit Discourse Connective
Recognition

Connectives in Chinese are more obscure than
those in English. For example, we extract 358
types of connective from the training data. Among
them, 193 (or 54%) types of connective occur once
while 197 (or 55%) types consist of two or more
words. Being worse, 32 (or 9%) types of con-
nective span two or more sentences. Our system
keeps 326 (or 91%) types of connective that locate
in one sentence as our connective set. That is to
say, we ignore those connectives that locate in two
or more sentences. The distribution of connective
in training data suggests that the connective set is
an open set. Given a piece of text, we first use
the connective set to collect connective candidates.
Then we identify each connective candidate is a
functional connective or not. Different from pre-
vious work that defines diverse linguistic features,
varying from lexical knowledge to syntactic parse
trees, we use tree kernel approach to explore im-
plicitly structured features by directly computing
the similarity between two subtrees. Given a parse
tree and a connective candidate in it, we first find
the lowest IP node that fully covers the connec-
tive. Then we collect left and right siblings along
the path from the IP node to each connective word.
For instance, sentence “由于新组建的国家队
新队员将占一半，而她们的技术水平尚
待提高，因此面临的任务是艰巨的 ” and

2for inter-EDU pair candidate, we manually create a top
node and take the parse trees of the two consecutive sentences
as children of top node.

IP	  

PP	  

P	  

由于	  

PU	   ADVP	  

AD	  

因此	  

VP	   PU	  

IP	  

……	   ……	  ，	   。	  

Figure 3: An example of subtree extraction for
connective recognition.

a discontinuous connective candidate “由于 ... 因
此” in it, we extract a subtree as shown in Figure 3.

2.5 Linking connective with EDU pairs

So far we have recognized both valid EDU pairs
and explicit discourse connectives. Our next step
is to link a connective to EDU pairs. Note that it
is possible for a connective to link to one or more
EDU pairs. To decide if a connective and an EDU
pair is relevant, we continue to use tree kernel ap-
proach. The subtrees extraction algorithm is very
similar to that of valid EDU pair recognition. The
algorithm first finds the lowest ancestor node that
covers the two EDUs and the connective. Then it
collects left and right siblings along the path from
the lowest ancestor node to connective word, and
to the two basic VP nodes, respectively. For in-
stance, in sentence “由于 新 组建 的 国家队
新 队员 将 占 一半 ， 而 她们 的 技术 水平
尚 待 提高 ， 因此 面临 的 任务 是 艰巨 的
”, we are about to predict if the connection exist
between a discontinuous connective “由于 ... 因
此” and an EDU pair colored in blue and green in
Figure 4. To this end, the subtree extraction al-
gorithm first looks for their lowest ancestor, i.e.,
the top IP in Figure 4, then the algorithm collect
all siblings along the paths from the lowest ances-
tor node (i.e., IP) to each connective word (i.e., P
and ADVP), and to the two basic VPs (i.e., the two
colored VPs). Figure 4 also shows the extracted
subtree.

Explicit discourse relations. If one or more
valid EDU pairs are predicted to have connec-
tion to a connective,3 we construct an explicit dis-

3If none EDU pair is predicted to have connection to a
connective, we take the pair with the highest probability as
the one linking to the connective.

81



IP	

PP	

P	

由于	

PU	 ADVP	

AD	

因此	

VP	 PU	

IP	

……							…...						......	 …...			……	，	 。	

NP	

IP	 PU	 IP	CC	

NP	 VP	

Figure 4: An example of subtree extraction for
linking a connective with an EDU pair.

course relation by merging all the first EDUs of the
EDU pairs as Arg1 of the connective, and merging
all the second EDUs of the EDU pairs as Arg2.

Non-explicit discourse relations. If a valid
EDU pair is not linked to any explicit connective,
we construct a non-explicit discourse relation by
regarding the first EDU as Arg1 and the second as
Arg2.

2.6 Sense Classification for Explicit discourse
relations

Once an explicit discourse relation is identified,
the sense classifier is used to predict its sense. Due
to the fact the connective themselves are strong
hint for their sense, we follow (Lin et al., 2014) to
define a few lexical features to train a sense classi-
fier: the connective words themselves, their part-
of-speeches and the previous words of each con-
nective word.

2.7 Sense Classification for Non-explicit
discourse relations

Due to the absence of discourse connectives, sense
prediction for non-explicit discourse relations is
more difficult. Following the work of Kong et al.
(2015) on non-explicit sense classification in En-
glish, we define the following groups of features:

• First three words of Arg1/Arg2: This set of
features include the first three words in Arg1
and Arg2.

• Production rules: According to Lin et al.
(2009), the syntactic structure of one argu-
ment may constrain the relation type and the
syntactic structure of the other argument. We

extract production rules from training data
with frequency larger than 5 times. Then for
each production rule pr, we add features pr-
in-arg1=1, pr-in-arg2=1, pr-in-arg1arg2=1
if it occurs in Arg1, Arg2, and both, respec-
tively.

• Dependency rules: Similar to the above
features of production rules, three sets of
features dr-in-arg1=1, dr-in-arg2=1, dr-in-
arg1arg2=1 if it occurs in Arg1, Arg2, and
both, respectively.

• Word pairs: We include all word pairs by
choosing one word from Arg1 and the other
from Arg2.

• Brown cluster pairs: Similar to the above
features of word pairs, we include all Brown
cluster pairs by choosing one word cluster
from Arg1 and the other from Arg2.

Besides the above features, the research on
English sense classification for non-explicit dis-
course relations has explored other useful features
about polarity, modality, and verb class (Karin et
al., 2006). Unfortunately, the shared task on Chi-
nese does not provide relevant resources to obtain
those features.

3 Experimentation

We evaluate our system on the Chinese dataset
provided in the close track of the CoNLL-2016
Shared Task. All our kernel-based classifiers
(e.g., valid EDU pair recognizer, connective rec-
ognizer, and linker connecting connectives with
EDU pairs) and flat feature-based classifiers (e.g.,
sense classifiers for either explicit discourse re-
lations or non-explicit discourse relations) are
trained using SVMLight toolkit for tree kernel.4

Table 1 shows our official performance on the
development, test and blind test sets, respectively.
From the table, we observe:

• For argument recognition, the performance of
Arg2 is much better than that of Arg1 on the
development and test datasets. This is similar
to the performance trend in English. How-
ever, the performance gap between Arg1 and
Arg2 recognition is very small on the blind
test dataset.

4http://disi.unitn.it/moschitti/TK1.0-software/Tree-
Kernel.htm

82



Dev Test Blind test
P R F1 P R F1 P R F1

Explicit

Connective 79.22 83.56 81.33 75.00 80.00 77.42 63.07 65.99 64.50
Arg1 45.45 47.95 46.67 40.62 43.33 41.94 36.57 38.26 37.40
Arg2 58.44 61.64 60.00 53.12 56.67 54.84 39.05 40.85 39.93
Arg1 & Arg2 33.77 35.62 34.67 28.12 30.00 29.03 22.79 23.84 23.31
Overall 35.62 33.77 34.67 27.78 26.04 26.88 21.15 20.14 20.63

Non-Explicit

Connective - - - - - - - - -
Arg1 65.69 54.32 59.47 62.95 55.67 59.08 54.20 52.36 53.27
Arg2 72.55 60.00 65.68 69.92 61.82 65.62 55.70 53.81 54.74
Arg1 & Arg2 55.56 45.95 50.30 52.37 46.31 49.15 42.67 41.22 41.93
Overall 32.97 39.87 36.09 34.24 38.72 36.34 23.35 24.17 23.75

All

Connective 79.22 83.56 81.33 75.00 80.00 77.42 63.07 65.99 64.50
Arg1 65.01 56.21 60.29 61.10 56.05 58.46 54.64 53.90 54.27
Arg2 71.54 61.85 66.34 68.79 63.10 65.83 53.64 52.91 53.27
Arg1 & Arg2 52.74 45.60 48.91 48.79 44.76 46.69 38.55 38.03 38.29
Overall 33.77 35.62 34.67 34.07 37.14 35.54 23.31 23.61 23.46

Table 1: Official results (%) of our parser on development, test and blind test sets. Group Explicit
indicates the performance with respect to explicit discourse relations; group Non-Explicit indicates the
performance with respect to non-explicit discourse relations, and group all indicates the performance
with respect to all discourse relations, including both explicit and non-explicit ones.

• With respect to explicit discourse relations,
the sense classification works almost per-
fectly on development data (e.g., almost no
performance gap from Arg1 & Arg2 to Over-
all. It also works well on the test and blind
test sets.

• With respect to non-explicit discourse rela-
tions, the sense classification works much
worse than that of explicit sense classifi-
cation. The performance gap caused by
non-explicit sense classification reaches 14%
18%.

• The overall performance on all discourse re-
lations is dominated by non-explicit ones.
This is because larger size of non-explicit dis-
course relations. For example, the size of
non-explicit discourse relations is 3.6 times
of that of explicit ones in training data.

• Our system achieves similar results on devel-
opment set and test set. However, the perfor-
mance on blind test decreases sharply, prob-
ably due to the differences in genres and the
bad quality of parse trees.

4 Conclusion

In this paper we have described our submission to
the CoNLL-2016 shared task on end-to-end Chi-
nese shallow discourse parsing. Our system is
evaluated on the closed track of the CoNLL-2016
shared task and achieves 35.54% and 23.46% in

F1-measure on the official test set and blind test
set, respectively.

Acknowledgments

The authors would like to thank the two anony-
mous reviewers for providing helpful suggestions.
This research is supported by Project 61472264,
61401295, 61305088 and 61402314 under the Na-
tional Natural Science Foundation of China.

References
Regina Barzilay and Mirella Lapata. 2005. Modeling

local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 141–148.

Kipper Karin, Korhonen Anna, Ryant Neville, and
Palmer Martha. 2006. Extending verbnet with novel
verb classes. In Proceedings of the 5th International
Conference on Language Resources and Evaluation.

Fang Kong, Hwee Tou Ng, and Guodong Zhou. 2014.
A constituent-based approach to argument labeling
with joint inference in discourse parsing. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing, pages 68–77.

Fang Kong, Sheng Li, and Guodong Zhou. 2015. The
sonlp-dp system in the conll-2015 shared task. In
Proceedings of the Nineteenth Conference on Com-
putational Natural Language Learning - Shared
Task, pages 32–36.

Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-
bin Hou. 2012. Employing compositional seman-
tics and discourse consistency in Chinese event ex-
traction. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language

83



Processing and Computational Natural Language
Learning, pages 1006–1016.

Yancui Li, Wenhe Feng, Jing Sun, Fang Kong, and
Guodong Zhou. 2014. Building Chinese dis-
course corpus with connective-driven dependency
tree structure. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 2105–2114.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
997–1006.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151–184.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th International Conference
on Language Resources and Evaluation.

Mei Tu, Yu Zhou, and Chengqing Zong. 2014. En-
hancing grammatical cohesion: Generating transi-
tional expressions for SMT. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
850–860.

Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
Prasad, Christopher Bryant, and Attapol Ruther-
ford. 2015. The CoNLL-2015 shared task on shal-
low discourse parsing. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning - Shared Task, pages 1–16.

Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, At-
tapol T. Rutherford, Bonnie Webber, Chuan Wang,
and Hongmin Wang. 2016. The CoNLL-2016
shared task on multilingual shallow discourse pars-
ing. In Proceedings of the Twentieth Conference on
Computational Natural Language Learning: Shared
Task.

Yuping Zhou and Nianwen Xue. 2012. PDTB-style
discourse annotation of Chinese text. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics, pages 69–77.

84


