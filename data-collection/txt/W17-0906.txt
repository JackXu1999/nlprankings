



















































LSDSem 2017 Shared Task: The Story Cloze Test


Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46–51,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

LSDSem 2017 Shared Task: The Story Cloze Test

Nasrin Mostafazadeh1, Michael Roth2,3, Annie Louis4,
Nathanael Chambers5, James F. Allen1,6

1 University of Rochester 2 University of Illinois at Urbana-Champaign 3 University of Edinburgh

4 University of Essex 5 United States Naval Academy 6 Florida Institute for Human & Machine Cognition

{nasrinm,james}@cs.rochester.edu mroth@coli.uni-saarland.de

aplouis@essex.ac.uk nchamber@usna.edu

Abstract

The LSDSem’17 shared task is the Story
Cloze Test, a new evaluation for story un-
derstanding and script learning. This test
provides a system with a four-sentence
story and two possible endings, and the
system must choose the correct ending to
the story. Successful narrative understand-
ing (getting closer to human performance
of 100%) requires systems to link vari-
ous levels of semantics to commonsense
knowledge. A total of eight systems par-
ticipated in the shared task, with a variety
of approaches including end-to-end neural
networks, feature-based regression mod-
els, and rule-based methods. The highest
performing system achieves an accuracy
of 75.2%, a substantial improvement over
the previous state-of-the-art.

1 Introduction

Building systems that can understand stories or
can compose meaningful stories has been a long-
standing ambition in natural language understand-
ing (Charniak, 1972; Winograd, 1972; Turner,
1994; Schubert and Hwang, 2000). Perhaps the
biggest challenge of story understanding is hav-
ing commonsense knowledge for comprehending
the underlying narrative structure. However, rich
semantic modeling of the text’s content involving
words, sentences, and even discourse is crucially
important. The workshop on Linking Lexical,
Sentential and Discourse-level Semantics (LSD-
Sem)1 is committed to encouraging computational
models and techniques which involve multiple lev-
els of semantics.

1http://www.coli.uni-saarland.de/
˜mroth/LSDSem/

The LSDSem’17 shared task is the Story Cloze
Test (SCT; Mostafazadeh et al., 2016). The SCT
is one of the recent proposed frameworks on
evaluating story comprehension and script learn-
ing. In this test, the system reads a four-sentence
story along with two alternative endings. It
is then tasked with choosing the correct end-
ing. Mostafazadeh et al. (2016) summarize the
outcome of experiments conducted using several
models including the state-of-the-art script learn-
ing approaches. They suggest that current meth-
ods are only slightly better than random perfor-
mance and more powerful models will require
richer modeling of the semantic space of stories.

Given the wide gap between human (100%)
and state-of-the-art system (58.5%) performance,
the time was ripe to hold the first shared task on
SCT. In this paper, we present a summary on the
first organized shared task on SCT with eight par-
ticipating systems. The submitted approaches to
this non-blind challenge ranged from simple rule-
based methods, to linear classifiers and end-to-
end neural models, to hybrid models that lever-
age a variety of features on different levels of lin-
guistic analysis. The highest performing system
achieves an accuracy of 75.2%, which substan-
tially improves the previously established state-of-
the-art. We hope that our findings and discussions
can help reshape upcoming evaluations and shared
tasks involving story understanding.

2 The Story Close Test (SCT)

In the SCT task, the system should choose the
right ending to a given four-sentence story. Hence,
this task can be seen as a reading comprehension
test in which the binary choice question is always,
‘Which of the two endings is the most plausible
correct ending to the story?’. Table 1 shows three
example SCT cases.

46



Context Right Ending Wrong Ending

Sammy’s coffee grinder was broken. He needed something
to crush up his coffee beans. He put his coffee beans in a
plastic bag. He tried crushing them with a hammer.

It worked for Sammy. Sammy was not that much into
coffee.

Gina misplaced her phone at her grandparents. It wasnt
anywhere in the living room. She realized she was in the
car before. She grabbed her dads keys and ran outside.

She found her phone in the car. She didnt want her phone any-
more.

Sarah had been dreaming of visiting Europe for years. She
had finally saved enough for the trip. She landed in Spain
and traveled east across the continent. She didn’t like how
different everything was.

Sarah decided that she preferred
her home over Europe.

Sarah then decided to move to
Europe.

Table 1: Example Story Cloze Test instances from the Spring 2016 release.

Story Title Story

The Hurricane Morgan and her family lived in Florida. They heard a hurricane was coming. They decided to
evacuate to a relative’s house. They arrived and learned from the news that it was a terrible storm.
They felt lucky they had evacuated when they did.

Marco Votes
For President

Marco was excited to be a registered voter. He thought long and hard about who to vote for.
Finally he had decided on his favorite candidate. He placed his vote for that candidate. Marco
was proud that he had finally voted.

Spaghetti
Sauce

Tina made spaghetti for her boyfriend. It took a lot of work, but she was very proud. Her
boyfriend ate the whole plate and said it was good. Tina tried it herself, and realized it was
disgusting. She was touched that he pretended it was good to spare her feelings.

Table 2: Example ROCStories instances from the Winter 2017 release.

As described in Mostafazadeh et al. (2016), the
SCT cases are collected through Amazon Mechan-
ical Turk (Mturk) on the basis of the ROCStories
corpus, a collection of five-sentence everyday life
stories which are full of stereotypical sequence of
events. To construct SCT cases, they randomly
sampled complete five-sentence stories from the
ROCStories corpus and presented only the first
four sentences of each story to the Mturk work-
ers. Then, for each story, a worker was asked to
write a ‘right ending’ and a ‘wrong ending’. This
resulting set was further filtered by human verifi-
cation: they compile each SCT case into two inde-
pendent five-sentence stories, once with the ‘right
ending’ and once with the ‘wrong ending’. Then,
for each story they asked three crowd workers to
verify if the given five-sentence story makes sense
as a meaningful story, rating on the scale of {-1,
0, 1}. Then they retain the cases in which the
‘right ending’ had three 1 ratings and the ‘wrong
ending’ had three 0 ratings. This verification step
ensures that there are no boundary cases of ‘right
ending’ and ‘wrong ending’ for human. Finally,
any stories used in creating this SCT set are re-
moved from the original ROCStories corpus.

3 Shared Task Setup

For the shared task, we provided the same dataset
as created by Mostafazadeh et al. (2016), which
consists of a development and test set each con-
taining 1,871 stories with two alternative end-
ings. At this stage, we used this already exist-
ing non-blind dataset with established baselines
to build up momentum for researching the task.
This dataset can be accessed through http://
cs.rochester.edu/nlp/rocstories/.

As the training data, we released an extended set
of ROCStories2, called ROCStories Winter 2017.
We followed the same crowdsourcing setup de-
scribed in Mostafazadeh et al. Table 2 provides
three example stories in this dataset. As these ex-
amples show, these are complete stories and do not
come with a wrong ending3. Although we pro-
vided the additional ROCStories, the participants
were encouraged to use or construct any training
data of their choice. Overall, the participants were
provided three datasets with the statistics listed in
Table 3.

Following Mostafazadeh et al. (2016), we eval-

2The extended ROCStories dataset can be accessed via
http://cs.rochester.edu/nlp/rocstories/.

3The ROCStories corpus can be used for a variety of ap-
plications ranging from story generation to script learning.

47



ROCStories (training data) 98,159
Story Cloze validation set, Spring 2016 1,871
Story Cloze test set, Spring 2016 1,871

Table 3: The size of the provided shared task
datasets.

uate the systems in terms of accuracy, which we
measure as #correct#test cases . Any other details re-
garding our shared task can be accessed via our
shared task page http://cs.rochester.
edu/nlp/rocstories/LSDSem17/.

4 Submissions

The Shared Task was conducted through CodaLab
competitions4. We received a total of 18 registra-
tions, out of which eight teams participated: four
teams from the US, three teams from Germany and
one team from India.

In the following, we provide short paragraphs
summarizing our baseline and approaches of the
submissions. More details can be found in the re-
spective system description papers.

msap (University of Washington). Linear clas-
sifier based on language modeling probabilities of
the entire story, and linguistic features of only the
ending sentences (Schwartz et al., 2017). These
ending “style” features include sentence length as
well as word and character n-gram in each candi-
date ending (independent of story). These style
features have been shown useful in other tasks
such as age, gender, or native language detection.

cogcomp (University of Illinois). Linear classi-
fication system that measures a story’s coherence
based on the sequence of events, emotional trajec-
tory, and plot consistency. This model takes into
account frame-based and sentiment-based lan-
guage modeling probabilities as well as a topical
consistency score.

acoli (Goethe University Frankfurt am Main)
and tbmihaylov (Heidelberg University). Two
resource-lean approaches that only make use of
pretrained word representations and compositions
thereof (Schenk and Chiarcos, 2017; Mihaylov
and Frank, 2017). Composition functions are
learned as part of a feed-forward and LSTM neural
networks, respectively.

4The Story Cloze CodaLab page can be accessed
here: https://competitions.codalab.org/
competitions/15333

ukp (Technical University of Darmstadt).
Combination of a neural network-based (Bi-
LSTM) classifier and a traditional feature-rich ap-
proach (Bugert et al., 2017). Linguistic features
include aspects of sentiment, negation, pronomi-
nalization and n-gram overlap between the story
and possible endings.

roemmele (University of Southern California).
Binary classifier based on a recurrent neural net-
work that operates over (sentence-level) Skip-
thought embeddings (Roemmele et al., 2017). For
training, different data augmentation methods are
explored.

mflor (Educational Testing Service). Rule-
based combination of two systems that score pos-
sible endings in terms of how well they lexically
cohere with and fit the sentiment of the given
story (Flor and Somasundaran, 2017). Sentiment
is given priority, and the model backs off to lexical
coherence based on pointwise mutual information
scores.

Pranav Goel (IIT Varanasi). Ensemble model
that takes into account scores from two systems
that measure overlap in sentiment and sentence
similarity between the story and the two possible
endings (Goel and Singh, 2017).

ROCNLP (baseline) Two feed-forward neural
networks trained jointly on ROCStories to project
the four-sentences context and the right fifth sen-
tence into the same vector space. This model is
called Deep Structured Semantic Model (DSSM)
(Huang et al., 2013) and had outperformed all
the other baselines reported in Mostafazadeh et al.
(2016).

5 Results

An overview of the models and the resources
used in each participating system, along with their
quantative results, is given in Table 4. Given
that the DSSM model was previously trained on
about 50K ROCStories, we retrained this model
on our full dataset of 98,159 stories. We include
the results of this model under ROCNLP in Ta-
ble 4. With accuracy values in a range from 60%
to 75.2%, we observe that all teams outperform the
baseline model. The best result in this shared task
has been achieved by msap, the participating team
from the University of Washington.

48



Rank CodaLab Id Model ROCStories Pre-trained
Embeddings

Other Resources Accuracy

1 msap Logistic
regression

Spring 2016,
Winter 2017

− NLTK Tokenizer, Spacy POS
tagger

0.752

2 cogcomp Logistic
regression

Spring 2016,
Winter 2017

Word2Vec UIUC NLP pipeline, FrameNet,
two sentiment lexicons

0.744

3 tbmihaylov LSTM − Word2Vec − 0.728
4 ukp BiLSTM Spring 2016,

Winter 2017
GloVe Stanford CoreNLP, DKPro TC 0.717

5 acoli SVM − GloVe,
Word2Vec

− 0.700

6 roemmele RNN Spring 2016,
Winter 2017

Skip-Thought − 0.672

7 mflor Rule-based − − VADER sentiment lexicon, Gi-
gaword corpus PMI scores

0.621

8 Pranav Goel Logistic
regression

Spring 2016,
Winter 2017

Word2Vec VADER sentiment lexicon,
SICK data set

0.604

9 ROCNLP
(baseline)

DSSM Spring 2016,
Winter 2017

− − 0.595

Table 4: Overview of models and resources used by the participating teams. For each team only their
best performing system on the Spring 2016 Test Set is included, as submitted to CodaLab. Please refer
to the system description papers for a list of other models. Human is reported to perform at 100%.

6 Discussion

We briefly highlight some observations regarding
modeling choices and results.

Embeddings. All but two teams made use of
pretrained embeddings for words or sentences.
tbmihaylov (Mihaylov and Frank, 2017) exper-
imented with various pretrained embeddings in
their resource-lean model and found that the
choice of embeddings has a considerable impact
on model accuracy. Interestingly, the best partici-
pating team used no pretrained embeddings at all.

Neural networks. The six highest scoring mod-
els all include neural network architectures in one
way or another. While the teams ranked 3–6 at-
tempt to utilize hidden layers directly for predic-
tion, the top two teams use the output of neural
language models to generate different combina-
tions of features. Further, while the third place
team’s best model was an LSTM, their logistic re-
gression classifier with Word2Vec-based features
achieved similar performance. The combination
of different neural features (including non-neural
ones) appears to have made the difference in the
top system’s ablation tests.

Sentiment. Three teams report concurrently that
a sentiment model alone can achieve 60–65%

accuracy but performance seems to vary depen-
dent on implementation details. This is notable
in that the sentiment baseline which chose the
ending with a matching sentiment to the context
(presented in Mostafazadeh et al. (2016)) did not
achieve accuracy above random chance. One dif-
ference is that these more successful approaches
used sentiment lexicons to score words and sen-
tences, whereas Mostafazadeh et al. used the auto-
matic sentiment classifier in Stanford’s CoreNLP.
Finally, mflor (Flor and Somasundaran, 2017) an-
alyzed the Story Cloze Test Validation (Spring
2016) set and found that 78% of the stories have
sentiment bearing words in the first sentences and
in at least one possible ending. Evaluating on
that subset showed increased performance, further
suggesting that sentiment is an important factor in
alternate ending prediction.

Stylistic Features on Endings. One of the mod-
els proposed by msap (Schwartz et al., 2017) ig-
nored the entire story, building features only from
the ending sentences. They trained a linear clas-
sifier on the right and wrong ending sentences
adopting style features that have been shown use-
ful in other tasks such as gender or native lan-
guage detection. This model achieved remarkably
good performance at 72.4%, indicating that there

49



are characteristics inherent to right/wrong endings
independent of story reasoning. It is not clear
whether these results generalize to novel story
ending predictions, beyond the particular Spring
2016 sets. Whether this model captures an artifact
of the test set creation, or it indicates general fea-
tures about how stories are ended must remain for
future investigation.

Negative results. Some papers describe addi-
tional experiments with features and methods
that are not part of the submitted system, be-
cause their inclusion resulted in sub-optimal per-
formance. For example, Pranav Goel (Goel
and Singh, 2017) discuss additional similarity
measures based on doc2vec sentence representa-
tions (Le and Mikolov, 2014); tbmihaylov (Mi-
haylov and Frank, 2017) experiment with Con-
ceptNet Numberbatch embeddings (Speer and
Chin, 2016); and mflor (Flor and Somasundaran,
2017) showcase results with alternative sentiment
dictionaries such as MPQA (Wilson et al., 2005).

7 Conclusions

All participants in the Story Cloze shared task of
LSDSem outperformed the previously published
best result of 58.5%, and the new state-of-the-
art accuracy dramatically increased to 75.2% with
the help of a well-designed RNNLM and unique
stylistic features on the ending sentences.

One of the main takeaways from the 8 submis-
sions is that the detection of correct ending sen-
tences requires a variety of different reasoners. It
appears from both results and post-analysis that
sentiment is one factor in correct detection. How-
ever, it is also clear that coherence is critical, as
the systems with language models all observed
increases in prediction accuracy. Beyond these,
the best performing system showed that there are
stylistic features isolated in the ending sentences,
suggesting yet another area of further investigation
for the next phases of this task.

As the first shared task on SCT, we decided not
to hold a blind challenge. For the future blind
challenges, the question is how robust are the pre-
sented approaches to novel test cases and how well
can they generalize out of the scope of the cur-
rent evaluation sets. We speculate that the models
which use generic language understanding and se-
mantic cohesion criteria rather than relying on cer-
tain intricacies of the testing corpora can general-
ize more successfully, which should be carefully

assessed in future.
Although this shared task was successful at set-

ting a new state-of-the-art for SCT, clearly, there
is still a long way towards achieving human-level
performance of 100% on even the current test set.
We are encouraged by the high level of participa-
tion in the LSDSem 2017 shared task, and hope
the new models and results encourage further re-
search in story understanding. Our findings can
help direct the creation of the next SCT datasets
towards enforcing deeper story understanding.

Acknowledgment

We would like to thank the wonderful crowd work-
ers whose daily story writing made collecting the
additional dataset possible. We thank Alyson Gre-
alish and Ahmed Hassan Awadallah for their help
in the quality control of ROCStories corpus. This
work was supported in part by grant W911NF-
15-1-0542 with the US Defense Advanced Re-
search Projects Agency (DARPA) as a part of the
Communicating with Computers (CwC) program,
a grant from the Office of Naval Research, Army
Research Office (ARO), and a DFG Research Fel-
lowship (RO 4848/1-1).

References
Michael Bugert, Yevgeniy Puzikov, Andreas Rckl, Ju-

dith Eckle-Kohler, Teresa Martin, Eugenio Martnez-
Cmara, Daniil Sorokin, Maxime Peyrard, and Iryna
Gurevych. 2017. LSDSem 2017: Exploring data
generation methods for the story cloze test. In Pro-
ceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(LSDSem), Valencia, Spain. Association for Compu-
tational Linguistics.

Eugene Charniak. 1972. Toward a Model of Children’s
Story Comprehension. Ph.D. thesis, MIT.

Michael Flor and Swapna Somasundaran. 2017. Sen-
timent analysis and lexical cohesion for the story
cloze task. In Proceedings of the 2nd Work-
shop on Linking Models of Lexical, Sentential
and Discourse-level Semantics (LSDSem), Valencia,
Spain. Association for Computational Linguistics.

Pranav Goel and Anil Kumar Singh. 2017. IIT (BHU):
System description for LSDSem’17. In Proceed-
ings of the 2nd Workshop on Linking Models of Lex-
ical, Sentential and Discourse-level Semantics (LS-
DSem), Valencia, Spain. Association for Computa-
tional Linguistics.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep

50



structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
International Conference on Information & Knowl-
edge Management, (CIKM ’13), pages 2333–2338,
New York, NY, USA. ACM.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of the 31st International Conference on
Machine Learning, Beijing, China.

Todor Mihaylov and Anette Frank. 2017. Simple story
ending selection baselines. In Proceedings of the
2nd Workshop on Linking Models of Lexical, Senten-
tial and Discourse-level Semantics (LSDSem), Va-
lencia, Spain. Association for Computational Lin-
guistics.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 839–849, San
Diego, California, June. Association for Computa-
tional Linguistics.

Melissa Roemmele, Sosuke Kobayashi, Naoya Inoue,
and Andrew Gordon. 2017. An RNN-based bi-
nary classifier for the story cloze test. In Proceed-
ings of the 2nd Workshop on Linking Models of Lex-
ical, Sentential and Discourse-level Semantics (LS-
DSem), Valencia, Spain. Association for Computa-
tional Linguistics.

Niko Schenk and Christian Chiarcos. 2017. Resource-
lean modeling of coherence in commonsense sto-
ries. In Proceedings of the 2nd Workshop on Linking
Models of Lexical, Sentential and Discourse-level
Semantics (LSDSem), Valencia, Spain. Association
for Computational Linguistics.

Lenhart K. Schubert and Chung Hee Hwang. 2000.
Episodic logic meets little red riding hood: A com-
prehensive, natural representation for language un-
derstanding. In Natural Language Processing and
Knowledge Representation: Language for Knowl-
edge and Knowledge for Language. MIT/AAAI
Press.

Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017. Story
cloze task: UW NLP system. In Proceedings of the
2nd Workshop on Linking Models of Lexical, Senten-
tial and Discourse-level Semantics (LSDSem), Va-
lencia, Spain. Association for Computational Lin-
guistics.

Robert Speer and Joshua Chin. 2016. An ensemble
method to produce high-quality word embeddings.
arXiv preprint arXiv:1604.01692.

Scott R. Turner. 1994. The creative process: A com-
puter model of storytelling and creativity. Hillsdale:
Lawrence Erlbaum.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 347–354, Vancouver, British
Columbia, Canada, October.

Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Inc., Orlando, FL, USA.

51


