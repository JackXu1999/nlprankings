



















































Neural Lexicons for Slot Tagging in Spoken Language Understanding


Proceedings of NAACL-HLT 2019, pages 83–89
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

83

Neural Lexicons for Slot Tagging in Spoken Language Understanding

Kyle Williams
Microsoft

One Microsoft Way
Redmond, Washington, 98052

kyle.williams@microsoft.com

Abstract

We explore the use of lexicons in neural mod-
els for slot tagging in spoken language under-
standing. We develop models that encode lex-
icon information as features for use in a Long-
short term memory neural network. Experi-
ments are performed on data from 4 domains
from an intelligent assistant under conditions
that often occur in an industry setting, where
there may be: 1) large amounts of training
data, 2) limited amounts of training data for
new domains, and 3) cross domain training.
Results show that the use of neural lexicon in-
formation leads to a significant improvement
in slot tagging, with improvements in the F-
score of up to 12%.

1 Introduction

Spoken language understanding (SLU) is an im-
portant component of systems that interface with
users, such as intelligent assistants. These systems
are becoming increasingly popular as a means for
people to accomplish tasks in their homes and on
mobile devices. These tasks might include switch-
ing on the lights or booking a taxi. Typically, an
SLU system detects the domain, intent, and se-
mantic slots of an utterance (Li et al., 2017) and
uses the information to perform actions.

It is common to use lexicons (also known as
gazettes or dictionaries) to improve the perfor-
mance of SLU systems (Ratinov and Roth, 2009).
Lexicons are typically collections of phrases that
are semantically related and thus provide knowl-
edge that can aid the SLU system. For instance, a
lexicon called holidays might contain the phrases
Thanksgiving, Christmas Eve, Labor Day. Simi-
larly, a lexicon called days of the week would con-
tain Monday, Tuesday, Wednesday, etc. There are
many ways lexicons can be built, such as by using
domain experts or by harvesting information from

knowledge graphs, such as DBPedia1. In an indus-
try setting, it is also possible that lexicons already
exist for other natural language applications.

Previous work has shown how lexicons can be
used to improve slot tagging with Conditional
Random Fields (Ratinov and Roth, 2009), where
slot tagging refers to the process of identifying
semantic entities of interest in an utterance. For
instance, given the utterance ”book a taxi to the
airport”, a slot tagging model might identify taxi
as a transport type and airport as a destination.
In this paper, we investigate the effect of integrat-
ing these types of lexicons into Long-Short Term
Memory (LSTM) neural models in an industry set-
ting. We focus on LSTM models since they have
been shown to produce state-of-the-art results in
many natural language tasks. We consider inte-
grating lexicon features into a Long-short term
memory neural network in two ways: 1) by con-
sidering lexicon membership as binary features
and 2) by embedding the lexicons and allowing
the model to learn the representation as part of the
end-to-end training of the neural network.

To evaluate these approaches, we measure the
performance of models on data from four domains
belonging to an intelligent assistant under three
data scenarios that commonly occur in production
SLU systems. The first scenario is when there is
a considerable amount of training data available
to train a SLU system, as may occur if a sizeable
investment has been made to collect data. The sec-
ond scenario is when there is only a small amount
of training data available, as may be the case when
the SLU is expanded to cover new domains for
which very little training data exists. The third sce-
nario is cross domain slot prediction, where we use
a model trained on utterances from one domain to
identify entities in utterances belonging to another

1https://wiki.dbpedia.org/



84

domain. This setting commonly occurs when one
attempts to leverage existing SLU models for use
in a new domain.

2 Related Work

There have been many previous studies involving
spoken language understanding for the slot tag-
ging problem. Yao et al. (2014) investigate the
use of LSTMs for slot tagging and compare the
performance of the LSTM-based model to a stan-
dard RNN and a Conditional Random Field. Their
results show the LSTM to outperform the two
other models. Mesnil et al. (2015) evaluate several
RNN-based models for slot tagging and show the
RNN-based models to outperform the CRF-based
model. Ma and Hovy (2016) propose a LSTM-
CNN-CRF model, which induces character repre-
sentations using a convolutional neural network.
The character representations are then combined
with word embeddings and fed into an LSTM, and
lastly the output of the LSTM is fed into a CRF de-
coder. In Kurata et al. (2016), the authors use an
encoder-labeler approach to first encode sentences
into fixed size vectors and then use the encoded
state as the initial state for a labeling LSTM.

The reason that many researchers have been us-
ing LSTMs for natural language understanding is
due to their ability to model long-term dependen-
cies. However, some researchers have proposed
other architectures. For instance, Shi et al. (2016)
propose the Recurrent Support Vector Machine
(RSVM), which uses a recurrent neural network
to induce a feature representation and a structured
support vector machine to perform structured pre-
diction on the output of the RNN.

Dugas and Nichols (2016) use lexicon em-
bedding features for named entity recognition in
tweets. They produce lexicon embeddings that
are concatenated with word embeddings; how-
ever, our work differs in the inclusion of the addi-
tional neural lexicon models, experiments compar-
ing them under varying data conditions that com-
monly occur in an industry setting, and our evalua-
tion is based on spoken utterances from an intelli-
gent assistant rather than tweets. Furthermore, we
analyze cases where lexicons are useful and cases
where they are not.

3 Neural Lexicon Models

Our proposed neural lexicon models are based on
an Long Short-Term Memory (LSTM) architec-

ture. The architecture is shown visually in Figure
1 and described in detail below.

In each of our models we induce a feature
representation based on the characters and words
that appear in an input sequence of words. We
closely follow the approach of previous studies
(Kim et al., 2017; Lample et al., 2016) and in-
duce both character and word embeddings using
bidirectional LSTMs. As in Kim et al. (2017), for
a given sequence of words W = w1, w2, ..., wn
where word wi has character wi(j) at position j.
We define the following:

• Character embedding: ec for each c ∈ C
• Character LSTM: φCf , φCb
• Word embedding: ew for each w ∈W
• Word LSTM: φWf , φWb ,

where φCf , φ
C
b , φ

W
f , φ

W
b refer to the forward and

backward character and word LSTMs. A character
sensitive word representation vi is computed as:

fCj = φ
C
f (ewi(j), f

C
j−1), ∀j = 1...|wi| (1)

bCj = φ
C
b (ewi(j), b

C
j+1),∀j = |wi|...1 (2)

vi = f
C
|wi| ⊕ b

C
1 ⊕ ewi , (3)

where⊕ represents the vector concatenation oper-
ation whereby the final states of the forward and
backward LSTMs are concatenated with the word
embedding. Next the model computes:

fWi = φ
W
f (vi, f

W
i−1), ∀i = 1...n (4)

bWi = φ
W
b (vi, b

W
i+1),∀i = n...1 (5)

In other words, the forward and backward word
LSTMs are used to induce character and context
sensitive word representations. Finally, the states
of the forward and backward LSTMs are concate-
nated to induce the final word representation ri:

ri = f
W
i ⊕ bWi , (6)

for each word wi, i = 1...n. These ri are the word
representations that we use for slot prediction.

In this study we focus on slot tagging and pre-
dict the tag of each word wi, i = 1...n using
ri, i = 1...n. To do this we add a feed for-
ward layer g, which takes as input the ri at each
timestep. We take the softmax of the the output
of g to produce probabilities of semantic tags for
each wordwi. We then minimize the cross entropy
loss:

Losstag = −
∑
i

pi log qi, (7)



85

Char-level

Bidirectional

LSTM

Word-level

Bidirectional

LSTM

Character

embedding
Word

embedding
…

…

𝑔1
𝑡𝑔1
𝑡

𝑦1
𝑡

𝜙𝑓
𝑤𝜙𝑓
𝑤

𝜙𝑏
𝑤𝜙𝑏
𝑤

𝑔2
𝑡𝑔2
𝑡

𝑦2
𝑡

Feedforward

Utterance

𝜙𝑓
𝑤𝜙𝑓
𝑤

𝜙𝑏
𝑤𝜙𝑏
𝑤

𝑦𝑛
𝑡

𝜙𝑓
𝑤𝜙𝑓
𝑤

𝜙𝑏
𝑤𝜙𝑏
𝑤

𝑔𝑛
𝑡𝑔𝑛
𝑡

𝑤1

…

𝑐1,1 𝑐1,2 𝑐1,𝑚

…
𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝑤2

…

𝑐2,1 𝑐2,2 𝑐2,𝑚

…
𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝑤𝑛

…

𝑐𝑛,1 𝑐𝑛,2 𝑐𝑛,𝑚

…
𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

𝜙𝑓
𝑐𝜙𝑓
𝑐

𝜙𝑏
𝑐𝜙𝑏
𝑐

…

Lexicon

membership/

embedding

Figure 1: Overall network architecture

where pi is the distribution of the true labels and
qi is the distribution of the predicted labels.

3.1 Lexicon Membership Model
Having described the basic form of our slot tag-
ging model, we now describe how we extend the
model to include lexicon features.

Assume that we have a collection of L lexi-
cons, with each lexicon containing a collection
of words and phrases belonging to that lexicon.
For instance, a lexicon called holidays might con-
tain: Thanksgiving, Christmas, Labor Day. Sim-
ilarly, a lexicon called days of the week would
contain Monday, Tuesday, Wednesday, etc. For
each word wi in an utterance we generate the uni-
gram, bi-gram, and tri-gram beginning at word
wi, and we refer to this triple as ti. For in-
stance, for the utterance ”book a taxi to the air-
port”, t1 = [”book”, ”book a”, ”book a taxi”], t2
= [a”, ”a taxi”, ”a taxi to”], etc. We use tji , j =
1, 2, 3, to refer to the j-th element of ti, i.e., the
uni-gram, bi-gram, or tri-gram.

For each word wi in the input utterance, we
define a membership lexicon feature vector lexi
of length |L|, where each element lexi(l), l =
1, 2, ..., |L|, is defined as:

lexi(l) =

{
1 if tji in lexicon l, j = 1, 2, 3
0 otherwise

(8)

In other words, every word wi has an associ-
ated feature vector lexi of length |L|. The k-th

element of lexi is 1 if a uni-gram, bi-gram, or tri-
gram rooted at wi exists in the k-th lexicon and is
zero otherwise.

The word representation (Eq. 3) for each word
is modified to include lexicon information:

vlexi = f
C
|wi| ⊕ b

C
1 ⊕ ewi ⊕ lexi. (9)

The effect of this is to append a binary feature
lexicon membership feature vector to the word
representation. We then input the vlexi into the
word LSTM layer to induce the character, lex-
icon, and context sensitive word representation.
This can be seen visually in Figure 1 where the
lexicon membership features are appended to the
word embeddings and output of the character-
level LSTM before being fed into the word-level
LSTM. We refer to this model as the Lexicon
Membership Model (MMember).

3.2 Lexicon Embeddings Model

In the next model we propose to embed the lex-
icon information. Similar to the case of the word
embeddings, we define lexicon embeddings as fol-
lows:

• Lexicon embedding: el for each l ∈ L,
• No Lexicon embedding: eo,

that is, each of the |L| lexicons is represented by
an embedding and eo represent an additional em-
bedding, which is used in cases where a uni-gram,



86

bi-gram, or tri-gram rooted at a word does not be-
long to any lexicon. We then define the embedded
lexicon feature vector as:

ELexi =

{
el if t

j
i in lexicon l, j = 1, 2, 3, l ∈ L

eo otherwise.
(10)

In other words, if a uni-gram, bi-gram, or tri-
gram rooted at wi appears in a lexicon l, we assign
the embedding of lexicon l to ELexi. If two or
more of the n-grams rooted at a word wi match a
lexicon, we use the longest match.

We then adapt Eq. 9 to use the lexicon embed-
dings instead of the lexicon membership feature:

vELexi = f
C
|wi| ⊕ b

C
1 ⊕ ewi ⊕ ELexi. (11)

The effect of this is to append a lexicon em-
bedding to the word representation. As was the
case with before, we input the vELexi into the word
LSTM layer to induce the character, lexicon, and
context sensitive word representation as in Eq. 4-
6. This can be seen visually in Figure 1 where
the lexicon embedding features are appended to
the word embeddings and output of the character-
level LSTM before being fed into the word-level
LSTM. We refer to this model as the Lexicon Em-
beddings Model (MEmbed).

4 Experiments

We conduct experiments under three settings in or-
der to evaluate how the lexicon models affect slot
tagging performance.

• All Training Data: In this setting we use all
of the available training data for each domain
when training the slot tagging models.
• Limited Training Data: In this setting we

simulate the constrained data scenario that
often occurs when expanding an SLU system
to support new domains and limit the amount
of training data available.
• Cross Domain: In this setting we consider

cross domain prediction and train on one do-
main and then predict common slots in other
domains.

4.1 Data
We conduct all of our experiments on data from
four domains belonging to an intelligent assistant.
The source of the data is user utterances, which

Domain Slots Train Validation Test

Recipes 10 20,000 6,809 1,186
Services 10 1800 194 500
Location 4 136,783 34,954 51,976
Time 4 129,340 34,644 46,803

Table 1: Description of datasets.

were transcribed by a speech-to-text system and
then had their semantic slots labeled by trained an-
notators. The domains that we use are Recipes,
Services, Location, and Time. The Recipes do-
main focuses on assisting users with recipes. The
Services domain is used to help users find ser-
vices, such as car repairs. The Location and Time
domains identify location and time information
in user utterances. These four domains differ in
terms of the types of queries they contain, as well
as their data sizes and the size of their lexicons.
Table 1 shows details on the slots and amount of
data available for each domain. As can be seen
from the table, the results differ with the Location
and Time domains having large amounts of train-
ing data available, while the Services domain has
very limited data.

4.2 Lexicon Descriptions
We now describe the lexicons associated with each
domain. The lexicons were created as part of the
data pipeline for an intelligent assistant. Table 2
lists the domains and the number of lexicons asso-
ciated with each of them. For the Recipes domain
there are 3 lexicons: ingredients, recipe names,
and cocktail names, with members such as: ba-
nana pudding and mojito. For the Services domain
there is only 1 lexicon, which contains types of
services, such as: pet sitting. The Location do-
main has 13 lexicons for countries, cities, schools,
etc. Finally, the Time domain contains 20 lexicons
for days of the week, holidays, etc.

The size of the lexicons also varies. For in-
stance, the lexicons in the Time domain usually
have tens of members. By contrast, in the Location
domain, there are over 300,000 city names and
only about 400 airport names. There are almost
30,000 recipes compared to about 300 cocktails
and ingredients in the Recipes domain. Lastly, for
the Services domain there are about 2,300 services
types. As this analysis has shown, the properties
of the lexicons vary largely among the datasets.

We also analyze the lexicon prevalence in the



87

Domain Number of Lexicons Prevalence

Recipes 3 28.95%
Services 1 29.78%
Location 13 99.13%
Time 20 81.63%

Table 2: Prevalence of lexicon features in training data.

training data. For the training data we compute
how many utterances have a sequence of words
that belongs to at least one lexicon. These results
are shown in the last column of Table 2, where it
can be seen that the prevalence of lexicons differs
vastly across the domains. For instance, 99.13%
of the training utterances in the Location domain
contain a word or phrase that belongs to a lexi-
con. By comparison, for the Recipes domain, only
28.95% of utterances contain a word or phrase that
belongs to a lexicon. As will be seen later, we gen-
erally see larger improvements in domains with
larger lexicon coverage.

4.3 Methodology
Having described our models and data, we now
describe our experimental methodology. For all
experiments we randomly initialize all model pa-
rameters and shuffle the training dataset for each
epoch. To account for randomization, we repeat
each experiment 10 times and report the mean of
the F-1 metric. To test for significance we make
use of the Wilcoxon signed-rank test.

For each experiment, we allow for up to 30
epochs of training and employ early stopping
when there is no improvement in the lowest loss
on the validation set for 5 epochs. We use a batch
size of 10 and set the learning rate to 5×10−4. We
set the dropout probability to 0.5. To train the net-
work, we make use of stochastic gradient descent
and the Adam optimization algorithm (Kingma
and Ba, 2014). We train the network end-to-end to
predict the slot tags for each utterance, thus allow-
ing the network to learn the character, word and
lexicon representations automatically.

Following previous studies, we set the size of
the character and lexicon embeddings to 25 and
the size of the word embeddings to 100. The
character and lexicon LSTMs have 25 units and
the word LSTMs have 100 units.To evaluate our
model we report the F-1 score as adapted for en-
tity recognition (Tjong Kim Sang and De Meulder,
2003).

Method Recipes Services Location Time

LSTM BL 91.20 78.11 84.79 94.05
CRF 90.21 74.51 86.07 93.43
CRF+Lex 90.69 75.47 87.72 93.55
MEmbed 91.27 77.37 86.01† 94.28†
MMember 91.16 78.13 85.42 94.15

Table 3: F1 score for different models using full train-
ing set.

4.3.1 Baselines
We consider three baselines:

LSTM Baseline (LSTM BL): We consider a
baseline LSTM model that does not include any
lexicon information. This model is described by
Equations 1-7.

Conditional Random Field (CRF): Linear
chain CRF where we make use of n-gram features
and brown cluster-based features.

Conditional Random Field + Lexicon Fea-
tures (CRF+Lex): The same CRF as above, ex-
cept we include binary features indicating lexicon
membership.

4.4 Results

4.4.1 Full Dataset
This experiment uses all of the available training
data. The results are shown in Table 3. For the
Recipes domain, the highest F1 score is achieved
by the MEmbed model; however, the difference is
not statistically significant compared to the base-
line LSTM. For this domain, the LSTM models
all outperform the CRF models. For the Services
domain, we observe that the MMember model
achieves the highest F1 score; however, it is also
not statistically significant compared to the base-
line LSTM. Once again, the LSTM models outper-
form the CRF models. For the Location domain
we observe a statistically significant improvement
in performance for the MEmbed model compared
to the baseline LSTM. This improvement exceeds
1%. However, the CRF model outperforms the
LSTM models. Lastly, for the the Time domain we
observe a significant improvement in the F1 score
compared to the baseline LSTM for the MEmbed

model. Furthermore, all LSTM models outper-
form the CRF baselines.

The results in this experiment show that the
MEmbed model achieves a statistically significant
improvement over the baseline in two of the four



88

Method Recipes Services Location Time

Baseline 52.74 46.83 40.01 68.75
MLoss 48.11 41.08 31.75 67.69
MEmbed 57.86 52.54† 51.40 74.09†
MMember 58.97† 48.98 52.78† 70.97

Table 4: F1 score for different models using 1,000 sam-
ples during each training iteration.

datasets. As was previously discussed, these are
datasets where the lexicons cover a majority pro-
portion of the training data. Thus, the results
indicate that the inclusion of lexicon informa-
tion is useful if there is large lexicon coverage
in the training data. As a general observation,
the LSTM-based models tend outperform the CRF
models in 3 of the 4 domains, which is similar to
the findings of previous studies (Yao et al., 2014)

4.4.2 Limited Training Data
In this experiment, we investigate how the pro-
posed models perform in the case of limited train-
ing data. We follow a similar methodology as be-
fore with the following changes: 1) during each
training epoch we randomly sample 1,000 train-
ing samples; 2) we use a batch size of 1; 3) we
lower the learning rate to 5× 10−5. The results of
this experiment are shown in Table 4.

As can be seen from the table, there are large
improvements in all domains when the lexicon-
based models are used. In the Recipes domain the
F1 score is 52.74% for the baseline and is 58.97%
for the best performing MMember model. For
the Services model, the MEmbed model achieves
an F1 score of 52.54% compared to 46.83% for
the baseline. In the Location domain the F1
score is 12.77% higher than the baseline using
the MMember model, and for the Time domain
the improvement is 5.34% better using the highest
performing MEmbed model. The MMember and
MEmbed models thus achieve the highest F1 score
on two domains each. However, it is interest-
ing to note that when the MEmbed model outper-
forms the MMember model is it usually by about
3-4%. By contrast, when theMMember model per-
forms best it usually only performs better than the
MEmbed model by around 1%.

The results on these smaller datasets suggests
that lexicons can have a significant effect on slot
tagging performance when training data is limited.
In these cases, the additional knowledge provided

Method Location-Services Time-Services

Baseline 20.09 84.91
MLoss 20.24 85.42
MEmbed 19.98 86.84†
MMember 20.88 86.65

Table 5: F1 score for models trained on LOCATION
and TIME and tested on SERVICES.

by the lexicons leads to large improvement in per-
formance. This is an encouraging result for SLU
systems that are being extended to new domains
as it is sometimes possible to acquire lexicons at a
low cost via sources such as DBPedia.

4.4.3 Cross Domain
In this experiment, we use the models trained for
the Location and Time domains to predict common
slots in the Services domain. Since some of the
labels in the Services domain do not exist in the
Location and Time models, we assign those labels
a tag of Other. The results of this experiment are
shown in Table 5. When training on Location and
predicting Services the highest precision and F1
scores are achieved by the MMember model; how-
ever, the improvement is not significant. When
training on Time and predicting Services the high-
est performance is achieved by theMEmbed model
and is statistically significant.

5 Discussion and Conclusion

Our results show that the MEmbed model, which
represents lexicon information with embeddings,
performs well across domains and experiments.
For instance, it achieves a significantly better per-
formance than the baselines in the Location and
Time domains when all available training data is
used. In that experiment, the Location and Time
domains had relatively large lexicon coverage.
The results suggest that lexicons can help improve
performance when large amounts of training data
are available and when lexicon coverage is high.
For the Services and Recipes domains, where lex-
icon coverage was low, the lexicon-based models
led to no significant improvement in performance.

When the training data was limited, the
MEmbed model achieved significantly better per-
formance than the baseline on 2 of the 4 domains.
In the other 2 domains, the MMember model per-
formed best. The experiments showed that, when



89

training data is small, the use of lexicons can lead
to large improvement in slot tagging performance.
For instance, Table 4 shows improvements in the
F1 score of about 12% for the Location domain
and of around 6% for the other domains.

These findings have strong implications for an
industry setting. The experiments clearly show
that lexicons can be very useful to improve slot
tagging when training data is limited, as is the case
when expanding to new domains. In these cases,
practitioners can benefit greatly by acquiring lex-
icons from online sources, such as knowledge
bases, or using existing lexicons that may have
been previously collected. Lexicons can also be
beneficial in cases where there are large amounts
of training data, but only if the lexicon coverage
is high. Our experiments show that using lexicons
as embedding features generally leads to good im-
provements in a variety of situations.

References
Fabrice Dugas and Eric Nichols. 2016. DeepNNNER:

Applying BLSTM-CNNs and Extended Lexicons to
Named Entity Recognition in Tweets. In Proceed-
ings of the 2nd Workshop on User Generated Text,
pages 138–144.

Young-Bum Kim, Karl Stratos, and Dongchan Kim.
2017. Domain Attention with an Ensemble of Ex-
perts. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics,
pages 643–653.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
Method for Stochastic Optimization. arXiv preprint
arXiv:1412.6980.

Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu.
2016. Leveraging sentence-level information with
encoder lstm for semantic slot filling. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2077–2083.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural Architectures for Named Entity Recognition.
arXiv preprint arXiv:1603.01360.

Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng
Gao, and Asli Celikyilmaz. 2017. Investigation of
Language Understanding Impact for Reinforcement
Learning Based Dialogue Systems. arXiv preprint
arXiv:1703.07055.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end Se-
quence Labeling via Bi-directional LSTM-CNNS-
CRF. arXiv preprint arXiv:1603.01354.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu,
et al. 2015. Using Recurrent Neural Networks
for Slot Filling in Spoken Language Understanding.
IEEE/ACM Transactions on Audio, Speech and Lan-
guage Processing (TASLP), 23(3):530–539.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155.

Yangyang Shi, Kaisheng Yao, Hu Chen, Dong Yu, Yi-
Cheng Pan, and Mei-Yuh Hwang. 2016. Recurrent
Support Vector Machines For Slot Tagging In Spo-
ken Language Understanding. In Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 393–399.

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147.

Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu,
Geoffrey Zweig, and Yangyang Shi. 2014. Spo-
ken Language Understanding Using Long Short-
term Memory Neural Networks. In Spoken Lan-
guage Technology Workshop (SLT), 2014 IEEE,
pages 189–194. IEEE.


