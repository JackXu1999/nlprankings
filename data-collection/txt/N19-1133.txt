



















































Star-Transformer


Proceedings of NAACL-HLT 2019, pages 1315–1325
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1315

Star-Transformer

Qipeng Guo†
qpguo16@fudan.edu.cn

Xipeng Qiu† ∗
xpqiu@fudan.edu.cn

Pengfei Liu†
pfliu14@fudan.edu.cn

Yunfan Shao†
yfshao15@fudan.edu.cn

Xiangyang Xue†
xyxue@fudan.edu.cn

†Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
†School of Computer Science, Fudan University

‡New York University

Zheng Zhang‡
zz@nyu.edu

Abstract
Although Transformer has achieved great suc-
cesses on many NLP tasks, its heavy struc-
ture with fully-connected attention connec-
tions leads to dependencies on large train-
ing data. In this paper, we present Star-
Transformer, a lightweight alternative by care-
ful sparsification. To reduce model complex-
ity, we replace the fully-connected structure
with a star-shaped topology, in which every
two non-adjacent nodes are connected through
a shared relay node. Thus, complexity is re-
duced from quadratic to linear, while preserv-
ing the capacity to capture both local compo-
sition and long-range dependency. The ex-
periments on four tasks (22 datasets) show
that Star-Transformer achieved significant im-
provements against the standard Transformer
for the modestly sized datasets.

1 Introduction

Recently, the fully-connected attention-based
models, like Transformer (Vaswani et al., 2017),
become popular in natural language processing
(NLP) applications, notably machine translation
(Vaswani et al., 2017) and language modeling
(Radford et al., 2018). Some recent work also
suggest that Transformer can be an alternative
to recurrent neural networks (RNNs) and convo-
lutional neural networks (CNNs) in many NLP
tasks, such as GPT (Radford et al., 2018), BERT
(Devlin et al., 2018), Transformer-XL (Dai et al.,
2019) and Universal Transformer (Dehghani et al.,
2018).

More specifically, there are two limitations of
the Transformer. First, the computation and mem-

∗Corresponding Author.
‡work done at NYU Shanghai, now with AWS Shanghai

AI Lab.

h1

h2

h3

h4

h5

h6

h7

h8

s

h1

h2

h3

h4

h5

h6

h7

h8

Figure 1: Left: Connections of one layer in Trans-
former, circle nodes indicate the hidden states of in-
put tokens. Right: Connections of one layer in Star-
Transformer, the square node is the virtual relay node.
Red edges and blue edges are ring and radical connec-
tions, respectively.

ory overhead of the Transformer are quadratic to
the sequence length. This is especially problem-
atic with long sentences. Transformer-XL (Dai
et al., 2019) provides a solution which achieves the
acceleration and performance improvement, but it
is specifically designed for the language modeling
task. Second, studies indicate that Transformer
would fail on many tasks if the training data is
limited, unless it is pre-trained on a large corpus.
(Radford et al., 2018; Devlin et al., 2018).

A key observation is that Transformer does not
exploit prior knowledge well. For example, the
local compositionality is already a robust induc-
tive bias for modeling the text sequence. How-
ever, the Transformer learns this bias from scratch,
along with non-local compositionality, thereby in-
creasing the learning cost. The key insight is then
whether leveraging strong prior knowledge can
help to “lighten up” the architecture.

To address the above limitation, we pro-
posed a new lightweight model named “Star-
Transformer”. The core idea is to sparsify the



1316

architecture by moving the fully-connected topol-
ogy into a star-shaped structure. Fig-1 gives an
overview. Star-Transformer has two kinds of con-
nections. The radical connections preserve the
non-local communication and remove the redun-
dancy in fully-connected network. The ring con-
nections embody the local-compositionality prior,
which has the same role as in CNNs/RNNs. The
direct outcome of our design is the improvement
of both efficiency and learning cost: the compu-
tation cost is reduced from quadratic to linear as a
function of input sequence length. An inherent ad-
vantage is that the ring connections can effectively
reduce the burden of the unbias learning of local
and non-local compositionality and improve the
generalization ability of the model. What remains
to be tested is whether one shared relay node is
capable of capturing the long-range dependencies.

We evaluate the Star-Transformer on three NLP
tasks including Text Classification, Natural Lan-
guage Inference, and Sequence Labelling. Experi-
mental results show that Star-Transformer outper-
forms the standard Transformer consistently and
has less computation complexity. An additional
analysis on a simulation task indicates that Star-
Transformer preserve the ability to handle with
long-range dependencies which is a crucial feature
of the standard Transformer.

In this paper, we claim three contributions as the
following and our code is available on Github 1:

• Compared to the standard Transformer, Star-
Transformer has a lightweight structure but
with an approximate ability to model the
long-range dependencies. It reduces the
number of connections from n2 to 2n, where
n is the sequence length.

• The Star-Transformer divides the labor of se-
mantic compositions between the radical and
the ring connections. The radical connections
focus on the non-local compositions and the
ring connections focus on the local composi-
tion. Therefore, Star-Transformer works for
modestly sized datasets and does not rely on
heavy pre-training.

• We design a simulation task “Masked Sum-
mation” to probe the ability dealing with
long-range dependencies. In this task,

1https://github.com/dmlc/dgl and https:
//github.com/fastnlp/fastNLP

we verify that both Transformer and Star-
Transformer are good at handling long-range
dependencies compared to the LSTM and
BiLSTM.

2 Related Work

Recently, neural networks have proved very suc-
cessful in learning text representation and have
achieved state-of-the-art results in many different
tasks.

Modelling Local Compositionality A popular
approach is to represent each word as a low-
dimensional vector and then learn the local seman-
tic composition functions over the given sentence
structures. For example, Kim (2014); Kalchbren-
ner et al. (2014) used CNNs to capture the seman-
tic representation of sentences, whereas Cho et al.
(2014) used RNNs.

These methods are biased for learning local
compositional functions and are hard to capture
the long-term dependencies in a text sequence. In
order to augment the ability to model the non-
local compositionality, a class of improved meth-
ods utilizes various self-attention mechanisms to
aggregate the weighted information of each word,
which can be used to get sentence-level represen-
tations for classification tasks (Yang et al., 2016;
Lin et al., 2017; Shen et al., 2018a). Another class
of improved methods augments neural networks
with a re-reading ability or global state while pro-
cessing each word (Cheng et al., 2016; Zhang
et al., 2018).

Modelling Non-Local Compositionality There
are two kinds of methods to model the non-local
semantic compositions in a text sequence directly.

One class of models incorporate syntactic tree
into the network structure for learning sentence
representations (Tai et al., 2015; Zhu et al., 2015).

Another type of models learns the dependencies
between words based entirely on self-attention
without any recurrent or convolutional layers, such
as Transformer (Vaswani et al., 2017), which
has achieved state-of-the-art results on a machine
translation task. The success of Transformer has
raised a large body of follow-up work. Therefore,
some Transformer variations are also proposed,
such as GPT (Radford et al., 2018), BERT (Devlin
et al., 2018), Transformer-XL (Dai et al., 2019) ,
Universal Transformer (Dehghani et al., 2018) and
CN3 (Liu et al., 2018a).

https://github.com/dmlc/dgl
https://github.com/fastnlp/fastNLP
https://github.com/fastnlp/fastNLP


1317

However, those Transformer-based methods
usually require a large training corpus. When ap-
plying them on modestly sized datasets, we need
the help of semi-supervised learning and unsu-
pervised pretraining techniques (Radford et al.,
2018).

Graph Neural Networks Star-Transformer is
also inspired by the recent graph networks (Gilmer
et al., 2017; Kipf and Welling, 2016; Battaglia
et al., 2018; Liu et al., 2018b), in which the in-
formation fusion progresses via message-passing
across the whole graph.

The graph structure of the Star-Transformer is
star-shaped by introducing a virtual relay node.
The radical and ring connections give a better bal-
ance between the local and non-local composition-
ality. Compared to the previous augmented mod-
els (Yang et al., 2016; Lin et al., 2017; Shen et al.,
2018a; Cheng et al., 2016; Zhang et al., 2018), the
implementation of Star-Transform is purely based
on the attention mechanism similar to the standard
Transformer, which is simpler and well suited for
parallel computation.

Due to its better parallel capacity and lower
complexity, the Star-Transformer is faster than
RNNs or Transformer, especially on modeling
long sequences.

3 Model

3.1 Architecture

The Star-Transformer consists of one relay node
and n satellite nodes. The state of i-th satellite
node represents the features of the i-th token in a
text sequence. The relay node acts as a virtual hub
to gather and scatter information from and to all
the satellite nodes.

Star-Transformer has a star-shaped structure,
with two kinds of connections in the: the radical
connections and the ring connections.

Radical Connections For a network of n satel-
lite nodes, there are n radical connections. Each
connection links a satellite node to the shared re-
lay node. With the radical connections, every two
non-adjacent satellite nodes are two-hop neigh-
bors and can receive non-local information with
a two-step update.

Ring Connections Since text input is a se-
quence, we bake such prior as an inductive bias.
Therefore, we connect the adjacent satellite nodes

to capture the relationship of local compositions.
The first and last nodes are also connected. Thus,
all these local connections constitute a ring-shaped
structure. Note that the ring connections allow
each satellite node to gather information from its
neighbors and plays the same role to CNNs or
bidirectional RNNs.

With the radical and ring connections, Star-
Transformer can capture both the non-local and lo-
cal compositions simultaneously. Different from
the standard Transformer, we make a division of
labor, where the radical connections capture non-
local compositions, whereas the ring connections
attend to local compositions.

3.2 Implementation

The implementation of the Star-Transformer is
very similar to the standard Transformer, in which
the information exchange is based on the attention
mechanism (Vaswani et al., 2017).

Multi-head Attention Just as in the standard
Transformer, we use the scaled dot-product atten-
tion (Vaswani et al., 2017). Given a sequence of
vectors H ∈ Rn×d, we can use a query vector
q ∈ R1×d to soft select the relevant information
with attention.

Att(q,K,V) = softmax(
qKT√

d
)V, (1)

where K = HWK ,V = HWV , and WK ,WV

are learnable parameters.
To gather more useful information from H, sim-

ilar to multi-channels in CNNs, we can use multi-
head attention with k heads.

MultiAtt(q,H) = (a1 ⊕ · · · ⊕ ak)WO, (2)

ai = Att(qW
Q
i ,HW

K
i ,HW

V
i ), i ∈ [1, k] (3)

where ⊕ denotes the concatenation operation, and
WQi ,W

K
i ,W

V
i ,W

O are learnable parameters.

Update Let st ∈ R1×d and Ht ∈ Rn×d de-
note the states for the relay node and all the n
satellite nodes at step t. When using the Star-
Transformer to encode a text sequence of length
n, we start from its embedding E = [e1; · · · ; en],
where ei ∈ R1×d is the embedding of the i-th to-
ken.

We initialize the state with H0 = E and s0 =
average(E).



1318

The update of the Star-Transformer at step t can
be divided into two alternative phases: (1) the up-
date of the satellite nodes and (2) the update of the
relay node.

At the first phase, the state of each satellite node
hi are updated from its adjacent nodes, includ-
ing the neighbor nodes hi−1,hi+1 in the sequence,
the relay node st, its previous state, and its corre-
sponding token embedding.

Cti = [h
t−1
i−1;h

t−1
i ;h

t−1
i+1; e

i; st−1], (4)

hti = MultiAtt(h
t−1
i ,C

t
i), (5)

where Cti denotes the context information for the
i-th satellite node. Thus, the update of each satel-
lite node is similar to the recurrent network, except
that the update fashion is based on attention mech-
anism. After the information exchange, a layer
normalization operation (Ba et al., 2016) is used.

hti = LayerNorm(ReLU(h
t
i)), i ∈ [1, n]. (6)

At the second phase, the relay node st summa-
rizes the information of all the satellite nodes and
its previous state.

st = MultiAtt(st−1, [st−1;Ht]), (7)

st = LayerNorm(ReLU(st)). (8)

By alternatively updating update the satellite
and relay nodes, the Star-Transformer finally cap-
tures all the local and non-local compositions for
an input text sequence.

Position Embeddings To incorporate the se-
quence information, we also add the learnable po-
sition embeddings, which are added with the token
embeddings at the first layer.

The overall update algorithm of the Star-
Transformer is shown in the Alg-1.

3.3 Output
After T rounds of update, the final states of HT

and sT can be used for various tasks such as se-
quence labeling and classification. For different
tasks, we feed them to different task-specific mod-
ules. For classification, we generate the fix-length
sentence-level vector representation by applying a
max-pooling across the final layer and mixing it
with sT , this vector is fed into a Multiple Layer
Perceptron (MLP) classifier. For the sequence la-
beling task, the HT provides features correspond-
ing to all the input tokens.

Algorithm 1 The Update of Star-Transformer
Input: Number of layers T , embedding of input
tokens e1, · · · , en

1: // Initialization
2: h01, · · · ,h0n ← e1, · · · , en
3: s0 ← average(e1, · · · , en)
4: for t from 1 to T do
5: // update the satellite nodes
6: for i from 1 to n do
7: Cti = [h

t−1
i−1;h

t−1
i ;h

t−1
i+1; e

i; st−1]

8: hti = MultiAtt(h
t−1
i ,C

t
i)

9: hti = LayerNorm(ReLU(h
t
i))

10: // update the relay node
11: st = MultiAtt(st−1, [st−1;Ht])
12: st = LayerNorm(ReLU(st))

4 Comparison to the standard
Transformer

Since our goal is making the Transformer
lightweight and easy to train with modestly sized
dataset, we have removed many connections com-
pared with the standard Transformer (see Fig-1).
If the sequence length is n and the dimension
of hidden states is d, the computation complex-
ity of one layer in the standard Transformer is
O(n2d). The Star-Transformer has two phases,
the update of ring connections costs O(5nd) (the
constant 5 comes from the size of context infor-
mation C), and the update of radical connections
costs O(nd), so the total cost of one layer in the
Star-Transformer is O(6nd).

In theory, Star-Transformer can cover all the
possible relationships in the standard Transformer.
For example, any relationship hi → hj in the
standard Transformer can be simulated by hi →
s→ hj . The experiment on the simulation task in
Sec-5.1 provides some evidence to show the vir-
tual node s could handle long-range dependencies.
Following this aspect, we can give a rough anal-
ysis of the path length of dependencies in these
models. As discussed in the Transformer paper
(Vaswani et al., 2017), the maximum dependency
path length of RNN and Transformer are O(n),
O(1), respectively. Star-Transformer can pass the
message from one node to another node via the
relay node so that the maximum dependency path
length is also O(1), with a constant two comparing
to Transformer.

Compare with the standard Transformer, all po-
sitions are processed in parallel, pair-wise connec-



1319

Dataset Train Dev. Test |V | H DIM #head head DIM

Masked Summation 10k 10k 10k - 100 10 10

SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50

MTL-16 †

(Liu et al.,
2017)

Apparel Baby Books Camera
DVD Electronics Health IMDB
Kitchen Magazines MR Music
Software Sports Toys Video

1400 200 400 8k∼28k 300 6 50

SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100

PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50

CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50

OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50

Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of
hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16†

consists of 16 datasets, each of them has 1400/200/400 samples in train/dev/test.

tions are replaced with a “gather and dispatch”
mechanism. As a result, we accelerate the Trans-
former 10 times on the simulation task and 4.5
times on real tasks. The model also preserves the
ability to handle long input sequences. Besides
the acceleration, the Star-Transformer achieves
significant improvement on some modestly sized
datasets.

5 Experiments

We evaluate Star-Transformer on one simulation
task to probe its behavior when challenged with
long-range dependency problem, and three real
tasks (Text Classification, Natural Language Infer-
ence, and Sequence Labelling). All experiments
are ran on a NVIDIA Titan X card. Datasets used
in this paper are listed in the Tab-1. We use the
Adam (Kingma and Ba, 2014) as our optimizer.
On the real task, we set the embedding size to
300 and initialized with GloVe (Pennington et al.,
2014). And the symbol “Ours + Char” means an
additional character-level pre-trained embedding
JMT (Hashimoto et al., 2017) is used. There-
fore, the total size of embedding should be 400
which as a result of the concatenation of GloVe
and JMT. We also fix the embedding layer of the
Star-Transformer in all experiments.

Since semi- or unsupervised model is also a fea-
sible solution to improve the model in a parallel
direction, such as the ELMo (Peters et al., 2018)
and BERT (Devlin et al., 2018), we exclude these
models in the comparison and focus on the rele-
vant architectures.

5.1 Masked Summation

In this section, we introduce a simulation task on
the synthetic data to probe the efficiency and non-
local/long-range dependencies of LSTM, Trans-
former, and the Star-Transformer. As mentioned
in (Vaswani et al., 2017), the maximum path
length of long-range dependencies of LSTM and
Transformer are O(n) and O(1), where n is the
sequence length. The maximum dependency path
length of Star-Transformer is O(1) with a constant
two via the relay node. To validate the ability
to deal with long-range dependencies, we design
a simulation task named “Masked Summation”.
The input of this task is a matrix X ∈ Rn×d, it has
n columns and each column has d elements. The
first dimension indicates the mask value Xi0 ∈
{0, 1}, 0 means the column is ignored in summa-
tion. The rest d − 1 elements are real numbers in
drawn uniformly from the range [0, 1). The tar-
get is a d− 1 dimensional vector which equals the
summation of all the columns with the mask value
1. There is an implicit variable k to control the
number of 1 in the input. Note that a simple base-
line is always guessing the value k/2.

The evaluation metric is the Mean Square
Error (MSE), and the generated dataset has
(10k/10k/10k) samples in (train/dev/test) sets. The
Fig-2 show a case of the masked summation task.

The mask summation task asks the model to
recognize the mask value and gather columns in
different positions. When the sequence length
n is significantly higher than the number of the
columns k, the model will face the long-range de-
pendencies problem. The Fig-3a shows the per-



1320

Input

1

0.3

0.4

0

0.5

0.7

0

0.1

0.2

1

0.5

0.9

0

0.4

0.2

0

0.6

0.8

0

0.1

0.3

1

0.1

0.6

0.9

1.9

Target

Figure 2: An example of the masked summation task, the input is a sequence of n vectors, each vector has d
dimension, and there are total k vectors which have the mask value equals 1. The target is the summation of such
masked vectors. In this figure, n = 8, k = 3, d = 3.

(a) MSE loss on the masked summation when
n = 200, k = 10, d = 10. The k/2 line means
the MSE loss when the model always guess the ex-
ception value k/2.

(b) Test Time, k = 10, d = 10

formance curves of models on various lengths.
Although the task is easy, the performance of
LSTM and BiLSTM dropped quickly when the se-
quence length increased. However, both Trans-
former and Star-Transformer performed consis-
tently on various lengths. The result indicates the
Star-Transformer preserves the ability to deal with
the non-local/long-range dependencies.

Besides the performance comparison, we also
study the speed with this simulation task since we
could ignore the affection of padding, masking,
and data processing. We also report the inference
time in the Fig-3b, which shows that Transformer
is faster than LSTM and BiLSTM a lot, and Star-
Transformer is faster than Transformer, especially
on the long sequence.

Model Acc

BiLSTM (Li et al., 2015) 49.8
Tree-LSTM (Tai et al., 2015) 51.0
CNN-Tensor (Lei et al., 2015) 51.2
Emb + self-att (Shen et al., 2018a) 48.9
BiLSTM + self-att (Yoon et al., 2018) 50.4
CNN + self-att (Yoon et al., 2018) 50.6
Dynamic self-att (Yoon et al., 2018) 50.6
DiSAN (Shen et al., 2018a) 51.7

Transformer 50.4
Ours 52.9

Table 2: Test Accuracy on SST dataset.

5.2 Text Classification

Text classification is a basic NLP task, and we se-
lect two datasets to observe the performance of
our model in different conditions, Stanford Senti-
ment Treebank(SST) dataset (Socher et al., 2013)
and MTL-16 (Liu et al., 2017) consists of 16 small
datasets on various domains. We truncate the se-
quence which its length higher than 256 to ensure
the standard Transformer can run on a single GPU
card.

For classification tasks, we use the state of the
relay node sT plus the feature of max pooling on
satellite nodes max(HT ) as the final representa-
tion and feed it into the softmax classifier. The
description of hyper-parameters is listed in Tab-1
and Appendix.

Results on SST and MTL-16 datasets are listed
in Tab-2,3, respectively. On the SST, the Star-
Transformer achieves 2.5 points improvement
against the standard Transformer and beat the most
models.

Also, on the MTL-16, the Star-Transformer
outperform the standard Transformer in all 16
datasets, the improvement of the average accu-
racy is 4.2. The Star-Transformer also gets bet-
ter results compared with existing works. As
we mentioned in the introduction, the standard
Transformer requires large training set to re-
veal its power. Our experiments show the Star-



1321

Dataset Acc (%) Test Time (ms) Len.Star-Transformer Transformer BiLSTM SLSTM Star-Transformer Transformer BiLSTM

Apparel 88.25 82.25 86.05 85.75 11 34 114 65
Baby 87.75 84.50 84.51 86.25 11 50 141 109
Books 87.00 81.50 82.12 83.44 11 55 135 131
Camera 92.25 86.00 87.05 90.02 11 52 125 116
DVD 86.00 77.75 83.71 85.52 11 58 143 139
Electronics 82.75 81.50 82.51 83.25 10 49 136 105
Health 87.50 83.50 85.52 86.50 10 45 131 85
IMDB 88.00 82.50 86.02 87.15 12 77 174 201
Kitchen 85.50 83.00 82.22 84.54 11 47 130 92
Magazines 92.75 89.50 92.52 93.75 11 51 136 115
MR 79.00 77.25 75.73 76.20 12 14 26 22
Music 83.50 79.00 78.74 82.04 11 53 137 118
Software 90.50 85.25 86.73 87.75 11 53 140 117
Sports 86.25 84.75 84.04 85.75 10 49 134 103
Toys 88.00 82.00 85.72 85.25 11 46 137 96
Video 86.75 84.25 84.73 86.75 11 56 146 131

Average 86.98 82.78 84.01 85.38 10.94 49.31 130.3 109.1

Table 3: Test Accuracy over MTL-16 datasets. “Test Time” means millisecond per batch on the test set (batch size
is 128). “Len.” means the average sequence length on the test set.

Transformer could work well on the small dataset
which only has 1400 training samples. Results
of the time-consuming show the Star-Transformer
could be 4.5 times fast than the standard Trans-
former on average.

5.3 Natural Language Inference

Natural Language Inference (NLI) asks the model
to identify the semantic relationship between a
premise sentence and a corresponding hypothesis
sentence. In this paper, we use the Stanford Nat-
ural Language Inference (SNLI) (Bowman et al.,
2015) for evaluation. Since we want to study how
the model encodes the sentence as a vector rep-
resentation, we set Star-Transformer as a sentence
vector-based model and compared it with sentence
vector-based models.

In this experiment, we follow the previous work
(Bowman et al., 2016) to use concat(r1, r2, ‖r1 −
r2‖, r1 − r2) as the classification feature. The
r1, r2 are representations of premise and hypoth-
esis sentence, it is calculated by sT + max(HT )
which is same with the classification task. See Ap-
pendix for the detail of hyper-parameters.

As shown in Tab-4, the Star-Transformer out-
performs most typical baselines (DiSAN, SPINN)
and achieves comparable results compared with
the state-of-the-art model. Notably, our model
beats standard Transformer by a large margin,
which is easy to overfit although we have made
a careful hyper-parameters’ searching for Trans-
former.

Model Acc

BiLSTM (Liu et al., 2016) 83.3
BiLSTM + self-att (Liu et al., 2016) 84.2
300D SPINN-PI (Bowman et al., 2016) 83.2
Tree-based CNN (Mou et al., 2016) 82.1
4096D BiLSTM-max (Conneau et al., 2017) 84.5
300D DiSAN (Shen et al., 2018a) 85.6
Residual encoders (Nie and Bansal, 2017) 86.0
Gumbel TreeLSTM (Choi et al., 2018) 86.0
Reinforced self-att (Shen et al., 2018b) 86.3
2400D Multiple DSA (Yoon et al., 2018) 87.4

Transformer 82.2
Star-Transformer 86.0

Table 4: Test Accuracy on SNLI dataset.

The SNLI dataset is not a small dataset in NLP
area, so improving the generalization ability of the
Transformer is a significant topic.

The best result in Tab-4 (Yoon et al., 2018)
using a large network and fine-tuned hyper-
parameters, they get the best result on SNLI but
an undistinguished result on SST, see Tab-2.

5.4 Sequence Labelling

To verify the ability of our model in sequence la-
beling, we choose two classic sequence labeling
tasks: Part-of-Speech (POS) tagging and Named
Entity Recognition (NER) task.

Three datasets are used as our benchmark: one
POS tagging dataset from Penn Treebank (PTB)
(Marcus et al., 1993), and two NER datasets
from CoNLL2003 (Sang and Meulder, 2003),
CoNLL2012 (Pradhan et al., 2012). We use the fi-



1322

Model Adv Tech
POS NER
PTB CoNLL2003 CoNLL2012

char CRF Acc F1 F1

(Ling et al., 2015) X X 97.78 - -
(Collobert et al., 2011) X X 97.29 89.59 -
(Huang et al., 2015) X X 97.55 90.10 -
(Chiu and Nichols, 2016a) X X - 90.69 86.35
(Ma and Hovy, 2016) X X 97.55 91.06 -
(Nguyen et al., 2016) X X - 91.2 -
(Chiu and Nichols, 2016b) X X - 91.62 86.28
(Zhang et al., 2018) X X 97.55 91.57 -
(Akhundov et al., 2018) X X 97.43 91.11 87.84

Transformer 96.31 86.48 83.57
Transformer + Char X 97.04 88.26 85.14
Star-Transformer 97.14 90.93 86.30
Star-Transformer + Char X 97.64 91.89 87.64
Star-Transformer + Char + CRF X X 97.68 91.98 87.88

Table 5: Results on sequence labeling tasks. We list the “Advanced Techniques” except widely-used pre-trained
embeddings (GloVe, Word2Vec, JMT) in columns. The “Char” indicates character-level features, it also includes
the Capitalization Features, Suffix Feature, Lexicon Features, etc. The “CRF” means an additional Conditional
Random Field (CRF) layer.

Model SNLI CoNLL03 MSAcc Acc MSE

Star-Transformer 86.0 90.93 0.0284
variant (a) -radical 84.0 89.35 0.1536
variant (b) -ring 77.6 79.36 0.0359

Table 6: Test Accuracy on SNLI dataset, CoNLL2003
NER task and the Masked Summation n = 200, k =
10, d = 10.

nal state of satellite nodes HT to classify the label
in each position. Since we believe that the com-
plex neural network could be an alternative of the
CRF, we also report the result without CRF layer.

As shown in Tab-5, Star-Transformer achieves
the state-of-the-art performance on sequence la-
beling tasks. The “Star-Transformer + Char”
has already beat most of the competitors. Star-
Transformer could achieve such results without
CRF, suggesting that the model has enough ca-
pability to capture the partial ability of the CRF.
The Star-Transformer also outperforms the stan-
dard Transformer on sequence labeling tasks with
a significant gap.

5.5 Ablation Study
In this section, we perform an ablation study to
test the effectiveness of the radical and ring con-
nections.

We test two variants of our models, the first
variants (a) remove the radical connections and
only keep the ring connections. Without the radi-
cal connections, the maximum path length of this

variant becomes O(n). The second variant (b) re-
moves the ring connections and remains the rad-
ical connections. Results in Tab-6 give some in-
sights, the variant (a) loses the ability to han-
dle long-range dependencies, so it performs worse
on both the simulation and real tasks. However,
the performance drops on SNLI and CoNLL03
is moderate since the remained ring connections
still capture the local features. The variant (b)
still works on the simulation task since the max-
imum path length stays unchanged. Without the
ring connections, it loses its performance heavily
on real tasks. Therefore, both the radical and ring
connections are necessary to our model.

6 Conclusion and Future Works

In this paper, we present Star-Transformer which
reduce the computation complexity of the standard
Transformer by carefully sparsifying the topology.
We compare the standard Transformer with other
models on one toy dataset and 21 real datasets
and find Star-Transformer outperforms the stan-
dard Transformer and achieves comparable results
with state-of-the-art models.

This work verifies the ability of Star-
Transformer by excluding the factor of unsu-
pervised pre-training. In the future work, we
will investigate the ability of Star-Transformer
by unsupervised pre-training on the large corpus.
Moreover, we also want to introduce more NLP
prior knowledge into the model.



1323

Acknowledgments

We would like to thank the anonymous re-
viewers for their valuable comments. The re-
search work is supported by Shanghai Munic-
ipal Science and Technology Commission (No.
17JC1404100 and 16JC1420401), National Key
Research and Development Program of China
(No. 2017YFB1002104), and National Natural
Science Foundation of China (No. 61672162 and
61751201).

References
Adnan Akhundov, Dietrich Trautmann, and Georg

Groh. 2018. Sequence labeling: A practical ap-
proach. CoRR, abs/1808.03926.

Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer normalization. CoRR, abs/1607.06450.

Peter W Battaglia, Jessica B Hamrick, Victor Bapst,
Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Ma-
teusz Malinowski, Andrea Tacchetti, David Raposo,
Adam Santoro, Ryan Faulkner, et al. 2018. Rela-
tional inductive biases, deep learning, and graph net-
works. arXiv preprint arXiv:1806.01261.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In EMNLP, pages 632–642. The Association
for Computational Linguistics.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In ACL (1).
The Association for Computer Linguistics.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
EMNLP, pages 551–561.

Jason Chiu and Eric Nichols. 2016a. Sequential label-
ing with bidirectional lstm-cnns. In Proc. Interna-
tional Conf. of Japanese Association for NLP, pages
937–940.

Jason P. C. Chiu and Eric Nichols. 2016b. Named en-
tity recognition with bidirectional lstm-cnns. TACL,
4:357–370.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of EMNLP.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.
Learning to compose task-specific tree structures. In
AAAI, pages 5094–5101. AAAI Press.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP, pages
670–680. Association for Computational Linguis-
tics.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G.
Carbonell, Quoc V. Le, and Ruslan Salakhutdi-
nov. 2019. Transformer-xl: Attentive language
models beyond a fixed-length context. CoRR,
abs/1901.02860.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Lukasz Kaiser. 2018. Univer-
sal transformers. CoRR, abs/1807.03819.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Justin Gilmer, Samuel S Schoenholz, Patrick F Riley,
Oriol Vinyals, and George E Dahl. 2017. Neural
message passing for quantum chemistry. In ICML,
pages 1263–1272.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple NLP
tasks. In EMNLP, pages 1923–1933. Association
for Computational Linguistics.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR, abs/1508.01991.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on EMNLP, pages 1746–1751.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Tao Lei, Regina Barzilay, and Tommi S. Jaakkola.
2015. Molding cnns for text: non-linear, non-
consecutive convolutions. In EMNLP, pages 1565–
1575. The Association for Computational Linguis-
tics.



1324

Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard H.
Hovy. 2015. When are tree structures necessary for
deep learning of representations? In EMNLP, pages
2304–2314. The Association for Computational Lin-
guistics.

Zhouhan Lin, Mo Feng, Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. 2017. A structured
self-attentive sentence embedding. arXiv preprint
arXiv:1703.03130.

Wang Ling, Chris Dyer, Alan W. Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luı́s Marujo,
and Tiago Luı́s. 2015. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In EMNLP, pages 1520–
1530. The Association for Computational Linguis-
tics.

Pengfei Liu, Shuaichen Chang, Xuanjing Huang, Jian
Tang, and Jackie Chi Kit Cheung. 2018a. Contextu-
alized non-local neural networks for sequence learn-
ing. arXiv preprint arXiv:1811.08600.

Pengfei Liu, Jie Fu, Yue Dong, Xipeng Qiu, and Jackie
Chi Kit Cheung. 2018b. Multi-task learning over
graph structures. arXiv preprint arXiv:1811.10211.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In ACL (1), pages 1–10. Association for Com-
putational Linguistics.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference us-
ing bidirectional LSTM model and inner-attention.
CoRR, abs/1605.09090.

Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
ACL (1). The Association for Computer Linguistics.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan,
and Zhi Jin. 2016. Natural language inference by
tree-based convolution and heuristic matching. In
ACL (2). The Association for Computer Linguistics.

Dat Ba Nguyen, Martin Theobald, and Gerhard
Weikum. 2016. J-NERD: joint named entity recog-
nition and disambiguation with rich linguistic fea-
tures. TACL, 4:215–229.

Yixin Nie and Mohit Bansal. 2017. Shortcut-stacked
sentence encoders for multi-domain inference. In
RepEval@EMNLP, pages 41–45. Association for
Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP, pages 1532–1543.
ACL.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In NAACL-HLT, pages 2227–2237. As-
sociation for Computational Linguistics.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In EMNLP-
CoNLL Shared Task, pages 1–40. ACL.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
CoNLL, pages 142–147. ACL.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018a. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI, pages 5446–5455.
AAAI Press.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen
Wang, and Chengqi Zhang. 2018b. Reinforced self-
attention network: a hybrid of hard and soft attention
for sequence modeling. In IJCAI, pages 4345–4352.
ijcai.org.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In EMNLP, pages 1631–1642. ACL.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In ACL (1), pages 1556–1566. The Asso-
ciation for Computer Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS, pages 6000–6010.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of NAACL,
pages 1480–1489.

Deunsol Yoon, Dongbok Lee, and SangKeun Lee.
2018. Dynamic self-attention : Computing atten-
tion over words dynamically for sentence embed-
ding. CoRR, abs/1808.07383.

Yue Zhang, Qi Liu, and Linfeng Song. 2018. Sentence-
state LSTM for text representation. In ACL (1),
pages 317–327. Association for Computational Lin-
guistics.



1325

Xiao-Dan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In ICML, pages 1604–1612.


