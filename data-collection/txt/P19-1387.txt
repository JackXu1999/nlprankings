



















































StRE: Self Attentive Edit Quality Prediction in Wikipedia


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3962–3972
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3962

StRE: Self Attentive Edit Quality Prediction in Wikipedia

Soumya Sarkar∗1, Bhanu Prakash Reddy*2, Sandipan Sikdar3 Animesh Mukherjee4

IIT Kharagpur, India1,2,4,RWTH Aachen, Germany3

soumya015@iitkgp.ac.in1,bhanu77prakash@gmail.com2

sandipan.sikdar@cssh.rwth-aachen.de3, animesh@cse.iitkgp.ac.in4

Abstract

Wikipedia can easily be justified as a behe-
moth, considering the sheer volume of con-
tent that is added or removed every minute to
its several projects. This creates an immense
scope, in the field of natural language pro-
cessing toward developing automated tools for
content moderation and review. In this paper
we propose Self Attentive Revision Encoder
(StRE) which leverages orthographic similar-
ity of lexical units toward predicting the qual-
ity of new edits. In contrast to existing propo-
sitions which primarily employ features like
page reputation, editor activity or rule based
heuristics, we utilize the textual content of
the edits which, we believe contains superior
signatures of their quality. More specifically,
we deploy deep encoders to generate repre-
sentations of the edits from its text content,
which we then leverage to infer quality. We
further contribute a novel dataset containing
∼ 21M revisions across 32K Wikipedia pages
and demonstrate that StRE outperforms exist-
ing methods by a significant margin – at least
17% and at most 103%. Our pre-trained model
achieves such result after retraining on a set as
small as 20% of the edits in a wikipage. This,
to the best of our knowledge, is also the first at-
tempt towards employing deep language mod-
els to the enormous domain of automated con-
tent moderation and review in Wikipedia.

1 Introduction

Wikipedia is the largest multilingual encyclopedia
known to mankind with the current English ver-
sion consisting of more than 5M articles on highly
diverse topics which are segregated into cate-
gories, constructed by a large editor base of more
than 32M editors (Hube and Fetahu, 2019). To en-
courage transparency and openness, Wikipedia al-
lows anyone to edit its pages albeit with certain

∗*Both authors contributed equally

guidelines for them1.
Problem: The inherent openness of Wikipedia has
also made it vulnerable to external agents who
intentionally attempt to divert the unbiased, ob-
jective discourse to a narrative which is aligned
with the interest of the malicious actors. Our pilot
study on manually annotated 100 Wikipedia pages
of four categories (25 pages each category) shows
us that at most 30% of the edits are reverted (See
Fig 1). Global average of number of reverted dam-
aging edits is∼ 9%2. This makes manual interven-
tion to detect these edits with potential inconsis-
tent content, infeasible. Wikipedia hence deploys
machine learning based classifiers (West et al.,
2010; Halfaker and Taraborelli, 2015) which pri-
marily leverage hand-crafted features from three
aspects of revision (i) basic text features like re-
peated characters, long words, capitalized words
etc. (ii) temporal features like inter arrival time be-
tween events of interest (iii) dictionary based fea-
tures like presence of any curse words or informal
words (e.g., ‘hello’, ‘yolo’). Other feature based
approaches include (Daxenberger and Gurevych,
2013; Bronner and Monz, 2012) which generally
follow a similar archetype.
Proposed model: In most of the cases, the ed-
its are reverted because they fail to abide by the
edit guidelines, like usage of inflammatory word-
ing, expressing opinion instead of fact among oth-
ers (see Fig 2). These flaws are fundamentally
related to the textual content rather than tempo-
ral patterns or editor behavior that have been de-
ployed in existing methods. Although dictionary
based approaches do look into text to a small ex-
tent (swear words, long words etc.), they account
for only a small subset of the edit patterns. We
further hypothesize that owing to the volume and

1en.wikipedia.org/Wikipedia:List of policies
2stats.wikimedia.org/EN/PlotsPngEditHistoryTop.htm



3963

Company Person Technology Concept
Category

0

1000

2000

3000

4000

5000

6000

7000
Ed

it
 C
ou

nt
Total Edits
Reverted Edits

Figure 1: Average number of edits and average number
of damaging edits, i.e., reverted edits for four different
categories of pages. A fraction (at most 30%) of user
generated edits are damaging edits.

variety of Wikipedia data, it is impossible to de-
velop a feature driven approach which can en-
compass the wide array of dependencies present
in text. In fact, we show that such approaches
are inefficacious in identifying most of the damag-
ing edits owing to these obvious limitations. We
hence propose Self Atttentive Revision Encoder
(StRE) which extracts rich feature representations
of an edit that can be further utilized to predict
whether the edit has damaging intent. In specific,
we use two stacked recurrent neural networks to
encode the semantic information from sequence
of characters and sequence of words which serve
a twofold advantage. While character embeddings
extract information from out of vocabulary tokens,
i.e., repeated characters, misspelled words, mali-
cious capitalized characters, unnecessary punctu-
ation etc., word embeddings extract meaningful
features from curse words, informal words, imper-
ative tone, facts without references etc. We further
employ attention mechanisms (Bahdanau et al.,
2014) to quantify the importance of a particular
character/word. Finally we leverage this learned
representation to classify an edit to be damaging
or valid. Note that StRE is reminiscent of struc-
tured self attentive model proposed in (Lin et al.,
2017) albeit used in a different setting.
Findings: To determine the effectiveness of our
model, we develop an enormous dataset consisting
of∼ 21M edits across 32K wikipages. We observe
that StRE outperforms the closest baseline by at
least 17% and at most 103% in terms of AUPRC.
Since it is impossible to develop an universal
model which performs equally well for all cat-
egories, we develop a transfer learning (Howard
and Ruder, 2018) set up which allows us to de-
ploy our model to newer categories without train-
ing from scratch. This further allows us to employ

Facebook and Myspace are social 
networking sites

Facebook is lot better than Myspace 
is every aspects.

Dr Eric Schmidt, FORMER CEO OF NOVELL, 
took over Google’s CEO when co-founder 

Larry Page stepped down

[[Eric Schmidt]], took over Google’s CEO 
when co-founder Larry Page stepped down

Google is trying to become the jack of 
all trades 

Google is foraying into other 
businesses, which other companies 

have recently dominated. 

Facebook formerly known as thefacebook is 
a social networking service for high school, 

college, university communities

Facebook formerly known as thefacebook 
is RETARTED as well as a social networking 
service for high school, college, university 

communities

Figure 2: Examples of edits in Facebook and Google
Wikipedia page. The blue bubbles are the original
sentences. The orange bubbles indicate damaging ed-
its while the green bubbles indicate ‘good faith’ edits.
Good faith edits are unbiased formal English sentence
while damaging edits often correspond to incoherent
use of language, abusive language, imperative mood,
opinionated sentences etc.

our model to pages with lower number of edits.
Contributions: Our primary contributions in this
paper are summarized below -
(i) We propose a deep neural network based model
to predict edit quality in Wikipedia which utilizes
language modeling techniques, to encode seman-
tic information in natural language.
(ii) We develop a novel dataset consisting of
∼ 21M unique edits extracted from ∼ 32K
Wikipedia pages. In fact our proposed model
outperforms all the existing methods in detecting
damaging edits on this dataset.
(iii) We further develop a transfer learning set up
which allows us to deploy our model to newer cat-
egories without the need for training from scratch.

Code and sample data related to the pa-
per are available at https://github.com/
bhanu77prakash/StRE.

2 The Model

In this section we give a detailed description of
our model. We consider an edit to be a pair of
sentences with one representing the original (Por)
while the other representing the edited version
(Ped). The input to the model is the concatenation
of Por and Ped (say P = {Por||Ped}) separated by a
delimiter (‘||’). We assume P consists of wi words
and ci characters. Essentially we consider two lev-
els of encoding - (i) character level to extract pat-
terns like repeated characters, misspelled words,
unnecessary punctuation etc. and (ii) word level
to identify curse words, imperative tone, opinion-
ated phrases etc. In the following we present how

https://github.com/bhanu77prakash/StRE
https://github.com/bhanu77prakash/StRE


3964

we generate a representation of the edit and utilize
it to detect malicious edits. The overall architec-
ture of StRE is presented in Fig 3.

2.1 Word encoder
Given an edit P with wi, i ∈ [0,L] words, we first
embed the words through a pre-trained embedding
matrix We such that xi = Wewi. This sequence of
embedded words is then provided as an input to
a bidirectional LSTM (Hochreiter and Schmidhu-
ber, 1997) which provides representations of the
words by summarizing information from both di-
rections.

xi =Wewi, i ∈ [0,L] (1)
−→v i =

−−−→
LST M(xi), i ∈ [0,L] (2)

←−v i =
←−−−
LST M(xi), i ∈ [L,0] (3)

We obtain the representation for each word by
concatenating the forward and the backward hid-
den states vi = [−→v i,←−v i]. Since not all words con-
tribute equally to the context, we deploy an atten-
tion mechanism to quantify the importance of each
word. The final representation is then a weighted
aggregation of the words.

ui = σ(Wwvi +bw) (4)

βi =
exp(uTi uw)

∑Ti=0 exp(uTi uw)
(5)

Rw = ∑
i

βivi (6)

To calculate attention weights (αi) for a hidden
state hi, it is first fed through a single layer per-
ceptron and then a softmax function is used to cal-
culate the weights. Note that we use a word con-
text vector uw which is randomly initialized and
is learnt during the training process. The use of
context vector as a higher level representation of a
fixed query has been argued in (Sukhbaatar et al.,
2015; Kumar et al., 2016). Note that the attention
score calculation is reminiscent of the one pro-
posed in (Yang et al., 2016).

2.2 Character encoder
The character encoder module is similar to the
word encoder module with minor differences. For-
mally we consider P ({Por||Ped}) as a sequence of
T characters ci, i ∈ [0,T ]. Instead of using pre-
trained embeddings as in case of word encoder, we
define an embedding module, parameters of which
is also learned during training which is basically

an MLP. Each embedded character is then passed
through a bidirectional LSTM to obtain the hidden
states for each character. Formally, we have

yi = σ(Wcci +bc), i ∈ [0,T ] (7)
−→
h i =

−−−→
LST M(yi), i ∈ [0,T ] (8)

←−
h i =

←−−−
LST M(yi), i ∈ [T,0] (9)

We next calculate the attention scores for each
hidden state hi as

zi = σ(Wchi +bc) (10)

αi =
exp(zTi uc)

∑Ti=0 exp(zTi uc)
(11)

Rc = ∑
i

αihi (12)

Note that uc is a character context vector which
is learned during training.

2.3 Edit classification
The edit vector Ep (for an edit P) is the concate-
nation of character and word level encodings Ep =
[Rc,Rw] which we then use to classify whether an
edit is valid or damaging. Typically, we perform

p = so f tmax(WpEp +bp)

Finally we use binary cross entropy between pre-
dicted and the true labels as our training loss.

2.4 Transfer learning setup
Note that it is not feasible to train the model from
scratch every time a new page in an existing or a
new category is introduced. Hence we propose a
transfer learning setup whereby, for a new page,
we use the pre-trained model and only update the
weights of the dense layers during training. The
advantages are twofold - (i) the model needs only
a limited amount of training data and hence can
easily be trained on the new pages and (ii) we ben-
efit significantly on training time.

3 Dataset

Wikipedia provides access to all Wikimedia
project pages in the form of xml dumps, which
is periodically updated3. We collect data from
dumps made available by English Wikipedia
project on June 2017 which contains information
about 5.5M pages.

3https://dumps.wikimedia.org/enwiki/20181120



3965

C1 C2 CT

h1

h1

h2

h2

hT

hT

Concatenation &
softmaxuc

v1

v1

v2

v2

vL

vL

w1 w2 wL

𝛃
𝟐

Uw

𝜶
𝟐

Figure 3: Overall architecture of StRE. The charac-
ter encoding and the word encoding components of the
model are shown in the left and right respectively. This
is followed by the attention layer followed by concate-
nation and softmax.

We extract a subset of pages related to the Com-
puter Science category in Wikipedia. Utilizing the
category hierarchy4 (Auer et al., 2007) (typically
a directed graph containing parent and child cate-
gories), we extract all articles under the Computer
Science category up to a depth of four levels which
accounts for 48.5K Wikipedia pages across 1.5K
categories5. We filter out pages with at least 100
edits which leaves us with 32K pages. For each
page in our dataset we performed pairwise differ-
ence operation6 between its current and previous
versions to obtain a set of pairs with each consist-
ing of a sentence and its subsequent modified ver-
sion.
Edit quality: In order to train our model to iden-
tify quality edits from damaging edits we need a
deterministic score for an edit. Our quality score
is based on the intuition that if changes intro-
duced by the edit are preserved, it signals that the
edit was beneficial, whereas if the changes are re-
verted, the edit likely had a negative effect. This
idea is adapted from previous work of Adler et al.
(2011).

Consider a particular article and denote it by vk
its k-th revision (i.e., the state of the article after
the k-th edit). Let d(u,v) be the Levenshtein dis-
tance between two revisions. We define the quality
of edit k from the perspective of the article’s state
after `≥ 1 subsequent edits as

qk|` =
d(vk−1,vk+`)−d(vk,vk+`)

d(vk−1,vk)
.

4Dbpedia.org
5Typical categories include ‘Computational Science’,

‘Artificial Intelligence’ etc.
6https://docs.python.org/2/library/difflib.html

Resources Count

Pages 32394
Total edits 21,848960
Positive edits 15,791575
Negative edits 6,057385

Table 1: Summary of the dataset.

Intuitively, the quantity qk|` captures the propor-
tion of work done on edit k that remains in revision
k+` and it varies between qk|` ∈ [−1,1], when the
value falls outside this range, it is capped within
these two values. We compute the mean quality
of the edit by averaging over multiple future revi-
sions as follows

qk =
1
L

L

∑̀
=1

qk|`

where L is the minimum among the number of
subsequent revisions of the article. We have taken
L = 10, which is consistent with the previous work
of Yardım et al. (2018).
Edit label: For each pair of edits we compute the
edit quality score. If quality score is ≥ 0 we label
an edit to be −1, i.e., done in good faith. However
all edits with quality score < 0 are labeled 1, i.e.,
damaging edits. We further check that bad qual-
ity edits are indeed damaging edits by calculating
what fraction of low score edits are reverted and
what fraction of high score edits are not reverted.
This result is illustrated in Figure 4. Information
whether an edit is reverted or not can be calculated
by mining Wikipedia’s revert graph following the
same technique illustrated by (Kittur et al., 2007).
The results clearly show that a large proportion of
bad quality edits are indeed reverted by the edi-
tors and similarly a large fraction of good quality
edits are not reverted. Though bad quality edits
are often reverted, all reverted edits are not bad.
Malicious agents often engage in interleaving re-
verts, i.e., edit wars (Kiesel et al., 2017) as well
as pesudo reverts. Hence we use quality metric to
label damaging edits which is well accepted in the
literature (Adler et al., 2008). We provide a sum-
mary of the data in Table 1. Our final data can be
represented by a triplet < si,s f , l > where si is the
initial sentence, s f is the modified sentence and l
indicates the edit label.

4 Experiments

In this section we demonstrate the effectiveness of
our model compared to other existing techniques.



3966

−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
Quality Score

0.002

0.019

0.187
Fr
ac

ti
on

 o
f R

ev
er
te
d 
Ed

it
s

−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
Quality Score

0.007

0.068

0.676

Fr
ac
ti
on

 o
f N

on
 R
ev

er
te
d 
Ed

it
s

Figure 4: Distribution of the quality score for the revert edits (left) and non-reverted edits(right). The y-axis is in
log scale. The plot shows that a large proportion of low quality edits are reverted and a large proportion of high
quality edits are not reverted; hence this observation acts as a validation for our quality score metric.

Typically, we consider two sets of experiments -
(i) category level and (ii) page level. In category
level experiments (see section 4.3) we first form a
random sample of data points belonging to pages
in a fixed category. Our objective is to first train
on edits related to a fixed page category and test
on new edits belonging to pages of the same cat-
egory. We further show through rigorous experi-
ments that existing approaches of transfer learning
and fine tuning (Howard and Ruder, 2018) can be
applied to increase the efficacy of our approach. In
page level experiments in section 4.4, we abandon
the category constraint (as in case of category level
experiments) and train (test) on edits irrespective
of the category of the page which it belongs to and
demonstrate that our model is equally effective.

4.1 Baseline approaches
We use two variants of our proposed model – word
embedding with attention (Word+Att), character
embedding with attention (Char+Att) as two base-
lines to compare to our model. We also compare
existing feature based and event based approaches
for edit quality prediction. We give a brief descrip-
tion of the other state-of-the-art baselines in the
subsequent subsections.

4.1.1 ORES
The Objective Revision Evaluation Service
(ORES) (Wikimedia, 2019) is a web service
developed by Wikimedia foundation that provides
a machine learning-based scoring system for edits.
More specifically, given an edit, ORES infers
whether an edit causes damage using linguistic
features and edit based features (e.g., size of the
revision etc.)

4.1.2 ORES++
In order to make it more competitive, we further
augment ORES by adding linguistic quality indi-

cators as additional features obtained from the Em-
path tool Fast et al. (2016). This tool scores edits
on 16 lexical dimensions such as ‘ugliness’, ‘irri-
tability’, ‘violence’ etc. We also use the count of
POS tags following Manning et al. (2014) as well
as the count of mispelled words as features using
aspell dictionary Atkinson (2006).

4.1.3 Interrank
Interrank (Yardım et al., 2018) is a recent quality-
prediction method which does not use any explicit
content-based features but rather predicts quality
of an edit by learning editor competence and page
reputation from prior edit actions. The perfor-
mance of Interrank has been revealed to be very
close to ORES.

4.2 Model configuration
We use 300 dimensional pre-trained word Glove
vector (Pennington et al., 2014) and 300 dimen-
sional ASCII character embedding (Woolf, 2017).
We also use 64 dimensional hidden layer in our
model, followed by attention layer and three stacks
of dense layer. Our context vector in the atten-
tion layer is of 64 dimensions and dense layers
are 256, 64 and 16 dimensions. We further utilize
dropout probability of 0.5 in the dense layers. We
also employ binary cross entropy as loss function
and Adam (Kingma and Ba, 2014) optimizer with
learning rate 0.01 and weight decay of 0.0001 to
train our model. The batch size is set to 250.

4.3 Category level experiments
In this set of experiments we essentially train and
test on pages in the same category.

4.3.1 Page specific model
As a first step towards determining the potential of
our model, we train our model on a set of edits of a
particular page and predict on the rest. To this aim



3967

we manually annotate top 100 pages in terms of to-
tal number of edits, into four categories, i.e., com-
pany, concept, technology, and person. Such gran-
ular level category annotation is not available from
Wikipedia hierarchy which directs us towards an-
notation. In each category we tabulate the count
of positive and negative datapoints in Table 2).
For each page we randomly select 80% edits for
training, 10% edits for validation and 10% edits
as held out set. We train our model on 80% and
tune it on the validation set. We finally test on
the held out set. The same procedure is followed
for Word+Att, Char+Att, ORES++. For all these
models the AUPRC (mean,std) across all pages are
presented in Table 3. Since ORES is already a pre-
trained model we test on the combined held out set
of the pages. Note that Interrank is not designed
for page level training and further requires large
training data. Hence, for Interrank we train on the
combined training set of the pages and test on the
combined held out set. Results obtained on the
held out set are reported in Table 3. Our exper-
iments clearly show that StRE outperforms base-
lines by a significant margin (at least 10%). We
also see that individual components of our model,
i.e., Char+Att and Word+Att do not perform as
well as StRE which further validates our architec-
ture. Moreover, Interrank performs poorly despite
combined dataset which shows that language mod-
elling is essential in edit quality prediction.

Edits Company Concept Technology Person
+ve examples 813400 227308 294125 79035
-ve Examples 649078 124323 169091 28505

Total examples 1462478 351631 463216 107540

Table 2: Total number of data points along with posi-
tive and negative samples for the top five pages in terms
of edit count in each category.

4.3.2 New page: same category
We now explore a more challenging setup for our
model whereby instead of training and testing on
edits of a specific annotated category, we train on
edits of pages of a particular category but test on
a previously unseen (during training) page of the
same category. Specifically, for a given category,
we train our model on 90% of pages and test our
models on unseen page edits in the same category
from our annotated dataset. The obtained results
are tabulated in Table 4(a). Results show that such
an approach is indeed fruitful and can be applied
on pages which has very few edits utilizing intra-

category pages with large edit counts.
Transfer learning results: Our results can be fur-
ther improved by applying ideas of transfer learn-
ing. For each new page, we can initialize our
model by pre-trained weights learned from train-
ing on other intra-category pages. We can then
train the dense layer with only 20% of new data-
points randomly selected from the new page and
test on the remaining 80%. This approach is
adapted from the state-of-the-art transfer learning
approaches (Howard and Ruder, 2018; Dehghani
et al., 2017) where it has been shown to work on
diverse NLP tasks. Such an approach achieves at
least 3% and at most 27% improvement over prior
results.

4.3.3 New page: different category
We now probe into how our model performs when
tested on a page belonging to a previously unseen
category. As a proof of concept, we train on all
pages belonging to three categories (inter-category
training) and test on a new page from the fourth
category. We perform this experiment considering
each of the four categories as unknown one by one
in turn. The obtained results are presented in Ta-
ble 4(b). Clearly, the results are inferior compared
to intra-category training which corroborates with
our argument that different category of pages have
unique patterns of edits.
Transfer learning results: However, we alleviate
the above problem by utilizing transfer learning
approaches. In specific, we initialize our model
with weights pre-trained on inter-category pages
and train only the final dense layer on 20% of
the new edits from the fourth category. Results
point that we can obtain significant improvements,
i.e., at least 10% and at most 28%. This is very
a promising direction to pursue further investiga-
tions, since it is very likely that abundant edits may
be present in distant categories while very limited
edits may manifest in a niche category that has low
visibility.

4.3.4 Multi category training
Finally, we proceed toward a category agnostic
training paradigm. Essentially, we hold out 10%
pages of the annotated set for each category. We
train on all remaining pages irrespective of the cat-
egory information and test on the held out pages
from each category. We report the results in Ta-
ble 4(c). Since our model learns from edits in all
category of pages, we are able to obtain better re-



3968

Company Concept Technology Person
Models AUPRC AUPRC AUPRC AUPRC
ORES 0.72 0.76 0.71 0.63

ORES++ 0.84±0.03 0.85±0.03 0.87±0.02 0.85±0.03
Interrank 0.35 0.47 0.42 0.38
Word+Att 0.63±0.02 0.74±0.03 0.72±0.01 0.78±0.02
Char+Att 0.91±0.01 0.84±0.02 0.83±0.02 0.81±0.02

StRE 0.95±0.02 0.89±0.01 0.91±0.01 0.87±0.02

Table 3: AUPRC scores, with the best results in bold and gray background on the annotated dataset.

Category Testing withoutRetraining
Testing with

20% Retraining
Person 0.81 0.85

Concept 0.77 0.91
Company 0.76 0.88

Technology 0.68 0.88

(a) Intra category AUPRC.

Category Testing withoutRetraining
Testing with

20% Retraining
Person 0.67 0.82

Concept 0.63 0.81
Company 0.71 0.82

Technology 0.72 0.89

(b) Inter category AUPRC.

Category Testing withoutRetraining
Testing with

20% Retraining
Person 0.71 0.83

Concept 0.85 0.90
Company 0.74 0.86

Technology 0.77 0.84

(c) Category agnostic AUPRC.

Table 4: Results for intra-category, inter-category and category agnostic predictions without and with transfer
learning. The transfer learning approach is always beneficial.

sults from inter category setup. We further employ
transfer learning (as in previous sections) on the
new page which improves the results significantly
(at least 6% and at most 16%).

To summarize the results in this section, we ob-
serve that testing on a previously unseen category
leads to under-performance. However, retraining
the dense layers with a few training examples dras-
tically improves the performance of our model.

4.4 Page level experiments

We now consider an experimental setup agnostic
of any category. In this setting, to train our model
we form a set of edits which comprises 20% of our
total edits in the dataset. This edits are taken from
the pages which have largest edit count. Quan-
titatively, we impose a cap on the total number
of edits to be 20% of the entire edit count. Sub-
sequently, we start pooling training data from the
largest page, followed by the second largest page
and so on until our budget is fulfilled. The whole
data so accumulated is divided into 80% training,
10% validation and 10% test sets. Results on this
10% held out data are reported in Table 5 as train-
ing AUPRC. We compare our model against other
text based and event based quality predictor base-
lines. Since ORES is an already pre-trained web
based service, we obtained AUPRC on the 10%
held out set. In case of Interrrank, 90% of the
data is used for training and 10% is used as held
out set (as reported in the paper (Yardım et al.,
2018)). Results show that our model performs sig-
nificantly better than the baselines (by 24% in case
of ORES and by 131% in case of Interrank).
Transfer learning results: For each of the re-

Model TrainingAUPRC
Testing
AUPRC

ORES 0.77 0.75
Interrank 0.41 0.42
Word+Att 0.64 0.77±0.1
Char+Att 0.92 0.83±0.09

StRE 0.95 0.88±0.09

Table 5: Comaprison between StRE and baselines on
complete dataset.

maining pages in our data we first utilize our pre-
trained model from the last step. However, we
train the dense layers with randomly selected 20%
datapoints from the page to be tested. The remain-
ing data is used for testing. We follow this pro-
cedure for all remaining pages and calculate the
mean test AUPRC along with standard deviation
which we report in Table 5. In case of ORES we
evaluate on the 80% data. In case of Interrrank,
we merge all remaining data into a single dataset
and use 90% of the data for training and 10% for
test. We show that transfer learning approach can
be useful in this setting and we obtain 17% im-
provement compared to ORES and 103% improve-
ment compared to Interrank.

5 Discussion

Model retraining: We demonstrate in our exper-
iments that a fraction of edits from unseen pages
results in the improvement over pretrained models.
We further investigate the model performance if
we increase the volume of the retraining data (re-
sults shown for the intra-category setup, all other
setups show exactly similar trend). We vary the
unseen data used for fine tuning the model from



3969

Original version Revised version

Google Maps offers detailed streetmaps
and route planning information.

Google Maps offers detailed streetmaps
and route planning information in
United States and Canada.

Proponents argued that privacy complaints
are baseless.

Proponents of trusted computing argue that
privacy complaints have been addressed in
the existing specifications - possibly as a
result of criticism of early versions of the
specifications.

Table 6: Anecdotal examples of edits in Google Maps and Internet Privacy wikipage. Here the general model
fails to identify negative examples while retraining the dense layer learns better representations and identifies the
negative examples correctly. Page specific tokens are colored in blue.

5% to 50% and show that growth in AUPRC sta-
bilizes (see Fig 5) which validates our proposal to
utilize a smaller fraction.

10 20 30 40 50
Training Size

0.855

0.860

0.865

0.870

0.875

0.880

AU
PR

C

Figure 5: AUPRC using transfer learning in intra-
category setup with gradual increase in retraining per-
centages. Similar trends are obtained with the other
setups.

Anecdotal examples: In order to obtain a deeper
understanding of the results, we explore few ex-
amples where the general model fails while re-
training the dense layers leads to correct classi-
fication. In Table 6 we present two such exam-
ples. Note that our general model (without re-
training the dense layers) wrongly classifies them
as damaging edits while retraining leads to cor-
rect classification. We believe that retraining the
dense layers leads to obtaining superior represen-
tation of edits, whereby, page specific words like
‘streetmaps’, ‘route planning’ in Google Maps or
‘privacy complaints’, ‘trusted computing’ in Inter-
net Privacy are more pronounced.
Timing benefits: Another potential benefit is the
amount of time saved per epoch as we are only
back propagating through the dense layers. To
quantify the benefit in terms of time, we select
a random sample of pages and train one version
of our model end-to-end across all layers and an-

other version only up to the dense layer. For our
model, the average time taken per epoch achieves
∼ 5x improvement over the traditional approach.
The performance in the two cases are almost same.
In fact, for some cases the traditional end-to-end
training leads to inferior results as LSTM layers
fail to learn the best weights with so few exam-
ples.

6 Related work

Edit quality prediction in Wikipedia has mostly
been pursued in the lines of vandalism detection.
Kumar et al. (2015) developed a system which
utilized novel patterns embedded in user editing
history, to predict potential vandalism. Similar
feature based approach has also been applied in
both standard (Green and Spezzano, 2017) and sis-
ter projects of Wikipedia such as wikidata (Hein-
dorf et al., 2016; Sarabadani et al., 2017). Yuan
et al. (2017) propose to use a modified version
of LSTM to solve this problem, hence avoiding
feature engineering. A complementary direction
of investigation has been undertaken by (Daxen-
berger and Gurevych, 2013; Bronner and Monz,
2012) who bring forth a feature driven approach,
to distinguish spam edit from a quality edit. A fea-
ture learning based approach has been proposed
by Agrawal and Dealfaro (2016); Yardım et al.
(2018) which observes all the past edits of a user
to predict the quality of the future edits. Tempo-
ral traces generated by edit activity has also been
shown (Tabibian et al., 2017) to be a key indica-
tor toward estimating reliability of edits and page
reputation. One of the major problems in these ap-
proaches is that they require user level history in-
formation which is difficult to obtain because the
same user may edit different Wikipedia pages of
diverse categories and it will be time consuming
to comb through millions of pages for each user.



3970

There has also been no work to understand the
possibility of predicting edit quality based on ed-
its in pages in a common category. However, there
has been no work to leverage advanced machinery
developed in language modeling toward predicting
edit quality.
Transfer learning: Several works (Long et al.,
2015b; Sharif Razavian et al., 2014) in computer
vision (CV) focus on transfer learning approach
as deep learning architectures in CV tend to learn
generic to specific tasks from first to last layer.
More recently (Long et al., 2015a; Donahue et al.,
2014) have shown that fine tuning the last or sev-
eral of the last layers and keeping the rest of the
layers frozen can have similar benefits. In natu-
ral language processing (NLP) literature, (Severyn
and Moschitti, 2015) showed that unsupervised
language model based embedding can be tuned us-
ing a distant large corpus and then further applied
on a specialized task such as sentiment classifica-
tion. This approach of weak supervision followed
by full supervision to learn a confident model (De-
hghani et al., 2017; Howard and Ruder, 2018; Jan
et al., 2016) has been shown to reduce training
times in several NLP tasks. In this paper we apply
a similar framework for the first time in predicting
the edit quality in Wikipedia pages in one category
by initializing parameters from a trained model of
a different category. This is very effective in cases
where the former category page has limited num-
ber of data points.

7 Conclusion

In this paper we proposed a novel deep learning
based model StRE for quality prediction of edits
in Wikipedia. Our model combines word level as
well as character level signals in the orthography
of Wikipedia edits for extracting a rich representa-
tion of an edit. We validate our model on a novel
data set comprising millions of edits and show ef-
ficacy of our approach compared to approaches
that utilize handcrafted features and event based
modelling. One of the remarkable findings of this
study is that only 20% of training data is able to
boost the performance of the model by a signifi-
cant margin.

To the best of our knowledge, this is the first
work which attempts to predict edit quality of a
page by learning signals from similar category
pages as well as cross category pages. We fur-
ther show applications of recent advances in trans-

fer learning in this problem and obtain significant
improvements in accuracy without compromising
training times. We believe this work will usher
considerable interest in understanding linguistic
patterns in Wikipedia edit history and application
of deep models in this domain.

References
B Thomas Adler, Luca De Alfaro, Santiago M Mola-

Velasco, Paolo Rosso, and Andrew G West. 2011.
Wikipedia vandalism detection: Combining natural
language, metadata, and reputation features. In In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, pages 277–288.
Springer.

B Thomas Adler, Luca De Alfaro, Ian Pye, and Vish-
wanath Raman. 2008. Measuring author contribu-
tions to the wikipedia. In Proceedings of the 4th
International Symposium on Wikis, page 15. ACM.

Rakshit Agrawal and Luca Dealfaro. 2016. Predict-
ing the quality of user contributions via lstms. In
Proceedings of the 12th International Symposium on
Open Collaboration, page 19. ACM.

Kevin Atkinson. 2006. Gnu aspell 0.60. 4.

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722–735. Springer.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Amit Bronner and Christof Monz. 2012. User edits
classification using document revision histories. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 356–366. Association for Computa-
tional Linguistics.

Johannes Daxenberger and Iryna Gurevych. 2013. Au-
tomatically classifying edit categories in wikipedia
revisions. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 578–589.

Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe,
and Jaap Kamps. 2017. Learning to learn from
weak supervision by full supervision. arXiv preprint
arXiv:1711.11383.

Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. Decaf: A deep convolutional activation fea-
ture for generic visual recognition. In International
conference on machine learning, pages 647–655.



3971

Ethan Fast, Binbin Chen, and Michael S Bernstein.
2016. Empath: Understanding topic signals in large-
scale text. In Proceedings of the 2016 CHI Con-
ference on Human Factors in Computing Systems,
pages 4647–4657. ACM.

Thomas Green and Francesca Spezzano. 2017. Spam
users identification in wikipedia via editing behav-
ior. In Eleventh International AAAI Conference on
Web and Social Media.

Aaron Halfaker and Dario Taraborelli. 2015. Artifi-
cial intelligence service ores gives wikipedians x-ray
specs to see through bad edits.

Stefan Heindorf, Martin Potthast, Benno Stein, and
Gregor Engels. 2016. Vandalism detection in wiki-
data. In Proceedings of the 25th ACM International
on Conference on Information and Knowledge Man-
agement, pages 327–336. ACM.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 328–339.

Christoph Hube and Besnik Fetahu. 2019. Neural
based statement classification for biased language.
In Proceedings of the Twelfth ACM International
Conference on Web Search and Data Mining, pages
195–203. ACM.

Deriu Jan et al. 2016. Sentiment classification using
an ensemble of convolutional neural networks with
distant supervision. Proceedings of SemEval (2016),
pages 1124–1128.

Johannes Kiesel, Martin Potthast, Matthias Hagen, and
Benno Stein. 2017. Spatio-temporal analysis of re-
verted wikipedia edits. In Eleventh International
AAAI Conference on Web and Social Media.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Aniket Kittur, Bongwon Suh, Bryan A Pendleton, and
Ed H Chi. 2007. He says, she says: conflict and
coordination in wikipedia. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 453–462. ACM.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In International Con-
ference on Machine Learning, pages 1378–1387.

Srijan Kumar, Francesca Spezzano, and VS Subrah-
manian. 2015. Vews: A wikipedia vandal early
warning system. In Proceedings of the 21th ACM
SIGKDD international conference on knowledge
discovery and data mining, pages 607–616. ACM.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Jonathan Long, Evan Shelhamer, and Trevor Darrell.
2015a. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition,
pages 3431–3440.

Mingsheng Long, Yue Cao, Jianmin Wang, and
Michael I Jordan. 2015b. Learning transferable fea-
tures with deep adaptation networks. arXiv preprint
arXiv:1502.02791.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd annual
meeting of the association for computational lin-
guistics: system demonstrations, pages 55–60.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Amir Sarabadani, Aaron Halfaker, and Dario Tara-
borelli. 2017. Building automated vandalism detec-
tion tools for wikidata. In Proceedings of the 26th
International Conference on World Wide Web Com-
panion, pages 1647–1654. International World Wide
Web Conferences Steering Committee.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Unitn: Training deep convolutional neural network
for twitter sentiment classification. In Proceedings
of the 9th international workshop on semantic eval-
uation (SemEval 2015), pages 464–469.

Ali Sharif Razavian, Hossein Azizpour, Josephine Sul-
livan, and Stefan Carlsson. 2014. Cnn features off-
the-shelf: an astounding baseline for recognition. In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition workshops, pages 806–
813.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

Behzad Tabibian, Isabel Valera, Mehrdad Farajtabar,
Le Song, Bernhard Schölkopf, and Manuel Gomez-
Rodriguez. 2017. Distilling information reliability
and source trustworthiness from digital traces. In
Proceedings of the 26th International Conference
on World Wide Web, pages 847–855. International
World Wide Web Conferences Steering Committee.

https://blog.wikimedia.org/2015/11/30/artificial-intelligence-x-ray-specs/
https://blog.wikimedia.org/2015/11/30/artificial-intelligence-x-ray-specs/
https://blog.wikimedia.org/2015/11/30/artificial-intelligence-x-ray-specs/


3972

Andrew G West, Sampath Kannan, and Insup Lee.
2010. Stiki: an anti-vandalism tool for wikipedia
using spatio-temporal analysis of revision metadata.
In Proceedings of the 6th International Symposium
on Wikis and Open Collaboration, page 32. ACM.

Wikimedia. 2019. Objective revision evaluation ser-
vice ORES.

Max Woolf. 2017. Pretrained word embeddings file
into a character embeddings.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Ali Batuhan Yardım, Victor Kristof, Lucas Maystre,
and Matthias Grossglauser. 2018. Can who-
edits-what predict edit survival? arXiv preprint
arXiv:1801.04159.

Shuhan Yuan, Panpan Zheng, Xintao Wu, and Yang Xi-
ang. 2017. Wikipedia vandal early detection: from
user behavior to user embedding. In Joint European
Conference on Machine Learning and Knowledge
Discovery in Databases, pages 832–846. Springer.

https://www.mediawiki.org/wiki/ORES
https://www.mediawiki.org/wiki/ORES
https://github.com/minimaxir/char-embeddings
https://github.com/minimaxir/char-embeddings

