



















































The Phenogrammar of Coordination


Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 28–36,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

The Phenogrammar of Coordination

Chris Worth
Department of Linguistics
The Ohio State University
worth@ling.osu.edu

Abstract

Linear Categorial Grammar (LinCG) is a
sign-based, Curryesque, relational, logi-
cal categorial grammar (CG) whose cen-
tral architecture is based on linear logic.
Curryesque grammars separate the ab-
stract combinatorics (tectogrammar) of
linguistic expressions from their concrete,
audible representations (phenogrammar).
Most of these grammars encode linear or-
der in string-based lambda terms, in which
there is no obvious way to distinguish right
from left. Without some notion of direc-
tionality, grammars are unable to differen-
tiate, say, subject and object for purposes
of building functorial coordinate struc-
tures. We introduce the notion of a phe-
nominator as a way to encode the term
structure of a functor separately from its
“string support”. This technology is then
employed to analyze a range of coordina-
tion phenomena typically left unaddressed
by Linear Logic-based Curryesque frame-
works.

1 Overview

Flexibility to the notion of constituency in con-
junction with introduction (and composition) rules
has allowed categorial grammars to successfully
address an entire host of coordination phenomena
in a transparent and compositional manner. While
“Curryesque” CGs as a rule do not suffer from
some of the other difficulties that plague Lambek
CGs, many are notably deficient in one area: co-
ordination. Lest we throw the baby out with the
bathwater, this is an issue that needs to be ad-
dressed. We take the following to be an exemplary
subset of the relevant data, and adopt a fragment
methodology to show how it may be analyzed.

(1) Tyrion and Joffrey drank.

(2) Joffrey whined and sniveled.
(3) Tyrion slapped and Tywin chastised Jof-

frey.

The first example is a straightforward instance
of noun phrase coordination. The second and third
are both instances of what has become known in
the categorial grammar literature as “functor co-
ordination”, that is, the coordination of linguistic
material that is in some way incomplete. The third
is particularly noteworthy as being an example of
a “right node raising” construction, whereby the
argument Joffrey serves as the object to both of
the higher NP-Verb complexes. We will show that
all three examples can be given an uncomplicated
account in the Curryesque framework of Linear
Categorial Grammar (LinCG), and that (2) and
(3) have more in common than not.

Section 1 provides an overview of the data and
and central issues surrounding an analysis of co-
ordination in Curryesque grammars. Section 2 in-
troduces the reader to the framework of LinCG,
and presents the technical innovations at the heart
of this paper. Section 3 gives lexical entries and
derivations for the examples in section 1, and sec-
tion 4 discusses our results and suggests some di-
rections for research in the near future, with refer-
ences following.

1.1 Curryesque grammars and Linear
Categorial Grammar

We take as our starting point the Curryesque (af-
ter Curry (1961)) tradition of categorial grammars,
making particular reference to those originating
with Oehrle (1994) and continuing with Abstract
Categorial Grammar (ACG) of de Groote (2001),
Muskens (2010)’s Lambda Grammar (λG), Kub-
ota and Levine’s Hybrid Type-Logical Catego-
rial Grammar (Kubota and Levine, 2012) and
to a lesser extent the Grammatical Framework
of Ranta (2004), and others. These dialects

28



of categorial grammar make a distinction be-
tween Tectogrammar, or “abstract syntax”, and
Phenogrammar, or “concrete syntax”. Tec-
togrammar is primarily concerned with the struc-
tural properties of grammar, among them co-
occurrence, case, agreement, tense, and so forth.
Phenogrammar is concerned with computing a
pre-phonological representation of what will even-
tually be produced by the speaker, and encom-
passes word order, morphology, prosody, and the
like.

Linear Categorial Grammar (LinCG) is a sign-
based, Curryesque, relational, logical categorial
grammar whose central architecture is based on
linear logic. Abbreviatory overlap has been
a regrettably persistent problem, and LinCG is
the same in essence as the framework varyingly
called Linear Grammar (LG) and Pheno-Tecto-
Differentiated Categorial Grammar (PTDCG), and
developed in Smith (2010), Mihalicek (2012),
Martin (2013), Pollard and Smith (2012), and Pol-
lard (2013). In LinCG, the syntax-phonology and
syntax-semantics interfaces amount to noting that
the logics for the phenogrammar, the tectogram-
mar, and the semantics operate in parallel. This
stands in contrast to ‘syntactocentric’ theories of
grammar, where syntax is taken to be the fun-
damental domain within which expressions com-
bine, and then phonology and semantics are ‘read
off’ of the syntactic representation. LinCG is con-
ceptually different in that it has relational, rather
than functional, interfaces between the three com-
ponents of the grammar. Since we do not interpret
syntactic types into phenogrammatical or semantic
types, this allows us a great deal of freedom within
each logic, although in practice we maintain a
fairly tight connection between all three compo-
nents. Grammar rules take the form of derivational
rules which generate triples called signs, and they
bind together the three logics so that they operate
concurrently. While the invocation of a grammar
rule might simply be, say, point-wise application,
the ramifications for the three systems can in prin-
ciple be different; one can imagine expressions
which exhibit type asymmetry in various ways.

By way of example, one might think of ‘focus’
as an operation which has reflexes in all three as-
pects of the grammar: it applies pitch accents to
the target string(s) in the phenogrammar (the dif-
ference between accented and unaccented words
being reflected in the phenotype), it creates ‘low-

ering’ operators in the tectogrammar (that is, ex-
pressions which scope within a continuation), and
it ‘focuses’ a particular meaningful unit in the se-
mantics. A focused expression might share its tec-
totype ((NP ( S) ( S) with, say, a quantified
noun phrase, but the two could have different phe-
notypes, reflecting the accentuation or lack thereof
by placing the resulting expression in the domain
of prosodic boundary phenomena or not. Never-
theless, the system is constrained by the fact that
the tectogrammar is based on linear logic, so if we
take some care when writing grammar rules, we
should still find resource sensitivity to be at the
heart of the framework.

1.2 Why coordination is difficult for
Curryesque grammars

Most Curryesque CGs encode linear order in
lambda terms, and there is no obvious way to dis-
tinguish ‘right’ from ‘left’ by examining the types
(be they linear or intuitionistic).1 This is not a
problem when we are coordinating strings directly,
as de Groote and Maarek (2007) show, but an anal-
ysis of the more difficult case of functor coordi-
nation remains elusive.2 Without some notion of
directionality, grammars are unable to distinguish
between, say, subject and object. This would seem
to predict, for example, that λs. s · SLAPPED ·
JOFFREY and λs. TYRION · SLAPPED · s would
have the same syntactic category (NP ( S in the
tectogrammar, and St → St in the phenogram-
mar), and would thus be compatible under coor-
dination, but this is generally not the case. What
we need is a way to examine the structure of a
lambda term independently of the specific string
constants that comprise it. To put it another way,
in order to coordinate functors, we need to be able
to distinguish between what Oehrle (1995) calls
their string support, that is, the string constants
which make up the body of a particular functional
term, and the linearization structure such functors
impose on their arguments.

2 Linear Categorial Grammar (LinCG)

Curryesque grammars separate the notion of linear
order from the abstract combinatorics of linguis-

1A noteworthy exception is Ranta’s Grammatical Frame-
work (GF), explored in, e.g. Ranta (2004) and Ranta
(2009). GF also makes distinctions between tectogrammar
and phenogrammar, though it has a somewhat different con-
ception of each.

2A problem explicitly recognized by Kubota (2010) in
section 3.2.1.

29



tic expressions, and as such base their tectogram-
mars around logics other than bilinear logic; the
Grammatical Framework is based on Martin-Löf
type theory, and LinCG and its cousins ACG and
λG use linear logic. Linear logic is generally de-
scribed as being “resource-sensitive”, owing to the
lack of the structural rules of weakening and con-
traction. Resource sensitivity is an attractive no-
tion, theoretically, since it allows us to describe
processes of resource production, consumption,
and combination in a manner which is agnostic
about precisely how resources are combined. Cer-
tain problems which have been historically tricky
for Lambek categorial grammars (medial extrac-
tion, quantifier scope, etc.) are easily handled by
LinCG.

Since a full introduction to the framework is re-
grettably impossible given current constraints, we
refer the interested reader to the references in sec-
tion 1.1, which contain a more in-depth discus-
sion of the potential richness of the architecture
of LinCG. We do not wish to say anything new
about the semantics or the tectogrammar of coor-
dination in the current discussion, so we will ex-
pend our time fleshing out the phenogrammatical
component of the framework, and it is to this topic
that we now turn.

2.1 LinCG Phenogrammar
LinCG grammar rules take the form of tripartite
inference rules, indicating what operations take
place pointwise within each component of the
signs in question. There are two main gram-
mar rules, called application (App) for combining
signs, and abstraction (Abs) for creating the po-
tential for combination through hypothetical rea-
soning. Aside from the lexical entries given as
axioms of the theory, it is also possible to obtain
typed variables using the rule of axiom (Ax), and
we make use of this rule in the analysis of right
node raising found in section 3.4. While the tec-
togrammar of LinCG is based on a fragment of
linear logic, the phenogrammatical and semantic
components are based on higher order logic. Since
we are concerned only with the phenogrammatical
component here, we have chosen to simplify the
exposition by presenting only the phenogrammat-
ical part of the rules of application and abstraction:

Ax
f : A ` f : A

Γ ` f : A→ B ∆ ` a : A
App

Γ,∆ ` (f a) : B

Γ, x : A ` b : B
AbsΓ ` λx : A. b : A→ B

We additionally stipulate the following familiar
axioms governing the conversion and reduction of
lambda terms:3

` λx : A. b = λy : A. [y/x]b (α-conversion)
` (λx. b a) = [a/x]b (β-reduction)
As is common to any number of Curryesque

frameworks, we encode the phenogrammatical
parts of LinCG signs with typed lambda terms
consisting of strings, and functions over strings.4

We axiomatize our theory of strings in the familiar
way:

` � : St
` · : St→ St→ St
` ∀stu : St. s · (t · u) = (s · t) · u
` ∀s : St. � · s = s = s · �

The first axiom asserts that the empty string � is
a string. The second axiom asserts that concate-
nation, written ·, is a (curried) binary function on
strings. The third axiom represents the fact that
concatenation is associative, and the fourth, that
the empty string is a two-sided identity for con-
catenation. Because of the associativity of con-
catenation, we will drop parentheses as a matter
of convention.

The phenogrammar of a typical LinCG sign will
resemble the following (with one complication to
be added shortly):

` λs. s · SNIVELED : St→ St
Since we treat St as the only base type, we will

generally omit typing judgments in lambda terms
when no confusion will result. Furthermore, we
use SMALL CAPS to indicate that a particular con-
stant is a string. So, the preceding lexical entry
provides us with a function from some string s, to
strings, which concatenates the string SNIVELED
to the right of s.

2.1.1 Phenominators
The center of our analysis of coordination is
the notion of a phenominator (short for pheno-
combinator), a particular variety of typed lambda
term. Intuitively, phenominators serve the same
purpose for LinCG that bilinear (slash) types do
for Lambek categorial grammars. Specifically,

3The exact status of the rule of η-conversion with respect
to this framework is currently unclear, and since we do not
make use of it, we omit its discussion here.

4although other structures have been proposed, e.g. the
node sets found in Muskens (2001).

30



they encode the linearization structure of a func-
tor, that is, where arguments may eventually oc-
cur with respect to its string support. To put it an-
other way, a phenominator describes the structure
a functor “projects”, in terms of linear order.

From a technical standpoint, we would like to
define a phenominator as a closed monoidal linear
lambda term, i.e. a term containing no constants
other than concatenation and the empty string.
The idea is that phenominators are the terms of
the higher order theory of monoids, and they in
some ways describe the abstract “shape” of pos-
sible string functions. For those accustomed to
thinking of “syntax” as being word order, then
phenominators can be thought of as a kind of syn-
tactic combinator. In practice, we will make use
only of what we call the unary phenominators, the
types of which we will refer to using the sort Φ
(with ϕ used by custom as a metavariable over
unary phenominators, i.e. terms whose type is in
Φ). These are not unary in the strict sense, but they
will have as their centerpiece one particular string
variable, which will be bound with the highest
scope. We will generally abbreviate phenomina-
tors by the construction with which they are most
commonly associated: VP for verb phrases and
intransitive verbs, TV for transitive verbs, DTV
for ditransitive verbs, QNP for quantified noun
phrases, and RNR for right node raising construc-
tions. Here are examples of some of the most com-
mon phenominators we will make use of and the
abbreviations we customarily use for them:

Phenominator Abbreviation
λs.s (omitted)
λvs.s · v VP
λvst.t · v · s TV
λvstu.u · v · s · t DTV
λvP.(P v) QNP
λvs.v · s RNR

As indicated previously, the first argument of a
phenominator always corresponds to what we re-
fer to (after Oehrle (1995)) as the string support
of a particular term. With the first argument dis-
pensed with, we have chosen the argument order
of the phenominators out of general concern for
what we perceive to be fairly uncontroversial cat-
egorial analyses of English grammatical phenom-
ena. That is, transitive verbs take their object ar-
guments first, and then their subject arguments, di-
transitives take their first and second object argu-
ments, followed by their subject argument, etc. As

long as the arguments in question are immediately
adjacent to the string support at each successive
application, it is possible to permute them to some
extent without losing the general thrust of the anal-
ysis. For example, the choice to have transitive
verbs take their object arguments first is insignifi-
cant.5 Since strings are implicitly under the image
of the identity phenominator λs.s, we will consis-
tently omit this subscript.

We will be able to define a function we call
say, so that it will have the following property:

` ∀s : St.∀ϕ : Φ.say (ϕ s) = s
That is, say is a left inverse for unary phenomi-
nators.

The function say is defined recursively via cer-
tain objects we call vacuities. The idea of a vacu-
ity is that it be in some way an “empty argument”
to which a functional term may apply. If we are
dealing with functions taking string arguments, it
seems obvious that the vacuity on strings should
be the empty string �. If we are dealing with
second-order functions taking St→ St arguments,
for example, quantified noun phrases like every-
one, then the vacuity on St → St should be the
identity function on strings, λs.s. Higher vacuities
than these become more complicated, and defin-
ing all of the higher-order vacuities is not entirely
straightforward, as certain types are not guaran-
teed to have a unique vacuity. Fortunately, we can
do it for any higher-function taking as an argument
another function under the image of a phenomina-
tor – then the vacuity on such a function is just the
phenominator applied to the empty string.6 The
central idea is easily understood when one asks
what, say, a vacuous transitive verb sounds like.
The answer seems to be: by itself, nothing, but
it imposes a certain order on its arguments. One
practical application of this clause is in analyzing
so-called “argument cluster coordination”, where
this definition will ensure that the argument cluster
gets linearized in the correct manner. This analy-
sis is regrettably just outside the scope of the cur-
rent inquiry, though the notion of the phenomina-

5Since we believe it is possible to embed Lambek catego-
rial grammars in LinCG, this fact reflects that the calculus we
are dealing with is similar to the associative Lambek Calcu-
lus.

6A reviewer suggests that this concept may be related to
the “context passing representation” of Hughes (1995), and
the association of a nil term with its continuation with re-
spect to contexts is assuredly evocative of the association of
the vacuity on a phenominator-indexed type with the contin-
uation of � with respect to a phenominator.

31



tor can be profitably employed to provide exactly
such an analysis by adopting and reinterpreting a
categorial account along the lines of the one given
in Dowty (1988).

We formally define vacuities as follows:

vacSt→St =def λs.s
vacτϕ =def (ϕ �)

The reader should note that as a special case of the
second clause, we have

vacSt = vacStλs.s = (λs.s �) = �

This in turn enables us to define say:
saySt =def λs.s
sayτ1→τ2 =def λk : τ1 → τ2. sayτ2(k vacτ1)
say(τ1→τ2)ϕ =def sayτ1→τ2

For an expedient example, we can apply say to
our putative lexical entry from earlier, and verify
that it will reduce to the string SNIVELED as de-
sired:

saySt→St λs. s · SNIVELED
= λk : St→ St.
(saySt(k vacSt)) λs. s · SNIVELED
= saySt (λs. s · SNIVELED vacSt)
= saySt (λs. s · SNIVELED �)
= saySt � · SNIVELED
= saySt SNIVELED
= λs.s SNIVELED
= SNIVELED

2.1.2 Subtyping by unary phenominators
In order to augment our type theory with the
relevant subtypes, we turn to Lambek and Scott
(1986), who hold that one way to do subtyping is
by defining predicates that amount to the charac-
teristic function of the particular subtype in ques-
tion, and then ensuring that these predicates meet
certain axioms embedding the subtype into the su-
pertype. We will be able to write such predicates
using phenominators. A unary phenominator is
one which has under its image a function whose
string support is a single contiguous string. With
this idea in place, we are able to assign subtypes
to functional types in the following way.

For τ a (functional) type, we write τϕ (with ϕ a
phenominator) as shorthand for τϕ′ , where:

ϕ′ = λf : τ . ∃s : St.f = (ϕ s)
Then ϕ′ constitutes a subtyping predicate in the
manner of Lambek and Scott (1986). For example,
let τ = St→ St and ϕ = λvs.s·v. Let us consider
the following (putative) lexical entry (pheno only):

` λs′. s′ · SNIVELED : (St→ St)VP

Then our typing is justified along the following
lines:
τϕ ::= (St→ St)VP

::= (St→ St)λvs.s·v
::= (St→ St)λf :St→St. ∃t:St.f=(λvs.s·v t)

So applying the subtyping predicate to the term in
question, we have

(λf : St→ St. ∃t : St.
f = (λvs.s · v t) λs′. s′ · SNIVELED)
= ∃t : St.λs′. s′ · SNIVELED = (λvs.s · v t)
= ∃t : St.λs′. s′ · SNIVELED = λs. s · t
= ∃t : St.λs. s · SNIVELED = λs. s · t

which is true with t = SNIVELED, and the term is
shown to be well-typed.

3 Analysis

The basic strategy underlying our analysis of coor-
dination is that in order to coordinate two linguis-
tic signs, we need to track two things: their lin-
earization structure, and their string support. If we
have access to the linearization structure of each
conjunct, then we can check to see that it is the
same, and the signs are compatible for coordina-
tion. Furthermore, we will be able to maintain this
structure independent of the actual string support
of the individual signs.

Phenominators simultaneously allow us to
check the linearization structure of coordination
candidates and to reconstruct the relevant lin-
earization functions after coordination has taken
place. The function say addresses the second
point. For a given sign, we can apply say to it
in order to retrieve its string support. Then, we
will be able to directly coordinate the resulting
strings by concatenating them with a conjunction
in between. Finally, we can apply the phenomi-
nator to the resulting string and retrieve the new
linearization function, containing the entire coor-
dinate structure as its string support.

3.1 Lexical entries
In LinCG, lexical entries constitute the (nonlogi-
cal) axioms of the proof theory. First we consider
the simplest elements of our fragment, the phenos
for the proper names Joffrey, Tyrion, and Tywin:

(4) a. ` JOFFREY : St
b. ` TYRION : St
c. ` TYWIN : St

Next, we consider the intransitive verbs drank,
sniveled and whined. :

32



(5) a. ` λs. s · DRANK : (St→ St)VP
b. ` λs. s · SNIVELED : (St→ St)VP
c. ` λs. s · WHINED : (St→ St)VP

Each of these is a function from strings to strings,
seeking to linearize its ‘subject’ string argument to
the left of the verb. They are under the image of
the “verb phrase” phenominator λvs.s · v.

The transitive verbs chastised and slapped seek
to linearize their first string argument to the right,
resulting in a function under the image of the VP
phenominator, and their second argument to the
left, resulting in a string.

(6) a. ` λst. t · CHASTISED · s
: (St→ St→ St)TV

b. ` λst. t · SLAPPED · s
: (St→ St→ St)TV

Technically, this type could be written (St →
(St → St)VP)TV, but for the purposes of coordina-
tion, the present is sufficient. Each of these entries
is under the image of the “transitive verb” phe-
nominator λvst.t · v · s.

Finally, we come to the lexical entry schema for
and:

(7) ` λc1 : τϕ. λc2 : τϕ.
ϕ ((sayτϕ c2) · AND · (sayτϕ c1))
: τϕ → τϕ → τϕ

We note first that it takes two arguments of iden-
tical types τ , and furthermore that these must be
under the image of the same phenominator ϕ. It
then returns an expression of the same subtype.7

This mechanism bears more detailed examination.
First, each conjunct is subjected to the function
say, which, given its type, will return the string
support of the conjunct. Then, the resulting strings
are concatenated to either side of the string AND.
Finally, the phenominator of each argument is ap-
plied to the resulting string, creating a function
identical to the linearization functions of each of
the conjuncts, except with the coordinated string
in the relevant position.

3.2 String coordination
String coordination is direct and straightforward.
Since string-typed terms are under the image of

7Since ϕ occurs within both the body of the term and the
subtyping predicate, we note that this effectively takes us into
the realm of dependent types. Making the type theory of the
phenogrammar precise is an ongoing area of research, and we
are aware that constraining the type system is of paramount
importance for computational tractability.

the identity phenominator, and since saySt is also
defined to be the identity on strings, the lexical en-
try we obtain for and simply concatenates each
argument string to either side of the string AND.
We give the full term reduction here, although this
version of and can be shown to be equal to the fol-
lowing:
` λc1c2 : St. c2 · AND · c1 : St→ St→ St

Since our terms at times become rather large,
we will adopt a convention where proof trees are
given with numerical indexes instead of sequents,
with the corresponding sequents following below
(at times on multiple lines). We will from time
to time elide multiple steps of reduction, noting in
passing the relevant definitions to consider when
reconstructing the proof.

6

1 2
3 4

5
7

1. ` λc1 : St. λc2 : St.
λs.s ((saySt c2) · AND · (saySt c1))
: St→ St→ St

2. ` JOFFREY : St
3. ` λc2 : St.
λs.s ((saySt c2) · AND · (saySt JOFFREY))
= λc2 : St.
λs.s ((saySt c2) · AND · (λs.s JOFFREY))
= λc2 : St. λs.s ((saySt c2)·AND·JOFFREY)
: St→ St

4. ` TYRION : St
5. ` λs.s ((saySt TYRION) · AND · JOFFREY) :

St
= λs.s ((λs.s TYRION)·AND·JOFFREY) : St
= λs.s (TYRION · AND · JOFFREY) : St
= TYRION · AND · JOFFREY : St

6. ` λs. s · DRANK : (St→ St)VP
7. ` TYRION · AND · JOFFREY · DRANK : St

3.3 Functor coordination
Here, in order to understand the term appearing
in each conjunct, it is helpful to notice that the
following equality holds (with f a function from
strings to strings, under the image of the VP phe-
nominator):

f : (St→ St)VP ` say(St→St)VP f
= saySt→St f
= saySt (f vacSt)
= saySt (f �)
= λs.s (f �)
= (f �) : St

33



This says that to coordinate VPs, we will first need
to reduce them to their string support by feeding
their linearization functions the empty string. For
the sake of brevity, this term reduction will be
elided from steps 5 and 8 in the derivations be-
low. Steps 2 and 6 constitute the hypothesizing
and subsequent withdrawal of an ‘object’ string
argument t′, as do steps 10 and 14 (s′). Format-
ting restrictions prohibit rule-labeling on the proof
trees, so we note that these are each instances of
the rules of axiom (Ax) and abstraction (Abs), re-
spectively.

1 2
3 4

5 6
7

1. ` λc1 : (St→ St)VP. λc2 : (St→ St)VP.
λvs.s · v
((say(St→St)VP c2) · AND · (say(St→St)VP c1))
: (St→ St)VP → (St→ St)VP → (St→ St)VP

2. ` λs. s · SNIVELED : (St→ St)VP
3. ` λc2 : (St→ St)VP. λvs.s · v

((say(St→St)VP c2) · AND
· (say(St→St)VP λs. s · SNIVELED))
...
= λc2 : (St→ St)VP. λvs.s · v
((say(St→St)VP c2) · AND · SNIVELED)
: (St→ St)VP → (St→ St)VP

4. ` λs. s · WHINED : (St→ St)VP
5. ` λvs.s · v ((say(St→St)VP λs. s · WHINED)
· AND · SNIVELED)
...
= (λvs.s · v WHINED · AND · SNIVELED)
= λs. s · WHINED · AND · SNIVELED
: (St→ St)VP

6. ` JOFFREY : St
7. ` JOFFREY · WHINED · AND · SNIVELED : St

3.4 Right node raising
In the end, ‘right node raising’ constructions prove
only to be a special case of functor coordination.
The key here is the licensing of the ‘rightward-
looking’ functors, which are under the image of
the phenominator λvs.v · s. As was the case with
the ‘leftward-looking’ functor coordination exam-
ple in section 3.3, this analysis is essentially the
same as the well-known Lambek categorial gram-
mar analysis originating in Steedman (1985) and
continuing in Dowty (1988) and Morrill (1994).

The difference is that we encode directionality in
the phenominator, rather than in the type. Since
our system does not include function composition
as a rule, but as a theorem, we will need to make
use of hypothetical reasoning in order to permute
the order of the string arguments in order to con-
struct expressions with the correct structure.8

As was the case with the functor coordina-
tion example in section 3.3, applying say to the
conjuncts passes them the empty string, reducing
them to their string support, as shown here:

f : (St→ St)RNR ` say(St→St)RNR f
= saySt→St f
= saySt (f vacSt)
= saySt (f �)
= λs.s (f �)
= (f �) : St

As before, this reduction is elided in the proof
given below, occurring in steps 8 and 15.

7

1 2
3 4

5
6

8

9 10
11 12

13
14

15 16
17

1. ` λst. t · CHASTISED · s : (St→ St→ St)TV
2. t′ : St ` t′ : St
3. t′ : St ` λt. t · CHASTISED · t′ : (St→ St)VP
4. ` TYWIN : St
5. t′ : St ` TYWIN · CHASTISED · t′ : St
6. ` λt′. TYWIN ·CHASTISED ·t′ : (St→ St)RNR
7. ` λc1 : (St→ St)RNR. λc2 : (St→ St)RNR.
λvs.v · s ((say(St→St)RNR c2)
· AND · (say(St→St)RNR c1))
: (St→ St)RNR → (St→ St)RNR
→ (St→ St)RNR

8. ` λc2 : (St→ St)RNR. λvs.v · s
((say(St→St)RNR c2) · AND
·(say(St→St)RNR λt′. TYWIN·CHASTISED·t′))
...
= λc2 : (St→ St)RNR. λvs.v · s

8Regrettably, space constraints prohibit a discussion veri-
fying the typing for the ‘right node raised’ terms. The reader
can verify that the terms are in fact well-typed given the sub-
typing schema in section 2.1.2. It is possible to write infer-
ence rules that speak directly to the introduction and elimina-
tion of the relevant functional subtypes, but these are omitted
here for the sake of brevity.

34



((say(St→St)RNR c2)
· AND · TYWIN · CHASTISED)
: (St→ St)RNR → (St→ St)RNR

9. ` λst. t · SLAPPED · s : (St→ St→ St)TV
10. s′ : St ` s′ : St
11. s′ : St ` λt. t · SLAPPED · s′ : (St→ St)VP
12. ` TYRION : St
13. s′ : St ` TYRION · SLAPPED · s′ : St
14. ` λs′. TYRION · SLAPPED · s′ : (St→ St)RNR
15. ` λvs.v · s ((say(St→St)RNR

λs′. TYRION · SLAPPED · s′)
· AND · TYWIN · CHASTISED)
...
= (λvs.v · s TYRION · SLAPPED
· AND · TYWIN · CHASTISED)
= λs. TYRION · SLAPPED · AND
· TYWIN · CHASTISED · s : (St→ St)RNR

16. ` JOFFREY : St
17. ` TYRION · SLAPPED · AND
· TYWIN · CHASTISED · JOFFREY : St

4 Discussion

We provide a brief introduction to the framework
of Linear Categorial Grammar (LinCG). One of
the primary strengths of categorial grammar in
general has been its ability to address coordina-
tion phenomena. Coordination presents a uniquely
particular problem for grammars which distin-
guish between structural combination (tectogram-
mar) and the actual linear order of the strings gen-
erated by such grammars (part of phenogrammar).
Due to the inability to distinguish ‘directionality’
in string functors within a standard typed lambda
calculus, a general analysis of coordination seems
difficult.

We have elaborated LinCG’s concept of
phenogrammar by introducing phenominators,
closed monoidal linear lambda terms. We have
shown how the recursive function say provides a
left inverse for unaryphenominators, and we have
defined a more general notion of an ‘empty cate-
gory’ known as a vacuity, which say is defined
in terms of. It is then possible to describe sub-
types of functional types suitable to make the rele-
vant distinctions. These technologies enable us to
give analyses of various coordination phenomena
in LinCG, extending the empirical coverage of the
framework.

4.1 Future work
It is possible to give an analysis of argument clus-
ter coordination using phenominators, instantiat-
ing the lexical entry for and with τ as the type
(St→ St→ St→ St)DTV → (St→ St)VP and ϕ as
λvPs. s·(P���)·v, and using hypothetical reason-
ing. Regrettably, the necessity of brevity prohibits
a detailed account here.

Given that phenominators provide access to the
structure of functional terms which concatenate
strings to the right and left of their string support,
it is our belief that any Lambek categorial gram-
mar analysis can be recast in LinCG by an algo-
rithmic translation of directional slash types into
phenominator-indexed functional phenotypes, and
we are currently in the process of evaluating a po-
tential translation algorithm from directional slash
types to phenominators. This should in turn pro-
vide us with most of the details necessary to de-
scribe a system which emulates the HTLCG of
Kubota and Levine (2012), which provides anal-
yses of various gapping phenomena, greatly in-
creasing the overall empirical coverage.

There are a number of coordination phenomena
that require modifications to the tectogrammatical
component. We would like to be able to analyze
unlike category coordinations like rich and an ex-
cellent cook in the manner of Bayer (1996), as well
as Morrill (1996), which would require the addi-
tion of some variety of sum types in the tectogram-
mar. Further muddying the waters is so-called “it-
erated” or “list” coordination, which requires the
ability to generate coordinate structures contain-
ing a number of conjuncts with no coordinating
conjunction, as in Thurston, Kim, and Steve.

It is our intent to extend the use of phenom-
inators to analyze intonation as well, and we
expect that they can be fruitfully employed to
give accounts of focus, association with focus,
contrastive topicalization, “in-situ” topicalization,
alternative questions, and any number of other
phenomena which are at least partially realized
prosodically.

Acknowledgements

I am grateful to Carl Pollard, Bob Levine, Yusuke
Kubota, Manjuan Duan, Gerald Penn, the TTNLS
2014 committee, and two anonymous reviewers
for their comments. Any errors or misunderstand-
ings rest solely on the shoulders of the author.

35



References
Samuel Bayer. 1996. The coordination of unlike cate-

gories. Language, pages 579–616.

Haskell B. Curry. 1961. Some Logical Aspects of
Grammatical Structure. In Roman. O. Jakobson, ed-
itor, Structure of Language and its Mathematical As-
pects, pages 56–68. American Mathematical Soci-
ety.

Philippe de Groote and Sarah Maarek. 2007. Type-
theoretic Extensions of Abstract Categorial Gram-
mars. In Reinhard Muskens, editor, Proceedings
of Workshop on New Directions in Type-Theoretic
Grammars.

Philippe de Groote. 2001. Towards Abstract Catego-
rial Grammars. In Association for Computational
Linguistics, 39th Annual Meeting and 10th Confer-
ence of the European Chapter, Proceedings of the
Conference, pages 148–155.

David Dowty. 1988. Type raising, functional composi-
tion, and non-constituent conjunction. In Richard T.
Oehrle, Emmon Bach, and Deirdre Wheeler, editors,
Categorial Grammars and Natural Language Struc-
tures, volume 32 of Studies in Linguistics and Phi-
losophy, pages 153–197. Springer Netherlands.

John Hughes. 1995. The design of a pretty-printing li-
brary. In Advanced Functional Programming, pages
53–96. Springer.

Yusuke Kubota and Robert Levine. 2012. Gapping
as like-category coordination. In D. Béchet and
A. Dikovsky, editors, Logical Aspects of Computa-
tional Linguistics (LACL) 2012.

Yusuke Kubota. 2010. (In)flexibility of Constituency in
Japanese in Multi-Modal Categorial Grammar with
Structured Phonology. Ph.D. thesis, The Ohio State
University.

J. Lambek and P.J. Scott. 1986. Introduction to
higher order categorical logic. Cambridge Univer-
sity Press.

Scott Martin. 2013. The Dynamics of Sense and Impli-
cature. Ph.D. thesis, The Ohio State University.

Vedrana Mihalicek. 2012. Serbo-Croatian Word Or-
der: A Logical Approach. Ph.D. thesis, The Ohio
State University.

Glyn V. Morrill. 1994. Type Logical Grammar.
Kluwer Academic Publishers.

Glyn Morrill. 1996. Grammar and logic*. Theoria,
62(3):260–293.

Reinhard Muskens. 2001. Lambda grammars and the
syntax-semantics interface. In Proceedings of the
Thirteenth Amsterdam Colloquium, pages 150–155.
Universiteit van Amsterdam.

Reinhard Muskens. 2010. New Directions in Type-
Theoretic Grammars. Journal of Logic, Lan-
guage and Information, 19(2):129–136. DOI
10.1007/s10849-009-9114-9.

Richard Oehrle. 1994. Term-Labeled Categorial Type
Systems. Linguistics and Philosophy, 17:633–678.

Dick Oehrle. 1995. Some 3-dimensional systems of
labelled deduction. Logic Journal of the IGPL, 3(2-
3):429–448.

Carl Pollard and E. Allyn Smith. 2012. A unified
analysis of the same, phrasal comparatives, and su-
perlatives. In Anca Chereches, editor, Proceedings
of SALT 22, pages 307–325. eLanguage.

Carl Pollard. 2013. Agnostic Hyperintensional Se-
mantics. Synthese. to appear.

Aarne Ranta. 2004. Grammatical Framework: A
Type-Theoretical Grammar Formalism. Journal of
Functional Programming, 14:145–189.

Aarne Ranta. 2009. The GF resource grammar library.
Linguistic Issues in Language Technology, 2(1).

E. Allyn Smith. 2010. Correlational Comparison in
English. Ph.D. thesis, The Ohio State University.

Mark Steedman. 1985. Dependency and coördination
in the grammar of dutch and english. Language,
pages 523–568.

36


