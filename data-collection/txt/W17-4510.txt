



















































Learning to Score System Summaries for Better Content Selection Evaluation.


Proceedings of the Workshop on New Frontiers in Summarization, pages 74–84
Copenhagen, Denmark, September 7, 2017. c©2017 Association for Computational Linguistics

Learning to Score System Summaries for Better Content Selection
Evaluation.

Maxime Peyrard and Teresa Botschen and Iryna Gurevych
Research Training Group AIPHES and UKP Lab

Computer Science Department, Technische Universität Darmstadt
www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de

Abstract

The evaluation of summaries is a challeng-
ing but crucial task of the summarization
field. In this work, we propose to learn an
automatic scoring metric based on the hu-
man judgements available as part of classi-
cal summarization datasets like TAC-2008
and TAC-2009. Any existing automatic
scoring metrics can be included as fea-
tures, the model learns the combination
exhibiting the best correlation with human
judgments. The reliability of the new met-
ric is tested in a further manual evaluation
where we ask humans to evaluate sum-
maries covering the whole scoring spec-
trum of the metric. We release the trained
metric as an open-source tool.

1 Introduction

The task of automatic multi-document summariza-
tion is to convert source documents into a con-
densed text containing the most important infor-
mation. In particular, the question of evaluation is
notably difficult due to the inherent lack of gold
standard.

The evaluation can be done manually by involv-
ing humans in the process of scoring a given sys-
tem summary. For example, with the Responsive-
ness metric, human annotators score summaries
on a LIKERT scale ranging from 1 to 5. Later, the
Pyramid scheme was introduced to evaluate con-
tent selection with high inter-annotator agreement
(Nenkova et al., 2007).

Manual evalations are meaningful and reliable
but are also expensive and not reproducible. This
makes them unfit for systematic comparison.

Due to the necessity of having cheap and re-
producible metrics, a significant body of research

was dedicated to the study of automatic evalua-
tion metrics. Automatic metrics aim to produce
a semantic similarity score between the candidate
summary and a pool of reference summaries pre-
viously written by human annotators (Lin, 2004;
Yang et al., 2016; Ng and Abrecht, 2015). Some
variants rely only on the source documents and the
candidate summary ignoring the reference sum-
maries (Louis and Nenkova, 2013; Steinberger and
Ježek, 2012).

In order to select the best automatic metric,
we typically consider manual evalution metrics as
our gold standard, then a good automatic met-
ric should reliably predict how well a summarizer
would perform if human evaluation was conducted
(Owczarzak et al., 2012; Lin, 2004; Rankel et al.,
2013).

In practice, we use the human judgment datasets
like the ones constructed during the manual evalu-
ation of the Text Analysis Conference (TAC). The
system summaries submitted to the shared tasks
were manually scored by trained human annota-
tors following the Responsiveness and/or the Pyra-
mid schemes. An automatic metric is considered
good if it ranks the system summaries similarly as
humans did.

Currently, ROUGE (Lin, 2004) is the accepted
standard for automatic evaluation of content selec-
tion because of its simplicity and its good correla-
tion with human judgments. However, previous
works on evaluation metrics comparison averaged
scores of summaries over topics for each system
and then computed the correlation with averaged
scores given by humans. ROUGE works well in
this scenario which compares only systems after
aggregating their scores for many summaries. We
call this scenario system-level correlation analy-
sis.

A more natural analysis, which we use in this
work, is to compute the correlation between the

74



candidate metric and human judgments for each
topic indivually and then average these correla-
tions over topics. In this scenario, which we
call summary-level correlation analysis, the per-
formance of ROUGE significantly drops meaning
that on average ROUGE does not really identify
summary quality, it can only rank systems after
aggregation of many topics.

In order to advance the field of summarization
we need to have more consistent metrics correlat-
ing well with humans on every topic and capable
of estimating the quality of individual summaries
(not just systems).

We propose to rely on human judgment datasets
to learn an automatic scoring metric. The learned
metric presents the advantage of being explicitly
trained to exhibit high correlation with the “gold-
standard” human judgments at the summary level
(and not just at the system level). The setup is
also convenient because any already existing auto-
matic metric can be incorporated as a feature and
the model learns the best combination of features
matching human judgments.

We should worry whether the learned metric is
reliable. Indeed, typical human judgment datasets
(like the ones from TAC-2008 or TAC-2009) con-
tain manual scores only for several system sum-
maries which have a limited range of quality. We
conduct a manual evaluation specifically designed
to test the metric accross its whole scoring spec-
trum.

To summarize our contributions: We performed
a summary-level correlation analysis to compare
a large set of existing evaluation metrics. We
learned a new evaluation metric as a combination
of existing ones to maximize the summary-level
correlation with human judgments. We conducted
a manual evaluation to test whether learning from
available human judgment datasets yields a reli-
able metric accross its whole scoring spectrum.

2 Related Work

Automatic evaluation of content has been the sub-
ject of a lot of research. Many automatic metrics
have been developed and we present here some of
the most important ones.

ROUGE (Lin, 2004) simply computes the n-
gram overlap between a system summary and a
pool of reference summaries. It has become a
de-facto standard metric because of its simplicity
and high correlation with human judgments at the

system-level. Afterwards, Ng and Abrecht (2015)
extended ROUGE with word embeddings. Instead
of hard lexical matching of n-grams, ROUGE-WE
uses soft matching based on the cosine similarity
of word embedding.

Recently, a line of research aimed at creating
strong automatic metrics by automating the Pyra-
mid scoring scheme (Harnly et al., 2005). Yang
et al. (2016) proposed PEAK, a metric where the
components requiring human input in the original
Pyramid annotation scheme are replaced by state-
of-the-art NLP tools. It is more semantically mo-
tivated than ROUGE and approximates correctly
the manual Pyramid scores but it is computation-
ally expensive making it difficult to use in practice.

Some other metrics do not make use of the ref-
erence summaries, they compute a score based
only on the candidate summary and the source
documents (Lin et al., 2006; Louis and Nenkova,
2013). One representative of this class is the
Jensen Shannon (JS) divergence, an information-
theoretic measure comparing system summaries
and source documents with their underlying prob-
ability distributions of n-grams. JS divergence is
simply the symmetric version of the well-known
Kullback-Leibler (KL) divergence (Haghighi and
Vanderwende, 2009).

Little work has been done on the topic of
learning an evaluation metric. Conroy and Dang
(2008) previously investigated the performances
of ROUGE metrics in comparison with human
judgments and proposed ROSE (ROUGE Opti-
mal Summarization Evaluation) a linear combi-
nation of ROUGE metrics to maximize correla-
tion with human responsiveness. We also look for
a combination of features which correlates well
with human judgements but, in contrast to Con-
roy and Dang (2008), we include a wider set of
metrics: ROUGE scores, other evaluation met-
rics (like Jensen-Shannon divergence) and features
typically used by summarization systems.

Hirao et al. (2007) also proposed a related ap-
proach. They used a voting based regression to
score summaries with human judgments as gold
standard. Our setup is different because we train
and evaluate our metric with the summary-level
correlation analysis instead of the system-level
one. Our experiments are done on multi-document
datasets whereas they use single-documents. Fi-
nally, we also perform a further manual evaluation
to test the metric outside of its training domain.

75



3 Approach

Let a dataset D contain m topics. A given topic
ti consists of a set of documents Di, a set of ref-
erence summaries θi, a set of n system summaries
Si and the scores given by humans to the n sum-
maries of Si noted Ri. We note si,j the j-th sum-
mary of the i-th topic and rhi,j the score it received
from manual evaluation:

ti = (Di, θi,Si,Ri)
Si = [si,1, . . . , si,n]
Ri = [rhi,1, . . . , rhi,n]

(1)

An automatic evaluation metric is a function
taking as input a document set Di, a set of ref-
erence summaries θi and a candidate system sum-
mary s and outputs a score. For simplicity, we
note: σ(Di, θi, s) = σi(s) the score of s as a sum-
mary of the i-th topic according to some scoring
metric σ.

We search an automatic scoring function σ such
that σi(si,j) correlates well with the manual scores
rhi,j .

The final score can be computed at the system-
level by aggregating scores over topics before and
then computing the correlation or at the summary-
level by computing the correlation for each topic
and then averaging over topics. We briefly present
the difference between the two in the following
paragraphs.

System-level correlation Let K be any corre-
lation metric operating on two lists of scored el-
ements, then the system-level correlation is com-
puted by the following formula:

Ksysavg = K([
m∑
i

σi(si,1), . . . ,
m∑
i

σi(si,n)],

[
m∑
i

rhi,1, . . . ,
m∑
i

rhi,n]) (2)

Both terms in K are lists of size n. The scores
for the summaries of the l-th summarizer are ag-
gregated to form the l-th element of the lists. The
correlation is computed on the two aggregated
lists. Therefore, Ksysavg only indicates whether the
evaluation metrics can rank systems correctly af-
ter aggregation of many summary scores but it
ignores individual summaries. It has been used
before because evaluation metrics were initially
tasked to compare systems.

Summary-level correlation Instead, we advo-
cate for the summary-level correlation which is
computed by the following formula:

Ksummavg =
1
m
·
∑
ti∈D

K([σi(si,1), . . . , σi(si,n)],

[rhi,1, . . . , r
h
i,n]) (3)

Here, we compute the correlation between human
judgments and automatic scores for each topic and
then average the correlation scores over topics.
This measures how well evaluation metrics cor-
relate with human judgments for summaries and
not only for systems which is important in order
to have finer grain of understanding.

From now on, when we refer to correlation with
human judgments we will refer to the summary-
level correlation.

Correlation metrics There exist many possible
choices for K. As different correlation metrics
measure different properties, we use three comple-
mentary metrics: Pearson’s r, Spearman’s ρ and
Normalized Discounted Cumulative Gain (Ndcg).

Pearson’s r is a value correlation metric which
depicts linear relationships between the scores
produced by the automatic metric and the human
judgments.

Spearman’s ρ is a rank correlation metric which
compares the ordering of systems induced by the
automatic metric and the ordering of systems in-
duced by human judgments.

Ndcg is a metric that compares ranked lists and
puts more emphasis on the top elements by log-
arithmic decay weighting. Intuitively, it captures
how well the automatic metric can recognize the
best summaries.

3.1 Features
The choice of features is a crucial part of every
learning setup. Here, we can benefit from the
large amount of previous works studying signals
of summary quality. We can classify these signals
in three categories.

First, any existing automatic scoring metric can
be a feature. These metrics use the candidate sum-
mary and the reference summary to output a score.

The second category contains the previous sum-
marization systems having an explicit formulation
of summary quality. These systems can implicitly
score any summary, then they extract the summary
with maximal score via optimization techniques

76



(Gillick and Favre, 2009; Haghighi and Vander-
wende, 2009). Optimization-based systems have
recently become popular (McDonald, 2007). Such
features score the candidate summary based only
on the document sources and the summary itself.

The last category contains the metrics produc-
ing a score based only on the summary. Examples
of such metrics include readability or redundancy.

Clearly, features using reference summaries
(existing automatic metrics) are expected to be
more useful for our task. However, it has been
shown that some metrics of the second cate-
gory (like JS divergence) also contain useful sig-
nal to approximate human judgments (Louis and
Nenkova, 2013). Therefore, we use features com-
ing from all three categories expecting that they
are sensitive to different properties of a good sum-
mary.

We considered only features cheap to compute
in order to deliver a simple and efficient tool. We
now briefly present the selected features.

Features using reference summaries
ROUGE-N (Lin, 2004) computes the n-gram
overlap between the candidate summary and
the pool of reference summaries. We include
as features the variants identified by Owczarzak
et al. (2012) as strongly correlating with humans:
ROUGE-2 recall with stemming and stopwords
not removed (giving the best agreement with
human evaluation), and ROUGE-1 recall (the
measure with the highest ability to identify the
better summary in a pair of system summaries).

ROUGE-L (Lin, 2004) considers each sentence
of the candidate and reference summaries as se-
quences of words (after stemming). It interprets
the longest common subsequence between sen-
tences as a similarity measure. An overall score
for the candidate summary is given by combining
the scores of individual sentences. One advantage
of using ROUGE-L is that it does not require con-
secutive matches but in-sequence matches reflect-
ing sentence-level word order.

JS divergence measures the dissimilarity be-
tween two probability distributions. In summa-
rization, it was also used to compare the n-gram
probability distribution of a summary and souce
documents (Louis and Nenkova, 2013), but here
we employ it for comparing the n-gram probability
distribution of the candidate summary with the ref-
erence summaries. Thus, it yields an information-
theoretic measure of the dissimilarity between the

candidate summary and the reference summaries.
If θi is the set of reference summaries for the

i-th topic, then we compute the following score:

JSref (s, θi) =
1
|θi|

∑
ref∈θi

JS(s, ref) (4)

ROUGE-WE (Ng and Abrecht, 2015) is the
variant of ROUGE-N replacing the hard lexical
matching by a soft matching based on the cosine
similarity of word embeddings. We use ROUGE-
WE-1 and ROUGE-WE-2 as part of our features.

FrameNet-based metrics ROUGE-WE pro-
poses a statistical approach (word embeddings) to
alleviate the hard lexical matching of ROUGE. We
also include a linguistically motivated one. We
replace all nouns and verbs of the reference and
candidate summaries with their FrameNet (Baker
et al., 1998) frames. This frame annotation is
done with the best-performing system configura-
tion from Hartmann et al. (2017) pre-trained on all
FrameNet data. It assigns a frame to a word based
on the word itself and the surrounding context in
the sentence.

Frames are more abstract than words, thus dif-
ferent but related words might be associated with
the same frames depending on the meaning of the
words in the respective context. ROUGE-N can
now match related words through their frames. We
also use the unigram and bigram variants (Frame-
N).

Semantic Vector Space Similarities In gen-
eral, automatic evaluation metrics comparing sys-
tem summaries with reference summaries propose
a kind of semantic similarity between summaries.
Finding good automatic evaluation metric is hard
because the task of textual semantic similarity is
challenging. With the development of word em-
beddings (Mikolov et al., 2013), several seman-
tic similarities have arisen exploiting the inherent
similarities built in vector space models. We in-
clude one such metric: AV GSIM , the cosine sim-
ilarity between the average word embeddings of
the system summary and the reference summaries.
To reduce noise, we exclude stopwords.

Features using document sources are inspired
by existing summarization systems:

TF?IDF comes from the seminal work from
Luhn (1958). Each sentence in the summary is
scored according to the TF*IDF of its term. The
score of the summary is the sum of the scores of

77



its sentences. We computed the version based on
unigrams and bigrams (TF∗IDF-N).

N-gram Coverage is inspired by the strong
summarizer ICSI (Gillick and Favre, 2009). Each
n-gram in the summary is scored with the fre-
quency it has in the source documents. The fi-
nal score of the system summary is the sum of
the scores of its n-grams. We also use the variants
based on unigrams and bigrams (Cov-N).

KL and JS measures the KL or JS divergence
between the word distributions in the summary
and source documents. We use as features both
KL and JS based on unigram and bigram distribu-
tions (KL-N and JS-N).

Features using the candidate summary only
Finally, we also include a redundancy metric based
on n-gram repetition in the summary. It is the
number of unique n-grams divided by the total
number of n-grams in the summary. We also use
unigrams and bigrams (Red-N).

3.2 Model
For a given topic ti, let φ be the function taking as
input a document set Di, a set of reference sum-
maries θi and a system summary s and output-
ing the set of features described earlier. We note
φ(Di, θi, s) = φi(s), the feature set representing s
as a summary of the topic i.

We aim to learn a function σω with parameters
ω scoring summaries similarly as humans would.
If σω(φi(s)) is the score given by the learned met-
ric to the summary s, we look for the set of pa-
rameters ω which maximizes the summary-level
correlation defined by equation 3. It means we are
trying to solve the following problem:

argmax
ω

∑
ti∈D

K([σω(φi(si,1)), . . . , σω(φi(si,n))],

[rhi,1, . . . , r
h
i,n]) (5)

We can approach this problem either with a
learning-to-rank or with a regression framework.
Learning-to-rank seems well suited because it cap-
tures the fact that we are interested in ranking sum-
maries, however we selected the regression ap-
proach in order to keep the model simple. It solves
a different but closely related problem:

argmax
ω

∑
ti∈D

n∑
j

‖σω(φi(si,j))− rhi,j‖2
2

(6)

The regression finds the parameters predicting
the scores closest to the ones given by humans.
We use an off-the-shelf implementation of Support
Vector Regression (SVR) from scikit-learn (Pe-
dregosa et al., 2011).

4 Experiments

We conducted both automatic and manual testing
of the learned metric. We present here the datasets
and results of the experiments.

4.1 Datasets

We use two multi-document summarization
datasets from the Text Analysis Conference (TAC)
shared tasks: TAC-2008 and TAC-2009.1 TAC-
2008 and TAC-2009 contain 48 and 44 topics, re-
spectively. Each topic consists of 10 news articles
to be summarized in a maximum of 100 words.
We use only the so-called initial summaries (A
summaries), but not the update part.

For each topic, there are 4 human reference
summaries. In both editions, all system sum-
maries and the 4 reference summaries were man-
ually evaluated by NIST assessors for readability,
content selection (with Pyramid) and overall re-
sponsiveness. At the time of the shared tasks, 57
systems were submitted to TAC-2008 and 55 to
TAC-2009. For our experiments, we use the Pyra-
mid and the responsiveness annotations.

With our notations, for example with TAC-
2009, we have n = 55 scored system summaries,
m = 44 topics, Di contains 10 documents and θi
contains 4 reference summaries.

We also use the recently created German dataset
DBS-corpus (Benikova et al., 2016). It contains
10 topics consisting of 4 to 14 documents each.
The summaries have variable sizes and are about
500 words long. For each topic, 5 summaries were
evaluated by trained human annotators but only for
content selection with Pyramid.

We experiment with this dataset because it con-
tains heterogeneous sources (different text types)
in German about the educational domain. This
contrasts with the English homogeneous news
documents from TAC-2008 and TAC-2009. Thus,
we can test our technique in a different summa-
rization setup.

1http://tac.nist.gov/2009/
Summarization/, http://tac.nist.gov/2008/
Summarization/

78



4.2 Correlation Analysis
Baselines Each feature presented earlier is eval-
uated individually. 2 Indeed, they all produce
scores for summaries meaning we can measure
their correlation with human judgments. Classical
evaluation metrics, like ROUGE-N variants, are
therefore also included in this analysis and serve
as baselines. Identifying which metrics have high
correlation with human judgments constitutes an
initial feature analysis.

Most of the features do not need language de-
pendent information, except those requiring word
embeddings or frame identification based on a
frame inventory. We do not include the frame
identification features when experimenting with
the German DBS-corpus. However, for the other
language dependent features, we used the Ger-
man word embeddings developed by Reimers
et al. (2014). For the English datasets, we use
dependency-based word embeddings (Levy and
Goldberg, 2014).

The performances of the baselines on TAC-
2008 and TAC-2009 are displayed in Table 1, and
Table 2 depicts scores for the DBS-corpus. In or-
der to have an insightful view, we report the scores
for the three correlation metrics presented in the
previous section: Pearson’s r, Spearman’s ρ and
Ndcg.

Feature Analysis There are fewer scored sum-
maries per topic in the DBS-corpus (5 compared
to 55 in TAC-2008). Shorter ranked lists gener-
ally have higher scores which explains the over-
all higher correlation scores in the DBS-corpus. It
also contains longer summaries (500 words com-
pared to 100 words for TAC) which provides a
reason behind the better performances of JS fea-
tures. Indeed, word frequency distributions are
more representative for longer texts.

First, we see that classical evaluation metrics
like ROUGE-N have lower correlation when com-
puted at the summary-level. Here the correlations
are around 0.60 spearman’s ρwhile they often sur-
pass 0.90 in the system-level scenario (Lin, 2004).

However, the experiments confirm that
ROUGE-N, especially ROUGE-2, are strong
when compared to other available metrics. Even
the more semantically motivated metrics like
ROUGE-N-WE or Frame-N (ROUGE-N enriched
with frame annotations) can not outperform

2We do not include Red-N in the result table because it
does not aim to measure content selection

the simple ROUGE-N. The added semantic
information might be too noisy to really give
improvements. Simple lexical comparison still
seems to be better for evaluation of summaries.

Interestingly, it is the other simple evaluation
metric JSref −N which competes with ROUGE-
N. This metric only compares the distribution of
n-grams in the reference summaries with the dis-
tribution of n-grams in the candidate summary and
it outperforms ROUGE-N for pearson’s r. How-
ever, ROUGE-N still outperforms JSref − N for
Ndcg. It indicates that this metric can be comple-
mentary with ROUGE-N even though it was rarely
used for evaluation before.

Finally, we observe that the features not using
the reference summaries have poor performances.
It is troubling because these are the strategies used
by classical summarization systems in order to
decide which summary to extract. Overall, they
have Ndcg scores higher than 0.5 meaning they
can decently identify some of the best summaries
explaining why these systems can produce good
summaries.

Our Models For each dataset, we trained two
models. The first model (S3full for Supervised
Summarization Scorer) uses all the available fea-
tures for training. However, the previous feature
analysis revealed that some features are poor. We
hypothesized that they might harm the learning
process. Therefore we trained a second model
S3best using only 6 of the best features.

3 We nor-
malize human scores so that they every topic has
the same mean.

Both models are trained and tested in a leave-
one-out cross-validation scenario ensuring proper
testing of the approach. The results for TAC-2008
and TAC-2009 are presented in Table 1 while the
results for the DBS-corpus are in Table 2. For
comparison we also added the correlation between
pyramid and responsiveness when both annota-
tions are available.

Model analysis As expected we observe that us-
ing the restricted set of non-noisy features gives
stronger results. S3best is the best metric and out-
performs the classical ROUGE-N. Thanks to the
combination of ROUGE-N and JSref −N , it gets
the best of both worlds and has consistent perfor-
mances accross datasets and correlation measures.

3ROUGE-1, ROUGE-2, ROUGE-WE-1, ROUGE-WE-2,
JSref − 1 and JSref − 2

79



TAC-2008 TAC-2009
responsiveness Pyramid responsiveness Pyramid

r ρ Ndcg r ρ Ndcg r ρ Ndcg r ρ Ndcg

TF∗IDF-1 .1760 .2248 .5040 .1833 .2376 .3594 .1874 .2226 .3912 .2423 .2845 .2349
TF∗IDF-2 .0478 .1540 .5962 .0496 .1827 .4833 .0476 .1674 .5079 .0972 .2337 .3949
Cov-1 .2552 .2635 .6137 .2812 .3035 .5140 .2267 .2212 .5627 .2765 .2871 .4776
Cov-2 .1056 .1878 .6154 .1136 .2287 .5228 .1382 .0787 .5602 .1170 .1336 .4936
KL-1 .1774 .2240 .4922 .1996 .2682 .3470 .1696 .2220 .4139 .2328 .2939 .2568
KL-2 .0042 .1654 .6188 .0038 .1921 .5160 .0602 .1373 .6311 .0355 .2011 .5641
JS-1 .2517 .2771 .4411 .2811 .3214 .2839 .2160 .2352 .3896 .2742 .3119 .2273
JS-2 .0409 .1708 .5874 .0447 .2058 .4804 .0013 .1548 .5646 .0310 .2166 .4734
ROUGE-1 .7035 .5786 .9304 .7479 .6329 .9125 .7043 .5657 .8901 .8085 .6922 .9323
ROUGE-2 .6955 .5725 .9333 .7184 .6358 .9064 .7271 .5837 .9039 .8031 .6949 .9272
ROUGE-1-WE .5714 .4503 .9042 .5798 .4587 .8434 .5865 .4377 .8724 .6534 .5163 .8792
ROUGE-2-WE .5665 .3971 .8972 .5563 .3888 .8258 .6072 .4130 .8749 .6712 .4811 .8709
ROUGE-L .6815 .5207 .9300 .7028 .5688 .8937 .7305 .5631 .9083 .7799 .6529 .9159
AV GSIM .1351 .0904 .6890 .0747 .0543 .5521 .2389 .1557 .6861 .2306 .1597 .5956
Frame-1 .6587 .5083 .9174 .6861 .5294 .8867 .6786 .5270 .8827 .7626 .6280 .9158
Frame-2 .6769 .5190 .9194 .6917 .5560 .8885 .7152 .5555 .9000 .7814 .6486 .9191
JSref − 1 .6907 .5642 .3786 .7527 .6481 .1862 .7125 .5834 .3091 .8328 .7286 .1214
JSref − 2 .6943 .5579 .3961 .7187 .6253 .2101 .7291 .5862 .3195 .8105 .7007 .1342
S3full .6960 .5582 .9256 .7537 .6520 .9073 .7310 .5522 .9002 .8384 .7240 .9373
S3best .7154 .5954 .9330 .7545 .6527 .9077 .7386 .5952 .9015 .8429 .7315 .9354

Pyramid .7030 .6604 .8528 — — — .7152 .6386 .8520 — — —

Table 1: Correlation of automatic metrics with human judgments for TAC-2008 and TAC-2009.

Thanks to the combination of metrics, our
model has more consistent performances accross
different correlation metrics. It especially benefits
from the complementarity of ROUGE and JSref .

While the improvements are sometimes good,
they are not dramatic. A bigger and more diverse
training data should give further improvements.
With a better training set, it might even not be
necessary to manually remove the noisy features
as the model will learn when to ignore which fea-
tures.

4.3 Percentage of failure

By analysing the average correlation between the
different metrics and human judgments over all
topics, we only get an average overview. It would
be useful to estimate the number of topics on
which a metric fails or works. One could plot
cumulative distribution graphs where the x-axis is
the correlation range (from 0 to 1 in absolute val-
ues) and the y-axis indicates the number of top-
ics on which the metric’s correlation with humans
was above the given x point. However, this would
require 460 plots (3 datasets * 20 metrics * 6 cor-
relations measures) which would not be readable.

Instead, we define a threshold for each corre-
lation measure and count the percentage of top-
ics for which the metric’s correlation with humans
was below the threshold. The threshold value is

Pyramid
r ρ Ndcg

TF∗IDF-1 .2902 .2016 .8077
TF∗IDF-2 .2903 .2396 .8181
Cov-1 .0997 .0544 .8891
Cov-2 .0991 .0638 .8965
KL-1 .7299 .6992 .7348
KL-2 .3089 .1967 .8316
JS-1 .2909 .1680 .8324
JS-2 .1531 .1385 .8496
ROUGE-1 .7016 .7412 .9841
ROUGE-2 .8272 .8892 .9985
ROUGE-1-WE .6842 .7140 .9782
ROUGE-2-WE .7643 .7937 .9914
ROUGE-L .7908 .8268 .9957
AV GSIM .7844 .8309 .9924
JSref − 1 .9712 .8732 .6881
JSref − 2 .9689 .8793 .6879
S3full .9077 .8781 .9988
S3best .9483 .8755 .9988

Table 2: Correlation of automatic metrics with hu-
man judgments for the DBS-corpus.

80



TAC-2008 TAC-2009
responsiveness Pyramid responsiveness Pyramid

r ρ Ndcg r ρ Ndcg r ρ Ndcg r ρ Ndcg

ROUGE-1 .2500 .3958 .0208 .1250 .3125 .1250 .2727 .4318 .2272 .0455 .1364 .0223
ROUGE-2 .3125 .4167 .0208 .2708 .2292 .1667 .2500 .3864 .2272 .0682 .1591 .0000
ROUGE-1-WE .7083 .7708 .1042 .6875 .6875 .4583 .5455 .7500 .2500 .4318 .5682 .2955
ROUGE-2-WE .6667 .8333 .1667 .6667 .8333 .6458 .5455 .7727 .2500 .3409 .6364 .3636
JSref − 1 .2917 .4375 1.000 .1042 .2917 1.000 .2045 .4091 1.000 .0227 .1136 1.000
JSref − 2 .3542 .4375 1.000 .2708 .3125 1.000 .2500 .3864 1.000 .0227 .0909 1.000
S3best .2500 .2917 .0208 .1458 .2708 .1458 .2272 .3409 .2272 .0227 .1136 .0227

Table 3: Percentage of topics for which the correlation between the metric and human judgments is
below the chosen thresholds for TAC-2008 and TAC-2009.

an indicator of when the metrics fails to correctly
model human judgments on a given topic. We
chose: 0.65 for pearson’s r, 0.55 for spearman’s
ρ and 0.85 for Ndcg. The values are chosen ar-
bitrarily but in order to get a meaningful picture,
if we choose a threshold too low then all metrics
are always above, if the threshold is too high all
metrics are always below. We report the scores for
the set of best features and our best metric S3best on
TAC datasets in Table 3.

We observe that our metric performs well and
has low percentage of failure. It exhibits again its
robustness accross different correlation measures.
We also observe the strong performances of the
JSref especially the unigram version, however it
fails completely for the Ndcg metrics which indi-
cates that it always has problems to identify the
top best summaries even though its overall corre-
lation is good. Again this confirms that our metric
benefits from the complementarity of JSref and
ROUGE because ROUGE has performs well with
Ndcg.

4.4 Manual annotation
Our models are trained with human judgment
datasets constructed during the shared tasks,
meaning that only some system summaries and the
4 references summaries have been evaluated by
humans. Systems have a limited range of quality
as they rarely propose excellent summaries, and
bad summaries are usually due to unrelated errors
(like empty summaries). This is a concern because
our learned metric will certainly perform well in
this quality range, but it should also perform well
outside of this range. It has to be capable to cor-
rectly recognize the new and better summaries that
will be proposed by future systems.

As the learning is constrained to a specific qual-
ity range, we need to check that the whole scoring

spectrum of the metric correlates well with hu-
mans. We check that what is considered upper-
bound (resp. random) by the metric is also consid-
ered as excellent (resp. bad) by humans.

Annotation setup We collect summaries by em-
ploying a meta-heuristic solver introduced re-
cently for extractive MDS by Peyrard and Eckle-
Kohler (2016). Specifically, we use the tool pub-
lished with their paper.4

Their meta-heuristic solver implements a Ge-
netic Algorithm to create and iteratively optimize
summaries over time. In this implementation, the
individuals of the population are the candidate
solutions which are valid extractive summaries.
Each summary is represented by a binary vector
indicating for each sentence in the source docu-
ment whether it is included in the summary or not.
The size of the population is a hyper-parameter
that we set to 100. Two evolutionary operators are
applied: the mutation and the reproduction. Mu-
tations happen to several randomly chosen sum-
maries by randomly removing one of its sentences
and adding a new one that does not violate the
length constraint. The reproduction is performed
by randomly extracting a valid summary from
the union of sentences of randomly selected par-
ent summaries. Both operators are controlled by
hyper-parameters which we set to their default val-
ues.

We use our metric S3best as the fitness func-
tion and, after the algorithm converges, the final
population is a set of summaries ranging from al-
most random to almost upper-bound. For 15 topics
of TAC-2009, we automatically selected 10 sum-
maries of various quality from the final population
and asked two humans to score them following the

4https://github.com/UKPLab/
coling2016-genetic-swarm-MDS

81



Responsiveness
r ρ Ndcg

Best baseline .6945 .6701 .9210

S3full .7198 .6818 .9323
S3best .7318 .6936 .9355

Table 4: Correlation of automatic metrics with hu-
man accross the whole scoring spectrum of S3best.

guidelines used during DUC and TAC for assess-
ing responsiveness. To select the summaries, we
ranked them according to their S3best scores and for
a population of 100 we picked 10 evenly spaced
summaries (the first, the tenth and so on). We
observe an inter-annotator agreement of 0.74 Co-
hen’s κ. The results are displayed in Table 4 where
S3best is compared to the best baseline (ROUGE-2)
and S3full.

The S3best metric gets consistent correlation
scores with human judgments as it had with re-
sponsiveness in the previous experiements (on
TAC-2009, for responsiveness, S3best has 0.7386
pearson’s r, 0.5952 spearman’s ρ and 0.9015
Ndcg) . It is a strong indicator that the metric is
reliable even outside of its training domain. It also
outperforms ROUGE-2 in this experiment.

5 Discussion

The experiments showed that even semanti-
cally motivated metrics struggle to outperform
ROUGE-N. However, the simple JSref and
ROUGE-N using only n-gram are the best base-
lines. Reporting these two metrics together might
be more insightful than simply reporting ROUGE-
N because they are complementary. Our learned
metric is benefiting from this complementarity to
achieve its scores.

However, finding a good evaluation metric for
summarization is a challenging task which is still
not solved. We proposed to tackle this problem by
learning the metric to approximate human judg-
ments with a regression framework. A learning-to-
rank approach could give stronger results because
it might be easier to rank summaries. Even after
normalization human scores are noisy and topic-
dependent. We expect ranking to be more trans-
ferable from one topic to another. Here, we con-
strained ourselves to a simple approach in order
to provide a user-friendly tool and the regression
offered a simple and effective solution.

Our experiments revealed that the available

human judgment datasets are somehow limited.
While it is possible to learn a reliable combination
of existing metrics, one would need better and big-
ger human judgment datasets to really get strong
improvements. In particular, it is important to ex-
tend the coverage of these datasets because we rely
on them to compare evaluation metrics. These an-
notations are the key to understand what humans
consider to be good summaries. Statistical analy-
sis on such datasets will likely be beneficial to de-
velop both evaluation metrics and summarization
systems (Peyrard and Eckle-Kohler, 2017).

The metric was evaluated on English news
datasets and on a German dataset of heterogeneous
sources but a wider study might be needed in or-
der to measure the generalization of the learned
metric to other datasets and domains. Such gener-
alization capabilities would be interesting because
one would not need to re-train a new metric for
every domain.

We believe it is important to develop evaluation
metrics correlating well with human judgments at
the summary-level. This gives a more insight-
ful and reliable metric. If the metric is reliable
enough, one can use it as a target to train super-
vised summarization systems (Takamura and Oku-
mura, 2010; Sipos et al., 2012) and approach sum-
marization as a principled machine learning task.

6 Conclusion

We presented an approach to learn an automatic
evaluation metrics correlating well with human
judgments at the summary-level. The metric is a
combination of existing automatic scoring strate-
gies learned via regression. We release the metric
as an open-source tool. 5 We hope this study will
encourage more work on learning evaluation met-
rics and improving the human judgement datasets.
Better human judgment datasets will be greatly
beneficial for improving both evaluation metrics
and summarization systems.

Acknowledgments

This work has been supported by the German Re-
search Foundation (DFG) as part of the Research
Training Group “Adaptive Preparation of Informa-
tion from Heterogeneous Sources” (AIPHES) un-
der grant No. GRK 1994/1, and via the German-
Israeli Project Cooperation (DIP, grant No. GU
798/17-1).

5https://github.com/UKPLab/emnlp-ws-2017-s3

82



References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.

1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 86–90. Association for Computational Lin-
guistics.

Darina Benikova, Margot Mieskes, Christian M.
Meyer, and Iryna Gurevych. 2016. Bridging the gap
between extractive and abstractive summaries: Cre-
ation and evaluation of coherent extracts from het-
erogeneous sources. In Proceedings of the 26th In-
ternational Conference on Computational Linguis-
tics (COLING), pages 1039 – 1050.

John M. Conroy and Hoa Trang Dang. 2008. Mind
the Gap: Dangers of Divorcing Evaluations of Sum-
mary Content from Linguistic Quality. In Proceed-
ings of the 22Nd International Conference on Com-
putational Linguistics (COLING), volume 1, pages
145–152.

Dan Gillick and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proceedings of the
Workshop on Integer Linear Programming for Nat-
ural Language Processing, pages 10–18, Boulder,
Colorado. Association for Computational Linguis-
tics.

Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-document Summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 362–370, Boulder, Col-
orado. Association for Computational Linguistics.

Aaron Harnly, Rebecca Passonneau, and Owen Ram-
bow. 2005. Automation of Summary Evaluation
by the Pyramid Method. In Proceedings of the In-
ternational Conference Recent Advances in Natu-
ral Language Processing (RANLP), pages 226–232,
Borovets, Bulgaria.

Silvana Hartmann, Ilia Kuznetsov, Teresa Martin, and
Iryna Gurevych. 2017. Out-of-domain FrameNet
Semantic Role Labeling. In Proceedings of the 15th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2017),
pages 471–482. Association for Computational Lin-
guistics.

Tsutomu Hirao, Manabu Okumura, Norihito Yasuda,
and Hideki Isozaki. 2007. Supervised Automatic
Evaluation for Summarization with Voted Regres-
sion Model. Information Processing and Manage-
ment, 43(6):1521–1535.

Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL), volume 2, pages 302–
308.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop, pages 74–81, Barcelona, Spain. Associa-
tion for Computational Linguistics.

Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-
Yun Nie. 2006. An Information-Theoretic Approach
to Automatic Evaluation of Summaries. In Pro-
ceedings of the Human Language Technology Con-
ference at NAACL, pages 463–470, New York City,
USA.

Annie Louis and Ani Nenkova. 2013. Automati-
cally Assessing Machine Summary Content With-
out a Gold Standard. Computational Linguistics,
39(2):267–300.

Hans Peter Luhn. 1958. The Automatic Creation of
Literature Abstracts. IBM Journal of Research De-
velopment, 2:159–165.

Ryan McDonald. 2007. A Study of Global Inference
Algorithms in Multi-document Summarization. In
Proceedings of the 29th European Conference on
IR Research, pages 557–564, Rome, Italy. Springer-
Verlag.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Ani Nenkova, Rebecca Passonneau, and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing Human Content Selection Variation in Summa-
rization Evaluation. ACM Transactions on Speech
and Language Processing (TSLP), 4(2).

Jun-Ping Ng and Viktoria Abrecht. 2015. Better sum-
marization evaluation with word embeddings for
rouge. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1925–1930, Lisbon, Portugal. Associa-
tion for Computational Linguistics.

Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An Assessment of
the Accuracy of Automatic Evaluation in Summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization, pages 1–9, Montreal, Canada. Associa-
tion for Computational Linguistics.

Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

83



Maxime Peyrard and Judith Eckle-Kohler. 2016.
A General Optimization Framework for Multi-
Document Summarization Using Genetic Algo-
rithms and Swarm Intelligence. In Proceedings of
the 26th International Conference on Computational
Linguistics (COLING 2016), pages 247 – 257, Os-
aka, Japan. The COLING 2016 Organizing Commit-
tee.

Maxime Peyrard and Judith Eckle-Kohler. 2017. A
principled framework for evaluating summarizers:
Comparing models of summary quality against hu-
man judgments. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2017), volume Volume 2: Short Pa-
pers. Association for Computational Linguistics.

Peter A. Rankel, John M. Conroy, Hoa Trang Dang,
and Ani Nenkova. 2013. A Decade of Automatic
Content Evaluation of News Summaries: Reassess-
ing the State of the Art. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 131–136, Sofia, Bulgaria.
Association for Computational Linguistics.

Nils Reimers, Judith Eckle-Kohler, Carsten Schnober,
Jungi Kim, and Iryna Gurevych. 2014. GermEval-
2014: Nested Named Entity Recognition with Neu-
ral Networks. In Workshop Proceedings of the 12th
Edition of the KONVENS Conference, pages 117–
120.

Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin Learning of Sub-
modular Summarization Models. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
224–233, Avignon, France. Association for Compu-
tational Linguistics.

Josef Steinberger and Karel Ježek. 2012. Evaluation
measures for text summarization. Computing and
Informatics, 28(2):251–275.

Hiroya Takamura and Manabu Okumura. 2010. Learn-
ing to Generate Summary as Structured Output. In
Proceedings of the 19th ACM international Confer-
ence on Information and Knowledge Management,
pages 1437–1440, Toronto , ON, Canada. Associa-
tion for Computing Machinery.

Qian Yang, Rebecca Passonneau, and Gerard de Melo.
2016. PEAK: Pyramid Evaluation via Automated
Knowledge Extraction. In Proceedings of the 30th
AAAI Conference on Artificial Intelligence (AAAI
2016), Phoenix, AZ, USA. AAAI Press.

84


