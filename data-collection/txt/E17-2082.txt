



















































Integrating Semantic Knowledge into Lexical Embeddings Based on Information Content Measurement


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 509–515,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Integrating Semantic Knowledge into Lexical Embeddings Based on
Information Content Measurement

Hsin-Yang Wang
Institute of Information Science

Academia Sinica
Nankang, Taipei, Taiwan

wang@iis.sinica.edu.tw

Wei-Yun Ma
Institute of Information Science

Academia Sinica
Nankang, Taipei, Taiwan

ma@iis.sinica.edu.tw

Abstract

Distributional word representations are
widely used in NLP tasks. These rep-
resentations are based on an assumption
that words with a similar context tend to
have a similar meaning. To improve the
quality of the context-based embeddings,
many researches have explored how to
make full use of existing lexical resources.
In this paper, we argue that while we incor-
porate the prior knowledge with context-
based embeddings, words with different
occurrences should be treated differently.
Therefore, we propose to rely on the mea-
surement of information content to con-
trol the degree of applying prior knowl-
edge into context-based embeddings - dif-
ferent words would have different learning
rates when adjusting their embeddings. In
the result, we demonstrate that our embed-
dings get significant improvements on two
different tasks: Word Similarity and Ana-
logical Reasoning.

1 Introduction

Distributed word representation maps each word
into a real-valued vector. The produced vector
has implied the abstract meaning of the word
for their syntactic (Collobert and Weston, 2008;
Luong et al., 2013; Mnih and Hinton, 2007;
Turian et al., 2010) and semantic (Huang et al.,
2012; Socher et al., 2013b) information. These
vectors have been used as features in a variety of
applications, such as information retrieval (Salton
and McGill, 1984), document classification
(Sebastiani, 2002), question answering (Tellex et
al., 2003), name entity recognition (Turian et al.,
2010), and syntactic parsing (Socher et al., 2013a).

In past few years, several unsupervised meth-
ods for word embeddings (Collobert et al., 2011;
Dhillon et al., 2012; Lebret and Collobert, 2014;
Li and Zhang, 2015; Mikolov et al., 2013a; Pen-
nington et al., 2014) have been proposed and have
had great results in various evaluations. Through
exploiting local context of target words, these
algorithms learn word embeddings by maximizing
the contextual distribution of a large corpus.

Knowledge bases provide rich semantic relat-
edness between words, which are more likely
to capture the desired semantics on certain NLP
tasks. To improve the quality of context-based
embeddings, some researchers attempted to incor-
porate knowledge base, such as WordNet (Miller,
1995) and Paraphrase Database (Ganitkevitch et
al., 2013) into the learning process. Recent work
has shown that aggregating the knowledge base
information into context-based embeddings can
significantly improve the embeddings (Bian et al.,
2014; Chang et al., 2013; Faruqui et al., 2015;
Xu et al., 2014; Yih et al., 2012; Yu and Dredze,
2014).

One implicit but critical reason of the success
on using knowledge bases, based on our insight,
is that knowledge bases can complement the em-
bedding quality of those words which lack enough
statistics of word occurrences, such as enough
occurrences or diversity of their context. These
words may suffer the difficulty obtaining mean-
ingful information from the given corpus. Follow-
ing this idea, we argue that while incorporating
prior knowledge into context-based embeddings,
words with different statistics of word occurrences
should be treated differently. With this idea, we
propose to rely on the measurement of informa-
tion content to control the degree of applying prior
knowledge into context-based embeddings.

509



2 Learning Embeddings

In this section, we will first review word2vec,
a popular context-based embedding approach,
and then introduce Relation Constrained Model
(RCM) to incorporate prior knowledge. Finally
we propose our approach to utilize the both two
models, making words with different statistics of
word occurrences be treated differently while in-
corporating prior knowledge.

2.1 Context-based Embedding
Context-based embedding has two main model
families: global matrix factorization methods,
such as latent semantic analysis (LSA) (Bullinaria
and Levy, 2007; Lebret and Collobert, 2014; Pen-
nington et al., 2014; Rohde et al., 2006) and local
context window methods (Bengio, 2013; Collobert
and Weston, 2008; Mikolov et al., 2013a). Both
training models learn the embedding by using the
statistical information of the word context from
a large corpus. In this paper, we adopt continu-
ous bag-of-word (CBOW) in word2vec (Mikolov
et al., 2013a) as our context-based embedding
model. CBOW is an unsupervised learning al-
gorithm using a neural language models, given a
target word wt and its c neighboring words, the
model is aimed at maximizing the log-likelihood
of each word given its context.

The objective function is shown as following:

J =
1
T

T∑
t=1

log p
(
wt|wt+ct−c

)
(1)

In CBOW, p
(
wt|wt+ct−c

)
defined as:

exp(e′wt
> ·∑−c≤j≤c,j 6=0 ewt+j )∑

w exp(e′w
> ·∑−c≤j≤c,j 6=0 ewt+j ) (2)

where ew and e′w represent the input and output
embeddings respectively.

CBOW use stochastic gradient descent to learn
embeddings, the update of e

′
w and ewj are:

e′w − α(σ(f(w))− I[w=wt]) ·
t+c∑

j=t−c
ewj (3)

ewj − α
∑
w

(σ(f(w))− I[w=wt]) · e′w (4)

where

σ(x) = exp{x}/(1 + exp{x}) (5)
I[x] is 1 when x is true,f(w) = e

′>
w

∑t+c
j=t−c ewj ,

α is learning rate.

2.2 Relation Constrained Model(RCM)
RCM (Yu and Dredze, 2014) designed a simple
but effective method to incorporate prior knowl-
edge into context-based embeddings. Given a set
of relation pairs (w,wi) in a given knowledge
base, by maximizing the log probability of w and
wi, the model aims to increase the similarity be-
tween w and wi. To simplify the formula, we can
define R as a set of relations between w and wi.
Rw is the subset of R which involve word w.

The objective function is shown as following:

J =
1
N

N∑
i=1

∑
w∈Rwi

log p (w|wi) (6)

where

p(w|wi) = exp(e′w>ewi)/
∑
w̄

exp(e′w̄
>
ewi) (7)

The objective function of RCM is similar to the
CBOW but without the context. RCM only revise
output embeddings e′w and e′wi when it trains with
CBOW jointly.

RCM use stochastic gradient descent to learn
embeddings, the update of e

′
w and e

′
wi are:

e′w − α(σ(f ′(w))− I[w∈Rwi ]) · e
′
wi (8)

e′wi − α
∑
w

(σ(f ′(w))− I[w∈Rwi ]) · e
′
w (9)

where

σ(x) = exp{x}/(1 + exp{x}) (10)

I[x] is 1 when x is true, f
′
(w) = e

′
w
>
e
′
wi , α is

the learning rate.

2.3 Information Content Measurement
No matter which kind of context-based embedding
approach, statistics of word occurrences play a pri-
mary role. Under this statement, the embedding
quality of those words which lack enough statistics
of word occurrences, such as enough occurrences
or diversity of their context, may suffer the diffi-
culty obtaining meaningful information from the
given corpus. We argue that while incorporating
prior knowledge into context-based embeddings,
words with different statistics of word occurrences
should be treated differently. With this idea, we
investigate several score functions SIC to adjust
the learning rate, aiming to make words with less

510



statistical information be adjusted more via prior
knowledge, and words with richer statistical infor-
mation be adjusted less.

The update formula of e
′
w and e

′
wi are:

e′w − (SIC(w,wi) ∗ α)(σ(f ′(w))− I[w∈Rwi ]) · e
′
wi (11)

e′wi−(SIC(wi, w)∗α)
∑
w

(σ(f ′(w))−I[w∈Rwi ])·e
′
w (12)

In this paper, we propose three kinds of score
functions to control the adjustment: Threshold,
Function(Freq.), and Function(Ent.).

a. Threshold: The first one is a binary indicator
based on a threshold of word frequency. We can
distinguish the word relations into two groups.

SIC(w,wi) =

{
1, if fw < fthres. and fwi ≥ fthres.
0, otherwise

(13)

This strategy will only revise low frequency
word in a word relation pair, when one word of
the relation word pair has low frequency and the
other has high frequency.

b. Function (Freq.): In contrast to the previous
strategy, we make the score function smoother, we
use a relative value between two words frequen-
cies and a hyperbolic tangent function to deter-
mine the score.

SIC(w,wi) = tanh (
fwi
fw

) (14)

This strategy still can revise relatively lower
frequency word in a word relation pair, when
one word of the relation word pair has relatively
lower frequency and the other has relatively high
frequency. This scoring function is based on our
assumption that if a word has relatively higher
occurrence, its embedding quality is better, so it
does not need to be adjusted much.

c. Function (Ent.): In addition to the word’s
frequency, in fact, we believe that the contextual
diversity plays a critical role of affecting the qual-
ity of word embedding. Therefore, we propose
a score function based on the conditional entropy
(information content) from the information theory.

We define the score function as the follows:

SIC(w,wi) = tanh (
H(C|wi)
H(C|w) ) (15)

H(C|w) =
∑

j

p(cj , w) log
p(w)
p(cj , w)

(16)

where C is a set of all context words of w, and
cj is the jth context word of w.

In here, the occurrence probability of w (de-
noted as p(w)) and the occurrence probability of
w with its context cj (denoted as p(cj , w)) are de-
fined as:

p(w) ≡
∑

cj∈Context(w)
p(cj , w) (17)

p(cj , w) ≡ #(cj , w)∑
w

∑
ck∈Context(w) #(ck, w)

(18)

The output value of this entropy function con-
ditions on two main points. First, as we defined in
Equ. 16, if there’s a high frequency word w, the
output value will be high. Second, for a word w
with many different contextual words, the output
value will be higher. This score function is based
on our assumption that if a word has context with
higher diversity, its embedding quality is supposed
to be better and does not need to be adjusted much.

3 Experiments

We conduct two experiments to evaluate our ap-
proach: Word Similarity and Analogical Reason-
ing. These two experiments directly test the qual-
ity of information embedded in the word vec-
tor. We integrate semantic information from
knowledge bases using the four strategies: Base-
line(Joint), Threshold, Function(Freq.), and Func-
tion(Ent.). We compare our proposed methods un-
der the setting of using both prior knowledge and
context to adjust the embeddings.

3.1 Experiment Setup
3.1.1 Training Data
We use New York Times (NYT) 1994-97 sub-
set from Gigaword v5.0 (Parker et al., 2011) as
the training corpus for CBOW, which is the same
setting as (Yu and Dredze, 2014). After pre-
processing of tokenization, the final training cor-
pus contains 555.4 million tokens. We use two
knowledge bases: Paraphrase Database (PPDB)
(Ganitkevitch et al., 2013) and WordNet (Miller,
1995). For PPDB, we use the XXL package,

511



Resource Method MEN-3k RW WS353 WS353r WS353s Average
CBOW 63.6 33.9 57.7 46.7 68.5 54.1

PPDB

Baseline (Joint) 66.3 36.8 59.6 48.7 70.4 56.4
Threshold 66.0 35.5 60.2 50.2 71.0 56.6

Function (Freq.) 68.7 37.4 61.1 51.8 71.3 58.1
Function (Ent.) 68.6 37.8 60.5 50.3 71.1 57.7

WordNet

Baseline (Joint) 66.4 35.7 59.6 49.9 69.7 56.3
Threshold 66.3 35.5 59.8 49.4 71.2 56.4

Function (Freq.) 66.3 35.4 58.9 49.5 68.9 55.8
Function (Ent.) 66.6 35.2 58.2 48.3 68.1 55.3

Table 1: Spearman rank correlation on word similarity task. All embeddings are 300 dimensions. The
best result for each dataset is highlighted in bold.

which shows the best result in (Yu and Dredze,
2014). It contains 587,439 synonym word pairs.
For WordNet, we extract relation pairs from syn-
onym. It contains 132,046 word pairs.

3.1.2 Parameter Setting

We set all our embedding size to 300, which is a
suitable embedding size mentioned in (Melamud
et al., 2016). The training iteration for RCM is
100. Learning rate for CBOW is 0.025. We exper-
iment on an array of learning rates for the Base-
line(Joint) and the best one is 0.0001. While the
learning rate for Threshold remains 0.0001, we at-
tempt various learning rates for Function(Freq.)
and Function(Ent.) and the best one is 0.001,
which is larger than 0.0001. This setting can be
actually explained by that the output values of the
two functions are between 0 to 1, which is used
to decrease the learning rate. In other words, the
learning rate of the two functions needs to be set
a larger value than the baseline in order to be de-
creased by the two functions.

The Window Size is 5. Negative Sample is
15. We experiment on the threshold values of
10, 50 and 100. In our experiments, 50 gets
the best result. We first learn the embeddings
using CBOW with a random initialization, and
take this pre-trained embeddings to initialize a
joint model, where CBOW and RCM are jointly
trained, and their learning rates are adjusted by us-
ing our proposed functions. Following (Yu and
Dredze, 2014), we use asynchronous stochastic
gradient ascent in training, where the threads to the
CBOW and RCM are set to be a balance of 12:1
and the shared embeddings are updated by each
thread based on training data within the thread.
We let the CBOW threads to control convergence;
training stops when CBOW threads finish process-
ing the data. The joint model without using our
proposed functions is taken as the baseline system,

denoted by Baseline(Joint)

3.2 Word Similarity Task

The aim of word similarity task is to check
whether a given word would have the similarity
score which closely corresponds to human judges.
These datasets contain relatedness scores for pairs
of words; the cosine similarity of the embedding
for two words should have high correlation. We
use five datasets to evaluate: MEN-3k (Bruni et
al., 2014), RW (Luong et al., 2013), WordSim-
353 (Finkelstein et al., 2002), also the parti-
tioned dataset from WordSim-353, separated into
the dataset into two different relations, WS353-
Similarity and WS353-Relatedness (Agirre et al.,
2009; Zesch et al., 2008).

Table 1 shows that comparing to the baseline, all
of our proposed three methods get significant im-
provement. The results support our argument that
incorporating prior knowledge into context-based
embeddings can complement the embedding qual-
ity of those words which lack enough statistics of
word occurrences.

Resource Method Google MSR Avg.
CBOW 43.0 52.0 47.5

PPDB

Baseline (Joint) 46.8 54.9 50.9
Threshold 46.2 54.2 50.2

Function (Freq.) 46.8 55.6 51.2
Function (Ent.) 46.8 55.0 50.9

WordNet

Baseline (Joint) 45.9 53.9 49.9
Threshold 46.3 53.9 50.1

Function (Freq.) 45.8 53.7 49.8
Function (Ent.) 46.6 53.9 50.2

Table 2: Accuracy on analogical reasoning task.
All embeddings are 300 dimensions. The best re-
sult for each dataset is highlighted in bold.

3.3 Analogical Reasoning Task

Analogical reasoning task was popularized by
(Mikolov et al., 2013b). The dataset is composed

512



Resource Method MEN-3k RW WS353 WS353r WS353s Average
CBOW 14.4 9.1 27.7 16.8 37.3 21.1

PPDB

Baseline (Joint) 21.4 9.7 33.6 22.5 41.6 25.8
Threshold 22.7 9.7 34.1 22.2 42.5 26.2

Function (Freq.) 22.4 9.8 33.9 22.7 41.3 26.0
Function (Ent.) 22.2 9.7 34.6 23.7 41.9 26.4

WordNet

Baseline (Joint) 21.4 9.7 33.2 22.5 40.5 25.5
Threshold 22.1 10.0 34.4 23.4 41.5 26.3

Function (Freq.) 22.2 10.1 34.6 23.3 42.5 26.5
Function (Ent.) 22.2 9.8 33.2 21.9 41.4 25.7

Table 3: Spearman rank correlation on word similarity task. All embeddings are 300 dimensions. The
corpus is the same as Table 1, but the size is 1/100. The best result for each dataset is highlighted in bold.

of analogous word pairs. It contains pairs of tu-
ples of word relations that follow a common syn-
tactic relation. The goal of this task is to find a
term c for a given term d so that c:d best resem-
bles a sample relationship a:b. We use the vector
offset method (Levy and Goldberg, 2014; Mikolov
et al., 2013b), computing ed = ea − eb + ec and
returning the vector which has the highest cosine
similarity to ed. We use two datasets, Googles
analogy dataset (Mikolov et al., 2013b), which
contains 19,544 questions, about half of the ques-
tions are syntactic analogies and another half of a
more semantic nature, and MSR analogy dataset
(Mikolov et al., 2013b), which contains 8,000 syn-
tactic analogy questions.

Table 2 shows the similar result as Word Simi-
larity and demonstrates our proposed methods are
stable and can be applied to different tasks.

3.4 Corpus Size

We also apply our models on the corpus with a
smaller size. The same corpus is used but its size is
1/100. All parameters are the same except that the
threads to the CBOW and RCM are set to be a bal-
ance of 2:1, and only the learning rates of positive
samples are adjusted by our functions. The results
are shown in Table 3 and Table 4, which shows
our proposed models also improve the CBOW and
outperform the baseline. In our experiments, we
find out that for a smaller corpus, adjusting the
learning rates of both positive samples and nega-
tive samples can not gain as much improvement as
only using positive samples. Our conjecture is that
since the quality of the embeddings trained from a
smaller corpus might not be as high as the ones
trained from a larger corpus, and the number of
negative samples is much more than the positive
sample (15:1 in our setting) each time, negative
sample with the learning rate adjustment are more
likely to mislead the training for a smaller corpus.

Resource Method Google MSR Avg.
CBOW 3.5 7.5 5.5

PPDB

Baseline (Joint) 4.7 8.9 6.8
Threshold 4.7 9.5 7.1

Function (Freq.) 4.8 9.2 7.0
Function (Ent.) 4.7 9.1 6.9

WordNet

Baseline (Joint) 4.5 8.8 6.7
Threshold 4.6 8.9 6.8

Function (Freq.) 4.8 9.3 7.1
Function (Ent.) 4.8 9.2 7.0

Table 4: Accuracy on analogical reasoning task.
All embeddings are 300 dimensions. The corpus
is the same as Table 2, but the size is 1/100. The
best result for each dataset is highlighted in bold.

4 Conclusion

In this paper, we argue that while applying prior
knowledge into context-based embeddings, statis-
tics of word occurrences should be considered,
which based on the assumption that a embedding
with more contextual information is supposed to
have higher quality, and thus should be treated in
a different way while incorporating with knowl-
edge bases. We propose three models and demon-
strate our embeddings got improved on two differ-
ent tasks: Word Similarity and Analogical Rea-
soning. The implementation is based on RCM
package and we have released the code for aca-
demic use. 1 In the future, under this framework,
we plan to further investigate other possible score
functions of learning rate based on information
theory or dynamic consideration of training pro-
cess for the incorporation of context and knowl-
edge base information.

Acknowledgments

The authors would like to thank the anonymous
reviewers for their constructive suggestions to im-
prove the quality of the paper.

1https://github.com/hywangntut/KBE

513



References

Eneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association of Computational Linguistics -
Human Language Technologies, pages 19–27, Boul-
der, Colorado.

Yoshua Bengio. 2013. Deep learning of representa-
tions: Looking forward. In Proceedings of the Sta-
tistical Language and Speech Processing, pages 1–
37, Tarragona, Spain.

Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Proceedings of the Machine Learning and
Knowledge Discovery in Databases, pages 132–148,
Nancy, France.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artificial Intelligence Research, 49:1–47.

John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510–526.

Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1602–
1612, Seattle, USA.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the International Conference on Ma-
chine Learning, pages 160–167, Helsinki, Finland.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,
and Lyle H. Ungar. 2012. Two step CCA: a
new spectral method for estimating vector models of
words. In Proceedings of the International Confer-
ence on Machine Learning, pages 1551–1558, Edin-
burgh, Scotland.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retrofitting word vectors to semantic lexi-
cons. In Proceedings of the Conference of the North
American Chapter of the Association of Computa-
tional Linguistics - Human Language Technologies,
pages 1606–1615, Denver, Colorado.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: the
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116–131.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: the paraphrase
database. In Proceedings of the Conference of the
North American Chapter of the Association of Com-
putational Linguistics - Human Language Technolo-
gies, pages 758–764, Atlanta, USA.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the Association for
Computational Linguistics, pages 873–882, Jeju,
Korea.

Rémi Lebret and Ronan Collobert. 2014. Word em-
beddings through hellinger PCA. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 482–
490, Gothenburg, Sweden.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the As-
sociation for Computational Linguistics, pages 302–
308, Baltimore, USA.

Ping Li and Cun-Hui Zhang. 2015. Compressed sens-
ing with very sparse gaussian random projections.
In Proceedings of the International Conference on
Artificial Intelligence and Statistics, pages 617–625,
San Diego, USA.

Thang Luong, Richard Socher, and Christopher D.
Manning. 2013. Better word representations with
recursive neural networks for morphology. In Pro-
ceedings of the Conference on Computational Natu-
ral Language Learning, pages 104–113, Sofia, Bul-
garia.

Oren Melamud, David McClosky, Siddharth Patward-
han, and Mohit Bansal. 2016. The role of context
types and dimensionality in learning word embed-
dings. In Proceedings of the Conference of the North
American Chapter of the Association of Computa-
tional Linguistics - Human Language Technologies,
pages 1030–1040, San Diego, USA.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013a. Distributed rep-
resentations of words and phrases and their com-
positionality. In Proceedings of the Conference
on Neural Information Processing Systems, pages
3111–3119, Long Beach, USA.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics - Human Lan-
guage Technologies, pages 746–751, Atlanta, USA.

514



George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39–41.

Andriy Mnih and Geoffrey E. Hinton. 2007. Three
new graphical models for statistical language mod-
elling. In Proceedings of the International Confer-
ence of Machine Learning, pages 641–648, Corval-
lis, Oregon.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edi-
tion.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543, Doha, Qatar.

Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2006. An improved model of
semantic similarity based on lexical co-occurence.
Communications of the ACM, 8:627–633.

Gerard Salton and Michael McGill. 1984. Introduc-
tion to Modern Information Retrieval. McGraw-Hill
Book Company.

Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34:1–47.

Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with compo-
sitional vector grammars. In Proceedings of the As-
sociation for Computational Linguistics, pages 455–
465, Sofia, Bulgaria.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1631–1642, Seattle, USA.

Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In Proceedings of the International
Conference on Research and Development in Infor-
mation Retrieval, pages 41–47, Toronto, Canada.

Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word representations: A simple and gen-
eral method for semi-supervised learning. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 384–394, Uppsala, Sweden.

Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014.
RC-NET: a general framework for incorporating
knowledge into word representations. In Proceed-
ings of the International Conference on Conference
on Information and Knowledge Management, pages
1219–1228, Shanghai, China.

Wen-tau Yih, Geoffrey Zweig, and John C. Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1212–1222,
Jeju, Korea.

Mo Yu and Mark Dredze. 2014. Improving lexical
embeddings with semantic knowledge. In Proceed-
ings of the Association for Computational Linguis-
tics, pages 545–550, Baltimore, USA.

Torsten Zesch, Christof Müller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of the Conference on Arti-
ficial Intelligence, pages 861–866, Chicago, Illinois.

515


