



















































A Discriminative Topic Model using Document Network Structure


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 686–696,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

A Discriminative Topic Model using Document Network Structure

Weiwei Yang
Computer Science

University of Maryland
College Park, MD

wwyang@cs.umd.edu

Jordan Boyd-Graber
Computer Science

University of Colorado
Boulder, CO

Jordan.Boyd.Graber@
colorado.edu

Philip Resnik
Linguistics and UMIACS
University of Maryland

College Park, MD
resnik@umd.edu

Abstract

Document collections often have links be-
tween documents—citations, hyperlinks,
or revisions—and which links are added is
often based on topical similarity. To model
these intuitions, we introduce a new topic
model for documents situated within a net-
work structure, integrating latent blocks
of documents with a max-margin learning
criterion for link prediction using topic-
and word-level features. Experiments on
a scientific paper dataset and collection
of webpages show that, by more robustly
exploiting the rich link structure within a
document network, our model improves
link prediction, topic quality, and block
distributions.

1 Introduction

Documents often appear within a network struc-
ture: social media mentions, retweets, and fol-
lower relationships; Web pages by hyperlinks; sci-
entific papers by citations. Network structure in-
teracts with the topics in the text, in that docu-
ments linked in a network are more likely to have
similar topic distributions. For instance, a cita-
tion link between two papers suggests that they
are about a similar field, and a mentioning link
between two social media users often indicates
common interests. Conversely, documents’ sim-
ilar topic distributions can suggest links between
them. For example, topic model (Blei et al., 2003,
LDA) and block detection papers (Holland et al.,
1983) are relevant to our research, so we cite them.
Similarly, if a social media user A finds another
user B with shared interests, then A is more likely
to follow B.

Our approach is part of a natural progression
of network modeling in which models integrate

more information in more sophisticated ways.
Some past methods only consider the network it-
self (Kim and Leskovec, 2012; Liben-Nowell and
Kleinberg, 2007), which loses the rich information
in text. In other cases, methods take both links and
text into account (Chaturvedi et al., 2012), but they
are modeled separately, not jointly, limiting the
model’s ability to capture interactions between the
two. The relational topic model (Chang and Blei,
2010, RTM) goes further, jointly modeling topics
and links, but it considers only pairwise document
relationships, failing to capture network structure
at the level of groups or blocks of documents.

We propose a new joint model that makes fuller
use of the rich link structure within a document
network. Specifically, our model embeds the
weighted stochastic block model (Aicher et al.,
2014, WSBM) to identify blocks in which docu-
ments are densely connected. WSBM basically cat-
egorizes each item in a network probabilistically
as belonging to one of L blocks, by reviewing
its connections with each block. Our model can
be viewed as a principled probabilistic extension
of Yang et al. (2015), who identify blocks in a doc-
ument network deterministically as strongly con-
nected components (SCC). Like them, we assign a
distinct Dirichlet prior to each block to capture its
topical commonalities. Jointly, a linear regression
model with a discriminative, max-margin objec-
tive function (Zhu et al., 2012; Zhu et al., 2014) is
trained to reconstruct the links, taking into account
the features of documents’ topic and word distri-
butions (Nguyen et al., 2013), block assignments,
and inter-block link rates.

We validate our approach on a scientific pa-
per abstract dataset and collection of webpages,
with citation links and hyperlinks respectively, to
predict links among previously unseen documents
and from those new documents to training docu-
ments. Embedding the WSBM in a network/topic

686






LL

a

b

y

A
DD

D

Figure 1: Weighted Stochastic Block Model

model leads to substantial improvements in link
prediction over previous models; it also improves
block detection and topic interpretability. The key
advantage in embedding WSBM is its flexibility
and robustness in the face of noisy links. Our re-
sults also lend additional support for using max-
margin learning for a “downstream” supervised
topic model (McAuliffe and Blei, 2008), and that
predictions from lexical as well as topic features
improves performance (Nguyen et al., 2013).

The rest of this paper is organized as follows.
Section 2 introduces two previous link-modeling
methods, WSBM and RTM. Section 3 presents our
methods to incorporate block priors in topic mod-
eling and include various features in link predic-
tion, as well as the aggregated discriminative topic
model whose posterior inference is introduced in
Section 4. In Section 5 we show how our model
can improve link prediction and (often) improve
topic coherence.

2 Dealing with Links

2.1 Weighted Stochastic Block Model
WSBM (Aicher et al., 2014) is a generalized
stochastic block model (Holland et al., 1983;
Wang and Wong, 1987, SBM) and predicts non-
negative integer-weight links, instead of binary-
weight links. A block is a collection of doc-
uments which are densely connected with each
other but sparsely connected with documents in
other blocks. WSBM assumes that a document be-
longs to exactly one block. A link connecting two
documents in blocks l and l′ has a weight gen-
erated from a Poisson distribution with parame-
ters Ωl,l′ which has a Gamma prior with param-
eters a and b, as Figure 1 shows.

The whole generative process is:

1. For each pair of blocks (l, l′) ∈ {1, . . . , L}2
(a) Draw inter-block link rate Ωl,l′ ∼ Gamma(a, b)

2. Draw block distribution µ ∼ Dir(γ)
3. For each document d ∈ {1, . . . , D}

(a) Draw block assignment yd ∼ Mult(µ)

Figure 2: SCC can be distracted by spurious links
connecting two groups, while WSBM maintains the
distinction.

 
K



'dN

dN
d

'd

',ddB

dz dw

'dz 'dw



Figure 3: A Two-document Segment of RTM

4. For each link (d, d′) ∈ {1, . . . , D}2
(a) Draw link weight Ad,d′ ∼ Poisson(Ωyd,yd′ )

WSBM is a probabilistic block detection algo-
rithm and more robust than some deterministic al-
gorithms like SCC, which is vulnerable to noisy
links. For instance, we would intuitively say Fig-
ure 2 has two blocks—as denoted by coloring—
whether or not the dashed link exists. If the dashed
link does not exist, both WSBM and SCC can iden-
tify two blocks. However, if the dashed link does
exist, SCC will return only one big block that con-
tains all nodes, while WSBM still keeps the nodes
in two reasonable blocks.

2.2 Relational Topic Model
RTM (Chang and Blei, 2010) is a downstream
model that generates documents and links simul-
taneously (Figure 3). Its generative process is:

1. For each topic k ∈ {1, . . . ,K}
(a) Draw word distribution φk ∼ Dir(β)
(b) Draw topic regression parameter ηk ∼ N (0, ν2)

2. For each document d ∈ {1, . . . , D}
(a) Draw topic distribution θd ∼ Dir(α)
(b) For each token td,n in document d

i. Draw topic assignment zd,n ∼ Mult(θd)
ii. Draw word wd,n ∼ Mult(φzd,n)

3. For each explicit link (d, d′)
(a) Draw link weight Bd,d′ ∼ Ψ(· | zd,zd′ ,η)

In the inference process, the updating of topic
assignments is guided by links so that linked doc-
uments are more likely to have similar topic distri-
butions. Meanwhile, the linear regression (whose

687





'
K


L



dN

D

 z w

y

Figure 4: Graphical Model of BP-LDA

output is fed into link probability function Ψ) is
updated to maximize the network likelihood using
current topic assignments.

3 Discriminative Topic Model with Block
Prior and Various Features

Our model is able to identify blocks from the net-
work with an embedded WSBM, extract topic pat-
terns of each block as prior knowledge, and use all
this information to reconstruct the links.

3.1 LDA with Block Priors (BP-LDA)

As argued in the introduction, linked documents
are likely to have similar topic distributions, which
can be generalized to documents in the same
block. Inspired by this intuition and the block
assignment we obtain in the previous section, we
want to extract some prior knowledge from these
blocks. Thus we propose an LDA with block
priors, hence BP-LDA, as shown in Figure 4, which
has the following generative process:

1. For each topic k ∈ {1, . . . ,K}
(a) Draw word distribution φk ∼ Dir(β)

2. For each block l ∈ {1, . . . , L}
(a) Draw topic distribution πl ∼ Dir(α′)

3. For each document d ∈ {1, . . . , D}
(a) Draw topic distribution θd ∼ Dir(απyd)
(b) For each token td,n in document d

i. Draw topic assignment zd,n ∼ Mult(θd)
ii. Draw word wd,n ∼ Mult(φzd,n)

Unlike conventional LDA, which uses an un-
informative topic prior, BP-LDA puts a Dirich-
let prior π on each block to capture the block’s
topic distribution and use it as an informative prior
when drawing each document’s topic distribution.
In other words, a document’s topic distribution—
i.e., what the document is about—is not just in-
formed by the words present in the document but
the broader context of its network neighborhood.





K



'dN

dN
d

'd


LL



dw

'dw

dz

'dz


dy

'dy





',ddB

Topical Feature Lexical Feature Block Feature

Figure 5: A two-document segment of VF-RTM.
Various features are denoted by grayscale. Bd,d′ is
observed, but we keep it in white background to
avoid confusion.

3.2 RTM with Various Features (VF-RTM)

Building on Chang and Blei (2010), we want to
generate the links between documents based on
various features, hence VF-RTM. In addition to
topic distributions, VF-RTM also includes docu-
ments’ word distributions (Nguyen et al., 2013)
and the link rate of two documents’ assigned
blocks, with the intent that these additional fea-
tures improve link generation. VF-RTM involves
the relationship between a pair of documents, so
it is difficult to show the whole model; therefore
Figure 5 illustrates with a two-document segment.
The generative process is:

1. For each pair of blocks (l, l′) ∈ {1, . . . , L}2
(a) Draw block regression parameter ρl,l′ ∼ N (0, ν2)

2. For each topic k ∈ {1, . . . ,K}
(a) Draw word distribution φk ∼ Dir(β)
(b) Draw topic regression parameter ηk ∼ N (0, ν2)

3. For each word v ∈ {1, . . . , V }
(a) Draw lexical regression parameter τv ∼ N (0, ν2)

4. For each document d ∈ {1, . . . , D}
(a) Draw topic distribution θd ∼ Dir(α)
(b) For each token td,n in document d

i. Draw topic assignment zd,n ∼ Mult(θd)
ii. Draw word wd,n ∼ Mult(φzd,n)

5. For each explicit link (d, d′)
(a) Draw link weight

Bd,d′ ∼ Ψ(· | yd, yd′ ,Ω,zd,zd′ ,wd,wd′ ,η, τ ,ρ)

Links are generated by a link probability func-
tion Ψ which takes the regression value Rd,d′ of
documents d and d′ as an argument. Assuming
documents d and d′ belong to blocks l and l′ re-
spectively, Rd,d′ is

Rd,d′ = η
T(zd ◦ zd′) + τT(wd ◦wd′) + ρl,l′Ωl,l′ , (1)

688



where zd is a K-length vector with each el-
ement zd,k = 1Nd

∑
n 1 (zd,n = k); wd is a

V -length vector with each element wd,v =
1

Nd

∑
n 1 (wd,n = v); ◦ denotes the Hadamard

(element-wise) product;1 η, τ , and ρ are the
weight vectors and matrix for topic-based, lexical-
based and rate-based predictions, respectively.

A common choice of Ψ is a sigmoid (Chang
and Blei, 2010). However, we instead use hinge
loss so that VF-RTM can use the max-margin prin-
ciple, making more effective use of side informa-
tion when inferring topic assignments (Zhu et al.,
2012). Using hinge loss, the probability that doc-
uments d and d′ are linked is

Pr (Bd,d′) = exp (−2 max(0, ζd,d′)) , (2)

where ζd,d′ = 1−Bd,d′Rd,d′ . Positive and negative
link weights are denoted by 1 and -1, respectively,
in contrast to sigmoid loss.

3.3 Aggregated Model
Finally, we put all the pieces together and propose
LBH-RTM: RTM with lexical weights (L), block
priors (B), and hinge loss (H). Its graphical model
is given in Figure 6.

1. For each pair of blocks (l, l′) ∈ {1, . . . , L}2
(a) Draw inter-block link rate Ωl,l′ ∼ Gamma(a, b)
(b) Draw block regression parameter ρl,l′ ∼ N (0, ν2)

2. Draw block distribution µ ∼ Dir(γ)
3. For each block l ∈ {1, . . . , L}

(a) Draw topic distribution πl ∼ Dir(α′)
4. For each topic k ∈ {1, . . . ,K}

(a) Draw word distribution φk ∼ Dir(β)
(b) Draw topic regression parameter ηk ∼ N (0, ν2)

5. For each word v ∈ {1, . . . , V }
(a) Draw lexical regression parameter τv ∼ N (0, ν2)

6. For each document d ∈ {1, . . . , D}
(a) Draw block assignment yd ∼ Mult(µ)
(b) Draw topic distribution θd ∼ Dir(απyd)
(c) For each token td,n in document d

i. Draw topic assignment zd,n ∼ Mult(θd)
ii. Draw word wd,n ∼ Mult(φzd,n)

7. For each link (d, d′) ∈ {1, . . . , D}2
(a) Draw link weight Ad,d′ ∼ Poisson(Ωyd,yd′ )

8. For each explicit link (d, d′)
(a) Draw link weight

Bd,d′ ∼ Ψ(· | yd, yd′ ,Ω,zd,zd′ ,wd,wd′ ,η, τ ,ρ)

A and B are assumed independent in the
model, but they can be derived from the same set
of links in practice.

1As Chang and Blei (2010) point out, the Hadamard prod-
uct is able to capture similarity between hidden topic repre-
sentations of two documents.

Algorithm 1 Sampling Process
1: Set λ = 1 and initialize topic assignments
2: for m = 1 to M do
3: Optimize η, τ , and ρ using L-BFGS
4: for d = 1 to D do
5: Draw block assignment yd
6: for each token n do
7: Draw a topic assignment zd,n
8: end for
9: for each explicit link (d, d′) do

10: Draw λ−1d,d′ (and then λd,d′)
11: end for
12: end for
13: end for

Link set A is primarily used to find blocks, so
it treats all links deterministically. In other words,
the links observed in the input are considered ex-
plicit positive links, while the unobserved links are
considered explicit negative links, in contrast to
the implicit links inB.

In terms of link setB, while it adopts all explicit
positive links from the input, it does not deny the
existence of unobserved links, or implicit negative
links. Thus B consists of only explicit positive
links. However, to avoid overfitting, we sample
some implicit links and add them to B as explicit
negative links.

4 Posterior Inference

Posterior inference (Algorithm 1) consists of the
sampling of topic and block assignments and the
optimization of weight vectors and matrix.2 We
add an auxiliary variable λ for hinge loss (see Sec-
tion 4.2), so the updating of λ is not necessary
when using sigmoid loss.

The sampling procedure is an iterative process
after initialization (Line 1). In each iteration,
we first optimize the weight vectors and matrix
(Line 3) before updating documents’ block assign-
ments (Line 5) and topic assignments (Line 7).
When using hinge loss, the auxiliary variableλ for
every explicit link needs to be updated (Line 10).

4.1 Sampling Block Assignments

Block assignment sampling is done by Gibbs sam-
pling, using the block assignments and links in A

2More details about sampling procedures and equations in
this section (including the sampling and optimization equa-
tions using sigmoid loss) are available in the supplementary
material.

689







'


K



L
 

LL

'dN

dN
d

'd

a

b

dy

'dy

',ddA ',ddB

dz dw

'dz 'dw









Figure 6: The graphical model of LBH-RTM for two documents, in which a weighted stochastic block
model is embedded (γ, µ, y, a, b, Ω, and A). Each document’s topic distribution has an informative
prior π. The model predicts links between documents (B) based on topics (z), words (w), and inter-
block link rates (Ω), using a max-margin objective.

excluding document d and its related links.3 The
probability that d is assigned to block l is

Pr(yd = l |A−d,y−d, a, b, γ) ∝
(
N−dl + γ

)
×

∏
l′

(
S−de (l, l

′) + b
)S−dw (l,l′)+a(

S−de (l, l′) + b+ Se(d, l′)
)S−dw (l,l′)+a+Sw(d,l′)

Sw(d,l
′)−1∏

i=0

(
S−dw (l, l

′) + a+ i
)
, (3)

where Nl is the number of documents assigned to
block l; −d denotes that the count excludes doc-
ument d; Sw(d, l) and Sw(l, l′) are the sums of
link weights from document d to block l and from
block l to block l′, respectively:

Sw(d, l) =
∑

d′:yd′=l

Ad,d′ (4)

Sw(l, l
′) =

∑
d:yd=l

Sw(d, l
′). (5)

Se(d, l) is the number of possible links from doc-
ument d to l (i.e., assuming document d connects
to every document in block l), which equals Nl.
The number of possible links from block l to l′

is Se(l, l′) (i.e., assuming every document in
block l connects to every document in block l′):

Se(l, l
′) =

{
Nl ×Nl′ l 6= l′

1
2
Nl(Nl − 1) l = l′. (6)

If we rearrange the terms of Equation 3 and put
the terms which have Sw(d, l′) together, the value

3These equations deal with undirected edges, but they can
be adapted for directed edges. See supplementary material.

of Sw(d, l′) increases (i.e., document d is more
densely connected with documents in block l′), the
probability of assigning d to block l decreases ex-
ponentially. Thus if d is more densely connected
with block l and sparsely connected with other
blocks, it is more likely to be assigned to block l.

4.2 Sampling Topic Assignments
Following Polson and Scott (2011), by introducing
an auxiliary variable λd,d′ , the conditional prob-
ability of assigning td,n, the n-th token in docu-
ment d, to topic k is

Pr(zd,n = k |z−d,n,w−d,n, wd,n = v, yd = l)

∝
(
N−d,nd,k + απ

−d,n
l,k

) N−d,nk,v + β
N−d,nk,· + V β∏

d′
exp

(
− (ζd,d′ + λd,d′)

2

2λd,d′

)
, (7)

whereNd,k is the number of tokens in document d
that are assigned to topic k;Nk,v denotes the count
of word v assigned to topic k; Marginal counts
are denoted by ·; −d,n denotes that the count ex-
cludes td,n; d′ denotes all documents that have
explicit links with document d. The block topic
prior π−d,nl,k is estimated based on the maximal
path assumption (Cowans, 2006; Wallach, 2008):

π−d,nl,k =

∑
d′:yd′=l

N−d,nd′,k + α
′∑

d′:yd′=l
N−d,nd′,· +Kα

′ . (8)

the link prediction argument ζd,d′ is

ζd,d′ = 1−Bd,d′
(
ηk
Nd,·

Nd′,k
Nd′,·

+R−d,nd,d′

)
. (9)

690



where

R−d,nd,d′ =
K∑

k=1

ηk
N−d,nd,k
Nd,·

Nd′,k
Nd′,·

+

V∑
v=1

τv
Nd,v
Nd,·

Nd′,v
Nd′,·

+ ρyd,yd′Ωyd,yd′ . (10)

Looking at the first term of Equation 7, the
probability of assigning td,n to topic k depends
not only on its own topic distribution, but also the
topic distribution of the block it belongs to. The
links also matter: Equation 9 gives us the intuition
that a topic which could increase the likelihood of
links is more likely to be selected, which forms
an interaction between topics and the link graph—
the links are guiding the topic sampling while up-
dating topic assignments is maximizing the likeli-
hood of the link graph.

4.3 Parameter Optimization
While topic assignments are updated iteratively,
the weight vectors and matrix η, τ , and ρ are
optimized in each global iteration over the whole
corpus using L-BFGS (Liu and Nocedal, 1989). It
takes the likelihood of generatingB using η, τ , ρ,
and current topic and block assignments as the ob-
jective function, and optimizes it using the par-
tial derivatives with respect to every weight vec-
tor/matrix element.

The log likelihood ofB using hinge loss is

L(B) ∝−
∑
d,d′

R2d,d′ − 2(1 + λd,d′)Bd,d′Rd,d′
2λd,d′

−
K∑

k=1

η2k
2ν2
−

V∑
v=1

τ2v
2ν2
−

L∑
l=1

L∑
l′=1

ρ2l,l′

2ν2
. (11)

We also need to update the auxiliary vari-
able λd,d′ . Since the likelihood of λd,d′ fol-
lows a generalized inverse Gaussian distribution
GIG

(
λd,d′ ; 12 , 1, ζ

2
d,d′

)
, we sample its recipro-

cal λ−1d,d′ from an inverse Gaussian distribution as

Pr
(
λ−1d,d′ |z,w,η, τ ,ρ

)
= IG

(
λ−1d,d′ ;

1

|ζd,d′ | , 1
)
. (12)

5 Experimental Results

We evaluate using the two datasets. The first one is
CORA dataset (McCallum et al., 2000). After re-
moving stopwords and words that appear in fewer
than ten documents, as well as documents with no

Model PLRCORA WEBKB
RTM (Chang and Blei, 2010) 419.33 141.65
LCH-RTM (Yang et al., 2015) 459.55 150.32

BS-RTM 391.88 127.25
LBS-RTM 383.25 125.41
LBH-RTM 360.38 111.79

Table 1: Predictive Link Rank Results

words or links, our vocabulary has 1,240 unique
words. The corpus has 2,362 computer science pa-
per abstracts with 4,231 citation links.

The second dataset is WEBKB. It is already pre-
processed and has 1,703 unique words in vocabu-
lary. The corpus has 877 web pages with 1,608
hyperlinks.

We treat all links as undirected. Both datasets
are split into 5 folds, each further split into a devel-
opment and test set with approximately the same
size when used for evaluation.

5.1 Link Prediction Results

In this section, we evaluate LBH-RTM and its varia-
tions on link prediction tasks using predictive link
rank (PLR). A document’s PLR is the average rank
of the documents to which it has explicit positive
links, among all documents, so lower PLR is better.

Following the experiment setup in Chang and
Blei (2010), we train the models on the train-
ing set and predict citation links within held-out
documents as well as from held-out documents
to training documents. We tune two important
parameters—α and negative edge ratio (the ratio
of the number of sampled negative links to the
number of explicit positive links)—on the devel-
opment set and apply the trained model which per-
forms the best on the development set to the test
set.4 The cross validation results are given in Ta-
ble 1, where models are differently equipped with
lexical weights (L), WSBM prior (B), SCC prior (C),
hinge loss (H), and sigmoid loss (S).5 Link pre-
diction generally improves with incremental appli-
cation of prior knowledge and more sophisticated
learning techniques.

The embedded WSBM brings around 6.5% and
10.2% improvement over RTM in PLR on the

4We also tune the number of blocks for embedded WSBM
and set it to 35 (CORA) and 20 (WEBKB). The block topic
priors are not applied on unseen documents, since we don’t
have available links.

5The values of RTM are different from the result reported
by Chang and Blei (2010), because we re-preprocessed the
CORA dataset and used different parameters.

691



CORA and WEBKB datasets, respectively. This
indicates that the blocks identified by WSBM are
reasonable and consistent with reality. The lexi-
cal weights also help link prediction (LBS-RTM),
though less for BS-RTM. This is understandable
since word distributions are much sparser and do
not make as significant a contribution as topic dis-
tributions. Finally, hinge loss improves PLR sub-
stantially (LBH-RTM), about 14.1% and 21.1% im-
provement over RTM on the CORA and WEBKB
datasets respectively, demonstrating the effective-
ness of max-margin learning.

The only difference between LCH-RTM and
LBH-RTM is the block detection algorithm. How-
ever, their link prediction performance is poles
apart—LCH-RTM even fails to outperform RTM.
This implies that the quality of blocks identified
by SCC is not as good as WSBM, which we also
illustrate in Section 5.4.

5.2 Illustrative Example

We illustrate our model’s behavior qualitatively
by looking at two abstracts, Koplon and Sontag
(1997) and Albertini and Sontag (1992) from the
CORA dataset, designated K and A for short.

Paper K studies the application of Fourier-type
activation functions in fully recurrent neural net-
works. Paper A shows that if two neural networks
have equal behaviors as “black boxes”, they must
have the same number of neurons and the same
weights (except sign reversals).

From the titles and abstracts, we can easily find
that both of them are about neural networks (NN).
They both contain words like neural, neuron, net-
work, recurrent, activation, and nonlinear, which
corresponds to the topic with words neural, net-
work, train, learn, function, recurrent, etc. There
is a citation between K and A. The ranking of this
link improves as the model gets more sophisti-
cated (Table 2), except LCH-RTM, which is con-
sistent with our PLR results.

In Figure 7, we also show the proportions of
topics that dominate the two documents accord-
ing to the various models. There are multiple top-
ics dominating K and A according to RTM (Fig-
ure 7(a)). As the model gets more sophisticated,
the NN topic proportion gets higher. Finally, only
the NN topic dominates the two documents when
LBH-RTM is applied (Figure 7(e)).

LCH-RTM gives the highest proportion to the
NN topic (Figure 7(b)). However, the NN topic

Model Rank of the Link
RTM 1,265

LCH-RTM 1,385
BS-RTM 635

LBS-RTM 132
LBH-RTM 106

Table 2: PLR of the citation link between example
documents K and A (described in Section 5.2)

Model FET LLRCORA WEBKB CORA WEBKB
RTM 0.1330 0.1312 3.001 6.055

LCH-RTM 0.1418 0.1678 3.071 6.577
BS-RTM 0.1415 0.1950 3.033 6.418

LBS-RTM 0.1342 0.1963 2.984 6.212
LBH-RTM 0.1453 0.2628 3.105 6.669

Table 3: Average Association Scores of Topics

splits into two topics and the proportions are not
assigned to the same topic, which greatly brings
down the link prediction performance. The split-
ting of the NN topic also happens in other mod-
els (Figure 7(a) and 7(d)), but they assign propor-
tions to the same topic(s). Further comparing with
LBH-RTM, the blocks detected by SCC are not im-
proving the modeling of topics and links—some
documents that should be in two different blocks
are assigned to the same one, as we will show in
Section 5.4.

5.3 Topic Quality Results

We use an automatic coherence detection
method (Lau et al., 2014) to evaluate topic quality.
Specifically, for each topic, we pick out the top n
words and compute the average association score
of each pair of words, based on the held-out
documents in development and test sets.

We choose n = 25 and use Fisher’s exact
test (Upton, 1992, FET) and log likelihood ra-
tio (Moore, 2004, LLR) as the association mea-
sures (Table 3). The main advantage of these mea-
sures is that they are robust even when the refer-
ence corpus is not large.

Coherence improves with WSBM and max-
margin learning, but drops a little when adding
lexical weights except the FET score on the WE-
BKB dataset, because lexical weights are intended
to improve link prediction performance, not topic
quality. Topic quality of LBH-RTM is also better
than that of LCH-RTM, suggesting that WSBM ben-
efits topic quality more than SCC.

692



0.0 0.2 0.4 0.6 0.8 1.0

NN-1

NN-2

Sequential Model

Vision

Belief Network

Knowledge Base

Parallel Computing

A K

(a) RTM Topic Proportions

0.0 0.2 0.4 0.6 0.8 1.0

NN-1

NN-2

Sequential Model

Algorithm Bound

A K

(b) LCH-RTM Topic Proportions

0.0 0.2 0.4 0.6 0.8 1.0

NN

System Behavior

Research Grant

Optimization-1

Optimization-2

A K

(c) BS-RTM Topic Proportions

0.0 0.2 0.4 0.6 0.8 1.0

NN-1

NN-2

Random Process

Optimization

Evolutionary Comput.

A K

(d) LBS-RTM Topic Proportions

0.0 0.2 0.4 0.6 0.8 1.0

NN

Bayesian Network

Linear Function

MCMC

A K

(e) LBH-RTM Topic Proportions

Figure 7: Topic proportions given by various models on our two illustrative documents (K and A, de-
scribed in described in Section 5.2). As the model gets more sophisticated, the NN topic proportion gets
higher and finally dominates the two documents when LBH-RTM is applied. Though LCH-RTM gives the
highest proportion to the NN topic, it splits the NN topic into two and does not assign the proportions to
the same one.

Block 1 2
#Nodes 42 84

#Links in the Block 55 142
#Links across Blocks 2

Table 4: Statistics of Blocks 1 (learning theory)
and 2 (Bayes nets), which are merged in SCC.

5.4 Block Analysis
In this section, we illustrate the effectiveness of
the embedded WSBM over SCC.6 As we have
argued, WSBM is able to separate two internally
densely-connected blocks even if there are few
links connecting them, while SCC tends to merge
them in this case. As an example, we focus
on two blocks in the CORA dataset identified by
WSBM, designated Blocks 1 and 2. Some statis-
tics are given in Table 4. The two blocks are
very sparsely connected, but comparatively quite
densely connected inside either block. The two
blocks’ topic distributions also reveal their differ-
ences: abstracts in Block 1 mainly focus on learn-
ing theory (learn, algorithm, bound, result, etc.)
and MCMC (markov, chain, distribution, converge,
etc.). Abstracts in Block 2, however, have higher

6We omit the comparison of WSBM with other models, be-
cause this has been done by Aicher et al. (2014). In addition,
WSBM is a probabilistic method while SCC is deterministic.
They are not comparable quantitatively, so we compare them
qualitatively.

weights on Bayesian networks (network, model,
learn, bayesian, etc.) and Bayesian estimation (es-
timate, bayesian, parameter, analysis, etc.), which
differs from Block 1’s emphasis. Because of the
two inter-block links, SCC merges the two blocks
into one, which makes the block topic distribution
unclear and misleads the sampler. WSBM, on the
other hand, keeps the two blocks separate, which
generates a high-quality prior for the sampler.

6 Related Work

Topic models are widely used in information re-
trieval (Wei and Croft, 2006), word sense dis-
ambiguation (Boyd-Graber et al., 2007), dialogue
segmentation (Purver et al., 2006), and collabora-
tive filtering (Marlin, 2003).

Topic models can be extended in either up-
stream or downstream way. Upstream models
generate topics conditioned on supervisory in-
formation (Daumé III, 2009; Mimno and Mc-
Callum, 2012; Li and Perona, 2005). Down-
stream models, on the contrary, generates topics
and supervisory data simultaneously, which turns
unsupervised topic models to (semi-)supervised
ones. Supervisory data, like labels of documents
and links between documents, can be generated
from either a maximum likelihood estimation ap-
proach (McAuliffe and Blei, 2008; Chang and

693



Blei, 2010; Boyd-Graber and Resnik, 2010) or a
maximum entropy discrimination approach (Zhu
et al., 2012; Yang et al., 2015).

In block detection literature, stochastic block
model (Holland et al., 1983; Wang and Wong,
1987, SBM) is one of the most basic generative
models dealing with binary-weighted edges. SBM
assumes that each node belongs to only one block
and each link exists with a probability that de-
pends on the block assignments of its connect-
ing nodes. It has been generalized for degree-
correction (Karrer and Newman, 2011), bipartite
structure (Larremore et al., 2014), and categorial
values (Guimerà and Sales-Pardo, 2013), as well
as nonnegative integer-weight network (Aicher et
al., 2014, WSBM).

Our model combines both topic model and
block detection in a unified framework. It takes
text, links, and the interaction between text and
links into account simultaneously, contrast to the
methods that only consider graph structure (Kim
and Leskovec, 2012; Liben-Nowell and Kleinberg,
2007) or separate text and links (Chaturvedi et al.,
2012).

7 Conclusions and Future Work

We introduce LBH-RTM, a discriminative topic
model that jointly models topics and document
links, detecting blocks in the document net-
work probabilistically by embedding the weighted
stochastic block model, rather via connected-
components as in previous models. A separate
Dirichlet prior for each block captures its topic
preferences, serving as an informed prior when
inferring documents’ topic distributions. Max-
margin learning learns to predict links from docu-
ments’ topic and word distributions and block as-
signments.

Our model better captures the connections and
content of paper abstracts, as measured by predic-
tive link rank and topic quality. LBH-RTM yields
topics with better coherence, though not all tech-
niques contribute to the improvement. We sup-
port our quantitative results with qualitative anal-
ysis looking at a pair of example documents and
at a pair of blocks, highlighting the robustness of
embedded WSBM over blocks defined as SCC.

As next steps, we plan to explore model varia-
tions to support a wider range of use cases. For
example, although we have presented a version of
the model defined using undirected binary weight

edges in the experiment, it would be straightfor-
ward to adapt to model both directed/undirected
and binary/nonnegative real weight edges. We are
also interested in modeling changing topics and
vocabularies (Blei and Lafferty, 2006; Zhai and
Boyd-Graber, 2013). In the spirit of treating links
probabilistically, we plan to explore application
of the model in suggesting links that do not ex-
ist but should, for example in discovering missed
citations, marking social dynamics (Nguyen et al.,
2014), and identifying topically related content in
multilingual networks of documents (Hu et al.,
2014).

Acknowledgment

This research has been supported in part, under
subcontract to Raytheon BBN Technologies, by
DARPA award HR0011-15-C-0113. Boyd-Graber
is also supported by NSF grants IIS/1320538,
IIS/1409287, and NCSE/1422492. Any opinions,
findings, conclusions, or recommendations ex-
pressed here are those of the authors and do not
necessarily reflect the view of the sponsors.

References
Christopher Aicher, Abigail Z. Jacobs, and Aaron

Clauset. 2014. Learning latent block structure in
weighted networks. Journal of Complex Networks.

Francesca Albertini and Eduardo D. Sontag. 1992. For
neural networks, function determines form. In Pro-
ceedings of IEEE Conference on Decision and Con-
trol.

David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the International
Conference of Machine Learning.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research.

Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent Dirichlet allocation. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.

Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proceedings of Empirical Methods in Natu-
ral Language Processing.

Jonathan Chang and David M. Blei. 2010. Hierarchi-
cal relational models for document networks. The
Annals of Applied Statistics.

694



Snigdha Chaturvedi, Hal Daumé III, Taesun Moon, and
Shashank Srivastava. 2012. A topical graph kernel
for link prediction in labeled graphs. In Proceedings
of the International Conference of Machine Learn-
ing.

Philip J. Cowans. 2006. Probabilistic Document Mod-
elling. Ph.D. thesis, University of Cambridge.

Hal Daumé III. 2009. Markov random topic fields.
In Proceedings of the Association for Computational
Linguistics.

Roger Guimerà and Marta Sales-Pardo. 2013. A net-
work inference method for large-scale unsupervised
identification of novel drug-drug interactions. PLoS
Computational Biology.

Paul W. Holland, Kathryn Blackmond Laskey, and
Samuel Leinhardt. 1983. Stochastic blockmodels:
First steps. Social Networks.

Yuening Hu, Ke Zhai, Vlad Eidelman, and Jordan
Boyd-Graber. 2014. Polylingual tree-based topic
models for translation domain adaptation. In Pro-
ceedings of the Association for Computational Lin-
guistics.

Brian Karrer and Mark EJ Newman. 2011. Stochastic
blockmodels and community structure in networks.
Physical Review E.

Myunghwan Kim and Jure Leskovec. 2012. La-
tent multi-group membership graph model. In Pro-
ceedings of the International Conference of Machine
Learning.

Renée Koplon and Eduardo D. Sontag. 1997. Using
Fourier-neural recurrent networks to fit sequential
input/output data. Neurocomputing.

Daniel B. Larremore, Aaron Clauset, and Abigail Z. Ja-
cobs. 2014. Efficiently inferring community struc-
ture in bipartite networks. Physical Review E.

Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In Proceedings of the Association for Computational
Linguistics.

Fei-Fei Li and Pietro Perona. 2005. A Bayesian hier-
archical model for learning natural scene categories.
In Computer Vision and Pattern Recognition.

David Liben-Nowell and Jon Kleinberg. 2007. The
link-prediction problem for social networks. Jour-
nal of the American Society for Information Science
and Technology.

Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming.

Benjamin Marlin. 2003. Modeling user rating profiles
for collaborative filtering. In Proceedings of Ad-
vances in Neural Information Processing Systems.

Jon D. McAuliffe and David M. Blei. 2008. Super-
vised topic models. In Proceedings of Advances in
Neural Information Processing Systems.

Andrew Kachites McCallum, Kamal Nigam, Jason
Rennie, and Kristie Seymore. 2000. Automating the
construction of Internet portals with machine learn-
ing. Information Retrieval.

David Mimno and Andrew McCallum. 2012. Topic
models conditioned on arbitrary features with
Dirichlet-multinomial regression. In Proceedings of
Uncertainty in Artificial Intelligence.

Robert Moore. 2004. On log-likelihood-ratios and the
significance of rare events. In Proceedings of Em-
pirical Methods in Natural Language Processing.

Viet-An Nguyen, Jordan Boyd-Graber, and Philip
Resnik. 2013. Lexical and hierarchical topic regres-
sion. In Proceedings of Advances in Neural Infor-
mation Processing Systems.

Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah Cai, Jennifer Midberry, and Yuanxin Wang.
2014. Modeling topic control to detect influence
in conversations using nonparametric topic models.
Machine Learning.

Nicholas G. Polson and Steven L. Scott. 2011.
Data augmentation for support vector machines.
Bayesian Analysis.

Matthew Purver, Thomas L. Griffiths, Konrad P.
Körding, and Joshua B. Tenenbaum. 2006. Unsu-
pervised topic modelling for multi-party spoken dis-
course. In Proceedings of the Association for Com-
putational Linguistics.

Graham JG Upton. 1992. Fisher’s exact test. Journal
of the Royal Statistical Society.

Hanna M. Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.

Yuchung J. Wang and George Y. Wong. 1987. Stochas-
tic blockmodels for directed graphs. Journal of the
American Statistical Association.

Xing Wei and W. Bruce Croft. 2006. LDA-based doc-
ument models for ad-hoc retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.

Weiwei Yang, Jordan Boyd-Graber, and Philip Resnik.
2015. Birds of a feather linked together: A discrim-
inative topic model using link-based priors. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.

Ke Zhai and Jordan Boyd-Graber. 2013. Online latent
Dirichlet allocation with infinite vocabulary. In Pro-
ceedings of the International Conference of Machine
Learning.

695



Jun Zhu, Amr Ahmed, and Eric P. Xing. 2012.
MedLDA: Maximum margin supervised topic mod-
els. Journal of Machine Learning Research.

Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang.
2014. Gibbs max-margin topic models with data
augmentation. Journal of Machine Learning Re-
search.

696


