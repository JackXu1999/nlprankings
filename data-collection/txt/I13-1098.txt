










































Sense Disambiguation: From Natural Language Words to Mathematical Terms


International Joint Conference on Natural Language Processing, pages 809–814,
Nagoya, Japan, 14-18 October 2013.

Sense disambiguation:
from natural language words to mathematical terms

Minh-Quoc Nghiem, 1 Giovanni Yoko Kristianto, 2 Goran Topić, 3 Akiko Aizawa 2,3
1 The Graduate University for Advanced Studies, Tokyo, Japan

2 The University of Tokyo, Tokyo, Japan
3 National Institute of Informatics, Tokyo, Japan

{nqminh, giovanni, goran topic, aizawa}@nii.ac.jp

Abstract

This paper addresses the open prob-
lem of mathematical term sense disam-
biguation. We introduce a method that
uses a MathML parallel markup corpus
to generate relevant training and testing
datasets. Based on the dataset generated,
we use Support Vector Machine classifier
to disambiguate the sense of mathematical
terms. Experimental results indicate we
can generate such data automatically and
with reasonable accuracy.

1 Introduction

Word-sense disambiguation (WSD) refers to the
process of identifying the correct sense or mean-
ing of a word in a sentence when the word has
multiple meanings. WSD remains a difficult open
problem in natural language processing. Current
WSD systems are based on supervised, unsuper-
vised, and knowledge-based approaches (Navigli,
2009). This paper focuses on the problem of dis-
ambiguating the sense of mathematical terms oc-
curring within normal text, an aspect little dis-
cussed to date.

The problem of achieving automated under-
standing of mathematical expressions can be il-
lustrated quite clearly. For instance, depend-
ing on context, the mathematical term δ can
be interpreted to refer to Kronecker Delta,
Dirac Delta, Discrete Delta, or simply
to a variable δ. Another example is i, which
can be interpreted to mean the imaginary
constant, the index variable, or the
bound variable of an operation. Other ex-
amples include α, β, σ, φ, ω, Φ, B, H , x, y,
sim. In many such cases, disambiguation can
play a crucial role in the automated understand-
ing, translation, and calculation of mathematical
expressions.

One major issue in early research on machine
understanding of mathematical terms found in text
was the lack of evaluation datasets. A previ-
ous study (Wolska et al., 2011) was based on a
small evaluation set of 200 mathematical expres-
sions annotated by experts. Clearly, large sam-
ples of sense-tagged data would require signif-
icant human annotation and labor. Fortunately,
then, Ide et al. (2002) showed that sense distinc-
tions derived from cross-lingual information are at
least as reliable as those made by human annota-
tors. The novel research described in our paper
presents a fully automated method for generating
large samples of mathematical terms with sense-
tagged data.

As part of the effort described here to address
mathematical term sense disambiguation (MTSD),
we first propose a method that uses a MathML par-
allel markup corpus to generate training and test-
ing datasets. Second, we propose heuristics that
improve alignment results for the parallel markup
corpus. Third, we present a classification-based
approach to the MTSD problem. To the best of
our knowledge, this study is the first to make use
of parallel corpora to address MTSD.

The rest of this paper is organized as follows:
Sections 2 and 3 provide a brief overview of the
background and related work; Section 4 presents
our methods; Section 5 describes the experimental
setup and results; Section 6 concludes the paper
and points to directions for future research.

2 Background

Web pages and documents represent mathe-
matical expressions in many formats: images,
TEX, MathML (Ausbrooks et al., 2010), Open-
Math (Buswell et al., 2004), OMDoc (Kohlhase,
2006), or the ISO/IEC standard Office Open
XML (Miller et al., 2009). This paper uses
MathML markup, a format recommended by the
W3C Math Working Group, as a standard for rep-

809



resenting mathematical formulas. MathML uses
presentation markup to capture notational struc-
tures and content markup to capture mathematical
structures and mathematical meaning. MathML
parallel markup provides both forms of markup for
the same mathematical expression. Figure 1 shows
the MathML presentation and content markup for
the expression arctan(0)=0 1.

Presentation MathML
<mrow>

<mrow>
<msup>

<mi>tan</mi>
<mrow>

<mo>-</mo>
<mn>1</mn>

</mrow>
</msup>
<mo>(</mo>
<mn>0</mn>
<mo>)</mo>

</mrow>
<mo>=</mo>
<mn>0</mn>

</mrow>

ContentMathML
<apply>

<eq/>
<apply>

<arctan/>
<cn>0</cn>

</apply>
<cn>0</cn>

</apply>

Figure 1: MathML presentation and content
markup for the expression arctan(0)=0

Natural language sentences and presentation
mathematical expressions have several key sim-
ilarities and differences. A token element in a
mathematical expression can be regarded as a
word in a sentence. In presentation markup, token
elements are divided into four main types: identi-
fiers (<mi>x</mi>), operators (<mo>+</mo>),
numbers (mn>2</mn>), and text (<mtext>non
zero</mtext>). A sentence may contain cer-
tain layout elements, such as subscripts or super-
scripts, while a mathematical expression may con-
tain numerous layout elements, such as <mrow>,
<msup>, <munderover>, and <mfrac>. As
noted by Ausbrooks et al. (2010), mathematical
notation, while more rigorous than natural lan-
guage, is ambiguous and context-dependent.

1http://functions.wolfram.com/01.14.03.0001.01

3 Related Work

Several studies have shown encouraging results
for WSD based on parallel corpora (Diab and
Resnik, 2002; Tufiş et al., 2004; Chan and Ng,
2005; Carpuat and Wu, 2007; Padó and Lapata,
2009; Lefever and Hoste, 2010; Lefever et al.,
2011). Ide et al. (2002) used translation equiva-
lents derived from parallel aligned corpora to de-
termine sense distinctions applicable to automatic
sense-tagging. They evaluated their work using a
subset of 33 nouns covering a range of occurrence
frequencies and degrees of ambiguity (Ide et al.,
2001), with results indicating no significant dif-
ference in agreement rates for the algorithm and
for human annotators. The main limitation of this
study is its dependence on aligned corpora, which
are not easily obtainable.

Wolska et al. (Wolska and Grigore, 2010; Wol-
ska et al., 2011) presented a knowledge-poor
method for identifying the denotation of simple
symbolic expressions in mathematical discourse.
Based on statistical co-occurrence measures, the
system sorted a simple symbolic expression under
one of seven predefined concepts. Here, the au-
thors found that lexical information from the lin-
guistic context immediately surrounding the ex-
pression improved results. This approach achieves
66% agreement with the gold standard of manual
annotation by experts. From our perspective, the
predefined concepts are closely related to syntactic
function, not the semantics of the terms.

4 Our Approach

4.1 Generating the Datasets

We compiled our MTSD data using parallel
MathML markup expressions gathered from the
Web. First, using a set of heuristic rules, we pre-
processed the parallel MathML markup expres-
sions. We then used the GIZA++ toolkit to obtain
node-to-node aligned data. Based on the node-to-
node aligned data, we created subtree-to-subtree
aligned data. Finally, we extracted ambiguous
terms from the subtree-to-subtree aligned data to
obtain data for MTSD. Figure 2 gives the steps
taken to generate the data.

A crucial step in generating MTSD data is
achieving alignment between the Presentation side
and the Content side of the expressions. Given
a set of several MathML parallel markup expres-
sions, we used the automated word alignment

810



MathML 
parallel markup 

expressions 

GIZA++ 
alignment 

Preprocess 
(heuristic rules) 

Processed 
expressions 

Node-to-node 
aligned data 

Subtree-to-subtree 
alignment 

Subtree-to-subtree 
aligned data 

Ambiguous 
term extraction Data for MTSD 

Figure 2: Steps for generating the data for MTSD.

GIZA++ (Och and Ney, 2003) to obtain alignment
between the Presentation terms and Content terms.
Developed to train word-based translation models,
the GIZA++ toolkit is not directly applicable to
a tree-based corpus. One common solution is to
convert the tree into a sentence by extracting the
leaf nodes of the tree and to form a sequence (Sun
et al., 2010). While this approach works well for
natural language text, it is less effective with math-
ematical expressions, since the intermediate nodes
of these expressions contain layout information.

Before using GIZA++, to enhance alignment
precision, we apply two heuristic rules to the pre-
sentation tree based on information on its struc-
ture. The first heuristic rule converts the inter-
mediate layout nodes (except mrow) to leaves on
the tree by moving them to the position of their
first child. When moving an intermediate layout
node, we create a temporary (‘temp’) node to re-
place the moved node and to keep the other child
nodes intact. Unnecessary parentheses, which in-
dicate that the expressions in the parentheses be-
long together, are also removed. Figure 3 illus-
trates an example of this heuristic. In this exam-
ple, we moved the msup node to a leaf of the tree
and removed a pair of parentheses, <mo>(</mo>
and <mo>)</mo>, near <mn>0</mn> node.

The second heuristic rule moves operator (mo)
nodes to the beginning of the subtree if that sub-
tree contains operator nodes. This rule reduces
cross alignments, since most notations in con-
tent MathML are prefix notations and placed in
leaf nodes. In Figure 3, the <mo>=</mo> node
is moved to the first position of the tree. The
<mo>-</mo> node is not moved because it is al-
ready the first child of its parent node. This figure
also shows alignment results for GIZA++ before

and after applying heuristic rules for the expres-
sion arctan(0)=0.

To extract more complex mathematical terms,
we expand the node-to-node alignments to
subtree-to-subtree alignments. In this study, we
expanded the subtree alignment only to the par-
ent of the mi nodes. The criteria used here to
achieve subtree aligned pair are similar to that
used by Tinsley et al. (2007). First, a node can be
linked only once. Second, descendants of a pre-
sentation node can link only to descendants of its
content counterpart. Third, ancestors of a presen-
tation node can link only to ancestors of its content
counterpart (a node counts as its own ancestor).

If one presentation node links to more than one
content node, we keep only the link with the high-
est alignment score, as given by Equation 1. The
number of alignments between the presentation
tree treeP and the content tree treeC is the sum
of (1) the number of alignments from the leaf chil-
dren of treeP to the leaf children of treeC and
(2) the number of alignments from the leaf chil-
dren of tree to the leaf children of treeP . For
more accurate results, we removed node-to-node
alignments if alignment probabilities fell below a
certain threshold (0.2). In Equation 1, Pchild and
Cchild, respectively, refer to the child nodes of
treeP and treeC . The blue lines in Figure 3 rep-
resent the expanded alignments between subtrees.

score(treeP, treeC) =
# alignments

# Pchild + # Cchild
(1)

Based on the alignment results, we extracted
pairs of presentation mathematical terms and their
associated content terms. A mutually aligned pre-
sentation subtree and content subtree form a pair.
This paper will consider only mathematical terms
containing mi (e.g. tan−1, Ai, Ai(0), Γ, Γ(23)).
Only terms associated with ambiguous mapping
are retained to generate training and testing data.

4.2 Disambiguating Mathematical Terms
We created a labeled training set, then used Sup-
port Vector Machines (SVM) to learn a classifier
from this labeled data. Assume that a presenta-
tion term e has n ways of translating to content
MathML term. Then, for each mathematical ex-
pression, we create one positive instance by com-
bining e and its correct translation. We also create
n1 negative instances by combining e and its in-
correct translations. We will assign each instance

811



mrow

mo:=

mrow

mn:0msup

eq

temp

mn:0mi:tan

mrow

mo:-

arctan

mn:1

cn:0 cn:0

apply

apply

mrow

mrow

mo:= mn:0

msup

mn:0mi:( mi:)mi:tan

mrow

mo:-

arctan

mn:1

cn:0eq cn:0

apply

apply

Before preprocessing After preprocessing

Figure 3: Example of alignment results for GIZA++ before and after applying the heuristic rules for the
expression arctan(0)=0. Red lines represent alignments from presentation nodes to content nodes;
green lines represent alignments from content nodes to presentation nodes; blue lines represent expanded
alignments between subtrees.

to one of two classes, depending on the candidate
translation: The class is ‘true’ if the content term
is the correct translation of the presentation term;
otherwise, the class is ‘false.’

We can divide the features used in SVM dis-
ambiguation into two main groups: presenta-
tion MathML and text features. Presentation
MathML features are extracted from the presenta-
tion MathML markup of the mathematical expres-
sions. Mathematical compendium websites often
group expressions into several categories. The
only text feature we use here is the name of the
category to which a mathematical expression be-
longs. Table 1 shows the features we used for clas-
sification.

Table 1: Features used for classification
Feature Description

Only child Is it the only child of its parent
Preceded by mo Is it preceded by an mo node
Followed by mo Is it followed by an mo node
mo’s name The name of the followed mo
Parent’s name The name of its parent node
Node name The name of the node
Identifier’s
name

The name of the first mi child

Category
Relation between category
name & candidate translation

Our experiment involved seven presentation
MathML features. The first determines whether
the term is the only child of its parent. The next
three features encode the relationship between the
term and the surrounding mo elements. The last
three features represent the parent’s name, the
term’s own name, and the first mi child’s name.
Since mathematical terms differ from natural lan-
guage words, the features differ as well.

5 Evaluation

5.1 Evaluation Setup

For these experiments, we collected parallel
MathML markup expressions from the Wolfram
Functions Site2 (WFS), the world’s largest collec-
tion of formulas and graphics related to mathemat-
ical functions. All mathematical expressions on
WFS are available in MathML parallel markup.
For simplicity, we excluded long expressions con-
taining more than 30 leaf nodes. We collected a
total of 20,314 mathematical expressions.

5.2 Evaluation Results

We began by investigating the quality of the gen-
erated MTSD data. Using WFS data, we gener-
ated 2,925 different mathematical terms. There are
390 distinct ambiguous terms and 2,535 distinct

2http://functions.wolfram.com/

812



unambiguous terms. Of the ambiguous terms, 90
distinct terms are single mi elements. There are
67,987 instances contain all the ambiguous terms
in our data. Table 2 shows the generated data.

Table 2: Generated data
Type Distinct term

Ambiguous mi terms 90
Other ambiguous terms 300
Unambiguous terms 2,535

The table shows that only 14% of the extracted
mathematical terms are ambiguous. One possi-
ble explanation: In WFS data, people tend to use
one meaning for a fixed notation. Another: The
system depends on the quality of the alignment
output. The aligner may ignore an alignment if
the probability of the alignment is low. This also
causes errors in sense extraction if a sub-tree is
aligned with a single term but the links are not
fully connected: for example, tan−1 (Presenta-
tion) and arctan (Content).

Within the scope of this paper, we focused on
the single mi element terms. (The same method
can be expanded to encompass additional ambigu-
ous terms.) We manually verified these single mi
element terms to assess the quality of the gener-
ated MTSD data. Of 247 extracted senses, 197
were correct, an accuracy rate of 79.76% for the
generated data. Each mi element term has an av-
erage of 2.74 senses. The term with the most
senses was <mi>C</mi>, which had six senses:
Catalan, CatalanNumber, C, GegenbauerC, Cyclo-
tomic, and FresnelC.

Next, we set up an experiment using libSVM3

in the Weka toolkit (Hall et al., 2009) to examine
sense disambiguation results for each presentation
MathML term. The data we used contained the 90
distinct ambiguous mi terms. In this evaluation,
we compared the results for systems using dif-
ferent training data: automatically extracted data
and manually verified data. We also compared
the results of our approach to the ‘most frequent’
method, which chooses the interpretation of high-
est probability. Since in the real world not every
mathematical expression is associated with its cat-
egory name, we also set up another experiment to
assess the performance of our approach with and
without the ‘category’ feature.

We built two models using nine-tenths of the au-
3http://www.csie.ntu.edu.tw/∼cjlin/libsvm/

tomatically extracted data and nine-tenths of the
manually verified data. Both systems set aside
one-tenth of the verified data for testing. Classi-
fication accuracies were computed over the set of
binary decisions. We used the default libSVM pa-
rameters. Table 3 gives the disambiguation accu-
racy for ambiguous mi terms.

Table 3: Sense disambiguation accuracy for am-
biguous mi terms

Method Extracteddata
Verified

data
All feature 91.40 93.94
Without
‘category’ feature

91.22 92.41

Most frequent 85.01 89.76

The results in Table 3 indicate reasonable re-
sults for the automatically extracted data. We
gained improvements ranging from 1.2 to 2.5 per-
cent by building a model using manually verified
data. The classifier with ‘category’ feature slightly
outperformed the classifier without the ‘category’
feature. Overall, the results here were approx-
imately 4 to 7 percent more accurate than for
the ‘most frequent’ method. The explanation for
the relatively high scores for the ‘most frequent’
method is that mathematical elements often have a
preferred meaning.

The results suggest we can make direct use of
automatically generated data when working on the
MTSD problem. For mathematical expressions
in MathML parallel markup, the generated data
is good enough without manual checking. The
results also show that the text feature-i.e., the
category of the mathematical term-contributes to
system performance. While this improvement is
modest, it suggests that features aside from the
mathematical term itself can be helpful. However,
the system works well even without this feature.

6 Conclusion

This paper presents an approach to creating train-
ing data for the mathematical term sense dis-
ambiguation problem. Combining word-to-word
alignment models and heuristic alignments, this
approach shows that we can generate reasonably
accurate MTSD data using parallel corpora. The
data generated can then be used to train a classifier
that allows automatic sense-tagging of mathemat-
ical expressions.

813



In contrast to natural language text, mathemati-
cal expressions require specific processing meth-
ods. More work needs to be done to estab-
lish the features best-suited to mathematical terms
in a larger dataset. An extension of the model
with more text and context features, in addition
to the category feature, should prove interesting.
Since the alignments between presentation and
the content tree affect the generated data, improv-
ing alignment accuracy may boost system perfor-
mance.

Acknowledgments

This research was supported in part by the Japan
Society for the Promotion of Science (JSPS)
via a Grant-in-Aid for Scientific Research (Grant
24300062).

References
Ron Ausbrooks, Stephen Buswell, David Carlisle,

Giorgi Chavchanidze, Stéphane Dalmas, Stan De-
vitt, Angel Diaz, Sam Dooley, Roger Hunter, Patrick
Ion, et al. 2010. Mathematical markup lan-
guage (MathML) version 3.0. W3C recommenda-
tion. World Wide Web Consortium.

Stephen Buswell, Olga Caprotti, David P Carlisle,
Michael C Dewar, Marc Gaetano, and Michael
Kohlhase. 2004. The open math standard version
2.0. Technical report.

Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense dis-
ambiguation. In In The 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007, pages 61–72.

Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
Proceedings of the 20th national conference on Ar-
tificial intelligence - Volume 3, pages 1037–1042.

Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
255–262.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18.

Nancy Ide, Toma Erjavec, and Dan Tufis. 2001. Au-
tomatic sense tagging using parallel corpora. In In
Proceedings of the 6 th Natural Language Process-
ing Pacific Rim Symposium, pages 212–219.

Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002.
Sense discrimination with parallel corpora. In
Proceedings of the ACL-02 workshop on Word
sense disambiguation: recent successes and future
directions-Volume 8, pages 61–66.

Michael Kohlhase. 2006. An Open Markup Format
for Mathematical Documents (Version 1.2). Lecture
Notes in Artificial Intelligence, no. 4180. Springer
Verlag, Heidelberg.

Els Lefever and Véronique Hoste. 2010. Semeval-
2010 task 3: Cross-lingual word sense disambigua-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ’10, pages
15–20.

Els Lefever, Véronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
word sense disambiguation. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 317–322.

Frederic P. Miller, Agnes F. Vandome, and John
McBrewster. 2009. Office Open XML. Alpha Press.

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:1–69.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.

Sebastian Padó and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307–
340.

Jun Sun, Min Zhang, and Chew Lim Tan. 2010.
Exploring syntactic structural features for sub-tree
alignment using bilingual tree kernels. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 306–315.

John Tinsley, Ventsislav Zhechev, Mary Hearne,
and Andy Way. 2007. Robust language pair-
independent sub-tree alignment. In In Proceedings
of MT Summit XI -07.

Dan Tufiş, Radu Ion, and Nancy Ide. 2004. Fine-
grained word sense disambiguation based on par-
allel corpora, word alignment, word clustering and
aligned wordnets. In Proceedings of the 20th inter-
national conference on Computational Linguistics,
COLING ’04.

Magdalena Wolska and Mihai Grigore. 2010. Symbol
declarations in mathematical writing. In Towards a
Digital Mathematics Library. Paris, France, July 7-
8th, 2010, pages 119–127.

Magdalena Wolska, Mihai Grigore, and Michael
Kohlhase. 2011. Using discourse context to inter-
pret object-denoting mathematical expressions. In
Towards a Digital Mathematics Library. Bertinoro,
Italy, July 20-21st, 2011, pages 85–101.

814


