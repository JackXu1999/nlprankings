








































Proceedings of the INLG and SIGDIAL 2014 Joint Session


Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 6–15,
Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics

Generating effective referring expressions using charts

Nikos Engonopoulos and Alexander Koller
University of Potsdam, Germany

{engonopo|akoller}@uni-potsdam.de

Abstract
We present a novel approach for generat-
ing effective referring expressions (REs).
We define a synchronous grammar formal-
ism that relates surface strings with the
sets of objects they describe through an ab-
stract syntactic structure. The grammars
may choose to require or not that REs are
distinguishing. We then show how to com-
pute a chart that represents, in finite space,
the complete (possibly infinite) set of valid
REs for a target object. Finally, we pro-
pose a probability model that predicts how
the listener will understand the RE, and
show how to compute the most effective
RE according to this model from the chart.

1 Introduction
The fundamental challenge in the generation of re-
ferring expressions (REG) is to compute an RE
which is effective, i.e. understood as intended by
the listener. Throughout the history of REG, we
have approximated this as the problem of generat-
ing distinguishing REs, i.e. REs that are only satis-
fied by a unique individual in the domain. This has
been an eminently successful approach, as doc-
umented e.g. in the overview article of Krahmer
and van Deemter (2012) and a variety of recent
shared tasks involving RE generation (Gatt and
Belz, 2010; Belz et al., 2008; Koller et al., 2010).

Nonetheless, reducing effectiveness to unique-
ness is limiting in several ways. First, in complex,
real-world scenes it may not be feasible to gener-
ate fully distinguishing REs, or these may have to
be exceedingly complicated. It is also not neces-
sary to generate distinguishing REs in such situa-
tions, because listeners are very capable of taking
the discourse and task context into account to re-
solve even ambiguous REs. Conversely, listeners
can misunderstand even a distinguishing RE, so
uniqueness is no guarantee for success. We pro-
pose instead to define and train a probabilistic RE

resolution model P (a|t), which directly captures
the probability that the listener will resolve a given
RE t to some object a in the domain. An RE t will
then be “good enough” if P (a⇤|t) is very high for
the intended target referent a⇤.

Second, in an interactive setting like the GIVE
Challenge (Koller et al., 2010), the listener may
behave in a way that offers further information on
how they resolved the generated RE. Engonopou-
los et al. (2013) showed how an initial estimate
of the distribution P (a|t) can be continuously up-
dated based on the listener’s behavior, and that this
can improve a system’s ability to detect misunder-
standings. It seems hard to achieve this in a prin-
cipled way without an explicit model of P (a|t).

In this paper, we present an algorithm that gen-
erates the RE t that maximizes P (a⇤|t), i.e. the
RE that has the highest chance to be understood
correctly by the listener according to the proba-
bilistic RE resolution model. This is a challeng-
ing problem, since the algorithm must identify that
RE from a potentially infinite set of valid alterna-
tives. We achieve this by using a chart-based al-
gorithm, a standard approach in parsing and real-
ization, which has (to our knowledge) never been
used in REG.

We start by defining a synchronous grammar
formalism that relates surface strings to their in-
terpretations as sets of objects in a given domain
(Section 3). This formalism integrates REG with
surface realization, and allows us to specify in the
grammar whether REs are required to be distin-
guishing. We then show how to compute a chart
for a given grammar and target referent in Sec-
tion 4. Section 5 defines a log-linear model for
P (a|t), and presents a Viterbi-style algorithm for
computing the RE t from the chart that maximizes
P (a⇤|t). Section 6 concludes by discussing how
to apply our algorithm to the state-of-the-art ap-
proaches of Krahmer et al. (2003) and Golland et
al. (2010), and how to address a particular chal-
lenge involving cycles that arises when dealing

6



with probabilistic listener models.

2 Related Work

RE generation is the task of generating a natural-
language expression that identifies an object to the
listener. Since the beginnings of modern REG
(Appelt, 1985; Dale and Reiter, 1995), this prob-
lem has been approximated as generating a dis-
tinguishing description, i.e. one which fits only
one object in the domain and not any of the oth-
ers. This perspective has made it possible to apply
search-based (Kelleher and Kruijff, 2006), logic-
based (Areces et al., 2008) and graph-based (Krah-
mer et al., 2003) methods to the problem, and
overall has been one of the success stories of NLG.

However, in practice, human speakers fre-
quently overspecify, i.e. they include information
in an RE beyond what is necessary to make it
distinguishing (Wardlow Lane and Ferreira, 2008;
Koolen et al., 2011). An NLG system, too, might
include redundant information in an RE to make it
easier to understand for the user. Conversely, an
RE that is produced by a human can often be eas-
ily resolved by the listener even if it is ambiguous.
Here we present an NLG system that directly uses
a probabilistic model of RE resolution, and is ca-
pable of generating ambiguous REs if it predicts
that the listener will understand them.

Most existing REG algorithms focus on gener-
ating distinguishing REs, and then select the one
that is best according to some criterion, e.g. most
human-like (Krahmer et al., 2003; FitzGerald et
al., 2013) or most likely to be understood (Garoufi
and Koller, 2013). By contrast, Mitchell et al.
(2013) describe a stochastic algorithm that com-
putes human-like, non-relational REs that may not
be distinguishing. Golland et al. (2010) are close
to our proposal in spirit, in that they use a log-
linear probability model of RE resolution to com-
pute a possibly non-distinguishing RE. However,
they use a trivial REG algorithm which is limited
to grammars that only permit a (small) finite set of
REs for each referent. This is in contrast to gen-
eral REG, where there is typically an infinite set
of valid REs, especially when relational REs (“the
button to the left of the plant”) are permitted.

Engonopoulos et al. (2013) describe how to up-
date an estimate for P (a|t) based on a log-linear
model based on observations of the listener’s be-
havior. They use a shallow model based on a string
t and not an RE derived from a grammar, and they
do not discuss how to generate the best t. The al-

gorithm we develop here fills this gap.
Our formalism for REG can be seen as a syn-

chronous grammar formalism; it simultaneously
derives strings and their interpretations, connect-
ing the two by an abstract syntactic representa-
tion. This allows performing REG and surface re-
alization with a single algorithm, along the lines
of SPUD (Stone et al., 2003) and its planning-
based implementation, CRISP (Koller and Stone,
2007). Probabilistic synchronous grammars are
widely used in statistical machine translation (Chi-
ang, 2007; Graehl et al., 2008; Jones et al., 2012)
and semantic parsing (Zettlemoyer and Collins,
2005; Wong and Mooney, 2007). Lu and Ng
(2011) have applied such grammars to surface re-
alization. Konstas and Lapata (2012) use related
techniques for content selection and surface real-
ization (with simple, non-recursive grammars).

Charts are standard tools for representing a
large space of possible linguistic analyses com-
pactly. Next to their use in parsing, they have also
been applied to surface realization (Kay, 1996;
Carroll et al., 1999; Kaplan and Wedekind, 2000).
To our knowledge, ours is the first work using
charts for REG. This is challenging because the
input to REG is much less structured than in pars-
ing or realization.

3 Grammars for RE generation

We define a new grammar formalism that we use
for REG, which we call semantically intepreted
grammar (SIG). SIG is a synchronous grammar
formalism that relates natural language strings
with the sets of objects in a given domain which
they describe. It uses regular tree grammars
(RTGs) to describe languages of derivation trees,
which then project to strings and sets.

3.1 Derivation trees
We describe the abstract syntax of an RE by its
derivation tree, which is a tree over some ranked
signature ⌃ of symbols representing lexicon en-
tries and grammatical constructions. A (ranked)
signature is a finite set of symbols r 2 ⌃, each
of which is assigned an arity ar(r) 2 N0. A tree
over the signature ⌃ is a term r(t1, . . . , tn), where
r 2 ⌃, n = ar(r), and t1, . . . , tn are trees over ⌃.
We write T⌃ for the set of all trees over ⌃.

Fig. 1b shows an example derivation tree for
the RE “the square button” over the signature
⌃ = {def |1, square|1, button|0}, where r|n indi-
cates that the symbol r has arity n. In term nota-

7



(a) (b) (c)

{b2}
IR

 ������ def

square

button

IS
���! “the square button”

0

@
\1

square button

1

A
0

BBB@

•

the •

square button

1

CCCA

Figure 1: A SIG derivation tree (b) with its inter-
pretations (a, c).

tion, it is def (square(button)).

String interpretation. We interpret derivation
trees simultaneously as strings and sets. First, let
� be a finite alphabet, and let �⇤ be the string al-
gebra over �. We define a string interpretation
over � as a function IS that maps each r|n 2 ⌃
to a function IS(r) : (�⇤)n ! �⇤. For instance,
we can assign string interpretations to our exam-
ple signature ⌃ as follows; we write w1 • w2 for
the concatenation of the strings w1 and w2.

IS(def )(w1) = the • w1
IS(square)(w1) = square • w1

IS(button) = button

Since the arity of IS(r) is the same as the ar-
ity of r for any r 2 ⌃, we can use IS to recur-
sively map derivation trees to strings. Starting at
the leaves, we map the tree r(t1, . . . , tn) to the
string IS(r)(IS(t1), . . . , IS(tn)), where IS(ti) is
the string that results from recursively applying IS
to the subtree ti. In the example, the subtree button
is mapped to the string “button”. We then get
the string for the subtree square(button) by con-
catenating this with “square”, obtaining the string
“square button” and so on, as shown in Fig. 1c.

Relational interpretation. We further define a
relational interpretation IR, which maps each
r|n 2 ⌃ to a function IR(r) : R(U)n ! R(U),
where R(U) is a class of relations. We define IR
over some first-order model structure M = hU,Li,
where U is a finite universe U of individuals and L
interprets a finite set of predicate symbols as rela-
tions over U . We let R(U) be the set of all k-place
relations over U for all k � 0. The subsets of U
are the special case of k = 1. We write k(R) for
the arity of a relation R 2 R(U).

For the purposes of this paper, we construct IR
by combining the following operations:

• The denotations of the atomic predicate sym-
bols of M ; see Fig. 2 for an example.

U = {b1, b2, b3} button = {b1, b2, b3}
round = {b1, b3} square = {b2}
left of = {hb1, b2i, hb2, b3i}
right of = {hb2, b1i, hb3, b2i}

Figure 2: A simple model, illustrated as a graph.

• proji(R) = {ai | ha1, . . . , ak(R)i 2 R} is
the projection to the i-th component; if i >
k(R), it evaluates to ;.

• R1 \i R2 = {ha1, . . . , ak(R1)i 2 R1 | ai 2
R2} is the intersection on the i-th component
of R1; if i > k(R1), it evaluates to ;.

• For any a 2 U , uniqa(R) evaluates to {a} if
R = {a}, and to ; otherwise.

• For any a 2 U , membera(R) evaluates to
{a} if a 2 R, and to ; otherwise.

For the example, we assume that we want to
generate REs over the scene shown in Fig. 2; it
consists of the universe U = {b1, b2, b3} and inter-
prets the atomic predicate symbols button, square,
round, left of, and right of. Given this, we can
assign a relational interpretation to the derivation
tree in Fig. 1b using the following mappings:

IR(def )(R1) = R1
IR(square)(R1) = square \1 R1

IR(button) = button

We evaluate a derivation tree to a relation as we
did for strings (cf. Fig. 1a). The subtree button
maps to the denotation of the symbol button, i.e.
{b1, b2, b3}. The subtree square(button) evaluates
to the intersection of this set with the set of square
individuals, i.e. {b2}; this is also the relational in-
terpretation of the entire derivation tree. We thus
see that “the square button” is an RE that describes
the individual b2 uniquely.

3.2 Semantically interpreted grammars
Now we define grammars that describe relations
between strings and relations over U . We achieve
this by combining a regular tree grammar (RTG,
(Gécseg and Steinby, 1997; Comon et al., 2007)),
describing a language of derivation trees, with a
string interpretation and a relational interpretation.
An RTG G = (N,⌃, S, P ) consists of a finite
set N of nonterminal symbols, a ranked signa-
ture ⌃, a start symbol S 2 N , and a finite set
P of production rules A ! r(B1, ..., Bn), where

8



A,B1, . . . , Bn 2 N and r|n 2 ⌃. We say that
a tree t2 2 T⌃ can be derived in one step from
t1 2 T⌃, t1 ) t2, if it can be obtained by replac-
ing an occurrence of B in t1 with t and P con-
tains the rule B ! t. A tree tn 2 T⌃ can be
derived from t1, t1 )⇤ tn, if there is a sequence
t1 ) . . . ) tn of length n � 0. For any nontermi-
nal A, we write LA(G) for the set of trees t 2 T⌃
with A )⇤ t. We simply write L(G) for LS(G)
and call it the language of G.

We define a semantically interpreted grammar
(SIG) as a triple G = (G, IS , IR) of an RTG G
over some signature ⌃, together with a string inter-
pretation IS over some alphabet � and a relational
interpretation IR over some universe U , both of
which interpret the symbols in ⌃. We assume that
every terminal symbol r 2 ⌃ occurs in at most
one rule, and that the nonterminals of G are pairs
Ab of a syntactic category A and a semantic index
b = ix(Ab). A semantic index indicates the indi-
vidual in U to which a given constituent is meant
to refer, see e.g. (Kay, 1996; Stone et al., 2003).
Note that SIGs can be seen as specific Interpreted
Regular Tree Grammars (Koller and Kuhlmann,
2011) with a set and a string interpretation.

We ignore the start symbol of G. Instead, we
say that given some individual b 2 U and syntactic
category A, the set of referring expressions for b is
REG(A, b) = {t 2 LAb(G) | IR(t) = {b}}, i.e.
we define an RE as a derivation tree that G can
derive from Ab and whose relational interpretation
is {b}. From t, we can read off the string IS(t).1

3.3 An example grammar

Consider the SIG G in Fig. 3 for example. The
grammar is written in template form. Each rule
is instantiated for all semantic indices specified
in the line above; e.g. the symbol round denotes
the set {b1, b3}, therefore there are rules Nb1 !
roundb1(Nb1) and Nb3 ! roundb3(Nb3). The val-
ues of IR and IS for each symbol are specified
below the RTG rule for that symbol.

We can use G to generate NPs that refer to the
target referent b2 given the model shown in Fig. 2
by finding trees in LNPb2 (G) that refer to {b2}.
One such tree is t1 = def b2(squareb2(buttonb2)),
a more detailed version of the tree in Fig. 1b.
It can be derived by NPb2 ) def b2(Nb2) )
def b2(squareb2(Nb2)) ) t1. Because IR(t1) =
{b2}, we see that t1 2 REG(NP, b2); it represents

1Below, we will often write the RE as a string when the
derivation tree is clear.

for all a 2 U :
NPa ! defa(Na)
IS(defa)(w1) = the • w1
IR(defa)(R1) = membera(R1)

for all a 2 button:
Na ! buttona
IS(buttona) = button
IR(buttona) = button

for all a 2 round:
Na ! rounda(Na)
IS(rounda)(w1) = round • w1
IR(rounda)(R1) = round \1 R1

for all a 2 square:
Na ! squarea(Na)
IS(squarea)(w1) = square • w1
IR(squarea)(R1) = square \1 R1

for all a, b 2 left of:
Na ! leftofa,b(Na,NPb)
IS(leftofa,b)(w1, w2) = w1 • to • the • left • of • w2
IR(leftofa,b)(R1, R2) = proj1((left of \1 R1) \2 R2)

for all a, b 2 right of:
Na ! rightofa,b(Na,NPb)
IS(rightofa,b)(w1, w2) = w1 • to • the • right • of • w2
IR(rightofa,b)(R1, R2) = proj1((right of \1 R1) \2 R2)

Figure 3: An example SIG grammar.

the string IS(t1) = “the square button”.
A second derivation tree for b2 is t2 =

def b2(squareb2(squareb2(buttonb2))), correspond-
ing to IS(t2) = “the square square button”. It de-
rives from NPb2 in four steps, and has IR(t2) =
{b2}. Even the small grammar G licences an infi-
nite set of REs for b2, all of which are semantically
correct. Avoiding the generation of nonsensical
REs like “the square square button” is a techni-
cal challenge to which we will return in Section 6.
G can also derive relational REs; for instance, the
derivation tree in Fig. 6 for the string “the button
to the left of the square button” is in REG(NP, b1).

Finally, G considers the non-distinguishing t3 =
def b2(buttonb2) (for “the button”) a valid RE for
b2. This is because memberb2 will quietly project
the set {b1, b2, b3} (to which buttonb2 refers) to
{b2}. As discussed in previous sections, we want
to allow such non-unique REs and delegate the
judgment about their quality to the probability
model. It would still be straightforward, however,
to impose a hard uniqueness constraint, by simply
changing IR(def a)(R1) to uniqa(R1) in Fig. 3.
This would yield IR(t3) = ;, i.e. t3 would no
longer be in REG(NP, b2).

4 Chart-based RE generation

We now present a chart-based algorithm for gener-
ating REs with SIG grammars. Charts allow us to
represent all REs for a target referent compactly,
and can be computed efficiently. We show in Sec-
tion 5 that charts also lend themselves well to com-
puting the most effective RE.

9



Nb1/{b1, b2, b3} ! buttonb1
Nb2/{b1, b2, b3} ! buttonb2
Nb3/{b1, b2, b3} ! buttonb3
Nb1/{b1, b3} ! roundb1 (Nb1/{b1, b2, b3})
Nb3/{b1, b3} ! roundb3 (Nb3/{b1, b2, b3})
Nb1/{b1, b3} ! roundb1 (Nb1/{b1, b3})
Nb3/{b1, b3} ! roundb3 (Nb3/{b1, b3})
Nb2/{b2} ! squareb2 (Nb2/{b1, b2, b3})
Nb2/{b2} ! squareb2 (Nb2/{b2})
NPb2/{b2} ! def b2 (Nb2/{b1, b2, b3})
NPb2/{b2} ! def b2 (Nb2/{b2})
Nb1/{b1} ! leftof b1,b2 (Nb1/{b1, b2, b3},NPb2/{b2})
Nb1/{b1} ! leftof b1,b2 (Nb1/{b1, b3},NPb2/{b2})
Nb1/{b1} ! leftof b1,b2 (Nb1/{b1},NPb2/{b2})
Nb1/{b1} ! roundb1 (Nb1/{b1})
NPb1/{b1} ! def b1 (Nb1/{b1, b2, b3})
NPb1/{b1} ! def b1 (Nb1/{b1, b3})
NPb1/{b1} ! def b1 (Nb1/{b1})
Nb3/{b3} ! rightof b3,b2 (Nb3/{b1, b2, b3},NPb2/{b2})
Nb3/{b3} ! rightof b3,b2 (Nb3/{b1, b3},NPb2/{b2})
Nb3/{b3} ! rightof b3,b2 (Nb3/{b3},NPb2/{b2})
Nb3/{b3} ! roundb3 (Nb3/{b3})
NPb3/{b3} ! def b3 (Nb3/{b1, b2, b3})
NPb3/{b3} ! def b3 (Nb3/{b1, b3})
NPb3/{b3} ! def b3 (Nb3/{b3})
Nb2/{b2} ! leftof b2,b3 (Nb2/{b1, b2, b3},NPb3/{b3})
Nb2/{b2} ! rightof b2,b1 (Nb2/{b1, b2, b3},NPb1/{b1})
Nb2/{b2} ! leftof b2,b3 (Nb2/{b2},NPb3/{b3})
Nb2/{b2} ! rightof b2,b1 (Nb2/{b2},NPb1/{b1})

Figure 4: The chart for the grammar in Fig. 3.

4.1 RE generation charts

Generally speaking, a chart is a packed data struc-
ture which describes how larger syntactic repre-
sentations can be recursively built from smaller
ones. In applications such as parsing and sur-
face realization, the creation of a chart is driven
by the idea that we consume some input (words
or semantic atoms) as we build up larger struc-
tures. The parallel to this intuition in REG is that
“larger” chart entries are more precise descriptions
of the target, which is a weaker constraint than
input consumption. Nonetheless, we can define
REG charts whose entries are packed representa-
tions for large sets of possible REs, and compute
them in terms of these entries instead of RE sets.

Technically, we represent charts as RTGs over
an extended set of nonterminals. A chart for gener-
ating an RE of syntactic category A for an individ-
ual b 2 U is an RTG C = (N 0,⌃, S0, P 0), where
N 0 ✓ N ⇥ R(U) and S0 = Ab/{b}. Intuitively,
the nonterminal Ab/{a1, . . . , an} expresses that
we intend to generate an RE for b from A, but each
RE that we can derive from the nonterminal actu-
ally denotes the referent set {a1, . . . , an}.

A chart for the grammar in Fig. 3 is shown
in Fig. 4. To generate an NP for b2, we let
its start symbol be S0 = NPb2/{b2}. The rule
Nb2/{b1, b2, b3} ! buttonb2 says that we can gen-
erate an RE t with IR(t) = {b1, b2, b3} from the
nonterminal symbol Nb2 by expanding this symbol
with the grammar rule Nb2 ! buttonb2 . Similarly,

A ! r(B1, ..., Bn) in G
B01 = B1/R1, ..., B

0
n = Bn/Rn in N 0

Add A0 = A/IR(r)(R1, ..., Rn) to N 0
Add rule A0 ! r(B01, ..., B0n) to P 0

Figure 5: The chart computation algorithm.

the rule Nb2/{b2} ! squareb2(Nb2/{b1, b2, b3})
expresses that we can generate an RE with
IR(t) = {b2} by expanding the nonterminal sym-
bol Nb2 into squareb2(t

0
), where t0 is any tree that

the chart can generate from Nb2/{b1, b2, b3}.

4.2 Computing a chart
Given a SIG G, a syntactic category A, and a
target referent b, we can compute a chart C for
REG(A, b) using the parsing schema in Fig. 5.
The schema assumes that we have a rule A !
r(B1, . . . , Bn) in G; in addition, for each 1 
i  n it assumes that we have already added
the nonterminal B0i = Bi/Ri to the chart, in-
dicating that there is a tree ti with Bi )⇤ ti
and IR(ti) = Ri. Then we know that t =
r(t1, . . . , tn) can be derived from A and that R0 =
IR(t) = IR(r)(R1, . . . , Rn). We can therefore
add the nonterminal A0 = A/R0 and the produc-
tion rule A0 ! r(B01, . . . , B0n) to the chart; this
rule can be used as the first step in a derivation of t
from A0. We can optimize the algorithm by adding
A0 and the rule only if R0 6= ;.

The algorithm terminates when it can add no
more rules to the chart. Because U is finite, this
always happens after a finite number of steps, even
if there is an infinite set of REs. For instance, the
chart in Fig. 4 describes an infinite language of
REs, including “the square button”, “the button to
the left of the round button”, “the button to the left
of the button to the right of the square button”, etc.
Thus it represents relational REs that are nested
arbitrarily deeply through a finite number of rules.

After termination, the chart contains all rules by
which a nonterminal can be decomposed into other
(productive) nonterminals. As a result, L(C) con-
tains exactly the REs for b of category A:

Theorem 1 If C is a chart for the SIG G, the syn-
tactic category A, and the target referent b, then
L(C) = REG(A, b).

5 Computing best referring expressions

The chart algorithm allows us to compactly rep-
resent all REs for the target referent. We now
show how to compute the best RE from the chart.
We present a novel probability model P (b|t) for
RE resolution, and take the “best” RE to be the

10



Figure 6: The derivation tree for “the button to the
left of the square button”.

one with the highest chance to be understood as
intended. Next to the best RE itself, the algo-
rithm also computes the entire distribution P (b|t),
to support later updates in an interactive setting.

Nothing in our algorithm hinges on this par-
ticular model; it can also be used with any other
scoring model that satisfies a certain monotonicity
condition which we spell out in Section 5.2.

5.1 A log-linear model for effective REs

We model the probability P (b|t) that the listener
will resolve the RE t to the object b using a
log-linear model with a set of feature functions
f(a, t,M), where a is an object, t is a derivation
tree, and M is the relational interpretation model.

We focus on features that only look at informa-
tion that is local to a specific subtree of the RE,
such as the label at the root. For instance, a feature
fround(a, t0,M) might return 1 if the root label of
t0 is rounda and a is round in M , and 0 otherwise.
Another feature fdef (a, t0,M) might return 1/k if
t0 is of the form def b(t00), R = IR(t00) has k el-
ements, and a 2 R; and 0 otherwise. This fea-
ture counterbalances the ability of the grammar in
Fig. 3 to say “the w” even when w is a non-unique
description by penalizing descriptions with many
possible referents through lower feature values.

When generating a relational RE, the derivation
tree naturally splits into separate regions, each of
which is meant to identify a specific object. These
regions are distinguished by the semantic indices
in the nonterminals that derive them; e.g., in Fig. 6,
the subtree for “the square button” is an attempt to
refer to b2, whereas the RE as a whole is meant to
refer to b1. To find out how effective the RE is as
a description of b1, we evaluate the features at all
nodes in the region top(t) containing the root of t.

Each feature function fi is associated with a
weight wi. We obtain a score tuple sc(t0) for some
subtree t0 of an RE as follows:

sc(t0) = hs(a1, t
0,M), . . . , s(am, t

0,M)i,

t b1 b2 b3
“the button” 0.33 0.33 0.33
“the round button” 0.45 0.10 0.45
“the button to the left
of the square button” 0.74 0.14 0.12

Figure 7: Probability distributions for some REs t.

where U = {a1, . . . , am} and s(a, t0,M) =Pn
i=1wi · fi(a, t

0,M). We then combine these
into a score tuple score(t) =

P
u2top(t) sc(t.u)

for the whole RE t, where t.u is the subtree of
t below the node u. Finally, given a score tuple
s = hs1, . . . , smi for t, we define the usual log-
linear probability distribution as

P (ai|t) = prob(ai, s) =
esiPm
j=1 e

sj
.

The best RE for the target referent b is then

bestG(A, b) = argmax
t2REG(A,b)

prob(b, sc(t)).

For illustration, we consider a number of REs
for b1 in our running example. We use fround and
fdef and let wround = wdef = 1. In this case, the
RE “the button” has a score tuple h1/3, 1/3, 1/3i,
which is the sum of the tuple h0, 0, 0i for fround
(since the RE does not use the “round” rule) and
the tuple h1/3, 1/3, 1/3i for fdef (since “button”
is three-way ambiguous in M ). This yields a uni-
form probability distribution over U (see Fig. 7).
By contrast, “the round button” gets h3/2, 0, 3/2i,
resulting in the distribution in the second line of
Fig. 7. This RE is judged better than “the button”
because it assigns a higher probability to b1.

Relational REs involve derivation trees with
multiple regions, only the top one of which is di-
rectly counted for P (b|t) (see Fig. 6). We incorpo-
rate the quality of the other regions through appro-
priate features. In the example, we use a feature
fleftof (a, t0,M) =

P
b:ha,bi2left of P (b|t

00
), where

t00 is the second subtree of t0. This feature com-
putes the probability that the referent to which the
listener resolves t00 is actually to the right of a,
and will thus take a high value if t00 is a good
RE for b2. Assuming a probability distribution of
P (b2|t0) = 0.78 and P (b1|t0) = P (b3|t0) = 0.11
for t0 =“the square button”, we get the tuple
h0.78, 0.11, 0i for fleftof , yielding the third line
of Fig. 7 for wleftof = 1.

11



5.2 Computing the best RE
We compute bestG(A, b) from the chart by adapt-
ing the Viterbi algorithm. Our key data structure
assigns a score tuple is(A0) to each nonterminal
A0 in the chart. Intuitively, if the semantic index
of A0 is b, then is(A0) is the score tuple sc(t) for
the tree t 2 LA0(C) which maximizes P (b|t). We
also record this best tree as bt(A0). Thus the al-
gorithm is correct if, after running it, we obtain
bestG(A, b) = bt(Ab/{b}).

As is standard in chart algorithms, we limit our
attention to features whose values can be com-
puted bottom-up by local operations. Specifically,
we assume that if A0 ! r(B01, . . . , B0n) is a rule in
the chart and ti is the best RE for B0i for all i, then
the best RE for A0 that can be built using this rule
is r(t1, . . . , tn). This means that features must be
monotonic, i.e. that the RE that seemed locally
best for B0i leads to the best RE overall.

Under this assumption, we can compute is(A0)
and bt(A0) bottom-up as shown in Fig. 8. We it-
erate over all nonterminals A0 in the chart in a
fixed linear order, which we call the evaluation
order. Then we compute is(A0) and bt(A0) by
maximizing over the rules for A0. Assume that
the best RE for A0 can be constructed using the
rule A0 ! r(B01, . . . , B0n). Then if, at the time we
evaluate A0, we have fully evaluated all the B0i in
the sense that bt(B0i) is actually the best RE for
B0i, the algorithm will assign the best RE for A

0

to bt(A0), and its score tuple to is(A0). Thus, if
we call an evaluation order exact if the nontermi-
nals on the right-hand side of each rule in the chart
come before the nonterminal on the left-hand side,
we can inductively prove the following theorem:

Theorem 2 If the evaluation order is exact, then
for every nonterminal A0 in the chart, we ob-
tain bt(A0) = argmaxt2LA0 (C) P (ix(A

0
)|t) and

is(A0) = sc(bt(A0)).

In other words, the algorithm is correct if the
evaluation order is exact. If it is not, we might
compute a sub-optimal RE as bt(A0), which un-
derestimates is(A0). The choice of evaluation or-
der is thus crucial.

6 Evaluating charts with cycles

It remains to show how we can determine an ex-
act evaluation order for a given chart. One way to
think about the problem is to consider the order-
ing graph O(C) of the chart C (see Fig. 9 for an
example). This is a directed graph whose nodes

1: for nonterminals A0 in evaluation order do
2: for rules r of the form A0 ! r(B01, . . . , B0n) do
3: a = ix(A0)
4: t0 = r(bt(B01), . . . , bt(B0n))

5: s = sc(t0) +
nX

i=1
ix(B0i)=a

is(B0i)

6: if prob(a, s) > prob(a, is(A0)) then
7: is(A0) = s
8: bt(A0) = t0

Figure 8: Computing the best RE.

are the nonterminals of the chart; for each rule
A0 ! r(B01, . . . , B

0
n) in C, it has an edge from

B0i to A
0 for each i. If this graph is acyclic, we

can simply compute a topological sort of O(C)
to bring the nodes into a linear order in which
each B0i precedes A

0. This is enough to evalu-
ate charts using certain simpler models. For in-
stance, we can apply our REG algorithm to the
log-linear model of Golland et al. (2010). Because
they only generate REs with a bounded number of
relations, their grammars effectively only describe
finite languages. In such a case, our charts are al-
ways acyclic, and therefore a topological sort of
O(C) yields an exact evaluation order.

This simple approach will not work with gram-
mars that allow arbitrary recursion, as they can
lead to charts with cycles (indicating an infinite
set of valid REs). E.g. the chart in Fig. 4 contains
a rule Nb2/{b2} ! squareb2(Nb2/{b2}) (shown
in Fig. 9), which can be used to construct the RE
t0 = “the square square button” in addition to the
RE t = “the square button”. Such cycles can be
increasing with respect to a log-linear probability
model, i.e. the model considers t0 a better RE than
t. Indeed, t has a score tuple of h0, 2, 0i, giving
P (b2|t) = 0.78. By contrast, t0 has a score tuple
of h0, 3, 0i, thus P (b2|t0) = 0.91. This can be con-
tinued indefinitely, with each addition of “square”
increasing the probability of being resolved to b2.
Thus, there is no best RE for b2; every RE can be
improved by adding another copy of “square”.

In such a situation, it is a challenge to even
compute any score for every nonterminal without
running into infinite loops. We can achieve this
by decomposing O(C) into its strongly connected
components (SCCs), i.e. the maximal subgraphs in
which each node is reachable from any other node.
We then consider the component graph O0(C); its
nodes are the SCCs of O(C), and it has an edge
from c1 to c2 if O(C) has an edge from some
node in c1 to some node in c2. O0(C) is acyclic
by construction, so we can compute a topological

12



Figure 9: A fragment of the ordering graph for the
chart in Fig. 4. Dotted boxes mark SCCs.

Figure 10: A fragment of a chart ordering graph
for a grammar with enriched nonterminals.

sort and order all nonterminals from earlier SCCs
before all nonterminals from later SCCs. Within
each SCC, we order the nonterminals in the order
in which they were discovered by the algorithm in
Fig. 5. This yields a linear order on nonterminals,
which at least ensures that by the time we evaluate
a nonterminal A0, there is at least one rule for A0
whose right-hand nonterminals have all been eval-
uated; so is(A0) gets at least some value.

In our example, we obtain the order
Nb2/{b1, b2, b3}, Nb2/{b2}, NPb2/{b2}. The
rule Nb2/{b2} ! squareb2(Nb2/{b2}) will thus
not be considered in the evaluation of Nb2/{b2},
and the algorithm returns “the square button”.
The algorithm computes optimal REs for acyclic
charts, and also for charts where all cycles are
decreasing, i.e. using the rules in the cycle make
the RE worse. This enables us, for instance, to
encode the REG problem of Krahmer et al. (2003)
into ours by using a feature that evaluates the rule
for each attribute to its (negative) cost according
to the Krahmer model. Krahmer et al. assume that
every attribute has positive cost, and is only used
if it is necessary to make the RE distinguishing.
Thus all cycles in the chart are decreasing.

One limitation of the algorithm is that it does
not overspecify. Suppose that we extend the ex-
ample model in Fig. 2 with a color predicate
green = {b2}. We might then want to prefer
“the green square button” over “the square but-
ton” because it is easier to understand. But since
all square objects (i.e. {b2}) are also green, using
“green” does not change the denotation of the RE,
i.e. it is represented by a loop from Nb2/{b2} to
Nb2/{b2}, which is skipped by the algorithm. One
idea could be to break such cycles by the careful
use of a richer set of nonterminals in the gram-
mar; e.g., they might record the set of all attributes
that were used in the RE. Our example rule would
then become Nb2/{b2}/{square, green} !
greenb2(Nb2/{b2}/{square}), which the algo-

rithm can make use of (see Fig. 10).

7 Conclusion

We have shown how to generate REs using charts.
Based on an algorithm for computing a chart of all
valid REs, we showed how to compute the RE that
maximizes the probability of being understood as
the target referent. Our algorithm integrates REG
with surface realization. It generates distinguish-
ing REs if this is specified in the grammar; oth-
erwise, it computes the best RE without regard to
uniqueness, using features that prefer unambigu-
ous REs as part of the probability model.

Our algorithm can be applied to earlier models
of REG, and in these cases is guaranteed to com-
pute optimal REs. The probability model we intro-
duced here is more powerful, and may not admit
“best” REs. We have shown how the algorithm
can still do something reasonable in such cases,
but this point deserves attention in future research,
especially with respect to overspecification.

We evaluated the performance of our chart al-
gorithm on a number of randomly sampled in-
put scenes from the GIVE Challenge, which con-
tained 24 objects on average. Our implementa-
tion is based on the IRTG tool available at irtg.
googlecode.com. While in the worst case the
chart computation is exponential in the input size,
in practice runtimes did not exceed 60 ms for the
grammar shown in Fig. 3.

We have focused here on computing best REs
given a probability model. We have left train-
ing the model and evaluating it on real-world data
for future work. Because our probability model
focuses on effectiveness for the listener, rather
than human-likeness, our immediate next step is to
train it on an interaction corpus which records the
reactions of human listeners to system-generated
REs. A further avenue of research is to deliber-
ately generate succinct but ambiguous REs when
the model predicts them to be easily understood.
We will explore ways of achieving this by combin-
ing the effectiveness model presented here with a
language model that prefers succinct REs.

Acknowledgments. We thank Emiel Krahmer,
Stephan Oepen, Konstantina Garoufi, Martı́n Vil-
lalba and the anonymous reviewers for their useful
comments and discussions. The authors were sup-
ported by the SFB 632 “Information Structure”.

13



References
Douglas E. Appelt. 1985. Planning English sentences.

Cambridge University Press.

Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. In Proceedings of the 5th Inter-
national Natural Language Generation Conference
(INLG).

Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2008. The GREC challenge 2008: Overview and
evaluation results. In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion (INLG).

John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
the 7th European Workshop on Natural Language
Generation.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.

Hubert Comon, Max Dauchet, Rémi Gilleron, Christof
Löding, Florent Jacquemard, Denis Lugiez, Sophie
Tison, and Marc Tommasi. 2007. Tree automata
techniques and applications. Available on http:
//tata.gforge.inria.fr/.

Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean Maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.

Nikos Engonopoulos, Martin Villalba, Ivan Titov, and
Alexander Koller. 2013. Predicting the resolution
of referring expressions from user behavior. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Seattle.

Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.

Konstantina Garoufi and Alexander Koller. 2013.
Generation of effective referring expressions in situ-
ated context. Language and Cognitive Processes.

Albert Gatt and Anja Belz. 2010. Introducing shared
task evaluation to NLG: The TUNA shared task
evaluation challenges. In E. Krahmer and M. The-
une, editors, Empirical Methods in Natural Lan-
guage Generation, number 5790 in LNCS, pages
264–293. Springer.

Ferenc Gécseg and Magnus Steinby. 1997. Tree lan-
guages. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, chap-
ter 1, pages 1–68. Springer-Verlag.

Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).

Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3).

B. Jones, J. Andreas, D. Bauer, K.-M. Hermann, and
K. Knight. 2012. Semantics-based machine transla-
tion with hyperedge replacement grammars. In Pro-
ceedings of COLING.

Ron Kaplan and Jürgen Wedekind. 2000. LFG gener-
ation produces context-free languages. In Proceed-
ings of the 18th COLING.

Martin Kay. 1996. Chart generation. In Proceedings
of the 34th ACL.

John Kelleher and Geert-Jan Kruijff. 2006. Incremen-
tal generation of spatial referring expressions in situ-
ated dialogue. In In Proceedings of Coling-ACL ’06,
Sydney Australia.

Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, pages 2–13. Association for
Computational Linguistics.

Alexander Koller and Matthew Stone. 2007. Sentence
generation as a planning problem. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics (ACL).

Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The First Challenge on
Generating Instructions in Virtual Environments.
In E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, number
5790 in LNAI, pages 337–361. Springer.

Yannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th ACL.

Ruud Koolen, Albert Gatt, Martijn Goudbeek, and
Emiel Krahmer. 2011. Factors causing overspec-
ification in definite descriptions. Journal of Prag-
matics, 43:3231–3250.

Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.

Emiel Krahmer, Sebastiaan van Erk, and André Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.

Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of EMNLP.

14



Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2013. Generating expressions that refer to vis-
ible objects. In Proceedings of NAACL-HLT, pages
1174–1184.

Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with communicative intentions: The SPUD
system. Computational Intelligence, 19(4):311–
381.

Liane Wardlow Lane and Victor Ferreira. 2008.
Speaker-external versus speaker-internal forces on
utterance form: Do cognitive demands override
threats to referential success? Journal of Experi-
mental Psychology: Learning, Memory, and Cogni-
tion, 34:1466–1481.

Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of the 45th
ACL.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence (UAI).

15


