



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 562–570
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1052

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 562–570
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1052

Deep Pyramid Convolutional Neural Networks for Text Categorization

Rie Johnson
RJ Research Consulting

Tarrytown, NY, USA
riejohnson@gmail.com

Tong Zhang
Tencent AI Lab

Shenzhen, China
bradymzhang@tencent.com

Abstract

This paper proposes a low-complexity
word-level deep convolutional neural net-
work (CNN) architecture for text catego-
rization that can efficiently represent long-
range associations in text. In the literature,
several deep and complex neural networks
have been proposed for this task, assum-
ing availability of relatively large amounts
of training data. However, the associated
computational complexity increases as the
networks go deeper, which poses serious
challenges in practical applications. More-
over, it was shown recently that shallow
word-level CNNs are more accurate and
much faster than the state-of-the-art very
deep nets such as character-level CNNs
even in the setting of large training data.
Motivated by these findings, we carefully
studied deepening of word-level CNNs to
capture global representations of text, and
found a simple network architecture with
which the best accuracy can be obtained
by increasing the network depth without
increasing computational cost by much.
We call it deep pyramid CNN. The pro-
posed model with 15 weight layers out-
performs the previous best models on six
benchmark datasets for sentiment classifi-
cation and topic categorization.

1 Introduction

Text categorization is an important task whose ap-
plications include spam detection, sentiment clas-
sification, and topic classification. In recent years,
neural networks that can make use of word or-
der have been shown to be effective for text cat-
egorization. While simple and shallow convolu-
tional neural networks (CNNs) (Kim, 2014; John-

son and Zhang, 2015a) were proposed for this
task earlier, more recently, deep and more com-
plex neural networks have also been studied, as-
suming availability of relatively large amounts of
training data (e.g., one million documents). Exam-
ples are deep character-level CNNs (Zhang et al.,
2015; Conneau et al., 2016), a complex combi-
nation of CNNs and recurrent neural networks
(RNNs) (Tang et al., 2015), and RNNs in a word-
sentence hierarchy (Yang et al., 2016).

A CNN is a feedforward network with convo-
lution layers interleaved with pooling layers. Es-
sentially, a convolution layer converts to a vector
every small patch of data (either the original data
such as text or image or the output of the pre-
vious layer) at every location (e.g., 3-word win-
dows around every word), which can be processed
in parallel. By contrast, an RNN has connec-
tions that form a cycle. In its typical application
to text, a recurrent unit takes words one by one
as well as its own output on the previous word,
which is parallel-processing unfriendly. While
both CNNs and RNNs can take advantage of word
order, the simple nature and parallel-processing
friendliness of CNNs make them attractive partic-
ularly when large training data causes computa-
tional challenges.

There have been several recent studies of CNN
for text categorization in the large training data
setting. For example, in (Conneau et al., 2016),
very deep 32-layer character-level CNNs were
shown to outperform deep 9-layer character-level
CNNs of (Zhang et al., 2015). However, in
(Johnson and Zhang, 2016), very shallow 1-layer
word-level CNNs were shown to be more accu-
rate and much faster than the very deep character-
level CNNs of (Conneau et al., 2016). Although
character-level approaches have merit in not hav-
ing to deal with millions of distinct words, shallow
word-level CNNs turned out to be superior even

562

https://doi.org/10.18653/v1/P17-1052
https://doi.org/10.18653/v1/P17-1052


when used with only a manageable number (30K)
of the most frequent words. This demonstrates the
basic fact – knowledge of word leads to a powerful
representation. These results motivate us to pur-
sue an effective and efficient design of deep word-
level CNNs for text categorization. Note, however,
that it is not as simple as merely replacing charac-
ters with words in character-level CNNs; doing so
rather degraded accuracy in (Zhang et al., 2015).

We carefully studied deepening of word-level
CNNs in the large-data setting and found a deep
but low-complexity network architecture with
which the best accuracy can be obtained by in-
creasing the depth but not the order of computation
time – the total computation time is bounded by a
constant. We call it deep pyramid CNN (DPCNN),
as the computation time per layer decreases ex-
ponentially in a ‘pyramid shape’. After convert-
ing discrete text to continuous representation, the
DPCNN architecture simply alternates a convo-
lution block and a downsampling layer over and
over1, leading to a deep network in which inter-
nal data size (as well as per-layer computation)
shrinks in a pyramid shape. The network depth
can be treated as a meta-parameter. The computa-
tional complexity of this network is bounded to be
no more than twice that of one convolution block.
At the same time, as described later, the ‘pyramid’
enables efficient discovery of long-range associa-
tions in the text (and so more global information),
as the network is deepened. This is why DPCNN
can achieve better accuracy than the shallow CNN
mentioned above (hereafter ShallowCNN), which
can use only short-range associations. Moreover,
DPCNN can be regarded as a deep extension of
ShallowCNN, which we proposed in (Johnson and
Zhang, 2015b) and later tested with large datasets
in (Johnson and Zhang, 2016).

We show that DPCNN with 15 weight lay-
ers outperforms the previous best models on six
benchmark datasets for sentiment classification
and topic classification.

2 Word-level deep pyramid CNN
(DPCNN) for text categorization

Overview of DPCNN: DPCNN is illustrated in
Figure 1a. The first layer performs text region em-
bedding, which generalizes commonly used word

1Previous deep CNNs (either on image or text) tend to
be more complex and irregular, having occasional increase of
the number of feature maps.

embedding to the embedding of text regions cov-
ering one or more words. It is followed by stack-
ing of convolution blocks (two convolution lay-
ers and a shortcut) interleaved with pooling layers
with stride 2 for downsampling. The final pool-
ing layer aggregates internal data for each docu-
ment into one vector. We use max pooling for all
pooling layers. The key features of DPCNN are as
follows.

• Downsampling without increasing the num-
ber of feature maps (dimensionality of layer
output, 250 in Figure 1a). Downsampling
enables efficient representation of long-range
associations (and so more global informa-
tion) in the text. By keeping the same num-
ber of feature maps, every 2-stride downsam-
pling reduces the per-block computation by
half and thus the total computation time is
bounded by a constant.

• Shortcut connections with pre-activation and
identity mapping (He et al., 2016) for en-
abling training of deep networks.

• Text region embedding enhanced with un-
supervised embeddings (embeddings trained
in an unsupervised manner) (Johnson and
Zhang, 2015b) for improving accuracy.

2.1 Network architecture
Downsampling with the number of feature
maps fixed After each convolution block, we
perform max-pooling with size 3 and stride 2.
That is, the pooling layer produces a new inter-
nal representation of a document by taking the
component-wise maximum over 3 contiguous in-
ternal vectors, representing 3 overlapping text re-
gions, but it does this only for every other possible
triplet (stride 2) instead of all the possible triplets
(stride 1). This 2-stride downsampling reduces the
size of the internal representation of each docu-
ment by half.

A number of models (Simonyan and Zisser-
man, 2015; He et al., 2015, 2016; Conneau et al.,
2016) increase the number of feature maps when-
ever downsampling is performed, causing the to-
tal computational complexity to be a function of
the depth. In contrast, we fix the number of fea-
ture maps, as we found that increasing the num-
ber of feature maps only does harm – increasing
computation time substantially without accuracy
improvement, as shown later in the experiments.

563



3 conv, 250

Region embedding

3 conv, 250

3 conv, 250

Pooling, /2

3 conv, 250

+

Pooling

+

Downsampling

Repeat

Unsupervised 
embeddings

conv:W

pre-activation

optional

“A good buy !”

Downsampling

Repeat

W σ(x)+b 

activation

optional

(a) Our proposed model DPCNN

Pooling

Region embedding

Unsupervised 
embeddings

“A good buy !”

(b) cf. ShallowCNN [JZ15b]

3x3 conv, 64

3x3 conv, 64

3x3 conv, 128, /2

3x3 conv, 128

+

3x3 conv,128

3x3 conv, 128

+

3x3 conv, 256, /2

3x3 conv, 256

+

+

R
ep

eat 

snipped 

R
ep

eat 

7x7 conv, 64, /2

pooling, /2

image

R
ep

eat 

(c) cf. ResNet for image [HZRS15]

Figure 1: (a) Our proposed model DPCNN. (b,c) Previous models for comparison. ⊕ indicates addition.
The dotted red shortcuts in (c) perform dimension matching. DPCNN is dimension-matching free.

With the number of feature maps fixed, the compu-
tation time for each convolution layer is halved (as
the data size is halved) whenever 2-stride down-
sampling is performed, thus, forming a ‘pyramid’.

Computation per layer is halved 
after every pooling. 
Computation per layer is halved 
after every pooling. 

Therefore, with DPCNNs, the total computation
time is bounded by a constant – twice the com-
putation time of a single block, which makes our
deep pyramid networks computationally attrac-
tive.

In addition, downsampling with stride 2 essen-
tially doubles the effective coverage (i.e., cover-
age in the original document) of the convolution
kernel; therefore, after going through downsam-
pling L times, associations among words within
a distance in the order of 2L can be represented.
Thus, deep pyramid CNN is computationally effi-
cient for representing long-range associations and
so more global information.

Shortcut connections with pre-activation To
enable training of deep networks, we use additive
shortcut connections with identity mapping, which
can be written as z + f(z) where f represents
the skipped layers (He et al., 2016). In DPCNN,
the skipped layers f(z) are two convolution layers
with pre-activation. Here, pre-activation refers to
activation being done before weighting instead of
after as is typically done. That is, in the convolu-

tion layer of DPCNN, Wσ(x) +b is computed at
every location of each document where a column
vector x represents a small region (overlapping
with each other) of input at each location, σ(·) is a
component-wise nonlinear activation, and weights
W and biases b (unique to each layer) are the pa-
rameters to be trained. The number of W’s rows is
the number of feature maps (also called the num-
ber of filters (He et al., 2015)) of this layer. We set
activation σ(·) to the rectifier σ(x) = max(x, 0).
In our implementation, we fixed the number of
feature maps to 250 and the kernel size (the size
of the small region covered by x) to 3, as shown in
Figure 1a.

With pre-activation, it is the results of linear
weighting (Wσ(x) + b) that travel through the
shortcut, and what is added to them at a ⊕ (Figure
1a) is also the results of linear weighting, instead
of the results of nonlinear activation (σ(Wx +
b)). Intuitively, such ‘linearity’ eases training of
deep networks, similar to the role of constant er-
ror carousels in LSTM (Hochreiter and Schmid-
huder, 1997). We empirically observed that pre-
activation indeed outperformed ‘post-activation’,
which is in line with the image results (He et al.,
2016).

No need for dimension matching Although the
shortcut with pre-activation was adopted from the
improved ResNet of (He et al., 2016), our model
is simpler than ResNet (Figure 1c), as all the

564



shortcuts are exactly simple identity mapping (i.e.,
passing data exactly as it is) without any compli-
cation for dimension matching. When a shortcut
meets the ‘main street’, the data from two paths
need to have the same dimensionality so that they
can be added; therefore, if a shortcut skips a layer
that changes the dimensionality, e.g., by down-
sampling or by use of a different number of fea-
ture maps, then a shortcut must perform dimen-
sion matching. Dimension matching for increased
number of feature maps, in particular, is typically
done by projection, introducing more weight pa-
rameters to be trained. We eliminate the compli-
cation of dimension matching by not letting any
shortcut skip a downsampling layer, and by fixing
the number of feature maps throughout the net-
work. The latter also substantially saves compu-
tation time as mentioned above, and we will show
later in our experiments that on our tasks, we do
not sacrifice anything for such a substantial effi-
ciency gain.

2.2 Text region embedding

A CNN for text categorization typically starts with
converting each word in the text to a word vec-
tor (word embedding). We take a more general
viewpoint as in (Johnson and Zhang, 2015b) and
consider text region embedding – embedding of a
region of text covering one or more words.

Basic region embedding We start with the ba-
sic setting where there is no unsupervised embed-
ding. In the region embedding layer we compute
Wx + b for each word of a document where in-
put x represents a k-word region (i.e., window)
around the word in some straightforward manner,
and weights W and bias b are trained with the
parameters of other layers. Activation is delayed
to the pre-activation of the next layer. Now let
v be the size of vocabulary, and let us consider
the following three types of straightforward rep-
resentation of a k-word region for x: (1) sequen-
tial input: the kv-dimensional concatenation of k
one-hot vectors; (2) bow input: a v-dimensional
bag-of-word (bow) vector; and (3) bag-of-n-gram
input: e.g., a bag of word uni, bi, and trigrams con-
tained in the region. Setting the region size k = 1,
they all become word embedding.

A region embedding layer with the sequential
input is equivalent to a convolution layer applied
to a sequence of one-hot vectors representing a
document, and this viewpoint was taken to de-

scribe the first layer of ShallowCNN in (Johnson
and Zhang, 2015a,b). From the region embedding
viewpoint, ShallowCNN is DPCNN’s special case
in which a region embedding layer is directly fol-
lowed by the final pooling layer (Figure 1b).

A region embedding layer with region size k >
1 seeks to capture more complex concepts than
single words in one weight layer, whereas a net-
work with word embedding uses multiple weight
layers to do this, e.g., word embedding followed
by a convolution layer. In general, having fewer
layers has a practical advantage of easier optimiza-
tion. Beyond that, the optimum input type and
the optimum region size can only be determined
empirically. Our preliminary experiments indi-
cated that when used with DPCNN (but not Shal-
lowCNN), the sequential input has no advantage
over the bow input – comparable accuracy with k
times more weight parameters; therefore, we ex-
cluded the sequential input from our experiments2.
The n-gram input turned out to be prone to over-
fitting in the supervised setting, likely due to its
high representation power, but it is very useful as
the input to unsupervised embeddings, which we
discuss next.

Enhancing region embedding with unsuper-
vised embeddings In (Johnson and Zhang,
2015b, 2016), it was shown that accuracy was
substantially improved by extending ShallowCNN
with unsupervised embeddings obtained by tv-
embedding training (‘tv’ stands for two views). We
found that accuracy of DPCNN can also be im-
proved in this manner. Below we briefly review
tv-embedding training and then describe how we
use the resulting unsupervised embeddings with
DPCNN.

The tv-embedding training requires two views.
For text categorization, we define a region of text
as view-1 and its adjacent regions as view-2. Then
using unlabeled data, we train a neural network
of one hidden layer with an artificial task of pre-
dicting view-2 from view-1. The obtained hidden
layer, which is an embedding function that takes
view-1 as input, serves as an unsupervised embed-
ding function in the model for text categorization.
In (Johnson and Zhang, 2015b), we showed theo-
retical conditions on views and labels under which

2This differs from ShallowCNN where the sequential in-
put is often superior to bow input. We conjecture that when
bow input is used in DPCNN, convolution layers following
region embedding make up for the loss of local word order
caused by bow input, as they use word order.

565



AG Sogou Dbpedia Yelp.p Yelp.f Yahoo Ama.f Ama.p
# of training documents 120K 450K 560K 560K 650K 1.4M 3M 3.6M
# of test documents 7.6K 60K 70K 38K 50K 60K 650K 400K
# of classes 4 5 14 2 5 10 5 2
Average #words 45 578 55 153 155 112 93 91

Table 1: Data. Note that Yelp.f is a balanced subset of Yelp 2015. The results on these two datasets are
not comparable.

unsupervised embeddings obtained this way are
useful for classification.

For use with DPCNN, we train several unsu-
pervised embeddings in this manner, which dif-
fer from one another in the region size and the
vector representations of view-1 (input region) so
that we can benefit from diversity. The region
embedding layer of DPCNN computes Wx +∑

u∈U W
(u)z(u) + b , where x is the discrete in-

put as in the basic region embedding, and z(u) is
the output of an unsupervised embedding function
indexed by u. We will show below that use of
unsupervised embeddings in this way consistently
improves the accuracy of DPCNN.

3 Experiments

We report the experiments with DPCNNs in com-
parison with previous models and alternatives.
The code is publicly available on the internet.

3.1 Experimental setup
Data and data preprocessing To facilitate com-
parisons with previous results, we used the eight
datasets compiled by Zhang et al. (2015), summa-
rized in Table 1. AG and Sogou are news. Db-
pedia is an ontology. Yahoo consists of questions
and answers from the ‘Yahoo! Answers’ website.
Yelp and Amazon (‘Ama’) are reviews where ‘.p’
(polarity) in the names indicates that labels are bi-
nary (positive/negative), and ‘.f’ (full) indicates
that labels are the number of stars. Sogou is in
Romanized Chinese, and the others are in English.
Classes are balanced on all the datasets. Data pre-
processing was done as in (Johnson and Zhang,
2016). That is, upper-case letters were converted
to lower-case letters. Unlike (Kim, 2014; Zhang
et al., 2015; Conneau et al., 2016), variable-sized
documents were handled as variable-sized without
any shortening or padding; however, the vocabu-
lary size was limited to 30K words. For example,
as also mentioned in (Johnson and Zhang, 2016),
the complete vocabulary of the Ama.p training set

contains 1.3M words. A vocabulary of 30K words
is only a small portion of it, but it covers about
98% of the text and produced good accuracy as
reported below.

Training protocol We held out 10K documents
from the training data for use as a validation set on
each dataset, and meta-parameter tuning was done
based on the performance on the validation set.

To minimize a log loss with softmax, mini-
batch SGD with momentum 0.9 was conducted for
n epochs (nwas fixed to 50 for AG, 30 for Yelp.f/p
and Dbpedia, and 15 for the rest) while the learn-
ing rate was set to η for the first 45n epochs and
then 0.1η for the rest3. The initial learning rate η
was considered to be a meta-parameter. The mini-
batch size was fixed to 100. Regularization was
done by weight decay with the parameter 0.0001
and by optional dropout (Hinton et al., 2012) with
0.5 applied to the input to the top layer. In some
cases overfitting was observed, and so we per-
formed early stopping, based on the validation per-
formance, after reducing the learning rate to 0.1η.
Weights were initialized by the Gaussian distribu-
tion with zero mean and standard deviation 0.01.
The discrete input to the region embedding layer
was fixed to the bow input, and the region size was
chosen from {1,3,5}, while fixing output dimen-
sionality to 250 (same as convolution layers).

Details of unsupervised embedding training
To facilitate comparison with ShallowCNN, we
matched our unsupervised embedding setting ex-
actly with that of (Johnson and Zhang, 2016).
That is, we trained the same four types of tv-
embeddings, which are embeddings of 5- and 9-
word regions, each of which represents the in-
put regions by either 30K-dim bow or 200K-dim

3This learning rate scheduling method was used also in
(Johnson and Zhang, 2015a,b, 2016). It was meant to reduce
learning rate when error plateaus, as is often done on image
tasks, e.g., (He et al., 2015), though for simplicity, the timing
of reduction was fixed for each dataset.

566



Models Deep
Unsup.

Yelp.p Yelp.f Yahoo Ama.f Ama.p
embed.

1 DPCNN + unsupervised embed. X tv 2.64 30.58 23.90 34.81 3.32
2 ShallowCNN + unsup. embed. [JZ16] tv 2.90 32.39 24.85 36.24 3.79
3 Hierarchical attention net [YYDHSH16] X w2v – – 24.2 36.4 –
4 [CSBL16]’s char-level CNN: best X 4.28 35.28 26.57 37.00 4.28
5 fastText bigrams (Joulin et al., 2016) 4.3 36.1 27.7 39.8 5.4
6 [ZZL15]’s char-level CNN: best X 4.88 37.95 28.80 40.43 4.93
7 [ZZL15]’s word-level CNN: best X (w2v) 4.60 39.58 28.84 42.39 5.51
8 [ZZL15]’s linear model: best 4.36 40.14 28.96 44.74 7.98

Table 2: Error rates (%) on larger datasets in comparison with previous models. The previous results are
roughly sorted in the order of error rates (best to worst). The best results and the second best are shown
in bold and italic, respectively. ‘tv’ stands for tv-embeddings. ‘w2v’ stands for word2vec. ‘(w2v)’ in
row 7 indicates that the best results among those with and without word2vec pretraining are shown. Note
that ‘best’ in rows 4&6–8 indicates that we are giving an ‘unfair’ advantage to these models by choosing
the best test error rate among a number of variations presented in the respective papers.
[JZ16]: Johnson and Zhang (2016), [YYDHSH16]: Yang et al. (2016), [CSBL16]: Conneau et al. (2016), [ZZL15]: Zhang

et al. (2015)

bags of {1,2,3}-grams, retaining only the most fre-
quent 30K words or 200K {1,2,3}-grams. Train-
ing was done on the labeled data (disregarding the
labels), setting the training objectives to the pre-
diction of adjacent regions of the same size as the
input region (i.e., 5 or 9). Weighted square loss∑

i,j αi,j(zi[j] − pi[j])2 was minimized where i
goes through instances, z represents the target re-
gions by bow, p is the model output, and the
weights αi,j were set to achieve the negative sam-
pling effect. The dimensionality of unsupervised
embeddings was set to 300 unless otherwise spec-
ified. Unsupervised embeddings were fixed during
the supervised training – no fine-tuning.

3.2 Results

In the results below, the depth of DPCNN was
fixed to 15 unless otherwise specified. Making
it deeper did not substantially improve or degrade
accuracy. Note that we count as depth the number
of hidden weight layers including the region em-
bedding layer but excluding unsupervised embed-
dings, therefore, 15 means 7 convolution blocks of
2 layers plus 1 layer for region embedding.

3.2.1 Main results

Large data results We first report the error rates
of our full model (DPCNN with 15 weight lay-
ers plus unsupervised embeddings) on the larger
five datasets (Table 2). To put it into perspective,
we also show the previous results in the literature.

The previous results are roughly sorted in the or-
der of error rates from best to worst. On all the
five datasets, DPCNN outperforms all of the pre-
vious results, which validates the effectiveness of
our approach.

DPCNN can be regarded as a deep extension
of ShallowCNN (row 2), sharing region embed-
ding enhancement with diverse unsupervised em-
beddings. Note that ShallowCNN enhanced with
unsupervised embeddings (row 2) was originally
proposed in (Johnson and Zhang, 2015b) as a
semi-supervised extension of (Johnson and Zhang,
2015a), and then it was tested on the large datasets
in (Johnson and Zhang, 2016). The performance
improvements of DPCNN over ShallowCNN indi-
cates that the added depth is indeed useful, captur-
ing more global information. Yang et al. (2016)’s
hierarchical attention network (row 3) consists of
RNNs in the word level and the sentence level.
It is more complex than DPCNN due to the use
of RNNs and linguistic knowledge for sentence
segmentation. Similarly, Tang et al. (2015) pro-
posed to use CNN or LSTM to represent each sen-
tence in documents and then use RNNs. Although
we do not have direct comparison with Tang et
al.’s model, Yang et al. (2016) reports that their
model outperformed Tang et al.’s model. Conneau
et al. (2016) and Zhang et al. (2015) proposed deep
character-level CNNs (row 4&6). Their models
underperform our DPCNN with relatively large
differences in spite of their deepness. Our mod-

567



30

31

32

33

34

35

36

0 25 50 75

E
rr

o
r 

ra
te

 (
%

)

Computation time

Yelp.f character-leve CNN [CSBL16]

ShallowCNN [JZ16]

ShallowCNN+100-dim u.embed.

ShallowCNN+300-dim u.embed.

DPCNN

DPCNN+100-dim u.embed.

DPCNN+300-dim u.embed.

leve CNN [CSBL16]

dim u.embed.

dim u.embed.

dim u.embed.

dim u.embed.
30

31

32

33

34

35

0 5 10 15 20

E
rr

o
r 

ra
te

 (
%

)

Computation time

Yelp.f

Figure 2: Error rates and computation time. DPCNN, ShallowCNN, and Conneau et al. (2016)’s
character-level CNN. The x-axis is the time in seconds spent for categorizing 10K documents using
our implementation on Tesla M2070. The right figure is a close-up of x ∈ [0, 20] of the left figure.
Though shown on one particular dataset Yelp.f, the trend is the same on the other four large datasets.

els are word-level and therefore use the knowledge
of word boundaries which character-level mod-
els have no access to. While this is arguably not
an apple-to-apple comparison, since word bound-
aries can be obtained for free in many languages,
we view our model as much more useful in prac-
tice. Row 7 shows the performance of deep word-
level CNN from (Zhang et al., 2015), which was
designed to match their character-level models in
complexity. Its relatively poor performance shows
that it is not easy to design a high-performance
deep word-level CNN.

Computation time In Figure 2, we plot error
rates in relation to the computation time – the time
spent for categorizing 10K documents using our
implementation on a GPU. The right figure is a
close-up of x ∈ [0, 20] of the left figure. It stands
out in the left figure that the character-level CNN
of (Conneau et al., 2016) is much slower than
DPCNNs. This is partly because it increases the
number of feature maps with downsampling (i.e.,
no pyramid) while it is deeper (32 weight layers),
and partly because it deals with characters – there
are more characters than words in each document.
DPCNNs are more accurate than ShallowCNNs at
the expense of more computation time due to the
depth (15 layers vs. 1 layer). Nevertheless, their
computation time is comparable – the points of
both fit in the same range [0, 20]. The efficiency
of DPCNNs is due to the exponential decrease of
per-layer computation due to downsampling with
the number of feature maps being fixed.

Comparison with non-pyramid variants Fur-
thermore, we tested the following two ‘non-
pyramid’ models for comparison. The first model
doubles the number of feature maps at every other
downsampling so that per-layer computation is

31.5

32

32.5

33

0 10 20 30

E
rr

o
r 

ra
te

 (
%

)

Computation time

Yelp.f

No downsampling

Increase #feature maps

DPCNN

Increase #feature maps

Figure 3: Comparison with non-pyramid models.
Models of depth 11 and 15 are shown. No unsu-
pervised embeddings.

kept approximately constant4. The second model
performs no downsampling. Otherwise, these
two models are the same as DPCNN. We show
in Figure 3 the error rates of these two varia-
tions (labeled as ‘Increase #feature maps’ and ‘No
downsampling’, respectively) in comparison with
DPCNN. The x-axis is the computation time, mea-
sured by the seconds spent for categorizing 10K
documents. For all types, the models of depth 11
and 15 are shown. Clearly, DPCNN is more ac-
curate and computes faster than the others. Figure
3 is on Yelp.f, and we observed the same perfor-
mance trend on the other four large datasets.

Small data results Now we turn to the results
on the three smaller datasets in Table 3. Again,
the previous models are roughly sorted from best
to worst. For these small datasets, the DPCNN
performances with 100-dim unsupervised embed-

4Note that if we double the number of feature maps, it
would increase the computation cost of the next layer by 4
times as it doubles the dimensionality of both input and out-
put. On image, downsampling with stride 2 cancels it out
as it makes data 4 times smaller by shrinking both horizon-
tally and vertically, but text is one dimensional, and so down-
sampling with stride 2 merely halves data. That is why we
doubled the number of feature maps at every other downsam-
pling instead of at every downsampling to avoid exponential
increase of computation time.

568



Models Deep
Unsup.

AG Sogou Dbpedia
embed.

1 DPCNN + unsupervised embed. X tv 6.87 1.84 0.88
2 ShallowCNN + unsup. embed. [JZ16] tv 6.57 1.89 0.84
3 [ZZL15]’s linear model: best 7.64 2.81 1.31
4 [CSBL16]’s deep char-level CNN: best X 8.67 3.18 1.29
5 fastText bigrams (Joulin et al., 2016) 7.5 3.2 1.4
6 [ZZL15]’s word-level CNN : best X (w2v) 8.55 4.39 1.37
7 [ZZL15]’s deep char-level CNN: best X 9.51 4.88 1.55

Table 3: Error rates (%) on smaller datasets in comparison with previous models. The previous results
are roughly sorted in the order of error rates (best to worst). Notation follows that of Table 2.

3.2

3.3

3.4

3.5

0 2 4 6 8

E
rr

o
r 

ra
te

 (
%

)

Time

Yelp.p

31

32

33

34

0 2 4 6 8
Time

Yelpf

24.5

25

25.5

26

0 2 4
Time

Yahoo

6

Yahoo

35.5

36

36.5

37

37.5

0 1 2 3 4 5
Time

Ama.f

3.6

3.8

4

4.2

0 1 2 3 4 5
Time

Ama.p

ShallowCNN

DPCNN

Figure 4: Error rates of DPCNNs with various depths (3, 7, and 15). The x-axis is computation time.
No unsupervised embeddings.

dings are shown, which turned out to be as good
as those with 300-dim unsupervised embeddings.
One difference from the large dataset results is
that the strength of shallow models stands out.
ShallowCNN (row 2) rivals DPCNN (row 1), and
Zhang et al.’s best linear model (row 3) moved
up from the worst performer to the third best per-
former. The results are in line with the general fact
that more complex models require more training
data, and with the paucity of training data, simpler
models can outperform more complex ones.

3.2.2 Empirical studies
We present some empirical results to validate the
design choices. For this purpose, the larger five
datasets were used to avoid the paucity of training
data.

Depth Figure 4 shows error rates of DPCNNs
with 3, 7, and 15 weight layers (blue circles
from left to right). For comparison, the Shal-
lowCNN results (green ‘x’) from (Johnson and
Zhang, 2016) are also shown. The x-axis repre-
sents the computation time (seconds for catego-
rizing 10K documents on a GPU). For simplicity,
the results without unsupervised embeddings are
shown for all. The error rate improves as the depth
increases. The results confirm the effectiveness of
our strategy of deepening the network.

Unsupervised embeddings To study the effec-
tiveness of unsupervised embeddings, we experi-
mented with variations of DPCNN that differ only
in whether/how to use unsupervised embeddings
(Table 4). First, we compare DPCNNs with and
without unsupervised embeddings. The model
with unsupervised embeddings (row 1, copied
from Table 2 for easy comparison) clearly outper-
forms the one without them (row 4), which con-
firms the effectiveness of the use of unsupervised
embeddings. Second, in the proposed model (row
1), a region embedding layer receives two types
of input, the output of unsupervised embedding
functions and the high-dimensional discrete input
such as a bow vector. Row 2 shows the results ob-
tained by using unsupervised embeddings to pro-
duce sole input (i.e., no discrete vectors provided
to the region embedding layer). Degradations of
error rates are up to 0.32%, small but consistent.
Since the discrete input add almost no computa-
tion cost due to its sparseness, its use is desir-
able. Third, a number of previous studies used
unsupervised word embedding to initialize word
embedding in neural networks and then fine-tune
it as training proceeds (pretraining). The model
in row 3 does this with DPCNN using word2vec
(Mikolov et al., 2013). The word2vec training
was done on the training data (ignoring the labels),

569



Unsupervised embeddings Yelp.p Yelp.f Yahoo Ama.f Ama.p
1 tv-embed. (additional input) 2.64 30.58 23.90 34.81 3.32
2 tv-embed. (sole input) 2.68 30.66 24.09 35.13 3.45
3 word2vec (pretraining) 2.93 32.08 24.11 35.30 3.65
4 – 3.30 31.61 24.64 35.61 3.64

Table 4: Error rates (%) of DPCNN variations that differ in use of unsupervised embeddings. The rows
are roughly sorted from best to worst.

same as tv-embedding training. This model (row
3) underperformed our proposed model (row 1).
We attribute the superiority of the proposed model
to its use of richer information than a word embed-
ding. These results support our approach.

4 Conclusion

This paper tackled the problem of designing high-
performance deep word-level CNNs for text cat-
egorization in the large training data setting. We
proposed a deep pyramid CNN model which has
low computational complexity, and can efficiently
represent long-range associations in text and so
more global information. It was shown to outper-
form the previous best models on six benchmark
datasets.

References
Alexis Conneau, Holger Schwenk, Loı̈c Barrault,

and Yann LeCun. 2016. Very deep convolu-
tional networks for natural language processing.
arXiv:1606.01781v1 (6 June 2016 version) .

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep residual learning for image recog-
nition. arXiv:1512.03385 .

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Identity mappings in deep residual net-
works. arXiv:1603.05027 .

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
arXiv:1207.0580 .

Sepp Hochreiter and Jürgen Schmidhuder. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Rie Johnson and Tong Zhang. 2015a. Effective use
of word order for text categorization with convolu-
tional neural networks. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics Human Language Technologies
(NAACL HLT).

Rie Johnson and Tong Zhang. 2015b. Semi-supervised
convolutional neural networks for text categoriza-
tion via region embedding. In Advances in Neural
Information Processing Systems 28 (NIPS 2015).

Rie Johnson and Tong Zhang. 2016. Con-
volutional neural networks for text categoriza-
tion: Shallow word-level vs. deep character-level.
arXiv:1609.00718 .

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efficient
text classification. arXiv:1607.01795v3 (9 Aug 2016
version) .

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP). pages 1746–1751.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems 26 (NIPS 2013).

Karen Simonyan and Andrew Zisserman. 2015. Very
deep convolutional networks for large-scale image
recognition. In Proceedings of International Con-
ference on Learning Representations (ICLR).

Duyu Tang, Bing Qin, and Ting Liu. 2015. Docu-
ment modeling with gated recurrent neural network
for sentiment classification. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP).

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics Human
Language Technologies (NAACL HLT).

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems 28 (NIPS 2015).

570


	Deep Pyramid Convolutional Neural Networks for Text Categorization

