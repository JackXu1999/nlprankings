



















































A Usage-Based Model of Early Grammatical Development


Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 46–54,
Baltimore, Maryland USA, June 26 2014. c©2014 Association for Computational Linguistics

A Usage-Based Model of Early Grammatical Development
Barend Beekhuizen

LUCL
Leiden University

b.f.beekhuizen@hum.leidenuniv.nl

Rens Bod
ILLC

University of Amsterdam
l.w.m.bod@uva.nl

Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science

University of Toronto
afsaneh,suzanne@cs.toronto.edu

Arie Verhagen
LUCL

Leiden University
a.verhagen@hum.leidenuniv.nl

Abstract

The representations and processes yield-
ing the limited length and telegraphic style
of language production early on in acqui-
sition have received little attention in ac-
quisitional modeling. In this paper, we
present a model, starting with minimal lin-
guistic representations, that incrementally
builds up an inventory of increasingly long
and abstract grammatical representations
(form+meaning pairings), in line with the
usage-based conception of language ac-
quisition. We explore its performance on
a comprehension and a generation task,
showing that, over time, the model bet-
ter understands the processed utterances,
generates longer utterances, and better ex-
presses the situation these utterances in-
tend to refer to.

1 Introduction

A striking aspect of language acquisition is the dif-
ference between children’s and adult’s utterances.
Simulating early grammatical production requires
a specification of the nature of the linguistic repre-
sentations underlying the short, telegraphic utter-
ances of children. In the usage-based view, young
children’s grammatical representions are thought
to be less abstract than adults’, e.g. by having
stricter constraints on what can be combined with
them (cf. Akhtar and Tomasello 1997; Bannard
et al. 2009; Ambridge et al. 2012). The represen-
tations and processes yielding the restricted length
of these early utterances, however, have received
little attention. Following Braine (1976), we adopt
the working hypothesis that the early learner’s
grammatical representations are more limited in
length (or: arity) than those of adults.

Similarly, in computational modeling of gram-
mar acquisition, comprehension has received more

attention than language generation. In this pa-
per we attempt to make the mechanisms underly-
ing early production explicit within a model that
can parse and generate utterances, and that in-
crementally learns constructions (Goldberg, 1995)
on the basis of its previous parses. The model’s
search through the hypothesis space of possible
grammatical patterns is highly restricted. Start-
ing from initially small and concrete representa-
tions, it learns incrementally long representations
(syntagmatic growth) as well as more abstract
ones (paradigmatic growth). Several models ad-
dress either paradigmatic (Alishahi and Stevenson,
2008; Chang, 2008; Bannard et al., 2009) or syn-
tagmatic (Freudenthal et al., 2010) growth. This
model aims to explain both, thereby contribut-
ing to the understanding of how different learning
mechanisms interact. As opposed to other models
involving grammars with semantic representations
(Alishahi and Stevenson, 2008; Chang, 2008), but
similar to Kwiatkowski et al. (2012), the model
starts without an inventory of mappings of single
words to meanings.

Based on motivation from usage-based and con-
struction grammar approaches, we define several
learning principles that allow the model to build
up an inventory of linguistic representations. The
model incrementally processes pairs of an utter-
ance U , consisting of a string of words w1 . . . wn,
and a set of situations S, one of which is the situa-
tion the speaker intends to refer to. The other situ-
ations contribute to propositional uncertainty (the
uncertainty over which proposition the speaker is
trying to express; Siskind 1996). The model tries
to identify the intended situation and to understand
how parts of the utterance refer to certain parts of
that situation. To do so, the model uses its growing
inventory of linguistic representations (Section 2)
to analyze U , producing a set of structured seman-
tic analyses or parses (Fig. 1, arrow 1; Section 3).

46



The resulting best parse, U and the selected situa-
tion are then stored in a memory buffer (arrow 2),
which is used to learn new constructions (arrow
3) using several learning mechanisms (Section 4).
The learned constructions can then be used to gen-
erate utterances as well. We describe two experi-
ments: in the comprehension experiment (Section
5), we evaluate the model’s ability to parse the
stream of input items. In the generation experi-
ment (Section 6), the model generates utterances
on the basis of a given situation and its linguistic
knowledge. We evaluate the generated utterances
given different amounts of training items to con-
sider the development of the model over time.

2 Representations

We represent linguistic knowledge as construc-
tions: pairings of a signifying form and a signi-
fied (possibly incomplete) semantic representation
(Goldberg, 1995). The meaning is represented as
a graph with the nodes denoting entities, events,
and their relations, connected by directed unla-
beled edges. The conceptual content of each node
is given by a set of semantic features. We assume
that meaning representations are rooted trees. The
signifying form consists of a positive number of
constituents. Every constituent has two elements:
a phonological form, and a pointer to a node in the
signified meaning (in line with Verhagen 2009).
Both can be specified, or one can be left empty.
Constituents with unspecified phonological forms
are called open, denoted with � in the figures. The
head constituent of a construction is defined as
the constituent that has a pointer to the root node
of the signified meaning. We furthermore require
that no two constituents point to the same node of
the signified meaning.

This definition generalizes over lexical ele-
ments (one phonologically specified constituent)
as well as larger linguistic patterns. Fig. 2, for in-
stance, shows two larger constructions being com-
bined with each other. We call the set of construc-
tions the learner has at some moment in time the
constructicon C (cf. Goldberg 2003).

3 Parsing

3.1 Parsing operations
We first define a derivation d as an assembly
of constructions in C, using four parsing opera-
tions defined below. In parsing, derivations are
constrained by the utterance U and the situations

utterance

situation 1

situation n

situation 2

...

situations

input item

construction 1

construction 2

construction 3

construction n

constructicon

analysis

(utterance, intended situation, analysis)

...

...

memory buffer

1 1
2

3(utterance, intended situation, analysis)

(utterance, intended situation, analysis)

Figure 1: The global flow of the model

S, whereas in production, only a situation s con-
strains the derivation. The leaf nodes of a deriva-
tion must consist of phonological constraints of
constructions that (in parsing) are satisfied by U .
All constructions used in a derivation must map to
the same situation s ∈ S. A construction cmaps to
s iff the meaning of c constitutes a subgraph of s,
with the features on each of the nodes in the mean-
ing of c being a subset of the features on the corre-
sponding node of s. Moreover, each construction
must map to a different part of s. This constitutes
a mutual exclusivity effect in analyzing U : every
part of the analysis must contribute to the compos-
ite meaning. A derivation d thus gives us a map-
ping between the composed meaning of all con-
structions used in d and one situation s ∈ S. The
aggregate mapping specifies a subgraph of s that
constitutes the interpretation of that derivation.

The central parsing operation is the COMBINA-
TION operator ◦. In ci ◦ cj , the leftmost open con-
stituent of ci is combined with cj . Fig. 2 illus-
trates COMBINATION. COMBINATION succeeds if
both the semantic pointer of the leftmost open con-
stituent of ci and the semantic pointer of the head
constituent of cj map to the same semantic node
of a situation s

Initially, the model has few constructions to an-
alyze the utterance with. Therefore, we define
three other operations that allow the model to cre-
ate a derivation over the full utterance without
combining constructions. First, a known or un-
known word that cannot be fit into a derivation,
can be IGNOREd. Second, an unknown word can
be used to fill an open constituent slot of a con-
struction with the BOOTSTRAP operator. Boot-
strapping entails that the unknown word will be
associated with the semantics of the node. Finally,
the learner can CONCATENATE multiple deriva-
tions, by linearly sequencing them, thus creating a
more complex derivation without combining con-

47



{move}

{agent,mover} {location,goal}

{animate,Adam} {surface}

PHON: Adam

SEM:

PHON: ɛ

SEM:

{move}

{agent,mover} {location,goal}

{animate,Adam}

situation

O =

PHON: put

SEM:

PHON: it

SEM:

{move}

{patient,moved} {location,goal}

{object,entity} {surface}

{patient,moved}

{object,entity}

PHON: Adam

SEM:

PHON: ɛ

SEM:

{move}

{agent,mover} {location,goal}

{animate,Adam}

PHON: put

SEM:

PHON: it

SEM:

{move}

{patient,moved} {location,goal}

{object,entity} {surface}

Leftmost open constituent of 
this construction, pointing to 
{move}-node of meaning

Figure 2: Combining constructions. The dashed lines represent semantic pointers, either from con-
stituents to the constructional meaning (black) or from the constructions to the situation (red and blue).

bootstrap

w
3

ignore

meaning

c
1

c
2

meaning

c
1

w
1

w
2

w
4

c
1

meaning
concatenate

Figure 3: The CONCATENATE, IGNORE and
BOOTSTRAP operators (internal details of the con-
structions left out).

structions. This allows the learner to interpret a
larger part of the situation than with COMBINA-
TION only. The resulting sequences may be ana-
lyzed in the learning process as constituting one
larger construction, consisting of the parts of the
concatenated derivations. Fig. 3 illustrates these
three operations.

3.2 Selecting the best analysis

Multiple derivations can be highly similar in the
way they map parts of U to parts of an s ∈ S. We
define a parse to be a set of derivations that have
the same internal structure and the same mappings
to a situation, but that use different constructions
in doing so (cf. multiple licensing; Kay 2002). We
take the most probable parse of U to be the best
analysis of U . The most probable parse points to a
situation, which the model then assumes to be the
identified situation or sidentified. If no parse can be

made, sidentified is selected at random from S.
The probability of a parse p is given by the sum

of the probabilities of the derivations d subsumed
under that parse, which in turn are defined as the
product of the probabilities of the constructions c
used in d.

P (p) =
∑
d∈p

P (d) (1)

P (d) =
∏
c∈d

P (c) (2)

The probability of a construction P (c) is given
by its relative frequency (count) in the construc-
ticon C, smoothed with Laplace smoothing. We
assume that the simple parsing operations of IG-
NORE, BOOTSTRAP, and CONCATENATION reflect
usages of an unseen construction with a count of
0.

P (c) =
c.count+ 1∑

c′∈C
c′.count+ |C|+ 1 (3)

The most probable parse, U and sidentified are
added to the memory buffer. The memory buffer
has a pre-set maximal length, discarding the oldest
exemplars upon reaching this length. In the future,
we plan to consider more realistic mechanisms for
the memory buffer, such as graceful degradation,
and attention effects.

48



4 Learning mechanisms

The model uses the best parse of the utterance to
develop its knowledge of the constructions in the
constructicon C. Two simple operations, UPDATE
and ASSOCIATION, are used to create initial con-
structions and reinforce existing ones respectively.
Two additional operations, PARADIGMATIZATION
and SYNTAGMATIZATION, are key to the model’s
ability to extend these initial representations by
inducing novel constructions that are richer and
more abstract than existing ones.

4.1 Direct learning from the best parse

The best parse is used to UPDATE C. For this
mechanism, the model uses the concrete mean-
ing of sidentified rather than the (potentially more
abstract) meaning of the constructions in the best
parse.1 Every construction in the parse is assigned
the subgraph of sidentified it maps to as its new
meaning, and the count of the adjusted construc-
tion is incremented with 1, or added to C with a
count of 1, if it does not yet exist. This includes
applications of the BOOTSTRAP operation, creat-
ing a mapping of the previously unknown word to
a situational meaning.

ASSOCIATE constitutes a form of simple cross-
situational learning over the memory buffer. The
intuition is that co-occurring word sequences
and meaning components that remain unanalyzed
across multiple parses might themselves comprise
the form-meaning pairing of a construction. If the
unanalyzed parts of two situations contain an over-
lapping subgraph, and the unanalyzed parts of two
utterances an overlapping subsequence of words,
the two are mapped to each other and added to C
with a count of 0.

4.2 Qualitative extension of the best parse

Syntagmatization Some of the processes de-
scribed thus far yield analyses of the input in
which constructions are linearly associated but
lack appropriate relational structure among them.
The model requires a process, which we call SYN-
TAGMATIZATION, that enables it to induce further
hierarchical structure.

In order for the learner to acquire constructions
in which the different constituents point to differ-
ent parts of the construction’s meaning, the ASSO-

1This follows Langacker’s (2009) claim that the processed
concrete usage events should leave traces in the learner’s
mind.

CIATE operation does not suffice. We assume that
the learner is able to learn such constructions by
using concatenated derivations. The process we
propose is SYNTAGMATIZATION. In this process,
the various concatenated derivations are taken as
constituents of a novel construction. This instanti-
ates the idea that joint processing of two (or more)
events gradually leads to a joint representation of
these, previously independent, events.

More precisely, the process starts by taking the
top nodes T of the derivations in the best parse,
where T consists of the single top node if no CON-
CATENATION has been applied, or the set of con-
catenated nodes of the parse tree if CONCATENA-
TION has been applied (e.g. for the derivation in
Fig. 3, |T | = 2). For each top node t ∈ T , we take
the root node of the construction’s meaning, and
define its semantic frame to consist of all children
(roles) and grandchildren (role-fillers) of the node
in the situation it maps to. The model then forms a
novel construction csyn by taking all the construc-
tions in the parse whose semantic root nodes point
to a node in this semantic frame, referring to those
as the set R of semantically related constructions.
As the novel meaning of csyn, the model takes the
subgraph of the situation mapped to by the joint
mapping of all constructional meanings of con-
structions in R.
R, as well as all phonologically specified con-

stituents of t itself, are then linearized as the con-
stituents of csyn. The novel construction thus con-
stitutes a construction with a higher arity, ‘joining’
several previously independent constructions. Fig.
4 illustrates the syntagmatization mechanism.

Paradigmatization Due to our usage-driven ap-
proach, all learning mechanisms so far give us
maximally concrete constructions. In order for the
model to generalize beyond the observed input,
some degree of abstraction is needed. The model
does so with the PARADIGMATIZATION mecha-
nism. This mechanism recursively looks for min-
imal abstractions (cf. Tomasello 2003, 123) over
the constructions in C and adds those to C, thus
creating a full-inheritance network (cf. Langacker
1989, 63-76).

An abstraction over a set of constructions is
made if there is an overlapping subgraph between
the meanings of the constructions, where every
node of the subgraph is the non-empty feature
set intersection between two mapped nodes of the
constructional meanings. Furthermore, the con-

49



{act}

{volitional,...}

{animate,
hearer}

{independent-
exist}

SEM SEM

PHON: you PHON: ɛ

{act,move}

{volitional,...} {independent-
exist}

SEM

PHON: take

concatenate

{object,entity,
ball}

SEM

PHON: ball

{act}

{volitional,...}

{animate,
hearer}

{independent-
exist}

SEM SEM

PHON: you PHON: take

{object,entity,ba
ll}

SEM

PHON: ball

A derivation over the utterance you take it.

A novel, syntagmatized construction

Figure 4: The SYNTAGMATIZATION mechanism. The mechanism takes a derivation as its input and
reinterprets it as a novel construction of higher arity).

stituents must be mappable: both constructions
have the same number of constituents and the
paired constituents point to a mapped node of the
meaning. The meaning of the abstracted construc-
tion is then set to this overlapping subgraph, which
is the lowest possible semantic abstraction over
the constructions. The constituents of this new ab-
straction have a specified phonological form if the
more concrete constructions share the same word,
and an unspecified one otherwise. The count of an
abstracted construction is given by the cardinality
of the set of its direct descendants in the network.
This generalizes Bybee’s (1995) idea about type
frequency as a proxy for productivity to a network
structure. Fig. 5 illustrates the paradigmatization
mechanism.

5 Experimental set-up

The model is incrementally presented with U, S
pairings based on Alishahi & Stevenson’s (2010)
generation procedure. In this procedure, an utter-
ance and a semantic frame expressing its meaning
(a situation) are generated. The generation pro-
cedure follows distributions occurring in a corpus
of child-directed speech. As we are interested in
the performance of the model under propositional
uncertainty, we add a parametrized number of ran-
domly sampled situations, so that S consists of the
situation the speaker intends to refer to (scorrect)
and a number of situations the speaker does not
intend to refer to.2 Here, we set the number of ad-

2We are currently researching the effects of sampling non-
correct situations that have a greater likelihood of overlap

ditional situations to be 1 or 5; the other parameter
of the model, the size of the memory buffer, is set
to 5 exemplars.

For the comprehension experiment, we eval-
uate the model’s performance parsing the input
items, averaging over every 50 U, S pairs. We
track the ability to identify the intended situation
from S. Identification succeeds if the best parse
maps to scorrect, i.e. if sidentified = scorrect. Next,
situation coverage expresses what proportion of
sidentified has been interpreted and thus how rich the
meanings of the used constructions are. It is de-
fined as the number of nodes of the interpretation
of the best parse, divided by the number of nodes
of sidentified. Finally, utterance coverage tells us
what proportion of U has been parsed with con-
structions (excluding IGNORED; including BOOT-
STRAPPED words). The measure expresses the
proportion of the signal that the learner (correctly
or incorrectly) is able to interpret.

For exploring language production, the model
receives a situation, and (given the constructicon)
finds the most probable, maximally expressive,
fully lexicalized derivation expressing it. That is:
among all derivations terminating in phonologi-
cally specified constituents, it selects the deriva-
tions that cover the most semantic nodes of the
given situation. In the case of multiple such
derivations, it selects the most probable one, fol-
lowing the probability model in Section 3. We
only allow for the COMBINATION operator in the
derivations, as BOOTSTRAPPING and IGNORE re-

with the intended situation, to reflect more realistic input (cf.
Siskind 1996).

50



{cause,move}

{volitional,...}

{animate,hearer}

{patient,...}

{location,entity,chair}

SEM SEM SEM

PHON: you PHON: take PHON: chair

{cause,move}

{volitional,...}

{animate,hearer}

{patient,...}

{location,entity,table}

{cause,move}

{volitional,...}

{animate,hearer}

{patient,...}

{location,entity}

SEM SEM SEM

PHON: you PHON: take PHON: ɛ

SEM SEM SEM

PHON: you PHON: take PHON: table

A phonologically empty 
constituent, generalizing 
over chair and table 

The set intersection of 
{location,entity,chair} and 
{location,entity, table}

Figure 5: The PARADIGMATIZATION mechanism. The construction on top is an abstraction obtained
over the two constructions at the bottom.

fer to words in a given U , and CONCATENATE is a
back-off method for analyzing more of U than the
constructicon allows for. The situations used in the
generation experiment do not occur in the training
items, so that we truly measure the model’s ability
to generate utterances for novel situations.

The phonologically specified leaf nodes of the
best derivation constitute the generated utterance
Ugen. Ugen is evaluated on the basis of its mean
length, in number of words, its situation cover-
age, as defined in the comprehension experiment,
and its utterance precision and utterance recall.
To calculate these, we take the maximally overlap-
ping subsequenceUoverlap between the actual utter-
ance Uact associated with the situation and Ugen.
Utterance precision (how many words are gener-
ated correctly) and utterance recall (how many of
the correct words are generated) are defined as:

Utterance precision =
|Uoverlap|
|Ugen| (4)

Utterance recall =
|Uoverlap|
|Uact| (5)

Because the U, S-pairs on which the model was
trained, are generated randomly, we show results
for comprehension and production averaged over
5 simulations.

6 Experiments

A central motivation for the development of this
model is to account for early grammatical produc-
tion: can we simulate the developmental pattern
of the growth of utterance length and a growing
potential for generalization? The same construc-
tions underlying these productions should, at the
same time, also account for the learner’s increas-
ing grasp of the meaning of U . To explore the
model’s performance in both domains, we present
a comprehension and a generation experiment.

6.1 Comprehension results

Fig. 6a gives us the results over time of the com-
prehension measures given a propositional un-
certainty of 1, i.e. one situation besides scorrect
in S. Overall, the model understands the utter-
ances increasingly well. After 2000 input items,
the model identifies scorrect in 95% of the cases.
With higher levels of propositional uncertainty
(not shown here), performance is still relatively
robust: given 5 incorrect situations in S, scorrect
is identified in 62% of all cases (random guess-
ing gives a score of 17%, or 16 ). Similarly, the
proportion of the situation interpreted and the pro-
portion of the utterance analyzed go up over time.
This means that the model builds up an increasing
repertoire of constructions that allow it to analyze
larger parts of the utterance and the situations it
identifies. It is important to realize that these mea-

51



0 500 1000 1500 2000

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

time

pr
op

or
tio

n

measures

situation coverage
utterance coverage
identification

(a) Comprehension results over time

0 500 1000 1500 2000

0.
0

0.
5

1.
0

1.
5

2.
0

2.
5

3.
0

time

ut
te

ra
nc

e 
le

ng
th

 in
 w

or
ds

(b) Length of Ugen over time

0 500 1000 1500 2000

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

time

pr
op

or
tio

n

measures

situation coverage
utterance precision
utterance recall

(c) Generation results over time

Figure 6: Quantitative results for the comprehension and generation experiments

sures do not display what proportion of the utter-
ance or situation is analyzed correctly.

6.2 Generation results
Quantitative results Fig. 6b shows that the av-
erage utterance length increases over time. This
indicates that the number of constituents of the
used constructions grows. Next, Fig. 6c shows the
performance of the model on the generation task.
After 2000 input items, the model generates pro-
ductions expressing 93% of the situation, with an
utterance precision of 0.91, and an utterance recall
of 0.81. Given a propositional uncertainty of 5,
these go down to 79%, 0.76 and 0.59 respectively.

Comparing the utterance precision and recall
over time, we can see that the utterance preci-
sion is high from the start, whereas the recall
gradually increases. This is in line with the ob-
servation that children predominantly produce er-
rors of omission (leaving linguistic material out an
adult speaker would produce), and few errors of
comission (producing linguistic material an adult
speaker would not produce).

Qualitative results Tracking individual produc-
tions given specific situations over time allows us
to study in detail what the model is doing. Here,
we look at one case qualitatively. Given the sit-
uation for which the Uact is she put them away,
the model generates, over time, the utterances in
Table 1. The brackets show the internal hierarchi-
cal structure of the derivation. This development
illustrates several interesting aspects of the model.
First, as discussed earlier, the model mostly makes
errors of omission: earlier productions leave out
more words found in the adult utterances. Only at
t = 550, the model makes an error of commission,
using the word in erroneously.

[[
sh

e]
pu

t]

[s
he

[p
ut

]]

[[
sh

e]
[p

ut
][

in
]]

[[
sh

e]
pu

tt
he

m
[a

w
ay

]]

[[
sh

e]
pu

t[
th

em
]]

[[
sh

e]
pu

tt
he

m
[a

w
ay

]]

[[
sh

e]
pu

t[
th

em
]a

w
ay

]

[[
sh

e]
pu

tt
he

m
aw

ay
]

t 50 500 550 600 950 1000 1050 1400

Table 1: Generations over time t for one situation.

Starting from t = 600 (except at t = 950),
the model generates the correct utterance, but the
derivations leading to this production differ. At
t = 550, for instance, the learner combines a
completely non-phonologically specific construc-
tion for which the constituents refer to the agent,
action and goal location, with three ‘lexical’ con-
structions that fill in the words for those items..
The constructions used after t = 550 are all more
specific, combining 3, or even only 2 constructions
(t ≥ 1400) where the entire sequence of words
“put them away” arises from a single construction.

Using less abstract constructions over time
seems contrary to the usage-based idea that con-
structions become more abstract over the course of
acquisition. However, this result follows from the
way the probability model is defined. More spe-
cific constructions that are able to account for the
input will entail fewer combinations, and a deriva-
tion with fewer combination operations will often
be more likely than one with more such opera-
tions. Given equal expressivity of the situation,
the former derivation will be selected over the lat-
ter in generation.

The effect is indeed in line with another concept
hypothesized to play a role in language acquisition
on a usage-based account, viz. pre-emption (Gold-

52



{cause,move}

{volitional,...,,
cause-location}

{location,
destination}

{animate,..}

{affected,...,
stationary}

{object,artefact} {object, artefact}

SEM SEM SEM SEM SEM
PHON: putPHON: ɛ PHON: ɛ PHON: ɛ PHON: ɛ

{rest,act}

{volitional
, ...}

{animate,...}

{destination,
location}

{location,entity}

SEM SEM SEM
PHON:ɛ PHON:ɛ PHON:ɛ

{act}

{volitional,...}

{animate,
hearer}

{indepen-
dent-exist}

{object,artefact}

SEM SEM SEM
PHON: you PHON: itPHON

(b) (c)

(a)

Figure 7: Some representations at t = 2000

berg, 2006, 94-95). Pre-emption is the effect that
a language user will select a more concrete rep-
resentation over the combination of more abstract
ones. The effect can be reconceptualized in this
model as an epiphenomenon of the way the prob-
ability model works: simply because combining
fewer constructions in a derivation is often more
probable than combining more constructions, the
former derivation will be selected over the lat-
ter. Pre-emption is typically invoked to explain the
blocking of overgeneralization patterns, and an in-
teresting future step will be to see if the model can
simulate developmental patterns for well-known
cases of overgeneralization errors.

The potential for abstraction The paradigma-
tization operation allows the model to go beyond
observed concrete instances of form-meaning
pairings: without it, unseen situations could never
be fully expressed. Despite this potential, we have
seen that the model relies on highly concrete con-
structions. The concreteness of the used patterns,
however, does not imply the absence of more ab-
stract representations. Fig. 7 gives three exam-
ples of constructions in C in one simulation. Con-
struction (a) could be seen as a verb-island con-
struction (Tomasello, 1992, 23-24). The second
constituent is phonologically specified with put,
and the other arguments are open, but mapped to
specific semantic functions. This pattern allows
for the expression of many caused-motion events.
Construction (b) is the inverse of (a): the argu-
ments are phonologically specified, but the verb-
slot is open. This would be a case of a pronominal
argument frame [you V it], which have been found
to be helpful in the bootstrapping of verbal mean-

ings (Tomasello, 2001). Finally, (c) presents a case
of full abstraction. This construction licenses ut-
terances such as I sit here, you stay there and er-
roneous ones like he sits on (which, again, will be
pre-empted in the generation of utterances if more
concrete constructions licence he sits on it).

Summarizing, abstract constructions are ac-
quired, but only used for those cases in which no
concrete construction is available. This is in line
with the usage-based hypotheses that abstract con-
structions do emerge, but that for much of lan-
guage production, a language user can rely on
highly concrete patterns. A next step will be
to measure the development of abstractness and
length over the constructions themselves, rather
than the parses and generations they allow.

7 Conclusion

This, admittedly complex, model forms an attempt
to model different learning mechanisms in interac-
tion from a usage-based constructionist perspec-
tive. Starting with an empty set of linguistic rep-
resentations, the model acquires words and gram-
matical constructions simultaneously. The learn-
ing mechanisms allow the model to build up in-
creasingly abstract, as well as increasingly long
constructions. With these developing representa-
tions, we showed how the model gets better over
time at understanding the input item, performing
relatively robustly under propositional uncertainty.

Moreover, in the generation experiment, the
model shows patterns of production (increasingly
long utterances) similar to those of children. An
important future step will be to look at these pro-
ductions more closely and investigate if they also
converge on more detailed patterns of develop-
ment in the production of children (e.g. item-
specificity, as hypothesized on the usage-based
view). Despite highly concrete constructions suf-
ficing for most of production, inspection of the ac-
quired representations tells us that more abstract
constructions are learned as well. Here, an inter-
esting next step would be to simulate patterns of
overgeneralization in children’s production.

Acknowledgements

We would like to thank three anonymous review-
ers for their valuable and thoughtful comments.
We gratefully acknowledge the funding of BB
through NWO of the Netherlands (322.70.001)
and AF and SS through NSERC of Canada.

53



References

Nameera Akhtar and Michael Tomasello. 1997.
Young Children’s Productivity With Word Or-
der and Verb Morphology. Developmental Psy-
chology, 33(6):952–965.

Afra Alishahi and Suzanne Stevenson. 2008 A
Computational Model of Early Argument Struc-
ture Acquisition. Cognitive Science, 32(5):789–
834.

Afra Alishahi and Suzanne Stevenson. 2010. A
computational model of learning semantic roles
from child-directed language. Language and
Cognitive Processes, 25(1):50–93.

Ben Ambridge, Julian M Pine, and Caroline F
Rowland. 2012. Semantics versus statistics in
the retreat from locative overgeneralization er-
rors. Cognition, 123(2):260–79.

Colin Bannard, Elena Lieven, and Michael
Tomasello. 2009. Modeling children’s early
grammatical knowledge. Proceedings of the
National Academy of Sciences of the United
States of America, 106(41):17284–9.

Martin D.S. Braine. 1976. Children’s first word
combinations. University of Chicago Press,
Chicago, IL.

Joan Bybee. 1995. Regular morphology and the
lexicon. Language and Cognitive Processes, 10
(5):425–455.

Nancy C.-L. Chang. 2008. Constructing Gram-
mar: A computational model of the emergence
of early constructions. Dissertation, University
of California, Berkeley.

Daniel Freudenthal, Julian Pine, and Fernand Go-
bet. 2010. Explaining quantitative variation in
the rate of Optional Infinitive errors across lan-
guages: a comparison of MOSAIC and the Vari-
ational Learning Model. Journal of Child Lan-
guage, 37(3):643–69.

Adele E. Goldberg. 1995. Constructions. A
Construction Grammar Approach to Argument
Structure. Chicago University Press, Chicago,
IL.

Adele E Goldberg. 2003. Constructions: a new
theoretical approach to language. Trends in
Cognitive Sciences, 7(5):219–224.

Adele E. Goldberg. 2006. Constructions at Work.
The Nature of Generalization in Language. Ox-
ford University Press, Oxford.

Paul Kay. 2002. An Informal Sketch of a Formal
Architecture for Construction Grammar. Gram-
mars, 5:1–19.

Tom Kwiatkowski, Sharon Goldwater, Luke
Zettlemoyer, and Mark Steedman. 2012. A
Probabilistic Model of Syntactic and Seman-
tic Acquisition from Child-Directed Utterances
and their Meanings. In Proceedings EACL.

Ronald W. Langacker. 1989. Foundations of Cog-
nitive Grammar, Volume I. Stanford University
Press.

Ronald W. Langacker. 2009. A dynamic view of
usage and language acquisition. Cognitive Lin-
guistics, 20(3):627–640.

Jeffrey M Siskind. 1996. A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):39–
91.

Michael Tomasello. 1992. First Verbs: A study
of early grammatical development. Cambridge
University Press, Cambridge, UK.

Michael Tomasello. 2001 Perceiving intentions
and learning words in the second year of life.
In Melissa Bowerman and Stephen C. Levinson,
editors, Language Acquisition and Conceptual
Development, chapter 5, pages 132–158. Cam-
bridge University Press, Cambridge, UK.

Michael Tomasello. 2003. Constructing a lan-
guage: A Usage-Based Theory of Language
Acquisition. Harvard University Press, Cam-
bridge, MA.

Arie Verhagen. 2009 The conception of construc-
tions as complex signs. Emergence of struc-
ture and reduction to usage. Constructions and
Frames, 1:119–152.

54


