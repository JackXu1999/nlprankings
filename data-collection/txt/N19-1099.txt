




































Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process


Proceedings of NAACL-HLT 2019, pages 940–950
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

940

Integration of Knowledge Graph Embedding into Topic Modeling with
Hierarchical Dirichlet Process

Dingcheng Li, Siamak Zamani Dadaneh, Jingyuan Zhang, Ping Li
Cognitive Computing Lab (CCL)

Baidu Research USA
10900 NE Eighth St, Bellevue, WA 98004, USA
1195 Bordeaux Dr, Sunnyvale, CA 94089, USA

lidingcheng@baidu.com, zamanys4@gmail.com
zhangjingyuan03@baidu.com, liping11@baidu.com

Abstract

Leveraging domain knowledge is an effective
strategy for enhancing the quality of inferred
low-dimensional representations of documents
by topic models. In this paper, we develop
topic modeling with knowledge graph em-
bedding (TMKGE), a Bayesian nonparamet-
ric model to employ knowledge graph (KG)
embedding in the context of topic modeling,
for extracting more coherent topics. Specifi-
cally, we build a hierarchical Dirichlet process
(HDP) based model to flexibly borrow infor-
mation from KG to improve the interpretabil-
ity of topics. An efficient online variational in-
ference method based on a stick-breaking con-
struction of HDP is developed for TMKGE,
making TMKGE suitable for large document
corpora and KGs. Experiments on three pub-
lic datasets illustrate the superior performance
of TMKGE in terms of topic coherence and
document classification accuracy, compared to
state-of-the-art topic modeling methods.

1 Introduction

Topic models, such as Probabilistic Latent Seman-
tic Analysis (PLSA) (Hofmann, 2017) and La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003),
play significant roles in helping machines inter-
pret text documents. Topic models consider doc-
uments as a bag of words. Given the word in-
formation, topic models formulate documents as
mixtures of latent topics, where these topics are
generated via the multinomial distributions over
words. Bayesian methods are utilized to extract
topical structures from the document-word fre-
quency representations of the text corpus. With-
out supervision, however, it is found that the top-
ics generated from these models are often not
interpretable (Chang et al., 2009; Mimno et al.,
2011). In recent studies, incorporating knowledge
of different forms as a supervision has become a
powerful strategy for discovering meaningful top-
ics (Andrzejewski et al., 2009).

president
male
born
wife

Barack Obama
United States
Honolulu

Michelle Obama

wasBornIn
hasGender
livesIn
locatedIn

Word Co-occurrence Entity Embeddings Relation Embeddings

Latent Topics

Words Entities Relations

Documents Knowledge Graphs
Online stochastic
Optimization with
Natural gradients for
varitional inferences

Figure 1: An overview of the proposed TMKGE frame-
work. Entities are shared by both documents and
knowledge graphs. Entity embeddings generated by
TransE a knowledge graph embedding package are
passed into TMKGE to generate hidden topics.

Most conventional approaches take prior do-
main knowledge into account to improve the topic
coherence (Andrzejewski et al., 2009; Andrze-
jewski and Zhu, 2009; Hu et al., 2014; Jagar-
lamudi et al., 2012; Doshi-Velez et al., 2015).
One commonly used domain knowledge is based
on word correlations (Andrzejewski et al., 2009;
Chen et al., 2013; Chen and Liu, 2014). For exam-
ple, must-links and cannot-links among words are
generated by domain experts to help topic mod-
eling (Andrzejewski et al., 2009). Another useful
form of knowledge for topic discoveries is based
on word semantics (Andrzejewski and Zhu, 2009;
Chemudugunta et al., 2008; Hu et al., 2014; Ja-
garlamudi et al., 2012; Doshi-Velez et al., 2015).
In particular, word embedding (Pennington et al.,
2014; Goldberg and Levy, 2014), in which bag of
words are transformed into vector representations
so that contexts are embedded into those word vec-
tors, are used as semantic regularities to enhance
topic models (Nguyen et al., 2015; Li et al., 2016;
Das et al., 2015; Batmanghelich et al., 2016).



941

Knowledge graph (KG) embedding (Bordes
et al., 2013) learns a low-dimensional continuous
vector space for entities and relations to preserve
the inherent structure of KGs. Yao et al. (2017)
proposes KGE-LDA to incorporate embeddings of
KGs into topic models to extract better topic rep-
resentations for documents and shows promising
performance. However, KGE-LDA forces words
and entities to have identical latent representa-
tions, which is a rather restrictive assumption that
prevents the topic model from recovering correct
underlying latent structures of the data, especially
in scenarios where only partial KGs are available.

This paper develops topic modeling with knowl-
edge graph embedding (TMKGE), a hierarchical
Dirichlet process (HDP) based model to extract
more coherent topics by taking advantage of the
KG structure. Unlike KGE-LDA, the proposed
TMKGE allows for more flexible sharing of in-
formation between words and entities, by using a
multinomial distribution to model the words and a
multivariate Gaussian mixture to model the enti-
ties. With this approach, we introduce two propor-
tional vectors, one for words and one for entities.
In contrast, KGE-LDA only uses one, shared by
both words and entities. Similar to HDP, TMKGE
includes a collection of Dirichlet processes (DPs)
at both corpus and document levels. The atoms of
corpus-level DP form the base measure for doc-
ument levels DPs of words and entities. There-
fore, the atoms of corpus-level DP can represent
word topics, entity mixture components, or both of
them. Figure 1 provides an overview of TMKGE,
where two sources of inputs, bag of words and
KG embedding, extracted from corpus and KGs
respectively, are passed into TMKGE.

As a nonparametric model, TMKGE does not
assume a fix number of topics or entity mixture
components as constraints. Instead, it learns the
number of topics and entity mixture components
automatically from the data. Furthermore, an ef-
ficient online variational inference algorithm is
developed, based on Sethuraman’s stick-breaking
construction of HDP (Sethuraman, 1994). We in
fact construct stick-breaking inference in a mini-
batch fashion (Wang et al., 2011; Bleier, 2013),
to derive a more efficient and scalable coordinate-
accent variational inference for TMKGE.

Summary of contributions: TMKGE is a
Bayesian nonparametric model to extract more co-
herent topics by taking advantage of knowledge

graph structures. We introduce two proportional
vectors for more flexible sharing of information
between words and entities. We derive an efficient
and scalable parameter estimation algorithm via
online variational inference. Finally, we empiri-
cally demonstrate the effectiveness of TMKGE in
topic discovering and document classification.

2 Background and Related Work

Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a popular probabilistic model that learns
latent topics from documents and words, by us-
ing Dirichlet priors to regularize the topic distri-
butions. The generated topics from LDA mod-
els, however, are often not interpretable (Chang
et al., 2009; Mimno et al., 2011), in part because
LDA models are unsupervised without using prior
knowledge or external resources.

In recent years, prior knowledge are leveraged
to guide the process of topic modeling (Andrze-
jewski and Zhu, 2009; Hu et al., 2014; Jagar-
lamudi et al., 2012; Doshi-Velez et al., 2015).
For example, the deep forest LDA (DF-LDA)
model (Andrzejewski et al., 2009) is proposed
to incorporate must-links and cannot-links among
words into topic modeling. One weakness of
the DF-LDA model is that the link information
is domain-dependent. Later, general knowledge
based LDA is introduced to leverage must-links
from multiple domains (Chen et al., 2013). More
recently, MetaLDA (Zhao et al., 2017) proposes to
improve topic modeling by incorporating diverse
meta information as priors for both document hy-
perparameter α and word hyperparameter β.

Besides the word correlations, word semantics
are also utilized as one type of useful knowl-
edge for topic modeling (Chemudugunta et al.,
2008; Hu et al., 2014; Jagarlamudi et al., 2012).
Word embeddings, as a low-dimensional continu-
ous vectors of words (Mikolov et al., 2013; Bengio
et al., 2003; Pennington et al., 2014) are regarded
to be an efficient representations of word seman-
tics. Latent Feature Topic Modeling (LFTM) is
proposed to use pre-trained word embeddings in
topic modeling (Nguyen et al., 2015). It incorpo-
rates the embedding of a word and its topics into
the traditional multinomial distribution over words
as the probability function of topic modeling. Top-
icVec extends LFTM by combining a word and its
local contextual words together into the conven-
tional multinomial distribution over words. It also



942

learns embedding representations for topics (Li
et al., 2016). Gaussian-LDA goes further to im-
prove topic modeling (Das et al., 2015) by taking
into considerations the continuous nature of word
embeddings. Shi et al. (2017) constructs a more
unified framework, STE (skip-gram topic embed-
ding) to address the problem of polysemy. Li
et al. (2019) proposes a unified framework TMSA
(Topic Modeling and Sparse Autoencoder) to im-
prove topic discovery and word embedding simul-
taneously via a mutual learning mechanism.

Hu et al. (2016) proposes topic-based embed-
dings for learning from large knowledge graphs
(KGE). KGE learns low-dimensional continuous
vector space for both entities and relations to pre-
serve the inherent structure of knowledge graphs.
A Bayesian method is introduced by considering
the embeddings of entities and relations as top-
ics. Later, Yao et al. (2017) proposes knowledge
graph embedding LDA (KGE-LDA) to encode en-
tity embeddings learned from knowledge graphs
into LDA and show that knowledge graph em-
beddings boost topic discoveries. Inspired by this
work, we explore to utilize entity embeddings to
encode prior knowledge for topic modeling.

3 Method

This section presents the TMKGE model and an
efficient online variational inference for learning
its parameters. We first provide a review of hierar-
chical Dirichlet process (HDP) (Teh et al., 2005).

3.1 Preliminaries of HDP
Dirichlet process (DP) (MacEachern and Müller,
1998) G ∼ DP(γ0, G0), with a base measure G0
and a concentration parameter γ0 > 0, is the dis-
tribution of a random probability measure G over
a measurable space (Ω,B), such that for any mea-
surable disjoint partition (A1, ..., AQ) of Ω,

(G(A1), ..., G(AQ)) ∼ Dir(γ0G0(A1), ..., γ0G0(AQ))

where “Dir” denotes a Dirichlet distribution.

Hierarchical Dirichlet process (HDP) (Teh et al.,
2005), introduced for dealing with multiple (D)
groups of data, is a distribution over a set of ran-
dom probability measures over (Ω,B): one prob-
ability measure Gd ∼ DP(α0, G0) for each group
d ∈ {1, 2, ..., D}, and a global probability mea-
sure G0 ∼ DP(γ0, H) with a base measure H .

Stick-breaking construction Teh et al. (2005)

shows that the draws from G0 and Gd can be ex-
pressed as weighted sums of point masses:

G0 =

∞∑
k=0

βkδφk , Gd =

∞∑
k=0

πdkδφk .

A more convenient stick-breaking construction,
especially for deriving closed-form variational
inference (Wang et al., 2011), is Sethuraman
(1994)’s construction, which proceeds as follows.
First, the global-level DP draw is represented as

β′k ∼ Beta(1, γ0), βk = β′k
k−1∏
`=1

(1− β′`),

Note that the distribution for β = {βk}∞k=1 is
also commonly written as β ∼ GEM(γ0) (Pit-
man, 2002). Subsequently, the group-level draws
are constructed as

ψdt ∼G0, π′dt = Beta(1, α0),

πdt =π
′
dt

t−1∏
`=1

(1− π′d`), Gd =
∞∑
t=1

πdtδψdt . (1)

Alternatively, the group-level atoms {ψdt}∞t=1 can
be represented as ψdt = φcdt , where the auxiliary
indicator variables cdt are independently drawn
from a multinomial Mult(β).

Teh et al. (2008) also proposes a collapsed in-
ference method as an alternative of stick-breaking
inference. However, following Fox et al. (2011),
we stick to the uncollapsed HDP model consider-
ing our truncated Dirichlet process has more com-
putational efficiency and is simple to implement.

3.2 The TMKGE Model

𝒑𝟎

𝒎𝟎

𝝊𝟎

𝓦𝟎

𝜶𝟎

𝝓𝒌

𝝁𝒌

𝜦𝒌

𝑫 𝑵𝒅
𝒘

𝑴𝒅𝒆

𝑲𝒘𝒏

𝒆𝒎

𝒖𝒎

𝒅

𝑮𝒅
(𝒆)

𝑮𝒅
(𝒘)

𝑮𝟎

𝜸𝟎

Figure 2: Graphical representation of the TMKGE
framework. There are two components, the lower of
which is the one for words and the upper of which is
the one for entities. Both components share the Dirich-
let process as priors. Since entities are represented with
knowledge graph embeddings, therefore, each entity is
generated with Gaussian priors while the one for words
is still generated with Dirichlet priors.



943

Figure 2 is the graphical representation of
TMKGE. Let D denote the number of docu-
ments in the corpus, where each document d ∈
{1, 2, ..., D} contains N (w)d words and N

(e)
d enti-

ties. Throughout this work, superscripts (w) and
(e) indicate word and entity related parameters, re-
spectively. In each document d, the n-th word is
represented by wdn, where each word belongs to
a vocabulary of size V , i.e., wdn ∈ {1, 2, ..., V }.
Furthermore, the P -dimensional embedding of the
m-th entity is edm, where the total number of
unique entities in the corpus is E. We assume that
entity embeddings are obtained from the “com-
plete” knowledge graph, and hence they contain
information independent of the corpus. In this pa-
per, we use TransE (Bordes et al., 2013), a simple
and effective tool for knowledge encoding, to cal-
culate the embeddings of entities extracted from
the documents. We should mention that we re-
move the normalization step of TransE and thus
the output vectors (edm) do not have unit `2 norm.

TMKGE builds upon HDP for joint modeling
of word topics and entity mixtures. At the corpus
level, word topics and entity mixtures correspond
to atoms of a Dirichlet process G0 ∼ DP(γ0, H).
At the document level, word topics and entity mix-
ture components are atoms of independent DPs,
with shared base measureG0. Mathematically, for
document d, we have

G
(w)
d ∼ DP(α0, G0), G

(e)
d ∼ DP(α0, G0),

where G(w)d and G
(e)
d are word and entity related

DPs. Sethuraman’s construction in (1) yields

G
(w)
d =

∞∑
t=1

π
(w)
dt δψ(w)

dt

, G
(e)
d =

∞∑
t=1

π
(e)
dt δψ(e)

dt

. (2)

These DPs are then used to assign words and en-
tities to topics and mixture components, respec-
tively. In document d, let z(w)dn denote the topic as-
signed to the n-th word, and z(e)dm denote the mix-
ture component assigned to the m-th entity. Using
the mixing proportions of DPs in (2), we have

p(z
(w)
dn = t) = π

(w)
dt , p(z

(e)
dn = t) = π

(e)
dt .

For simplicity, we use index t to denote both word
and entity related atoms, although they can corre-
spond to different atoms of the global DPs.

The mixing proportions of corpus-level DP are
used to map the document atoms to the shared
global atoms. More precisely, we introduce the
word and entity atoms mapping auxiliary variables

c
(w)
d = {c

(w)
dt }

∞
t=1 and c

(e)
d = {c

(e)
dt }
∞
t=1. The map-

ping probabilities then can be expressed as

p(c
(w)
dt = k) = βk, p(c

(e)
dt = k) = βk.

TMKGE allows flexible sharing of information
between knowledge graphs and documents. This
is an important advantage, as in practice only par-
tial relational information are available, and thus
strictly forcing the topics and entity mixtures to
share components may lead to reducing the power
of model to correctly recover the latent structure
of the data. Furthermore, the nonparametric na-
ture of the model enables the automatic discovery
of number of atoms for both words and entities, at
document and corpus levels.

Each atom of corpus DP (G0) corresponds to
a set of parameters for both words and entities.
Atom k contains topic-word Dirichlet distribution
φk = (φk1, ..., φkV )

T , and entity Gaussian mix-
ture parameters {µk,Λk}. Given φk and topic as-
signment variables, the generative process for n-th
word of document d is

z
(w)
dn ∼ Mult(π

(w)
d ),

(wdn|z(w)dn = t, c
(w)
dt = k,φk) ∼ Mult(φk).

In a similar fashion, the generative process ofm-th
entity of document d is

z
(e)
dm ∼ Mult(π

(e)
d ),

(edm|z(e)dm = t, c
(e)
dt = k,µk,Λk) ∼ N(µk,Λ

−1
k ),

where µk and Λk are the mean vector and preci-
sion matrix of multivariate Gaussian distribution.

Furthermore, we impose conjugate priors on
both word and entity components parameters as:

φk ∼ Dir(η, ..., η), µk ∼ N
(
m0, (ρ0Λk)

−1),
Λk ∼ Wishart(ν0,W 0).

3.3 Online Variational Inference

In this section, inspired by (Wang et al., 2011), we
propose an online variational inference algorithm
for efficient learning of TMKGE model parame-
ters. We use a fully factorized variational distri-
bution based on stick-breaking construction, and
perform online mean-field variational inference.

In addition to topic parameters φk and en-
tity mixture parameters {µk,Λk}, other param-
eters of interest are corpus-level stick propor-
tions β′ = {β′k}∞k=1, document-level stick propor-
tions for words π′(w)d = {π

′(w)
dt }

∞
t=1 and entities

π
′(e)
d = {π

′(e)
dt }

∞
t=1, topic assignments for words



944

z
(w)
d = {z

(w)
dn }

N
(w)
d

n=1 , mixture assignments for en-

tities z(e)d = {z
(e)
dm}

N
(e)
d

m=1, and mapping variables
c
(w)
d and c

(e)
d . Denote Θ

(w) and Θ(e) respectively
the word and entity related parameters. Then the
variational distribution factorizes as

q(β′,Θ(w),Θ(e)) = q(β′)q(Θ(w))q(Θ(e)).

For corpus-level stick proportions, we assume a
Beta distribution:

q(β′) =

K−1∏
k=1

Beta(β′k|uk, vk),

where the number of global atoms is truncated at
K, thereby q(β′K = 1) = 1. For the word related
parameters Θ(w), we have

q(Θ(w)) =q(c(w))q(z(w))q(π′(w))q(φ),

q(c(w)) =

D∏
d=1

T−1∏
t=1

Mult(ϕ(w)dt ),

q(z(w)) =

D∏
d=1

N
(w)
d∏
n=1

Mult(ς(w)dt ),

q(π′(w)) =

D∏
d=1

T−1∏
t=1

Beta(π′dt|a
(w)
dt , b

(w)
dt ),

q(φ) =

K∏
k=1

Dir(λk).

The variational distributions for entity related
parameters have a similar form to the above distri-
butions, except the Gaussian mixture parameters,
which are expressed as follows:

q(µk) = N
(
mk, (ρkΛk)

−1), q(Λk) = Wishart(νk,W k).
In standard variational inference theory, the ev-

idence lower bound (ELBO), which is the lower
bound to the marginal log likelihood of the ob-
served data, is maximized to find the best varia-
tional approximation to the true intractable poste-
rior. Given the modeling framework of TMKGE,
the ELBO can be written as

L(q) =
∑
d

{
E
[

log
(
p(wd|c(w)d ,z

(w)
d ,φ)p(c

(w)
d |β

′)

× p(z(w)d |π
′(w)
d )p(ed|c

(e)
d ,z

(e)
d ,µ,Λ)

× p(c(e)d |β
′)p(z

(e)
d |π

′(e)
d )p(π

′(w)
d |α0)p(π

′(e)
d |α0)

)]
+H

(
q(c

(w)
d )

)
+H

(
q(z

(w)
d )

)
+H

(
q(π′

(w)
d )

)
+H

(
q(c

(e)
d )
)

+H
(
q(z

(e)
d )
)

+H
(
q(π′

(e)
d )
)}

+ E
[

log
(
p(β′)p(φ)p(µ,Λ)

)]
+H

(
q(β′)

)
+H

(
q(φ)

)
+H

(
q(µ,Λ)

)
,

where H(·) is the entropy term for variational
distribution. By taking derivatives of this lower
bound with respect to each variational parameter,
we derive the coordinate ascent update steps.

We develop an online variational inference for
TMKGE, to process large datasets (Wang et al.,
2011; Hoffman et al., 2010). Given the existing
corpus-level parameters, first a document d is sam-
pled and then its optimal document-level varia-
tional parameters are computed. For word related
variational parameters, these updates include

a
(w)
dt = 1 +

∑
n

ς
(w)
dnt ,

b
(w)
dt = α0 +

∑
n

T∑
s=t+1

ς
(w)
dns,

ϕ
(w)
dtk ∝ exp

(∑
n

ς
(w)
dnsEq

[
log p(wdn|φk)

]
Eq
[

log βk
])
,

ς
(w)
dnt ∝ exp

(∑
k

ϕ
(w)
dtkEq

[
log p(wdn|φk)

]
Eq
[

log π
(w)
dt

])
(3)

where expectations are with respect to variational
distributions and have closed forms. For entity re-
lated variational parameters, similar updates can
be derived, with the term Eq

[
log p(edm|µk,Λk)

]
replacing Eq

[
log p(wdn|φk)

]
. Following Wang

et al. (2011), for the corpus-level variational pa-
rameters, we use the following gradients:

∂λkv = −λkv + η +D
∑
t

ϕ
(w)
dtk

(∑
n

ς
(w)
dntI[wdn = v]

)
,

∂mk = −mk +
D
∑
m,t ϕ

(e)
dtkς

(e)
dmtedm + ρ0m0

Drk + ρ0
,

∂ρk = −ρk + ρ0 +Drk,
∂νk = −νk + ν0 +Drk,

∂W k = −W k +
(
W−10 +D

∑
m,t

ϕ
(e)
dtkς

(e)
dmtedme

T
dm

)−1
,

∂uk = −uk + 1 +D
∑
t

(
ϕ

(w)
dtk + ϕ

(e)
dtk

)
,

∂vk = −vk + γ0 +D
∑
t

K∑
`=k+1

(
ϕ

(w)
dt` + ϕ

(e)
dt`

)
, (4)

where rk is defined as
∑

m,t ϕ
(e)
dtkς

(e)
dmt. The

corpus-level parameters are then updated using
these gradients (among them, the first, the fifth and
the sixth are natural gradients while the other four
are approximations from the posterior of Gaus-
sian Wishart scale matrix W . It appears difficult
to obtain natural gradients for those four.) and a
learning rate parameter �t. For instance, for topic-
words distribution parameters we have

λ← λ+ �t0∂λ. (5)



945

number of top words and PMI scores
model parameters data source 5 10 15 20 25 30
TMKGE K=300, T=20

20 Newsgroups
20.8 91.1 210.0 380.0 602.0 876.0

HDP K=300, T=20 20.0 91.6 212.6 384.1 598.4 868.7
LDA K=100 13.5 64.6 163.4 285.0 455.2 671.1
KGE-LDA K=30 18.9 69.8 187.5 320.6 482.7 616.5
TMKGE K=300, T=20

NIPS
16.6 97.1 160.3 299.6 474.5 685.5

HDP K=300, T=20 16.7 66.8 157.2 280.2 444.0 643.1
LDA K=100 13.9 67.6 161.9 297.0 471.2 681.1
KGE-LDA K=30 14.3 97.2 163.4 285.3 453.3 645.4
TMKGE K=300, T=20

Ohsumed
21.6 123.3 237.3 407.7 624.2 895.5

HDP K=300, T=20 15.6 70.7 168.2 338.9 582.9 864.9
LDA K=100 11.9 65.6 131.9 257.0 481.2 691.1
KGE-LDA K=30 15.6 116.5 185.4 354.2 585.4 795.6

Table 1: Topic Coherence of all models on three datasets with different number of top words. A higher PMI score
implies a more coherent topic. Improvements of TMKGE over other methods are significant.

The rest of corpus-level variational parameters
in (4) can be similarly updated. To ensure that
the parameters converge to a stationary point, the
learning rate satisfies (Hoffman et al., 2010; Sato,
2001)

∑∞
t0=1

�t0 =∞ and
∑∞

t0=1
�2t0 <∞.

Following Wang et al. (2011), we use �t0 =
(τ0 + t0)

−κ, where κ ∈ (0.5, 1] and τ0 > 0. To
improve the stability of online variational infer-
ence, we use a mini-batch of documents to com-
pute the natural gradients. That is, the contribu-
tion of the single document d in (4) is replaced
by sum of contributions of documents in the mini-
batch S , and the factor D is replaced by D/|S|.
The overall scheme of online variational inference
for TMKGE is shown in Algorithm 1.

Algorithm 1 Online variational inference for the
proposed TMKGE framework.
Initialize corpus-level variational parameters.
while Stopping criterion is not met do

Sample a random document d from the corpus.
Update a(w)d , b

(w)
d , ϕ

(w)
d and ς

(w)
d using (3).

Update a(e)d , b
(e)
d , ϕ

(e)
d and ς

(e)
d similar to (3).

Compute the natural gradients using (4).
Set �t0 = (τ0 + t0)

−κ and t0 ← t0 + 1.
Update all corpus-level parameters as (5).

end

4 Experiments

We evaluate TMKGE on two experimental tasks
and compare its performance to those of LDA,
HDP and KGE-LDA. For LDA and HDP, we use
the online variational inference implementations.
More precisely, we will evaluate our framework
by the test whether it finds coherent and meaning-

ful topics and the test whether it can achieve good
performance in document classification.

We run our experiments on three popular
datasets; 20 Newsgroups, NIPS and the Ohsumed
corpus. The 20 Newsgroups dataset contains
18,846 documents evenly categorized into 20 dif-
ferent categories.

The NIPS dataset contains 1,740 papers from
the NIPS conference. The Ohsumed corpus is
from the MEDLINE database. We consider the
13,929 unique Cardiovascular diseases abstracts in
the first 20,000 abstracts of the year 1996. Each
document in the set has one or more associated
categories from the 23 disease categories. The
documents belonging to multiple categories are
eliminated so that 7,400 documents belonging to
only one category remain. The datasets are tok-
enized with Stanford CoreNLP (Manning et al.,
2014). After standard pre-processing (such as
removing stop words), there are 20,881 distinct
words in the 20 Newsgroups dataset, 14,482 dis-
tinct words in the NIPS dataset and 8,446 distinct
words in the Ohsumed dataset.

4.1 External knowledge source

The knowledge graph we employ is Word-
Net (Miller, 1995). WordNet is a large lexical
knowledge graph. Entities in WordNet are syn-
onyms which express distinct concepts. Relations
in WordNet mainly involve conceptual-semantic
and lexical relations. We use a subset of Word-
Net (WN18) introduced in Bordes et al. (2011) and
employed in Yao et al. (2017) as well. WN18 con-
tains 151,442 triplets with 40,943 entities and 18
relations. We link tokenized words to entities in
WN18 via NLTK (Bird and Loper, 2004).



946

20 Newsgroups NIPS Ohsumed
lord tcp/ip kuwait distribution network tube vietnam hemagglutinin shbg
God drive iraq gaussian learning regression veterans anti-tumor patients
elohim system kuwaiti posterior model svs mthfr mthfr globulin
jesus computer sabah covariance neural support income tumor testicular
subject information abdulla ensemble data fraction white pbl hormone
israel space gulf matrix figure erros proportion antibody levels
armenian windows amir KL information vapnik drinking meh group
christ data ahmed divergence units algorithm era ab test sex
john message sheikh approximate problem smola lifetime verapamil binding
group software saudi algorithm recognition vector interview radioactivity treatment
101.2 98.5 119.3 152.6 91.1 106.3 105.4 135.4 152.2

20 Newsgroups NIPS Ohsumed
internet drive car distribution control kernel gene cancer treatment
mail windows cars bayesian trajectory support dna tumor therapy
email dos engine gaussian robot xi protein survival dose
list card oil prior controller vector region tumors drug
message disk miles posterior arm margin genetic carcinoma effects
address mac dealer probability model examples analysis breast placebo
fax scsi speed variables forward set mutation stage trial
network memory buy markov motor kernels sequence malignant oral
send system ford distribution trajectories svm molecular chemotherapy mg
e-mail apple drive approximation inverse machines mrna primary effective
89.2 84.4 63.6 154.8 86.2 88.3 149.9 107.7 106.5

Table 2: Example topics learned from three datasets by TMKGE with K = 300 and T = 20, and KGE-LDA with
K = 30. The last row for each model is the topic coherence computed using the 4,776,093 Wikipedia documents
as reference. Some medical short words: pbl = Peripheral blood leucocyte, meh = Mean erythrocyte hemoglobin.

4.2 Model parameters
In the experiments, for each method, we report the
results based on the hyperparameter settings that
obtain the best performances. For TMKGE and
HDP, we report the results for K = 300, T = 20
and K = 100, T = 10 cases. For LDA and
KGE-LDA, respectively, we have K = 100 and
K = 30. Throughout this work we fix the di-
mension of entity embedding as P = 5. For on-
line variational inference, we run the algorithms
for 1000 iterations, with mini-batch size of 100.

4.3 Topic Coherence
We assess the performance of the proposed
TMKGE model based on topic coherence. Topic
coherence has been shown to be more consis-
tent with human judgment than other typical topic
model metrics such as perplexity (Chang et al.,
2009; Newman et al., 2010). We perform both
quantitative and qualitative analysis of the topics
discovered by TMKGE, and compare its perfor-
mance to those of LDA, HDP and KGE-LDA.

4.3.1 Quantitative Analysis
We evaluate the coherence of discovered topics
by the point-wise mutual information (PMI) Topic
Coherence metric. The PMI Topic Coherence is
implemented following Newman et al. (2010):

PMI(k) =

N∑
j=2

j−1∑
i=1

log
p(wi, wj)

p(wi)p(wj)

where k refers to a topic, N refers to the number
of top words of k, p(wi) is the probability that wi
appears in a document, p(wi, wj) is the probability
that wi and wj co-occur in the same document. A
higher PMI score implies a more coherent topic.
Following KGE-LDA, 4,776,093 Wikipedia arti-
cles are employed for obtaining topic coherence
scores. Different from Yao et al. (2017), which
used a fixed value of N (the number of top words,
e.g. N = 5 or N = 10), we vary N in a range
from 5 to 30. (Lau and Baldwin, 2016) suggests
that calculating topic coherence over several dif-
ferent cardinalities and averaging results in a sub-
stantially more stable evaluation.

Table 1 shows the average topic coherence for
different methods and datasets. We can observe
that for the three datasets, TMKGE achieves high-
est topic coherence in almost all top word sizes.
In the few cases which TMKGE does not rank
highest, there only exist subtle differences with
the top performing result. This shows that knowl-
edge graph embedding improves the coherence of
discovered topics. Further, for the top 10 words,
the topic coherence of all three datasets are higher
than those obtained by KGE-LDA. This shows
that topic modeling based on HDP for both en-
tity embedding and words enjoys incomparable
advantages over LDA-based modeling.



947

4.3.2 Qualitative Analysis

Table 2 shows example topics with their PMI
scores learned from the three corpora by KGE-
LDA and our TMKGE model. For comparison,
we report similar topics to those listed in the KGE-
LDA paper. It can be seen that TMKGE finds quite
closely related words in a topic. For example, for
the second column of 20 Newsgroups, topic words
from both TMKGE and KGE-LDA are related to
computers. However, it can be noted that words
from TMKGE focus more on the core words of
computer science. In contrast, words from the
same topic in KGE-LDA seems to be closer to the
brand, such as windows, mac or apple. In addition,
topics found from TMKGE are more diverse than
those found in KGE-LDA. For 20 Newsgroups,
the three topics we list here refer to theology, com-
puter science and middle east respectively while
the three topics from KGE-LDA refer to inter-
net, computer and car respectively. Both TMKGE
and KGE-LDA discover probability-related and
machine learning topics with different top words
from NIPS dataset. Roughly speaking, KGE-
LDA discovers gene-related, cancer-related and
treatment-related topics from Ohsumed corpus.
TMKGE discovers more diverse and more specific
topics. For example, one topic TMKGE discov-
ers is about Vietnamese veterans, cancer-related
and sexual-disease topics. From the perspective
of topic coherence, we can also see that TMKGE
obtains higher PMI score in most of those top-
ics. The whole trend is consistent with the average
PMI score reported in the last section. Overall,
TMKGE performs better than other topic models,
including LDA, HDP and KGE-LDA in terms of
average PMI and also in qualitative case studies.

4.4 Document Classification

We evaluate our proposed method through docu-
ment classification, we follow the approach in (Li
and McCallum, 2006) for document classification.

We have conducted a five-way classification on
the comp subject of 20 Newsgroups dataset and
on the top five most frequent labels of Ohsumed
dataset (no labels for nips dataset), where each
class of documents is divided into 75% training
and 25% testing. For each class, the LDA, HDP
and TMKGE models are trained on the training
documents, and then the predictive likelihood for
the test documents is calculated using the E-step
in the variational inference procedure of LDA. A

document is classified correctly if its correspond-
ing model produces the highest likelihood.

class LDA HDP KGE-LDA TMKGE
20 Newsgroup

pc 68.6 78.9 67.2 78.9
os 71.7 80.7 70.7 82.3
mac 82.0 87.1 68.1 86.5
windows.x 84.0 83.5 64.4 84.9
graphics 81.2 81.9 65.4 83.0

Ohsumed
C04 50.6 73.0 59.1 73.8
C10 46.2 63.0 54.4 64.9
C14 51.5 44.6 33.2 52.3
C21 86.5 89.5 83.7 89.7
C23 68.2 81.9 75.3 86.1

Table 3: Document classification accuracy a five-way
classification on the comp subject of 20 Newsgroups
dataset and on the top five most frequent labels of
ohsumed dataset (no labels for NIPS dataset).

Table 3 presents the average classification ac-
curacy for TMKGE, HDP and LDA over five re-
peated simulations. The table includes the classifi-
cation accuracy for KGE-LDA, where the learned
topic proportions are used as features for SVM
classifier. For the majority of document classes,
TMKGE has the best classification accuracy, ex-
cept for the class mac. As shown, the SVM clas-
sifier based on KGE-LDA has significantly worst
performance. For more complete comparisons,
we run experiments on all subjects of 20 News-
groups and also report experimental results pub-
lished in Shi et al. (2017) in Table 4. TMKGE
achieves the best performance on all models.

Model Acc (%) Model Acc (%)
BOW 79.7 STE-Diff 82.9

Skip-Gram 75.4 LDA 77.5
TWE 81.5 TMSA 83.5
PV 75.4 HDP 82.4

GPU-DMM 48.0 KGE-LDA 70.5
STE-Same 80.4 TMKGE 88.8

Table 4: Document classification: all subjects of 20
Newsgroups dataset for more complete comparisons.
Clearly shown is the best performances of TMKGE

A few points can be observed from the superior
performance of TMKGE. Firstly, it looks the ad-
dition of unnormalized knowledge graph embed-
ding into TMKGE as a proportional vector to the
word vector boosts the performance. Secondly,
the selection of HDP over LDA plays an essen-
tial role. This can be indicated from the poor
performance of KGE-LDA (which is even worse
than BOW). More impressively, TMKGE achieves



948

even much better performances than STE-Diff,
TWE and TMSA, all of which involve the integra-
tion of word embedding and topic modeling. Im-
pressively, TMKGE shows its supremacy over the
state of the art model, TMSA with high margins.
This shows that the knowledge graph structure in-
cluded into the entity embedding conveys more in-
formation than pure word embedding. Meanwhile,
this also shows that the two proportional vectors
generated with online HDP enables the flexible
sharing of information between words and entities.
Accordingly, more coherent topics are extracted
and the classification result are boosted as well.

5 Conclusion

This paper presents TMKGE, a Bayesian nonpara-
metric model based on hierarchical Dirichlet pro-
cess for incorporation of entity embeddings from
external knowledge graphs into topic modeling.
The proposed method allows for flexible sharing
of information between documents and knowl-
edge graph. Specifically, TMKGE avoids forcing
the words and entities to identical latent factors,
thus making it a suitable framework for scenarios
where only partial relational information are avail-
able. Furthermore, as a Bayesian nonparameteric
model, TMKGE learns the number of word top-
ics and entity mixture components automatically
from the data. We have derived an efficient and
scalable online variational inference for TMKGE.

Comprehensive experiments on three different
datasets suggest that TMKGE significantly outper-
forms SOA methods in terms of both topic coher-
ence and document classification accuracy.

References
David Andrzejewski and Xiaojin Zhu. 2009. Latent

dirichlet allocation with topic-in-set knowledge. In
SemiSupLearn ’09 Proceedings of the NAACL HLT
2009 Workshop on Semi-Supervised Learning for
Natural Language Processing, pages 43–48, Boul-
der, Colorado.

David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proceed-
ings of the 26th annual international conference on
machine learning (ICML), pages 25–32, Montreal,
Canada.

Kayhan Batmanghelich, Ardavan Saeedi, Karthik
Narasimhan, and Sam Gershman. 2016. Nonpara-
metric spherical topic modeling with word embed-
dings. In Proceedings of the 54th Annual Meet-

ing of the Association for Computational Linguistics
(ACL), Berlin, Germany.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Steven Bird and Edward Loper. 2004. NLTK: the nat-
ural language toolkit. In Proceedings of the ACL
2004 on Interactive poster and demonstration ses-
sions, page 31. Association for Computational Lin-
guistics.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Arnim Bleier. 2013. Practical collapsed stochas-
tic variational inference for the HDP. CoRR,
abs/1312.0412.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems (NIPS), pages 2787–2795, Lake
Tahoe, NV.

Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured em-
beddings of knowledge bases. In Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelli-
gence (AAAI), San Francisco, CA.

Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-Graber, and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Advances in Neural Information Processing Systems
(NIPS), pages 288–296, Vancouver, Canada.

Chaitanya Chemudugunta, America Holloway,
Padhraic Smyth, and Mark Steyvers. 2008. Mod-
eling documents by combining semantic concepts
with unsupervised statistical learning. In Pro-
ceedings of the 7th International Semantic Web
Conference (ISWC), pages 229–244, Karlsruhe,
Germany.

Zhiyuan Chen and Bing Liu. 2014. Mining topics in
documents: standing on the shoulders of big data.
In Proceedings of the 20th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD), pages 1116–1125, New York,
NY.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013. Discovering coherent topics using general
knowledge. In Proceedings of the 22nd ACM in-
ternational conference on Information & Knowledge
Management (CIKM), pages 209–218, San Fran-
cisco, CA.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-



949

dings. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Nat-
ural Language Processing of the Asian Federation
of Natural Language Processing (ACL), pages 795–
804, Beijing, China.

Finale Doshi-Velez, Byron C Wallace, and Ryan
Adams. 2015. Graph-sparse LDA: A topic model
with structured sparsity. In Proceedings of the
Twenty-Ninth AAAI Conference on Artificial Intelli-
gence (AAAI), pages 2575–2581, Austin, TX.

Emily B Fox, Erik B Sudderth, Michael I Jordan, and
Alan S Willsky. 2011. A sticky HDP-HMM with ap-
plication to speaker diarization. The Annals of Ap-
plied Statistics, pages 1020–1056.

Yoav Goldberg and Omer Levy. 2014. word2vec ex-
plained: deriving mikolov et al.’s negative-sampling
word-embedding method. CoRR, abs/1402.3722.

Matthew Hoffman, Francis R Bach, and David M Blei.
2010. Online learning for latent dirichlet allocation.
In Advances in Neural Information Processing Sys-
tems (NIPS), pages 856–864, Vancouver, Canada.

Thomas Hofmann. 2017. Probabilistic latent semantic
indexing. In ACM SIGIR Forum, pages 211–218.
ACM.

Changwei Hu, Piyush Rai, and Lawrence Carin. 2016.
Topic-based embeddings for learning from large
knowledge graphs. In Proceedings of the 19th
International Conference on Artificial Intelligence
and Statistics (AISTATS), pages 1133–1141, Cadiz,
Spain.

Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2014. Interactive topic modeling.
Machine learning, 95(3):423–469.

Jagadeesh Jagarlamudi, Hal Daumé III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 204–213,
Avignon, France.

Jey Han Lau and Timothy Baldwin. 2016. The sensitiv-
ity of topic coherence evaluation to topic cardinality.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL HLT), pages 483–487, San Diego, CA.

Dingcheng Li, Jingyuan Zhang, and Ping Li. 2019.
Tmsa: A mutual learning model for topic discov-
ery and wordembedding. In Proceedings of the
SIAM conference on Data Mining (SDM), Calgary,
Canada.

Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan
Miao. 2016. Generative topic embedding: a contin-
uous representation of documents. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 666–675,
Berlin, Germany.

Wei Li and Andrew McCallum. 2006. Pachinko al-
location: Dag-structured mixture models of topic
correlations. In Proceedings of the 23rd inter-
national conference on Machine learning (ICML),
pages 577–584, Pittsburgh, PA.

Steven N MacEachern and Peter Müller. 1998. Esti-
mating mixture of dirichlet process models. Jour-
nal of Computational and Graphical Statistics,
7(2):223–238.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd annual
meeting of the association for computational lin-
guistics (ACL): system demonstrations, pages 55–
60, Baltimore, MD.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems (NIPS), pages 3111–3119, Lake Tahoe, NV.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 262–272, Edinburgh, UK.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL), pages 100–108, Los Angeles,
LA.

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015. Improving topic models with
latent feature word representations. Transactions
of the Association for Computational Linguistics,
3:299–313.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543, Doha, Qatar.

Jim Pitman. 2002. Poisson–dirichlet and gem invari-
ant distributions for split-and-merge transformations
of an interval partition. Combinatorics, Probability
and Computing, 11(5):501–514.

Masa-Aki Sato. 2001. Online model selection based
on the variational bayes. Neural computation,
13(7):1649–1681.



950

Jayaram Sethuraman. 1994. A constructive definition
of dirichlet priors. Statistica sinica, pages 639–650.

Bei Shi, Wai Lam, Shoaib Jameel, Steven Schockaert,
and Kwun Ping Lai. 2017. Jointly learning word
embeddings and latent topics. In Proceedings of
the 40th International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR), pages 375–384, Shinjuku, Tokyo.

Yee W Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2005. Sharing clusters among re-
lated groups: Hierarchical dirichlet processes. In
Advances in neural information processing systems
(NIPS), pages 1385–1392, Vancouver, Canada.

Yee W Teh, Kenichi Kurihara, and Max Welling. 2008.
Collapsed variational inference for HDP. In Ad-
vances in neural information processing systems
(NIPS), pages 1481–1488, Vancouver, Canada.

Chong Wang, John Paisley, and David Blei. 2011. On-
line variational inference for the hierarchical dirich-
let process. In Proceedings of the Fourteenth Inter-
national Conference on Artificial Intelligence and
Statistics (AISTATS), pages 752–760, Fort Laud-
erdale, FL.

Liang Yao, Yin Zhang, Baogang Wei, Zhe Jin, Rui
Zhang, Yangyang Zhang, and Qinfei Chen. 2017.
Incorporating knowledge graph embeddings into
topic modeling. In Proceedings of the Thirty-First
AAAI Conference on Artificial Intelligence (AAAI),
pages 3119–3126, San Francisco, CA.

He Zhao, Lan Du, Wray L. Buntine, and Gang Liu.
2017. Metalda: A topic model that efficiently in-
corporates meta information. In Proceedings of the
2017 IEEE International Conference on Data Min-
ing (ICDM), pages 635–644, New Orleans, LA.


