



















































A Trio Neural Model for Dynamic Entity Relatedness Ranking


Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 31–41
Brussels, Belgium, October 31 - November 1, 2018. c©2018 Association for Computational Linguistics

31

A Trio Neural Model for Dynamic Entity Relatedness Ranking

Tu Ngoc Nguyen
L3S Research Center
tunguyen@L3S.de

Tuan Tran
Robert Bosch GmbH

anhtuan.tran2@de.bosch.com

Wolfgang Nejdl
L3S Research Center
nejdl@L3S.de

Abstract

Measuring entity relatedness is a fundamen-
tal task for many natural language processing
and information retrieval applications. Prior
work often studies entity relatedness in static
settings and an unsupervised manner. How-
ever, entities in real-world are often involved
in many different relationships, consequently
entity-relations are very dynamic over time.
In this work, we propose a neural network-
based approach for dynamic entity relatedness,
leveraging the collective attention as supervi-
sion. Our model is capable of learning rich
and different entity representations in a joint
framework. Through extensive experiments
on large-scale datasets, we demonstrate that
our method achieves better results than com-
petitive baselines.

1 Introduction

Measuring semantic relatedness between entities
is an inherent component in many text mining ap-
plications. In search and recommendation, the
ability to suggest most related entities to the
entity-bearing query has become a standard fea-
ture of popular Web search engines (Blanco et al.,
2013). In natural language processing, entity re-
latedness is an important factor for various tasks,
such as entity linking (Hoffart et al., 2012) or word
sense disambiguation (Moro et al., 2014).

However, prior work on semantic relatedness
often neglects the time dimension and consider
entities and their relationships as static. In prac-
tice, many entities are highly ephemeral (Jiang
et al., 2016), and users seeking information re-
lated to those entities would like to see fresh infor-
mation. For example, users looking up the entity
Taylor Lautner during 2008–2012 might want to
be recommended with entities such as The Twi-
light Saga, due to Lautner’s well-known perfor-
mance in the film series; however the same query

in August 2016 should be served with entities re-
lated to his appearances in more recent films such
as “Scream Queens”, “Run the Tide”. In addition,
much of previous work resorts to deriving seman-
tic relatedness from co-occurrence -based compu-
tations or heuristic functions without direct opti-
mization to the final goal. We believe that desir-
able framework should see entity semantic relat-
edness as not separate but an integral part of the
process, for instance in a supervised manner.

In this work, we address the problem of en-
tity relatedness ranking, that is, designing the se-
mantic relatedness models that are optimized for
ranking systems such as top-k entity retrieval or
recommendation. In this setting, the goal is not
to quantify the semantic relatedness between two
entities based on their occurrences in the data,
but to optimize the partial order of the related
entities in the top positions. This problem dif-
fers from traditional entity ranking (Kang et al.,
2015) in that the entity rankings are driven by
user queries and are optimized to their (ad-hoc) in-
formation needs, while entity relatedness ranking
also aims to uncover the meanings of the the relat-
edness from the data. In other words, while con-
ventional entity semantic relatedness learns from
data (editors or content providers’ perspectives),
and entity ranking learns from the user’s perspec-
tive, the entity relatedness ranking takes the trade-
off between these views. Such a hybrid approach
can benefit applications such as exploratory entity
search (Miliaraki et al., 2015), where users have
a specific goal in mind, but at the same time are
opened to other related entities.

We also tackle the issue of dynamic ranking and
design the supervised-learning model that takes
into account the temporal contexts of entities, and
proposes to leverage collective attention from pub-
lic sources. As an illustration, when one looks into
the Wikipedia page of Taylor Lautner, each navi-



32

Figure 1: The dynamics of collective attention for
related entities of Taylor Lautner in 2016.

gation to other Wikipedia pages indicates the user
interest in the corresponding target entity given her
initial interest in Lautner. Collectively, the naviga-
tion traffic observed over time is a good proxy to
the shift of public attention to the entity (Figure 1).

In addition, while previous work mainly focuses
on one aspect of the entities such as textual profiles
or linking graphs , we propose a trio neural model
that learns the low level representations of entities
from three different aspects: Content, structures
and time aspects. For the time aspect, we pro-
pose a convolutional model to embed and attend
to local patterns of the past temporal signals in the
Euclidean space. Experiments show that our trio
model outperforms traditional approaches in rank-
ing correlation and recommendation tasks. Our
contributions are summarized as follows.

• We present the first study of dynamic en-
tity relatedness ranking using collective at-
tention.

• We introduce an attention-based convolu-
tional neural networks (CNN) to capture the
temporal signals of an entity.

• We propose a joint framework to incorporate
multiple views of the entities, both from con-
tent provider and from user’s perspectives,
for entity relatedness ranking.

2 Related Work

2.1 Entity Relatedness and Recommendation
Most of existing semantic relatedness measures
(e.g. derived from Wikipedia) can be divided into
the following two major types: (1) text-based,
(2) graph-based. For the first, traditional meth-
ods mainly focus on a high-dimensional semantic

space based on occurrences of words ( Gabrilovich
and Markovitch (2007, 2009)) or concepts ( Ag-
garwal and Buitelaar (2014)). In recent years, em-
bedding methods that learn low-dimensional word
representations have been proposed. Hu et al.
(2015) leverages entity embedding on knowledge
graphs to better learn the distributional seman-
tics. Ni et al. (2016) use an adapted version of
Word2Vec, where each entity in a Wikipedia page
is considered as a term. For the graph-based ap-
proaches, these measures usually take advantage
of the hyperlink structure of entity graph (Wit-
ten and Milne, 2008; Guo and Barbosa, 2014).
Recent graph embedding techniques (e.g., Deep-
Walk (Perozzi et al., 2014)) have not been directly
used for entity relatedness in Wikipedia, yet its
performance is studied and shown very compet-
itive in recent related work (Zhao et al., 2015;
Ponza et al., 2017).

Entity relatedness is also studied in connec-
tion with the entity recommendation task. The
Spark (Blanco et al., 2013) system firstly intro-
duced the task for Web search, Yu et al. (2014);
Zhang et al. (2016a) exploit user click logs and
entity pane logs for global and personalized en-
tity recommendation. However, these approaches
are optimized to user information needs, and also
does not target the global and temporal dimension.
Recently, Zhang et al. (2016b); Tran et al. (2017)
proposed time-aware probabilistic approaches that
combine ‘static’ entity relatedness with tempo-
ral factors from different sources. Nguyen et al.
(2018) studied the task of time-aware ranking for
entity aspects and propose an ensemble model to
address the sub-features competing problem.

2.2 Neural Network Models
Neural Ranking. Deep neural ranking among
IR and NLP can be generally divided into two
groups: representation-focused and interaction-
focused models. The representation-focused ap-
proach (Huang et al., 2013) independently learns
a representation for each ranking element (e.g.,
query and document) and then employ a similar-
ity function. On the other hand, the interaction-
focused models are designed based on the early
interactions between the ranking pairs as the input
of network. For instance, Lu and Li (2013); Guo
et al. (2016) build interactions (i.e., local match-
ing signals) between two pieces of text and trains a
feed-forward network for computing the matching
score. This enables the model to capture various



33

interactions between ranking elements, while with
former, the model has only the chance of isolated
observation of input elements.

Attention networks. In recent years, attention-
based NN architectures, which learn to focus their
“attention” to specific parts of the input, have
shown promising results on various NLP tasks.
For most cases, attentions are applied on sequen-
tial models to capture global context (Luong et al.,
2015). An attention mechanism often relies on a
context vector that facilitates outputting a “sum-
mary” over all (deterministic soft) or a sample
(stochastic hard) of input states. Recent work
proposed a CNN with attention-based framework
to model local context representations of textual
pairs (Yin et al., 2016), or to combine with LSTM
to model time-series data (Ordóñez and Roggen,
2016; Lin et al., 2017) for classification and trend
prediction tasks.

3 Problem

3.1 Preliminaries

We denote as named entities any real-world ob-
jects registered in a database. Each entity has a
textual document (e.g. content of a home page),
and a sequence of references to other entities (e.g.,
obtained from semantic annotations), called the
entity link profile. All link profiles constitute an
entity linking graph. In addition, two types of in-
formation are included to form the entity collec-
tive attention.

Temporal signals. Each entity can be asso-
ciated with a number of properties such as view
counts, content edits, etc. Given an entity e and
a time point n, given D properties, the temporal
signals set, in the form of a (univariate or multi-
variate) time series X ∈ RD×T consists of T real-
valued vector xn−T , · · · ,xn−1 , where xt ∈ RD cap-
tures the past signals of e at time point t.

Entity Navigation. In many systems, the user
navigation between two entities is captured, e.g.,
search engines can log the total click-through of
documents of the target entity presented in search
results of a query involving the source entity. Fol-
lowing learning to rank approaches (Kang et al.,
2015), we use this information as the ground truth
in our supervised models. Given two entities
e1,e2, the navigation signal from e1 to e2 at time
point t is denoted by yt{e1,e2}.

3.2 Problem Definition
In our setting, it is not required to have a pre-
defined, static function quantifying the semantic
relatedness between two entities. Instead, it can
capture a family of functions F where the prior
distribution relies on time parameter. We formal-
ize the concepts below.

Dynamic Entity Relatedness between two en-
tities es,et , where es is the source entity and et is
the target entity, in a given time t, is a function (de-
noted by ft(es,et)) with the following properties.

• asymmetric: ft(ei,e j) 6= ft(e j,ei)

• non-negativity: f (ei,e j)≥ 0

• indiscernibility of identicals: ei = e j →
f (ei,e j) = 1

Dynamic Entity Relatedness Ranking. Given
a source entity es and time point t, rank the candi-
date entities et’s by their semantic relatedness.

4 Approach Overview

4.1 Datasets and Their Dynamics
In this work we use Wikipedia data as the case
study for our entity relatedness ranking problem
due to its rich knowledge and dynamic nature.
It is worth noting that despite experimenting on
Wikipedia, our framework is universal can be ap-
plied to other sources of entity with available
temporal signals and entity navigation. We use
Wikipedia pages to represent entities and page
views as the temporal signals (details in sec-
tion 6.1).

Clickstream. For entity navigation, we use the
clickstream dataset generated from the Wikipedia
webserver logs from February until September,
2016. These datasets contain an accumulation of
transitions between two Wikipedia articles with
their respective counts on a monthly basis. We
study only actual pages (e.g. excluding disam-
biguation or redirects). In the following, we pro-
vide the first analysis of the clickstream data to
gain insights into the temporal dynamics of the en-
tity collective attention in Wikipedia.

Figure 2a illustrates the distribution of entities
by click frequencies, and the correlation of top
popular entities (measured by total navigations)
across different months is shown in Figure 2b. In
general, we observe that the user navigation activ-
ities in the top popular entities are very dynamic,



34

(a) Click times distribution (b) Correlation of top-k entities (c) Correlation by # of navigations

Figure 2: Click (navigation) times distribution and ranking correlation of entities in September 2016.

% new
es

% with new
et

% w. new
et in top-30

# new et
(avg.)

08-2016 24.31 71.18 15.54 18.25
04-2016 30.61 66.72 53.44 42.20

Table 1: Statistics on the dynamic of clickstream,
es denote source entities, et related entities.

and changes substantially with regard to time. Fig-
ure 2c visualizes the dynamics of related entities
toward different ranking sections (e.g., from rank
0 to rank 20) of different months, in terms of their
correlation scores. It can be interpreted that the
entities that stay in top-20 most related ones tend
to be more correlated than entities in bottom-20
when considering top-100 related entities.

As we show in Table 1, there are 24.31% of en-
tities in top-10,000 most active entities of Septem-
ber 2006 do not appear in the same list the previ-
ous month. And 30.61% are new compared with
5 months before. In addition, there are 71% of
entities in top-10,000 having navigations to new
entities compared to the previous month, with ap-
prox. 18 new entities are navigated to, on aver-
age. Thus, the datasets are naturally very dynamic
and sensitive to change. The substantial amount of
missing past click logs on the newly-formed rela-
tionships also raises the necessity of an dynamic
measuring approach.

Figure 3 shows the overall architecture of our
framework, which consists of three major compo-
nents: time-, graph- and content-based networks.
Each component can be considered as a separate
sub-ranking network. Each network accepts a tu-
ple of three elements/representations as an input
in a pair-wise fashion, i.e., the source entity es, the
target entity et with higher rank (denoted as e(+))
and the one with lower rank (denoted as e(−)). For
the content network, each element is a sequence
of terms, coming from entity textual representa-
tion. For the graph network, we learn the embed-

Figure 3: The trio neural model for entity ranking.

dings from the entity linking graph. For the time
network, we propose a new convolutional model
learning from the entity temporal signals. More
detailed are described as follows.

4.2 Neural Ranking Model Overview

The entity relatedness ranking can be handled by
a point-wise ranking model that learns to predict
relatedness score directly. However, as the navi-
gational frequency distribution is often skewed at
top, supervisions guided by long-tail navigations
would be prone to errors. Hence instead of learn-
ing explicitly a calibrated scoring function, we opt
for a pair-wise ranking approach. When apply-
ing to ranking top-k entities, this approach has
the advantage of correctly predicting partial orders
of different relatedness functions ft at any time
points regardless of their non-transitivity (Cheng
et al., 2012).

This work builds upon the idea of interaction-
based deep neural models, i.e. learning soft se-
mantic matches from the source-target entity pairs.
Note that, we do not aim for a Siamese archi-
tecture (Chopra et al., 2005) (i.e., in representa-
tion-based models), where the weight parameters



35

are shared across networks. The reason is that,
the conventional kind of network produces a sym-
metric relation, violating the asymmetric prop-
erty of the relatedness function ft (section 3.2).
Concretely, each deep network ψ consists of an
input layer z0, n− 1 hidden layers and an out-
put layer zn. Each hidden layer zi is a fully-
connected network that computes the transforma-
tion: zi = σ(wi · zi−1 + bi), where wi and bi are
the weight matrix and bias at hidden layer i, σ is
a non-linear function such as the rectified linear
unit(ReLU). The final score under the trio setup is
summed from multiple networks.

φ(< es,e(+),e(−) >) = φtime +φgraph +φcontent
(1)

In the next section we describe the input repre-
sentations z0 for each network.

5 Entity Relatedness Ranking

5.1 Content-based representation learning

To learn the entity representation from its content,
we rely on entity textual document (word-based)
as well as its link profile (entity-based) (sec-
tion 3.1). Since the vocabulary size of entities and
words is often very large, conventional one-hot
vector representation becomes expensive. Hence,
we adopt the word hashing technique from (Huang
et al., 2013), that breaks a term into character tri-
graphs and thus can dramatically reduce the size
of the vector dimensionality. We then rely on em-
beddings to learn the distributed representations
and build up the soft semantic interactions via in-
put concatenation. Let E : V → Rm be the em-
bedding function, V is the vocabulary and m is
the embedding size. w : V → R, is the weight-
ing function that learns the global term impor-
tance and a weighted element-wise sum of word
embedding vectors -compositionality function ⊕,
the word-based representation for entity e is hence
⊕|ew|i=1(E(wi),w(wi)). For entity-based representa-
tion, we break down the surface form of a linked
entity into bag-of-words and apply analogously.
The concatenation of the two representations for
the tuple < es,e(+),e(−) > is then input to the deep
feed-forward network.

5.2 Graph-based representation

To obtain the graph embedding for each entity, we
adopt the idea of DeepWalk (Perozzi et al., 2014),
which learns the embedding by predicting the ver-

tex sequence generated by random walk. Con-
cretely, given an entity e, we learn to predict the
sequence of entity references Se – which can be
considered as the graph-wise context in the Skip-
gram model. We then adopt the matching his-
togram mapping in (Guo et al., 2016) for the soft
interaction of the ranking model. Specifically, de-
note the bag of entities representation of es as Ces ,
and that of et as Cet ; we discretize the soft match-
ing (calculated by cosine similarity of the embed-
ding vectors) of each entity pair in (Ces ,Cet ) into
different bins. The logarithmic numbers of the
count values of each bin then constitute the in-
teraction vector. This soft-interaction in a way is
similar in the idea with the traditional link-based
model (Witten and Milne, 2008), where the relat-
edness measure is based on the overlapping of in-
coming links.

5.3 Attention-based CNN for temporal
representation

For learning representation from entity temporal
signals, the intuition is to model the low-level tem-
poral correlation between two multivariate time
series. Specifically, we learn to embed these time
series of equal size T into an Euclidean space,
such that similar pairs are close to each other. Our
embedding function takes the form of a convolu-
tional neural network (CNN), shown in Figure 4.
The architecture rests on four basic layers: a 1-
D convolutional (that restricts the slide only along
the time window dimension, following (Zheng
et al., 2014)), a batch-norm, an attention-based
and a fully connected layer.

Convolution layer: A 1-D convolution opera-
tion involves applying a filter w f ∈R1×w×D (i.e., a
matrix of weight parameters) to each subsequence
X ie of window size m to produce a new abstraction.

qi =w fLit:t+m−1,D+b; si =BN(qi); hi =ReLU(si)
(2)

where Lit:t+w−1,D denotes the concatenation of
w vectors in the lookup layer representing the sub-
sequence X ie, b is a bias term. The convolutional
layer is followed by a batch normalization (BN)
layer (Ioffe and Szegedy, 2015), to speed up the
convergence and help improve generalization.

Attention Mechanism: We apply an atten-
tion layer on the convolutional outputs. Con-
ceptually, attention mechanisms allow NN mod-
els to focus selectively on only the important fea-



36

Figure 4: The attentional CNN for time series rep-
resentation.

tures, based on the attention weights that often
derived from the interaction with the target or
within the input itself (self-attention) (Vaswani
et al., 2017). We adopt the former approach, with
the intuition that the time-spatial patterns should
not be treated equally, but the ones near the stud-
ied time should gain more focus. To ensure that
each feature in Fci that associates with different
timestamps are rewarded differently, the attention
weights are guided by a time-decay weight func-
tion, in a recency-favor fashion. More formally,
let A ∈ RT−w+1×1 be the time context vector and
Fci ∈ R1×(T−w+1) the output of convolution for X .
Then the kth column of the re-weighted feature
map Fhi is derived by:

Fhi [:,k] = A[k] ·Fci [:,k],k = 1 · · ·T −w+1 (3)

The time context vector a is generated by a
decay weight function, since each column k in
the vector is associated with a time tk which is
T − k+w time units away from studied time t.

Decay weight function: we leverage the Poly-
nomial Curve for the function. PD(ti, t) =

1
(t−ti)α+1 , whereas α defines the decay rate. It is
worth noting that when α is increased, the atten-
tion layer acts just like a pooling one 1. Stacking
up multiple convolutional layers is possible, in this
case |A| is the size of the previous layer. The at-
tention layer is only applied to the last convolution
layer in our architecture. The output of the atten-
tion layer is then passed to a fully-connected layer
with non-linear activation to obtain the temporal
representation.

1Note that, for clear visualization, we put flattening before
attention layer in Figure 4

5.4 Learning and Optimization
Finally, we describe the optimization and training
procedure of our network. We use a Logarithmic
loss that can lead to better probability estimation
at the cost of accuracy 2. Our network minimizes
the cross-entropy loss function as follows:

L =− 1
N

N

∑
i=1

[P{es,e1,e2}i log ȳi

+(1−P{es,e1,e2}i) log(1− ȳi)]+λ |θ |
2
2 (4)

where N is the training size, ȳ is the output
of the sigmoid layer on the predicted label. θ
contains all the parameters of the network and
λ |θ |22 is the L2 regularization. P{es,e(+),e(−)}i is the
probability that e(+) is ranked higher than e(−)
derived from entity navigation, P{es,e(+),e(−)}i =

yt(i){es,e(+)}/(y
t(i)
{es,e(+)}

+ yt(i){es,e(−)}), where t(i) is the
observed time point of the training instance i. The
network parameters are updated using Adam opti-
mizer (Kingma and Ba, 2014).

6 Experiments

6.1 Dataset
To recap from Section 4.1, we use the click stream
datasets in 2016. We also use the corresponding
Wikipedia article dumps, with over 4 million enti-
ties represented by actual pages. Since the length
of the content of an Wikipedia article is often long,
in this work, we make use of only its abstract sec-
tion. To obtain temporal signals of the entity, we
use page view statistics of Wikipedia articles and
aggregate the counts by month. We fetch the data
from June, 2014 up until the studied time, which
results in the length of 27 months.

Seed entities and related candidates. To ex-
tract popular and trending entities, we extract from
the clickstream data the top 10,000 entities based
on the number of navigations from major search
engines (Google and Bing), at the studied time.
Getting the subset of related entity candidates –
for efficiency purposes– has been well-addressed
in related work (Guo and Barbosa, 2014; Ponza
et al., 2017). In this work, we do not leverage a
method and just assume the use of an appropriate
one. In the experiment, we resort to choose only

2Other ranking-based loss such as Hinge loss favours over
sparsity and accuracy (in the sense of direct punishing mis-
classification via margins) at the cost of probability estima-
tion. The logistic loss distinguishes better between examples
whose supervision scores are close.



37

Counts
Total seed entities 10,000
Total entities 1,420,819
Candidate per entities (avg.) 142

Training seed entities 8,000
Dev. seed entities 1,000
Test seed entities 1,000

Training pairs 100,650K
Dev. pairs 12,420K
Test pairs 12,590K

Table 2: Statistics of the dataset.

candidates which are visited from the seed entities
at studied time. We filtered out entity-candidate
pairs with too few navigations (less than 10) and
considered the top-100 candidates.

6.2 Models for Comparison
In this paper, we compare our models against the
following baselines.

Wikipedia Link-based (WLM): Witten and
Milne (2008) proposed a low-cost measure of
semantic relatedness based on Wikipedia entity
graph, inspired by Normalized Google Distance.

DeepWalk (DW): DeepWalk (Perozzi et al.,
2014) learned representations of vertices in a
graph with a random walk generator and language
modeling. We chose not to compare with the ma-
trix factorization approach in (Zhao et al., 2015),
as even though it allows the incorporation of dif-
ferent relation types (i.e., among entity, category
and word), the iterative computation cost over
large graphs is very expensive. When consider
only entity-entity relation, the performance is re-
ported rather similar to DW.

Entity2Vec Model (E2V): or entity embedding
learning using Skip-Gram (Mikolov et al., 2013)
model. E2V utilizes textual information to capture
latent word relationships. Similar to Zhao et al.
(2015); Ni et al. (2016), we use Wikipedia arti-
cles as training corpus to learn word vectors and
reserved hyperlinks between entities.

ParaVecs (PV): Le and Mikolov (2014); Dai
et al. (2015) learned document/entity vectors via
the distributed memory (ParaVecs-DM) and dis-
tributed bag of words (ParaVecs-DBOW) models,
using hierarchical softmax. We use Wikipedia ar-
ticles as training corpus to learn entity vectors.

RankSVM: Ceccarelli et al. (2013) learned
entity relatedness from a set of 28 handcrafted
features, using the traditional learning-to-rank
method, RankSVM. We put together additional
well-known temporal features (Kanhabua et al.,
2014; Zhang et al., 2016b) (i.e., time series cross

correlation, trending level and predicted popular-
ity based on page views) and report the results of
the extended feature set.

For our approach, we tested different combina-
tions of content (denoted as ContentEmb), graph,
(GraphEmb) and time (TS-CNN-Att) networks.
We also test the content and graph networks with
pretrained entity representations (i.e., ParaVecs-
DM and DeepWalk).

6.3 Experimental Setup
Evaluation procedures. The time granularity is
set to months. The studied time tn of our experi-
ments is September 2016. From the seed queries,
we use 80% for training, 10% for development and
10% for testing, as shown in Table 2. Note that, for
the time-aware setting and to avoid leakage and
bias as much as possible, the data for training and
development (including supervision) are up until
time tn−1. In specific, for content and graph data,
only tn−1 is used.

Metrics. We use 2 correlation coefficient meth-
ods, Pearson and Spearman, which have been used
often throughout literature, cf. (Dallmann et al.,
2016; Ponza et al., 2017). The Pearson index
focuses on the difference between predicted-vs-
correct relatedness scores, while Spearman fo-
cuses on the ranking order among entity pairs. Our
work studies on the strength of the dynamic re-
latedness between entities, hence we focus more
on Pearson index. However, traditional correlation
metrics do not consider the positions in the ranked
list (correlations at the top or bottom are treated
equally). For this reason, we adjust the metric to
consider the rankings at specific top-k positions,
which consequently can be used to measure the
correlation for only top items in the ranking (based
to the ground truth). In addition, we use Normal-
ized Discounted Cumulative Gain (NDCG) mea-
sure to evaluate the recommendation tasks.

Implementation details. All neural models
are implemented in TensorFlow. Initial learning
rate is tuned amongst {1.e-2, 1.e-3, 1.e-4, 1.e-5}.
The batch size is tuned amongst {50, 100, 200}.
The weight matrices are initialized with samples
from the uniform distribution (Glorot and Ben-
gio, 2010). Models are trained for maximum 25
epochs. The hidden layers for each network are
among {2, 3, 4}, while for hidden nodes are {128,
256, 512}. Dropout rate is set from {0.2, 0.3,
0.5}. The pretrained DW is empirically set to 128
dimensions, and 200 for PV. For CNN, the filter



38

number are in {10, 20, 30}, window size in {4,
5, 6}, convolutional layers in {1, 2, 3} and decay
rate α in {1.0, 1.5,· · · ,7.5}. 2 conv- layers with
window size 5 and 4, number of filters of 20 and
25 respectively are used for decay hyperparameter
analysis.

6.4 Experimental Tasks

We evaluate our proposed method in two differ-
ent scenarios: (1) Relatedness ranking and (2) En-
tity recommendation. The first task evaluates how
well we can mimic the ranking via the entity nav-
igation. Here we use the raw number of naviga-
tions in Wikipedia clickstream. The second task
is formulated as: given an entity, suggest the top-k
most related entities to it right now. Since there
is no standard ground-truth for this temporal task,
we constructed two relevance ground-truths. The
first one is the proxy ground-truth, with relevance
grade is automatically assigned from the (top-100)
most navigated target entities. The graded rele-
vance score is then given as the reversed rank or-
der. For this, all entities in the test set are used.
The second one is based on the human judgments
with 5-level graded relevance scale, i.e., from 4
- highly relevant to 0 - not (temporally) relevant.
Two human experts evaluate on the subset of 20
entities (randomly sampled from the test set), with
600 entity pairs (approx. 30 per seed, using pool-
ing method). The ground-truth size is comparable
the widely used ground-truth for static relatedness
assessment, KORE (Hoffart et al., 2012). The Co-
hen’s Kappa agreement is 0.72. Performance of
the best-performed models on this dataset is then
tested with paired t-test against the WLM baseline.

6.5 Results on Relatedness Ranking

We report the performance of the relatedness rank-
ing on the left side of Table 3, with the Pear-
son and Spearman metrics. Among existing base-
lines, we observe that link-based approaches i.e.,
WLM and DeepWalk perform better than others
for top-k correlation. Whereas, temporal mod-
els yield substantial improvement overall. Specif-
ically, the TS-CNN-Att performs better than the
no-attention model in most cases, improves 11%
for Pearson@10, and 3% when considering the to-
tal rank. Our trio model performs well overall,
gives best results for total rank. The duo models
(combine base with either pretrained DW or PV)
also deliver improvements over the sole tempo-
ral ones. We also observer additional gains while

combining of temporal base with pretrained DW
and PV altogether.

6.6 Results on Entity Recommendation

Here we report the results on the nDCG metrics.
Table 3 (right-side) demonstrates the results for
two ground-truth settings (proxy and human). We
can observe the good performance of the baselines
for this task over conventional temporal models,
significantly for proxy setting. It can be explained
that, ‘static’ entity relations are ranked high in
the non time-aware baselines, hence are still re-
warded when considering a fine-grained grading
scale (100 level). The margin becomes smaller
when comparing in human setting, with the stan-
dard 5-level scale. All the models with pretrained
representations perform poorly. It shows that for
this task, early interaction-based approach is more
suitable than purely based on representation.

6.7 Additional Analysis

We present an anecdotic example of top-selected
entities for Kingsman: The Golden Circle in
Table 4. While the content-based model favors
old relations like the preceding movies, TS-CNN
puts popular actress Halle Berry or the recent re-
leased X-men: Apocalypse on top. The latter
is not ideal as there is not a solid relationship be-
tween the two movies. One implication is that the
two entities are ranked high is more because of the
popularity of themself than the strength of the rela-
tionship toward the source entity. The Trio model
addresses the issue by taking other perspectives
into account, and also balances out the recency
and long-term factors, gives the best ranking per-
formance.

Analysis on decay hyper-parameter. We give
a study on the effect of decay parameter on per-
formance. Figure 5a illustrates the results on
Pearsonall and nDCG@10 for the trio model. It
can be seen that while nDCG slightly increases,
Pearson score peaks while α in the range [1.5,3.5].
Additionally, we show the convergence analysis
on α for TS-CNN-Att in Figure 6. Bigger α tends
to converge faster, but to a significant higher loss
when α is over 5.5 (omitted from the Figure).

Performances on different entity types. We
demonstrate in Figures 5b and 5c the model per-
formances on the person and event types. WLM
performs poorer for the latter, that can be inter-
preted as link-based methods tend to slowly adapt



39

Model Pearson ×100 ρ×100 nDCG (proxy) nDCG (human)@10 @30 @50 all all @3 @10 @20 @3 @10 @20
B

as
el

in
es

WLM 27.6 28.3 24.0 19.4 12.1 0.63 0.59 0.62 0.50 0.46 0.52
RankSVM 28.5 34.7 31.4 20.7 27.5 0.65 0.61 0.64 0.52 0.61 0.65
Entity2Vec 18.6 22.0 21.8 20.5 18.7 0.62 0.60 0.61 0.54 0.53 0.54
DeepWalk 31.3 30.9 21.4 17.6 10.1 0.41 0.43 0.47 0.34 0.38 0.45
ParaVecs-DBOW 18.6 22.0 21.8 20.5 16.0 0.62 0.60 0.61 0.50 0.50 0.55
ParaVecs-DM 19.0 23.0 23.2 22.3 18.3 0.66 0.63 0.63 0.49 0.52 0.58

M
od

el
A

bl
at

io
n

TS-CNN 51.9 51.0 43.0 35.8 26.5 0.41 0.43 0.47 0.40 0.43 0.48
TS-CNN-Att (Base) 57.9 49.7 44.7 37.1 24.9 0.43 0.44 0.49 0.38 0.45 0.50
Base+PV 60.6 44.2 41.4 36.4 11.2 0.41 0.43 0.47 0.49 0.51 0.55
Base+DW 43.5 36.5 35.7 32.7 31.0 0.44 0.48 0.53 0.47 0.51 0.52
Base+PV+DW 56.9 46.1 43.4 32.9 28,4 0.41 0.44 0.48 0.49 0.54 0.57

ContentEmb+GraphEmb 48.9 40.1 49.9 37.5 27.9 0.67 0.62 0.70 0.61 0.69 0.65
Base+ContentEmb 67.1 54.2 53.4 43.7 26.5 0.67 0.69 0.71 0.61 0.72 0.74
Base+GraphEmb 55.2 50.2 41.3 31.5 35.5 0.71 0.75 0.78 0.65∓ 0.78∓ 0.81∓

Trio 58.6 54.3 50.2 45.4 43.5 0.75 0.78 0.83 0.74∓ 0.82∓ 0.85∓

Table 3: Performance of different models on task (1) Pearson, Spearman’s ρ ranking correlation, and
task (2) recommendation (measured by nDCG). Bold and underlined numbers indicate best and second-
to-best results. ∓ shows statistical significant over WLM (p < 0.05).

 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  1  2  3  4  5  6  7

alpha

Pearsonall

nDCG@10

(a) Decay parameter for time-series
embedding.

(b) Model performances for person-
type entities.

(c) Model performances for social
event-type entities.

Figure 5: Performance results for variation of decay parameter and different entity types.

 1

 10

 100

 1000

 10000

 100000

 0  500000  1x10
6

 1.5x10
6

 2x10
6

 2.5x10
6

 3x10
6

V
a
lid

a
ti
o
n
 L

o
s
s

iterations

alpha=2.5
alpha=2.0
alpha=1.5
alpha=1.0
alpha=0.5

Figure 6: Convergence of decay parameters.

for recent trending entities. The temporal models
seem to capture these entites better.

7 Conclusion
In this work, we presented a trio neural model to
solve the dynamic entity relatedness ranking prob-
lem. The model jointly learns rich representations
of entities from textual content, graph and tempo-
ral signals. We also propose an effective CNN-
based attentional mechanism for learning the tem-

Models
PV-DM TS-CNN-Att Temp+PV Trio

Secret Service Halle Berry Elton John Mark Strong
Spider-Man X-Men Taron Egerton Jeff Bridges

Taron Egerton Jeff Bridges Edward Holcroft Julianne More

Table 4: Different top-k rankings for entity Kings-
man: The Golden Circle. Italic means irrelevance.

poral representation of an entity. Experiments
on ranking correlations and top-k recommenda-
tion tasks demonstrate the effectiveness of our ap-
proach over existing baselines. For future work,
we aim to incorporate more temporal signals, and
investigate on different ‘trainable’ attention mech-
anisms to go beyond the time-based decay, for in-
stance by incorporating latent topics.

Acknowledgments. This work is funded by the
ERC Advanced Grant ALEXANDRIA (grant no.
339233). We thank the reviewers for the sugges-
tions on the content and structure of the paper.



40

References

Nitish Aggarwal and Paul Buitelaar. 2014. Wikipedia-
based distributional semantics for entity relatedness.
In 2014 AAAI Fall Symposium Series.

Roi Blanco, Berkant Barla Cambazoglu, Peter Mika,
and Nicolas Torzec. 2013. Entity recommendations
in web search. In ISWC, pages 33–48. Springer.

Diego Ceccarelli, Claudio Lucchese, Salvatore Or-
lando, Raffaele Perego, and Salvatore Trani. 2013.
Learning relatedness measures for entity linking. In
Proceedings of the 22nd ACM international con-
ference on Information & Knowledge Management,
pages 139–148. ACM.

Weiwei Cheng, Eyke Hüllermeier, Willem Waegeman,
and Volkmar Welker. 2012. Label ranking with
partial abstention based on thresholded probabilistic
models. In F. Pereira, C. J. C. Burges, L. Bottou, and
K. Q. Weinberger, editors, Advances in Neural In-
formation Processing Systems 25, pages 2501–2509.
Curran Associates, Inc.

Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with
application to face verification. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
539–546. IEEE.

Andrew M Dai, Christopher Olah, and Quoc V Le.
2015. Document embedding with paragraph vec-
tors. arXiv preprint arXiv:1507.07998.

Alexander Dallmann, Thomas Niebler, Florian Lem-
merich, and Andreas Hotho. 2016. Extracting se-
mantics from random walks on wikipedia: Compar-
ing learning and counting methods.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI’07, pages 1606–1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.

Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation for natural
language processing. Journal of Artificial Intelli-
gence Research, 34:443–498.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the thirteenth in-
ternational conference on artificial intelligence and
statistics, pages 249–256.

Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce
Croft. 2016. A deep relevance matching model
for ad-hoc retrieval. In Proceedings of the 25th
ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM.

Zhaochen Guo and Denilson Barbosa. 2014. Robust
entity linking via random walks. In Proceedings of
the 23rd ACM International Conference on Confer-
ence on Information and Knowledge Management,
pages 499–508. ACM.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen,
Martin Theobald, and Gerhard Weikum. 2012.
Kore: keyphrase overlap relatedness for entity dis-
ambiguation. In Proceedings of the 21st ACM inter-
national conference on Information and knowledge
management, pages 545–554. ACM.

Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao,
and Eric Xing. 2015. Entity hierarchy embedding.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1292–1300.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion & knowledge management, pages 2333–2338.
ACM.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. In International
Conference on Machine Learning, pages 448–456.

Tingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, Baobao
Chang, Sujian Li, and Zhifang Sui. 2016. Towards
time-aware knowledge graph completion. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 1715–1724.

Changsung Kang, Dawei Yin, Ruiqiang Zhang, Nico-
las Torzec, Jianzhang He, and Yi Chang. 2015.
Learning to rank related entities in web search. Neu-
rocomputing, 166:309–318.

Nattiya Kanhabua, Tu Ngoc Nguyen, and Claudia
Niederée. 2014. What triggers human remember-
ing of events? a large-scale analysis of catalysts
for collective memory in wikipedia. In Digital Li-
braries (JCDL), 2014 IEEE/ACM Joint Conference
on, pages 341–350. IEEE.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

Tao Lin, Tian Guo, and Karl Aberer. 2017. Hybrid neu-
ral networks for learning the trend in time series.

http://papers.nips.cc/paper/4811-label-ranking-with-partial-abstention-based-on-thresholded-probabilistic-models.pdf
http://papers.nips.cc/paper/4811-label-ranking-with-partial-abstention-based-on-thresholded-probabilistic-models.pdf
http://papers.nips.cc/paper/4811-label-ranking-with-partial-abstention-based-on-thresholded-probabilistic-models.pdf
http://dl.acm.org/citation.cfm?id=1625275.1625535
http://dl.acm.org/citation.cfm?id=1625275.1625535


41

Zhengdong Lu and Hang Li. 2013. A deep architec-
ture for matching short texts. In Advances in Neural
Information Processing Systems, pages 1367–1375.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Iris Miliaraki, Roi Blanco, and Mounia Lalmas. 2015.
From selena gomez to marlon brando: Understand-
ing explorative entity search. In Proceedings of the
24th International Conference on World Wide Web,
pages 765–775. International World Wide Web Con-
ferences Steering Committee.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: a unified approach. Transactions of the
Association for Computational Linguistics, 2:231–
244.

Tu Ngoc Nguyen, Nattiya Kanhabua, and Wolfgang
Nejdl. 2018. Multiple models for recommending
temporal aspects of entities. In The Semantic Web
- 15th International Conference, ESWC 2018, Her-
aklion, Crete, Greece, June 3-7, 2018, Proceedings,
pages 462–480.

Yuan Ni, Qiong Kai Xu, Feng Cao, Yosi Mass, Dafna
Sheinwald, Hui Jia Zhu, and Shao Sheng Cao.
2016. Semantic documents relatedness using con-
cept graph representation. In Proceedings of the
Ninth ACM International Conference on Web Search
and Data Mining, WSDM ’16, pages 635–644, New
York, NY, USA. ACM.

Francisco Javier Ordóñez and Daniel Roggen. 2016.
Deep convolutional and lstm recurrent neural net-
works for multimodal wearable activity recognition.
Sensors, 16(1):115.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: Online learning of social rep-
resentations. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 701–710. ACM.

Marco Ponza, Paolo Ferragina, and Soumen
Chakrabarti. 2017. A two-stage framework for
computing entity relatedness in wikipedia. In
Proceedings of the 2017 ACM on Conference on
Information and Knowledge Management, CIKM
’17, pages 1867–1876, New York, NY, USA. ACM.

Nam Khanh Tran, Tuan Tran, and Claudia Niederée.
2017. Beyond time: Dynamic context-aware entity
recommendation. In European Semantic Web Con-
ference, pages 353–368. Springer.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Ian H Witten and David N Milne. 2008. An effective,
low-cost measure of semantic relatedness obtained
from wikipedia links.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2016. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
Transactions of the Association of Computational
Linguistics, 4(1):259–272.

Xiao Yu, Hao Ma, Bo-June Paul Hsu, and Jiawei Han.
2014. On building entity recommender systems us-
ing user click log and freebase knowledge. In Pro-
ceedings of WSDM, pages 263–272. ACM.

Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing
Xie, and Wei-Ying Ma. 2016a. Collaborative
knowledge base embedding for recommender sys-
tems. In Proceedings of the 22nd ACM SIGKDD in-
ternational conference on knowledge discovery and
data mining, pages 353–362. ACM.

Lei Zhang, Achim Rettinger, and Ji Zhang. 2016b.
A probabilistic model for time-aware entity recom-
mendation. In International Semantic Web Confer-
ence, pages 598–614. Springer.

Yu Zhao, Zhiyuan Liu, and Maosong Sun. 2015. Rep-
resentation learning for measuring entity relatedness
with rich information. In Twenty-Fourth Interna-
tional Joint Conference on Artificial Intelligence.

Yi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J Leon
Zhao. 2014. Time series classification using multi-
channels deep convolutional neural networks. In
International Conference on Web-Age Information
Management, pages 298–310. Springer.

https://doi.org/10.1007/978-3-319-93417-4_30
https://doi.org/10.1007/978-3-319-93417-4_30
https://doi.org/10.1145/2835776.2835801
https://doi.org/10.1145/2835776.2835801
https://doi.org/10.1145/3132847.3132890
https://doi.org/10.1145/3132847.3132890

