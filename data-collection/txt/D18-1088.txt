



















































Neural Latent Extractive Document Summarization


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779–784
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

779

Neural Latent Extractive Document Summarization

Xingxing Zhang†, Mirella Lapata‡, Furu Wei† and Ming Zhou†
†Microsoft Research Asia, Beijing, China

‡Institute for Language, Cognition and Computation,
School of Informatics, University of Edinburgh, UK

{xizhang,fuwei,mingzhou}@microsoft.com,mlap@inf.ed.ac.uk

Abstract

Extractive summarization models require
sentence-level labels, which are usually
created heuristically (e.g., with rule-based
methods) given that most summarization
datasets only have document-summary pairs.
Since these labels might be suboptimal, we
propose a latent variable extractive model
where sentences are viewed as latent variables
and sentences with activated variables are
used to infer gold summaries. During training
the loss comes directly from gold summaries.
Experiments on the CNN/Dailymail dataset
show that our model improves over a strong
extractive baseline trained on heuristically
approximated labels and also performs
competitively to several recent models.

1 Introduction

Document summarization aims to automatically
rewrite a document into a shorter version while
retaining its most important content. Of the
many summarization paradigms that have been
identified over the years (see Mani 2001 and
Nenkova and McKeown 2011 for comprehensive
overviews), two have consistently attracted atten-
tion: extractive approaches generate summaries
by copying parts of the source document (usually
whole sentences), while abstractive methods may
generate new words or phrases which are not in
the document.

A great deal of previous work has focused on
extractive summarization which is usually mod-
eled as a sentence ranking or binary classifi-
cation problem (i.e., sentences which are top
ranked or predicted as True are selected as
summaries). Early attempts mostly leverage
human-engineered features (Filatova and Hatzi-
vassiloglou, 2004) coupled with binary classi-
fiers (Kupiec et al., 1995), hidden Markov models
(Conroy and O’leary, 2001), graph based methods

(Mihalcea, 2005), and integer linear programming
(Woodsend and Lapata, 2010).

The successful application of neural network
models to a variety of NLP tasks and the avail-
ability of large scale summarization datasets (Her-
mann et al., 2015; Nallapati et al., 2016) has
provided strong impetus to develop data-driven
approaches which take advantage of continuous-
space representations. Cheng and Lapata (2016)
propose a hierarchical long short-term memory
network (LSTM; Hochreiter and Schmidhuber
1997) to learn context dependent sentence repre-
sentations for a document and then use yet an-
other LSTM decoder to predict a binary label for
each sentence. Nallapati et al. (2017) adopt a
similar approach, they differ in their neural ar-
chitecture for sentence encoding and the features
used during label prediction, while Narayan et al.
(2018) equip the same architecture with a training
algorithm based on reinforcement learning. Ab-
stractive models (Nallapati et al., 2016; See et al.,
2017; Paulus et al., 2017) are based on sequence-
to-sequence learning (Sutskever et al., 2014; Bah-
danau et al., 2015), however, most of them under-
perform or are on par with the baseline of simply
selecting the leading sentences in the document as
summaries (but see Paulus et al. 2017 and Celiky-
ilmaz et al. 2018 for exceptions).

Although seemingly more successful than their
abstractive counterparts, extractive models require
sentence-level labels, which are not included in
most summarization datasets (only document and
gold summary pairs are available). Sentence la-
bels are usually obtained by rule-based methods
(Cheng and Lapata, 2016) or by maximizing the
ROUGE score (Lin, 2004) between a subset of sen-
tences and the human written summaries (Nalla-
pati et al., 2017). These methods do not fully
exploit the human summaries, they only create
True/False labels which might be suboptimal.



780

Document
Encoder

sent1 sent2 sent3 sent4

Document
Decoder

Latent
Variables

1 0 1 0

sum sent1 sum sent2

Sentence
Encoder

sent3 = w1 w2 w3

MeanPool

Figure 1: Latent variable extractive summarization
model. senti is a sentence in a document and sum senti
is a sentence in a gold summary of the document.

In this paper we propose a latent variable ex-
tractive model and view labels of sentences in
a document as binary latent variables (i.e., ze-
ros and ones). Instead of maximizing the likeli-
hood of “gold” standard labels, the latent model
directly maximizes the likelihood of human sum-
maries given selected sentences. Experiments on
the CNN/Dailymail dataset (Hermann et al., 2015)
show that our latent extractive model improves
upon a strong extractive baseline trained on rule-
based labels and also performs competitively to
several recent models.

2 Model

We first introduce the neural extractive summa-
rization model upon which our latent model is
based on. We then describe a sentence compres-
sion model which is used in our latent model and
finally move on to present the latent model itself.

2.1 Neural Extractive Summarization

In extractive summarization, a subset of sentences
in a document is selected as its summary. We
model this problem as an instance of sequence la-
beling. Specifically, a document is viewed as a se-
quence of sentences and the model is expected to
predict a True or False label for each sentence,
where True indicates that the sentence should be
included in the summary. It is assumed that during
training sentences and their labels in each docu-

ment are given (methods for obtaining these labels
are discussed in Section 3).

As shown in the lower part of Figure 1, our
extractive model has three parts: a sentence en-
coder to convert each sentence into a vector, a
document encoder to learn sentence representa-
tions given surrounding sentences as context, and
a document decoder to predict sentence labels
based on representations learned by the document
encoder. Let D = (S1, S2, . . . , S|D|) denote a
document and Si = (wi1, w

i
2, . . . , w

i
|Si|) a sen-

tence in D (where wij is a word in Si). Let
Y = (y1, . . . , y|D|) denote sentence labels. The
sentence encoder first transforms Si into a list
of hidden states (hi1,h

i
2, . . . ,h

i
|Si|) using a Bidi-

rectional Long Short-Term Memory Network (Bi-
LSTM; Hochreiter and Schmidhuber 1997; Schus-
ter and Paliwal 1997). Then, the sentence encoder
yields vi, the representation of Si, by averaging
these hidden states (also see Figure 1):

vi =
1

|Si|
∑
j

hij (1)

In analogy to the sentence encoder, the doc-
ument encoder is another Bi-LSTM but applies
on the sentence level. After running the Bi-
LSTM on a sequence of sentence representations
(v1,v2, . . . ,v|D|), we obtain context dependent
sentence representations (hE1 ,h

E
2 , . . . ,h

E
|D|).

The document decoder is also an LSTM which
predicts sentence labels. At each time step, it
takes the context dependent sentence representa-
tion of Si produced by the document encoder as
well as the prediction in the previous time step:

hDi = LSTM(h
D
i−1,

[
We e(yi−1)

hEi

]
) (2)

where We ∈ Rd×2 is the label embedding ma-
trix (d is the hidden dimension for the document
decoder LSTM) and yi−1 is the prediction at time
step i−1; the predicted label distribution for yi is:

p(yi|y1:i−1,hDi−1) = softmax(Wo hDi ) (3)

where Wo ∈ R2×d.
The model described above is usually trained

by minimizing the negative log-likelihood of sen-
tence labels in training documents; it is almost
identical to Cheng and Lapata (2016) except that



781

we use a word-level long short-term memory net-
work coupled with mean pooling to learn sentence
representations, while they use convolutional neu-
ral network coupled with max pooling (Kim et al.,
2016).

2.2 Sentence Compression
We train a sentence compression model to map a
sentence selected by the extractive model to a sen-
tence in the summary. The model can be used to
evaluate the quality of a selected sentence with re-
spect to the summary (i.e., the degree to which it is
similar) or rewrite an extracted sentence according
to the style of the summary.

For our compression model we adopt a stan-
dard attention-based sequence-to-sequence archi-
tecture (Bahdanau et al., 2015; Rush et al., 2015).
The training set for this model is generated from
the same summarization dataset used to train the
exractive model. Let D = (S1, S2, . . . , S|D|) de-
note a document and H = (H1, H2, . . . ,H|H|)
its summary. We view each sentence Hi in the
summary as a target sentence and assume that
its corresponding source is a sentence in D most
similar to it. We measure the similarity between
source sentences and candidate targets using
ROUGE, i.e., Sj = argmaxSj ROUGE(Sj , Hi)
and 〈Sj , Hi〉 is a training instance for the compres-
sion model. The probability of a sentence Ĥi be-
ing the compression of Ŝj (i.e., ps2s(Ĥi|Ŝj)) can
be estimated with a trained compression model.

2.3 Latent Extractive Summarization
Training the extractive model described in Sec-
tion 2.1 requires sentence-level labels which are
obtained heuristically (Cheng and Lapata, 2016;
Nallapati et al., 2017). Our latent variable model
views sentences in a document as binary variables
(i.e., zeros and ones) and uses sentences with acti-
vated latent variables (i.e., ones) to infer gold sum-
maries. The latent variables are predicted with
an extractive model and the loss during training
comes from gold summaries directly.

Let D = (S1, S2, . . . , S|D|) denote a document
and H = (H1, H2, . . . ,H|H|) its human summary
(Hk is a sentence inH). We assume that there is a
latent variable zi ∈ {0, 1} for each sentence Si in-
dicating whether Si should be selected, and zi = 1
entails it should. We use the extractive model
from Section 2.1 to produce probability distribu-
tions for latent variables (see Equation (3)) and ob-
tain them by sampling zi ∼ p(zi|z1:i−1,hDi−1) (see

Figure 1). C = {Si|zi = 1}, the set of sentences
whose latent variables equal to one, are our current
extractive summaries. Without loss of generality,
we denote C = (C1, . . . , C|C|). Then, we estimate
how likely it is to infer the human summary H
from C. We estimate the likelihood of summary
sentence Hl given document sentence Ck with the
compression model introduced in Section 2.2 and
calculate the normalized1 probability skl:

skl = exp

(
1

|Hl|
log ps2s(Hl|Ck)

)
(4)

The score Rp measures the extent to which H can
be inferred from C:

Rp(C,H) =
1

|C|

|C|∑
k=1

|H|
max
l=1

skl (5)

For simplicity, we assume one document sentence
can only find one summary sentence to explain it.
Therefore, for all Hl, we only retain the most ev-
ident skl. Rp(C,H) can be viewed as the “preci-
sion” of document sentences with regard to sum-
mary sentences. Analogously, we also define Rr,
which indicates the extent to whichH can be cov-
ered by C:

Rr(C,H) =
1

|H|

|H|∑
l=1

|C|
max
k=1

skl (6)

Rr(C,H) can be viewed as the “recall” of docu-
ment sentences with regard to summary sentences.
The final scoreR(C,H) is the weighted sum of the
two:

R(C,H) = αRp(C,H) + (1− α)Rr(C,H) (7)

Our use of the terms “precision” and “recall” is
reminiscent of relevance and coverage in other
summarization work (Carbonell and Goldstein,
1998; Lin and Bilmes, 2010; See et al., 2017).

We train the model by minimizing the negative
expected R(C,H):

L(θ) = −E(z1,...,z|D|)∼p(·|D)[R(C,H)] (8)

where p(·|D) is the distribution produced by the
neural extractive model (see Equation (3)). Unfor-
tunately, computing the expectation term is pro-
hibitive, since the possible latent variable combi-
nations are exponential. In practice, we approx-
imate this expectation with a single sample from

1We also experimented with unnormalized probabilities
(i.e., excluding the exp in Equation (4)), however we ob-
tained inferior results.



782

the distribution of p(·|D). We use the REIN-
FORCE algorithm (Williams, 1992) to approxi-
mate the gradient of L(θ):

∇L(θ) ≈∑|D|
i=1∇ log p(zi|z1:i−1,hDi−1)[R(C,H)− bi]

Note that the model described above can be
viewed as a reinforcement learning model, where
R(C,H) is the reward. To reduce the variance of
gradients, we also introduce a baseline linear re-
gression2 model bi (Ranzato et al., 2016) to es-
timate the expected value of R(C,H). To avoid
random label sequences during sampling, we use
a pre-trained extractive model to initialize our la-
tent model.

3 Experiments

Dataset and Evaluation We conducted exper-
iments on the CNN/Dailymail dataset (Hermann
et al., 2015; See et al., 2017). We followed
the same pre-processing steps as in See et al.
(2017). The resulting dataset contains 287,226
document-summary pairs for training, 13,368 for
validation and 11,490 for test. To create sen-
tence level labels, we used a strategy similar to
Nallapati et al. (2017). We label the subset of
sentences in a document that maximizes ROUGE
(against the human summary) as True and all
other sentences as False. Using the method de-
scribed in Section 2.2, we created a compression
dataset with 1,045,492 sentence pairs for training,
53,434 for validation and 43,382 for testing. We
evaluated our models using full length F1 ROUGE
(Lin, 2004) and the official ROUGE-1.5.5.pl
script. We report ROUGE-1, ROUGE-2, and
ROUGE-L.

Implementation We trained our extractive
model on an Nvidia K80 GPU card with a batch
size of 32. Model parameters were uniformly ini-
tialized to [− 1√

c
, 1√

c
] (c is the number of columns

in a weight matrix). We used Adam (Kingma and
Ba, 2014) to optimize our models with a learning
rate of 0.001, β1 = 0.9, and β2 = 0.999. We
trained our extractive model for 10 epochs and
selected the model with the highest ROUGE on
the validation set. We rescaled the gradient when
its norm exceeded 5 (Pascanu et al., 2013) and

2The linear regression model bt is trained by minimiz-
ing the mean squared error between the prediction of bt and
R(C,H).

Model R-1 R-2 R-L
LEAD3 40.34 17.70 36.57
LEAD3 (Nallapati et al., 2017) 39.20 15.70 35.50
abstract 35.46 13.30 32.65
pointer+coverage 39.53 17.28 36.38
abstract-RL 41.16 15.75 39.08
abstract-ML+RL 39.87 15.82 36.90
SummaRuNNer 39.60 16.20 35.30
EXTRACT-CNN 40.11 17.52 36.39
REFRESH (Narayan et al., 2018) 40.00 18.20 36.60
EXTRACT 40.62 18.45 37.14
LATENT 41.05 18.77 37.54
LATENT+COMPRESS 36.69 15.43 34.33

Table 1: Results of different models on the
CNN/Dailymail test set using full-length F1 ROUGE-1
(R-1), ROUGE-2 (R-2), and ROUGE-L (R-L).

regularized all LSTMs with a dropout rate of 0.3
(Srivastava et al., 2014; Zaremba et al., 2014). We
also applied word dropout (Iyyer et al., 2015) at
rate 0.2. We set the hidden unit size d = 300 for
both word-level and sentence-level LSTMs and all
LSTMs had one layer. We used 300 dimensional
pre-trained FastText vectors (Joulin et al., 2017)
to initialize our word embeddings. The latent
model was initialized from the extractive model
(thus both models have the same size) and we
set the weight in Equation (7) to α = 0.5. The
latent model was trained with SGD, with learning
rate 0.01 for 5 epochs. During inference, for both
extractive and latent models, we rank sentences
with p(yi = True|y1:i−1,D) and select the top
three as summary (see also Equation (3)).

Comparison Systems We compared our model
against LEAD3, which selects the first three lead-
ing sentences in a document as the summary
and a variety of abstractive and extractive mod-
els. Abstractive models include a sequence-to-
sequence architecture (Nallapati et al., 2016); ab-
stract), its pointer generator variant (See et al.
2017; pointer+coverage), and two reinforce-
ment learning-based models (Paulus et al. 2017;
abstract-RL and abstract-ML+RL). We also com-
pared our approach against an extractive model
based on hierarchical recurrent neural networks
(Nallapati et al. 2017; SummaRuNNer), the model
described in Section 2.1 (EXTRACT) which en-
codes sentences using LSTMs, a variant which
employs CNNs instead (Cheng and Lapata 2016;
EXTRACT-CNN), as well as a similar system based
on reinforcement learning (Narayan et al. 2018;
REFRESH).



783

Results As shown in Table 1, EXTRACT, our
extractive model outperforms LEAD3 by a wide
margin. EXTRACT also outperforms previously
published extractive models (i.e., SummaRuNNer,
EXTRACT-CNN, and REFRESH). However, note
that SummaRuNNer generates anonymized sum-
maries (Nallapati et al., 2017) while our models
generate non-anonymized ones, and therefore the
results of EXTRACT and SummaRuNNer are not
strictly comparable (also note that LEAD3 results
are different in Table 1). Nevertheless, EXTRACT
exceeds LEAD3 by +0.75 ROUGE-2 points
and +0.57 in terms of ROUGE-L, while Sum-
maRuNNer exceeds LEAD3 by +0.50 ROUGE-
2 points and is worse by −0.20 points in terms
of ROUGE-L. We thus conclude that EXTRACT
is better when evaluated with ROUGE-2 and
ROUGE-L. EXTRACT outperforms all abstractive
models except for abstract-RL. ROUGE-2 is lower
for abstract-RL which is more competitive when
evaluated against ROUGE-1 and ROUGE-L.

Our latent variable model (LATENT; Section
2.3) outperforms EXTRACT, despite being a strong
baseline, which indicates that training with a loss
directly based on gold summaries is useful. Dif-
ferences among LEAD3, EXTRACT, and LATENT
are all significant with a 0.95 confidence inter-
val (estimated with the ROUGE script). Inter-
estingly, when applying the compression model
from Section 2.2 to the output of our latent model
( LATENT+COMPRESS ), performance drops con-
siderably. This may be because the compres-
sion model is a sentence level model and it re-
moves phrases that are important for creating the
document-level summaries.

4 Conclusions

We proposed a latent variable extractive summa-
rization model which leverages human summaries
directly with the help of a sentence compression
model. Experimental results show that the pro-
posed model can indeed improve over a strong ex-
tractive model while application of the compres-
sion model to the output of our extractive system
leads to inferior output. In the future, we plan to
explore ways to train compression models tailored
to our summarization task.

Acknowledgments

We thank the EMNLP reviewers for their valuable
feedback and Qingyu Zhou for preprocessing the

CNN/Dailymail dataset. We gratefully acknowl-
edge the financial support of the European Re-
search Council (award number 681760; Lapata).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In In Proceedings of
the 3rd International Conference on Learning Rep-
resentations, San Diego, California.

Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 335–336. ACM.

Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep communicating agents for
abstractive summarization. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 1662–1675, New Orleans, Louisiana.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 484–494, Berlin, Germany.

John M Conroy and Dianne P O’leary. 2001. Text sum-
marization via hidden markov models. In Proceed-
ings of the 24th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 406–407. ACM.

Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop, pages 104–111, Barcelona, Spain.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701. Curran Associates, Inc.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered com-
position rivals syntactic methods for text classifica-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), vol-
ume 1, pages 1681–1691, Beijing, China.



784

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Pa-
pers, pages 427–431, Valencia, Spain.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In In Proceedings of the 30th AAAI Con-
ference on Artificial Intelligence, pages 2741–2749,
Phoenix, Arizona.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 68–73. ACM.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain.

Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 912–920, Los Angeles, California.

Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Pub Co.

Rada Mihalcea. 2005. Language independent extrac-
tive summarization. In Proceedings of the ACL In-
teractive poster and demonstration sessions, pages
49–52, Ann Arbor, Michigan.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of doc-
uments. In In Proceedings of the 31st AAAI Con-
ference on Artificial Intelligence, pages 3075–3091,
San Francisco, California.

Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al. 2016. Abstractive text summa-
rization using sequence-to-sequence rnns and be-
yond. arXiv preprint arXiv:1602.06023.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 1747–1759, New Orleans, Louisiana.

Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2–3):103–233.

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the difficulty of training recurrent
neural networks. In Proceedings of the 28th Inter-
national Conference on Machine Learning, pages
1310–1318, Atlanta, Georgia.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

MarcAurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In Proceed-
ings of the 4th International Conference on Learn-
ing Representations, San Juan, Puerto Rico.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389, Lisbon, Portugal.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073–
1083, Vancouver, Canada.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of machine learning re-
search, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112. urran Associates,
Inc.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 565–574, Up-
psala, Sweden.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.


