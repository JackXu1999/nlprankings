



















































Polarity Computations in Flexible Categorial Grammar


Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 124–129
New Orleans, June 5-6, 2018. c©2018 Association for Computational Linguistics

Polarity Computations in Flexible Categorial Grammar

Hai Hu
Linguistics Department

Indiana University
Bloomington, IN 47405 USA
huhai@indiana.edu

Lawrence S. Moss
Mathematics Department

Indiana University
Bloomington, IN 47405 USA
lsm@cs.indiana.edu

Abstract

This paper shows how to take parse trees in
CCG and algorithmically find the polarities of
all the constituents. Our work uses the well-
known polarization principle corresponding to
function application, and we have extended
this with principles for type raising and com-
position. We provide an algorithm, extending
the polarity marking algorithm of van Ben-
them. We discuss how our system works in
practice, taking input from the C&C parser.

1 Introduction

The main goal of this work is to take input from
text and then to automatically determine the polar-
ity of all the words. For example, we aim to find
the arrows in sentences like Every dog↓ scares ↑ at
least two↓ cats↑, Every dog↓ and no cat↓ sleeps=,
and Most rabbits= hop↑. The ↑ notation means
that whenever we use the given sentence truth-
fully, if we replace the marked word w with an-
other word which is “≥ w,” then the resulting sen-
tence will still be true. So we have a semantic
inference. The ↓ notation means the same thing,
except that when we substitute using a word ≤ w,
we again preserve truth. Finally, the = notation
means that we have neither property in general; in
a valid semantic inference statement, we can only
replace the word with itself rather than with some-
thing larger or smaller.

For example, if we had a collection of back-
ground facts like cats ≤ animals, beagles ≤ dogs,
scares ≤ startles, and one ≤ two, then our ↑ and
↓ notations on Every dog↓ scares ↑ at least two↓

cats↑ would allow us to conclude Every beagle
startles at least one animal.

The goal of the paper is to provide a computa-
tional system to determine the notations ↑, ↓,= on
input text to the best extent possible, either using
hand-created parses, or output from a popular and

freely available CCG parser C&C (Clark and Cur-
ran, 2007).

Using our polarity tool, we get a very easy first
step on automatic inference done with little or no
representation. We discuss potential applications
to textual inference.

Theory We extend polarity determination for
categorial grammar (CG) (see Sánchez-Valencia
(1991); van Benthem (1986); van Eijck (2007);
Lavalle-Martı́nez et al. (2017)). These papers only
consider the Ajdukiewicz/Bar-Hillel (AB) flavor
of CG, where the rules are restricted to applica-
tion rules (>) and (<). There is a consensus that
application rules alone are too restrictive to give
wide-coverage grammars. We thus extend this
work to the full set of flexible combinators used
in CCG. We prove that our system is sound, in a
precise sense. Further, we show how to incorpo-
rate boolean reasoning (Keenan and Faltz, 1984)
to get a more complete system.

A working system We have implemented our
algorithm in Python. This implementation handles
sentences from the C&C parser (Clark and Curran,
2007). This is a non-trivial step on top of the the-
oretical advance because the parses delivered by
the C&C parser deviate in several respects from
the semantically-oriented input that one would like
for this kind of work.

2 An Ordered Syntax-semantics
Interface

The basis of the semantics is the syntax-semantics
interface in formal semantics, especially in CG
and CCG (Keenan and Faltz, 1984; Carpenter,
1998; Steedman, 2000; Jacobson, 2014).

Our syntax in this small paper will consist of the
lexicon shown in our examples. Here is an exam-

124



ple of a CCG derivation:

Fido : npr
Fido: s/(s\npr)

T
ch: (s\npr)/npr

Fido chased : s/npr
B

Felix : npr
Fido chased Felix : s

>

(1)
This tree is not the simplest one for Fido chased
Felix. We chose it to remind the reader of the CCG
rules of type-raising (T) and composition (B).

Let us fix a semantics. We first select the base
types e and t. We generate complex types from
these by using function types x → y. We adopt
a few standard abbreviations. We then fix a map
from the CG categories into the types. We choose
s 7→ t, n 7→ e→ t, npr 7→ e, np 7→ (e→ t)→ t,
etc. (We use npr for proper names.)

A model M is a set M together with interpre-
tations of all the lexical items by objects of the
appropriate semantic type. We use M as the se-
mantic space for the type e, 2 = {F,T} for type
t, and the full set of functions for higher types.
The interpretations of some words are fixed: deter-
miners, conjunctions and relative pronouns. The
model thus interprets intransitive verbs by (et, t)t,
and transitive verbs by (et, t)((et, t)t). By the Jus-
tification Theorem in Keenan and Faltz (1984), we
in fact may obtain these using simpler and more
natural data: for proper names we need only ob-
jects of type e, for intransitive verbs we need only
et, and for transitive verbs eet.

Let S be a sentence in our fragment, and let Π
be a parse tree for S. Associated to Π we have
a semantic parse tree, giving us a term tS in the
typed lambda calculus over the base types e and
t. This term may be interpreted in each modelM.
For example, the interpretation corresponding to
(1) is the boolean value in the model

(λx.x[[Fido]] ◦ [[chased]])[[Felix]].

Polarities ↑ and ↓ In order to say what the polar-
ity symbols mean, we need to enrich our semantic
spaces from sets to preorders (Moss, 2012; Icard
and Moss, 2014).

A preorder P = (P,≤) is a set P with a rela-
tion ≤ on P which is reflexive and transitive. Fix
a modelM. Then each type x gives rise to a pre-
order Px. We order Pt by F < T. For Pe we take
the flat preorder on the universe set M underlying
the model. For the higher types x → y, we take
the set (Px → Py) of all functions and endow it

with the pointwise order. In this way every one of
our semantic types is naturally endowed with the
structure of a preorder in every model.

A function f : P → Q is monotone (or order
preserving) if p ≤ q in P implies f(p) ≤ f(q) in
Q. And f is antitone (or order inverting) if p ≤ q
in P implies f(q) ≤ f(p) inQ.

Each sentence S in our fragment is now inter-
preted in an ordered setting. This is the (math-
ematical) meaning of our ↑ and ↓ arrows in this
paper. For example, when we write every dog↓

barks↑, this means: for all models M, all m1 ≤
m2 in Pet (for dog), and all n1 ≤ n2 in P(et)t
(for barks), we have in 2 that [[every]] m2 n1 ≤
[[every]] m1 n2.

Order-enriched types using +, −, and · Fol-
lowing Dowty (1994) we incorporate monotonic-
ity information into the types. Function types
x→ y split into three versions: the monotone ver-
sion x +→ y, the antitone version x −→ y, and the
full version x ·→ y. (What we wrote before as
x → y is now x ·→ y.) These are all preorders
using the pointwise order. We must replace all of
the ordinary slash types by versions of them which
have markings on them.

Lexicon with order-enriched types We use S
for t, N or et for e ·→ t = e +→ t, NP for N ·→ t,
NP+ for N +→ t, and NP− for N −→ t. Note that
we have a different font than our syntactic types s,
n, and np. Then we use NP +→ S for intransitive
verbs, NP+ or NP− for noun phrases with deter-
miners, e for proper names. For the determiners,
our lexicon then uses the order-enriched types in
different ways:

word type

every N −→ NP+
some N +→ NP+

word type

no N −→ NP−
most N ·→ NP+

3 Polarizing a Parse Tree

In this section, we specify the rules (see Fig-
ure 1) by which we put markings and polarities
on each node of a CCG parse tree, based on a
marked/order-enriched lexicon. The next section
discusses the algorithm.

Input A parse tree T in CCG as in (1), and a
marked lexicon.

Output We aim to convert T to a different tree
T ∗ satisfying the following properties: (1) The se-
mantic terms in T and T ∗ should denote the same

125



(x
m→ y)d xmd

yd
>

(x
m→ y)d (y n→ z)md

(x
mn−→ z)d

B
xmd

((x
m→ y) +→ y)d

T

(e→ x)=

(NP +→ x)=
I

(e→ x)d

(NP+ +→ x)d
J

(e→ x)flip d

(NP− +→ x)d
K

Figure 1: The top line contains core rules of marking and polarity. The letters m and n stand for one of
the markings +, −, or ·; d stands for ↑ or ↓ (but not =). In (I), (J), and (K), x must be a boolean category.
See charts in the text for the operations m, d 7→ md and m,n 7→ mn.

function in each model. (2) The lexical items in
T ∗ must receive their types from the typed lexi-
con. (3) The polarity of the root of T ∗ must be ↑.
(4) At each node in T ∗, one of the rules in our sys-
tem must be matched. Most of the rules are listed
in Figure 1.

Example For T in (1), T ∗ could be as in (2):
Fido↑ : e

Fido↑ : et +→ t
T

chased↑ : e +→ et
Fido chased↑ : e +→ t

B
Felix↑ : e

Fido chased Felix↑ : t
>

(2)
The signs + and − on the arrows are markings;

markings apply to arrows only. We have a third
marking, ·, but this does not figure into (2). Mark-
ings are used to tell if a function is interpreted (in
every model) by a function which is always mono-
tone (+), always antitone (−), or neither in general
(·). The arrows ↑ and ↓ are polarities. We also
have a third polarity, =. Polarities are for specific
occurrences.

Explanation of the operations on markings and
polarities Each rule in Figure 1 is actually a
number of other rules, and we have summarized
things in terms of several operations. The chart on
the left is for combining two markings m and n,
and the one on the right is for combining a mark-
ing m and a polarity d, obtaining a new polarity.

PPPPPm
n + − ·

+ + − ·
− − + ·
· · · ·

PPPPPd
m + − ·

↑ ↑ ↓ =
↓ ↓ ↑ =

flip ↑ = ↓ flip ↓ = ↑

Comments on the rules In Figure 1, x, y and z
are variables ranging over marked types.

The application rule (>) is essentially taken
from van Benthem (1986) (see also Lavalle-

Martı́nez et al. (2017) for a survey of related al-
gorithms); we expect that our logical system will
give rise to several algorithms.

To illustrate (>), let us take m = − and d = ↑.
We then have the (>) rule

(x
−→ y)↑ x↓
y↑

>

(3)

This means: for all preorders P and Q, all f, g :
P
−→ Q and all p1, p2 ∈ P , if f ≤ g and p2 ≤ p1,

then f(p1) ≤ g(p2).
If we were to change x↓ to x↑ in (3), we would

change our statement by replacing “p2 ≤ p1” with
“p1 ≤ p2”. If we changed it to x=, we would
use “p1 = p2”. In this way, we can read off a
large number of true facts about preorders from
our rules.

There are similar results concerning (B). Here
is an example of how (B) is used, taken from (2).
Fido has type NP+ = (et) +→ t, and chased above
it has type NP+ +→ (et). So the application of (B)
results in Fido chased with type NP+ +→ t.

The rules (I), (J), and (K) are new. In them, x
must be Boolean. That is, it must belong to the
smallest collection B containing t and with the
property that if z ∈ B, then (y ·→ z) ∈ B for all
y. B is thus the collection of types whose interpre-
tations are naturally endowed with the structure of
a complete atomic boolean algebra (Keenan and
Faltz, 1984). Indeed, the soundness of (J) and (K)
follows from the proof of the Justification Theo-
rem (op. cit).

Figure 2 contains two applications of the (K)
rules. First, the lexical entry for chased is e →
et. The first application of (K) promotes this to
NP− +→ et. The NP receives a − because its
argument no cat is of type NP−. Note that the
polarity flips when we do this. If we had used
(J), the promotion would be to NP+ +→ et, and

126



no dog↓

no dog↑ : NP−
>

ch↑ : e → et
ch↓ : NP− +→ et

K no cat
↑

no cat↓ : NP−
>

chased no cat↓ : e → t
>

chased no cat↑ : NP− +→ S
K

no dog chased no cat↑ : S
<

Figure 2: Two applications of the (K) rules.

there would be no polarity flipping. This would
be used in sentence where the object VP was some
cat or every cat. The second application promoted
chased no cat from the type et to NP− +→ S, again
with a polarity flip. If we had used (I), we would
have obtained NP +→ S. However, this would have
trivialized the polarity to =, and this effect would
have been propagated up the tree. Rule (I) would
be needed for the sentence most dogs chased no
cat.

Several rules are not shown including “back-
wards” versions of (>), (B), and (T), and also ver-
sions where all polarizations are =. This is a tech-
nical point that is not pertinent to this short ver-
sion. We should mention that due to these rules,
every tree may be polarized in a trivial way, by us-
ing = at all nodes. So we are really interested in
the maximally informative polarizations, the ones
that make the most predictions.

Boolean connectives, etc. We take and and or
to be polymorphic of the types B m→ (B m→ B),
when B is a Boolean category and m = +,−, or
·. Negation flips polarities. Relative pronouns and
relative clauses also can be handled. Adjectives
are taken to be N +→ N.

Other combinators This paper only discusses
(T) and (B), but we also have rules for the other
combinators used in CG, such as (S) and (W). For
example, the (S) combinator is defined by Sfg =
λx.(fx)(gx). In our system, the corresponding
polarization rule is

(x
m→ (y n→ z))d (x mn−→ y)nd

(x
m→ z)d

S

This combinator is part of the standard presenta-
tion of CCG, but it is less important in this paper
because the C&C parser does not deliver parses
using it.

4 Algorithmic Aspects

We have an algorithm1 that takes as input a CCG
tree as in (1) and outputs some tree with markings
and polarities, a tree which satisfies the conditions
that we have listed. The algorithm has two phases,
similar to van Benthem’s algorithm (van Benthem,
1986) for work with the Ajdukiewicz/Bar-Hillel
variant of CG (only application rules). Phase 1
goes down the tree from leaves to root and adds
the markings, based on the rules in Figure 1. The
markings on the leaves are given in the lexicon.
The rest of Phase 1 is non-deterministic. We can
see this from our set of rules: there are many cases
where one conclusion (on top of the line) permits
several possible conclusions. As we go down the
tree, we frequently need to postpone the choice.

Phase 2 of the algorithm computes the polar-
ities, again following the rules, starting with the
root. One always puts ↑ on the root, and then goes
up the tree. This part of the algorithm is straight-
forward.

The overall algorithm is in fact non-
deterministic for two reasons. As we explained,
Phase 1 has a non-deterministic feature. In addi-
tion, it is always possible to polarize everything
with = and make similar uninformative choices
for the markings. We are really interested in the
most informative polarization, the one with the
fewest number of = polarities.

Soundness We have proved a soundness theo-
rem for the system. Though too complicated to
state in full, it might be summarized informally,
as follows. Suppose we have a sentence S in En-
glish, and suppose that the lexical items in S are
given semantics that conform to our assumptions.
(This means that the semantics of the lexical en-
tries must belong to the appropriate types.) Then
any semantic statement about the ↑, ↓, = mark-
ing predicted by our system is correct. See Moss
(2018) for details.

Completeness We have not proven the com-
pleteness of our system/algorithm, and indeed this
is an open question. What completeness would
mean for a system like ours is that whenever we
have an input CCG parse tree and a polarization
of its words which is semantically valid in the
sense that it holds no matter how the nouns, verbs,
etc. are interpreted, then our algorithm would de-
tect this. This completeness would be a property

1https://github.com/huhailinguist/ccg2mono

127



of the rules and also of the polarization algorithm.
The experience with similar matters in Icard and
Moss (2013) suggests that completeness will be
difficult.

Efficiency of our algorithm Our polarization is
quite fast on the sentences which we have tried it
on. We conjecture that it is in polynomial time,
but the most obvious complexity upper bound to
the polarization problem is NP. The reason that the
complexity is not “obviously polynomial” is that
for each of the type raising steps in the input tree,
one has three choices of the raise. In more detail,
suppose that the input tree contains

x
(x→ y)→ y T

Then our three choices for marking are: (x +→
y)

+→ y, (x −→ y) +→ y, and (x ·→ y) +→ y.
Our implementation defers the choice until more
of the tree is marked. But prima facie, there are
an exponential number of choices. All of these re-
marks also apply to the applications of (I), (J), and
(K); these do not occur in the input tree, and the
algorithm must make a choice somehow. Thus we
do not know the worst-case complexity of our al-
gorithm.

5 What Our System Can Currently Do

We tokenized input sentences using the script from
the ccg2lambda system (Martı́nez-Gómez et al.,
2016). The tokenized sentences were then parsed
using the C&C parser (Clark and Curran, 2007),
which is trained on the CCGbank (Hockenmaier
and Steedman, 2007). Then we run our algorithm.

We are able to take simple sentences all the way
through. For example, our system correctly deter-
mines the polarities in

No↑ man↓ walks↓

Every↑ man↓ and↑ some↑ woman↑ sleeps↑

Every↑ man↓ and↑ no↑ woman↓ sleeps=

If↑ some↓ man↓ walks↓, then↑ no↑ woman↓ runs↓

Every↑ man↓ does↓ n’t↑ hit↓ every↓ dog↑

No↑ man↓ that↓ likes↓ every↓ dog↑ sleeps↓

Most↑ men= that= every= woman= hits= cried↑

Every↑ young↓ man↓ that↑ no↑ young↓ woman↓

hits↑ cried↑

As shown, our algorithm polarizes all words in the
input. For determiners, this actually is useful. It
is (arguably) background knowledge, for example

that every ≤ some; at least two ≤ at least one ≡
some, no ≤ at most one ≤ at most two, etc. These
would not be part of the algorithm in this paper,
but rather they would be background facts that fig-
ure into inference engines built on this work.

Problems Our end-to-end system is sound in
the sense that it polarizes the correctly input se-
mantic representations. However, it is limited by
the quality of the parses coming from the C&C
parser. While the parser has advantages, its out-
put is sometimes not the optimal for our purposes.
For example, it will assign the supertag N/N to
most, but NP/N to other quantifiers. Thus in or-
der to handle most, one has to manually change
the parse trees. It also parses relative clauses as
(no dog) (who chased a cat) died rather than (no
(dog who chased a cat)) died. Furthermore, the
parser sometimes behaves differently on intransi-
tive verbs likes walks than on cries. Currently,
we manually fix the trees when they systemati-
cally deviate from our desired parses (e.g. relative
clauses). Finally, as with any syntactic parser, it
only delivers one parse. So ambiguous sentences
are not treated in any way by our work.

6 Future Work: Inference, and
Connections with Other Approaches

We certainly plan to use the algorithm in connec-
tion with inference, since this has always been the
a primary reason to study monotonicity and polar-
ity. Indeed, once one has correct polarity mark-
ings, it is straightforward to use those to do in-
ference from any background facts which can be
expressed as inequalities. This would cover taxo-
nomic statements like dog≤ animal and also pred-
ications like John isa swimmer. Our future work
will show logical systems built this way.

Connections This paper invites connections to
other work in the area, especially MacCartney and
Manning (2009) and Nairn et al. (2006), which
shared similar aims as ours, but were not done in
the CCG context. We also think of work on au-
tomatic discovery of downward-entailments (Che-
ung and Penn, 2012; Danescu et al., 2009), and
other work on natural logic (Fyodorov et al., 2003;
Zamansky et al., 2006; Moss, 2015; Abzianidze,
2017). Additionally, our work could be incorpo-
rated in several ways into textual entailment sys-
tems (e.g. Dagan et al., 2013).

128



References
Lasha Abzianidze. 2017. Langpro: Natural language

theorem prover. CoRR, abs/1708.09417.

Johan van Benthem. 1986. Essays in Logical Seman-
tics. Reidel, Dordrecht.

Bob Carpenter. 1998. Type-Logical Semantics. MIT
Press.

Jackie Cheung and Gerald Penn. 2012. Unsupervised
detection of downward-entailing operators by maxi-
mizing classification certainty. In Proc. 13th EACL,
pages 696–705.

Stephen Clark and James R Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. Computational Linguistics,
33(4):493–552.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing Textual Entail-
ment. Synthesis Lectures on Human Language
Technologies. Morgan & Claypool Publishers.

Cristian Danescu, Lillian Lee, and Richard Ducott.
2009. Without a ‘doubt’? Unsupervised discovery
of downward-entailing operators. In Proceedings of
NAACL HLT.

David Dowty. 1994. The role of negative polarity
and concord marking in natural language reasoning.
In Proceedings of Semantics and Linguistic Theory
(SALT) IV.

Jan van Eijck. 2007. Natural logic for natural language.
In Logic, Language, and Computation, volume 4363
of LNAI, pages 216–230. Springer-Verlag.

Yaroslav Fyodorov, Yoad Winter, and Nissim Fyo-
dorov. 2003. Order-based inference in natural logic.
Log. J. IGPL, 11(4):385–417. Inference in compu-
tational semantics: the Dagstuhl Workshop 2000.

Julia Hockenmaier and Mark Steedman. 2007. Ccg-
bank: a corpus of ccg derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.

Thomas F. Icard and Lawrence S. Moss. 2013. A
complete calculus of monotone and antitone higher-
order functions. In Proceedings, TACL 2013, vol-
ume 23 of EPiC Series, pages 96–99. Vanderbilt
University.

Thomas F. Icard and Lawrence S. Moss. 2014. Recent
progress on monotonicity. Linguistic Issues in Lan-
guage Technology, 9(7):167–194.

Pauline Jacobson. 2014. An Introduction to the Syn-
tax/Semantics Interface. Oxford University Press.

Edward L. Keenan and Leonard M. Faltz. 1984.
Boolean Semantics for Natural Language. Springer.

José-de-Jesús Lavalle-Martı́nez, Manuel Montes-y
Gómez, Luis Villaseñor-Pineda, Héctor Jiménez-
Salazar, and Ismael-Everardo Bárcenas-Patiño.
2017. Equivalences among polarity algorithms.
Studia Logica.

Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In IWCS-8,
Proceedings of the Eighth International Conference
on Computational Semantics, pages 140–156.

Pascual Martı́nez-Gómez, Koji Mineshima, Yusuke
Miyao, and Daisuke Bekki. 2016. ccg2lambda: A
compositional semantics system. In Proceedings
of ACL 2016 System Demonstrations, pages 85–
90, Berlin, Germany. Association for Computational
Linguistics.

Lawrence S. Moss. 2012. The soundness of internal-
ized polarity marking. Studia Logica, 100(4):683–
704.

Lawrence S. Moss. 2015. Natural logic. In Handbook
of Contemporary Semantic Theory, Second Edition,
pages 646–681. Wiley-Blackwell.

Lawrence S. Moss. 2018. Foundations of polarity de-
termination for flexible categorial grammars. Un-
published ms.

Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5 (Inference in Com-
putational Semantics), Buxton, UK.

Victor Sánchez-Valencia. 1991. Studies on Natural
Logic and Categorial Grammar. Ph.D. thesis, Uni-
versiteit van Amsterdam.

Mark Steedman. 2000. The Syntactic Process. MIT
Press.

Anna Zamansky, Nissim Francez, and Yoad Winter.
2006. A ‘natural logic’ inference system using the
Lambek calculus. Journal of Logic, Language, and
Information, 15(3):273–295.

129


