



















































Coherence Modeling Improves Implicit Discourse Relation Recognition


Proceedings of the SIGDIAL 2018 Conference, pages 344–349,
Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics

344

Coherence Modeling Improves Implicit Discourse Relation Recognition

Noriki Nishida and Hideki Nakayama
Graduate School of Information Science and Technology

The University of Tokyo
{nishida,nakayama}@nlab.ci.i.u-tokyo.ac.jp

Abstract

The research described in this paper exam-
ines how to learn linguistic knowledge as-
sociated with discourse relations from un-
labeled corpora. We introduce an unsu-
pervised learning method on text coher-
ence that could produce numerical repre-
sentations that improve implicit discourse
relation recognition in a semi-supervised
manner. We also empirically examine two
variants of coherence modeling: order-
oriented and topic-oriented negative sam-
pling, showing that, of the two, topic-
oriented negative sampling tends to be
more effective.

1 Introduction

Shallow discourse parsing aims to automatically
identify discourse relations (e.g., comparisons) be-
tween adjacent sentences. When connectives such
as however explicitly appear, discourse relations
are relatively easy to classify, as connectives pro-
vide strong cues (Pitler et al., 2008). In contrast, it
remains challenging to identify discourse relations
across sentences that have no connectives.

One reason for this inferior performance is a
shortage of labeled instances, despite the diversity
of natural language discourses. Collecting anno-
tations about implicit relations is highly expensive
because it requires linguistic expertise. 1 A variety
of semi-supervised or unsupervised methods have
been explored to alleviate this issue. Marcu and
Echihabi (2002) proposed generating synthetic in-
stances by removing connectives from sentence
pairs. This idea has been extended in many works

1The Penn Discourse Treebank (PDTB) 2.0 cor-
pus (Prasad et al., 2008), which is the current largest corpus
for discourse relation recognition, contains only about 16K
annotated instances in total.

and remains a core approach in the field (Zhou
et al., 2010; Patterson and Kehler, 2013; Lan et al.,
2013; Rutherford and Xue, 2015; Ji et al., 2015;
Liu et al., 2016; Braud and Denis, 2016; Lan et al.,
2017; Wu et al., 2017). However, these meth-
ods rely on automatically detecting connectives in
unlabeled corpora beforehand, which makes it al-
most impossible to utilize parts of unlabeled cor-
pora in which no connectives appear.2 In addi-
tion, as Sporleder and Lascarides (2008) discov-
ered, it is difficult to obtain a generalized model
by training on synthetic data due to domain shifts.
Though several semi-supervised methods do not
depend on detecting connectives (Hernault et al.,
2010, 2011; Braud and Denis, 2015), these meth-
ods are restricted to manually selected features,
linear models, or word-level knowledge transfer.

In this paper, our research question is how to ex-
ploit unlabeled corpora without explicitly detect-
ing connectives to learn linguistic knowledge as-
sociated with implicit discourse relations.

Our core hypothesis is that unsupervised learn-
ing about text coherence could produce numerical
representations related to discourse relations. Sen-
tences that compose a coherent document should
be connected with syntactic or semantic rela-
tions (Hobbs, 1985; Grosz et al., 1995). In partic-
ular, we expect that there should be latent relations
among local sentences. In this study, we hypoth-
esize that parameters learned through coherence
modeling could contain useful information for
identifying (implicit) discourse relations. To ver-
ify this hypothesis, we develop a semi-supervised
system whose parameters are first optimized for
coherence modeling and then transferred to im-
plicit discourse relation recognition. We also em-
pirically examine two variants of coherence mod-

2For example, nearly half of the sentences in the British
National Corpus hold implicit discourse relations and do not
contain connectives (Sporleder and Lascarides, 2008).



345

Figure 1: An example of order-oriented and topic-oriented negative sampling in coherence modeling.

eling: (1) order-oriented negative sampling and
(2) topic-oriented negative sampling. An example
is shown in Figure 1.

Our experimental results demonstrate that co-
herence modeling improves Macro F1 on implicit
discourse relation recognition by about 3 points on
first-level relation classes and by about 5 points
on second-level relation types. Coherence model-
ing is particularly effective for relation categories
with few labeled instances, such as temporal re-
lations. In addition, we find that topic-oriented
negative sampling tends to be more effective than
the order-oriented counterpart, especially on first-
level relation classes.

2 Coherence Modeling

In this study, we adopt the sliding-window ap-
proach of Li and Hovy (2014) to form a con-
ditional probability that a document is coherent.
That is, we define the probability that a given doc-
ument X is coherent as a product of probabilities
at all possible local windows, i.e.,

P (coherent|X, θ) =
∏
x∈X

P (coherent|x, θ), (1)

where P (coherent|x, θ) denotes the conditional
probability that the local clique x is coherent
and θ denotes parameters. Clique x is a tuple
of a central sentence and its left and right sen-
tences, (s−, s, s+). Though larger window sizes
may allow the model to learn linguistic properties
and inter-sentence dependencies over broader con-
texts, it increases computational complexity dur-
ing training and suffers from data sparsity prob-
lem.

We automatically build a dataset D = P ∪ N
for coherence modeling from an unlabeled corpus.
Here,P andN denote sets of positive and negative
instances, respectively. Given a source corpus C of

|C| sentences s1, s2, . . . , s|C|, we collect positive
instances as follows:

P = {(si−1, si, si+1) | i = 2, . . . , |C| − 1}. (2)

Text coherence can be corrupted by two aspects,
which correspond to how to build negative set N .

The first variant is order-oriented negative sam-
pling, i.e.,

N = {x′ | x′ ∈ φ(x) ∧ x ∈ P} (3)

where φ(x) denotes the set of possible permuta-
tions of x, excluding x itself.

The second variant is topic-oriented negative
sampling, i.e.,

N = {(s−, s′, s+) | s′ ∈ C ∧ (s−, s, s+) ∈ P}
(4)

where s′ denotes a sentence randomly sampled
from a uniform distribution over the entire cor-
pus C. We call this method topic-oriented be-
cause topic consistency shared across a clique
(s−, s, s+) is expected to be corrupted by replac-
ing s with s′.

3 Model Architecture

We develop a simple semi-supervised model with
neural networks. An overall view is shown in Fig-
ure 2. Our model mainly consists of three com-
ponents: sentence encoder E, coherence classi-
fier Fc, and implicit discourse relation classifier
Fr. The parameters of E are shared across the
two tasks: coherence modeling and implicit dis-
course relation recognition. In contrast, Fc and Fr
are optimized separately. Though it is possible to
develop more complex architectures (such as with
word-level matching (Chen et al., 2016), a soft-
attention mechanism (Liu and Li, 2016; Rönnqvist
et al., 2017), or highway connections (Qin et al.,



346

1st-Level Relation Classes 2nd-Level Relation Types Coherence
Acc. (%) Macro F1 (%) Acc. (%) Macro F1 (%) Acc. (%)

IRel only 51.49 42.29 37.49 24.81 N/A
IRel + O-Coh (Small) 52.16 41.39 37.77 25.46 57.96
IRel + O-Coh (Large) 52.29 42.48 41.29 30.70 64.24
IRel + T-Coh (Small) 51.70 40.84 37.91 25.35 83.04
IRel + T-Coh (Large) 53.54 45.03 41.39 29.67 91.53

Table 1: The results of implicit discourse relation recognition (multi-class classification) and coher-
ence modeling (binary classification). IRel and O/T-Coh denote that the model is trained on implicit
discourse relation recognition and order/topic-oriented coherence modeling respectively. “Small” and
“large” correspond to the relative size of the used unlabeled corpus: 37K (WSJ) and 22M (BLLIP)
positive instances, respectively.

(unsupervised learning) (supervised learning)
discourse relation recognitioncoherence modeling

Figure 2: The semi-supervised system we devel-
oped. The model consists of sentence encoder E,
coherence classifier Fc, and implicit discourse re-
lation classifier Fr.

2016)), such architectures are outside the scope of
this study, since the effectiveness of incorporating
coherence-based knowledge would be broadly or-
thogonal to the model’s complexity.

3.1 Sentence Encoder

Sentence encoder E transforms a symbol se-
quence (i.e., a sentence) into a continuous vector.
First, a bidirectional LSTM (BiLSTM) is applied
to a given sentence of n tokens w1, . . . , wn, i.e.,

−→
h i = FwdLSTM(

−→
h i−1, wi) ∈ RD, (5)

←−
h i = BwdLSTM(

←−
h i+1, wi) ∈ RD (6)

where FwdLSTM and BwdLSTM denote forward
and backward LSTMs, respectively. We initial-
ize the hidden states to zero vectors, i.e.,

−→
h 0 =←−

h n+1 = 0. In our preliminary experiments, we
tested conventional pooling functions (e.g., sum-
mation, average, or maximum pooling); we found
that the following concatenation tends to yield

higher performances:

h =
(−→
h>L ,
←−
h>1

)>
∈ R2D. (7)

We use Eq. 7 as the aggregation function through-
out our experiments.

3.2 Classifiers
We develop two multi-layer perceptrons (MLPs)
with ReLU nonlinearities followed by softmax
normalization each for Fc and Fr. The MLP in-
puts are the concatenation of sentence vectors.
Thus, the dimensionalities of the input layers are
2D×3 and 2D×2 respectively. The MLPs consist
of input, hidden, and output layers.

4 Experiments

4.1 Preparation
We used the Penn Discourse Treebank (PDTB) 2.0
corpus (Prasad et al., 2008) as a dataset for implicit
discourse relation recognition. We followed the
standard section partition, which is to use Sections
2–20 for training, Sections 0-1 for development,
and Sections 21–22 for testing. We evaluate multi-
class classifications with first-level relation classes
(four classes) and second-level relation types (11
classes).

We used the Wall Street Journal (WSJ) ar-
ticles (Marcus et al., 1993)3 or the BLLIP
North American News Text (Complete) (Mc-
Closky et al., 2008)4 to build a coherence model-
ing dataset, resulting in about 48K (WSJ) or 23M
(BLLIP) positive instances. We inserted a spe-
cial symbol “〈ARTICLE BOUNDARY〉” to each

3We used the raw texts in LDC99T42 Treebank-3:
https://catalog.ldc.upenn.edu/LDC99T42

4https://catalog.ldc.upenn.edu/LDC2008T13



347

Acc. (%) Macro F1 (%)
Rutherford and Xue (2015) 57.10 40.50
Liu et al. (2016) 57.27 44.98
Braud and Denis (2016)5 52.81 42.27
Wu et al. (2017) 58.85 44.84
IRel only 51.49 42.29
IRel only* 52.72 42.61
IRel + T-Coh (Large) 53.54 45.03
IRel + T-Coh (Large)* 56.60 46.90

Table 2: Comparison with previous works that
exploit unlabeled corpora on first-level relation
classes. An asterisk indicates that word embed-
dings are fine-tuned (which slightly decreases per-
formance on second-level relation types due to
overfitting).

Exp. Cont. Comp. Temp.
# of training data 6,673 3,235 1,855 582
IRel only 66.40 53.49 39.48 32.31
IRel + T-Coh 67.48 54.94 40.41 35.60

Table 3: Results on one-vs.-others binary clas-
sification in implicit discourse relation recogni-
tion. The evaluation metric is Macro F1 (%).
We evaluate on the first-level relation classes:
Expansion, Contingency, Comparison,
and Temporal.

article boundary. For the WSJ corpus, we split
the sections into training/development/test sets in
the same way with the implicit relation recogni-
tion. For the BLLIP corpus, we randomly sampled
10,000 articles each for the development and test
sets. Negative instances are generated following
the procedure described in Section 2. Note that
this procedure requires neither human annotation
nor special connective detection.

We set the dimensionalities of the word embed-
dings, hidden states of the BiLSTM, and hidden
layers of the MLPs to 100, 200, and 100, respec-
tively. GloVe (Pennington et al., 2014) was used
to produce pre-trained word embeddings on the
BLLIP corpus. To avoid overfitting, we fixed the
word embeddings during training in both coher-
ence modeling and implicit relation recognition.
Dropout (ratio 0.2) was applied to word embed-
dings and MLPs’s layers. At every iteration dur-
ing training in both tasks, we configured class-
balanced batches by resampling.

5The values are taken from Wu et al. (2017).

Figure 3: Results on implicit discourse rela-
tion recognition (first-level classes), with different
numbers of training instances. The error bars show
one standard deviation over 10 trials.

4.2 Results

To verify whether unsupervised learning on co-
herence modeling could improve implicit dis-
course relation recognition, we compared the
semi-supervised model (i.e., implicit discourse
relation recognition (IRel) + coherence model-
ing with order/topic-oriented negative sampling
(O/T-Coh)) with the baseline model (i.e., IRel
only). The evaluation metrics are accuracy (%)
and Macro F1 (%). We report the mean scores
over 10 trials. Table 1 shows that coherence mod-
eling improves Macro F1 by about 3 points in
first-level relation classes and by about 5 points
in second-level relation types. Coherence model-
ing also outperforms the baseline in accuracy. We
observed that the higher the coherence modeling
performance (see Small vs. Large), the higher the
implicit relation recognition score. These results
support our claim that coherence modeling could
learn linguistic knowledge that is useful for iden-
tifying discourse relations.

We also found that topic-oriented negative sam-
pling tends to outperform its order-oriented coun-
terpart, especially on first-level relation classes.
We suspect that this is because order-oriented co-
herence modeling is more fine-grained and chal-
lenging than topic-oriented identification, result-
ing in poor generalization. For example, there
could be order-invariant cliques that still hold co-
herence relations after random shuffling, whereas
topic-invariant cliques hardly exist. Indeed, train-
ing on order-oriented negative sampling con-
verged to lower scores than that of topic-oriented
negative sampling (see coherence accuracy).

Next, for reference, we compared our system
with previous work that exploits unlabeled cor-



348

pora. As shown in Table 2, we found our model to
outperform previous systems in Macro F1. In this
task, Macro F1 is more important than accuracy
because the class balance in the test set is highly
skewed. Note that these previous models rely on
previously detected connectives in the unlabeled
corpus, whereas our system is free from such de-
tection procedures.

To assess the effectiveness of coherence mod-
eling on different relation classes, we trained and
evaluated the models on one-vs-others binary clas-
sification. That is, we treated each of the first-level
relation classes (4 classes) as the positive class and
others as the negative class. Table 3 shows that
coherence modeling is effective, especially for the
Temporal relation which has relatively fewer la-
beled instances than others, indicating that coher-
ence modeling could compensate for the shortage
of labeled data.

We also performed an ablation study to dis-
cover the performance contribution from coher-
ence modeling by changing the number of train-
ing instances used in implicit relation recognition.
Here, we assume that in real-world situations, we
do not have sufficient labeled data. We downsam-
pled from the original training set and maintained
the balance of classes as much as possible. As
shown in Figure 3, coherence modeling robustly
yields improvements, even if we reduced the la-
beled instances to 10%.

5 Conclusion

In this paper, we showed that unsupervised learn-
ing on coherence modeling improves implicit dis-
course relation recognition in a semi-supervised
manner. Our approach does not require detecting
explicit connectives, which makes it possible to
exploit entire unlabeled corpora. We empirically
examined two variants of coherence modeling and
show that topic-oriented negative sampling tends
to be more effective than the order-oriented coun-
terpart on first-level relation classes.

It still remains unclear whether the coherence-
based knowledge is complemental to those by pre-
vious work. It is also interesting to qualitatively
inspect the differences of learned properties be-
tween order-oriented and topic-oriented negative
sampling. We will examine this line of research in
future.

Acknowledgments

The authors would like to thank anonymous re-
viewers for their constructive and helpful sugges-
tions on this work. This work was supported by
JSPS KAKENHI Grant Number 16H05872.

References
Chloé Braud and Pascal Denis. 2015. Comparing word

representations for implicit discourse relation classi-
fication. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2015).

Chloé Braud and Pascal Denis. 2016. Learning
connective-based word representations for implicit
discourse relation identification. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2016).

Jifan Chen, Qi Zhang, Pengfei Liu, and Xuanjing
Huang. 2016. Discourse relations detection via a
mixed generative-discriminative framework. In Pro-
ceedings of the 30th Conference on Artificial Intelli-
gence (AAAI 2016).

Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for modelling
the local coherence of discourse. Computational
Linguistis, 21(2):203–225.

Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2010).

Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Semi-supervised discourse relation
classification with structure learning. In Proceed-
ings of the 12th International Conference on Com-
putational Linguistics and Intelligent Text Process-
ing (CICLing 2011).

Jerry R. Hobbs. 1985. On the coherence and structure
of discourse. Technical Report CSLI-85-37, Center
for the Study of Language and Information (CSLI),
Stanford University.

Yangfeng Ji, Gongbo Zhang, and Jacob Eisenstein.
2015. Closing the gap: domain adaptation from ex-
plicit to implicit discourse relations. In Proceedings
of the 53st Annual Meeting of the Association for
Computational Linguistics (ACL 2015).

Man Lan, Jianxiang Wang, Yuanbin Wu, Zheng-yu
Niu, and Haifeng Wang. 2017. Multi-task attention-
based neural networks for implicit discourse re-
lationship representation and identification. In
Proceedings of the 2017 Conference of Empirical
Methods in Natural Language Processing (EMNLP
2017).



349

Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging
synthetic discourse data via multi-task learning for
implicit discourse relation recognition. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL 2013).

Jiwei Li and Eduard Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2014).

Yang Liu and Sujian Li. 2016. Recognizing implicit
discourse relations via repeated reading: neural net-
works with multi-level attention. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2016).

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang
Sui. 2016. Implicit discourse relation classification
via multi-task neural networks. In Proceedings of
the 30th Conference on Artificial Intelligence (AAAI
2016).

Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002).

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.

David McClosky, Eugene Charniak, and Mark John-
son. 2008. Bllip north american news text, com-
plete. Linguistic Data Consortium.

Gary Patterson and Andrew Kehler. 2013. Predicting
the presence of discourse connectives. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2013).

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representations. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2014).

Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily
identifiable discourse relations. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING 2008).

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0.
In Proceedings of the Sixth International Confer-
ence on Language Resources and Evaluation (LREC
2008).

Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016. A
stacking gated neural architecture for implicit dis-
course relation classification. In Proceedings of the

2016 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2016).

Samuel Rönnqvist, Niko Schenk, and Christian Chiar-
cos. 2017. A recurrent neural model with attention
for the recognition of chinese implicit discourse re-
lations. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2017).

Attapol T. Rutherford and Nianwen Xue. 2015. Im-
proving the inference of implicit discourse relations
via classifying explicit discourse connectives. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2015).

Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: an assessment. Natural Language En-
gineering, 14(03).

Changxing Wu, Xiaodong Shi, Yidong Chen, Jinsong
Su, and Boli Wang. 2017. Improving implicit dis-
course relation recognition with discourse-specific
word embeddings. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2017).

Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recog-
nition. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING
2010).


