



















































Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 527–533
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Identifying and Tracking Sentiments and Topics from Social Media Texts
during Natural Disasters

Min Yang1,4, Jincheng Mei2, Heng Ji3, Wei Zhao4, Zhou Zhao5, Xiaojun Chen1
∗

1 Shenzhen University 2 University of Alberta
3 Rensselaer Polytechnic Institute 4 Tencent AI Lab

5 Zhejiang University

Abstract

We study the problem of identifying the
topics and sentiments and tracking their
shifts from social media texts in dif-
ferent geographical regions during emer-
gencies and disasters. We propose a
location-based dynamic sentiment-topic
model (LDST) which can jointly model
topic, sentiment, time and Geolocation
information. The experimental results
demonstrate that LDST performs very
well at discovering topics and sentiments
from social media and tracking their shifts
in different geographical regions during
emergencies and disasters1.

1 Introduction

Social media has become pervasive in our daily
life, and it is a great way to spread important in-
formation efficiently. Using social media (e.g.,
Twitter, Facebook, Pinterest), people can conve-
niently inform others and express support during
emergencies and disasters.

Nowadays, the social media is keeping produc-
ing a huge amount of information. Unlike before,
people are not only interested in identifying the
static topics and sentiments from given texts, but
also, perhaps more concerned with tracking the
evolution of topics and sentiments among differ-
ent geographical regions. On the one hand, this
new requirement can be very helpful, especially
in the case of emergencies, such as pre-disaster
preparation and post-disaster relief in local natural
disasters (Beigi et al., 2016). For example, peo-
ple’s sentiments on the topics related to medical
and rescue may guide the management and distri-
bution of emergency supplies. On the other hand,

∗ Corresponding author (xjchen@szu.edu.cn)
1Data and codes are available at https://goo.gl/uee3QK

the existing models do not take the temporal evo-
lution and the impact of location over topics and
sentiments into consideration, which makes them
unable to fulfill the new requirement of tracking
the evolution of topics and sentiments in different
geographical places.

In this paper, we aim to identify the topics and
sentiments and track their shifts in different ge-
ographical regions during emergencies and dis-
asters. We are inspired by several observations.
First, people are interested in not only the overall
sentiment or topic distribution of the documents
but also the sentiments towards specific topics. For
example, a person may be happy with that the dis-
aster passed away, but at the meanwhile he/she
may be unsatisfied with the post-disaster relief.
Second, most existing sentiment-topic models ig-
nore the temporal evolution of topics and senti-
ment in a time-variant data corpus such as the
Twitter stream. There are strong evidences which
indicate that people’s attitudes toward a disaster
will gradually change over time with the distri-
bution of emergency supplies (Beigi et al., 2016;
Caragea et al., 2014; Mandel et al., 2012). Third,
people in different places tend to have different
opinions towards particular topics. This motivated
us to find the influence of specific topics and the
relationship between different topics in different
regions, which may improve people’s awareness
to help themselves during disasters.

We propose a location-based dynamic
sentiment-topic model (LDST) which gener-
alizes latent Dirichlet allocation (LDA) (Blei
et al., 2003), by jointly modeling topic, sentiment,
time and geolocation information. After learning
the LDST model, we can identify the topics and
sentiments held by people in different locations
over time. Our model works in an unsupervised
way, and we learn the model according to the
frequency of terms co-occurring in different

527



contexts. To leverage the prior knowledge, we
construct a small set of seed words for each
topic of interest to enable the model to group
semantically related terms into the same topic.
Consequently, the topic words will be more
related to the seed words of the same topic.

We conduct experiments using a Hurricane
Sandy Twitter corpus which consists of 159,880
geotagged Twitter posts from the geographic area
and time period of the 2012 Hurricane Sandy. We
show the evolution of people’s topics and senti-
ments, which change according to not only the
time the disaster happens, but also people’s loca-
tions during the hurricane Sandy.

2 Related work

Sentiment analysis is widely applied in many
fields, such as business intelligence, politics, so-
ciology. The papers by Pang and Lee (Pang and
Lee, 2008) and Liu (Liu, 2012) described most of
the existing techniques for sentiment analysis and
opinion mining, which could be categorized into
lexicon-based approaches (Kennedy and Inkpen,
2006; Turney, 2002; Yang et al., 2014a,b) and
corpus-based approaches (Pang et al., 2002; Yang
et al., 2015; Wan, 2009).

Recently, researchers have turned their atten-
tion to exploring sentiment analysis on the so-
cial media posts of individuals during natural dis-
asters and emergencies (Beigi et al., 2016; Bus-
caldi and Hernandez-Farias, 2015; Caragea et al.,
2014; Kryvasheyeu et al., 2015; Mandel et al.,
2012; Shalunts et al., 2014). For example, a sen-
timent analysis system is applied for Italian to a
set of tweets during the Genoa flooding (Buscaldi
and Hernandez-Farias, 2015). They attempted to
identify trending topics, toponym and sentiments
that might be relevant from a disaster manage-
ment perspective. However, the existing studies
only focused on sentiment analysis on the docu-
ment level, without considering the specific topics
in the document.

Meanwhile, topic models, such LDA (Blei
et al., 2003), have become popular in extracting
interesting topics. Some recent work incorpo-
rates context information into LDA, such as time
(Wang and McCallum, 2006; Zhao et al., 2014)
and authorship (Steyvers et al., 2004; Yang et al.,
2016) to make topic models fit expectations bet-
ter. Some studies also attempt to detect sentiment
and topic simultaneously from documents (Der-

mouche et al., 2015; Lin et al., 2012; Mukherjee
et al., 2014; He et al., 2012). Nevertheless, none
of existing methods takes advantage of temporal
and geographical information to identify and track
people’s topics and sentiment during emergencies
and disasters.

3 Model

In this section, we firstly introduce the generative
process of the LDST model. Then we present the
inference algorithm for estimating the model pa-
rameters.

3.1 Model Description

We assume the corpus consists a set of authors,
a set of locations, and a collection of documents
with specific timestamps. Formally, we use V and
U to denote the sets of locations and authors, re-
spectively. A document d ∈ D is a short text writ-
ten by an author u ∈ U in location v ∈ V at time t.
Also, let S be the number of distinct sentiment la-
bels, and T be the total number of topics, where S
and T are predefined constant values. Since each
tweet is a short text, studying them individually is
not very informative. We thus use pooling meth-
ods to construct aggregated documents for each lo-
cation or each author. For a venue v, we use Av to
define the set of all authors that have written docu-
ments in location v, and dv (location document) to
refer to the union of all the documents written in
location v. Ndv is the number of words in location
document dv.

Figure 1: Graphical representation of our algo-
rithm

Our model generalizes LDA by jointly model-
ing topic, sentiment, time and geolocation infor-
mation. Figure 1 shows the graphical model of

528



LDST. The formal definition of the generative pro-
cess of LDST is as follows:

1. For each sentiment label s,
a. For each topic z under sentiment s,

– Draw a word distribution φs,z ~ Dir(βs,z).
2. For each author u,

a. Draw ωu ∼ Beta(γ)
b. Draw a sentiment distribution χ(a)u ∼ Dir(τ)
c. For each sentiment label s,

– Draw a topic distribution θ(a)u,s ∼ Dir(ρ)
3. For each location document dv,

a. Draw a sentiment distribution χ(v)dv ∼ Dir(σ)
b. For each sentiment label s,

– Draw a topic distribution θ(v)dv,s ~ Dir(α)
c. For each term wdvi in the location document dv ,

– Draw a author u ∼ Av uniformly,
– Draw a switch c ∼ Bernoulli(ωu)
– if c = 0,

* Draw a sentiment sdvi ∼Mult(χ(a)u )
* Draw a topic zdvi from Mult(θ

(a)
u,sdvi

)
– if c = 1,

* Draw a sentiment sdvi ∼Mult(χ(v)dv )
* Draw a topic zdvi ∼Mult(θ(v)dv,sdvi)

– Draw a word wdvi ∼Mult(φsdvi,zdvi)
– Draw a timestamp tdvi from Beta(ψsdvi,zdvi)

In the model, α, β, ρ, σ, τ and γ are hyperpa-
rameters. The latent sentiments and topics depend
on the document venues and personalities of the
author. We use ω to control the influence from the
venue and the author. In particular, ω is the param-
eter of a Bernoulli distribution, from which a bi-
nary variable c is generated to determine whether
the document is influenced by venue or user.

3.2 Inference Algorithm

We use Collapsed Gibbs sampling (Porteous et al.,
2008) to estimate the unknown latent variables
{φ, ω, θa, θv, χa, χv, ψ}. The posterior distribu-
tion of the hidden variables for each word wdv ,i
(i-th word in venue document dv) is calculated
as follows (to simplify, we use Θ to refer to
<u−dv,i, c−dv,i, s−dv,i, z−dv,i,w, t, Av>):

P (udv ,i = u, cdv ,i = 0, sdv ,i = s, zdv ,i = z | Θ)

∝ = n
−dv ,i
u,c (0) + γ′

n−dv ,iu,c + γ + γ′
· n

−dv ,i
u,s + τs∑

s′
(n−dv ,iu,s′ + τs′)

· n
−dv ,i
u,s,z + ρz∑

z′
(n−dv ,iu,s,z′ + ρz′)

· n
−dv ,i
s,z,w + βw∑

w′
(n−dv ,is,z,w′ + βw′)

· (1− t)
ψ

(1)
s,z−1 · tψ(2)s,z−1

Beta(ψ(1)s,z , ψ
(2)
s,z )

(1)

P (udv ,i = u, cdv ,i = 1, sdv ,i = s, zdv ,i = z | Θ)

∝ = n
−dv ,i
u,c (1) + γ

n−dv ,iu,c + γ + γ′
· n

−dv ,i
v,s + σs∑

s′
(n−dv ,iv,s′ + σs′)

· n
−dv ,i
v,s,z + αz∑

z′
(n−dv ,iv,s,z′ + αz′)

· n
−dv ,i
s,z,w + βw∑

w′
(n−dv ,is,z,w′ + βw′)

· (1− t)
ψ

(1)
s,z−1 · tψ(2)s,z−1

Beta(ψ(1)s,z , ψ
(2)
s,z )

(2)

where nu,c(0) and nu,c(1) are the numbers of
times that c = 0 and c = 1 are sampled for user u,
respectively, and we have nu,c , nu,c(0) +nu,c(1).
nu,s is the number of times that sentiment s is sam-
pled from the distribution χau specific to user u,
and nv,s is the number of times that sentiment s is
sampled from the distribution χvdv specific to doc-
ument venue v. nu,s,z is the number of times that
topic z is sampled from the distribution θau,s spe-
cific to user u and sentiment s, and nv,s,z is the
number of times that topic z is sampled from the
distributionθvdvs specific to document venue v and
sentiment s. ns,z,w is the number of times that
word w is sampled from the distribution φs,z spe-
cific to sentiment s and topic z. The superscript
−dv, i denotes a quantity excluding the current
word −dv, i.

After Gibbs sampling, {φ, ω, θa, θv, χa, χv, ψ}
can be estimated as follows:\

φ̂s,z,w =
ns,z,w + βw∑

w’
(ns,z,w’ + βw′)

,

ω̂u =
nuc(1) + γ
nu,c + γ + γ′

,

θ̂vv,s,z =
nv,s,z + αz∑

z’
(nv,s,z’ + αz′)

,

θ̂au,s,z =
nu,s,z + ρz∑

z’
(nu,s,z’ + ρz′)

,

χ̂au,s =
nu,s + τs∑

s’
(nu,s’ + τs′)

,

χ̂vdv ,s =
nv,s + σs∑

s’
(nv,s’ + σs′)

,

ψ̂(1)s,z = m̄s,z ·
(
m̄s,z · (1− m̄s,z)

ξ2s,z
− 1
)
,

ψ(2)s,z = (1− m̄s,z) ·
(
m̄s,z · (1− m̄s,z)

ξ2s,z
− 1
)
.

529



where msz and ξ2sz indicate the sample mean and
the biased sample variance of the timestamps be-
longing to sentiment s and topic z, respectively.

3.3 Defining the Prior Knowledge
In our model, the prior knowledge is employed to
guide the generative process of topics. The prior
knowledge can be obtained from natural disaster
experts. We collect a small set of seed words
for each topic of interest during emergency and
disaster. For each topic, the LDST model draws
the word distribution from a biased Dirichlet prior
Dir(β). Each vector β.,z ∈ RV is constructed
from the sets of seed words, where

β.,z := λ1(1V − Λ.,z) + λ2Λ.,z (3)
Here, Λ.,z,w = 1 if and only if word w is a seed
word for topic z, otherwise Λ.,z,w = 0. The
scalars λ1 and λ2 are hyperparameters. Intuitively,
when λ1 < λ2 , the biased prior ensures that the
seed words are more probably drawn from the as-
sociated topic.

4 Experiments

4.1 Hurricane Sandy Twitter Datasets
This dataset contains nearly 15 million tweets
posted on Twitter while Hurricane Sandy was hit-
ting the United States. Tweets were collected from
October 25, 2012 to November 4, using the key-
words ’hurricane’ and ’sandy’ (Zubiaga and Ji,
2014). In this paper, we only keep the geotagged
tweets. The final experimental dataset consists
of 159,880 geotagged tweets. The original geo-
graphical information is expressed by using lon-
gitude and latitude in decimal degree. We set the
granularity of location as a state via Google Maps
Geocoding API2 and analyze the tweets within the
United States.

4.2 Baseline Methods
We evaluate and compare our model with several
baseline methods as follows:
LDA: We use gensim toolkit to do inference for
LDA model (Blei et al., 2003).
ToT: Topics over Time, a non-Markov continu-
ous time model proposed in (Wang and McCal-
lum, 2006).
JST: The first Joint Sentiment-Topic model to
identify the sentiment-topic pairs (Lin and He,
2009) .

2https://developers.google.com/maps/

TS: Topic-Sentiment model proposed in (Der-
mouche et al., 2015).
LDST-w/oS: This is the LDST model without em-
ploying prior knowledge (seed words). We use this
method to evaluate the influence of seed words.

4.3 Implementation details

In our implementation, we set topic number T =
50, and the prior hyperparameters γ = 0.5, τ =
σ = 0.1, ρ = α = 50/T . βs,z is calculated using
the set of seed words with λ1 = 0.1 and λ2 = 0.8.

As described in Section 3.3, we use a small
set of seed words as our topic prior knowledge.
Specifically, the seed words list contains 5 to 10
seed words for each of the five topics of inter-
est3 about Hurricane impact, public utility, food,
shelter, medical, respectively. We choose these
five topics based on the actual requirements of our
project. However, it is important to note that the
model is flexible and do not need to have seed
words for every topic.

4.4 Experiment results

4.4.1 Quantitative evaluation
We first compare our model with the baseline
models in terms of perplexity which is a widely
used measurement of how well a probability
model predicts a sample. The lower the perplexity,
the better the model. We calculate the average per-
plexity (log-likelihood) using 1000 held-out docu-
ments which are randomly selected from the test
data. The average test perplexity of each word is
calculated as exp{− 1N

∑
w log p(w)}, where N is

the total number of words in the held-out test doc-
uments. Table 1 shows the perplexity results for
Hurricane Sandy dataset. Our model outperforms
the baseline models. In particular, the perplexity
of our model is 1122, which is 40 and 116 lower
than that of JST and TS. The perplexity of LDST
is 35 lower than that of LDST-w/oS, which indi-
cates that the seed words can further improve the
performance of our model.

LDA ToT JST TS LDST-w/oS LDST

1320 1244 1162 1238 1157 1122

Table 1: Comparison of test perplexity per word

Following the same evaluation as in (Lin et al.,
2012), we also present and discuss the experimen-
tal results of sentiment classification. The docu-

3Details available at https://goo.gl/ykbrXZ

530



ment sentiment is classified based on the probabil-
ity of sentiment label given document, which can
be approximated by χ̂(a)u and χ̂vdv . Similar to (Lin
et al., 2012), we only consider the probability of
positive and negative label given document, with
the neutral label probability being ignored. We
define that a document d is classified as a positive-
sentiment document if its probability of positive
sentiment label given document is greater than its
probability of negative sentiment label given doc-
ument, and vice versa. The ground truth of senti-
ment classification labels of tweets are set by us-
ing human annotation. Specifically, we randomly
select 1000 documents from the dataset, and la-
bel each document as positive, negative or neutral
manually. We measure the performance of our
model using the tweets with positive or negative
labels. The classification accuracies are summa-
rization in Table 2. LDST significantly outperforms
other methods on test data. This verifies the effec-
tiveness of the proposed approach.

LDA ToT JST TS LDST-w/oS LDST

0.574 0.592 0.622 0.597 0.635 0.646

Table 2: Sentiment classification accuracy.

4.4.2 Qualitative evaluation

We present the sentiments and topics discovered
by LDST to see whether LDST captured mean-
ingful semantics. We analyze the extracted top-
ics under positive and negative sentiment labels.
Due to the limited space, we only report Hurri-
cane impact and public utility topics under pos-
itive and negative sentiments for New York and
Florida states on Oct. 27, 2012 and Oct. 30,
2012. For each topic, we visualize it using the top
5 words which are most likely generated from the
topic. As shown in Table 3, the extracted topics
are quite informative and coherent. For example,
the first topic (left) is closely related to Hurricane
impact, and the other one (right) is related to pub-
lic utility. The results show that our model can
extract topic and sentiment simultaneously under
different time and location. First, at the same pe-
riod, people in different location had different top-
ics of interest and sentiments. Taking the public
utility topic as an example, on Oct. 30, people
from Florida were concerned with post-disaster re-
lief, while people from New York focused on the
outage of the public utility. Second, for the people
in the same location, they were interested in dif-

New York Florida

pos

pray island florida work

response careful prepare service

volunteer volunteer boat island

usa preparation protect customer

florida state power system

neg

destroy rising sandy power

homeless moves storm outage

flood arrival broken damage

eastcoast shortages landfall energy

damage cause disaster utilities

New York Florida

pos

aid nation volunteer restore

alert people claimed system

rescue delivered crew resources

york safety power good

shelter companies relief rebuild

neg

hirricane power east damage

weatherd metro criticize effect

death destroyed eastern weeks

halloween outage election millions

flood destructive tourism delay

Table 3: Hurricane impact (left) and public util-
ity (right) topics under positive and negative sen-
timent labels for New York and Florida states on
Oct. 27, 2012 (above) and Oct. 30, 2012 (bottom).

ferent topics, and had different sentiments towards
the topics. For example, the people in Florida paid
close attention to the damage of the public utility
on Oct. 27, while they changed their attention to
post-disaster relief of the public utility on Oct. 30.

5 Conclusion

In this paper, we propose a location-based dy-
namic sentiment-topic (LDST) model, which can
jointly model sentiment, topic, temporal, and ge-
olocation information.

Acknowledgement

Heng Ji’s work was supported by U.S. DARPA
LORELEI Program No. HR0011-15-C-0115. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.

531



References
Ghazaleh Beigi, Xia Hu, Ross Maciejewski, and Huan

Liu. 2016. An overview of sentiment analysis in
social media and its applications in disaster relief.
In Sentiment Analysis and Ontology Engineering,
pages 313–340. Springer.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. The Journal of
machine Learning research, 3:993–1022.

Davide Buscaldi and Irazú Hernandez-Farias. 2015.
Sentiment analysis on microblogs for natural dis-
asters management: a study on the 2014 genoa
floodings. In Proceedings of the 24th International
Conference on World Wide Web, pages 1185–1188.
ACM.

Cornelia Caragea, Anna Squicciarini, Sam Stehle,
Kishore Neppalli, and Andrea Tapia. 2014. Map-
ping moods: geo-mapped sentiment analysis during
hurricane sandy. In Annual Conference for Informa-
tion Systems for Crisis Response and Management.

Mohamed Dermouche, Leila Kouas, Julien Velcin, and
Sabine Loudcher. 2015. A joint model for topic-
sentiment modeling from text. In Proceedings of the
30th Annual ACM Symposium on Applied Comput-
ing, pages 819–824. ACM.

Yulan He, Chenghua Lin, Wei Gao, and Kam-Fai
Wong. 2012. Tracking sentiment and topic dynam-
ics from social media. In Sixth International AAAI
Conference on Weblogs and Social Media.

Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment classification of movie reviews using contex-
tual valence shifters. Computational intelligence,
22(2):110–125.

Yury Kryvasheyeu, Haohui Chen, Esteban Moro, Pas-
cal Van Hentenryck, and Manuel Cebrian. 2015.
Performance of social network sensors during hur-
ricane sandy. PLoS one, 10(2):e0117288.

Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM conference on Information
and knowledge management, pages 375–384. ACM.

Chenghua Lin, Yulan He, Richard Everson, and Stefan
Ruger. 2012. Weakly supervised joint sentiment-
topic detection from text. In Proceedings of the In-
ternational Conference on Information and Knowl-
edge Management, pages 1134–1145. IEEE.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.

Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during hurricane irene. In Proceedings of the Sec-
ond Workshop on Language in Social Media, pages
27–36. Association for Computational Linguistics.

Subhabrata Mukherjee, Gaurab Basu, and Sachindra
Joshi. 2014. Joint author sentiment topic model. In
Proceedings of the 2014 SIAM International Confer-
ence on Data Mining, pages 370–378. SIAM.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 79–86. Association for
Computational Linguistics.

Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent dirichlet al-
location. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 569–577. ACM.

Gayane Shalunts, Gerhard Backfried, and Prinz Prinz.
2014. Sentiment analysis of german social media
data for natural disasters. In Annual Conference for
Information Systems for Crisis Response and Man-
agement.

Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi,
and Thomas Griffiths. 2004. Probabilistic author-
topic models for information discovery. In Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 306–315. ACM.

Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, pages 417–424. Association for Computa-
tional Linguistics.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Annual
Meeting on Association for Computational Linguis-
tics, pages 235–243. Association for Computational
Linguistics.

Xuerui Wang and Andrew McCallum. 2006. Topics
over time: a non-markov continuous-time model of
topical trends. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 424–433. ACM.

Min Yang, Jincheng Mei, Fei Xu, Wenting Tu, and
Ziyu Lu. 2016. Discovering author interest evolu-
tion in topic modeling. In Proceedings of the 39th
International ACM SIGIR conference on Research
and Development in Information Retrieval, pages
801–804. ACM.

Min Yang, Baolin Peng, Zheng Chen, Dingju Zhu, and
Kam-Pui Chow. 2014a. A topic model for building
fine-grained domain-specific emotion lexicon. Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 421–426.

532



Min Yang, Wenting Tu, Ziyu Lu, Wenpeng Yin, and
Kam-Pui Chow. 2015. Lcct: a semisupervised
model for sentiment classification. In The 2015 An-
nual Conference of the North American Chapter of
the ACL. Association for Computational Linguistics.

Min Yang, Dingju Zhu, Rashed Mustafa, and Kam-Pui
Chow. 2014b. Learning domain-specific sentiment
lexicon with supervised sentiment-aware lda. The
21st Eruopean Conference on Artificial Intelligence,
pages 927–932.

Zhou Zhao, James Cheng, and Wilfred Ng. 2014. Truth
discovery in data streams: A single-pass probabilis-
tic approach. In Proceedings of the 23rd ACM
International Conference on Conference on Infor-
mation and Knowledge Management, pages 1589–
1598. ACM.

Arkaitz Zubiaga and Heng Ji. 2014. Tweet, but verify:
epistemic study of information verification on twit-
ter. Social Network Analysis and Mining, 4(1):163.

533


