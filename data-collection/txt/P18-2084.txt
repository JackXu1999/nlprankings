



















































A Document Descriptor using Covariance of Word Vectors


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 527–532
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

527

A Document Descriptor using Covariance of Word Vectors

Marwan Torki
Alexandria University, Egypt
mtorki@alexu.edu.eg

Abstract

In this paper, we address the problem of
finding a novel document descriptor based
on the covariance matrix of the word vec-
tors of a document. Our descriptor has a
fixed length, which makes it easy to use in
many supervised and unsupervised appli-
cations. We tested our novel descriptor in
different tasks including supervised and un-
supervised settings. Our evaluation shows
that our document covariance descriptor
fits different tasks with competitive perfor-
mance against state-of-the-art methods.

1 Introduction

Retrieving documents that are similar to a query
using vectors has a long history. Earlier meth-
ods modeled documents and queries using vec-
tor space models via bag-of-words (BOW) rep-
resentation (Salton and Buckley, 1988). Other
representations include latent semantic indexing
(LSI) (Deerwester et al., 1990), which can be used
to define dense vector representation for documents
and/or queries. The past few years have witnessed a
big interest in distributed representation for words,
sentences, paragraphs and documents. This was
achieved by leveraging deep learning methods that
learn word vector representation. Introduction of
neural language models (Bengio et al., 2003) us-
ing deep learning allowed to learn word vector
representation (word embedding for simplicity).
The seminal work of Mikolov et al. introduced an
efficient way to compute dense vectorized repre-
sentation of words (Mikolov et al., 2013a,b). A
more recent step was taken to move beyond dis-
tributed representation of words. This is to find a
distributed representation for sentences, paragraphs
and documents. Most of the presented works study
the interrelationship between words in a text snip-

Figure 1: Doc1 is about “pets” and Doc2 is about
“travel”. Top: The first two dimensions of a word
embedding for each document. Bottom Left: The
embedding of the words of the two documents. The
Mean vectors and the paragraph vectors are shown.
Covariance matrices are shown via the confidence
ellipses. Bottom Right: Corresponding covari-
ance matrices are represented as points in a new
space.

pet (Hill et al., 2016; Kiros et al., 2015; Le and
Mikolov, 2014) in an unsupervised fashion. Other
methods build a task specific representation (Kim,
2014; Collobert et al., 2011).

In this paper we propose to use the covariance
matrix of the word vectors in some document to de-
fine a novel descriptor for a document. We call our
representation DoCoV descriptor. Our descriptor
obtains a fixed-length representation of the para-
graph which captures the interrelationship between
the dimensions of the word embedding via the co-
variance matrix elements. This makes our work dis-
tinguished from to the work of (Le and Mikolov,
2014; Hill et al., 2016; Kiros et al., 2015) where
they study the interrelationship of words in the text



528

snippet.

1.1 Toy Example

We show a toy example to highlight the differences
between DoCoV vector, the Mean vector and para-
graph vector (Le and Mikolov, 2014). First, we
used Gensim library1 to generate word vectors and
paragraph vectors using a dummy training corpus.
Next, we formed two hypothetical documents; first
document contains words about “pets” and second
document contains words about “travel”. In figure 1
we show on the top part the first two dimensions of
a word embedding for each document separately.
On the bottom Left, we show embedding of the
two documents’ words in the same space. We also
show the Mean vectors and the paragraph vectors.
In the word embedding space the covariance ma-
trices are represented via the confidence ellipses.
On the bottom right we show the corresponding
covariance matrices as points in a new space after
vectorization step.

1.2 Motivation and Contributions

Below we describe our motivation towards the pro-
posal of our novel representation:
(1) Some neural-based paragraph representations
such as paragraph vectors (Le and Mikolov, 2014)
, FastSent (Hill et al., 2016) use a shared space
between the words and paragraphs. This is counter
intuitive, as the paragraph is a different entity other
than the words. Figure 1 illustrates that point, we
do not see a clear interpretation of why the para-
graph vectors (Le and Mikolov, 2014) are posi-
tioned in the space as in figure 1.
(2) The covariance matrix represents the second
order summary statistic of multivariate data. This
distinguishes the covariance matrix from the mean
vector. In figure 1 we visualize the covariance ma-
trix using confidence ellipse representation.We see
that the covariance encodes the shape of the density
composed of the words of interest. In the earlier
example the Mean vectors of two dissimilar doc-
uments are put close by the word embedding. On
the other hand, the covariance matrices capture the
distinctness of the two documents.
(3) The use of the covariance as a spatial descriptor
for multivariate data has a great success in different
domains like computer vision (Tuzel et al., 2006;
Hussein et al., 2013; Sharaf et al., 2015) and brain
signal analysis (Barachant et al., 2013). With this

1https://radimrehurek.com/gensim/

global success of this representation, we believe
this method can be useful for text-related tasks.
(4) The computation of the covariance descriptor is
known to be fast and highly parallelizable. More-
over, there is no inference steps involved while
computing the covariance matrix given its observa-
tions. This is an advantage compared to existing
methods for generating paragraph vectors, such as
(Le and Mikolov, 2014; Hill et al., 2016).

Our contribution in this work is two-fold:
(1) We propose the Document-Covariance descrip-
tor (DoCoV) to represent every document as the
covariance of the word embedding of its words. To
the best of our knowledge, we are the first to explic-
itly compute covariance descriptors on word em-
bedding such as word2vec (Mikolov et al., 2013b)
or similar word vectors.
(2) We empirically show the effectiveness of our
novel descriptor in comparison to the state-of-the-
art methods in various unsupervised and supervised
classification tasks. Our results show that our de-
scriptor can attain comparable accuracy to state-of-
the-art methods in a diverse set of tasks.

1.3 Related Work

We can see the word embedding at the core of
recent state-of-art methods for solving many tasks
like semantic textual similarity, sentiment analysis
and more. Among the approaches of finding word
embedding are (Pennington et al., 2014; Levy and
Goldberg, 2014; Mikolov et al., 2013b). These
alternatives share the same objective of finding a
fixed-length vectorized representation for words
to capture the semantic and syntactic regularities
between words.

These efforts paved the way for many re-
searchers to judge document similarity based on
word embedding. Some efforts aimed at find-
ing a global representation of a text snippet us-
ing a paragraph-level representation such as para-
graph vectors (Le and Mikolov, 2014). Recently
other neural-based sentence and paragraph level
representations appeared to provide a fixed length
representation like Skip-Thought Vectors (Kiros
et al., 2015) and FastSent (Hill et al., 2016).
Some efforts focused on defining a Word Mover
Distance(WMD) based on word level representa-
tion (Kusner et al., 2015).

Prior to this work, we proposed earlier trials
for using covariance features in community ques-
tion answering (Malhas et al., 2016b,a; Torki et al.,

https://radimrehurek.com/gensim/


529

2017). In these trials we used the covariance fea-
tures in combination with lexical and semantic fea-
tures. Close to our work is (Nikolentzos et al.,
2017), they build an implicit representation of doc-
uments using multidimensional Gaussian distribu-
tion. Then they compute a similarity kernel to be
used in document classification task. Our work is
distinguished from (Nikolentzos et al., 2017) as
we compute an explicit descriptor for any docu-
ment. Moreover, we use linear models which scale
much better than non-linear kernels as introduced
in (Nikolentzos et al., 2017).

2 Document Covariance Descriptor

We present our DoCoV descriptor. First, we define
a document observation matrix. Second, we show
how to extract our DoCoV descriptor.
Document Observation Matrix
Given a d-dimensional word embedding model and
an n-terms document. We can define a document
observation matrix O ∈ Rn×d. In the matrix O, a
row represents a term in the document and columns
represent the d-dimensional word embedding rep-
resentation for that term.

Assume that we have observed n terms of a d-
dimensional random variable; we have a data ma-
trix O(n× d) :

O =


x11 · · · x1d
x21 · · · x2d

...
. . .

...
xn1 · · · xnd

 (1)

The rows xi =
[
x1 x2 · · · xd

]T ∈ Rd, denote
the i-th observation of a d-dimensional random
variable X ∈ Rd. The “sample mean vector” of
the n observations ∈ Rd is given by the vector x̄
of the means x̄j of the d variables:

x̄ =
[
x̄1 x̄2 · · · x̄d

]T ∈ Rd (2)
From hereafter, when we mention the Mean vector
we mean the sample Mean Vector x̄.
Document-Covariance Descriptor (DoCoV)
Given an observation matrix O for a document,

we compute the covariance matrix entriesfor every
pair of dimensions (X,Y ).

σX,Y =

∑N
i=1(xi − x̄)(yi − ȳ)

N
(3)

The matrix C ∈ Rd×d is a symmetric matrix and
is defined as

C =


σ2X1 σX1X2 · · · σX1Xd
σX1X2 σ

2
X2

· · · σX2Xd
...

...
. . .

...
σX1Xd σX2Xd · · · σ2Xd

(4)
We Compute a vectorized representation of the

matrix C as the stacking of the upper triangular
part of matrix C as in eq. 5. This process produces
a vector v ∈ Rd(d+1)/2. The Euclidean distance
between vectorized matrices is equivalent to the
Frobenius norm of the original covariance matrices.

v = vect(C) =

{√
2Cp,q if p < q

Cp,q if p = q
(5)

3 Experimental Evaluation

We show an extensive comparative evaluation for
unsupervised paragraph representation approaches.
We test the unsupervised semantic textual similarity
task. Next, we show a comparative evaluation for
text classification benchmarks.

3.1 Effect of Word Embedding Source and
Dimensionality on Classification Results

We evaluate classification performance over the
IMDB movie reviews dataset using error rate as
the evaluation measure. We report our results using
a linear SVM classifier.We chose the default pa-
rameters for Linear SVM classifier in scikit-learn
library2.

The IMDB movie review dataset was first pro-
posed by Maas et al. (Maas et al., 2011) as a bench-
mark for sentiment analysis. The dataset consists
of 100K IMDB movie reviews and each review has
several sentences. The 100K reviews are divided
into three datasets: 25K labelled training instances,
25K labelled test instances and 50K unlabelled
training instances. Each review has one label rep-
resenting the sentiment of it: Positive or Negative.
These labels are balanced in both the training and
the test set.

The objective is to show that theDoCoV de-
scriptor can be used with different alternatives for
word representations. Also, the experiment shows
that pre-trained models are giving the best results,
namely the word2vec model built on Google news.
This alleviates the need of computing a problem

2 http://scikit-learn.org/

http://scikit-learn.org/


530

specific word embedding. In some cases there is
no available data to construct the word embedding.
To illustrate that we tried different alternatives for
word representation.
(1) We computed our own skipgram models using
Gensim Library. We used the Training and unla-
belled subsets of IMDB dataset to obtain different
embedding by setting number of dimensions to 100,
200 and 300.
(2) We used pre-trained GloVe models trained on
wikipedia2014 and Gigaword5. We tested the avail-
able different dimensionality 100, 200 and 300. We
also used the 300 dimensions GloVe model that
used commoncrawl with 42 Billion tokens We call
the last one Lrg. This model provides word vectors
of 300 dimensions for each word.
(3) We used pre-trained word2vec model trained
on Google news. We call it Gnews. This model
provides word vectors of 300 dimensions for each
word. Table 1 shows the results when using DoCoV
computed at different dimensions of word embed-
ding in classification. The table also compares
classification performance when using DoCoV to
the performance when using the Mean of word em-
bedding as a baseline. Also, we show the effect of
fusing DoCoV with other feature sets. We mainly
experiment with the following sets: DoCoV, Mean,
and bag-of-words (BOW). We use the mean and
DoCoV features.

Observations
From the results we can observe the following
(1) We observe that the DoCoV is consistently out-
performing the Mean vector for different dimen-
sionality of the word embedding regardless of the
embedding source.
(3) The best performing feature concatenation is
DoCoV+BOW. This ensures that the concatenation
in fact is benefiting from both representations.
(3) In general the best results are achieved using
the available 300-dimensions Gnews word embed-
ding. In the subsequent experiments we will use
that embedding such that we do not need to build a
different word embedding for every task on hand.

Unsupervised Semantic Textual Similarity
We conduct a comparative evaluation against the
state-of-the-art approaches in unsupervised para-
graph representation. We follow the setup used
in (Hill et al., 2016).
Datasets and Baselines
We contrast our results against the methods re-
ported in (Hill et al., 2016). The competing meth-

ods are the paragraph vectors (Le and Mikolov,
2014), skip-thought vectors (Kiros et al., 2015),
Fastsent (Hill et al., 2016), Sequential (Denois-
ing) Autoencoders (SDAE) (Hill et al., 2016). The
Mean vector baseline is also implemented. Also,
we use the sum of the similarities generated by
the DoCoV and the mean vectors. All of our re-
sults are reported using the freely available Gnews
word2vec of dim = 300. We use same evaluation
measures (Hill et al., 2016). We use the Pearson
correlation and Spearman correlation with the man-
ual relatedness judgements.

The semantic sentence relatedness datasets
used in the comparative evaluation the SICK
dataset (Marelli et al., 2014) consists of 10,000
pairs of sentences and relatedness judgements and
the STS 2014 dataset (Agirre et al., 2014) consists
of 3,750 pairs and ratings from six linguistic do-
mains.
Results and Discussion
We show the correlation values between the simi-
larities computed via DoCoV and the human judge-
ments. We contrast the performance of other repre-
sentations in table 2.

We observe that DoCoV representation outper-
forms other representations in this task. Other mod-
els such as skipthought vectors (Kiros et al., 2015)
and SDAE (Hill et al., 2016) requires building an
encoder-decoder model which takes time3 to learn.
For other models like paragraph vectors (Le and
Mikolov, 2014) and Fastsent vectors (Hill et al.,
2016), they require a gradient descent inference
step to compute the paragraph/sentence vectors.
Using the DoCoV, we just require a pre-trained
word embedding model and we do not need any
additional training like encoder-decoder models or
inference steps via gradient descent.
Text Classification Benchmarks
The datasets used in this experiment form a text-

classification benchmark for sentence and para-
graph classification. Towards the end of this sec-
tion we can clearly identify the value of the DoCoV
descriptor as a generic descriptor for text classifica-
tion tasks.
Datasets and Baselines
We contrast our results against the same methods
of unsupervised paragraph representations. In ad-
dition to the results of DoCoV we examined con-
catenation of BoW with tf-idf weighting and Mean
vectors with our DoCoV descriptors. We use linear

3Up to weeks.



531

Table 1: Error-Rate performance when changing word vectors dimensionality.
Model/Dim
BOW 9.66%
Gensim Mean DoCoV DoCoV DoCoV DoCoV

+Mean Bow +Mean+Bow
d=100 14.13% 11.64% 11.16% 9.39% 9.44%
d=200 12.86% 11.08% 10.80% 9.39% 9.58%
d=300 12.83% 11.08% 10.85% 9.41% 9.47%
Glove Mean DoCoV DoCoV DoCoV DoCoV

+Mean Bow +Mean+Bow
d=100 20% 13.07% 12.88% 9.63% 9.62%
d=200 16.95% 12.36% 12.22% 9.64% 9.65%
d=300 16.29% 12.00% 11.91% 9.63% 9.66%
d=300,Lrg 14.94% 11.70% 11.56% 9.5% 9.6%
Gnews Mean DoCoV DoCoV DoCoV DoCoV

+Mean Bow +Mean+Bow
d=300 14.03% 11.11% 10.75% 9.32% 9.6%

Table 2: Spearman/Pearson correlations on unsupervised (relatedness) evaluations.
STS-2014 SICKModel News Forums Wordnet Twitter Images Headlines All

P2vec (Le and Mikolov, 2014) 0.42/0.46 0.33/0.34 0.51/0.48 0.54/0.57 0.32/0.30 0.46/0.47 0.44/0.44 0.44/0.46
FastSent (Hill et al., 2016) 0.44/0.45 0.14/0.15 0.39/0.34 0.42/0.43 0.55/0.60 0.43/0.44 0.27/0.29 0.57/0.60
FastSent+AE (Hill et al., 2016) 0.58/0.59 0.41/0.36 0.74/0.70 0.63/0.66 0.74/0.78 0.57/0.59 0.63/0.64 0.61/0.72
Skip-Thought (Kiros et al., 2015) 0.56/0.59 0.41/0.40 0.69/0.64 0.70/0.74 0.63/0.65 0.58/0.60 0.62/0.62 0.60/0.65
SAE (Hill et al., 2016) 0.17/0.16 0.12/0.12 0.30/0.23 0.28/0.22 0.49/0.46 0.13/0.11 0.12/0.13 0.32/0.31
SAE+embs (Hill et al., 2016) 0.52/0.54 0.22/0.23 0.60/0.55 0.60/0.60 0.64/0.64 0.41/0.41 0.42/0.43 0.47/0.49
SDAE (Hill et al., 2016) .07/0.04 0.11/0.13 0.33/0.24 0.44/0.42 0.44/0.38 0.36/0.36 0.17/0.15 0.46/0.46
SDAE+embs (Hill et al., 2016) 0.51/0.54 0.29/0.29 0.56/0.50 0.57/0.58 0.59/0.59 0.43/0.44 0.37/0.38 0.46/0.46
Mean 0.65/0.68 0.46/0.45 0.75/0.78 0.71/0.75 0.76/0.78 0.59/0.64 0.64/0.66 0.63/0.73
DoCoV 0.62/0.68 0.50/0.51 0.77/0.79 0.69/0.75 0.78/0.80 0.60/0.63 0.67/0.70 0.61/0.69
DoCoV+Mean 0.64/0.70 0.51/0.51 0.79/0.78 0.71/0.76 0.78/0.81 0.61/0.65 0.67/0.70 0.62/0.71

Table 3: Accuracy of sentence representation mod-
els on text classification benchmarks.

Representation \Dataset MR CR Trec Subj Overall
Mean 77.4 79.2 80 91.3 81.98
BOW +tf-idf weights 77.1 78.5 89.3 89.3 83.55
P2vec (Le and Mikolov, 2014) 74.8 78.1 91.8 90.5 83.8
Skip-uni (Kiros et al., 2015) 75.5 79.3 91.4 92.1 84.58
bi-skip (Kiros et al., 2015) 73.9 77.9 89.4 92.5 84.43
comb-skip (Kiros et al., 2015) 76.5 80.1 92.2 93.6 85.6
FastSent (Hill et al., 2016) 70.8 78.4 76.8 88.7 78.68
FastSentAE (Hill et al., 2016) 71.8 76.7 80.4 88.8 79.43
SAE (Hill et al., 2016) 62.6 68 80.2 86.1 74.23
SAE+embs (Hill et al., 2016) 73.2 75.3 80.4 89.8 79.68
SDAE (Hill et al., 2016) 67.6 74 77.6 89.3 77.13
SDAE+embs (Hill et al., 2016) 74.6 78 78.4 90.8 80.45
COV 79.7 79.4 89.5 92.8 85.35
COV+Mean 80.2 80.1 90.3 93.1 85.93
COV+Bow 80.7 80.5 91.8 93.3 86.58
COV+Mean+BOW 81.1 81.5 91.6 93.2 86.85

SVM for all the tasks. All of our results are re-
ported using the freely available Gnews word2vec
of dim = 300. We use classification accuracy as
the evaluation measure for this experiment as (Hill
et al., 2016).
The subsets used in comparative benchmark evalua-
tion are: Movie Reviews MR (Pang and Lee, 2005),
Subjectivity Subj (Pang and Lee, 2004),Customer
Reviews CR (Hu and Liu, 2004) and TREC Ques-
tion TREC (Li and Roth, 2002).
Results and Discussion
Table 3 shows the results of our variants against
state-of-art algorithms that use unsupervised para-
graph representation.

We observe that DoCoV is consistently better
than the Mean vector and BOW with tf-idf weights.
Also, DoCoV is improving consistently when con-
catenated with baselines such as Mean vector and
BOW vectors. This means each feature is captur-
ing different discriminating information. This justi-
fies the choice of concatenating DoCoV with other
features. We further observe that DoCoV is con-
sistently better than the paragraph vectors (Le and
Mikolov, 2014), Fastsent and SDAE (Hill et al.,
2016). The overall accuracy of DoCoV is high-
lighted and it outperforms other methods on the
text classification benchmark.

4 Conclusion

We presented a novel descriptor to represent text
on any level such as sentences, paragraphs or docu-
ments. Our representation is generic which makes
it useful for different supervised and unsupervised
tasks. It has fixed-length property which makes
it useful for different learning algorithms. Also,
our descriptor requires minimal training. We do
not require a encoder-decoder model or a gradient
descent iterations to be computed.

Empirically we showed the effectiveness of the
descriptor in different tasks. We showed better
performance against other state-of-the-art methods
in supervised and unsupervised settings.



532

References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel

Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. Semeval-2014 task 10: Multilingual
semantic textual similarity. In SemEval.

Alexandre Barachant, Stéphane Bonnet, Marco Con-
gedo, and Christian Jutten. 2013. Classification of
covariance matrices using a riemannian-based ker-
nel for bci applications. Neurocomputing.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
JMLR.

Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In NAACL.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In SIGKDD.

Mohamed E Hussein, Marwan Torki, Mohammad Ab-
delaziz Gowayyed, and Motaz El-Saban. 2013. Hu-
man action recognition using a temporal hierarchy
of covariance descriptors on 3d joint locations. In
IJCAI.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
NIPS.

Matt J Kusner, Yu Sun, Nicholas I Kolkin, Kilian Q
Weinberger, et al. 2015. From word embeddings to
document distances. In ICML.

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In NIPS.

Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In ACL.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
ACL-HLT.

Rana Malhas, Marwan Torki, Rahma Ali, Tamer El-
sayed, and Evi Yulianti. 2016a. Real, live, and con-
cise: Answering open-domain questions with word
embedding and summarization. In TREC.

Rana Malhas, Marwan Torki, and Tamer Elsayed.
2016b. Qu-ir at semeval 2016 task 3: Learning to
rank on arabic community question answering fo-
rums with word embedding. In SemEval.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014. A sick cure for the evaluation of compo-
sitional distributional semantic models. In LREC.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS.

Giannis Nikolentzos, Polykarpos Meladianos, François
Rousseau, Yannis Stavrakas, and Michalis Vazir-
giannis. 2017. Multivariate gaussian document rep-
resentation from word embeddings for text catego-
rization. In EACL.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In ACL.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In EMNLP.

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management.

Amr Sharaf, Marwan Torki, Mohamed E Hussein, and
Motaz El-Saban. 2015. Real-time multi-scale action
detection from 3d skeleton data. In WACV.

Marwan Torki, Maram Hasanain, and Tamer Elsayed.
2017. Qu-bigir at semeval 2017 task 3: Using sim-
ilarity features for arabic community question an-
swering forums. In SemEval.

Oncel Tuzel, Fatih Porikli, and Peter Meer. 2006. Re-
gion covariance: A fast descriptor for detection and
classification. In Computer Vision–ECCV 2006.


