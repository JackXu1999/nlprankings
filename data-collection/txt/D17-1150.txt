



















































Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Towards Bidirectional Hierarchical Representations
for Attention-Based Neural Machine Translation

Baosong Yang† Derek F. Wong†∗ Tong Xiao‡ Lidia S. Chao† Jingbo Zhu‡
†NLP2CT Lab, Department of Computer and Information Science,

University of Macau, Macau, China
‡NiuTrans Lab, Northeastern University, Shenyang, China

nlp2ct.baosong@gmail.com, {derekfw,lidiasc}@umac.mo,
{xiaotong,zhujingbo}@mail.neu.edu.cn

Abstract

This paper proposes a hierarchical atten-
tional neural translation model which fo-
cuses on enhancing source-side hierarchi-
cal representations by covering both lo-
cal and global semantic information us-
ing a bidirectional tree-based encoder. To
maximize the predictive likelihood of tar-
get words, a weighted variant of an at-
tention mechanism is used to balance
the attentive information between lexical
and phrase vectors. Using a tree-based
rare word encoding, the proposed model
is extended to sub-word level to allevi-
ate the out-of-vocabulary (OOV) prob-
lem. Empirical results reveal that the
proposed model significantly outperforms
sequence-to-sequence attention-based and
tree-based neural translation models in
English-Chinese translation tasks.

1 Introduction

Neural machine translation (NMT) automatically
learns the abstract features of and semantic re-
lationship between the source and target sen-
tences, and has recently given state-of-the-art re-
sults for various translation tasks (Kalchbrenner
and Blunsom, 2013; Sutskever et al., 2014; Bah-
danau et al., 2015). The most widely used model is
the encoder-decoder framework (Sutskever et al.,
2014), in which the source sentence is encoded
into a dense representation, followed by a decod-
ing process which generates the target translation.
By exploiting the attention mechanism (Bahdanau
et al., 2015), the generation of target words is con-
ditional on the source hidden states, rather than
on the context vector alone. From a model archi-
tecture perspective, prior studies of the attentive

∗Corresponding author

S0

PRP1

I

VP2

VP3

VBP5

take

PRT6

up

NP4

NP7

DT9

a

NN10

position

PP8

IN11

in

NP12

the room

Figure 1: Induction of phrase and sentence rep-
resentations over the syntactic structure of a sen-
tence.

encoder-decoder translation model are mainly di-
vided into two types.

The sequence-to-sequence model treats a sen-
tence as a sequence of tokens. The most funda-
mental approaches transform the source sentence
sequentially into a fixed-length context vector,
and the annotation vector of each word summa-
rizes the preceding words (Sutskever et al., 2014;
Cho et al., 2014b). Although Bahdanau et al.
(2015) used a bidirectional recurrent neural net-
work (RNN) (Schuster and Paliwal, 1997) to con-
sider preceding and following words jointly, these
sequential representations are insufficient to fully
capture the semantics of a sentence, due to the fact
that they do not account for the syntactic interpre-
tations of sentence structure (Eriguchi et al., 2016;
Tai et al., 2015). By incorporating additional fea-
tures into a sequential model, Sennrich and Had-
dow (2016) and Stahlberg et al. (2016) suggest that
a greater amount of linguistic information can im-
prove the translation performance.

The tree-to-sequence model encodes a source
sentence according to a given syntactic tree

1432



over the sentence. The existing tree-based en-
coders (Tai et al., 2015; Eriguchi et al., 2016; Zhou
et al., 2016) recursively generate phrase (sentence)
representations in a bottom-up fashion, whereby
the annotation vector of each phrase is derived
from its constituent sub-phrases. As a result, the
learned representations are limited to local infor-
mation, while failing to capture the global mean-
ing of a sentence. As illustrated in Figure 1, the
phrases “take up”1 and “a position”2 have differ-
ent meanings in different contexts. However, in
composing the representations hVP3 and hNP7 for
phrases VP3 and NP7, the current approaches do
not account for the differences in meaning which
arise as a result of ignoring the neighboring con-
text as well as the remote context, i.e. hNP7 ←
hPP8 (sibling) and hVP3 ← hNP7 (child of sibling).
More specifically, at the encoding step t, the gen-
erated phrase is based on the results at the previous
time steps ht−1 and ht−2, but has no information
about the parent phrases ht′ for t′ > t.

To address the above problems, we propose a
novel architecture, a bidirectional hierarchical en-
coder, which extends the existing attentive tree-
structured models (Eriguchi et al., 2016). In con-
trast to the model of Eriguchi et al. (2016), we first
use a bidirectional RNN (Schuster and Paliwal,
1997) at lexical level to concatenate the forward
and backward states as the hidden states of source
words, to capture the preceding and following con-
texts (described in Section 3.1). Secondly, we pro-
pose a bidirectional tree-based encoder (described
in Section 3.2), in which the original bottom-up
encoding model is extended using an additional
top-down encoding process. In the bidirectional
hierarchical model, the vector representations of
the sentence, phrases as well as words, are there-
fore based on the global context rather than local
information.

To effectively leverage hierarchical representa-
tions in generating the target words, we adopt
a variant weighted tree-based attention mecha-
nism (described in Section 3.4) in which a time-
dependent gating scalar is used to control the pro-
portion of conditional information between the
word and phrase vectors. To alleviate the out-of-
vocabulary (OOV) problem, we further extend the
proposed tree-based model to the sub-word level

1Take up has the meanings of start doing something new,
use space/time, accept an offer, etc.

2Position has the meanings of location, job offer,
rank/status, etc.

I tak
e up a

pos
itio

n
in the roo

m
〈eos
〉

hl1 h
l
2 h

l
3 h

l
4 h

l
5 h

l
6 h

l
7 h

l
8 h

l
9

hp2,3 h
p
4,5 h

p
7,8

hp6,8

hp4,8
hp2,8

hp1,8

Figure 2: The tree-based model of Eriguchi et al.
(2016) comprising a structured and sequential en-
coder.

by integrating byte-pair encoding (BPE) (Sennrich
et al., 2016) into the tree-based model (as de-
scribed in Section 3.3). Experimental results for
the NIST English-to-Chinese translation task re-
veal that the proposed model significantly outper-
forms the vanilla tree-based (Eriguchi et al., 2016)
and sequential NMT models (Bahdanau et al.,
2015) (Section 4.1).

2 Tree-Based Neural Machine
Translation

A neural machine translation system (NMT) aims
to use a single neural network to build a transla-
tion model, which is trained to maximize the con-
ditional distribution of sentence pairs using a par-
allel training corpus (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Cho et al., 2014b,a).
By incorporating syntactic information, the tree-
based NMT exploits an additional syntactic struc-
ture of the source sentence to improve the trans-
lation. Since most existing NMTs generate one
target word at a time, given a source sentence
x = (x1, ..., xN ) and its corresponding syntactic
tree tr, the conditional probability of a target sen-
tence y = (y1, ..., yM ) is formally expressed as:

p(y | x, tr) =
M∏
1

p(yj | y1, ..., yj−1, x, tr; θ),

where θ represents the model parameters. A tree-
based NMT consists of a tree-based encoder and a
decoder.

2.1 Tree-Based Encoder
In a tree-based encoder, the source language x is
encoded according to a given syntactic structure

1433



tr of the sentence. As shown in Figure 2, Eriguchi
et al. (2016) employed a forward Long Short-Term
Memory (LSTM) (Hochreiter and Schmidhuber,
1997; Gers et al., 2000) recurrent neural network
(RNN) to encode the lexical nodes and a tree-
LSTM (Tai et al., 2015) to generate the phrase
representations in a bottom-up fashion. In the
present study, we utilize the gated recurrent unit
(GRU) (Cho et al., 2014b) instead of an LSTM, in
view of its comparable performance (Chung et al.,
2014) and since it yields even better results for cer-
tain tasks (Józefowicz et al., 2015). The lexical an-
notation vectors (hl1, ..., h

l
N ) are sequentially gen-

erated by using a GRU. The i-th leaf node vector
is calculated as:

hli = f
l
GRU (xi, h

l
i−1), (1)

where xi is the i-th source word embedding and
hli−1 denotes the previous hidden state. The parent
hidden state h↑i,j summarizes its left child h

↑
i,k and

right child h↑k+1,j (i < k < j) by applying the
tree-GRU (Zhou et al., 2016) as follows:

z↑i,j = σ(U
L
(z)h

↑
i,k + U

R
(z)h

↑
k+1,j + b

↑
(z))

r↑i,k = σ(U
L
(rL)h

↑
i,k + U

R
(rL)h

↑
k+1,j + b

↑
(rL))

r↑k+1,j = σ(U
L
(rR)h

↑
i,k + U

R
(rR)h

↑
k+1,j + b

↑
(rR))

h̃↑i,j = tanh(U
L
(h)(r

↑
i,k � h↑i,k)

+ UR(h)(r
↑
k+1,j � h↑k+1,j) + b↑(h))

h↑i,j = z
↑
i,j h̃
↑
i,j + (1− z↑i,j)(h↑i,k + h↑k+1,j),

where z↑i,j is the update gate; r
↑
i,k, r

↑
k+1,j are the

reset gates for the left and right children; h̃↑i,j de-
notes the candidate activation; UL(·) and U

R
(·) repre-

sent weight matrices; b↑(·) denote bias vectors; σ is
the logistic sigmoid function; and the operator �
denotes element-wise multiplication between vec-
tors. The phrase representations are recursively
built in an upward direction.

2.2 Decoding with a Tree-Based Attention
Mechanism

In generating the target words, we employ a se-
quential decoder with an input-feeding method
(Luong et al., 2015) and attention mechanism
(Bahdanau et al., 2015). The conditional proba-
bility of the j-th target word yj is calculated using

a non-linear function fsoftmax:

p(yj | y1, ..., yj−1, x, tr; θ) = fsoftmax(cj),
where cj is the composite hidden state, which con-
sists of a target hidden state sj and a context vector
dj :

cj = ftanh([sj , dj ]).

Given the previous target word yj−1, the concate-
nation of the previous hidden state sj−1 and the
previous context vector cj−1 (input-feeding) (Lu-
ong et al., 2015), sj , is calculated using a standard
sequential GRU network:

sj = fdecgru(yj−1, [sj−1, cj−1]).

The context vector dj is computed using an at-
tention model which is used to softly summarize
the attended part of the source-side representa-
tions. Eriguchi et al. (2016) adopted a tree-based
attention mechanism to consider both the word
and phrase vectors:

dj =
N∑
i=1

αj(i)hli +
N−1∑
k=1

αj(k)h
p
k, (2)

where hli is the i-th hidden state of the source word
at leaf level, and hpk is the k-th hidden state of the
source phrase. The weight αj(t) of node t is com-
puted by:

αj(t) =
exp(et)∑N

i=1 exp(e
l
i) +

∑N−1
k=1 exp(e

p
k)

et = (Va)T tanh(Uasj +Waht + ba),

where ht is the hidden state of the node. Va, Ua,
Wa and ba are the model parameters.

3 The Bidirectional Hierarchical Model

Although the tree-based encoder of Eriguchi et al.
(2016) has shown certain advantages in transla-
tion tasks involving distant language pairs, e.g.
English-Japanese, the representation of a phrase
relies solely on its child nodes, and the word rep-
resentation at leaf level only takes into account the
sequential information. We argue that the incor-
poration of more hierarchical information into the
representations may contribute to an improvement
in the translation. In particular, the use of global
information can help in distinguishing the differ-
ences between word meanings. Based on this hy-
pothesis, we propose an alternative architecture,
the bidirectional hierarchical model, to enhance
the source-side representations.

1434



h↓1 h
↓
2 h

↓
3 h

↓
4 h

↓
5 h

↓
6 h

↓
7 h

↓
8

h↓2,3 h
↓
4,5 h

↓
7,8

h↓6,8
h↓4,8

h↓2,8

h↓1,8

Figure 3: A top-down encoding process updates
the hidden states recursively from root to leaf
nodes. The red and blue lines denote the use of
different learning parameters.

3.1 Bidirectional Leaf-Node Encoding
As discussed in Section 1, the unidirectional re-
current neural network reads an input sequence
in order, from the first symbol to the last. In
order to generate leaf node annotation vectors
which jointly take into account both preceding and
following annotations, we exploit a bidirectional
RNN encoder (Bahdanau et al., 2015). The hidden
state of the i-th leaf node hli is the concatenation
of the forward and backward vectors:

hli = [
−→
h li,
←−
h li],

where
−→
h li is obtained by a rightward GRU, as

shown in Equation 1, and a leftward GRU calcu-
lates

←−
h li, as follows:

←−
h li = f

←
GRU (xi,

←−
h li−1),

where
←−
h li−1 is the previous hidden state.

3.2 Bidirectional Tree-Node Encoding
Since the hidden states of leaf nodes are derived
in a sequential, context-sensitive way, by gen-
erating phrase annotations in a bottom-up fash-
ion, the sequential context can be propagated to
tree nodes. However, the learned annotation vec-
tors still fail to capture global information from
the upper nodes. To enhance the representations
with global semantic information, we propose to
use a standard GRU recurrent network to update
representations in a top-down fashion, as shown
in Figure 3. The annotation vectors, which are
learned by the previous encoding steps, are fed to
the updating process.

First, we treat the bottom-up hidden state of root
h↑root, which covers the global meaning as well as

the syntactic information of the source sentence,
as the initial state of the top-down GRU network:

h↓root = h
↑
root.

Given an updated hidden state of the parent node
h↓i,j , the hidden states of left and right children h

↓
i,k

and h↓k+1,j are calculated as:

h↓i,k = f
ld
GRU (h

↑
i,k, h

↓
i,j)

h↓k+1,j = f
rd
GRU (h

↑
k+1,j , h

↓
i,j),

where h↑i,k and h
↑
k+1,j are the left and right

child annotation vectors generated via the bottom-
up tree-GRU network. Contrary to the simi-
lar top-down encoding for sentiment classifica-
tion (Kokkinos and Potamianos, 2017), which uses
same weighting parameters to handle both left and
right child nodes, f ldGRU and f

rd
GRU with different

parameters are applied in the proposed model to
distinguish the left and right structural informa-
tion. According to the definition of a GRU (Cho
et al., 2014b), f ldGRU uses an update gate z

↓
i,k, a

reset gate r↓i,k and a candidate activation h̃
↓
i,k to

generate h↓i,k, as follows:

z↓i,k = σ(W
ld
(z)h

↑
i,k + U

ld
(z)h

↓
i,j + b

ld
(z))

r↓i,k = σ(W
ld
(r)h

↑
i,k + U

ld
(r)h

↓
i,j + b

ld
(r))

h̃↓i,k = tanh(W
ld
(h)h

↑
i,k + U

ld
(h)(r

↓
i,k � h↓i,j) + bld(h))

h↓i,k = (1− z↓i,k)h↓i,j + z↓i,kh̃↓i,k, (3)
where W ld(·) and U

ld
(·) represent weight matrices,

and bld(·) denote bias vectors. f
rd
GRU is defined in

a similar way.
From a linguistic point of view, in the top-down

GRU network, the reset gate is able to retain the
useful global information and drop irrelevant in-
formation from the parent state h↓i,j , while the pro-
portions of the global context from the top-down
state h↓i,j , and the local context from the bottom-
up state h↑i,k are controlled by the update gate. As
it covers both the partial meaning of the phrase
and the whole meaning of the sentence, h↓i,k is re-
garded as the final representation of nodei,k:

hpi,k = h
↓
i,k.

With the propagation of information from root to
leaf nodes, the i-th leaf node representation is up-
dated as:

hli = h
↓
i .

1435



As each source-side hidden state of the leaf nodes
and tree nodes carries the hierarchical information
of the sentence, we interpret such an encoded state
as a hierarchical representation.

3.3 Handling Out-of-Vocabulary: Tree-Based
Rare Word Encoding

In NMT, the translation of rare words and un-
known words is an open problem, since the com-
putational cost increases with the size of the vo-
cabulary. Sennrich et al. (2016) proposed a sim-
ple and effective approach to handling out-of-
vocabulary by representing rare words as a se-
quence of sub-word units, which are segmented
using byte-pair encoding (BPE) (Gage, 1994).

x1 x2 x
1
3 x

2
3 x

3
3 x4 x5

h1 h2 h4 h5h13 h
2
3 h

3
3

h1,23

h1,33

Figure 4: Encoding sub-word units with an addi-
tional binary lexical tree, where x13, x

2
3, x

3
3 are the

sub-units of word x3.

We propose a variant tree-based rare word en-
coding approach which extends the tree-based
model to the sub-word level. Sub-word units
are encoded following an additional binary lexical
tree. For a sentence x = (x1, ..., xi, ..., xN ), BPE
segments the word xi into a sequence of sub-word
units (x1i , ..., x

n
i ). The binary lexical tree is sim-

ply built by composing two nodes in a rightwards
fashion, (((x1i , x

2
i ), x

3
i )...), x

n
i ), as shown in Fig-

ure 4. From the i-th leaf node, the original syn-
tactic tree is extended downwards using the binary
lexical tree, and the set of leaf nodes are replen-
ished as x = (x1, ..., x1i , x2i , ..., xni , ..., xN ). Sub-
word units can therefore be regarded as leaf nodes,
and can be encoded using the proposed encoder, as
illustrated in Figure 5. The experimental results in
Section 4.1 demonstrate the effectiveness of this
simple approach.

3.4 Decoder with Weighted Variant of
Attention Mechanism

Since each representation carries both local and
global information, in this case, attending fairly
to the lexical and phrase representations in each

I tak
e up a pos

i-
-tio

n in the roo
m
〈eos
〉

hl1 h
l
2 h

l
3 h

l
4 h

l
7 h

l
8 h

l
9 h

l
10h

l
5 h

l
6

hp2,3 h
p
5,6 h

p
8,9

hp4,6

hp4,9

hp2,9

hp1,9

hp7,9

Figure 5: Illustration of the bidirectional hierar-
chical encoder: representations are enhanced by
a bidirectional leaf-node encoding and a bidirec-
tional tree-node encoding. The green nodes indi-
cate the sub-word representations.

decoding step may cause the problem of over-
translation (repeatedly attending and translating
the same constituent of a sentence). An alterna-
tive approach is to balance the attentive informa-
tion between the lexical and phrase vectors in the
context vector. To effectively leverage these hi-
erarchical representations, we propose a weighted
variant of the tree-based attention mechanism (the
original is defined in Equation 2). Formally, the
calculation of the context vector dj at step j is
modified as:

dj = (1− βj)
n∑
i=1

αj(i)hli + βj
n−1∑
k=1

αj(k)h
p
k (4)

where βj ∈ [0, 1] is used to weight the expected
importance of the representations. Inspired by
work on a multi-modal NMT (Calixto et al., 2017)
which exploits a gating scalar (Xu et al., 2015) to
weight the image context vector, we use such a
scalar in our model in order to dynamically adapt
the weighting scalar. The gating scalar βj at step
j is calculated by :

βj = σ(Wβcj−1 + bβ),

where Wβ and bβ represent the model parame-
ters. In contrast with α, which denotes the cor-
respondence between each source annotation and
the current target hidden state, β is dominated by
the target composite hidden state alone. In other
words, β is a time-dependent scalar in relation to
the current target word, and therefore enables the
attention model to explicitly quantify how far the

1436



leaf and no-leaf states contribute to the word pre-
diction at each time step. In the proposed model,
the phrase and lexical context vectors are learned
by a single attention model, meaning that they
are dependent, and the gating scalar weights the
phrase and lexical context vectors in complemen-
tary fashion, as shown in Equation 4. This dis-
tinguishes the model from that introduced by Cal-
ixto et al. (2017), in which the context vectors of
the source sentence and image (bi-modal) are mea-
sured using two independent attention models and
the gating scalar is merely used to weight the im-
age context vector.

4 Experiments

4.1 Data

Training Dev Test
LDC En-Zh mt08 mt04 mt05 mt06
1,435,575 1,357 1,788 1,082 1,664

Table 1: Data used in the experiments.

We evaluate the proposed model on an English-
to-Chinese translation task. For reasons of com-
putational efficiency, we extracted 1.4M sentence
pairs, in which the maximum length of the sen-
tence was 40, from the LDC parallel corpus3 as
our training data. The models were developed
using NIST mt08 data and were examined using
NIST mt04, mt05, and mt06 data. The num-
ber of sentences in each dataset is shown in Ta-
ble 1. On the English side, we used the constituent
parser (Zeng et al., 2014, 2015) to produce a bi-
nary syntactic tree for each sentence, in constrast
to the use of the HPSG parser by Eriguchi et al.
(2016). On the Chinese side, the sentences are
segmented using the Chinese word segmentation
toolkit of NiuTrans (Xiao et al., 2012).

To avoid data sparsity, words referring to time,
date and number, which are low in frequency, are
generalized as ‘$time’, ‘$date’ and ‘$number’. In
addition, as described in Section 3.3, the vocab-
ularies are further compressed by segmenting the
rare words into sub-word units using BPE.

4.2 Experimental Settings
As shown in Table 2, which gives the statistics of
the token types, we limit the source and target vo-

3Our training data was selected from LDC2000T46,
LDC2000T50, LDC2003E14, LDC2004T08, LDC2004T08
and LDC2005T10.

Training Set Original Generalization BPE
|V | in En 159k 120k 40k
|V | in Zh 198k 125k 40k

Table 2: The vocabulary size of the training set
before and after applying the generalization and
BPE segmentation.

cabulary size to 40,000, in order to cover all the
English and Chinese tokens. The dimensions of
word embedding and hidden layer are respectively
set as 620 and 1,000. Due to the concatenation in
the bidirectional leaf-node encoding, the dimen-
sions of the forward and backward vectors, which
are half of those of the other hidden states, are set
to 500. In order to prevent over-fitting, the train-
ing data is shuffled following each epoch. More-
over, the model parameters are optimized using
AdaDelta (Zeiler, 2012), due to its capability for
dynamically adapting the learning rate. We set the
mini-batch size to 16 and the beam search size to
5. The accuracy of the translation relative to a ref-
erence is assessed using the BLEU metric (Pap-
ineni et al., 2002). In order to give an equitable
comparison, all the NMT models used for com-
parison are implemented or re-implemented using
GRU in our code, based on dl4mt4.

4.3 Enhanced Hierarchical Representations

Firstly, the effectiveness of the enhanced hierar-
chical representations is evaluated through a set of
experiments, the results of which are summarized
in Table 3.

Compared with the original tree-based en-
coder (Eriguchi et al., 2016), the model with bidi-
rectional leaf-node encoding (described in Sec-
tion 3.1) shows better performance. This also re-
veals that the future context at leaf level can con-
tribute to word prediction. Secondly, although
the representations of leaf nodes are learned in
a sequential, context-sensitive way, the transla-
tion quality is further improved by considering the
global semantic information in the top-down en-
coding (Section 3.2).

By incorporating the above enhancements into
the model, the proposed hierarchical encoder
yields significant improvements over both the se-
quential and the tree-based models. The problem
of OOV is alleviated by further extending the tree-

4https://github.com/nyu-dl/
dl4mt-tutorial

1437



Model BPE # of params MT04 MT05 MT06 Dev.
sequential encoder no 86.8M 31.26 23.98 24.02 17.20

+ sequential rare word encoding yes 86.8M 32.54 25.09 25.07 18.19
+ tree-based rare word encoding yes 104.1M 32.56 25.30 24.96 18.33

tree-based encoder no 95.0M 31.90 24.68 24.40 17.63
+ bidirectional leaf-node encoding no 92.0M 32.13 24.94 25.02 18.12
+ top-down encoding no 101.1M 32.85 25.37 25.30 18.26
+ tree-based rare word encoding yes 95.0M 33.02 25.62 25.24 18.59

hierarchical encoder (β = 0.5) no 104.1M 32.91↑ 25.55↑ 25.52↑ 18.46↑
hierarchical encoder (β = 0.5) yes 104.1M 33.81⇑ 26.47⇑ 26.31⇑ 19.41⇑

+ gating scalar yes 105.1M 34.33⇑ 26.72⇑ 26.58⇑ 20.10⇑
Table 3: Translation results for the various models. The first column shows the models; the second
column indicates whether the corresponding experiment uses BPE data. The number of parameters (M
= millions) in each model is given in the third column. The remaining columns are the translation
accuracies for the test sets and development set, evaluated using BLEU scores (%). “↑ / ⇑”: indicates
that the hierarchical encoder is significantly better than the vanilla tree-based encoder (p < 0.05/p <
0.01).

based model to sub-word level (Section 3.3). In
addition, we evaluate our tree-based rare word en-
coding method against the conventional rare word
encoding (Sennrich et al., 2016) using the sequen-
tial encoder (Bahdanau et al., 2015). The empir-
ical results confirm that our proposed tree-based
BPE method achieves performance comparable to
that of the standard BPE in the sequential model,
but is applicable to the tree-based NMT model.

Overall, the proposed hierarchical encoder
has demonstrated the ability to effectively model
source-side representations from both the sequen-
tial and structural context. The NMT systems
based on the proposed model significantly outper-
form those of conventional models using the se-
quential encoder and the tree-based encoder.

4.4 Weighted Attention Model
As discussed in Section 3.4, in order to effectively
leverage hierarchical representations in generating
the target word, we adopt a variant weighted tree-
based attention mechanism which incorporates a
scalar to control the proportion of conditional in-
formation between the word and phrase vectors.
By manually or automatically varying the weight
β, the utilization of the weighted attention model
is assessed for four cases:

• β = 0.0: We manually set the weight of
phrase vectors to 0.0; in other words, the de-
coder is forced to ignore the phrase vectors.
The final translation is therefore generated by
merely summarizing the leaf vectors.

• β = 0.5: The representations of non-leaf
nodes and leaf nodes participate equally in
the translation process. The decoder of this
case therefore employs the same attention
mechanism as that of the original model (Sec-
tion 2.2).

• β = 1.0: In the reverse of the first case, the
weight of the leaf nodes is manually set to
0.0. Thus, only the phrase vectors are used to
predict the target words.

• Gating scalar (GS): A gating scalar is used
for dynamically learning to control the pro-
portion in which the lexical and phrase con-
texts contribute to the generation of the target
words (Section 3.4).

Model BLEU Perplexity Avg. Length
β = 1.0 17.16 98.65 21.13
β = 0.5 19.41 94.73 23.08
β = 0.0 19.83 94.68 23.33

GS 20.10 94.18 23.24

Table 4: Translation results for the development
set. The last column indicates the average length
of translation sentences, and the average length of
reference sentences is 23.19.

The experimental results are shown in Table 4.
The model which attends only to lexical annota-
tion vectors (β = 0.0) gives slightly better per-
formance than that which uses equal weights for

1438



The organization wouldn’t use armed forces in areas outside its member states <eos>

0.00 0.00 0.10 0.38 1.37 0.54 2.85 5.17 15.69 15.77 29.32 15.47 0.31

0.06 0.41 0.76
0.55 5.231.90

1.004.95
0.240.17

0.93

α
(10−2)

Our: 该 组织 不会 在在在成员国以外 的 地区使用武力

Ref: 该 组织 不会 在 成员国 以外 的 地区 动用 军队

tr-enc: 该 组织 不会 在 成员国 境外 使用 武力

sq-enc: 该 组织 不会 使用 其 成员国 以外 的 武装力量

β: 0.17 0.14 0.22 0.22 0.27 0.22 0.19 0.44 0.14 0.56

Figure 6: Translations of an English sentence output using the NMT models with bidirectional hierar-
chical model (our), sequential encoder (seq-enc) and original tree-based encoder (tr-enc). Ref indicates
the reference Chinese sentence. The attention scores (α), which are noted over the source-side syntactic
tree, are output by the bidirectional hierarchical model at the step where the fourth target word “在” is
translated. The sequence of scores β denote the value of the gating scalar at each translation step.

lexical and phrase vectors (β = 0.5). The use
of global information contributes to distinguish-
ing the differences between word meanings, al-
though the similar semantic information in the
lexical and phrase representations aggravates the
over-translation problem observed in the transla-
tion results. However, we found that the model
which attends only to phrase representations tends
to generate shorter translation of an average of
21.13 words in length, as shown in the last column
of the first row of Table 4. Furthermore, the model
that neglects the leaf representations (β = 1.0)
is likely to underperform the others that are also
conditioned on the leaf nodes. Even though the
phrase representations are derived from the lexical
level via a bottom-up encoding, we believe it is
unable to fully capture the lexical information of
the source sentence. Through the use of the gating
scalar, the hierarchical model achieves progressive
improvements, as shown in Tables 3 and 4, the
problem of over-translation is also alleviated. The
representations of non-leaf nodes can be regarded
as supplements in the translation process.

5 Qualitative Analysis

Figure 6 shows an English sentence and its binary
tree representation, together with the correspond-
ing Chinese translations produced by the different
NMT models. All the models successfully give
the correct Chinese translation “该 组织 不会”
for the first three words of the English sentence
“the organization wouldn’t”. Differences appear
in the translation of the fourth word, and these lead
to markedly different meanings. The translation

“使用 其 成员国 以外 的 武装力量” output by
the sequential model, means “use the armed forces
other than its member states” where “other than
its member states” is incorrectly interpreted as a
complement to “armed forces”. This is caused by
the intrinsic limitations of the sequential model,
whereby it is unable to properly interpret the syn-
tactic relationship of words. By explicitly incorpo-
rating the syntactic information, both the proposed
hierarchical model and the tree-based model can
accurately attend to the dashed section of Figure 6,
and the translations can be correctly generated to
reflect the meaning of the source sentence. The
distinction between the translations produced by
the original tree-based model and our hierarchical
model is the interpretation of the words “areas out-
side”. The tree-based model interprets it into “境
外 (outside)”, while our model correctly translates
it into “以外的地区 (areas outside)”. We believe
that, with the help of global and local contextual
information, our model is able to capture the short
as well as long range dependencies.

We conducted an in-depth analysis of the BPE
segmented units of rare words. It was observed
that the sub-word units could be categorized into
three groups. The first group of units involve
the phonetic Romanization (Pinyin) of Chinese.
In translation, these are simply transliterated into
the corresponding Chinese characters. As shown
in the second row of Table 5, “Liu/jing/min” is
a person’s name. The segmented units are the
phonetic representations. Both models can suc-
cessfully transliterate this into the Chinese equiv-
alent, “刘/敬/民”. The second group of sub-

1439



Source Reference Hierarchical Sequential
liu/jing/min 刘/敬/民 刘/敬/民 刘/敬/民

Liú/jı̀ng/mı́n Liú/jı̀ng/mı́n Liú/jı̀ng/mı́n
adventur/er 探险家 探险家 探险者

Tàn xiǎn jiā Tàn xiǎn jiā Tàn xiǎn zhě
hi/k/ed 上调 上升 发生

Shàng tiáo Shàng shēng Fā shēng

Table 5: Translation examples of sub-words,
where ‘/’ indicates a separation between sub-word
units. The first two columns show the segmented
words and their Chinese references. The last two
columns report the translations given by the hier-
archical and sequential models respectively.

word units are likely to represent the word mor-
phemes. The words are segmented into sub-word
units, which are to some extent close to the lin-
guistic word stems and suffixes. For example,
the word “adventurer” is segmented into “adven-
tur/er”, which is correctly translated into the Chi-
nese translations “探险/家” and “探险/者” respec-
tively by the hierarchical and sequential models,
while the third group of sub-word units offer no
linguistic interpretation. It is easy to see, using
the BPE algorithm, that the identification of sub-
word units is merely based on their frequency in
the training data, with the result that not all units
are well-formed linguistic morphemes. However,
an interesting finding arises regarding the transla-
tion of these segmented units. In the sequential
model, the word is incorrectly translated; how-
ever, it can be correctly translated by the hierar-
chical model. Taking “hi/k/ed” as an example,
the sequential model gives an incorrect translation
“发生(happened)”, while the hierarchical model
translates it into “上升(rise)” which is a synonym
of “hiked”. This result indicates that in our hi-
erarchical model, the parent node of hierarchical
representation for sub-word units “hi/k/ed” is bet-
ter able to capture the meaning of the word as a
whole; this cannot be captured independently by
the sequential model.

6 Conclusion

In this paper, we propose an improved NMT sys-
tem with a novel bidirectional hierarchical en-
coder, which enhances the source-side representa-
tions of a sentence, that is, both phrases and words,
with local and global context information. By in-
troducing a tree-based rare word encoding, the hi-

erarchical model is extended to sub-word level in
order to alleviate the problem of OOVs. To ef-
fectively leverage the enhanced hierarchical repre-
sentations, we also propose a weighted variant of
the attention model which dynamically adjusts the
proportion of conditional information between the
lexical and phrase annotation vectors. Experimen-
tal results for NIST English-Chinese translation
tasks demonstrate that the proposed model signif-
icantly outperforms the vanilla tree-based and se-
quential NMT models.

Acknowledgments

This work was supported in part by the Na-
tional Natural Science Foundation of China
(Grant No. 61672555), a Multiyear Research
Grant from the University of Macau (Grant
Nos. MYRG2017-00087-FST, MYRG2015-
00175-FST and MYRG2015-00188-FST) and the
Science and Technology Development Fund of
Macau (Grant No. 057/2014/A). The work of
Tong Xiao and Jingbo Zhu was supported in part
by the National Natural Science Foundation of
China (Grant Nos. 61672138 and 61432013), the
Fundamental Research Funds for the Central Uni-
versities, and the Opening Project of Beijing Key
Laboratory of Internet Culture and Digital Dis-
semination Research.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations.

Iacer Calixto, Qun Liu, and Nick Campbell. 2017.
Doubly-Attentive Decoder for Multi-modal Neural
Machine Translation. CoRR, abs/1702.01287.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry
Bahdanau, and Yoshua Bengio. 2014a. On
the Properties of Neural Machine Translation:
Encoder-Decoder Approaches. In Proceedings of
SSST@EMNLP 2014, Eighth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
pages 103–111.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014b. Learn-
ing Phrase Representations using RNN Encoder-
Decoder for Statistical Machine Translation. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1724–
1734.

1440



Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical Evaluation
of Gated Recurrent Neural Networks on Sequence
Modeling. In Advances in neural information pro-
cessing systems (NIPS 2014) Deep Learning and
Representation Learning Workshop.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-Sequence Attentional Neu-
ral Machine Translation. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 823–833.

Philip Gage. 1994. A New Algorithm for Data Com-
pression. The C Users Journal, 12(2):23–38.

Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.
2000. Learning to Forget: Continual Prediction with
LSTM. Neural Computation, 12(10):2451–2471.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Rafal Józefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An Empirical Exploration of Re-
current Network Architectures. In Proceedings
of the 32nd International Conference on Machine
Learning, pages 2342–2350.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709.

Filippos Kokkinos and Alexandros Potamianos. 2017.
Structural Attention Neural Networks for Improved
Sentiment Analysis. CoRR, abs/1701.01811.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing, pages 1412–1421.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311–318.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional Recurrent Neural Networks. IEEE Transac-
tions on Signal Processing, 45(11):2673–2681.

Rico Sennrich and Barry Haddow. 2016. Linguistic In-
put Features Improve Neural Machine Translation.
In Proceedings of the First Conference on Machine
Translation, pages 83–90.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, pages 1715–1725.

Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill
Byrne. 2016. Syntactically Guided Neural Machine
Translation. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, pages 299–305.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Advances in neural information process-
ing systems (NIPS 2014), pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved Semantic Representa-
tions From Tree-Structured Long Short-Term Mem-
ory Networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Feder-
ation of Natural Language Processing, pages 1556–
1566.

Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012. NiuTrans: An Open Source Toolkit for
Phrase-based and Syntax-based Machine Transla-
tion. In Proceedings of the 52th Annual Meeting of
the Association for Computational Linguistics, Sys-
tem Demonstrations, pages 19–24.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C Courville, Ruslan Salakhutdinov, Richard S
Zemel, and Yoshua Bengio. 2015. Show, Attend and
Tell: Neural Image Caption Generation with Visual
Attention. In Proceedings of the 32nd International
Conference on Machine Learning, pages 77–81.

Matthew D. Zeiler. 2012. ADADELTA: An Adaptive
Learning Rate Method. CoRR, abs/1212.5701.

Xiaodong Zeng, Derek F. Wong, Lidia S. Chao,
and Isabel Trancoso. 2015. Graph-Based Lexi-
con Regularization for PCFG With Latent Annota-
tions. IEEE/ACM Transactions on Audio, Speech,
and Language Processing, 23(3):441–450.

Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Isabel
Trancoso, Liangye He, and Qiuping Huang. 2014.
Lexicon Expansion for Latent Variable Grammars.
Pattern Recognition Letters, (42):47–55.

Yao Zhou, Cong Liu, and Yan Pan. 2016. Modelling
Sentence Pairs with Tree-structured Attentive En-
coder. In Proceedings of 26th International Con-
ference on Computational Linguistics, pages 2912–
2922.

1441


