



















































Unsupervised Ranking Model for Entity Coreference Resolution


Proceedings of NAACL-HLT 2016, pages 1012–1018,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Unsupervised Ranking Model for Entity Coreference Resolution

Xuezhe Ma and Zhengzhong Liu and Eduard Hovy
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213, USA

{xuezhem, liu}@cs.cmu.edu, ehovy@cmu.edu

Abstract

Coreference resolution is one of the first stages
in deep language understanding and its impor-
tance has been well recognized in the natu-
ral language processing community. In this
paper, we propose a generative, unsupervised
ranking model for entity coreference resolu-
tion by introducing resolution mode variables.
Our unsupervised system achieves 58.44% F1
score of the CoNLL metric on the English data
from the CoNLL-2012 shared task (Pradhan et
al., 2012), outperforming the Stanford deter-
ministic system (Lee et al., 2013) by 3.01%.

1 Introduction

Entity coreference resolution has become a critical
component for many Natural Language Processing
(NLP) tasks. Systems requiring deep language un-
derstanding, such as information extraction (Wellner
et al., 2004), semantic event learning (Chambers and
Jurafsky, 2008; Chambers and Jurafsky, 2009), and
named entity linking (Durrett and Klein, 2014; Ji et
al., 2014) all benefit from entity coreference infor-
mation.

Entity coreference resolution is the task of iden-
tifying mentions (i.e., noun phrases) in a text or
dialogue that refer to the same real-world entities.
In recent years, several supervised entity corefer-
ence resolution systems have been proposed, which,
according to Ng (2010), can be categorized into
three classes — mention-pair models (McCarthy
and Lehnert, 1995), entity-mention models (Yang
et al., 2008a; Haghighi and Klein, 2010; Lee et
al., 2011) and ranking models (Yang et al., 2008b;

Durrett and Klein, 2013; Fernandes et al., 2014)
— among which ranking models recently obtained
state-of-the-art performance. However, the manu-
ally annotated corpora that these systems rely on are
highly expensive to create, in particular when we
want to build data for resource-poor languages (Ma
and Xia, 2014). That makes unsupervised ap-
proaches, which only require unannotated text for
training, a desirable solution to this problem.

Several unsupervised learning algorithms have
been applied to coreference resolution. Haghighi
and Klein (2007) presented a mention-pair non-
parametric fully-generative Bayesian model for un-
supervised coreference resolution. Based on this
model, Ng (2008) probabilistically induced corefer-
ence partitions via EM clustering. Poon and Domin-
gos (2008) proposed an entity-mention model that
is able to perform joint inference across mentions
by using Markov Logic. Unfortunately, these un-
supervised systems’ performance on accuracy sig-
nificantly falls behind those of supervised systems,
and are even worse than the deterministic rule-based
systems. Furthermore, there is no previous work
exploring the possibility of developing an unsuper-
vised ranking model which achieved state-of-the-
art performance under supervised settings for entity
coreference resolution.

In this paper, we propose an unsupervised genera-
tive ranking model for entity coreference resolution.
Our experimental results on the English data from
the CoNLL-2012 shared task (Pradhan et al., 2012)
show that our unsupervised system outperforms the
Stanford deterministic system (Lee et al., 2013) by
3.01% absolute on the CoNLL official metric. The

1012



contributions of this work are (i) proposing the first
unsupervised ranking model for entity coreference
resolution. (ii) giving empirical evaluations of this
model on benchmark data sets. (iii) considerably
narrowing the gap to supervised coreference reso-
lution accuracy.

2 Unsupervised Ranking Model

2.1 Notations and Definitions

In the following, D = {m0,m1, . . . ,mn} repre-
sents a generic input document which is a sequence
of coreference mentions, including the artificial root
mention (denoted by m0). The method to detect
and extract these mentions is discussed later in Sec-
tion 2.6. Let C = {c1, c2, . . . , cn} denote the
coreference assignment of a given document, where
each mention mi has an associated random vari-
able ci taking values in the set {0, i, . . . , i − 1};
this variable specifiesmi’s selected antecedent (ci ∈
{1, 2, . . . , i − 1}), or indicates that it begins a new
coreference chain (ci = 0).

2.2 Generative Ranking Model

The following is a straightforward way to build a
generative model for coreference:

P (D,C) = P (D|C)P (C)
=

n∏
j=1

P (mj |mcj )
n∏
j=1

P (cj |j) (1)

where we factorize the probabilities P (D|C) and
P (C) into each position j by adopting appropri-
ate independence assumptions that given the coref-
erence assignment cj and corresponding corefer-
ent mention mcj , the mention mj is independent
with other mentions in front of it. This indepen-
dent assumption is similar to that in the IBM 1
model on machine translation (Brown et al., 1993),
where it assumes that given the corresponding En-
glish word, the aligned foreign word is independent
with other English and foreign words. We do not
make any independent assumptions among different
features (see Section 2.4 for details).

Inference in this model is efficient, because we
can compute cj separately for each mention:

c∗j = argmax
cj

P (mj |mcj )P (cj |j)

The model is a so-called ranking model because it
is able to identify the most probable candidate an-
tecedent given a mention to be resolved.

2.3 Resolution Mode Variables

According to previous work (Haghighi and Klein,
2009; Ratinov and Roth, 2012; Lee et al., 2013),
antecedents are resolved by different categories of
information for different mentions. For example,
the Stanford system (Lee et al., 2013) uses string-
matching sieves to link two mentions with similar
text and precise-construct sieve to link two men-
tions which satisfy special syntactic or semantic
relations such as apposition or acronym. Moti-
vated by this, we introduce resolution mode vari-
ables Π = {π1, . . . , πn}, where for each mention
j the variable πj ∈ {str, prec, attr} indicates in
which mode the mention should be resolved. In
our model, we define three resolution modes —
string-matching (str), precise-construct (prec), and
attribute-matching (attr) — and Π is deterministic
when D is given (i.e. P (Π|D) is a point distribu-
tion). We determine πj for each mention mj in the
following way:

• πj = str, if there exists a mention mi, i < j
such that the two mentions satisfy the String
Match sieve, the Relaxed String Match sieve,
or the Strict Head Match A sieve in the Stanford
multi-sieve system (Lee et al., 2013).

• πj = prec, if there exists a mention mi, i < j
such that the two mentions satisfy the Speaker
Identification sieve, or the Precise Constructs
sieve.

• πj = attr, if there is no mention mi, i < j
satisfies the above two conditions.

Now, we can extend the generative model in Eq. 1
to:

P (D,C) = P (D,C,Π)

=
n∏
j=1

P (mj |mcj , πj)P (cj |πj , j)P (πj |j)

where we define P (πj |j) to be uniform distribution.
We model P (mj |mcj , πj) and P (cj |πj , j) in the fol-

1013



Mode π Feature Description
prec Mention Type the type of a mention. We use three mention types: Proper,Nominal, Pronoun

str

Mention Type the same as the mention type feature under prec mode.
Exact Match boolean feature corresponding to String Match sieve in Stanford system.

Relaxed Match boolean feature corresponding to Relaxed String Match sieve in Stanford system.
Head Match boolean feature corresponding to Strict Head Match A sieve in Stanford system.

attr

Mention Type the same as the mention type feature under prec mode.
Number the number of a mention similarly derived from Lee et al. (2013).
Gender the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009).
Person the person attribute from Lee et al. (2013). We assign person attributes to all mentions, not only pronouns.

Animacy the animacy attribute same as Lee et al. (2013).
Semantic Class semantic classes derived from WordNet (Soon et al., 2001).

Distance sentence distance between the two mentions. This feature is for parameter q(k|j, π)
Table 1: Feature set for representing a mention under different resolution modes. The Distance feature is for parameter q, while all
other features are for parameter t.

Algorithm 1: Learning Model with EM
1 Initialization: Initialize θ0 = {t0, q0}
2 for t = 0 to T do
3 set all counts c(. . .) = 0
4 for each document D do
5 for j = 1 to n do
6 for k = 0 to j − 1 do
7 Ljk =

t(mj |mk,πj)q(k|πj ,j)
j−1∑
i=0

t(mj |mi,πj)q(i|πj ,j)

8 c(mj ,mk, πj) += Ljk
9 c(mk, πj) += Ljk

10 c(k, j, πj) += Ljk
11 c(j, πj) += Ljk

// Recalculate the parameters

12 t(m|m′, π) = c(m,m′,π)c(m′,π)
13 q(k, j, π) = c(k,j,π)c(j,π)

lowing way:

P (mj |mcj , πj) = t(mj |mcj , πj)
P (cj |πj , j) =

{
q(cj |πj , j) πj = attr
1
j otherwise

where θ = {t, q} are parameters of our model. Note
that in the attribute-matching mode (πj = attr)
we model P (cj |πj , j) with parameter q, while in
the other two modes, we use the uniform distribu-
tion. It makes sense because the position informa-
tion is important for coreference resolved by match-
ing attributes of two mentions such as resolving pro-
noun coreference, but not that important for those
resolved by matching text or special relations like
two mentions referring the same person and match-
ing by the name.

Corpora # Doc # Sent # Word # Entity # Mention
Gigaword 3.6M 75.4M 1.6B - -
ON-Dev 343 9,142 160K 4,546 19,156
ON-Test 348 9,615 170K 4,532 19,764

Table 2: Corpora statistics. “ON-Dev” and “ON-Test” are the
development and testing sets of the OntoNotes corpus.

2.4 Features

In this section, we describe the features we use to
represent mentions. Specifically, as shown in Ta-
ble 1, we use different features under different reso-
lution modes. It should be noted that only the Dis-
tance feature is designed for parameter q, all other
features are designed for parameter t.

2.5 Model Learning

For model learning, we run EM algorithm (Demp-
ster et al., 1977) on our Model, treating D as ob-
served data and C as latent variables. We run EM
with 10 iterations and select the parameters achiev-
ing the best performance on the development data.
Each iteration takes around 12 hours with 10 CPUs
parallelly. The best parameters appear at around the
5th iteration, according to our experiments.The de-
tailed derivation of the learning algorithm is shown
in Appendix A. The pseudo-code is shown is Algo-
rithm 1. We use uniform initialization for all the pa-
rameters in our model.

Several previous work has attempted to use
EM for entity coreference resolution. Cherry and
Bergsma (2005) and Charniak and Elsner (2009)
applied EM for pronoun anaphora resolution. Ng
(2008) probabilistically induced coreference parti-
tions via EM clustering. Recently, Moosavi and
Strube (2014) proposed an unsupervised model uti-

1014



CoNLL’12 English development data CoNLL’12 English test data

MUC B3 CEAFm CEAFe Blanc CoNLL MUC B3 CEAFm CEAFe Blanc CoNLL

MIR 65.39 54.89 – 51.36 – 57.21 64.64 52.52 – 49.11 – 55.42

Stanford 64.96 54.49 59.39 51.24 56.03 56.90 64.71 52.26 56.01 49.32 53.92 55.43

Multigraph 66.22 56.41 60.87 52.61 58.15 58.41 65.41 54.38 58.60 50.21 56.03 56.67

Our Model 67.89 57.83 62.11 53.76 60.58 59.83 67.69 55.86 59.66 51.75 57.78 58.44

IMS 67.15 55.19 58.86 50.94 56.22 57.76 67.58 54.47 58.17 50.21 55.41 57.42

Latent-Tree 69.46 57.83 – 54.43 – 60.57 70.51 57.58 – 53.86 – 60.65

Berkeley 70.44 59.10 – 55.57 – 61.71 70.62 58.20 – 54.80 – 61.21

LaSO 70.74 60.03 65.01 56.80 – 62.52 70.72 58.58 63.45 59.40 – 61.63

Latent-Strc 72.11 60.74 – 57.72 – 63.52 72.17 59.58 – 55.67 – 62.47

Model-Stack 72.59 61.98 – 57.58 – 64.05 72.59 60.44 – 56.02 – 63.02

Non-Linear 72.74 61.77 – 58.63 – 64.38 72.60 60.52 – 57.05 – 63.39

Table 3: F1 scores of different evaluation metrics for our model, together with two deterministic systems and one unsupervised
system as baseline (above the dashed line) and seven supervised systems (below the dashed line) for comparison on CoNLL 2012

development and test datasets.

lizing the most informative relations and achieved
competitive performance with the Stanford system.

2.6 Mention Detection

The basic rules we used to detect mentions are simi-
lar to those of Lee et al. (2013), except that their sys-
tem uses a set of filtering rules designed to discard
instances of pleonastic it, partitives, certain quanti-
fied noun phrases and other spurious mentions. Our
system keeps partitives, quantified noun phrases and
bare NP mentions, but discards pleonastic it and
other spurious mentions.

3 Experiments

3.1 Experimental Setup

Datasets. Due to the availability of readily parsed
data, we select the APW and NYT sections of Giga-
word Corpus (years 1994-2010) (Parker et al., 2011)
to train the model. Following previous work (Cham-
bers and Jurafsky, 2008), we remove duplicated
documents and the documents which include fewer
than 3 sentences. The development and test data
are the English data from the CoNLL-2012 shared
task (Pradhan et al., 2012), which is derived from
the OntoNotes corpus (Hovy et al., 2006). The cor-
pora statistics are shown in Table 2. Our system is
evaluated with automatically extracted mentions on
the version of the data with automatic preprocessing
information (e.g., predicted parse trees).

Evaluation Metrics. We evaluate our model
on three measures widely used in the literature:
MUC (Vilain et al., 1995), B3 (Bagga and Bald-
win, 1998), and Entity-based CEAF (CEAFe) (Luo,
2005). In addition, we also report results on an-
other two popular metrics: Mention-based CEAF
(CEAFm) and BLANC (Recasens and Hovy, 2011).
All the results are given by the latest version of
CoNLL-2012 scorer 1

3.2 Results and Comparison

Table 3 illustrates the results of our model together
as baseline with two deterministic systems, namely
Stanford: the Stanford system (Lee et al., 2011)
and Multigraph: the unsupervised multigraph sys-
tem (Martschat, 2013), and one unsupervised sys-
tem, namely MIR: the unsupervised system using
most informative relations (Moosavi and Strube,
2014). Our model outperforms the three baseline
systems on all the evaluation metrics. Specifically,
our model achieves improvements of 2.93% and
3.01% on CoNLL F1 score over the Stanford sys-
tem, the winner of the CoNLL 2011 shared task, on
the CoNLL 2012 development and test sets, respec-
tively. The improvements on CoNLL F1 score over
the Multigraph model are 1.41% and 1.77% on the
development and test sets, respectively. Comparing

1http://conll.cemantix.org/2012/
software.html

1015



with the MIR model, we obtain significant improve-
ments of 2.62% and 3.02% on CoNLL F1 score.

To make a thorough empirical comparison with
previous studies, Table 3 (below the dashed line)
also shows the results of some state-of-the-art su-
pervised coreference resolution systems — IMS:
the second best system in the CoNLL 2012 shared
task (Björkelund and Farkas, 2012); Latent-Tree:
the latent tree model (Fernandes et al., 2012) ob-
taining the best results in the shared task; Berkeley:
the Berkeley system with the final feature set (Dur-
rett and Klein, 2013); LaSO: the structured percep-
tron system with non-local features (Björkelund and
Kuhn, 2014); Latent-Strc: the latent structure sys-
tem (Martschat and Strube, 2015); Model-Stack:
the entity-centric system with model stacking (Clark
and Manning, 2015); and Non-Linear: the non-
linear mention-ranking model with feature represen-
tations (Wiseman et al., 2015). Our unsupervised
ranking model outperforms the supervised IMS sys-
tem by 1.02% on the CoNLL F1 score, and achieves
competitive performance with the latent tree model.
Moreover, our approach considerably narrows the
gap to other supervised systems listed in Table 3.

4 Conclusion

We proposed a new generative, unsupervised rank-
ing model for entity coreference resolution into
which we introduced resolution mode variables to
distinguish mentions resolved by different cate-
gories of information. Experimental results on the
data from CoNLL-2012 shared task show that our
system significantly improves the accuracy on dif-
ferent evaluation metrics over the baseline systems.

One possible direction for future work is to dif-
ferentiate more resolution modes. Another one is to
add more precise or even event-based features to im-
prove the model’s performance.

Acknowledgements

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of DARPA.

References
Amit Bagga and Breck Baldwin. 1998. Algorithms

for scoring coreference chains. In The first interna-
tional conference on language resources and evalu-
ation workshop on linguistics coreference, volume 1,
pages 563–566. Citeseer.

Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In Proceedings
of ACL-2006, pages 33–40, Sydney, Australia, July.
Association for Computational Linguistics.

Anders Björkelund and Richárd Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of EMNLP-CoNLL-
2012 - Shared Task, pages 49–55, Jeju Island, Korea,
July. Association for Computational Linguistics.

Anders Björkelund and Jonas Kuhn. 2014. Learning
structured perceptrons for coreference resolution with
latent antecedents and non-local features. In Proceed-
ings of ACL-2014, pages 47–57, Baltimore, Maryland,
June. Association for Computational Linguistics.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational linguistics, 19(2):263–311.

Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of ACL-2008: HLT, pages 789–797, Columbus,
Ohio, June. Association for Computational Linguis-
tics.

Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of ACL-2009, pages 602–610,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.

Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In Proceedings of EACL
2009, pages 148–156, Athens, Greece, March.

Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution. In
Proceedings of CoNLL-2005, pages 88–95, Ann Ar-
bor, Michigan, June.

Kevin Clark and Christopher D. Manning. 2015. Entity-
centric coreference resolution with model stacking. In
Proceedings of ACL-IJCNLP-2015, pages 1405–1415,
Beijing, China, July. Association for Computational
Linguistics.

Arthur P Dempster, Nan M Laird, and Donald B Rubin.
1977. Maximum likelihood from incomplete data via
the em algorithm. Journal of the royal statistical soci-
ety. Series B (methodological), pages 1–38.

Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In Pro-
ceedings of EMNLP-2013, pages 1971–1982, Seattle,

1016



Washington, USA, October. Association for Computa-
tional Linguistics.

Greg Durrett and Dan Klein. 2014. A joint model for
entity analysis: Coreference, typing, and linking. In
Proceedings of the Transactions of the Association for
Computational Linguistics.

Eraldo Fernandes, Cı́cero dos Santos, and Ruy Milidiú.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Proceedings of EMNLP-CoNLL-2012 - Shared Task,
pages 41–48, Jeju Island, Korea, July. Association for
Computational Linguistics.

Eraldo Rezende Fernandes, Cı́cero Nogueira dos Santos,
and Ruy Luiz Milidiú. 2014. Latent trees for corefer-
ence resolution. Computational Linguistics.

Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of ACL-2007, pages 848–855,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.

Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP-2009, pages 1152–1161, Sin-
gapore, August. Association for Computational Lin-
guistics.

Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of NAACL-2010, pages 385–393, Los Angeles,
California, June. Association for Computational Lin-
guistics.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of NAACL-2006,
pages 57–60, New York City, USA, June. Association
for Computational Linguistics.

Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from web-scale n-grams for un-
supervised person mention detection. In Proceedings
of PACLIC-2009, pages 220–229.

Heng Ji, HT Dang, J Nothman, and B Hachey. 2014.
Overview of tac-kbp2014 entity discovery and linking
tasks. In Proc. Text Analysis Conference (TAC2014).

Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
CoNLL-2011: Shared Task, pages 28–34, Portland,
Oregon, USA, June. Association for Computational
Linguistics.

Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Comput. Linguist.,
39(4):885–916, December.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of EMNLP-2005,
pages 25–32, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.

Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
allel guidance and entropy regularization. In Pro-
ceedings of ACL-2014, pages 1337–1348, Baltimore,
Maryland, June.

Sebastian Martschat and Michael Strube. 2015. Latent
structures for coreference resolution. Transactions of
the Association for Computational Linguistics, 3:405–
418.

Sebastian Martschat. 2013. Multigraph clustering for
unsupervised coreference resolution. In ACL-2013:
Student Research Workshop, pages 81–88, Sofia, Bul-
garia, August. Association for Computational Linguis-
tics.

Joseph F McCarthy and Wendy G Lehnert. 1995. Using
decision trees for conference resolution. In Proceed-
ings of IJCAI-1995, pages 1050–1055. Morgan Kauf-
mann Publishers Inc.

Nafise Sadat Moosavi and Michael Strube. 2014. Unsu-
pervised coreference resolution by utilizing the most
informative relations. In Proceedings of COLING-
2014, pages 644–655.

Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of EMNLP-2008, pages
640–649, Honolulu, Hawaii, October. Association for
Computational Linguistics.

Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
ACL-2010, pages 1396–1411, Uppsala, Sweden, July.
Association for Computational Linguistics.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edition.
Linguistic Data Consortium, LDC2011T07.

Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of EMNLP-2008, pages 650–659,
Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unrestricted
coreference in ontonotes. In Proceedings of EMNLP-
CoNLL-2012 - Shared Task, pages 1–40, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

Lev Ratinov and Dan Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In Pro-
ceedings of EMNLP-CoNLL-2012, pages 1234–1244,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.

1017



Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evaluation.
Natural Language Engineering, 17(04):485–510.

Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational lin-
guistics, 27(4):521–544.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th conference on Message understanding,
pages 45–52. Association for Computational Linguis-
tics.

Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Proceedings of the 20th
conference on Uncertainty in artificial intelligence,
pages 593–601. AUAI Press.

Sam Wiseman, Alexander M. Rush, Stuart Shieber, and
Jason Weston. 2015. Learning anaphoricity and an-
tecedent ranking features for coreference resolution.
In Proceedings of ACL-IJCNLP-2015, pages 1416–
1426, Beijing, China, July. Association for Computa-
tional Linguistics.

Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008a. An entity-mention model
for coreference resolution with inductive logic pro-
gramming. In Proceedings of ACL-2008, pages 843–
851.

Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2008b. A
twin-candidate model for learning-based anaphora res-
olution. Computational Linguistics, 34(3):327–356.

Appendix A. Derivation of Model Learning
Formally, we iteratively estimate the model parame-
ters θ, employing the following EM algorithm:

E-step: Compute the posterior probabilities of C,
P (C|D; θ), based on the current θ.

M-step: Calculate the new θ′ that maximizes
the expected complete log likelihood,
EP (C|D;θ)[logP (D,C; θ′)]

For simplicity, we denote:

P (C|D; θ) = P̃ (C|D)
P (C|D; θ′) = P (C|D)

In addition, we use τ(πj |j) to denote the probability
P (πj |j) which is uniform distribution in our model.

Moreover, we use the following notation for conve-
nience:

θ(mj ,mk, j, k, πj) = t(mj |mk, πj)q(k|πj , j)τ(πj |j)

Then, we have

EP̃ (c|D)[logP (D,C)]
=
∑
C
P̃ (C|D) logP (D,C)

=
∑
C
P̃ (C|D)( n∑

j=1
log t(mj |mcj , πj) + log q(cj |πj , j) + log τ(πj |j)

)
=

n∑
j=1

j−1∑
k=0

Ljk
(
log t(mj |mk, πj) + log q(k|πj , j) + log τ(πj |j)

)
Then the parameters t and q that maximize
EP̃ (c|D)[logP (D,C)] satisfy that

t(mj |mk, πj) = Ljkn∑
i=1

Lik

q(k|πj , j) = Ljkj−1∑
i=0

Lji

where Ljk can be calculated by

Ljk =
∑

C,cj=k

P̃ (C|D) =
∑

C,cj=k
P̃ (C,D)∑

C

P̃ (C,D)

=

∑
C,cj=k

n∏
i=1

θ̃(mi,mci ,ci,i,πi)

∑
C

n∏
i=1

θ̃(mi,mci ,ci,i,πi)

=
θ̃(mj ,mk,k,j,πj)

∑
C(−j)

P̃ (C(−j)|D)
j−1∑
i=0

θ̃(mj ,mi,i,j,πj)
∑

C(−j)
P̃ (C(−j)|D)

= θ̃(mj ,mk,k,j,πj)j−1∑
i=0

θ̃(mj ,mi,i,j,πj)

= t̃(mj |mk,πj)q̃(k|πj ,j)τ̃(πj |j)j−1∑
i=0

t̃(mj |mi,πj)q̃(i|πj ,j)τ̃(πj |j)

= t̃(mj |mk,πj)q̃(k|πj ,j)j−1∑
i=0

t̃(mj |mi,πj)q̃(i|πj ,j)

where C(−j) = {c1, . . . , cj−1, cj+1, . . . , cn}. The
above derivations correspond to the learning algo-
rithm in Algorithm 1.

1018


