



















































Joint Bootstrapping Machines for High Confidence Relation Extraction


Proceedings of NAACL-HLT 2018, pages 26–36
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Joint Bootstrapping Machines for High Confidence Relation Extraction

Pankaj Gupta1,2, Benjamin Roth2, Hinrich Schütze2
1Corporate Technology, Machine-Intelligence (MIC-DE), Siemens AG Munich, Germany

2CIS, University of Munich (LMU) Munich, Germany
pankaj.gupta@siemens.com | pankaj.gupta@campus.lmu.de

{beroth, inquiries}@cis.lmu.de

Abstract
Semi-supervised bootstrapping techniques for
relationship extraction from text iteratively ex-
pand a set of initial seed instances. Due to the
lack of labeled data, a key challenge in boot-
strapping is semantic drift: if a false positive
instance is added during an iteration, then all
following iterations are contaminated. We in-
troduce BREX, a new bootstrapping method
that protects against such contamination by
highly effective confidence assessment. This
is achieved by using entity and template seeds
jointly (as opposed to just one as in previous
work), by expanding entities and templates in
parallel and in a mutually constraining fash-
ion in each iteration and by introducing higher-
quality similarity measures for templates. Ex-
perimental results show that BREX achieves
an F1 that is 0.13 (0.87 vs. 0.74) better than
the state of the art for four relationships.

1 Introduction

Traditional semi-supervised bootstrapping rela-
tion extractors (REs) such as BREDS (Batista
et al., 2015), SnowBall (Agichtein and Gravano,
2000) and DIPRE (Brin, 1998) require an initial
set of seed entity pairs for the target binary rela-
tion. They find occurrences of positive seed en-
tity pairs in the corpus, which are converted into
extraction patterns, i.e., extractors, where we de-
fine an extractor as a cluster of instances generated
from the corpus. The initial seed entity pair set is
expanded with the relationship entity pairs newly
extracted by the extractors from the text iteratively.
The augmented set is then used to extract new re-
lationships until a stopping criterion is met.

Due to lack of sufficient labeled data, rule-
based systems dominate commercial use (Chiti-
cariu et al., 2013). Rules are typically defined
by creating patterns around the entities (entity ex-
traction) or entity pairs (relation extraction). Re-
cently, supervised machine learning, especially

deep learning techniques (Gupta et al., 2015;
Nguyen and Grishman, 2015; Vu et al., 2016a,b;
Gupta et al., 2016), have shown promising results
in entity and relation extraction; however, they
need sufficient hand-labeled data to train models,
which can be costly and time consuming for web-
scale extractions. Bootstrapping machine-learned
rules can make extractions easier on large corpora.
Thus, open information extraction systems (Carl-
son et al., 2010; Fader et al., 2011; Mausam et al.,
2012; Mesquita et al., 2013; Angeli et al., 2015)
have recently been popular for domain specific or
independent pattern learning.

Hearst (1992) used hand written rules to gen-
erate more rules to extract hypernym-hyponym
pairs, without distributional similarity. For en-
tity extraction, Riloff (1996) used seed entities to
generate extractors with heuristic rules and scored
them by counting positive extractions. Prior work
(Lin et al., 2003; Gupta et al., 2014) investigated
different extractor scoring measures. Gupta and
Manning (2014) improved scores by introducing
expected number of negative entities.

Brin (1998) developed the bootstrapping rela-
tion extraction system DIPRE that generates ex-
tractors by clustering contexts based on string
matching. SnowBall (Agichtein and Gravano,
2000) is inspired by DIPRE but computes a TF-
IDF representation of each context. BREDS
(Batista et al., 2015) uses word embeddings
(Mikolov et al., 2013) to bootstrap relationships.

Related work investigated adapting extractor
scoring measures in bootstrapping entity extrac-
tion with either entities or templates (Table 1) as
seeds (Table 2). The state-of-the-art relation ex-
tractors bootstrap with only seed entity pairs and
suffer due to a surplus of unknown extractions
and the lack of labeled data, leading to low con-
fidence extractors. This in turn leads to to low
confidence in the system output. Prior RE sys-

26



BREE Bootstrapping Relation Extractor with Entity pair
BRET Bootstrapping Relation Extractor with Template
BREJ Bootstrapping Relation Extractor in Joint learning
type a named entity type, e.g., person
typed entity a typed entity, e.g.,  “Obama”,person¡
entity pair a pair of two typed entities
template a triple of vectors (~v�1, ~v0, ~v1) and an entity pair
instance entity pair and template (types must be the same)
γ instance set extracted from corpus
i a member of γ, i.e., an instance
xpiq the entity pair of instance i
xpiq the template of instance i
Gp a set of positive seed entity pairs
Gn a set of negative seed entity pairs
Gp a set of positive seed templates
Gn a set of negative seed templates
G   Gp, Gn,Gp,Gn ¡
kit number of iterations
λcat cluster of instances (extractor)
cat category of extractor λ
λNNHC Non-Noisy-High-Confidence extractor (True Positive)
λNNLC Non-Noisy-Low-Confidence extractor (True Negative)
λNHC Noisy-High-Confidence extractor (False Positive)
λNLC Noisy-Low-Confidence extractor (False Negative)

Table 1: Notation and definition of key terms

tems do not focus on improving the extractors’
scores. In addition, SnowBall and BREDS used
a weighting scheme to incorporate the importance
of contexts around entities and compute a similar-
ity score that introduces additional parameters and
does not generalize well.

Contributions. (1) We propose a Joint Boot-
strapping Machine1 (JBM), an alternative to the
entity-pair-centered bootstrapping for relation ex-
traction that can take advantage of both entity-pair
and template-centered methods to jointly learn
extractors consisting of instances due to the oc-
currences of both entity pair and template seeds.
It scales up the number of positive extractions
for non-noisy extractors and boosts their confi-
dence scores. We focus on improving the scores
for non-noisy-low-confidence extractors, resulting
in higher recall. The relation extractors boot-
strapped with entity pair, template and joint seeds
are named as BREE, BRET and BREJ (Table 1),
respectively.

(2) Prior work on embedding-based con-
text comparison has assumed that relations
have consistent syntactic expression and has
mainly addressed synonymy by using embeddings
(e.g.,“acquired” – “bought”). In reality, there is
large variation in the syntax of how relations are
expressed, e.g., “MSFT to acquire NOK for $8B”

1github.com/pgcool/Joint-Bootstrapping-Machines

vs. “MSFT earnings hurt by NOK acquisition”.
We introduce cross-context similarities that com-
pare all parts of the context (e.g., “to acquire” and
“acquisition”) and show that these perform better
(in terms of recall) than measures assuming con-
sistent syntactic expression of relations.

(3) Experimental results demonstrate a 13%
gain in F1 score on average for four relationships
and suggest eliminating four parameters, com-
pared to the state-of-the-art method.

The motivation and benefits of the proposed
JBM for relation extraction is discussed in depth
in section 2.3. The method is applicable for both
entity and relation extraction tasks. However, in
context of relation extraction, we call it BREJ.

2 Method

2.1 Notation and definitions

We first introduce the notation and terms (Table 1).
Given a relationship like “x acquires y”, the

task is to extract pairs of entities from a corpus for
which the relationship is true. We assume that the
arguments of the relationship are typed, e.g., x and
y are organizations. We run a named entity tagger
in preprocessing, so that the types of all candidate
entities are given. The objects the bootstrapping
algorithm generally handles are therefore typed
entities (an entity associated with a type).

For a particular sentence in a corpus that states
that the relationship (e.g., “acquires”) holds be-
tween x and y, a template consists of three vectors
that represent the context of x and y. ~v�1 repre-
sents the context before x, ~v0 the context between
x and y and ~v1 the context after y. These vectors
are simply sums of the embeddings of the corre-
sponding words. A template is “typed”, i.e., in
addition to the three vectors it specifies the types
of the two entities. An instance joins an entity pair
and a template. The types of entity pair and tem-
plate must be the same.

The first step of bootstrapping is to extract a set
of instances from the input corpus. We refer to this
set as γ. We will use i and j to refer to instances.
xpiq is the entity pair of instance i and xpiq is the
template of instance i.

A required input to our algorithm are sets of
positive and negative seeds for either entity pairs
(Gp and Gn) or templates (Gp and Gn) or both.
We define G to be a tuple of all four seed sets.

We run our bootstrapping algorithm for kit iter-
ations where kit is a parameter.

27



A key notion is the similarity between two in-
stances. We will experiment with different sim-
ilarity measures. The baseline is (Batista et al.,
2015)’s measure given in Figure 4, first line: the
similarity of two instances is given as a weighted
sum of the dot products of their before contexts
(~v�1), their between contexts (~v0) and their after
contexts (~v1) where the weights wp are parame-
ters. We give this definition for instances, but it
also applies to templates since only the context
vectors of an instance are used, not the entities.

The similarity between an instance i and a clus-
ter λ of instances is defined as the maximum sim-
ilarity of i with any member of the cluster; see
Figure 2, right, Eq. 5. Again, there is a straight-
forward extension to a cluster of templates: see
Figure 2, right, Eq. 6.

The extractors Λ can be categorized as follows:

ΛNNHC � tλ P Λ| λ ÞÑ Rloomoon
non�noisy

^ cnfpλ,Gq ¥ τcnfu (1)

ΛNNLC � tλ P Λ|λ ÞÑ R^ cnfpλ,Gq   τcnfu (2)
ΛNHC � tλ P Λ|λ ÞÑ Rloomoon

noisy

^ cnfpλ,Gq ¥ τcnfu (3)

ΛNLC � tλ P Λ|λ ÞÑ R^ cnfpλ,Gq   τcnfu (4)

where R is the relation to be bootstrapped. The
λcat is a member of Λcat. For instance, a λNNLC
is called as a non-noisy-low-confidence extractor if
it represents the target relation (i.e., λ ÞÑ R), how-
ever with the confidence below a certain threshold
(τcnf ). Extractors of types ΛNNHC and ΛNLC are
desirable, those of types ΛNHC and ΛNNLC un-
desirable within bootstrapping.

2.2 The Bootstrapping Machines: BREX
To describe BREX (Figure 1) in its most general
form, we use the term item to refer to an entity
pair, a template or both.

The input to BREX (Figure 2, left, line 01) is
a set γ of instances extracted from a corpus and
Gseed, a structure consisting of one set of positive
and one set of negative seed items. Gyield (line 02)
collects the items that BREX extracts in several it-
erations. In each of kit iterations (line 03), BREX
first initializes the cache Gcache (line 04); this cache
collects the items that are extracted in this itera-
tion. The design of the algorithm balances ele-
ments that ensure high recall with elements that
ensure high precision.

High recall is achieved by starting with the
seeds and making three “hops” that consecutively
consider order-1, order-2 and order-3 neighbors

.

.

.

.

.

.

Seeds

Seed Occurrences 
  Extractors

 Candidate 
 Instances

  Output 
Instances

(4)Compute Extractors’ Confidence(5)Identify positives, negatives, unknowns

Augment Initial Seed sets with Output Instances in  

Generate
Extractors

  Find
Occurr
-ences

cluster

Hop-1
Hop-2

Hop-3

     1

     2

     3

     4

cluster

(1)
(2)

(3)

(6)

Figure 1: Joint Bootstrapping Machine. The red and
blue filled circles/rings are the instances generated
due to seed entity pairs and templates, respectively.
Each dashed rectangular box represents a cluster of in-
stances. Numbers indicate the flow. Follow the nota-
tions from Table 1 and Figure 2.

of the seeds. On line 05, we make the first hop:
all instances that are similar to a seed are col-
lected where “similarity” is defined differently for
different BREX configurations (see below). The
collected instances are then clustered, similar to
work on bootstrapping by Agichtein and Gravano
(2000) and Batista et al. (2015). On line 06, we
make the second hop: all instances that are within
τ sim of a hop-1 instance are added; each such in-
stance is only added to one cluster, the closest one;
see definition of µ: Figure 2, Eq. 8. On line 07, we
make the third hop: we include all instances that
are within τ sim of a hop-2 instance; see definition
of ψ: Figure 2, Eq. 7. In summary, every instance
that can be reached by three hops from a seed is
being considered at this point. A cluster of hop-2
instances is named as extractor.

High precision is achieved by imposing, on line
08, a stringent check on each instance before its
information is added to the cache. The core func-
tion of this check is given in Figure 2, Eq. 9. This
definition is a soft version of the following hard
max, which is easier to explain:

cnfpi,Λ,Gq � maxtλPΛ|iPψpλqu cnfpi, λ,Gq
We are looking for a cluster λ in Λ that li-

censes the extraction of i with high confidence.
cnfpi, λ,Gq (Figure 2, Eq. 10), the confidence of
a single cluster (i.e., extractor) λ for an instance,
is defined as the product of the overall reliability of
λ (which is independent of i) and the similarity of
i to λ, the second factor in Eq. 10, i.e., simpi, λq.
This factor simpi, λq prevents an extraction by a
cluster whose members are all distant from the in-
stance – even if the cluster itself is highly reliable.

28



Algorithm: BREX

01 INPUT: γ, Gseed
02 Gyield :� Gseed
03 for kit iterations:
04 Gcache :� H
05 Θ :�


pti P γ|matchpi,Gyieldqq

06 Λ :� tµpθ,Θq|θ P Θu
07 for each i P


λPΛ ψpλq:

08 if checkpi,Λ,Gyieldq :
09 addpi,Gcacheq
10 GyieldY� Gcache
11 OUTPUT: Gyield, Λ

simpi, λq � maxi1Pλsimpi, i1q (5)

simpi,Gq � maxtPGsimpi, tq (6)

ψpλq � ti P γ|simpi, λq ¥ τ simu (7)

µpθ,Θq � ti P γ|simpi, θq � d^

d � max
θPΘ

simpi, θq ¥ τ simu (8)

cnfpi,Λ,Gq � 1�
¹

tλPΛ|iPψpλqu

p1�cnfpi, λ,Gqq (9)

cnfpi, λ,Gq � cnfpλ,Gqsimpi, λq (10)
cnfpλ,Gq � 1

1� wn
N�pλ,Gnq
N�pλ,Gpq � wu

N0pλ,Gq
N�pλ,Gpq

(11)

N0pλ,Gq � |ti P λ|xpiq R pGp YGnqu| (12)

Figure 2: BREX algorithm (left) and definition of key concepts (right)

BREE BRET BREJ
Seed Type Entity pairs Templates Joint (Entity pairs + Templates)

(i) N�pλ,Glq |tiPλ|xpiqPGlu| |tiPλ|simpi,Glq¥τsimu| |tiPλ|xpiqPGlu|�|tiPλ|simpi,Glq¥τsimu|
(ii)pwn, wuq p1.0, 0.0q p1.0, 0.0q p1.0, 0.0q
05 matchpi,Gq xpiq P Gp simpi,Gpq¥τsim xpiq P Gp _ simpi,Gpq¥τsim
08 checkpi,Λ,Gq cnfpi,Λ,Gq¥τcnf cnfpi,Λ,Gq¥τcnf cnfpi,Λ,Gq¥τcnf ^ simpi,Gpq¥τsim
09 addpi,Gq GpY� txpiqu GpY� txpiqu GpY� txpiqu, GpY� txpiqu

Figure 3: BREX configurations

The first factor in Eq. 10, i.e., cnfpλ,Gq, as-
sesses the reliability of a cluster λ: we compute
the ratio N�pλ,GnqN�pλ,Gpq , i.e., the ratio between the num-
ber of instances in λ that match a negative and pos-
itive gold seed, respectively; see Figure 3, line (i).
If this ratio is close to zero, then likely false pos-
itive extractions are few compared to likely true
positive extractions. For the simple version of the
algorithm (for which we set wn � 1, wu � 0),
this results in cnfpλ,Gq being close to 1 and the
reliability measure it not discounted. On the other
hand, if N�pλ,GnqN�pλ,Gpq is larger, meaning that the rela-
tive number of likely false positive extractions is
high, then cnfpλ,Gq shrinks towards 0, resulting
in progressive discounting of cnfpλ,Gq and lead-
ing to non-noisy-low-confidence extractor, partic-
ularly for a reliable λ. Due to lack of labeled
data, the scoring mechanism cannot distinguish
between noisy and non-noisy extractors. There-
fore, an extractor is judged by its ability to extract
more positive and less negative extractions. Note
that we carefully designed this precision compo-
nent to give good assessments while at the same

time making maximum use of the available seeds.
The reliability statistics are computed on λ, i.e.,
on hop-2 instances (not on hop-3 instances). The
ratio N�pλ,GnqN�pλ,Gpq is computed on instances that di-
rectly match a gold seed – this is the most reliable
information we have available.

After all instances have been checked (line 08)
and (if they passed muster) added to the cache
(line 09), the inner loop ends and the cache is
merged into the yield (line 10). Then a new loop
(lines 03–10) of hop-1, hop-2 and hop-3 exten-
sions and cluster reliability tests starts.

Thus, the algorithm consists of kit iterations.
There is a tradeoff here between τ sim and kit. We
will give two extreme examples, assuming that
we want to extract a fixed number of m instances
where m is given. We can achieve this goal either
by setting kit=1 and choosing a small τ sim, which
will result in very large hops. Or we can achieve
this goal by setting τ sim to a large value and run-
ning the algorithm for a larger number of kit. The
flexibility that the two hyperparameters kit and τ sim
afford is important for good performance.

29



simmatchpi, jq �
°

pPt�1,0,1u wp~vppiq~vppjq ; sim
asym
cc pi, jq � maxpPt�1,0,1u ~vppiq~v0pjq (13)

simsym1cc pi, jq � max
�

maxpPt�1,0,1u ~vppiq~v0pjq,maxpPt� 1,0,1u ~vppjq~v0piq
�

(14)

simsym2cc pi, jq � max
��
~v�1piq � ~v1piq

�
~v0pjq,

�
~v�1pjq � ~v1pjq

�
~v0piq, ~v0piq~v0pjq

	
(15)

Figure 4: Similarity measures. These definitions for instances equally apply to templates since the definitions only
depend on the “template part” of an instance, i.e., its vectors. (value is 0 if types are different)

i

N+(   ,G  ) 

N

01

+0Y+1

01

+0Y+1 N

N+0
Y+1

NY+1 Y+1N+0

12 1 0

+0

BREE BREJBRET

p

N+(   ,G  ) p

N+(   ,     ) p N+(   ,     ) p N+(   ,     ) p

Figure 5: Illustration of Scaling-up Positive Instances.
i: an instance in extractor, λ. Y: YES and N: NO

2.3 BREE, BRET and BREJ

The main contribution of this paper is that we
propose, as an alternative to entity-pair-centered
BREE (Batista et al., 2015), template-centered
BRET as well as BREJ (Figure 1), an instantiation
of BREX that can take advantage of both entity
pairs and templates. The differences and advan-
tages of BREJ over BREE and BRET are:

(1) Disjunctive Matching of Instances: The
first difference is realized in how the three algo-
rithms match instances with seeds (line 05 in Fig-
ure 3). BREE checks whether the entity pair of
an instance is one of the entity pair seeds, BRET
checks whether the template of an instance is one
of the template seeds and BREJ checks whether
the disjunction of the two is true. The disjunc-
tion facilitates a higher hit rate in matching in-
stances with seeds. The introduction of a few
handcrafted templates along with seed entity pairs
allows BREJ to leverage discriminative patterns
and learn similar ones via distributional semantics.
In Figure 1, the joint approach results in hybrid
extractors Λ that contain instances due to seed oc-
currences Θ of both entity pairs and templates.

(2) Hybrid Augmentation of Seeds: On line
09 in Figure 3, we see that the bootstrapping step is
defined in a straightforward fashion: the entity pair
of an instance is added for BREE, the template for
BRET and both for BREJ. Figure 1 demonstrates

I1: ’s purchase of

I2: ’s acquisition of

I1: ’s purchase of

             (BREE)

I1: ’s purchase of

I2: ’s acquisition of

(BRET)                      (BREJ)

Seed Entity Pair:          = {<Google, DoubleClick>}

Seed Templates:            = {[X] ’s acquisition of [Y]}

Matched Instances:
       I1: <Google> 's purchase of <DoubleClick> is intriguing. 
       I2: <Google> 's acquisition of <DoubleClick> is approved.
       I3: <Dynegy> 's purchase of <Enron> triggered a clause. 
       I4: <Google> 's acquisition of <YouTube> was in its final stages.

Generate Extractor

     Positive: {I1, I2}
    Negative: {I3, I4}

Positive: {I1, I2, I3, I4} Positive: {I1, I2, I1, I2, I3, I4}
Negative: {I3, I4}

I2: ’s acquisition of

I3: ’s purchase of

I4: ’s acquisition of

I1: ’s purchase of

I2: ’s acquisition of

I3: ’s purchase of

I4: ’s acquisition of

                      = {<Google, Microsoft>}

        = {[X] competitor of [Y]}

: { }  : {I1, I2, I3, I4} : {I1, I2, I3, I4}

Output Instances

Match Seeds in Instances

2
= 2

4
= 0

2+4=
2+0

 = 0.5  = 1.0  = 0.75

Figure 6: An illustration of scaling positive extractions
and computing confidence for a non-noisy extractor
generated for acquired relation. The dashed rectangu-
lar box represents an extractor λ, where λ (BREJ) is
hybrid with 6 instances. Text segments matched with
seed template are shown in italics. Unknowns (bold in
black) are considered as negatives. Gcache is a set of
output instances where τcnf � 0.70.

the hybrid augmentation of seeds via red and blue
rings of output instances.

(3) Scaling Up Positives in Extractors: As dis-
cussed in section 2.2, a good measure of the qual-
ity of an extractor is crucial and N�, the number
of instances in an extractor λ that match a seed,
is an important component of that. For BREE and
BRET, the definition follows directly from the fact
that these are entity-pair and template-centered in-
stantiations of BREX, respectively. However, the
disjunctive matching of instances for an extrac-
tor with entity pair and template seeds in BREJ
(Figure 3 line “(i)” ) boosts the likelihood of find-
ing positive instances. In Figure 5, we demon-
strate computing the count of positive instances

30



Relationship Seed Entity Pairs Seed Templates

acquired
{Adidas;Reebok},{Google;DoubleClick},

{Widnes;Warrington},{Hewlett-Packard;Compaq}
{[X] acquire [Y]},{[X] acquisition [Y]},{[X] buy [Y]},

{[X] takeover [Y]},{[X] merger with [Y]}

founder-of
{CNN;Ted Turner},{Facebook;Mark Zuckerberg},
{Microsoft;Paul Allen},{Amazon;Jeff Bezos},

{[X] founded by [Y]},{[X] co-founder [Y]},{[X] started by [Y]},
{[X] founder of [Y]},{[X] owner of [Y]}

headquartered
{Nokia;Espoo},{Pfizer;New York},

{United Nations;New York},{NATO;Brussels},
{[X] based in [Y]},{[X] headquarters in [Y]},{[X] head office in [Y]},

{[X] main office building in [Y]},{[X] campus branch in [Y]}

affiliation
{Google;Marissa Mayer},{Xerox;Ursula Burns},

{ Microsoft;Steve Ballmer},{Microsoft;Bill Gates},
{[X] CEO [Y]},{[X] resign from [Y]},{[X] founded by [Y]},

{[X] worked for [Y]},{[X] chairman director [Y]}

Table 2: Seed Entity Pairs and Templates for each relation. [X] and [Y] are slots for entity type tags.

N�pλ,Gq for an extractor λ within the three sys-
tems. Observe that an instance i in λ can scale its
N�pλ,Gq by a factor of maximum 2 in BREJ if i
is matched in both entity pair and template seeds.
The reliability cnfpλ,Gq (Eq. 11) of an extractor λ
is based on the ratio N�pλ,GnqN�pλ,Gpq , therefore suggest-
ing that the scaling boosts its confidence.

In Figure 6, we demonstrate with an example
how the joint bootstrapping scales up the positive
instances for a non-noisy extractor λ, resulting in
λNNHC for BREJ compared to λNNLC in BREE.

Due to unlabeled data, the instances not match-
ing in seeds are considered either to be ig-
nored/unknown N0 or negatives in the confidence
measure (Eq. 11). The former leads to high con-
fidences for noisy extractors by assigning high
scores, the latter to low confidences for non-noisy
extractors by penalizing them. For a simple ver-
sion of the algorithm in the illustration, we con-
sider them as negatives and set wn � 1. Figure 6
shows the three extractors (λ) generated and their
confidence scores in BREE, BRET and BREJ. Ob-
serve that the scaling up of positives in BREJ
due to BRET extractions (without wn) discounts
cnfpλ,Gq relatively lower than BREE. The dis-
counting results in λNNHC in BREJ and λNNLC
in BREE. The discounting in BREJ is adapted for
non-noisy extractors facilitated by BRET in gener-
ating mostly non-noisy extractors due to stringent
checks (Figure 3, line “(i)” and 05). Intuitively,
the intermixing of non-noisy extractors (i.e., hy-
brid) promotes the scaling and boosts recall.

2.4 Similarity Measures

The before (~v�1) and after (~v1) contexts around
the entities are highly sparse due to large varia-
tion in the syntax of how relations are expressed.
SnowBall, DIPRE and BREE assumed that the
between (~v0) context mostly defines the syntac-
tic expression for a relation and used weighted
mechanism on the three contextual similarities in

ORG-ORG ORG-PER ORG-LOC
count 58,500 75,600 95,900

Table 3: Count of entity-type pairs in corpus

Parameter Description/ Search Optimal
|v�1| maximum number of tokens in before context 2
|v0| maximum number of tokens in between context 6
|v1| maximum number of tokens in after context 2
τsim similarity threshold [0.6, 0.7, 0.8] 0.7
τcnf instance confidence thresholds [0.6, 0.7, 0.8] 0.7
wn weights to negative extractions [0.0, 0.5, 1.0, 2.0] 0.5
wu weights to unknown extractions [0.0001, 0.00001] 0.0001
kit number of bootstrapping epochs 3

dimemb dimension of embedding vector, V 300
PMI PMI threshold in evaluation 0.5

Entity Pairs Ordered Pairs (OP ) or Bisets (BS) OP

Table 4: Hyperparameters in BREE, BRET and BREJ

pairs, simmatch (Figure 4). They assigned higher
weights to the similarity in between (p � 0) con-
texts, that resulted in lower recall. We introduce
attentive (max) similarity across all contexts (for
example, ~v�1piq~v0pjq) to automatically capture
the large variation in the syntax of how relations
are expressed, without using any weights. We in-
vestigate asymmetric (Eq 13) and symmetric (Eq
14 and 15) similarity measures, and name them as
cross-context attentive (simcc) similarity.

3 Evaluation

3.1 Dataset and Experimental Setup
We re-run BREE (Batista et al., 2015) for base-
line with a set of 5.5 million news articles from
AFP and APW (Parker et al., 2011). We use pro-
cessed dataset of 1.2 million sentences (released
by BREE) containing at least two entities linked to
FreebaseEasy (Bast et al., 2014). We extract four
relationships: acquired (ORG-ORG), founder-
of (ORG-PER), headquartered (ORG-LOC) and
affiliation (ORG-PER) for Organization (ORG),
Person (PER) and Location (LOC) entity types.
We bootstrap relations in BREE, BRET and BREJ,
each with 4 similarity measures using seed entity

31



Relationships #out P R F1 #out P R F1 #out P R F1 #out P R F1

B
R

E
E

baseline: BREE+simmatch config2: BREE+simasymcc config3: BREE+simsym1cc config4: BREE+simsym2cc
acquired 2687 0.88 0.48 0.62 5771 0.88 0.66 0.76 3471 0.88 0.55 0.68 3279 0.88 0.53 0.66

founder-of 628 0.98 0.70 0.82 9553 0.86 0.95 0.89 1532 0.94 0.84 0.89 1182 0.95 0.81 0.87

headquartered 16786 0.62 0.80 0.69 21299 0.66 0.85 0.74 17301 0.70 0.83 0.76 9842 0.72 0.74 0.73

affiliation 20948 0.99 0.73 0.84 27424 0.97 0.78 0.87 36797 0.95 0.82 0.88 28416 0.97 0.78 0.87

avg 10262 0.86 0.68 0.74 16011 0.84 0.81 0.82 14475 0.87 0.76 0.80 10680 0.88 0.72 0.78

B
R

E
T

config5: BRET+simmatch config6: BRET+simasymcc config7: BRET+simsym1cc config8: BRET+simsym2cc
acquired 4206 0.99 0.62 0.76 15666 0.90 0.85 0.87 18273 0.87 0.86 0.87 14319 0.92 0.84 0.87

founder-of 920 0.97 0.77 0.86 43554 0.81 0.98 0.89 41978 0.81 0.99 0.89 46453 0.81 0.99 0.89

headquartered 3065 0.98 0.55 0.72 39267 0.68 0.92 0.78 36374 0.71 0.91 0.80 56815 0.69 0.94 0.80

affiliation 20726 0.99 0.73 0.85 28822 0.99 0.79 0.88 44946 0.96 0.85 0.90 33938 0.97 0.81 0.89

avg 7229 0.98 0.67 0.80 31827 0.85 0.89 0.86 35393 0.84 0.90 0.86 37881 0.85 0.90 0.86

B
R

E
J

config9: BREJ+simmatch config10: BREJ+simasymcc config11: BREJ+simsym1cc config12: BREJ+simsym2cc
acquired 20186 0.82 0.87 0.84 35553 0.80 0.92 0.86 22975 0.86 0.89 0.87 22808 0.85 0.90 0.88

founder-of 45005 0.81 0.99 0.89 57710 0.81 1.00 0.90 50237 0.81 0.99 0.89 45374 0.82 0.99 0.90
headquartered 47010 0.64 0.93 0.76 66563 0.68 0.96 0.80 60495 0.68 0.94 0.79 57853 0.68 0.94 0.79

affiliation 40959 0.96 0.84 0.89 57301 0.94 0.88 0.91 55811 0.94 0.87 0.91 51638 0.94 0.87 0.90
avg 38290 0.81 0.91 0.85 54282 0.81 0.94 0.87 47380 0.82 0.92 0.87 44418 0.82 0.93 0.87

Table 5: Precision (P ), Recall (R) and F1 compared to the state-of-the-art (baseline). #out: count of output in-
stances with cnfpi,Λ,Gq ¥ 0.5. avg: average. Bold and underline: Maximum due to BREJ and simcc, respectively.

pairs and templates (Table 2). See Tables 3, 4 and
5 for the count of candidates, hyperparameters and
different configurations, respectively.

Our evaluation is based on Bronzi et al. (2012)’s
framework to estimate precision and recall of
large-scale RE systems using FreebaseEasy (Bast
et al., 2014). Also following Bronzi et al. (2012),
we use Pointwise Mutual Information (PMI) (Tur-
ney, 2001) to evaluate our system automatically,
in addition to relying on an external knowledge
base. We consider only extracted relationship in-
stances with confidence scores cnfpi,Λ,Gq equal
or above 0.5. We follow the same approach as
BREE (Batista et al., 2015) to detect the correct or-
der of entities in a relational triple, where we try to
identify the presence of passive voice using part-
of-speech (POS) tags and considering any form of
the verb to be, followed by a verb in the past tense
or past participle, and ending in the word ‘by’. We
use GloVe (Pennington et al., 2014) embeddings.

3.2 Results and Comparison with baseline

Table 5 shows the experimental results in the
three systems for the different relationships with
ordered entity pairs and similarity measures
(simmatch, simcc). Observe that BRET (config5)
is precision-oriented while BREJ (config9) recall-
oriented when compared to BREE (baseline). We
see the number of output instances #out are also
higher in BREJ, therefore the higher recall. The
BREJ system in the different similarity configura-

τ kit #out P R F1

0.6
1 691 0.99 0.21 0.35
2 11288 0.85 0.79 0.81

0.7
1 610 1.0 0.19 0.32
2 7948 0.93 0.75 0.83

0.8
1 522 1.0 0.17 0.29
2 2969 0.90 0.51 0.65

Table 6: Iterations (kit) Vs Scores with thresholds (τ )
for relation acquired in BREJ. τ refers to τsim and τcnf

τ #out P R F1 τ #out P R F1

B
R

E
E .60 1785 .91 .39 .55 .70 1222 .94 .31 .47

.80 868 .95 .25 .39 .90 626 .96 .19 .32

B
R

E
T .60 2995 .89 .51 .65 .70 1859 .90 .40 .55

.80 1312 .91 .32 .47 .90 752 .94 .22 .35

B
R

E
J .60 18271 .81 .85 .83 .70 14900 .84 .83 .83

.80 8896 .88 .75 .81 .90 5158 .93 .65 .77

Table 7: Comparative analysis using different thresh-
olds τ to evaluate the extracted instances for acquired

tions outperforms the baseline BREE and BRET in
terms of F1 score. On an average for the four rela-
tions, BREJ in configurations config9 and config10
results in F1 that is 0.11 (0.85 vs 0.74) and 0.13
(0.87 vs 0.74) better than the baseline BREE.

We discover that simcc improves #out and re-
call over simmatch correspondingly in all three sys-
tems. Observe that simcc performs better with
BRET than BREE due to non-noisy extractors in
BRET. The results suggest an alternative to the
weighting scheme in simmatch and therefore, the
state-of-the-art (simcc) performance with the 3 pa-
rameters (w�1, w0 and w1) ignored in bootstrap-

32



acquired founder-of headquartered affiliation
BREX E T J E T J E T J E T J
#hit 71 682 743 135 956 1042 715 3447 4023 603 14888 15052

Table 8: Disjunctive matching of Instances. #hit: the
count of instances matched to positive seeds in kit � 1

Attributes |Λ| AIE AES ANE ANNE ANNLC AP AN ANP

ac
qu

ir
ed

BREE 167 12.7 0.51 0.84 0.16 0.14 37.7 93.1 2.46

BRET 17 305.2 1.00 0.11 0.89 0.00 671.8 0.12 0.00

BREJ 555 41.6 0.74 0.71 0.29 0.03 313.2 44.8 0.14

fo
un

de
r-

of BREE 8 13.3 0.46 0.75 0.25 0.12 44.9 600.5 13.37

BRET 5 179.0 1.00 0.00 1.00 0.00 372.2 0.0 0.00

BREJ 492 109.1 0.90 0.94 0.06 0.00 451.8 79.5 0.18

he
ad

qu
ar

te
re

d

BREE 655 18.4 0.60 0.97 0.03 0.02 46.3 82.7 1.78

BRET 7 365.7 1.00 0.00 1.00 0.00 848.6 0.0 0.00

BREJ 1311 45.5 0.80 0.98 0.02 0.00 324.1 77.5 0.24

af
fil

ia
tio

n BREE 198 99.7 0.55 0.25 0.75 0.34 240.5 152.2 0.63

BRET 19 846.9 1.00 0.00 1.00 0.00 2137.0 0.0 0.00

BREJ 470 130.2 0.72 0.21 0.79 0.06 567.6 122.7 0.22

Table 9: Analyzing the attributes of extractors Λ
learned for each relationship. Attributes are: number of
extractors (|Λ|), avg number of instances in Λ (AIE),
avg Λ score (AES), avg number of noisy Λ (ANE),
avg number of non-noisy Λ (ANNE), avg number of
ΛNNLC below confidence 0.5 (ANNLC), avg number
of positives (AP) and negatives (AN), ratio of AN to
AP (ANP). The bold indicates comparison of BREE
and BREJ with simmatch. avg: average

ping. Observe that simasymcc gives higher recall
than the two symmetric similarity measures.

Table 6 shows the performance of BREJ in dif-
ferent iterations trained with different similarity
τsim and confidence τcnf thresholds. Table 7
shows a comparative analysis of the three systems,
where we consider and evaluate the extracted rela-
tionship instances at different confidence scores.

3.3 Disjunctive Seed Matching of Instances

As discussed in section 2.3, BREJ facilitates dis-
junctive matching of instances (line 05 Figure 3)
with seed entity pairs and templates. Table 8
shows #hit in the three systems, where the higher
values of #hit in BREJ conform to the desired
property. Observe that some instances in BREJ
are found to be matched in both the seed types.

3.4 Deep Dive into Attributes of Extractors

We analyze the extractors Λ generated in BREE,
BRET and BREJ for the 4 relations to demon-
strate the impact of joint bootstrapping. Table 9
shows the attributes of Λ. We manually annotate
the extractors as noisy and non-noisy. We compute
ANNLC and the lower values in BREJ compared
to BREE suggest fewer non-noisy extractors with
lower confidence in BREJ due to the scaled confi-

Relationships #out P R F1

B
R

E
E

acquired 387 0.99 0.13 0.23
founder-of 28 0.96 0.09 0.17

headquartered 672 0.95 0.21 0.34
affiliation 17516 0.99 0.68 0.80

avg 4651 0.97 0.28 0.39

B
R

E
T

acquired 4031 1.00 0.61 0.76
founder-of 920 0.97 0.77 0.86

headquartered 3522 0.98 0.59 0.73
affiliation 22062 0.99 0.74 0.85

avg 7634 0.99 0.68 0.80

B
R

E
J

acquired 12278 0.87 0.81 0.84
founder-of 23727 0.80 0.99 0.89

headquartered 38737 0.61 0.91 0.73
affiliation 33203 0.98 0.81 0.89

avg 26986 0.82 0.88 0.84

Table 10: BREX+simmatch:Scores when wn ignored

dence scores. ANNE (higher), ANNLC (lower), AP
(higher) and AN (lower) collectively indicate that
BRET mostly generates NNHC extractors. AP and
AN indicate an average of N�pλ,Glq (line “ (i)”
Figure 3) for positive and negative seeds, respec-
tively for λ P Λ in the three systems. Observe
the impact of scaling positive extractions (AP) in
BREJ that shrink N�pλ,GnqN�pλ,Gpq i.e., ANP. It facili-
tates λNNLC to boost its confidence, i.e., λNNHC
in BREJ suggested by AES that results in higher
#out and recall (Table 5, BREJ).

3.5 Weighting Negatives Vs Scaling Positives
As discussed, Table 5 shows the performance
of BREE, BRET and BREJ with the parameter
wn � 0.5 in computing extractors’ confidence
cnfpλ,Gq(Eq. 11). In other words, config9 (Ta-
ble 5) is combination of both weighted negative
and scaled positive extractions. However, we also
investigate ignoringwnp� 1.0q in order to demon-
strate the capability of BREJ with only scaling
positives and without weighting negatives. In
Table 10, observe that BREJ outperformed both
BREE and BRET for all the relationships due to
higher #out and recall. In addition, BREJ scores
are comparable to config9 (Table 5) suggesting
that the scaling in BREJ is capable enough to re-
move the parameter wn. However, the combina-
tion of both weighting negatives and scaling posi-
tives results in the state-of-the-art performance.

3.6 Qualitative Inspection of Extractors
Table 11 lists some of the non-noisy extrac-
tors (simplified) learned in different configura-
tions to illustrate boosting extractor confidence
cnfpλ,Gq. Since, an extractor λ is a cluster of
instances, therefore to simplify, we show one in-

33



config1: BREE + simmatch cnfpλ,Gq config5: BRET + simmatch cnfpλ,Gq config9: BREJ + simmatch cnfpλ,Gq config10: BREJ + simasymcc cnfpλ,Gq
acquired

[X] acquired [Y] 0.98 [X] acquired [Y] 1.00 [X] acquired [Y] 1.00 acquired by [X] , [Y] : 0.93
[X] takeover of [Y] 0.89 [X] takeover of [Y] 1.00 [X] takeover of [Y] 0.98 takeover of [X] would boost [Y] ’s earnings : 0.90

[X] ’s planned acquisition of [Y] 0.87 [X] ’s planned acquisition of[Y] 1.00 [X] ’s planned acquisition of [Y] 0.98 acquisition of [X] by [Y] : 0.95
[X] acquiring [Y] 0.75 [X] acquiring [Y] 1.00 [X] acquiring [Y] 0.95 [X] acquiring [Y] 0.95

[X] has owned part of [Y] 0.67 [X] has owned part of [Y] 1.00 [X] has owned part of [Y] 0.88 owned by [X] ’s parent [Y] 0.90
[X] took control of [Y] 0.49 [X] ’s ownership of [Y] 1.00 [X] took control of [Y] 0.91 [X] takes control of [Y] 1.00
[X] ’s acquisition of [Y] 0.35 [X] ’s acquisition of [Y] 1.00 [X] ’s acquisition of [Y] 0.95 acquisition of [X] would reduce [Y] ’s share : 0.90
[X] ’s merger with [Y] 0.35 [X] ’s merger with[Y] 1.00 [X] ’s merger with [Y] 0.94 [X] - [Y] merger between : 0.84

[X] ’s bid for [Y] 0.35 [X] ’s bid for [Y] 1.00 [X] ’s bid for [Y] 0.97 part of [X] which [Y] acquired : 0.83

founder-of
[X] founder [Y] 0.68 [X] founder [Y] 1.00 [X] founder [Y] 0.99 founder of [X] , [Y] : 0.97

[X] CEO and founder [Y] 0.15 [X] CEO and founder [Y] 1.00 [X] CEO and founder [Y] 0.99 co-founder of [X] ’s millennial center , [Y] : 0.94
[X] ’s co-founder [Y] 0.09 [X] owner [Y] 1.00 [X] owner [Y] 1.00 owned by [X] cofounder [Y] 0.95

[X] cofounder [Y] 1.00 [X] cofounder [Y] 1.00 Gates co-founded [X] with school friend [Y] : 0.99
[X] started by [Y] 1.00 [X] started by [Y] 1.00 who co-founded [X] with [Y] : 0.95

[X] was founded by [Y] 1.00 [X] was founded by [Y] 0.99 to co-found [X] with partner [Y] : 0.68
[X] begun by [Y] 1.00 [X] begun by [Y] 1.00 [X] was started by [Y] , cofounder 0.98

[X] has established [Y] 1.00 [X] has established [Y] 0.99 set up [X] with childhood friend [Y] : 0.96
[X] chief executive and founder [Y] 1.00 [X] co-founder and billionaire [Y] � 0.99 [X] co-founder and billionaire [Y] 0.97

headquartered
[X] headquarters in [Y] 0.95 [X] headquarters in [Y] 1.00 [X] headquarters in [Y] 0.98 [X] headquarters in [Y] 0.98

[X] relocated its headquarters from [Y] 0.94 [X] relocated its headquarters from [Y] 1.00 [X] relocated its headquarters from [Y] 0.98 based at [X] ’s suburban [Y] headquarters : 0.98
[X] head office in [Y] 0.84 [X] head office in [Y] 1.00 [X] head office in [Y] 0.87 head of [X] ’s operations in [Y] : 0.65

[X] based in [Y] 0.75 [X] based in [Y] 1.00 [X] based in [Y] 0.98 branch of [X] company based in [Y] 0.98
[X] headquarters building in [Y] 0.67 [X] headquarters building in [Y] 1.00 [X] headquarters building in [Y] 0.94 [X] main campus in [Y] 0.99

[X] headquarters in downtown [Y] 0.64 [X] headquarters in downtown [Y] 1.00 [X] headquarters in downtown [Y] 0.94 [X] headquarters in downtown [Y] 0.96
[X] branch offices in [Y] 0.54 [X] branch offices in [Y] 1.00 [X] branch offices in [Y] 0.98 [X] ’s [Y] headquarters represented : 0.98

[X] ’s corporate campus in [Y] 0.51 [X] ’s corporate campus in [Y] 1.00 [X] ’s corporate campus in [Y] 0.99 [X] main campus in [Y] 0.99
[X] ’s corporate office in [Y] 0.51 [X] ’s corporate office in [Y] 1.00 [X] ’s corporate office in [Y] 0.89 [X] , [Y] ’s corporate : 0.94

affiliation
[X] chief executive [Y] 0.92 [X] chief executive [Y] 1.00 [X] chief executive [Y] 0.97 [X] chief executive [Y] resigned monday 0.94

[X] secretary [Y] 0.88 [X] secretary [Y] 1.00 [X] secretary [Y] 0.94 worked with [X] manager [Y] 0.85
[X] president [Y] 0.87 [X] president [Y] 1.00 [X] president [Y] 0.96 [X] voted to retain [Y] as CEO : 0.98

[X] leader [Y] 0.72 [X] leader [Y] 1.00 [X] leader [Y] 0.85 head of [X] , [Y] : 0.99
[X] party leader [Y] 0.67 [X] party leader [Y] 1.00 [X] party leader [Y] 0.87 working with [X] , [Y] suggested : 1.00

[X] has appointed [Y] 0.63 [X] executive editor [Y] 1.00 [X] has appointed [Y] 0.81 [X] president [Y] was fired 0.90
[X] player [Y] 0.38 [X] player [Y] 1.00 [X] player [Y] 0.89 [X] ’s [Y] was fired : 0.43

[X] ’s secretary-general [Y] 0.36 [X] ’s secretary-general [Y] 1.00 [X] ’s secretary-general [Y] 0.93 Chairman of [X] , [Y] : 0.88
[X] hired [Y] 0.21 [X] director [Y] 1.00 [X] hired [Y] 0.56 [X] hired [Y] as manager : 0.85

Table 11: Subset of the non-noisy extractors (simplified) with their confidence scores cnfpλ,Gq learned in different
configurations for each relation. � denotes that the extractor was never learned in config1 and config5. : indicates
that the extractor was never learned in config1, config5 and config9. [X] and [Y] indicate placeholders for entities.

stance (mostly populated) from every λ. Each cell
in Table 11 represents either a simplified represen-
tation of λ or its confidence. We demonstrate how
the confidence score of a non-noisy extractor in
BREE (config1) is increased in BREJ (config9 and
config10). For instance, for the relation acquired,
an extractor {[X] acquiring [Y]} is generated by
BREE, BRET and BREJ; however, its confidence
is boosted from 0.75 in BREE (config1) to 0.95
in BREJ (config9). Observe that BRET generates
high confidence extractors. We also show extrac-
tors (marked by :) learned by BREJ with simcc
(config10) but not by config1, config5 and config9.

3.7 Entity Pairs: Ordered Vs Bi-Set

In Table 5, we use ordered pairs of typed entities.
Additionally, we also investigate using entity sets
and observe improved recall due to higher #out
in both BREE and BREJ, comparing correspond-
ingly Table 12 and 5 (baseline and config9).

4 Conclusion

We have proposed a Joint Bootstrapping Machine
for relation extraction (BREJ) that takes advantage

Relationships
BREE + simmatch BREJ + simmatch

#out P R F1 #out P R F1

acquired 2786 .90 .50 .64 21733 .80 .87 .83
founder-of 543 1.0 .67 .80 31890 .80 .99 .89

headquartered 16832 .62 .81 .70 52286 .64 .94 .76
affiliation 21812 .99 .74 .85 42601 .96 .85 .90

avg 10493 .88 .68 .75 37127 .80 .91 .85

Table 12: BREX+simmatch:Scores with entity bisets

of both entity-pair-centered and template-centered
approaches. We have demonstrated that the joint
approach scales up positive instances that boosts
the confidence of NNLC extractors and improves
recall. The experiments showed that the cross-
context similarity measures improved recall and
suggest removing in total four parameters.

Acknowledgments

We thank our colleagues Bernt Andrassy, Mark
Buckley, Stefan Langer, Ulli Waltinger and Us-
ama Yaseen, and anonymous reviewers for their
review comments. This research was supported by
Bundeswirtschaftsministerium (bmwi.de), grant
01MD15010A (Smart Data Web) at Siemens AG-
CT Machine Intelligence, Munich Germany.

34



References
Eugene Agichtein and Luis Gravano. 2000. Snow-

ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 15th ACM confer-
ence on Digital libraries. Association for Comput-
ing Machinery, Washington, DC USA, pages 85–94.

Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing. Association for Computational
Linguistics, Beijing, China, volume 1, pages 344–
354.

Hannah Bast, Florian Bäurle, Björn Buchhold, and El-
mar Haußmann. 2014. Easy access to the free-
base dataset. In Proceedings of the 23rd Interna-
tional Conference on World Wide Web. Association
for Computing Machinery, Seoul, Republic of Ko-
rea, pages 95–98.

David S. Batista, Bruno Martins, and Mário J. Silva.
2015. Semi-supervised bootstrapping of relation-
ship extractors with distributional semantics. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Lisbon, Portu-
gal, pages 499–504.

Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In International
Workshop on The World Wide Web and Databases.
Springer, Valencia, Spain, pages 172–183.

Mirko Bronzi, Zhaochen Guo, Filipe Mesquita, De-
nilson Barbosa, and Paolo Merialdo. 2012. Auto-
matic evaluation of relation extraction systems on
large-scale. In Proceedings of the Joint Workshop on
Automatic Knowledge Base Construction and Web-
scale Knowledge Extraction (AKBC-WEKEX). As-
sociation for Computational Linguistics, Montrèal,
Canada, pages 19–24.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
24th National Conference on Artificial Intelligence
(AAAI). Atlanta, Georgia USA, volume 5, page 3.

Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.
2013. Rule-based information extraction is dead!
long live rule-based information extraction systems!
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, Seattle, Wash-
ington USA, pages 827–832.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on

Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics, Ed-
inburgh, Scotland UK, pages 1535–1545.

Pankaj Gupta, Thomas Runkler, Heike Adel, Bernt
Andrassy, Hans-Georg Zimmermann, and Hinrich
Schütze. 2015. Deep learning methods for the ex-
traction of relations in natural language text. Tech-
nical report, Technical University of Munich, Ger-
many.

Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy.
2016. Table filling multi-task recurrent neural net-
work for joint entity and relation extraction. In Pro-
ceedings of the 26th International Conference on
Computational Linguistics: Technical Papers. Os-
aka, Japan, pages 2537–2547.

Sonal Gupta, Diana L. MacLean, Jeffrey Heer, and
Christopher D. Manning. 2014. Induced lexico-
syntactic patterns improve information extraction
from online medical forums. Journal of the Amer-
ican Medical Informatics Association 21(5):902–
909.

Sonal Gupta and Christopher Manning. 2014. Im-
proved pattern learning for bootstrapped entity ex-
traction. In Proceedings of the 18th Confer-
ence on Computational Natural Language Learning
(CoNLL). Association for Computational Linguis-
tics, Baltimore, Maryland USA, pages 98–108.

Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics. Nantes, France, pages 539–545.

Winston Lin, Roman Yangarber, and Ralph Grish-
man. 2003. Bootstrapped learning of semantic
classes from positive and negative examples. In Pro-
ceedings of ICML 2003 Workshop on The Contin-
uum from Labeled to Unlabeled Data in Machine
Learning and Data Mining. Washington, DC USA,
page 21.

Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning. Association
for Computational Linguistics, Jeju Island, Korea,
pages 523–534.

Filipe Mesquita, Jordan Schmidek, and Denilson Bar-
bosa. 2013. Effectiveness and efficiency of open re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Seattle, Washington USA, pages 447–457.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of the Work-
shop at the International Conference on Learning
Representations. ICLR, Scottsdale, Arizona USA.

35



Thien Huu Nguyen and Ralph Grishman. 2015. Rela-
tion extraction: Perspective from convolutional neu-
ral networks. In Proceedings of the 1st Workshop on
Vector Space Modeling for Natural Language Pro-
cessing. Association for Computational Linguistics,
Denver, Colorado USA, pages 39–48.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword. Linguis-
tic Data Consortium .

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Doha, Qatar, pages 1532–1543.

Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings
of the 13th National Conference on Artificial Intelli-
gence (AAAI). Portland, Oregon USA, pages 1044–
1049.

Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the
12th European Conference on Machine Learning.
Springer, Freiburg, Germany, pages 491–502.

Ngoc Thang Vu, Heike Adel, Pankaj Gupta, and Hin-
rich Schütze. 2016a. Combining recurrent and con-
volutional neural networks for relation classifica-
tion. In Proceedings of the 15th Annual Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT). Association for Com-
putational Linguistics, San Diego, California USA,
pages 534–539.

Ngoc Thang Vu, Pankaj Gupta, Heike Adel, and Hin-
rich Schütze. 2016b. Bi-directional recurrent neu-
ral network with ranking loss for spoken language
understanding. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP). IEEE, Shanghai, China,
pages 6060–6064.

36


