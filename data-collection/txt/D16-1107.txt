



















































Fluency detection on communication networks


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1025–1029,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Fluency detection on communication networks

Tom Lippincott and Benjamin Van Durme
Human Language Technology Center of Excellence

Johns Hopkins University
tom@cs.jhu.edu, vandurme@cs.jhu.edu

Abstract

When considering a social media corpus, we
often have access to structural information
about how messages are flowing between peo-
ple or organizations. This information is par-
ticularly useful when the linguistic evidence
is sparse, incomplete, or of dubious quality.
In this paper we construct a simple model to
leverage the structure of Twitter data to help
determine the set of languages each user is flu-
ent in. Our results demonstrate that imposing
several intuitive constraints leads to improve-
ments in performance and stability. We re-
lease the first annotated data set for exploring
this task, and discuss how our approach may
be extended to other applications.

1 Introduction

Language identification (LID) is an important first
step in many NLP pipelines since most downstream
tasks need to employ language-specific resources.
In many situations, LID is a trivial task that can be
addressed e.g. by a simple Naive Bayes classifier
trained on word and character n-gram data (Lui and
Baldwin, 2012): a document of significant length
will be quickly disambiguated based on its vocab-
ulary (King et al., 2014). However, social media
platforms like Twitter produce data sets in which
individual documents are extremely short, and lan-
guage use is idiosyncratic: LID performance on
such data is dramatically lower than on traditional
corpora (Bergsma et al., 2012; Carter et al., 2013).
The widespread adoption of social media throughout
the world amplifies the problem as less-studied lan-
guages lack the annotated resources needed to train

the most effective NLP models (e.g. treebanks for
statistical parsing, tagged corpora for part-of-speech
tagging, etc). All of this motivates the research com-
munity’s continued interest in LID (Zampieri et al.,
2014).

Tweet #1 Êîç ýâåðèñèíã þ äó èñ ìýäæèê
Tweet #2 omg favourite day of the week!

Table 1: Multilingual social media users often communicate in
different languages depending on their intended audience, such

as with these Russian and English tweets posted by the same

Twitter account

In this paper, we consider the closely-related task
of determining an actor’s fluencies, the set of lan-
guages they are capable of speaking and understand-
ing. The observed language data will be the same as
for LID, but is now considered to indicate a latent
property of the actor. This information has a num-
ber of downstream uses, such as providing a strong
prior on the language of the actor’s future communi-
cations, constructing monolingual data sets, and rec-
ommending appropriate content for display or fur-
ther processing.

This paper also focuses on the situation where
a very small amount of content has been observed
from the particular user. While this may seem
strange considering the volume of data generated by
social media, this is dominated by particularly active
users: for example, 30% of Twitter users post only
once per month (Leetaru et al., 2013). This content-
starved situation is exacerbated by certain use-cases,
such as responding to emergency events where sud-
den focus is directed at a particular location, or fo-
cusing on new users with shallow histories.

1025



2 Previous Work

Twitter and other social media platforms are a ma-
jor area of ongoing NLP research, including dedi-
cated workshops(NAA, 2015; ACL, 2014). Previ-
ous work has considered macroscopic properties of
the entire Twitter network (Gabielkov et al., 2014),
and pondered whether it is an “information” or “so-
cial” network (Myers et al., 2014). Studies have fo-
cused on determining user attributes such as gender
(Li et al., 2015), political allegiance (Volkova et al.,
2014), brand affinity (Pennacchiotti and Popescu,
2011a), sentiment analysis (West et al., 2014), and
more abstract roles (Beller et al., 2014). Such demo-
graphic information is known to help downstream
tasks (Hovy, 2015). Research involving social media
communication networks has typically focused on
homophily, the tendency of users to connect to oth-
ers with similar properties (Barberá, 2014). A num-
ber of papers have employed features drawn from
both the content and structure of network entities in
pursuit of latent user attributes (Pennacchiotti and
Popescu, 2011b; Campbell et al., 2014; Suwan et
al., 2015).

3 Definitions

We refer to the entities that produce and consume
communications as Actors, and the communications
(packets of language data) as Messages. Each mes-
sage occurs in a particular Language, and each actor
has a set of Fluencies, representing the ability to pro-
duce and consume a message in a given language.
We refer to a connected graph of such entities as
a Communication Network. For Twitter data, mes-
sages are simply associated with a single actor, who
is in turn associated with other actors via the “fol-
lowing” relationship, the actor’s “friends” in Twit-
ter’s terminology.1 We assume each message (tweet)
is written in a single language, and actors are either
fluent or not in each possible language.

1Note that, confusingly, Twitter’s “friend” relationship is not
symmetric: Mary’s friends are users she has decided to follow,
and not necessarily vise-versa.

4 Twitter Data Set

To build a suitable data set2 for fluency detection,
we first identified 1000 Twitter users who, accord-
ing to the Twitter LID system, have tweeted in Rus-
sian and at least one additional language. For each
of these “seed” users, we gather a local context
(a “snowflake”) as follows: we choose 20 of their
friends at random. For each of these friends, we
choose 15 of their friends (again, at random). Fi-
nally, we randomly pull 200 tweets for each identi-
fied user. The data set consists of 989 seed users,
165,042 friends, and 55,019,811 tweets. We pre-
serve all Twitter meta-data for the users and tweets,
such as location, follower count, hashtags, etc,
though for the purposes of this paper we are only
interested in the friendship structure and message
text. We then had an annotator determine the set
of languages each of the 1000 seed users is fluent
in. For each seed user, the annotator was presented
with their 200 tweets, grouped by Twitter language
ID, and was asked to 1) flag users that appear to be
bots and 2) list the languages they believe the user
is fluent in. These steps are reflected in Figure 4.
Over 50% (507) of the users were flagged as pos-
sible bots and not used in this study. The remaining
482 were observed employing 7 different languages:
Russian, Ukrainian, German, Polish, Bulgarian, Lat-
vian, and English. At most, a single user was found
to be fluent in three languages.

Figure 1: Structure of one snowflake in the Twitter Fluency
data set.

5 Structure-Aware Fluency Model

Our goal was to explicitly model each actor’s flu-
ency in different languages, using a model with sim-

2The full data set is available at www.university.edu/
link

1026



ple, interpretable parameters that can be used to en-
code well-motivated assumptions about the data. In
particular, we want to bias the model towards the
belief that actors typically speak a small number of
languages, and encode the belief that all actors par-
ticipating in a message are highly likely to be fluent
in its language. Our basic hypothesis is that, in ad-
dition to scores from traditional LID modules, such
a model will benefit from considering the behavior
of an actor’s interlocutors. To test this, we designed
a model that employs scores from an existing LID
system, and compare performance with and without
awareness of the communication network structure.
To demonstrate the effectiveness of the model in sit-
uations with sparse or unreliable linguistic content,
we perform experiments where the number of mes-
sages associated with each actor has been randomly
down-sampled.

Linear Programming Linear Programming (LP)
is a method for specifying constraints and cost func-
tions in terms of linear relationships between vari-
ables, and then finding the optimal solution that re-
spects the constraints. The restriction to linear equa-
tions ensures that the objective function is itself lin-
ear, and can be efficiently solved. If some or all
variables are restricted to take discrete values, re-
ferred to as (Mixed) Integer Linear Programming
(ILP), finding a solution becomes NP-hard, though
common special cases remain efficiently solvable.
We specify our model as an ILP with the hope
that it provides sufficient expressiveness for the
task, while remaining intuitive and tractable. Infer-
ence is performed using the Gurobi modeling toolkit
(Gurobi Optimization, 2015).

Model definition Given a communication net-
work with no LID information, ideally we would
like to determine the language of each message, and
the set of languages each actor is fluent in. Ini-
tially, we assume access to a probablistic LID sys-
tem that maps unicode text to a distribution over
possible languages. We use the following notation:
A1:T and M1:U are the actors and messages, respec-
tively. F (ai) is a binary vector indicating which lan-
guages we believe actor ai is fluent in. L(mi) is a
one-on binary vector indicating which language we
believe message mi is written in. P (mi) is the set of
actors participating in message mi: for Twitter data,

where messages are (usually) not directed at specific
users, we treat a user and the users’ friends as par-
ticipants. LID(mi) is a real vector representing the
probability of message mi being in each language,
according to the LID system.

To build our ILP model, we iterate over actors
and messages, defining constraints and the objective
function as we go. There are two types of structural
constraints: first, we restrict each message to have a
single language assignment:

∑
L(mi) = 1 (1)

Second, we ensure that all actors participating in
a given message are fluent in its language:

∀a ∈ P (mi), L(mi)× F (a) = 1 (2)

The objective function also has two components:
first, the language fit encourages the model to assign
each message a language that has high probability
according the the LID system:

LF =
∑

m∈M
L(m)× LID(m) (3)

Second, the structure fit minimizes the cardinal-
ity of the actors’ fluency sets (subject to the struc-
tural constraints), and thus avoids the trivial solution
where each actor is fluent in all languages:

SF = −
∑

a∈A

∑
F (a) (4)

Finally, the two components of the objective func-
tion are combined with an empirically-determined
language weight to get the complete objective func-
tion:

LW× LF + (1.0− LW)× SF (5)

Note that these are not all linear relationships:
in particular, the multiplication operator cannot be
used in ILP when the operands are both variables,
as in equation 2. There are however techniques that
can represent these situations in a linear program by
introducing helper variables and constraints (Biss-
chop, 2015).

1027



Language Identification Scores and Fluency
Baseline To get LID scores, we ran the VaLID sys-
tem (Bergsma et al., 2012) on each message, and
normalize the output into distributions over 261 pos-
sible languages. VaLID is trained on Wikipedia data
(i.e. out-of-domain relative to Twitter), although
it does employ hand-specified rules for sanitizing
tweet text, such as normalizing whitespace and re-
moving URLs and user tags. VaLID uses a data-
compression approach that is competitive with Twit-
ter’s in-house LID, despite no consideration of geo-
graphic or user priors. These language scores are
used in the structure-aware model to compute the
language fit.

Because VaLID makes no use of the communi-
cation network structure, we also use its scores to
create a baseline structure-unaware fluency model.
To get structure-unaware baseline scores for the flu-
ency identification task, we average the LID distri-
butions for each actor’s messages and consider them
fluent in a language if its probability is above an
empirically-determined threshold.

Tuning parameters We empirically determine the
thresholds for the baseline model and the language
weights for the structure-aware model via a simple
grid search, repeated 100 times. We randomly split
the data into 20%/80% tune/test sets, and evaluate
filter thresholds and language weights from 0 to 1
in .01 increments, with messages per actor ranging
between 1 and 10. We expected the baseline model
to have a consistent optimal threshold (though with
higher performance variance with fewer messages),
and this was borne out with optimal performance at
a threshold of 0.06, independent of the number of
messages per actor. For the structure-aware model,
the optimal language weight was 0.9, although the
entire range from 0.1–0.9 showed similar perfor-
mance. This result was surprising, as we expect the
structure-aware model to rely heavily on the struc-
tural fit when the number of messages is small, and
on the language fit when the number is large. This
trend doesn’t emerge because the structural fit actu-
ally relies on the language fit to make assignments
for the seed actor’s friends and their messages.

Figure 2: Performance of baseline and structure-aware models
as a function of the number of messages per actor used as ev-

idence. Each bar represents the average over 100 random tun-

ing/testing splits, with whiskers showing the standard deviation.

6 Results and discussion

Figure 2 compares the performance3 of the
structure-aware ILP model with the baseline model
as a function of the number of messages per ac-
tor, using the empirically-determined threshold and
language weight. At the left extreme, the models
only have a single, randomly-selected message from
each actor. As this number increases, the baseline
model improves as it becomes more likely to have
seen enough messages to reflect the actor’s full spec-
trum of language use. The structure-aware model is
able to make immediate use of the actor’s friends,
immediately reaching high performance even when
the language data is very sparse. Its most frequent
type of error is over-hypothesizing fluency in both
Ukrainian and Russian, when the user is in fact
monolingual, followed by incorrectly hypothesizing
fluency in English. This is understandable given the
similarity of the languages in the former case, and
the popularity of English expressions, titles, and the
like in the latter.

7 Conclusion

We have presented promising results from lever-
aging structural information from a communica-
tion network to improve performance on fluency
detection in situations where direct linguistic data
is sparse. In addition to defining the task itself,

3F-score calculated based on correct and hypothesized
fluency-assignments for each actor.

1028



we release an annotated data set for training and
evaluating future models. Planned future work in-
cludes a more flexible decoupling of the language
and structure fits (in light of Section 5), and mov-
ing from pre-existing LID systems to joint models
where LID scores are directly informed by structural
information.

References
2014. ACL Joint Workshop on Social Dynamics and Per-

sonal Attributes in Social Media.
Pablo Barberá. 2014. Birds of the same feather tweet

together: Bayesian ideal point estimation using twitter
data. Political Analysis, 23:76–91.

Charley Beller, Rebecca Knowles, Craig Harman, Shane
Bergsma, Margaret Mitchell, and Benjamin Van
Durme. 2014. I’m a belieber: Social roles via self-
identification and conceptual attributes. In Proceed-
ings of the 52rd Annual Meeting of the Association
for Computational Linguistics, pages 181–186, Balti-
more, Maryland, USA.

Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proc. Second Workshop on Language
in Social Media, pages 65–74.

Johannes Bisschop. 2015. Aimms optimization model-
ing.

W.M. Campbell, E. Baseman, and K. Greenfield. 2014.
langid.py: An off-the-shelf language identification
tool. In Proceedings of the Second Workshop on Natu-
ral Language Processing for Social Media, pages 59–
65, Dublin, Ireland.

Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog language identification: Overcoming
the limitations of short, unedited and idiomatic text.
Lang. Resour. Eval., 47(1):195–215, March.

Maksym Gabielkov, Ashwin Rao, and Arnaud Legout.
2014. Studying social networks at scale: Macroscopic
anatomy of the twitter social graph. In SIGMETRICS
’14, Austin, Texas, USA.

Inc. Gurobi Optimization. 2015. Gurobi optimizer refer-
ence manual.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics, pages 752–762, Beijing, China.

Ben King, Dragomir Radev, and Steven Abney. 2014.
Experiments in sentence language identification with
groups of similar languages. In Proceedings of the
First Workshop on Applying NLP Tools to Similar

Languages, Varieties and Dialects, pages 146–154,
Dublin, Ireland.

Kalev Leetaru, Shaowen Wang, Guofeng Cao, Anand
Padmanabhan, and Eric Shook. 2013. Mapping the
global twitter heartbeat: The geography of twitter.
First Monday, 18(5).

Shoushan Li, Jingjing Wang, Guodong Zhou, and Hanx-
iao Shi. 2015. Interactive gender inference with in-
teger linear programming. In Proceedings of the 24th
International Conference on Artificial Intelligence, IJ-
CAI’15, pages 2341–2347. AAAI Press.

Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 25–30, Jeju, Repub-
lic of Korea.

Seth A. Myers, Aneesh Sharma, Pankaj Gupta, and
Jimmy Lin. 2014. Information network or social net-
work? the structure of the twitter follow graph. In
WWW ’14 Companion, Seoul, Korea.

2015. NAACL International Workshop on Natural Lan-
guage Processing for Social Media.

Marco Pennacchiotti and Ana-Maria Popescu. 2011a.
Democrats, republicans and starbucks afficionados:
User classification in twitter. In KDD ’11, San Diego,
California, USA.

Marco Pennacchiotti and Ana-Maria Popescu. 2011b. A
machine learning approach to twitter user classifica-
tion. In Proceedings of the Fifth International AAAI
Conference on Weblogs and Social Media, pages 281–
288.

Shakira Suwan, Dominic Lee, and Carey Priebe. 2015.
Bayesian vertex nomination using content and context.
WIREs Comput Stat, 7:400–416.

Svitlana Volkova, Glen Coppersmith, and Benjamin Van
Durme. 2014. Inferring user political preferences
from streaming communications. In Proceedings of
the 52rd Annual Meeting of the Association for Com-
putational Linguistics, pages 186–196, Baltimore,
Maryland, USA.

Robert West, Hristo Paskov, Jure Leskovec, and Christo-
pher Potts. 2014. Exploiting social network struc-
ture for person-to-person sentiment analysis. Transac-
tions of the Association for Computational Linguistics,
2:297–310.

Marcos Zampieri, Liling Tang, Nikola Ljubešić, and Jörg
Tiedemann. 2014. Discriminating similar languages
shared task at coling 2014.

1029


