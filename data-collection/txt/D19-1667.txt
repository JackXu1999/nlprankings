



















































Charge-Based Prison Term Prediction with Deep Gating Network


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 6362–6367,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

6362

Charge-Based Prison Term Prediction with Deep Gating Network

Huajie Chen1∗ Deng Cai2∗ Wei Dai1 Zehui Dai1 Yadong Ding1
1NLP Group, Gridsum, Beijing, China

{chenhuajie,daiwei,daizehui,dingyadong}@gridsum.com
2The Chinese University of Hong Kong

thisisjcykcd@gmail.com

Abstract

Judgment prediction for legal cases has at-
tracted much research efforts for its practice
use, of which the ultimate goal is prison term
prediction. While existing work merely pre-
dicts the total prison term, in reality a defen-
dant is often charged with multiple crimes.
In this paper, we argue that charge-based
prison term prediction (CPTP) not only bet-
ter fits realistic needs, but also makes the to-
tal prison term prediction more accurate and
interpretable. We collect the first large-scale
structured data for CPTP and evaluate sev-
eral competitive baselines. Based on the ob-
servation that fine-grained feature selection is
the key to achieving good performance, we
propose the Deep Gating Network (DGN) for
charge-specific feature selection and aggrega-
tion. Experiments show that DGN achieves the
state-of-the-art performance.

1 Introduction

Judgment prediction (Kort, 1957; Ulmer, 1963;
Segal, 1984; Liu et al., 2004; Liu and Hsieh, 2006)
aims at automatically predicting the judgment re-
sult given a textual description of a legal case (An
example is given in Figure 1). Recently, there has
been a resurgent interest in this task due to the
availability of more data and new machine learn-
ing techniques (Luo et al., 2017; Zhong et al.,
2018b; Hu et al., 2018).

Judgment prediction can be decomposed into
several sub-tasks: (a) relevant law article extrac-
tion (Liu and Hsieh, 2006; Liu and Liao, 2005; Liu
et al., 2015), (b) charge prediction (Liu and Hsieh,
2006; Luo et al., 2017; Hu et al., 2018), (c) and
prison term prediction (Zhong et al., 2018a). The
dependencies among them have also been studied
by Zhong et al. (2018b). While effective methods
exist for sub-task (a) and (b), (e.g In CAIL2018

∗Both authors contributed equally.

Case description: On July 7, 2017, when the defendant Cui
XX was drinking in a bar, he came into conflict with Zhang
XX…… After arriving at the police station, he refused to
cooperate with the policeman and bited on the arm of the
policeman……

Result of judgment: Cui XX was sentenced to 12 months
imprisonment for creating disturbances and 12 months
imprisonment for obstructing public affairs……

l Charge#1 creating disturbances term 12 months

l Charge#2 obstructing public affairs term 12 months

Figure 1: An example of judgment prediction.

competition (Zhong et al., 2018a), both the charge
prediction and the article prediction have attained
Fmicro over 95%), the prison term prediction re-
mains the performance bottleneck.

In this paper, we improve the accuracy of prison
term prediction by decomposing it into a set of
charge-based prison term predictions (CPTPs). In
this way, more subtle and sophisticated interac-
tions between textual description and a specific
charge can be captured, resulting in more precise
term predictions for individual charges. Mean-
while, CPTPs also shed light on the prediction of
the total prison term.

On the other hand, CPTP also poses challenges
due to the following reasons: The case description
can be very lengthy and not all parts are relevant to
a specific charge. The charge-related descriptions
are often presented in an interleaving way, making
it difficult to associate a specific charge with its
corresponding information.

To address the above problems, we propose the
Deep Gating Network (DGN) for gradually fil-
tering and aggregating charge-specified informa-
tion at different levels of granularity. Specifically,
we stack multiple blocks of an LSTM layer and
a charge-specific gating layer for generating a fo-
cused charge-based representation of the case de-
scription. Finally, the whole document representa-



6363

tion is obtained by a convolutional neural network.
To conduct the experiments, we construct a new

dataset, which contains more than 200, 000 crim-
inal cases.1 To show the effectiveness of the
proposed approach, we compare it with several
strong baselines adapted from aspect-based senti-
ment classification (Wang et al., 2016; Tang et al.,
2016; Chen et al., 2017; Li et al., 2018). Experi-
ments show that our method achieves significantly
better results than all of them. In addition, when
we leverage the results of charge-based term pre-
dictions for the total prison term prediction, it also
surpasses several strong baselines that are directly
aimed at the total term prison prediction.

In summary, our contributions are as follows:

• We formally define the task of charge-based
prison term prediction and collect the first
dataset for it.

• We propose the Deep Gating Network
(DGN). Experiments show our method
achieves the state-of-the-art performance.

• We show that the accuracy of the total
term prediction is also improved by a sim-
ple heuristic integration of individual charge-
based term predictions.

2 Problem Definition & Dataset
Construction

We formally define the task of charge-based prison
term prediction as follows. The input are a case
description x = {x1, x2, · · · , xn} and a set of cor-
responding charges c = {c1, c2, · · · , ck}, where
n and k are the length of case description and the
number of charges respectively. The goal is to pre-
dict the prison terms y = {y1, y2, · · · , yk}, where
yj is the prison term corresponding to charge cj .

To the best of our knowledge, there is no exist-
ing structured dataset for the above task. We thus
collect and construct a dataset based on the pub-
lished records from the Supreme People’s Court
of China,2 where each criminal case document
includes the accusation by the procuratorate, the
court view, and the result of judgment. Follow-
ing Xiao et al. (2018), we take the accusation by
the procuratorate as the input textual description.
The charges and the corresponding prison terms

1The dataset can be found at https://github.com/
huajiechen/CPTP

2 http://wenshu.court.gov.cn/

#single #multiple total

Train 147, 580 42, 420 190, 000
Valid 19, 350 5, 602 24, 952
Test 18, 539 5, 258 23, 797

Table 1: The statistics of the proposed dataset. #sin-
gle/#multiple means single/multiple charge(s) cases.

Conv

Output

L

…

  

Charge 

Embedding

Word

Embedding

LSTM Layer

Gating Layer…

Figure 2: The architecture of DGN

are extracted from the result of judgment using
regular expressions like “sentence to months
imprisonment for ”. We build 238,749 well-
structured cases in total (An example is given in
Fig 1). The collected cases are further split into
the training set, the validation set, and the test set.
The statistics of the dataset are detailed in Table
1. The range of possible prison terms is [1, 240]
(in months). The dataset has a broad coverage of
common charges, 157 different types of charges
are involved.

3 Our Approach

Figure 2 gives an overview of our model, which
consists of two components: (1) Deep Gating
Network (DGN) for charge-based feature filtering
and aggregation. (2) Convolutional Neural Net-
work (CNN) for the whole document representa-
tion learning.

3.1 Deep Gating Network
At the bottom layer of DGN, each word xi is
mapped into a low dimensional vector h(0)i accord-
ing to a word embedding table.

DGN then starts to construct charge-specific
representations gradually. L identical blocks are

https://github.com/huajiechen/CPTP
https://github.com/huajiechen/CPTP
http://wenshu.court.gov.cn/


6364

hierarchically stacked. The l-th block takes the
output of the (l − 1)-th layer hl−1i as input. Each
block transforms its input semantic vectors into
more sophisticated and focused representations
based on gated feature selection and combination.

Specifically, each gating block consists of a bi-
LSTM layer for context aggregation and a gating
layer for charge-specific feature filtering.

h̃
(l)
i = [

−−−−→
LSTM(h(l−1)i );

←−−−−
LSTM(h(l−1)i )]

h
(l)
k = g

(l)
i � h̃

(l)
i

where g(l)i is the gate for i-th vector of the l-th gat-
ing block and � denotes element-wise multiplica-
tion. g(l)i is computed as:

g
(l)
i = sigmoid(W

(l)[h̃
(l)
i ; cj ] + b

(l))

where cj is target charge embedding. The gating
layers can select the charge-specific features ac-
cording to the target charge embedding.

3.2 Convolutional Neural Network

Convolutional Neural Network (CNN) has been
effective in modeling sequential data (Kim, 2014;
Hu et al., 2014; Pang et al., 2016). It uses con-
volution operations (with multiple groups of fil-
ters) for n-gram feature extraction. The sequence-
level representation is then obtained through max-
pooling, where the most salient n-gram features
are detected and selected.

In this work, we use a CNN with filter width in
[1, 2, 3, 4, 5]. The number of filters for each width
is 256. We concatenate the outputs of different
filters for the final document representation z.

3.3 Output and Training

The charge-specific document representation z is
passed to a fully connected layer with ReLU acti-
vation for the final prediction.

ŷj = ReLU(Woz + bo) (1)

where Wo and bo are trainable parameters.
Since the Mean Squared Error (MSE) loss can-

not reflect the relative deviation ratio between the
prediction and the ground-truth, we take the loga-
rithm before estimating their difference.

δj = | log(yj + 1)− log(ŷj + 1)| (2)

To alleviate the impact of outliers and stabilize
the training, we propose to use Huber Loss (Hu-
ber, 1964), a is set to 1 in experiments:

l(x, yj) =

{
0.5× δ2j , if δj < a
a(δj − 0.5a), otherwise

L(x, y) =
k∑

j=1

l(x, yj)

Total Term Prediction Although our model is
trained to predict the prison term for specific
charge, it can be readily adapted to predict the total
term by a simple heuristic integration of individ-
ual charge-based prison term predictions. There
are certain regulations for combined punishment
of crimes in Chinese legislation. For simplicity,
we take the average of the maximum and sum-
mation of individual charge-specific term predic-
tions. The total term prediction is also capped at
240 months.

ŷtotal = min(240,
maxj(ŷj) +

∑
j(ŷj)

2
)

4 Experiments

4.1 Evaluation Metrics
For evaluation, we adopt the official score function
(S metric) of the CAIL2018 Competition (Zhong
et al., 2018a). The score function measures the
log different δ between prediction value ŷ and gold
value y as in Eq 2. The final score s(δ) is a piece-
wise function that increases monotonically with
the value of δ. For more details about the S met-
ric, we refer interested readers to (Zhong et al.,
2018a). We also report the exact match (EM) rate
and error-tolerant accuracy Acc@p, where p is the
maximum acceptable error rate. Formally, a pre-
diction is considered “correct” if and only if its
value is in the range [y(1− p), y(1 + p)].

4.2 Compared Methods
The task of charged-based prison term prediction
is similar in spirit to aspect-based sentiment clas-
sification (Pang et al., 2008), where multiple clas-
sification decisions are made given one text de-
scription and different target entities. This sug-
gests that other neural architectures proposed for
aspect-based sentiment classification may also be
suitable for our task. The adaption from classifi-
cation to regression can be easily accomplished by
replacing the original final layer with that of Eq 1.
Specifically, we adapted the following models:



6365

Model S EM Acc@0.1 Acc@0.2

ATE-LSTM 66.49 7.72 16.12 33.89
MemNet 70.23 7.52 18.54 36.75
RAM 70.32 7.97 18.87 37.38
TNet 73.94 8.06 19.55 39.89
DGN 76.48 8.92 20.66 42.61

Table 2: Results on charge-based prison term predic-
tion(%).

• ATAE-LSTM (Wang et al., 2016): it con-
catenates aspect embedding and the output
of LSTM, and uses self-attention to obtain
aspect-based representation.

• MemNet (Tang et al., 2016): it uses multi-
hop attention over the word embeddings for
a sentence, where aspect embedding is re-
garded as the initial key.

• RAM (Chen et al., 2017): it also uses multi-
hop attention for aspect-specific representa-
tion learning, while the attention at different
time steps are aggregated by recurrent neural
network.

• TNet (Li et al., 2018): it has a similar ar-
chitecture to DGN. The major difference is
that it employs a Transformation Network for
mixing the information in aspect embedding
and token representations rather than the ex-
plicit gates in our model.

The aspect embedding in above models is replaced
by charge embedding in our experiments. In addi-
tion, we also compare with the popular models for
total term prediction (Zhong et al., 2018a,b):

• CNN (Kim, 2014): the case description is en-
coded by a CNN with multiple filter widths,
followed by max-pooling.

• RNN (Hochreiter and Schmidhuber, 1997):
bi-LSTM are used for case description en-
coding, where the final states are regarded as
the document representation.

• RCNN (Lai et al., 2015): we stack a CNN on
the top of LSTM states for final representa-
tion.

4.3 Main Results
The results of charge-based prison term predic-
tion are shown in Table 2. The proposed DGN

Model S EM Acc@0.1 Acc@0.2

CNN 67.24 8.41 16.96 35.58
RNN 67.27 8.04 16.79 35.11
RCNN 69.56 8.54 17.57 35.75
DGN 75.74 8.64 19.32 40.43

Table 3: Results on total term prediction(%).

� � � � � �
�����������

	���

	���

	���

	���

		��

�

��

��

��

��

��

��

�
��


��
�

�
���
���

Figure 3: Performance with different depths of DGN.

achieves the best results on all four metrics. In
addition, the margins between our model and oth-
ers are remarkably wide. It can be observed that
aspect-based sentiment models only give moder-
ate performance, which we attribute to that the
case description is so long that more rigorous fea-
ture selection, such as the treatment of DGN, is
needed. Our model selects and aggregates features
in a explicit way which is more efficient and ef-
fective in dealing with charge-specific descriptions
often spread out across lengthy case documents in
CPTP.

Table 3 presents the results of the total term
prediction. Although our method is not directly
trained to make the final prediction, the perfor-
mance of our model surpasses all baselines, which
confirms that the breakdown charge-based analy-
sis can indeed help the total prison term prediction.

4.4 Depth of DGN
To study the impact of the number of DGN blocks,
we test our model with various depths and show
the results in Fig 3.3 As shown, the performance
improves as the depth of DGN increases until it
reaches 3 when the performance begins to drop
likely due to overfitting.

4.5 Effects of Log Huber Loss
We compare Log Huber Loss (LHL) with Mean
Square Error (MSE), Mean Absolute Error (MAE)
and Huber Loss (HL). We also try Log Cosh Loss

3For simplicity, we only show S score and Acc@0.2.



6366

 	� ������� ����������

���

���

���

���
�	
��	


�
�
�

Figure 4: Compare of different loss functions. The re-
sult of LHL is regarded as unit 1.

(LCL), but it does not converge. As shown in Fig-
ure 4, Log Huber Loss performs best in all metrics
for all models. The improvement is most signifi-
cant in S metric. It also suggests that making the
loss function consistent with the evaluation metric
is beneficial.

4.6 Error Analysis

So far, our model has the best results on prison
term predictions. In this section, we aim to con-
duct an in-depth analysis and answer the follow-
ing questions: (1) In which cases, our model fails
to deliver accurate predictions? (2) What are the
prospects for further improvement? After care-
fully analyzing 100 examples, we roughly classify
them into the following categories.

Lengthy Description Some cases are extremely
complicated, especially for cases with gangs.
These descriptions are often lengthy and involve
multiple criminal suspects.

Incomplete Information In some cases, the in-
put case description does not contain sufficient in-
formation for precise prediction. Note we only
take the accusation by the procuratorate as input,
which is incomplete compared to the whole mate-
rials relevant to a case. For example, if a defendant
is recidivism within a shorter period, he/she shall
be given a heavier punishment.

Rare Cases Some special circumstances will in-
fluence the prison term, yet rarely happen in the
training set. For example, if a defendant cause in-
juries to others due to excessive defense, he/she
shall be given a lighter punishment. This knowl-
edge is easily understandable by humans, bu hard
to be learned by machine learning models.

5 Ethical Discussions

Although the research on prison term prediction
has considerable potential to improve efficiency
and fairness in criminal justice, there are certain
ethical concerns worth discussions.

First, does the training data provide unbiased
examples and sufficient? For example, some may
worry about that the model may treat people dif-
ferently based on race, social class, age and so on
(Tonry, 2014). Discrimination in the past may be
learned in models. Also, with the development of
our society, new forms of crimes will appear. A
model trained on historical data may fail in these
new cases.

Second, is the learned system robust enough?
Some subtle details may significantly affect the
result of judgment. For example, the amount of
theft and the number of drugs, these numerical val-
ues are often not uniform in different case descrip-
tions, causing it hard to learn by neural models.
Some infrequent words, such as named entities,
may also cause undesirable interference.

The mistake of legal judgment is serious, it is
about people losing years of their lives in prison,
or dangerous criminals being released to reoffend.
We should pay attention to how to avoid judges’
over-dependence on the system. It is necessary to
consider its application scenarios. In practice, we
recommend deploying our system in the “Review
Phase”, where other judges check the judgment re-
sult by a presiding judge. Our system can serve as
one anonymous checker.

In summary, the judgment prediction is an
emerging technology at its exploratory stage. We
should be aware of the risks and prevent any inap-
propriate use of the technology.

6 Conclusion

In this paper, we formally presented the task of
charge-based prison term prediction. We intro-
duced the first large-scale dataset for this task. To
tackle the problem of the noisy and entangled de-
scription of legal cases, we proposed the deep gat-
ing network for charge-specific information filter.
Experiments show that our model significantly im-
proves the accuracy of charge-based prison term
prediction, as well as the total term prediction. Fi-
nally, we discussed some ethical problems of the
proposed techniques that are worth cautious think-
ing.



6367

References
Peng Chen, Zhongqian Sun, Lidong Bing, and Wei

Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 452–461.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
Advances in neural information processing systems,
pages 2042–2050.

Zikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, and
Maosong Sun. 2018. Few-shot charge prediction
with discriminative legal attributes. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics, pages 487–498.

Peter J. Huber. 1964. Robust estimation of a loca-
tion parameter. Annals of Mathematical Statistics,
35(1):73–101.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751.

Fred Kort. 1957. Predicting supreme court decisions
mathematically: A quantitative analysis of the “right
to counsel” cases. American Political Science Re-
view, 51(1):1–12.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
classification. In Twenty-ninth AAAI conference on
artificial intelligence.

Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018.
Transformation networks for target-oriented senti-
ment classification. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 946–
956.

Chao-Lin Liu, Cheng-Tsung Chang, and Jim-How Ho.
2004. Case instance generation and refinement for
case-based criminal summary judgments in chinese.

Chao-Lin Liu and Chwen-Dar Hsieh. 2006. Exploring
phrase-based classification of judicial documents for
criminal charges in chinese. In International Sym-
posium on Methodologies for Intelligent Systems,
pages 681–690. Springer.

Chao-Lin Liu and Ting-Ming Liao. 2005. Classifying
criminal charges in chinese for web-based legal ser-
vices. In Asia-Pacific Web Conference, pages 64–75.
Springer.

Yi-Hung Liu, Yen-Liang Chen, and Wu-Liang Ho.
2015. Predicting associated statutes for legal prob-
lems. Information Processing & Management,
51(1):194–211.

Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang
Zhang, and Dongyan Zhao. 2017. Learning to pre-
dict charges for criminal cases with legal basis. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
2727–2736.

Bo Pang, Lillian Lee, et al. 2008. Opinion mining and
sentiment analysis. Foundations and Trends R© in In-
formation Retrieval, 2(1–2):1–135.

Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu,
Shengxian Wan, and Xueqi Cheng. 2016. Text
matching as image recognition. In Thirtieth AAAI
Conference on Artificial Intelligence.

Jeffrey A Segal. 1984. Predicting supreme court cases
probabilistically: The search and seizure cases,
1962-1981. American Political Science Review,
78(4):891–900.

Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect
level sentiment classification with deep memory net-
work. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 214–224.

Michael Tonry. 2014. Legal and ethical issues in the
prediction of recidivism. Federal Sentencing Re-
porter, 26(3):167–176.

S Sidney Ulmer. 1963. Quantitative analysis of judi-
cial processes: Some practical and theoretical appli-
cations. Law & Contemp. Probs., 28:164.

Yequan Wang, Minlie Huang, xiaoyan zhu, and
Li Zhao. 2016. Attention-based lstm for aspect-level
sentiment classification. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 606–615.

Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao
Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xi-
anpei Han, Zhen Hu, Heng Wang, and Jianfeng Xu.
2018. Cail2018: A large-scale legal dataset for judg-
ment prediction.

Haoxi Zhong, Chaojun Xiao, Zhipeng Guo, Cunchao
Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xi-
anpei Han, Zhen Hu, Heng Wang, and Jianfeng Xu.
2018a. Overview of cail2018: Legal judgment pre-
diction competition.

Haoxi Zhong, Guo Zhipeng, Cunchao Tu, Chaojun
Xiao, Zhiyuan Liu, and Maosong Sun. 2018b. Le-
gal judgment prediction via topological learning.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
3540–3549. Association for Computational Linguis-
tics.

https://doi.org/10.18653/v1/D17-1047
https://doi.org/10.18653/v1/D17-1047
https://doi.org/10.3115/v1/D14-1181
https://doi.org/10.3115/v1/D14-1181
http://aclweb.org/anthology/P18-1087
http://aclweb.org/anthology/P18-1087
http://aclweb.org/anthology/D17-1289
http://aclweb.org/anthology/D17-1289
https://doi.org/10.18653/v1/D16-1021
https://doi.org/10.18653/v1/D16-1021
https://doi.org/10.18653/v1/D16-1021
https://doi.org/10.18653/v1/D16-1058
https://doi.org/10.18653/v1/D16-1058
http://arxiv.org/abs/1807.02478
http://arxiv.org/abs/1807.02478
http://arxiv.org/abs/1810.05851
http://arxiv.org/abs/1810.05851
http://aclweb.org/anthology/D18-1390
http://aclweb.org/anthology/D18-1390

