



















































EM-Training for Weighted Aligned Hypergraph Bimorphisms


Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 60–69,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

EM-Training for Weighted Aligned Hypergraph Bimorphisms

Frank Drewes
Department of Computing Science

Umeå University
S-901 87 Umeå, Sweden
drewes@cs.umu.se

Kilian Gebhardt and Heiko Vogler
Department of Computer Science
Technische Universität Dresden

D-01062 Dresden, Germany
kilian.gebhardt@tu-dresden.de
heiko.vogler@tu-dresden.de

Abstract

We develop the concept of weighted
aligned hypergraph bimorphism where the
weights may, in particular, represent proba-
bilities. Such a bimorphism consists of an
R≥0-weighted regular tree grammar, two
hypergraph algebras that interpret the gen-
erated trees, and a family of alignments
between the two interpretations. Seman-
tically, this yields a set of bihypergraphs
each consisting of two hypergraphs and
an explicit alignment between them; e.g.,
discontinuous phrase structures and non-
projective dependency structures are bihy-
pergraphs. We present an EM-training al-
gorithm which takes a corpus of bihyper-
graphs and an aligned hypergraph bimor-
phism as input and generates a sequence
of weight assignments which converges to
a local maximum or saddle point of the
likelihood function of the corpus.

1 Introduction

In natural language processing alignments play an
important role. For instance, in machine translation
they show up as hidden information when training
probabilities of dictionaries (Brown et al., 1993) or
when considering pairs of input/output sentences
derived by a synchronous grammar (Lewis and
Stearns, 1968; Chiang, 2007; Shieber and Schabes,
1990; Nederhof and Vogler, 2012). As another
example, in language models for discontinuous
phrase structures and non-projective dependency
structures they can be used to capture the connec-
tion between the words in a natural language sen-
tence and the corresponding nodes of the parse tree
or dependency structure of that sentence.

In (Nederhof and Vogler, 2014) the generation
of discontinuous phrase structures has been for-

malized by the new concept of hybrid grammar.
Much as in the mentioned synchronous grammars,
a hybrid grammar synchronizes the derivations
of nonterminals of a string grammar, e.g., a lin-
ear context-free rewriting system (LCFRS) (Vijay-
Shanker et al., 1987), and of nonterminals of a
tree grammar, e.g., regular tree grammar (Brainerd,
1969) or simple definite-clause programs (sDCP)
(Deransart and Małuszynski, 1985). Additionally it
synchronizes terminal symbols, thereby establish-
ing an explicit alignment between the positions of
the string and the nodes of the tree. We note that
LCFRS/sDCP hybrid grammars can also generate
non-projective dependency structures.

In this paper we focus on the task of training an
LCFRS/sDCP hybrid grammar, that is, assigning
probabilities to its rules given a corpus of discon-
tinuous phrase structures or non-projective depen-
dency structures. Since the alignments are first
class citizens, we develop our approach in the
general framework of hypergraphs and hyperedge
replacement (HR) (Habel, 1992). We define the
concepts of bihypergraph (for short: bigraph) and
aligned HR bimorphism. A bigraph consists of
hypergraphs H1, λ, and H2, where λ represents
the alignment between H1 and H2. A bimorphism
B = (g,A1,Λ,A2) consists of a regular tree gram-
mar g generating trees over some ranked alphabet
Σ, two Σ-algebrasA1 andA2 which interpret each
symbol in Σ as an HR operation (thus evaluating
every tree to two hypergraphs), and a Σ-indexed
family Λ of alignments between the two interpreta-
tions of each σ ∈ Σ. The semantics of B is a set of
bigraphs.

For instance, each discontinuous phrase struc-
ture or non-projective dependency structure can be
represented as a bigraph (H1, λ,H2) whereH1 and
H2 correspond to the string component and the tree
component, respectively. Fig. 1 shows an example
of a bigraph representing a non-projective depen-

60



(a)

A hearing is scheduled on the issue today .

is
hearing

A on
issue

the

scheduled
today

.

(b)

(y
(0)
1 )

inp

is .
(y

(0)
1 )

out

hearing scheduled

A on today

issue

the

(y
(0)
1 )

inp

A hearing is scheduled on the issue today .
(y

(0)
1 )

out

H2

H1

λ

Figure 1: (a) A sentence with non-projective dependencies is represented in (b) by a bigraph (H1, λ,H2).
Both hypergraphs H1 and H2 contain a distinct hyperedge (box) for each word of the sentence. H1
specifies the linear order on the words. H2 describes parent-child relationships between the words, where
children form a list to whose start and end the parent has a tentacle. The alignment λ establishes a
one-to-one correspondence between the (input vertices of the) hyperedges in H1 and H2.

dency structure. We present each LCFRS/sDCP
hybrid grammar as a particular aligned HR bimor-
phism; this establishes an initial algebra semantics
(Goguen et al., 1977) for hybrid grammars.

The flexibility of aligned HR bimorphisms goes
well beyond hybrid grammars as they generalize
the synchronous HR grammars of (Jones et al.,
2012), making it possible to synchronously gen-
erate two graphs connected by explicit alignment
structures. Thus, they can for instance model align-
ments involving directed acyclic graphs like Ab-
stract Meaning Representations (Banarescu et al.,
2013) or Millstream systems (Bensch et al., 2014).

Our training algorithm takes as input an aligned
HR bimorphism B = (g,A1,Λ,A2) and a corpus
c of bigraphs. It is based on the dynamic program-
ming variant (Baker, 1979; Lari and Young, 1990;
Prescher, 2001) of the EM-algorithm (Dempster et
al., 1977) and thus approximates a local maximum
or saddle point of the likelihood function of c.

In order to calculate the significance of each
rule of g for the generation of a single bigraph
(H1, λ,H2) occurring in c, we proceed as usual,
constructing the reduct B � (H1, λ,H2) which
generates the singleton (H1, λ,H2) via the same
derivation trees as B and preserves the probabil-

ities. We show that the complexity of construct-
ing the reduct is polynomial in the size of g and
(H1, λ,H2) if B is an LCFRS/sDCP hybrid gram-
mar. However, as the algorithm itself is not limited
to this situation, we expect it to be useful in other
cases as well.

2 Preliminaries

Basic mathematical notation We denote the set
of natural numbers (including 0) by N and the set
N \ {0} by N . For n ∈ N, we denote {1, . . . , n}
by [n]. An alphabet A is a finite set of symbols.
We denote the set of all strings over A by A∗, the
empty string by ε, and A∗ \ {ε} by A+. We denote
the length of s ∈ A∗ by |s| and, for each i ∈ [|s|],
the ith item in s by s(i), i.e., s is identified with the
function s : [|s|]→ A such that s = s(1) · · · s(|s|).
We denote the range {s(1), . . . , s(|s|)} of s by [s].
The powerset of a set A is denoted by P(A). The
canonical extension of a function f : A → B to
f : P(A) → P(B) and to f : A∗ → B∗ are de-
fined as usual and denoted by f as well. We denote
the restriction of f : A→ B to A′ ⊆ A by f |A′ .

For an equivalence relation ∼ on B we denote
the equivalence class of b ∈ B by [b]∼ and the
quotient of B modulo ∼ by B/∼. For f : A →

61



B we define the function f/∼ : A → B/∼ by
f/∼(a) = [f(a)]∼. In particular, for a string s ∈
B∗ we let s/∼ = [s(1)]∼ · · · [s(|s|)]∼.

Terms, regular tree grammars, and algebras
A ranked alphabet is a pair (Σ, rk) where Σ is
an alphabet and rk : Σ→ N is a mapping associat-
ing a rank with each symbol of Σ. Often we just
write Σ instead of (Σ, rk). We abbreviate rk−1(k)
by Σk. In the following let Σ be a ranked alphabet.

Let A be an arbitrary set. We let Σ(A) denote
the set of strings {σ(a1, . . . , ak) | k ∈ N, σ ∈
Σk, a1, . . . , ak ∈ A} (where the parentheses and
commas are special symbols not in Σ). The set of
well-formed terms over Σ indexed by A, denoted
by TΣ(A), is defined to be the smallest set T such
that A ⊆ T and Σ(T ) ⊆ T . We abbreviate TΣ(∅)
by TΣ and write σ instead of σ() for σ ∈ Σ0.

A regular tree grammar (RTG)1 (Gécseg and
Steinby, 1984) is a tuple g = (Ξ,Σ, ξ0, R) where
Ξ is an alphabet (nonterminals), Ξ ∩ Σ = ∅, el-
ements in Σ are called terminals, ξ0 ∈ Ξ (initial
nonterminal), R is a ranked alphabet (rules); each
rule in Rk has the form ξ → σ(ξ1, . . . , ξk) where
ξ, ξ1, . . . , ξk ∈ Ξ, σ ∈ Σk. We denote the set of all
rules with left-hand side ξ by Rξ for each ξ ∈ Ξ.

Since RTGs are particular context-free gram-
mars, the concepts of derivation relation and gen-
erated language are inherited. The language of the
RTG g is the set of all well-formed terms in TΣ
generated by g; this language is denoted by L(g).

We define the Ξ-indexed family (Dξg | ξ ∈
Ξ) of mappings Dξg : TΣ → P(TR); for each
term t ∈ TΣ, Dξg(t) is the set of t’s deriva-
tion trees in TR which start with ξ and yield t.
Formally, for each ξ ∈ Ξ and σ(t1, . . . , tk) ∈
TΣ, the set D

ξ
g(σ(t1, . . . , tk)) contains each term

%(d1, . . . , dk) where % = (ξ → σ(ξ1, . . . , ξk))
is in R and di ∈ Dξig (ti) for each i ∈ [k].
We define Dg(t) =

⋃
ξ∈Ξ D

ξ
g(t) and D

ξ
g(TΣ) =⋃

t∈TΣ D
ξ
g(t). Finally, D

ξ0
g (TΣ, ξ) is the set of all

ζ ∈ TR({ξ}) such that there is a ζ ′ ∈ Dξ0g (TΣ)
which has a subtree whose root is in Rξ, and ζ is
obtained from ζ ′ by replacing exactly one of these
subtrees by ξ.

Example 2.1. Let Σ = Σ0 ∪ Σ2 where Σ0 =
{σ2, σ4, σ5} and Σ2 = {σ1, σ3}. Let g be an RTG

1in this context we use “tree” and “term” as synonyms

with initial nonterminal S and the following rules:

S → σ1(A,B) A→ σ2
B → σ3(C,D) C→ σ4 D → σ5

We observe that S ⇒∗g σ1(σ2, σ3(σ4, σ5)). Let η,
ζ, and ζ ′ be the following trees (in order):

B → σ3(C,D)
C → σ4 D → σ5

S → σ1(A,B)
A→ σ2 B

S → σ1(A,B)
A→ σ2 η

Then ζ ∈ DSg (TΣ, B) because ζ ′ ∈ DSg (TΣ) and
the left-hand side of the root of η is B. �

A Σ-algebra is a pair A = (A, (σA | σ ∈ Σ))
where A is a set and σA is a k-ary operation on A
for every k ∈ N and σ ∈ Σk. As usual, we will
sometimes use A to refer to its carrier set A or,
conversely, denote A by A (and thus σA by σA)
if there is no risk of confusion. The Σ-term alge-
bra is the Σ-algebra TΣ with σTΣ(t1, . . . , tk) =
σ(t1, . . . , tk) for every k ∈ N, σ ∈ Σk, and
t1, . . . , tk ∈ TΣ. For each Σ-algebra A there is
exactly one Σ-homomorphism, denoted by [[.]]A,
from the Σ-term algebra to A (Wechler, 1992).
Hypergraphs and hyperedge replacement In
the following let Γ be a finite set of labels. A Γ-
hypergraph is a tuple H = (V,E, att, lab, ports),
where V is a finite set of vertices,E is a finite set of
hyperedges, att : E → V ∗ \ {ε} is the attachment
of hyperedges to vertices, lab: E → Γ is the label-
ing of hyperedges, and ports ∈ V ∗ is a sequence
of (not necessarily distinct) ports. The set of all
Γ-hypergraphs is denoted by HΓ. The vertices in
V \ [ports] are also called internal vertices and
denoted by int(H).

For the sake of brevity, we shall in the following
simply call Γ-hypergraphs and hyperedges graphs
and edges, respectively. We illustrate a graph in fig-
ures as follows (cf., e.g., graph((σ2)A) in Fig. 2a).
A vertex v is illustrated by a circle, which is filled
and labeled by i in case that ports(i) = v. An edge
e with label γ and att(e) = v1 . . . vn is depicted as
a γ-labeled rectangle with n tentacles, lines point-
ing to v1, . . . , vn which are annotated by 1, . . . , n.
(We sometimes drop these annotations.)

If we are not interested in the particular set of
labels Γ, then we also call a Γ-graph simply graph
and write H instead of HΓ. In the following, we
will refer to the components of a graph H by index-
ing them with H unless they are explicitly named.

Let H and H ′ be graphs. H and H ′ are disjoint
if VH ∩ VH′ = ∅ and EH ∩ EH′ = ∅.

62



(a)

1
is .

2

e1 e2

3 4 3 4

3 4 1 2

1 2

2 4
1 3

1 2

graph((σ1)A)

3
hearing

4

A
1 2

3 4

3 4

1 2

1 2

graph((σ2)A)

1
e1

2

3
e2

4

1 2

1 2

graph((σ3)A)

1
scheduled

2

today

3 4

3 4

1 2

1 2

graph((σ4)A)

1
on

2

issue

the

3 4

3 4

3 4

1 2

1 2

1 2

graph((σ5)A)

(b)

[[σ1(σ2, σ3(σ4, σ5))]]A =

1
is .

2

[[σ2]]A [[σ3(σ4, σ5)]]A

3 4 3 4

3 4 1 2

1 2

2 4
1 3

1 2 =

1
is .

2

hearing [[σ3(σ4, σ5)]]A

A

3 4 3 4

3 4 1 2

3 4

1 2

1 2
3

4

1 2

1 2

=

1
is .

2

hearing [[σ4]]A

A [[σ5]]A

3 4 3 4

3 4 1 2

3 4 1 2

1 2

1
2

1 2

1 2

=

1
is .

2

hearing scheduled

A on today

1 2

1 2 1 2· · ·

3 4 3 4

3 4 3 4

3 4 3 4 3 4

1 2

1 2
1

2

1 2

Figure 2: (a) The (Σ, Γ)-HR algebra A and (b) the evaluation of the term σ1(σ2, σ3(σ4, σ5)) in A.

Let E = EH ∩ EH′ . If attH |E = attH′ |E and
labH |E = labH′ |E , then the union of the graphs
H andH ′ is the graphH∪H ′ = (VH ∪VH′ , EH ∪
EH′ , attH∪attH′ , labH∪labH′ , portsHportsH′).

For every F ⊆ EH let H \ F = (VH , E,
attH |E , labH |E ,portsH) where E = EH \ F .

Let k ∈ N and H,H1, . . . ,Hk ∈ H be pair-
wise disjoint. Let e1, . . . , ek ∈ EH be pairwise
distinct edges, called variables. Let I be the graph
H \ {e1, . . . , ek} ∪H1 ∪ · · · ∪Hk. The hyperedge
replacement (HR) of e1 by H1, . . . , and ek by Hk
in H (Bauderon and Courcelle, 1987; Habel and
Kreowski, 1987) yields the graph

H[e1/H1, . . . , ek/Hk]
= (VI/∼, EI , attI/∼, labI ,portsH/∼)

where∼ is the least equivalence relation on VI such
that attH(ei, j) ∼ portsHi(j) for every i ∈ [k]
and j ∈ [min(|attH(ei)|, |portsHi |)]. In the fol-
lowing, we call∼ the equivalence relation involved
in the HR that yields H[e1/H1, . . . , ek/Hk].

We assume that each variable ei is labeled by a
distinguished symbol ⊥ ∈ Σ and depict ei by ei
instead of ⊥ . Throughout this paper, we will not
distinguish between isomorphic graphs, i.e., graphs
that are identical up to a bijective renaming of ver-
tices and edges. However, since hyperedge replace-
ment is defined on concrete graphs and requires
that H,H1, . . . ,Hk are pairwise disjoint, we may
choose isomorphic copies of the involved graphs,
i.e., rename edges or vertices. To avoid the cum-
bersome conversion between abstract and concrete

graphs, we assume that this renaming is opaque.
In this sense, we may define an HR operation as a
total function fromHk toH as follows.

Let H be a graph. For pairwise distinct
edges e1, . . . , ek ∈ EH , the HR operation
H〈e1...ek〉 : Hk → H is given by

H〈e1...ek〉(H1, . . . ,Hk) = H[e1/H1, . . . , ek/Hk]

for all graphs H1, . . . ,Hk ∈ H.
A (Σ, Γ)-HR algebra (Courcelle, 1991) is a Σ-

algebra A = (HΓ, (σA)σ∈Σ) where, for every k ∈
N and σ ∈ Σk, we have σA = H〈e1...ek〉 for some
H ∈ HΓ and pairwise distinct e1, . . . , ek ∈ EH .
Then we denote H by graph(σA). An HR algebra
is a (Σ, Γ)-HR algebra for some Σ and Γ.

An example of a (Σ, Γ)-HR algebra A and the
application of [[.]]A to a term are given in Fig. 2.

3 Bigraphs and Aligned HR
Bimorphisms

Now we formally introduce our central notions
of bigraph and aligned HR bimorphism. A case
study with examples follows in the next section.
Throughout this section let ∆ and Ω be alphabets.

Definition 3.1. A bigraph of type (∆,Ω) is a triple
B = (H1, λ,H2) where H1 ∈ H∆ and H2 ∈ HΩ
are disjoint, and λ is an alignment of H1 and H2,
i.e., a graph with Vλ = VH1 ∪ VH2 , Eλ ∩ EH1 =
Eλ∩EH2 = ∅, att(e) ∈ (VH1)+ ·(VH2)+ for each
e ∈ Eλ, and portsλ = ε. �

63



(a) x(0)1 . . . x
(0)
m0

σ y
(0)
1

. . . y(0)n0

y
(1)
1

. . . y(1)m1 x
(1)
1

. . . x(1)n1

e1

y
(k)
1

. . . y(k)mk x
(k)
1

. . . x(k)nk

ek

· · ·

e
(0)
1 e

(0)
n0

e
(1)
1 e

(1)
m1 e

(n)
1 e

(n)
mk

(b)
σ1

(y(0)1 )
inp

is .
(y(0)1 )

out

y
(1)
1 x

(1)
1

e1

x
(2)
1 x

(2)
2

e2

(c)
inp

is .
out

x
(1)
1 x

(2)
1 x

(2)
2

(d)

inp
x

(2)
2

out

x
(1)
1

x
(2)
1

Figure 3: The graphs (a) HσIO, (b) H
σ1 , (c) Ls(0)1 M, and (d) Ls(1)1 M. The large circles and the dotted lines in

(a) and (b) visualize the underlying term structure; e.g., in (b) σ1 has two children because σ1 ∈ Σ2.

Definition 3.2. An aligned HR bimorphism of type
(Σ,∆,Ω) is a tuple B = (g,A1,Λ,A2), where
g is an RTG over Σ and A1,A2 are a (Σ,∆)-HR
algebra and a (Σ,Ω)-HR algebra, resp., such that
graph(σA1) and graph(σA2) are disjoint for each
σ ∈ Σ. Further, Λ is a Σ-indexed family (Λσ | σ ∈
Σ), each Λσ being an alignment of graph(σA1)
and graph(σA2). �

In the following, for each term t ∈ TΣ, we as-
sume (w.l.o.g.) that [[t]]A1 and [[t]]A2 are disjoint.
Definition 3.3. Let B = (g,A1,Λ,A2) be an
aligned HR bimorphism. The semantics of B, de-
noted by L(B), is the set of bigraphs defined as
follows.

First, let the B-alignment be the TΣ-indexed
family ΛB = (ΛB(t) | t ∈ TΣ) where ΛB(t)
is the alignment of [[t]]A1 and [[t]]A2 defined induc-
tively on t as follows. Let t = σ(t1, . . . , tk) ∈ TΣ.
For j ∈ [2], suppose that ∼j is the equivalence
relation involved in the hyperedge replacement
that yields graph(σAj )[e1/[[t1]]Aj , . . . , ek/[[tk]]Aj ].
We assume that Λσ, ΛB(t1), . . . , ΛB(tk) have pair-
wise disjoint sets of edges. Then we define the
B-alignment of [[t]]A1 and [[t]]A2 to be the graph
ΛB(t) = (Λσ∪ΛB(t1)∪· · ·∪ΛB(tk))/(∼1∪∼2)
and we let [[t]]B = ([[t]]A1 ,ΛB(t), [[t]]A2). Finally,
we define L(B) = {[[t]]B | t ∈ L(g)}. �
4 Case Study: Hybrid Grammars

We show how an LCFRS/sDCP hybrid grammar
(Nederhof and Vogler, 2014) can be represented as
an aligned HR bimorphism. These grammars deal
with sequence terms; hence, we first recall their
definition and show how to view sequence terms as
particular graphs.

4.1 A Graph View on Sequence Terms
Let Γ be a ranked alphabet and Y be a set disjoint
from Γ. The sets of terms and sequence-terms (s-
terms) over Γ indexed by Y (Seki and Kato, 2008)
are denoted by TΓ(Y ) and T ∗Γ (Y ), respectively,
and defined inductively as follows:

1. Y ⊆ TΓ(Y ),
2. if k ∈ N, γ ∈ Γk and si ∈ T ∗Γ (Y ) for each
i ∈ [k], then γ(s1, . . . , sk) ∈ TΓ(Y ), and

3. if n ∈ N and ti ∈ TΓ(Y ) for each i ∈ [n],
then 〈t1, . . . , tn〉 ∈ T ∗Γ (Y ).

Let s ∈ T ∗Γ (Y ). We say that s is linear if every
y ∈ Y occurs at most once in s. In the following
we only consider linear s-terms. We note that, if
Γ = Γ0, then s is essentially a string over Γ0 and
Y . If Γ = Γ1, then s corresponds to a sequence of
ordinary (unranked) terms over Γ1 indexed by Y .

Every linear s-term s can be represented as a
graph LsM: it has two distinct ports inp and out ,
representing the start and end of s, resp. For each
variable y ∈ Y , LsM has two distinct ports yinp and
yout . For each occurrence of a symbol γ ∈ Γk in
s, there is a γ-labeled edge with 2k + 2 tentacles
in LsM. The (2i− 1)-th and 2i-th tentacle (i ∈ [k])
point to the start and end vertex, respectively, of
the i-th child sequence of γ. The last two tentacles
point towards the predecessor and the successor of
γ, respectively: this may be a vertex separating two
symbols, the start or end vertex of a (sub-)sequence,
or the port realizing yinp or yout for some y ∈ Y .

For instance, the s-term

s
(0)
1 = 〈is(〈x(1)1 , x(2)1 〉), . (〈〉)〉

in T ∗Γ ({x(1)1 , x(2)1 , x(2)2 }) is represented by Ls(0)1 M in
Fig. 3c. The ports (x(2)2 )

inp and (x(2)2 )
out for x(2)2

64



are depicted as filled circles to the left and the right
of x(2)2 , and similarly for the other variables. Note
that (x(1)1 )

out and (x(2)1 )
inp coincide because x(1)1

is succeeded by x(2)1 in s
(0)
1 .

4.2 LCFRS, sDCP, and LCFRS/sDCP
Hybrid Grammars

Here we formalize LCFRS/sDCP hybrid grammars
as particular aligned HR bimorphisms, where the
algebras A1 and A2 are an LCFRS algebra and an
sDCP algebra, resp. Since both LCFRS and sDCP
can be viewed as particular types of attribute gram-
mars (AG), we first define the concept of AG alge-
bra and, in a second step, instantiate it to LCFRS
algebra and sDCP algebra.

For each σ ∈ Σk, let synσA = (n0, . . . , nk) ∈
N k+1 and inhσA = (m0, . . . ,mk) ∈ Nk+1 be tu-
ples defining the sets I = {y(0)j | j ∈ [n0]}∪{y(i)j |
i ∈ [k], j ∈ [mi]} and O = {x(0)r | r ∈
[m0]} ∪ {x(`)r | ` ∈ [k], r ∈ [n`]} of inside and
outside attributes, resp. (The abbreviations stem
from the AG notions synthesized attributes and in-
herited attributes.) The definition of an AG algebra
A follows the two-phase approach in (Engelfriet
and Heyker, 1992). In the first phase, for each
symbol σ in Σk, we define a graph HσIO of type
synσA, inh

σ
A as shown in Fig. 3a: there is a pair

of vertices for each inside attribute y(i)j and each

outside attribute x(`)r . For each inside attribute y
(i)
j

there is a edge e(i)j which connects y
(i)
j with all out-

side attributes. For the edge e(0)1 the tentacles are
shown completely in Fig. 3a; for the other edges
the tentacles to outside attributes are abridged for
clarity. The edges e1, . . . , ek correspond to the k
successors of σ.

In the second phase, we choose an I-indexed
family of s-terms (s(i)j ∈ T ∗Γ (O) | y(i)j ∈ I) such
that each x(`)r in O occurs exactly once in all s

(i)
j

together (single syntactic use restriction). Then
we replace each edge e(i)j by the graph Ls(i)j M; this
specifies a particular information flow. Formally,
we set σA = Hσ〈e1...ek〉 where

Hσ = HσIO[e
(i)
j /Ls(i)j M | y(i)j ∈ I] .

For instance, for σ1 ∈ Σ2 we have synσ1A =
(1, 1, 2) and inhσ1A = (0, 1, 0), and so we con-
struct Hσ1IO accordingly. Next, we choose Ls(0)1 M
and Ls(1)1 M as depicted in Fig. 3c and 3d, respec-

tively. Then Hσ1 = Hσ1IO[e
(0)
1 /Ls(0)1 M, e(1)1 /Ls(1)1 M]

is the graph in Fig. 3b where dashed lines indicate
the identification of vertices. Note that Hσ1 equals
graph((σ1)A) in Fig. 2a.

A (Σ, Γ)-HR algebra A is a (Σ, Γ)-attribute
grammar algebra ((Σ, Γ)-AG algebra), if each
symbol σ in Σ is interpreted as described above.
For instance, A of Fig. 2a is a (Σ, Γ)-AG algebra.

We observe that (Σ, Γ)-AG algebras have the
following property: for every edge e ∈ EHσ , if
labHσ(e) ∈ Γk then e has 2k + 2 tentacles. We
call the vertex attHσ(e)(k + 1) the input vertex
of e and denote it by inp(e). Note that no two
terminal edges in Hσ have the same input vertex.
This single-input property will be crucial for an effi-
cient representation of subgraphs during the reduct
construction (cf. Sec. 5.2).

Next we instantiate the concept of AG-algebra to
LCFRS algebras and to sDCP algebras. An LCFRS
does not have inherited attributes:

Definition 4.1. Let ∆ = ∆0 be a ranked alphabet.
A (Σ,∆)-AG algebraA is a (Σ,∆)-LCFRS algebra,
if inhσA = (0, . . . , 0) for all σ ∈ Σ. �
Definition 4.2. Let Ω = Ω1 be a ranked alphabet
and let A be a (Σ,Ω)-AG algebra. We say that A
is a (Σ,Ω)-sDCP algebra. �

Then the graph view on an LCFRS/sDCP
hybrid grammar is an HR bimorphism B =
(g,A1,Λ,A2), where
• g = (Ξ,Σ, ξ0, R) is an RTG,
• A1 is a (Σ,∆)-LCFRS algebra,
• A2 is a (Σ,Ω)-sDCP algebra, ∆ = Ω (regard-

ing ∆ and Ω as sets of symbols), and
• there are functions fan: Ξ → N , inh: Ξ →

N, and syn: Ξ → N such that fan(ξ0) = 1,
inh(ξ0) = 0, syn(ξ0) = 1, and for every (ξ →
σ(ξ1, . . . , ξk)) ∈ R, it holds that
– (fan(ξ), fan(ξ1), . . . , fan(ξk)) = synσA1 and
– (inh(ξ), inh(ξ1), . . . , inh(ξk)) = inhσA2 and

(syn(ξ), syn(ξ1), . . . , syn(ξk)) = synσA2 .

Moreover, we require the following: Let σ ∈ Σ and
Hj = graph(σAj ) for j ∈ [2]. For each e ∈ EΛσ
we have attΛσ(e) = inp(e1) inp(e2) where e1 ∈
EH1 , e2 ∈ EH2 , and labH1(e1) = labH2(e2).
Example 4.3. Let g be as in Ex. 2.1 and con-
sider the LCFRS/sDCP hybrid grammar B =
(g,A1,Λ,A2), where A1, Λ, and A2 are as speci-
fied in Fig. 4. Then the bigraph in Fig. 1b equals
[[σ1(σ2, σ3(σ4, σ5))]]B and is thus in L(B). �

65



(y
(0)
1 )

inp

is .
(y

(0)
1 )

out

e1 e2(x(1)1 )
inp (x

(2)
1 )

out

(x
(1)
1 )

out , (x
(2)
1 )

inp

(y
(1)
1 )

out ,(x(2)2 )
out

(y
(1)
1 )

inp ,(x(2)2 )
inp

(y
(0)
1 )

inp

e1 is e2 .
(y

(0)
1 )

out

(σ1)A2

(σ1)A1

Λσ1

(y
(0)
1 )

inp

hearing
(y

(0)
1 )

out

A
(x(0)1 )

inp (x(0)1 )
out

(y
(0)
1 )

inp
A hearing

(y
(0)
1 )

out

(σ2)A2

(σ2)A1

Λσ2

(y
(0)
1 )

inp

on
(y

(0)
1 )

out

issue

the

(y
(0)
1 )

inp

on the issue
(y

(0)
1 )

out

(σ5)A2

(σ5)A1

Λσ5

(y
(0)
1 )

inp

e1
(y

(0)
1 )

out (y
(0)
2 )

inp

e2
(y

(0)
2 )

out

(y
(0)
1 )

inp , (x
(1)
1 )

inp (x
(1)
1 )

out , (x
(2)
1 )

inp (x
(2)
1 )

out , (x
(1)
2 )

inp (x
(1)
2 )

out , (y
(0)
1 )

out

e1 e2

(σ3)A2

(σ3)A1

Λσ3

(y
(0)
1 )

inp

scheduled
(y

(0)
1 )

out

today

(y
(0)
1 )

inp

scheduled
(y

(0)
1 )

out (y
(0)
2 )

inp

today
(y

(0)
2 )

out

(σ4)A2

(σ4)A1

Λσ4

Figure 4: The interpretation of σ1, . . . , σ5 in A1, Λ, and A2.

5 EM Training

We present a training algorithm which takes as in-
put a weighted aligned HR bimorphism and a finite,
non-empty corpus c of bigraphs. It is essentially
the same as the training algorithm for probabilistic
context-free grammars (PCFG) (Baker, 1979; Lari
and Young, 1990; Nederhof and Satta, 2008). As
shown in (Prescher, 2001), this algorithm is a dy-
namic programming variant of the EM-algorithm
(Dempster et al., 1977). Thus, our algorithm gener-
ates a sequence of probability assignments which
converges to a probability assignment p̂; the like-
lihood of c under p̂ is a local maximum or saddle
point of the likelihood function of c.

5.1 Weighted Aligned HR Bimorphisms
We define weighted RTG in a similar way as PCFG
was defined in (Nederhof and Satta, 2006).

A weighted regular tree grammar (WRTG) is
a pair (g, p) where g = (Ξ,Σ, ξ0, R) is an RTG
and p : R → R≥0 is the weight assignment. A
weight assignment p is a probability assignment if∑

ρ∈Rξ p(ρ) = 1 for each ξ ∈ Ξ. We extend p to
the mapping p′ : Dg(TΣ)→ R≥0 on derivations as
follows: for each d = %(d1, . . . , dk) inDg(TΣ) we
define p′(d) = p(%) ·∏ki=1 p′(di). For each t ∈ TΣ
we define p′′(t) =

∑
d∈Dξg(t) p

′(d). We define the
mappings in : Ξ → R≥0 ∪ {∞} (inside weight)
and out : Ξ → R≥0 ∪ {∞} (outside weight) for
each ξ ∈ Ξ by

in(ξ) =
∑

d∈Dξg(TΣ)
p′(d) out(ξ) =

∑
d∈Dξ0g (TΣ,ξ)

p′′′(d)

where p′′′ : Dξ0g (TΣ, ξ) → R≥0 is defined in the
same way as p′, with the addition that p′′′(ξ) = 1.
As usual, we will drop the primes from p′, p′′, and
p′′′.
Definition 5.1. A weighted aligned HR bimor-
phism is a pair (B, p) = ((g,A1,Λ,A2), p) where
(g,A1,Λ,A2) is an aligned HR bimorphism and
(g, p) is a WRTG. �

5.2 Reduct Construction
Given a weighted aligned HR bimorphism (B, p) =
((g,A1,Λ,A2), p) and a bigraph (H1, λ,H2), we
restrict g to an RTG g′ such that only trees t ∈ L(g)
satisfying [[t]]B = (H1, λ,H2) are in L(g′). Also,
we show that if B is an LCFRS/sDCP hybrid gram-
mar, then g′ can be constructed in time polynomial
in the size of B and (H1, λ,H2).
Definition 5.2. Let m ∈ N and H ∈ H. A
graph H ′ is an m-subgraph of H if int(H ′) ⊆
int(H), EH′ ⊆ EH , labH′ = labH |EH′ , and
|portsH′ | ≤ m. Moreover, we require that there
is a mapping ϕ : VH′ → VH , called vertex map-
ping, such that ϕ(attH′(e)) = attH(e) for each
e ∈ EH , ϕ(v) = v for each v ∈ int(H ′), and
ϕ(int(H ′)) ∩ ϕ([portsH′ ]) = ∅. Moreover, for
each v ∈ int(H ′), if v ∈ [attH(e)] for some
e ∈ EH , then e ∈ EH′ . The set of allm-subgraphs
of H is denoted byHmS (H). �

For instance, graph((σ4)A) in Fig. 2a is a 2-
subgraph of the last graph in Fig. 2b. If a graph H
is the result of applying an HR operation to graphs
H1, . . . ,Hk, then each Hi is a |portsHi |-subgraph
of H . (For this, the mapping ϕ in Definition 5.2 is

66



needed, because some of the ports of Hi may be
identified with each other in H .) Hence, for the
reduct we consider only m-subgraphs of H , where
m is the maximal port length of HR operations
in A1 or A2. We observe that HmS (H) is finite
because we identify isomorphic graphs.

Definition 5.3. Let (B, p) = ((g,A1,Λ,A2), p)
be a weighted aligned HR bimorphism with g =
(Ξ,Σ, ξ0, R) and let (H1, λ,H2) be a bigraph.

We define (B, p) � (H1, λ,H2), the reduct
of (B, p) with respect to (H1, λ,H2), to
be the weighted aligned HR bimorphism
((g′,A1,Λ,A2), p′) where g′ and p′ are defined as
follows.

If [[.]]B−1(H1, λ,H2) = ∅, then g′ = ({ξ′0},Σ,
ξ′0, ∅) and p′ = ∅. Otherwise, let m ∈ N
be the maximum of all |portsgraph(σA1 )| and|portsgraph(σA2 )| where σ ∈ Σ. Now, we con-
struct g′ = (Ξ′,Σ, ξ′0, R′) where we abbreviate
HmS (H1)×HmS (λ)×HmS (H2) byHmS (H1, λ,H2):
• Ξ′ = Ξ × (HmS (H1, λ,H2) ∩ [[TΣ]]B),
• ξ′0 = (ξ0, H1, λ,H2), and
• for every rule % = (ξ → σ(ξ1, . . . , ξk)) ∈ R

and (s, η, r), (s1, η1, r1), . . . , (sk, ηk, rk) in
HmS (H1, λ,H2) ∩ [[TΣ]]B we have

%′ =
(
(ξ, s, η, r)→σ((ξ1, s1, η1, r1), . . . ,

(ξk, sk, ηk, rk))
) ∈ R′

if s = σA1(s1, . . . , sk), η = Λσ ∪η1∪ . . .∪ηk,
and r = σA2(r1, . . . , rk).

We define p′(%′) = p(%). �
Theorem 5.4. In Def. 5.3 the following hold:

1. L(g′) = [[.]]B−1(H1, λ,H2) ∩ L(g).
2. There is a deterministic tree relabeling φ from
Dg′ to Dg such that for all t ∈ L(g′) and
d′ ∈ Dg′(t), φ|Dg′ (t) is a bijection between
Dg′(t) and Dg(t), and p′(d′) = p(φ(d′)).

Proof. If [[.]]B−1(H1, λ,H2) = ∅, then L(g′) =
∅ by construction, and thus, both statements of
the theorem hold. Otherwise, the first statement
follows from the following claim.

Claim (*) For every n ∈ N, ξ ∈ Ξ, t ∈ TΣ, and
(s, η, r) ∈ (HmS (H1, λ,H2)) ∩ [[TΣ]]B it holds that
(ξ, s, η, r) ⇒ng′ t iff ξ ⇒ng t and [[t]]A1 = s and
ΛB(t) = η and [[t]]A2 = r.

For the proof of the second statement we define
φ((ξ, s, η, r)) = ξ for each (ξ, s, η, r) ∈ Ξ′, and
extend φ in the canonical way to derivation trees.

Then the statement is an immediate consequence of
the constructions of R′ and φ and Claim (*). �

Complexity We determine the complexity of
the reduct construction for the special case of
LCFRS/sDCP hybrid grammars. We assume that
the maximal lengthm of ports in the HR operations
is fixed and not part of the input. In preparation,
we determine an upper bound on |Ξ′|.

Let H ∈ H be such that from each vertex there
is an (undirected) path to a port. Given H each
H ′ ∈ HmS (H) ∩ [[TΣ]]A is uniquely determined
by its boundary representation (Lautemann, 1990;
Chiang et al., 2013; Groschwitz et al., 2015), which
consists of (a) the pair (portsH′ , ϕ(portsH′)),
(b) the set of boundary edges EBH′ of H

′ consisting
of all e ∈ EH′ such that attH′(e)∩ [portsH′ ] 6= ∅,
and (c) a function att′ : EBH′ → ([portsH′ ]∪{⊥})∗
such that att′(e)(i) = attH′(e)(i) if attH′(e)(i)
∈ [portsH′ ], and ⊥ otherwise. Note that (c) is nec-
essary because ϕ|[portsH′ ] might not be injective.

Now, for an arbitrary (Σ, Γ)-AG algebra A and
H ∈ LT ∗Γ (∅)M the following holds. Due to the
single-input property of H and of the involved HR
operations, for each m-subgraph H ′ the informa-
tion (b) and (c) can be inferred from (a). There are
Sm,k port sequences of length m with k distinct
vertices, where Sm,k is the Stirling number of the
second kind. For each port we choose a vertex in
VH . Thus, we obtain that |HmS (H) ∩ [[TΣ]]A| ≤∑m

k=0 Sm,k · |VH |k ≤ mm · |VH |m.
Next, we analyze the B-alignments of an

LCFRS/sDCP hybrid grammar B. Let (s, η, r) ∈
HmS (H1, λ,H2) and t ∈ TΣ with [[t]]B = (s, η, r).
Then Vη = Vs ∪ Vr. Let e ∈ Eλ and
e1 ∈ EH1 and e2 ∈ EH2 with attλ(e) =
inp(e1) inp(e2). (i) Assume that there is t′ ∈ TΣ
with [[t′]]B = (H1, λ,H2) and t is a subtree of t′.
Then e1 and e2 are unique by the single input prop-
erty. Hence, e1 ∈ Es iff e2 ∈ Er iff e ∈ Eη
because t is a subtree of t′ and by the structure of
Λσ. (ii) If there is no such t′, then the equivalences
under (i) may be violated, in which case (s, η, r)
can safely be pruned.

Thus, for each LCFRS/sDCP hybrid grammar
B and each (H1, λ,H2) with H1 ∈ LT ∗∆ (∅)M and
H2 ∈ LT ∗Ω(∅)M we obtain the upper bound |Ξ| ·
m2m · |VH1 |m · |VH2 |m on |Ξ′|. This bound can be
refined to |Ξ|·m2m ·|VH1 |2·fan

∗ ·|VH2 |2·(syn
∗+inh∗),

where f∗ = maxξ∈Ξ f(ξ) for f ∈ {fan, syn, inh}.
Constructing Ξ′ and R′ simultaneously with a de-
ductive parsing algorithm (Shieber et al., 1995) has

67



a worst-case time complexity in O(|Ξ′|k) where k
is the maximum rank of Σ.

5.3 EM Training Algorithm
In the first step of our training algorithm, a cor-
pus c′ : R → R≥0 is computed as follows. After
initialization (line 3), each bigraph B occurring
in c is considered (line 4), the reduct (B, pi) � B
is built (line 5), the inside/outside weights of the
new WRTG (g′, p′) are calculated (line 6), and ac-
cording to these weights and the current weight
assignment pi the count c′(%) of each rule is incre-
mented (lines 8–9). In the second step, the corpus
c′ is normalized (lines 10–14) and the result is the
next probability assignment pi+1 (line 15).

Algorithm 5.1 EM-training algorithms for
weighted aligned HR bimorphisms.
Input: weighted aligned HR bimorphism

(B, p0) = ((g,A1,Λ,A2), p0)
with g = (Ξ,Σ, ξ0, R), and a finite,
non-empty corpus c of bigraphs.

Output: sequence p1, p2, p3, . . . of improved
probability assignments for R.

1: i← 0
2: while true do
3: initialize counts c′ : R→ R≥0: c′(%)← 0
4: for B = (H1, λ,H2) s.t. c(B) > 0 do
5: ((g′,A1,Λ,A2), p′)← (B, pi) �B

with RTG g′ = (Ξ′,Σ, ξ′0, R′) and
det. tree rel. φ // cf. Thm 5.4

6: compute out and in for the WRTG
(g′, p′)

7: if in(ξ′0) 6= 0 then
8: for % = (ξ → σ(ξ1, . . . , ξk)) ∈ R do
9: c′(%)← c′(%) + c(B) · in(ξ′0)−1 ·∑

%′∈R′:
φ(%′)=% ∧ %′=(ξ′→σ(ξ′1,...,ξ′k))

out(ξ′) · pi(%) ·
k∏
j=1

in(ξ′j)

10: for ξ ∈ Ξ do
11: s←∑%∈Rξ c′(%)
12: for % ∈ Rξ do
13: if s = 0 then pi+1(%)← pi(%)·|Rξ|−1
14: else pi+1(%)← s−1 · c′(%)
15: output pi+1 and i← i+ 1

Acknowledgment

We thank the referees for their careful reading of
the manuscript.

References
James K. Baker. 1979. Trainable grammars for speech

recognition. In Speech Communication Papers for
the 97th Meeting of the Acoustical Society of Amer-
ica, pages 547–550.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proc. 7th Linguistic Annotation
Workshop, ACL 2013 Workshop.

Michel Bauderon and Bruno Courcelle. 1987. Graph
expressions and graph rewriting. Mathematical Sys-
tems Theory, 20:83–127.

Suna Bensch, Frank Drewes, Helmut Jürgensen, and
Brink van der Merwe. 2014. Graph transformation
for incremental natural language analysis. Theoreti-
cal Computer Science, 531:1–25.

Walter S. Brainerd. 1969. Tree generating regular sys-
tems. Inform. and Control, 14:217–231.

Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311.

David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proc. of the 51st Annual
Meeting of the Association for Computational
Linguistics, pages 924–932.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.

Bruno Courcelle. 1991. The monadic second-order
logic of graphs V: on closing the gap between de-
finability and recognizability. Theoretical Computer
Science, 80:153–202.

Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39:1–38.

Pierre Deransart and Jan Małuszynski. 1985. Relat-
ing logic programs and attribute grammars. J. Logic
Programming, 2:119–155.

Joost Engelfriet and Linda Heyker. 1992. Context-
free hypergraph grammars have the same term-
generating power as attribute grammars. Acta Infor-
matica, 29(2):161–210.

Ferenc Gécseg and Magnus Steinby. 1984. Tree Au-
tomata. Akadémiai Kiadó, Budapest. (See also
arXiv:1509.06233, 2015).

68



Joseph A. Goguen, James W. Thatcher, Eric G. Wagner,
and Jesse B. Wright. 1977. Initial algebra semantics
and continuous algebras. J. ACM, 24:68–95.

Jonas Groschwitz, Alexander Koller, and Christoph
Teichmann. 2015. Graph parsing with s-graph
grammars. In Proc. of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing, pages 1481–1490.

Annegret Habel and Hans-Jörg Kreowski. 1987. May
we introduce to you: Hyperedge replacement. In
Proc. of the Third Intl. Workshop on Graph Gram-
mars and Their Application to Computer Science,
pages 15–26.

Annegret Habel. 1992. Hyperedge Replacement:
Grammars and Languages, volume 643 of Lecture
Notes in Computer Science. Springer.

Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hy-
peredge replacement grammars. In M. Kay and
C. Boitet, editors, Proc. 24th Intl. Conf. on Com-
putational Linguistics (COLING 2012): Technical
Papers, pages 1359–1376.

Karim Lari and Steve J. Young. 1990. The estimation
of stochastic context-free grammars using the Inside-
Outside algorithm. Computer Speech and Language,
4:35–56.

Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Inf., 27(5):399–421.

Philip M. Lewis and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465–488.

Mark-Jan Nederhof and Giorgio Satta. 2006. Proba-
bilistic parsing strategies. J. ACM, 53(3):406–436.

Mark-Jan Nederhof and Giorgio Satta. 2008. Prob-
abilistic parsing. In Gemma Bel-Enguix, M. Do-
lores Jiménez-López, and Carlos Martı́n-Vide, edi-
tors, New Developments in Formal Languages and
Applications, volume 113 of Studies in Computa-
tional Intelligence, pages 229–258. Springer.

Mark-Jan Nederhof and Heiko Vogler. 2012. Syn-
chronous context-free tree grammars. In Proc. of
the 11th International Workshop on Tree Adjoin-
ing Grammars and Related Formalisms (TAG+11),
pages 55–63.

Mark-Jan Nederhof and Heiko Vogler. 2014. Hy-
brid grammars for discontinuous parsing. In Proc.
of 25th International Conference on Computational
Linguistics (COLING 2014), pages 1370–1381.

Detlef Prescher. 2001. Inside-outside estimation meets
dynamic EM. In Proc. of the 7th International Work-
shop on Parsing Technologies, pages 241–244.

Hiroyuki Seki and Yuki Kato. 2008. On the gener-
ative power of multiple context-free grammars and
macro grammars. IEICE - Transactions on Informa-
tion and Systems, E91-D(2):209–221.

Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. of
the 13th International Conference on Computational
Linguistics, volume 3, pages 253–258.

Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. J. Logic Programming, 24(1–2):3 –
36.

Krishnamurti Vijay-Shanker, David J. Weir, and Ar-
avind K. Joshi. 1987. Characterizing structural
descriptions produced by various grammatical for-
malisms. In Proc. of the 25th Annual Meeting of the
Association for Computational Linguistics, pages
104–111.

Wolfgang Wechler. 1992. Universal Algebra for Com-
puter Scientists, volume 25 of EATCS Monographs
on Theoretical Computer Science. Springer.

69


