



















































How Well Do Embedding Models Capture Non-compositionality? A View from Multiword Expressions


Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 27–34
Minneapolis, USA, June 6, 2019. c©2019 Association for Computational Linguistics

27

How Well Do Embedding Models Capture Non-compositionality? A View
from Multiword Expressions

Navnita Nandakumar Timothy Baldwin Bahar Salehi
School of Computing and Information Systems

The University of Melbourne
Victoria 3010, Australia

nnandakumar@student.unimelb.edu.au {tbaldwin|salehi.b}@unimelb.edu.au

Abstract

In this paper, we apply various embedding
methods to multiword expressions to study
how well they capture the nuances of non-
compositional data. Our results from a range
of word-, character-, and document-level em-
bbedings suggest that word2vec performs the
best, followed by fastText and infersent.
Moreover, we find that recently-proposed con-
textualised embedding models such as BERT
and ELMo are not adept at handling non-
compositionality in multiword expressions.

1 Introduction

Modern embedding models, including contextual
embeddings, have been shown to work impres-
sively well across a range of tasks (Peters et al.,
2018; Devlin et al., 2018). However, study of
their performance on data with a mix of composi-
tionality levels, whose meaning is often not easily
predicted from that of its constituent words, has
been limited (Salehi et al., 2015; Hakimi Parizi
and Cook, 2018; Nandakumar et al., 2018).

At present, there exists no definitive metric
to measure the modelling capabilities of an em-
bedding technique across a spectrum of non-
compositionality, especially in the case of newer,
contextualised representations, such as ELMo and
BERT.

In this study, we apply various embedding
methods to the task of determining the com-
positionality of English multiword expressions
(“MWEs”), specifically noun–noun and adjective–
noun pairs, to test their performance on data rep-
resenting a range of compositionality (Sag et al.,
2002). Compositionality prediction can be mod-
eled as a regression task (Baldwin and Kim, 2010)
that involves mapping an MWE onto a continu-
ous scale, representing its compositionality as a
whole or with respect to each of its components.
For example, application form can be considered

to be quite compositional, while sitting duck1 is
considered to be idiomatic or non-compositional.
Close shave2 could be seen as partially composi-
tional, heavily compositional with regards to the
first word and less compositional with regards
to the second. In this study, we focus on pre-
dicting the compositionality of the MWE as a
whole. Although we conduct our experiments on
English datasets, they can be applied to other lan-
guages with ease as we do not perform any kind of
language-specific manipulation of the data.

The main contributions of this paper are:
(i) we compare embeddings over 3 different

MWE datasets, focusing on noun–noun and
adjective–noun pairs;

(ii) we experiment with 7 character-, word-, and
document-level embedding models, includ-
ing contextualised models;

(iii) we show that, despite their success on a
range of other tasks, recent embedding learn-
ing methods lag behind simple word2vec in
capturing MWE non-compositionality.

2 Related Work

Although vector space models have been popu-
lar since the 1990s, it was only after Collobert
and Weston (2008) proposed a unified neural net-
work architecture to learning distributed word rep-
resentations and demonstrated its performance on
downstream tasks, that embedding learning estab-
lished a footing in NLP, with word2vec (Mikolov
et al., 2013a) being the catalyst to the “embedding
revolution”.

Language embeddings are an example of an
unsupervised representation learning application
done well. They are preferred primarily because
they can be learned from unannotated corpora and,

1A sitting duck means “a person or thing with no protec-
tion against an attack or other source of danger.”

2A close shave is “a narrow escape from danger or disas-
ter.”



28

therefore, eliminate the need for manual annota-
tion (which is expensive and time-consuming).

Salehi et al. (2015) were the first to apply word
embeddings to the task of predicting the compo-
sitionality of MWEs. The assumption is that the
compositionality of an MWE is proportional to
the relative similarity between each of the compo-
nents and the overall MWE, represented by their
respective embeddings. This method was recently
tuned variously by Cordeiro et al. (2019) and re-
mains state-of-the-art for the task of MWE com-
positionality prediction, but has the downside that
it requires automatic token-level pre-identification
of each MWE in the training corpus in order to
train a model (i.e. all occurrences of sitting duck
need to be pre-tokenised to a single token, such
as sitting duck). This is not ideal, as it means
the model will need to be retrained for a new
set of MWEs (as the tokenisation will necessarily
change). It also requires “complete” knowledge of
the MWEs before the training step, which is im-
practical in most cases.

Character-level embedding models
(Hakimi Parizi and Cook, 2018) are one possible
solution to the fixed-vocabulary problem, in
being able to handle an unbounded vocabulary,
including MWEs. Document embeddings (Le and
Mikolov, 2014; Conneau et al., 2017a) are also
highly relevant to dynamically generating embed-
dings for MWEs, as they generate representations
of arbitrary spans of text, which are potentially
able to capture the context of use of the MWE.

3 Methodology

Following Salehi et al. (2015) and Nandakumar
et al. (2018), we compute the overall composi-
tionality of an MWE with three broad metrics:
direct composition, paraphrase similarity, and a
combined metric. In all experiments, the similar-
ity of a pair of vectors is measured using cosine
similarity.

3.1 Direct Composition

Intuitively, an MWE appearing in similar contexts
to its components is likely to be compositional.
We directly compare the vector embedding of the
MWE (described in Section 4.2) with that of its
component words, in one of two ways: (1) per-
forming an element-wise sum to obtain a ‘com-
bined’ vector, which is then compared with the
vector of the MWE (Directpre); and (2) a post-hoc

combination of the scores obtained by individually
comparing the component vectors with that of the
MWE via a weighted sum (Directpost). Formally:

Directpre =cos(mwe,w1 +w2)

Directpost =α cos(mwe,w1)+

(1− α) cos(mwe,w2) ,

where: mwe, w1, and w2 are the embeddings for
the combined MWE, first component and second
component, respectively;3 w1+w2 is the element-
wise sum of the vectors of each of the component
words of the MWE; and α ∈ [0, 1] is a scalar
which allows us to vary the weight of the respec-
tive components in predicting the compositional-
ity of the compound. This helps us effectively
capture the compositionality of the MWE with re-
gards to each of its individual constituents.

We do not perform any tuning of α over held-
out data and are, as such, overfitting as we select
the best-performing α post hoc. We do, however,
present analysis of hyper-parameter sensitivity in
Section 5.

3.2 Paraphrase Similarity
Assuming access to paraphrases of an MWE, an-
other intuition is that if the MWE appears in sim-
ilar contexts to the component words of its para-
phrases, it is likely to be compositional (Shwartz
and Waterson, 2018). Each paraphrase provides an
interpretation of the semantics of the MWE, e.g.
ancient history is “in the past”, “old news” or “for-
ever ago” (note how each paraphrase brings out a
slightly different interpretation). The RAMISCH
MWE dataset (described in Section 4.1) provides
one or more paraphrases for each MWE contained
in it. We calculate the similarity of the embed-
dings of the MWE and its paraphrases using the
following three formulae:

Para first =cos(mwe,para1)

Para allpre =cos(mwe,
∑
i

parai)

Para allpost =
1

N

N∑
i=1

cos(mwe,parai) ,

where para1 and parai denote the embedding for
the first (most popular) and i-th paraphrases, re-
spectively.

3All methods are presented and evaluated in terms of two-
element MWEs in this work, but are trivially generalisable to
multi-element MWEs.



29

In the case of Para allpost, we considered com-
puting the maximum instead of the average (as we
report here) of the similarity scores between each
paraphrase and its MWE, following the intuition
that an MWE would be similar to at least one re-
ported paraphrase, rather than all of them. How-
ever, the results for the average similarity were
empirically higher across models.

3.3 Combined Metric

Finally, we present the combined results from the
two metrics stated above:

Combined =βmax
(
Directpre,Directpost

)
+

(1− β)max
(
Para first,Para allpre,

Para allpost
)
,

where β ∈ [0, 1] is a scalar weighting factor used
to balance the effects of the two methods, in order
to measure the extent to which the composition-
ality is determined by each of the methods. The
choice of the max operator here to combine the
sub-methods for each of the direct composition
and paraphrase methods is that all methods tend
to underestimate the compositionality (and empir-
ically, it was found to be superior to taking the
mean).

4 Experiments

4.1 Datasets

We used three datasets for our experiments, eval-
uating each model’s performance using Pearson’s
correlation coefficient (r) to compare the similar-
ity scores obtained with the annotated composi-
tionality scores provided in the dataset.

REDDY The dataset of Reddy et al. (2011) con-
tains 90 binary English noun compounds (“NCs”),
along with human-annotated scores of their over-
all compositionality and component-specific com-
positionality, both ranging from 0 to 5. For our
experiments, we consider the overall composition-
ality scores only.

RAMISCH Similar to REDDY, the English
dataset of Ramisch et al. (2016) contains 90 binary
noun compounds with annotated scores of compo-
sitionality ranging from 0 to 5, both overall and
component-specific (of which we use only the for-
mer). It also contains a list of paraphrases for each
NC, presented in decreasing order of popularity
among the annotators.

Dataset µ σ

REDDY 53.2 30.0
RAMISCH 52.6 35.0
DISCO 68.1 21.7

Overall 59.7 29.0

Table 1: Mean (µ) and standard deviation (σ) of the
compositionality scores for the three datasets used in
this research, over a normalised range [0, 100].

DISCOADJ The English dataset from the DiSCo
shared task (Biemann and Giesbrecht, 2011) con-
taining a total of 348 binary phrases, comprising
adjective–noun, verb–nounsubj, and verb–nounobj
pairs, along with their overall compositionality
rating ranging from 0 to 100. The phrases were ex-
tracted semi-automatically and their relations were
assigned by patterns and checked manually. The
compositionality scores were collected from Ama-
zon Mechanical Turk, where workers were pre-
sented 4–5 randomly sampled sentences from the
UK English WACKy corpora. We focus on the 144
adjective–noun pairs in this study.

The breakdown of compositionality scores
across the three datasets in Table 1 indicates there
is a reasonable distribution of data in terms of
compositionality, with REDDY and RAMISCH be-
ing roughly comparable and covering a broad (and
somewhat balanced) spectrum of compositionali-
ties, while DISCO is more skewed towards com-
positional usages, with lower standard deviation.

4.2 Embeddings

We made use of various embeddings, ranging from
character- to document-level, in our study. Below
is a description of each model along with how they
are trained. Where available, we made use of pre-
trained models as is standard practice in NLP. As
the different models were trained on different cor-
pora, we are not attempting to perform a controlled
comparative evaluation of the different models, so
much as a comparison of the standard pre-trained
versions of each. If we were to retrain our own
models over a standard dataset such as English
Wikipedia, we would expect the results for the
document-level embedding methods in particular
to drop.



30

4.2.1 Word-level
A word embedding captures the context of a word
in a document (in relation to other words) in the
form of a vector representation. It tokenises text at
the word level.

word2vec We trained word2vec (Mikolov
et al., 2013b) on a recent English Wikipedia
dump,4 after pre-processing (removing the for-
matting and punctuation) and concatenating each
occurrence of the multiword expressions in our
datasets (e.g. every occurrence of close shave in
the corpus becomes closeshave). We make the
greedy assumption that every occurence of the
component words in sequence is an occurrence of
the expression. We perform this token-level iden-
tification and manipulation of the corpus in order
to obtain a single embedding for the expression,
instead of a separate embeddings for the individ-
ual component words. In cases where the model
still fails to generate an embedding (2 for REDDY,
8 for RAMISCH and 25 for DISCO) for the expres-
sion (due to low token frequency), we assign a de-
fault compositionality score of 0.5 (neutral; based
on a range of [0, 1]). For paraphrases, we compute
an element-wise sum of the embeddings for each
of the component words to serve as the embedding
of the phrase. We do this because token-level iden-
tification of each paraphrase in the training corpus
is not practical.

4.2.2 Character-level
Character-level embeddings can generate vectors
for words based on n-gram character aggrega-
tions. This means they can generate embeddings
for out-of-vocabulary (OOV) words, as well new
words or misspelled words. It tokenises text at the
character level.

fastText We used the 300-dimensional fast-
Text model pre-trained on Common Crawl and
Wikipedia using CBOW (fastTextpre), as well as
one trained over the same Wikipedia corpus4 us-
ing skip-gram (fastText). Again, since fastText
(Bojanowski et al., 2017) assumes all words to
be whitespace delimited, we preprocess our MWE
and paraphrases the same way as above (removing
the space between them so that armchair critic be-
comes armchaircritic, say).

Contextualised Embeddings Unlike classical
embedding techniques, contextualised embed-

4Dated 07-Jan-2019

dings capture the semantics of a word or phrase
in a manner which is sensitised to the context of
usage.

We used the pretrained implementations of
ELMo (Peters et al., 2018) and BERT (Devlin
et al., 2018) found in the Flair framework.5 The
framework also has a contextualised string embed-
ding model of its own, also named Flair (Akbik
et al., 2018).

We supplied sentences extracted from the
Brown corpus where available in order to derive
a contextualised interpretation. We extracted 25
sentences at random per MWE, except where there
were fewer sentences in the corpus.

However, we also included a naive context-
independent implementation in our study, consis-
tent with the other models, following the intuition
that the relative compositionality of even a novel
compound can often be predicted from its com-
ponent words alone (e.g. giraffe potato having the
plausible compositional interpretation of a potato
shaped like a giraffe vs. couch intelligence having
no natural interpretation).

4.2.3 Document-level

Document embeddings aggregate from words to
documents, generating vector representations for
entire documents. Since document and sentence
embeddings are capable of generating a single em-
bedding for a span of text, we are able to gener-
ate representations of the MWEs and paraphrases
without preprocessing them (to remove space).
We treat each constituent word as a single word
document to generate embeddings.

infersent We used two versions of infersent
(Conneau et al., 2017b): infersentGloVe and in-
fersentfastText . Each generates a representation of
300 dimensions, trained over the 1,000,000 most
popular English words using GloVe (Pennington
et al., 2014) and fastText, respectively.

doc2vec We used the gensim implementation
of doc2vec (Le and Mikolov, 2014; Lau and
Baldwin, 2016) pretrained on Wikipedia data us-
ing the word2vec skip-gram models pretrained on
Wikipedia and AP News.6

5https://github.com/zalandoresearch/
flair

6https://github.com/jhlau/doc2vec

https://github.com/zalandoresearch/flair
https://github.com/zalandoresearch/flair
https://github.com/jhlau/doc2vec


31

Emb. method Directpre Directpost Para first Para allpre Para allpost Combined

Flair 0.165 0.295 (α = 0.1) 0.334 0.399 0.492 0.492 (β = 0.0)
Flaircontext 0.181 0.314 (α = 0.1) 0.357 0.411 0.522 0.522 (β = 0.0)
fastTextpre 0.395 0.446 (α = 0.7) 0.242 0.531 0.703 0.703 (β = 0.0)
fastText 0.464 0.532 (α = 0.7) 0.548 0.613 0.673 0.673 (β = 0.0)
BERT 0.071 0.086 (α = 1.0) 0.242 0.531 0.583 0.583 (β = 0.0)
BERTcontext 0.089 0.111 (α = 1.0) 0.267 0.546 0.601 0.601 (β = 0.0)
ELMo 0.420 0.459 (α = 0.6) 0.361 0.488 0.546 0.546 (β = 0.2)
ELMocontext 0.461 0.489 (α = 0.6) 0.373 0.492 0.552 0.627 (β = 0.2)
word2vec 0.581 0.571 (α = 0.6) 0.443 0.510 0.504 0.677 (β = 0.9)
infersentGloVe 0.321 0.427 (α = 0.7) 0.636 0.700 0.741 0.783 (β = 0.5)
infersentfastText 0.169 0.221 (α = 0.6) 0.488 0.712 0.636 0.774 (β = 0.0)
doc2vec −0.157 0.039 (α = 1.0) 0.388 0.334 0.373 0.419 (β = 0.3)

Table 2: Pearson correlation coefficient for compositionality prediction results on the RAMISCH dataset.

Emb. method Directpre Directpost

Flair −0.127 0.024 (α = 0.0)
Flaircontext 0.012 0.172 (α = 0.0)
fastTextpre 0.223 0.285 (α = 0.3,0.4)
fastText 0.217 0.287 (α = 0.3,0.4)
BERT 0.304 0.352 (α = 0.2)
BERTcontext 0.313 0.377 (α = 0.2)
ELMo 0.339 0.406 (α = 0.5)
ELMocontext 0.387 0.416 (α = 0.5)
word2vec 0.634 0.622 (α = 0.6)
infersentGloVe 0.413 0.500 (α = 0.5)
infersentfastText 0.401 0.527 (α = 0.6)
doc2vec −0.049 0.025 (α = 0.0)

Table 3: Pearson correlation coefficient for composi-
tionality prediction results on the REDDY dataset.

5 Results and Discussion

The results from our experiments on the
RAMISCH, REDDY and DISCO datasets can
be found in Tables 2, 3 and 4, respectively, with
the best performing αs and βs for each embedding
method.

We observe that the αs in Table 2 are high, im-
plying the compound nouns in RAMISCH are more
compositional in terms of their head (second)
nouns. Similarly, the lower α scores in Table 3
suggest REDDY’s compound nouns are more de-
pendent on their modifiers, or first nouns. Table 4,
on the other hand, shows the αs embracing the en-
tire range of [0, 1]. This suggests the adjective–
noun pairs in DISCO are spread in terms of their

Emb. method Directpre Directpost

Flair 0.261 0.291 (α = 0.4)
Flaircontext 0.280 0.315 (α = 0.4)
fastTextpre 0.339 0.353 (α = 0.6,0.7)
fastText 0.374 0.419 (α = 0.4)
BERT 0.154 0.177 (α = 0.3,0.4)
BERTcontext 0.163 0.189 (α = 0.3)
ELMo 0.253 0.287 (α = 0.5)
ELMocontext 0.301 0.319 (α = 0.5)
word2vec 0.427 0.419 (α = 0.4)
infersentGloVe 0.321 0.315 (α = 0.4)
infersentfastText 0.001 0.202 (α = 1.0)
doc2vec −0.023 0.003 (α = 0.0)

Table 4: Pearson correlation coefficient for composi-
tionality prediction results on the DISCOADJ dataset.

dependency on their constituents, which also de-
pends on the embedding method used. Overall, the
methods are sensitive to the choice of the α hyper-
parameter, with ELMo and infersent being partic-
ularly sensitive and showing substantial change in
output with change in α (Figures 1,2 and 3).

We see that for RAMISCH (Table 2), word2vec
achieves the highest scores among the direct com-
bination metrics, while infersent outperforms the
other methods among the paraphrase metrics, and
word2vec falls behind character embedding mod-
els like fastText, ELMo and BERT (even when
the latter two were performed without context).
The lower β scores also show the other models
favouring the paraphrase metrics, while the high
β score for word2vec shows its preference for di-



32

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

0

0.2

0.4

0.6

0.8

1

α

r
ELMo fastText

doc2vec infersentGloVe
infersentfastText word2vec

BERT Flair

Figure 1: Sensitivity analysis of α (REDDY)

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

0

0.2

0.4

0.6

0.8

1

α

r

ELMo fastText
doc2vec infersentGloVe

infersentfastText word2vec
BERT Flair

Figure 2: Sensitivity analysis of α (RAMISCH)

rect combination.
We observe that, consistent with its perfor-

mance on RAMISCH, word2vec performs the best
of all models for the direct combination methods.

Overall, we observe that word2vec is con-
sistent in providing the best results based on
the methods outlined in Section 3.1, while fast-
Text and infersent come a close second and
third, respectively. It is noteworthy, however,
that word2vec required explicit modelling of the
MWEs during the training procedure, while the
other models did not.

It is not surprising that infersent, being a
document-level embedding model, works better
with paraphrase data than the other models. How-
ever, doc2vec has really poor scores overall
across the three datasets. It does, however, re-
deem itself with the paraphrases, with substan-

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

0

0.2

0.4

0.6

0.8

1

α

r

ELMo fastText
doc2vec infersentGloVe

infersentfastText word2vec
BERT Flair

Figure 3: Sensitivity analysis of α (DISCO)

tially higher scores than the direct metric but still
quite a way behind the top-scoring methods.

We also see that the paraphrase metric seems
to achieve much greater results across all mod-
els, suggesting this could be a direction for fu-
ture study (noting the requirement for paraphrase
data for the MWE in order to apply this method,
which has inherent scalability limitations). The
combined metric seems to favour the paraphrase
results as well, based on the relative β values.

One of the reasons word2vec did not work as
well with the paraphrases could be the naive as-
sumption that the Directpre is a representation of
the paraphrase itself. As we see from the results
across the datasets and methods, Directpre does not
entirely capture the compositionality of the MWE,
so it is reasonable to assume that a paraphrase
would not be accurately represented by Directpre
either.

We see that fastText provides us with impres-
sive scores throughout, and we notice a slight im-
provement when trained on the same corpus as
word2vec. However, there is a huge gap in the
performance between word2vec and fastText, es-
pecially in the case of REDDY (which could be
an issue of a heavier representation of a particu-
lar level of compositionality, say).

We also notice that, unlike the noun compounds
in REDDY and RAMISCH, there is less variance in
the relative scores of each method in the case of
DISCOADJ, with overall results dropping appre-
ciably, and the best-performing word2vec drop-
ping back in raw r value compared to noun–noun
pairs.

In terms of the contextualised embeddings, we



33

notice that across the three models, there is only a
slight increase in correlation when contextualised
embeddings are used. This suggests that even with
context, these modern embedding techniques are
unable to capture non-compositionality as well as
their simpler counterparts.

Further analysis reveals that most models strug-
gle to accurately predict the compositionality of
idiomatic noun compounds, as well as semi-
compositional terms wherein one of the con-
stituent words are used in a metaphoric sense. In
REDDY, we observe this for silver bullet and snail
mail. Interestingly, while BERT struggles to ef-
fectively model compositionality throughout, it is
surprisingly the only model able to perfectly pre-
dict the compositionality of snail mail (which ap-
pears as an extreme outlier). This suggests that
BERT might be more successful using a different
metric. In the case of the adjective–noun phrases
in DISCO, we see that the models are still unable
to accurately predict the compositionality of non-
compositional phrases (like big fish, heavy metal
and red tape). This time, however, they are also
unable to capture mobile phone and floppy disk,
perhaps because of their relatively archaic use.

6 Conclusion

In this paper, we investigated the modelling capa-
bilities of various embedding techniques applied
to the specific task of predicting the MWE compo-
sitionality, to see how well they model a mixture
of compositionalities in the dataset. Our results
indicate that modern character- and document-
level embedding methods are inferior to the sim-
ple word2vec approach. However, the promis-
ing results of fastText and infersent across the
datasets indicate that, among the more modern
methods, they are better equiped to handle non-
compositionality as they did not require much
manipulation of the corpus or knowledge of the
MWEs beforehand. We also found that the para-
phrase metric results in greater correlation scores
across the models.

In future work, we intend to tune our hyperpa-
rameters over held-out data, and experiment with
other languages and language-independent tech-
niques, including other models.

Acknowledgments

We would like to thank the curators and annotators
of the datasets used in this study, and the anony-

mous reviewers for their insightful comments. We
would also like to thank Chris Biemann from the
University of Hamburg for making the dataset
from the DiSCo shared task available to us.

References
Alan Akbik, Duncan Blythe, and Roland Vollgraf.

2018. Contextual string embeddings for sequence
labeling. In COLING 2018, 27th International Con-
ference on Computational Linguistics, pages 1638–
1649, Santa Fe, USA.

Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
erau (eds.) Handbook of Natural Language Process-
ing, Second Edition, CRC Press, pages 267–292,
Boca Raton, USA.

Chris Biemann and Eugenie Giesbrecht. 2011. Dis-
tributional semantics and compositionality 2011:
Shared task description and results. In Proceedings
of the Workshop on Distributional Semantics and
Compositionality, pages 21–28, Portland, USA.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017a. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017b. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.

Silvio Cordeiro, Aline Villavicencio, Marco Idiart, and
Carlos Ramisch. 2019. Unsupervised composition-
ality prediction of nominal compounds. Computa-
tional Linguistics, 45(1).

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805.

https://people.eng.unimelb.edu.au/tbaldwin/pubs/handbook2009.pdf
https://people.eng.unimelb.edu.au/tbaldwin/pubs/handbook2009.pdf
http://aclweb.org/anthology/W11-1304
http://aclweb.org/anthology/W11-1304
http://aclweb.org/anthology/W11-1304
http://aclweb.org/anthology/Q17-1010
http://aclweb.org/anthology/Q17-1010
https://doi.org/10.1145/1390156.1390177
https://doi.org/10.1145/1390156.1390177
https://doi.org/10.1145/1390156.1390177
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070


34

Ali Hakimi Parizi and Paul Cook. 2018. Do character-
level neural network language models capture
knowledge of multiword expression compositional-
ity? In Proceedings of the Joint Workshop on Lin-
guistic Annotation, Multiword Expressions and Con-
structions (LAW-MWE-CxG-2018), pages 185–192,
Santa Fe, USA.

Jey Han Lau and Timothy Baldwin. 2016. An empiri-
cal evaluation of doc2vec with practical insights into
document embedding generation. In Proceedings
of the 1st Workshop on Representation Learning for
NLP, pages 78–86, Berlin, Germany.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Pro-
ceedings of the 31st International Conference on
Machine Learning (ICML 2014), pages 1188–1196,
Beijing, China.

Tomas Mikolov, G.s Corrado, Kai Chen, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceddings of Workshop
at the International Conference on Learning Repre-
sentations, pages 1–12.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Navnita Nandakumar, Bahar Salehi, and Timothy Bald-
win. 2018. A comparative study of embedding mod-
els in predicting the compositionality of multiword
expressions. In Proceedings of the Australasian
Language Technology Association Workshop 2018,
pages 71–76, Dunedin, New Zealand.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237, New Orleans, USA.

Carlos Ramisch, Silvio Cordeiro, Leonardo Zilio,
Marco Idiart, and Aline Villavicencio. 2016. How
naked is the naked truth? a multilingual lexicon of
nominal compound compositionality. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 156–161, Berlin, Germany.

Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in

compound nouns. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 210–218, Chiang Mai, Thailand.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1–15, Mexico City,
Mexico.

Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015.
A word embedding approach to predicting the com-
positionality of multiword expressions. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
977–983, Denver, USA.

Vered Shwartz and Chris Waterson. 2018. Olive oil
is made of olives, baby oil is made for babies: Inter-
preting noun compounds using paraphrases in a neu-
ral model. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 218–224,
New Orleans, USA.

http://aclweb.org/anthology/W18-4920
http://aclweb.org/anthology/W18-4920
http://aclweb.org/anthology/W18-4920
http://aclweb.org/anthology/W18-4920
http://aclweb.org/anthology/U18-1009
http://aclweb.org/anthology/U18-1009
http://aclweb.org/anthology/U18-1009
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/P16-2026
https://doi.org/10.18653/v1/P16-2026
https://doi.org/10.18653/v1/P16-2026
http://aclweb.org/anthology/I11-1024
http://aclweb.org/anthology/I11-1024
https://doi.org/10.3115/v1/N15-1099
https://doi.org/10.3115/v1/N15-1099

