




































Data-driven Morphology and Sociolinguistics for Early Modern Dutch

Marijn Schraagen Marjo van Koppen
Utrecht Institute of Linguistics OTS

Utrecht University
{m.p.schraagen,j.m.vankoppen}@uu.nl

Feike Dietz
Institute for Cultural Inquiry

Utrecht University
f.m.dietz@uu.nl

Abstract
The advent of Early Modern Dutch (start-
ing ∼1550) marked significant develop-
ments in language use in the Netherlands.
Examples include the loss of the case
marking system, the loss of negative par-
ticles and the introduction of new vocabu-
lary. These developments typically lead to
a lot of variation both within and between
language users. Linguistics research aims
to characterize and account for such vari-
ation patterns. Due to sparseness of digi-
tal resources and tools, research is still de-
pendent on traditional, qualitative analy-
sis. This paper describes an ongoing ef-
fort to increase the amount of tools and re-
sources, exploring two different routes: (i)
modernization of historical language and
(ii) adding linguistic and sociolinguistic
annotations to historical language directly.
This paper discusses and compares the ex-
perimental setup, and preliminary results
of these two routes and provides an out-
look on the envisioned linguistic and so-
ciolinguistic research approach.

1 Introduction

In the 16th century, the language situation in the
Netherlands changed substantially. One impor-
tant influence is the interest in standardization of
the Dutch language (Lambrecht, 1550; de Heuiter,
1581; Spiegel, 1584; Stevin, 1586). This stan-
dardization process was combined with ongoing
developments in case marking (Weerman and de
Wit, 1999), negation (Hoeksema, 1997), and var-
ious other lexical and morphosyntactic phenom-
ena (Howell, 2006). The extent to which these
developments were actually adopted by language
users differs between and within individual lan-
guage users (Bax and Streekstra, 2003; Nobels and
Rutten, 2014).

1637:
1888:

Ende
En

het
het

gout
goud van

deses
dit

lants
land

is
is

goet
goed

‘And the gold of that land is good’

Figure 1: Example parallel Bible translation

To date, most approaches to study these phe-
nomena have been qualitative in nature. In this pa-
per an ongoing effort is described to enrich Early
Modern text with linguistic and sociolinguistic in-
formation in a systematic way, to allow a quantita-
tive computational linguistic approach. The paper
explores two routes to develop such an approach:
a modernization route and a historical annotation
route. In Section 2 an approach to text moderniza-
tion is outlined. Section 3 describes an automatic
tagging approach with manual post-correction and
metadata enrichment. Section 4 provides a com-
parison between these two routes.

2 Modernization

In contrast to historical Dutch, modern Dutch is
a very well-resourced language for NLP applica-
tions. A translation modernization step allows
to use these resources for historical texts (Tjong
Kim Sang, 2016). The modernization process can
benefit from the similarity between the two his-
torically related language varieties (Koolen et al.,
2006). For the development of the modernization
method described in this paper, a parallel pair of
Dutch Bible translations from 16371 and 18882 is
used. The close parallelism of this training pair
(see Figure 1) allows for efficient application of
word pair extraction algorithms. The method con-
sists of a combination of three approaches: (i)
application of manual expert rewriting rules (cf.

1http://dbnl.nl/tekst/_sta001stat01_01/
2http://www.statenvertaling.net

Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language 47



Braun, 2002; Robertson and Willett, 1992), (ii) ex-
traction of a translation lexicon from parallel pairs
in training data (cf. Bollmann et al., 2011), and
(iii) the application of the existing statistical ma-
chine translation framework Moses (Köhn et al.,
2007), cf. (Pettersson et al., 2013; Scherrer and
Erjavec, 2016). In the remainder of this paper the
combination of the first two approaches is referred
to as the custom method, while the third approach
is called the SMT method.

The dataset is split into a training part (32,235
lines, 946,721 words, 87%) and a test part (5,000
lines, 140,812 words, 13%). The split is linear,
with the training part ranging from Genesis to the
(apocryphic) Book of Ezra, and the test part rang-
ing from Ezra to 3 Maccabees. For the SMT
method, a subset of 2000 lines (58,249 words) is
removed from the end of the training set to be used
as a development set for MERT.

In Table 1 the results of the modernization ap-
proach are listed. To compare translations, the
BLEU measure is used (Papineni et al., 2002).
This score has notable shortcomings as a mea-
sure of translation accuracy (Callison-Burch et al.,
2006), pertaining to phrase permutation and se-
mantic unawareness. However, these shortcom-
ings appear to be less severe for a modernization
task, where phrase-based translations and word re-
ordering are less likely to occur. Moreover, a cor-
rect translation is not the main goal of this method.
Instead, the modernization is intended to increase
accuracy of NLP methods, e.g., POS tagging, syn-
tactic parsing, frequency counts, lexicon lookup,
etc. It is not yet clear how well the BLEU score
correlates with accuracy of these methods, how-
ever some correlation is to be expected.

The details of the custom method are as follows:
as sub-baseline, no translation is performed. As
baseline all parallel sentences of equal length
have been extracted from the training data, and
all words with an unambiguous (i.e., always the
same) translation are used as a translation lexicon
for the test data. Next, all sentences are aligned on
word level to extract additional translation pairs.
Note that the baseline and the alignment are rela-
tively efficient, due to the close parallelism of the
source data. Then, manual modernization rules
are applied, specifically targeted to Early Modern
Dutch, such as case marker normalization, nega-
tion normalization, clitic separation, numeral nor-
malization. Note that phonetic rewriting is not

part of this step. Next, translation pairs are con-
structed for multiple word translations (e.g., de-
ses→ van dit in Figure 1, English: thisGEN → of
this). At this point, the test set is already suffi-
ciently modern to allow accurate POS tagging, at
least on the tokens that have been assigned the cor-
rect, modern translation. This POS information
can be used to translate a historical word in dif-
ferent ways conditional on the surrounding POS
tags. This is similar to the multiple word transla-
tion, except that the selection on POS tags allows
to generalize over the vocabulary. As an example,
the pronoun haer is likely to be translated as hen
(them) before a verb, and as hun (their) before a
noun. To limit sparseness issues, the main POS
tag is used without features. For both the multi-
ple word and the POS step, pairs have been se-
lected using hill-climbing, implemented as incre-
mental inclusion of those pairs that increase the
BLEU score on the training set. Note that, since
the pairs are extracted from the training set (i.e., a
development set is not used), the hill-climbing se-
lection is equivalent to selecting translations with
a true positive application rate of over 0.5. Finally,
a number of highly document-specific rules have
been applied to address differences in punctua-
tion between the two Bible translations. Examples
of rules and word pairs are provided in Table 2.

For the evaluation of the SMT method, a dif-
ferent sequence of steps is applied. First, a model
is built by Moses using the training set with basic
settings. Then, MERT tuning is applied using the
development set. Next, the capitalization model
of Moses, which turned out to be highly inaccu-

method steps BLEU
custom no translation 0.134

baseline 0.507
aligned 0.530
rules 0.581
multiple word 0.600
POS information 0.619
punctuation 0.627

SMT Moses basic 0.597
MERT tuning 0.616
capitalization 0.639

combination rules 0.644
multiple word, POS 0.647
punctuation 0.653

Table 1: Translation evaluation

Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language 48



input output notes translation
rewriting rules

stem+se stem hen pronoun clisis them
eens stems van een genitive of a
den/mijnen/welken de/mijn/welke case loss the/mine/which
alle de al de agreement loss all the
en [. . . ] negative negative negative concord
numeral ende num num+en+num
num num num+num

punctuation rules
`
; [upper case] :
; [lower case] ,
said, [upper case] :

extracted multiword pairs
haer zelfden zichzelf reflexive pronoun him/her/it/them/oneself
waer heen waarheen prepositional compound where to
leeuws tanden leeuwentanden case loss, compounding lion teeth
potte-backers kruik pottenbakkerskruik case loss, compounding pottery jar
rechteroog heupe rechterheup terminology shift right hand side hip

extracted Part-of-Speech pairs
alle+V allen all
alle+PRON al all
PUNCT+alle al all
daar+V er there

Table 2: Example implementations of translation steps

rate, is corrected using post-processing. Com-
bining the SMT and the custom method, manual
rewriting rules are applied on top of SMT, fol-
lowed by multiple word alignment, POS infor-
mation and punctuation rules.

2.1 Discussion
Both the method using manual rules combined
with automatic translation pair extraction as well
as the method using the Moses toolkit show a
substantial improvement over the baseline perfor-
mance. For the first method, the manual rule
component provides the largest share of the per-
formance improvement. This indicates (consis-
tent with, e.g., Pettersson et al., 2012) that lan-
guage development over time, at least in the case
of Bible translations, displays a high level of regu-
larity, which can be captured by a small number of
rewriting rules. Interestingly, the morphological
rules combined with translation pair extraction of-
fer sufficient coverage to omit phonetic rewriting
commonly used in language modernization. Note
that this behavior depends on similarity between

training and test vocabulary, which will be dis-
cussed further in Section 2.1.1.

The described method provides competitive
performance as compared to the SMT approach.
It can be considered promising that the results of
a state-of-the-art machine learning algorithm can
be reproduced using a relatively straightforward
approach. However, Table 1 also shows that the
combination of approaches offers very little im-
provement over the performance of the SMT algo-
rithm in isolation. Therefore, it is at present not
fully clear how to incorporate the custom transla-
tion pair extraction or manual morphological rules
into a combined methodology.

To obtain a better insight in the performance
of the various methods, a more extensive evalu-
ation is necessary. This includes the application
of the method on more diverse data and a sys-
tematic comparison between approaches. Further-
more, the evaluation could be extended into a more
application–oriented direction, i.e., by analyzing
results of NLP methods on modernized text.

Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language 49



2.1.1 CLIN27 Shared Task
The method presented in this paper has also been
entered into the CLIN27 Shared Task on Translat-
ing Historical Text3. The results of the system on
this task are considerably lower than in the present
evaluation, which can be contributed to several
factors.

First, the test set used for the Shared Task was
markedly different from the provided training set.
The test set contained genres such as theater plays,
letters, eulogies, administrative texts, journal en-
tries, and bullet point lists of activities. These gen-
res introduce a significant amount of new vocab-
ulary, for which the word-level vocabulary-based
method as presented in this paper is not particu-
larly well-suited. In the Shared Task the method
was extended with a set of phonetic rewriting
rules, which showed a large performance increase.
This is consistent with previous work on character-
level SMT approaches (e.g., Scherrer and Erjavec,
2013), which are essentially a way to automati-
cally extract phonetic rewriting rules from data.

Furthermore, the test set contained texts ranging
from 1607 to 1692. Various morphosyntactic or
spelling-related phenomena occurring in the 1637
training set which are targeted with specific rules
do not occur in later texts, such as negative con-
cord constructions. Application of these rules on
later texts actually decreases performance in cer-
tain cases, and should therefore be controlled by
time period constraints.

Additionally, the test set fot the Shared Task
was created with the specific goal of word-level
spelling modernization to facilitate POS tagging
(cf. Tjong Kim Sang, 2016). This resulted in
a rather artificial translation, preserving sentence
length and word order, leaving historical word
forms untranslated in case a modern tagger already
assigned a correct tag. As a result, in several cases
the current method provides an arguably better
translation which is nevertheless evaluated as an
error. Further analysis showed that, for a number
of participants in the Shared Task, manually re-
spelling a very small number of frequent errors re-
sulted in a substantial performance improvement.

For the reasons mentioned above, the results on
the CLIN27 Shared Task should not be considered
as a conclusive evaluation of the current method.
However, the results do indicate important aspects
of the current method, such as the impact of train-

3https://ifarm.nl/clin2017st/

ing vocabulary, and the influence of the goal ap-
plication on the translation requirements. Further
development of these issues is ongoing in the cur-
rent project.

3 Annotation

As stated above, text modernization allows for the
use of resources and tools for contemporary lan-
guage. However, this approach also introduces
incorrectly translated and non-translated tokens,
which limit the accuracy of NLP applications.
Moreover, certain information from the historical
text is lost. Modernization entails spelling nor-
malization, which means that, e.g., spelling dif-
ferences over time can no longer be studied. Other
topics of research, such as case marking or nega-
tion, may also be lost after modernization, or it
may prove difficult to link the modernized text to
the historical original. Therefore, an additional re-
search direction processes historical text directly,
using tools and resources for historical language
or using manual annotation. The annotation ef-
fort also allows extension to sociolinguistic infor-
mation, which is intrinsically outside the scope of
modernization approaches. The remainder of this
section describes the setup of the annotation task,
which are currently ongoing.

3.1 Part-of-speech tagging

A pilot project has been designed to annotate a
corpus of letters by the Dutch author and politi-
cian P.C. Hooft, written between 1600 and 1647.
In the absence of tools for Early Modern Dutch, a
POS tagger for Middle Dutch (1200–1500) is used
(van Halteren and Rem, 2013). Although Middle
Dutch is considerably different from Early Mod-
ern Dutch, several properties of interest are shared,
such as case marking and negation clitics. There-
fore, a tagger capable of marking such properties
is preferred over contemporary equivalents. How-
ever, as expected on Early Modern data, the over-
all accuracy of the tagger is low. Therefore, a man-
ual annotation effort is ongoing to check and cor-
rect all assigned tags (including morphosyntactic
features) in the corpus manually.

3.2 Sociolinguistic tagging

An accurately tagged corpus allows to discover
patterns on a morphosyntactic level. To ana-
lyze the development of such phenomena, non-
linguistic variables have to be taken into account.

Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language 50



Als
Con(sub)
If

nu
Adv(gen)
now

de
Det()
the

veerschujt
N(sg)
ferry

niet
Pro(neg)
nothing

anders
Adj(-s)
else

ujt
Adp(prtcl)
out

en
Adj(negcl)
not

leverde
V(past)
delivered

dan
Con(cmp)
than

den
Det(-n)
the

brief
N(sg)
letter

‘If the ferry would not deliver anything but the letter’

Figure 2: Example tagged sentence, showing a negation clitic

The letter corpus contains dated documents, there-
fore a straighforward variable is time, allowing
analysis of when certain developments have oc-
curred. Other variables of interest include the
topic of correspondence, the type of relation be-
tween the correspondents and the domain of the
correspondence (government, finance, literature,
etc.), the goal of the correspondence (invitation,
recommendation, request, etc.), and personal in-
formation about the correspondent (age, gender,
literary status). Furthermore, the rhetorical struc-
ture of a text is annotated, in terms of greeting,
opening, body, closing. This for instance allows
to verify the hypothesis that certain parts of let-
ters, e.g., the opening and closing sections, are
highly formulaic, and therefore do not exhibit lan-
guage development to the same degree as the body
text (Nobels and Rutten, 2014). Annotation is per-
formed by a pool of nine annotators. To measure
inter-annotator agreement, 10% of the corpus is
assigned to random pairs of annotators. The full
list of sociolinguistic variables is provided in the
Appendix.

In Figure 2 an example sentence with part-of-
speech tags is provided. This sentence contains
the negation clitic en, alongside the main negation
niet. Once the full corpus is properly tagged this
clitic can be studied systematically, e.g., to investi-
gate the neighbouring tags or lemmas of the clitic,
or to check whether or not the clitic is used more
often in formal writing.

3.3 Interoperability

To increase the practical accessibility of the an-
notation data, a collaboration with the Nederlab
project (Brugman et al., 2016) has been estab-
lished. Nederlab provides an online search inter-
face for the data in the Digital Library of Dutch
Literature4 using Corpus Query Processor (Evert
and Hardie, 2011), which allows to search for lin-
guistic annotation and metadata. For this collab-
oration, several interoperability issues need to be

4http://www.dbnl.org, in Dutch

addressed. The Adelheid tagger uses the CRM
tagset, which contains a set of features specific
for Middle Dutch. The Nederlab project uses the
CGN tagset (van Eynde et al., 2000), for which
both the main tags and the feature set differ con-
siderably from CRM. For the current pilot several
additional features are introduced to facilitate the
analysis of language development.

Apart from the tagset, the output format needs
to be converted as well. Nederlab uses the FoLiA
format (van Gompel and Reynaert, 2013), which
is a de facto standard XML linguistic annotation
format for Dutch, whereas Adelheid uses a custom
XML format. To facilitate integration with current
annotations and metadata in Nederlab, a word-
level alignment of the FoliA output is planned.

Further interoperability considerations include
incorporation of linked data, e.g., for correspon-
dents in the current dataset which may also be
found in encyclopedic resources, and using ex-
isting classification schemes, such as HISCO for
historical occupational titles (van Leeuwen et al.,
2002).

4 Data-driven historical linguistics

The two methods outlined in this paper are in-
tended to complement eachother in providing an
environment for computational historical linguis-
tics research. Modernization has the advantage
that research questions can be addressed using the
existing infrastructure for a modern language, in
terms of resources, approaches, evaluation data et
cetera. The disadvantage of this method is the in-
herent loss of information and the occurence of
translation errors, which entails that several top-
ics of interest cannot be studied using modernized
data, or that the validity of results is unclear. In
contrast, manual enrichment provides high-quality
linguistic annotations as well as the possibility to
include meta-linguistic information. The obvious
disadvantage of this method is the large amount of
time and/or financial resources necessary. How-
ever, if a sufficiently large amount of data is an-

Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language 51



notated (possibly in combination with automati-
cally derived annotations, cf. Hupkes and Bod,
2016), machine learning algorithms can be trained
to allow for automatic annotation. The combi-
nation of modernization and manual annotation
may prove valuable as a methodology in histor-
ical (socio-)linguistics. Future work in the cur-
rent project, however, is necessary to validate this
claim.

References
Marcel Bax and Nanne Streekstra. 2003. Civil

rites: ritual politeness in early modern Dutch
letter–writing. Journal of Historical Pragmatics,
4(2):303–325.

Marcel Bollmann, Florian Petran, and Stefanie Dipper.
2011. Rule-based normalization of historical texts.
In Proceedings of Language Technologies for Dig-
ital Humanities and Cultural Heritage Workshop,
pages 34–42. ACL.

Loes Braun. 2002. Information retrieval from Dutch
historical corpora. Master’s thesis, Maastricht Uni-
versity.

Hennie Brugman, Martin Reynaert, Nicoline van der
Sijs, René van Stipriaan, Erik Tjong Kim Sang,
and Antal van den Bosch. 2016. Nederlab: To-
wards a single portal and research environment for
diachronic Dutch text corpora. In Proceedings of
LREC 2016.

Chris Callison-Burch, Miles Osborne, and Philipp
Köhn. 2006. Re-evaluation the role of BLEU in ma-
chine translation research. In Proceedings of EACL,
pages 249–256. ACL.

Stefan Evert and Andrew Hardie. 2011. Twenty-first
century corpus workbench: Updating a query archi-
tecture for the new millennium. In Proceedings of
the Corpus Linguistics 2011 conference. University
of Birmingham.

Frank van Eynde, Jakub Zavrel, and Walter Daelemans.
2000. Part of speech tagging and lemmatisation for
the Spoken Dutch Corpus. In Proceedings of LREC
2000.

Maarten van Gompel and Martin Reynaert. 2013.
FoLiA: A practical XML format for linguistic
annotation-a descriptive and comparative study.
Computational Linguistics in the Netherlands Jour-
nal, 3:63–81.

Hans van Halteren and Margit Rem. 2013. Dealing
with orthographic variation in a tagger-lemmatizer
for fourteenth century Dutch charters. Language
Resources and Evaluation, 47(4):1233–1259.

Pontus de Heuiter. 1581. Nederduitse orthogra-
phie. Edited by G.R.W. Dibbets, 1972, Wolters-
Noordhoff.

Jack Hoeksema. 1997. Negation and negative concord
in Middle Dutch. Amsterdam Studies in the Theory
and History of Linguistic Science, 4:139–156.

Robert Howell. 2006. Immigration and koineisation:
the formation of Early Modern Dutch urban ver-
naculars. Transactions of the Philological Society,
104(2):207–227.

Dieuwke Hupkes and Rens Bod. 2016. Pos-tagging of
historical Dutch. In Proceedings of LREC 2016.

Philipp Köhn, Hieu Hoang, Alexandra Birch, et al.
2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of the 45th an-
nual meeting of the ACL on interactive poster and
demonstration sessions, pages 177–180. Association
for Computational Linguistics.

Marijn Koolen, Frans Adriaans, Jaap Kamps, and
Maarten de Rijke. 2006. A cross-language ap-
proach to historic document retrieval. In ECIR
2006: Proceedings of the 28th European Conference
on IR Research, pages 407–419. Springer.

Joos Lambrecht. 1550. Nederlandsche spellijnghe,
uutghesteld by vraghe ende antwoorde. Edited by
J.F.J. Heremans and F. Vanderhaeghen, 1882, C.
Annoot-Braeckman.

Marco van Leeuwen, Ineke Maas, and Andrew Miles.
2002. HISCO: Historical International Standard
Classification of Occupations. Cornell University
Press.

Judith Nobels and Gijsbert Rutten. 2014. Language
norms and language use in seventeenth-century
Dutch: negation and the genitive. In Gijsbert Rut-
ten, editor, Norms and usage in language history,
1600-1900. A sociolinguistic and comparative per-
spective., pages 21–48. John Benjamins Publishing
Company.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Eva Pettersson, Beáta Megyesi, and Joakim Nivre.
2012. Rule-based normalisation of historical text -
a diachronic study. In Proceedings of the First in-
ternational Workshop on Language Technology for
Historical Text, KONVENS, pages 333–341.

Eva Pettersson, Beáta Megyesi, and Jörg Tiedemann.
2013. An SMT approach to automatic annotation of
historical text. In Proceedings of the workshop on
computational historical linguistics at NODALIDA,
pages 54–69. Linköping.

Alexander Robertson and Peter Willett. 1992. Search-
ing for historical word-forms in a database of 17th-
century English text using spelling-correction meth-
ods. In Proceedings of ACM SIGIR ’92, pages 256–
265. ACM.

Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language 52



Yves Scherrer and Tomaž Erjavec. 2013. Modern-
izing historical Slovene words with character-based
SMT. In BSNLP 2013-4th Biennial Workshop on
Balto-Slavic Natural Language Processing.

Yves Scherrer and Tomaž Erjavec. 2016. Modernising
historical slovene words. Natural Language Engi-
neering, 22(6):881–905.

Hendrik Spiegel. 1584. Twe-spraack. Ruygh-bewerp.
Kort begrip. Rederijck-kunst. Edited by W.J.H.
Caron, 1962, Wolters-Noordhoff.

Simon Stevin. 1586. Uytspraeck van de weerdicheyt
der Duytsche tael. Chr. Plantijn.

Erik Tjong Kim Sang. 2016. Improving part-of-speech
tagging of historical text by first translating to mod-
ern text. In Proceedings of the International Work-
shop on Computational History and Data-Driven
Humanities, pages 54–64. Springer.

Fred Weerman and Petra de Wit. 1999. The decline
of the genitive in Dutch. Linguistics, 37(6):1155–
1192.

Appendix: sociolinguistic variables

• Purpose of the letter
– Express thanks
– Compliment/praise
– Excuse
– Ask for a favour
– Ask for information
– Ask for advice
– Admonish
– Inform
– Remember
– Persuade
– Order
– Allow
– Invite

• Topic of the letter
– Business
– Literature
– Domestic affairs
– Love
– Death
– News
– Religion/ethics

• Correspondent information
– name
– group or individual

for individuals:
– birth/death date
– gender
– occupation
– literary author
– relation to P.C. Hooft

• Letter structure
– Introductory greeting
– Opening (optional)
– Narratio
– Closing (optional)
– Final greeting

Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language 53


