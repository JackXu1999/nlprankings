



















































CUNI Submission for Low-Resource Languages in WMT News 2019


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 234–240
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

234

CUNI Submission for Low-Resource Languages in WMT News 2019

Tom Kocmi Ondřej Bojar

Charles University, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics

Malostranské náměstı́ 25, 118 00 Prague, Czech Republic
<surname>@ufal.mff.cuni.cz

Abstract
This paper describes the CUNI submission
to the WMT 2019 News Translation Shared
Task for the low-resource languages: Gujarati-
English and Kazakh-English. We participated
in both language pairs in both translation di-
rections. Our system combines transfer learn-
ing from different high-resource language pair
followed by training on backtranslated mono-
lingual data. Thanks to the simultaneous train-
ing in both directions, we can iterate the back-
translation process. We are using the Trans-
former model in a constrained submission.

1 Introduction

Recently, the rapid development of Neural Ma-
chine Translations (NMT) systems led to the
claims, that human parity has been reached (Has-
san et al., 2018) on a high-resource language pair
Chinese-English. However, NMT systems tend to
be very data hungry as Koehn and Knowles (2017)
showed the NMT lacks behind phrase based ap-
proaches in the low-resource scenarios. This lead
to the rise of attention in the low-resource NMT
in recent years, where the goal is to improve the
performance of a language pair that have only a
limited available parallel data.

In this paper, we describe our approach to
low-resource NMT. We use standard Transformer-
big model (Vaswani et al., 2017) and apply two
techniques to improve the performance on the
low-resource language, namely transfer learning
(Kocmi and Bojar, 2018) and iterative backtrans-
lation (Hoang et al., 2018).

A model trained solely on the authentic parallel
data of the low-resource NMT model has poor per-
formance, thus using it directly for the backtrans-
lation of monolingual data lead to poor translation.
Hence the transfer learning is as a great tool to first
improve the performance of the NMT system later
used for backtranslating the monolingual data.

The structure of this paper is organized as fol-
lows. First, we describe the transfer learning and
backtranslation, followed by a description of used
datasets and the NMT model architecture. Next,
we present our experiments, final submissions,
and followup analysis of synthetic training data
usage. The paper is concluded in Section 5.

2 Background

In this chapter, we first describe the technique of
transfer learning and iterative backtranslation, fol-
lowed by our training procedure that combines
both approaches.

2.1 Transfer learning
Kocmi and Bojar (2018) presented a trivial method
of transfer learning that uses a high-resource lan-
guage pair to train the parent model. After the
convergence, the parent training data are replaced
with the training data of the low-resource lan-
guage pair, and the training continues as if the re-
placement would not happen. The training contin-
ues without changing any parameters nor resetting
moments or learning rate.

This technique of fine-tuning the model param-
eters is often used in a domain adaptation scenario
on the same language pair. However, when us-
ing for different language pairs, there emerges a
problem with vocabulary mismatch. Kocmi and
Bojar (2018) overcome this problem by prepar-
ing the shared vocabulary for all languages in both
language pairs in advance. Their approach is to
prepare mixed vocabulary from training corpora
of both languages and generate wordpiece vocab-
ulary (Vaswani et al., 2017) from it.

We use the balanced vocabulary approach, that
combines an equal amount of parallel data from
both training corpora, low-resource as well as the
same amount from high-resource language pair.
Hence the low-resource language subwords are



235

Corpora Language pair Sentence pairs Words 1st lang. Words in English
Commoncrawl Russian-English 878k 17.4M 18.8M

News Commentary Russian-English 235k 5.0M 5.4M
UN corpus Russian-English 11.4M 273.2M 294.4M

Yandex Russian-English 1000k 18.7M 21.3M
CzEng 1.7 Czech-English 57.4M 546.2M 621.9M

Crawl Kazakh-English 97.7k 1.0M 1.3M
News commentary Kazakh-English 9.6k 174.1k 213.2k

Wiki titles Kazakh-English 112.7k 174.9k 204.5k
Bible Gujarati-English 7.8k 198.6k 177.1k

Dictionary Gujarati-English 19.3k 19.3k 28.8k
Govincrawl Gujarati-English 10.7k 121.2k 150.6k

Software Gujarati-English 107.6k 691.5k 681.3k
Wiki texts Gujarati-English 18.0k 317.9k 320.4k
Wiki titles Gujarati-English 9.2k 16.6k 17.6k

Table 1: The parallel training corpora used to train our models with counts of the total number of sentences as
well as the number of words (segmented on space). More details on the individual corpora can be obtained at
http://statmt.org/wmt19/.

represented in the vocabulary in the roughly same
amount as the high-resource language pair.

As Kocmi and Bojar (2018), showed the lan-
guage pair does not have to be linguistically re-
lated, and the most important criteria is the amount
of parent parallel data. For this reason, we have se-
lected Czech-English as a parent language pair for
Gujarati-English and Russian-English as a parent
for the Kazakh-English. The Russian was selected
due to the use of Cyrillic and being a high-resource
language pair. All language pairs share English.
We prepare Gujarati-English and Kazakh-English
systems separately from each other.

2.2 Backtranslation
The amount of available monolingual data typi-
cally exceeds the amount of available parallel data.
The standard technique of using monolingual data
in NMT is called backtranslation (Sennrich et al.,
2016). It uses a second model trained in the re-
verse direction to translate monolingual data to the
source language of the first model.

Backtranslated data are aligned with their
monolingual sentences to create synthetic parallel
corpora. The standard practice is to mix the au-
thentic parallel corpora to the synthetic. Although
it is not the only approach. (Popel, 2018) proposed
a scenario of alternating the training between us-
ing only synthetic and only authentic corpora in-
stead of mixing them.

This new corpus is used to train the first model
by using backtranslated data as the source and the

monolingual as the target side of the model.
Hoang et al. (2018) showed that backtranslation

can be iterated and with the second round of back-
translation, we improve the performance of both
models. However, the third round of backtransla-
tion does yield better results.

The performance of the backtranslation model
is essential. Especially in the low-resource sce-
nario, the baseline models trained only on the au-
thentic parallel data have a poor score (2.0 BLEU
for English→Gujarati) generate very low quality
backtranslated data. We have improved the base-
line with the transfer learning to improve perfor-
mance and generate the synthetic data of better
quality.

2.3 Training procedure
We are training two models in parallel, one for
each translation direction. Our training procedure
is as follows. We train four parent models on
the high-resource language pair until convergence:
two models, one for each direction, for both direc-
tions. We stop training the models if there was
no improvement bigger than 0.1 BLEU in the last
20% of the training time.

At this point, we run a hyperparameter search
on the Gujarati→English and update the parame-
ters for all following steps of all language pairs.

Afterward, we apply transfer learning on the au-
thentic dataset of the corresponding low-resource
language pair. We preserve the English side, thus
Czech→English is a parent to Gujarati→English

http://statmt.org/wmt19/


236

Corpora Lang. Sent. Words
News crawl 2018 EN 15.4M 344.3M
Common Crawl KK 12.5M 189.2M

News commentary KK 13.0k 218.7k
News crawl Kk 772.9k 10.3M

Common Crawl GU 3.7M 67.3M
Newscrawl GU 244.9k 3.3M

Emille GU 273.2k 11.4M

Table 2: Statistics of all monolingual data used for the
backtranslation. It shows the number of sentences in
each corpus and the number of words segmented on
space. We mixed together all corpora for each language
separately.

and English→Czech to English→Gujarati, like-
wise for the Russian-Kazakh.

After transfer learning, we select one of the
translation directions to translate monolingual
data. As a starting system for the backtranslation
process, we have selected the English→Gujarati
and Kazakh→English. This decision is moti-
vated by choosing the better performing model
in Kazakh-English language pair, and since the
Gujarati-English have a similar score for both di-
rections, we decided to select a model with En-
glish target side in contrast to Kazakh-English.

Following the backtranslation, we create syn-
thetic data by mixing them with authentic parallel
data and using to improve the performance of the
second system. We continue repeating this pro-
cess: Use the better system to backtranslate the
data, and use this data in order to build an even
better system in reverse direction.

We make two rounds of backtranslation for both
directions on Gujarati-English and only one round
of backtranslation on Kazakh-English due to the
time consumption of the NMT translation process.

At last, we take the model with the highest
BLEU score on the devset and average it with
seven previous checkpoints to create final model.

3 Datasets and Model

In this section, we describe the datasets used to
train our final models. All our models were trained
only on the data allowed for the WMT 2019 News
shared task. Hence our submission is constrained.

All used training data are presented in Table 1.
We used all available parallel corpora allowed and
accessible by WMT 2019 except for the Czech-
English language pair, where we used only the

CzEng 1.7. We have not clean any of the par-
allel corpora except deduplication and removing
pairs with the same source and target translations
in Wiki Titles dataset.

We used official WMT testsets from previ-
ous years as a development set. The year 2013
for Czech-English and Russian-English. For the
Gujarati-English, we used the official 2019 devel-
opment set. Lastly, for the Kazakh-English, the
organizers do not provide any development set.
Therefore we separated the first 2000 sentence
pairs from the News Commentary training set and
used as our development set.

The monolingual data used for the backtransla-
tion are shown in Table 2. We use all available
monolingual data for Gujarati and Kazakh. For
the English, we did not use all available English
monolingual data due to the backtranslation pro-
cess being time-consuming, therefore we use only
the 2018 News Crawl.

The available monolingual corpora are usually
of high quality. However, we noticed that the
Common Crawl contains many sentences in a dif-
ferent language and also long paragraphs, that are
not useful for sentence level translation.

Therefore, we used language identification tool
by Lui and Baldwin (2012) on the Common Crawl
corpus and dropped all sentences automatically
annotated as a different language than Gujarati or
Kazakh respectively. Followed by splitting the re-
maining sentences that are longer than 100 words
on all full stops, which led to an increase of sen-
tences.

3.1 Model

The Transformer model seems superior to other
NMT approaches as documented by several lan-
guage pairs in the manual evaluation of WMT18
(Bojar et al., 2018).

We are using version 1.11 of sequence-to-
sequence implementation of Transformer called
tensor2tensor1. We are using the Transformer
“big single GPU” configuration as described in
(Vaswani et al., 2017), model which translates
through an encoder-decoder with each layer in-
volving an attention network followed by a feed-
forward network. The architecture is much faster
than other NMT due to the absence of recurrent
layers.

1https://github.com/tensorflow/
tensor2tensor

https://github.com/tensorflow/tensor2tensor
https://github.com/tensorflow/tensor2tensor


237

0

5

10

15

20

25

2000 2250 2500 2750 3000

B
L

E
U

Steps (in thousands)

En-Gu Synth 1
En-Gu Synth 2

En-Gu Transfer Learning
Gu-En Synth 2
Gu-En Synth 1

Gu-En Transfer Learning

Figure 1: Learning curves for both directions of
Gujarati-English models. The BLEU score is uncased
and computed on the development set.

Popel and Bojar (2018) documented best prac-
tices to improve the performance of the model.
Based on their observation, we are using as an
optimizer Adafactor with inverse square root de-
cay. Based on our previous experiments (Kocmi
et al., 2018) we set the maximum number of sub-
words in a sentence to 100, which drops less than
0.1 percent of training sentences. However, it al-
lows increasing the maximum size of the batch to
4500 for our GPU. The experiments are trained on
a single GPU NVidia GeForce 1080 Ti.

4 Experiments

In this section, we describe our experiments start-
ing with hyperparameter search, our training pro-
cedure, and supporting experiments.

All reported results are calculated over the test-
set of WMT 2019 and evaluated with case sensi-
tive SacreBLEU (Post, 2018)2 if not specified oth-
erwise.

4.1 Hyperparameter search
Before the first step of transfer learning,
we have done a hyperparameter search on
Gujarati→English over the set of parameters that
are not fixed from the parent (like dimensions of

2The SacreBLEU signature is BLEU + case.mixed +
numrefs.1 + smooth.exp + tok.13a + version.1.2.12.

matrices or structure of layers). We examined
the following hyperparameters: learning rate,
dropout, layer prepostprocess dropout, label
smoothing, and attention dropout.

The performance before hyperparameter search
was 9.8 BLEU3 for Gujarati→English, this score
was improved to 11.0 BLEU. Based on the hy-
perparameter search we set the layer prepostpro-
cess dropout and label smoothing both to 0.2 in
the setup of Transformer-big.

These improvements show that transfer learning
is not strictly associated with parent setup and that
some parameters are possible to change. Although
it must be noted, that we experimented only with
a small subset of all hyperparameters and it is pos-
sible that other parameters could also be changed
without damaging the parent model.

In this paper, we are using these parameters
for all experiments (except for the parent mod-
els). Although applying hyperparameter search
on each model separately or even between before
each dataset switch is an interesting question, it is
over the scope of this paper.

4.2 Problems with backtranslation

The synthetic data have a quality similar with the
model by which they were produced. Since the
low-resource scenario has an overall low quality,
we observed, that the synthetic data contain many
relics:

• Repeated sequence of words: The State De-
partment has made no reference in state-
ments, statements, statements, statements ...
• Sentences in Czech or Russian, most proba-

bly due to the parent model.
• Source sentences generated untranslated.

To avoid these problems, we cleaned all syn-
thetic data in the following way. We had dropped
all sentences, that contained any repetitive se-
quence of words. Then we checked the sentences
by language identification tool (Lui and Baldwin,
2012) and dropped all sentences automatically an-
notated as a wrong language. The second step also
filtered out some remaining gibberish translations.

We have not used beam search during back-
translation of monolingual data in order to speed
up the translation process roughly 20 times com-
pared to the beam search of 8.

3This score is computed over devset with averaging of 8
latest models distanced one and half hour of training time.



238

Training dataset EN→GU GU→EN EN→KK KK→EN
Authentic (baseline) 2.0 1.8 0.5 4.2

Parent dataset 0.7 0.1 0.7 0.6
Authentic (transfer learning) 1© 9.1 9.2 6.2 1© 14.4
Synth generated by model 1© - 2© 14.2 2© 8.3 -
Synth generated by model 2© 3© 13.4 - - 17.3
Synth generated by model 3© - 4© 16.2 - -
Synth generated by model 4© 13.7 - - -

Averaging + beam 8 14.3 17.4 8.7 18.5

Table 3: Testset BLEU scores of our setup. Except for the baseline, each column shows improvements obtained
after fine-tuning a single model on different datasets beginning with the score on a trained parent model.

4.3 Final models

Following the training procedure describe in Sec-
tion 2.3, we trained the parent models for two mil-
lion steps. One exception from the described ap-
proach is that we used a subset of 2M monolingual
English data for the first round of backtranslation
by the English→Gujarati model to cut down on
the total consumed time.

Figure 1 shows the progress of training
Gujarati-English models in both directions. The
learning curves start at two millionth step as a vi-
sualization of the parent model training. We can
notice that after each change of parallel data, there
is a substantial increment of the performance. The
learning curve is visualized on the development
data, exact numbers for the testsets are in Table 3.

The baseline model in Table 3 is trained on the
authentic data only, and it seems that the amount
of parallel data is not sufficient to train the NMT
model for the investigated language pairs. The rest
of the rows shows incremental improvements of
the models based on an undertaken step. The last
step of model averaging takes the best perform-
ing model and averages it with the previous seven
checkpoints that are distanced on average one and
half hour of training time between each other.

We see that the transfer learning can be com-
bined with iterated backtranslation on a low-
resource language to obtain an improvement
of 12.3 BLEU compared to the baseline in
Gujarati→English and 15.6 in English→Gujarati.

For the final submission, we have se-
lected models at following steps: step
2.99M for English→Gujarati, step 3.03M
for Gujarati→English, step 2.48M for
English→Kazakh and step 2.47M for
Kazakh→English

4.4 Ratio of parallel data

Poncelas et al. (2018) showed that the balance be-
tween the synthetic and authentic data matters, and
there should always be a part of authentic parallel
data. We started our experiments with this intu-
ition. However, the low-resource scenario compli-
cates the setup since the amount of authentic data
is several times smaller than synthetic. In order to
balance the authentic and synthetic parallel data,
we duplicated the authentic data several times.

We notice that the performance did not change
from the setup that is using only synthetic
data. Thus we prepare an experiment, where
we do a second round of backtranslation on
Gujarati→English with a various ratio of authen-
tic and synthetic parallel data. For this experiment,
we duplicated the full authentic parallel corpora of
173k sentences into a subsampled synthetic paral-
lel corpus used in the second round of backtransla-
tion. We have randomly selected 3.6M sentences
from the synthetic corpora. The number of sen-
tences is equal to 20x size of synthetic corpora.
Therefore, we can present the ratio between au-
thentic and synthetic corpora in percentage. The
ratio in the legend of Figure 2 represent the ac-
tual ratio in the final corpus and not how much
times the corpus has been duplicated. The syn-
thetic is never duplicated, we only duplicate the
authentic corpora. For example, the ratio “au-
thentic:synthetic 1:2” means that the authentic has
been multiplied ten times because the synthetic is
twenty times bigger than the authentic corpora.

In Figure 2, we can see the difference between
the amount of synthetic and authentic data. It
seems that using only synthetic data generates the
best performance, and whenever we increase the
authentic part, the performance slowly decreases,
contrary to the Poncelas et al. (2018). It could be



239

18

20

22

24

26

28

2350 2450 2550 2650 2750

B
L

E
U

Steps (in thousands)

Synth only
Authentic:Synth 1:2
Authentic:Synth 1:1
Authentic:Synth 4:1
Authentic:Synth 8:1

Authentic only

Figure 2: Comparison of different ratio of authentic
and synthetic data.

due to the noise in the data, which implies that syn-
thetic data are cleaner and more suitable for train-
ing the model.

4.5 Synthetic from Scratch

In the previous section, we have shown that during
the iterative backtranslation of low-resource lan-
guages, the authentic data hurt the performance.
In this section, we use the various ratios of train-
ing data and train the model from scratch with-
out transfer learning or other backtranslation. No-
tably, all the parameters, as well as the wordpiece
vocabulary, are the same.

Table 4 present the result of using synthetic data
directly without any adaptation. It shows that hav-
ing more authentic data hurt the low-resource lan-
guages. However, the most surprising fact is that
training from scratch leads to significantly better
model than the model trained by transfer learn-
ing and two rounds of the backtranslation by 0.7
(cased) BLEU. Unfortunately, we proposed this
experiment after the submission. Therefore our fi-
nal system has worse performance.

We believe it could be a result of unconscious
overfitting to the development set because the per-
formance on the development set is higher for our
final model 26.9 BLEU compared to the perfor-
mance of 25.8 BLEU for the synthetic only train-

Training dataset cased uncased
Authentic (baseline) 1.8 2.2

Synthetic only 16.9 18.7
Auth:Synth 1:1 16.8 18.4
Auth:Synth 2:1 16.3 17.8
Auth:Synth 4:1 15.2 16.8

Final model 16.2 17.9

Table 4: BLEU scores for training English→Gujarati
from scratch on synthetic data from the second round
of backtranslation. Neither of models uses the aver-
aging or beam search. Thus the final model is our
submitted model before averaging and beam search
(the model 3©). The scores are equal to those from
http://matrix.statmt.org.

ing. It could have been because we used devel-
opment set three times during the training of the
final model: first to select the best model from the
transfer learning, then when selecting the best per-
forming model in the first round of backtransla-
tion and then third times during the second round
of backtranslation. On the other hand, training on
synthetic data from scratch used the development
set only once for selection of the best performing
model to evaluate.

Another possible explanation is that the final
model is already overspecialized on the data from
the first round of backtranslation, that it is not able
to adapt to the improved second synthetic data.

5 Conclusion

We participated in four translation directions on
a low-resource language pairs in the WMT 2019
News translation Shared Task. We combined
transfer learning with the iterated backtranslation
and obtained significant improvements.

We showed that mixing authentic data and back-
translated data in a low-resource scenario does not
affect the performance of the model: synthetic
data is far more critical. This is a different re-
sult from what Poncelas et al. (2018) observed on
higher-resource language pairs.

Lastly, in some scenarios, it is better to train the
model on backtranslated data from scratch instead
of fine-tuning the previous model.

In the future work, we want to investigate, why
the training from scratch on backtranslated has led
to better results. One of the reviewers suggested
keep mixing the Czech→English corpus even dur-
ing later stages of training as an additional source
of parallel data, which we would like to compare.

http://matrix.statmt.org


240

Acknowledgments

This study was supported in parts by the grants
SVV 260 453 of the Charles University, 18-
24210S of the Czech Science Foundation and
825303 (Bergamot) of the European Union. This
work has been using language resources and tools
stored and distributed by the LINDAT/CLARIN
project of the Ministry of Education, Youth and
Sports of the Czech Republic (LM2015071).

References
Ondřej Bojar, Christian Federmann, Mark Fishel,

Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, and Christof Monz. 2018. Find-
ings of the 2018 conference on machine translation
(wmt18). In Proceedings of the Third Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers, pages 272–307, Belgium, Brussels. Associa-
tion for Computational Linguistics.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu,
Renqian Luo, Arul Menezes, Tao Qin, Frank Seide,
Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce
Xia, Dongdong Zhang, Zhirui Zhang, and Ming
Zhou. 2018. Achieving human parity on auto-
matic chinese to english news translation. CoRR,
abs/1803.05567.

Vu Cong Duy Hoang, Philipp Koehn, Gholamreza
Haffari, and Trevor Cohn. 2018. Iterative back-
translation for neural machine translation. In Pro-
ceedings of the 2nd Workshop on Neural Machine
Translation and Generation, pages 18–24.

Tom Kocmi and Ondřej Bojar. 2018. Trivial Transfer
Learning for Low-Resource Neural Machine Trans-
lation. In Proceedings of the 3rd Conference on Ma-
chine Translation (WMT): Research Papers, Brus-
sels, Belgium.

Tom Kocmi, Roman Sudarikov, and Ondřej Bojar.
2018. Cuni submissions in wmt18. In Proceed-
ings of the Third Conference on Machine Transla-
tion, Volume 2: Shared Task Papers, pages 435–441,
Belgium, Brussels. Association for Computational
Linguistics.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39, Vancouver. Association
for Computational Linguistics.

Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 25–30, Jeju Island, Korea. Association for
Computational Linguistics.

Alberto Poncelas, Dimitar Shterionov, Andy Way,
Gideon Maillette de Buy Wenniger, and Pey-
man Passban. 2018. Investigating backtransla-
tion in neural machine translation. arXiv preprint
arXiv:1804.06189.

Martin Popel. 2018. Machine translation using syntac-
tic analysis. Univerzita Karlova.

Martin Popel and Ondej Bojar. 2018. Training Tips
for the Transformer Model. The Prague Bulletin of
Mathematical Linguistics, 110(1):43–70.

Matt Post. 2018. A Call for Clarity in Reporting BLEU
Scores. arXiv preprint arXiv:1804.08771.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 6000–6010. Curran As-
sociates, Inc.

http://www.aclweb.org/anthology/W18-6401
http://www.aclweb.org/anthology/W18-6401
http://www.aclweb.org/anthology/W18-6401
http://arxiv.org/abs/1803.05567
http://arxiv.org/abs/1803.05567
http://www.aclweb.org/anthology/W18-6416
http://www.aclweb.org/anthology/W17-3204
http://www.aclweb.org/anthology/W17-3204
http://www.aclweb.org/anthology/P12-3005
http://www.aclweb.org/anthology/P12-3005
https://content.sciendo.com/view/journals/pralin/110/1/article-p43.xml
https://content.sciendo.com/view/journals/pralin/110/1/article-p43.xml
https://arxiv.org/abs/1804.08771
https://arxiv.org/abs/1804.08771
http://www.aclweb.org/anthology/P16-1009
http://www.aclweb.org/anthology/P16-1009
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf

