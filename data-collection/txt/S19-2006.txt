



















































ANA at SemEval-2019 Task 3: Contextual Emotion detection in Conversations through hierarchical LSTMs and BERT


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 49–53
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

49

ANA at SemEval-2019 Task 3: Contextual Emotion detection in
Conversations through hierarchical LSTMs and BERT

Chenyang Huang, Amine Trabelsi, Osmar R. Zaı̈ane
Department of Computing Science, University of Alberta
{chuang8,atrabels,zaiane}@ualberta.ca

Abstract
This paper describes the system submitted by
ANA Team for the SemEval-2019 Task 3:
EmoContext. We propose a novel Hierarchi-
cal LSTMs for Contextual Emotion Detection
(HRLCE) model. It classifies the emotion
of an utterance given its conversational con-
text. The results show that, in this task, our
HRCLE outperforms the most recent state-of-
the-art text classification framework: BERT.
We combine the results generated by BERT
and HRCLE to achieve an overall score of
0.7709 which ranked 5th on the final leader
board of the competition among 165 Teams.

1 Introduction

Social media has been a fertile environment for the
expression of opinion and emotions via text. The
manifestation of this expression differs from tradi-
tional or conventional opinion communication in
text (e.g., essays). It is usually short (e.g. Twit-
ter), containing new forms of constructs, including
emojis, hashtags or slang words, etc. This con-
stitutes a new challenge for the NLP community.
Most of the studies in the literature focused on the
detection of sentiments (i.e. positive, negative or
neutral) (Mohammad and Turney, 2013).

Recently, emotion classification from social
media text started receiving more attention (Yad-
dolahi et al., 2017; Mohammad et al., 2018). Emo-
tions have been extensively studied in psychology
(Ekman, 1992; Plutchik, 2001). Their automatic
detection may reveal important information in so-
cial online environments, like online customer ser-
vice. In such cases, a user is conversing with an
automatic chatbot. Empowering the chatbot with
the ability to detect the user’s emotion is a step
forward towards the construction of an emotion-
ally intelligence agent. Giving the detected emo-
tion, an emotionally intelligent agent would gener-
ate an empathetic response. Although its potential

convenience, detecting emotion in textual conver-
sation has seen limited attention so far. One of the
main challenges is that one users utterance may
be insufficient to recognize the emotion (Huang
et al., 2018). The need to consider the context of
the conversion is essential in this case, even for
human, specifically given the lack of voice mod-
ulation and facial expressions. The usage of figu-
rative language, like sarcasm, and the class size’s
imbalance adds up to this problematic (Chatterjee
et al., 2019a).

Context 
LSTM

Context 
LSTM

Context 
LSTM

Utterance 
Encoder

Multi-head self-attention

Category
Classification

 

 

Utterance
LSTM

GloVe ELMo

DeepMoji

Utterance 
Encoder

Utterance 
Encoder

Utterance 
Encoder 

 

 

 Figure 1: An illustration of the HRLCE model

In this paper, we describe our model, which was
proposed for the SemEval 2019-Task 3 competi-
tion: Contextual Emotion Detection in Text (Emo-
Context). The competition consists in classify-
ing the emotion of an utterance given its conver-
sational context. More formally, given a textual
user utterance along with 2 turns of context in a
conversation, the task is to classify the emotion
of user utterance as Happy, Sad, Angry or Others
(Chatterjee et al., 2019b). The conversations are
extracted from Twitter.

We propose an ensemble approach composed
of two deep learning models, the Hierarchi-
cal LSTMs for Contextual Emotion Detection
(HRLCE) model and the BERT model (Devlin
et al., 2018). The BERT is a pre-trained language



50

model that has shown great success in many NLP
classification tasks. Our main contribution con-
sists in devising the HRLCE model.

Figure 1 illustrates the main components of the
HRLCE model. We examine a transfer learning
approach with several pre-trained models in or-
der to encode each user utterance semantically
and emotionally at the word-level. The pro-
posed model uses Hierarchical LSTMs (Sordoni
et al., 2015) followed by a multi-head self atten-
tion mechanism (Vaswani et al., 2017) for a con-
textual encoding at the utterances level.

The model evaluation on the competition’s test
set resulted in a 0.7709 harmonic mean of the
macro-F1 scores across the categories Happy, An-
gry, and Sad. This result ranked 5th in the final
leader board of the competition among 142 teams
with a score above the organizers’ baseline.

2 Overview

2.1 Embeddings for semantics and emotion

We use different kinds of embeddings that have
been deemed effective in the literature in capturing
not only the syntactic or semantic information of
the words, but also their emotional content. We
breifly describe them in this section.

GloVe, (Pennington et al., 2014) is a widely
used pre-trained vector representation that cap-
tures fine-grained syntactic and semantic regulari-
ties. It has shown great success in word similarity
tasks and Named Entity Recognition benchmarks.

ELMo, or Embeddings from Language Models,
(Peters et al., 2018) are deep contextualized word
representations. These representations enclose a
polysemy encoding, i.e., they capture the varia-
tion in the meaning of a word depending on its
context. The representations are learned functions
of the input, pre-trained with deep bi-directional
LSTM model. It has been shown to work well in
practice on multiple language understanding tasks
like question answering, entailment and sentiment
analysis. In this work, our objective is to detect
emotion accurately giving the context. Hence, em-
ploying such contextual embedding can be crucial.

DeepMoji (Felbo et al., 2017) is a pre-trained
model containing rich representations of emo-
tional content. It has been pre-trained on the task
of predicting the emoji contained in the text using
Bi-directional LSTM layers combined with an at-
tention layer. A distant supervision approach was
deployed to collect a massive (1.2 billion Tweets)

dataset with diverse set of noisy emoji labels on
which DeepMoji is pre-trained. This led to state-
of-the art performance when fine-tuning Deep-
Moji on a range of target tasks related to senti-
ment, emotion and sarcasm.

2.2 Hierarchical RNN for context

One of the building component of our proposed
model (see Figure 1) is the Hierarchical or Con-
text recurrent encoder-decoder (HRED) (Sordoni
et al., 2015). HRED architecture is used for en-
coding dialogue context in the task of multi-turn
dialogue generation task (Serban et al., 2016). It
has been proven to be effective in capturing the
context information of dialogue exchanges. It
contains two types of recurrent neural net (RNN)
units: encoder RNN which maps each utterance
to an utterance vector; context RNN which fur-
ther processes the utterance vectors. HRED is ex-
pected to produce a better representation of the
context in dialogues because the context RNN al-
lows the model to represent the information ex-
changes between the two speakers.

2.3 BERT

BERT, the Bidirectional Encoder Representations
for Transformers, (Devlin et al., 2018) is a pre-
trained model producing context representations
that can be very convenient and effective. BERT
representations can be fine-tuned to many down-
stream NLP tasks by adding just one additional
output layer for the target task, eliminating the
need for engineering a specific architecture for a
task. Using this setting, it has advanced the state-
of-the-art performances in 11 NLP tasks. Using
BERT in this work has slightly improved the final
result, when we combine it with our HRLCE in an
ensemble setting.

2.4 Importance Weighting

Importance Weighting (Sugiyama and Kawanabe,
2012) is used when label distributions between the
training and test sets are generally different, which
is the case of the competition datasets (Table 2). It
corresponds to weighting the samples according to
their importance when calculating the loss.

A supervised deep learning model can be re-
garded as a parameterized function f(x;θ). The
backpropagation learning algorithm through a dif-
ferentiable loss is a method of empirical risk mini-
mization (ERM). Denote (xtri , y

tr
i ), i ∈ [1 . . . ntr]



51

are pairs of training samples, testing samples are
(xte, yte), i ∈ [1 . . . nte].

The ratio P (x)te/P (x)tr is referred as the im-
portance of a sample x. When the label distribu-
tion of training data and testing data are different:
P (xte) 6= P (xtr), the training of the model fθ
is then called under covariate shift. In such situa-
tion, the parameter θ̂ should be estimated through
importance-weighted ERM:

argmin
θ

[ 1
ntr

ntr∑
i=1

P (xte)

P (xtr)
loss(ytri , f(x

tr
i ;θ)

]
.

(1)

3 Models

Denote the input x = [u1, u2, u3], where ui is the
ith penultimate utterance in the dialogue. y is the
emotion expressed in u3 while giving u1 and u2 as
context.

To justify the effectiveness of the modules in
HRLCE, we propose two baseline models: SA-
LSTM (SL) and SA-LSTM-DeepMoji (SLD). The
SL model is part of the SLD model, while the
later one composes the utterance encoder of our
HRLCE. Therefore, we illustrate the models con-
secutively in Sections 3.1, 3.2, and 3.3.

3.1 SA-LSTM (SL)
Let x be the concatenation of u1 ,u2, and u3.
Hereby, x = [x1, x2, · · · , xn], where xi is the
ith word in the combined sequence. Denote the
pre-trained GloVe model as G. As GloVe model
can be directly used by looking up the word xi,
we can use G(xi) to represent its output. On the
contrary, ELMo embedding is not just dependent
on the word xi, but on all the words of the in-
put sequence. When taking as input the entire
sequence x, n vectors can be extracted from the
pre-trained ElMo model. Denote the vectors as
E = [E1, E2, · · · , En]. Ei contains both contex-
tual and semantic information of word xi. We use
a two-layer bidirectional LSTM as the encoder of
the sequence x. For simplicity, we denote it as
LSTM e. In order to better represent the informa-
tion of xi, we use the concatenation of G(xi) and
Ei as the feature embedding of xi. Therefore, we
have the following recurrent progress:

het = LSTM
e([G(xt);Et], h

e
t−1). (2)

het is the hidden state of encoder LSTM at time
step t, and he0 = 0. Let h

e
x = [h

e
t , h

e
t , · · · , het ] be

F1 Happy Angry Sad Harm. Mean

SL
Dev 0.6430 0.7530 0.7180 0.7016
Test 0.6400 0.7190 0.7300 0.6939

SLD
Dev 0.6470 0.7610 0.7360 0.7112
Test 0.6350 0.7180 0.7360 0.6934

HRLCE
Dev 0.7460 0.7590 0.8100 0.7706
Test 0.7220 0.766 0.8180 0.7666

BERT
Dev 0.7138 0.7736 0.8106 0.7638
Test 0.7151 0.7654 0.8157 0.7631

Table 1: Macro-F1 scores and its harmonic means of
the four models

the n hidden states of encoder given the input x.
Self-attention mechanism has been proven to be
effective in helping RNN dealing with dependency
problems (Lin et al., 2017). We use the multi-head
version of the self-attention (Vaswani et al., 2017)
and set the number of channels for each head as 1.
Denote the self-attention module as SA, it takes as
input all the hidden states of the LSTM and sum-
marizes them into a single vector. This process
is represented as hsax = SA(h

e
x). To predict the

model, we append a fully connected (FC) layer to
project hsax on to the space of emotions. Denote
the FC layer as output. Let oSLx = output(h

sa
x ),

then the estimated label ofx is the argmaxi(o
SL
x ),

where i is ith value in the vector oSLx .

3.2 SA-LSTM-DeepMoji (SLD)
SLD is the combination of SA and DeepMoji. An
SLD model without the output layer is in fact the
utterance encoder of the proposed HRLCE, which
is illustrated in the right side of Figure 1. De-
note the DeepMoji model as D, when taking as
input x, the output is represented as hdx = D(x).
We concatenate hdx and h

sa
x as the feature rep-

resentation of sequence of x. Same as SL, an
FC layer is added in order to predict the label:
oSLDx = output([h

sa
x ;h

d
x]).

3.3 HRLCE
Unlike SL and SLD, the input of HRLCE is not
the concatenation of u1, u2, and u3.

Following the annotation in Section 3.1 and 3.2,
an utterance ui is firstly encoded as hsaui and h

d
ui .

We use another two layer bidirectional LSTM as
the context RNN, denoted as LSTM c. Its hidden
states are iterated through:

hct = LSTM
c([hsaut ;h

d
ut ], h

c
t−1), (3)

where hc0 = 0. The three hidden states h
c =

[hc1, h
c
2, h

c
3], are fed as the input to a self-attention



52

layer. The resulting vector SA(hc) is also pro-
jected to the label space by an FC layer.

3.4 BERT

BERT (Section 2.3) can take as input either a sin-
gle sentence or a pair of sentences. A “sentence”
here corresponds to any arbitrary span of contigu-
ous words. In this work, in order to fine-tune
BERT, we concatenate utterances u1 and u2 to
constitute the first sentence of the pair. u3 is the
second sentence of the pair. The reason behind
such setting is that we assume that the target emo-
tion y is directly related to u3, while u1 and u2
are providing additional context information. This
forces the model to consider u3 differently.

4 Experiment

4.1 Data preprocessing

From the training data we notice that emojis are
playing an important role in expressing emotions.
We first use ekphrasis package (Baziotis et al.,
2017) to clean up the utterances. ekphrasis cor-
rects misspellings, handles textual emotions (e.g.
‘:)))’), and normalizes tokens (hashtags, numbers,
user mentions etc.). In order to keep the semantic
meanings of the emojis, we use the emojis pack-
age1 to first convert them into their textual aliases
and then replace the “:” and “ ” with spaces.

4.2 Environment and hyper-parameters

We use PyTorch 1.0 for the deep learning frame-
work, and our code in Python 3.6 can be accessed
in GitHub2. For fair comparisons, we use the same
parameter settings for the common modules that
are shared by the SL, SLD, and HRLCE. The di-
mension of encoder LSTM is set to 1500 per di-
rection; the dimension of context LSTM is set to
800 per direction. We use Adam optimizer with
initial learning rate as 5e-4 and a decay ratio of 0.2
after each epoch. The parameters of DeepMoji are
set to trainable. We use BERT-Large pre-trained
model which contains 24 layers.

happy angry sad others size
Train 14.07% 18.26% 18.11% 49.56% 30160
Dev 5.15% 5.44% 4.54% 84.86% 2755
Test 4.28% 5.57% 4.45% 85.70% 5509

Table 2: Label distribution of train, dev, and test set

1https://pypi.org/project/emoji/
2https://github.com/chenyangh/SemEval2019Task3

According to the description in (CodaLab,
2019), the label distribution for dev and test sets
are roughly 4% for each of the emotions. How-
ever, from the dev set (Table 2) we know that the
proportions of each of the emotion categories are
better described as %5 each, thereby we use %5
as the empirical estimation of distribution P (xte).
We did not use the exact proportion of dev set as
the estimation to prevent the overfitting towards
dev set. The sample distribution of the train set
is used as P (xtr). We use Cross Entropy loss for
all the aforementioned models, and the loss of the
training samples are weighted according to Eq. 1.

4.3 Results and analysis
We run 9-fold cross validation on the training set.
Each iteration, 1 fold is used to prevent the models
from overfitting while the remaining folds are used
for training. Therefore, every model is trained 9
times to ensure stability. The inferences over dev
and test sets are performed on each iteration. We
use the majority voting strategy to merge the re-
sults from the 9 iterations. The results are shown
in Table 1. It shows that the proposed HRLCE
model performs the best. The performance of SLD
and SL are very close to each other, on the dev set,
SLD performs better than SL but they have almost
the same overall scores on the test set. The Macro-
F1 scores of each emotion category are very differ-
ent from each other: the classification accuracy for
emotion Sad is the highest in most of the cases,
while the emotion Happy is the least accurately
classified by all the models. We also noticed that
the performance on the dev set is generally slightly
better than that on the test set.

5 Conclusions

Considering the competitive results generated by
BERT, we combined BERT and our proposed
model in an ensemble and obtained 0.7709 on the
final test leaderboard. From a confusion matrix
of our final submission, we notice that there are
barely miss-classifications among the three cate-
gories (Angry, Sad, and Happy). For example, the
emotion Sad is rarely miss-classified as “Happy”
or “Angry”. Most of the errors correspond to clas-
sifying the emotional utterances in the Others cat-
egory. We think, as future improvement, the mod-
els need to first focus on the binary classifica-
tion “Others” versus “Not-Others”, then the “Not-
Others” are classified in their respective emotion.



53

References
Christos Baziotis, Nikos Pelekis, and Christos Doulk-

eridis. 2017. Datastories at semeval-2017 task
4: Deep lstm with attention for message-level and
topic-based sentiment analysis. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 747–754, Vancouver,
Canada. Association for Computational Linguistics.

Ankush Chatterjee, Umang Gupta, Manoj Kumar
Chinnakotla, Radhakrishnan Srikanth, Michel Gal-
ley, and Puneet Agrawal. 2019a. Understanding
emotions in text using deep learning and big data.
Computers in Human Behavior, 93:309–317.

Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019b. Semeval-2019
task 3: Emocontext: Contextual emotion detection
in text. In Proceedings of The 13th International
Workshop on Semantic Evaluation (SemEval-2019),
Minneapolis, Minnesota.

CodaLab. 2019. Semeval19 task 3: Emocon-
text. https://competitions.codalab.
org/competitions/19790#learn_the_
details-data-set-format.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Paul Ekman. 1992. An argument for basic emotions.
Cognition & emotion, 6(3-4):169–200.

Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad
Rahwan, and Sune Lehmann. 2017. Using millions
of emoji occurrences to learn any-domain represen-
tations for detecting sentiment, emotion and sar-
casm. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).

Chenyang Huang, Osmar R. Zaiane, Amine Trabelsi,
and Nouha Dziri. 2018. Automatic dialogue genera-
tion with expressed emotions. In 16th Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL),
New Orleans, USA.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Saif Mohammad, Felipe Bravo-Marquez, Moham-
mad Salameh, and Svetlana Kiritchenko. 2018.
Semeval-2018 task 1: Affect in tweets. In Proceed-
ings of The 12th International Workshop on Seman-
tic Evaluation, pages 1–17. Association for Compu-
tational Linguistics.

Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word–emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Robert Plutchik. 2001. The nature of emotions: Hu-
man emotions have deep evolutionary roots, a fact
that may explain their complexity and provide tools
for clinical practice. American Scientist, 89(4):344–
350.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Thirtieth AAAI
Conference on Artificial Intelligence.

Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,
Christina Lioma, Jakob Grue Simonsen, and Jian-
Yun Nie. 2015. A hierarchical recurrent encoder-
decoder for generative context-aware query sugges-
tion. In Proceedings of the 24th ACM International
on Conference on Information and Knowledge Man-
agement, pages 553–562. ACM.

Masashi Sugiyama and Motoaki Kawanabe. 2012. Ma-
chine learning in non-stationary environments: In-
troduction to covariate shift adaptation. MIT press.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Ali Yaddolahi, Ameneh Gholipour Shahraki, and Os-
mar R. Zaiane. 2017. Current state of text sentiment
analysis from opinion to emotion mining. ACM
Computing Surveys, 50(2):25:1–25:33.

https://competitions.codalab.org/competitions/19790#learn_the_details-data-set-format
https://competitions.codalab.org/competitions/19790#learn_the_details-data-set-format
https://competitions.codalab.org/competitions/19790#learn_the_details-data-set-format
https://doi.org/10.18653/v1/S18-1001

