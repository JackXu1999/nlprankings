



















































Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1371–1377
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1371

Extracting Multiple-Relations in One-Pass
with Pre-Trained Transformers

Haoyu Wang∗† Ming Tan∗† Mo Yu∗‡ Shiyu Chang‡
Dakuo Wang‡ Kun Xu§ Xiaoxiao Guo‡ Saloni Potdar†

†IBM Watson ‡IBM Research §Tencent AI Lab

Abstract

The state-of-the-art solutions for extracting
multiple entity-relations from an input para-
graph always require a multiple-pass encoding
on the input. This paper proposes a new so-
lution that can complete the multiple entity-
relations extraction task with only one-pass
encoding on the input corpus, and achieve a
new state-of-the-art accuracy performance,
as demonstrated in the ACE 2005 benchmark.
Our solution is built on top of the pre-trained
self-attentive models (Transformer). Since our
method uses a single-pass to compute all rela-
tions at once, it scales to larger datasets easily;
which makes it more usable in real-world ap-
plications. 1

1 Introduction

Relation extraction (RE) aims to find the semantic
relation between a pair of entity mentions from an
input paragraph. A solution to this task is essential
for many downstream NLP applications such as
automatic knowledge-base completion (Surdeanu
et al., 2012; Riedel et al., 2013; Verga et al.,
2016), knowledge base question answering (Yih
et al., 2015; Xu et al., 2016; Yu et al., 2017), and
symbolic approaches for visual question answer-
ing (Mao et al., 2019; Hu et al., 2019), etc.

One particular type of the RE task is multiple-
relations extraction (MRE) that aims to recognize
relations of multiple pairs of entity mentions from
an input paragraph. Because in real-world appli-
cations, whose input paragraphs dominantly con-
tain multiple pairs of entities, an efficient and ef-
fective solution for MRE has more important and
more practical implications. However, nearly all
existing approaches for MRE tasks (Qu et al.,

∗ Equal contributions from the corresponding authors:
{wanghaoy,mingtan,yum}@us.ibm.com. Part of
work was done when Kun was at IBM.

1
https://github.com/helloeve/mre-in-one-pass.

×12

……

Entity-aware Self-attention+ Feed-forward

…   in south suburbs  of Bagh  #dad and Iraqi artillery fired …

Linear

PART-WHOLE(e1,e2) ART(e1,e2)

Pool Pool

Linear

Figure 1: Model Architecture. Different pairs of entities,
e.g., (Iraqi and artillery), (southern suburbs, Baghdad) are
predicted simultaneously.

2014; Gormley et al., 2015; Nguyen and Grish-
man, 2015) adopt some variations of the single-
relation extraction (SRE) approach, which treats
each pair of entity mentions as an independent in-
stance, and requires multiple passes of encoding
for the multiple pairs of entities. The drawback of
this approach is obvious – it is computationally ex-
pensive and this issue becomes more severe when
the input paragraph is large, making this solution
impossible to implement when the encoding step
involves deep models.

This work presents a solution that can resolve
the inefficient multiple-passes issue of existing so-
lutions for MRE by encoding the input only once,
which significantly increases the efficiency and
scalability. Specifically, the proposed solution is
built on top of the existing transformer-based, pre-
trained general-purposed language encoders. In
this paper we use Bidirectional Encoder Represen-
tations from Transformers (BERT) (Devlin et al.,
2018) as the transformer-based encoder, but this
solution is not limited to using BERT alone. The
two novel modifications to the original BERT ar-
chitecture are: (1) we introduce a structured pre-
diction layer for predicting multiple relations for
different entity pairs; and (2) we make the self-
attention layers aware of the positions of all en-

https://github.com/helloeve/mre-in-one-pass


1372

tities in the input paragraph. To the best of our
knowledge, this work is the first promising solu-
tion that can solve MRE tasks with such high ef-
ficiency (encoding the input in one-pass) and ef-
fectiveness (achieve a new state-of-the-art perfor-
mance), as proved on the ACE 2005 benchmark.

2 Background

MRE is an important task as it is an essential
prior step for many downstream tasks such as au-
tomatic knowledge-base completion and question-
answering. Popular MRE benchmarks include
ACE (Walker et al., 2006) and ERE (Linguistic
Data Consortium, 2013). In MRE, given as a text
paragraph x = {x1, . . . , xN} and M mentions
e = {e1, . . . , eM} as input, the goal is to predict
the relation rij for each mention pair (ei, ej) ei-
ther belongs to one class of a list of pre-defined
relationsR or falls into a special class NA indicat-
ing no relation. This paper uses “entity mention”,
“mention” and “entity” interchangeably.

Existing MRE approaches are based on ei-
ther feature and model architecture selection tech-
niques (Xu et al., 2015; Gormley et al., 2015;
Nguyen and Grishman, 2015; F. Petroni and
Gemulla, 2015; Sorokin and Gurevych, 2017;
Song et al., 2018b), or domain adaptations ap-
proaches (Fu et al., 2017; Shi et al., 2018). But
these approaches require multiple passes of encod-
ing over the paragraph, as they treat a MRE task as
multiple passes of a SRE task.

3 Proposed Approach

This section describes the proposed one-pass en-
coding MRE solution. The solution is built upon
BERT with a structured prediction layer to en-
able BERT to predict multiple relations with one-
pass encoding, and an entity-aware self-attention
mechanism to infuse the relational information
with regard to multiple entities at each layer of
hidden states. The framework is illustrated in Fig-
ure 1. It is worth mentioning that our solution
can easily use other transformer-based encoders
besides BERT, e.g. (Radford et al., 2018).

3.1 Structured Prediction with BERT for
MRE

The BERT model has been successfully applied to
various NLP tasks. However, the final prediction
layers used in the original model is not applicable
to MRE tasks. The MRE task essentially requires

to perform edge predictions over a graph with en-
tities as nodes. Inspired by (Dozat and Manning,
2018; Ahmad et al., 2018), we propose that we
can first encode the input paragraph using BERT.
Thus, the representation for a pair of entity men-
tions (ei, ej) can be denoted as oi and oj respec-
tively. In the case of a mention ei consist of mul-
tiple hidden states (due to the byte pair encoding),
oi is aggregated via average-pooling over the hid-
den states of the corresponding tokens in the last
BERT layer. We then concatenate oi and oj de-
noted as [oi : oj ], and pass it to a linear classifier2

to predict the relation

P (rij |x, ei, ej) = softmax(WL[oi : oj ] + b), (1)

where WL ∈ R2dz×l. dz is the dimension of
BERT embedding at each token position, and l is
the number of relation labels.

3.2 Entity-Aware Self-Attention based on
Relative Distance

This section describes how we encode multiple-
relations information into the model. The key
concept is to use the relative distances between
words and entities to encode the positional infor-
mation for each entity. This information is prop-
agated through different layers via attention com-
putations. Following (Shaw et al., 2018), for each
pair of word tokens (xi, xj) with the input repre-
sentations from the previous layer as hi and hj ,
we extend the computation of self-attention zi as:

zi =

N∑
j=1

exp eij∑N
k=1 exp eik

(hjW
V + aVij), (2)

where eij = hiWQ(hjWK + aKij )/
√
dz. (3)

WQ,WK ,WV ∈ Rdz×dz are the parameters of
the model, and dz is the dimension of the output
from the self-attention layer.

Compared to standard BERT’s self-attention,
aVij ,a

K
ij ∈ Rdz are extra, which could be viewed as

the edge representation between the input element
xi and xj . Specifically, we devise aVij and a

K
ij

to encourage each token to be aware of the rela-
tive distance to different entity mentions, and vice
versa.

2We also tried to use MLP and Biaff instead of the linear
layer for the classification, which do not show better perfor-
mance compared to the linear classier, as shown in the exper-
iment section. We hypothesize that this is because the em-
beddings learned from BERT are powerful enough for linear
classifiers. Further experiments is needed to verify this.



1373

in of

suburbs

Bagh

and

fired

artillery

Iraqi

##dad

in

suburbs

of

Bagh

##dad

and

Iraqi

artillery

fired

Zero Vector!"($%&) !"(&%$)

south

south

Figure 2: Illustration of the tensor {aKij} introduced in self-
attention computation. Each red cell embedding is defined by
wd(i−j), as the distance from entity xi to token xj . Each blue
cell embedding is defined by wd(j−i), as the distance from
the entity xj to token xi . White cells are zero embeddings
since neither xi nor xj is entity. The {aVij} follows the same
pattern with independent parameters.

Adapted from (Shaw et al., 2018), we argue that
the relative distance information will not help if
the distance is beyond a certain threshold. Hence
we first define the distance function as:

d(i, j) = min(max(−k, (i− j)), k). (4)

This distance definition clips all distances to a re-
gion [−k, k]. k is a hyper-parameter to be tuned
on the development set. We can now define aVij
and aKij formally as:

aVij ,a
K
ij =


wVd(i,j), w

K
d(i,j), if xi ∈ e

wVd(j,i), w
K
d(j,i), if xj ∈ e

0, else.
(5)

As defined above, if either token xi or xj be-
longs to an entity, we will introduce a relative po-
sitional representation according to their distance.
The distance is defined in an entity-centric way as
we always compute the distance from the entity
mention to the other token. If neither xi nor xj are
entity mentions, we explicitly assign a zero vector
to aKij and a

V
ij . When both xi and xj are inside

entity mentions, we take the distance as d(i, j) to
make row-wise attention computation coherent as
depicted in Figure 2.

During the model fine-tuning, the newly
introduced parameters {wK−k, ..., wKk } and
{wV−k, ..., wVk } are trained from scratch.

4 Experiments

We demonstrate the advantage of our method on
a popular MRE benchmark, ACE 2005 (Walker

et al., 2006), and a more recent MRE benchmark,
SemEval 2018 Task 7 (Gábor et al., 2018). We
also evaluate on a commonly used SRE bench-
mark SemEval 2010 task 8 (Hendrickx et al.,
2009), and achieve state-of-the-art performance.

4.1 Settings

Data For ACE 2005, we adopt the multi-domain
setting and split the data following (Gormley et al.,
2015): we train on the union of news domain
(nw and bn), tune hyperparameters on half of the
broadcast conversation (bc) domain, and evalu-
ate on the remainder of broadcast conversation
(bc), the telephone speech (cts), usenet news-
groups (un), and weblogs (wl) domains. For Se-
mEval 2018 Task 7, we evaluate on its sub-task
1.1. We use the same data split in the shared task.
The passages in this task is usually much longer
compared to ACE. Therefore we adopt the follow-
ing pre-processing step – for the entity pair in each
relation, we assume the tokens related to their re-
lation labeling are always within a range from the
fifth token ahead of the pair to the fifth token af-
ter it. Therefore, the tokens in the original passage
that are not covered by the range of ANY input
relations, will be removed from the input.

Methods We compare our solution with pre-
vious works that predict a single relation per
pass (Gormley et al., 2015; Nguyen and Grishman,
2015; Fu et al., 2017; Shi et al., 2018), our model
that predicts single relation per pass for MRE, and
with the following naive modifications of BERT
that could achieve MRE in one-pass.
• BERTSP: BERT with structured prediction only,
which includes proposed improvement in 3.1.
• Entity-Aware BERTSP: our full model, which
includes both improvements in §3.1 and §3.2.
• BERTSP with position embedding on the final
attention layer. This is a more straightforward
way to achieve MRE in one-pass derived from pre-
vious works using position embeddings (Nguyen
and Grishman, 2015; Fu et al., 2017; Shi et al.,
2018). In this method, the BERT model encode
the paragraph to the last attention-layer. Then, for
each entity pair, it takes the hidden states, adds the
relative position embeddings corresponding to the
target entities, and finally makes the relation pre-
diction for this pair.
• BERTSP with entity indicators on input layer:
it replaces our structured attention layer, and adds
indicators of entities (transformed to embeddings)



1374

Method dev bc cts wl avg

Baselines w/o Domain Adaptation (Single-Relation per Pass)
Hybrid FCM (Gormley et al., 2015) - 63.48 56.12 55.17 58.26
Best published results w/o DA (from Fu et al.) - 64.44 54.58 57.02 58.68
BERT fine-tuning out-of-box 3.66 5.56 5.53 1.67 4.25

Baselines w/ Domain Adaptation (Single-Relation per Pass)
Domain Adversarial Network (Fu et al., 2017) - 65.16 55.55 57.19 59.30
Genre Separation Network (Shi et al., 2018) - 66.38 57.92 56.84 60.38

Multi-Relation per Pass
BERTSP (our model in §3.1) 64.42 67.09 53.20 52.73 57.67
Entity-Aware BERTSP (our full model) 67.46 69.25 61.70 58.48 63.14
BERTSP w/ entity-indicator on input-layer 65.32 66.86 57.65 53.56 59.36
BERTSP w/ pos-emb on final att-layer 67.23 69.13 58.68 55.04 60.95

Single-Relation per Pass
BERTSP (our model in §3.1) 65.13 66.95 55.43 54.39 58.92
Entity-Aware BERTSP (our full model) 68.90 68.52 63.71 57.20 63.14
BERTSP w/ entity-indicator on input-layer 67.12 69.76 58.05 56.27 61.36

Table 1: Main Results on ACE 2005.

directly to each token’s word embedding3. This
method is an extension of (Verga et al., 2018) to
the MRE scenario.

Hyperparameters For our experiments, most
model hyperparameters are the same as in pre-
training. We tune the training epochs and the new
hyperparameter k (in Eq. 4) on the development
set of ACE 2005. Since the SemEval task has
no development set, we use the best hyperparam-
eters selected on ACE. For the number of training
epochs, we make the model pass similar number
of training instances as in ACE 2005.

4.2 Results on ACE 2005

Main Results Table 1 gives the overall results
on ACE 2005. The first observation is that our
model architecture achieves much better results
compared to the previous state-of-the-art methods.
Note that our method was not designed for do-
main adaptation, it still outperforms those meth-
ods with domain adaptation. This result further
demonstrates its effectiveness.

Among all the BERT-based approaches, fine-
tuning the off-the-shelf BERT does not give a sat-
isfying result, because the sentence embeddings
cannot distinguish different entity pairs. The sim-
pler version of our approach, BERTSP, can suc-
cessfully adapt the pre-trained BERT to the MRE
task, and achieves comparable performance at the

3Note the usage of relative position embeddings does not
work for one-pass MRE, since each word corresponds to a
varying number of position embedding vectors. Summing up
the vectors confuses this information. It works for the single-
relation per pass setting, but the performance lags behind us-
ing only indicators of the two target entities.

prior state-of-the-art level of the methods without
domain adaptation.

Our full model, with the structured fine-tuning
of attention layers, brings further improvement of
about 5.5%, in the MRE one-pass setting, and
achieves a new state-of-the-art performance when
compared to the methods with domain adaptation.
It also beats the other two methods on BERT in
Multi-Relation per Pass.

Performance Gap between MRE in One-Pass
and Multi-Pass The MRE-in-one-pass models
can also be used to train and test with one entity
pair per pass (Single-Relation per Pass results in
Table 1). Therefore, we compare the same meth-
ods when applied to the multi-relation and single-
relation settings. For BERTSP with entity indica-
tors on inputs, it is expected to perform slightly
better in the single-relation setting, because of the
mixture of information from multiple pairs. A 2%
gap is observed as expected. By comparison, our
full model has a much smaller performance gap
between two different settings (and no consistent
performance drop over different domains).

The BERTSP is not expected to have a gap as
shown in the table.For BERTSP with position em-
beddings on the final attention layer, we train the
model in the single-relation setting and test with
two different settings, so the results are the same.

Training and Inference Time Through our ex-
periment,4 we verify that the full model with MRE
is significantly faster compared to all other meth-
ods for both training and inference. The training

4All evaluations were done on a single Tesla K80 GPU.



1375

Method dev bc cts wl avg

Linear 67.46 69.25 61.70 58.48 63.14
MLP 67.16 68.52 61.16 54.72 61.47
Biaff 67.06 68.22 60.39 55.60 61.40

Table 2: Our model with different prediction modules.

time for full model with MRE is 3.5x faster than
it with SRE. As for inference speed, the former
could reach 126 relation per second compared the
later at 23 relation per second. It is also much
faster when compared to the second best perform-
ing approach, BERTSP w/ pos-emb on final att-
layer, which is at 76 relation per second, as it runs
the last layer for every entity pair.

Prediction Module Selection Table 2 evaluates
the usage of different prediction layers, including
replacing our linear layer in Eq.(1) with MLP or
Biaff. Results show that the usage of the linear
predictor gives better results. This is consistent
with the motivation of the pre-trained encoders:
by unsupervised pre-training the encoders are ex-
pected to be sufficiently powerful thus adding
more complex layers on top does not improve the
capacity but leads to more free parameters and
higher risk of over-fitting.

4.3 Results on SemEval 2018 Task 7

The results on SemEval 2018 Task 7 are shown in
Table 3. Our Entity-Aware BERTSP gives com-
parable results to the top-ranked system (Rot-
sztejn et al., 2018) in the shared task, with slightly
lower Macro-F1, which is the official metric of the
task, and slightly higher Micro-F1. When predict-
ing multiple relations in one-pass, we have 0.9%
drop on Macro-F1, but a further 0.8% improve-
ment on Micro-F1. Note that the system (Rot-
sztejn et al., 2018) integrates many techniques
like feature-engineering, model combination, pre-
training embeddings on in-domain data, and arti-
ficial data generation, while our model is almost a
direct adaption from the ACE architecture.

On the other hand, compared to the top single-
model result (Luan et al., 2018), which makes
use of additional word and entity embeddings pre-
trained on in-domain data, our methods demon-
strate clear advantage as a single model.

4.4 Additional SRE Results

We conduct additional experiments on the relation
classification task, SemEval 2010 Task 8, to com-

Method Averaged F1Macro Micro

Top 3 in the Shared Task
(Rotsztejn et al., 2018) 81.7 82.8
(Luan et al., 2018) 78.9 -
(Nooralahzadeh et al., 2018) 76.7 -

Ours (single-per-pass) 81.4 83.1
Ours (multiple-per-pass) 80.5 83.9

Table 3: Results on SemEval 2018 Task 7, Sub-Task 1.1.

Method Macro-F1

Best published result (Wang et al., 2016) 88.0

BERT out-of-box 80.9
Entity-Aware BERT 88.8
BERTSP 88.8
Entity-Aware BERTSP 89.0

Table 4: Additional Results on SemEval 2010 Task 8.

pare with models developed on this benchmark.
From the results in Table 4, our proposed tech-
niques also outperforms the state-of-the-art on this
single-relation benchmark.

On this single relation task, the out-of-box
BERT achieves a reasonable result after fine-
tuning. Adding the entity-aware attention gives
about 8% improvement, due to the availability of
the entity information during encoding. Adding
structured prediction layer to BERT (i.e., BERTSP)
also leads to a similar amount of improvement.
However, the gap between BERTSP method with
and without entity-aware attention is small. This
is likely because of the bias of data distribution:
the assumption that only two target entities exist,
makes the two techniques have similar effects.

5 Conclusion

In summary, we propose a first-of-its-kind solution
that can simultaneously extract multiple relations
with one-pass encoding of an input paragraph for
MRE tasks. With the proposed structured predic-
tion and entity-aware self-attention layers on top
of BERT, we achieve a new state-of-the-art results
with high efficiency on the ACE 2005 benchmark.
Our idea of encoding a passage regarding mul-
tiple entities has potentially broader applications
beyond relation extraction, e.g., entity-centric pas-
sage encoding in question answering (Song et al.,
2018a). In the future work, we will explore the
usage of this method with other applications.



1376

References
Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma,

Eduard Hovy, Kai-Wei Chang, and Nanyun Peng.
2018. Near or far, wide range zero-shot cross-
lingual dependency parsing. arXiv preprint
arXiv:1811.00570.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Timothy Dozat and Christopher D Manning. 2018.
Simpler but more accurate semantic dependency
parsing. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), volume 2, pages 484–490.

L. Del Corro F. Petroni and R. Gemulla. 2015. Core:
Context-aware open relation extraction with factor-
ization machines. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).

Lisheng Fu, Thien Huu Nguyen, Bonan Min, and
Ralph Grishman. 2017. Domain adaptation for re-
lation extraction with domain adversarial neural net-
work. In Proceedings of the Eighth International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), volume 2, pages 425–429.

Kata Gábor, Davide Buscaldi, Anne-Kathrin Schu-
mann, Behrang QasemiZadeh, Haifa Zargayouna,
and Thierry Charnois. 2018. Semeval-2018 task 7:
Semantic relation extraction and classification in sci-
entific papers. In Proceedings of The 12th Inter-
national Workshop on Semantic Evaluation, pages
679–688.

Matthew R Gormley, Mo Yu, and Mark Dredze. 2015.
Improved relation extraction with feature-rich com-
positional embedding models. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1774–1784.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 94–99.
Association for Computational Linguistics.

Ronghang Hu, Anna Rohrbach, Trevor Darrell, and
Kate Saenko. 2019. Language-conditioned graph
networks for relational reasoning. arXiv preprint
arXiv:1905.04405.

Linguistic Data Consortium. 2013. Deft ere annotation
guidelines: Relations v1.1. 05.17.2013.

Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.
2018. The UWNLP system at SemEval-2018 task 7:

Neural relation extraction model with selectively in-
corporated concept embeddings. In Proceedings of
The 12th International Workshop on Semantic Eval-
uation, pages 788–792, New Orleans, Louisiana.
Association for Computational Linguistics.

Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B
Tenenbaum, and Jiajun Wu. 2019. The neuro-
symbolic concept learner: Interpreting scenes,
words, and sentences from natural supervision.
arXiv preprint arXiv:1904.12584.

Thien Huu Nguyen and Ralph Grishman. 2015.
Combining neural networks and log-linear mod-
els to improve relation extraction. arXiv preprint
arXiv:1511.05926.

Farhad Nooralahzadeh, Lilja Øvrelid, and Jan Tore
Lønning. 2018. SIRIUS-LTG-UiO at SemEval-
2018 task 7: Convolutional neural networks with
shortest dependency paths for semantic relation ex-
traction and classification in scientific papers. In
Proceedings of The 12th International Workshop on
Semantic Evaluation, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Lizhen Qu, Yi Zhang, Rui Wang, Lili Jiang, Rainer
Gemulla, and Gerhard Weikum. 2014. Senti-lssvm:
Sentiment-oriented multi-relation extraction with la-
tent structural svm. Transactions of the Association
for Computational Linguistics, 2:155–168.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. URL https://s3-
us-west-2. amazonaws. com/openai-assets/research-
covers/languageunsupervised/language under-
standing paper. pdf.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
74–84.

Jonathan Rotsztejn, Nora Hollenstein, and Ce Zhang.
2018. ETH-DS3Lab at SemEval-2018 task 7: Effec-
tively combining recurrent and convolutional neural
networks for relation classification and extraction.
In Proceedings of The 12th International Workshop
on Semantic Evaluation, New Orleans, Louisiana.
Association for Computational Linguistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. In NAACL-HLT, page 464468.

Ge Shi, Chong Feng, Lifu Huang, Boliang Zhang,
Heng Ji, Lejian Liao, and Heyan Huang. 2018.
Genre separation network with adversarial training
for cross-genre relation extraction. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 1018–1023.



1377

Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang,
Radu Florian, and Daniel Gildea. 2018a. Exploring
graph-structured passage representation for multi-
hop reading comprehension with graph neural net-
works. arXiv preprint arXiv:1809.02040.

Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel
Gildea. 2018b. N-ary relation extraction using
graph-state lstm. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2226–2235.

Daniil Sorokin and Iryna Gurevych. 2017. Context-
Aware Representations for Knowledge Base Rela-
tion Extraction. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1784–1789. Associa-
tion for Computational Linguistics.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 joint conference on empirical
methods in natural language processing and compu-
tational natural language learning, pages 455–465.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multi-
lingual relation extraction using compositional uni-
versal schema. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 886–896.

Patrick Verga, Emma Strubell, and Andrew McCallum.
2018. Simultaneously self-attending to all mentions
for full-abstract biological relation extraction. In
NAACL 2018, pages 872–884.

Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia, 57.

Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan
Liu. 2016. Relation classification via multi-level at-
tention cnns. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1298–1307.

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015. Semantic relation clas-
sification via convolutional neural networks
with simple negative sampling. arXiv preprint
arXiv:1506.07650.

Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,
and Dongyan Zhao. 2016. Question answering on
freebase via relation extraction and textual evidence.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 2326–2336.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), volume 1, pages 1321–1331.

Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos
Santos, Bing Xiang, and Bowen Zhou. 2017. Im-
proved neural relation detection for knowledge base
question answering. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 571–581.

https://doi.org/10.18653/v1/D17-1188
https://doi.org/10.18653/v1/D17-1188
https://doi.org/10.18653/v1/D17-1188

