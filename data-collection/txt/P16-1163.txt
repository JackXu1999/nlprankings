



















































Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1726–1735,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Implicit Discourse Relation Detection via a Deep Architecture with Gated
Relevance Network

Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang
Shanghai Key Laboratory of Data Science

School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China

jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn

Abstract

Word pairs, which are one of the most eas-
ily accessible features between two text
segments, have been proven to be very
useful for detecting the discourse relations
held between text segments. However, be-
cause of the data sparsity problem, the
performance achieved by using word pair
features is limited. In this paper, in or-
der to overcome the data sparsity problem,
we propose the use of word embeddings
to replace the original words. Moreover,
we adopt a gated relevance network to
capture the semantic interaction between
word pairs, and then aggregate those se-
mantic interactions using a pooling layer
to select the most informative interactions.
Experimental results on Penn Discourse
Tree Bank show that the proposed method
without using manually designed features
can achieve better performance on recog-
nizing the discourse level relations in all of
the relations.

1 Introduction

In a well-written document, no unit of the text is
completely isolated, discourse relations describe
how two units (e.g. clauses, sentences, and larger
multi-clause groupings) of discourse are logically
connected. Many downstream NLP applications
such as opinion mining, summarization, and event
detection, can benefit from those relations.

The task of automatically identify discourse re-
lation is relatively simple when explicit connec-
tives such as however and because are given (Pitler
et al., 2009). However, the identification becomes
much more challenging when such connectives are
missing. In fact, such implicit discourse relations

outnumber explicit relations in naturally occurring
text, and identify those relations have been shown
to be the performance bottleneck of an end-to-end
discourse parser (Lin et al., 2014).

Most of the existing researches used rich lin-
guistic features and supervised learning methods
to achieve the task (Soricut and Marcu, 2003;
Pitler et al., 2009; Rutherford and Xue, 2014).
Among their works, word pairs are heavily used
as an important feature, since word pairs like
(warm,cold) might directly trigger a contrast rela-
tion. However, because of the data sparsity prob-
lem (McKeown and Biran, 2013) and the lack of
metrics to measure the semantic relation between
those pairs, which is so-called the semantic gap
problem (Zhao and Grosky, 2002), the classifiers
based on word pairs in the previous studies did not
work well. Moreover, some text segment pairs are
more complicated, it is hard to determine the re-
lation held between them using only word pairs.
Consider the following sentence pair with a casual
relation as an example:

S1: Psyllium’s not a good crop.
S2: You get a rain at the wrong time and the

crop is ruined.

Intuitively, (good, wrong) and (good, ruined),
seem to be the most informative word pairs, and
it is likely that they will trigger a contrast rela-
tion. Therefore, we can see that another main dis-
advantage of using word pairs is the lack of con-
textual information, and using n-gram pairs will
again suffer from data sparsity problem.

Recently, the distributed word representa-
tions (Bengio et al., 2006; Mikolov et al., 2013)
have shown an advantage when dealing with data
sparsity problem (Braud and Denis, 2015), and
many deep learning based models are generat-
ing substantial interests in text semantic matching
and have achieved some significant progresses (Hu

1726



Psyllium's not a good 

crop. You get a rain at the 
wrong time and the 
crop is ruined.

f

Bidirectional LSTM Pooling Layer MLP

Gated Relevance Network

Bilinear Tensor Single Layer Network

Figure 1: The processing framework of the proposed approach.

et al., 2014; Qiu and Huang, 2015; Wan et al.,
2015). Inspired by their work, we in this pa-
per propose the use of word embeddings to re-
place the original words in the text segments to
fight against the data sparsity problem. Further
more, in order to preserve the contextual infor-
mation around the word embeddings, we encode
the text segment to its positional representation
via a recurrent neural network, specifically, we use
a bidirectional LSTM (Hochreiter and Schmidhu-
ber, 1997). Then, to overcome the semantic gap,
we propose the use of a gated relevance network
to capture the semantic interaction between those
positional representations. Finally, all the interac-
tions generated by the relevance network are fed
to a max pooling layer to get the strongest interac-
tions. We then aggregate them to predict the dis-
course relation through a multi-layer perceptron
(MLP). Our model is trained end to end by Back-
Propagation and Adagrad.

The main contribution of this paper can be sum-
marized as follows:

• We use word embeddings to replace the orig-
inal words in the text segments to overcome
data sparsity problem. In order to preserve

the contextual information, we further en-
code the text segment to its positional repre-
sentation through a recurrent neural network.

• To deal with the semantic gap problem, we
adopt a gated relevance network to capture
the semantic interaction between the interme-
diate representations of the text segments.

• Experimental results on PDTB (Prasad et al.,
2008) show that the proposed method can
achieve better performance in recognizing
discourse level relations in all of the relations
than the previous methods.

2 The Proposed Method

The architecture of our proposed method is shown
in figure 1. In the following of this section, we will
illustrate the details of the proposed framework.

2.1 Embedding Layer
To model the sentences with neural model, we
firstly need to transform the one-hot representa-
tion of word into the distributed representation.
All words of two text segments X and Y will be
mapped into low dimensional vector representa-
tions, which are taken as input of the network.

1727



Through this layer, we can filter the words appear
in low frequency, and we then map these words
to a special OOV (out of vocabulary) word em-
bedding. In addition, all the text segments in our
experiment are padded to have the same length.

2.2 Sentence Modeling with LSTM
Long Short-Term Memory network
(LSTM) (Hochreiter and Schmidhuber, 1997)
is a type of recurrent neural network (RNN),
and specifically addresses the issue of learning
long-term dependencies. Given a variable-length
sentence S = (x0, x1, ..., xT ), LSTM processes
it by incrementally adding up new content into
a single slot of maintained memory, with gates
controlling the extent to which new content should
be memorized, old content should be erased and
current content should be exposed. At position t,
the memory ct and the hidden state ht are updated
with the following equations:

c̃t
ot
it
ft

 =


tanh
σ
σ
σ

TA,b [ xtht−1
]
, (1)

ct = c̃t � it + ct−1 � ft, (2)
ht = ot � tanh (ct) , (3)

where it, ft, ot, denote the input, forget and out-
put gate at time step t respectively, and TA,b is an
affine transformation which depends on parame-
ters of the network A and b. σ denotes the logis-
tic sigmoid function and � denotes elementwise
multiplication.

Notice that the LSTM defined above only get
context information from the past. However, con-
text information from the future could also be cru-
cial. To capture the context from both past and
the future, we propose to use the bidirectional
LSTM (Schuster and Paliwal, 1997). Bidirectional
LSTM preserves the previous and future context
information by two separate LSTMs, one encodes
the sentence from start to the end, and the other
encodes the sentence from end to the start. There-
fore, at each position t of the sentence, we can ob-
tain two representations

−→
ht and

←−
ht . It is natural

to concatenate them to get the intermediate rep-
resentation at position t, i.e. ht = [

−→
ht ,
←−
ht ]. A

illustration for the bidirectional LSTM are shown
in Figure 2.

Given a sentence S = (x0, x1, ..., xT ), we can
now encode it with a bidirectional LSTM, and re-

Outputs

Backward Layer

Forward Layer

Input

ht-1 ht ht+1

xt-1 xt xt+1

ht-1 ht ht+1

ht-1 ht ht+1

···

···

···

···

Figure 2: A illustration of bidirectional LSTM.

place the word wt with ht, we can interpret ht as a
representation summarizing the word at position t
and its contextual information.

2.3 Gated Relevance Network
Given two text segments X = x1, x2, ..., xn and
Y = y1, y2, ..., ym, after the encoding procedure
with a bidirectional LSTM, we can get their posi-
tional representation Xh = xh1 , xh2 , ..., xhn and
Yh = yh1 , yh2 , ..., yhm . We then compute the rele-
vance score between every intermediate represen-
tation pair xhi and yhj with dimension dh. Tra-
ditional ways to measure their relevance includes
cosine distance, bilinear model (Sutskever et al.,
2009; Jenatton et al., 2012), single layer neural
network (Collobert and Weston, 2008), etc. We
then illustrate the bilinear model and the single
layer neural network in details.

Bilinear Model is defined as follows:

s(hxi , hyj ) = h
T
xiMhyj , (4)

where the only parameter M ∈ Rdh×dh . The bi-
linear model is a simple but efficient way to incor-
porate the strong linear interactions between two
vectors, while the main weakness of it is the lack
of ability to deal with nonlinear interaction.

Single Layer Network is defined as:

s(hxi , hyj ) = u
T f(V

[
hxi
hyj

]
+ b), (5)

where f is a standard nonlinearity applied
element-wise, V ∈ Rk×2dh , b ∈ Rk, and u ∈ Rk.
The single layer network could capture nonlinear
interaction, while at the expense of a weak inter-
action between two vectors.

Each of the two models have its own advan-
tages, and they can not take the place of each other.

1728



In our work, we propose to incorporate the two
models through the gate mechanism, so that the
model is more powerful to capture more complex
semantic interactions. The incorporated model,
namely gated relevance network (GRN), is de-
fined as:

s(hxi , hyj ) = u
T (g � hTxiM [1:r]hyj

+ (1− g)� f(V
[
hxi
hyj

]
) + b),

(6)

where f is a standard nonlinearity applied
element-wise, M [1:r] ∈ Rr×dh×dh is a bilinear
tensor and the tensor product hTxiM

[1:r]hyj results
in a vector m ∈ Rr, where each entry is com-
puted by one slice k = 1, 2, ..., r of the tensor:
mk = hTxiM

khyj , V ∈ Rr×2dh , b ∈ Rr, and
u ∈ Rr, g is a gate expressing how the output is
produced by the linear and nonlinear semantic in-
teractions between the input, defined as:

g = σ(Wg

[
hxi
hyj

]
+ bg), (7)

where Wg ∈ Rr×2dh , b ∈ Rr and σ denotes the
logistic sigmoid function.

The gated relevance network is a little bit simi-
lar to the Neural Tensor Network (NTN) proposed
by Socher et al. (2013):

s(hxi , hyj ) = u
T f(hTxiM

[1:r]hyj+V
[
hxi
hyj

]
+b).

(8)
Compared with NTN, the main advantage of

our model is we use a gate to tell how the lin-
ear and nonlinear interaction should be combined,
while in NTN, the interaction generated by bi-
linear model and single layer network are treated
equally. Also, NTN feeds the incorporated inter-
action to a nonlinearity, while we are not.

As we can see, for each pair of the intermediate
representation, the gated relevance network will
produce a semantic interaction score, thus, the en-
tire output of two text segments is an interaction
score matrix.

2.4 Max-Pooling Layer and MLP

The relation between two text segments is often
determined by some strong semantic interactions,
therefore, we adopt max-pooling strategy which
partitions the score matrix as shown in Figure 1

into a set of non-overlapping sub-regions, and for
each such sub-region, outputs the maximum value.
The pooling scores are further reshaped to a vector
and fed to a multi-layer perceptron (MLP). More
specifically, the vector obtained by the pooling
layer is fed into a full connection hidden layer to
get a more abstractive representation first, and then
connect to the output layer. For the task of classi-
fication, the outputs are probabilities of different
classes, which is computed by a softmax function
after the fully-connected layer. We name the full
architecture of our model Bi-LSTM+GRN.

2.5 Model Training

Given a text segment pair (X,Y ) and its label
l, the training objective is to minimize the cross-
entropy of the predicted and the true label distri-
butions, defined as:

L(X,Y ; l, l̂) = −
C∑
j=1

lj log(̂lj), (9)

where l is one-hot representation of the ground-
truth label l; l̂ is the predicted probabilities of la-
bels; C is the class number.

To minimize the objective, we use stochastic
gradient descent with the diagonal variant of Ada-
Grad (Duchi et al., 2011) with minibatches. The
parameter update for the i-th parameter θt,i at time
step t is as follows:

θt,i = θt−1,i − α√∑t
τ=1 g

2
τ,i

gt,i, (10)

where α is the initial learning rate and gτ ∈ R|θτ,i|
is the gradient at time step τ for parameter θτ,i.

3 Experiment

3.1 Dataset

The dataset we used in this work is Penn Dis-
course Treebank 2.0 (Prasad et al., 2008), which
is one of the largest available annotated corpora
of discourse relations. It contains 40,600 re-
lations, which are manually annotated from the
same 2,312 Wall Street Journal (WSJ) articles as
the Penn Treebank. We follow the recommended
section partition of PDTB 2.0, which is to use
sections 2-20 for training, sections 21-22 for test-
ing and the other sections for validation (Prasad
et al., 2008). For comparison with the previous
work (Pitler et al., 2009; Zhou et al., 2010; Park

1729



Table 1: The unbalanced sample distribution of
PDTB.

Relation Train Dev Test
Comparison 1894 401 146
Contingency 3281 628 276
Expansion 6792 1253 556
Temporal 665 93 68

and Cardie, 2012; Rutherford and Xue, 2014; Ji
and Eisenstein, 2015), we train four binary classi-
fiers to identify each of the top level relations, the
EntRel relations are merged with Expansion rela-
tions. For each classifier, we use an equal number
of positive and negative samples as training data,
because each of the relations except Expansion is
infrequent (Pitler et al., 2009) as what shows in
Table 1. The negative samples were chosen ran-
domly from training sections 2-20.

3.2 Experiment Protocols

In this part, we will mainly introduce the exper-
iment settings, including baselines and parameter
setting.

3.2.1 Baselines
The baselines for comparison with our proposed
method are listed as follows:

• LSTM: We use two single LSTM to encode
the two text segments, then concatenate them
and feed to a MLP to do the relation detec-
tion.

• Bi-LSTM: We use two single bidirectional
LSTM to encode the two text segments, then
concatenate them and feed to a MLP to do the
relation detection.

• Word+NTN: We use the neural tensor de-
fined in (8) to capture the semantic interac-
tion scores between every word embedding
pair, the rest of the method is the same as our
proposed method.

• LSTM+NTN: We use two single LSTM to
generate the positional text segments repre-
sentation. The rest of the method is the same
as Word-NTN.

• BLSTM+NTN: We use two single bidirec-
tional LSTM to generate the positional text

Table 2: Hyperparameters for our model in the ex-
periment.

Word Embedding size nw = 50
Initial learning rate ρ = 0.01
Minibatch size m = 32
Pooling Size (p, q) = (3, 3)
Number of tensor slice r = 2

segments representation. The rest of the
method is the same as Word-NTN.

• Word+GRN: We use the gated relevance net-
work proposed in this paper to capture the se-
mantic interaction scores between every word
embedding pair of the two text segments. The
rest of the method is the same as our model.

• LSTM+GRN: We use the gated relevance
network proposed in this paper to capture the
semantic interaction scores between every in-
termediate representation pair of the two text
segments generated by LSTM. The rest of the
method is the same as our model.

3.2.2 Parameter Setting
For the initialization of the word embeddings
used in our model, we use the 50-dimensional
pre-trained embeddings provided by Turian et al.
(2010), and the embeddings are fixed during train-
ing. We only preserve the top 10,000 words ac-
cording to its frequency of occurrence in the train-
ing data, all the text segments are padded to have
the same length of 50, the intermediate represen-
tations of LSTM are also set to 50. The other
parameters are initialized by randomly sampling
from uniform distribution in [-0.1,0.1].

For other hyperparameters of our proposed
model, we take those hyperparameters that
achieved best performance on the development
set, and keep the same parameters for other com-
petitors. The final hyper-parameters are show in
Table 2.

3.3 Result
The results on PDTB are show in Table3, from the
results, we have several experiment findings.

First of all, it is easy to notice that LSTM and
Bi-LSTM achieve lower performance than all of
the methods of using a tensor to capture the se-
mantic interactions between word pairs and the in-
termediate representation pairs. Because the main

1730



Table 3: The performances of different approaches on the PDTB.

Comparison Contingency Expansion Temporal
(Pitler et al., 2009) 21.96% 47.13% 76.42% 16.76%
(Zhou et al., 2010) 31.79% 47.16% 70.11% 20.30%
(Park and Cardie, 2012) 31.32% 49.82% 79.22% 26.57%
(Rutherford and Xue, 2014) 39.70% 54.42% 80.44% 28.69%
(Ji and Eisenstein, 2015) 35.93% 52.78% 80.02% 27.63%
LSTM 31.78% 45.39% 75.10% 19.65%
Bi-LSTM 31.97% 45.66% 75.13% 20.02%
Word+NTN 32.18% 46.45% 77.64% 21.60%
LSTM+NTN 36.82% 50.09% 79.88% 26.54%
Bi-LSTM+NTN 39.36% 53.74% 80.02% 28.41%
Word+GRN 32.67% 46.52% 77.68% 21.21%
LSTM+GRN 38.13% 52.25% 79.96% 27.15%
Bi-LSTM+GRN 40.17% 54.76% 80.62% 31.32%

disadvantage of using LSTM and Bi-LSTM to en-
code the text segment into a single representation
is that some important local information such as
key words can not be fully preserved when com-
pressing a long sentence into a single representa-
tion.

Second, the performance improves a lot when
using LSTM and Bi-LSTM to encode the text seg-
ments to positional representations instead of us-
ing word representations directly. We conclude
it is mainly because the following two reasons:
for one thing, some words are important only
when they are associated with their context, for
the other, the intermediate representations are the
high-level representation of the sentence at each
position, there is no doubt for they can obtain
much more semantic information than the words
along. In addition, Bi-LSTM also takes the future
information of the text segments into considera-
tion, resulting in a consistently better performance
than LSTM.

Third, take a comparison to the methods using
NTN and the methods using GRN, we can find
that the GRN performs consistently better. Such
results show that the gate we proposed to combine
the information of two aspects is actually useful.

At last, our proposed model, namely, Bi-
LSTM+GRN achieves best performance on all of
the relations. It not only shows the interaction be-
tween word pairs is useful, but also shows the way
we proposed to capture such information is use-
ful too. Further more, compared with the previ-
ous methods (Pitler et al., 2009; Park and Cardie,
2012; Rutherford and Xue, 2014), which used ei-

ther a lot of complex textual features and contex-
tual information about the two text segments or a
larger unannotated corpus to help the prediction,
our model only uses the the information of the two
text segments themselves, but yet achieves better
performance. It demonstrates that our model is
powerful in modeling the discourse relations.

3.4 Parameter Sensitivity

In this section, we evaluate our model through
different settings of the proposed gated relevance
network, the other hyperparameters are the same
as mentioned above and we use a bidirectional
LSTM to encode the text segments. The settings
are shown as follows:

• GRN-1 We set the parameters r = 1, M [1] =
I , V = 0, b = 0 and g = 1. The model can
be regarded as cosine similarity.

Cmp Ctg Exp Tmp
GRN-1 34.01% 50.23% 78.66% 21.33%
GRN-2 35.74% 50.91% 79.40% 23.28%
GRN-3 34.15% 51.78% 79.33% 26.16%
GRN-4 37.18% 52.36% 79.86% 25.60%
GRN-5 40.17% 54.76% 80.62% 31.32%
GRN-6 38.26% 52.08% 80.02% 28.51%

Table 4: Comparison of our model with different
parameter settings to the gated relevance network.
Cmp denotes the comparison relation, Ctg denotes
the contingency relation, Exp denotes the expan-
sion relation and Tmp denotes the temporal rela-
tion.

1731



• GRN-2 We set the parameters r = 1, V = 0,
b = 0, and g = 1. The model can be regarded
as the bilinear model.

• GRN-3 We set the parameters r = 1, M [1] =
0, and g = 0. The model can be regarded as
a single layer network.

• GRN-4 We set the parameters r = 1. This
model is the full GRN model.

• GRN-5 We set the parameters r = 2. This
model is the full GRN model.

• GRN-6 We set the parameters r = 3. This
model is the full GRN model.

The results for different parameter settings are
shown in Table 4. It is obvious that GRN-1
achieves a relatively lower performance, showing
that the cosine similarity is not enough to capture
the complex semantic interaction. Take a com-
parison on GRN-2 and GRN-3, we can see that
GRN-2 outperforms GRN-3 on Comparison and
Expansion relation, while achieves a lower perfor-
mance on the other two relations, moreover, the
combination method GRN-4 outperforms both of
the methods, demonstrating that the semantic in-
teractions captured by the bilinear model and the
single layer network are different. Hence, they can
not take the place of each other, and it is reason-
able to use a gate to combine them.

Among the methods of using the full GRN
model, GRN-5 which has 2 bilinear tensor slices
achieves the best performance. We explain this
phenomenon on two aspects, on one hand, we can
see each slice of the bilinear tensor as being re-
sponsible for one type of the relation, a bilinear
tensor with 2 slices is more suitable for training a
binary classifier than the original bilinear model.
On the other hand, increasing the number of slices
will increase the complexity of the model, thus
making it harder to train.

3.5 Case Study
In this section, we go back to the example men-
tioned above to show see what information be-
tween the text segment pairs is captured, and how
the positional sentence representations affect the
performance of our model.

The examples is listed below:

S1: Psyllium’s not a good crop.
S2: You get a rain at the wrong time and the

crop is ruined.

Yo
u

ge
t a

ra
in at th

e

wr
on

g
tim

e
an

d
th

e
cr

op is

ru
in

ed

Ps
yl
liu

m

is

no
t

a

go
od

cr
op

(a) Word+GRN

Yo
u

ge
t a

ra
in at th

e

wr
on

g
tim

e
an

d
th

e
cr

op is

ru
in

ed

Ps
yl
liu

m

is

no
t

a

go
od

cr
op

(b) Bi-LSTM+GRN

Figure 3: A visualization of the interaction score
matrix between two relatively complex sentences.
The darker patches denote the corresponding
scores generated by the gated relevance network
are higher.

In this case, the relation between the sentence
pair is Contingency, and the implicit connective
annotated by human is “because”. The pair is
likely to be classified to a wrong contrast rela-
tion if we only focus on the informative word
pairs (good,wrong) and (good,ruined). It is mainly
because their relation is highly depended on the
semantic of the whole sentence, and the words
should be considered with their context.

Figure 3 explains this phenomenon, in Fig-
ure 3a, we can see that the word pairs which as-
sociate with “not” get high scores, scores on the
other pairs are relatively arbitrary. It demonstrates
the word embedding model failed to learn which
part of the sentence should be focused, although
the useless word such as “Psyllium” and “a” are

1732



ignored, thus making it harder to identify the rela-
tion.

Take Figure 3b for a comparison, from the fig-
ure we can observe the pairs that associate with
“not” and “good” which are import context to de-
termine the semantic of the sentence get much
higher scores. Moreover, the scores increase along
with the sentence encoding procedure, especially
when the last informative word “ruined” appears.
Once again, some useless word are also ignored
by this model. It demonstrates the bidirectional
LSTM we used in our model could encode the
contextual information to the intermediate repre-
sentations, thus these information could help to
determine which part of the two sentence should
be focused when identifying their relation.

4 Related Work

Discourse relations, which link clauses in text, are
used to represent the overall text structure. Many
downstream NLP tasks such as text summariza-
tion, question answering, and textual entailment
can benefit from the task. Along with the in-
creasing requirement, many works have been con-
structed to automatically identify these relations
from different aspects (Pitler et al., 2008; Pitler
et al., 2009; Zhou et al., 2010; McKeown and Bi-
ran, 2013; Rutherford and Xue, 2014; Xue et al.,
2015).

For training and comparing the performance of
different methods, the Penn Discourse Treebank
(PDTB) 2.0, which is large annotated discourse
corpuses, were released in 2008 (Prasad et al.,
2008). The annotation methodology of it fol-
lows the lexically grounded, predicate-argument
approach. In PDTB, the discourse relations were
predefined by Webber (2004). PDTB-styled dis-
course relations hold in only a local contextual
window, and these relations are organized hierar-
chically. Also, every relation in PDTB has either
an explicit or an implicit marker. Since explicit
relations are easy to identify (Pitler et al., 2008),
existing methods achieved good performance on
the relations with explicit maker. In recent years,
researchers mainly focused on implicit relations.
For easily comparing with other methods, in this
work, we also use PDTB as the training and test-
ing corpus.

As we mentioned above, various approaches
have been proposed to do the task. Pitler et al.
(2009) proposed to train four binary classifiers us-

ing word pairs as well as other rich linguistic fea-
tures to automatically identify the top-level PDTB
relations. Park and Cardie (2012) achieved a
higher performance by optimizing the feature set.
McKeown and Biran (2013) aims at solving the
data sparsity problem, and they extended the work
of Pitler et al. (2009) by aggregating word pairs.
Rutherford and Xue (2014) used Brown clusters
and coreferential patterns as new features and im-
proved the baseline a lot. Braud and Denis (2015)
compared different word representations for im-
plicit relation classification. The word pairs fea-
ture have been studied by all of the work above,
showing its importance on discourse relation. We
follow their work, and incorporate word embed-
ding to deal with this problem.

There also exist some work performing this task
from other perspectives. Zhou et al. (2010) stud-
ied the problem from predicting implicit marker.
They used a language model to add implicit mark-
ers as an additional feature to improve perfor-
mance. Their approach can be seen as a semi-
supervised method. Ji and Eisenstein (2015) com-
putes distributed meaning representations for each
discourse argument by composition up the syn-
tactic parse tree. Chen et al. (2016) used vector
offsets to represent this relation between sentence
pairs, and aggregate this offsets through the Fisher
vector. Liu et al. (2016) used a a mutil-task deep
learning framework to deal with this problem, they
incorporate other similar corpus to deal with the
data sparsity problem.

Most of the previous works mentioned above
used rich linguistic features and supervised learn-
ing methods to achieve the task. In this paper,
we propose a deep architecture, which does not
need these manually selected features and addi-
tional linguistic knowledge base to do it.

5 Conclusion

In this work, we propose to use word embeddings
to fight against the data sparsity problem of word
pairs. In order to preserve contextual information,
we encode a sentence to its positional represen-
tation via a recurrent neural network, specifically,
a LSTM. To solve the semantic gap between the
word pairs, we propose to use a gated relevance
network which incorporates both the linear and
nonlinear interactions between pairs. Experiment
results on PDTB show the proposed model outper-
forms the existing methods using traditional fea-

1733



tures on all of the relations.

Acknowledgement

The authors wish to thank the anonymous review-
ers for their helpful comments. This work was par-
tially funded by National Natural Science Foun-
dation of China (No. 61532011, 61473092, and
61472088), the National High Technology Re-
search and Development Program of China (No.
2015AA015408).

References
Yoshua Bengio, Holger Schwenk, Jean-Sébastien

Senécal, Fréderic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.

Chloé Braud and Pascal Denis. 2015. Comparing word
representations for implicit discourse relation classi-
fication. In Empirical Methods in Natural Language
Processing (EMNLP 2015).

Jifan Chen, Qi Zhang, Pengfei Liu, and Xuanjing
Huang. 2016. Discourse relations detection via
a mixed generative-discriminative framework. In
AAAI.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems, pages 2042–2050.

Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes,
and Guillaume R Obozinski. 2012. A latent fac-
tor model for highly multi-relational data. In Ad-
vances in Neural Information Processing Systems,
pages 3167–3175.

Yangfeng Ji and Jacob Eisenstein. 2015. One vector is
not enough: Entity-augmented distributed semantics
for discourse relations. Transactions of the Associa-
tion for Computational Linguistics, 3:329–344.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A pdtb-styled end-to-end discourse parser. Natural
Language Engineering, 20(02):151–184.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
2016. Implicit discourse relation classification via
multi-task neural networks. In AAAI.

Kathleen McKeown and Or Biran. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 69–73. The Association for Compu-
tational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 108–112. Association for Com-
putational Linguistics.

Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008.
Easily identifiable discourse relations. Technical
Reports (CIS), page 884.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.

Xipeng Qiu and Xuanjing Huang. 2015. Con-
volutional neural tensor network architecture for
community-based question answering. In Proceed-
ings of the 24th International Joint Conference on
Artificial Intelligence (IJCAI), pages 1305–1311.

Attapol Rutherford and Nianwen Xue. 2014. Dis-
covering implicit discourse relations through brown
cluster pair representation and coreference patterns.
In EACL, volume 645, page 2014.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems,
pages 926–934.

1734



Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149–156. Association
for Computational Linguistics.

Ilya Sutskever, Joshua B Tenenbaum, and Ruslan R
Salakhutdinov. 2009. Modelling relational data us-
ing bayesian clustered tensor factorization. In Ad-
vances in neural information processing systems,
pages 1821–1828.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,
Liang Pang, and Xueqi Cheng. 2015. A deep ar-
chitecture for semantic matching with multiple po-
sitional sentence representations. arXiv preprint
arXiv:1511.08277.

Bonnie Webber. 2004. D-ltag: Extending lexicalized
tag to discourse. Cognitive Science, 28(5):751–779.

Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
PrasadO Christopher Bryant, and Attapol T Ruther-
ford. 2015. The conll-2015 shared task on shal-
low discourse parsing. In Proceedings of CoNLL,
page 2.

Rong Zhao and William I Grosky. 2002. Narrow-
ing the semantic gap-improved text-based web doc-
ument retrieval using visual features. Multimedia,
IEEE Transactions on, 4(2):189–200.

Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recog-
nition. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 1507–1514. Association for Computational
Linguistics.

1735


