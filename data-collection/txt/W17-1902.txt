



















































Automated WordNet Construction Using Word Embeddings


Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 12–23,
Valencia, Spain, April 4 2017. c©2017 Association for Computational Linguistics

Automated WordNet Construction Using Word Embeddings

Mikhail Khodak, Andrej Risteski, Christiane Fellbaum and
Sanjeev Arora

Computer Science Department, Princeton University, 35 Olden St., Princeton, New Jersey 08540

{mkhodak,risteski,fellbaum,arora}@cs.princeton.edu

Abstract

We present a fully unsupervised method
for automated construction of WordNets
based upon recent advances in distribu-
tional representations of sentences and
word-senses combined with readily avail-
able machine translation tools. The ap-
proach requires very few linguistic re-
sources and is thus extensible to multiple
target languages. To evaluate our method
we construct two 600-word test sets for
word-to-synset matching in French and
Russian using native speakers and evalu-
ate the performance of our method along
with several other recent approaches. Our
method exceeds the best language-specific
and multi-lingual automated WordNets in
F-score for both languages. The databases
we construct for French and Russian, both
languages without large publicly available
manually constructed WordNets, will be
publicly released along with the test sets.

1 Introduction

A WordNet is a lexical database for languages
based upon a structure introduced by the Princeton
WordNet (PWN) for English in which sets of cog-
nitive synonyms, or synsets, are interconnected
with arcs standing for semantic and lexical rela-
tions between them (Fellbaum, 1972). WordNets
are widely used in computational linguistics, in-
formation retrieval, and machine translation. Con-
structing one by hand is time-consuming and dif-
ficult, motivating a search for automated or semi-
automated methods. We present an unsupervised
method based on word embeddings and word-
sense induction and build and evaluate WordNets
for French and Russian. Our approach needs only
a large unannotated corpus like Wikipedia in the

target language and machine translation (MT) be-
tween that language and English.

A standard minimal WordNet design is to have
synsets connected by hyponym-hypernym rela-
tions and linked back to PWN (Global WordNet
Association, 2017). This allows for applications
to cross-lingual tasks and rests on the assumption
that synsets and their relations are invariant across
languages (Sagot and Fišer, 2008). For example,
while all senses of English word tie may not line
up with all senses of French word cravate, the
sense “necktie” will exist in both languages and
be represented by the same synset.

Thus MT is often used for automated Word-
Nets to generate a set of candidate synsets for each
word w in the target language by getting a set of
English translations of w and using them to query
PWN (we will refer to this as MT+PWN). The
number of candidate synsets produced may not be
small, even as large as a hundred for some polyse-
mous verbs. Thus one needs a way to select from
the candidates of w those synsets that are its true
senses. The main contributions of this paper is a
new word embedding-based method for matching
words to synsets and the release of two large word-
synset matching test sets for French and Russian.1

Though there has been some work using word-
vectors for WordNets (see Section 2), the result-
ing databases have been small, containing less
than 1000 words. Using embeddings for this task
is challenging due to the need for good ways to
use PWN synset information and account for the
breakdown of cosine-similarity for polysemous
words. We approach the first issue by representing
synset information using recent work on sentence-
embeddings by Arora et al. (2017). To handle pol-
ysemy we devise a sense clustering scheme based
on Word Sense Induction (WSI) via linear alge-

1 https://github.com/mkhodak/pawn

12



bra over word-vectors (Arora et al., 2016a). We
demonstrate how this sense purification procedure
effectively combines clustering with embeddings,
thus being applicable to many word-sense disam-
biguation (WSD) and induction-related tasks. Us-
ing both techniques yields a WordNet method that
outperforms other language-independent methods
as well as language-specific approaches such as
WOLF, the French WordNet used by the Natural
Language ToolKit (Sagot and Fišer, 2008; Bond
and Foster, 2013; Bird et al., 2009).

Our second contribution is the creation of two
new 600-word test sets in French and Russian that
are larger and more comprehensive than any cur-
rently available, containing 200 each of nouns,
verbs, and adjectives. They are constructed by pre-
senting native speakers with all candidate synsets
produced as above by MT+PWN and treating the
senses they pick out as “ground truth” for measur-
ing precision and recall. The motivation behind
separating by part-of-speech (POS) is that nouns
are often easier than adjectives and verbs, so re-
porting one number — as done by some past work
— allows high noun performance to mask low per-
formance on adjectives and verbs.

Using these test sets, we can begin addressing
the difficulties of evaluation for non-English auto-
mated WordNets due to the use of different and
unreported test data, incompatible metrics (e.g.
matching synsets to words vs. retrieving words for
synsets), and differing cross-lingual dictionaries.
In this paper we use the test sets to evaluate our
method and several other automated WordNets.

2 Related Work

There have been many language-specific ap-
proaches for building automated WordNets, no-
tably for Korean (Lee et al., 2000), French (Sagot
and Fišer, 2008; Pradet et al., 2013), and Persian
(Montazery and Faili, 2010). These approaches
also use MT+PWN to get candidate word-synset
pairs, but often use further resources — such as
bilingual corpora, expert knowledge, or WordNets
in related languages — to select correct senses.

The Korean construction depends on a classifier
trained on 3260 word-sense matchings that yields
93.6% precision and 77.1% recall, albeit only on
nouns. The Persian WordNet uses a scoring func-
tion based on related words between languages
(requiring expert knowledge and parallel corpora)
and achieves 82.6% precision, though without re-

porting recall and POS-separated statistics.
The most comparable results to ours are from

the Wordnet Libre du Français (WOLF) of Sagot
and Fišer (2008), which leverages multiple Euro-
pean WordNet projects. Our best method exceeds
this approach on our test set and benefits from hav-
ing far fewer resource requirements. The Wordnet
du Français (WoNeF) of Pradet et al. (2013) de-
pends on combining linguistic models by a voting
scheme. Their performance is found to be gener-
ally below WOLF’s, so we compare to the latter.

There has also been work on multi-language
WordNets, specifically the Extended Open Multi-
lingual Wordnet (OMW) (Bond and Foster, 2013),
which scraped Wiktionary, and the Universal Mul-
tilingual Wordnet (UWN) (de Melo and Weikum,
2009), which used multiple translations to rate
word-sense matches. In our evaluation both pro-
duce high-precision/low-coverage WordNets.

Finally, there have been recent vector ap-
proaches for an Arabic WordNet (Tarouti and
Kalita, 2016) and a Bengali WordNet (Nasirud-
din et al., 2014). The Arabic effort uses a cosine-
similarity threshold for correcting direct transla-
tion and reports a precision of 78.4% on synonym
matching, although its small size (943 synsets)
indicates a poor precision/recall trade-off. The
Bengali WordNet paper examines WSI on word
vectors, evaluating clustering methods on seven
words and achieving F1-scores of at best 52.1%. It
is likely that standard clustering techniques are in-
sufficient when one needs many thousands of clus-
ters, an issue we address via sparse coding.

Our use of distributional word embeddings to
construct WordNets is the latest in a long line of
their applications, e.g. approximating word simi-
larity and solving word-analogies (Mikolov et al.,
2013). The latter discovery was cited as the in-
spiration for the theoretical model in Arora et al.
(2016b), whose Squared-Norm (SN) vectors we
use; the computation is similar in form and per-
formance to GloVe (Pennington et al., 2014).

3 Methods for WordNet Construction

The basic WordNet method is as follows. Given a
target word w, we use a bilingual dictionary to get
its translations in English and let its set of candi-
date synsets be all PWN senses of the translations
(MT+PWN). We then assign a score to each synset
and accept as correct all synsets with score above
a threshold α If no synset is above the cutoff we

13



assign a score
to each

candidate synset 

French-English
Dictionary

French word

'dalle'
Princeton
WordNetflagstone

flag

slab

�
�
�
�
�
�
�
�

correct synsets:
flag.n.06 -
slab.n.01 �

retrieved synsets:
slab.n.01 �

Target
Word

get translations of target 
word       using a 

bilingual dictionary or 
machine translation (MT)

get set of candidate synsets
by querying Princeton 
WordNet (PWN) using 

translations of     

Translations

MT + PWN Synset
Scoring

Threshold
Matching

return all synsets
with score above
a threshold      

Candidates SynsetScore
Threshold

          0.41  

flag.n.01
flag.n.04
flag.n.06
flag.n.07
iris.n.01

masthead.n.01
pin.n.08
slab.n.01

score = 0.280
score = 0.222
score = 0.360
score = 0.161
score = 0.200
score = 0.195
score = 0.251
score = 0.521

Figure 1: The score-threshold procedure for French word w = dalle (flagstone, slab). Candidate synsets
generated by MT+PWN are given a score and matched to w if the score is above a threshold α.

accept only the highest-scoring synset. This score-
threshold method is illustrated in Figure 1.

Thus we need methods to assign high scores to
correct candidate synsets of w and low scores to
incorrect ones. We use unsupervised word vec-
tors in the target language computed from a large
text corpus (e.g. Wikipedia). Section 3.1 presents
a simple baseline that improves upon MT+PWN
via a cosine-similarity metric between w and each
synset’s lemmas. A more sophisticated synset rep-
resentation method using sentence-embeddings is
described in Section 3.2. Finally, in Section 3.3 we
discuss WSI-based procedures for improving the
score-threshold method for words with fine sense
distinctions or poorly-annotated synsets.

In this section we assume a vocabulary V of tar-
get language words with associated d-dimensional
unit word-vectors vw ∈ Rd for d � |V | (e.g.
d = 300 for vocabulary size 50000) trained on a
large text corpus. Each word w ∈ V also has a set
of candidate synsets found by MT+PWN. We call
synsets S, S′ in PWN related, denoted S ∼ S′, if
one is a hyponym, meronym, antonym, or attribute
of the other, if they share a verb group, or S = S′.

3.1 Baseline: Average Similarity Method

This method for scoring synsets can be seen as a
simple baseline. Given a candidate synset S, we
define TS ⊂ V as the set of translations of its
lemmas from English to the target language. The
score of S is then 1|TS |

∑
w′∈TS vw · vw′ , the av-

erage cosine similarity between w and the trans-
lated lemmas of S. Although straightforward, this
scoring method is quite noisy, as averaging word-
vectors dilutes similarity performance, and does
not use all synset information provided by PWN.

3.2 Method 1: Synset Representation
To improve upon this baseline we need a better
vector representation of S to score S via cosine
similarity with vw. Previous efforts in synset and
sense embeddings (Iacobacci et al., 2015; Rothe
and Schütze, 2015) often use extra resources such
as WordNet or BabelNet for the target language
(Navigli and Ponzetto, 2012). As such databases
are not always available, we propose a synset rep-
resentation uS that is unsupervised, needing no ex-
tra resources beyond MT and PWN, and leverages
recent work on sentence embeddings.

This new representation combines embeddings
of synset information given by PWN, e.g. synset
relations, definitions, and example sentences. To
create these embeddings we first consider the
question of how to represent a list of words L as a
vector in Rd. One way is to simply take the nor-
malized sum v̂(SUM)L of their word-vectors, where

v
(SUM)
L =

∑
w′∈L

vw′

Potentially more useful is to compute a vector
v̂

(SIF )
L via the sentence embedding formula of

Arora et al. (2017), based on smooth inverse fre-
quency (SIF) weighting, which (for a = 10−4 and
before normalization) is expressed as

v
(SIF )
L =

∑
w′∈L

a

a+ P{w′}vw′

SIF is similar in spirit to TF-IDF (Salton and
Buckley, 1988) and builds on work of Wieting et
al. (2016); it has been found to perform well on
other similarity tasks (Arora et al., 2017).

We find that SIF improves performance on sen-
tences but not on translated lemma lists (Figure 2),

14



likely because sentences contain many distractor
words that SIF will weight lower while the pres-
ence of distractors among lemmas is independent
of word frequency. Thus to compute the synset
score uS · vw we make the vector representation
uS of S the element-wise average of:

• v̂(SUM)TS
TS is the set of translations of
lemmas of S (as in Section 3.1).

• v̂(SUM)RS
RS =

( ⋃
S′∼S

TS′

)
\TS is the

set of lemma translations of
synsets S′ related to S.

• v̂(SIF )DS DS is the list of tokens in thetranslated definition of S.

• 1|Es|
∑
E∈ES

v̂
(SIF )
E

ES contains the lists of tokens
in the translated example sent-
ences of S (excluded if S has
no example sentences).

3.3 Method 2: Better Matching Using WSI

We found through examination that the score-
threshold method we have used so far performs
poorly in two main cases:

(a) the word w has no candidate synset with
score high-enough to clear the threshold.

(b) w has multiple closely related synsets that are
all correct matches but some of which have a
much lower score than others.

Here we address both issues by using sense infor-
mation found by applying a word-sense induction
method first introduced in Arora et al. (2016a).

We summarize their WSI-model – referred to
henceforth as Linear-WSI — in Section 3.3.1.
Then in Section 3.3.2 we devise a sense purifica-
tion procedure for constructing a word-cluster for
each induced sense of a word. Applying this pro-
cedure to construct word-cluster representations of
candidates synsets provides an additional metric
for the correctness of word-synset matches that
can be used to devise a w-specific threshold αw to
ameliorate problem (a). Meanwhile, using Linear-
WSI to associate similar candidate synsets of w to
each other provides a way to address problem (b).
We explain these methodologies in Section 3.3.3.

3.3.1 Summary of Linear-WSI Model
In Arora et al. (2016a) the authors posit that the
vector of a polysemous word can be linearly de-
composed into vectors associated to its senses.

Avg. Similarity
(Baseline)

Unweighted 
Summation

SIF-Weighted
Summation

Synset Rep.
(Method 1)

Synset Lemmas

Synset Lemmas +
Related Lemmas

Synset Definition

Synset Definition +
Example Sentences

All Synset Information

Average Lemma
Similarity

Synset Representation

0.55 0.60 0.65

Figure 2: F -score comparison between using un-
weighted summation and SIF-weighted summa-
tion for embedding PWN synset information.

Thus for w = tie — which can be an article of
clothing, a drawn match, and so on — we would
have vw ≈ avw(clothing) + bvw(match) + . . . for
a, b ∈ R. It is unclear how to find such sense-
vectors, but one expects different words to have
closely related sense-vectors, e.g. for w′ = bow
the vector vw′(clothing) would be close to the vec-
tor vw(clothing) of tie. Thus the Linear-WSI model
proposes using sparse coding, namely finding a set
of unit basis vectors a1, . . . , ak ∈ Rd s.t. ∀w ∈ V ,

vw =
k∑
i=1

Rwiai + ηw, (1)

for k > d, ηw a noise vector, and at most s coef-
ficients Rwi nonzero. The hope is that the sense-
vector vw(clothing) of tie is in the neighborhood of
a vector ai s.t. Rwi > 0. Indeed, for k = 2000 and
s = 5, Arora et al. (2016a) report that solving (1)
represents English word-senses as well as a com-
petent non-native speaker and significantly better
than older clustering methods for WSI.

3.3.2 Sense Purification

While (1) is a good WSI method, its use for build-
ing WordNets is hampered by its inability to pro-
duce more than a few thousand senses ai, as set-
ting a large k yields repetitive rather than different
senses. As this is far fewer than the number of
synsets in PWN, we use sense purification to ad-
dress this by extracting a cluster of words related
to ai as well as w to represent each sense. In ad-
dition to w and ai, the procedure takes as input a
search-space V ′ ⊂ V and set size n. Then we find

15



(pistol)

(shooting)

(to shoot)

(species)

(crossbow)
(kind)

(family)

(plant)

(coriander)

(cilantro)

(celery)

(garlic)

Figure 3: Isometric mapping of sense-cluster vectors for w = brilliant, fox, and лук (bow, onion). w is
marked by a star and each sense ai of w, shown by a large marker, has an associated cluster of words
with the same marker shape. Contours are densities of vectors close to w and at least one sense ai.

C ⊂ V ′ of size n containing w that maximizes
f = min

x=ai or
x=vw′ :w′∈C

Median{x · vw′ : w′ ∈ C} (2)

A cluster C maximizing this must ensure that nei-
ther ai nor any word w′ ∈ C (including w′ = w)
has low average cosine similarity with cluster-
words, resulting in a dense cluster close to both
w and ai. We explain this further in Appendix A.

We illustrate this method in Figure 3 by pu-
rifying the senses of English words brilliant
and fox and Russian word лук, which has the
senses “bow” (weapon), “onion” (vegetable), and
“onion” (plant). Note how correct senses are re-
covered across POS and language and for proper
and common noun senses.

3.3.3 Applying WSI to Synset Matching
The problem addressed by sense purification is
that senses ai induced by Linear-WSI have too
many related words; purification solves this by
extracting a cluster of words related to w from
the words close to ai. When translating WordNet
synsets, we have a similar problem in that transla-
tions of a synset’s lemmas may not be relevant to
the synset itself. Thus we can try to create a puri-
fied representation of each candidate synset S ofw
by extracting a cluster of translated lemmas close
to w and one of its induced senses. We run purifi-
cation on every sense ai in the sparse representa-
tion (1) of w, using as a search-space V ′ = VS the
set of translations of all lemmas of synsets S′ re-
lated to S in PWN (i.e. S′ ∼ S as defined in Sec-
tion 3). To each synset S we associate the sense
aS and corresponding cluster CS that are optimal
in the objective (2) among all senses ai of w.

Although we find a sense aS and purified repre-
sentation CS for each candidate synset of w, we

note that an incorrect synset is likely to have a
lower objective value (2) than a correct synset as
it likely has fewer words related to w in its search-
space VS . However, using fS = f(w, aS , CS) as a
synset score is difficult as some synsets have very
small search-spaces, leading to inconsistent scor-
ing even for correct synsets.

Instead we use fS as part of a w-specific thresh-
old αw = min{α, uS∗ · vw}, where uS is the
vector representation of S from Section 3.2 and
S∗ = arg max fS + uS · vw. This attempts to ad-
dress problem (a) of the score-threshold method
— that some words have no synsets above the cut-
off α and returning only the highest-scoring synset
in such cases will not retrieve multiple sense for
polysemous words. By the construction of αw, if
w is polysemous, a synset other than the one with
the highest score uS ·vw may have a better value of
fS and thus found to be S∗; then all synsets with
higher scores will be matched to w, allowing for
multiple matches even if no synset clears the cut-
off α. Otherwise if w is monosemous, we expect
the correct synset S to have the highest value for
both uS · vw and fS , making S∗ = S. Then if no
synset has score greater than α the threshold will
be set to uS · vw, the highest synset-score, so only
the highest scoring synset will be matched to w.

To address problem (b) of the threshold method
— that w might have multiple correct and closely
related candidate synsets of which only some
clear the cutoff — we observe that closely related
synsets of w will have similar search-spaces VS
and so are likely to be associated to the same sense
ai. For example, in Figure 4 the candidates of
dalle related to its correct meanings as a flagstone
or slab are associated to the same sense a892 while
distractor synsets related to the incorrect sense as a

16



Candidates SynsetScore
Threshold

            0.50 
Associated

Sense

French word

'dalle'

get a set of candidate 
synsets by querying 

Princeton WordNet using 
translations of       

MT + PWN ThresholdMatching
use sense purification to

associate a sense      
to each candidate synset

match to     all
candidate synsets 
with score above
a threshold       

�
�

Sense-Association
by Purification

Sense-Agglomeration
Procedure

perform cutoff matching with 
a lower threshold      on all 
synsets sharing a sense       

with an already matched synset 

target
word

Threshold
        0.25

correct synsets:
flag.n.06 �
slab.n.01 �

retrieved synsets:
flag.n.06 �
slab.n.01 �

sense =
sense =
sense =
sense =
sense =
sense =
sense =
sense =

�
�
�
�
�
�
�
�

flag.n.01
flag.n.04
flag.n.06
flag.n.07
iris.n.01

masthead.n.01
pin.n.08
slab.n.01

score = 0.280
score = 0.222
score = 0.360
score = 0.161
score = 0.200
score = 0.195
score = 0.251
score = 0.521

Figure 4: The score-threshold and sense-agglomeration procedure for French word w = dalle (flagstone,
slab). Candidate synsets are given a score and matched to w if they clear a high threshold αw (as in
Section 3.2). If an unmatched synset shares a sense ai with a matched synset, it is compared to a low
threshold β (the sense-agglomeration procedure in Section 3.3.3).

flag are mostly matched to other senses. This mo-
tivates the sense agglomeration procedure, which
uses threshold-clearing synsets to match synsets
with the same sense below the score-threshold.
For β < α, the procedure is roughly as follows:

1. Run score-threshold method with cutoff αw.

2. For each synset S with score uS · vw below
the threshold, check for a synset S′ with score
uS′ · vw above the threshold and aS′ = aS .

3. If uS ·vw ≥ β and the clustersCS , CS′ satisfy
a cluster similarity condition, match S to w.

We include the lower cutoff β because even sim-
ilar synsets may not both be senses of the same
word. The cluster similarity condition, available in
Appendix B.2, ensures the relatedness of synsets
sharing a sense ai, as an erroneous synset S′ may
be associated to the same sense as a correct one.

In summary, the improved synset matching ap-
proach has two steps: 1-conduct score-threshold
matching using the modified threshold αw; 2-run
the sense-agglomeration procedure on all senses
ai of w having at least one threshold-clearing
synset S. Although for fixed α both steps focus
on improving the recall of the threshold method, in
practice they allow α to be higher, so that both pre-
cision and recall are improved. For example, note
the recovery of a correct synset left unmatched by
the score-threshold method in the simplified de-
piction of sense-agglomeration shown in Figure 4.

4 Evaluation of Methods

We evaluate our method’s accuracy and coverage
by constructing and testing WordNets for French
and Russian. For both we train 300-dimensional
SN word embeddings (Arora et al., 2016b) on
co-occurrences of words occurring at least 1000
times, or having candidate PWN synsets and oc-
curring at least 100 times, in the lemmatized
Wikipedia corpus. This yields |V | ≈ 50000. For
Linear-WSI we run sparse coding with sparsity
s = 4 and basis-size k = 2000 and use set-size
n = 5 for purification. To get candidate synsets
we use Google and Microsoft Translators and the
dictionary of the translation company ECTACO,
while for sentence-length MT we use Microsoft.

4.1 Testsets

A common way to evaluate accuracy of an auto-
mated WordNet is to compare its synsets or word
matchings to a manually-constructed one. How-
ever, the existing ELRA French Wordnet 2 is not
public and half the size of ours while Russian
WordNets are either even smaller and not linked
to PWN3 or obtained via direct translation4.

Instead we construct test sets for each language
that allow for evaluation of our methods and oth-
ers. We randomly chose 200 each of adjectives,
nouns, and verbs from the set of target language
words whose English translations appear in the
synsets of the Core WordNet. Their “ground truth”

2 http://catalog.elra.info/product_
info.php?products_id=550

3 http://project.phil.spbu.ru/RussNet/
4 http://wordnet.ru/

17



Method POS F.5-Score∗ F1-Score∗ Precision∗ Recall∗ Coverage Synsets α β

Direct Translation
(MT + PWN)

Adj. 50.3 59.3 46.1 100.0 99.9 11271
Noun 56.2 64.6 52.2 100.0 100.0 74477
Verb 41.4 51.3 37.0 100.0 100.0 13017
Total 49.3 58.4 45.1 100.0 100.0 100174†

Wordnet Libre
du Français

(WOLF)
(Sagot and Fišer, 2008)

Adj. 66.3 58.6 78.1 53.4 84.8 6865
Noun 68.6 58.7 83.2 51.5 95.0 36667
Verb 60.8 48.4 81.0 39.6 88.2 7671
Total 65.2 55.2 80.8 48.2 92.2 52757†

Universal Wordnet
(de Melo and Weikum, 2009)

Adj. 64.5 51.5 88.3 42.3 69.2 7407
Noun 67.5 52.2 94.1 40.8 75.9 24670
Verb 55.4 39.5 88.0 28.5 76.2 5624
Total 62.5 47.7 90.1 37.2 75.0 39497†

Extended Open
Multilingual Wordnet

(Bond and Foster, 2013)

Adj. 58.4 40.8 90.9 28.4 54.7 2689
Noun 61.3 43.8 96.5 31.7 66.6 14936
Verb 47.8 29.4 95.9 18.6 57.7 2331
Total 55.9 38.0 94.5 26.2 63.2 20449†

Baseline:
Average Similarity

(Section 3.1)

Adj. 62.8±0.0 62.6±0.0 65.3±0.0 68.5±0.0 88.7 9687 0.31
Noun 67.3±0.0 65.4±0.1 71.6±0.1 69.0±0.1 92.2 37970 0.27
Verb 51.8±0.0 50.8±0.1 55.9±0.1 57.0±0.1 83.5 10037 0.26
Total 60.6±0.0 59.6±0.0 64.3±0.0 64.9±0.0 90.0 58962†

Method 1:
Synset Representation

(Section 3.2)

Adj. 65.9±0.0 60.4±0.0 75.9±0.1 59.5±0.1 85.1 8512 0.47
Noun 71.0±0.0 67.3±0.1 78.7±0.1 69.1±0.1 96.7 35663 0.41
Verb 61.6±0.0 53.0±0.0 78.7±0.1 49.8±0.1 89.9 8619 0.45
Total 66.2±0.0 60.2±0.0 77.8±0.0 59.5±0.1 93.7 53852†

Method 2:
Synset Representation

+ Linear-WSI
(Section 3.3)

Adj. 67.7±0.0 62.5±0.1 76.9±0.1 62.6±0.1 91.2 8912 0.56 0.42
Noun 73.0±0.0 66.0±0.1 83.7±0.1 62.0±0.2 90.9 34001 0.50 0.25
Verb 64.4±0.0 55.9±0.0 79.3±0.0 51.5±0.1 93.6 9262 0.46 0.28
Total 68.4±0.0 61.5±0.0 80.0±0.0 58.7±0.1 91.5 53208†

∗Micro-averages over a randomly held-out half of the data; parameters tuned on the other half. 95% asymptotic confidence
intervals found with 10000 randomized trials.
† Includes adverb synsets. For the last three methods they are matched with the same parameter values (α and β) as for adjectives.

Table 1: French WordNet Results

word senses are picked by native speakers, who
were asked to perform the same matching task de-
scribed in Section 3, i.e. select correct synsets
for a word given a set of candidates generated by
MT + PWN. For example, the French word foie
has one translation, liver with four PWN synsets:
1-“glandular organ”; 2-“liver used as meat”; 3-
“person with a special life style”; 4-“someone liv-
ing in a place.” The first two align with senses
of foie while the others do not, so the expert
marks the first two as good and the others as neg-
ative. Two native speakers for each language were
trained by an author with knowledge of WordNet
and at least 10 years of experience in each lan-
guage. Inconsistencies in the matchings of the two
speakers were resolved by the same author.

We get 600 words and about 12000 candidate
word-synset pairs for each language, with adjec-
tives and nouns having on average about 15 can-
didates and verbs having about 30. These num-
bers makes the test set larger than many others,

with French, Korean, and Persian WordNets cited
in Section 2 being evaluated on 183 pairs, 3260
pairs, and 500 words, respectively. Accuracy mea-
sured with respect to this ground truth estimates
how well an algorithm does compared to humans.

A significant characteristic of this test set is
its dependence on the machine translation sys-
tem used to get candidate synsets. While this can
leave out correct synset matches that the system
did not propose, by providing both correct and in-
correct candidate synsets we allow future authors
to focus on the semantic challenge of selecting
correct senses without worrying about finding the
best bilingual dictionary. This allows dictionary-
independent evaluation of automated WordNets,
an important feature in an area where the specific
translation systems used are rarely provided in
full. When comparing the performance of our con-
struction to that of previous efforts on this test set,
we do not penalize word-synset matches in which
the synset is not among the candidate synsets we

18



generate for that word, negating the loss of pre-
cision incurred by other methods due to the use
of different dictionaries. We also do not penalize
other WordNets for test words they do not contain.

In addition to precision and recall, we report the
Coverage statistic as the proportion of the Core set
of most-used synsets, a semi-automatically con-
structed set of about 5000 PWN frequent senses,
that are matched to at least one word (Fellbaum,
1972). While an imperfect metric given different
sense usage by language, the synsets are universal-
enough for it to be a good indicator of usability.

4.2 Evaluation Results

We report the evaluation of methods in Section 3
in Tables 1 & 2 alongside evaluations of UWN
(de Melo and Weikum, 2009), OWM (Bond and
Foster, 2013), and WOLF (Sagot and Fišer, 2008).
Parameters α and β were tuned to maximize the
micro-averaged F.5-score 1.25·Precision·Recall.25·Precision+Recall , used
instead of F1 to prioritize precision, which is of-
ten more important for application purposes.

Our synset representation method (Section 3.2)
exceeds the similarity baseline by 6% in F.5-score
for French and 10% for Russian. For French it is
competitive with the best other WordNet (WOLF)
and in both languages exceeds both multi-lingual
WordNets. Improving this method via Linear-WSI
(Section 3.3) leads to 2% improvement in F.5-
score for French and 1% for Russian. Our methods
also perform best in F1-score and Core coverage.

As expected from a Wiktionary-scraping
method, OMW achieves the best precision
across languages, although it and UWN have
low recall and Core coverage. The performance
of our best method for French exceeds that of
WOLF in F.5-score across POS while achieving
similar coverage. WOLF’s recall performance
is markedly lower than the evaluation in Sagot
and Fišer (2008, Table 4); we believe this stems
from our use of words matched to Core synsets,
not random words, leading to a more difficult
test set as common words are more-polysemous
and have more synsets to retrieve. There is no
comparable automated Russian-only WordNet,
with only semi-automated and incomplete efforts
(Yablonsky and Sukhonogov, 2006).

Comparing across POS, we do best on nouns
and worst on verbs, likely due to the greater pol-
ysemy of verbs. Between languages, performance
is similar for adjectives but slightly worse on Rus-

sian nouns and much worse on Russian verbs.
The discrepancy in verbs can be explained by a

difference in treating the reflexive case and aspec-
tual variants due to the grammatical complexity of
Russian verbs. In French, making a verb reflexive
requires adding a word while in Russian the verb
itself changes, e.g. to wash→to wash oneself is
laver→se laver in French but мыть→мыться in
Russian. Thus we do not distinguish the reflex-
ive case for French as the token found is the same
but for Russian we do, so both мыть and мыть-
ся may appear and have distinct synset matches.
Thus matching Russian verbs is challenging as the
reflexive usage of a verb is often contextually sim-
ilar to the non-reflexive usage. Another compli-
cation for Russian verbs is due to aspectual verb
pairs; thus to do has aspects (делать, сделать)
in Russian that are treated as distinct verbs while
in French these are just different tenses of the verb
faire. Both factors pose challenges for differentiat-
ing Russian verb senses by a distributional model.

Overall however the method is shown to be ro-
bust to how close the target language is to English,
with nouns and adjectives performing well in both
languages and the difference for verbs stemming
from an intrinsic quality rather than dissimilarity
with English. This can be further examined by
testing the method on a non-European language.

5 Conclusion and Future Work

We have shown how to leverage recent advances
in word embeddings for fully-automated WordNet
construction. Our best approach combining sen-
tence embeddings, and recent methods for WSI
obtains performance 5-16% above the naive base-
line in F.5-score as well as outperforming previ-
ous language-specific and multi-lingual methods.
A notable feature of our work is that we require
only a large corpus in the target language and au-
tomated translation into/from English, both avail-
able for many languages lacking good WordNets.

We further contribute new 600-word human-
annotated test sets split by POS for French and
Russian that can be used to evaluate future au-
tomated WordNets. These larger test sets give a
more accurate picture of a construction’s strengths
and weaknesses, revealing some limitations of
past methods. With WordNets in French and Rus-
sian largely automated or incomplete, the Word-
Nets we build also add an important tool for multi-
lingual natural language processing.

19



Method POS F.5-Score∗ F1-Score∗ Precision∗ Recall∗ Coverage Synsets α β

Direct Translation
(MT + PWN)

Adj. 50.2 59.6 45.9 100.0 99.6 11412
Noun 41.2 50.2 37.1 100.0 100.0 73328
Verb 32.5 41.7 28.6 100.0 100.0 13185
Total 41.3 50.5 37.2 100.0 99.9 99470†

Universal Wordnet
(de Melo and Weikum, 2009)

Adj. 52.4 38.8 80.3 29.6 51.0 11412
Noun 65.0 53.0 87.5 45.1 71.1 19564
Verb 48.1 34.8 74.8 25.7 65.0 3981
Total 55.1 42.2 80.8 33.4 67.1 30015†

Extended Open
Multilingual Wordnet

(Bond and Foster, 2013)

Adj. 58.7 41.3 91.7 29.2 55.3 2419
Noun 67.8 53.1 93.5 42.5 68.4 14968
Verb 51.1 34.8 84.5 23.9 56.6 2218
Total 59.2 43.1 89.9 31.9 64.2 19983†

Baseline:
Average Similarity

(Section 3.1)

Adj. 61.4±0.0 64.6±0.1 60.9±0.0 77.3±0.1 92.1 10293 0.24
Noun 55.9±0.0 54.8±0.1 59.9±0.1 59.9±0.1 77.0 32919 0.29
Verb 46.3±0.0 46.5±0.1 49.0±0.1 55.1±0.1 84.1 9749 0.21
Total 54.5±0.0 55.3±0.0 56.6±0.0 64.1±0.1 80.5 54372†

Method 1:
Synset Representation

(Section 3.2)

Adj. 69.5±0.0 64.1±0.0 78.1±0.0 61.7±0.1 84.2 8393 0.43
Noun 69.8±0.0 65.5±0.0 77.6±0.1 66.0±0.1 85.2 29076 0.46
Verb 54.2±0.0 51.1±0.1 63.3±0.1 57.4±0.1 91.2 8303 0.39
Total 64.5±0.0 60.2±0.0 73.0±0.0 61.7±0.1 86.3 46911†

Method 2:
Synset Representation

+ Linear-WSI
(Section 3.3)

Adj. 69.7±0.0 64.9±0.1 77.3±0.0 63.6±0.1 93.3 9359 0.43 0.35
Noun 71.6±0.0 67.6±0.0 78.1±0.0 68.0±0.1 91.0 31699 0.46 0.33
Verb 54.4±0.0 49.7±0.1 64.9±0.1 52.6±0.2 91.9 8582 0.44 0.33
Total 65.2±0.0 60.7±0.0 73.4±0.0 61.4±0.1 91.5 50850†

∗Micro-averages over a randomly held-out half of the data; parameters tuned on the other half. 95% asymptotic confidence
intervals found with 10000 randomized trials.
† Includes adverb synsets. For the last three methods they are matched with the same parameter values (α and β) as for adjectives.

Table 2: Russian WordNet Results

Further improvement to our work may come
from other methods in word-embeddings, such
as multi-lingual word-vectors (Faruqui and Dyer,
2014). Our techniques can also be combined with
others, both language-specific and multi-lingual,
for automated WordNet construction. In addition,
our method for associating multiple synsets to the
same sense can contribute to efforts to improve
PWN through sense clustering (Snow et al., 2007).
Finally, our sense purification procedure, which
uses word-vectors to extract clusters representing
word-senses, likely has further WSI and WSD ap-
plications; such exploration is left to future work.

Acknowledgments

We thank Angel Chang and Yingyu Liang for
helpful discussion at various stages of this pa-
per. This work was supported in part by NSF
grants CCF-1302518, CCF-1527371, Simons In-
vestigator Award, Simons Collaboration Grant,
and ONR-N00014-16-1-2329.

References

Michal Aharon, Michael Elad, and Alfred Bruckstein.
2006. K-svd: An algorithm for designing overcom-
plete dictionaries for sparse representation. IEEE
Transactions on Signal Processing, 54(11).

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
and Andrej Risteski. 2016a. Linear algebraic struc-
ture of word sense, with applications to polysemy.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
and Andrej Risteski. 2016b. Rand-walk: A la-
tent variable model approach to word embeddings.
Transactions of the Association for Computational
Linguistics, 4:385–399.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In International Conference on Learning
Representations. To Appear.

Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O’Reilly Media Inc.

Francis Bond and Ryan Foster. 2013. Linking and ex-
tending an open multilingual wordnet. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics.

20



Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management.

Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics.

Christiane Fellbaum. 1972. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.

Global WordNet Association. 2017. Wordnets in the
world.

Ignaci Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity. In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics.

Changki Lee, Geunbae Lee, and Seo JungYun. 2000.
Automated wordnet mapping using word sense dis-
ambiguation. In Proceedings of the 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space.

Mortaza Montazery and Heshaam Faili. 2010. Auto-
matic persian wordnet construction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics.

Mohammad Nasiruddin, Didier Schwab, and Andon
Tchechmedjiev. 2014. Induction de sens pour en-
richir des ressources lexicales. In 21éme Traitement
Automatique des Langues Naturelles.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of Empirical
Methods in Natural Language Processing.

Quentin Pradet, Gaël de Chalendar, and Jeanne Bague-
nier Desormeaux. 2013. Wonef, an improved, ex-
panded and evaluated automatic french translation
of wordnet. In Proceedings of the Seventh Global
Wordnet Conference.

Sascha Rothe and Hinrich Schütze. 2015. Autoex-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics.

Benoı̂t Sagot and Darja Fišer. 2008. Building a free
french wordnet from multilingual resources. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation Conference.

Gerald Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing & Management, 24(5).

Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning.

Feras Al Tarouti and Jugal Kalita. 2016. Enhancing
automatic wordnet construction using word embed-
dings. In Proceedings of the Workshop on Multilin-
gual and Cross-lingual Methods in NLP.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Towards universal paraphrastic sen-
tence embeddings. In International Conference on
Learning Representations.

Sergey Yablonsky and Andrej Sukhonogov. 2006.
Semi-automated english-russian wordnet construc-
tion: Initial resources, software and methods of
translation. In Proceedings of the Global WordNet
Conference.

A Purification Procedure

As discussed in Section 3.3.1, the Linear-WSI
model (Arora et al., 2016a) posits that there ex-
ists an overcomplete basis a1, . . . , ak ∈ Rd of unit
vectors such that each wordw ∈ V can be approx-
imately represented by a linear combination of at
most s basis vectors (see Equation 1). Finding the
basis vectors ai and their coefficientsRwi requires
solving the optimization problem

minimize ‖Y −RA‖2
subject to ‖Rw‖0 ≤ s ∀ w ∈ V

where Y ∈ R|V |×n has word-vectors as its rows,
R ∈ R|V |×k is the matrix of coefficients Rwi, and
A ∈ Rk×d has the overcomplete basis a1, . . . , ak
as its rows. This problem is non-convex and can
be solved approximately via the K-SVD algorithm
(Aharon et al., 2006).

Given a word w, the purification procedure is
a method for purifying the senses induced via
Linear-WSI (the vectors ai s.t. Rwi 6= 0) by rep-
resenting them as word-clusters related to both the
sense itself and the word. This is done so as to
create a more fine-grained collection of senses, as
Linear-WSI does not perform well for more than
a few thousand basis vectors, far fewer than the

21



number of word-senses (Arora et al., 2016a). The
procedure is inspired by the hope that words re-
lated to each sense of w will form clusters near
both vw and one of its senses ai. Given a fixed set-
size n and a search-space V ′ ⊂ V , we realize this
hope via the optimization problem

maximize
C⊂V ′

γ

subject to

γ ≤ Median{vx · vw′ : w′ ∈ C\{x}} ∀ x ∈ C
γ ≤ Median{ai · vw′ : w′ ∈ C}
w ∈ C, |C| = n

This problem is equivalent to maximizing (2) with
constraints |C| = n and w ∈ C ⊂ V ′. The ob-
jective value is constrained to be the lowest me-
dian cosine similarity between any word x ∈ C
or the sense ai and the rest of C, so optimizing it
ensures that the words in C are closely related to
each other and to ai. Forcing the cluster to contain
w leads to the words in C being close to w as well.

For computational purposes we solve this prob-
lem approximately using a greedy algorithm that
starts with C = {w} and repeatedly adds the word
in V ′\C that results in the best objective value un-
til |C| = n. Speed of computation is also a reason
for using a search-space V ′ ⊂ V rather than the
entire vocabulary as a source of words for the clus-
ter; we found that restricting V ′ to be all words w′

s.t. min{vw′ · vw, vw′ · ai} ≥ .2 dramatically re-
duces processing time with little performance loss.

The purification procedure represents only one
sense of w, so to perform WSI we generate clus-
ters for all senses ai s.t. Rwi > 0. If two senses
have clusters that share a word other than w, only
the cluster with the higher objective value is re-
turned. To find the clusters displayed in Figure 3
we use this procedure with cluster size n = 5 on
English and Russian SN vectors decomposed with
basis size k = 2000 and sparsity s = 4.

B Applying WSI to Synset Matching

We use the purification procedure to represent the
candidate synsets of a wordw by clusters of words
related to w and one if its senses ai. First, for each
synset S we define the search-space

VS =
⋃
S′∼S

TS′

where TS′ is the set of translations of lemmas of
S′ as in Section 3.1. Then given a word w, one of

its candidate synsets S, and a fixed set size n, we
run the following procedure:

1. For each sense ai in the sparse representation
(1) of w let Ci be the output cluster of the
purification procedure run on sense ai with
search-space V ′ = VS .

2. Return the (sense, sense-cluster) pair
(aS , CS) with the highest purification
procedure objective value among senses ai.

In the following methods we will assume that each
candidate synset S of w has a sense aS and sense-
cluster CS associated to it in this way. Examples
of such clusters for the French word dalle (flag-
stone, slab) are provided in Table 3.

B.1 Word-Specific Threshold
One application of Linear-WSI is the creation of
a word-specific threshold αw to use in the score-
threshold method instead of a global cutoff α. We
do this by using the quality of the sense cluster CS
of each candidate synset S of w as an indicator of
the correctness of that synset. Recalling that uS
is the synset representation of S as a vector (see
Section 3.2) and letting fS = f(w, aS , CS) be the
objective value (2), we find αw as follows:

1. Find S∗ = arg max
S is a candidate of w

fS + uS · vw.

2. Let αw = min{α, uS∗ · vw}.
As this modified threshold may be lower than
more than one candidate synset of w it allows for
multiple synset matches even when no synset has
score high enough to clear the threshold α.

B.2 Sense-Agglomeration Procedure
We use Linear-WSI more explicitly through the
sense-agglomeration procedure, which attempts to
recover unmatched synsets using matched synsets
sharing the same sense ai ofw. We define the clus-
ter similarity ρ between word-clusters C1, C2 ⊂
V as the median of all cosine-similarities of pairs
of words in their set product, i.e.

ρ(C1, C2) = Median{vx · vy : x ∈ C1, y ∈ C2}.
Then we say that two clusters C1, C2 are similar if

ρ(C1, C2) ≥ min{ρ(C1, C1), ρ(C2, C2)},
i.e. if their cluster similarity with each other ex-
ceeds at least one’s cluster similarity with itself.

22



Synset AssociatedSense aS
Purified Cluster Representation CS

Objective Value
f(w, aS , CS)

flag.n.01 a789 poteau (goalpost), flèche (arrow), haut (high, top), mât (matte) 0.23
flag.n.04 a892 flamme (flame), fanion (pennant), guidon, signal 0.06
flag.n.06 a892 dallage (paving), carrelage (tiling), pavement, pavage (paving) 0.36
flag.n.07 a1556 pan (section), empennage, queue, tail 0.14
iris.n.01 a1556 bœuf (beef), usine (factory), plante (plant), puant (smelly) 0.07
masthead.n.01 a1556 inscription, lettre (letter), catalogue (catalog), cotation (quotation) 0.10
pin.n.08 a1556 trou (hole), tertre (mound), marais (marsh), pavillon (house, pavillion) 0.17
slab.n.01 a892 carrelage (tiling), carreau (tile), tuile (tile), bâtiment (building) 0.27

Table 3: Purified Synset Representations of dalle (flagstone, slab). Note how the correct candidate
synsets (bolded) have clusters of words closely related to the correct meaning while the other clusters
have many unrelated words, leading to lower objective values.

Then given a global low threshold β ≤ α, for each
sense ai in the sparse representation (1), sense-
agglomeration consists of the following algorithm:

1. Let Mi be the set of candidate synsets S of w
that have aS = ai and score above the thresh-
old αw. Stop if Mi = ∅.

2. Let Ui be the set of candidate synsets S of w
that have aS = ai and score below the thresh-
old αw. Stop if Ui = ∅.

3. For each synset S ∈ Ui, ordered by synset-
score, check that CS is similar (in the above
sense) to all clusters CS′ for S′ ∈Mi and has
score higher than β. If both are true, add S to
Mi and remove S from Ui. Otherwise stop.

The sense-agglomeration procedure allows an un-
matched synset S to be returned as a correct synset
of w provided it shares a sense with a different
matched synset S′ and satisfies cluster similarity
and score-threshold constraints.

23


