



















































A New Dataset and Evaluation for Belief/Factuality


Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 82–91,
Denver, Colorado, June 4–5, 2015.

A New Dataset and Evaluation for Belief/Factuality

Vinodkumar Prabhakaran1, Tomas By2, Julia Hirschberg1, Owen Rambow3*,
Samira Shaikh4, Tomek Strzalkowski4, Jennifer Tracey5, Michael Arrigo5,

Rupayan Basu6, Micah Clark2, Adam Dalton2, Mona Diab7, Louise Guthrie2,
Anna Prokofieva1, Stephanie Strassel5, Gregory Werner7, Janyce Wiebe8, Yorick Wilks2

1Department of Computer Science, Columbia University, New York, NY, USA
2Florida Institute for Human and Machine Cognition (IHMC), FL, USA

3Center for Computational Learning Systems, Columbia University, New York, NY, USA
4State University of New York - University of Albany, NY, USA

5Linguistic Data Consortium (LDC), University of Pennsylvania, PA, USA
6Amazon.com Inc., CA, USA

7George Washington University, DC, USA
8University of Pittsburgh, PA, USA

*corresponding author; email address: rambow@ccls.columbia.edu

Abstract

The terms “belief” and “factuality” both re-
fer to the intention of the writer to present the
propositional content of an utterance as firmly
believed by the writer, not firmly believed, or
having some other status. This paper presents
an ongoing annotation effort and an associated
evaluation.

1 Introduction

This paper presents an ongoing project aimed at de-
veloping a community-wide evaluation of expressed
belief, also known as “factuality”. Belief and fac-
tuality are closely related to hedging, veridical-
ity, and modality. The project has grown out of
the DARPA DEFT project; participants include the
Linguistic Data Consortium (LDC) and three per-
former sites: Columbia University/George Wash-
ington University, the Florida Institute for Human
and Machine Cognition, and the University of Al-
bany. The goal of our research project is not lin-
guistic annotation, but the identification of meaning
which is expressed in a non-linguistic manner. Such
a meaning representation is useful for many applica-
tions; in our project we are specifically interested in

knowledge base population. A different part of the
DEFT program is concerned with the representation
of propositional meaning, following the tradition of
the ACE program in representing entities, relations
and events (ERE) (Doddington et al., 2004). The
work presented here is concerned with the attitude of
agents towards propositional content: do the agents
express a committed belief or a non-committed be-
lief in the propositional content? Our work has sev-
eral characteristics that set it apart from other work:
we are interested in annotation which can be done
fairly quickly; we are not interested in annotating
linguistic elements (such as trigger words); and we
are planning an integration with sentiment annota-
tion.

The structure of the paper is as follows: we start
out by situating our notion of “belief” with respect to
other notions of extra-propositional meaning (Sec-
tion 2); we then present our annotation in some de-
tail, with a special comparison to FactBank (Saurı́
and Pustejovsky, 2012). While the goal of this paper
is not to talk about computational systems that were
run as part of the evaluation (different publications
will be available for that purpose), we quickly sum-
marize their main characteristics so that the evalu-
ation results can be interpreted. We then turn to

82



the pilot evaluation we have performed, presenting
first the evaluation with respect to propositions (Sec-
tion 5) and then a qualitative evaluation. We con-
clude with a discussion of plans for the upcoming
open evaluation, scheduled for December 2015.

2 Terminology and Related Work

In this section, we identify how we define differ-
ent terms. Different papers may have different and
conflicting definitions of these terms, but for lack of
space we do not provide an overview over all defini-
tions.

While at first the terms “belief” and “factuality”
appear to relate to rather different things (a subjec-
tive state versus truth), in the NLP community they
in fact refer to the same phenomenon, while having
rather different connotations. The phenomenon is
the communicative intention of a writer1 to present
propositional content as something that she firmly
believes is true, weakly believes is true, or has some
other attitude towards, namely a wish or a reported
belief. The term “belief” here describes the cogni-
tive state of the writer (Diab et al., 2009), and comes
from artificial intelligence and cognitive science,
as in the Belief-Desire-Intention model of Bratman
(1999 1987). The term “factuality” describes the
communicative intention of the writer (Saurı́ and
Pustejovsky, 2012, p. 263) (our emphasis):

The fact that an eventuality is depicted
as holding or not does not mean that this
is the case in the world, but that this is
how it is characterized by its informant.
Similarly, it does not mean that this is
the real knowledge that informant has (his
true cognitive state regarding that event)
but what he wants us to believe it is.

We would like to emphasize that the terms “be-
lief” and “factuality” do not refer to the underly-
ing truth of propositions, only to the intention of the
writer to present them as, in her view, true. Thus, we
as researchers cannot determine what is true from an
analysis of factuality (or of belief). The term “factu-
ality” is often misunderstood in this respect, which

1For brevity, we will assume a female writer as the source
of utterances in this paper. Everything we say applies equally
to spoken and written communication, and equally to male and
female communicators.

is one of the reasons we prefer not to use it. In order
to understand the relation between belief/factuality
and truth, we need to distinguish two assumptions.
First, we may assume that the writer is not lying (as-
sumption of truthfulness). In this paper, we make
this first assumption. Second, we could assume that
the writer knows what is true (assumption of truth).
In this paper, we do not make this second assump-
tion. We discuss these two assumptions in turn.

We start with the assumption of truthfulness. In
the quote above, Saurı́ and Pustejovsky (2012) (apart
from distinguishing factuality from truth) also make
the point that the writer’s communicative intention
of making the reader believe she has a specific belief
state does not mean that she actually has that cogni-
tive state, since she may be lying. Lying is clearly an
important phenomenon that researchers have looked
into (Mihalcea and Strapparava, 2009; Ott et al.,
2011).2 However, we (as linguists interested in un-
derstanding how language enables communication)
feel that assuming the writer is truthful is a standard
assumption about communication which we should
in general make. This is because if we do not make
this assumption, we cannot explain why communi-
cation is possible at all, since discourse participants
would have no motivation to ever adopt another dis-
course participant’s belief as their own. We there-
fore do claim that we can infer belief from utter-
ances, while assuming that the writer is not lying,
and knowing that this assumption may be false in
certain cases.

We now turn to the second assumption, the as-
sumption of truth. Even if we assume that the
writer is not lying, the assumption of truth is not
required for communication to succeed; this is be-
cause the writer may be wrong, and this has no ef-
fect on the communication. For example, Ptolemy
successfully made many people believe that the sun
rotates around the earth, as was his (presumably)
honest communicative intention. Therefore, to us
as researchers interested in describing how language

2Sarcasm and irony differ from lying in that the communica-
tive intention and the cognitive state are aligned, but they do not
align with the standard interpretation of the utterance. Here, the
intention is that the reader recognizes that the form of the ut-
terance does not literally express the cognitive state. We leave
aside sarcasm and irony in this paper; for current computational
work on sarcasm detection, see for example (González-Ibáñez
et al., 2011).

83



is used to communicate, it does not matter that as-
tronomers now believe that Ptolemy was wrong, it
does not change our account of communication and
it does not change the communication that happened
two millennia ago. And since we do not need to
make the assumption that the writer knows what she
is talking about, we choose not to make this assump-
tion. In the case of Ptolemy, we leave this determi-
nation – what is actually true – to astronomers. In
other cases, we typically have models of trustwor-
thiness: if a writer sends her spouse a text message
saying she is hungry, the spouse has no reason to be-
lieve she is wrong. We leave this issue aside in this
paper.

The term “hedge” refers to words or phrases that
add ambiguity or uncertainty (Propositional Hedges)
or show the speakers lack of commitment to a propo-
sition (Relational Hedges). For example, The ball
is sort of blue contains a Relational Hedge (sort
of) and I think the ball is blue includes a proposi-
tional hedge (think). Propositional hedges indicate
non-committed belief. There has been a major effort
to annotate texts with hedging information (Farkas
et al., 2010), with an open evaluation. While be-
lief and hedging are closely related, we see the be-
lief/factuality annotation as more general than hedg-
ing (since it does not only include non-committed
belief), and also more semantic (since we are not
identifying language use but underlying meaning).

The term “modality” is used in formal seman-
tics as well as in descriptive linguistics. Many se-
manticists (e.g. (Kratzer, 1991; Kaufmann et al.,
2006)) define modality as quantification over pos-
sible worlds. Modality can be of two types: epis-
temic, which qualifies the speaker’s commitment,
and deontic, which concerns freedom to act. Be-
lief/factuality falls under epistemic modality. An-
other view of modality relates more to a speaker’s
attitude toward a proposition (e.g. (McShane et al.,
2004; Baker et al., 2010; Prabhakaran et al., 2012)),
which is closer to the way we model belief.

We interpret the term “veridical” as referring to
a property of certain words (usually verbs), namely
to mark the proposition expressed by their syntac-
tic complement clause as firmly believed (commit-
ted belief) by the writer (Kiparsky and Kiparsky,
1970). Veridicality as a property of lexical or
lexico-syntactic elements is thus a way of relating

belief/factuality to linguistic means of expressing
them, but we take the notion of belief/factuality as
being the underlying notion.

3 Annotation

3.1 Annotation Manual
The purpose of this annotation is to capture the com-
mitment of the writer’s belief in the propositions ex-
pressed in the text. The annotation for this project
marks beliefs held by the writer only. We exhaus-
tively annotate all (clausal) propositions in each doc-
ument with a four-way belief type distinction, with
the following categories.
Committed belief (CB) – the writer strongly be-
lieves that the proposition is true. Examples:

(1) a. The sun will rise tomorrow.
b. I know John and Katie went to Paris last

year.

Non-committed belief (NCB) – the writer believes
that the proposition is possibly or probably true, but
is not certain. Examples:

(2) a. It could rain tomorrow.
b. I think John and Katie went to Paris last

year.

Reported belief (ROB) – the writer attributes be-
lief (either committed or non-committed) to another
person or group. Note that this label is only applied
when the writer’s own belief in the proposition is
unclear. Examples:

(3) a. Channel 6 said it could rain tomorrow.
b. Sarah said that John and Katie went to Paris

last year.

Non-belief propositions (NA) – the writer ex-
pressed some other cognitive attitude toward the
proposition, such as desire or intention, or expressly
states that s/he has no belief about the proposition
(e.g., by asking a question). Examples:

(4) a. Is it going to rain tomorrow?
b. I hope John and Katie went to Paris last

year.

We do not make any effort to evaluate the truth
value of the propositions, only the expressed level
of belief in them held by the writer. Thus a strongly
held false belief would not appear any different from

84



a strongly held true belief. Similarly, lying, sarcasm,
irony, and other cases where the writer’s internal be-
lief may differ from the expressed belief are not cap-
tured. That is, we take all expressed beliefs at “face
value”. We also do not capture any cognitive atti-
tudes expressed about a proposition other than be-
lief. An NA tag signifies just that there is no be-
lief expressed about the proposition; it does not sig-
nify that there is another cognitive attitude expressed
(e.g., 4a). Similarly, a proposition tagged as CB
may also have other cognitive attitudes expressed
about them (e.g., in “John managed to go to Paris
last week”, the author is expressing CB towards the
proposition go, but also the success modality (Prab-
hakaran et al., 2012)); we do not capture them.

Annotators are not required to identify the full
text span of the proposition. Instead, we take ad-
vantage of the close relationship between the seman-
tics of the proposition and the syntactic structure of
the clause by marking only the head of the structural
unit containing the proposition (propositional head).
For each proposition, annotators mark a head word
and tag it with one of the four belief types. Note
that the syntactic head word (perhaps lemmatized)
can serve as a convenient name for the proposition,
so for the examples above, we can talk about the be-
lief in the ‘rain’ proposition and in the ‘go’ propo-
sition. When a sentence has a single clause con-
taining only one proposition, there will be only one
head word to identify (usually a verb, but see details
below on identifying heads of propositions). Many
sentences contain multiple propositions, and the an-
notation guidelines provide detailed instructions on
identifying head words. Note that the (b) examples
above contain an additional proposition which is not
marked; a full markup for example (3b) is below.

(5) Sarah said/CB that John and Katie went/ROB
to Paris last year.

This is equivalent to the following span-based anno-
tation:

(6) [CB Sarah said [ROB that John and Katie went
to Paris last year.]]

The general principles of head word selection for
each proposition can be summarized as follows:

1. Annotate the lexical verb of the clause express-
ing the proposition, if there is one.

2. If the verb of the clause is a copula, annotate
the head of the predicate that follows the copula
(noun for NP, preposition for PP, etc.).

3. Deontic modal auxiliaries, which signal a com-
plex proposition, are annotated in addition to
the lexical verb, as a separate belief.

All annotations are applied to a single whitespace-
delimited word. In cases where the head of a propo-
sition is a multiword expression (MWE), the head
of the MWE is selected. In cases of noun phrases
where no head is apparent (e.g. bok choy), the last
word of the MWE is selected.

3.2 Comparison with FactBank
As already explained (Section 2), we take the terms
“belief” and “factuality” to refer to the same phe-
nomenon underlyingly (with perhaps different em-
phases). Therefore, the FactBank annotation is basi-
cally compatible with ours. Our annotation is much
simpler than that of FactBank in order to allow for a
quicker annotation. We summarize the main points
of simplification here.

• We have taken the source always to be the
writer. As we will discuss in Section 7.1, we
will adopt the FactBank annotation in the next
iteration of our annotation.

• We do not distinguish between possible and
probable; this distinction may be hard to anno-
tate and not too valuable.

• We ignore negation. If present, we simply as-
sume it is part of the proposition which is the
target.

Werner et al. (2015) study the relation between
belief and factuality in more detail. They provide an
automatic way of mapping the annotations in Fact-
Bank to the 4-way distinction of speaker/writer’s be-
lief that we present in this paper.

3.3 Corpus and Annotation Results
The annotation effort for this phase of belief annota-
tion for DEFT produced a training corpus of 852,836
words and an evaluation corpus of 100,037 words.
All annotated data consisted of English text from
discussion forum threads. The discussion forum

85



threads were originally collected for the DARPA
BOLT program, and were harvested from a wide va-
riety of sites. Discussion forum sites were chosen
for harvesting in BOLT based on human judgement
that the site was likely to contain many threads dis-
cussing either current events or personal anecdotes.
For details on the BOLT collection, see Garland et
al. (2012). Threads longer than 1000 words were
truncated to produce documents consisting of one or
more consecutive posts from a single thread. Long
threads may generate multiple documents consisting
of non-overlapping sections of the same thread (e.g.,
document 1 contains posts 1-5, while document 2
contains posts 6-12, etc.). The distribution of the
four belief types in the training and evaluation cor-
pora can be seen in Table 1.

Annotations CB NCB ROB NA

Training Corpus

143240
79995 3890 7150 52205
(56%) (3%) (5%) (36%)

Evaluation Corpus

17553
8730 583 941 7299
(50%) (3%) (5%) (42%)

Table 1: Annotation Statistics

The source data pool, annotation procedures, and
annotators were the same for both the training and
evaluation datasets, with the exception of the fact
that the evaluation annotations received a full second
pass over the annotation by a senior annotator (not
the same as the first pass annotator) to increase con-
sistency and reduce annotator errors. The training
annotations were produced with a single annotation
pass, and quality control was conducted through a
second pass by a senior annotator on a sample of ap-
proximately 15% of the data. Inter-annotator agree-
ment on headword selection was 93% and agree-
ment on belief type labeling was 84%. Overall
observed agreement, combining headword selection
and belief type label, was 78% (Kappa score .60).
Agreement on belief type was least reliable on the
categories of ROB and NCB, both of which were
sometime erroneously marked as CB. Both of these
categories, in addition to being less frequent in the
corpus, have difficult edge cases in which the an-

notator must make a judgment based on the context
of the document (for example, deciding whether the
writer clearly shares a belief attributed to another
person for ROB).

4 Evaluation Systems

We conducted a multi-site pilot evaluation for the
task of identifying beliefs expressed in text. Three
performer sites took part in this evaluation. In this
section, we briefly describe the systems built at these
performer sites. The first two systems are rule-based
systems, whereas the third system is a supervised
learning system. We limit the discussion of these
systems to a high level, postponing the detailed sys-
tem descriptions to separate future publications.

4.1 System A
System A is adapted from a Sentiment Slot Filling
system which participated in the 2014 TAC KBP
SSF Evaluation (Shaikh et al., 2014). This system
uses the Stanford Parser to create a syntactic depen-
dency structure for every sentence in a given doc-
ument. Using the dependency tree, it extracts the
belief targets, which are usually the subjects of the
sentence. In addition, the system extracts belief re-
lations – a unary or binary predicate – typically a
verb, an adjective or a noun. The focus of this ver-
sion of System A is to identify propositional heads
that express belief of any type. Each relation so ex-
tracted was initially marked as CB. A few heuristics
were then applied to distinguish CBs from NCBs -
such as presence of hedge words (maybe, guess). In
addition, a few heuristics were added to tag relations
as NAs, for example when the predicates appear in a
question. The current version of System A does not
account for ROB tags.

4.2 System B
System B uses the dependency tree and part-of-
speech tags from the Stanford NLP tools, together
with a custom verb lexicon to recognize belief ex-
pressions. The tree is processed to convert ob-
jects and complements to a single format, and then
transformed into one or more belief triples (subject,
verb, object). The system maintains a database of
nested belief context, as in ‘X believes Y believes
Z’, but we did not notice many instances of this phe-
nomenon in the data. Partly because our System B

86



System A System B System C

Prec. Rec. F-meas. Prec. Rec. F-meas. Prec. Rec. F-meas.

CB 35.9 39.9 37.8 42.1 36.8 39.3 68.9 77.9 73.1
NCB 13.3 8.8 10.6 4.6 7.4 5.7 52.9 29.7 38.0
ROB 0.0 0.0 0.0 1.3 0.9 1.0 43.8 15.6 23.0
NA 40.3 5.9 10.2 35.8 4.8 8.4 80.1 62.0 69.9

Overall 35.5 22.5 27.6 34.4 20.6 25.8 72.0 66.4 69.1

Table 2: Results obtained for System A, System B, and System C on the final Evaluation dataset.

recognizes reported beliefs (ROB) independently of
the distinction between committed/non-committed
belief in the annotations, the heuristic rules (mainly
based on the presence of modal auxiliaries) that we
added for the purpose of classifying the beliefs (CB,
NCB, ROB, NA) did not work reliably in all cases.

4.3 System C

System C uses a supervised learning approach to
identify tokens denoting the heads of propositions
that denote author’s expressed beliefs. It approaches
this problem as a 5-way (CB, NCB, ROB, NA, nil)
multi-class classification task at the word level. Sys-
tem C is adapted from a previous system which uses
an earlier, simpler definition and annotation of be-
lief (Prabhakaran et al., 2010). The system uses lex-
ical and syntactic features for this task, which are
extracted using the part-of-speech tags and depen-
dency parses obtained from the Stanford CoreNLP
system. In addition to the features described in
(Prabhakaran et al., 2010), System C uses a set of
new features including features based on a dictio-
nary of hedge-words (Prokofieva and Hirschberg,
2014). The hedge features improved the NCB F-
measure by around 2.2 percentage points (an overall
F-measure improvement of 0.25 percentage points)
in experiments conducted on a separate development
set. It uses a quadratic kernel SVM to build the
model, which outperformed the linear kernel in ex-
periments conducted on the development set.

5 Proposition-Oriented Evaluation

We now describe the results obtained on a
proposition-oriented quantitative evaluation of these
systems. We focus on a system’s ability to correctly
identify the propositional heads of each type of be-

lief (CB, NCB, ROB, NA). Only the words denoting
heads of propositions will get one of these tags, and
hence the majority of words in our data will not have
any tags. We expect the system to find the proposi-
tional heads and to correctly assign their belief tags.

We use the entire Evaluation dataset described in
Section 3 for this evaluation (entirely unseen during
the development of the systems). We report preci-
sion, recall and F-measure for each belief type. We
also report their micro-averages as the overall result.
We compute F-measure as the harmonic mean be-
tween precision and recall. The best results obtained
by each system described in Section 4 are presented
in Table 2.

For System A, four different configurations were
run for the evaluation, in which the NCB and NA
tagging was either enabled or disabled. (The cur-
rent version does not account for ROB tags.) In Ta-
ble 2, Columns 2-4, we show the performance of
System A while all 3 tags (CB, NCB and NA) are
enabled. The results of other three configurations
are comparable. Any sentence where the belief tar-
get could not be located, either due to parsing error
or due to missing coreference (as supplied by ERE),
was discarded. This resulted in a relatively lower re-
call in the evaluation, but produced high precision
in a target-driven pilot evaluation (Section 6). The
results obtained by System B in the evaluation are
shown in Table 2: Columns 5-7. The results of Sys-
tem B, when ignoring the belief categories (i.e., on
identifying heads of propositions), were 83.6% pre-
cision and 50% recall. Table 2: Columns 8-10 shows
results obtained by System C trained on 80% of the
training dataset (the rest of the corpus was used as a
development set).

The supervised learning approach obtained over-

87



all better performance than rule based approach in
our evaluation. ROB and NCB were the most dif-
ficult classes to predict for all three systems (e.g.,
highest recall posted for ROB is only 15.6%). CB
was relatively easier to predict. NA was difficult to
predict using the rule based approach, but supervised
learning approach obtained reasonable performance
of 69.9 F-measure.

6 An Entity-Focused Evaluation:
Preliminary Investigation

In this section, we describe an initial investigation
towards an entity-focused evaluation. An entity-
focused evaluation tests a different kind of question
about beliefs: given an entity e, what beliefs does the
writer have about e? This entity-focused evaluation
draws its parallels from TAC KBP Sentiment Slot
Filling Evaluation (SSF) task. In the SSF, the task is
to determine a target entity given a source entity and
a sentiment between them. The goal is to populate
a knowledge base with information regarding enti-
ties and the sentiment relations between them. In
the same vein, an entity-focused belief task would
provide knowledge about the salient belief relations
between entities. For this purpose, we needed to de-
fine what is meant by “having a belief about an en-
tity” and agreed on the following preliminary rules.
The rules are entirely syntactic. In the following ex-
amples, the target entity is Mary, and the statement
after the arrow shows what the beliefs are about her
(and what the level of commitment by the writer is).

Adjunct clause case 1. If the target entity is con-
tained in a clause (lets call it the “core clause”) but
NOT in an adjunct clause which modifies the core
clause, we omit the adjunct clause (even though the
adjunct clause in some sense pertains to the core
clause but by virtue of being an adjunct, it is omiss-
able).

(7) While John was in/CB Paris, Mary left/CB Paul
−→ CB: Mary left Paul

Adjunct clause case 2. If the target is in an ad-
junct clause to a core clause where the target is not
mentioned, we retain both the adjunct clause as a
standalone belief, and the combination of the ad-
junct and core (i.e., we have two beliefs about the
entity).

(8) John was happy/CB when Mary left/CB Paul
−→ CB: John was happy when Mary left Paul
; CB: Mary left Paul

We devised similar rules for complement clauses,
we omit them here.

For the actual evaluation, we used files which also
had been hand-annotated for ACE entities. How-
ever, we did not have a gold annotation for entity-
focused belief, as this study is still contributing to-
wards a definition of this notion. Only two systems
participated, System A and System C. System A as
described in Section 4.1 already takes the notion of
entity into account. For System C, we used the parse
to determine the span associated with the annotated
headword, and counted a proposition whose span in-
cluded an entity to be about that entity. In order
to understand how these two ways of determining
entity-focused belief relate to each other, we com-
pared the two systems to each other. We obtained
an F-measure of 52%. We also hand evaluated the
positive claims of System C, obtaining an accuracy
of 48% on the positive claims. The errors are due
to parse errors, the presence of the entity in adjuncts
which do not appear germane (contradicting adjunct
clause case 2), the presence of irrelevant adjunct
clauses (counter to adjunct clause case 1), and to a
lack of clarity in the annotation standard. As an ex-
ample of the lack of clarity, consider the following
sentence from our evaluation corpus, with two kids
as target entity:

(9) I didn’t see these two kids (sic) names on the
news

two kids is a possessor of the direct object, and fell
into the span of the annotated see for System C, but
System A deemed the ‘see’ belief not to be about it.
We conclude that this purely syntactic definition of
“belief about an entity” is not satisfactory. The def-
inition of “belief about an entity” remains an open
question and we return to it in Section 7.3.

7 Plans for Next Round

7.1 Adding the Source
Currently, we are only annotating and evaluating the
writer’s beliefs. Beliefs attributed by the writer to
other sources are marked ROB. We intend to anno-
tate the source for all beliefs, using the method of

88



nested attribution pioneered by MPQA (Wiebe et al.,
2005) and adopted by FactBank (Saurı́ and Puste-
jovsky, 2012). Consider the following sentence.

(10) John believes Mary knows that the clock was
stolen

In the nested attribution approach, according to
the writer, according to John, Mary firmly believes
(CB) the ‘steal’ proposition. According to the writer,
John firmly believes the ‘know’ proposition and the
‘steal’ proposition (as indicated by the veridical verb
know). The writer herself firmly believes (CB) the
‘believe’ proposition, does not express an opinion on
the ‘know’ proposition (ROB), and also firmly be-
lieves (CB) the ‘steal’ proposition (again, the reader
infers this from the use of know). We intend to an-
notate all these levels of belief.

7.2 Defining the Target Proposition

In our work to date, we have assumed that the tar-
get of a belief is a proposition, and we have repre-
sented the proposition by the syntactic head word of
the clause which describes the proposition (which
is equivalent to a text span under syntactic projec-
tion). We are investigating extending this in sev-
eral manners. First, we are considering including
the heads of event noun phrases (the sudden collapse
of the building). Second, we are looking at using a
semantic representation for the proposition (as op-
posed the syntactic head of the text passage describ-
ing the proposition). We do not propose to develop
our own semantic representation, but instead we will
look to using existing relation and event representa-
tions based on the ACE program (Doddington et al.,
2004). These have the advantage that there are off-
the-shelf computational tools available for detecting
ACE relations and events; they have the disadvan-
tage that they do not cover all propositions we may
be interested in. An alternative would be the use of
a shallower semantic representation such as Prop-
Bank (Kingsbury et al., 2002), FrameNet (Baker et
al., 1998), or AMR (Banarescu et al., 2013).

7.3 Entities as Targets

In Section 6, we discussed an initial evaluation of a
belief being about an entity. In this section we dis-
cuss further guidelines for identifying belief targets,
i.e., when one can say that someone’s belief is about

a certain entity.
In general, the notion of belief “aboutness” is

fairly fuzzy and it may be difficult to circumscribe
precisely without some additional constraints. Sup-
pose then that one of the ultimate objectives of belief
extraction is to populate a knowledge base with be-
liefs held about specific entities: individuals, groups,
artifacts, etc., which adds this constraint that the
extracted belief is knowledge-base-worthy, or re-
portable. Some initial guidelines may go as sug-
gested below. The objective is to provide guidance
for a human assessor — not to propose a solution.
We should note that these guidelines generally tran-
scend any syntactic or structural considerations and
appeal directly to the annotators’ judgment. Further-
more, we note that these guidelines are not about ef-
fects relating to information structure – in one sense
of “being about”, the same sentence may be referred
to as being “about” different things in different con-
texts. We are aiming for a lexical-semantic, not a
pragmatic notion of aboutness.

A belief whose target is proposition p is about an
entity T if one of the following clauses holds:

1. p describes a property of T , where the prop-
erty is considered semi-permanent but not nec-
essarily limited to physical or mental charac-
teristics (e.g., red, long, brainy) and may also
include behavioral properties (smart, slow) as
well as characteristics bestowed on by others
(beloved).

2. T is an agent of p, i.e., T is said to be perform-
ing some activity, physical or mental: drive a
car, send a letter, etc.

3. T is directly involved in (or affected by) p but is
not an agent: this includes situations where T ’s
involvement may be passive but is nonetheless
required for p to be performed, e.g., receive a
letter, win a prize, etc.

We make no claim that the above list is exhaustive
or that there would not be exceptions to these rules.
For this reason we may also attempt to describe con-
ditions under which a belief is not about T . For ex-
ample: a belief target p is not about entity T even
though T may be mentioned within the scope of p
if:

89



4. T appears uninvolved in p and is apparently un-
affected by its execution, e.g., reading about,
waiting for, etc.

We intend to explore whether we can define this
notion of belief aboutness sufficiently well to obtain
good inter-annotator agreement.

7.4 Combining with Sentiment
We are planning on working on an annotation and an
evaluation that combines belief with sentiment. The
motivation for this is that belief and sentiment are
similar types of meaning: they are attitudes towards
propositions or entities which are expressed directly
or indirectly. The similarity can also be seen in the
fact that FactBank took the notion of nested source
from MPQA, which is a sentiment-annotated cor-
pus. Furthermore, many lexical items express both a
belief and a sentiment at once:

(11) I hope Bertha enjoys the oysters

Here, the writer expresses a positive sentiment to-
wards the ‘enjoy’ proposition, and at the same time
she is expressing a lack of certainty (NCB) in the
‘enjoy’ proposition.

7.5 Adding Spanish and Chinese
We will be extending our annotation (including
some of the extensions mentioned above) to Span-
ish and Chinese.

8 Conclusion

We have presented an ongoing annotation effort re-
lated to belief/factuality and an initial evaluation
based on that annotation effort. To our knowledge,
the annotated corpus is by far the largest corpus an-
notated in terms of belief/factuality. We have pre-
sented several proposed extensions to the annota-
tion. The linguistic resources described in this paper
will be published in the LDC catalog, making them
available to the broader research community. The
materials will be used in an open evaluation in late
2015 or early 2016. The evaluation will cover both
belief/factuality and sentiment.

Acknowledgments

This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are

those of the authors and do not reflect the official
policy or position of the Department of Defense or
the U.S. Government. We thank several anonymous
reviewers for their constructive feedback.

References
Collin F. Baker, J. Fillmore, and John B. Lowe. 1998.

The Berkeley FrameNet project. In 36th Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics (COLING-ACL’98), pages 86–90, Montréal.

Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori S. Levin, and Christine D.
Piatko. 2010. A modality lexicon and its use in auto-
matic tagging. In LREC.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, pages 178–186, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.

Michael E. Bratman. 1999 [1987]. Intention, Plans, and
Practical Reason. CSLI Publications.

Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68–73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.

George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program–tasks, data, and evaluation. In Proceed-
ings of the Fourth International Conference on Lan-
guage Resources and Evaluation (LREC’04), pages
837–840.

Richárd Farkas, Veronika Vincze, György Móra, János
Csirik, and György Szarvas. 2010. The conll-2010
shared task: Learning to detect hedges and their scope
in natural language text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 1–12, Uppsala, Sweden, July.
Association for Computational Linguistics.

Roberto González-Ibáñez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 581–586, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

90



Stefan Kaufmann, Cleo Condoravdi, and Valentina
Harizanov, 2006. Formal Approaches to Modality,
pages 72–106. Mouton de Gruyter.

Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.

Paul Kiparsky and Carol Kiparsky. 1970. Facts. In
Manfred Bierwisch and Karl Erich Heidolph, editors,
Progress in Linguistics, pages 143–173. Mouton, The
Hague, Paris.

Angelika Kratzer. 1991. Modality. In Arnim von Ste-
chow and Dieter Wunderlich, editors, Semantics: An
International Handbook of Contemporary Research.
Walter de Gruyter, Berlin.

Marjorie McShane, Sergei Nirenburg, and Ron
Zacharsky. 2004. Mood and modality: Out of
the theory and into the fray. Natural Language
Engineering, 19(1):57–89.

Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309–
312, Suntec, Singapore, August. Association for Com-
putational Linguistics.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309–319, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014–1022, Beijing,
China, August. Coling 2010 Organizing Committee.

Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow, and Benjamin Van Durme. 2012. Sta-
tistical modality tagging from rule-based annotations
and crowdsourcing. In Proceedings of the Workshop
on Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, pages 57–64, Jeju, Republic of
Korea, July. Association for Computational Linguis-
tics.

Anna Prokofieva and Julia Hirschberg. 2014. Hedg-
ing and speaker commitment. In 5th Intl. Workshop
on Emotion, Social Signals, Sentiment & Linked Open
Data, Reykjavik, Iceland.

Roser Saurı́ and James Pustejovsky. 2012. Are you sure
that this happened? assessing the factuality degree of
events in text. Computational Linguistics, 38(2):261–
299.

Samira Shaikh, Rob Giarrusso, Veena Ravishankar, and
Tomek Strzalkowski. 2014. The SUNY Albany Senti-
ment Slot Filling System. In Proceedings of the 2014
TAC KBP Sentiment Slot Filling Evaluation, Gaithers-
burg, Maryland, USA. NIST.

Gregory J. Werner, Vinodkumar Prabhakaran, Mona
Diab, and Owen Rambow. 2015. Committed belief
tagging on the factbank and lu corpora: A compara-
tive study. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, Denver, USA, June. Association for Com-
putational Linguistics.

Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language ann. Language Resources and Evaluation,
39(2/3):164–210.

91


