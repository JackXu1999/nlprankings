



















































Deep Learning Techniques for Humor Detection in Hindi-English Code-Mixed Tweets


Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 57–61
Minneapolis, June 6, 2019. c©2019 Association for Computational Linguistics

57

Deep Learning Techniques for Humor Detection in Hindi-English
Code-Mixed Tweets

Sushmitha Reddy Sane*1 Suraj Tripathi*2 Koushik Reddy Sane1 Radhika Mamidi1
1International Institute of Information Technology, Hyderabad

2Indian Institute of Technology, Delhi
{sushmithareddy.sane, koushikreddy.sane}@research.iiit.ac.in,

surajtripathi93@gmail.com, radhika.mamidi@iiit.ac.in

Abstract

We propose bilingual word embeddings based
on word2vec and fastText models (CBOW and
Skip-gram) to address the problem of Humor
detection in Hindi-English code-mixed tweets
in combination with deep learning architec-
tures. We focus on deep learning approaches
which are not widely used on code-mixed
data and analyzed their performance by exper-
imenting with three different neural network
models. We propose convolution neural net-
work (CNN) and bidirectional long-short term
memory (biLSTM) (with and without Atten-
tion) models which take the generated bilin-
gual embeddings as input. We make use of
Twitter data to create bilingual word embed-
dings. All our proposed architectures outper-
form the state-of-the-art results, and Attention-
based bidirectional LSTM model achieved an
accuracy of 73.6% which is an increment of
more than 4% compared to the current state-
of-the-art results.

1 Introduction

In the present day, we observe an exponential
rise in the number of individuals using Internet
Technology for different purposes like entertain-
ment, learning and sharing their experiences. This
led to a tremendous increase in content gener-
ated by users on social networking and micro-
blogging sites. Websites like Facebook, Twitter,
and Reddit (Danet and Herring, 2007) act as a plat-
form for users to reach large masses in real-time
and express their thoughts freely and sometimes
anonymously amongst communities and virtual
networks. These natural language texts depict var-
ious linguistic elements such as aggression, irony,
humor, and sarcasm. In recent years, automatic
detection of these elements (Davidov et al., 2010)
has become a research interest for both organiza-
tions and research communities.

* These authors contributed equally to this work.

The advancement in computer technologies
places increasing emphasis on systems and mod-
els that can effectively handle natural human lan-
guage. So far, the majority of the research in
natural language processing and deep learning is
focused on the English language as individuals
across the world use it widely. But, in multi-
lingual geographies like India, it is a natural phe-
nomenon for individuals to use more than one lan-
guage words in speech and in social media sites
like Facebook and Twitter (Cárdenas-Claros and
Isharyanti, 2009; Crystal, 2002). Data shows that
in India, there are about 314.9 million bilingual
speakers and most of these speakers tend to mix
two languages interchangeably in their commu-
nication. Researchers (Myers-Scotton, 1997) de-
fined this linguistic behavior as Code-mixing -
the embedding of linguistic units such as phrases,
words, and morphemes of one language into an ut-
terance of another language which produces utter-
ances consisting of words taken from the lexicons
of different languages.

The primary challenge with the code-mixed cor-
pus is the lack of data in general text-corpora,
(Nguyen and Doğruöz, 2013; Solorio and Liu,
2008a,b) for conducting experiments. In this pa-
per, we take up the task of detecting one critical
element of natural language (Kruger, 1996) which
plays a significant part in our linguistic, cognitive,
and social lives, i.e., Humor. Martin (Martin and
Ford, 2018) extensively studied the psychology of
humor and stated that it is ubiquitous across cul-
tures and it is a necessary part of all verbal com-
munication. The classification of some text as hu-
mor can be very subjective. Also, capturing Hu-
mor in higher order structures (de Oliveira et al.,
2017) through text processing is considered as a
challenging natural language problem. Pun detec-
tion in one-liners (Kao et al., 2016) and detection
of humor in Yelp reviews (de Oliveira et al., 2017)

https://en.wikipedia.org/wiki/Multilingualism in India



58

have also been studied in recent years.
Deep learning techniques (LeCun et al., 2015)

have contributed to significant progress in various
areas of research, including natural language un-
derstanding. Convolutional neural network based
networks have been used for sentence classifica-
tion (Kim, 2014), bidirectional LSTM networks
(biLSTM) were used for sequence tagging (Huang
et al., 2015), and attention based bidirectional
LSTM networks were used for relational classifi-
cation (Zhou et al., 2016) and topic-based senti-
ment analysis (Baziotis et al., 2017). In this work,
we propose three deep learning networks using
bilingual word embeddings as input and compare
it against the classification models presented in
(Khandelwal et al., 2018) using their annotated
corpus to detect one of the playful domains of lan-
guage: Humor. An example from the corpus:

“Subha ka bhula agar sham ko wapas ghar aa
jaye then we must thank GPS technology..”
“(If someone is lost in the morning and returns
home in the evening then we must thank GPS tech-
nology.)

This tweet is annotated as humorous. In par-
ticular, we are focused on code-mixed data as it
lacks the presence of bilingual word embeddings,
commonly used, to train any deep learning model
which is essential for understanding human behav-
ior, events, reviews, studying trends as well as lin-
guistic analysis (Vyas et al., 2014).

1.1 Corpus creation for Bilingual Word
Embeddings

The corpus used for training the bilingual word
embeddings is created using Twitter’s API.
Around 200k tweets are extracted using 1000 most
common words from the training corpus after re-
moving stop words. Preprocessing is done on the
sentences, and Twitter handles starting with “@”
or words that have any special symbol are re-
moved. URLs are replaced with the word “URL”.

1.2 Word2Vec
Code-mixed (Hindi-English) data need vector rep-
resentations of its words to train a deep learning
based model. However, our corpus being bilin-
gual in nature prohibits the use of any pre-trained
word2vec (Mikolov et al., 2013) representations.
As mentioned earlier, we used the collected Twit-
ter data to train the bilingual word embedding
model. We experimented with various hyperpa-
rameters like embedding size, window length, and

negative sampling. Based on the results, we final-
ized the following set of values for our main task -
humor detection.

• Embedding size: 300, Window length: 10,
Negative sampling

1.3 FastText

One of the limitations of word2vec model is the in-
ability to handle words with very low frequency in
the training corpus and out-of-vocabulary words
which might be present in the unseen text in-
stances. Example: people on social media write
words like “happppyyyyy”, “lolll”, etc. These
kinds of new words can’t have pre-trained word
embeddings. To address this problem in the bilin-
gual scenario, we analyzed the performance of
fastText (Bojanowski et al., 2017) word embed-
ding model, which considers subword informa-
tion, for generating word embeddings. FastText
learns character n-gram (Joulin et al., 2016) repre-
sentations and represents words as the sum of the
n-gram vectors, where n is a hyperparameter. We
kept hyperparameters like embedding size, win-
dow length, etc., same as in word2vec model to
compare their results.

1.4 Model Architecture

We propose three different deep learning archi-
tectures for the task of humor detection based on
CNN and biLSTM networks which take bilingual
word embeddings as input. We used cross-entropy
loss function and Adam optimizer for training all
our proposed architectures.

1.4.1 Model 1 - Convolutional Neural
Network (CNN)

We propose a CNN-based model, refer to Figure
1, which takes bilingual word embeddings as in-
put. CNN based model makes use of a set of
4 parallel 1D convolution layers to extract fea-
tures from the input embeddings. Features de-
rived from the convolution layer are then fed into
the global max-pool layer, which extracts one fea-
ture per filter. The extracted features from max-
pool layer are then flattened and passed to multi-
ple fully connected (FC) layers. Finally, classifica-
tion is performed using a softmax layer. We have
used various training techniques such as dropout
(.25 to .75) (Srivastava et al., 2014) and batch-
normalization (Ioffe and Szegedy, 2015) that helps



59

in reducing overfitting and sensitivity towards ini-
tial weights respectively. With the use of batch-
normalization, we also observed improvement in
the convergence rate.

Figure 1: Proposed CNN architecture

The model uses four parallel instances of convo-
lution layer with varying kernel sizes. We exper-
imented with various values for hyperparameters
such as the number of kernels, kernel sizes and fi-
nalized following values based on the performance
on the validation set:

• Kernel size:

fh1 = 3, f
h
2 = 6, f

h
3 = 9, f

h
4 = 12

• Number of kernels = 200, Stride = 1.

We analyzed the performance of the proposed
CNN based network with both word2vec and fast-
Text generated bilingual word embeddings and
presented their results in Table 2. Here, Model
1(a) refers to CNN with word2vec and Model 1(b)
refers to CNN with fastText based word represen-
tations respectively.

1.4.2 Model 2 - Bidirectional LSTM Network
Bidirectional LSTM architectures have been
proved to be very useful to model word sequences
and are robust to learn on data with long-range
temporal dependencies. We use the bidirectional
LSTM network on the input bilingual word em-
beddings to capture the compositional semantics
for the bilingual texts in our experiments.

The sentiment of each word in the sentence de-
pends on the context in which the word is used,
where context includes content in front of the word
as well as behind the word. To model these scenar-
ios, we make use of bidirectional LSTM network
which has been successfully applied in generating
context-dependent hidden representations as well
as capturing long-term dependencies in text clas-
sification tasks (Wang et al., 2016). We experi-
mented with a different number of hidden layers
and number of hidden units in each hidden layer

and finalized the value of 1 and 200 respectively
based on the results on the validation set. We used
similar architecture as showed in Figure 2 with
no attention mechanism and used concatenated

−→
ht

and
←−
h1 as input to the dense layer (#hidden units =

200) which is followed by the final softmax layer.
To analyze the effect of different bilingual word
embeddings, we make use of both word2vec, and
fastText generated embeddings. In Table 2, Model
2(a) refers to the use of word2vec with Model 2
and Model 2(b) refers to the use of fastText with
Model 2.

Figure 2: Proposed Attention based BiLSTM Model

1.4.3 Model 3 - Attention-based Bidirectional
LSTM Network

We further propose an attention-based mechanism
for the bidirectional LSTM network. The word-
level attention model learns which words in a
given sentence are more critical for determining
the overall emotion (humorous / non-humorous) of
the sentence. These words act as decisive points.
Some parts in sentences create noise, and this
mechanism helps to filter out those noises.

Input sentence x1, x2, ..., xt−1, xt represents
bilingual word embedding of the input text utter-
ance, which is fed into the hidden layer of the pro-
posed bidirectional LSTM network as input. As
presented in Figure 2, bidirectional LSTM archi-
tecture makes use of both forward and backward
hidden states at each time step. We used well-
known standard LSTM units for our architecture
and thus omitted the equations related to the cell
units. At each time step i, hi = [

−→
hi ;
←−
hi ] represents

complete hidden state representation. We make
use of the hidden state of each step to calculate
the weights for each word and weighted summa-
tion of all time steps. hα is used as an input to the



60

classifier in combination with
−→
ht and

←−
h1. Concate-

nated hidden states are passed on to a single dense
layer, followed by output softmax layer. We used
the same hyperparameter settings for hidden rep-
resentation and dense layer size as mentioned in
Model 2 to analyze the effect of adding attention to
the proposed model. We experimented with both
word2vec and fastText generated bilingual word
embeddings, and results are presented in Table 2.
Here, Model 3(a) refers to the use of word2vec
and Model 3(b) refers to the use of fastText with
Model 3 respectively.

Word-Hi Word-En Word2Vec FastText
pyaar love 0.64 0.78
nafrat hate 0.71 0.73
ldai fight 0.74 0.85
majak funny 0.62 0.71
gussa angry 0.78 0.76

Table 1: Similar meaning Hindi and English words
similarity scores with word2vec and fastText models

2 Results

The benchmark dataset that is published online by
(Khandelwal et al., 2018) is used for evaluating the
effectiveness of bilingual word embeddings and
proposed deep learning models. It contains 3543
annotated tweets where 1755 are labeled humor-
ous and 1698 as non-humorous. We make use of
5-fold cross-validation for generating our experi-
mental results. Using all the features (Khandelwal
et al., 2018), the baseline systems: kernel SVM,
random forest, extra tree, and naive Bayes pre-
sented the best accuracy of 69.3%. Going forward,
to the best of our knowledge, we are the first to
experiment with deep learning architectures using
bilingual word embedding for detecting humor in
code-mixed data. All of our models showed bet-
ter accuracies than current state-of-art-results, and
our proposed Attention-based bidirectional LSTM
achieved the best accuracy of 73.6%.

The challenges in this task are the linguistic
complexity of code-mixed data and lack of clean
data. To address phrasal repetitions, short and
simple constructions, non-grammatical words and
spelling errors in the data, larger corpora need to
be built and annotated in the geographies and com-
munities where multilingualism is observed.

In Table 1, we analyzed the generated bilin-
gual word embeddings by comparing the similar-

Model Accuracy
Random Forest* 65.2
Naive Bayes* 67.2
Extra tree* 67.8
Kernel SVM* 69.3
Model 1(a) 70.8
Model 1(b) 71.3
Model 2(a) 71.5
Model 2(b) 72.2
Model 3(a) 72.8
Model 3(b) 73.6

Table 2: Detailed accuracies achieved on the bench-
mark dataset by different models. *Random Forest,
Naive Bayes, Extra tree, and kernel SVM accuracies
are from (Khandelwal et al., 2018)

ity scores of Hindi and English words with sim-
ilar meaning. We observed that fastText model
showed better similarity scores than word2vec
model which indicates that bilingual word em-
beddings do get better with subword information
which is used in learning fastText word represen-
tations. In Table 2, we presented the results of our
proposed deep learning based architectures which
takes bilingual words embeddings generated from
word2vec and fastText skip-gram model. We also
experimented with CBOW versions of both learn-
ing strategies and achieved similar results.

3 Conclusion

In this paper, we address the problem of humor
detection in code-mixed Hindi-English data gener-
ated by bilingual users. We propose three different
deep learning based models which take bilingual
word embeddings as input. Both word2vec and
fastText based models are used for learning bilin-
gual word representations and also to demonstrate
the effectiveness of these techniques by present-
ing similarity scores of words with similar mean-
ing in Hindi and English languages. The proposed
attention-based biLSTM model worked best with
an accuracy of 73.6%. Compared to the state-
of-the-art models all our proposed deep learning
models performed better at detecting humor in
code-mixed data. For future work, we will gen-
erate aligned multilingual word embeddings and
compare them with vectors aligned with MUSE,
and pre-aligned fastText embeddings.

https://github.com/facebookresearch/MUSE



61

References
Christos Baziotis, Nikos Pelekis, and Christos Doulk-

eridis. 2017. Datastories at semeval-2017 task
4: Deep lstm with attention for message-level and
topic-based sentiment analysis. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 747–754.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Mónica Stella Cárdenas-Claros and Neny Isharyanti.
2009. Code-switching and code-mixing in internet
chatting: Betweenyes, ya,andsi-a case study. The
Jalt Call Journal, 5(3):67–78.

David Crystal. 2002. Language and the internet.
IEEE Transactions on Professional Communication,
45(2):142–144.

Brenda Danet and Susan C Herring. 2007. The multi-
lingual Internet: Language, culture, and communi-
cation online. Oxford University Press on Demand.

Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the four-
teenth conference on computational natural lan-
guage learning, pages 107–116. Association for
Computational Linguistics.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint
arXiv:1502.03167.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efficient text
classification. arXiv preprint arXiv:1607.01759.

Justine T Kao, Roger Levy, and Noah D Goodman.
2016. A computational model of linguistic humor
in puns. Cognitive science, 40(5):1270–1285.

Ankush Khandelwal, Sahil Swami, Syed S Akhtar,
and Manish Shrivastava. 2018. Humor detec-
tion in english-hindi code-mixed social media con-
tent: Corpus and baseline system. arXiv preprint
arXiv:1806.05513.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Arnold Kruger. 1996. The nature of humor in human
nature: Cross-cultural commonalities. Counselling
Psychology Quarterly, 9(3):235–241.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. nature, 521(7553):436.

Rod A Martin and Thomas Ford. 2018. The psychol-
ogy of humor: An integrative approach. Academic
press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Carol Myers-Scotton. 1997. Duelling languages:
Grammatical structure in codeswitching. Oxford
University Press.

Dong Nguyen and A Seza Doğruöz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 857–862.

Luke de Oliveira, ICME Stanford, and Alfredo Láinez
Rodrigo. 2017. Humor detection in yelp reviews.

Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973–981. Association for
Computational Linguistics.

Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for english-spanish code-switched text. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051–
1060. Association for Computational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Yogarshi Vyas, Spandana Gella, Jatin Sharma, Ka-
lika Bali, and Monojit Choudhury. 2014. Pos tag-
ging of english-hindi code-mixed social media con-
tent. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 974–979.

Yequan Wang, Minlie Huang, Li Zhao, et al. 2016.
Attention-based lstm for aspect-level sentiment clas-
sification. In Proceedings of the 2016 conference on
empirical methods in natural language processing,
pages 606–615.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
volume 2, pages 207–212.


