



















































Adversarial Evaluation of Multimodal Machine Translation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2974–2978
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2974

Adversarial Evaluation of Multimodal Machine Translation

Desmond Elliott∗
Department of Computer Science

University of Copenhagen
de@di.ku.dk

Abstract

The promise of combining vision and lan-
guage in multimodal machine translation is
that systems will produce better translations by
leveraging the image data. However, incon-
sistent results have lead to uncertainty about
whether the images actually improve transla-
tion quality. We present an adversarial eval-
uation method to directly examine the utility
of the image data in this task. Our evaluation
measures whether multimodal translation sys-
tems perform better given either the congruent
image or a random incongruent image, in ad-
dition to the correct source language sentence.
We find that two out of three publicly available
systems are sensitive to this perturbation of the
data, and recommend that all systems pass this
evaluation in the future.

1 Introduction

Multimodal machine translation is the task of
translating sentences situated in a visual context,
such as captioned images on social media. The
core argument of this area of research is that we
can produce better translations by exploiting both
the source language sentence and the visual con-
text (Elliott et al., 2015; Hitschler et al., 2016).
There is some evidence to support this argument
for human translation: Frank et al. (2018) found
that 13% of the German evaluation data in the
Multi30K dataset (Elliott et al., 2016) needed at
least one post-edit to reflect the joint meaning of
the visual and linguistic context. However, the
evidence that visual context helps computational
models is less clear. Consider the three teams that
submitted contrastive multimodal and text-only
variants of their systems to the 2017 Multimodal
Translation Shared Task (Elliott et al., 2017): the
University of Le Mans’ multimodal system out-
performed their text-only variant (Caglayan et al.,

∗Work carried out at the University of Edinburgh.

Two dogs play with an
orange toy in tall grass.

Model

Zwei Hunde spielen im
hohen Gras mit einem

orangen Spielzeug.

Figure 1: An adversarial evaluation for multimodal
translation. We measure the difference in perfor-
mance when a model sees a congruent image (left)
or an incongruent image (right).

2017); the Oregon State University text-only sys-
tem outperformed their multimodal variant (Ma
et al., 2017); and the performance of the Charles
University systems depended on the language pair
(Libovický and Helcl, 2017). In light of these re-
sults, we need a better understanding of the role of
visual context in multimodal translation systems.

We propose an adversarial evaluation Method
to determine whether multimodal translation sys-
tems are aware of the visual context. We introduce
a measure of image awareness to quantify the dif-
ference in performance in two settings: (i) when
a system is presented with congruent visual data;
(ii) when it is presented with incongruent visual
data. In both settings, a system is presented with
the correct source language sentence. See Figure
1 for an illustration of our evaluation. We hypoth-
esise that if a system is aware of the visual con-
text, i.e. it is actually using the image for trans-
lation, then the system will perform better when
it is presented with the congruent visual data than
incongruent visual data. Our evaluation is related



2975

to the foiled image captions evaluation, in which
the performance of an image captioning system is
measured when a single word is replaced with an
incorrect, but similar word (Shekhar et al., 2017);
the main difference is that we replace the visual
data instead of manipulating the text. Our work
is also related to a study of question-answering
systems, in which additional text was appended
to the end of a document (Jia and Liang, 2017).
They found that these additional text segments dis-
tracted QA systems from producing the correct an-
swer. In contrast, our evaluation does not manipu-
late the textual data, instead we replace the origi-
nal visual input with a random distractor.

We evaluate three publicly available multimodal
translation systems with our adversarial evalua-
tion. The main finding of this paper is that one
publicly available multimodal translation system
is not aware of the congruent image data. This
finding raises doubts about whether state-of-the-
art multimodal translation systems actually use the
visual context to produce better translations. We
conclude this paper by discussing whether this is
likely to be due to problems with the data or with
the model architectures.

2 Adversarial Evaluation

2.1 Image Awareness
We propose an adversarial evaluation method for
multimodal machine translation. This method
measures how a system performs when it is pre-
sented with the correct text data and either the con-
gruent image or with an incongruent image. In this
section we define two image awareness functions
to measure whether a multimodal translation sys-
tem is aware of the congruent visual data.

Let x be a source language sentence, y be a tar-
get language sentence, v be the congruent image,
and v̄ be an incongruent image. Image awareness
is calculated using an evaluable performance mea-
sure E . The overall image awareness of a model
M on an evaluation dataset D is:

∆-Awareness =
1

|D|

|D|∑
i

aM(xi, yi, vi, v̄i) (1)

The image awareness of a model M for a single
instance aM(xi, yi, vi, v̄i) is given by:

aM(xi, yi, vi, v̄i) = E(xi, yi, vi) − (2)
E(xi, yi, v̄i)

Under this definition, the output of the evalu-
able performance measure should be higher in the
presence of the congruent data than the incongru-
ent data, i.e. E(xi, yi, vi)> E(xi, yi, v̄i).1 If this is
the case, on average, then the overall image aware-
ness of a model ∆-Awareness is positive. This
can only happen when model outputs are evaluated
more favourably in the presence of the the congru-
ent image data than the incongruent image data.

2.2 Model-internal awareness ∆I
A model-internal image measure of awareness is
the difference in the probability assigned to the
target language sentence y in the congruent and
incongruent conditions. This is model-internal
because it has the same form as the maximum-
likelihood objective used to train the translation
model. In this case, E = p(y|x, ·), and the dif-
ference in performance for a single instance is:

aM = ∆I = p(yi|xi, vi)− p(yi|xi, v̄i) (3)

2.3 Model-external awareness ∆E
A model-external awareness measure could be a
text-similarity evaluation or human judgement. In
this paper, we use the Meteor text-similarity score
(Denkowski and Lavie, 2014) because it naturally
decomposes to the sentence level, and it is already
the de-facto evaluation metric for multimodal ma-
chine translation (Specia et al., 2016). Let E be
any text-similarity scoring function T that decom-
poses to the sentence level. The difference in per-
formance for a single instance is defined as:

aM = ∆E = T (xi, yi, vi)− T (xi, yi, v̄i) (4)

3 Systems Evaluation

We evaluate the image awareness of three pre-
trained multimodal translations systems that we
received by direct correspondence:

decinit: The initial state of the decoder net-
work is set with a learned transformation of
the visual data (Caglayan et al., 2017).

trgmul: The target language word embed-
dings are modulated by an element-wise mul-
tiplication with a learned transformation of
the visual data (Caglayan et al., 2017).

1This assumes that a higher score means better perfor-
mance for the performance measure E . Swap the order of
the operands if lower performance means better.



2976

C I ∆E-Awareness

trgmul 57.3 57.3 ± 0.2 -0.001 ± 0.002
decinit 57.0 56.8 ± 0.1 0.003 ± 0.001

hierattn 55.0 53.3 ± 0.3 0.019 ± 0.003

Table 1: Corpus-level Meteor scores in the
Congruent and Incongruent settings, along with
the Meteor-awareness results. Incongruent and
∆E-Awareness scores are the mean and standard
deviation of five permutations of the visual data.

hierattn: The decoder network learns to se-
lectively attend to a combination of the
source language and the visual data (Li-
bovický and Helcl, 2017).

Each system was trained on the 29,000 English–
German–image tuples in the Multi30K dataset (El-
liott et al., 2016). We evaluate the image aware-
ness of these systems using the 1,014 tuples in the
validation data, which is typically used for model
selection. We select the incongruent images v̄ by
randomly shuffling the order in which the images
v are associated with the source language text x.
In our evaluation, we report the mean and stan-
dard deviation of randomly shuffling the image
data five times. The code to evaluate your own
system is publicly available.2

3.1 Statistical test

To determine if a model passes the proposed eval-
uation, we conduct a non-parametric Wilcoxon
signed-rank test of the following hypothesis:

H1: Congruent images improve the quality
of multimodal translation compared to
incongruent images.

H0: Congruent images make no difference
to the quality of multimodal translation
compared to incongruent images.

We conduct this statistical test using the pairs
of values that are calculated in the process of com-
puting the the image awareness scores (Eq. 2), i.e.
E(xi, yi, vi) and E(xi, yi, v̄i).

We combine the k=5 separate p values from
each test using Fisher’s method and reject the null

2http://github.com/elliottd/awareness

trgmul decinit hierattn

0.6

0.4

0.2

0.0

0.2

0.4

0.6

0.8

Di
ffe

re
nc

e 
in

 M
et

eo
r s

co
re

Figure 2: Violin plots of the Meteor-awareness
scores for evaluated models. The white dot marks
the median value, the thick gray bar shows the in-
terquartile range, and the thin gray bar is the 95%
confidence interval. The width of the plots show
the kernel density estimate of the distributions.

hypothesis H0 if the result of the χ2 test with 2k
degrees of freedom is p ≤ 0.005.3

3.2 Results

Table 1 shows the corpus-level results of a Meteor-
based evaluation and the Meteor-awareness evalu-
ation. We find that images improve the quality of
the hierattn system (χ2 = 136.74, p < 0.0001),
and images also improve the quality of the decinit
system (χ2 = 32.79, p = 0.0003). Images make no
difference to the quality of the translations gener-
ated by the trgmul system (χ2 = 8.98, p = 0.533).
To complement these tests, Figure 2 shows vio-
lin plots of the Meteor-awareness scores. These
show that the translations generated by the trgmul
and decinit systems are most likely to result in no
difference in Meteor score between the congruent
and incongruent conditions.

We now turn our attention to the results of
the probability-awareness evaluation. Images im-
prove the quality of the trgmul system (χ2 =
52.55, p < 0.0001), and images also improve the
quality of the hierattn system (χ2 = 622.03, p <
0.0001). Images make no difference to the quality
of decinit system (χ2 = 6.49, p = 0.772).

Figure 3 shows examples of translations pro-

3We use a stricter threshold to reduce the chance of false
positive findings (Benjamin et al., 2018).

http://github.com/elliottd/awareness


2977

Man with Mardi Gras beads
around his neck holding

pole with banner.

Model

Ein Mann mit kahlem
Kopf ist um seinen Hals
hält und hält eine Stange
mit einem Banner.

Ein Mann mit einem
Hawaii Hemd auf dem Hals
hält eine Stange mit einem
Banner um seinen Hals.

Meteor

Mann mit Mardi-Gras-Perlen um den
Hals trägt Stange mit Banner.

45.9 40.9

(a) Congruent is better than Incongruent

Two cyclists cross the
street on a very breezy

California day.

Model

Zwei Radfahrer
überqueren auf einer
stark befahrenen Straße
am Abend die Straße.

Zwei Radfahrer
überqueren auf einer
stark befahrenen
Straße die Straße.

Meteor

Zwei Radfahrer überqueren die Straße an
einem sehr windigen Tag in Kalifornien.

35.3 35.6

(b) Incongruent is better than Congruent

Figure 3: Examples of the difference in Meteor awareness for the hierattn system. In each example,
the source sentence is shown at the top and the reference sentence is shown at the bottom, both in
Typewriter font. The congruent image is on the left, and the incongruent image is on the right.

duced by the hierattn system for sentences paired
with congruent / incongruent images. Figure 3 (a)
shows an example with high positive difference in
Meteor score. The incongruent image causes the
translation system to refer to an unseen Hawai-
ian shirt. In neither setting does the system trans-
late the phrase “Mardi Gras”. Figure 3 (b) shows
an example with a negative difference in Meteor
score. The congruent image results in a long trans-
lation with poor coverage of the reference, which
Meteor punishes more severely than the shorter
translation arising from the incongruent image. In
neither setting does the model translate the prepo-
sitional phrase “on a very breezy California day”.

4 Discussion

4.1 Data problems

We posit that the current Multi30K training data
does not necessarily require systems to use the vi-
sual context to solve the translation task. Elliott
et al. (2016) note that the German translation data
was produced without showing the translators the
images, and Frank et al. (2018) found that 13% of
the Multi30K test data needed to be post-edited to
reflect the joint semantics of both modalities. We
recommend that entirety of the German Multi30K
training data should be post-edited so that future
systems are more likely to require a joint under-

standing of the visual and linguistic context.4 We
note that a similar issue was found in a visual ques-
tion answering dataset, resulting in the creation of
a new “balanced” dataset (Goyal et al., 2017).

4.2 The role of model architectures
The key difference between the systems evaluated
in this paper is how they use the visual context.
The hierattn system learns a timestep-dependent
context vector over a location-preserving 3D vol-
ume of image features, whereas the trgmul and
decinit systems use an average-pool of the 3D
location-preserving features. In our evaluation, the
only system that is aware of the congruent image
data for both types of image-awareness is the hi-
erattn system that learns a spatial context over the
image. Learning to attend to specific regions of
the image may prove to be crucial to improving
translations with visual context.

5 Conclusion

We proposed an adversarial evaluation method to
determine whether multimodal translation systems
are aware of the visual context. This evalua-
tion method measures the difference in the perfor-

4We planned to repeat the adversarial evaluation with the
Multi30K French data, which was created by showing the an-
notators the images. However, we did not receive pre-trained
models for all the systems for English–French translation.



2978

mance of a system given the congruent or an in-
congruent image as additional context. We found
that two out of three publicly available multimodal
translation systems were improved by the congru-
ent visual context, when compared to the incon-
gruent visual context. We encourage researchers
to use this method to evaluate their own sys-
tems. Future work includes augment existing mul-
timodal translation models with an additional ad-
versarial objective that forces the model to per-
form better in the presence of the congruent im-
age than a random incongruent image. We will
also apply this evaluation method to other tasks
that use additional context, e.g. images in visual-
question answering, or part-of-speech tags in neu-
ral machine translation.

Acknowledgements

Barry Haddow asked if we knew whether the ad-
ditional image data actually improved the quality
of multimodal machine translation. I claimed that
there was indisputable evidence, given the results
of the human evaluation at the 2017 Multimodal
Translation Shared Task. I am not sure if I agree
with myself any more.

Desmond Elliott is supported by an Amazon
Research Award. Stella Frank, Ákos Kádár, Emiel
van Miltenburg, and the Edinburgh NLP group
provided valuable feedback on this paper.

References
Daniel J Benjamin, James O Berger, Magnus Johan-

nesson, Brian A Nosek, E-J Wagenmakers, Richard
Berk, Kenneth A Bollen, Björn Brembs, Lawrence
Brown, Colin Camerer, et al. 2018. Redefine statis-
tical significance. Nature Human Behaviour, 2(1):6.

Ozan Caglayan, Walid Aransa, Adrien Bardet, Mer-
cedes Garcı́a-Martı́nez, Fethi Bougares, Loı̈c Bar-
rault, Marc Masana, Luis Herranz, and Joost
van de Weijer. 2017. LIUM-CVC Submissions for
WMT17 Multimodal Translation Task. In Proceed-
ings of the Second Conference on Machine Transla-
tion, pages 432–439, Copenhagen, Denmark.

Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 376–380, Baltimore, Maryland, USA.

Desmond Elliott, Stella Frank, Loı̈c Barrault, Fethi
Bougares, and Lucia Specia. 2017. Findings of
the Second Shared Task on Multimodal Machine
Translation and Multilingual Image Description. In

Proceedings of the Second Conference on Machine
Translation, Volume 2: Shared Task Papers, pages
215–233, Copenhagen, Denmark.

Desmond Elliott, Stella Frank, and Eva Hasler. 2015.
Multi-Language Image Description with Neural Se-
quence Models. CoRR, abs/1510.04709.

Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-
cia Specia. 2016. Multi30K: Multilingual English-
German Image Descriptions. In Proceedings of the
5th Workshop on Vision and Language, pages 70–
74.

Stella Frank, Desmond Elliott, and Lucia Specia. 2018.
Assessing multilingual multimodal image descrip-
tion: Studies of native speaker preferences and
translator choices. Natural Language Engineering,
24(3):393–413.

Yash Goyal, Tejas Khot, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2017. Making the V
in VQA Matter: Elevating the Role of Image Under-
standing in Visual Question Answering. In Confer-
ence on Computer Vision and Pattern Recognition.

Julian Hitschler, Shigehiko Schamoni, and Stefan Rie-
zler. 2016. Multimodal Pivots for Image Caption
Translation. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 2399–
2409.

Robin Jia and Percy Liang. 2017. Adversarial Exam-
ples for Evaluating Reading Comprehension Sys-
tems. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2021–2031.

Jindřich Libovický and Jindřich Helcl. 2017. Attention
Strategies for Multi-Source Sequence-to-Sequence
Learning. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 196–202, Vancouver, Canada.

Mingbo Ma, Dapeng Li, Kai Zhao, and Liang Huang.
2017. OSU Multimodal Machine Translation Sys-
tem Report. In Proceedings of the Second Con-
ference on Machine Translation, Volume 2: Shared
Task Papers, pages 465–469, Copenhagen, Den-
mark.

Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich,
Aurélie Herbelot, Moin Nabi, Enver Sangineto, and
Raffaella Bernardi. 2017. FOIL it! Find One mis-
match between Image and Language caption. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 255–265, Vancouver, Canada.

Lucia Specia, Stella Frank, Khalil Sima’an, and
Desmond Elliott. 2016. A Shared Task on Mul-
timodal Machine Translation and Crosslingual Im-
age Description. In Proceedings of the First Con-
ference on Machine Translation, pages 543–553,
Berlin, Germany.


