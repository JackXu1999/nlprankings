



















































Semantic Structure Analysis of Noun Phrases using Abstract Meaning Representation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 851–856,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Semantic Structure Analysis of Noun Phrases
using Abstract Meaning Representation

Yuichiro Sawai Hiroyuki Shindo
Graduate School of Information Science
Nara Institute of Science and Technology

8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{sawai.yuichiro.sn0,shindo,matsu}@is.naist.jp

Yuji Matsumoto

Abstract

We propose a method for semantic struc-
ture analysis of noun phrases using Ab-
stract Meaning Representation (AMR).
AMR is a graph representation for the
meaning of a sentence, in which noun
phrases (NPs) are manually annotated
with internal structure and semantic re-
lations. We extract NPs from the AMR
corpus and construct a data set of NP
semantic structures. We also propose a
transition-based algorithm which jointly
identifies both the nodes in a semantic
structure tree and semantic relations be-
tween them. Compared to the baseline,
our method improves the performance of
NP semantic structure analysis by 2.7
points, while further incorporating exter-
nal dictionary boosts the performance by
7.1 points.

1 Introduction

Semantic structure analysis of noun phrases (NPs)
is an important research topic, which is beneficial
for various NLP tasks, such as machine transla-
tion and question answering (Nakov and Hearst,
2013; Nakov, 2013). Among the previous works
on NP analysis are internal NP structure analy-
sis (Vadas and Curran, 2007; Vadas and Curran,
2008), noun-noun relation analysis of noun com-
pounds (Girju et al., 2005; Tratz and Hovy, 2010;
Kim and Baldwin, 2013), and predicate-argument
analysis of noun compounds (Lapata, 2002).

The goal of internal NP structure analysis is
to assign bracket information inside an NP (e.g.,
(lung cancer) deaths indicates that the phrase lung
cancer modifies the head deaths). In noun-noun
relation analysis, the goal is to assign one of the
predefined semantic relations to a noun compound
consisting of two nouns (e.g., assigning a relation

disaster! prevention! awareness!

ARG1!

prevent-01!disaster! awareness!

topic!

Figure 1: disaster prevention awareness in AMR.
Predicate-argument relation ARG1, noun-noun re-
lation topic, and internal structure (disaster pre-
vention) awareness are expressed.

purpose to a noun compound cooking pot, mean-
ing that pot is used for cooking). On the other
hand, in predicate-argument analysis, the goal is
to decide whether the modifier noun is the subject
or the object of the head noun (e.g., car is the ob-
ject of love in car lover, while child is the subject
of behave in child behavior).

Previous NP researches have mainly focused on
these different subproblems of NP analysis using
different data sets, rather than targeting general
NPs simultaneously. However, these subproblems
of NP analysis tend to be highly intertwined when
processing texts. For the purpose of tackling the
task of combined NP analysis, we make use of the
Abstract Meaning Representation (AMR) corpus.

AMR is a formalism of sentence semantic
structure by directed, acyclic, and rooted graphs,
in which semantic relations such as predicate-
argument relations and noun-noun relations are
expressed. In this paper, we extract substructures
corresponding to NPs (shown in Figure 1) from
the AMR Bank1, and create a data set of NP se-
mantic structures. In general, AMR substructures
are graphs. However, since we found out that NPs
mostly form trees rather than graphs in the AMR
Bank, we can assume that AMR substructures cor-
responding to NPs are trees. Thus, we define our
task as predicting the AMR tree structure, given a
sequence of words in an NP.

The previous method for AMR parsing takes a
1http://amr.isi.edu/

851



Train Dev Test
3504 463 398

Table 1: Statistics of the extracted NP data

two-step approach: first identifying distinct con-
cepts (nodes) in the AMR graph, then defin-
ing the dependency relations between those con-
cepts (Flanigan et al., 2014). In the concept iden-
tification step, unlike POS tagging, one word is
sometimes assigned with more than one concept,
and the number of possible concepts is far more
than the number of possible parts-of-speech. As
the concept identification accuracy remains low,
such a pipeline method suffers from error propaga-
tion, thus resulting in a suboptimal AMR parsing
performance.

To solve this problem, we extend a transition-
based dependency parsing algorithm, and propose
a novel algorithm which jointly identifies the con-
cepts and the relations in AMR trees. Compared
to the baseline, our method improves the perfor-
mance of AMR analysis of NP semantic structures
by 2.7 points, and using an external dictionary fur-
ther boosts the performance by 7.1 points.

2 Abstract Meaning Representation

2.1 Extraction of NPs

We extract substructures (subtrees) correspond-
ing to NPs from the AMR Bank (LDC2014T12).
In the AMR Bank, there is no alignment be-
tween the words and the concepts (nodes) in the
AMR graphs. We obtain this alignment by using
the rule-based alignment tool by Flanigan et al.
(2014). Then, we use the Stanford Parser (Klein
and Manning, 2003) to obtain constituency trees,
and extract NPs that contain more than one noun
and are not included by another NP. We ex-
clude NPs that contain named entities, because
they would require various kinds of manually
crafted rules for each type of named entity. We
also exclude NPs that contain possessive pronouns
or conjunctions, which prove problematic for the
alignment tool. Table 1 shows the statistics of the
extracted NP data.

2.2 Previous Method for AMR Analysis

We adopt the method proposed by Flanigan et
al. (2014) as our baseline, which is a two-step
pipeline method of concept identification step and

retired!

retire-01!

plant!

plant!

worker!

work-01!

person!

ARG0-of!

a

"!

Figure 2: Concept identification step of (Flanigan
et al., 2014) for a retired plant worker. ∅ denotes
an empty concept.

relation identification step. Their method is de-
signed for parsing sentences into AMR, but here,
we use this method for parsing NPs.

In their method, concept identification is formu-
lated as a sequence labeling problem (Janssen and
Limnios, 1999) and solved by the Viterbi algo-
rithm. Spans of words in the input sentence are
labeled with concept subgraphs. Figure 2 illus-
trates the concept identification step for an NP a
retired plant worker.

After the concepts have been identified, these
concepts are fixed, and the dependency rela-
tions between them are identified by an algorithm
that finds the maximum spanning connected sub-
graph (Chu and Liu, 1965), which is similar to the
maximum spanning tree (MST) algorithm used for
dependency parsing (McDonald et al., 2005).

They report that using gold concepts yields
much better performance, implying that joint iden-
tification of concepts and relations can be helpful.

3 Proposed Method

In this paper, we propose a novel approach for
mapping the word sequence in an NP to an AMR
tree, where the concepts (nodes) corresponding to
the words and the dependency relations between
those concepts must be identified. We extend the
arc-standard algorithm by Nivre (2004) for AMR
parsing, and propose a transition-based algorithm
which jointly identifies concepts and dependency

retired!

retire-01!

plant!

plant!

worker!

work-01!

person!

ARG0-of!

a

ARG0-of!

ARG2!

Figure 4: a retired plant worker in AMR

852



Previous action σ1 σ0 β R
0 (initial state) [a retired plant worker] ∅
1 SHIFT(EMPTY(a)) ∅ [retired plant worker] ∅
2 EMPTY-REDUCE [retired plant worker] ∅
3 SHIFT(DICTPRED(retired)) retire-01 [plant worker] ∅

4 SHIFT(LEMMA(plant)) retire-01 plant [worker] ∅

5 SHIFT(KNOWN(worker)) plant
person

work-01

  ARG0-of [ ] ∅

6 LEFT-REDUCE(ARG2, nchild) retire-01
person

work-01

  ARG0-of [ ] {(work-01) ARG2−−−→ (plant)}

7 LEFT-REDUCE(ARG0-of, nroot)
person

work-01

  ARG0-of [ ] {(work-01)
ARG2−−−→ (plant),

(person) ARG0-of−−−−→ (retire-01)}

Figure 3: Derivation of an AMR tree for a retired plant worker (σ0 and σ1 denote the top and the second
top of the stack, respectively.)

Action Current state Next state
SHIFT(c(wi)) (σ, [wi|β], R) ([σ|c(wi)], β, R)
LEFT-REDUCE(r, n) ([σ|ci|cj ], β, R) ([σ|cj ], β, R ∪ {nroot(ci) r←− n(cj)})
RIGHT-REDUCE(r, n) ([σ|ci|cj ], β, R) ([σ|ci], β, R ∪ {n(ci) r−→ nroot(cj)})
EMPTY-REDUCE ([σ|φ], β, R) (σ, β, R)

Table 2: Definitions of the actions

relations. Our algorithm is similar to (Hatori et al.,
2011), in which they perform POS tagging and de-
pendency parsing jointly by assigning a POS tag
to a word when performing SHIFT, but differs in
that, unlike POS tagging, one word is sometimes
assigned with more than one concept. In our al-
gorithm, the input words are stored in the buffer
and the identified concepts are stored in the stack.
SHIFT identifies a concept subtree associated with
the top word in the buffer. REDUCE identifies the
dependency relation between the top two concept
subtrees in the stack. Figure 3 illustrates the pro-
cess of deriving an AMR tree for a retired plant
worker, and Figure 4 shows the resulting AMR
tree.

Table 2 shows the definition of each action and
state transition. A state is a triple (σ,β, R), where
σ is a stack of identified concept subtrees, β is a
buffer of words, and R is a set of identified rela-
tions. SHIFT(c(wi)) extracts the top word wi in
the buffer, generates a concept subtree c(wi) from
wi, and pushes c(wi) onto the stack. The concept
subtree c(wi) is generated from wi by using one of
the rules defined in Table 3. LEFT-REDUCE(r, n)
pops the top two subtrees ci, cj from the stack,

identifies the relation r between the root nroot(ci)
of ci and the node n(cj) in cj , adds r to R, and
pushes cj back onto the stack. Here, n denotes a
mapping from a subtree to its specific node, which
allows for attachment to an arbitrary concept in
a subtree. Since the sizes of the subtrees were
at most two in our data set, n ∈ {nroot, nchild},
where nroot is a mapping from a subtree to its root,
and nchild is a mapping from a subtree to the direct
child of its root. RIGHT-REDUCE(r, n) is defined
in the same manner. EMPTY-REDUCE removes an
empty subtree ∅ at the top of the stack. EMPTY-
REDUCE is always performed immediately after
SHIFT(∅) generates an empty subtree ∅. In the
initial state, the stack σ is empty, the buffer β con-
tains all the words in the NP, and the set of the
identified relations R is empty. In the terminal
state, the buffer β is empty and the stack σ con-
tains only one subtree. The resulting AMR tree is
obtained by connecting all the subtrees generated
by the SHIFT actions with all the relations in R.

The previous method could not generate un-
seen concepts in the training data, leading to low
recall in concept identification. In contrast, our
method defines five rules (Table 3), three of which

853



Rule Concept subtree to generate Example of subtree generation
EMPTY an empty concept subtree ∅ fighters→ ∅
KNOWN a subtree aligning to the word in the training data fighters→ (person) ARG0-of−−−−→ (fight-01) | ...
LEMMA a subtree with the lemma of the word as the only concept fighters→ (fighter)
DICTPRED a subtree with a predicate form of the word as the only concept fighters→ (fight-01) | (fight-02) | ...
DICTNOUN a subtree with a noun form of the word as the only concept fighters→ (fight)

Table 3: Rules for generating a concept subtree (Vertical lines indicate multiple candidate subtrees.)

(LEMMA, DICTPRED, and DICTNOUN) allow for
generation of unseen concepts from any word.

3.1 Features

The feature set φ(s, a) for the current state s and
the next action a is the direct product (all-vs-all
combinations from each set) of the feature set
φstate(s) for the current state and the feature set
φaction(s, a) for the next action.

φ(s, a) = φstate(s)× φaction(s, a)

φstate(s) is the union of the feature sets defined
in Table 4, where w(c) denotes the word from
which the subtree c was generated.

Table 5 shows the feature set
φaction((σ, [wi|β], R), a) for each action a, where
rule(wi, c) is a function that returns the rule
which generated the subtree c from the top word
wi in the buffer. In order to allow different SHIFT
actions and different LEFT-RIGHT/REDUCE
actions to partially share features, Table 5 de-
fines features of different granularities for each
action. For example, although SHIFT((run-01))
and SHIFT((sleep-01)) are different actions, they
share the features “S”, “S”◦“DICTPRED” because
they share the generation rule DICTPRED.

4 Experiments

We conduct an experiment using our NP data set
(Table 1). We use the implementation 2 of (Flani-
gan et al., 2014) as our baseline. For the baseline,
we use the features of the default settings.

The method by Flanigan et al. (2014) can only
generate the concepts that appear in the training
data. On the other hand, our method can gen-
erate concepts that do not appear in the training
data using the concept generation rules LEMMA,
DICTPRED, and DICTNOUN in Table 3. For a fair
comparison, first, we only use the rules EMPTY
and KNOWN. Then, we add the rule LEMMA,
which can generate a concept of the lemma of the

2https://github.com/jflanigan/jamr

Name Definition
LEM {w(σ1).lem, w(σ0).lem, β0.lem,

w(σ1).lem◦w(σ0).lem, w(σ0).lem◦β0.lem}
SUF {w(σ1).suf, w(σ0).suf, β0.suf,

w(σ1).suf ◦ w(σ0).suf, w(σ0).suf ◦ β0.suf}
POS {w(σ1).pos, w(σ0).pos, β0.pos,

w(σ1).pos ◦w(σ0).pos, w(σ0).pos ◦ β0.pos}
DEP {w(σ1).dep, w(σ0).dep, β0.dep,

w(σ1).dep◦w(σ0).dep, w(σ0).dep◦β0.dep}
HEAD {w(σ1).off, w(σ0).off, β0.off,

w(σ1).off ◦ w(σ0).off, w(σ0).off ◦ β0.off}
ROOT {nroot(σ1), nroot(σ0), nroot(σ1) ◦ nroot(σ0)}
MID all words between w(σ1) and w(σ0) ∪

all words between w(σ0) and β0

Table 4: Feature sets for the state (.lem is the
lemma, .suf is the prefix of length 3, .pos is the
part-of-speech, .dep is the dependency label to the
parent word, .off is the offset to the parent word,
and ◦ denotes concatenation of features.)

Action a φaction((σ, [wi|β], R), a)
SHIFT(c) {“S”, “S” ◦ rule(wi, c),

“S” ◦ rule(wi, c) ◦ c}
LEFT-REDUCE(r, n) {“L-R”, “L-R”◦r, “L-R”◦r◦n}
RIGHT-REDUCE(r, n) {“R-R”, “R-R”◦r, “R-R”◦r◦n}
EMPTY-REDUCE {“E-R”}

Table 5: Feature sets for the action

word. Finally, we add the rules DICTPRED and
DICTNOUN. These two rules need conversion from
nouns and adjectives to their verb and noun forms,
For this conversion, we use CatVar v2.1 (Habash
and Dorr, 2003), which lists categorial variations
of words (such as verb run for noun runner). We
also use definitions of the predicates from Prop-
Bank (Palmer et al., 2005), which AMR tries to
reuse as much as possible, and impose constraints
that the defined predicates can only have semantic
relations consistent with the definition.

During the training, we use the max-violation
perceptron (Huang et al., 2012) with beam size 8
and average the parameters. During the testing, we
also perform beam search with beam size 8.

Table 6 shows the overall performance on NP
semantic structure analysis. We evaluate the per-
formance using the Smatch score (Cai and Knight,

854



Method P R F1
(Flanigan et al., 2014) 75.5 61.1 67.5
Our method (EMPTY/KNOWN) 78.0 63.8 70.2
Our method+LEMMA 75.7 75.2 75.4
Our method+LEMMA/DICT 77.3 77.3 77.3

Table 6: Performance on NP semantic structure
analysis

Method P R F1
(Flanigan et al., 2014) 88.4 71.4 79.0
Our method (EMPTY/KNOWN) 88.9 72.2 79.7
Our method+LEMMA 84.8 84.2 84.5
Our method+LEMMA/DICT 85.8 85.6 85.7

Table 7: Performance on concept identification

2013), which reports precision, recall, and F1-
score for overlaps of nodes, edges, and roots in
semantic structure graphs. Compared to the base-
line, our method improves both the precision and
recall, resulting in an increasing of F1-score by 2.7
points. When we add the LEMMA rule, the re-
call increases by 11.4 points because the LEMMA
rule can generate concepts that do not appear in
the training data, resulting in a further increase of
F1-score by 5.2 points. Finally, when we add the
DICT rules, the F1-score improves further by 1.9
points.

Table 7 shows the performance on concept iden-
tification. We report precision, recall, and F1-
score against the correct set of concepts. For each
condition, we observe the same tendency in per-
formance increases as Table 6. Thus, we conclude
that our method improves both the concept identi-
fication and relation identification performances.

5 Conclusion

In this paper, we used Abstract Meaning Repre-
sentation (AMR) for semantic structure analysis
of noun compounds (NPs). We extracted substruc-
tures corresponding to NPs from the AMR Bank,
and created a data set of NP semantic structures.
Then, we proposed a novel method which jointly
identifies concepts (nodes) and dependency rela-
tions in AMR trees. We confirmed that our method
improves the performance on NP semantic struc-
ture analysis, and that incorporating an external
dictionary further boosts the performance.

Acknowledgements

This work was supported by JSPS KAKENHI
Grant Number 15K16053, 26240035.

References
Shu Cai and Kevin Knight. 2013. Smatch: An eval-

uation metric for semantic feature structures. pages
748–752.

Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrimi-
native graph-based parser for the Abstract Meaning
Representation. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1426–1436.

Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun com-
pounds. Computer Speech and Language, 19:479–
496.

Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
the 2003 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 17–23.

Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint POS tag-
ging and dependency parsing in Chinese. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing, pages 1216–1224.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151.

Jacques Janssen and Nikolaos Limnios. 1999. Semi-
Markov models and applications. Springer, Octo-
ber.

Su Nam Kim and Timothy Baldwin. 2013. A lexi-
cal semantic approach to interpreting and bracketing
English noun compounds. Natural Language Engi-
neering, 19(3):385–407.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 423–430.

Maria Lapata. 2002. The disambiguation of nomi-
nalizations. Computational Linguistics, 28(3):357–
388.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP), pages 523–530.

855



Preslav I. Nakov and Marti A. Hearst. 2013. Semantic
interpretation of noun compounds using verbal and
other paraphrases. ACM Transactions on Speech
and Language Processing, 10(3):1–51.

Preslav Nakov. 2013. On the interpretation of noun
compounds: Syntax, semantics, and entailment.
Natural Language Engineering, 19(1):291–330.

Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.

Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 678–687.

David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, pages 240–247.

David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 335–343.

856


