



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 26–31
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2005

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 26–31
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2005

A Principled Framework for Evaluating Summarizers: Comparing
Models of Summary Quality against Human Judgments

Maxime Peyrard and Judith Eckle-Kohler
Research Training Group AIPHES and UKP Lab

Computer Science Department, Technische Universität Darmstadt
www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de

Abstract

We present a new framework for evaluat-
ing extractive summarizers, which is based
on a principled representation as optimiza-
tion problem. We prove that every ex-
tractive summarizer can be decomposed
into an objective function and an opti-
mization technique. We perform a com-
parative analysis and evaluation of sev-
eral objective functions embedded in well-
known summarizers regarding their corre-
lation with human judgments. Our com-
parison of these correlations across two
datasets yields surprising insights into the
role and performance of objective func-
tions in the different summarizers.

1 Introduction

The task of extractive summarization (ES) can nat-
urally be cast as a discrete optimization problem
where the text source is considered as a set of sen-
tences and the summary is created by selecting an
optimal subset of the sentences under a length con-
straint (McDonald, 2007; Lin and Bilmes, 2011).

In this work, we go one step further and mathe-
matically prove that ES is equivalent to the prob-
lem of choosing (i) an objective function θ for
scoring system summaries, and (ii) an optimizer
O. We use (θ, O) to denote the resulting decompo-
sition of any extractive summarizer. Our proposed
decomposition enables a principled analysis and
evaluation of existing summarizers, and addresses
a major issue in the current evaluation of ES.

This issue concerns the traditional “intrinsic”
evaluation comparing system summaries against
human reference summaries. This kind of evalu-
ation is actually an end-to-end evaluation of sum-
marization systems which is performed after θ has
been optimized by O. This is highly problematic

from an evaluation point of view, because first,
θ is typically not optimized exactly, and second,
there might be side-effects caused by the particu-
lar optimization technique O, e.g., a sentence ex-
tracted to maximize θ might be suitable because of
other properties not included in θ. Moreover, the
commonly used evaluation metric ROUGE yields
a noisy surrogate evaluation (despite its good cor-
relation with human judgments) compared to the
much more meaningful evaluation based on hu-
man judgments. As a result, the current end-to-
end evaluation does not provide any insights into
the task of automatic summarization.

The (θ,O) decomposition we propose addresses
this issue: it enables a well-defined and principled
evaluation of extractive summarizers on the level
of their components θ and O. In this work, we fo-
cus on the analysis and evaluation of θ, because
θ is a model of the quality indicators of a sum-
mary, and thus crucial in order to understand the
properties of “good” summaries. Specifically, we
compare θ functions of different summarizers by
measuring the correlation of their θ functions with
human judgments.

Our goal is to provide an evaluation framework
which the research community could build upon
in future research to identify the best possible θ
and use it in optimization-based systems. We be-
lieve that the identification of such a θ is the cen-
tral question of summarization, because this op-
timal θ would represent an optimal definition of
summary quality both from an algorithmic point
of view and from the human perspective.

In summary, our contribution is twofold: (i) We
present a novel and principled evaluation frame-
work for ES which allows evaluating the objec-
tive function and the optimization technique sep-
arately and independently. (ii) We compare well-
known summarization systems regarding their im-
plicit choices of θ by measuring the correlation

26

https://doi.org/10.18653/v1/P17-2005
https://doi.org/10.18653/v1/P17-2005


of their θ functions with human judgments on
two datasets from the Text Analysis Conference
(TAC). Our comparative evaluation yields surpris-
ing results and shows that extractive summariza-
tion is not solved yet.

The code used in our experiments, includ-
ing a general evaluation tool is available at
github.com/UKPLab/acl2017-theta_
evaluation_summarization.

2 Evaluation Framework

2.1 (θ,O) decomposition
Let D = {si} be a document collection consid-
ered as a set of sentences. A summary S is then a
subset of D, or we can say that S is an element of
P(D), the power set of D.
Objective function We define an objective func-
tion to be a function that takes a summary of the
document collection D and outputs a score:

θ : P(D) → R
S 7→ θD(S) (1)

Optimizer Then the task of ES is to select the
set of sentences S∗ with maximal θ(S∗) under a
length constraint:

S∗ = argmax
S

θ(S)

len(S) =
∑

s∈S
len(s) ≤ c (2)

We use O to denote the technique which solves
this optimization problem. O is an operator which
takes an objective function θ from the set of all
objective functions Θ and a document collection
D from the set of all document collections D, and
outputs a summary S∗:

O : Θ×D → S
(θ,D) 7→ S∗ (3)

Decomposition Theorem Now we show that the
problem of ES is equivalent to the problem of
choosing a decomposition (θ, O).

We formalize an extractive summarizer σ as a
set function which takes a document collection
D ∈ D and outputs a summary SD,σ ∈ P(D).
With this formalism, it is clear that every (θ,O) tu-
ple forms a summarizer because O(θ, ·) produces
a summary from a document collection.

But the other direction is also true: for every ex-
tractive summarizer there exists at least one tuple
(θ, O) which perfectly describes the summarizer:

Theorem 1 ∀σ, ∃(θ,O) such that:
∀D ∈ D, σ(D) = O(θ,D)

This theorem is quite intuitive, especially since
it is common to use a similar decomposition in
optimization-based summarization systems. In the
next section we illustrate the theorem by way of
several examples, and provide a rigorous proof of
the existence in the supplemental material.

2.2 Examples of θ
We analyze a range of different summarizers re-
garding their (mostly implicit) θ.
ICSI (Gillick and Favre, 2009) is a global linear
optimization that extracts a summary by solving a
maximum coverage problem considering the most
frequent bigrams in the source documents. ICSI
has been among the best systems in a classical
ROUGE evaluation (Hong et al., 2014). For ICSI,
the identification of θ is trivial because it was for-
mulated as an optimization task. If ci is the i-th
bigram selected in the summary and wi its weight
computed from D, then:

θICSI(S) =
∑

ci∈S
ci ∗ wi (4)

LexRank (Erkan and Radev, 2004) is a well-
known graph-based approach. A similarity graph
G(V,E) is constructed where V is the set of sen-
tences and an edge eij is drawn between sentences
vi and vj if and only if the cosine similarity be-
tween them is above a given threshold. Sentences
are scored according to their PageRank score inG.
We observe that θLexRank is given by:

θLexRank(S) =
∑

s∈S
PRG(s) (5)

where PR is the PageRank score of sentence s.
KL-Greedy (Haghighi and Vanderwende, 2009)
minimizes the Kullback Leibler (KL) divergence
between the word distributions in the summary
and D (i.e θKL = −KL). Recently, Peyrard and
Eckle-Kohler (2016) optimized KL and Jensen
Shannon (JS) divergence with a genetic algorithm.
In this work, we use KL and JS for both unigram
and bigram distributions.
LSA (Steinberger and Jezek, 2004) is an approach
involving a dimensionality reduction of the term-
document matrix via Singular Value Decomposi-
tion (SVD). The sentences extracted should cover
the most important latent topics:

θLSA =
∑

t∈S
λt (6)

27



where t is a latent topic identified by SVD on the
term-document matrix and λt the associated sin-
gular value.
Edmundson (Edmundson, 1969) is an older
heuristic method which scores sentences accord-
ing to cue-phrases, overlap with title, term fre-
quency and sentence position. θEdmundson is sim-
ply a weighted sum of these heuristics.
TF?IDF (Luhn, 1958) scores sentences with the
TF*IDF of their terms. The best sentences are then
greedily extracted. We use both the unigram and
bigram versions in our experiments.

3 Experiments

Now we compare the summarizers analyzed above
by measuring the correlation of their θ functions
with human judgments.

Datasets We use two multi-document summa-
rization datasets from the Text Analysis Confer-
ence (TAC) shared task: TAC-2008 and TAC-
2009.1 TAC-2008 and TAC-2009 contain 48 and
44 topics, respectively. Each topic consists of 10
news articles to be summarized in a maximum of
100 words. We use only the so-called initial sum-
maries (A summaries), but not the update part.

For each topic, there are 4 human reference
summaries along with a manually created Pyramid
set. In both editions, all system summaries and
the 4 reference summaries were manually evalu-
ated by NIST assessors for readability, content se-
lection (with Pyramid) and overall responsiveness.
At the time of the shared tasks, 57 systems were
submitted to TAC-2008 and 55 to TAC-2009. For
our experiments, we use the Pyramid and the re-
sponsiveness annotations.

System Comparison For each θ, we compute
the scores of all system and all manual summaries
for any given topic. These scores are compared
with the human scores. We include the manual
summaries in our computation because this yields
a more diverse set of summaries with a wider
range of scores. Since an ideal summarizer would
create summaries as well as humans, an ideal θ
would also be able to correctly score human sum-
maries with high scores.

For comparison, we also report the correlation
between pyramid and responsiveness.

Correlations are measured with 3 metrics: Pear-
1http://tac.nist.gov/2009/

Summarization/, http://tac.nist.gov/2008/
Summarization/

son’s r, Spearman’s ρ and Normalized Discounted
Cumulative Gain (Ndcg). Pearson’s r is a value
correlation metric which depicts linear relation-
ships between the scores produced by θ and the
human judgments. Spearman’s ρ is a rank correla-
tion metric which compares the ordering of sys-
tems induced by θ and the ordering of systems
induced by human judgments. Ndcg is a metric
that compares ranked lists and puts more emphasis
on the top elements by logarithmic decay weight-
ing. Intuitively, it captures how well θ can rec-
ognize the best summaries. The optimization sce-
nario benefits from high Ndcg scores because only
summaries with high θ scores are extracted.

Previous work on correlation analysis averaged
scores over topics for each system and then com-
puted the correlation between averaged scores
(Louis and Nenkova, 2013; Nenkova et al., 2007).
An alternative and more natural option which we
use here is to compute the correlation for each
topic and average these correlations over topics
(CORRELATION-AVERAGE). Since we want to
estimate how well θ functions measure the quality
of summaries, we find the summary level averag-
ing more meaningful.

Analysis The results of our correlation analysis
are presented in Table 1.

In our (θ,O) formulation, the end-to-end ap-
proach maps a set of documents to exactly one
summary selected by the system. We call the (clas-
sical and well known) evaluation of this single
summary end-to-end evaluation because it mea-
sures the end product of the system. This is in con-
trast to our proposed evaluation of the assumption
made by individual summarizers shown in Table 1.
A system summary was extracted by a given sys-
tem because it was high scoring using its θ, but we
ask the question whether optimizing this θ made
sense in the first place.

We first observe that scores are relatively low.
Summarization is not a solved problem and the
systems we investigated can not identify correctly
what makes a good summary. This is in contrast
to the picture in the classical end-to-end evaluation
with ROUGE where state-of-the-art systems score
relatively high. Some Ndcg scores are higher (for
TAC-2008) which explains why these systems can
extract relatively good summaries in the end-to-
end evaluation. In this classical evaluation, only
the single best summary is evaluated, which means
that a system does not need to be able to rank all

28



TAC-2008 TAC-2009
responsiveness Pyramid responsiveness Pyramid

θ r ρ Ndcg r ρ Ndcg r ρ Ndcg r ρ Ndcg

TF∗IDF-1 .1777 .2257 .5031 .1850 .2386 .3575 .1996 .2282 .3826 .2514 .2890 .2280
TF∗IDF-2 .0489 .1548 .5952 .0507 .1833 .4811 .0061 .1736 .4984 .1073 .2383 .3844
ICSI .1069 .1885 .6153 .1147 .2294 .5228 .1050 .1821 .5707 .1379 .2466 .5016
JS-1 .2504 .2762 .4411 .2798 .3205 .2804 .2021 .2282 .3896 .2616 .3042 .2272
JS-2 .0383 .1698 .5873 .0410 .2038 .4804 .0284 .1475 .5646 .0021 .2084 .4734
LexRank .1995 .1821 .6618 .2498 .2168 .5935 .2831 .2585 .6028 .3714 .3421 .5764
LSA .0437 .1137 .6772 .1144 .1131 .5997 .2965 .2127 .6641 .3677 .2935 .6467
Edmunds. .2223 .2686 .6372 .2665 .3164 .5521 .2598 .2604 .5852 .3647 .3720 .5594
KL-1 .1796 .2249 .4899 .2016 .2690 .3439 .1827 .2275 .4047 .2423 .2981 .2466
KL-2 .0023 .1661 .6165 .0023 .1928 .5135 .0437 .1435 .6171 .0211 .2060 .5462

Pyramid .7031 .6606 .8528 — — — .7174 .6414 .8520 — — —

Table 1: Correlation of θ functions with human judgments across various systems.

possible summaries correctly.
We see that systems with high end-to-end

ROUGE scores (according to Hong et al. (2014))
do not necessarily have a good model of summary
quality. Indeed, the best performing θ functions
are not part of the systems performing best with
ROUGE. For example, ICSI is the best system ac-
cording to ROUGE, but it is not clear that it has
the best model of summary quality. In TAC-2009,
LexRank, LSA and the heuristic Edmundson have
better correlations with human judgments. The
difference with end-to-end evaluation might stem
from the fact that ICSI solves the optimization
problem exactly, while LexRank and Edmundson
use greedy optimizers. There might also be some
side-effects from which ICSI profits: extracting
sentences to improve θ might lead to accidentally
selecting suitable sentences, because θ can merely
correlate well with properties of good summaries,
while not modeling these properties itself.

It is worth noting that systems perform differ-
ently on TAC2009 and TAC2008. There are sev-
eral differences between TAC2008 and TAC2009
like redundancy level or guidelines for annota-
tions; for example, responsiveness is scored out
of 5 in 2008 and out of 10 in 2009. The LSA sum-
marizer ranks among the best systems in TAC2009
with pearson’s r but is closer to the worst sys-
tems in TAC2008. While this is difficult to ex-
plain we hypothesize that the model of summary
quality from LSA is sensitive to the slight vari-
ations and therefore not robust. In general, any
system which claims to have a better θ than previ-
ous works should indeed report results on several
datasets to ensure robustness and generality.

Interestingly, we observe that the correlation be-
tween Pyramid and responsiveness is better than in

any system, but still not particularly high. Respon-
siveness is an overall annotation while Pyramid is
a manual measure of content only. These results
confirm the intuition that humans take into account
much more aspects when evaluating summaries.

4 Related Work and Discussion

While correlation analyses on human judgment
data have been performed in the context of validat-
ing automatic summary evaluation metrics (Louis
and Nenkova, 2013; Nenkova et al., 2007; Lin,
2004), there is no prior work which uses these data
for a principled comparison of summarizers.

Much previous work focused on efficient opti-
mizers O, such as ILP, which impose constraints
on the θ function. Linear (Gillick and Favre, 2009)
and submodular (Lin and Bilmes, 2011) θ func-
tions are widespread in the summarization com-
munity because they can be optimized efficiently
and effectively via ILP (Schrijver, 1986) and the
greedy algorithm for submodularity (Fujishige,
2005). A greedy approach is often used when θ
does not have convenient properties that can be
leveraged by a classical optimizer (Haghighi and
Vanderwende, 2009).

Such interdependencies of O and θ limit the ex-
pressiveness of θ. However, realistic θ functions
are unlikely to be linear or submodular, and in
the well-studied field of optimization there exist
a range of different techniques developed to tackle
difficult combinatorial problems (Schrijver, 2003;
Blum and Roli, 2003).

A recent example of such a technique adapted to
extractive summarization are meta-heuristics used
to optimize non-linear, non-submodular objec-
tive functions (Peyrard and Eckle-Kohler, 2016).

29



Other methods like Markov Chain Monte Carlo
(Metropolis et al., 1953) or Monte-Carlo Tree
Search (Suttner and Ertel, 1991; Silver et al.,
2016) could also be adapted to summarization and
thus become realistic choices for O. General pur-
pose optimization techniques are especially ap-
pealing, because they offer a decoupling of θ and
O and allow investigating complex θ functions
without making any assumption on their mathe-
matical properties. In particular, this supports fu-
ture work on identifying an “optimal” θ as a model
of relevant quality aspects of a summary.

5 Conclusion

We presented a novel evaluation framework for ES
which is based on the proof that ES is equivalent
to the problem of choosing an objective function
θ and an optimizer O. This principled and well-
defined framework allows evaluating θ and O of
any extractive summarizer – separately and inde-
pendently. We believe that our framework can
serve as a basis for future work on identifying an
“optimal” θ function, which would provide an an-
swer to the central question of what are the prop-
erties of a “good” summary.

Acknowledgments

This work has been supported by the German Re-
search Foundation (DFG) as part of the Research
Training Group “Adaptive Preparation of Informa-
tion from Heterogeneous Sources” (AIPHES) un-
der grant No. GRK 1994/1, and via the German-
Israeli Project Cooperation (DIP, grant No. GU
798/17-1).

References
Christian Blum and Andrea Roli. 2003. Metaheuris-

tics in Combinatorial Optimization: Overview and
Conceptual Comparison. ACM Computing Surveys
35(3):268–308.

H. P. Edmundson. 1969. New Methods in Automatic
Extracting. Journal of the Association for Comput-
ing Machinery 16(2):264–285.

Günes Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based Lexical Centrality As Salience in Text
Summarization. Journal of Artificial Intelligence
Research pages 457–479.

Satoru Fujishige. 2005. Submodular functions and op-
timization. Annals of discrete mathematics. Else-
vier, Amsterdam, Boston, Paris.

Dan Gillick and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Boulder, Colorado, pages 10–18.

Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-document Summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Boulder, Colorado, pages 362–370.

Kai Hong, John Conroy, benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A Reposi-
tory of State of the Art and Competitive Base-
line Summaries for Generic News Summarization.
In Proceedings of the Ninth International Con-
ference on Language Resources and Evaluation
(LREC’14). European Language Resources Asso-
ciation (ELRA), Reykjavik, Iceland, pages 1608–
1616.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop. Association for Computational Linguis-
tics, Barcelona, Spain, pages 74–81.

Hui Lin and Jeff A. Bilmes. 2011. A Class of Sub-
modular Functions for Document Summarization.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Portland, Oregon, USA, pages
510–520.

Annie Louis and Ani Nenkova. 2013. Automati-
cally Assessing Machine Summary Content With-
out a Gold Standard. Computational Linguistics
39(2):267–300.

Hans Peter Luhn. 1958. The Automatic Creation of
Literature Abstracts. IBM Journal of Research De-
velopment 2:159–165.

Ryan McDonald. 2007. A Study of Global Inference
Algorithms in Multi-document Summarization. In
Proceedings of the 29th European Conference on IR
Research. Springer-Verlag, Rome, Italy, pages 557–
564.

Nicholas Metropolis, Arianna Rosenbluth, Marshall
Rosenbluth, Augusta Teller, and Edward Teller.
1953. Equation of State Calculations by Fast Com-
puting Machines. Journal of Chemical Physics
21:1087 – 1092.

Ani Nenkova, Rebecca Passonneau, and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing Human Content Selection Variation in Summa-
rization Evaluation. ACM Transactions on Speech
and Language Processing (TSLP) 4(2).

30



Maxime Peyrard and Judith Eckle-Kohler. 2016.
A General Optimization Framework for Multi-
Document Summarization Using Genetic Algo-
rithms and Swarm Intelligence. In Proceedings of
the 26th International Conference on Computational
Linguistics (COLING 2016). The COLING 2016 Or-
ganizing Committee, Osaka, Japan, pages 247 – 257.

Alexander Schrijver. 1986. Theory of Linear and In-
teger Programming. John Wiley & Sons, Inc., New
York, NY, USA.

Alexander Schrijver. 2003. Combinatorial Optimiza-
tion - Polyhedra and Efficiency. Springer, New
York.

David Silver, Aja Huang, Chris J. Maddison, Arthur
Guez, Laurent Sifre, George van den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, Sander Dieleman, Do-
minik Grewe, John Nham, Nal Kalchbrenner, Ilya
Sutskever, Timothy Lillicrap, Madeleine Leach, Ko-
ray Kavukcuoglu, Thore Graepel, and Demis Has-
sabis. 2016. Mastering the game of Go with
deep neural networks and tree search. Nature
529(7587):484–489.

Josef Steinberger and Karel Jezek. 2004. Using latent
semantic analysis in text summarization and sum-
mary evaluation. In Proceedings of the 7th Inter-
national Conference on Information Systems Imple-
mentation and Modelling (ISIM ’04). Rožnov pod
Radhoštěm, Czech Republic, pages 93–100.

Christian Suttner and Wolfgang Ertel. 1991. Using
Back-Propagation Networks for Guiding the Search
of a Theorem Prover. International Journal of Neu-
ral Networks Research & Applications 2(1):3–16.

31


	A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments

