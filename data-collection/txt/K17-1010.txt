



















































Learning What is Essential in Questions


Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 80–89,
Vancouver, Canada, August 3 - August 4, 2017. c©2017 Association for Computational Linguistics

CoNLL’17

Learning What is Essential in Questions

Daniel Khashabi†
Univ. of Pennsylvania

danielkh@cis.upenn.edu

Tushar Khot Ashish Sabharwal
Allen Institute for AI

tushark,ashishs@allenai.org

Dan Roth†
Univ. of Pennsylvania

danroth@cis.upenn.edu

Abstract

Question answering (QA) systems are eas-
ily distracted by irrelevant or redundant
words in questions, especially when faced
with long or multi-sentence questions in
difficult domains. This paper introduces
and studies the notion of essential ques-
tion terms with the goal of improving
such QA solvers. We illustrate the im-
portance of essential question terms by
showing that humans’ ability to answer
questions drops significantly when essen-
tial terms are eliminated from questions.
We then develop a classifier that reliably
(90% mean average precision) identifies
and ranks essential terms in questions. Fi-
nally, we use the classifier to demonstrate
that the notion of question term essen-
tiality allows state-of-the-art QA solvers
for elementary-level science questions to
make better and more informed decisions,
improving performance by up to 5%.

We also introduce a new dataset of over
2,200 crowd-sourced essential terms anno-
tated science questions.

1 Introduction

Understanding what a question is really about is
a fundamental challenge for question answering
systems that operate with a natural language in-
terface. In domains with multi-sentence ques-
tions covering a wide array of subject areas, such
as standardized tests for elementary level science,
the challenge is even more pronounced (Clark,
2015). Many QA systems in such domains

† Most of the work was done when the first and last
authors were affiliated with the University of Illinois, Urbana-
Champaign.

derive significant leverage from relatively shal-
low Information Retrieval (IR) and statistical cor-
relation techniques operating on large unstruc-
tured corpora (Kwok et al., 2001; Clark et al.,
2016). Inference based QA systems operating on
(semi-)structured knowledge formalisms have also
demonstrated complementary strengths, by using
optimization formalisms such as Semantic Pars-
ing (Yih et al., 2014), Integer Linear Program
(ILP) (Khashabi et al., 2016), and probabilistic
logic formalisms such as Markov Logic Networks
(MLNs) (Khot et al., 2015).

These QA systems, however, often struggle
with seemingly simple questions because they are
unable to reliably identify which question words
are redundant, irrelevant, or even intentionally dis-
tracting. This reduces the systems’ precision and
results in questionable “reasoning” even when the
correct answer is selected among the given alter-
natives. The variability of subject domain and
question style makes identifying essential question
words challenging. Further, essentiality is context
dependent—a word like ‘animals’ can be critical
for one question and distracting for another. Con-
sider the following example:

One way animals usually respond to a sudden drop in tem-
perature is by (A) sweating (B) shivering (C) blinking (D)
salivating.

A state-of-the-art optimization based QA system
called TableILP (Khashabi et al., 2016), which
performs reasoning by aligning the question to
semi-structured knowledge, aligns only the word
‘animals’ when answering this question. Not sur-
prisingly, it chooses an incorrect answer. The issue
is that it does not recognize that “drop in tempera-
ture” is an essential aspect of the question.

Towards this goal, we propose a system that can
assign an essentiality score to each term in the
question. For the above example, our system gen-

80



0

0.25

0.5

0.75

1

O
ne

w
ay
	  

an
im
al
s

us
ua
lly
	  

re
sp
on

d	   to
	   a	  

su
dd

en

dr
op

	   in

te
m
pe
ra
tu
re
	   is by

Chart	  Title

Figure 1: Essentiality scores generated by our
system, which assigns high essentiality to “drop”
and “temperature”.

erates the scores shown in Figure 1, where more
weight is put on “temperature” and “sudden drop”.
A QA system, when armed with such information,
is expected to exhibit a more informed behavior.

We make the following contributions:
(A) We introduce the notion of question term

essentiality and release a new dataset of 2,223
crowd-sourced essential term annotated questions
(total 19K annotated terms) that capture this con-
cept.1 We illustrate the importance of this con-
cept by demonstrating that humans become sub-
stantially worse at QA when even a few essential
question terms are dropped.

(B) We design a classifier that is effective at pre-
dicting question term essentiality. The F1 (0.80)
and per-sentence mean average precision (MAP,
0.90) scores of our classifier supercede the closest
baselines by 3%-5%. Further, our classifier gener-
alizes substantially better to unseen terms.

(C) We show that this classifier can be used
to improve a surprisingly effective IR based QA
system (Clark et al., 2016) by 4%-5% on previ-
ously used question sets and by 1.2% on a larger
question set. We also incorporate the classifier
in TableILP (Khashabi et al., 2016), resulting in
fewer errors when sufficient knowledge is present
for questions to be meaningfully answerable.

1.1 Related Work
Our work can be viewed as the study of an inter-
mediate layer in QA systems. Some systems im-
plicitly model and learn it, often via indirect sig-
nals from end-to-end training data. For instance,
Neural Networks based models (Wang et al., 2016;
Tymoshenko et al., 2016; Yin et al., 2016) implic-
itly compute some kind of attention. While this is
intuitively meant to weigh key words in the ques-
tion more heavily, this aspect hasn’t been system-

1 Annotated dataset and classifier available at https:
//github.com/allenai/essential-terms

atically evaluated, in part due to the lack of ground
truth annotations.

There is related work on extracting question
type information (Li and Roth, 2002; Li et al.,
2007) and applying it to the design and analysis of
end-to-end QA systems (Moldovan et al., 2003).
The concept of term essentiality studied in this
work is different, and so is our supervised learn-
ing approach compared to the typical rule-based
systems for question type identification.

Another line of relevant work is sentence com-
pression (Clarke and Lapata, 2008), where the
goal is to minimize the content while maintain-
ing grammatical soundness. These approaches
typically build an internal importance assignment
component to assign significance scores to various
terms, which is often done using language models,
co-occurrence statistics, or their variants (Knight
and Marcu, 2002; Hori and Sadaoki, 2004). We
compare against unsupervised baselines inspired
by such importance assignment techniques.

In a similar spirit, Park and Croft (2015) use
translation models to extract key terms to prevent
semantic drift in query expansion.

One key difference from general text summa-
rization literature is that we operate on questions,
which tend to have different essentiality charac-
teristics than, say, paragraphs or news articles. As
we discuss in Section 2.1, typical indicators of es-
sentiality such as being a proper noun or a verb
(for event extraction) are much less informative for
questions. Similarly, while the opening sentence
of a Wikipedia article is often a good summary, it
is the last sentence (in multi-sentence questions)
that contains the most pertinent words.

In parallel to our effort, Jansen et al. (2017) re-
cently introduced a science QA system that uses
the notion of focus words. Their rule-based system
incorporates grammatical structure, answer types,
etc. We take a different approach by learning a
supervised model using a new annotated dataset.

2 Essential Question Terms

In this section, we introduce the notion of essential
question terms, present a dataset annotated with
these terms, and describe two experimental studies
that illustrate the importance of this notion—we
show that when dropping terms from questions,
humans’ performance degrades significantly faster
if the dropped terms are essential question terms.

Given a question q, we consider each non-

81



stopword token in q as a candidate for being an
essential question term. Precisely defining what
is essential and what isn’t is not an easy task and
involves some level of inherent subjectivity. We
specified three broad criteria: 1) altering an es-
sential term should change the intended meaning
of q, 2) dropping non-essential terms should not
change the correct answer for q, and 3) grammat-
ical correctness is not important. We found that
given these relatively simple criteria, human anno-
tators had a surprisingly high agreement when an-
notating elementary-level science questions. Next
we discuss the specifics of the crowd-sourcing task
and the resulting dataset.

2.1 Crowd-Sourced Essentiality Dataset

We collected 2,223 elementary school science
exam questions for the annotation of essential
terms. This set includes the questions used by
Clark et al. (2016)2 and additional ones obtained
from other public resources such as the Internet
or textbooks. For each of these questions, we
asked crowd workers3 to annotate essential ques-
tion terms based on the above criteria as well as a
few examples of essential and non-essential terms.
Figure 2 depicts the annotation interface.

The questions were annotated by 5 crowd work-
ers,4 and resulted in 19,380 annotated terms. The
Fleiss’ kappa statistic (Fleiss, 1971) for this task
was κ = 0.58, indicating a level of inter-annotator
agreement very close to ‘substantial’. In particu-
lar, all workers agreed on 36.5% of the terms and
at least 4 agreed on 69.9% of the terms. We use
the proportion of workers that marked a term as
essential to be its annotated essentiality score.

On average, less than one-third (29.9%) of the
terms in each question were marked as essential
(i.e., score > 0.5). This shows the large propor-
tion of distractors in these science tests (as com-
pared to traditional QA datasets), further showing
the importance of this task. Next we provide some
insights into these terms.

We found that part-of-speech (POS) tags are not
a reliable predictor of essentiality, making it diffi-
cult to hand-author POS tag based rules. Among

2These are the only publicly available state-level science
exams. http://www.nysedregents.org/Grade4/Science/

3We use Amazon Mechanical Turk for crowd-sourcing.
4A few invalid annotations resulted in about 1% of the

questions receiving fewer annotations. 2,199 questions re-
ceived at least 5 annotations (79 received 10 annotations due
to unintended question repetition), 21 received 4 annotations,
and 4 received 3 annotations.

the proper nouns (NNP, NNPS) mentioned in the
questions, fewer than half (47.0%) were marked
as essential. This is in contrast with domains such
as news articles where proper nouns carry per-
haps the most important information. Nearly two-
thirds (65.3%) of the mentioned comparative ad-
jectives (JJR) were marked as essential, whereas
only a quarter of the mentioned superlative ad-
jectives (JJS) were deemed essential. Verbs were
marked essential less than a third (32.4%) of the
time. This differs from domains such as math
word problems where verbs have been found to
play a key role (Hosseini et al., 2014).

The best single indicator of essential terms, not
surprisingly, was being a scientific term5 (such as
precipitation and gravity). 76.6% of such terms
occurring in questions were marked as essential.

In summary, we have a term essentiality an-
notated dataset of 2,223 questions. We split this
into train/development/test subsets in a 70/9/21 ra-
tio, resulting in 483 test sentences used for per-
question evaluation.

We also derive from the above an annotated
dataset of 19,380 terms by pooling together all
terms across all questions. Each term in this larger
dataset is annotated with an essentiality score in
the context of the question it appears in. This
results in 4,124 test instances (derived from the
above 483 test questions). We use this dataset for
per-term evaluation.

2.2 The Importance of Essential Terms

Here we report a second crowd-sourcing experi-
ment that validates our hypothesis that the ques-
tion terms marked above as essential are, in fact,
essential for understanding and answering the
questions. Specifically, we ask: Is the question
still answerable by a human if a fraction of the
essential question terms are eliminated? For in-
stance, the sample question in the introduction is
unanswerable when “drop” and “temperature” are
removed from the question: One way animals usu-
ally respond to a sudden * in * is by ?

To this end, we consider both the annotated es-
sentiality scores as well as the score produced by
our trained classifier (to be presented in Section
3). We first generate candidate sets of terms to
eliminate using these essentiality scores based on a
threshold ξ ∈ {0, 0.2, . . . , 1.0}: (a) essential set:
terms with score≥ ξ; (b) non-essential set: terms

5We use 9,144 science terms from Khashabi et al. (2016).

82



Figure 2: Crowd-sourcing interface for annotating essential terms in a question, including the criteria for
essentiality and sample annotations.

Figure 3: Crowd-sourcing interface for verifying the validity of essentiality annotations generated by the
first task. Annotators are asked to answer, if possible, questions with a group of terms dropped.

with score < ξ. We then ask crowd workers to try
to answer a question after replacing each candidate
set of terms with “***”. In addition to four orig-
inal answer options, we now also include “I don’t
know. The information is not enough” (cf. Fig-
ure 3 for the user interface).6 For each value of ξ,
we obtain 5 × 269 annotations for 269 questions.
We measure how often the workers feel there is
sufficient information to attempt the question and,
when they do attempt, how often do they choose
the right answer.

Each value of ξ results in some fraction of terms
to be dropped from a question; the exact num-
ber depends on the question and on whether we

6It is also possible to directly collect essential term groups
using this task. However, collecting such sets of essential
terms would be substantially more expensive, as one must
iterate over exponentially many subsets rather than the linear
number of terms used in our annotation scheme.

use annotated scores or our classifier’s scores. In
Figure 4, we plot the average fraction of terms
dropped on the horizontal axis and the correspond-
ing fraction of questions attempted on the verti-
cal axis. Solid lines indicate annotated scores and
dashed lines indicate classifier scores. Blue lines
(bottom left) illustrate the effect of eliminating es-
sential sets while red lines (top right) reflect elim-
inating non-essential sets.

We make two observations. First, the solid blue
line (bottom-left) demonstrates that dropping even
a small fraction of question terms marked as es-
sential dramatically reduces the QA performance
of humans. E.g., dropping just 12% of the terms
(with high essentiality scores) makes 51% of the
questions unanswerable. The solid red line (top-
right), on the other hand, shows the opposite trend
for terms marked as not-essential: even after drop-

83



0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
fraction of question terms dropped

0

0.2

0.4

0.6

0.8

1
fr

ac
tio

n 
of

 q
ue

st
io

ns
 a

tte
m

pt
ed

Annotation:drop-essentials-above-x
Annotation:drop-essentials-below-x
Classifier:drop-essentials-above-x
Classifier:drop-essentials-below-x

Figure 4: The relationship between the frac-
tion of question words dropped and the fraction
of the questions attempted (fraction of the ques-
tions workers felt comfortable answering). Drop-
ping most essential terms (blue lines) results in
very few questions remaining answerable, while
least essential terms (red lines) allows most ques-
tions to still be answerable. Solid lines indicate
human annotation scores while dashed lines indi-
cate predicted scores.

ping 80% of such terms, 65% of the questions re-
mained answerable.

Second, the dashed lines reflecting the results
when using scores from our ET classifier are very
close to the solid lines based on human annotation.
This indicates that our classifier, to be described
next, closely captures human intuition.

3 Essential Terms Classifier

Given the dataset of questions and their terms an-
notated with essential scores, is it possible to learn
the underlying concept? Towards this end, given a
question q , answer options a, and a question term
ql, we seek a classifier that predicts whether ql is
essential for answering q. We also extend it to pro-
duce an essentiality score et(ql, q, a) ∈ [0, 1].7 We
use the annotated dataset from Section 2, where
real-valued essentiality scores are binarized to 1 if
they are at least 0.5, and to 0 otherwise.

We train a linear SVM classifier (Joachims,
1998), henceforth referred to as ET classifier.
Given the complex nature of the task, the fea-
tures of this classifier include syntactic (e.g., de-
pendency parse based) and semantic (e.g., Brown

7The essentiality score may alternatively be defined as
et(ql, q), independent of the answer options a. This is more
suitable for non-multiple choice questions. Our system uses
a only to compute PMI-based statistical association features
for the classifier. In our experiments, dropping these features
resulted in only a small drop in the classifier’s performance.

cluster representation of words (Brown et al.,
1992), a list of scientific words) properties of ques-
tion words, as well as their combinations. In total,
we use 120 types of features (cf. Appendix ?? of
our Extended edition (Khashabi et al., 2017)).

Baselines. To evaluate our approach, we devise
a few simple yet relatively powerful baselines.

First, for our supervised baseline, given
(ql, q, a) as before, we ignore q and compute how
often is ql annotated as essential in the entire
dataset. In other words, the score for ql is the
proportion of times it was marked as essential in
the annotated dataset. If the instance is never ob-
server in training, we choose an arbitrary label
as prediction. We refer to this baseline as la-
bel proportion baseline and create two variants of
it: PROPSURF based on surface string and PRO-
PLEM based on lemmatizing the surface string.
For unseen ql, this baseline makes a random guess
with uniform distribution.

Our unsupervised baseline is inspired by work
on sentence compression (Clarke and Lapata,
2008) and the PMI solver of Clark et al. (2016),
which compute word importance based on co-
occurrence statistics in a large corpus. In a cor-
pus C of 280 GB of plain text (5 × 1010 to-
kens) extracted from Web pages,8 we identify un-
igrams, bigrams, trigrams, and skip-bigrams from
q and each answer option ai. For a pair (x, y)
of n-grams, their pointwise mutual information
(PMI) (Church and Hanks, 1989) in C is defined
as log p(x,y)p(x)p(y) where p(x, y) is the co-occurrence
frequency of x and y (within some window) in C.
For a given word x, we find all pairs of question n-
grams and answer option n-grams. MAXPMI and
SUMPMI score the importance of a word x by
max-ing or summing, resp., PMI scores p(x, y)
across all answer options y for q. A limitation of
this baseline is its dependence on the existence of
answer options, while our system makes essential-
ity predictions independent of the answer options.

We note that all of the aforementioned baselines
produce real-valued confidence scores (for each
term in the question), which can be turned into bi-
nary labels (essential and non-essential) by thresh-
olding at a certain confidence value.

8Collected by Charles Clarke at the University of Water-
loo, and used previously by Turney (2013).

84



3.1 Evaluation

We consider two natural evaluation metrics for es-
sentiality detection, first treating it as a binary pre-
diction task at the level of individual terms and
then as a task of ranking terms within each ques-
tion by the degree of essentiality.

Binary Classification of Terms. We consider
all question terms pooled together as described
in Section 2.1, resulting in a dataset of 19,380
terms annotated (in the context of the correspond-
ing question) independently as essential or not.
The ET classifier is trained on the train subset, and
the threshold is tuned using the dev subset.

AUC Acc P R F1
MAXPMI † 0.74 0.67 0.88 0.65 0.75
SUMPMI † 0.74 0.67 0.88 0.65 0.75
PROPSURF 0.79 0.61 0.68 0.64 0.66
PROPLEM 0.80 0.63 0.76 0.64 0.69
ET Classifier 0.79 0.75 0.91 0.71 0.80

Table 1: Effectiveness of various methods for
identifying essential question terms in the test set,
including area under the PR curve (AUC), accu-
racy (Acc), precision (P), recall (R), and F1 score.
ET classifier substantially outperforms all super-
vised and unsupervised (denoted with †) baselines.

For each term in the corresponding test set of
4,124 instances, we use various methods to pre-
dict whether the term is essential (for the corre-
sponding question) or not. Table 1 summarizes
the resulting performance. For the threshold-based
scores, each method was tuned to maximize the
F1 score based on the dev set. The ET classifier
achieves an F1 score of 0.80, which is 5%-14%
higher than the baselines. Its accuracy at 0.75 is
statistically significantly better than all baselines
based on the Binomial9 exact test (Howell, 2012)
at p-value 0.05.

As noted earlier, each of these essentiality iden-
tification methods are parameterized by a thresh-
old for balancing precision and recall. This allows
them to be tuned for end-to-end performance of
the downstream task. We use this feature later
when incorporating the ET classifier in QA sys-
tems. Figure 5 depicts the PR curves for vari-
ous methods as the threshold is varied, highlight-
ing that the ET classifier performs reliably at var-
ious recall points. Its precision, when tuned to
optimize F1, is 0.91, which is very suitable for

9Each test term prediction is assumed to be a binomial.

0 0.2 0.4 0.6 0.8 1
Recall

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

P
re

ci
si
o
n

MaxPMI
SumPMI
PropSurf
PropLemma
ET

Figure 5: Precision-recall trade-off for various
classifiers as the threshold is varied. ET classifier
(green) is significantly better throughout.

AUC Acc P R F1
MAXPMI † 0.75 0.63 0.81 0.65 0.72
SUMPMI † 0.75 0.63 0.80 0.66 0.72
PROPSURF 0.57 0.51 0.49 0.61 0.54
PROPLEM 0.58 0.49 0.50 0.59 0.54
ET Classifier 0.78 0.71 0.88 0.71 0.78

Table 2: Generalization to unseen terms: Effec-
tiveness of various methods, using the same met-
rics as in Table 1. As expected, supervised meth-
ods perform poorly, similar to a random baseline.
Unsupervised methods generalize well, but the ET
classifier again substantially outperforms them.

high-precision applications. It has a 5% higher
AUC (area under the curve) and outperforms base-
lines by roughly 5% throughout the precision-
recall spectrum.

As a second study, we assess how well our clas-
sifier generalizes to unseen terms. For this, we
consider only the 559 test terms that do not appear
in the train set.10 Table 2 provides the resulting
performance metrics. We see that the frequency
based supervised baselines, having never seen the
test terms, stay close to the default precision of
0.5. The unsupervised baselines, by nature, gener-
alize much better but are substantially dominated
by our ET classifier, which achieves an F1 score
of 78%. This is only 2% below its own F1 across
all seen and unseen terms, and 6% higher than the
second best baseline.

Ranking Question Terms by Essentiality.
Next, we investigate the performance of the ET
classifier as a system that ranks all terms within
a question in the order of essentiality. Thus,

10In all our other experiments, test and train questions are
always distinct but may have some terms in common.

85



System MAP
MAXPMI † 0.87
SUMPMI † 0.85
PROPSURF 0.85
PROPLEM 0.86
ET Classifier 0.90

Table 3: Effectiveness of various methods for
ranking the terms in a question by essentiality.
† indicates unsupervised method. Mean-Average
Precision (MAP) numbers reflect the mean (across
all test set questions) of the average precision of
the term ranking for each question. ET classifier
again substantially outperforms all baselines.

unlike the previous evaluation that pools terms
together across questions, we now consider each
question as a unit. For the ranked list produced by
each classifier for each question, we compute the
average precision (AP).11 We then take the mean
of these AP values across questions to obtain
the mean average precision (MAP) score for the
classifier.

The results for the test set (483 questions) are
shown in Table 3. Our ET classifier achieves a
MAP of 90.2%, which is 3%-5% higher than the
baselines, and demonstrates that one can learn to
reliably identify essential question terms.

4 Using ET Classifier in QA Solvers

In order to assess the utility of our ET classifier,
we investigate its impact on two end-to-end QA
systems. We start with a brief description of the
question sets.

Question Sets. We use three question sets of 4-
way multiple choice questions.12 REGENTS and
AI2PUBLIC are two publicly available elementary
school science question set. REGENTS comes with
127 training and 129 test questions; AI2PUBLIC
contains 432 training and 339 test questions that
subsume the smaller question sets used previ-
ously (Clark et al., 2016; Khashabi et al., 2016).
REGTSPERTD set, introduced by Khashabi et al.
(2016), has 1,080 questions obtained by automat-
ically perturbing incorrect answer choices for 108
New York Regents 4th grade science questions.

11We rank all terms within a question based on their es-
sentiality scores. For any true positive instance at rank k, the
precision at k is defined to be the number of positive instances
with rank no more than k, divided by k. The average of all
these precision values for the ranked list for the question is
the average precision.

12Available at http://allenai.org/data.html

We split this into 700 train and 380 test questions.
For each question, a solver gets a score of 1 if it

chooses the correct answer and 1/k if it reports a
k-way tie that includes the correct answer.

QA Systems. We investigate the impact of
adding the ET classifier to two state-of-the-art
QA systems for elementary level science ques-
tions. Let q be a multiple choice question with
answer options {ai}. The IR Solver from Clark
et al. (2016) searches, for each ai, a large corpus
for a sentence that best matches the (q, ai) pair.
It then selects the answer option for which the
match score is the highest. The inference based
TableILP Solver from Khashabi et al. (2016), on
the other hand, performs QA by treating it as
an optimization problem over a semi-structured
knowledge base derived from text. It is designed
to answer questions requiring multi-step inference
and a combination of multiple facts.

For each multiple-choice question (q, a), we use
the ET classifier to obtain essential term scores sl
for each token ql in q; sl = et(ql, q, a). We will be
interested in the subset ω of all terms Tq in q with
essentiality score above a threshold ξ: ω(ξ; q) =
{l ∈ Tq | sl > ξ}. Let ω(ξ; q) = Tq \ ω(ξ; q). For
brevity, we will write ω(ξ) when q is implicit.

4.1 IR solver + ET

To incorporate the ET classifier, we create a pa-
rameterized IR system called IR + ET(ξ) where,
instead of querying a (q, ai) pair, we query
(ω(ξ; q), ai).

While IR solvers are generally easy to imple-
ment and are used in popular QA systems with
surprisingly good performance, they are often also
sensitive to the nature of the questions they re-
ceive. Khashabi et al. (2016) demonstrated that
a minor perturbation of the questions, as embod-
ied in the REGTSPERTD question set, dramatically
reduces the performance of IR solvers. Since the
perturbation involved the introduction of distract-
ing incorrect answer options, we hypothesize that
a system with better knowledge of what’s impor-
tant in the question will demonstrate increased ro-
bustness to such perturbation.

Table 4 validates this hypothesis, showing the
result of incorporating ET in IR, as IR + ET(ξ =
0.36), where ξ was selected by optimizing end-to-
end performance on the training set. We observe a
5% boost in the score on REGTSPERTD, showing
that incorporating the notion of essentiality makes

86



Dataset Basic IR IR + ET

REGENTS 59.11 60.85
AI2PUBLIC 57.90 59.10
REGTSPERTD 61.84 66.84

Table 4: Performance of the IR solver without
(Basic IR) and with (IR + ET) essential terms. The
numbers are solver scores (%) on the test sets of
the three datasets.

the system more robust to perturbations.
Adding ET to IR also improves its performance

on standard test sets. On the larger AI2PUBLIC
question set, we see an improvement of 1.2%.
On the smaller REGENTS set, introducing ET
improves IRsolver’s score by 1.74%, bringing
it close to the state-of-the-art solver, TableILP,
which achieves a score of 61.5%. This demon-
strates that the notion of essential terms can be
fruitfully exploited to improve QA systems.

4.2 TableILP solver + ET

Our essentiality guided query filtering helped the
IR solver find sentences that are more relevant to
the question. However, for TableILP an added
focus on essential terms is expected to help only
when the requisite knowledge is present in its rel-
atively small knowledge base. To remove con-
founding factors, we focus on questions that are,
in fact, answerable.

To this end, we consider three (implicit) require-
ments for TableILP to demonstrate reliable behav-
ior: (1) the existence of relevant knowledge, (2)
correct alignment between the question and the
knowledge, and (3) a valid reasoning chain con-
necting the facts together. Judging this for a ques-
tion, however, requires a significant manual effort
and can only be done at a small scale.

Question Set. We consider questions for which
the TableILP solver does have access to the req-
uisite knowledge and, as judged by a human, a
reasoning chain to arrive at the correct answer.
To reduce manual effort, we collect such ques-
tions by starting with the correct reasoning chains
(‘support graphs’) provided by TableILP. A human
annotator is then asked to paraphrase the corre-
sponding questions or add distracting terms, while
maintaining the general meaning of the question.
Note that this is done independent of essentiality
scores. For instance, the modified question below
changes two words in the question without affect-
ing its core intent:

Original question: A fox grows thicker fur as a season
changes. This adaptation helps the fox to (A) find food(B)
keep warmer(C) grow stronger(D) escape from predators
Generated question: An animal grows thicker hair as a
season changes. This adaptation helps to (A) find food(B)
keep warmer(C) grow stronger(D) escape from predators

While these generated questions should ar-
guably remain correctly answerable by TableILP,
we found that this is often not the case. To in-
vestigate this, we curate a small dataset QR with
12 questions (cf. Appendix C of the extended ver-
sion (Khashabi et al., 2017)) on each of which, de-
spite having the required knowledge and a plausi-
ble reasoning chain, TableILP fails.

Modified Solver. To incorporate question term
essentiality in the TableILP solver while maintain-
ing high recall, we employ a cascade system that
starts with a strong essentiality requirement and
progressively weakens it.

Following the notation of Khashabi et al.
(2016), let x(ql) be a binary variable that denotes
whether or not the l-th term of the question is used
in the final reasoning graph. We enforce that terms
with essentiality score above a threshold ξ must be
used: x(ql) = 1, ∀l ∈ ω(ξ). Let TableILP+ET(ξ)
denote the resulting system which can now be used
in a cascading architecture.

TableILP+ET(ξ1) → TableILP+ET(ξ2) → ...
where ξ1 < ξ2 < . . . < ξk is a sequence of
thresholds. Questions unanswered by the first
system are delegated to the second, and so on. The
cascade has the same recall as TableILP, as long as
the last system is the vanilla TableILP. We refer to
this configuration as CASCADES(ξ1, ξ2, . . . , ξk).

This can be implemented via repeated calls to
TableILP+ET(ξj) with j increasing from 1 to k,
stopping if a solution is found. Alternatively, one
can simulate the cascade via a single extended ILP
using k new binary variables zj with constraints:
|ω(ξj)| ∗ zj ≤

∑
l∈ω(ξj) x(ql) for j ∈ {1, . . . , k},

and addingM ∗∑kj=1 zj to the objective function,
for a sufficiently large constant M .

We evaluate CASCADES(0.4, 0.6, 0.8, 1.0) on
our question set, QR. By employing essential-
ity information provided by the ET classifier,
CASCADES corrects 41.7% of the mistakes made
by vanilla TableILP. This error-reduction illus-
trates that the extra attention mechanism added
to TableILP via the concept of essential question
terms helps it cope with distracting terms.

87



5 Conclusion

We introduced the concept of essential question
terms and demonstrated its importance for ques-
tion answering via two empirical findings: (a)
humans becomes substantially worse at QA even
when a few essential question terms are dropped,
and (b) state-of-the-art QA systems can be im-
proved by incorporating this notion. While text
summarization has been studied before, questions
have different characteristics, requiring new train-
ing data to learn a reliable model of essentiality.
We introduced such a dataset and showed that our
classifier trained on this dataset substantially out-
performs several baselines in identifying and rank-
ing question terms by the degree of essentiality.

Acknowledgments

The authors would like to thank Peter Clark,
Oyvind Tafjord, and Peter Turney for valuable dis-
cussions and insights.

This work is supported by DARPA under agree-
ment number FA8750-13-2-0008. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of DARPA or the U.S.
Government.

References
P. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D.

Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational linguis-
tics 18(4):467–479.

K. W. Church and P. Hanks. 1989. Word association
norms, mutual information and lexicography. In
27th Annual Meeting of the Association for Compu-
tational Linguistics. pages 76–83.

P. Clark. 2015. Elementary school science and math
tests as a driver for AI: take the Aristo challenge! In
29th AAAI/IAAI. Austin, TX, pages 4019–4021.

P. Clark, O. Etzioni, T. Khot, A. Sabharwal, O. Tafjord,
P. Turney, and D. Khashabi. 2016. Combining re-
trieval, statistics, and inference to answer elemen-
tary science questions. In 30th AAAI.

J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search 31:399–429.

J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin
76(5):378.

C. Hori and F. Sadaoki. 2004. Speech summarization:
an approach through word extraction and a method
for evaluation. IEICE TRANSACTIONS on Informa-
tion and Systems 87(1):15–25.

Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In 2014 EMNLP. pages 523–533.

D. Howell. 2012. Statistical methods for psychology.
Cengage Learning.

P. Jansen, R. Sharp, M. Surdeanu, and P. Clark. 2017.
Framing qa as building and ranking intersentence
answer justifications. Computational Linguistics .

T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. Machine learning: ECML-98 pages 137–142.

D. Khashabi, T. Khot, A. Sabharwal, P. Clark, O. Et-
zioni, and D. Roth. 2016. Question answering via
integer programming over semi-structured knowl-
edge (extended version). In Proc. 25th Int. Joint
Conf. on Artificial Intelligence (IJCAI).

D. Khashabi, T. Khot, A. Sabharwal, and D. Roth.
2017. Learning what is essential in questions (ex-
tended version).

T. Khot, N. Balasubramanian, E. Gribkoff, A. Sab-
harwal, P. Clark, and O. Etzioni. 2015. Exploring
Markov logic networks for question answering. In
2015 EMNLP. Lisbon, Portugal.

K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence
139(1):91–107.

C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scaling
question answering to the web. In WWW.

F. Li, X. Zhang, J. Yuan, and X. Zhu. 2007. Classifying
what-type questions by head noun tagging. In Proc.
22nd Int. Conf. on Comput. Ling. (COLING).

X. Li and D. Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics - Volume 1.
Association for Computational Linguistics, Strouds-
burg, PA, USA, COLING ’02, pages 1–7.

D. Moldovan, M. Paşca, S. Harabagiu, and M. Sur-
deanu. 2003. Performance issues and error analy-
sis in an open-domain question answering system.
ACM Transactions on Information Systems (TOIS)
21(2):133–154.

88



J. H. Park and W. B. Croft. 2015. Using key concepts
in a translation model for retrieval. In Proceedings
of the 38th International ACM SIGIR Conference
on Research and Development in Information Re-
trieval. ACM, pages 927–930.

P. D. Turney. 2013. Distributional semantics beyond
words: Supervised learning of analogy and para-
phrase. TACL 1:353–366.

Kateryna Tymoshenko, Daniele Bonadiman, and
Alessandro Moschitti. 2016. Convolutional neural
networks vs. convolution kernels: Feature engineer-
ing for answer sentence reranking. In HLT-NAACL.

B. Wang, K. Liu, and J. Zhao. 2016. Inner attention
based recurrent neural networks for answer selec-
tion. In ACL.

W.-t. Yih, X. He, and C. Meek. 2014. Semantic pars-
ing for single-relation question answering. In Proc.
52nd Annual Meeting of the Ass. for Comp. Linguis-
tics (ACL). pages 643–648.

W. Yin, S. Ebert, and H. Schütze. 2016. Attention-
based convolutional neural network for machine
comprehension. In NAACL HCQA Workshop.

89


