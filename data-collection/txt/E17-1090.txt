



















































Psycholinguistic Models of Sentence Processing Improve Sentence Readability Ranking


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 958–968,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Psycholinguistic Models of Sentence Processing Improve Sentence
Readability Ranking

David M. Howcroft1 and Vera Demberg1,2
1 Department of Language Science and Technology

2 Department of Computer Science
Saarland Informatics Campus, Saarland University

Saarbrücken, Germany
{howcroft,vera}@coli.uni-saarland.de

Abstract

While previous research on readability has
typically focused on document-level mea-
sures, recent work in areas such as nat-
ural language generation has pointed out
the need of sentence-level readability mea-
sures. Much of psycholinguistics has fo-
cused for many years on processing mea-
sures that provide difficulty estimates on
a word-by-word basis. However, these
psycholinguistic measures have not yet
been tested on sentence readability rank-
ing tasks. In this paper, we use four
psycholinguistic measures: idea density,
surprisal, integration cost, and embedding
depth to test whether these features are
predictive of readability levels. We find
that psycholinguistic features significantly
improve performance by up to 3 percent-
age points over a standard document-level
readability metric baseline.

1 Introduction

Previous work on readability has classified or
ranked texts based on document-level measures
such as word length, sentence length, number of
different phrasal categories & parse tree depth (Pe-
tersen, 2007), and discourse coherence (Graesser
et al., 2004), inter alia. However, not all appli-
cations that need readability ratings deal with long
documents. For many applications in text simplifi-
cation, computer-aided language learning (CALL)
systems, authorship tools, translation, and infor-
mation retrieval, sentence-level readability metrics
are direly needed.

For instance, an automatic text simplification
system must begin by asking which portions of a
text need to be simplified. To this end, a measure
that can assign ratings on a sentence-by-sentence

level can help target simplification only to those
sentences which need it most, and such measures
also serve to confirm that the resulting ‘simplified’
sentence is in fact simpler than the original sen-
tence.

Similarly, CALL and other pedagogical systems
will benefit if it is possible to predict which por-
tions of a text will be harder for students. Au-
thorship tools can offer more specific editorial ad-
vice when they know why individual sentences
can cause difficulties for readers. Translation tools
can aim to preserve not just meaning but also the
approximate difficulty of the sentences they are
translating or use a sentence-level difficulty metric
to target output that is easier to understand. Fur-
thermore, information retrieval systems also bene-
fit when they can return not merely relevant texts,
but also texts appropriate to the reading level of the
user. Recently there has been an increased interest
in sentential models of text difficulty in the auto-
matic text simplification and summarization com-
munities in particular (Vajjala and Meurers, 2014;
Macdonald and Siddharthan, 2016).

One area that has produced a lot of research on
sentence level processing difficulty is psycholin-
guistics. Over the past three decades, a number of
theories of human sentence processing (i.e. read-
ing) have been proposed and validated in a large
variety of experimental studies. The most im-
portant sentence processing theories have further-
more been implemented based on broad-coverage
tools, so that estimates for arbitrary sentences can
be generated automatically. For example, eye-
tracking studies of reading times on a large corpus
of newspaper text have found that measures such
as integration cost and surprisal provide partial
explanations for subjects’ reading behavior (Dem-
berg and Keller, 2008).

This paper leverages these implemented mea-
sures based on psycholinguistic theories of sen-

958



tence processing in order to test whether they can
help to more accurately score individual sentences
with respect to their difficulty. In the process, we
evaluate the contributions of the individual fea-
tures to our models, testing their utility in examin-
ing fine-grained distinctions in sentence difficulty.
Section 2 reviews the literature on readability in
general before we shift to psycholinguistic theo-
ries of sentence processing in Section 4. In Sec-
tion 5 we discuss our methods, including the cor-
pora used, how features were extracted, and the
set up for our averaged perceptron models. Sec-
tion 6 presents our findings which we connect to
related work on sentence-level readability models
in 3. Finally we offer our conclusions and sugges-
tions for future work in Section 7.

2 Readability

Chall’s (1958) comprehensive review of readabil-
ity research in the first half of the 20th century
divides the early work in readability into “survey
and experimental studies” and “quantitative asso-
ciational studies”. Studies of the former category
took place during the 1930s and 1940s and in-
cluded surveys of expert and reader opinion as
well as experimental studies which manipulated
texts according to one variable at a time in order to
determine the effects of those variables on readers.
The results of these studies suggest that, once you
have managed to control for reader interest in the
content of a text, the most important factor with re-
spect to its readability is its ‘style’, e.g. its “scope
of vocabulary and...kinds of sentences” (Gray and
Leary, 1935, as quoted in (Chall, 1958)).

Our study belongs to the second class, relating
the features of a text to its ordering relative to some
other texts. The earliest work in this direction
was by L. A. Sherman, who proposed a quanti-
tative analysis of text difficulty based on the num-
ber of clauses per sentence, among other features
(Sherman, 1893). Where Sherman’s pedagogical
focus was on literature, Lively & Pressey (1923)
focused on vocabulary as a bottleneck in science
education. Work in this vein led to the develop-
ment of a number of readability formulae in the
mid-20th century1, including the familiar Flesch-
Kincaid Grade-Level score (Kincaid et al., 1975).

1For a comprehensive review of the literature up to 1958,
we recommend (Chall, 1958). For a more recent review of the
literature, we recommend Chapter 2 of (Vajjala, 2015). For
an introduction to some of the major studies from the 20th
century, we recommend the self-published (Dubay, 2007).

These formulae typically use a linear com-
bination of average word length and average
sentence length, though some also incorporate
a vocabulary-diversity term. The simple, two-
feature versions of these models are still widely
used, and inspired our BASELINE model.

More recently, Petersen (2007) sought to apply
familiar natural language processing techniques to
the problem of identifying text difficulty for non-
native readers. In particular, she used a number of
parse-based features which captured, for example,
the average number of noun and verb phrases per
sentence and the height of the parse tree. Petersen
trained SVM classifiers to classify texts as belong-
ing to one of four primary school grade levels
based on the Weekly Reader educational newspa-
per2. These document-level models achieved F -
scores in the range of 0.5 to 0.7, compared to the
F -scores between 0.25 and 0.45 achieved by the
Flesch-Kincaid Reading Ease score for the same
texts.

Recent work has also looked at features related
to discourse and working memory constraints.
Feng et al. (2009) worked on a model of readabil-
ity for adults with intellectual disabilities. Consid-
ering working memory constraints, they extracted
features related to the number of entities men-
tioned in a document and the ‘lexical chains’ (Gal-
ley and McKeown, 2003) that connected them.
They found that their features resulted in a bet-
ter correlation (Pearson’s r = −0.352) compared
to both Flesch-Kincaid score (r = −0.270) and
a number of ‘basic’ linguistic features based on
those used by Petersen & Ostendorf (2009) (r =
−0.283).3

Coh-Metrix (Graesser et al., 2004) also includes
a number of measures related to discourse coher-
ence, for example. Such features are not suited to
the problem of determining the difficulty of sen-
tences in isolation, but they have also been shown
to better predict readability for second-language
learners compared to ‘traditional’ readability mea-
sures like those described above (Crossley et al.,
2011).

2http://www.weeklyreader.com
3 Correlations here are negative because Feng et al. corre-

lated predicted reading levels with the performance of adults
with intellectual disabilities on comprehension tests. The
adults with disabilities are expected to perform worse on the
comprehension test as the grade level of the text increases.

959



3 Measuring Sentence Complexity

Classification Few studies to date have addressed
sentence-level readability for English. Napoles &
Dredze (2010) built their own corpus with docu-
ments from English and Simple English Wikipedia
to train both document- and sentence-level classi-
fiers. Using bag-of-words features, unigram and
bigram part-of-speech features, type-token ratio,
the proportion of words appearing on a list of
easier words, and parse features similar to Pe-
tersen’s, their binary classifier achieved an accu-
racy of 80.8% on this task. The structure of this
task, however, is not suited to text simplification
applications, because the sentences are not con-
trolled for meaning. Classifying a sentence in iso-
lation as more likely to be from Simple Wikipedia
or English Wikipedia is not as useful as a model
trained to differentiate sentences carrying the same
meaning. This work is not directly comparable to
that of Vajjala & Meurers (2012; 2014) or sub-
sequent work on ranking sentences by their com-
plexity due to the differences in choice of corpora
and task structure.

In the medical domain, Kauchak et al. (2014)
also looked at sentence-level classification, iden-
tifying sentences as being either simple or diffi-
cult. Their features included word length, sen-
tences length, part-of-speech counts, average un-
igram frequencies and standard deviation, and the
proportion of words not on a list of the five thou-
sand most frequent words as well as three domain-
specific features based on an ontology of medical
terminology.

Ranking Vajjala & Meurers (Vajjala and Meur-
ers, 2014; Vajjala, 2015) were the first to look
at ranking sentences rather than classifying them,
having observed that the distributions of pre-
dicted reading levels across the two subcorpora of
the Parallel Wikipedia corpus (Zhu et al., 2010,
PWKP) were different. While the Simple English
portion of the corpus was clearly skewed toward
the lower grade levels, it appears that the English
portion of the corpus was evenly distributed across
all grade levels, making binary-classification diffi-
cult.

This led Vajjala & Meurers to develop a rank-
ing model using the predicted reading levels from
a multiclass classifier trained on whole docu-
ments. For each sentence pair, they assumed that
the English Wikipedia sentence should be clas-
sified at a higher level than the Simple English

Wikipedia sentence. Using a hard cut-off (i.e.
rank(sentenglish) > rank(sentsimple)), their
model achieved about 59% accuracy, although this
improved to 70% by relaxing the inequality con-
straint to include equality. Based on the find-
ing that 30% of sentence pairs from the PWKP
corpus are incorrectly ranked despite lying within
one reading level of each other, we hypothesize
that finer-grained distinctions may be necessary to
tease apart the differences in related pairs of sen-
tences.

Offline Psycholinguistic Features While Va-
jjala & Meurers (2012; 2014) do use some
psycholinguistically-motivated features, their fea-
tures are primarily lexical in nature and therefore
complementary to ours, which depend on the sen-
tence processing context. They drew psycholin-
guistic features from the MRC psycholinguistic
database (Wilson, 1988), including word famil-
iarity, concreteness, imageability, meaningfulness,
and age of acquisition. These features were cou-
pled with a second age of acquisition database and
values related to the average number of senses per
word.

Towards online considerations More recently,
Ambati et al. (2016) used an incremental parser
to extend Vajjala & Meurers work. Since hu-
man processing is incremental, they reasoned, fea-
tures from an incremental parser might be more
informative than features extracted from a non-
incremental parser. To this end, they used the
incremental derivations from a combinatory cat-
egorial grammar (CCG) parser. Ambati et al.
ran several models on the English and Simple
English Wikipedia data set (Hwang et al., 2015,
ESEW):one using only the syntactic features from
(Vajjala and Meurers, 2014); another (INCCCG)
using only features from the incremental parser;
and INCCCG+, incorporating morpho-syntactic
and psycholinguistic features from (Vajjala and
Meurers, 2014). At the sentence level, they
include sentence length, number of CCG con-
stituents in the final parse, and the depth of the
CCG derivation. They also use count features
for the number of times each CCG derivation rule
is applied (e.g. forward application, type-raising).
Finally, they include counts of different CCG syn-
tactic categories as well as the average ‘complex-
ity’ of the syntactic categories. While the parser
they use is inspired by human behavior, in that it
is an incremental parser, these features do not re-

960



late to any specific linguistic theory of sentence
processing.

The work presented here is most comparable to
that of Vajjala & Meurers and Ambati et al., as we
all address the problem of ranking sentences ac-
cording to their linguistic complexity. Our study
is the only one of the three to examine features
based on theories of online sentence processing.
Ambati et al. (2016) provide accuracy information
for their own features as well as Vajjala & Meur-
ers’ (2014) features on the English and Simple En-
glish Wikipedia corpus (ESEW) which we use, but
used a 60-20-20 training-dev-test split where we
used 10-fold cross-validation, making the results
not directly comparable.

4 Theories of Online Sentence Processing

For our purposes, we focus on readability as read-
ing ease and on linguistic constraints in particu-
lar, rather than constraints of medium (relating to
e.g. legibility), reader interest, or comprehensibil-
ity. Without directly modeling comprehensibility,
we assume that making material easier to read will
also make it easier to comprehend. Here we focus
on four psycholinguistic theories of human sen-
tence processing: idea density, surprisal, integra-
tion cost, and embedding depth.

Kintsch (1972) defined propositional idea den-
sity as the ratio of propositions or ideas to words
in the sentences.4 Keenan & Kintsch conducted
two different experiments in order to examine free
reading behavior as well as subjects’ performance
in speeded reading conditions. They found that
“the number of propositions [in a text] had a large
effect upon reading times, [but] it could only ac-
count for 21% of their variance” when subjects
were allowed to read freely. Subjects’ overall re-
call was worse for more dense texts in the speeded
reading condition. In addition to effects of idea
density, they found that propositions which were
presented as surface-form modifiers (as opposed
to, e.g., main verbs) were “very poorly recalled”
and that propositions playing a subordinate role
relative to another proposition were also less-well
recalled. Finally, propositions involving a proper
name were generally recalled better than similar
propositions involving, e.g., a common noun.

While Kintsch & Keenan (1973) looked at the

4 This notion of idea density is closely related to Perfetti’s
(1969) notion of lexical density insofar as both are related to
the number of so-called content words in the text.

influence of propositional idea density on reading
times and recall for both individual sentences as
well as short paragraphs, work since the 1970s has
been limited to the level of multiple sentences and
used primarily as an indicator of cognitive deficits
(Ferguson et al., 2014; Bryant et al., 2013; Farias
et al., 2012; Riley et al., 2005). This paper returns
to the examination of idea density’s applicability
for individual sentences.

Surprisal, on the other hand, has been widely
examined in theories of language comprehension
at a variety of levels, including the word- and
sentence-levels. Surprisal is another word for
Shannon (1948) information, operationalized in
linguistics as the probability of the current word
conditioned on the preceding sequence of words:

surprisal(wn) = −log(P (wn|w1 . . . wn−1)) (1)

where wi is the ith word in the sentence and
P (w1 . . . wi) denotes the probability of the se-
quence of i words w1 . . . wi.

One reason psycholinguists consider surprisal
as a factor in sentence processing difficulty is
that it makes sense in a model of language users
as rational learners. Levy (2008) argues the ra-
tional reader’s attention must be spread across
all possible analyses for the sentence being ob-
served. Based on prior experience, the reader ex-
pects some analyses to be more probable than oth-
ers and therefore allocates more resources to those
analyses. In this analysis, surprisal is derived as
a measure of the cost paid when the reader mis-
allocates resources: when a new word invalidates
a highly probable analysis, the reader has effec-
tively ‘wasted’ whatever resources were allocated
to that analysis. The notion of surprisal is also
used in theories of language production, see the
Uniform Information Density hypothesis (Jaeger,
2006; Levy and Jaeger, 2007; Jaeger, 2010, UID).

While surprisal focuses on predictability effects
in sentence processing, Gibson’s (1998; 2000) De-
pendency Locality Theory (DLT) focuses on the
memory cost of recalling referents and integrating
new ones into a mental representation. DLT pro-
poses that the the distance between syntactic heads
and dependents, measured by the number of inter-
vening discourse referents, approximates the diffi-
culty that the listener or reader will have integrat-
ing the two units. This model maintains that the
act of creating a new discourse referent and hold-
ing it in memory makes it more difficult to recall

961



a previous discourse referent and connect that dis-
course referent to the current one.5

In addition to integration cost, DLT proposes a
storage cost associated with the number of open
dependencies that must be maintained in mem-
ory. The notion of connected components in van
Schijndel et al.’s (2012; 2013) incremental pars-
ing model picks up this idea. Related models
were also suggested earlier by Yngve (1960) and
Miller’s (1956a; 1956b) whose work was based
on results showing that human working memory
is limited to 7± 2 items. Yngve’s mechanistic, in-
cremental model of language production consid-
ered the evaluation of phrase structure grammars
(PSGs) in a system with finite memory, exploring
the structure speakers must keep track of during
production and how grammars might be structured
to avoid overtaxing working memory.

Van Schijndel et al. develop this idea further in
the context of a hierarchical sequence model of
parsing. In this incremental model of parsing, at
each stage the reader has an active state (e.g. S
for sentence) and an awaited state (e.g. VP for
verb phrase).6 At each new word, the parser must
decide between continuing to analyze the current
connected component or hypothesizing the start of
a new one.7

These measures provide an idealized represen-
tation of the number of different states a human
parser must keep track of at any point in time.
We refer to this number of states as the embed-
ding depth of a sentence at a particular word, and
the ModelBlocks parser of van Schijndel et al.
(2012) calculates this number of states averaged
over the beam of currently plausible parses. Also
of interest is the embedding difference, which is
the embedding depth at the present word relative
to the previous word, elaborated upon in the fol-
lowing example.

Consider the state described above (i.e. that
of being in the active state S and awaiting state
VP) might be reached after a reader has observed
a noun phrase, resulting in the state S/VP. This

5 Gildea & Temperley (2010) measure dependencies in
terms of word span, such that adjacent words have a depen-
dency length of one. This approach produces similar diffi-
culty estimates nouns and verbs, with the caveat that dis-
tances are systematically increased, and is defined for all
words in a sentence.

6 In Combinatory Categorial Grammar notation, this state
is denoted S/VP.

7 These connected components are the parsing analogues
to the constituents awaiting expansion in Yngve’s analysis.

means that the word sequence observed so far will
be consistent with a sentence if the reader now ob-
serves a verb phrase. If, however, the next word
in the input is inconsistent with the start of a verb
phrase (e.g. the relative clause marker that), then
this parse will be ruled out and another must be
considered. At this point the parser must hypothe-
size the beginning of a new connected component,
i.e. a new syntactic substructure that must be com-
pleted before continuing to parse the top-level of
the sentence. Therefore, the parser must now keep
track of two states: (1) the fact that we are still
looking for a VP to complete the overall sentence;
and (2) the fact that we now have a relative clause
to parse before we can complete the current NP. In
this example, we are at embedding depth 1 or 0 up
until we encounter the word that, which increases
the embedding depth by 1, resulting in a nonzero
embedding difference score.

4.1 Experimental Evidence

We have already explained the experimental find-
ings of Kintsch & Keenan (1973) with respect to
idea density, but what behavioral evidence is there
to suggest that the remaining theories are valid?

Demberg & Keller (2008) examined the rela-
tionship between both surprisal and integration
cost and eye-tracking times in the Dundee cor-
pus (Kennedy and Pynte, 2005) Demberg & Keller
found that increased surprisal significantly corre-
lated with reading times. Although they found that
integration cost did not significantly contribute to
predicting eye-tracking reading times in general,
its contribution was significant when restricted to
nouns and verbs. They also found that surprisal
and integration cost were uncorrelated, suggest-
ing that they should be considered complementary
factors in a model of reading times. Another eye-
tracking study divided surprisal into lexical and
synactic components, finding that lexical surprisal
was a significant factor but not syntactic surprisal
(Roark et al., 2009).

Wu et al. (2010) examined surprisal, entropy
reduction, and embedding depth in a study of
psycholinguistic complexity metrics. Their study
of the reading times of 23 native English speak-
ers reading four narratives indicated that embed-
ding difference was a significant predictor of read-
ing times for closed class words. Moreover, this
contribution was independent of the contribution
of surprisal, indicating that the two measures are

962



capturing different components of the variance in
reading times. Since integration cost was a sig-
nificant predictor of reading times for nouns and
verbs (i.e. not closed class words) and embedding
depth was a significant predictor of reading times
for closed class words, integration cost and em-
bedding depth should also be complementary to
each other.

5 Methods

5.1 Corpora

We used two corpora in this work. The English
and Simple English Wikipedia corpus of Hwang
et al. (2015, ESEW) is a new corpus of more than
150k sentence pairs designed to address the flaws
of the Parallel Wikipedia Corpus of Zhu et al.
(2010, PWKP), which was previously dominant
in work on text simplification, by using a more
sophisticated method of aligning pairs of English
and Simple English sentences. We used the sec-
tion labeled as having ‘good’ alignments for our
work and assumed that, in every sentence pair, the
Simple English sentence should be ranked as eas-
ier than the English sentence (rank=1 < rank=2
in Table 1). This provides a large corpus with
noisy labels, as there are likely to be instances
where the English and Simple English sentences
are not substantially different or the English sen-
tence is the easier one.8

For a more controlled corpus, we use Vaj-
jala’s (2015) One Stop English (OSE) corpus.
This corpus consists of 1577 sentence triples,
drawn from news stories edited to three diffi-
culty levels: elementary, intermediate, and ad-
vanced. Vajjala used TF ∗ IDF and cosine simi-
larity scores to align sentences from stories drawn
from onestopenglish.com. While One Stop
English does not publish an explanation of their
methods for creating these texts, they are at least
created by human editors for pedagogical pur-
poses, so the labels should be more consistent and
reliable than those associated with the ESEW cor-
pus.

The three levels of OSE make it possible to
compare system performance on sentence pairs
which are close to one another in difficulty (e.g.
‘advanced’ versus ‘intermediate’ sentences) with

8Indeed, 37, 095 of the 154, 805 sentence pairs have
the same sentence for both English and Simple English
Wikipedia and were therefore excluded from our experi-
ments.

performance on pairs which are further apart, as
with ‘advanced’ sentences paired with their ‘ele-
mentary’ counterparts. In this paper we will refer
to the pairs of advanced and elementary sentences
as OSEfar, the remaining pairs as OSEnear, and
the full OSE dataset as OSEall. An example triple
of sentences from the corpus is given in Table 2.

5.2 Feature Extraction and Feature Sets

We used two parsers to extract 22 features from
the corpora. The ModelBlocks parser provided
features based on surprisal and embedding depth
while the Stanford parser9 provided the depen-
dency parses used to calculate integration cost and
idea density features. Both parsers are trained and
perform near the state of the art on the standard
sections of the Wall Street Journal section of the
Penn Treebank.

From ModelBlocks’ complexity feature ex-
traction mode, we took the lexical and syntactic
surprisal features. We used the average lexical sur-
prisal and average syntactic surprisal as idealized
measures of the channel capacity required to read
a sentence. While this underestimates the chan-
nel capacity required to process a sentence, it is
at least internally consistent, insofar as a sentence
with higher average surprisal overall is likely to
require a higher channel capacity as well. We also
used the maximum of each form of surprisal as a
measure of the maximum demand on cognitive re-
sources. These features comprise the SURPRISAL
model.

We also calculated average and maximum val-
ues for the embedding depth and embedding dif-
ference output from ModelBlocks. The aver-
age provides an estimate of the typical memory
load throughout a sentence, while the (absolute)
embedding difference is a measure of how many
times a reader needs to push or pop a connected
component to or from their memory store. These
features comprise the EMBEDDING model.

To extract the remaining features, we first ran
the Stanford dependency parser on both corpora.
The program icy-parses uses part-of-speech
tags and head-dependent relations to determine
the total, average, and maximum integration cost
across a sentence. Here average integration cost
functions as another kind of memory load esti-
mate while the maximum value models the most-

9http://nlp.stanford.edu/software/lex-
parser.shtml

963



Rank Sentence
2 Gingerbread was brought to Europe in 992 by the Armenian monk Gregory of Nicopolis

-LRB- Gregory Makar -RRB- -LRB- Grégoire de Nicopolis -RRB- .
1 Armenian monk Gregory of Nicopolis -LRB- Gregory Makar -RRB- -LRB- Grgoire de

Nicopolis -RRB- brought ginger bread to Europe in 992 .

Table 1: Example sentences from English (2) and Simple (1) English Wikipedia.

Rank Sentence
3 It is a work-hard, play-hard ethic that many of the world’s billionaires might subscribe to but

it would be a huge change for most workers and their employers.
2 It is a ‘work-hard, play-hard’ way of thinking that many of the world’s billionaires might

agree with but it would be a huge change for most workers and their employers.
1 Many of the world’s billionaires might agree with this way of thinking but it would be a very

big change for most workers and their employers.

Table 2: Example sentences from One Stop English, at levels advanced (3), intermediate (2), and ele-
mentary (1). The pair 3–1 is in OSEfar, the pairs 3–2 and 2–1 are in OSEnear, and all three pairs are
in OSEall.

difficult-to-integrate point in the sentence. These
features comprise the INTEGRATIONCOST model.

Finally, we use a modified version of the IDD3
library from Andre Cunha (Cunha et al., 2015)
to extract idea density decomposed across three
types of propositional idea: predications, modifi-
cations, and connections.10 Here we use only av-
eraged features, as the crucial measure is the idea
density rather than the raw number of ideas being
expressed. These features comprise the IDEAD-
ENSITY model.

As a point of comparison for these models, we
created a BASELINE which used only sentence
length and the average word length as features.

We also created models based on features
grouped by the parser used to extract them: SUR-
PRISAL+EMBED for the ModelBlocks parser
and IDEA+INTEGRATION for the Stanford parser.
While ModelBlocks achieves competitive ac-
curacies, it is much slower than other state-of-the-
art parsers available today. Therefore we wanted
to provide a point of comparison regarding the rel-
ative utility of these parsers: grouping features by
parser allows us to assess the trade-off between
model accuracy and the time necessary for feature
extraction.

Finally, we considered combinations
of the parser-grouped features with the
baseline (BASE+SURPRISAL+EMBED
and BASE+IDEA+INTEGRATION) and a

10Code available at: https://github.com/
dmhowcroft/idd3.

FULLMODEL using the baseline features and
all of the psycholinguistic features.

Replication The scripts required for replica-
tion are available at https://github.com/
dmhowcroft/eacl2017-replication.
This includes pointers to the corpora, pre-
processing scripts and settings for the parsers, as
well as scripts for feature extraction and running
the averaged perceptron model.

5.3 Ranking as Classification

In order to rank sentences, we need some way of
generating a complexity score for each sentence.
Using a perceptron model allows us to train a sim-
ple linear scoring model by converting the ranking
task into a classification task.

Suppose we have two sentences s1 and s2 with
feature vectors s1 and s2 such that s1 is more com-
plex than s2. Then we want to train a perceptron
model such that

score(s1) > score(s2) (2)
W · s1 > W · s2 (3)

W · (s1 − s2) > 0 (4)

We refer to the vector s1− s2 as a vector of dif-
ference features. In order to train the model, we
take all pairs of sentences present in a given cor-
pus and create a difference vector as above. In half
of the cases, we flip the sign of the difference vec-
tor, creating a binary classification task with bal-
anced classes. The learning problem is now to

964



n.s.

n.s.

61.48 65.04 63.84 55.33 69.76 65.57 64.68
50

60

70
Su

rp
ris

al
Em

be
dd

in
g

In
te

gr
at

io
nC

os
t

Id
ea

D
en

si
ty

Ps
yc

ho
lin

gu
is

tic
Su

rp
ris

al
+E

m
be

d
Id

ea
+I

nt
eg

ra
tio

n

Model

A
cc

ur
ac

y
Model Accuracy for Psycholinguistic Feature Sets

*All differences significant except those marked 'n.s.'

Figure 1: Results on the ESEW corpus for each
set of psycholinguistic features individually (first
4 columns) and altogether (5th column), with the
feature sets based on the ModelBlocks and
Stanford parsers in the last two columns.

classify each difference vector based on whether
the first term in the difference was the ‘easier’ or
the ‘harder’ sentence

Note that the benefit to this approach is that the
resulting weight vector W learned via the classifi-
cation task can be used directly to score individual
sentences as well, with the expectation that higher
scores will correspond to more difficult sentences.

We use an averaged perceptron model (Collins,
2002) implemented in Python as our classifier.

6 Analysis & Results

The feature sets for individual psycholinguistic
theories only achieve accuracies between 55% and
65% (see the first 4 columns of Fig. 1). Com-
bining all of these features into the PSYCHOLIN-
GUISTIC model improves performance to nearly
70% (column 5). Looking at the feature sets
grouped by parser (columns 6 and 7), we see
that the combination of surprisal and embedding
depth (from the ModelBlocks parser) signifi-
cantly outperforms the combination of integration
cost and idea density (from the Stanford Parser).
However, the strength of the features derived from
ModelBlocks seems to be primarily driven by
the EMBEDDING features, while the strength of
the dependency-parse-derived features appears to
stem from INTEGRATIONCOST.

71.24 72.58 72.39 73.22

60

65

70

Ba
se

lin
e

Ba
se

+

   
   

   
   

 S
ur

pr
is

al
+

   
   

   
   

 E
m

be
d

Ba
se

+

   
   

   
   

 Id
ea

+ 

   
   

   
   

 In
te

gr
at

io
n F

ul
l

Model

A
cc

ur
ac

y

Accuracy for Overall and By−Parser Comparisons

*All differences significant except those marked 'n.s.'

Figure 2: Results for the baseline model, our two
parser-grouped feature sets, and the full model on
the ESEW corpus.

Moving to Figure 2, we see that our BASE-
LINE features achieved an accuracy of 71.24%, de-
spite using only average word length and sentence
length. This is 1.48 percentage points higher than
the 69.76% accuracy of the PSYCHOLINGUIS-
TIC model, which includes surprisal, embedding
depth, integration cost, and idea density. How-
ever, the FULL model (column 4) outperforms the
BASELINE by a statistically significant11 1.98 per-
centage points (p � 0.01). This confirms our pri-
mary hypothesis: psycholinguistic features based
on online sentence processing can improve models
of sentence complexity beyond a simple baseline.

To address the secondary hypothesis, we turn
to the OSE data in Figure 3. The best model for
this corpus uses the baseline features combined
with embedding depth and surprisal features ex-
tracted from ModelBlocks. In both OSEfar
and OSEnear we gain about 3 points over the
baseline when adding these features (3.18 and
3.25 points, respectively), which is similar to the
gains for the FULL model over the baseline. The
fact that the increase in performance between the
BASELINE model and the best performing model
does not differ between the OSEnear and the
OSEfar datasets suggests a lack of support for our
secondary hypothesis that these features are espe-

11 Using McNemar’s (1947) test throughout, as is stan-
dard for paired samples like ours, with Bonferroni correction
where appropriate.

965



n.s.

n.s.

*

n.s.

n.s.

75.1
77.94

75.5
77.24

82.11
85.29

82.87
84.38

71.36
74.61

71.85
74.23

60

70

80

O
SE

_a
ll

O
SE

_f
ar

O
SE

_n
ea

r

Corpus

A
cc

ur
ac

y

Model

Baseline

Baseline+Surprisal+Embed

Baseline+Idea+Integration

Full

Model Accuracy for Psycholinguistic Feature Sets

*All differences significant except those marked 'n.s.'

Figure 3: Results for the baseline model, our two parser-grouped feature sets, and the full model on the
OSE corpus, with additional breakdown by level proximity.

cially helpful for distinguishing items of similar
difficulty levels.

These results warrant a full comparison to the
work of Ambati et al. (2016), despite the dif-
ferences in our evaluation sets. Ambati et al.
found that their features based on incremental
CCG derivations achieved an accuracy of 72.12%,
while the offline psycholinguistic features of Va-
jjala & Meurers came in at 74.58%, 1.36 per-
centage points better than our 73.22%. Finally, a
model combining all of Vajjala & Meurers featurs
with the incremental CCG features achieved a per-
formance of 78.87%. Since the features examined
in our study are complementary to those proposed
by these two previous studies, a model combining
all of these features should further improve in ac-
curacy.

7 Conclusion

We examined features for the ranking of sen-
tences by their complexity, training linear models
on two corpora using features derived from psy-
cholinguistic theories of online sentence process-
ing: idea density, surprisal, integration cost, and
embedding depth.

Surprisal coupled with embedding depth and
our baseline features (average word length & sen-
tence length) performed as well as the full model

across all subsets of the OSE corpus. Integration
cost and idea density were less effective, suggest-
ing that the gain in speed from running a faster
dependency parser may not be worth it. Instead,
it is necessary to use the slower ModelBlocks
parser to extract the more useful features.

Overall, our strongest model combined the
baseline features and the online psycholinguistic
features. Because these features are complemen-
tary to features which have been explored in other
work (Vajjala and Meurers, 2014; Ambati et al.,
2016), the next step in future work is to combine
all of these features and conduct a more compari-
son between the features proposed here and those
examined in earlier work. In the meantime, we
have demonstrated that features derived from psy-
cholinguistic theories of sentence processing can
be used to improve models for ranking sentences
by readability.

Acknowledgments

Thanks are due to Matthew Crocker, Michael
White, Eric Fosler-Lussier, William Schuler, Det-
mar Meurers, Marten van Schijndel, and Sowmya
Vajjala for discussions and guidance during the de-
velopment of this work. We are supported by DFG
collaborative research center SFB 1102 ‘Informa-
tion Density and Linguistic Encoding’.

966



References
Ram Bharat Ambati, Siva Reddy, and Mark Steedman.

2016. Assessing relative sentence complexity us-
ing an incremental ccg parser. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1051–1057. As-
sociation for Computational Linguistics.

Lucy Bryant, Elizabeth Spencer, Alison Ferguson,
Hugh Craig, Kim Colyvas, and Linda Worrall.
2013. Propositional Idea Density in aphasic dis-
course. Aphasiology, (July):1–18, jun.

Jeanne S. Chall. 1958. Readability: an appraisal of re-
search and application. The Ohio State University,
Columbus, OH, USA.

Michael Collins. 2002. Ranking Algorithms for
NamedEntity Extraction: Boosting and the Voted
Perceptron. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 489–496, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

Scott A. Crossley, David B. Allen, and Danielle S. Mc-
Namara. 2011. Text readability and intuitive sim-
plification: A comparison of readability formulas.
Reading in a Foreign Language, 23(1):84–101.

Andre Luiz Verucci Da Cunha, Lucilene Bender De
Sousa, Leticia Lessa Mansur, and Sandra Maria
Aluı́sio. 2015. Automatic Proposition Extraction
from Dependency Trees: Helping Early Prediction
of Alzheimer’s Disease from Narratives. 2015 IEEE
28th International Symposium on Computer-Based
Medical Systems, pages 127–130.

Vera Demberg and Frank Keller. 2008. Data from
Eye-tracking Corpora as Evidence for Theories
of Syntactic Processing Complexity. Cognition,
109(2):193–210.

William H. Dubay. 2007. Unlocking Language: The
Classic Readability Studies.

Sarah Tomaszewski Farias, Vineeta Chand, Lisa
Bonnici, Kathleen Baynes, Danielle Harvey, Dan
Mungas, Christa Simon, and Bruce Reed. 2012.
Idea density measured in late life predicts subse-
quent cognitive trajectories: Implications for the
measurement of cognitive reserve. Journals of
Gerontology - Series B Psychological Sciences and
Social Sciences, 67 B(6):677–686.

Lijun Feng, Noémie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In Proc. of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL), pages 229–237.

Alison Ferguson, Elizabeth Spencer, Hugh Craig, and
Kim Colyvas. 2014. Propositional Idea Density in
women’s written language over the lifespan: Com-
puterized analysis. Cortex, 55(1):107–121, jun.

M. Galley and K. McKeown. 2003. Improving word
sense disambiguation in lexical chaining. In Proc. of
the 18th International Joint Conference on Artificial
Intelligence, pages 1486–1488.

Edward Gibson. 1998. Linguistic complexity: locality
of syntactic dependencies. Cognition, 68(1):1–76.

Edward Gibson. 2000. The Dependency Locality The-
ory: A Distance-Based Theory of Linguistic Com-
plexity. In Y Miyashita, A Marantz, and W O’Neil,
editors, Image, Language, Brain, chapter 5, pages
95–126. MIT Press, Cambridge, Massachusetts.

Daniel Gildea and David Temperley. 2010. Do Gram-
mars Minimize Dependency Length? Cognitive Sci-
ence, 34:286–310.

Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse, and Zhiqiang Cai. 2004. Coh-metrix:
analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments, & Computers,
36(2):193–202.

William S. Gray and Bernice E. Leary. 1935. What
makes a book readable. University of Chicago
Press, Chicago, Illinois, USA.

William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,
and Wei Wu. 2015. Aligning Sentences from Stan-
dard Wikipedia to Simple Wikipedia. In Proc. of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Denver, Colorado, USA.

T. Florian Jaeger. 2006. Redundancy and Syntactic
Reduction in Spontaneous Speech. Unpublished dis-
sertation, Stanford University.

T. Florian Jaeger. 2010. Redundancy and reduc-
tion: speakers manage syntactic information density.
Cognitive Psychology, 61(1):23–62, aug.

David Kauchak, Obay Mouradi, Christopher Pentoney,
and Gondy Leroy. 2014. Text simplification tools:
Using machine learning to discover features that
identify difficult text. Proceedings of the Annual
Hawaii International Conference on System Sci-
ences, pages 2616–2625.

Alan Kennedy and Joël Pynte. 2005. Parafoveal-on-
foveal effects in normal reading. Vision Research,
45(2):153–168.

J. Peter Kincaid, Robert P. Fishburne, Jr., Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation
of new readability formulas (Automated Readabil-
ity Index, Fog Count and Flesch Reading Ease For-
mula) for Navy enlisted personnel. Technical re-
port, Naval Technical Training Command, Memphis
- Millington, TN, USA.

Walter Kintsch and Janice Keenan. 1973. Reading
Rate and of Propositions Retention as a Function
of the Number in the Base Structure of Sentences.
Cognitive Psychology, 5:257–274.

967



Walter Kintsch. 1972. Notes on the structure of seman-
tic memory. In Endel Tulving and Wayne Donald-
son, editors, Organization of memory, pages 247–
308. Academic Press, New York, New York, USA.

Roger Levy and T. Florian Jaeger. 2007. Speakers op-
timize information density through syntactic reduc-
tion. Advances in Neural Information Processing
Systems 20 (NIPS).

Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126–77, mar.

Bertha A. Lively and S. L. Pressey. 1923. A Method
for Measuring the ‘Vocabulary Burden’ of Text-
books. Educational Administration and Supervi-
sion, IX:389–398.

Iain Macdonald and Advaith Siddharthan, 2016. Pro-
ceedings of the 9th International Natural Language
Generation conference, chapter Summarising News
Stories for Children, pages 1–10. Association for
Computational Linguistics.

Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153–157.

George A. Miller. 1956a. Human memory and the
storage of information. IRE Transactions on Infor-
mation Theory (IT-2), 2(3):129–137, Sep.

George A. Miller. 1956b. The magical number seven,
plus or minus two: some limits on our capacity for
processing information. Psychological Review.

Courtney Napoles and Mark Dredze. 2010. Learn-
ing Simple Wikipedia : A Cogitation in Ascertaining
Abecedarian Language. Computational Linguistics,
(June):42–50.

Charles A. Perfetti. 1969. Lexical density and phrase
structure depth as variables in sentence retention.
Journal of Verbal Learning and Verbal Behavior,
8(6):719–724.

Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assessment.
Computer Speech and Language, 23:89–106.

Sarah E. Petersen. 2007. Natural Language Process-
ing Tools for Reading Level Assessment and Text
Simplification for Bilingual Education. PhD thesis,
University of Washington.

Kathryn P. Riley, David A. Snowdon, Mark F.
Desrosiers, and William R. Markesbery. 2005.
Early life linguistic ability, late life cognitive func-
tion, and neuropathology: findings from the Nun
Study. Neurobiology of Aging, 26(3):341–347, Mar.

Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.

In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
324–333. Association for Computational Linguis-
tics.

Claude E. Shannon. 1948. A Mathematical Theory of
Communication. The Bell System Technical Jour-
nal, 27(3):379–423.

L. A. Sherman. 1893. Analytics of Literature. Ginn &
Company, Boston, Massachusetts, USA.

Sowmya Vajjala and Detmar Meurers. 2012. On Im-
proving the Accuracy of Readability Classification
using Insights from Second Language Acquisition.
In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications
(BEA7). Association for Computational Linguistics.

Sowmya Vajjala and Detmar Meurers. 2014. As-
sessing the relative reading level of sentence pairs
for text simplification. In Proc. of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), Gothenburg,
Sweden. Association for Computational Linguistics.

Sowmya Vajjala. 2015. Analyzing Text Complexity and
Text Simplification: Connecting Linguistics, Pro-
cessing and Educational Applications. Phd thesis,
Eberhard Karls Universitaet Tuebingen.

Marten van Schijndel, Andy Exley, and William
Schuler, 2012. Proceedings of the 3rd Workshop
on Cognitive Modeling and Computational Linguis-
tics (CMCL 2012), chapter Connectionist-Inspired
Incremental PCFG Parsing, pages 51–60. Associ-
ation for Computational Linguistics.

Marten van Schijndel, Andy Exley, and William
Schuler. 2013. A model of language processing as
hierarchic sequential prediction. Topics in Cognitive
Science, 5(3):522–40.

Michael D. Wilson. 1988. The MRC Psycholinguistic
Database: Machine Readable Dictionary, Version 2.
Behavior Research Methods, Instruments, & Com-
puters, 20(1):6–11.

Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1189–1198. Associa-
tion for Computational Linguistics.

Victor H. Yngve. 1960. A Model and an Hypothe-
sis for Language Structure. American Philosophical
Society, 104(5):444–466.

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353–1361. Coling
2010 Organizing Committee.

968


