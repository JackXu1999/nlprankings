



















































Dialogue Natural Language Inference


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731–3741
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3731

Dialogue Natural Language Inference

Sean Welleck
New York University
wellecks@nyu.edu

Jason Weston
Facebook AI Research
New York University

Arthur Szlam
Facebook AI Research

Kyunghyun Cho
New York University

Facebook AI Research
CIFAR Azrieli Global Scholar

Abstract

Consistency is a long standing issue faced by
dialogue models. In this paper, we frame the
consistency of dialogue agents as natural lan-
guage inference (NLI) and create a new natu-
ral language inference dataset called Dialogue
NLI. We propose a method which demon-
strates that a model trained on Dialogue NLI
can be used to improve the consistency of a
dialogue model, and evaluate the method with
human evaluation and with automatic metrics
on a suite of evaluation sets designed to mea-
sure a dialogue model’s consistency.

1 Introduction

A long standing issue faced by dialogue models is
consistency (Li et al., 2016; Vinyals et al., 2015;
Zhang et al., 2018). An example from (Vinyals
et al., 2015) shows a two-round dialogue in which
their neural sequence model first responds to what
is your job? with i’m a lawyer, then responds to
what do you do? with i’m a doctor. Even when in-
consistencies are relatively rare and semantically
plausible, they are jarring, and because semantic
plausibility is not enough to root them out, pre-
venting them is challenging.

One approach to increasing the consistency of a
chit-chat dialogue model was proposed in (Zhang
et al., 2018), where the dialogue agent was given
a set of personal facts describing its character (a
persona) and produces utterances that reflect the
persona. The intended outcome is that the agent
produces utterances consistent with its given per-
sona. However, these models still face the consis-
tency issue, as shown in Figure 1.

Separately, the framework of Natural Language
Inference (NLI) (Bowman et al., 2015; Dagan
et al., 2006; Maccartney and Manning, 2009) in-
volves learning a mapping between a sentence pair
and an entailment category. It is hypothesized that

the NLI task is a proxy for general goals in natu-
ral language processing, such as language under-
standing (Bowman et al., 2015; Williams et al.,
2018). Thus, the NLI task has been used for
learning general sentence representations (Con-
neau et al., 2017) and for evaluating NLP mod-
els (Poliak et al., 2018a; Wang et al., 2018), with
the expectation that such models will be useful in
downstream tasks.

Despite this expectation, leveraging an NLI
model for a downstream task remains an under-
explored research direction. An NLI model may
improve downstream task performance if prop-
erly used, while downstream tasks may yield new
datasets or identify issues with existing NLI mod-
els, thus expanding the NLI research domain.

In this paper, we reduce the problem of consis-
tency in dialogue to natural language inference.
We first create a dataset, Dialogue NLI,1 which
contains sentence pairs labeled as entailment, neu-
tral, or contradiction.

Then, we demonstrate that NLI can be used to
improve the consistency of dialogue models using
a simple method where utterances are re-ranked
using a NLI model trained on Dialogue NLI. The
method results in fewer persona contradictions on
three evaluation sets. The evaluation sets can be
used independently to automatically evaluate a di-
alogue model’s persona consistency, reducing the
need for human evaluation. We discuss several fu-
ture research directions involving this approach.

2 Dialogue Consistency and Natural
Language Inference

First, we review the dialogue generation and nat-
ural language inference problems as well as the
notions of consistency used throughout.

1The dataset is available at wellecks.github.io/
dialogue_nli.

wellecks.github.io/dialogue_nli
wellecks.github.io/dialogue_nli


3732

Figure 1: Persona-based dialogue with a Key-Value
Memory Network trained on Persona-Chat (Zhang et al.,
2018).

Figure 2: Relating triples, persona sentences, and utter-
ances to derive annotated sentence pairs. Shown here is
a “relation swap” contradiction.

Dialogue Generation Dialogue generation can
be framed as next utterance prediction, in which
an utterance (a sequence of tokens representing
a sentence) ut+1 is predicted given a conversa-
tion prefix u≤t. A sequence of utterances is in-
terpreted as a dialogue between agents. For in-
stance, an alternating two-agent dialogue which
starts with agent A and ends with agent B is writ-
ten as uA1 , u

B
2 , u

A
3 , u

B
4 , ..., u

B
T .

Persona-Based Dialogue In persona-based di-
alogue, each agent is associated with a persona,
PA and PB . An utterance is now predicted using
the conversation prefix u≤t and the agents own
persona, e.g. PA for agent A. It is assumed that
an agent’s utterances are conditionally dependent
on its persona, which can be interpreted as the ut-
terances being representative of, or reflecting, the
persona.

A typical approach for representing the persona
is to use a set of sentences P = {p1, ..., pm}.

Consistency A consistency error, or contradic-
tion, occurs when an agent produces an utterance
that contradicts one of their previous utterances.
Similarly, a persona consistency error, or persona
contradiction, occurs when an agent produces an
utterance that contradicts a subset of its persona.

A contradiction may be a clear logical contra-
diction, e.g. I have a dog vs. I do not have a
dog, but in general is less clearly defined. As a
result, in addition to logical contradictions, we in-
terpret a consistency error as being two utterances
not likely to be said by the same persona. For in-
stance, “i’m looking forward to going to the bas-
ketball game this weekend!” vs. “i don’t like at-
tending sporting events”, as well as “i’m a lawyer”
vs. “i’m a doctor” would be viewed here as con-

tradictions, although they are not strict logical in-
consistencies.

Similarly, a persona consistency error is inter-
preted here as an utterance which is not likely to
be said given a persona described by a given set
of persona sentences, in addition to logical contra-
dictions.

Natural Language Inference Natural Lan-
guage Inference (NLI) assumes a dataset
D = {(s1, s2)i, yi}Ni=1 which associates an
input pair (s1, s2) to one of three classes
y ∈ {entailment, neutral, contradiction}. Each
input item sj comes from an input space Sj ,
which in typical NLI tasks is the space of natural
language sentences, i.e. sj is a sequence of words
(w1, ..., wK) where each word wk is from a
vocabulary V .

The input (s1, s2) are referred to as the premise
and hypothesis, respectively, and each label is in-
terpreted as meaning the premise entails the hy-
pothesis, the premise is neutral with respect to
the hypothesis, or the premise contradicts the hy-
pothesis. The problem is to learn a function
fNLI(s1, s2) → {E,N,C} which generalizes to
new input pairs.

Reducing Dialogue Consistency to NLI Iden-
tifying utterances which contradict previous utter-
ances or an agent’s persona can be reduced to nat-
ural language inference by assuming that contra-
dictions are contained in a sentence pair. That
is, given a persona PA = {pA1 , ..., pAm} for agent
A and a length-T dialogue uA1 , u

B
2 , ...u

A
T−1, u

B
T ,

it is assumed that a dialogue contradiction for
agent A is contained in an utterance pair (uAi , u

A
j ),

and a persona contradiction is contained in a pair
(uAi , p

A
k ). Similarly, we assume that entailments



3733

and neutral interactions, defined in Section 3, are
contained in sentence pairs. We do not consider re-
lationships which require more than two sentences
to express.

Under this assumption, we can use a natural lan-
guage inference model fNLI to identify entailing,
neutral, or contradicting utterances.

Section 3 proposes a dialogue-derived dataset
for training fNLI, and Section 4 proposes a method
which incorporates fNLI with a dialogue model for
next utterance prediction.

3 Dialogue NLI Dataset

The Dialogue NLI dataset consists of sentence
pairs labeled as entailment (E), neutral (N), or con-
tradiction (C).

Sentences Sentences originate from a two-agent
persona-based dialogue dataset. A dialogue be-
tween agents A and B consists of a sequence of
utterances uA1 , u

B
2 , u

A
3 , u

B
4 , ..., u

B
T , and each agent

has a persona represented by a set of persona sen-
tences {pA1 , ..., pAmA} and {p

B
1 , ..., p

B
mB
}. The Di-

alogue NLI dataset consists of (ui, pj) and (pi, pj)
pairs2 from the Persona-Chat dataset (Zhang et al.,
2018)3.

3.1 Triple Generation

In order to determine labels for our dataset, we
require human annotation of the utterances and
persona sentences in PersonaChat, as the orig-
inal dataset does not contain this information.
We perform such annotation by first associat-
ing a human-labeled triple (e1, r, e2) with each
persona sentence, and a subset of all the utter-
ances, detailed in 3.2. Each triple contains the
main fact conveyed by a persona sentence, such
as (i, have pet, dog) for the persona sen-
tence I have a pet dog, or a fact mentioned in an
utterance, such as No, but my dog sometimes does.

Persona sentences and utterances are grouped
by their triple (e.g. see Figure 2), and pairs (u, p)
and (p, p) are defined as entailment, neutral, or
contradiction based on their triple according to the
criteria below. For examples and summary, we re-
fer readers to Tables 1–2.

2 We also release additional (ui, uj) pairs, but experi-
ments in this paper are not based on them.

3The dataset collection process is applicable to other
persona-based dialogue datasets such as (Mazaré et al.,
2018).

Entailment Each unique pair of sentences that
share the same triple are labeled as entailment.

Neutral Neutral pairs are obtained with three
different methods.

First, a miscellaneous utterance is a (u, p) pair
of which u is not associated with any triple. This
includes greetings (how are you today?) and
sentences unrelated to a persona sentence (the
weather is ok today), so such utterances are as-
sumed to be neutral with respect to persona sen-
tences.

The second method, persona pairing, takes ad-
vantage of the fact that each ground-truth persona
is typically neither redundant nor contradictory. A
persona sentence pair (p, p′) is first selected from
a persona if p and p′ do not share the same triple.
Then each sentence associated with the same triple
as p is paired with each sentence associated with
the same triple as p′.

Lastly, we specify relation swaps (r, r′) for cer-
tain relations (see Appendix A.2) whose triples
are assumed to represent independent facts, such
as have vehicle and have pet. A sentence
pair, whose first sentence is associated with a
triple (·, r, ·) and whose second sentence has triple
(·, r′, ·), is labeled as neutral. See Table 1 for an
example.

Contradiction We obtain contradictions using
three methods. See Figure 2 for an example.

First, the relation swap method is
used by specifying contradicting relation
pairs (r, r′) (see Appendix A.2), such as
(like activity,dislike), then pairing
each sentence associated with the triple (e1, r, e2)
with each sentence associated with (e1, r′, e2).

Similarly, an entity swap consists of specify-
ing relations, e.g., physical attribute, that
would yield a contradiction when the value of e2
is changed to a different value e′2, e.g., short→
tall (see Appendix A.3). Sentences associated
with (e1, r, e2) are then paired with sentences as-
sociated with (e1, r, e′2).

Finally, a numeric contradiction is obtained by
first selecting a sentence which contains a num-
ber that appears in the associated triple (see Table
1). A contradicting sentence is generated by re-
placing the sentence’s numeric surface form with
a different randomly sampled integer in the num-
ber or text form.



3734

Triple Premise Hypothesis Triple Label
(i, like activity, chess) i listen to a bit of every-

thing . it helps me fo-
cus for my chess tour-
naments .

i like to play chess . (i, like activity, chess) E

- how are you today? i drink espresso . (i, like drink, espresso) N

(i, like goto, spain) i love spain so much , i
been there 6 times .

i think i will retire in a
few years .

(i, want do, retire) N

(i, have vehicle, car) my vehicle is older
model car .

i have pets . (i, have pet, pets) N

(i, dislike, cooking) i really do not enjoy
preparing food for my-
self .

i like to cook with food
i grow in my garden .

(i, like activity, cooking) C

(i, physical attribute, short) height is missing from
my stature .

i am 7 foot tall . (i, physical attribute, tall) C

(i, have family, 3 sister) i have a brother and 3
sisters .

i have a brother and
four sisters .

(i, have family, 4 sister) C

Table 1: Examples from the validation set.

Train Valid Test Test-Gold
Data Type Label (u, p) (p, p) (u, p) (p, p) (u, p) (p, p) (u, p) (p, p)
Matching Triple E 43,000 57,000 5,000 500 4,500 900 3,712 615

Misc. Utterance N 50,000 - 3,350 - 3,000 - 2,282 -
Persona Pairing N 20,000 10,000 2,000 - 2,000 - 1,466 -
Relation Swap N 20,000 - 150 - 400 - 260 -

Relation Swap C 19,116 2,600 85 14 422 50 279 44
Entity Swap C 47,194 31,200 4,069 832 3,400 828 2,246 591
Numerics C 10,000 - 500 - 1,000 - 881 -

Dialogue NLI Overall 310,110 16,500 16,500 12,376

Table 2: Dialogue NLI Dataset Properties. (u, p) and (p, p) refer to (utterance, persona sentence) and (persona
sentence, persona sentence) pairs, respectively. Numerics consist of (u, u) (u, p) and (p, p) pairs.

3.2 Triple Annotation

Each persona sentence is annotated with a
triple (e1, r, e2) using Amazon Mechanical Turk
task. We first define a schema consisting of
〈category〉〈relation〉〈category〉 rules, such as
〈person〉have pet〈animal〉, where the relation
comes from a fixed set of relation types R, listed
in Appendix A.1. Given a sentence, the annotator
selects a relation r from a drop-down populated
with the values in R. The annotator then selects
the categories and values of the entities e1 and e2
using drop-downs that are populated based on the
schema rules. An optional drop-down contains nu-
meric values for annotating entity quantities (e.g.,
3 brothers). If selected, the numeric value is con-
catenated to the front of the entity value. The an-
notator can alternatively input an out-of-schema

entity value in a text-box. Using this method, each
of the 10,832 persona sentences is annotated with
a triple (e1, r, e2), where r ∈ R, e1 ∈ E1, and
e2 ∈ E2. Here E1 is the set of all annotated e1 from
the drop-downs or the text-box, and E2 is similarly
defined.

Finally, utterances are associated with a triple
as follows. Let p be a persona sentence with triple
(e1, r, e2). We start with all utterances, U , from
agents that have p in their persona. An utterance
u ∈ U is then associated with the triple (e1, r, e2)
and persona sentence p when e2 is a sub-string of
u, or word similarity4 sim(u, p) ≥ τ is suitably
large.

4 We use cosine similarity between the mean of TF-IDF
weighted GloVe (Pennington et al., 2014) word vectors and
set τ = 0.9.



3735

3.3 Statistics
Table 2 summarizes the dataset and its underlying
data types. The label, triple, and data type are sup-
plied as annotations for each sentence pair. We
additionally create a gold-standard test set (Test
Gold) by crowdsourcing three label annotations
for each example in the test set. We keep each
test example for which two or more annotators
agreed with its dataset label. All sentences in Di-
alogue NLI were generated by humans during the
crowdsourced dialogue collection process of the
Persona-Chat dataset (Zhang et al., 2018). The re-
sulting sentence pairs are thus drawn from a natu-
ral dialogue domain that differs from existing NLI
datasets, which are either drawn from different do-
mains such as image captions or created using syn-
thetic templates (Bowman et al., 2015; Demszky
et al., 2018; Khot et al., 2018; Marelli et al., 2014;
Poliak et al., 2018b; Wang et al., 2018; Williams
et al., 2018).

4 Consistent Dialogue Agents via
Natural Language Inference

We now present a method which demonstrates that
natural language inference can be used to improve
the consistency of dialogue agents. Candidate ut-
terances are re-ranked based on whether the candi-
date is predicted to contradict a persona sentence.
If the NLI model predicts that a candidate contra-
dicts a persona sentence, the candidate’s score is
penalized, with the penalty weighted by the NLI
model’s confidence5 scaled by a constant.

Specifically, assume a dialogue model
fdialogue(P, u≤t, U) → (s1, s2, ..., s|U |) and a
Dialogue NLI model fNLI(u, p) → {E,N,C}.
Given a persona P = {p1, ..., pm}, previous
utterances u≤t, and a set of candidate next-
utterances U , the dialogue model outputs a ranked
list of scores s1, s2, ..., s|U | corresponding to
next-utterance candidates u1, u2, ..., u|U |.

The NLI model is then run on each (ui, pj) pair,
predicting a label yi,j ∈ {E,N,C} with confi-
dence ci,j . A contradiction score is computed for
each candidate as:

scontradicti =

0, if yi,j 6= C ∀ pj ∈ Pmax
j:yi,j=C

ci,j , otherwise.

That is, if the candidate ui does not contra-
dict any persona sentence pj according to the NLI

5 In our experiments, the softmax output corresponding to
the contradiction class from Dialogue NLI.

Model Valid Test Test
Gold

ESIM 86.31 88.20 92.45
InferSent 85.82 85.68 89.96

InferSent SNLI 47.86 46.36 47.03
InferSent Hyp. Only 55.98 57.19 51.52
Most Common Class 33.33 34.54 34.96

ESIM Gold Triples 99.52 99.46 99.69

Table 3: Dialogue NLI Results

model, scontradicti is zero. If ui contradicts one or
more persona sentences, scontradicti is the highest
confidence, ci,j , out of the contradicting (ui, pj).6

New candidate scores are then computed as

sre-ranki = si − λ(s1 − sk)scontradicti (1)

and the candidates are sorted according to sre-rank.
Hyper-parameters λ and k control the NLI model’s
influence in re-ranking. For example, if the top
candidate has a contradiction score of 1.0, then
with λ = 1, it will be moved to the k’th position in
the ranking. λ = 0 corresponds to no re-ranking.

5 Experiments

5.1 Experiment 1: NLI
Models Many recently proposed NLI models
can be categorized into sentence encoding based
methods of the form fMLP(genc(s1), genc(s2)),
and attention-based methods of the form
fMLP(gattn(s1, s2)) (Lan and Xu, 2018). We
thus choose and train representative models
of each type which have achieved competitive
performance on existing NLI benchmark datasets.
For the sentence encoding method, we use In-
ferSent (Conneau et al., 2017), which encodes a
sentence using a bidirectional LSTM followed
by max-pooling over the output states. As the
representative attention-based method we use
the enhanced sequential inference model (ESIM,
(Chen et al., 2017)), which computes an attention
score for each word pair.

We also report results from a model trained and
evaluated using the hypothesis sentence only (In-
ferSent Hyp. Only) (Gururangan et al., 2018; Po-
liak et al., 2018c), a model trained on the existing
SNLI dataset (Bowman et al., 2015) but evaluated

6 Future work could consider filtering previous-utterance
contradictions (ui, uj) as well.



3736

Data Type Example Pred. Actual
Matching Triple

(p, p)
i am a hopeless bookworm. Neutral Entailwhen i have some spare time i read.

Matching Triple
(u, p)

i am from italy. i love the early mornings. Neutral Entaili like getting up bright and early.

Misc. Utterance i do not understand football or baseball. Contradict Neutrali am employed as an engineer.

Persona Pairing i lift weights every chance i get. Entail Neutrali work in a warehouse driving a forklift.

Relation Swap
(p, p)

canines make me shake with fear. Entail Contradicti love dogs but hate cats.

Relation Swap
(u, p)

i am heavy into fitness although i am rather large. Entail Contradicti do not like exercise or physical activity.

Entity Swap
(p, p)

hawaii is where i reside. Neutral Contradicti do not drive because i live in new york.

Entity Swap
(u, p)

tell me it was vegan food please , that is all i eat. Neutral Contradicti eat ham.

Numerics i have two part time jobs. Neutral Contradicti have 7 part time jobs.

Table 4: Example ESIM mispredictions by data type on Test Gold.

Data Type N Accuracy
Matching Triple (p, p) 615 83.58
Matching Triple (u, p) 3,712 91.25
Misc. Utterance 2,282 96.85
Persona Pairing 1,466 94.48
Relation Swap (p, p) 44 79.55
Relation Swap (u, p) 539 80.71
Entity Swap (p, p) 591 93.40
Entity Swap (u, p) 2,246 92.43
Numerics 881 96.25

Table 5: ESIM Accuracy by data type on Test Gold.

on Dialogue NLI (InferSent SNLI), and a model
which returns the most common class from the Di-
alogue NLI training set (Most Common Class).

Results Table 3 shows the performance of the
two NLI models and three baselines on the Dia-
logue NLI validation and test sets. The test perfor-
mance of ESIM (88.2%) and InferSent (85.68%)
is similar to the performance reported on the ex-
isting SNLI dataset (88.0% (Chen et al., 2017)
and 85.5% (Conneau et al., 2017), respectively),
while the results on the Dialogue NLI gold test set
(92.45%, 89.96%) are higher. As in Table 3, how-
ever, an InferSent model trained on SNLI performs
poorly when evaluated on the proposed Dialogue
NLI (47.03%). This is likely due to a mismatch
in sentence distributions between SNLI, which is

derived from image captions, and Dialogue NLI,
whose sentences more closely resemble down-
stream dialogue applications. The hypothesis-
only performance (51.52%) is lower than the
hypothesis-only baseline for SNLI (69.00% (Po-
liak et al., 2018c)), and shows that using informa-
tion from both the utterance and persona sentence
is necessary to achieve good performance on Dia-
logue NLI.

ESIM’s reasonably strong performance on Dia-
logue NLI suggests that the model may be useful
in a downstream task - a claim which we verify
in Experiment 5.1. However, there is also room
for improvement. In particular, we report the per-
formance of a model which takes the ground-truth
triples as input instead of sentences. As shown
in the last row of Table 3, each sentence’s un-
derlying triple contains sufficient information to
achieve near-perfect accuracy (99.69%). We also
show ESIM’s accuracy by data type on Test Gold
in Table 5, along with example mispredictions in
Table 4. The accuracies and examples suggest that
the NLI model could be improved further.

5.2 Experiment 2: Consistency in Dialogue
This experiment evaluates the effect of the re-
ranking method from Section 4 on the dialogue
model’s persona consistency.

Experiment Setup The re-ranking method of
Section 4 uses a dialogue next utterance prediction



3737

Haves Likes Attributes
Orig. Rerank Orig. Rerank Orig. Rerank

Hits@1 ↑ 30.2 37.3 16.9 18.7 35.2 36.4
Contradict@1 ↓ 32.5 8.96 17.6 4.1 8.0 5.7

Entail@1 ↑ 55.2 74.6 77.9 90.6 87.5 88.6

Table 6: Effect of NLI re-ranking on persona consistency in dialogue. The reported metrics are percentages
computed over each validation set.

Figure 3: Example from the Likes Evaluation Set, showing dialogue model candidates, NLI model predictions,
and reranked candidates using the method proposed in Section 4.

model and the Dialogue NLI model.
For the dialogue model we train a key-value

memory network (Zhang et al., 2018) on the
Persona-Chat dataset, which uses persona sen-
tences and the conversation prefix as context.
This model achieved the best performance on
Persona-Chat in (Zhang et al., 2018). We
train the model using ParlAI (Miller et al.,
2017) on the personachat:self original
task, using the hyper-parameters given for the
KVMemnnAgent in the ConvAI2 competition.
For the NLI model we use the ESIM model trained
on Dialogue NLI, based on the results of Experi-
ment 5.

To study the effect of re-ranking on persona
consistency, we form evaluation sets which con-
tain next-utterances which are likely to yield per-
sona contradiction or entailment, as follows.

Evaluation Sets Each example is formed
by first finding a next-utterance ut+1 in the
Persona-Chat validation set which has an
associated triple (e1, r, e2) of interest, e.g.
(i,like music,country). If a sentence in
the agent’s profile P has triple (e1, r, e2), we form
the validation example (P, u≤t, ut+1). Figure 3
shows an example.

Each example is associated with candidates U ,
consisting of the ground-truth utterance ut+1, 10
entailment candidates with the same triple as ut+1,

10 contradicting candidates with a different triple
than that of ut+1, and 10 random candidates. The
dialogue model must avoid ranking a contradicting
candidate highly.

Specifically, suppose the ground-truth next-
utterance ut+1 is associated with triple (e1, r, e2),
e.g., (i,have pet,dog). Entailment candidates
are utterances u from the validation or training
sets such that u is associated with triple (e1, r, e2).
Since by construction a sentence in the profile also
has triple (e1, r, e2), these candidates entail a pro-
file sentence. A contradicting candidate is an ut-
terance associated with a specified contradicting
triple (e′1, r

′, e′2), e.g., (i,not have,dog).
We construct three evaluation sets, Haves,

Likes, and Attributes using this process.

Metrics We introduce variants of the ranking
metric Hits@k, called Contradict@k and En-
tail@k. Contradict@k measures the proportion
of top-k candidates returned by the model which
contradict candidates, averaged over examples.
This measures the propensity of a model to highly
rank contradictions. Contradiction@1 is the pro-
portion of consistency errors made by the model.
For this metric lower values are better, in contrast
to Hits@k.

Entail@k measures the proportion of top-k
candidates returned by the model which are entail-
ment candidates, averaged over examples. Entail-



3738

Overall Score ↑ % Consistent ↑ % Contradiction ↓
Raw Calibrated Raw Calibrated Raw Calibrated

KV-Mem 2.11± 1.12 2.21± 0.26 0.24 0.27± 0.07 0.23 0.25± 0.08
KV-Mem + NLI 2.34± 1.21 2.38± 0.26 0.28 0.35± 0.08 0.19 0.16± 0.06

Table 7: Human evaluation results (mean± standard deviation).

ment candidates share the same underlying triple
as the ground-truth next utterance, so this metric
rewards highly ranked candidates that convey sim-
ilar meaning and logic to the ground-truth utter-
ance. Thus it can be interpreted as a more permis-
sive version of Hits@k.

Results Table 6 shows re-ranking results on the
three evaluation sets (λ = 1.0, k = 10). The
NLI re-ranking improves all three metrics on all
the evaluation sets. Overall dialogue performance
improves, as measured by Hits@1. The NLI re-
ranking substantially reduces the number of con-
tradicting utterances predicted by the model, and
increases the number of utterances which entail a
profile sentence, as seen in the Contradict@1 and
Entail@1 scores.

Figure 3 shows an example dialogue with candi-
dates, contradictions predicted by the NLI model,
and the corresponding re-ranked candidates.

5.3 Experiment 3: Human Evaluation

This experiment evaluates the effect of the pro-
posed NLI re-ranking method on a dialogue
model’s consistency, where consistency is judged
by human annotators in an interactive persona-
based dialogue setting.

Experiment Setup We use ParlAI (Miller et al.,
2017) which integrates with Amazon Mechanical
Turk for human evaluation. A human annotator
is paired with a model, and each is randomly as-
signed a persona from 1,155 persona sets. The hu-
man and model are then asked to make a conver-
sation of at least either five or six turns (randomly
decided). After the conversation, the annotator as-
signs three scores to the conversation, described
below. Each annotator is allowed to participate in
at most ten conversations per model, and we col-
lect 100 conversations per model. Two models are
evaluated: the same key-value memory network
used in Experiment 5.1 without re-ranking (KV-
Mem), and with re-ranking (KV-Mem + NLI).

Scoring and Calibration Following a conversa-
tion, an annotator is shown the conversation and
the model’s persona, and assigns three scores: an
overall score of how well the model represented
its persona ({1,2,3,4,5}), a marking of each model
utterance that was consistent with the model’s per-
sona ({0,1}), and a marking of each model utter-
ance that contradicted a previous utterance or the
model’s persona ({0,1}).

We use Bayesian calibration to adjust for anno-
tator bias, following (Kulikov et al., 2018). We as-
sume a model with observed scores Sij and latent
variables Mi for the unobserved score of model
i and Bj for the bias of annotator j. We then
estimate the posterior mean and variance for the
unobserved scores given the observed scores. We
use Pyro (Bingham et al., 2018) and the no-u-turn
sampler (Hoffman and Gelman, 2014) for poste-
rior inference. See Appendix C for details.

Results Table 7 shows the human evaluation re-
sults. The natural language inference re-ranking
improves all the metrics, notably the fine-grained
consistency score (0.27 vs. 0.35) and contradic-
tion score (0.25 vs. 0.16). The results are consis-
tent with the conclusions from the automatic eval-
uation in Experiment 5.1.

6 Conclusion

In this paper, we demonstrated that natural lan-
guage inference can be used to improve perfor-
mance on a downstream dialogue task. To do
so, we created a new dialogue-derived dataset
called Dialogue NLI, a re-ranking method for in-
corporating a Dialogue NLI model into a dia-
logue task, and an evaluation set which measures
a model’s persona consistency. The dataset of-
fers a new domain for natural language inference
models, and suggests avenues such as devising al-
ternative methods for using natural language in-
ference components in downstream tasks. Future
work may also incorporate contradiction informa-
tion into the dialogue model itself, and extend to
generic contradictions.



3739

References
Eli Bingham, Jonathan P Chen, Martin Jankowiak,

Fritz Obermeyer, Neeraj Pradhan, Theofanis Kar-
aletsos, Rohit Singh, Paul Szerlip, Paul Horsfall,
and Noah D Goodman. 2018. Pyro: Deep Uni-
versal Probabilistic Programming. arXiv preprint
arXiv:1810.09538.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
Christopher D Manning, and Stanford Linguistics.
2015. A large annotated corpus for learning natu-
ral language inference. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 632–642. Association for
Computational Linguistics.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
Natural Language Inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1657–1668. Association for Computational Linguis-
tics.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loc
Loc Barrault, and Antoine Bordes. 2017. Super-
vised Learning of Universal Sentence Representa-
tions from Natural Language Inference Data. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
670–680, Copenhagen, Denmark. Association for
Computational Linguistics.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. pages 177–190. Springer, Berlin,
Heidelberg.

Dorottya Demszky, Kelvin Guu, and Percy Liang.
2018. Transforming Question Answering Datasets
Into Natural Language Inference Datasets. arXiv
preprint arXiv:1809.02922.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R Bowman, and
Noah A Smith. 2018. Annotation Artifacts in Natu-
ral Language Inference Data. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 2 (Short Pa-
pers), pages 107–112, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Matthew D. Hoffman and Andrew Gelman. 2014. The
no-u-turn sampler: Adaptively setting path lengths
in hamiltonian monte carlo. J. Mach. Learn. Res.,
15(1):1593–1623.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
SCITAIL: A Textual Entailment Dataset from Sci-
ence Question Answering. In AAAI.

Diederik P Kingma and Jimmy Lei Ba. 2014. Adam: A
Method For Stochastic Optimization. arXiv preprint
arXiv:1412.6980.

Ilya Kulikov, Alexander H Miller, Kyunghyun Cho,
and Jason Weston. 2018. Importance of a Search
Strategy in Neural Dialogue Modelling. arXiv
preprint:1811.00907.

Wuwei Lan and Wei Xu. 2018. Neural Network Mod-
els for Paraphrase Identification, Semantic Textual
Similarity, Natural Language Inference, and Ques-
tion Answering. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3890–3902, Santa Fe, New Mexico, USA. As-
sociation for Computational Linguistics.

Jiwei Li, Michel Galley, Chris Brockett, Georgios P
Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A
Persona-Based Neural Conversation Model. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 994–1003, Berlin, Germany. Associ-
ation for Computational Linguistics.

Bill Maccartney and Christopher D Manning. 2009.
An extended model of natural logic. Technical re-
port.

M Marelli, S Menini, M Baroni, L Bentivogli,
R Bernardi, and R Zamparelli. 2014. A SICK cure
for the evaluation of compositional distributional se-
mantic models. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2014), Reykjavik, Iceland. European
Language Resources Association (ELRA).

Pierre-Emmanuel Mazaré, Samuel Humeau, Martin
Raison, and Antoine Bordes. 2018. Training Mil-
lions of Personalized Dialogue Agents. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, Brussels, Belgium.
Association for Computational Linguistics.

Alexander H Miller, Will Feng, Adam Fisch, Jiasen Lu,
Dhruv Batra, Antoine Bordes, Devi Parikh, and Ja-
son Weston. 2017. ParlAI: A Dialog Research Soft-
ware Platform. arXiv preprint:1705.06476.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global Vectors for Word
Representation. In Empirical Methods in Natural
Language Processing (EMNLP), pages 1532–1543.

Adam Poliak, Yonatan Belinkov, James Glass, and
Benjamin Van Durme. 2018a. On the Evaluation
of Semantic Phenomena in Neural Machine Trans-
lation Using Natural Language Inference. In Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 2 (Short Papers), pages 513–523, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Adam Poliak, Aparajita Haldar, Rachel Rudinger, J Ed-
ward Hu, Ellie Pavlick, Aaron Steven White, and
Benjamin Van Durme. 2018b. Collecting Diverse
Natural Language Inference Problems for Sentence

https://arxiv.org/pdf/1810.09538.pdf
https://arxiv.org/pdf/1810.09538.pdf
https://doi.org/10. 18653/v1/D15-1075
https://doi.org/10. 18653/v1/D15-1075
https://doi.org/10.18653/v1/P17-1152
https://doi.org/10.18653/v1/P17-1152
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://doi.org/10.1007/11736790{_}9
https://doi.org/10.1007/11736790{_}9
http://aclweb.org/anthology/N18-2017
http://aclweb.org/anthology/N18-2017
http://dl.acm.org/citation.cfm?id=2627435.2638586
http://dl.acm.org/citation.cfm?id=2627435.2638586
http://dl.acm.org/citation.cfm?id=2627435.2638586
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1412.6980.pdf
https://beamdream.github.io/
https://beamdream.github.io/
http://aclweb.org/anthology/C18-1328
http://aclweb.org/anthology/C18-1328
http://aclweb.org/anthology/C18-1328
http://aclweb.org/anthology/C18-1328
http://www.aclweb.org/anthology/P16-1094
http://www.aclweb.org/anthology/P16-1094
http://www.aclweb.org/anthology/W09-3714
http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf
http://aclweb.org/anthology/D18-1298
http://aclweb.org/anthology/D18-1298
http://parl.ai.
http://parl.ai.
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://aclweb.org/anthology/N18-2082
http://aclweb.org/anthology/N18-2082
http://aclweb.org/anthology/N18-2082
http://aclweb.org/anthology/D18-1007
http://aclweb.org/anthology/D18-1007


3740

Representation Evaluation. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 67–81. Association for
Computational Linguistics.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018c.
Hypothesis Only Baselines in Natural Language In-
ference. In The Seventh Joint Conference on Lexical
and Computational Semantics (*SEM).

Oriol Vinyals, Google Quoc, and V Le. 2015. A Neu-
ral Conversational Model. In ICML Deep Learning
Workshop.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
GLUE: A Multi-Task Benchmark and Analysis Plat-
form for Natural Language Understanding. arXiv
preprint arXiv:1804.07461.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2018. A Broad-Coverage Challenge Corpus
for Sentence Understanding through Inference. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1112–1122, New
Orleans, Louisiana. Association for Computational
Linguistics.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing Dialogue Agents: I have a dog, do you
have pets too? In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 2204–
2213, Melbourne, Australia. Association for Com-
putational Linguistics.

A Dataset Details

A.1 Schema
Relation Types :
place origin, live in citystatecountry,
live in general, nationality, em-
ployed by company, employed by general,
has profession, previous profession, job status,
teach, school status, has degree, attend school,
like general, like food, like drink, like animal,
like movie, like music, like read, like sports,
like watching, like activity, like goto, dislike,
has hobby, has ability, member of, want do,
want job, want, favorite food, favorite color, fa-
vorite book, favorite movie, favorite music,
favorite music artist, favorite activity, fa-
vorite drink, favorite show, favorite place,
favorite hobby, favorite season, favorite animal,
favorite sport, favorite, own, have, have pet,
have sibling, have children, have family,

have vehicle, physical attribute, misc attribute,
has age, marital status, gender, other.

Additional triples with a not have relation were
extracted using a dependency tree pattern.

Entity Categories : ability, activity, animal,
color, citystate, country, company, cuisine,
degree type, drink, family, food, gender, gen-
eral location, job status, language, marital,
media genres, media other, movie title, mu-
sic artist, music genre, music instrument, noun,
number, organization, person, person attribute,
person label, personality trait, profession,
read author, read genre, read title, read other,
school name, school status, school type, season,
sport type, subject, time, vehicle, location, other.

A.2 Relation Swaps

Relation swaps for contradictions include
(have *, not have),
(own, not have),
(has hobby, not have),
(like *, dislike),
(favorite *, dislike).

Neutral relation swaps include (have x,
have y), e.g. have pet, have sibling.
Additional (have * A, not have B) swaps
were defined for entities A which are a super-type
of B, namely (A,B) pairs ({pet, animal}, {dog,
cat}), ({sibling}, {brother, sister}), ({child, kid},
{son, daughter}), ({vehicle}, {car, truck}); this
includes sentence pairs such as “i have a sibling”,
“i do not have a sister”. Similarly, (not have
B, have * A) swaps were defined using the
(A, B) pairs above.

A.3 Entity Swaps

For contradictions, swapping entities for the fol-
lowing relation types was assumed to yield a con-
tradiction:

attend school, employed by company,
employed by general, favorite animal, fa-
vorite book, favorite color, favorite drink,
favorite food, favorite hobby, favorite movie,
favorite music, favorite music artist, fa-
vorite place, favorite season, favorite show,
favorite sport, gender, has profession, job status,
live in citystatecountry, marital status, na-
tionality, place origin, previous profession,
school status, want job.

Additionally, for physical attribute,
misc attribute, or other relations, an en-

http://aclweb.org/anthology/D18-1007
https://leonidk.com/
https://leonidk.com/
https://arxiv.org/pdf/1804.07461.pdf
https://arxiv.org/pdf/1804.07461.pdf
http://aclweb.org/anthology/N18-1101
http://aclweb.org/anthology/N18-1101
http://aclweb.org/anthology/P18-1205
http://aclweb.org/anthology/P18-1205
http://aclweb.org/anthology/P18-1205


3741

tity swap was done using all WordNet antonym
pairs in the personality trait and person attribute
entity categories, as well as the swaps ({blonde},
{brunette}), ({large}, {tiny}), ({carnivore, om-
nivore}, {vegan, vegetarian}), ({depressed},
{happy, cheerful}), ({clean}, {dirty}) where each
entity in the left set is swapped with each entity in
the right set.

B Experiment Details

Experiment 1 The InferSent model used the
Adam (Kingma and Lei Ba, 2014) optimizer with
learning rate 0.001, and otherwise used the hyper-
parameters from the open source implementation7.
The ESIM model used a 1-layer bidirectional
LSTM with hidden dimension 1024 and Adam op-
timizer with learning rate 0.0001, with the remain-
ing hyper-parameters set to those used by the In-
ferSent model.

C Score Calibration

1-5 star rating Let Mi ∼ N (µi, 12) be the un-
observed, underlying quality of the i-th approach,
where µi ∼ U(1, 5). Let Aj ∼ N (0, 12) be the
unobserved annotator bias, indicating whether the
j-th annotator is more or less generous. We ob-
serve a score given by the j-th annotator to the
i-th approach, and this score follows a normal dis-
tribution with its mean given by the sum of the
underlying model score and annoator bias, i.e.,
Sij ∼ N (Mi+Aj , 12). We observe some of these
scores, and given these scores, the goal is to infer
E[Mi] and V[Mi] for all i.

Utterance-pair selection Each annotator is
asked to label each utterance-pair as consistent
and/or contradictory with respect to the personas.
In this case, the unobserved, underlying model
score is modelled as a pre-sigmoid normal vari-
able, i.e., Mi ∼ N (0, 12), and the annotator bias
as a usual normal variable, i.e., Aj ∼ N (0, 12),
similarly to the 1-5 star rating case above. We
however also introduce a turn bias Tk ∼ N (0, 12)
to incorporate the potential degradation of a neu-
ral dialogue model as the conversation lengthens.
An observed score for each utterance pair then fol-
lows a Bernoulli distribution with its mean given
as the sigmoid of the sum of these three latent vari-
ables, i.e., Sijk ∼ B(sigmoid(Mi+Aj+Tk)). The

7https://github.com/facebookresearch/InferSent

goal of inference is to compute E[sigmoid(Mi)]
and V[sigmoid(Mi)].


