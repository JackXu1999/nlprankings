



















































Pretraining-Based Natural Language Generation for Text Summarization


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 789–797
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

789

Pretraining-Based Natural Language Generation for Text Summarization

Haoyu Zhang†, Jingjing Cai‡, Jianjun Xu†, Ji Wang†
†HPCL, College of Computer, National University of Defense Technology, China

‡Dalian Neusoft University of Information, China
{zhanghaoyu10, jjxu, wj}@nudt.edu.cn

caijingjing@neusoft.edu.cn

Abstract

In this paper, we propose a novel pretraining-
based encoder-decoder framework, which can
generate the output sequence based on the in-
put sequence in a two-stage manner. For the
encoder of our model, we encode the input
sequence into context representations using
BERT. For the decoder, there are two stages
in our model, in the first stage, we use a
Transformer-based decoder to generate a draft
output sequence. In the second stage, we
mask each word of the draft sequence and feed
it to BERT, then by combining the input se-
quence and the draft representation generated
by BERT, we use a Transformer-based decoder
to predict the refined word for each masked
position. To the best of our knowledge, our
approach is the first method which applies the
BERT into text generation tasks. As the first
step in this direction, we evaluate our proposed
method on the text summarization task. Exper-
imental results show that our model achieves
new state-of-the-art on both CNN/Daily Mail
and New York Times datasets.

1 Introduction

Text summarization generates summaries from in-
put documents while keeping salient information.
It is an important task and can be applied to sev-
eral real-world applications. Many methods have
been proposed to solve the text summarization
problem (See et al., 2017; Nallapati et al., 2017;
Zhou et al., 2018; Gehrmann et al., 2018). There
are two main text summarization techniques: ex-
tractive and abstractive. Extractive summarization
generates summary by selecting salient sentences
or phrases from the source text, while abstractive
methods paraphrase and restructure sentences to
compose the summary. We focus on abstractive
summarization in this work as it is more flexible
and thus can generate more diverse summaries.

Recently, many abstractive approaches are in-
troduced based on neural sequence-to-sequence
framework (Paulus et al., 2018; See et al., 2017;
Gehrmann et al., 2018; Li et al., 2018). Based on
the sequence-to-sequence model with copy mech-
anism, (See et al., 2017) incorporates a cover-
age vector to track and control attention scores on
source text. (Paulus et al., 2018) introduce intra-
temporal attention processes in the encoder and
decoder to address the repetition and incoherent
problem.

There are two issues in previous abstractive
methods: 1) these methods use left-context-only
decoder, thus do not have complete context when
predicting each word. 2) they do not utilize the
pre-trained contextualized language models on the
decoder side, so it is more difficult for the decoder
to learn summary representations, context interac-
tions and language modeling together.

Recently, BERT has been successfully used in
various natural language processing tasks, such
as textual entailment, name entity recognition and
machine reading comprehensions. In this paper,
we present a novel natural language generation
model based on pre-trained language models (we
use BERT in this work). As far as we know, this
is the first work to extend BERT to the sequence
generation task. To address the above issues of
previous abstractive methods, in our model, we de-
sign a two-stage decoding process to make good
use of BERT’s context modeling ability. On the
first stage, we generate the summary using a left-
context-only-decoder. On the second stage, we
mask each word of the summary and predict the
refined word one-by-one using a refine decoder.
To further improve the naturalness of the gener-
ated sequence, we cooperate reinforcement objec-
tive with the refine decoder.

The main contributions of this work are:
1. We propose a natural language generation



790

model based on BERT, making good use of the
pre-trained language model in the encoder and de-
coder process, and the model can be trained end-
to-end without handcrafted features.

2. We design a two-stage decoder process.
In this architecture, our model can generate each
word of the summary considering both sides’ con-
text information.

3. We conduct experiments on the benchmark
datasets CNN/Daily Mail and New York Times.
Our model achieves a 33.48 average of ROUGE-
1, ROUGE-2 and ROUGE-L on the CNN/Daily
Mail, which is state-of-the-art. On the New York
Times dataset, our model achieves about 5.6% rel-
ative improvement over ROUGE-1.

2 Background

2.1 Text Summarization

In this paper, we focus on single-document multi-
sentence summarization and propose a supervised
abstractive model based on the neural attentive
sequence-to-sequence framework which consists
of two parts: a neural network for the encoder
and another network for the decoder. The encoder
encodes the input sequence to intermediate repre-
sentation and the decoder predicts one word at a
time step given the input sequence representation
vector and previous decoded output. The goal of
the model is to maximize the probability of gen-
erating the correct target sequences. In the encod-
ing and generation process, the attention mecha-
nism is used to concentrate on the most important
positions of text. The learning objective of most
sequence-to-sequence models is to minimize the
negative log likelihood of the generated sequence
as shown in following equation, where y∗t is the
t-th ground-truth summary token.

Loss = − log
|y|∑
t=1

P (y∗t |y∗<t, X) (1)

However, with this objective, traditional se-
quence generation models consider only one di-
rection context in the decoding process, which
could cause performance degradation since com-
plete context of one token contains preceding and
following tokens, thus feeding only preceded de-
coded words to the decoder so that the model may
generate unnatural sequences. For example, at-
tentive sequence-to-sequence models often gener-
ate sequences with repeated phrases which harm

the naturalness. Some previous works mitigate
this problem by improving the attention calcula-
tion process, but in this paper we show that feeding
bi-directional context instead of left-only-context
can better alleviate this problem.

2.2 Bi-Directional Pre-Trained Context
Encoders

Recently, context encoders such as ELMo, GPT,
and BERT have been widely used in many NLP
tasks. These models are pre-trained on a huge un-
labeled corpus and can generate better contextual-
ized token embeddings, thus the approaches built
on top of them can achieve better performance.

Since our method is based on BERT, we illus-
trate the process briefly here. BERT consists of
several layers. In each layer there is first a multi-
head self-attention sub-layer and then a linear
affine sub-layer with the residual connection. In
each self-attention sub-layer the attention scores
eij are first calculated as Eq. (2) (3), in which de
is output dimension, and WQ,WK are parameter
matrices.

aij =
(hiWQ)(hjWK)

T

√
de

(2)

eij =
exp eij∑N

k=1 exp eik
(3)

Then the output is calculated as Eq. (4), which
is the weighted sum of previous outputs h added
by previous output hi. The last layer outputs is
context encoding of input sequence.

oi = hi +
N∑
j=1

eij(hjWV ) (4)

Despite the wide usage and huge success,
there is also a mismatch problem between these
pre-trained context encoders and sequence-to-
sequence models. The issue is that while us-
ing a pre-trained context encoder like BERT, they
model token-level representations by conditioning
on both direction context. During pre-training,
they are fed with complete sequences. How-
ever, with a left-context-only decoder, these pre-
trained language models will suffer from incom-
plete and inconsistent context and thus cannot gen-
erate good enough context-aware word representa-
tions, especially during the inference process.



791

3 Model

In this section, we describe the structure of our
model, which learns to generate an abstractive
multi-sentence summary from a given source doc-
ument.

Based on the sequence-to-sequence framework
built on top of BERT, we first design a refine de-
coder at word-level to tackle the two problems de-
scribed in the above section. We also introduce a
discrete objective for the refine decoders to reduce
the exposure bias problem. The overall structure
of our model is illustrated in Figure 1.

3.1 Problem Formulation

We denote the input document as X =
{x1, . . . , xm}where xi ∈ X represents one source
token. The corresponding summary is denoted
as Y = {y1, . . . , yL}, L represents the summary
length.

Given input document X , we first predict the
summary draft by a left-context-only decoder, and
then using the generated summary draft we can
condition on both context sides and refine the con-
tent of the summary. The draft will guide and con-
strain the refine process of summary.

3.2 Summary Draft Generation

The summary draft is based on the sequence-to-
sequence model. On the encoder side the in-
put document X is encoded into representation
vectors H = {h1, . . . , hm}, and then fed to
the decoder to generate the summary draft A =
{a1, . . . , a|a|}.

3.2.1 Encoder
We simply use BERT as the encoder. It first maps
the input sequence to word embeddings and then
computes document embeddings as the encoder’s
output, denoted by following equation.

H = BERT (x1, . . . , xm) (5)

3.2.2 Summary Draft Decoder
In the draft decoder, we first introduce BERT’s
word embedding matrix to map the previous sum-
mary draft outputs {y1, . . . , yt−1} into embed-
dings vectors {q1, . . . , qt−1} at t-th time step.
Note that as the input sequence of the decoder is
not complete, we do not use the BERT network to
predict the context vectors here.

Then we introduce an N layer Transformer de-
coder to learn the conditional probabilityP (A|H).
Transformer’s encoder-decoder multi-head atten-
tion helps the decoder learn soft alignments be-
tween summary and source document. At the t-th
time step, the draft decoder predicts output prob-
ability conditioned on previous outputs and en-
coder hidden representations as shown in Eq. (6),
in which q<t = {q1, . . . , qt−1}. Each generated
sequence will be truncated in the first position of
a special token ’[PAD]’. The total summary draft
decoder progress is shown in Stage 1 of Figure 1.

P vocabt (w) = fdec(q<t, H) (6)

Ldec =
∑|a|

t=1− logP (at = y∗t |a<t, H) (7)

As Eq. (7) shows, the decoder’s learning objec-
tive is to minimize negative likelihood of condi-
tional probability, in which y∗t is the t-th ground
truth word of summary.

However a decoder with this structure is not
sufficient enough: if we use the BERT network
in this decoder, then during training and infer-
ence, in-complete context(part of sentence) is fed
into the BERT module, and although we can fine-
tune BERT’s parameters, the input distribution is
quite different from the pre-train process, and thus
harms the quality of generated context representa-
tions.

If we just use the embedding matrix here, it will
be more difficult for the decoder with fresh param-
eters to learn to model representations as well as
vocabulary probabilities, from a relative small cor-
pus compared to BERT’s huge pre-training corpus.
In a word, the decoder cannot utilize BERT’s abil-
ity to generate high quality context vectors, which
will also harm performance.

This issue exists when using any other contextu-
alized word representations, so we design a refine
process to mitigate it in our approach which will
be described in the next sub-section.

3.2.3 Copy Mechanism
As some summary tokens are out-of-vocabulary
words and occurs in input document, we incor-
porate copy mechanism (Gu et al., 2016) based
on the Transformer decoder, we will describe it
briefly.

At decoder time step t, we first calculate the at-
tention probability distribution over source docu-
ment X using the bi-linear dot product of the last



792

Multi-Head 
Attention Decoder

BERT

Summary Draft Output
(Shifted Right)

Summary Draft 
Output

× 𝑁𝑁
× 𝐿𝐿

BERT

Multi-Head 
Attention Decoder

× 𝑁𝑁

× 𝐿𝐿

Stage 1 Stage 2

Document embedding

Document

Summary Draft embedding

Summary Output

Mask

Figure 1: Model Overview, N represents decoder layer number and L represents summary length.

layer decoder output of Transformer ot and the en-
coder output hj , shown in Eq. (8) (9).

ujt = otWchj (8)

αjt =
expujt∑N

k=1 expu
k
t

(9)

We then calculate copying gate gt ∈ [0, 1],
which makes a soft choice between selecting
from source and generating from vocabulary,
Wc,Wg, bg are parameters:

gt = sigmoid(Wg · [ot, h] + bg) (10)

Using gt we calculate the weighted sum of copy
probability and generation probability to get the
final predicted probability of extended vocabulary
V + X , where X is the set of out of vocabulary
words from the source document. The final prob-
ability is calculated as follow:

Pt(w) = (1− gt)P vocabt (w) + gt
∑

i:wi=w

αit (11)

3.3 Summary Refine Process
The main reason to introduce the refine process
is to enhance the decoder using BERT’s contex-
tualized representations, so we do not modify the
encoder and reuse it during this process.

On the decoder side, we propose a new word-
level refine decoder. The refine decoder receives
a generated summary draft as input, and outputs
a refined summary. As Figure 1 Stage 2 shows,
it first masks each word in the summary draft one
by one, then feeds the draft to BERT to generate

context vectors. Finally it predicts a refined sum-
mary word using an N layer Transformer decoder
which is the same as the draft decoder. At t-th time
step the t-th word of input summary is masked, and
the decoder predicts the refined word given other
words of the summary.

The learning objective of this process is shown
in Eq. (12), yt is the t-th summary word and y∗t
for the ground-truth summary word, and a6=t =
{a1, . . . , at−1, at+1, . . . , a|y|}.

Lrefine =

|y|∑
t=1

− logP (yt = y∗t |a6=t, H) (12)

From the view of BERT or other contextualized
embeddings, the refine decoding process provides
a more complete input sequence which is consis-
tent with their pre-training processes. Intuitively,
this process works as follows: first the draft de-
coder writes a summary draft based on a docu-
ment, and then the refine decoder edits the draft.
It concentrates on one word at a time, based on the
source document as well as other words.

We design the word-level refine decoder be-
cause this process is similar to the cloze task in
BERT’s pre-train process, therefore by using the
ability of the contextual language model the de-
coder can generate more fluent and natural se-
quences.

The parameters are shared between the draft de-
coder and refine decoder, as we find that using in-
dividual parameters the model’s performance de-
grades a lot. The reason may be that we use teach-
forcing during training, and thus the word-level re-
fine decoder learns to predict words given all the



793

other ground-truth words of summary. This ob-
jective is similar to the language model’s pre-train
objective, and is probably not enough for the de-
coder to learn to generate refined summaries. So
in our model all decoders share the same parame-
ters.

3.3.1 Mixed Objective
For summarization, ROUGE is usually used as
the evaluation metric, however during model train-
ing the objective is to maximize the log likeli-
hood of generated sequences. This mis-match
harms the model’s performance. Similar to pre-
vious work (Kryściński et al., 2018), we add a dis-
crete objective to the model, and optimize it by
introducing the policy gradient method. The dis-
crete objective for the summary draft process is as
shown in Eq. (13), where as is the draft summary
sampled from predicted distribution, and R(as) is
the reward score compared with the ground-truth
summary, we use ROUGE-L in our experiment.
To balance between optimizing the discrete objec-
tive and generating readable sequences, we mix
the discrete objective with maximum-likelihood
objective. Eq. (14) shows the final objective for
the draft process, note here Ldec is −logP (a|x).
In the refine process we introduce similar objec-
tives.

Lrldec = R(a
s) · [− log(P (as|x))] (13)

L̂dec = γ ∗ Lrldec + (1− γ) ∗ Ldec (14)

3.4 Learning and Inference
During model training, the objective of our model
is sum of the two processes, jointly trained using
”teacher-forcing” algorithm. During training we
feed the ground-truth summary to each decoder
and minimize the following objective.

Lmodel = L̂dec + L̂refine (15)

At test time, each time step we choose the pre-
dicted word by ŷ = argmaxy′P (y′|x), use beam
search to generate the draft summaries, and use
greedy search to generate the refined summaries.

4 Experiment

4.1 Settings
In this work, all of our models are built on
BERTBASE , although another larger pre-trained
model with better performance (BERTLARGE)

has published but it costs too much time and GPU
memory. We use WordPiece embeddings with a
30,000 vocabulary which is the same as BERT.
We set the layer of transformer decoders to 12(8
on NYT50), and set the attention heads number
to 12(8 on NYT50), set fully-connected sub-layer
hidden size to 3072. We train the model using
an Adam optimizer with learning rate of 3e − 4,
β1 = 0.9, β2 = 0.999 and � = 10−9 and use
a dynamic learning rate during the training pro-
cess. For regularization, we use dropout (Srivas-
tava et al., 2014) and label smoothing (Szegedy
et al., 2016) in our models and set the dropout rate
to 0.15, and the label smoothing value to 0.1. We
set the RL objective factor γ to 0.99.

During training, we set the batch size to 36, and
train for 4 epochs(8 epochs for NYT50 since it has
many fewer training samples), after training the
best model are selected from last 10 models based
on development set performance. Due to GPU
memory limit, we use gradient accumulation, set
accumulate step to 12 and feed 3 samples at each
step. We use beam size 4 and length penalty of 1.0
to generate logical form sequences.

During inference, we filter repeated tri-grams in
beam-search process by setting word probability
to zero if it will generate an tri-gram which exists
in the existing summary. It is a nice method to
avoid phrase repetition since the two datasets sel-
dom contains repeated tri-grams in one summary.

4.1.1 Datasets
To evaluate the performance of our model, we
conduct experiments on CNN/Daily Mail dataset,
which is a large collection of news articles and
modified for summarization. Following (See et al.,
2017) we choose the non-anonymized version of
the dataset, which consists of more than 280,000
training samples and 11490 test set samples.

We also conduct experiments on the New York
Times(NYT) dataset which also consists of many
news articles. The original dataset can be applied
here.1 In our experiment, we follow the dataset
splits and other pre-process settings of (Durrett
et al., 2016). We first filter all samples without
a full article text or abstract and then remove all
samples with summaries shorter than 50 words.
Then we choose the test set based on the date of
publication(all examples published after January
1, 2007). The final dataset contains 22,000 train-

1http://duc.nist.gov/



794

Model ROUGE-1 ROUGE-2 ROUGE-L R-AVG
Extractive
lead-3 (See et al., 2017) 40.34 17.70 36.57 31.54
SummmaRuNNer (Nallapati et al., 2017) 39.60 16.20 35.30 30.37
Refresh (Narayan et al., 2018) 40.00 18.20 36.60 31.60
DeepChannel (Shi et al., 2018) 41.50 17.77 37.62 32.30
rnn-ext + RL (Chen and Bansal, 2018) 41.47 18.72 37.76 32.65
MASK-LMglobal (Chang et al., 2019) 41.60 19.10 37.60 32.77
NeuSUM (Zhou et al., 2018) 41.59 19.01 37.98 32.86
Abstractive
PointerGenerator+Coverage (See et al., 2017) 39.53 17.28 36.38 31.06
ML+RL+intra-attn (Paulus et al., 2018) 39.87 15.82 36.90 30.87
inconsistency loss(Hsu et al., 2018) 40.68 17.97 37.13 31.93
Bottom-Up Summarization
(Gehrmann et al., 2018) 41.22 18.68 38.34 32.75

DCA (Celikyilmaz et al., 2018) 41.69 19.47 37.92 33.11
Ours
One-Stage 39.50 17.87 36.65 31.34
Two-Stage 41.38 19.34 38.37 33.03
Two-Stage + RL 41.84 19.70 38.91 33.48

Table 1: ROUGE F1 results for various models and ablations on the CNN/Daily Mail test set. R-AVG calculates
average score of Rouge-1, Rouge-2 and Rouge-L.

ing samples and 3,452 test samples and is called
NYT50 since all summaries are longer than 50
words.

We tokenize all sequences of the two datasets
using the WordPiece tokenizer. After tokenizing,
the average article length and summary length of
CNN/Daily Mail are 691 and 51, and NYT50’s av-
erage article length and summary length are 1152
and 75. We truncate the article length to 512, and
the summary length to 100 in our experiment(max
summary length is set to 150 on NYT50 as its av-
erage golden summary length is longer).

4.1.2 Evaluation Metrics

On CNN/Daily Mail dataset, we report the full-
length F-1 score of the ROUGE-1, ROUGE-2
and ROUGE-L metrics, calculated using PyRouge
package2 and the Porter stemmer option. On
NYT50, following (Paulus et al., 2018) we eval-
uate limited length ROUGE recall score(limit the
generated summary length to the ground truth
length). We split NYT50 summaries into sen-
tences by semicolons to calculate the ROUGE
scores.

2pypi.python.org/pypi/pyrouge/0.1.3

4.2 Results and Analysis

Table 1 gives the results on CNN/Daily Mail
dataset, we compare the performance of many re-
cent approaches with our model. We classify them
to two groups based on whether they are extractive
or abstractive models. As the last line of the table
lists, the ROUGE-1 and ROUGE-2 score of our
full model is comparable with DCA, and outper-
forms on ROUGE-L. Also, compared to extrac-
tive models NeuSUM and MASK-LMglobal, we
achieve slight higher ROUGE-1. Except the above
four scores, our model outperforms these models
on all the other scores, and since we have 95%
confidence interval of at most ± 0.20, these im-
provements are statistically significant. The test
set outputs of our model are released for further
study.3

4.2.1 Ablation Analysis
As the last four lines of Table 1 show, we conduct
an ablation study on our model variants to analyze
the importance of each component. We use three
ablation models for the experiments. One-Stage:
A sequence-to-sequence model with copy mecha-
nism based on BERT; Two-Stage: Adding the re-
fine decoder to the One-Stage model; Two-Stage

3https://1drv.ms/u/s!AvPUdqbfQJ503Qffg8HZzV98iosq



795

+ RL: Full model with refine process cooperated
with RL objective.

First, we compare the Two-Stage+RL model
with Two-Stage ablation, we observe that the full
model outperforms by 0.30 on average ROUGE,
suggesting that the reinforcement objective helps
the model effectively. Then we analyze the effect
of refine process by removing it from the Two-
Stage model, we observe that without the refine
process the average ROUGE score drops by 1.69.
The ablation study proves that each module is nec-
essary for our full model, and the improvements
are statistically significant on all metrics.

Figure 2: Average ROUGE-L improvement on
CNN/Daily mail test set samples with different golden
summary length.

4.2.2 Effects of Summary Length
To evaluate the impact of summary length on
model performance, we compare the average
rouge score improvements of our model with dif-
ferent length of ground-truth summaries. As the
above sub-figure of Figure 2 shows, compared to
Pointer-Generator with Coverage, on length inter-
val 40-80(occupies about 70% of test set) the im-
provements of our model are higher than shorter
samples, confirms that with better context repre-
sentations, in longer documents our model can
achieve higher performance.

As shown in the below sub-figure of Figure 2,
compared to extractive baseline: Lead-3 (See
et al., 2017), the advantage of our model will fall
when golden summary length is greater than 80.
This probably because that we truncate the long
documents and golden summaries and cannot get
full information, it could also because that the
training data in these intervals is too few to train

an abstractive model, so simple extractive method
will not fall too far behind.

4.3 Additional Results on NYT50

Table 2 reports experiment results on the NYT50
corpus. Since the short summary samples are fil-
tered, NYT50 has average longer summaries than
CNN/Daily Mail. So the model needs to catch
long-term dependency of the sequences to gener-
ate good summaries.

The first two lines of Table 2 show results of
the two baselines introduced by (Durrett et al.,
2016): these baselines select first n sentences, or
select the first k words from the original document.
Also we compare performance of our model with
two recent models, we see 2.39 ROUGE-1 im-
provements compared to the ML+RL with intra-
attn approach(previous SOTA) carries over to this
dataset, which is a large margin. On ROUGE-2,
our model also get an improvement of 0.51. The
experiment proves that our approach can outper-
form competitive methods on different data distri-
butions.

5 Related work

5.1 Text Summarization

Text summarization models are usually classi-
fied to abstractive and extractive ones. Recently,
extractive models like DeepChannel (Shi et al.,
2018), rnn-ext+RL (Chen and Bansal, 2018) and
NeuSUM (Zhou et al., 2018) achieve higher per-
formances using well-designed structures. For ex-
ample, DeepChannel propose a salience estima-
tion network and iteratively extract salient sen-
tences. (Zhang et al., 2018) train a sentence com-
pression model to teach another latent variable ex-
tractive model.

Also, several recent works focus on improving
abstractive methods. (Gehrmann et al., 2018) de-
sign a content selector to over-determine phrases
in a source document that should be part of the
summary. (Hsu et al., 2018) introduce incon-
sistency loss to force words in less attended sen-
tences(which determined by extractive model) to
have lower generation probabilities. (Li et al.,
2018) extend seq2seq model with an information
selection network to generate more informative
summaries.



796

Model R-1 R-2
First sentences 28.60 17.30
First k words 35.70 21.60
Full (Durrett et al., 2016) 42.20 24.90
ML+RL+intra-attn (Paulus et al., 2018) 42.94 26.02
Two-Stage + RL (Ours) 45.33 26.53

Table 2: Limited length ROUGE recall results on the NYT50 test set.

5.2 Pre-trained language models
Pre-trained word vectors (Mikolov et al., 2013;
Pennington et al., 2014; Bojanowski et al., 2017)
have been widely used in many NLP tasks. More
recently, pre-trained language models (ELMo,
GPT and BERT), have also achieved great success
on several NLP problems such as textual entail-
ment, semantic similarity, reading comprehension,
and question answering (Peters et al., 2018; Rad-
ford et al., 2018; Devlin et al., 2018).

Some recent works also focus on leverag-
ing pre-trained language models in summariza-
tion. (Radford et al., 2017) pretrain a language
model and use it as the sentiment analyser when
generating reviews of goods. (Kryściński et al.,
2018) train a language model on golden sum-
maries, and then use it on the decoder side to in-
corporate prior knowledge.

In this work, we use BERT(which is a pre-
trained language model using large scale unla-
beled data) on the encoder and decoder of a
seq2seq model, and by designing a two stage de-
coding structure we build a competitive model for
abstractive text summarization.

6 Conclusion and Future Work

In this work, we propose a two-stage model based
on sequence-to-sequence paradigm. Our model
utilize BERT on both encoder and decoder sides,
and introduce reinforce objective in learning pro-
cess. We evaluate our model on two bench-
mark datasets CNN/Daily Mail and New York
Times, the experimental results show that com-
pared to previous systems our approach effectively
improves performance.

Although our experiments are conducted on
summarization task, our model can be used in
most natural language generation tasks, such as
machine translation, question generation and para-
phrasing. The refine decoder and mixed objective
can also be applied on other sequence generation
tasks, and we will investigate on them in future

work.

References
Piotr Bojanowski, Edouard Grave, Armand Joulin, and

Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep Communicating Agents
for Abstractive Summarization. arXiv preprint
arXiv:1803.10357.

Ming-Wei Chang, Kristina Toutanova, Kenton Lee, and
Jacob Devlin. 2019. Language Model Pre-training
for Hierarchical Document Representations. arXiv
preprint arXiv:1901.09128.

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting. arXiv preprint arXiv:1805.11080.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.
2016. Learning-based single-document summariza-
tion with compression and anaphoricity constraints.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

Sebastian Gehrmann, Yuntian Deng, and Alexander M
Rush. 2018. Bottom-up abstractive summarization.
arXiv preprint arXiv:1808.10792.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Li. 2016. Incorporating Copying Mechanism in
Sequence-to-Sequence Learning. In ACL.

Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui
Min, Jing Tang, and Min Sun. 2018. A uni-
fied model for extractive and abstractive summa-
rization using inconsistency loss. arXiv preprint
arXiv:1805.06266.

Wojciech Kryściński, Romain Paulus, Caiming Xiong,
and Richard Socher. 2018. Improving abstrac-
tion in text summarization. arXiv preprint
arXiv:1808.07913.

https://doi.org/10.18653/v1/N18-1150
https://doi.org/10.18653/v1/N18-1150
http://arxiv.org/abs/1901.09128
http://arxiv.org/abs/1901.09128
http://aclweb.org/anthology/P/P16/P16-1188.pdf
http://aclweb.org/anthology/P/P16/P16-1188.pdf
https://doi.org/10.18653/v1/P16-1154
https://doi.org/10.18653/v1/P16-1154


797

Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo
Wang. 2018. Improving Neural Abstractive Docu-
ment Summarization with Explicit Information Se-
lection Modeling. In EMNLP, pages 1787–1796.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments. In Proceedings of the Thirty-First AAAI Con-
ference on Artificial Intelligence, pages 3075–3081.

Shashi Narayan, Shay B Cohen, and Mirella Lap-
ata. 2018. Ranking Sentences for Extractive Sum-
marization with Reinforcement Learning. arXiv
preprint arXiv:1802.08636.

Romain Paulus, Caiming Xiong, Richard Socher, and
Palo Alto. 2018. A deep reinforced model for ab-
stractive summarization. ICLR, pages 1–13.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Alec Radford, Rafal Józefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. CoRR, abs/1704.01444.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL, pages 1073–1083.

Jiaxin Shi, Chen Liang, Lei Hou, Juanzi Li, Zhiyuan
Liu, and Hanwang Zhang. 2018. DeepChannel:
Salience Estimation by Contrastive Learning for Ex-
tractive Document Summarization. CoRR.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. 2016. Re-
thinking the inception architecture for computer vi-
sion. In 2016 IEEE Conference on Computer Vision

and Pattern Recognition, CVPR 2016, pages 2818–
2826.

Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming
Zhou. 2018. Neural latent extractive document sum-
marization. arXiv preprint arXiv:1808.07187.

Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,
Ming Zhou, and Tiejun Zhao. 2018. Neural docu-
ment summarization by jointly learning to score and
select sentences. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2018, Melbourne, Australia, July 15-
20, 2018, Volume 1: Long Papers, pages 654–663.

http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14636
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14636
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14636
http://aclweb.org/anthology/N18-1158
http://aclweb.org/anthology/N18-1158
http://arxiv.org/abs/arXiv:1711.05136v4
http://arxiv.org/abs/arXiv:1711.05136v4
http://arxiv.org/abs/1704.01444
http://arxiv.org/abs/1704.01444
https://doi.org/10.18653/v1/P17-1099
https://doi.org/10.18653/v1/P17-1099
https://doi.org/arXiv:1811.02394v2
https://doi.org/arXiv:1811.02394v2
https://doi.org/arXiv:1811.02394v2
https://doi.org/10.1109/CVPR.2016.308
https://doi.org/10.1109/CVPR.2016.308
https://doi.org/10.1109/CVPR.2016.308
https://aclanthology.info/papers/P18-1061/p18-1061
https://aclanthology.info/papers/P18-1061/p18-1061
https://aclanthology.info/papers/P18-1061/p18-1061

