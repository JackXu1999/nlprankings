



















































UniMelb at SemEval-2016 Task 3: Identifying Similar Questions by combining a CNN with String Similarity Measures


Proceedings of SemEval-2016, pages 851–856,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

UniMelb at SemEval-2016 Task 3: Identifying Similar Questions by
Combining a CNN with String Similarity Measures
Doris Hoogeveen1,2

dhoogeveen@student.unimelb.edu.au

Huizhi Liang2
oklianghuizi@gmail.com

Long Duong1,2
lduong@student.unimelb.edu.au

Yitong Li2
yitongl4@student.unimelb.edu.au

Bahar Salehi1,2
bsalehi@student.unimelb.edu.au

Timothy Baldwin2
tb@ldwin.net

1NICTA
2Department of Computing and Information Systems

The University of Melbourne
VIC, Australia

Abstract

This paper describes the results of the partici-
pation of The University of Melbourne in the
community question-answering (CQA) task
of SemEval 2016 (Task 3-B). We obtained a
MAP score of 70.2% on the test set, by com-
bining three classifiers: a NaiveBayes clas-
sifier and a support vector machine (SVM)
each trained over lexical similarity features,
and a convolutional neural network (CNN).
The CNN uses word embeddings and machine
translation evaluation scores as features.

1 Introduction

In this paper we present the system we submitted for
the community question-answering (CQA) task of
the SemEval 2016 workshop (Task 3-B: Nakov et al.
(2016)). By finding an automatic way to answer new
questions based on existing ones, we unlock an enor-
mous wealth of information stored in online CQA
archives. In the task as specified for the SemEval
workshop, we were given 70 query questions. Each
question had at most ten candidate questions, which
we were to re-rank according to their similarity to
the query question. Each question consisted of a title
and a description. The data was taken from the Qatar
Living forum.1 The training data set was small: 267
queries, with ten candidate duplicate questions each.
The candidate questions were originally labelled ac-
cording to the three classes: RELEVANT, PERFECT-
MATCH and IRRELEVANT. Subsequently, however,
RELEVANT and PERFECTMATCH were merged into
a single class. In an ideal ranking, documents with

1http://www.qatarliving.com/forum

relevant labels were to be ranked higher than the IR-
RELEVANT documents.

The system we submitted combines the predic-
tions of three different classifiers through simple
voting. The first two classifiers (naive Bayes and
SVM) made use of semantic similarity measures as
features, and the third one was a convolutional neu-
ral network (CNN) that used word embeddings and
machine translation evaluation scores as input. The
combined system achieved a MAP score of 70.2%
on the test set.

2 Approach

Our system combined the scores of three different
classifiers based on simple voting. If at least two of
the three classifiers considered a candidate question
relevant to a query question, it was considered to be
relevant. The candidate questions were then ranked
according to this judgement, with the relevant ones
on top (in any order, since this was not taken into
account in the official evaluation). In this section we
will describe the details of the three classifiers.

2.1 String Similarity Features (SS1)

Our first set of features consisted of string similar-
ity measures, which we selected based on our recent
success in applying these features to measure the
compositionality of multiword expressions (Salehi
and Cook, 2013), to estimate semantic textual simi-
larity (Gella et al., 2013), and to detect cross-lingual
textual entailment (Graham et al., 2013).

To measure the string similarity between two
questions, the titles and the descriptions of the ques-
tions were lemmatized using NLTK (Bird, 2006).

851



We used two string similarity measures in this study:
longest common substring and the Smith-Waterman
algorithm.2 The output of each measure was nor-
malized to the range of [0, 1], where 0 indicated
that the questions were completely different, while
1 showed that they were identical. More details on
the string similarity measures and how we normalise
the scores are described in Salehi and Cook (2013).

Our primary experiments showed that measuring
string similarity between the titles of questions led
to a higher accuracy than using the question descrip-
tions. Therefore, we only considered the titles in our
final run. Ultimately, in order to combine all string
similarity measures into one score, we used the lin-
ear kernel SVM as implemented in the scikit-learn
package3, using the default parameters.

2.2 String Similarity Features 2 (SS2)

For our second model, we used five more lexical
similarity measures as features: the Jaccard simi-
larity, cosine similarity calculated over binary term
vectors, the overlap coefficient, de Sørensen-Dice
coefficient, and Kullback-Leibler (KL) divergence.
With these features we obtained the best results by
using both the title and the description of the ques-
tion. In contrast to our first classifier, no stemming
or lemmatization was applied because it was found
not to make any difference. The classifier we used
was naive Bayes.4

2.3 Convolutional Neural Network (CNN)

The third classifier we used was a convolutional neu-
ral network (“CNN”). CNN structures have been
shown to be very successful in speech recognition
and computer vision tasks (Graves et al., 2013;
Krizhevsky et al., 2012). Recently, they have also
been applied to natural language tasks, and again,
have achieved good results (Collobert and Weston,
2008; Yin and Schütze, 2015).

Kalchbrenner et al. (2014) developed a CNN-
based model that can be used for sentence modelling

2We also experimented with Levenshtein and modified Lev-
enshtein, but we did not observe any improvement when using
these features.

3http://scikit-learn.org
4We used the version implemented in the Weka toolkit:

http://www.cs.waikato.ac.nz/ml/weka/, using the default
settings.

problems. With several combinations of convolu-
tional filters and dynamic k-max pooling filters, the
model is very good at capturing features on both the
local word level and the global sentence level. The
word-level features are combined in several stages
to model sentences. This characteristic of capturing
meaning at different levels is particularly attractive
for the target domain, as two questions with simi-
lar meaning may have a very different surface form.
A neural network that can model meaning at the
sentence level may recognise two questions as be-
ing similar even though they have very little lexical
overlap. This is the reason we decided to use Kalch-
brenner’s CNN for our task, enhanced with some as-
pects of Yin and Schütze (2015)’s model, who used
a CNN for Paraphrase Identification (PI).

In our approach, we used the CNN to compare
two sentences at different levels in the model. The
similarity scores obtained in this way were com-
bined with several machine translation evaluation
scores and fed into a multi-Layer perceptron (MLP)
classifier to get a final similarity score. We decided
to make this final classifier a neural network too so
that we could get a non-linear output for the newly
added features. The expectation is that this will im-
prove the classification.

In the following subsections we explain the details
of the CNN: how to model sentences on different
levels and how to generate the features using sen-
tence embeddings. We also explain some other fea-
tures that we added, and how we trained the model.

2.3.1 Model Overview
Figure 1 show the CNN model. Each to-

kenised input sentence S consists of n words
{w1, w2, · · · , wn}, where n denotes the length of
the sentence. Each word has a word vector en ∈ Rd,
where d is the dimension of the word vector. All
the word vectors combined form a sentence matrix
embedding E ∈ Rd×n, which is used as the input
to our CNN model. Different input sentences will
have different lengths, but this is not a problem at
this stage. We deal with this issue when comparing
the sentences to obtain features, as explained in Sec-
tion 2.3.2.

For each convolutional level l, we convolved the
input matrix with a wide one-dimensional convo-
lution filter, and generated a convolutional matrix

852



Embedding Matrix

Sentence 1 Sentence 2Cross-unigram

E10 E
2
0

1-D Wide Convolution

k-max Pooling
E11 E

2
1

1-D Wide Convolution
+ k-max Pooling

E12 E
2
2

1-D Wide Convolution
+ k-max Pooling

E13 E
2
3

Feature Layer

MT scoresMax-out &Flatten

Figure 1: The Convolutional Neural Network.

C ∈ Rd×(n+m−1), where m is the filter width,
which is set to 3 (see the red highlighting in Fig-
ure 1). We then applied a non-linear function (rec-
tified linear units (ReLU)) to get the convolutional
layer C′ = ReLU(C). C and C′ are combined
in Figure 1 and shown as ”1-D Wide Convolution”.
Next, we applied Kalchbrenner et al. (2014)’s dy-
namic k-max pooling approach on C′. For each di-
mension of C′, we extracted a maximum of k fea-
tures and calculated the pooling layer. The output of
the pooling layer is El ∈ Rd×kl . El is the sentence
embedding of level l and is used as the input for the
next level (l + 1) of the CNN.

After a series of such convolution operations, we
get a deep CNN structure as a representation of each
question.

2.3.2 Features
As explained in the previous section, the input

features to the CNN consisted of word embeddings.
We used constant word embeddings directly from
Mikolov et al. (2013)’s pre-trained word embed-
dings model, with dimension d = 300.

After applying the convolution operations on a
question pair (S1,S2), we obtained an embed-
ding set for each question: {E10,E11, · · · ,E1l } and
{E20,E21, · · · ,E2l }, where l is the number of levels
in our CNN structure. Different embeddings repre-
sent the semantics of the question with a different
granularity. E0 corresponds to the input word em-

beddings, which represent the semantics of the ques-
tion at the word level. Each increase in subscript
represents a convolutional level in the model. The
higher up we get, the more convolved the features
will be, until we end up with El, which represents
the document-level meaning of the question.

To obtain the features for the final classifier, we
determined the similarity of the embedding matrices
E1l and E

2
l for each level, by comparing each vector

in E1l to each vector in E
2
l ; a process known on the

word level as cross-unigram comparison. The simi-
larity of the vectors was calculated using both cosine
similarity and Euclidean distance. This resulted in
two matrices per question pair per level, which we
concatenated. To reduce the size of these low-level
matrices we applied a two-dimensional maxout fil-
ter on them. The height and width of the filter were
adjusted for different sentence lengths of the ques-
tions S1 and S2, to ensure that the output always
had the same length. Next we flattened the matrix
into a vector and used this as our sentence embed-
ding similarity features that formed part of the input
to our final classifier.

Apart from the sentence embedding similarity
features, we also used several machine translation
evaluation measures as extra features before apply-
ing the final classifier. Machine translation evalu-
ation measures are designed to detect whether two
sentences have a similar meaning or not. They have

853



Training Development Test
Set Set Set

query questions 267 50 70
archived questions 2,669 500 700

Table 1: The number of query and archived question pairs in
the SemEval-2016 dataset

been shown to be useful features for paraphrase
identification (Madnani et al., 2012), a task very
similar to ours. The measures we used were BLEU
(Papineni et al., 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), Ter (Snover
et al., 2006), Ter-Plus (Snover et al., 2009), and
MaxSim (Chan and Ng, 2008). After adding these
additional features to the sentence embedding simi-
larity features, we fed our feature vectors into a Mul-
tiple Layer Perceptron (MLP) classifier to get a final
similarity score.

2.3.3 Training the model

In this section we will explain how we trained
our model. Our CNN consisted of three convolu-
tional layers, with a MLP classifier at the top. The
MLP combined three fully-connected hidden layers,
which contained 512 nodes each and ReLU as its
activation function, with a softmax layer on the top.

For the network training, we used AdaDelta
(Zeiler, 2012) to update the weights of the model,
and set the initial learning rate to α = 10−4.
Dropout (Srivastava et al., 2014) was added to the
input layer of the MLP, with L2-regularization. The
dropout rate was set to 0.5.

3 Experiments

In this section, we will describe the experimental
setup and the results of our experiments.

3.1 Dataset

Table 1 presents basic statistics for the SemEval-
2016 Task 3-B dataset. Each query question was
paired up with at most ten archived questions, which
we had to re-rank according to relevance. The
dataset was partitioned into three components: (1)
a training set, (2) a development set, and (3) a test
set.

3.2 Results

To evaluate the effectiveness of the different feature
sets, we report on the results of both the combined
model and the separate classifiers in Tables 2 and
3. Baselines 1 and 2 are the official baselines as
given by the SemEval organisers. The information
retrieval (IR) baseline was produced using Google
to rank the candidate questions.

We use the Mean Average Precision (MAP) as
our primary evaluation metric,5 but also report the
Average Recall (AvgRec), Mean Reciprocal Rank
(MRR), Precision (P), Recall (R), F1-measure (F1),
and Accuracy (Acc) scores.

On the development set, we obtained the best
MAP score with the majority voting model, but on
the test set we did not see the same result. On the
test set, the CNN model by itself obtained the best
results. It is interesting to see that all three mod-
els separately performed better on the test set than
on the development set, but the combined model did
not.

Although models SS1 and SS2 both make use
of string similarity measures, they produced differ-
ent classification outputs for 60.2% of the develop-
ment set queries. SS1’s predictions differed from
the CNN’s in 54.8% of the development queries, and
SS2’s predictions differed from the CNN’s in 72.6%
of the development queries. The fact that the three
models produced such different results, while each
performed reasonably well, was the motivation for
combining them.

One reason for the different results on the test and
development sets might be the difference in the class
balance. In the development set, 43% of the candi-
date questions were labeled as relevant, and 57% as
irrelevant. In the test set this was 33% and 67% re-
spectively. The training data resembled the develop-
ment set more than the test set, with 45% relevant
and 55% irrelevant candidate questions. We suspect
that more training data is needed to obtain consistent
results.

It would be interesting to see whether the scores
improve when we add the string similarity features
to the CNN directly (thereby losing the majority vot-
ing component), in the same way as we added the

5This is also what the official ranking of the participating
systems is based on.

854



Test Set
Model MAP(%) AvgRec(%) MRR Acc(%) P(%) R(%) F1(%)

Baseline 1(IR) 74.75 88.30 83.79 - - - -
Baseline 2 (random) 46.98 67.92 50.96 45.20 40.43 32.58 73.82

SS1 71.11 85.71 81.20 70.00 55.02 54.08 54.55
SS2 72.95 88.51 82.26 70.29 55.92 50.64 53.15
CNN 73.04 87.72 82.21 73.43 60.09 60.09 60.09

Majority voting 70.20 86.21 78.58 74.57 63.96 54.08 58.60
Table 2: The official evaluation results for the SemEval-2016 Test Set

Development Set
Model MAP(%) AvgRec(%) MRR Acc(%) P(%) R(%) F1(%)

Baseline 1(IR) 71.35 86.11 76.67 - - - -
Baseline 2 (random) 55.95 73.23 62.23 48.80 44.42 76.64 56.16

SS1 70.06 85.95 77.33 68.80 68.35 50.47 58.06
SS2 68.96 85.58 74.00 64.60 64.12 39.25 48.70
CNN 72.71 87.86 79.33 75.20 76.47 60.75 67.71

Majority voting 73.13 88.93 80.00 72.20 64.48 78.04 70.61
Table 3: The official evaluation results for the SemEval-2016 Development Set

machine translation evaluation features. We leave
this for future work.

4 Summary

In this paper, we proposed a method based on the
combination of three different classifiers for the task
of duplicate question ranking, in the context of Se-
mEval 2016 Task 3-B. The classifiers we combined
were a CNN using word embeddings and machine
translation evaluation metrics, and two classifiers
that used lexical similarity features: a naive Bayes
classifier and a support vector machine (SVM). The
results we obtained on the test set were quite differ-
ent from the results on the development set, which
may be explained by the small size of the training
data set.

References

Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the 2005 ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, volume 29, pages 65–72.

Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the 2006 International

Conference on Computational Linguistics (COLING),
Interactive Presentation Sessions, pages 69–72.

Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
Maximum Similarity Metric for Machine Translation
Evaluation. In Proceedings of the 46th Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLTNAACL), pages 55–62.

Ronan Collobert and Jason Weston. 2008. A Unified
Architecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In Pro-
ceedings of the 25th International Conference on Ma-
chine Learning, pages 160–167.

George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-Gram Co-
occurrence Statistics. In Proceedings of the 2nd Hu-
man Language Technology Conference (HLT), pages
138–145.

Spandana Gella, Bahar Salehi, Marco Lui, Karl Grieser,
Paul Cook, and Timothy Baldwin. 2013. UniMelb
NLP-CORE: Integrating Predictions from Multiple
Domains and Feature Sets for Estimating Semantic
Textual Similarity. Proceedings of the 2nd Joint
Conference on Lexical and Computational Semantics
(*SEM), pages 207–216.

Yvette Graham, Bahar Salehi, and Timothy Baldwin.
2013. Umelb: Cross-lingual Textual Entailment with
Word Alignment and String Similarity Features. Pro-
ceedings of the 2nd Joint Conference on Lexical and
Computational Semantics (*SEM), page 133.

855



Alan Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech Recognition with Deep Re-
current Neural Networks. In Proceedings of the 2013
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pages 6645–6649.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A Convolutional Neural Network for
Modelling Sentences. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 655–665.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
2012. ImageNet Classification with Deep Convolu-
tional Neural Networks. In Advances in Neural Infor-
mation Processing Systems, pages 1097–1105.

Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-Examining Machine Translation Metrics for
Paraphrase Identification. In Proceedings of the 50th
Human Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLTNAACL), pages 182–190.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and Their Composition-
ality. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Preslav Nakov, Lluı́s Màrquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2016.
SemEval-2016 Task 3: Community Question Answer-
ing. In Proceedings of the 10th International Work-
shop on Semantic Evaluation (SemEval).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311–318.

Bahar Salehi and Paul Cook. 2013. Predicting the Com-
positionality of Multiword Expressions Using Trans-
lations in Multiple Languages. In Proceedings of the
2nd Joint Conference on Lexical and Computational
Semantics (*SEM), pages 266–275.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the Association for Machine
Translation in the Americas, pages 223–231.

Matthew G Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase, Se-
mantic, and Alignment Enhancements to Translation
Edit Rate. Machine Translation, 23(2-3):117–127.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A Simple Way to Prevent Neural Networks
from Overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Wenpeng Yin and Hinrich Schütze. 2015. Convolu-
tional Neural Network for Paraphrase Identification.
In Proceedings of the 53rd Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 901–911.

Matthew D Zeiler. 2012. ADADELTA: An Adaptive
Learning Rate Method. arXiv:1212.5701.

856


