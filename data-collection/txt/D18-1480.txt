



















































Spherical Latent Spaces for Stable Variational Autoencoders


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4503–4513
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4503

Spherical Latent Spaces for Stable Variational Autoencoders

Jiacheng Xu and Greg Durrett
Department of Computer Science
The University of Texas at Austin

{jcxu,gdurrett}@cs.utexas.edu

Abstract

A hallmark of variational autoencoders
(VAEs) for text processing is their combi-
nation of powerful encoder-decoder models,
such as LSTMs, with simple latent distribu-
tions, typically multivariate Gaussians. These
models pose a difficult optimization problem:
there is an especially bad local optimum
where the variational posterior always equals
the prior and the model does not use the latent
variable at all, a kind of “collapse” which is
encouraged by the KL divergence term of the
objective. In this work, we experiment with
another choice of latent distribution, namely
the von Mises-Fisher (vMF) distribution,
which places mass on the surface of the unit
hypersphere. With this choice of prior and
posterior, the KL divergence term now only
depends on the variance of the vMF distribu-
tion, giving us the ability to treat it as a fixed
hyperparameter. We show that doing so not
only averts the KL collapse, but consistently
gives better likelihoods than Gaussians across
a range of modeling conditions, including
recurrent language modeling and bag-of-
words document modeling. An analysis of
the properties of our vMF representations
shows that they learn richer and more nuanced
structures in their latent representations than
their Gaussian counterparts.1

1 Introduction

Recent work has established the effectiveness of
deep generative models for a range of tasks in
NLP, including text generation (Hu et al., 2017;
Yu et al., 2017), machine translation (Zhang et al.,
2016), and style transfer (Shen et al., 2017; Zhao
et al., 2017a). Variational autoencoders, which
have been explored in past work for text mod-
eling (Miao et al., 2016; Bowman et al., 2016),

1The code and dataset are available at: https://
github.com/jiacheng-xu/vmf_vae_nlp

posit a continuous latent variable which is used
to capture latent structure in the data. Typical
VAE implementations assume the prior of this la-
tent space is a multivariate Gaussian; during train-
ing, a Kullback-Leibler (KL) divergence term in
loss function encourages the variational posterior
to approximate the prior. One major limitation of
this approach observed by past work is that the KL
term may encourage the posterior distribution of
the latent variable to “collapse” to the prior, effec-
tively rendering the latent structure unused (Bow-
man et al., 2016; Chen et al., 2016).

In this paper, we propose to use the von Mises-
Fisher (vMF) distribution rather than Gaussian for
our latent variable. vMF places a distribution over
the unit hypersphere governed by a mean parame-
ter µ and a concentration parameter κ. Our prior
is a uniform distribution over the unit hypersphere
(κ = 0) and our family of posterior distributions
treats κ as a fixed model hyperparameter. Since
the KL divergence only depends on κ, we can
structurally prevent the KL collapse and make our
model’s optimization problem easier. We show
that this approach is actually more robust than try-
ing to flexibly learn κ, and a wide range of settings
for fixed κ lead to good performance. Our model
systematically achieves better log likelihoods than
analogous Gaussian models while having higher
KL divergence values, showing that it more suc-
cessfully makes use of the latent variables at the
end of training.

Past work has suggested several other tech-
niques for dealing with the KL collapse in the
Gaussian case. Annealing the weight of KL term
(Bowman et al., 2016) still leaves us with brit-
tleness in the optimization process, as we show
in Section 2. Other prior work (Yang et al.,
2017; Semeniuta et al., 2017) focuses on using
CNNs rather than RNNs as the decoder in order
to weaken the model and encourage the use of the

https://github.com/jiacheng-xu/vmf_vae_nlp
https://github.com/jiacheng-xu/vmf_vae_nlp


4504

Enc

The

q�(z|x)
<latexit sha1_base64="eRnYshr18z9idMJqHhSd9zf6ZDU=">AAAB9HicbVDLTgJBEOzFF+IL9OhlIjHBC9n1okcSLx4xkUcCGzI7zMKEmdllZhbFle/w4kFjvPoNfoM3/8bhcVCwkk4qVd3p7gpizrRx3W8ns7a+sbmV3c7t7O7tH+QLh3UdJYrQGol4pJoB1pQzSWuGGU6bsaJYBJw2gsHV1G+MqNIskrdmHFNf4J5kISPYWMkfdtJ23GeT0sPj/VknX3TL7gxolXgLUqwU7sQnAFQ7+a92NyKJoNIQjrVueW5s/BQrwwink1w70TTGZIB7tGWpxIJqP50dPUGnVumiMFK2pEEz9fdEioXWYxHYToFNXy97U/E/r5WY8NJPmYwTQyWZLwoTjkyEpgmgLlOUGD62BBPF7K2I9LHCxNiccjYEb/nlVVI/L3tu2bvxipUyzJGFYziBEnhwARW4hirUgMAQnuAFXp2R8+y8Oe/z1oyzmDmCP3A+fgBzy5P+</latexit><latexit sha1_base64="0h8uwGYeses6getlEMEU7gks/3s=">AAAB9HicbVA9T8MwEL3wWcpXgZHFokIqS5SwwFiJhbFI9ENqo8pxndaq7aS2U1FCfwcLAwix8mPY+De4bQZoedJJT+/d6e5emHCmjed9O2vrG5tb24Wd4u7e/sFh6ei4oeNUEVonMY9VK8SaciZp3TDDaStRFIuQ02Y4vJn5zTFVmsXy3kwSGgjclyxiBBsrBaNu1kkGbFp5fHq46JbKnuvNgVaJn5My5Kh1S1+dXkxSQaUhHGvd9r3EBBlWhhFOp8VOqmmCyRD3adtSiQXVQTY/eorOrdJDUaxsSYPm6u+JDAutJyK0nQKbgV72ZuJ/Xjs10XWQMZmkhkqyWBSlHJkYzRJAPaYoMXxiCSaK2VsRGWCFibE5FW0I/vLLq6Rx6fqe69/55aqbx1GAUziDCvhwBVW4hRrUgcAInuEV3pyx8+K8Ox+L1jUnnzmBP3A+fwDP2JIJ</latexit>

q�(z|x)
<latexit sha1_base64="eRnYshr18z9idMJqHhSd9zf6ZDU=">AAAB9HicbVDLTgJBEOzFF+IL9OhlIjHBC9n1okcSLx4xkUcCGzI7zMKEmdllZhbFle/w4kFjvPoNfoM3/8bhcVCwkk4qVd3p7gpizrRx3W8ns7a+sbmV3c7t7O7tH+QLh3UdJYrQGol4pJoB1pQzSWuGGU6bsaJYBJw2gsHV1G+MqNIskrdmHFNf4J5kISPYWMkfdtJ23GeT0sPj/VknX3TL7gxolXgLUqwU7sQnAFQ7+a92NyKJoNIQjrVueW5s/BQrwwink1w70TTGZIB7tGWpxIJqP50dPUGnVumiMFK2pEEz9fdEioXWYxHYToFNXy97U/E/r5WY8NJPmYwTQyWZLwoTjkyEpgmgLlOUGD62BBPF7K2I9LHCxNiccjYEb/nlVVI/L3tu2bvxipUyzJGFYziBEnhwARW4hirUgMAQnuAFXp2R8+y8Oe/z1oyzmDmCP3A+fgBzy5P+</latexit><latexit sha1_base64="0h8uwGYeses6getlEMEU7gks/3s=">AAAB9HicbVA9T8MwEL3wWcpXgZHFokIqS5SwwFiJhbFI9ENqo8pxndaq7aS2U1FCfwcLAwix8mPY+De4bQZoedJJT+/d6e5emHCmjed9O2vrG5tb24Wd4u7e/sFh6ei4oeNUEVonMY9VK8SaciZp3TDDaStRFIuQ02Y4vJn5zTFVmsXy3kwSGgjclyxiBBsrBaNu1kkGbFp5fHq46JbKnuvNgVaJn5My5Kh1S1+dXkxSQaUhHGvd9r3EBBlWhhFOp8VOqmmCyRD3adtSiQXVQTY/eorOrdJDUaxsSYPm6u+JDAutJyK0nQKbgV72ZuJ/Xjs10XWQMZmkhkqyWBSlHJkYzRJAPaYoMXxiCSaK2VsRGWCFibE5FW0I/vLLq6Rx6fqe69/55aqbx1GAUziDCvhwBVW4hRrUgcAInuEV3pyx8+K8Ox+L1jUnnzmBP3A+fwDP2JIJ</latexit>

place was

Enc Enc Enc

great

Dec

The place was

Dec Dec Dec

great

 

Linear Linear

µµ log(�)log(�)

N (µ,�)N (µ,�)
zz

 

Gaussian
 

Linear Linear

µ
<latexit sha1_base64="yIvAXoT3KNkKepj28SOU7utUJ/g=">AAAB6nicbZDLSgMxFIZP6q3WW1Vw4yZYFFdlphtdDrhxWdFeoB1KJs20oUlmSDJCGfoI3bhQxK1P5M63Mb0stPWHwMf/n0POOVEquLGe940KG5tb2zvF3dLe/sHhUfn4pGmSTFPWoIlIdDsihgmuWMNyK1g71YzISLBWNLqb5a1npg1P1JMdpyyUZKB4zCmxznrsyqxXrnhVby68Dv4SKgGangUAUO+Vv7r9hGaSKUsFMabje6kNc6Itp4JNSt3MsJTQERmwjkNFJDNhPh91gi+d08dxot1TFs/d3x05kcaMZeQqJbFDs5rNzP+yTmbj2zDnKs0sU3TxUZwJbBM82xv3uWbUirEDQjV3s2I6JJpQ665TckfwV1deh2at6ntV/8GvBFewUBHO4QKuwYcbCOAe6tAACgOYwiu8IYFe0Dv6WJQW0LLnFP4Iff4A3qSO2w==</latexit><latexit sha1_base64="gfs6aeX3qDUUDl4RruVxsg1XFMA=">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI9FYkTsbLUlsLDEKksCF7C17sGF377I7Z0Iu/AQbC42x9RfZ+W9c4AoFXzLJy3szmZkXpVJY9P1vr7S2vrG5Vd6u7Ozu7R9UD4/aNskM4y2WyMR0Imq5FJq3UKDkndRwqiLJH6Pxzcx/fOLGikQ/4CTloaJDLWLBKDrpvqeyfrXm1/05yCoJClKDAs1+9as3SFimuEYmqbXdwE8xzKlBwSSfVnqZ5SllYzrkXUc1VdyG+fzUKTlzyoDEiXGlkczV3xM5VdZOVOQ6FcWRXfZm4n9eN8P4OsyFTjPkmi0WxZkkmJDZ32QgDGcoJ45QZoS7lbARNZShS6fiQgiWX14l7ct64NeDu6DWOC/iKMMJnMIFBHAFDbiFJrSAwRCe4RXePOm9eO/ex6K15BUzx/AH3ucPVAKNtw==</latexit>

µ
<latexit sha1_base64="yIvAXoT3KNkKepj28SOU7utUJ/g=">AAAB6nicbZDLSgMxFIZP6q3WW1Vw4yZYFFdlphtdDrhxWdFeoB1KJs20oUlmSDJCGfoI3bhQxK1P5M63Mb0stPWHwMf/n0POOVEquLGe940KG5tb2zvF3dLe/sHhUfn4pGmSTFPWoIlIdDsihgmuWMNyK1g71YzISLBWNLqb5a1npg1P1JMdpyyUZKB4zCmxznrsyqxXrnhVby68Dv4SKgGangUAUO+Vv7r9hGaSKUsFMabje6kNc6Itp4JNSt3MsJTQERmwjkNFJDNhPh91gi+d08dxot1TFs/d3x05kcaMZeQqJbFDs5rNzP+yTmbj2zDnKs0sU3TxUZwJbBM82xv3uWbUirEDQjV3s2I6JJpQ665TckfwV1deh2at6ntV/8GvBFewUBHO4QKuwYcbCOAe6tAACgOYwiu8IYFe0Dv6WJQW0LLnFP4Iff4A3qSO2w==</latexit><latexit sha1_base64="gfs6aeX3qDUUDl4RruVxsg1XFMA=">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI9FYkTsbLUlsLDEKksCF7C17sGF377I7Z0Iu/AQbC42x9RfZ+W9c4AoFXzLJy3szmZkXpVJY9P1vr7S2vrG5Vd6u7Ozu7R9UD4/aNskM4y2WyMR0Imq5FJq3UKDkndRwqiLJH6Pxzcx/fOLGikQ/4CTloaJDLWLBKDrpvqeyfrXm1/05yCoJClKDAs1+9as3SFimuEYmqbXdwE8xzKlBwSSfVnqZ5SllYzrkXUc1VdyG+fzUKTlzyoDEiXGlkczV3xM5VdZOVOQ6FcWRXfZm4n9eN8P4OsyFTjPkmi0WxZkkmJDZ32QgDGcoJ45QZoS7lbARNZShS6fiQgiWX14l7ct64NeDu6DWOC/iKMMJnMIFBHAFDbiFJrSAwRCe4RXePOm9eO/ex6K15BUzx/AH3ucPVAKNtw==</latexit>


<latexit sha1_base64="tRL2/a9HhTfH2DRYKzaLgVOnVDI=">AAACC3icdVDLSgMxFL3js9ZX1aWbYFG6kDIj2taVBTcuK9gHdIaSSTNtbOZBkhHK0E8Q3Oo3uHMnLtz4Ef6FG/em0wpV9ECSwz3n5ibHjTiTyjTfjbn5hcWl5cxKdnVtfWMzt7XdkGEsCK2TkIei5WJJOQtoXTHFaSsSFPsup013cD7WmzdUSBYGV2oYUcfHvYB5jGClSw17gKMId3J5s3hSMU8rx2hCSuUpKZeQVTRT5M8eC5+vAFDr5D7sbkhinwaKcCxl2zIj5SRYKEY4HWXtWNIIkwHu0bamAfapdJL0tSM7Pdqi5zrJeIZ1OLOP0L72d5EXCr0ChVLz7H0J9qUc+q52+lj15W9tXPxLa8fKqzgJC6JY0YBMBnkxRypE42BQlwlKFB9qgolg+ieI9LHAROn4sjqi7xzQ/6RxVLTMonVp5asHMEEGdmEPCmBBGapwATWoA4FruIN7eDBujSfj2XiZWOeMac8O/IDx9gWTMJuW</latexit><latexit sha1_base64="c0dO0Ms7V617D4SkJi9XoQIXotc=">AAACC3icdVDdSsMwGE3n35x/Uy+9CQ7FCymt6DbvBt54OcH9QFtGmqVbXJqWJBVG2SMI3upreCfe+hC+hY9g2lWYogeSHL5zvnzJ8WNGpbKsD6O0tLyyulZer2xsbm3vVHf3ujJKBCYdHLFI9H0kCaOcdBRVjPRjQVDoM9LzJ1eZ3rsnQtKI36ppTLwQjTgNKEZKl7ruBMUxGlRrlnnRtC6b53BO6o2CNOrQNq0cNVCgPah+usMIJyHhCjMkpWNbsfJSJBTFjMwqbiJJjPAEjYijKUchkV6av3bm5ocjRr6XZjPs04V9Bo+0fwiDSOjFFczNi/elKJRyGvraGSI1lr+1rPiX5iQqaHop5XGiCMfzQUHCoIpgFgwcUkGwYlNNEBZU/wTiMRIIKx1fRUf0nQP8n3TPTNsy7Ru71jouwiqDA3AIToANGqAFrkEbdAAGd+ARPIFn48F4MV6Nt7m1ZBQ9++AHjPcvfn+Yzg==</latexit>


<latexit sha1_base64="tRL2/a9HhTfH2DRYKzaLgVOnVDI=">AAACC3icdVDLSgMxFL3js9ZX1aWbYFG6kDIj2taVBTcuK9gHdIaSSTNtbOZBkhHK0E8Q3Oo3uHMnLtz4Ef6FG/em0wpV9ECSwz3n5ibHjTiTyjTfjbn5hcWl5cxKdnVtfWMzt7XdkGEsCK2TkIei5WJJOQtoXTHFaSsSFPsup013cD7WmzdUSBYGV2oYUcfHvYB5jGClSw17gKMId3J5s3hSMU8rx2hCSuUpKZeQVTRT5M8eC5+vAFDr5D7sbkhinwaKcCxl2zIj5SRYKEY4HWXtWNIIkwHu0bamAfapdJL0tSM7Pdqi5zrJeIZ1OLOP0L72d5EXCr0ChVLz7H0J9qUc+q52+lj15W9tXPxLa8fKqzgJC6JY0YBMBnkxRypE42BQlwlKFB9qgolg+ieI9LHAROn4sjqi7xzQ/6RxVLTMonVp5asHMEEGdmEPCmBBGapwATWoA4FruIN7eDBujSfj2XiZWOeMac8O/IDx9gWTMJuW</latexit><latexit sha1_base64="c0dO0Ms7V617D4SkJi9XoQIXotc=">AAACC3icdVDdSsMwGE3n35x/Uy+9CQ7FCymt6DbvBt54OcH9QFtGmqVbXJqWJBVG2SMI3upreCfe+hC+hY9g2lWYogeSHL5zvnzJ8WNGpbKsD6O0tLyyulZer2xsbm3vVHf3ujJKBCYdHLFI9H0kCaOcdBRVjPRjQVDoM9LzJ1eZ3rsnQtKI36ppTLwQjTgNKEZKl7ruBMUxGlRrlnnRtC6b53BO6o2CNOrQNq0cNVCgPah+usMIJyHhCjMkpWNbsfJSJBTFjMwqbiJJjPAEjYijKUchkV6av3bm5ocjRr6XZjPs04V9Bo+0fwiDSOjFFczNi/elKJRyGvraGSI1lr+1rPiX5iQqaHop5XGiCMfzQUHCoIpgFgwcUkGwYlNNEBZU/wTiMRIIKx1fRUf0nQP8n3TPTNsy7Ru71jouwiqDA3AIToANGqAFrkEbdAAGd+ARPIFn48F4MV6Nt7m1ZBQ9++AHjPcvfn+Yzg==</latexit>

vMF(µ,)
<latexit sha1_base64="b8e+DqOSDe0azlZrX59sVujM2Vc=">AAACKHicbZDJSgNBEIZr4hbjFvXoZTAIESTMeFBvBgTxIkQwC2SG0NPpJE16FrprgnGYx/HiU3j3IuJCrj6JneWgxoKGj/+v6q7+vUhwhZY1MjILi0vLK9nV3Nr6xuZWfnunpsJYUlaloQhlwyOKCR6wKnIUrBFJRnxPsLrXvxj79QGTiofBLQ4j5vqkG/AOpwS11MqfO8juMBlcX6ZFx4+PEmdyZ1N2PTexSifWuI7mIHX6JIpIetjKF6zSRLLMebBnUChn7z+eAKDSyr867ZDGPguQCqJU07YidBMikVPB0pwTKxYR2idd1tQYEJ8pN5kslZoHWmmbnVDqE6A5UX9OJMRXauh7utMn2FN/vbH4n9eMsXPmJjyIYmQBnT7UiYWJoTlOzWxzySiKoQZCJde7mrRHJKGos83pEOy/X56H2nHJtkr2jV0oF2FaWdiDfSiCDadQhiuoQBUoPMAzvMG78Wi8GJ/GaNqaMWYzu/CrjK9v+rWk5A==</latexit><latexit sha1_base64="hkYDJnN9V7PIt1+E40Kf8rPQ+Nc=">AAACKHicbZDLSsNAFIYnXmu9RV26CRahQimJC3VnQRA3QgV7gSSUyXTaDp1JwsxJsYQ8jhtfxY2IIt36JE7TLrT1wMDH/58zc+YPYs4U2PbEWFldW9/YLGwVt3d29/bNg8OmihJJaINEPJLtACvKWUgbwIDTdiwpFgGnrWB4M/VbIyoVi8JHGMfUF7gfsh4jGLTUMa89oE+Qju5vs7Inkkrq5Xe6sh/4qV29sKdVWYLMG+I4xtlZxyzZ1VyyrWVw5lBC86p3zHevG5FE0BAIx0q5jh2Dn2IJjHCaFb1E0RiTIe5TV2OIBVV+mi+VWada6Vq9SOoTgpWrvydSLJQai0B3CgwDtehNxf88N4HelZ+yME6AhmT2UC/hFkTWNDWryyQlwMcaMJFM72qRAZaYgM62qENwFr+8DM3zqmNXnQenVCvP4yigY3SCyshBl6iG7lAdNRBBz+gVfaBP48V4M76Myax1xZjPHKE/ZXz/ANFmoow=</latexit>

vMF(µ,)
<latexit sha1_base64="b8e+DqOSDe0azlZrX59sVujM2Vc=">AAACKHicbZDJSgNBEIZr4hbjFvXoZTAIESTMeFBvBgTxIkQwC2SG0NPpJE16FrprgnGYx/HiU3j3IuJCrj6JneWgxoKGj/+v6q7+vUhwhZY1MjILi0vLK9nV3Nr6xuZWfnunpsJYUlaloQhlwyOKCR6wKnIUrBFJRnxPsLrXvxj79QGTiofBLQ4j5vqkG/AOpwS11MqfO8juMBlcX6ZFx4+PEmdyZ1N2PTexSifWuI7mIHX6JIpIetjKF6zSRLLMebBnUChn7z+eAKDSyr867ZDGPguQCqJU07YidBMikVPB0pwTKxYR2idd1tQYEJ8pN5kslZoHWmmbnVDqE6A5UX9OJMRXauh7utMn2FN/vbH4n9eMsXPmJjyIYmQBnT7UiYWJoTlOzWxzySiKoQZCJde7mrRHJKGos83pEOy/X56H2nHJtkr2jV0oF2FaWdiDfSiCDadQhiuoQBUoPMAzvMG78Wi8GJ/GaNqaMWYzu/CrjK9v+rWk5A==</latexit><latexit sha1_base64="hkYDJnN9V7PIt1+E40Kf8rPQ+Nc=">AAACKHicbZDLSsNAFIYnXmu9RV26CRahQimJC3VnQRA3QgV7gSSUyXTaDp1JwsxJsYQ8jhtfxY2IIt36JE7TLrT1wMDH/58zc+YPYs4U2PbEWFldW9/YLGwVt3d29/bNg8OmihJJaINEPJLtACvKWUgbwIDTdiwpFgGnrWB4M/VbIyoVi8JHGMfUF7gfsh4jGLTUMa89oE+Qju5vs7Inkkrq5Xe6sh/4qV29sKdVWYLMG+I4xtlZxyzZ1VyyrWVw5lBC86p3zHevG5FE0BAIx0q5jh2Dn2IJjHCaFb1E0RiTIe5TV2OIBVV+mi+VWada6Vq9SOoTgpWrvydSLJQai0B3CgwDtehNxf88N4HelZ+yME6AhmT2UC/hFkTWNDWryyQlwMcaMJFM72qRAZaYgM62qENwFr+8DM3zqmNXnQenVCvP4yigY3SCyshBl6iG7lAdNRBBz+gVfaBP48V4M76Myax1xZjPHKE/ZXz/ANFmoow=</latexit>

z
<latexit sha1_base64="F7AAQ+Rti1v5W/XgDZXATGyS37c=">AAAB6HicbZC7SgNBFIbPxluMt6ilzWBQUoVdG+0M2FgmYC6QhDA7OZuMmZ1dZmaFuOQJbCwUsbX2IXwGOx/E3sml0MQfBj7+/xzmnOPHgmvjul9OZmV1bX0ju5nb2t7Z3cvvH9R1lCiGNRaJSDV9qlFwiTXDjcBmrJCGvsCGP7ya5I07VJpH8saMYuyEtC95wBk11qred/MFt+RORZbBm0Ph8r34/QEAlW7+s92LWBKiNExQrVueG5tOSpXhTOA41040xpQNaR9bFiUNUXfS6aBjcmKdHgkiZZ80ZOr+7khpqPUo9G1lSM1AL2YT87+slZjgopNyGScGJZt9FCSCmIhMtiY9rpAZMbJAmeJ2VsIGVFFm7G1y9gje4srLUD8reW7Jq3qF8inMlIUjOIYieHAOZbiGCtSAAcIDPMGzc+s8Oi/O66w048x7DuGPnLcfoSaPpw==</latexit><latexit sha1_base64="iYfk9Say81DmhzpOeAlLTL4UJlw=">AAAB6HicbVA9TwJBEJ3DL8Qv1NJmI9FYkTsbLElsLCGRjwQuZG+Zg5W9vcvungle+AU2Fhpj60+y89+4wBUKvmSSl/dmMjMvSATXxnW/ncLG5tb2TnG3tLd/cHhUPj5p6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWByO/c7j6g0j+W9mSboR3QkecgZNVZqPg3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1c9t+o1vUr9Mo+jCGdwDlfgQQ3qcAcNaAEDhGd4hTfnwXlx3p2PZWvByWdO4Q+czx/f5Yzf</latexit>

z
<latexit sha1_base64="F7AAQ+Rti1v5W/XgDZXATGyS37c=">AAAB6HicbZC7SgNBFIbPxluMt6ilzWBQUoVdG+0M2FgmYC6QhDA7OZuMmZ1dZmaFuOQJbCwUsbX2IXwGOx/E3sml0MQfBj7+/xzmnOPHgmvjul9OZmV1bX0ju5nb2t7Z3cvvH9R1lCiGNRaJSDV9qlFwiTXDjcBmrJCGvsCGP7ya5I07VJpH8saMYuyEtC95wBk11qred/MFt+RORZbBm0Ph8r34/QEAlW7+s92LWBKiNExQrVueG5tOSpXhTOA41040xpQNaR9bFiUNUXfS6aBjcmKdHgkiZZ80ZOr+7khpqPUo9G1lSM1AL2YT87+slZjgopNyGScGJZt9FCSCmIhMtiY9rpAZMbJAmeJ2VsIGVFFm7G1y9gje4srLUD8reW7Jq3qF8inMlIUjOIYieHAOZbiGCtSAAcIDPMGzc+s8Oi/O66w048x7DuGPnLcfoSaPpw==</latexit><latexit sha1_base64="iYfk9Say81DmhzpOeAlLTL4UJlw=">AAAB6HicbVA9TwJBEJ3DL8Qv1NJmI9FYkTsbLElsLCGRjwQuZG+Zg5W9vcvungle+AU2Fhpj60+y89+4wBUKvmSSl/dmMjMvSATXxnW/ncLG5tb2TnG3tLd/cHhUPj5p6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWByO/c7j6g0j+W9mSboR3QkecgZNVZqPg3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1c9t+o1vUr9Mo+jCGdwDlfgQQ3qcAcNaAEDhGd4hTfnwXlx3p2PZWvByWdO4Q+czx/f5Yzf</latexit>

 

vMF

Figure 1: The Neural Variational RNN (NVRNN) language model based on a Gaussian prior (left) and a vMF
prior (right). The encoder model first computes the parameters for the variational approximation qφ(z|x) (see
dotted box); we then sample z and generate the word sequence x given z. We show samples from N (0, I) and
vMF(·,κ = 100); the latter samples lie on the surface of the unit sphere. While κ can be predicted from the encoder
network, we find experimentally that fixing it leads to more stable optimization and better performance.

latent code, but the gains are limited and chang-
ing the decoder in this way requires ad hoc model
engineering and careful tuning of various decoder
capacity parameters. Our method is orthogonal to
the choice of the decoder and can be combined
with any of these approaches. Using vMF distribu-
tions in VAEs also leaves us the flexibility to mod-
ify the prior in other ways, such as using a product
distribution with a uniform (Guu et al., 2018) or
piecewise constant term (Serban et al., 2017a).

We evaluate our approach in two generative
modeling paradigms. For both RNN language
modeling and bag-of-words document modeling,
we find that vMF is more robust than a Gaussian
prior, and our model learns to rely more on the la-
tent variable while achieving better held-out data
likelihoods. To better understand the contrast be-
tween these models, we design and conduct a se-
ries of experiments to understand the properties of
the Gaussian and vMF latent code spaces, which
make different structural assumptions. Unsurpris-
ingly, these latent code distributions capture much
of the same information as in a bag of words, but
we show that vMF can more readily go beyond
this, capturing ordering information more effec-
tively than a Gaussian code.

2 Variational Autoencoders for Text

Bowman et al. (2016) propose a variational au-

toencoder model for generative text modeling in-
spired by Kingma and Welling (2013). Instead
of modeling p(x) directly as in vanilla language
models, VAEs introduce a continuous latent vari-
able z and take the form p(z)p(x|z). To train a
VAE, we optimize the marginal likelihood p(x) =∫
pθ(z)p(x|z)dz. The marginal log likelihood can

be written as:

log pθ(x) = KL(qφ(z|x)||pθ(z|x)) + L(θ, φ;x)

L(θ, φ;x) = −KL(qφ(z|x)||pθ(z))
+ Eqφ(z|x) log pθ(x|z) (1)

qφ(z|x), a variational approximation to the pos-
terior p(z|x), can be variously interpreted as a
recognition model or encoder, parameterized by
a neural network to encode the sentence x into a
dense code z. L(θ, φ;x) is often called the ev-
idence lower bound (ELBO). The first term of
ELBO is the KL divergence of the approximate
posterior from prior and the second term is an ex-
pected reconstruction error.

Since KL divergence is always non-negative,
we can use L(θ, φ;x) as a lower bound of
marginal likelihood log pθ(x). We optimize
L(θ, φ;x), jointly learning the recognition model
parameters φ and generative model parameters θ.

As the choice of prior p(z), most previous work
uses a centered multivariate Gaussian pθ(z) =



4505

N (z; 0, I). Since Gaussians are a location-scale
family of distributions, using them for both the
prior and posterior allows us to apply the reparam-
eterization trick and differentiate through the sam-
pling stage z ∼ Eqφ(z|x) when optimizing ELBO
in practice (Kingma and Welling, 2013).

2.1 Case Study: NVRNN

A Neural Variational RNN (NVRNN) for lan-
guage modeling is described in Bowman et al.
(2016) and depicted in Figure 1. The goal of the
NVRNN model is to extract a high level represen-
tation of a sentence into z and reconstruct the sen-
tence with a neural language model.

We denote a sequence of words as x =
{x1, x2, · · · , xn}. Unlike in vanilla language mod-
eling, an NVRNN conditions on the latent vari-
able z at each step of the generation pθ(x|z) =
pθ(x1|z)

∏n
i=1 p(xi|x1, . . . , xi−1, z). This proba-

bility distribution is modeled using a recurrent
model like an LSTM (Hochreiter and Schmidhu-
ber, 1997) as illustrated in Figure 1. There is
nothing unique about this choice; other recurrent
sequence models like a CNN or a Transformer
(Vaswani et al., 2017) could be used.

2.2 Posterior Collapse

When training a VAE, we update θ and φ si-
multaneously. Optimizing Eq. 1 gives two gradi-
ent terms: an update from the reconstruction loss
(likelihood of the correct labels) and an update
from the KL divergence. While the reconstruction
loss term encourages the z to convey useful in-
formation to this model, the KL term consistently
tries to regularize q(z|x) towards the prior on ev-
ery gradient update. This may trap the model in a
bad local optimum where qφ(z|x) = pθ(z) for all
x: in this case, z is simply a noise source, which is
useless to the model, so the model has learned to
ignore it and will not make large enough gradient
updates to break q(z|x) out of this optimum.

Bowman et al. (2016) termed this issue KL col-
lapse and proposed an annealing schedule to han-
dle it, where the weight of the KL term is in-
creased over the course of training.2 In this way,
the model initially learns to use the latent code but
is then regularized towards the prior as training
progresses. However, this trick is not sufficient
to avert KL collapse in all scenarios, particularly

2Reweighting the KL term is also used in methods like β-
VAE (Higgins et al., 2017) and InfoVAE (Zhao et al., 2017b).

No annealing Sigmoid annealing
3-layer 1-layer 3-layer 1-layer

KL 0.00 3.37 1.05 6.52
NLL 135 129 132 125

Table 1: Development set KL and NLL values for two
NVRNN models trained on the Penn Treebank with
and without the annealing technique of Bowman et al.
(2016). The higher-capacity 3-layer model collapses
when no annealing is used, and while annealing works
to improve performance, it still does not perform as
well as the 1-layer variant. By contrast, a variant of the
1-layer model with vMF gives an NLL value of 117 and
a KL of 18.6, a stronger result relying more heavily on
the latent variable.

when strong decoders are used and z has a minor
impact on pθ(x|z).

Table 1 shows experiments in a similar setup to
that of Bowman et al. (2016). We train an NVRNN
model on the Penn Treebank with four different
hyperparameter settings. We either use a 3-layer
LSTM encoder or a 1-layer LSTM and use or do
not use a sigmoid annealing schedule (increase the
KL weight from 0 to 1 over the first 20 epochs).
We observe the best performance using the 1-layer
model with annealing. One might conclude from
this table that the annealing trick has worked since
both models achieve better performance when an-
nealing is used. But in fact, a vMF-based model
can do better than either (NLL of 117), and more-
over, we have no way of knowing that a better an-
nealing scheme might not achieve even higher per-
formance after training. Furthermore, the higher-
capacity 3-layer model can theoretically do any-
thing the 1-layer model can, so its lower perfor-
mance indicates that our training is derailed either
by overfitting or getting stuck in a local optimum
where the latent variable is unused.3

Getting the best performance out of a VAE
is, therefore, a challenging problem that requires
careful tuning of the objective function and opti-
mization procedure (Bowman et al., 2016; Zhao
et al., 2017b; Higgins et al., 2017). Beyond the
well-documented problem of KL collapse, an op-
timizer may simply get stuck in a local optimum
during training and as a result, fail to find a model
that most effectively exploits the latent variable.

The solution we advocate for in this paper is

3In our experiments, we found significant variance in
collapse frequency due to other hyperparameters includ-
ing whether the encoder is a unidirectional or bidirectional
LSTM.



4506

to change the distribution for the latent space and
simplify the optimization problem. In the next sec-
tion, we describe the von Mises-Fisher distribution
and its use in VAE, where it forces the model to
put the latent representations on the surface of the
unit hypersphere rather than squeezing everything
to the origin. Critically, this distribution lets us
fix the value of the KL term by fixing the distri-
bution’s concentration parameter κ; this averts the
KL collapse and leads to good model performance
across two generative modeling paradigms.

3 von Mises-Fisher VAE

The von Mises-Fisher distribution is a distribution
on the (d − 1)-dimensional sphere in Rd. The
vMF distribution is defined by a direction vector
µ with ||µ||= 1 and a concentration parameter
κ ≥ 0. The PDF of the vMF distribution for the
d-dimensional unit vector x is defined as:

fd(x;µ, κ) = Cd(κ) exp(κµ
Tx) (2)

Cd(κ) =
κd/2−1

(2π)d/2Id/2−1(κ)
(3)

where Iv stands for the modified Bessel function
of the first kind at order v.

Figure 1 shows samples from vMF distributions
with various µ vectors (arrows), d = 3, and κ =
100. This is a high κ value, leading to samples that
are tightly clustered around µ, which is the mean
and mode of the distribution. When κ = 0, the
distribution degenerates to a uniform distribution
over the hypersphere independent of µ.

Past work has used vMF as an emission distri-
bution in unsupervised clustering models (Baner-
jee et al., 2005), VAE for other domains (Davidson
et al., 2018; Hasnat et al., 2017), and a generative
editing model for text (Guu et al., 2018). We focus
specifically on the empirical properties of vMF for
text modeling and conduct a systematic examina-
tion of how this prior affects VAE models com-
pared to using a Gaussian.

VAE with vMF We will use vMF as both our
prior and variational posterior in our VAE mod-
els. Otherwise, the setup for our VAE remains
the same as in the Gaussian case established in
Section 2. Our prior is the uniform distribution
vMF(·, κ = 0). Since true posterior pθ(z|x) is in-
tractable, we will approximate it with a variational
posterior qφ(z|x) = vMF(z;µ, κ) where the mean

(a) Gaussian                 (b) vMF

y
<latexit sha1_base64="5oEBU6/J0uEMhIwbKY5U1r5C6l0=">AAAB6HicbZC7SwNBEMbn4iuJr6ilzWIQrMKdjZYBG8sEzAOSEPb25pI1e3vH7p5wHKktbCwUsRP/JDv/GzePQhM/WPjxfTPszPiJ4Nq47rdT2Njc2t4plsq7e/sHh5Wj47aOU8WwxWIRq65PNQousWW4EdhNFNLIF9jxJzezvPOASvNY3pkswUFER5KHnFFjrWY2rFTdmjsXWQdvCdV66TH4AIDGsPLVD2KWRigNE1TrnucmZpBTZTgTOC33U40JZRM6wp5FSSPUg3w+6JScWycgYazsk4bM3d8dOY20ziLfVkbUjPVqNjP/y3qpCa8HOZdJalCyxUdhKoiJyWxrEnCFzIjMAmWK21kJG1NFmbG3KdsjeKsrr0P7sua5Na/pVesuLFSEUziDC/DgCupwCw1oAQOEJ3iBV+feeXbenPdFacFZ9pzAHzmfP1j9jrw=</latexit><latexit sha1_base64="3XZZcnMrWfXVmS75j/o68eB3ZSk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0m82GPBi8cW7Ae0oWy2k3btZhN2N0II/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O6Wt7Z3dvfJ+5eDw6PikenrW1XGqGHZYLGLVD6hGwSV2DDcC+4lCGgUCe8HsbuH3nlBpHssHkyXoR3QiecgZNVZqZ6Nqza27S5BN4hWkBgVao+rXcByzNEJpmKBaDzw3MX5OleFM4LwyTDUmlM3oBAeWShqh9vPloXNyZZUxCWNlSxqyVH9P5DTSOosC2xlRM9Xr3kL8zxukJmz4OZdJalCy1aIwFcTEZPE1GXOFzIjMEsoUt7cSNqWKMmOzqdgQvPWXN0n3pu65da/t1ZpuEUcZLuASrsGDW2jCPbSgAwwQnuEV3pxH58V5dz5WrSWnmDmHP3A+fwDhY4zo</latexit>

y
<latexit sha1_base64="5oEBU6/J0uEMhIwbKY5U1r5C6l0=">AAAB6HicbZC7SwNBEMbn4iuJr6ilzWIQrMKdjZYBG8sEzAOSEPb25pI1e3vH7p5wHKktbCwUsRP/JDv/GzePQhM/WPjxfTPszPiJ4Nq47rdT2Njc2t4plsq7e/sHh5Wj47aOU8WwxWIRq65PNQousWW4EdhNFNLIF9jxJzezvPOASvNY3pkswUFER5KHnFFjrWY2rFTdmjsXWQdvCdV66TH4AIDGsPLVD2KWRigNE1TrnucmZpBTZTgTOC33U40JZRM6wp5FSSPUg3w+6JScWycgYazsk4bM3d8dOY20ziLfVkbUjPVqNjP/y3qpCa8HOZdJalCyxUdhKoiJyWxrEnCFzIjMAmWK21kJG1NFmbG3KdsjeKsrr0P7sua5Na/pVesuLFSEUziDC/DgCupwCw1oAQOEJ3iBV+feeXbenPdFacFZ9pzAHzmfP1j9jrw=</latexit><latexit sha1_base64="3XZZcnMrWfXVmS75j/o68eB3ZSk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0m82GPBi8cW7Ae0oWy2k3btZhN2N0II/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O6Wt7Z3dvfJ+5eDw6PikenrW1XGqGHZYLGLVD6hGwSV2DDcC+4lCGgUCe8HsbuH3nlBpHssHkyXoR3QiecgZNVZqZ6Nqza27S5BN4hWkBgVao+rXcByzNEJpmKBaDzw3MX5OleFM4LwyTDUmlM3oBAeWShqh9vPloXNyZZUxCWNlSxqyVH9P5DTSOosC2xlRM9Xr3kL8zxukJmz4OZdJalCy1aIwFcTEZPE1GXOFzIjMEsoUt7cSNqWKMmOzqdgQvPWXN0n3pu65da/t1ZpuEUcZLuASrsGDW2jCPbSgAwwQnuEV3pxH58V5dz5WrSWnmDmHP3A+fwDhY4zo</latexit>

x
<latexit sha1_base64="ArQDmPuZhEWjzhLfwzv7Yl0y4Nk=">AAAB6HicbZC7SgNBFIbPxluMt6ilzWAQrMKujXYGbCwTMBdIljA7OZuMmZ1dZmbFuOQJbCwUsfUZfBI7S9/EyaXQxB8GPv7/HOacEySCa+O6X05uZXVtfSO/Wdja3tndK+4fNHScKoZ1FotYtQKqUXCJdcONwFaikEaBwGYwvJrkzTtUmsfyxowS9CPalzzkjBpr1e67xZJbdqciy+DNoXT58fBdAYBqt/jZ6cUsjVAaJqjWbc9NjJ9RZTgTOC50Uo0JZUPax7ZFSSPUfjYddExOrNMjYazsk4ZM3d8dGY20HkWBrYyoGejFbGL+l7VTE174GZdJalCy2UdhKoiJyWRr0uMKmREjC5QpbmclbEAVZcbepmCP4C2uvAyNs7Lnlr2aV6q4MFMejuAYTsGDc6jANVShDgwQHuEZXpxb58l5dd5mpTln3nMIf+S8/wBT2I91</latexit><latexit sha1_base64="LhVct7gVOonX0WkdvspbXq5uSSU=">AAAB6HicbVBNT8JAEJ3iF+IX6tHLRmLiibRe9EjixSMkFkigIdtlCivbbbO7NZKGX+DFg8Z49Sd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT5p6yRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zczv3OIyrNE3lvpikGMR1JHnFGjZVaT4Nqza27C5B14hWkBgWag+pXf5iwLEZpmKBa9zw3NUFOleFM4KzSzzSmlE3oCHuWShqjDvLFoTNyYZUhiRJlSxqyUH9P5DTWehqHtjOmZqxXvbn4n9fLTHQT5FymmUHJlouiTBCTkPnXZMgVMiOmllCmuL2VsDFVlBmbTcWG4K2+vE7aV3XPrXstr9ZwizjKcAbncAkeXEMD7qAJPjBAeIZXeHMenBfn3flYtpacYuYU/sD5/AHf34zn</latexit>

x
<latexit sha1_base64="ArQDmPuZhEWjzhLfwzv7Yl0y4Nk=">AAAB6HicbZC7SgNBFIbPxluMt6ilzWAQrMKujXYGbCwTMBdIljA7OZuMmZ1dZmbFuOQJbCwUsfUZfBI7S9/EyaXQxB8GPv7/HOacEySCa+O6X05uZXVtfSO/Wdja3tndK+4fNHScKoZ1FotYtQKqUXCJdcONwFaikEaBwGYwvJrkzTtUmsfyxowS9CPalzzkjBpr1e67xZJbdqciy+DNoXT58fBdAYBqt/jZ6cUsjVAaJqjWbc9NjJ9RZTgTOC50Uo0JZUPax7ZFSSPUfjYddExOrNMjYazsk4ZM3d8dGY20HkWBrYyoGejFbGL+l7VTE174GZdJalCy2UdhKoiJyWRr0uMKmREjC5QpbmclbEAVZcbepmCP4C2uvAyNs7Lnlr2aV6q4MFMejuAYTsGDc6jANVShDgwQHuEZXpxb58l5dd5mpTln3nMIf+S8/wBT2I91</latexit><latexit sha1_base64="LhVct7gVOonX0WkdvspbXq5uSSU=">AAAB6HicbVBNT8JAEJ3iF+IX6tHLRmLiibRe9EjixSMkFkigIdtlCivbbbO7NZKGX+DFg8Z49Sd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT5p6yRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zczv3OIyrNE3lvpikGMR1JHnFGjZVaT4Nqza27C5B14hWkBgWag+pXf5iwLEZpmKBa9zw3NUFOleFM4KzSzzSmlE3oCHuWShqjDvLFoTNyYZUhiRJlSxqyUH9P5DTWehqHtjOmZqxXvbn4n9fLTHQT5FymmUHJlouiTBCTkPnXZMgVMiOmllCmuL2VsDFVlBmbTcWG4K2+vE7aV3XPrXstr9ZwizjKcAbncAkeXEMD7qAJPjBAeIZXeHMenBfn3flYtpacYuYU/sD5/AHf34zn</latexit>

N (µ,�)
<latexit sha1_base64="Vb/PRRJ9hLRmz1nKZl/X7PrPees=">AAACAHicbVA9SwNBEJ2LXzF+nVpY2CwGIYKEOxstAzZWEsF8QO4Ie5u9ZMnu3bG7J8QjjX/FxkIRWyt/g4Xgv3EvSaGJDwYe780wMy9IOFPacb6twtLyyupacb20sbm1vWPv7jVVnEpCGyTmsWwHWFHOItrQTHPaTiTFIuC0FQwvc791R6VicXSrRwn1Be5HLGQEayN17QNPYD0gmGfX44on0lNPsb7AJ1277FSdCdAicWekXCvef30AQL1rf3q9mKSCRppwrFTHdRLtZ1hqRjgdl7xU0QSTIe7TjqERFlT52eSBMTo2Sg+FsTQVaTRRf09kWCg1EoHpzM9V814u/ud1Uh1e+BmLklTTiEwXhSlHOkZ5GqjHJCWajwzBRDJzKyIDLDHRJrOSCcGdf3mRNM+qrlN1b9xyrQJTFOEQjqACLpxDDa6gDg0gMIZHeIYX68F6sl6tt2lrwZrN7MMfWO8/cuCYcA==</latexit><latexit sha1_base64="mn7FVZ/eOdhmayzk+38W7f7q+kw=">AAACAHicbVBNS8NAEN34WetX1IMHL8EiVJCSeNFjwYsnqWA/oAllst20S3c3YXcjlJCLf8WLB0W8+jO8+W/ctDlo64OBx3szzMwLE0aVdt1va2V1bX1js7JV3d7Z3du3Dw47Kk4lJm0cs1j2QlCEUUHammpGeokkwENGuuHkpvC7j0QqGosHPU1IwGEkaEQxaCMN7GOfgx5jYNldXvd5euErOuJwPrBrbsOdwVkmXklqqERrYH/5wxinnAiNGSjV99xEBxlITTEjedVPFUkAT2BE+oYK4EQF2eyB3DkzytCJYmlKaGem/p7IgCs15aHpLM5Vi14h/uf1Ux1dBxkVSaqJwPNFUcocHTtFGs6QSoI1mxoCWFJzq4PHIAFrk1nVhOAtvrxMOpcNz214916tWS/jqKATdIrqyENXqIluUQu1EUY5ekav6M16sl6sd+tj3rpilTNH6A+szx9JkZYY</latexit>

N (µ,�)
<latexit sha1_base64="Vb/PRRJ9hLRmz1nKZl/X7PrPees=">AAACAHicbVA9SwNBEJ2LXzF+nVpY2CwGIYKEOxstAzZWEsF8QO4Ie5u9ZMnu3bG7J8QjjX/FxkIRWyt/g4Xgv3EvSaGJDwYe780wMy9IOFPacb6twtLyyupacb20sbm1vWPv7jVVnEpCGyTmsWwHWFHOItrQTHPaTiTFIuC0FQwvc791R6VicXSrRwn1Be5HLGQEayN17QNPYD0gmGfX44on0lNPsb7AJ1277FSdCdAicWekXCvef30AQL1rf3q9mKSCRppwrFTHdRLtZ1hqRjgdl7xU0QSTIe7TjqERFlT52eSBMTo2Sg+FsTQVaTRRf09kWCg1EoHpzM9V814u/ud1Uh1e+BmLklTTiEwXhSlHOkZ5GqjHJCWajwzBRDJzKyIDLDHRJrOSCcGdf3mRNM+qrlN1b9xyrQJTFOEQjqACLpxDDa6gDg0gMIZHeIYX68F6sl6tt2lrwZrN7MMfWO8/cuCYcA==</latexit><latexit sha1_base64="mn7FVZ/eOdhmayzk+38W7f7q+kw=">AAACAHicbVBNS8NAEN34WetX1IMHL8EiVJCSeNFjwYsnqWA/oAllst20S3c3YXcjlJCLf8WLB0W8+jO8+W/ctDlo64OBx3szzMwLE0aVdt1va2V1bX1js7JV3d7Z3du3Dw47Kk4lJm0cs1j2QlCEUUHammpGeokkwENGuuHkpvC7j0QqGosHPU1IwGEkaEQxaCMN7GOfgx5jYNldXvd5euErOuJwPrBrbsOdwVkmXklqqERrYH/5wxinnAiNGSjV99xEBxlITTEjedVPFUkAT2BE+oYK4EQF2eyB3DkzytCJYmlKaGem/p7IgCs15aHpLM5Vi14h/uf1Ux1dBxkVSaqJwPNFUcocHTtFGs6QSoI1mxoCWFJzq4PHIAFrk1nVhOAtvrxMOpcNz214916tWS/jqKATdIrqyENXqIluUQu1EUY5ekav6M16sl6sd+tj3rpilTNH6A+szx9JkZYY</latexit>

N (µ0,�0)
<latexit sha1_base64="V07EaOwL2hngjh+ao4Lg9H0ZA4I=">AAACcXiclZFNSyNBEIZrxo+N8SvrepFFaQyCooSZvazHwO7BkyhsTCCJoadTSRp7eobunoXskP/mb9jDwv6JvexdrUwUNHqxYOi3nnqLnq6KUiWtC4K/nr+wuLT8obRSXl1b39isfNy6sklmBDZEohLTirhFJTU2nHQKW6lBHkcKm9HNt2m9+RONlYn+4cYpdmM+1HIgBXeEksp36EAMHByMQNCpIIdzmMBhwTO4prwDCBosZYbUk3vKUyKSCFLHBE6IWMqHhec9nUd3971KNagFRbDXInwU1Xrp159bALjoVX53+onIYtROKG5tOwxS1825cVIonJQ7mcWUixs+xDZJzWO03byY2IQdEOmzQWLo044V9HlHzmNrx3FEzpi7kZ2vTeFbtXbmBqfdXOo0c6jF7KJBpphL2HT8rC8NCqfGJLgwkv6ViRE3XDhaUpmGEM4/+bW4+lILg1p4GVbrhzCLEnyGfVpaCF+hDmdwAQ1a5z9v29v19rz//o7P/P2Z1fceez7Bi/CPHwACs6po</latexit><latexit sha1_base64="IUiEzLIxATEk6ubzOe1mFd1bcIM=">AAACcXiclZFLS8NAEMcn8V1f9XERURaLoCgl8aJHQQ+eRMGq0FbZbKft0s0m7G6EEvolvfklvHhXp7GCr4sDYf/zm/+w2ZkoVdK6IHjy/LHxicmp6ZnS7Nz8wmJ5afnaJpkRWBOJSsxtxC0qqbHmpFN4mxrkcaTwJuqdDOs3D2isTPSV66fYjHlHy7YU3BFKyqfQgBg4OOiCoFNBDucwgJ2CZ3BHeQMQNFjKDKlP95CnRCQRpI4B7BOxlHcKz386d1/f7suVoBoUwX6LcCQqMIqL+/Jjo5WILEbthOLW1sMgdc2cGyeFwkGpkVlMuejxDtZJah6jbebFxAZsm0iLtRNDn3asoF87ch5b248jcsbcde3P2hD+Vatnrn3UzKVOM4dafFzUzhRzCRuOn7WkQeFUnwQXRtK/MtHlhgtHSyrREMKfT/4trg+qYVANL8PK8c5oHNOwDlu0tBAO4RjO4AJqtM5nb9Xb8Da9F3/NZ/7Wh9X3Rj0r8C38vXfZVagQ</latexit>

N (µ0,�0)
<latexit sha1_base64="V07EaOwL2hngjh+ao4Lg9H0ZA4I=">AAACcXiclZFNSyNBEIZrxo+N8SvrepFFaQyCooSZvazHwO7BkyhsTCCJoadTSRp7eobunoXskP/mb9jDwv6JvexdrUwUNHqxYOi3nnqLnq6KUiWtC4K/nr+wuLT8obRSXl1b39isfNy6sklmBDZEohLTirhFJTU2nHQKW6lBHkcKm9HNt2m9+RONlYn+4cYpdmM+1HIgBXeEksp36EAMHByMQNCpIIdzmMBhwTO4prwDCBosZYbUk3vKUyKSCFLHBE6IWMqHhec9nUd3971KNagFRbDXInwU1Xrp159bALjoVX53+onIYtROKG5tOwxS1825cVIonJQ7mcWUixs+xDZJzWO03byY2IQdEOmzQWLo044V9HlHzmNrx3FEzpi7kZ2vTeFbtXbmBqfdXOo0c6jF7KJBpphL2HT8rC8NCqfGJLgwkv6ViRE3XDhaUpmGEM4/+bW4+lILg1p4GVbrhzCLEnyGfVpaCF+hDmdwAQ1a5z9v29v19rz//o7P/P2Z1fceez7Bi/CPHwACs6po</latexit><latexit sha1_base64="IUiEzLIxATEk6ubzOe1mFd1bcIM=">AAACcXiclZFLS8NAEMcn8V1f9XERURaLoCgl8aJHQQ+eRMGq0FbZbKft0s0m7G6EEvolvfklvHhXp7GCr4sDYf/zm/+w2ZkoVdK6IHjy/LHxicmp6ZnS7Nz8wmJ5afnaJpkRWBOJSsxtxC0qqbHmpFN4mxrkcaTwJuqdDOs3D2isTPSV66fYjHlHy7YU3BFKyqfQgBg4OOiCoFNBDucwgJ2CZ3BHeQMQNFjKDKlP95CnRCQRpI4B7BOxlHcKz386d1/f7suVoBoUwX6LcCQqMIqL+/Jjo5WILEbthOLW1sMgdc2cGyeFwkGpkVlMuejxDtZJah6jbebFxAZsm0iLtRNDn3asoF87ch5b248jcsbcde3P2hD+Vatnrn3UzKVOM4dafFzUzhRzCRuOn7WkQeFUnwQXRtK/MtHlhgtHSyrREMKfT/4trg+qYVANL8PK8c5oHNOwDlu0tBAO4RjO4AJqtM5nb9Xb8Da9F3/NZ/7Wh9X3Rj0r8C38vXfZVagQ</latexit>

x
<latexit sha1_base64="ArQDmPuZhEWjzhLfwzv7Yl0y4Nk=">AAAB6HicbZC7SgNBFIbPxluMt6ilzWAQrMKujXYGbCwTMBdIljA7OZuMmZ1dZmbFuOQJbCwUsfUZfBI7S9/EyaXQxB8GPv7/HOacEySCa+O6X05uZXVtfSO/Wdja3tndK+4fNHScKoZ1FotYtQKqUXCJdcONwFaikEaBwGYwvJrkzTtUmsfyxowS9CPalzzkjBpr1e67xZJbdqciy+DNoXT58fBdAYBqt/jZ6cUsjVAaJqjWbc9NjJ9RZTgTOC50Uo0JZUPax7ZFSSPUfjYddExOrNMjYazsk4ZM3d8dGY20HkWBrYyoGejFbGL+l7VTE174GZdJalCy2UdhKoiJyWRr0uMKmREjC5QpbmclbEAVZcbepmCP4C2uvAyNs7Lnlr2aV6q4MFMejuAYTsGDc6jANVShDgwQHuEZXpxb58l5dd5mpTln3nMIf+S8/wBT2I91</latexit><latexit sha1_base64="LhVct7gVOonX0WkdvspbXq5uSSU=">AAAB6HicbVBNT8JAEJ3iF+IX6tHLRmLiibRe9EjixSMkFkigIdtlCivbbbO7NZKGX+DFg8Z49Sd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT5p6yRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zczv3OIyrNE3lvpikGMR1JHnFGjZVaT4Nqza27C5B14hWkBgWag+pXf5iwLEZpmKBa9zw3NUFOleFM4KzSzzSmlE3oCHuWShqjDvLFoTNyYZUhiRJlSxqyUH9P5DTWehqHtjOmZqxXvbn4n9fLTHQT5FymmUHJlouiTBCTkPnXZMgVMiOmllCmuL2VsDFVlBmbTcWG4K2+vE7aV3XPrXstr9ZwizjKcAbncAkeXEMD7qAJPjBAeIZXeHMenBfn3flYtpacYuYU/sD5/AHf34zn</latexit>

x
<latexit sha1_base64="ArQDmPuZhEWjzhLfwzv7Yl0y4Nk=">AAAB6HicbZC7SgNBFIbPxluMt6ilzWAQrMKujXYGbCwTMBdIljA7OZuMmZ1dZmbFuOQJbCwUsfUZfBI7S9/EyaXQxB8GPv7/HOacEySCa+O6X05uZXVtfSO/Wdja3tndK+4fNHScKoZ1FotYtQKqUXCJdcONwFaikEaBwGYwvJrkzTtUmsfyxowS9CPalzzkjBpr1e67xZJbdqciy+DNoXT58fBdAYBqt/jZ6cUsjVAaJqjWbc9NjJ9RZTgTOC50Uo0JZUPax7ZFSSPUfjYddExOrNMjYazsk4ZM3d8dGY20HkWBrYyoGejFbGL+l7VTE174GZdJalCy2UdhKoiJyWRr0uMKmREjC5QpbmclbEAVZcbepmCP4C2uvAyNs7Lnlr2aV6q4MFMejuAYTsGDc6jANVShDgwQHuEZXpxb58l5dd5mpTln3nMIf+S8/wBT2I91</latexit><latexit sha1_base64="LhVct7gVOonX0WkdvspbXq5uSSU=">AAAB6HicbVBNT8JAEJ3iF+IX6tHLRmLiibRe9EjixSMkFkigIdtlCivbbbO7NZKGX+DFg8Z49Sd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT5p6yRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zczv3OIyrNE3lvpikGMR1JHnFGjZVaT4Nqza27C5B14hWkBgWag+pXf5iwLEZpmKBa9zw3NUFOleFM4KzSzzSmlE3oCHuWShqjDvLFoTNyYZUhiRJlSxqyUH9P5DTWehqHtjOmZqxXvbn4n9fLTHQT5FymmUHJlouiTBCTkPnXZMgVMiOmllCmuL2VsDFVlBmbTcWG4K2+vE7aV3XPrXstr9ZwizjKcAbncAkeXEMD7qAJPjBAeIZXeHMenBfn3flYtpacYuYU/sD5/AHf34zn</latexit>

y
<latexit sha1_base64="5oEBU6/J0uEMhIwbKY5U1r5C6l0=">AAAB6HicbZC7SwNBEMbn4iuJr6ilzWIQrMKdjZYBG8sEzAOSEPb25pI1e3vH7p5wHKktbCwUsRP/JDv/GzePQhM/WPjxfTPszPiJ4Nq47rdT2Njc2t4plsq7e/sHh5Wj47aOU8WwxWIRq65PNQousWW4EdhNFNLIF9jxJzezvPOASvNY3pkswUFER5KHnFFjrWY2rFTdmjsXWQdvCdV66TH4AIDGsPLVD2KWRigNE1TrnucmZpBTZTgTOC33U40JZRM6wp5FSSPUg3w+6JScWycgYazsk4bM3d8dOY20ziLfVkbUjPVqNjP/y3qpCa8HOZdJalCyxUdhKoiJyWxrEnCFzIjMAmWK21kJG1NFmbG3KdsjeKsrr0P7sua5Na/pVesuLFSEUziDC/DgCupwCw1oAQOEJ3iBV+feeXbenPdFacFZ9pzAHzmfP1j9jrw=</latexit><latexit sha1_base64="3XZZcnMrWfXVmS75j/o68eB3ZSk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0m82GPBi8cW7Ae0oWy2k3btZhN2N0II/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O6Wt7Z3dvfJ+5eDw6PikenrW1XGqGHZYLGLVD6hGwSV2DDcC+4lCGgUCe8HsbuH3nlBpHssHkyXoR3QiecgZNVZqZ6Nqza27S5BN4hWkBgVao+rXcByzNEJpmKBaDzw3MX5OleFM4LwyTDUmlM3oBAeWShqh9vPloXNyZZUxCWNlSxqyVH9P5DTSOosC2xlRM9Xr3kL8zxukJmz4OZdJalCy1aIwFcTEZPE1GXOFzIjMEsoUt7cSNqWKMmOzqdgQvPWXN0n3pu65da/t1ZpuEUcZLuASrsGDW2jCPbSgAwwQnuEV3pxH58V5dz5WrSWnmDmHP3A+fwDhY4zo</latexit>

y
<latexit sha1_base64="5oEBU6/J0uEMhIwbKY5U1r5C6l0=">AAAB6HicbZC7SwNBEMbn4iuJr6ilzWIQrMKdjZYBG8sEzAOSEPb25pI1e3vH7p5wHKktbCwUsRP/JDv/GzePQhM/WPjxfTPszPiJ4Nq47rdT2Njc2t4plsq7e/sHh5Wj47aOU8WwxWIRq65PNQousWW4EdhNFNLIF9jxJzezvPOASvNY3pkswUFER5KHnFFjrWY2rFTdmjsXWQdvCdV66TH4AIDGsPLVD2KWRigNE1TrnucmZpBTZTgTOC33U40JZRM6wp5FSSPUg3w+6JScWycgYazsk4bM3d8dOY20ziLfVkbUjPVqNjP/y3qpCa8HOZdJalCyxUdhKoiJyWxrEnCFzIjMAmWK21kJG1NFmbG3KdsjeKsrr0P7sua5Na/pVesuLFSEUziDC/DgCupwCw1oAQOEJ3iBV+feeXbenPdFacFZ9pzAHzmfP1j9jrw=</latexit><latexit sha1_base64="3XZZcnMrWfXVmS75j/o68eB3ZSk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0m82GPBi8cW7Ae0oWy2k3btZhN2N0II/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O6Wt7Z3dvfJ+5eDw6PikenrW1XGqGHZYLGLVD6hGwSV2DDcC+4lCGgUCe8HsbuH3nlBpHssHkyXoR3QiecgZNVZqZ6Nqza27S5BN4hWkBgVao+rXcByzNEJpmKBaDzw3MX5OleFM4LwyTDUmlM3oBAeWShqh9vPloXNyZZUxCWNlSxqyVH9P5DTSOosC2xlRM9Xr3kL8zxukJmz4OZdJalCy1aIwFcTEZPE1GXOFzIjMEsoUt7cSNqWKMmOzqdgQvPWXN0n3pu65da/t1ZpuEUcZLuASrsGDW2jCPbSgAwwQnuEV3pxH58V5dz5WrSWnmDmHP3A+fwDhY4zo</latexit>

µ
<latexit sha1_base64="Ow/7BpI0blvX54diWwtluAnM+UY=">AAAB6nicbZC7SwNBEMbnfCbxFbW0WQyCVbiz0TJgYxnRPCAJYW9vL1myu3fszgnhSG1lY6GIjYV/kZ3/jZtHoYkfLPz4vhl2ZsJUCou+/+2trW9sbm0XiqWd3b39g/LhUdMmmWG8wRKZmHZILZdC8wYKlLydGk5VKHkrHF1P89YDN1Yk+h7HKe8pOtAiFoyis+66KuuXK37Vn4msQrCASq34GH0AQL1f/upGCcsU18gktbYT+Cn2cmpQMMknpW5meUrZiA54x6GmittePht1Qs6cE5E4Me5pJDP3d0dOlbVjFbpKRXFol7Op+V/WyTC+6uVCpxlyzeYfxZkkmJDp3iQShjOUYweUGeFmJWxIDWXorlNyRwiWV16F5kU18KvBbVCp+TBXAU7gFM4hgEuowQ3UoQEMBvAEL/DqSe/Ze/Pe56Vr3qLnGP7I+/wBzo+PlQ==</latexit><latexit sha1_base64="M7MPKdaZCbLFnKrqB8vswjpiNlk=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stAzYWEY0H5AcYW8zlyzZ3Tt294Rw5CfYWChi6y+y89+4Sa7QxAcDj/dmmJkXpYIb6/vfXmljc2t7p7xb2ds/ODyqHp+0TZJphi2WiER3I2pQcIUty63AbqqRykhgJ5rczv3OE2rDE/VopymGko4Ujzmj1kkPfZkNqjW/7i9A1klQkBoUaA6qX/1hwjKJyjJBjekFfmrDnGrLmcBZpZ8ZTCmb0BH2HFVUognzxakzcuGUIYkT7UpZslB/T+RUGjOVkeuU1I7NqjcX//N6mY1vwpyrNLOo2HJRnAliEzL/mwy5RmbF1BHKNHe3EjammjLr0qm4EILVl9dJ+6oe+PXgPqg1/CKOMpzBOVxCANfQgDtoQgsYjOAZXuHNE96L9+59LFtLXjFzCn/gff4AVwSNwQ==</latexit>

µ
<latexit sha1_base64="Ow/7BpI0blvX54diWwtluAnM+UY=">AAAB6nicbZC7SwNBEMbnfCbxFbW0WQyCVbiz0TJgYxnRPCAJYW9vL1myu3fszgnhSG1lY6GIjYV/kZ3/jZtHoYkfLPz4vhl2ZsJUCou+/+2trW9sbm0XiqWd3b39g/LhUdMmmWG8wRKZmHZILZdC8wYKlLydGk5VKHkrHF1P89YDN1Yk+h7HKe8pOtAiFoyis+66KuuXK37Vn4msQrCASq34GH0AQL1f/upGCcsU18gktbYT+Cn2cmpQMMknpW5meUrZiA54x6GmittePht1Qs6cE5E4Me5pJDP3d0dOlbVjFbpKRXFol7Op+V/WyTC+6uVCpxlyzeYfxZkkmJDp3iQShjOUYweUGeFmJWxIDWXorlNyRwiWV16F5kU18KvBbVCp+TBXAU7gFM4hgEuowQ3UoQEMBvAEL/DqSe/Ze/Pe56Vr3qLnGP7I+/wBzo+PlQ==</latexit><latexit sha1_base64="M7MPKdaZCbLFnKrqB8vswjpiNlk=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stAzYWEY0H5AcYW8zlyzZ3Tt294Rw5CfYWChi6y+y89+4Sa7QxAcDj/dmmJkXpYIb6/vfXmljc2t7p7xb2ds/ODyqHp+0TZJphi2WiER3I2pQcIUty63AbqqRykhgJ5rczv3OE2rDE/VopymGko4Ujzmj1kkPfZkNqjW/7i9A1klQkBoUaA6qX/1hwjKJyjJBjekFfmrDnGrLmcBZpZ8ZTCmb0BH2HFVUognzxakzcuGUIYkT7UpZslB/T+RUGjOVkeuU1I7NqjcX//N6mY1vwpyrNLOo2HJRnAliEzL/mwy5RmbF1BHKNHe3EjammjLr0qm4EILVl9dJ+6oe+PXgPqg1/CKOMpzBOVxCANfQgDtoQgsYjOAZXuHNE96L9+59LFtLXjFzCn/gff4AVwSNwQ==</latexit>

µ0
<latexit sha1_base64="KmVvxc+m8hEXOeZD/iuYF2PG400=">AAACAnicbVA9SwNBEJ2LX0n8OrUSm8MgWIU7Gy0DNpYRzAfkYtjbTJIlu3vH7p4QjmAj+DdsbCwUsfVX2Plv3HwUmvhg4PHeDDPzooQzbXz/28mtrK6tb+QLxc2t7Z1dd2+/ruNUUazRmMeqGRGNnEmsGWY4NhOFREQcG9HwcuI37lBpFssbM0qwLUhfsh6jxFip4x6GIr3NQpQ6VSiIGWRhopjA8bjjlvyyP4W3TII5KVUKj90nAKh23K+wG9NUoDSUE61bgZ+YdkaUYZTjuBimGhNCh6SPLUslEajb2fSFsXdila7Xi5Utabyp+nsiI0LrkYhs5+RKvehNxP+8Vmp6F+2MySQ1KOlsUS/lnom9SR5elymkho8sIVQxe6tHB0QRamxqRRtCsPjyMqmflQO/HFwHpYoPM+ThCI7hFAI4hwpcQRVqQOEenuEV3pwH58V5dz5mrTlnPnMAf+B8/gAmepoB</latexit><latexit sha1_base64="u4g1fRgAJxmx7e9K22B6Dd0aZ9E=">AAACAnicbVDLSsNAFJ34rPUVdSVugkVwVRI3uiy4cVnBPqCpZTK9aYfOTMI8hBKCG3/FjQtF3PoV7vwbJ20W2nrgwuGce7n3nihlVGnf/3ZWVtfWNzYrW9Xtnd29fffgsK0SIwm0SMIS2Y2wAkYFtDTVDLqpBMwjBp1ocl34nQeQiibiTk9T6HM8EjSmBGsrDdzjkJv7LAShjASO9TgLU0k55PnArfl1fwZvmQQlqaESzYH7FQ4TYjgITRhWqhf4qe5nWGpKGOTV0ChIMZngEfQsFZiD6mezF3LvzCpDL06kLaG9mfp7IsNcqSmPbGdxpVr0CvE/r2d0fNXPqEiNBkHmi2LDPJ14RR7ekEogmk0twURSe6tHxlhiom1qVRtCsPjyMmlf1AO/HtwGtYZfxlFBJ+gUnaMAXaIGukFN1EIEPaJn9IrenCfnxXl3PuatK045c4T+wPn8Aa7gmC0=</latexit>

µ0
<latexit sha1_base64="KmVvxc+m8hEXOeZD/iuYF2PG400=">AAACAnicbVA9SwNBEJ2LX0n8OrUSm8MgWIU7Gy0DNpYRzAfkYtjbTJIlu3vH7p4QjmAj+DdsbCwUsfVX2Plv3HwUmvhg4PHeDDPzooQzbXz/28mtrK6tb+QLxc2t7Z1dd2+/ruNUUazRmMeqGRGNnEmsGWY4NhOFREQcG9HwcuI37lBpFssbM0qwLUhfsh6jxFip4x6GIr3NQpQ6VSiIGWRhopjA8bjjlvyyP4W3TII5KVUKj90nAKh23K+wG9NUoDSUE61bgZ+YdkaUYZTjuBimGhNCh6SPLUslEajb2fSFsXdila7Xi5Utabyp+nsiI0LrkYhs5+RKvehNxP+8Vmp6F+2MySQ1KOlsUS/lnom9SR5elymkho8sIVQxe6tHB0QRamxqRRtCsPjyMqmflQO/HFwHpYoPM+ThCI7hFAI4hwpcQRVqQOEenuEV3pwH58V5dz5mrTlnPnMAf+B8/gAmepoB</latexit><latexit sha1_base64="u4g1fRgAJxmx7e9K22B6Dd0aZ9E=">AAACAnicbVDLSsNAFJ34rPUVdSVugkVwVRI3uiy4cVnBPqCpZTK9aYfOTMI8hBKCG3/FjQtF3PoV7vwbJ20W2nrgwuGce7n3nihlVGnf/3ZWVtfWNzYrW9Xtnd29fffgsK0SIwm0SMIS2Y2wAkYFtDTVDLqpBMwjBp1ocl34nQeQiibiTk9T6HM8EjSmBGsrDdzjkJv7LAShjASO9TgLU0k55PnArfl1fwZvmQQlqaESzYH7FQ4TYjgITRhWqhf4qe5nWGpKGOTV0ChIMZngEfQsFZiD6mezF3LvzCpDL06kLaG9mfp7IsNcqSmPbGdxpVr0CvE/r2d0fNXPqEiNBkHmi2LDPJ14RR7ekEogmk0twURSe6tHxlhiom1qVRtCsPjyMmlf1AO/HtwGtYZfxlFBJ+gUnaMAXaIGukFN1EIEPaJn9IrenCfnxXl3PuatK045c4T+wPn8Aa7gmC0=</latexit>

Figure 2: Visualization of optimization of how q varies
over time for a single example during learning. In the
Gaussian case, the KL term tends to pull the model to-
wards the prior (moving from µ, σ to µ′, σ′), whereas
in the vMF case there is no such pressure towards a
single distribution.

direction µ is the output of encoding neural net-
works (Figure 1, right side) and κ is treated as a
constant.

Before we can implement a VAE, we need to
derive an expression for KL divergence in order to
optimize ELBO (Equation 1) and give a sampling
algorithm that admits the reparameterization trick
(Kingma and Welling, 2013).

KL divergence With vMF(·, 0) as our prior, the
KL divergence is:4

KL(vMF(µ, κ)||vMF(·, 0)) = κ
Id/2(κ)

Id/2−1(κ)

+

(
d

2
− 1
)

log κ− d
2

log(2π)− log Id/2−1(κ)

+
d

2
log π + log 2− log Γ

(
d

2

)
Critically, this only depends on κ, not on µ. κ will
be treated as a fixed hyperparameter, so this term
will be constant for our model; KL collapse will
therefore be rendered impossible.

Figure 2 shows a visualization of the learning
trajectories of Gaussian and vMF VAE. For the
Gaussian VAE, the KL divergence in the objec-
tive function tends to pull the posterior towards the
prior centered at the origin and, therefore, make
the optimization difficult as mentioned before. For
the vMF VAE, given fixed κ, there is no such vac-
uous state and µ can vary freely.

4Our KL divergence agrees with that of Davidson et al.
(2018) (see their appendix for a derivation), and we have ver-
ified it empirically. The equation in Guu et al. (2018) gives
slightly different KL values, though differences are small
(<5%) for most κ and dimension values we encounter.



4507

0 100 200
0.00

0.25

0.50

0.75

1.00
C

os
Cos, d=25
Cos, d=100

KL, d=25
KL, d=100

0

20

40

60

K
L

Figure 3: Visualization of the interaction between
κ, KL, and dimensionality in vMF. Cos represents
the cosine similarity between µ and samples from
vMFd(µ, κ) which reflects how disperse the distribu-
tion is. KL is defined as KL with a uniform vMF prior,
KL(vMFd(µ, κ)||vMF(·, 0)). Higher κ values yield
higher cosine similarities, but also higher KL costs.

Figure 3 shows the KL value and concentra-
tion of vMF(µ, κ) for two different dimensional-
ities. KL increases monotonically with κ, as does
concentration measured by cosine similarity. To
get a fixed cosine dispersion as dimensionality in-
creases, higher κ values are needed, resulting in
higher KL values.

Sampling from vMF Following the implemen-
tation of Guu et al. (2018), we use the rejection
sampling scheme of Wood (1994) to sample a
“change magnitude” w. Our sample is then given
by z = wµ + v

√
1− w2, where v is a randomly

sampled unit vector tangent to the hypersphere at
µ. Neither v nor w depends on µ, so we can now
take gradients of z with respect to µ as required.

4 Experiments on Language Modeling

We first evaluate our vMF approach in the
NVRNN setting. We will return to this model and
analyze its properties further in Sections 6 and 7
after showing experiments on document modeling.

Dataset For NVRNN, we use the Penn Treebank
(Marcus et al., 1993), also used in Bowman et al.
(2016), and Yelp 2013 (Xu et al., 2016). Examples
in the Yelp dataset are much longer and more di-
verse than those from PTB, requiring more under-
standing of high-level semantics to generate a co-
herent sequence. Yelp has a long tail of very long
reviews, so we truncate the examples to a maxi-
mum length of 50 words; this still gives an aver-
age length over twice as long as in the PTB setting.

Name Train Dev Test Len Vocab

PTB 42068 3370 3761 21.1 10K
Yelp 62522 7773 8671 49.5 15K

20NG 11268 - 7505 96.1 2K
RC 794414 - 10000 116.8 10K

Table 2: Statistics of the datasets used in our experi-
ments. Len stands for the average length of an example.
Vocab is the vocabulary size; these follow prior work.

Statistics about all datasets used in this paper are
shown in Table 2.

Settings We evaluate our NVRNN as in Bow-
man et al. (2016) and explore two different set-
tings. In the Standard setting, the input to the
RNN at each time step is the concatenation of the
latent code z and the ground truth word from the
last time step, while the Inputless setting does not
use the prior word. The more powerful decoder
of the Standard setting makes the latent represen-
tations inherently less useful. In the Inputless set-
ting, the decoder needs to predict the whole se-
quence with only the help of given latent code. In
this case, a high-quality representation of the sen-
tence is badly needed and the model is driven to
learn it.

Our implementation of VAE uses a one layer
unidirectional LSTM as both encoder and decoder.
We use an embedding size of 100 and hidden units
of size 400 in the LSTM. The dimension of the la-
tent code is chosen from {25, 50, 100} by tuning
on the development set. We use SGD to optimize
all models with decayed learning rate and gradient
clipping. For Yelp, the sentiment bit, which ranges
from 1 to 5, is also embedded into a 50 dimension
vector and input for every time step of the decod-
ing phase.

Results Experimental results of the NVRNN are
shown in Table 3. We report negative log likeli-
hood (NLL)5 and perplexity (PPL) on the test set.
We follow the implementation reported in Bow-
man et al. (2016) where the KL term weight is
annealed for the Gaussian VAE; vMF VAE works
well without weight annealing. The vMF distri-
bution gives a performance boost in all datasets
in both the Standard and Inputless settings. Even
in the Standard setting, our model is able to suc-
cessfully use nonzero KL values to achieve better

5Reported values are actually a lower bound on the true
NLL, computed from ELBO by sampling z.



4508

Model
PTB Yelp

Standard Inputless Standard Inputless
NLL PPL NLL PPL NLL PPL NLL PPL

RNNLM (2016) 100 ( – ) 116 135 ( – ) >600 – – – –
G-VAE (2016) 101 (2) 119 125 (15) 380 – – – –

RNNLM (Ours) 100 ( – ) 114 134 ( – ) 596 199 ( – ) 55 300 ( – ) 432
G-VAE (Ours) 99 (4.4) 109 125 (6.3) 379 199 (0.5) 55 274 (13.4) 256

vMF-VAE (Ours) 96 (5.7) 98 117 (18.6) 262 198 (6.4) 54 242 (48.5) 134

Table 3: Experimental results of NVRNN on the test sets of PTB and Yelp. The upper RNNLM and G-VAE shows
the result from Bowman et al. (2016). KL divergence is shown in the parenthesis, along with total NLL. Best
results are in bold. vMF consistently uses higher KL term weights but achieves comparable or better NLL and
perplexity values across all four settings.

G
-0

.2
G

-0
.5

G
-1

v-
20

v-
40

v-
60

v-
80

v-
10

0
v-

12
0

v-
14

0

2.5

3.0

3.5

4.0

4.5

5.0

5.5

6.0

6.5

Lo
ss

537
428 379 423

328
277 262

297
361

414
KLD

Figure 4: Comparison of Gaussian- and vMF-NVRNN
with different hyper-parameters. All models are trained
on PTB in the Inputless setting where the latent dimen-
sion is 50. G-α indicates Gaussian VAE with KL an-
nealed by the given constant α, and V-κ indicates VAE
with κ set to the given value. The green bar reflects the
amount of KL loss while the total height reflects the
whole objective. Numbers above bars are perplexity.
vMF is more highly tunable and also achieves stronger
results across a wide range of κ values.

perplexities, and even when KL collapse does not
appear to be the case (e.g., G-VAE on the PTB-
Standard setting), a Gaussian family of distribu-
tions results in lower KLs and worse log likeli-
hoods, possibly due to optimization challenges.
In the Inputless setting, we see large gains: vMF
VAE reduces PPL from 379 to 262 in PTB, and
from 256 to 134 in Yelp compared to Gaussian
VAE.

Trade-off Comparison Besides the overall per-
plexity, we are also interested in the trade-off be-

tween reconstruction loss and KL, and the con-
tribution of KL to the whole objective. Figure 4
shows the ability of our model to explicitly control
the balance between the KL and the reconstruction
term. First, we “permanently” anneal the Gaussian
VAE by setting the weight of the KL term to a con-
stant smaller than 1 (0.2 and 0.5 in our case). We
find that this trick does mitigate the KL collapse,
but the overall performance is worse. Therefore,
this is not only a numerical game about the KL
vs. NLL trade-off but a deeper challenge of how
to structure models to learn effective latent repre-
sentations.

For vMF VAE, when we gradually increase the
value of κ, the concentration of the distribution
around the mean direction µ is higher and samples
from vMF are closer to µ. The model achieves
the best perplexity when κ = 80. The reconstruc-
tion error is bounded around 4.5 due to the dif-
ficulty of the task and limited capacity of LSTM
decoder. While κ is a hyperparameter that needs
to be tuned, the model is overall not very sensitive
to it, and we show in Section 7 that reasonable κ
values transfer across similar tasks.

5 Experiments on Document Modeling

We also investigate how vMF VAE performs in a
different setting, one less plagued by the KL col-
lapse issue. Specifically, the Neural Variational
Document Model (NVDM), proposed by Miao
et al. (2016), is a VAE-based unsupervised doc-
ument model. This model follows the VAE frame-
work introduced in Section 2. Our document rep-
resentation is an indicator vector x of word pres-
ence or absence in the document. Since this is
a fixed-size representation, we use 2-layer MLPs
with 400 hidden units for both the encoder q(z|x)
and decoder p(x|z); the decoder places a simple



4509

Model Dim 20NG RCV1

fDARN (2014) 50 917 724200 - 598

G-NVDM (2016) 50 836 563200 852 550

v-NVDM (Ours)
25 793 558
50 830 529

200 851 609

Table 4: Test set perplexities for the document mod-
eling task. Feedforward Deep Auto Regressive Neu-
ral Network (fDARN) is implemented by Mnih and
Gregor (2014). Gaussian-based NVDM (G-NVDM) is
proposed in Miao et al. (2016). Dim indicates the di-
mension of the latent code. Our v-NVDM model out-
performs past models by a substantial margin.

multinomial distribution over words in the vocabu-
lary, and the probability of a document is the prod-
uct of the probabilities of its words.

Dataset For NVDM, we use two standard news
corpus, 20 News Groups (20NG) and the Reuters
RCV1-v2, which were used in Miao et al. (2016).6

Results Experimental results7 are shown in Ta-
ble 4. In contrast with NVRNN, the NVDM fully
relies on the power of latent code to predict the
word distribution, so we never observe a KL col-
lapse, yet vMF still achieves better performance
than Gaussian. As shown in Figure 3, in order
to keep the same amount of dispersion in samples
from the variational posterior, larger latent dimen-
sions need larger κ values and correspondingly
larger KL term values. For 20NG, which is much
smaller than RCV1, smaller dimensions therefore
give better performance. For both datasets, the set-
tings of κ = 100, dim = 25 and κ = 150, dim ∈
{50, 200} work well.

6 What do our VAEs encode?

We design more probing tasks to demonstrate
what is encoded in latent representations induced
by vMF VAE. One additional model variant we ex-
plore here is the NVRNN-BoW model. This is a
variant of NVRNN where the decoder additionally

6The preprocessed version can be downloaded from
https://github.com/ysmiao/nvdm

7We do not compare to results from Serban et al. (2017a).
Compared to our current results, that work reports very strong
performance on 20NG and very weak performance on RCV1;
we believe they either used different preprocessing or made
a mistake in reporting the results, but could not confirm this
with the authors.

P (x|z,BoW) Standard InputlessNLL PPL NLL PPL

RNNLM 79 (–) 43 106 (–) 152
G-VAE 79 (0.0) 43 106 (0.4) 153
v-VAE 73 (0.2) 33 93 (11.4) 82

Table 5: Experimental results of NVRNN-BoW on
PTB; i.e., the decoder also conditions on a bag of words
representation of the sentence to generate. In this case,
the Gaussian models exhibit KL collapse but vMF can
still learn effectively.

conditions on the vector BoW = 1n
∑n

i=1 e(xi),
the average word embedding value of the sen-
tence x. While an artificial setting, this lets us
see how effectively the latent code can capture in-
formation other than simple word choice by mak-
ing a form of this information independently avail-
able. Table 5 shows results in this setting, where
once again we see the KL collapse problem for
the Gaussian models and better performance from
vMF on perplexity in both the Standard and Input-
less settings.

Is the latent code more than a bag of words?
For all of these models, one hypothesis is that
the encoder may be learning to memorize the bag
of words and then preferentially generate words
in that bag from the decoder. To verify this, we
investigate whether the BoW representation and
the learned latent code can be reconstructed from
each other. Specifically, given a sentence x we
can compute BoW as defined above and µ =
enc(x), the latent encoding of x as represented
by the mean vector output by the encoder. We
can use a simple multilayer perceptron to to try
to map from the bag of words to the latent code:
µ̂ = MLP (BoW ), then learn the parameters of
the MLP by minimizing ‖µ̂ − µ‖2 on a sample.
The same process can be used to learn a mapping
from µ back to the bag of words.

Table 6 shows averaged cosine similarities of
our reconstructions under both Gaussian and vMF
models. For vMF, µ can reconstruct the bag-of-
words more accurately than the bag-of-words can
reconstruct µ, indicating that the latent code in
vMF captures more information beyond the bag
of words.

We repeat this experiment in a separate
NVRNN model where the decoder can explicitly
condition on the BoW vector described above.
The results are shown in the right column of Ta-
ble 6. Our model, v-VAE, achieves a lower cosine

https://github.com/ysmiao/nvdm


4510

Model NVRNN NVRNN-BoW
Setting µ→ BoW BoW → µ µ→ BoW

G-VAE 0.74 0.74 0.32
v-VAE 0.77 0.57 0.23

Table 6: Average cosine similarity when trying to re-
construct the latent code µ from the bag of words and
vice versa. In vMF, the latent code contains more infor-
mation beyond the bag of words, as shown by the lower
cosine similarity when predicting BoW → µ (0.57).
When the latent code is learned in a model conditioned
on the bag of words (right column), it predicts the bag
of words much less well, indicating that the model suc-
cessfully learns orthogonal information.

similarity than G-VAE (0.23 vs. 0.32), indicating
that it capturing less redundant information and
using the latent space to more efficiently model
other properties of the data.

Sensitivity to word order Table 6 shows that
NVRNN with vMF encodes information beyond
the bag of words; a natural hypothesis is that it
is encoding word order. We can more directly in-
vestigate this in the context of both NVRNN and
NVRNN-BoW settings. Inspired by Zhao et al.
(2017a), we propose an experiment probing the
sensitivity to randomly swapping adjacent pairs of
words for the encoding in the Inputless setting on
PTB. We vary the probability of swapping each
word pair and see how the latent code changes as
the number of swaps increases. Ideally, our mod-
els should capture ordering information and there-
fore be sensitive to this change.

Figure 5 shows the results. v-VAE’s representa-
tions are more sensitive than those of the G-VAE:
they change faster as swaps become more likely.8

In the NVRNN-BoW setting, we see that the mod-
els are even more sensitive. vMF enables us to
more easily learn this kind of desirable informa-
tion in our sentence encodings.

7 Controlling Variance with κ

A core aspect of our approach so far has been treat-
ing κ as a fixed hyperparameter. Fixing κ is ben-
eficial from an optimization standpoint: it makes
it more difficult for the model to get stuck in local
optima. But it also reduces the model’s flexibility,
since we can no longer predict per-example κ val-
ues, and it introduces another parameter that the

8The Gaussian VAE here makes very little use of the latent
variable, hence why the representations change very little.

0.0 0.2 0.4 0.6
Chance of Swap

0.4

0.6

0.8

1.0

A
vg

 C
os

in
e 

Si
m

ila
rit

y

G-VAE
G-VAE-BoW
v-VAE
v-VAE-BoW

Figure 5: Sensitivity of latent codes to swapping ad-
jacent words of encoding sequence. Cosine similar-
ity is measured between the latent code (encoded mean
vector) of the original sentence and the sentence after
swaps are applied. We see that vMF is more highly sen-
sitive to swaps in both the NVRNN and NVRNN-BoW
settings, indicating that its latent space likely encodes
more ordering information.

system designer must tune.
Fortunately, a wide range of κ values appear

to work well for the tasks we consider. Figure 6
shows how the concentration parameter κ changes
the results on PTB when the latent dimension and
other hyperparameters are held fixed. We have
ordered the tasks left-to-right from “hardest” to
“easiest” in terms of necessity of latent represen-
tation: the Inputless setting needs heavy informa-
tion from the latent code to reconstruct the sen-
tence, whereas the Standard-BoW setting has an
extremely strong decoder to predict the next word.
We see that in each of these cases, a wide range of
κ values works, and moreover reasonable κ values
transfer between the two Standard and between the
two Inputless settings, indicating that the overall
approach is not highly sensitive to these hyperpa-
rameter values.

Brittleness of Learning κ Throughout this
work, we have treated κ as a fixed parameter.
However, we can treat κ in the same way as σ in
the Gaussian case and learn it on a per-instance
basis. The KL divergence of vMF is differentiable
with respect to κ given gradients of the modified
Bessel function of the first kind,9 allowing us to
change the concentration on a per-instance basis.
However, this reintroduces the issue of KL col-
lapse: the KL term will encourage κ to be as low

9∇κId(κ) = 12 (Id−1(κ) + Id+1(κ))



4511

Inp
utle

ss

Inp
utle

ss-B
oW

Stan
dard

Stan
dard

-Bo
W

5
20
35
50
65
80
95

552 128 110 33

421 107 104 35

306 89 98 35

326 82 112 37

298 95 140 64

262 99 155 76

278 108 171 90

0.0

0.2

0.4

0.6

0.8

1.0

Figure 6: Perplexity of v-VAE in different settings with
different κ values when the latent dimension is 50.
Darker colors correspond to perplexity values closer to
the best observed for that setting. For each task, we
see that there is a range of κ values that work well, and
these transfer between comparable tasks.

as possible, potentially making the latent variable
vacuous.

In practice, we observe that it is necessary to
clip κ values to a certain range for numerical rea-
sons. Within this range, the model gravitates to-
wards the smallest κ values and performs substan-
tially worse than models trained with our fixed κ
approach. This indicates that even with the vMF
model, the optimization problem posed by ELBO
is simply a hard one and the approach of fixing
KL divergence is a surprisingly good optimization
technique.

8 Related Work

Applications of VAE in NLP Deep generative
models have achieved impressive successes in do-
mains adjacent to NLP such as image genera-
tion (Gregor et al., 2015; Oord et al., 2016a) and
speech generation (Chung et al., 2015; Oord et al.,
2016b). VAEs specifically (Kingma and Welling,
2013; Rezende et al., 2014) have been a popular
model variant in NLP. They have been applied to
tasks including document modeling (Miao et al.,
2016), language modeling (Bowman et al., 2016),
and dialogue generation (Serban et al., 2017b).
VAEs can be also be applied for semi-supervised
classification (Xu et al., 2017). Recent twists on
the standard VAE approach including combining
VAE and holistic attribute discriminators for con-
ditional generation (Hu et al., 2017) and using a
more flexible latent space regularized by an adver-
sarial method (Zhao et al., 2017a).

VAE Objective Several pieces of recent work
have highlighted the issues with optimizing the
VAE objective. Alemi et al. (2018) shed light on
the problem from the perspective of information
theory. Zhao et al. (2017b) and Higgins et al.
(2017) both propose various reweightings of the
objective along with theoretical and empirical jus-
tification.

Choices of Priors for VAE Some past work has
explored various priors for VAE. Serban et al.
(2017a) proposed a piecewise constant distribu-
tion which deals with multiple modes, but which
sacrifices the property of continuous interpolation.
Guu et al. (2018) also applied vMF in a VAE
model, but used theirs specifically in the sentence-
editing case. Davidson et al. (2018) explored vMF
in a VAE model for MNIST and a link prediction
task. Hasnat et al. (2017) applied the vMF distri-
bution for facial recognition. Other past work has
used different decoders, including CNNs (Yang
et al., 2017) and CNN-RNN hybrids (Semeniuta
et al., 2017). Changing the decoder is a change
largely orthogonal to changing the prior: it can
alleviate the KL vanishing issue, but it does not
necessarily scale to new settings and does not give
explicit control over utilization of the latent code.

9 Conclusion

In this paper, we propose the use of a von Mises-
Fisher VAE to resolve optimization issues in vari-
ational autoencoders for text. This choice of dis-
tribution allows us to explicitly control the bal-
ance between the capacity of the decoder and the
utilization of the latent representation in a princi-
pled way. Experimental results demonstrate that
the proposed model has better performance than a
Gaussian VAE across a range of settings. Further
analysis shows that vMF VAE is more sensitive to
word order information and makes more effective
use of the latent code space.

Acknowledgments

This work was partially supported by NSF Grant
IIS-1814522, a Bloomberg Data Science Grant,
and an equipment grant from NVIDIA. The au-
thors acknowledge the Texas Advanced Comput-
ing Center (TACC) at The University of Texas at
Austin for providing HPC resources used to con-
duct this research. Thanks as well to the anony-
mous reviewers for their helpful comments.



4512

References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dil-

lon, Rif Saurous, and Kevin Murphy. 2018. Fixing
a Broken ELBO. International Conference on Ma-
chine Learning.

Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh,
and Suvrit Sra. 2005. Clustering on the unit hyper-
sphere using von Mises-Fisher distributions. Jour-
nal of Machine Learning Research, 6.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew Dai, Rafal Jozefowicz, and Samy Bengio.
2016. Generating Sentences from a Continuous
Space. Proceedings of The 20th SIGNLL Confer-
ence on Computational Natural Language Learning.

Xi Chen, Diederik P Kingma, Tim Salimans, Yan
Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. 2016. Vari-
ational Lossy Autoencoder. arXiv preprint
arXiv:1611.02731.

Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth
Goel, Aaron C Courville, and Yoshua Bengio. 2015.
A Recurrent Latent Variable Model for Sequential
Data. Advances in Neural Information Processing
Systems.

Tim Davidson, Luca Falorsi, Nicola Cao, Thomas Kipf,
and Jakub Tomczak. 2018. Hyperspherical Varia-
tional Auto-Encoders. 34th Conference on Uncer-
tainty in Artificial Intelligence.

Karol Gregor, Ivo Danihelka, Alex Graves, Danilo
Rezende, and Daan Wierstra. 2015. DRAW: A Re-
current Neural Network For Image Generation. In-
ternational Conference on Machine Learning.

Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,
and Percy Liang. 2018. Generating Sentences by
Editing Prototypes. Transactions of the Association
for Computational Linguistics, 6:437–450.

Md Hasnat, Julien Bohn, Jonathan Milgram, Stphane
Gentric, and Liming Chen. 2017. von Mises-Fisher
Mixture Model-based Deep learning: Application to
Face Verification. arXiv preprint arXiv:1706.04264.

Irina Higgins, Loic Matthey, Arka Pal, Christopher
Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. 2017. β-VAE:
Learning Basic Visual Concepts with a Constrained
Variational Framework. International Conference
on Learning Representations.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
Short-Term Memory. Neural Computation, 9.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Toward con-
trolled generation of text. International Conference
on Machine Learning.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding Variational Bayes. arXiv preprint
arXiv:1312.6114.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
put. Linguist., 19(2):313–330.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural
Variational Inference for Text Processing. Interna-
tional Conference on Machine Learning.

Andriy Mnih and Karol Gregor. 2014. Neural Varia-
tional Inference and Learning in Belief Networks.
International Conference on Machine Learning.

Aaron Oord, Nal Kalchbrenner, and Koray
Kavukcuoglu. 2016a. Pixel Recurrent Neural
Networks. International Conference on Machine
Learning.

Aaron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. 2016b. WaveNet: A Genera-
tive Model for Raw Audio. arXiv preprint
arXiv:1609.03499.

Danilo Rezende, Shakir Mohamed, and Daan Wierstra.
2014. Stochastic Backpropagation and Approxi-
mate Inference in Deep Generative Models. Inter-
national Conference on Machine Learning.

Stanislau Semeniuta, Aliaksei Severyn, and Erhardt
Barth. 2017. A Hybrid Convolutional Variational
Autoencoder for Text Generation. Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing.

Iulian Serban, Alexander G Ororbia, Joelle Pineau, and
Aaron Courville. 2017a. Piecewise Latent Variables
for Neural Variational Text Processing. Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing.

Iulian Serban, Alessandro Sordoni, Ryan Lowe, Lau-
rent Charlin, Joelle Pineau, Aaron Courville, and
Yoshua Bengio. 2017b. A Hierarchical Latent Vari-
able Encoder-Decoder Model for Generating Dia-
logues. AAAI Conference on Artificial Intelligence.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style Transfer from Non-Parallel
Text by Cross-Alignment. Advances in Neural In-
formation Processing Systems.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ukasz
Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. Advances in Neural Information Pro-
cessing Systems.

Andrew Wood. 1994. Simulation of the von Mises
Fisher Distribution. Communications in Statistics -
Simulation and Computation, 23(1):157–164.



4513

Jiacheng Xu, Danlu Chen, Xipeng Qiu, and Xuan-
jing Huang. 2016. Cached Long Short-Term Mem-
ory Neural Networks for Document-Level Senti-
ment Classification. Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing.

Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan. 2017.
Variational Autoencoder for Semi-Supervised Text
Classification. AAAI Conference on Artificial Intel-
ligence.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved Varia-
tional Autoencoders for Text Modeling using Di-
lated Convolutions. International Conference on
Machine Learning.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. SeqGAN: Sequence Generative Adversarial
Nets with Policy Gradient. AAAI Conference on Ar-
tificial Intelligence.

Biao Zhang, Deyi Xiong, Hong Duan, and Min Zhang.
2016. Variational Neural Machine Translation. Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing.

Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander M
Rush, and Yann LeCun. 2017a. Adversarially Reg-
ularized Autoencoders. arXiv.

Shengjia Zhao, Jiaming Song, and Stefano Ermon.
2017b. InfoVAE: Information Maximizing Varia-
tional Autoencoders. arXiv.


