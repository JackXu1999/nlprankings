








































Facilitating Multi-Lingual Sense Annotation:
Human Mediated Lemmatizer

Pushpak Bhattacharyya
IIT Bombay

pushpakbh@gmail.com

Ankit Bahuguna
IIT Bombay / TU Munich

ankitbahuguna@outlook.com

Lavita Talukdar
IIT Bombay

lavita.talukdar@gmail.com

Bornali Phukan
IIT Bombay

bornali31phukan@gmail.com

Abstract

Sense marked corpora is essential for super-
vised word sense disambiguation (WSD). The
marked sense ids come from wordnets. How-
ever, words in corpora appear in morphed
forms, while wordnets store lemma. This sit-
uation calls for accurate lemmatizers. The
lemma is the gateway to the wordnet. How-
ever, the problem is that for many languages,
lemmatizers do not exist, and this problem is
not easy to solve, since rule based lemmatiz-
ers take time and require highly skilled lin-
guists.Satistical stemmers on the other hand do
not return legitimate lemma.

We present here a novel scheme for creating
accurate lemmatizers quickly. These lemma-
tizers are human mediated. The key idea is that
a trie is created out of the vocabulary of the
language. The lemmatizing process consists in
navigating the trie, trying to find a match be-
tween the input word and an entry in the trie.
At the point of first mismatch, the yield of the
subtree rooted at the partially matched node is
output as the list of possible lemma. If the cor-
rect lemma does not appear in the list- as noted
by a human lexicographer- backtracking is ini-
tiated. This can output more possibilities. A
ranking function filters and orders the output
list of lemma.

We have evaluated the performance of this
human mediated lemmatizer for eighteen In-
dian Languages and five European languages.
We have compared accuracy values against
well known lemmatizers/stemmers like Mor-
pha, Morfessor and Snowball stemmers, and
observed superior performance in all cases.
Our work shows a way of speedily creating
human assisted accurate lemmatizers, thereby
removing a difficult roadblock in many NLP
tasks, e.g., sense annotation.

1 Introduction

Supervised WSD- the ruling paradigm for high
accuracy sense determination- requires sense
marked corpus in large quantity. Sense annota-
tion is a difficult job, requiring linguistic expertise,
knowledge of the topic and domain, and most im-
portantly a fine sense of word meanings.

Most often the sense annotation task is accom-
plished by using a Sense Marker Tool, like the one
described in Chatterjee et. al. (2010). This partic-
ular tool, equipped with an easy to use GUI facili-
tates the task of manually marking each word with
the correct sense of the word, as available in the
wordnet of the language. The tool has the word-
net sense repository resident in its memory. It dis-
plays the senses of word for the human annotator
to choose from. The mentioned tool is extensively
used by a number of language groups in India to
produce high quality sense marked corpus. Figure
1 shows the Sense Marker Tool, where a user is
marking the sense of the word “banks” in English.

Figure 1: Sense Marking Tool - manually marking
correct sense for English word “banks”

Now it is obvious that if the wordnet is to be ac-
cessed for the senses to be displayed, the lemma
of the words must be available. The lemma is the
gateway to the wordnet. Lemmatization is an im-



portant activity in many NLP applications. We
stress at this point that our focus of attention is
lemmatization and not stemming. As a process,
stemming aims to reduce a set of words into a can-
nonical form which may or may not be a dictio-
nary word of the language. Lemmatization, on the
other hand, always produces a legal root word of
the language. To give an example, a stemmer can
give rise to “ladi” from “ladies”. But a lemmatizer
will have to produce “lady”. And if the senses of
‘ladies’ has to be found, the lemma ‘lady’ is re-
quired.

There are three basic approaches to lemmatiza-
tion, viz., affix removal (rule based systems), sta-
tistical (supervised/unsupervised) and hybrid (rule
based + statistical). Developing a rule based
system is an uphill task, requiring a number of
language experts and enormous amount of time.
Purely statistical systems fail exploit linguistic
features, and produce non-dictionary lexemes. A
hybrid system utilizes both the above mentioned
approaches.

Figure 2: Sense Marking Tool - Input Language:
Hindi

In this paper, we discuss an alternative approach
to lemmatization which is quick, takes user help
and is exact. The key idea is that a trie is created
out of the vocabulary of the language. The lemma-
tizing process consists in navigating the trie, trying
to find a match between the input word and an en-
try in the trie. At the point of first mismatch- i.e.,
maximum prefix matching, the yield of the subtree
rooted at the partially matched node is output as
the list of possible lemma. If the correct lemma
does not appear in the list- as noted by a human
lexicographer- a backtracking is initiated. This can
output more possibilities, since the yield of a node
at a higher level of the trie is output. A ranking

function filters and orders the output list of lemma.
Our ultimate goal is to integrate the human me-

diated lemmatizer with the Sense Marking Tool
(Figure 2).Currently, the tool supports the follow-
ing 9 languages: English, Hindi, Marathi, Tamil,
Telugu, Kannada, Malayalam, Bengali and Pun-
jabi.

We give an example to give a feel for how our
lemmatiser works. When a linguist tries to mark
the correct sense of the inflected Assamese word
as shown in Figure 3 (‘gharalai’ meaning ‘in the
house’), in the current scenario no output is dis-
played, but with our lemmatizer integrated, it will
show the possible lemma of that word.

Here, the correct lemma is shown at the top of
the list, and the lemma can pull out the senses from
the wordnet for the linguist to tag with.

The remainder of this paper is organized as fol-
lows. We describe related work and background
in section 2. Section 3 explains the core of our
human mediated lemmatiser. Implementation de-
tails are in Section 4. Experiments and results are
discussed in Section 5. Comparison with existing
stemmers/lemmatizers are in Section 6. Section
7 concludes the paper and points to future direc-
tions.

2 Related Work and Background

Lovins described the first stemmer (Lovins, 1968),
which was developed specifically for IR/NLP ap-
plications. His approach consisted of the use of
a manually developed list of 294 suffixes, each
linked to 29 conditions, plus 35 transformation
rules. For an input word, the suffix with an
appropriate condition is checked and removed.
Porter developed the Porter stemming algorithm
(Porter, 1980) which became the most widely
used stemming algorithm for English language.
Later, he developed stemmers that covered Ro-
mance (French, Italian, Portuguese and Spanish),
Germanic (Dutch and German) and Scandinavian
languages (Danish, Norwegian and Swedish), as



Figure 3: Tool shows no output for inflected Assamese word “ghoroloi” (highlighted) meaning to home.

well as Finnish and Russian (Porter, 2006). These
stemmers were described in a very high level lan-
guage known as Snowball1.

A number of statistical approaches have been
developed for stemming. Notable works in-
clude: Goldsmith’s unsupervised algorithm for
learning morphology of a language based on the
Minimum Description Length (MDL) framework
(Goldsmith, 2001; 2006). Creutz uses prob-
abilistic maximum a posteriori (MAP) formu-
lation for unsupervised morpheme segmentation
(Creutz, 2005; 2007).

A few approaches are based on the application
of Hidden Markov models (Melucci et al., 2003).
In this technique, each word is considered to be
composed of two parts “prefix” and “suffix”. Here,
HMM states are divided into two disjoint sets:
Prefix state which generates the first part of the
word and Suffix state which generates the last part
of the word, if the word has a suffix. After a com-
plete and trained HMM is available for a language,
stemming can be performed directly.

Plisson proposed the most accepted rule based
approach for lemmatization (Plisson et al., 2008).
It is based on the word endings, where suffixes
are removed or added to get the normalized word
form. In another work, a method to automati-
cally develop lemmatization rules to generate the
lemma from the full form of a word was dis-
cussed (Jongejan et al., 2009). The lemmatizer
was trained on Danish, Dutch, English, German,
Greek, Icelandic, Norwegian, Polish, Slovene and
Swedish full form-lemma pairs respectively.

Kimmo (Karttunen et al., 1983) is a two level
morphological analyser containing a large set of
morphophonemic rules. The work started in 1980
and the first implementation n LIST was available
3 years later.

Tarek El-Shishtawy proposed the first non-
statistical Arabic lemmatizer algorithm (El-

1See http://snowball.tartarus.org/

Shishtawy et al., 2012). He makes use of different
Arabic language knowledge resources to generate
accurate lemma form and its relevant features that
support IR purposes and a maximum accuracy of
94.8% is reported.

OMA is a Turkish Morphological Analyzer
which gives all possible analyses for a given word
with the help of finite state technology. Two-level
morphology is used to build the lexicon for a lan-
guage (Ozturkmenoglu et al., 2012).

Grzegorz Chrupala (Chrupala et al., 2006) pre-
sented a simple data-driven context-sensitive ap-
proach to lemmatizating word forms. Shortest
Edit Script (SES) between reversed input and out-
put strings is computed to achieve this task. An
SES describes the transformations that have to be
applied to the input string (word form) in order to
convert it to the output string (lemma).

As for lemmatizers for Indian languages, the
earliest work by Ramanathan and Rao (2003) used
manually sorted suffix list and performed longest
match stripping for building a Hindi stemmer. Ma-
jumdar et. al (2007) developed YASS: Yet An-
other Suffix Stripper. Here conflation was viewed
as a clustering problem with a-priory unknown
number of clusters. They suggested several dis-
tance measures rewarding long matching prefixes
and penalizing early mismatches. In a recent work
related to Affix Stacking languages like Marathi,
(Dabre et al., 2012) Finite State Machine (FSM)
is used to develop a Marathi morphological Ana-
lyzer. In another approach, A Hindi Lemmatizer
is proposed, where suffixes are stripped according
to various rules and necessary addition of charac-
ter(s) is done to get a proper root form (Paul et
al., 2013). GRALE is a graph based lemmatizer
for Bengali comprising two steps (Loponen et al.,
2013). In the first, step it extracts the set of fre-
quent suffixes and in the second step, a human
manually identifies the case suffixes. Words are
often considered as node and edge from node u to
v exist if only v can be generated from u by addi-



tion of a suffix.
Unlike the above mentioned rule based and sta-

tistical approaches, our human mediated lemma-
tizer uses the properties of a “trie” data structure
which allows retrieving possible lemma of a given
inflected word, with human help at critical steps.

3 Our Approach to lemmatization

The scope of our work is suffix based morphology.
We do not consider infix and prefix morphology.
We first setup the data structure ‘Trie’ (Cormen et
al., 2001) using the words in the wordnet of the
language. Next, we match, byte by byte, the input
word form and wordnet words. The output is all
wordnet words which have the maximum prefix
match with the input word. This is the “direct”
variant of our lemmatizer.

The second or “backtrack” variant prints the re-
sults ‘n’ levels previous to the maximum matched
prefix obtained in the ‘direct’ variant. The value
of ‘n’ is usr controlled. A ranking function then
decides the final output displayed to the user.

In Figure 5, a sample trie diagram is shown con-
sisting of Hindi words given at Figure 4. The
words are stored starting from a node subsequent
to the root node, in a character by character uni-
code byte order.

Figure 4: List of sample words.

At each level, we insert the characters of the
word in an alphabetical order pertaining to uni-
code standard for different languages from left to
right.

We illustrate the search in the trie with the ex-
ample of the inflected word “lwEkyA ” (ladkiyan,
i.e., girls). Our lemmatizer gives the following
output:

Figure 5: A simple trie storing Hindi words (ka-
marband, kamara, kamari, kamal, lad, ladakpan,
ladka, ladki and ladna)

From this result set, a trained lexicographer can
easily pick the root word as “lwkF” (ladki, i.e.,
girl). It is the user controlled backtracking which
is novel and very useful in our lemmatizer. We
elaborate on backtracking below.

3.1 Backtracking

We explain backtracking using the sample words
in Figure 6 and the trie as shown in Figure 7.

Figure 6: List of sample words : Backtracking.

We take the example of asl�l� (aslele) which
is an inflected form of the Marathi word asZ�
(asane). Here, when we call the first iterative
procedure without backtracking, the word aslF
(asali) is given as output. Although, being a valid
wordnet word, it is not the correct root form of
the word asl�l�. Hence, we perform a back-
track from asl (asal) to as (asa) thereby get-
ting asZ� (asane) as one of the outputs.

An example involving two levels of backtrack-
ing is that of a Marathi word kApsAlA (kApsAlA),
which is an inflected form of the root word kAp� s,
(kApusa) i.e., cotton). Here are the results from
our lemmatizer:



Figure 7: Search in Trie - Backtracking in case of
Marathi word “asl�l�” (aslele)

Thus, we can find the root word kAp� s at 10th
position in the result list of two level backtrack.

3.2 Ranking Lemmatizer Results
Our lemmatizer by default prints out all the pos-
sible root forms given a queried word. We have
employed a heuristic to filter the results and hence
minimize the size of the result set:

1. Only those output matches are accepted
whose length is smaller than or equal to the
length of the queried word. This is based on
an assumption that the root word length shall
not be greater than the length of its equiva-
lent inflected word form (we agree that this is
not universally true; hence the word ’heuris-
tic’ for the pruning strategy).

2. The filtered results are sorted on the basis of
the length (in an ascending order) which in
most cases displays the root word earlier in a
given set of words.

For example, in case of the Marathi word
“asl�l�” (aslele) the ranking function receives a
number of words as input, after a first level back-
track is performed (as shown in Figure 8). The

function then applies the ranking heuristic based
on length and then sorts them in their ascending
order. Thus, the final lemmatizer output is gener-
ated with the root word asZ� (asane) in first posi-
tion.

Figure 8: Ranking of results - For Marathi word
“asl�l�” (aslele)

4 Implementation

We have developed an on-line interface (Figure
9) and a downloadable Java based executable jar
which can be used to test our implementation.
The interface allows input from 18 different Indian
languages (Hindi, Assamese, Bengali, Bodo, Gu-
jarati, Kannada, Kashmiri, Konkani, Malayalam,
Manipuri, Marathi, Nepali, Sanskrit, Tamil, Tel-
ugu, Punjabi, Urdu and Odiya) and 5 European
languages (Italian, Danish, Hungarian, French and
English) and they are linked to their respective lex-
ical databases. For easy typing, a virtual keyboard
is also provided. The first iteration of stemming
is performed by clicking “Find” and backtracking
is performed by clicking “Backtrack”. The back-
track utility allows us to backtrack upto 8 levels
and the level of backtrack is displayed in a field.
This was implemented, since there are several in-
flected words in our test data which required a
backtrack of more than one level to get the root
word. The interface also has a facility to upload a
text document related to a specific language. We
can then download the results in an output file con-
taining possible stems of all the words present in
the input document. The human annotator can
thus choose the correct lemma from the list of pos-
sible stems associated with each word.

5 Experiments and Results

We performed several experiments to evaluate the
performance of the lemmatizer. The basis of our



Figure 9: Online Interface - Language Indepen-
dent Human Mediated Lemmatizer

evaluation was that for every inflected input word,
a set of output root forms will be generated by
the lemmatizer. Even if one of the result in the
top 10 from this set matches the root in the
gold standard, then we consider our result to
be correct. Following this approach the accuracy
of inflected nouns undergoing stemming is very
high. Although, due to readjustment in verbs, for
the first iteration without backtracking, our lem-
matizer gives a fairly low accuracy. This can be
further improved through backtracking, when we
traversed the trie, one level up at a time. The first
iteration of stemming gives result among the best
five outputs and the backtracking approach gives
among the best ten outputs. Interestingly, for in-
flected words in Italian our lemmatizer performs
better when we include the results of one level
backtrack as compared to a non-backtrack vari-
ant. The results for various languages are shown in
Table 1 based on the lemmatizer’s default variant,
i.e., without using backtrack feature.

5.1 Preparation of Gold Data

We prepared the gold data to perform evaluation
for Hindi and Marathi languages, by using the do-
main specific sense marked corpus2 which con-
tains inflected words along with their root word
forms. This corpus was created by trained lexicog-
raphers. Similarly, we had a sense marked corpus
for Bengali, Assamese, Punjabi and Konkani.

For Dravidian languages like Malayalam and
Kannada and for European languages like French
and Italian, we had to perfrom manual evaluation
as the sense marked data was unavailable. We did
this, with a list of inflected words provided by na-
tive speakers and found remarkable precision in
result.

2See http://www.cfilt.iitb.ac.in/wsd/
annotated_corpus/

Language Corpus Total Precision
Type Words Value

Hindi Health 8626 89.268
Hindi Tourism 16076 87.953
Bengali Health 11627 93.249
Bengali Tourism 11305 93.199
Assamese General 3740 96.791
Punjabi Tourism 6130 98.347
Marathi Health 11510 87.655
Marathi Tourism 13176 85.620
Konkani Tourism 12388 75.721
Malayalam* General 135 100.00
Kannada* General 39 84.615
Italian* General 42 88.095 #
French* General 50 94.00

Table 1: Precision calculation for output produced
by our lemmatizer based on the first variant, i.e.,
without using backtrack feature. * denotes manual
evaluation and # denotes one level backtracking.

5.2 Analysis
Errors are due to the following:

1. Agglutination in Marathi and Dravidian lan-
guages.

Marathi and Dravidian languages like Kan-
nada and Malayalam show the process of ag-
glutination3. It is a process where a complex
word is formed by combining morphemes
each of which have a distinct grammatical or
semantic meaning. Such words do not pro-
duce correct results in the first go, and we
need to back track the trie upto a certain level
to get the correct root form.

2. Suppletion.

The term suppletion4 implies that a gap in the
paradigm was filled by a form “supplied” by
a different paradigm. For example, the root
word “go”, has an irregular past tense form
“went”. For these irregular words the out-
put of the lemmatizer is incorrect, as the root
words are stored in their regular forms in the
trie.

We have reported the precision scores (Table 1)
for all the languages (except Italian) on the ”di-
rect” variant of our lemmatizer, i.e., without using

3See http://en.wikipedia.org/wiki/
Agglutination

4See https://en.wikipedia.org/wiki/
Suppletion



Corpus Name Human Mediated Lemmatizer Morpha Snowball Morfessor
English General 89.20 90.17 53.125 79.16
Hindi General 90.83 NA NA 26.14
Marathi General 96.51 NA NA 37.26

Table 2: Comparative Evaluation (precision values) of Human Mediated Lemmatizer [Without using
Backtracking] against other classic stemming systems like Morpha, Snowball and Morfessor

backtrack feature. It is clear from our examples
in Marathi viz. “asl�l�” (aslele) and the scores
reported in Italian that precision improves when
backtracking is used.

6 Comparative evaluation with existing
lemmatizers/stemmer

We compared performance of our system
against three most commonly used lemmatiz-
ers/stemmers, viz. Morpha (Guido et al., 2001),
Snowball 5 and Morfessor (Creutz, 2005; 2007).
The results are shown in Table 2. Our lemmatizer
works better than Morfessor for Hindi (up by
64%) and Marathi (up by 59%). For English, our
lemmatizer outperforms Snowball by almost 36%
and Morfessor by almost 10%. Although, as an
exception, Morpha lemmatizer works better by
about 1% in this case.

Snowball and Morpha lemmatizers are not
available for Indian Languages and thus the re-
sults are marked with ‘NA’. Morfessor being sta-
tistical in nature does not capture all the linguistic
and morphological phenomenons associated with
Indian languages and Snowball and Morpha are
strictly rule based in nature and do not use any
linguistic resource to validate the output. We
have, of course, compared our lemmatizer with
in-house rule based Hindi and Marathi morph an-
alyzers. The Marathi Morphological Analyzer
(Dabre et al., 2012) has an accuracy of 72.18%,
with a usability of 94.33%, where as the in-house
Hindi Morphological Analyzer6 accuracy is close
to 100% with average usability around 96.5% (Ta-
ble 3). Here, usability is the percentage of total
number of words analyzed out of total words in
corpus.

However, these morph analysers have taken
years to build with many false starts and false hits
and misses. Compared to this, our human medi-

5See http://snowball.tartarus.org/
index.php

6See http://www.cfilt.iitb.ac.in/
˜ankitb/ma/

Nouns Verbs
Total words in test Corpus 14475 13160
Correctly analyzed words 13453 13044
Unidentified words 1022 116

Table 3: Hindi Morphological Analyzer - Results

ated lemmatizer was constructed in a few weeks’
time. Once it was realized that we need lemma-
tizers for accessing senses of words, the system
was built in no time. It is the preparation of gold
data, generation of accuracy values and compari-
son with existing systems that took time.

7 Conclusion and future work

We gave an approach for developing light weight
and quick-to-create human mediated lemmatizer.
We applied the lemmatizer to 18 different Indian
languages and 5 European languages. Our ap-
proach uses the longest prefix match functionality
of a trie data structure. Without using any man-
ually created rule list or statistical measure, we
were able to find lemma of the input word within
a ranked list of 5-15 outputs. The human annota-
tor can thus choose from a small set of results and
proceed with sense marking, thereby greatly help-
ing the overall task of Machine Translation and
Word Sense Disambiguation. We also confirm the
fact that the combination of man and machine can
identify the root to near 100 percent accuracy.

In future, we want to further prune of output list,
making the ranking much more intelligent. Inte-
gration of the human mediated lemmatizer to all
languages’ sense marking task needs to be com-
pleted. Also we want to expand our work to in-
clude languages from different linguistic families.

References
Arindam Chatterjee, Salil Rajeev Joshi, Mitesh M.

Khapra and Pushpak Bhattacharyya 2010. Intro-
duction to Tools for IndoWordnet and Word Sense
Disambiguation, The 3rd IndoWordnet Workshop,
Eighth International Conference on Natural Lan-



guage Processing (ICON 2010), IIT Kharagpur, In-
dia.

Grzegorz Chrupala 2006. Simple data-driven context-
sensitive lemmatization , Chrupaa, Grzegorz (2006)
Simple data-driven context-sensitive lemmatization.
In: SEPLN 2006, 13-15 September 2006, Zaragoza,
Spain.

Thomas H. Cormen, Clifford Stein, Ronald L. Rivest
and Charles E. Leiserson 2001. Introduction
to Algorithms, 2nd Edition, ISBN:0070131511,
McGraw-Hill Higher Education.

Mathis Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0., Technical
Report A81, Publications in Computer and Informa-
tion Science, Helsinki University of Technology.

Mathis Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning, Association for Computing Machinery
Transactions on Speech and Language Processing,
4(1):1-34

Raj Dabre, Archana Amberkar and Pushpak Bhat-
tacharyya 2012. Morphology Analyser for Affix
Stacking Languages: a case study in Marathi, COL-
ING 2012, Mumbai, India, 10-14 Dec, 2012.

Tarek El-Shishtawy and Fatma El-Ghannam 2012. An
Accurate Arabic Root-Based Lemmatizer for Infor-
mation Retrieval Purposes, IJCSI International Jour-
nal of Computer Science Issues, Vol. 9, Issue 1, No
3, January 2012 ISSN (Online): 1694-0814.

John A. Goldsmith 2001. Unsupervised Learning of
the morphology of a Natural Language, Computa-
tional Linguistics, 27(2): 153-198

John A. Goldsmith 2006. An algorithm for the unsu-
pervised Learning of morphology, Natural Language
Engineering, 12(4): 353-371

Bart Jongejan and Hercules Dalianis 2009. Automatic
training of lemmatization rules that handle morpho-
logical changes in pre-, in- and suffixes alike, Pro-
ceedings of the 47th Annual Meeting of the ACL
and the 4th IJCNLP of the AFNLP, pages 145 - 153,
Suntec, Singapore, 2-7 August 2009

Lauri Karttunen 1983. KIMMO: A General Mor-
phological Processor , Texas Linguistic Forum, 22
(1983), 163-186.

Aki Loponen, Jiaul H. Paik and Kalervo Jarvelin 2013.
UTA Stemming and Lemmatization Experiments in
the FIRE Bengali Ad Hoc Task , Multilingual Infor-
mation Access in South Asian Languages Lecture
Notes in Computer Science Volume 7536, 2013, pp
258-268

J.B. Lovins 1968. Development of a stemming algo-
rithm, Mechanical Translations and Computational
Linguistics Vol.11 Nos 1 and 2, pp. 22-31.

Prasenjit Majumder, Mandar Mitra, Swapan K. Parui,
Gobinda Kole, Pabitra Mitra, and Kalyankumar
Datta. 2007. YASS: Yet another suffix stripper, As-
sociation for Computing Machinery Transactions on
Information Systems, 25(4):18-38.

Prasenjit Majumder, Mandar Mitra and Kalyankumar
Datta 2007. Statistical vs Rule-Based Stemming for
Monolingual French Retrieval, Evaluation of Multi-
lingual and Multi-modal Information Retrieval, Lec-
ture Notes in Computer Science vol. 4370, ISBN
978-3-540-74998-1, Springer, Berlin, Heidelberg.

James Mayfield and Paul McNamee 2003. Single N-
gram Stemming, SIGIR ‘03, Toronto, Canada.

Massimo Melucci and Nicola Orio 2003. A novel
method of Stemmer Generation Based on Hid-
den Markov Models, CIKM ‘03, New Orleans,
Louisiana, USA.

Guido Minnen,John Carroll and Darren Pearce. 2001.
Applied morphological processing of English, Natu-
ral Language Engineering, 7(3). 207-223.

Okan Ozturkmenoglu and Adil Alpkocak 2012. Com-
parison of different lemmatization approaches for
information retrieval on Turkish text collection , In-
novations in Intelligent Systems and Applications
(INISTA), 2012 International Symposium on.

Plisson Joël, Lavrac Nada, Mladenic Dunja 2008. A
rule based approach to word lemmatization, Pro-
ceedings of 7th International Multi-conference In-
formation Society, IS 2004, Institute Jozef Stefen,
Ljubljana, pp.83-86, 2008

Snigdha Paul, Nisheeth Joshi and Iti Mathur 2013.
Development of a Hindi Lemmatizer, CoRR,
DBLP:journals/corr/abs/1305.6211 2013

M.F. Porter 1980. An algorithm for suffix stripping,
Program; 14, 130-137

M.F. Porter 2006. Stemming algorithms for var-
ious European languages, Available at [URL]
http://snowball.tartarus.org/
texts/stemmersoverview.html As
seen on May 16, 2013

Ananthakrishnan Ramanathan, and Durgesh D Rao
2003. A Lightweight Stemmer for Hindi. , Work-
shop on Computational Linguistics for South-Asian
Languages, EACL


