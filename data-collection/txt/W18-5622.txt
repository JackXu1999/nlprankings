



















































Evaluation of a Sequence Tagging Tool for Biomedical Texts


Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 193–203
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

193

Evaluation of a Sequence Tagging Tool for Biomedical Texts

Julien Tourille1,6, Matthieu Doutreligne1,2, Olivier Ferret3,
Nicolas Paris1,2,4, Aurélie Névéol1, Xavier Tannier4,5

1LIMSI, CNRS, Université Paris-Saclay, 2WIND-DSI, AP-HP, 3CEA, LIST,
4Sorbonne Université, 5Inserm, LIMICS, 6Univ. Paris-Sud

{julien.tourille,matthieu.doutreligne,aurelie.neveol}@limsi.fr,
olivier.ferret@cea.fr, nicolas.paris@aphp.fr,

xavier.tannier@sorbonne-universite.fr

Abstract

Many applications in biomedical natural lan-
guage processing rely on sequence tagging as
an initial step to perform more complex analy-
sis. To support text analysis in the biomedical
domain, we introduce Yet Another SEquence
Tagger (YASET), an open-source multi pur-
pose sequence tagger that implements state-of-
the-art deep learning algorithms for sequence
tagging. Herein, we evaluate YASET on part-
of-speech tagging and named entity recogni-
tion in a variety of text genres including ar-
ticles from the biomedical literature in En-
glish and clinical narratives in French. To
further characterize performance, we report
distributions over 30 runs and different sizes
of training datasets. YASET provides state-
of-the-art performance on the CoNLL 2003
NER dataset (F1=0.87), MEDPOST corpus
(F1=0.97), MERLoT corpus (F1=0.99) and
NCBI disease corpus (F1=0.81). We believe
that YASET is a versatile and efficient tool that
can be used for sequence tagging in biomedi-
cal and clinical texts.

1 Introduction

Many applications in biomedical Natural Lan-
guage Processing (NLP), including relation ex-
traction or text classification, rely on sequence tag-
ging as an initial step to perform more complex
analysis. To support text analysis in the biomed-
ical domain, we present Yet Another SEquence
Tagger (YASET), an open-source multi-purpose
sequence tagger written in Python. YASET aims at
providing NLP researchers with fast and accurate
implementations of cutting-edge deep learning se-
quence tagging models. YASET is built using Ten-
sorFlow (Abadi et al., 2015), an open source soft-
ware library for numerical computation. The code
is licensed under the version 3 of the GNU Gen-

eral Public License1 and is freely available on-
line2. The main contributions of this work are:

• a fast and accurate implementation of
a state-of-the-art sequence tagging model
based on Long Short-Term Memory Net-
works (LSTMs) (Hochreiter and Schmidhu-
ber, 1997). The architecture is similar to the
one described in Lample et al. (2016) and is
able to process mini-batches for faster train-
ing. Furthermore, YASET supports the use
of handcrafted features in combination with
word and character embeddings;

• an easy-to-use interface based on a central
configuration file that is used to setup exper-
iments, with default parameters that are suit-
able for most sequence tagging tasks;

• an evaluation on various biomedical corpora
and on the CoNLL 2003 corpus, studying the
stability of our model and the effect of train-
ing data size. We compare YASET perfor-
mance with state-of-the-art results published
in the literature.

2 Related Work

Several open-source implementations for se-
quence tagging based on neural network architec-
tures have become available over the last years.
Lample et al. (2016) provide a Python implemen-
tation3 for the two models presented in their pa-
per. They are implemented with Theano (Al-Rfou
et al., 2016), a Python library for deep learning.

1https://www.gnu.org/licenses/gpl-3.0.
en.html

2https://github.com/jtourille/yaset
3https://github.com/glample/tagger

https://www.gnu.org/licenses/gpl-3.0.en.html
https://www.gnu.org/licenses/gpl-3.0.en.html
https://github.com/jtourille/yaset
https://github.com/glample/tagger


194

NeuroNER4 (Dernoncourt et al., 2017) targets
non-expert users and is based on Lample’s Bi-
LSTM model. The authors intended to make the
tool easy to use by providing automatic format
conversion from the brat format (Stenetorp et al.,
2012) to the input format and from the output for-
mat to the brat format. The tool produces sev-
eral plots during training for performance analy-
sis. It is implemented in Python and makes use
of the TensorFlow library. Both implementations
of the Bi-LSTM model suffer from a very long
training time which makes them cumbersome to
use. YASET offers a faster implementation of the
model by allowing mini-batch training and by us-
ing the pipeline API of TensorFlow.

Rei and Yannakoudakis (2016) released a
Python implementation5 of different models pre-
sented in their works (Rei and Yannakoudakis,
2016; Rei et al., 2016; Rei, 2017). One major dif-
ference with YASET resides in the possibility to
use a language modeling objective during training.

Recently, Yang and Zhang (2018) introduced
NCRF++6, a tool presented as the neural ver-
sion of CRF++7 and implemented with Py-
Torch (Paszke et al., 2017). The tool is very close
to YASET, with the possibility to define hand-
crafted word features and to perform nbest decod-
ing.

Another type of neural network model, based
on Convolutional Neural Networks (CNNs) for
character level representation, is presented in Ma
and Hovy (2016). The authors implemented the
architecture with PyTorch. The code is freely
available online8. The same type of architecture
is implemented in the tool released by Reimers
and Gurevych (2017). It is implemented with
Keras (Chollet et al., 2015) and is freely available
online9.

Outside the peer-reviewed scientific environ-
ment, many other implementations are freely
available online. However, we will not review
them in this paper.

4https://github.com/
Franck-Dernoncourt/NeuroNER

5https://github.com/marekrei/
sequence-labeler

6https://github.com/jiesutd/NCRFpp
7https://taku910.github.io/crfpp
8https://github.com/XuezheMax/

NeuroNLP2
9https://github.com/UKPLab/

emnlp2017-bilstm-cnn-crf

3 Neural Network Model

There is currently one neural network model im-
plemented in YASET. This model is mostly based
on Lample et al. (2016). However, similar ar-
chitectures are presented in other work (Collobert
et al., 2011; Ma and Hovy, 2016; Rei and Yan-
nakoudakis, 2016; Rei et al., 2016). Other network
architectures will be implemented in the future.

3.1 Main Component
Our approach relies on LSTMs. The architecture
of our model is presented in Figure 1. For a given
sequence of tokens, represented as vectors, we
compute representations of left and right contexts
of the sequence at every token. These represen-
tations are computed using two LSTMs (forward
and backward LSTM in Figure 1).

Figure 1: YASET neural network architecture
overview. The example is extracted from the THYME
corpus (Styler IV et al., 2014) used during the shared
tasks Clinical TempEval (Bethard et al., 2015, 2016,
2017). One of the objectives was to extract medical
events. In the example, the event Surgery is marked as
a medical event with the type N/A.

These representations are concatenated and
passed through a tanh activation layer whose size
is equal to the size of the concatenated vector.

Finally, the output of the last layer is linearly
projected to a n-dimensional vector representing
the number of categories. Following previous
work (Dernoncourt et al., 2017; Lample et al.,
2016; Ma and Hovy, 2016), we add a final Condi-
tional Random Field (CRF) layer to take into ac-
count the previous label during training and pre-
diction.

https://github.com/Franck-Dernoncourt/NeuroNER
https://github.com/Franck-Dernoncourt/NeuroNER
https://github.com/marekrei/sequence-labeler
https://github.com/marekrei/sequence-labeler
https://github.com/jiesutd/NCRFpp
https://taku910.github.io/crfpp
https://github.com/XuezheMax/NeuroNLP2
https://github.com/XuezheMax/NeuroNLP2
https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf
https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf


195

3.2 Input Embeddings
Vectors representing tokens are built by concate-
nating a character-based embedding, a word em-
bedding and one embedding per feature.

An overview of the embedding computation is
presented in Figure 2. Following Lample et al.
(2016), the character-based representation is com-
puted with a Bi-LSTM whose parameters are de-
fined by users. First, a random embedding is
generated for every character in the training cor-
pus. Token characters are then processed with a
forward and backward LSTM architecture. The
final character-based representation results from
the concatenation of the forward and backward
representations. The character-based representa-
tion is then concatenated to a pre-trained word-
embedding and one embedding per feature. We
apply dropout on the final input embedding.

Figure 2: YASET input embedding computation
overview. Embeddings result from the concatenation
of a pre-trained word embedding, a character-level rep-
resentation of the token and one embedding per cate-
gorical feature.

4 Tool Overview

In this section, we present a general overview of
YASET. First we describe input data formats (se-
quences and pre-trained word embeddings). Then
we present the input pipeline, the network training
phase, the implemented evaluation metrics and the
management of its parameters.

4.1 Input and Output Data
YASET takes CoNLL-like10 formatted files as in-
put. Sequences are separated by empty lines and
there must be one token per line. For each token,

10http://universaldependencies.org/
docs/format.html

the first and last columns are reserved namely for
the token itself and the token label. Each token
must have a label.

Users can add several categorical features. Fea-
ture columns must be specified in the configura-
tion file by providing their indexes. Besides the
first and last columns, and the feature columns,
users can add other columns. They will be ignored
by the system.

YASET can take training and development files
as input but can also create development instances
if there is no development file provided by users.
In this case, users can specify the train/dev split ra-
tio and may specify a random seed for experiment
reproducibility.

Train and development instances consistency is
checked upon startup. Each label and feature val-
ues from the development instances must appear
in the train instances. An example of YASET in-
put file format is presented in Figure 3.

In prediction mode, test data is supplied in
the same format, without the token label column,
which will be added with the predicted labels.
...
Lien NNP I-NP I-PER
. . O O

China NNP I-NP I-LOC
says VBZ I-VP O
time NN I-NP O
right RB I-ADVP O
for IN I-PP O
Taiwan NNP I-NP I-LOC
talks NNS I-NP O
. . O O

BEIJING VBG I-VP I-LOC
1996-08-22 CD I-NP O
...

Figure 3: Example extracted from the CoNLL 2003
shared task corpus (Sang and Meulder, 2003). For each
token (col. #1), there is a label (col. #4, IOB format)
and two categorical features (cols. #2 and #3), the part-
of-speech tag and a chunk label (IOB format).

4.2 Word Embeddings
YASET supports embeddings in the
word2vec (Mikolov et al., 2013b) or Gen-
sim (Řehůřek and Sojka, 2010) formats. Other
formats of embeddings, for instance FastText (Bo-
janowski et al., 2017) or Glove (Pennington et al.,
2014), must first be converted to either accepted
format.

http://universaldependencies.org/docs/format.html
http://universaldependencies.org/docs/format.html


196

Out Of Vocabulary (OOV) tokens can be ad-
dressed by two strategies. In the first one, users
provide a vector for OOV tokens. In this case,
the vector is expected to be part of the provided
word embedding matrix and users must specify the
vector ID. In the second strategy, we follow the
methodology described in Lample et al. (2016).
We insert a randomly initialized vector in the em-
bedding matrix that will be used for OOV tokens.
Then we replace singletons in the training corpus
by the OOV token with a probability of p, which
is defined by users. Word embeddings can be fine-
tuned during training in both cases by setting the
appropriate flag to true in the configuration file.

4.3 Input Pipeline
Minimizing the memory footprint was one of the
core objectives during the development of the tool.
YASET leverages the pipeline API of TensorFlow
to build a lightweight and fast input pipeline. In-
put instances are stored in the binary TensorFlow
format. This avoids the need to store the whole
dataset in memory. Mini-batches are extracted
randomly and fed to the network.

Training instances can be bucketized according
to their lengths. This possibility can speedup train-
ing with large corpora. Buckets boundaries are
automatically computed. To assert effective ran-
domization during training, buckets will contain
enough instances for several mini-batches.

4.4 Neural Network Training
YASET alternates between two phases during
training. In the iteration phase, mini-batches are
extracted from the input pipeline and fed to the
neural network model. Optimization is performed
according to the parameters set by users in the con-
figuration file.

At the end of each iteration, YASET enters the
evaluation phase where the current model state is
used to make predictions on the development in-
stances. The performance score is logged for fur-
ther analysis and a snapshot of the model is kept
if the performance score is the best obtained so far
on the development instances. Since model snap-
shots can take a lot of memory, we only keep the
best one, i.e. the one that performs best on the de-
velopment instances.

Network training is performed via back-
propagation. Users can select the neural network
model architecture (only one available for now),

the maximum number of iterations n and the pa-
tience criterion p. Training will stop if the max-
imum number n is reached or if there are p iter-
ations without performance improvement on the
development instances.

Users can also set several parameters related to
the learning algorithm such as the initial learning
rate, and gradient clipping and exponential decay
factors. Finally, users can select the mini-batch
size, the number of CPU cores to use and the eval-
uation metric.

Another set of parameters are related to the neu-
ral network model. These parameters allow to se-
lect the dropout rate and the different hidden layer
and embedding sizes.

4.5 Metrics
Model performance is measured on the develop-
ment instances at the end of each iteration. Two
metrics are currently implemented in YASET: ac-
curacy and CoNLL. The former measures the frac-
tion of correctly predicted labels among all the
predicted labels. The latter is an entity-based met-
ric which outputs precision, recall and F1-measure
scores. Precision measures the fraction of cor-
rectly predicted entities among all predicted enti-
ties. Recall assesses the fraction of correctly pre-
dicted entities among all the entities that should
have been detected. F1-measure is the harmonic
mean of precision and recall. The implementa-
tion of the CoNLL metric is inspired by the of-
ficial evaluation script used during the CoNLL-
2003 shared task (Sang and Meulder, 2003) and
by the Python portage of the script by Sampo
Pyysalo11. In the case of performance evaluation
with the CoNLL metric, token labels must follow
either the IOB, IOBE or IOBES tagging scheme12.

4.6 Parameters
YASET parameters are fully customizable and
centralized in one configuration file which is used
to setup experiments. YASET also targets end-
users from a broader community by providing hy-
perparameter value suggestions and insights on
how to choose them for various sequence-tagging
situations.

11https://github.com/spyysalo/
conlleval.py

12Inside, Outside, Begin, End, Single

https://github.com/spyysalo/conlleval.py
https://github.com/spyysalo/conlleval.py


197

5 Experiments

We demonstrate the performance of YASET by
applying the model to four different corpora se-
lected to cover a variety of languages, text gen-
res, sequence types and annotation densities. We
focus our effort on biomedical texts, using Med-
Post (Smith et al., 2004), a corpus of biomedi-
cal abstracts annotated with Part-of-Speech (PoS)
tags, the NCBI Disease corpus (Islamaj Do-
gan and Lu, 2012), a dataset of biomedical ab-
stracts annotated with disease related entities and
MERLoT (Campillos et al., 2018), a corpus of
clinical documents written in French which were
annotated with two different Named Entity Recog-
nition (NER) tag sets (Protected Health Informa-
tion (PHI) and biomedical entities).

We also apply YASET on the CoNLL 2003 En-
glish NER corpus (Sang and Meulder, 2003), a
classic benchmark corpus for NER in the general
domain.

5.1 Presentation of the Corpora
We use the corpus partition provided with the
dataset distributions when available. For NCBI
and CoNLL 2003, models are trained on the train
sets and evaluated on the test sets while the devel-
opment sets are used to determine early stopping.
For MERLoT and MedPost, partition details are
outlined below. The code used to preprocess and
analyze the corpora is available online13.

The MedPost corpus is a collection of tok-
enized MEDLINE abstracts annotated with PoS
tags. It contains 6,701 sentences (≈182,000 to-
kens). The corpus is divided in 13 files of differ-
ent sizes, from which we extracted one file to serve
as development set (tag mb.ioc). The tag set con-
tains originally 63 unique entities that we grouped
in a coarser set of 51 entities. Grouping affected
punctuation signs, which were assigned a unique
PUNCT tag.

The NCBI corpus contains 793 MEDLINE ab-
stracts with 11,350 annotations of diseases and
disease modifiers. There are 4 different entities.
Train and development sets contain 6,594 sen-
tences (≈151,000 tokens).

MERLoT is a French clinical corpus with sev-
eral levels of annotations. It contains 500 med-
ical reports with a total of 25,087 sentences
(≈177,000 tokens). We used it for two tasks. The

13https://github.com/strayMat/
bio-medical_ner

first one is de-identification with a tag set com-
prising 11 types of PHI (e.g. names, dates). The
second one is medical NER with 19 entity classes
(e.g. anatomy, disorder).

The medical entity annotations include nested
entities. Because this study aims to experiment
on a variety of datasets using the same model, we
did not attempt to extract several levels of nested
entities. When nested entities occur, our experi-
ments only addressed the outer layer correspond-
ing to the largest entity. For example, the mention
“cancer du sein” (breast cancer) was originally
annotated with one DISORDER entity (cancer du
sein, breast cancer) and one ANATOMY entity
(sein, breast). Nested entity removal reduced the
annotations to only one tag per token and in this
case, the DISORDER annotation was kept while
the ANATOMY annotation was removed. In to-
tal, 5,218 entities (8.4% of the complete set) were
pruned. For our experiments, we also removed
the headers and footers of the documents, which
were not available to annotators working towards
the gold standard and could cause ambiguities
with some entity classes (e.g. person or hospi-
tal). This results in a smaller corpus, compared to
the corpus used for de-identification experiments
(≈123,000 tokens).

The CoNLL 2003 corpus is a common dataset
for evaluating NER algorithms. It is based on the
Reuters corpus (Lewis et al., 2004) and is the only
dataset outside the biomedical domain used in our
experiments. The training and development set
together contain 18,451 sentences (≈256,000 to-
kens) annotated with 4 unique entities.

A detailed overview of the corpus characteris-
tics is available in Table 1.

5.2 Experimental Setup
In this section, we present the experimental setup
used for our experiments. First, we describe how
we selected the hyper-parameters. Then, we detail
the pre-trained word embeddings that were used in
this work. Finally, we present the neural network
training parameters.

5.2.1 Hyper-parameter Selection
We used Hyperopt14 (Bergstra et al., 2011, 2013)
to select a set of hyper-parameters that performed
well on the MERLoT de-identification dataset.
Due to heavy computation times, we re-used this

14http://hyperopt.github.io/hyperopt/

https://github.com/strayMat/bio-medical_ner
https://github.com/strayMat/bio-medical_ner
http://hyperopt.github.io/hyperopt/


198

Corpus # sent. # tok. # ann. Entities

Name: MedPost
Task: POS
Domain: MEDLINE abstracts

6,701 182,319 182,319
(100%)

51 POS tagging detailed in Smith et al. (2004)

Name: NCBI disease
Task: NER
Domain: MEDLINE abstracts

7,279 151,005 11,350
(7.5%)

DiseaseClass, SpecificDisease, Composite-
Mention, Modifier

Name: MERLoT medical
Task: NER
Domain: Medical reports from the
Hepato-gastro-enterology and Nutrition
ward

5,137 123,942 56,680
(46%)

Concept Idea, MedicalProcedure, Hospital,
Persons, Temporal, BiologicalProcessOrFunc-
tion, Devices, Measurement, Disorder, As-
pect, Chemicals Drugs, Dosage, SignOrSymp-
tom, Anatomy, Localization, Livingbeings,
Strength, AdministrationRoute, Drugform

Name: MERLoT de-identification
Task: NER
Domain: Same as MERLoT medical

25,599 177,158 31,723
(18%)

first name, last name, initials, address, zip code,
town, date, hospital, identifier, phone number,
email

Name: CoNLL 2003
Task: NER
Domain: News articles from the Reu-
ters corpus

18,451 256,145 42,646
(17%)

PER, ORG, LOC, MISC

Table 1: Overview of the corpora. The number of annotations corresponds to the number of annotated tokens to
be comparable to the size of the corpus measured in number of tokens

set of hyper-parameters on the other datasets. Pa-
rameter search space includes the number of units
of the character Bi-LSTM, the number of units
of the main Bi-LSTM, the dimension of the char-
acter embeddings and the dropout rate. The re-
tained setup uses character embeddings of size 24,
32 units for the character-level Bi-LSTM, 64 units
for the main Bi-LSTM and a dropout rate of 0.5.

5.2.2 Embeddings
Pre-trained word embeddings were shown to boost
the performance in various NLP tasks (Collobert
et al., 2011; Mikolov et al., 2013a) and specifically
in NER (Lample et al., 2016; Dernoncourt et al.,
2017).

For each corpus, we used pre-trained word em-
beddings created with datasets that were consis-
tent in genre and domain with the corpora used
in this work. All word embedding models were
computed using word2vec. For CoNLL 2003, we
trained a model on Google News. For NCBI dis-
ease and MedPost, which both comprise abstracts
from biomedical articles, we trained a model on
the PubMed corpus. For MERLoT, we trained a
model on the corpus from which MERLoT is ex-
tracted. It contains about 138,000 clinical notes
written in French (Campillos et al., 2018).

For each of these word vector models, we com-
puted low-dimensional representations by apply-
ing Principal Component Analysis (PCA) on the
original high-dimensional word vectors (300 di-

mensions). During hyper-parameter search, we
varied the dimension of the word embeddings and
observed important impacts on the performance
(up to 2.5 points of F1). We finally picked word
vectors of dimension 25 (reduced by PCA), which
showed to be the most efficient. This choice im-
proved the scores as well as diminished drastically
the computation times.

We also ran a few experiments using Fast-
Text (Bojanowski et al., 2017) embeddings
trained on Wikipedia. Preliminary results on
CoNLL 2003 show a major performance improve-
ment. An analysis of the impact of the word em-
bedding model on performance will be conducted
in following work.

5.2.3 Training
In all our experiments, the network was trained us-
ing the Adam optimizer (Kingma and Ba, 2015)
with an initial learning rate of 0.001. Early-
stopping was set to 10 epochs.

6 Results

In this section, we present the performance ob-
tained on the corpora using the experimental setup
described in the previous section. First, we report
corpus-specific results. Then, we study the impact
of the corpus size on performance.



199

Figure 4: Distributions of F1 scores for the four corpora over 30 trainings for each corpus. Vertical lines refer to
each sample. The scales are specific to each dataset for a proper visualization of each distribution.

6.1 Corpus Specific Results
Reimers and Gurevych (2017) report that neu-
ral network model training is highly non-
deterministic and is subject to the random seed
choice. Because of this variability during the train-
ing phase, it is crucial to report results on numer-
ous trainings. Therefore we ran 30 experiments
for each task presented in this work. We report F1-
score statistics in Table 2. We also plot F1-score
distributions for each task in Figure 4.

Our experiments show that performance varies
across datasets, reflecting the heterogeneous diffi-
culties of the tasks, inherent to the nature of the
labels and the quality of the annotations. We no-
tice that the standard deviations are similar for all
NER tasks. It suggests that the variability of the
score is independent from the task and mainly due
to the model architecture. Previous work perfor-
mances are reported in Table 3 for every dataset.

Task Dataset Mean F1 σ Max. Diff.

NER NCBI disease 81.33 0.83 3.27 (4.1%)
NER MERLoT medical 82.87 0.36 1.40 (1.7%)
NER CoNLL 2003 87.31 0.41 1.76 (2.0%)
POS MedPost 97.83 0.39 1.44 (1.5%)
NER MERLoT de-identification 99.40 0.14 0.568 (0.57%)

Table 2: Performance (%) of YASET on the 4 datasets.

Although our model does not show state-of-the-art
performances for all of them, it obtains competi-
tive results, demonstrating the generalization abil-
ity of the selected shared set of hyper-parameters
and embeddings.

Dataset Model F1

NCBI
Dang et al. (2018) 84.41
Islamaj Dogan and Lu (2012) 81.80
This paper 81.33

MERLoT med.a Campillos et al. (2018) 81.40This paper 82.87

CoNLL 2003

Lample et al. (2016) 90.94
Ma and Hovy (2016) 91.21
Yang and Zhang (2018) 91.35
Peters et al. (2018) 92.22
This paper 87.31

MedPosta Smith et al. (2004) 97.43This paper 97.83

MERLoT PHI Grouin and Névéol (2014) 94.00This paper 99.40

a Corpora where the experimental set-up differed between
our experiments and that of prior work. For MEDPOST,
we used 51 categories instead of 63; for MERLoT med.
we removed nested entities.

Table 3: State of the art results obtained in prior work
on the datasets.



200

Figure 5: Effect of the training set size on the F1 score for the MERLoT dataset with de-identification annotations.
These scores are computed over 6 training iterations per chunk. Vertical bars show the standard deviation σ.

6.2 Performance According to Training Data
Size

The development of a labeled dataset for training
annotation models is often a heavy investment in
time and resources. Having some insight on the
performance of the model for different training
data sizes is crucial. Thus, we investigate the im-
pact of this parameter on model performance. Fo-
cusing on the de-identification task of MERLoT,
we split the training set into 20 subsets of equal
sizes. Then, we built sub-training sets of grow-
ing size, that we refer as chunks. This resulted in
20 chunks where small chunks are subsets of the
bigger one. Finally, we train YASET 6 times on
each chunk, measuring how the performance im-
proves with the size of the chunks. Figure 5 shows
the progression of the F1-score according to the
number of tokens. Table 4 presents the model per-
formance according to the dataset size.

Dataset 5% 10% 25% 50% 100%

MERLoT PHI 91.79(0.55)
95.86
(0.47)

97.60
(0.30)

98.41
(0.33)

99.06
(0.19)

Table 4: Performance (F1) against train set size (as per-
centage of the total training set indicated on the first
row). Standard errors appear between parentheses.

We observe that the performance improves log-
arithmically with every chunk added in the train-
ing data as shown in Table 4. This finding is sim-
ilar to the observation of Sun et al. (2017) for vi-
sion tasks. Further addition of data will slightly
improve the performance as the maximum perfor-
mance plateau is almost reached.

7 Conclusion and Future work

We propose an easy-to-use annotation tool im-
plementing a state-of-the-art Bi-LSTM-CRF neu-
ral architecture. We apply the tool to PoS tag-
ging and NER on clinical, biomedical and gen-
eral domain texts. By running multiple experi-
ment for each dataset, we confirm previous obser-
vations that neural network training is highly non-
deterministic and dependent on the random seed
choice.

Although we did not search for optimal hyper-
parameters for each task, we obtain high perfor-
mance in all our experiments, suggesting that tai-
loring hyper-parameters for a specific task im-
proves only lightly the performance of the model
and the neural network architecture implemented
in YASET is robust with regards to the perfor-
mance obtained.

Concerning the impact of the training dataset
size on model performance, we confirm the intu-
ition that adding more data allows to improve the
performance of neural network models. However,
result analysis suggests that the growth is logarith-
mic with the training data size.

One limitation of our model is its inability to di-
rectly handle nested entities such as those found in
the MERLoT medical dataset and commonly used
in the biomedical and clinical domains. When
filtering these nested entities, specific classes are
heavily impacted, including anatomy, disorder, lo-
calization, and medical procedure entities. Sev-
eral strategies have been proposed to handle such
cases. Campillos et al. (2018) present a 3-
layer CRF model that annotates different non-



201

overlapping clinical entities at each layer. Ju et al.
(2018) present a dynamic end-to-end neural net-
work model capable of handling an undetermined
number of nesting levels. Katiyar and Cardie
(2018) model the task as an hypergraph whose
structure is learned with an LSTM network.

Future research will focus on the influence of
word embedding models which were shown to sig-
nificantly impact on performance. Specifically,
models taking into account sub-token informa-
tion (Bojanowski et al., 2017) or emphasizing con-
text (Peters et al., 2018) should be further ex-
plored. Moreover, other neural network models
for NER such as the ones proposed by Rei et al.
(2016) and Ma and Hovy (2016) will be investi-
gated and implemented in YASET. Having a cen-
tralized implementation of different NER models
will allow us to compare their performances on
various corpora.

Acknowledgements
The authors thank the Biomedical Informatics De-
partment at the Rouen University Hospital for
providing access to the LERUDI corpus for this
work. This work was supported in part by the
French National Agency for Research under grant
CABeRneT ANR-13-JS02-0009-01 and by Labex
Digicosme, operated by the Foundation for Scien-
tific Cooperation (FSC) Paris-Saclay, under grant
CÔT.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, et al.

2015. TensorFlow: Large-Scale Machine Learning
on Heterogeneous Systems.

Rami Al-Rfou, Guillaume Alain, Amjad Almahairi,
et al. 2016. Theano: A Python framework for fast
computation of mathematical expressions. CoRR.

James Bergstra, Daniel Yamins, and David Cox. 2013.
Making a Science of Model Search: Hyperparame-
ter Optimization in Hundreds of Dimensions for Vi-
sion Architectures. In Proceedings of the 30th In-
ternational Conference on Machine Learning, pages
115–123.

James S. Bergstra, Rémi Bardenet, Yoshua Bengio, and
Balázs Kégl. 2011. Algorithms for Hyper-Parameter
Optimization. In Advances in Neural Information
Processing Systems 24, pages 2546–2554.

Steven Bethard, Leon Derczynski, Guergana Savova,
James Pustejovsky, and Marc Verhagen. 2015.

SemEval-2015 Task 6: Clinical TempEval. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, pages 806–814. Association for
Computational Linguistics.

Steven Bethard, Guergana Savova, Wei-Te Chen, et al.
2016. SemEval-2016 Task 12: Clinical TempEval.
In Proceedings of the 10th International Workshop
on Semantic Evaluation, pages 1052–1062. Associ-
ation for Computational Linguistics.

Steven Bethard, Guergana Savova, Martha Palmer,
and James Pustejovsky. 2017. SemEval-2017 Task
12: Clinical TempEval. In Proceedings of the
11th International Workshop on Semantic Evalua-
tion, pages 565–572. Association for Computational
Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching Word Vectors with
Subword Information. Transactions of the Associa-
tion of Computational Linguistics, 5:135–146.

Leonardo Campillos, Louise Deléger, Cyril Grouin,
et al. 2018. A French Clinical Corpus with Com-
prehensive Semantic Annotations: Development of
the Medical Entity and Relation LIMSI annOtated
Text corpus (MERLOT). Language Resources and
Evaluation, 52(2):571–601.

François Chollet et al. 2015. Keras. https://
github.com/fchollet/keras.

Ronan Collobert, Jason Weston, Léon Bottou, et al.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493–2537.

Thanh Hai Dang, Hoang-Quynh Le, Trang M Nguyen,
and Sinh T Vu. 2018. D3NER: biomedical named
entity recognition using CRF-biLSTM improved
with fine-tuned embeddings of various linguistic in-
formation. Bioinformatics.

Franck Dernoncourt, Ji Young Lee, Özlem Uzuner, and
Peter Szolovits. 2017. De-identification of Patient
Notes with Recurrent Neural Networks. Journal
of the American Medical Informatics Association,
24(3):596–606.

Cyril Grouin and Aurélie Névéol. 2014. De-
identification of clinical notes in French: towards a
protocol for reference corpus development. Journal
of Biomedical Informatics, 50:151–161.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Rezarta Islamaj Dogan and Zhiyong Lu. 2012. An
improved corpus of disease mentions in PubMed
citations. Proceedings of the 2012 Workshop on
Biomedical Natural Language Processing, pages
91–99.

https://github.com/fchollet/keras
https://github.com/fchollet/keras


202

Meizhi Ju, Makoto Miwa, and Sophia Ananiadou.
2018. A Neural Layered Model for Nested Named
Entity Recognition. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1446–1459. Associ-
ation for Computational Linguistics.

Arzoo Katiyar and Claire Cardie. 2018. Nested Named
Entity Recognition Revisited. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 861–871. Asso-
ciation for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural Architectures for Named Entity Recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270. Association for Computational Lin-
guistics.

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A New Benchmark Collection for
Text Categorization Research. Journal of Machine
Learning Research, 5:361–397.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end Se-
quence Labeling via Bi-directional LSTM-CNNs-
CRF. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1064–1074. Association for Computa-
tional Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Adam Paszke, Sam Gross, Soumith Chintala, et al.
2017. Automatic differentiation in PyTorch. In
NIPS-W.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Proceedings of the 14th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 1532–
1543. Association for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, et al.
2018. Deep Contextualized Word Representations.
In Proceedings of the 2018 Conference of the North

American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 2227–2237. Association for Computa-
tional Linguistics.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50. Eu-
ropean Language Resources Association.

Marek Rei. 2017. Semi-supervised Multitask Learning
for Sequence Labeling. In Proceedings of the 55th
Conference of the Association for Computational
Linguistics, pages 2121–2130. Association for Com-
putational Linguistics.

Marek Rei, Gamal Crichton, and Sampo Pyysalo. 2016.
Attending to Characters in Neural Sequence La-
beling Models. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics,
pages 309–318.

Marek Rei and Helen Yannakoudakis. 2016. Com-
positional Sequence Labeling Models for Error De-
tection in Learner Writing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1181–1191. Association
for Computational Linguistics.

Nils Reimers and Iryna Gurevych. 2017. Reporting
Score Distributions Makes a Difference: Perfor-
mance Study of LSTM-networks for Sequence Tag-
ging. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 338–348.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-Independent Named Entity Recog-
nition. In Proceedings of the 7th Conference on Nat-
ural Language Learning. Association for Computa-
tional Linguistics.

Laurence H. Smith, Thomas C. Rindflesch, and
W. John Wilbur. 2004. MedPost: a Part-of-
speech Tagger for BioMedical Text. Bioinformatics,
20(14):2320–2321.

Pontus Stenetorp, Sampo Pyysalo, Goran Topić, et al.
2012. brat: a Web-based Tool for NLP-Assisted
Text Annotation. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, volume Demonstra-
tion Papers, pages 102–107. Association for Com-
putational Linguistics.

William F. Styler IV, Steven Bethard, Sean Finan, et al.
2014. Temporal Annotation in the Clinical Domain.
Transactions of the Association for Computational
Linguistics, 2:143–154.

Chen Sun, Abhinav Shrivastava, Saurabh Singh, and
Abhinav Gupta. 2017. Revisiting Unreasonable Ef-
fectiveness of Data in Deep Learning Era. CoRR,
abs/1707.02968.



203

Jie Yang and Yue Zhang. 2018. NCRF++: An
Open-source Neural Sequence Labeling Toolkit. In
Proceedings of ACL 2018, System Demonstrations,
pages 74–79. Association for Computational Lin-
guistics.


