



















































Mining Evidences for Concept Stock Recommendation


Proceedings of NAACL-HLT 2018, pages 2103–2112
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Mining Evidences for Concept Stock Recommendation

Qi Liu and Yue Zhang
Singapore University of Technology and Design,

8 Somapah Road, Singapore 487372
{qi liu, yue zhang}@sutd.edu.sg

Abstract

We investigate the task of mining relevant
stocks given a topic of concern on emerg-
ing capital markets, for which there is lack
of structural understanding. Deep learning is
leveraged to mine evidences from large scale
textual data, which contain valuable market in-
formation. In particular, distributed word sim-
ilarities trained over large scale raw texts are
taken as a basis of relevance measuring, and
deep reinforcement learning is leveraged to
learn a strategy of topic expansion, given a
small amount of manually labeled data from fi-
nancial analysts. Results on two Chinese stock
market datasets show that our method outper-
forms a strong baseline using information re-
trieval techniques.

1 Introduction

Stock prices are affected by events. For exam-
ple, recent announcement of a state plan to build
a new economic region, Xiong’an near Beijing,
by the Chinese government has led to the rise of
hundreds of stocks, which can directly or indi-
rectly benefit from the plan. As a second example,
the winning of a lawsuit against IP (Intellectual
Property) breach can strengthen investors’ confi-
dence on technological and entertainment com-
panies. We refer to the topics or themes of such
events (e.g. Xiong’an and IP) as concepts and their
relevant stocks as concept stocks. Given a news
event, it can be highly useful for investors to find
a list of relevant concept stocks for making invest-
ment decisions.

For popular concepts, lists of relevant concept
stocks can be found from analyst reports from
financial websites. On the other hand, concepts
are dynamic and flexible. In addition, insights can
be relatively scarce for emerging capital markets,
such as the Chinese market, which had been closed
to foreign investments before 2015. It is therefore

雄安
Xiong’an

白洋淀
Baiyangdian

安新县
Anxin county

容城
Rongcheng

保定市
Baoding	city

河北省
Hebei	province

冀东
East Hebei

京津冀
Beijing-Tianjin-
Hebei

河北省政府
Hebei	government

石家庄市
Shijiazhuang	city

京津冀一体化
Coordinated	development	
of	Beijing-Tianjin-Hebei

经济带
Economic district

滨海新区
Binhai new	area

Figure 1: Concept relatedness. A ⇒ B indicates that
B is one of the 3 most related concepts to A.

a challenging research question how to automat-
ically find out potentially relevant stocks given a
topic of interest, from a large market of multi-
thousand equities.

Intuitively, evidences between concepts and
stocks exist in text documents over the Internet.
For example, news articles report events and com-
panies involved. In addition, company filings such
as annual/quarter reports contain factual knowl-
edge about stocks, which can also be useful back-
ground information. For example, knowing that a
company invests heavily on research is useful for
correlating the company with IP-protection laws.
Such evidence-mining process can involve mul-
tiple steps. As shown in Figure 1, starting from
concept, Xiong’an, one might learn that the new
economical region is located in the Baiyangdian
area, which is further located in Hebei province.
By further reading, one can infer that the new eco-
nomic region is related with the coordinated devel-
opment plan for the Beijing-Tianjin-Hebei region,
and therefore benefit a wider range of stocks.

Based on the intuition above, we build a neu-
ral model for mining evidences for concept stock
recommendation. The basis of our model is dis-
tributed similarities between concepts and stocks,
obtained from embeddings trained over large-scale
raw documents. Embedding similarities encode
correlations from direct narrative evidence within
context windows. To further include a multi-step

2103



evidence mining, we build an iterative model for
concept expansion, augmenting a given concept
by iteratively adding more relevant concepts from
background documents. As demonstrated in Fig-
ure 1, this process can be ambiguous, since there
can be multiple directions for further reading given
a set of concepts. We leverage a small amount of
manually labeled data, downloaded from financial
analysis websites, for guiding evidence mining.

In particular, we take a reinforcement learning
method, which regards the evidence mining pro-
cess as a decision process. The starting point is
a given input concept, such as Xiong’an or Elec-
tronic Vehicle. At each step, a decision is made to
stop further reading, or to continue adding related
concepts to the set of concepts being considered.
Existing concepts can also be removed from fur-
ther consideration. Documents that discuss each
concept are used to support the decision. After the
process stops, relevant stocks to the set of concepts
are recommended. The decision process is guided
using a neural network model structure, trained
with a loss function over the quality of the finally
recommended stocks.

Results on two Chinese datasets show that our
method outperforms a strong ranking-based base-
line, which utilizes only direct evidences. Our
method can be easily adapted for other markets
given the availability of a small amount of train-
ing data. Our code is released1.

2 Related Work

Our work is related to information retrieval and
query expansion, where a concept can be regarded
as a query and relevant stocks can be regarded as
retrieved results. We rely on external evidence for
correlating concepts and stocks.

Ranking is an important problem in informa-
tion retrieval. We focus on ranking using neu-
ral models here. One line of work (Shen et al.,
2014a,b) models queries and documents using
convolutional neural network and ranks the doc-
uments pair-wise or list-wise. Another related
method (Cao et al., 2015) adopts recursive neu-
ral networks to rank sentences for multi-document
summarization. These methods requires massive
annotated data, which is expensive to obtain for
concept stock recommendation.

Query Expansion: One line of work (Cao
1https://github.com/leuchine/

concept-stock-recommendation

et al., 2008; Preston and Colman, 2000) uti-
lizes a feedback-based relevance model to expand
queries. Another line applies language modeling
to estimate conditional probabilities of concepts
given a query, and expands the query with the most
probable concepts (Bai et al., 2005; Carpineto and
Romano, 2012). Recently, word embeddings are
adopted for query expansion (Kuzi et al., 2016;
Diaz et al., 2016). Our framework belongs to this
line of work with a difference that we use rein-
forcement learning to dynamically expand queries
instead of following handcrafted rules such as us-
ing k-nearest neighbors.

Reinforcement Learning: Our work aligns
with existing work using reinforcement learning
to collect evidences. Narasimhan et al. (2016)
utilize external evidence to improve information
extraction. While the work requires handcrafted
features, our model uses dense embedding fea-
tures. Athukorala et al. (2016) devise an interac-
tive search engine balancing exploration and ex-
ploitation. Their work relies on user interaction
to make decisions. In contrast, our work does
not rely on active feedback, which can be ex-
pensive to obtain under our settings. Rodrigo and
Cho (2017) introduce a query reformulation sys-
tem based on reinforcement learning that rewrites
a long and complex query to maximize the number
of relevant documents returned. Differently, we do
not assume complex queries and focus on recom-
mending relevant stocks in our system. Zhong et
al. (2017) solves a different problem, i.e. trans-
lating natural language questions to corresponding
SQL queries.

3 Problem Definition

Our task is to find stocks relevant to a concept
according to a variety of data sources, such as
news, tweets and company files. Formally, given
a concept c, a set of m stocks {oi}mi=1 and n
data sources {Si}ni=1, where each Si is a set
of documents {Dij}

|Si|
j=1 and each D

i
j is a se-

quence of words w1, w2...w|Dij |, we assume the
relevant stocks of the concept are revealed in the
data sources (e.g. we discover PetroChina as a
concept stock of ‘petroleum’ from the document
‘PetroChina acquires Keppel’s entire stake in Sin-
gapore Petroleum’) and the task is to automati-
cally discover these relations and select a subset
of stocks as c’s concept stocks based on the data
sources {Si}ni=1.

2104



Bi-LSTM Bi-LSTM.	.	.

Evidence	Document	{F1c,o} Evidence	Document	{Fnc,o}

Concept	c

.	.	.

Softmax

E1o

.	.	.

Embedding

Eno

.	.	.

Data	Source	S1 Data	Source	Sn.	.	.
Stock-concept		
Pair	(c,	o)

(a) Ranking Baseline

[c, [ce]]

Global	Embedding	E

.	.	.Bi-LSTM Bi-LSTM.	.	.

.	.	.

Linear	Layer

Bi-LSTM
Query

Reward

Q(z,	a;	θ)

Envn

.	.	.

Eiv1
Evidences	Documents{Fncontext}Evidences	Documents	{F1context}

Data	Source	S1 Data	Source	Sn.	.	.([c, [ce]], vi) 

.	.	.

(b) Reinforcement Learning for Query Expansion

Figure 2: Concept Stock Recommendation Models

4 Representation

Motivated by the success of embedding-based
models (Mikolov et al., 2013; Pennington et al.,
2014) in capturing semantic regularities, we use
embeddings to represent concepts, stocks and doc-
uments.

In particular, we adopt Chinese word segmenta-
tion (Yang et al., 2017) to obtain words from doc-
uments. Doc2Vec (Le and Mikolov, 2014) is then
used on the documents of each data source Si to
obtain a local word embedding matrix Ei and a
local document embedding matrix F i, where each
column of Ei (F i) corresponds to a word (doc-
ument) vector representation of Si. In particular,
we use embeddings, Eic and E

i
o as the local con-

cept representation of c and the local stock repre-
sentation of o in data source Si, respectively. Fur-
thermore, we obtain a global word embedding ma-
trix E by averaging the local embedding matrices,
E1...En, where Ec and Eo are regarded as the
global concept representation of c and the global
stock representation of o, respectively.

We propose a ranking baseline and a reinforce-
ment learning model for query expansion based on
these representations.

5 Ranking Baseline

Inspired by Shen et al. (2014a; 2014b), our rank-
ing baseline discriminatively projects the repre-
sentations of concepts and evidences of stocks into
a semantic space for measuring their relevances.

Mining Evidences: Formally, given a concept c
and a stock o, we consult the data sources, retriev-
ing the set of documents {Dic,o} most relevant to
(c, o) from each data source Si as evidences.

To obtain evidences, we use c’s local embed-
ding Eic and o’s local embedding E

i
o for repre-

senting the stock-concept pair (c, o). Cosine simi-
larities are calculated between Eic + E

i
o and each

column of F i for measuring the semantic related-
ness of each document to (c, o). Suppose that the
columns are normalized, the scores are calculated
as:

score({Dij}|Si|j=1) = (F i)T (Eic + Eio) (1)

q (q is set as 5 empirically) documents {Dic,o}with
the maximum scores are selected as evidences
from each Si. When |F i| is large, we use approx-
imate k-nearest-neighbor algorithms, namely Lo-
cality Sensitive Hashing (Datar et al., 2004), to im-
prove efficiency.

Learning to Rank o Given c: The overall
framework for measuring relevances is shown in
Figure 2 (a). Given a concept c and stock o, for
each data source Si, the local stock representation
Eio and the local document representations of the
q most relevant documents, denoted as {F ic,o}, are
sequentially fed into Long Short-Term Memory
(LSTM) (Hochreiter and Schmidhuber, 1997) to
acquire a semantic representation of the evidences.

A bidirectional extension (Graves and Schmid-
huber, 2005) is applied to capture semantics
both left-to-right and right-to-left. As a result,
two sequences of hidden states are obtained, i.e.
�
h1,

�
h2...

�
hq+1 and

�
h1,

�
h2...

�
hq+1. We concatenate�

ht and
�
ht at each time step to obtain the final hid-

den states h1, h2...hq+1.
Average pooling (Boureau et al., 2010) is ap-

plied on the hidden states h1, h2...hq+1 to obtain

2105



evidence representation Iic,o,

Iic,o =

∑q+1
t=1 ht
q + 1

(2)

We concatenate all Iic,o with the global concept
representation of c (i.e. the average of local repre-
sentations, E1c ...E

n
c ) and feed the result to a soft-

max layer to obtain the probability of a stock o
being c’s concept stock, denoted as p(o|c). Given
a concept c, all stocks are ranked by the probabili-
ties.

Given a set of gold-standard concept stock data,
supervised learning is conducted to learn p(o|c).
The loss function is defined as:

E(c,o,y)[−log p(o|c)y−log (1−p(o|c))(1−y)] (3)

Here y is 1 when o is a concept stock of c, and
0 otherwise. Equation 3 maximizes p(o|c) (1 −
p(o|c)) when y = 1 (y = 0). AdaGrad (Duchi
et al., 2011) is applied to update parameters.

6 Recommendation via Query Expansion

The ranking baseline can require large amounts of
annotated data to deliver satisfying performance
(Shen et al., 2014a,b), which can be costly. In ad-
dition, the algorithm has to deal with highly im-
balanced datasets, since there are thousands of
stocks in a stock market, but only a few are related
to a concept c, which greatly harms the perfor-
mance of discriminatively trained algorithms (Wu
and Chang, 2003).

We take a different approach, utilizing the same
data sources and representations as the ranking
baseline. To better leverage a small amount of su-
pervision data, we apply reinforcement learning
to expand the query concept c, consulting sup-
porting evidences from the data sources {Si}mi=1.
We leverage embedding similarities as a basis for
concept-stock relatedness. The advantage is that
embeddings can be trained over large scale raw
texts unsupervisedly, without the need for manu-
ally labeled stock lists.

6.1 Direct Semantic Relatedness

Embeddings represent similarities between con-
cepts and stocks if they co-exist literally in a con-
text window during embedding training. As a re-
sult, irrelevant (relevant) stocks are less (more)
similar to the concept c, since they infrequently

(frequently) co-occur, which alleviates the prob-
lem brought by imbalanced datasets in that irrele-
vant stocks can be spotted at ease.

Global representations of c and o is utilized to
obtain a direct relevance score f̂(c, o):

f̂(c, o) = Ec · Eo, (4)

where · denotes the dot product operation. The
stocks are ranked by f̂(c, o) and concept stocks
are these with the maximum cosine similarities.

6.2 Indirect Semantic Relatedness by Query
Expansion

While f̂(c, o) measures direct relevance between
c and o in embedding contexts, we want to find
those o′ that are indirectly relevant to c by reason-
ing as shown in Figure 1. Query expansion (Kuzi
et al., 2016; Diaz et al., 2016) is used to this end.
One naive baseline is expanding the concept cwith
its k-nearest-neighbor concepts, denoted as [ce],
from global matrix E measured by cosine simi-
larity. Relevance between the expanded concepts
[c, [ce]] and o is calculated as:

f̂([c, [ce]], o) = E[c,[ce]] · Eo
= (Ec +

∑

ce∈[ce]
Ece) · Eo (5)

We define E[c,[ce]] as the addition of Ec and each
Ece . The baseline is relatively inflexible since a
fixed number of k expansion concepts are selected
for all c. In contrast, the reasoning procedures
shown in Figure 1 can take an arbitrary number
of steps. Besides, the naive baseline does not in-
corporate supervision, thus being unable to decide
whether the selected concepts are beneficial for
concept stock recommendation.

We use reinforcement learning to tackle this is-
sue, directly learning how to expand queries from
a few labeled cases. Given c, our method works
iteratively, expanding the concept until it expects
further expansions are not desired. For each can-
didate concept to expand c, a decision is made by
the model on whether it will improve, worsen or
have no effect on recommendation accuracies.

Based on these, we model query expansion with
a Markov Decision Process (MDP) to discrimina-
tively select expansion concepts for c to maximize
recommendation accuracies, while requiring much
less training data compared to the ranking base-
line.

2106



The overall framework is shown in Figure 2 (b).
Formally, a MDP is a list [Z,A, T,R], where Z =
{z} is a set of states, A = {a} is a set of actions,
T (z, a) is a transition function, which determines
the next state z′ = T (z, a) after performing action
a on z, and R is a reward function. We describe
each in detail below:

States: Each state z is a list of lists:

z = [[c, [ce]], [v
1, {F 1context}]...[vn, {Fncontext}]],

(6)
where [c, [ce]] consists of the input concept c and
its expansion concept list [ce] so far. In the start
state, [ce] is empty, and thus [c, [ce]] = [c, [ ]].
[vi, {F icontext}] consists of a new candidate con-
cept vi and its supporting evidences {F icontext}.
Since globally trained embeddings underperform
locally trained embeddings (Diaz et al., 2016) for
query expansion, we use local embeddings Ei to
suggest candidate concepts vi, instead of using
global embedding E. vi is obtained by finding the
most similar concept of [c, [ce]] from local em-
beddings Ei. {F icontext} is the document repre-
sentations of the q most relevant documents to
([c, [ce]], v

i) as evidences. Formally, {F icontext} is
the document representations of the q documents
with the maximum scores, score′({Dij}

|Si|
j=1) =

(F i)T (Ei[c,[ce]] + E
i
vi
). As a result, at each state,

we have n candidate concepts v1...vn. The neural
agent chooses at most one concept to be added to
[ce] based on the evidences.

Action: The agent can take four types of ac-
tions. (1) Add one of n candidate concepts to [ce]
(2) Reject all n candidate concepts. (3) Remove
the last added concept from [ce] (4) Stop the pro-
cess.

State Transition: After taking an action a on
a state z, a new state z′ is yielded by the tran-
sition function T (z, a). If one of the candidate
concepts vi is chosen, vi is added to [ce]. In ad-
dition, the new state z′ is obtained by updat-
ing [v1, {F 1context}]...[vn, {Fncontext}] by finding
the most similar concepts of the new [c, [ce]′]
and their supporting evidences among the local
embeddings. If action (2) is chosen, [c, [ce]] re-
mains, while the v1...vn is replaced with the sec-
ond most similar concepts of [c, [ce]] among the
local embeddings, and the process repeats until
action (1), (3) or (4) is chosen. If (3) is chosen,
the last added concept is removed from [ce], and
[v1, {F 1context}]...[vn, {Fncontext}] are updated ac-
cording to the new [c, [ce]′]. If (4) is chosen, the

query expansion process finishes. The final [c, [ce]]
is the result of query expansion.

Neural Agent: Given a state z, the neural
agent chooses one action to take among the four
types of actions. To this end, [c, [ce]] and each
[vi, {F icontext}] are fed into separate Bi-LSTM to
obtain a concept representation and candidate con-
cept representations, respectively. We further con-
catenate these representations and use a linear
layer to obtain Q-values Q(z, a; θ) for each ac-
tion a (Sutton and Barto, 1998). Note that we do
not use softmax to normalize the Q-values since
Q-value is the expectation of discounted sums of
rewards by definition instead of probabilities. The
action with the maximum Q-value is chosen.

Reward: A reward r is associated at each step
specified by the reward function R, which evalu-
ates the goodness of action a on state z. We use
the difference of mean average precision (MAP)
(Christopher et al., 2008) before and after an ac-
tion a as the reward function:

R(z, a, z′) =MAP (z′)−MAP (z), (7)

where MAP is defined as:

MAP (z) =
1

|ω(c)|
∑

o∈ω(c)
Precision@rank(o; z, E) (8)

and

Precision@K =

∑
o′∈ω(c) 1(rank(o

′; z, E) ≤ K)
K

(9)

ω(c) is the set of concept stocks of the concept
c in training data. rank(o; z, E) is the rank of the
stock o, which is calculated by utilizing [c, [ce]] of
z and global embedding E to rank all stocks using
Equation 5. 1 is the indicator function. Therefore,
MAP measures the goodness of the ranking, which
is large if the stocks in ω(c) are ranked higher
compared to the others. Reward r is positive if
[c, [ce]

′] of z′ can rank stocks better compared to
[c, [ce]] of z and negative otherwise.

We choose MAP based on two reasons: (1)
MAP provides a measure of quality, which has
been shown to have good discrimination and sta-
bility. Besides, MAP is roughly the average area
under the precision-recall curve for a set of queries
(Christopher et al., 2008). Thus, optimizing MAP
can indirectly improve both precision and recall.
(2) MAP provides smoother scores than other met-
rics such as Precision@K and Recall@K.

In summary, at each step, the MDP framework
chooses an action a based on a state z, obtaining a

2107



Algorithm 1 Training Phase of MDP for Query Expansion
1: Initialize experience memory M
2: Initialize action network with random weights θ
3: Initialize target network with weights θtarget ← θ
4: for episode from 1 to N do
5: for each concept c do
6: Obtain start state z ←get state([c, [ ]], E1...En)
7: while true do
8: if random() < � then
9: Select a random action a

10: else
11: Send state z to neural agent
12: Obtain action a from action network
13: end if
14: Obtain new state z′ ← T (z, a)
15: Calculate reward, r ← R(z, a, z′)
16: Store sample (z, a, z′, r) to M
17: Update state z ← z′
18: Sample mini-batch (zt, at, z′t, rt) from M
19: Calculate sample estimate using Equation 11
20: Perform a batch gradient descent step on

the action network, updating parameters θ
using Equation 12

21: Update θtarget ← θ at every C steps.
22: if a == action (4) then
23: break
24: end if
25: end while
26: Send the final [c, [ce]] to E and rank the stocks
27: end for
28: end for

new state z′ and a reward r, which forms a sample,
(z, a, z′, r). The process repeats until action (4) is
chosen.

6.3 Learning

We adopt Q-learning (Sutton and Barto, 1998) to
optimize the neural agent, which uses a function
Q(z, a) to represent Q-values and the recursive
Bellman equation to perform Q-value iteration,
when observing a new sample (z, a, z′, r). Since
the state space Z can be extremely large in prac-
tice, we represent the Q-value function Q(z, a)
with a neural agent shown in Figure 2 (b) named
the action network parametrized by θ (Mnih et al.,
2015). The deep Q-learning method has the abil-
ity to capture nonlinear features and achieve bet-
ter performance compared with traditional meth-
ods (Narasimhan et al., 2015). Formally,

Q(z, a) = Q(z, a; θ) (10)

To improve learning stability, sample reward es-
timates are obtained from a separate target net-
work with the same architecture as the action
network (Mnih et al., 2015), parametrized by
θtarget. Formally, the sample reward estimate of

(z, a, z′, r) is:

y
′
=

{
r if a = action(4)

r + γmaxanew∈AQ(z
′, anew; θtarget) otherwise

(11)

Note that if the action (4) is taken, y′ = r since the
process stops at state z and no further action will
be taken so that the sum of rewards is r.

To learn the model parameters θ, the action net-
work outputs Q(z, a; θ) should be close to sample
estimates obtained from target network. Thus, we
introduce an experience memory M to save his-
tory samples and select a mini-batch of samples
according to a uniform distribution. We use the
mean square error as the loss function:

E(z,a,z′,r)∼U(M)[(Q(z, a; θ)− y′)2] (12)

The training phase is shown in Algorithm 1. In
lines 8-12, we use �-greedy exploration, which en-
courages the agent to explore unknown state space
(Sutton and Barto, 1998).

7 Experiments

7.1 Datasets

We construct two datasets from the Chinese web-
sites, Jinrongjie2 and Tonghuashun3, respectively,
which are two mass medias for China stock mar-
kets. These two websites periodically publish their
concept stock lists, which are manually collected
and analyzed by their financial professionals. We
observe high quote change correlations of the
stocks of each concept c and their lists are com-
monly used by investors to select stocks, which
confirms the credibility of these datasets. The
Jinrongjie dataset consists of 206 concepts and
each concept has an average of 25.4 manually
suggested concept stocks. For the Tonghuashun
dataset, there are 900 concepts and 15.6 manually
suggested concept stocks on average.

There are two main stock exchanges in China,
the Shanghai Stock Exchange4 and the Shenzhen
Stock Exchange5. We crawled stock lists from
their official websites, with 3326 stocks in total.

We utilize four public data sources, S1 to S4,
the statistics of which are shown in Table 1.

2金融界 http://stock.jrj.com.cn/concept/
3同 花 顺 http://stock.10jqka.com.cn/

gngyw_list/
4上海证券交易所 http://www.sse.com.cn/

assortment/stock/list/share/
5深圳证券交易所 http://www.szse.cn/

2108



Source # Docs Avg # Words
News 255,318 2753
Report 12, 431 19,145
Wikipedia 2,143 3745
Search Engine 6,130 1846

Table 1: Data Source statistics

S1: News is crawled from Sina Finance News6,
which originates from 2009 to 2017.

S2: Reports consists of annual and quarterly com-
pany reports crawled from Sina Finance7.

S3: Wikipedia includes relevant wikipedia pages
of the concepts and stocks, if any, which can
provide some background knowledge.

S4: Search Engine includes open-domain infor-
mation for the concepts and stocks obtained
using Bing API 8. We adopt search engine
results for representing heterogeneous web
texts. The top-ranked webpages are crawled.

Given the Jinrongjie and Tonghuashun datasets,
we randomly select 70%, 10% and 20% of the
concepts as training, development and testing sets,
respectively.

7.2 Baselines and Parameter Settings
We compare our method with four baselines:

Search is a naive information retrieval baseline,
which sends the concept c and each stock o to an
inverted index and obtains a list of top-k ranked
documents (k = 5 in experiments) by a fixed rank-
ing metric, Ocapi BM25 (Robertson et al., 2009).
The stocks are ranked by the average of top-k doc-
uments’ BM25 scores.

Rank is our ranking baseline. Five top-ranked
documents from each source are fed into the
model. All 3326 stocks are ranked for each con-
cept.

Semantics ranks the stocks using Equation 4,
which is the naive semantic relatedness f̂(c, o).

Semantics+ extends Semantics by including 8
most similar words to expand original concepts.

Semantics++ extends Semantics by including
the most similar words with similarities larger than
0.65 to expand original concepts. On average, 6.3
concepts are included.

6http://finance.sina.com.cn/
7http://finance.sina.com.cn/focus/

ssgsnb2016/
8https://azure.microsoft.com/

en-us/services/cognitive-services/
bing-web-search-api/

Jinrongjie
Method P@5 P@10 R@30 MAP
Search 0.402 0.315 0.338 0.296
Semantics 0.45 0.367 0.380 0.332
Semantics+ 0.471 0.370 0.391 0.352
Semantics++ 0.478 0.375 0.396 0.359
Rank 0.467 0.376 0.402 0.365
RL 0.524* 0.427* 0.428* 0.398*

Tonghuashun
Method P@5 P@10 R@30 MAP
Search 0.387 0.302 0.315 0.278
Semantics 0.437 0.347 0.360 0.327
Semantics+ 0.448 0.356 0.374 0.345
Semantics++ 0.453 0.362 0.380 0.351
Rank 0.458 0.373 0.381 0.356
RL 0.507* 0.402 0.422* 0.378*

Table 2: Concept stock recommendation results on Jin-
rongjie and Tonghuashun. ∗ denotes statistical signifi-
cance using Wilcoxon signed rank test (p < 0.05)

For our model, denoted as RL, we set the win-
dow size as 80, the embedding size as 300 and the
vocabulary size as 100, 000 in view of the large
variety of phrases after word segmentation to train
Doc2Vec. Also, the experience memory size is set
to 50, 000 and older training samples are aban-
doned. The � value is set as 1 at the start and grad-
ually decreases to 0.1 after 3000 annealing steps.
We perform a training phase after every 3 decision
steps. The mini-batch size is set to 50. Dropout is
applied to avoid overfitting and the dropout rate is
0.5. We set the learning rate for AdaGrad as 0.01.
Gradient clipping (Pascanu et al., 2013) is adopted
to prevent gradient exploding and vanishing dur-
ing training process.

7.3 Recommendation Accuracies

We use four metrics, mean average precision
(MAP), precision at 5 and 10 (P@5, P@10) and
recall at 30 (R@30) to evaluate the algorithms.
The results are shown in Table 2.

From Table 2, the first observation is that RL
outperforms the baselines on both datasets, which
demonstrates the effectiveness of combining se-
mantic relatedness with query expansion based
on reinforcement learning. The baseline Rank
achieves the second best results. The large gap be-
tween RL and Rank indicates that RL is much
easier to train compared to Rank on small data.

Second, we observe that Semantics+ improves
over Semantics, which shows that query expan-
sion has the potentials to alleviate concept ambi-
guities and benefit concept stock recommendation.
Semantics++ can outperform Semantics+ by con-
sidering semantic similarities. Also, compared to

2109



100 200 300 400 500 600 700 800
# training concept

0.10

0.15

0.20

0.25

0.30

0.35

0.40

0.45

0.50

M
A

P

Rank RL

Figure 3: Learning Curve

200 300 400 500 600 700 800
# testing concept

50

100

150

200

250

300

ti
m

e
 (

s)

Rank

RL

Semantics+

Semantics

Search

Figure 4: Efficiency

RL, we conclude that query expansion based on
reinforcement learning could better utilize train-
ing data and significantly outperform naive query
expansion methods.

The last observation is that Search performs the
worst among the methods. This sheds light on the
limitations of traditional search models and con-
firms the effectiveness of semantic modeling by
word embedding and neural models.

7.4 Influence of Size of Training Data

We increase the amount of training concepts and
study whether RL is easier to train than Rank. The
results on Tonghuashun is shown in Figure 3 (sim-
ilar patterns are demonstrated using Jinrongjie).
With more training concepts, the MAPs of both
methods increase. However, RL consistently out-
performs Rank and the margin becomes larger.
Thusly, we conclude that RL requires less data
than Rank to achieve similar performance.

7.5 Efficiency Comparison

Figure 4 shows the efficiency of all algorithms
on testing data. The three unsupervised algorithm
Search, Semantics and Semantics+ are more effi-
cient compared to the supervised algorithm, Rank
and RL. RL is more efficient compared to Rank,
since Rank has to rank every stock to obtain con-
cept stocks.

中字头 (Sino)
Semantics+ Rank RL

中远海控ZYHK 中国国航ZGGH 中国交建ZGJJ
中国交建ZGJJ 中国中铁ZGZT 中国中冶ZZZY
石油济柴SYJC 中国石油ZGSY 中国建筑ZGJZ
中国一重ZGYZ 中储股份ZCGF 中国中铁ZGZT
招商轮船ZSLC 中国中冶ZGZY 中国铁建ZGTJ

特斯拉 (Tesla)
Semantics+ Rank RL

万向钱潮WXQC 协鑫集成XXJC 万向钱潮WXQC
协鑫集成XXJC 比亚迪BYD 国机汽车GJQC
天汽模TQM 亚太股份YTGF 天汽模TQM
亚太股份YTGF 天汽模TQM 亚太股份YTGF
爱康科技AKKJ 长城汽车CCQC 上海临港SHLG

智能物流 (Intelligent Logistics)
Semantics+ Rank RL
东杰智能DJZN 华胜天成HSTC 飞力达FLD
亿阳信通YYXT 飞力达FLD 中储股份ZZGF
飞力达FLD 美菱电器MLDQ 东杰智能DJZN
美克家居MKJJ 东杰智能DJZN 圆通速递YTSD
天成自控TCZK 圆通速递YTSD 华鹏飞HPF

Table 3: Example concept stocks, where stock indicates
a incorrectly recognized concept stock.

7.6 Case Study

Data Sources Effectiveness: To study the effec-
tiveness of data sources, we count how many con-
cepts are chosen from each data source during
query expansion. For the Tonghuashun test data
(similar tendencies are observed for Jinrongjie),
761, 689, 199, 344 concepts are selected for S1-
S4, respectively. Accumulated rewards of these
concepts for S1-S4 are 76.13, 61.32, 7.49 and
14.10, respectively. We conclude that News and
Reports are relatively more effective for improv-
ing recommendation accuracies.

Recommended Stocks: To obtain a better un-
derstanding of our method, we examine the sym-
bols of the top-5 selected stocks of concepts and
some examples are shown in Table 3.

We notice that RL can effectively extend con-
cepts with relevant concepts. For example, the
algorithm extends 中字头 (Sino) with 中国
(China) and国企 (State-owned enterprises),特斯
拉 (Tesla) with入华 (into China),电动车 (Elec-
tric cars) and 马斯克 (Musk) and 智能物流 (In-
telligent Logistics) with 物流 (Logistics), CSN
(China Smart Logistic Network), 仓储 (Ware-
housing) and 配送 (Delivery), which results in
more accurate concept stocks.

For 特斯拉 (Tesla), RL made two mistakes
due to rumor and ambiguity. For example, 上
海临港SHLG is chosen because of rumors that
Tesla will establish a new factory there. 万向钱
潮WXQC is mistakenly chosen because 万向钱
潮WXQC is called China’s Tesla in some news
due to its investments in electric cars. In contract,
Semantics+ and Rank are limited by lack of su-

2110



pervision and highly unbalanced datasets, respec-
tively. For example, Rank mistakenly chooses
美菱电器MLDQ in that it confuses 智能家电
(Smart Appliances) with 智能物流 (Intelligent
Logistics). We conclude that RL is capable of ex-
panding concepts with relevant concepts that helps
find more revelant stocks.

Conclusion

We have investigated a reinforcement learning
method to automatically mine evidences from
large-scale text data for measuring the correla-
tion between a concept and a list of stocks. Com-
pared to standard information retrieval methods,
our method leverages a small amount of training
data for obtaining a flexible strategy of query ex-
pansion, thus being able to disambiguate contexts
in exploration. Results on two Chinese datasets
show that our method is highly competitive for our
task, thus providing a tool for investors to gain un-
derstandings of emerging markets.

Acknowledgments

We thank the anonymous reviewers for their in-
sightful comments. We would like to thank Yumin
Zhou for her insightful discussion and assisting
coding. Yue Zhang is the corresponding author.

References
Kumaripaba Athukorala, Alan Medlar, Antti

Oulasvirta, Giulio Jacucci, and Dorota Glowacka.
2016. Beyond relevance: Adapting explo-
ration/exploitation in information retrieval. In
Proceedings of the 21st International Conference on
Intelligent User Interfaces. ACM, pages 359–369.

Jing Bai, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005. Query expansion using term
relationships in language models for information re-
trieval. In CIKM. ACM, pages 688–695.

Y-Lan Boureau, Jean Ponce, and Yann LeCun. 2010.
A theoretical analysis of feature pooling in visual
recognition. In ICML. pages 111–118.

Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good expan-
sion terms for pseudo-relevance feedback. In SIGIR.
ACM, pages 243–250.

Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming
Zhou. 2015. Ranking with recursive neural net-
works and its application to multi-document sum-
marization. In AAAI. pages 2153–2159.

Claudio Carpineto and Giovanni Romano. 2012. A sur-
vey of automatic query expansion in information re-
trieval. ACM Computing Surveys 44(1):1.

D Manning Christopher, Raghavan Prabhakar, and
SCHÜTZE Hinrich. 2008. Introduction to informa-
tion retrieval. An Introduction To Information Re-
trieval 151:177.

Mayur Datar, Nicole Immorlica, Piotr Indyk, and Va-
hab S Mirrokni. 2004. Locality-sensitive hashing
scheme based on p-stable distributions. In Proceed-
ings of the twentieth annual symposium on Compu-
tational geometry. ACM, pages 253–262.

Fernando Diaz, Bhaskar Mitra, and Nick Craswell.
2016. Query expansion with locally-trained word
embeddings. arXiv preprint arXiv:1605.07891 .

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research pages 2121–2159.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works pages 602–610.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. MIT Press, pages 1735–1780.

Saar Kuzi, Anna Shtok, and Oren Kurland. 2016.
Query expansion using word embeddings. In CIKM.
ACM, pages 1929–1932.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
pages 1188–1196.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS. pages 3111–3119.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
et al. 2015. Human-level control through deep rein-
forcement learning. Nature pages 529–533.

Karthik Narasimhan, Tejas Kulkarni, and Regina
Barzilay. 2015. Language understanding for text-
based games using deep reinforcement learning.
arXiv preprint arXiv:1506.08941 .

Rodrigo Nogueira and Kyunghyun Cho. 2017. Task-
oriented query reformulation with reinforcement
learning. arXiv preprint arXiv:1704.04572 .

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. ICML 28:1310–1318.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 conference
on empirical methods in natural language process-
ing (EMNLP). pages 1532–1543.

2111



Carolyn C Preston and Andrew M Colman. 2000. Op-
timal number of response categories in rating scales:
reliability, validity, discriminating power, and re-
spondent preferences. Acta psychologica pages 1–
15.

Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends R© in Information Re-
trieval pages 333–389.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014a. A latent semantic
model with convolutional-pooling structure for in-
formation retrieval. In CIKM.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014b. Learning semantic
representations using convolutional neural networks
for web search. In WWW. ACM, pages 373–374.

Richard S Sutton and Andrew G Barto. 1998. Re-
inforcement learning: An introduction. MIT press
Cambridge.

Gang Wu and Edward Y Chang. 2003. Class-boundary
alignment for imbalanced dataset learning. In ICML
2003 workshop on learning from imbalanced data
sets II, Washington, DC. pages 49–56.

Jie Yang, Yue Zhang, and Fei Dong. 2017. Neural word
segmentation with rich pretraining. In ACL.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103 .

2112


