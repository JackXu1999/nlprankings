



















































Multi-agent Learning for Neural Machine Translation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 856–865,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

856

Multi-agent Learning for Neural Machine Translation

Tianchi Bi, Hao Xiong, Zhongjun He, Hua Wu and Haifeng Wang
Baidu Inc. No. 10, Shangdi 10th Street

Beijing, 100085, China
{bitianchi, xionghao05, hezhongjun, wu hua, wanghaifeng}@baidu.com

Abstract
Conventional Neural Machine Translation
(NMT) models benefit from the training with
an additional agent, e.g., dual learning, and
bidirectional decoding with one agent decod-
ing from left to right and the other decoding in
the opposite direction. In this paper, we extend
the training framework to the multi-agent sce-
nario by introducing diverse agents in an in-
teractive updating process. At training time,
each agent learns advanced knowledge from
others, and they work together to improve
translation quality. Experimental results on
NIST Chinese-English, IWSLT 2014 German-
English, WMT 2014 English-German and
large-scale Chinese-English translation tasks
indicate that our approach achieves absolute
improvements over the strong baseline sys-
tems and shows competitive performance on
all tasks.

1 Introduction

Training with more than one agents has attracted
intensive research interest in recent years, for ex-
ample, dual learning (He et al., 2016; Xia et al.,
2017, 2018) and bidirectional decoding (Liu et al.,
2016; Zhang et al., 2019b). The former method
leverages the duality between the two related
agents as the feedback signal to regularize train-
ing, while the latter targets the agreement between
one agent decoding from left to right (L2R) while
the other decoding in opposite direction (R2L).
Both methods enhance the translation models by
introducing a regularization term into the training
objective.

The effectiveness of these two methods lies in
the fact that appropriate regularization can help
each agent to learn from the superior models while
integrating their advantages (e.g., good transla-
tions for prefixes for L2R, and good translation
quality for suffixes for R2L). As shown in the Ta-
ble 1, due to the exposure bias problem (Ranzato

Src 此次 豪雨灾情 ,半数遇难者遭洪水
冲走 溺 毙 ; 其他 则 因 房屋 倒塌 或
电线走火触电致死。

Ref In this disaster caused by torrential rains,
half of the victims were carried away and
drowned in the floods; the others died
because their houses collapsed or from
electrocution caused by short circuits.

L2R In the torrential rain, half of the vic-
tims were washed away by floods, while
others died of electricity caused by col-
lapsed houses or electric wires.

R2L Half of the victims were drowned by
floods, while others were killed by
collapsed houses or electrocuted by
sparkling wires.

Table 1: In this sample, NMT systems with different
implementations present diverse translation errors for
one long sentence. The agent decoding from left to
right (L2R) tends to generate better prefixes and bad
suffixes (bold letters), and the R2L agent has the oppo-
site preference.

et al., 2016), the agent trained to decode from
left to right tends to generate better prefixes and
bad suffixes, and the agent decoding in the re-
verse direction demonstrates the opposite prefer-
ence. By introducing additional Kullback-Leibler
(KL) divergences between the probability distri-
butions defined by L2R and R2L models into the
NMT training objective (Zhang et al., 2019b), it is
possible for two models to learn advantages from
each other.

According to the empirical achievements of pre-
vious studies on training with two agents, it is nat-
ural to consider the training with more than two
agents, and to extend our study to the multi-agent
scenario. However, training with more than two
agents is more complex, and we face two critical



857

problems. First, when deploying the multi-agent
system, should we focus on the diversity or the
strength of each agent? (Wong et al., 2005; Mar-
colino et al., 2013) Second, learning in multi-agent
scenario is many-to-many, as opposed to the rel-
atively simpler one-to-one learning in two-agent
training, and requires an effective learning strat-
egy.

There have been many alternatives to improve
the diversity of models even based on the Trans-
former model (Vaswani et al., 2017). For exam-
ple, decoding in the opposite direction usually re-
sults in different preferences: good prefixes and
bad prefixes (Zhang et al., 2019b). Rather, self-
attention with relative position representations en-
hances the generalization to sequence lengths un-
seen during training (Shaw et al., 2018). Further-
more, increasing the size of layers in the encoder is
expected to specialize in word sense disambigua-
tion (Tang et al., 2018; Domhan, 2018). In this
paper, we investigate the effects of two teams of
multi-agents: a team of alternative agents men-
tioned above, and a uniform team of different ini-
tialization for the same model.

To resolve the second problem, we simplify the
many-to-many learning to the one-to-many (one
teacher vs. many students) learning, extending
ensemble knowledge distillation (Fukuda et al.,
2017; Freitag et al., 2017; Liu et al., 2018; Zhu
et al., 2018). During the training, each agent per-
forms better by learning from the ensemble model
(Teacher) of all agents integrating the knowledge
distillation (Hinton et al., 2015; Kim and Rush,
2016) into the training objective. This proce-
dure can be viewed as the introduction of an ad-
ditional regularization term into the training ob-
jective, with which each agent can learn advan-
tages from the ensemble model gradually. With
this method, each agent is optimized not only to
the maximization of the likelihood of the train-
ing data, but also to the minimization of the di-
vergence between its own model and the ensemble
model.

However, the above learning strategy converges
on the local optimum rapidly in our empirical
studies. It seems that each agent tends to focus
on learning from the ensemble model while com-
pletely ignoring its own exploration. To alleviate
this problem, we train each agent to learn from
the ensemble models when necessary, and to dis-
till the knowledge based on the translation quality

(BLEU score) of the ensemble model. This means
we evaluate the quality of the ensemble model to
see whether it is good enough to be studied. Con-
sequently, the knowledge distillation from the en-
semble model to the agent stems from the better
translation generated by the ensemble model.

We evaluate our model on NIST Chinese-
English Translation Task, IWSLT 2014 German-
English Translation task, large-scale WMT 2014
English-German Translation task and large-scale
Chinese-English Translation Task. Extensive ex-
perimental results indicate that our model signifi-
cantly improves the translation quality, compared
to the strong baseline systems. Moreover, our
model also reports competitive performance on the
German-English and English-German translation
tasks, achieving 36.27 and 29.67 BLEU scores, re-
spectively.

To the best of our knowledge, this is the first
work on training NMT model with more than two
agents 1. The contributions of this paper are sum-
marized as follows:

• We extend the study on training with two
agents to the multi-agent scenario, and pro-
pose a general learning strategy to train mul-
tiple agents.

• We investigate the effects of the diversity and
the strength of each agent in the multi-agent
training scenario.

• We simplify complex many-to-many learning
in multi-agent learning to one-to-many learn-
ing by forcing each agent learning knowledge
from ensemble model as necessary.

• Extensive experiments on multiple transla-
tion tasks confirm that our model signifi-
cantly improves the translation quality and
reports the new state-of-the-art results on
IWSLT 2014 German-English and competi-
tive results on WMT 2014 English-German
translation tasks.

2 Background

Conventional autoregressive NMT models (Bah-
danau et al., 2015; Sutskever et al., 2014) decode
target tokens sequentially, which indicates that the

1Although Wang et al. (2019) proposed a work named
multi-agent dual learning contemporarily, the training in their
work is conducted on the two agents with fixed parameters of
other agents.



858

determination of the current token is conditioned
by the previous generated sequence. Formally, at
time step t, the generation of the current token yt
is determined by the following equation:

p(yt) = p(yt|y<t, x; θ) (1)

where y<t represents the previously generated
sequence, and θ are the parameters of the represen-
tation of the source sequence and the partial target
sequence.

Furthermore, the usual training criterion is to
minimize the negative log-likelihood for each
sample from the training data,

LNLL = −
T∑

t=0

|V|∑
k=1

1{yt = k}logp(yt = k|y<t, x; θ) (2)

where T is the length of the target sequence,
|V| is the size of vocabulary and 1{·} is the in-
dicator function. This objective can be viewed
as minimizing the cross-entropy between the cor-
rect translation y∗ and the model generation
p(yt|y<t, x; θ).

3 Multi-agent Learning

Empirical studies indicate that one agent is trained
to perform better through learning advantages
from the other agent in the two-agent scenario,
namely one-to-one learning (Figure 1.(a)). The
training objective of the agent is regularized,
which is to learn better models by leveraging the
relationship between the two agents as feedback,
i.e., duality in dual learning problem, and agree-
ment in bidirectional decoding.

Extending the study to the multi-agent scenario
is feasible and desirable, as multiple agents might
supply more reliable and diverse advantages com-
pared to the two-agents scenario. However, the
agent is expected to learn advantages from each
other in the multi-agent scenario, which results in
a complex many-to-many learning problem(Figure
1.(b)).

Instead of tackling many-to-many learning, we
force the agent to learn from a common Teacher
by introducing ensemble knowledge distillation,
thus reduce the learning to one-to-many (Figure
1.(c)). With this learning strategy, each agent can
learn to improve the performance in an interactive
updating process.

Figure 1: Illustration for different learning approaches.

As opposed to ensemble Knowledge Distilla-
tion (KD) (Figure 1.(d)), the important difference
is that in the knowledge distillation of an ensem-
ble of models, the Teacher network is fixed after
pre-training, during the training process. While in
our framework, the state of the Teacher network is
updated at each iteration, and its performance can
be further improved by the improvements of each
agent explicitly, in an interactive updating process.
In some ways the ensemble KD can be viewed as
a particular case of our model, as we fix the update
of the Teacher network in the training framework.

3.1 Overall Framework

For the sake of simplicity, four variant agents
are referred to as Agent1, Agent2, Agent3 and
Agent4. We begin by pre-training each agent in-
dependently (Figure 2.(a)), and then enhance the
model in the multi-agent scenario in two steps: 1)
Generating Ensemble Model (Figure 2.(b)), and 2)
One-to-Many Learning (Figure 2.(c)). The perfor-
mance of each agent is improved in an interactive
updating process, through repeating the above two
steps.

3.2 Generate Ensemble Model

As pointed out in the work of Liu et al. (2018),
ensemble models can empirically alleviate prob-
lems existing in the standard NMT model, such as
ambiguities in the training data, and discrepancy
between training and testing.

According to the practical advantages of ensem-



859

Figure 2: In this example, four agents decode the similar sentence with different model capacity. (a): At first, each
agent is pre-trained to generate the translation independently. (b) The ensemble model is generated by the average
prediction from each agent. (c): The One-to-Many learning distills the knowledge from the ensemble model to
each agent as necessary. The performance of each agent is improved explicitly in an interactive updating process,
through repeating the process (b) and (c).

ble models, it is relevant to force agents to learn
from the ensemble model, instead of learning from
each other separately. Following previous work,
we develop our ensemble model by averaging the
model distributions of all agents.

Formally, the model distribution of the i-th
agent agenti is defined as p(yit|yi<t, x; θ). Assume
we have N agents, the model distribution of the
ensemble model can be formulated as:

q(yt|y<t, x; θt) =
1

N

N∑
i=1

p(yit|yi<t, x; θi) (3)

where θt are parameters for representing the en-
semble model. Notably, we do not train it in the
training process.

In the above formula, the probability
q(yt|y<t, x; θt) is one reliable estimator of
the model distribution, as the majority of the
agents are likely to generate correct sequence.
From this perspective, we expect that more agents
will lead to better and more robust performance.

3.3 One-to-Many Learning

In the one-to-many learning framework, the en-
semble model acts as the Teacher network, which
distills knowledge to each agent iteratively.

Rather than minimizing cross-entropy with the
observed data, we minimize the cross-entropy
with the probability distribution from the ensem-

ble model for each agent.

LiKD =−
T∑

t=0

|V|∑
k=1

q(yt = k|y<t, x; θt)×

logp(yit = k|yi<t, x; θ)

(4)

With the above formula, the agent is optimized
to the minimization of the model divergence be-
tween its own model and the ensemble model.

However, integrating the above regularization
term into the training objective straightforwardly
is problematic in practice. The agent tends to fo-
cus on learning from the ensemble model rather
than exploring its own prediction to converge
rapidly. Consequently, the model converges at a
suboptimal point, and fails to enhance the perfor-
mance by learning from each other due to the lack
of evaluation of the ensemble model.

To alleviate this problem, we train each agent
to learn from the ensemble model when necessary,
distilling the knowledge conditioned by the trans-
lation quality of the ensemble model. We evaluate
the quality of the ensemble model to see whether
it is good enough to be studied. The knowledge
distillation from the ensemble model to the agent
comes from the better translation generated by the
ensemble model. Otherwise, the agent is forced to
learn its own distribution.

Let < Xg, Yg > be one sentence pair in the
training corpus, the quality of the translation se-
quence generated by the ensemble model is mea-



860

sured by the BLEU score,

score(Yt) = BLEU(Yt, Yg) (5)

where Yt is generated by the model distribution
q(yt|y<t, x; θt). The quality of the translation se-
quence generated by each agent can also be de-
fined using the similar metric,

score(Y is ) = BLEU(Y
i
s , Yg) (6)

where Y is is generated by the model distribution
p(yit|yi<t, x; θ) of each agent.

Conditioned by the translation quality of the en-
semble model, we modify the training objective
for each agent as follows:

LiKD = −
T∑

t=0

|V|∑
k=1

S × logp(yit = k|yi<t, x; θ) (7)

where S is defined as:

S =

{
q(yt = k|y<t, x; θt) score(Yt) > score(Y is )
1s{yit = k} otherwise

(8)

where 1s{·} is an indicator, conditioned by the
existence of yit = k in the sequence Y

i
s .

In practice, each agent is not only optimized to
maximize the likelihood of the training data, but
also to minimize the model divergence between its
own model and the ensemble model:

Lia = λiLNLL + (1− λi)LiKD (9)

where λi is a hyperparameter, balancing the
weight of two factors.

From the perspective of learning, the agent
learns to minimize the training objective by gen-
erating competitive translations, which leads to a
global improvement for all agents. On the other
hand, a sequence-level training objective might al-
leviate the exposure bias problem implicitly (Ran-
zato et al., 2016).

3.4 Joint Learning

In the work of Zhang et al. (2019b), they proposed
a relatively complex joint learning framework for
training two agents. In this paper, according to the

Algorithm 1: Multi-agent Learning
Input: N variant agents;

1 Pre-train each agent agenti independently;
2 repeat
3 for each training sample (Xg, Yg) do
4 Ensemble Model: q(yt|y<t, x; θt);
5 Generate sequence:

Yt ← argmax
0≤t<T

q(yt|y<t, x; θt);

6 for each agent agenti do
7 Generate sequence:

Y is ← argmax
0≤t<T

p(yit|yi<t, x; θ);

8 Compute KD loss: LiKD ←
(q(yt), p(y

i
t), Yt, Y

i
s , Xg, Yg);

9 Compute NLL loss:
LNLL ← (p(yit), Xg, Yg) ;

10 Compute Agent loss:
Lia ← λLNLL + (1− λ)LiKD;

11 end
12 Model loss: Lfinal =

∑N
i=1 Lia;

13 Update gradients for each agent;
14 end
15 until convergence;

previous experiments, we find that a simple multi-
task learning technique without sharing any mod-
ules presents promising performance,

Lfinal =
N∑
i=1

Lia (10)

In Algorithm 1, we describe the overall proce-
dure of our approach. It deserves noting that in
line 3, we assume the model reads a single pair of
training examples per timestep for simplistic de-
scription, while in practice the model reads a batch
size of samples at each step.

4 Experiments

In this paper, we evaluate our model on four
translation tasks: NIST Chinese-English Transla-
tion Task, IWSLT 2014 German-English Transla-
tion Task, WMT 2014 English-German Transla-
tion Task and large-scale Chinese-English Trans-
lation Task.

4.1 Data Preprocessing
To compare with previous studies, we conduct
byte-pair encoding (Sennrich et al., 2016) for Chi-
nese, English and German sentences, setting the



861

MODEL MT02 MT03 MT04 MT08 AVERAGE IMPROVEMENTS
Results for Best Agent Results for each Agent

Wang et al. (2018) - 46.60 47.73 - - -
a.L2R 48.53 47.07 48.43 42.21 46.56 -
b.R2L 47.06 45.58 47.14 41.04 45.20 -
c.Enc 48.86 47.54 48.57 42.93 46.97 -
d.Rel 48.12 48.19 48.33 42.51 46.78 -
a+b 48.82 47.65 48.45 42.49 46.86/45.42 +0.33/+0.20
a+c 48.79 48.30 49.32 43.44 47.3/47.27 +0.80/+0.30
a+d 48.76 48.40 48.74 43.27 47.19/47.09 +0.63/+0.31
c+d 49.45 49.01 49.52 43.71 47.62/47.79 +0.65/+1.01
a×2 48.64 47.98 49.08 43.07 46.96/47.10 +0.40/+0.54
d×2 48.23 48.78 48.90 43.85 47.44/47.31 +0.66/+0.53
a+b+c 49.32 48.72 49.32 44.34 47.69/45.62/47.74 +1.13/+0.42/+0.77
a+b+d 49.29 48.90 49.52 44.21 47.73/45.71/47.65 +1.17/+0.51/+0.87
a+c+d 49.42 49.13 49.68 44.66 47.72/48.21/47.94 +1.16/+1.24/+0.96
a×3 48.75 48.09 49.19 43.18 47.07/47.28/47.20 +0.51/+0.72/+0.64
d×3 48.47 49.02 49.42 44.09 47.51/47.46/47.57 +0.73/+0.68/+0.79
a+b+c+d 49.52 48.94 49.61 44.70 47.95/46.10/48.3/47.95 +1.39/+0.90/+1.33/+1.17

Table 2: BLEU score for the representative models in multi-agent training on NIST Chinese-English translation.
X×Y stands for Y agents with the indentical model X but from different initializing seeds.

vocabulary size to 20K and 18K for Chinese-
English, a joint 20K vocabulary for German-
English, and a joint 32K vocabulary for English-
German, respectively.

For Chinese-English task, the training data con-
sists of about 1.5M sentence pairs extracted from
LDC corpora 2. We choose the NIST 2006
(MT06) dataset for validation, and NIST 2002-
2004 (MT02-04), as well as NIST 2008 (MT08)
datasets for testing. For large-scale Chinese-
English task, the training data consists of about
40M sentence pairs extracted from web data.

4.2 Agent Variants

To increase the diversity of agents, we implement
the following variants of the Transformer based
system: L2R, the officially released open source
toolkit for running Transformer model, R2L, the
standard Transformer decodes in reversed direc-
tion, Enc, the standard Transformer with 30 layers
in the encoder, and Rel, the reimplementation of
self-attention with relative position (Shaw et al.,
2018).

2LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards )

4.3 Training Details

We implement our models using PaddlePaddle 3,
an end-to-end open source deep learning platform
developed by Baidu. It provides a complete suite
of deep learning libraries, tools and service plat-
forms to make the research and development of
deep learning simple and reliable.

We use the hyperparameters of the base ver-
sion for the standard Transformer model of NIST
Chinese-English and IWSLT German-English
translation tasks, except the smaller token size
(batch size = 320). For WMT English-German
and large-scale Chinese-English translation tasks,
we use the hyperparameters of the big version. As
described in the previous section, we use a hyper-
parameter λi for each agent to balance the pref-
erence between learning from the observed data
and the ensemble model. According to the perfor-
mance of each agent after pre-training, we set this
value as follows:

λi = 0.5 + max(−0.5,min(0.5,
Bi −Bavg

10
)) (11)

where Bi is the BLEU score obtained by the pre-
training of the i − th agent, and the Bavg is the
average BLEU score of all agents.

The above formula suggests the agent learns
more from the ensemble model as its performance

3https://github.com/paddlepaddle/
paddle

https://github.com/paddlepaddle/paddle
https://github.com/paddlepaddle/paddle


862

Task L2R Rel KD-4 Dual-5 Rel-4
De-En 33.63 34.91 35.53 34.70 36.27

Table 3: BLEU score on IWSLT 2014 German-English
translation. KD-4 stands for ensemble knowledge dis-
tillation with four agents. Dual-5 is the SOTA model
from the work of Wang et al. (2019). And Rel-4 is our
best model (Rel) training with four diverse agents.

is worse than the majority vote, rather than focus-
ing on exploring by its own prediction.

We train our model with parallelization at data
batch level. For NIST Chinese-English task, it
takes about 1.5 days to train models on 8 NVIDIA
P40 GPUs, 5 days for WMT English-German task,
8 hours for IWSLT German-English task and 7
days for large-scale Chinese-English task. The de-
tailed training process is as follows, first we train
each agent until BLEU doesn’t improve any more
and then execute Algorithm 1 for one-to-many
learning which takes 30K-40K steps to converge.

4.4 Chinese-English Results

Study on different numbers of agents. We first
assess the impact of diverse agents on translation
quality. From Table 2, we see that the model’s
performance consistently improves as the number
of agents increases, and we observe that 1) The
four baseline systems with different implementa-
tions present diverse translation quality, in particu-
lar Rel with refined position encoding achieves the
best performance. 2) The improvement of each
agent after multi-agent learning is dependent on
the performance of the co-trained agent (+0.33,
+0.8, +0.63 improvements obtained by L2R when
training with R2L, Enc and Rel). 3) Better results
can be obtained by increasing the number of train-
ing agents (e.g., 47.3→ 47.73→ 47.95 for L2R).

From the overall results, increasing the num-
ber of agents in multi-agent learning significantly
improves the performance of each agent (at most
+1.39, +0.9, +1.33, +1.17 in a+b+c+d), which
suggests that each agent learns advantages from
the other agents, and more agents might lead to
further improvement. More importantly, our im-
provements are obtained from the advanced train-
ing strategy without any modification in decoding
stage, which indicates its practicability in deploy-
ment.
Study on uniform agents. To measure the impor-
tance of diversity in multi-agents, we conduct mul-

Models En-De
ConvS2S Gehring et al. (2017) 25.2
Transformer Vaswani et al. (2017) 28.4
Rel Shaw et al. (2018) 29.2
DynamicConv Wu et al. (2019) 29.7
Back-translation Sergey et al. (2018) 35.00
Dual-3 Wang et al. (2019) 29.92
Dual-3 + Mono Data 30.67
L2R 28.37
Rel 29.16
Rel-4 29.67

Table 4: BLEU score on newstest2014 for WMT
English-German translation.

tiple experiments by initializing the similar model
with different initialization seeds to generate mul-
tiple agents. As reported in Table 2, although the
single Rel model achieves the best performance
compared to the other three single agents, the d
× 2 (47.44) and d × 3 (47.57) perform worse
than the best counterparts c+d (47.79) and a+c+d
(47.94). Moreover, when training with more than
two agents, the performance of d × 3 (47.57)
is even lower than arbitrary diverse counterparts
(a+b+c (47.74); a+b+d (47.73)).

These results suggest that multi-agent learning
is better conducted with diverse agents instead of
the uniform agents with excellent single perfor-
mance. On the other hand, in our multi-agent
learning, training with more agents brings consis-
tent improvements even when deployed by identi-
cal models from different initialization seeds.

4.5 German-English and English-German
Results

We work with the IWSLT German-English trans-
lation to compare our model to the SOTA model.
From Table 3, we observe that both KD-4 and Rel-
4 models achieve the best result on this dataset
(35.53 and 36.27), which manifests the effective-
ness of using multiple agents.

Even when trained with four agents, our model
outperforms the dual model trained with five
agents (34.70), and reports a SOTA score on this
translation task. Moreover, we argue that fine-
tuning with L2R to obtain a better performance
could further improve the performance of Rel-4,
as there exists a large gap between L2R and Rel,
which affects the learning efficiency.

We further investigate the performance of our



863

Models AVERAGE
L2R(baseline) 33.74
L2R+R2L+Enc+Rel 34.60

Table 5: BLEU score for the L2R model in multi-
agent training on large-scale corpus of Chinese-English
translation. Because we only use L2R model in Baidu
Translate, so part of results are presented.

model on the WMT English-German translation
task, which achieves a competitive result of 29.67
BLEU score on newstest2014. From Table 4, we
can see that another related work, multi-agent dual
learning (Wang et al., 2019) achieves promising
results. This confirms that training with more
agents leads to better translation quality, and re-
veals the relevance of using multiple agents.

Although Sergey et al. (2018) reports a BLEU
score of 35.0 on this dataset, they leverage refined
training corpus and a large number of monolin-
gual data. We argue that our model can bring
further improvement using their back-translation
technique. Moreover, the goal of this paper is in-
troducing a general learning framework for multi-
agent learning rather than exhaustively fine-tuning
to report a SOTA results. We argue that the perfor-
mance of our model can be further improved using
an advanced single agent, such as DynamicConv
(Wu et al., 2019) and Dual (Wang et al., 2019).

4.6 Large-Scale Chinese-English Results
In order to prove that multi-agent learning can
yield improvement in large-scale corpus, we con-
ducted experiments on 40M Chinese-English sen-
tence pairs with the same data processing method
as NIST. We built test sets of more than 10K sen-
tences which cover various domains such as news,
spoken language and query logs. From Table 5, we
can see that multi-agent learning can increase the
average baseline BLEU score of several test sets
by 0.86. This technique has already been applied
to Baidu Translate.

4.7 Contrastive Evaluation
In this paper, we evaluate our model on the
two types of contrastive evaluation datasets:
Lingeval97 4 and ContraWSD 5. The former is
utilized to measure the performance of different
models on dealing with subject-verb agreement in

4https://github.com/rsennrich/
lingeval97

5https://github.com/a-rios/ContraWSD

Models SVA (%) WSD (%)
L2R 89.06 78.05
R2L 88.92 77.97
Enc 89.13 79.07
Rel 89.21 78.75
L2R+R2L 89.28 77.94
L2R+Enc 89.42 78.29
L2R+Rel 89.39 78.47
L2R+R2L+Enc 89.26 78.36
L2R+R2L+Rel 89.58 78.28
L2R+Enc+Rel 89.39 78.84
L2R+R2L+Enc+Rel 89.47 78.62

Table 6: Accuracy of subject-verb agreement (SVA)
and word sense disambiguation (WSD) for different
models. We report the performance of L2R in differ-
ent models for comparison of using different agents.

English-German translation, while the latter eval-
uates the performance on word sense disambigua-
tion in German-English translation. We suggest
the readers refer to the work (Sennrich, 2017; Tang
et al., 2018) for detailed descriptions of the two
datasets.

From Table 6, we can see that the L2R training
with multiple agents improves the accuracy both
on SVA and WSD, and we observe that: 1) The
L2R improves its ability of SVA with the help of
the other agents significantly. 2) The Enc presents
better advantage in resolving WSD than in SVA,
while Rel has the opposite preference. 3) Al-
though the R2L presents ordinary BLEU score, it
still does help the L2R resolve the SVA.

5 Related Work

The most related work is recently proposed by
Wang et al. (2019), who introduced a multi-agent
algorithm for dual learning. However, the major
differences are 1) the training in their work is con-
ducted on the two agents while fixing the param-
eters of other agents. 2) they use identical agent
with different initialization seeds. 3) our model is
simple yet effective. Actually, it is easy to incorpo-
rate additional agent trained with their dual learn-
ing strategy in our model, to further improve the
performance. More importantly, both of our work
and their work indicate that using more agents can
improve the translation quality significantly.

Another related work is ensemble knowledge
distillation (Fukuda et al., 2017; Freitag et al.,
2017; Liu et al., 2018; Zhu et al., 2018), in which

https://github.com/rsennrich/lingeval97
https://github.com/rsennrich/lingeval97
https://github.com/a-rios/ContraWSD


864

the ensemble model of all agents is leveraged as
the Teacher network, to distill the knowledge for
the corresponding Student network. However, as
described in the previous section, the knowledge
distillation is one particular case of our model, as
the performance of the Teacher in their model is
fixed, and cannot be further improved by the learn-
ing process.

Our work is also motivated by the work of train-
ing with two agents, including dual learning (He
et al., 2016; Xia et al., 2017, 2018), and bidirec-
tional decoding (Liu et al., 2016; Zhang et al.,
2018, 2019b,a). Our method can be viewed as
a general learning framework to train multiple
agents, which explores the relationship among all
agents to enhance the performance of each agent
efficiently.

6 Conclusions and Future Work

In this paper, we propose a universal yet effective
learning method for training multiple agents. In
particular, each agent learns advantages from the
ensemble model when necessary. The knowledge
distillation from the ensemble model to the agent
stems the better translation generated by the en-
semble model, which enables each agent to learn
high-quality knowledge while retaining its own
exploration.

Extensive experimental results prove that our
model brings an absolute improvement over
the baseline system, reporting SOTA results on
IWSLT 2014 German-English and competitive re-
sults on WMT 2014 English-German translation
tasks.

In the future, we will focus on training with
more agents by translating apart of one sentence
considering its advantage for each agent, rather
than translating the whole sentence.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

Tobias Domhan. 2018. How much attention do you
need? a granular analysis of neural machine trans-
lation architectures. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1799–1808.

Markus Freitag, Yaser Al-Onaizan, and Baskaran
Sankaran. 2017. Ensemble distillation for
neural machine translation. arXiv preprint
arXiv:1702.01802.

Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata,
Samuel Thomas, Jia Cui, and Bhuvana Ramabhad-
ran. 2017. Efficient knowledge distillation from an
ensemble of teachers. Proc. Interspeech 2017, pages
3697–3701.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning. In Proc. of ICML.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual learn-
ing for machine translation. In Advances in Neural
Information Processing Systems, pages 820–828.

Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network.
In NIPS Deep Learning and Representation Learn-
ing Workshop.

Yoon Kim and Alexander M Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, pages 1317–1327.

Lemao Liu, Masao Utiyama, Andrew Finch, and
Eiichiro Sumita. 2016. Agreement on target-
bidirectional neural machine translation. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
411–416.

Yijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin,
and Ting Liu. 2018. Distilling knowledge for
search-based structured prediction. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 1393–1402. Association for Computa-
tional Linguistics.

Leandro Soriano Marcolino, Albert Xin Jiang, and
Milind Tambe. 2013. Multi-agent team formation:
diversity beats strength? In Twenty-Third Interna-
tional Joint Conference on Artificial Intelligence.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In International
Conference on Learning Representations.

Rico Sennrich. 2017. How grammatical is character-
level neural machine translation? assessing mt qual-
ity with contrastive translation pairs. In Proceedings
of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vol-
ume 2, Short Papers, volume 2, pages 376–382.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual

http://arxiv.org/abs/1503.02531
http://aclweb.org/anthology/P18-1129
http://aclweb.org/anthology/P18-1129


865

Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1715–1725.

Edunov Sergey, Ott Myle, Auli Michael, and David
Grangier. 2018. Understanding back-translation at
scale. In Proceedings of the conference on empirical
methods in natural language processing, pages 489–
500. Association for Computational Linguistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), volume 2, pages
464–468.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Gongbo Tang, Mathias Müller, Annette Rios, and Rico
Sennrich. 2018. Why self-attention? a targeted eval-
uation of neural machine translation architectures.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4263–4272. Association for Computational Linguis-
tics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Mingxuan Wang, Jun Xie, Zhixing Tan, Jinsong Su,
Deyi Xiong, and Chao Bian. 2018. Neural machine
translation with decoding history enhanced atten-
tion. In Proceedings of the 27th International Con-
ference on Computational Linguistics, pages 1464–
1473.

Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin,
ChengXiang Zhai, and Tie-Yan Liu. 2019. Multi-
agent dual learning. In International Conference for
Learning Representation (ICLR).

KY Michael Wong, SW Lim, and Zhuo Gao. 2005. Ef-
fects of diversity on multiagent systems: Minority
games. Physical Review E, 71(6):066103.

Felix Wu, Angela Fan, Alexei Baevski, Yann N.
Dauphin, and Michael Auli. 2019. Pay less atten-
tion with lightweight and dynamic convolutions. In
ICLR.

Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Neng-
hai Yu, and Tie-Yan Liu. 2017. Dual supervised
learning. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages
3789–3798. JMLR. org.

Yingce Xia, Xu Tan, Fei Tian, Tao Qin, Nenghai Yu,
and Tie-Yan Liu. 2018. Model-level dual learning.
In International Conference on Machine Learning,
pages 5379–5388.

Jiajun Zhang, Long Zhou, Yang Zhao, and Chengqing
Zong. 2019a. Synchronous bidirectional inference
for neural sequence generation. arXiv preprint
arXiv:1902.08955.

Xiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Ron-
grong Ji, and Hongji Wang. 2018. Asynchronous
bidirectional decoding for neural machine transla-
tion. arXiv preprint arXiv:1801.05122.

Zhirui Zhang, Shuangzhi Wu, Shujie Liu, Mu Li, Ming
Zhou, and Enhong Chen. 2019b. Regularizing neu-
ral machine translation by target-bidirectional agree-
ment. In AAAI.

Xiatian Zhu, Shaogang Gong, et al. 2018. Knowledge
distillation by on-the-fly native ensemble. In Ad-
vances in Neural Information Processing Systems,
pages 7528–7538.

http://aclweb.org/anthology/D18-1458
http://aclweb.org/anthology/D18-1458

