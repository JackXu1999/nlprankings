



















































ResSim at SemEval-2017 Task 1: Multilingual Word Representations for Semantic Textual Similarity


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 154–158,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

ResSim at SemEval-2017 Task 1:
Multilingual Word Representations for Semantic Textual Similarity

Johannes Bjerva
Center for Language and Cognition Groningen

University of Groningen
The Netherlands

j.bjerva@rug.nl

Robert Östling
Department of Linguistics

Stockholm University
Sweden

robert@ling.su.se

Abstract

Shared Task 1 at SemEval-2017 deals with
assessing the semantic similarity between
sentences, either in the same or in differ-
ent languages. In our system submission,
we employ multilingual word representa-
tions, in which similar words in different
languages are close to one another. Us-
ing such representations is advantageous,
since the increasing amount of available
parallel data allows for the application of
such methods to many of the languages in
the world. Hence, semantic similarity can
be inferred even for languages for which
no annotated data exists. Our system is
trained and evaluated on all language pairs
included in the shared task (English, Span-
ish, Arabic, and Turkish). Although de-
velopment results are promising, our sys-
tem does not yield high performance on
the shared task test sets.

1 Introduction

Semantic Textual Similarity (STS) is the task of
assessing the degree to which two sentences are
semantically similar. Within the SemEval STS
shared tasks, this is measured on a scale ranging
from 0 (no semantic similarity) to 5 (complete se-
mantic similarity) (Cer et al., 2017). Monolingual
STS is an important task, for instance for evalua-
tion of machine translation (MT) systems, where
estimating the semantic similarity between a sys-
tem’s translation and the gold translation can aid
both system evaluation and development. The task
is already a challenging one in a monolingual set-
ting, e.g., when estimating the similarity between
two English sentences. In this paper, we tackle the
more difficult case of cross-lingual STS, e.g., es-
timating the similarity between an English and an

Arabic sentence.
Previous approaches to this problem have fo-

cussed on two main approaches. On the one hand,
MT approaches have been attempted (e.g. Lo et al.
(2016)), which allow for monolingual similarity
assessment, but suffer from the fact that involv-
ing a fully-fledged MT system severely increases
system complexity. Applying bilingual word rep-
resentations, on the other hand, bypasses this issue
without inducing such complexity (e.g. Aldarmaki
and Diab (2016)). However, bilingual approaches
do not allow for taking advantage of the increas-
ing amount of STS data available for more than
one language pair.

Currently, there are several methods available
for obtaining high quality multilingual word rep-
resentations. It is therefore interesting to investi-
gate whether language can be ignored entirely in
an STS system after mapping words to their re-
spective representations. We investigate the utility
of multilingual word representations in a cross-
lingual STS setting. We approach this by com-
bining multilingual word representations with a
deep neural network, in which all parameters are
shared, regardless of language combinations.

The contributions of this paper can be summed
as follows: i) we show that multilingual input rep-
resentations in some cases can be used to train an
STS system without access to training data for a
given language; ii) we show that access to data
from other languages in some cases improves sys-
tem performance for a given language.

2 Multilingual Word Representations

2.1 Multilingual Skip-gram

The skip-gram model has become one of the most
popular manners of learning word representations
in NLP (Mikolov et al., 2013). This is in part
owed to its speed and simplicity, as well as the per-

154



formance gains observed when incorporating the
resulting word embeddings into almost any NLP
system. The model takes a word w as its input,
and predicts the surrounding context c. Formally,
the probability distribution of c given w is defined
as

p(c|w;θ) = exp(~c
T~w)

Σc∈V exp(~cT~w)
, (1)

where V is the vocabulary, and θ the parameters of
word emeddings (~w) and context embeddings (~c).
The parameters of this model can then be learned
by maximising the log-likelihood over (w,c) pairs
in the dataset D,

J(θ) = ∑
(w,c)∈D

log p(c|w;θ). (2)

Guo et al. (2016) provide a multilingual exten-
sion for the skip-gram model, by requiring the
model to not only learn to predict English con-
texts, but also multilingual ones. This can be seen
as a simple adaptation of Firth (1957, p.11), i.e.,
you shall judge a word by the multilingual com-
pany it keeps. Hence, the vectors for, e.g., dog
and perro ought to be close to each other in such
a model. This assumes access to multilingual par-
allel data, as word alignments are used in order
to determine which words comprise the multilin-
gual context of a word. Whereas Guo et al. (2016)
only evaluate their approach on the relatively simi-
lar languages English, French and Spanish, we ex-
plore a more typological diverse case, as we ap-
ply this method to English, Spanish and Arabic.
We use the same parameter settings as Guo et al.
(2016).

2.2 Learning embeddings
We train 100-dimensional multilingual embed-
dings on the Europarl (Koehn, 2005) and UN
corpora (Ziemski et al., 2016). Word align-
ment, which is required for the training of mul-
tilingual embeddings, is performed using the Ef-
maral word-alignment tool (Östling and Tiede-
mann, 2016). This allows us to extract a large
amount of multilingual (w,c) pairs. We then use
these pairs in order to learn multilingual embed-
dings, by applying the word2vecf tool (Levy and
Goldberg, 2014).

3 Method

3.1 System architecture
We use a relatively simple neural network ar-
chitecture, consisting of an input layer with pre-

trained word embeddings and a network of fully
connected layers. Given word representations for
each word in our sentence, we take the simplis-
tic approach of averaging the vectors across each
sentence. The resulting sentence-level represen-
tations are then concatenated and passed through
a single fully connected layer, prior to the output
layer. In order to prevent any shift from occur-
ring in the embeddings, we do not update these
during training. The intuition here, is that we do
not want the representation for, e.g., dog to be up-
dated, which might push it further away from that
of perro. We expect this to be especially important
in cases where we train on a single language, and
evaluate on another.

We apply dropout (p = 0.5) between each layer
(Srivastava et al., 2014). All weights are ini-
tialised using the approach in Glorot and Bengio
(2010). We use the Adam optimisation algorithm
(Kingma and Ba, 2014), monitoring the categori-
cal cross entropy of a one-hot representation of the
(rounded) sentence similarity score, while sanity-
checking against the scores obtained as measured
with Pearson correlation. All systems are trained
using a batch size of 40 sentence pairs, over a max-
imum of 50 epochs, using early stopping. Hyper-
parameters are kept constant in all conditions.

3.2 Data

We use all available data from previous editions of
the SemEval shared tasks on (cross-lingual) STS.
An overview of the available data is shown in Ta-
ble 1.

Language pair N sentences

English / English 3750
English / Spanish 1000
English / Arabic 2162
Spanish / Spanish 1620
Arabic / Arabic 1081

Table 1: Available data for (cross-lingual) STS
from the SemEval shared task series.

4 Experiments and Results

We aim to investigate whether using a multilin-
gual input representation and shared weights al-
low us to ignore languages in STS. We first train
and evaluate single-source trained systems (i.e. on
a single language pair), and evaluate this both us-

155



ing the same language pair as target, and on all
other target language pairs.1 Secondly, we inves-
tigate the effect of bundling training data together,
investigating which language pairings are helpful
for each other. We measure performance between
gold similarities and system output using the Pear-
son correlation measure, as this is standard in the
SemEval STS shared tasks. We first present re-
sults on the development sets, and finally the offi-
cial shared task evaluation results.

4.1 Single-source training

Results when training on a single source cor-
pus are shown in Table 2. Training on the tar-
get language pair generally yields the highest
results, except for one case. When evaluating
on Arabic/Arabic sentence pairs, training on En-
glish/Arabic texts yields comparable, or slightly
better, performance than when training on Ara-
bic/Arabic.

HHHHHHTest
Train en/en en/es en/ar es/es ar/ar

en/en 0.69 0.07 -0.04 0.64 0.54
en/es 0.19 0.27 0.00 0.18 -0.04
en/ar -0.44 0.37 0.73 -0.10 0.62
es/es 0.61 0.07 0.12 0.65 0.50
ar/ar 0.59 0.52 0.73 0.59 0.71

Table 2: Single-source training results (Pearson
correlations). Columns indicate training language
pairs, and rows indicate testing language pairs.
Bold numbers indicate best results per row.

4.2 Multi-source training

We combine training corpora in order to investi-
gate how this affects evaluation performance on
the language pairs in question. In the first con-
dition, we copy the single-source setup, except
for that we also add in the data belonging to the
source-pair at hand, e.g., training on both En-
glish/Arabic and Arabic/Arabic when evaluating
on Arabic/Arabic (see Table 3).

We observe that the monolingual language pair-
ings (en/en, es/es, ar/ar) appear to be beneficial for
one another. We therefore run an ablation exper-
iment, in which we train on two out of three of
these language pairs, and evaluate on all three. Not

1This setting can be seen as a sort of model transfer.

HHHHHHTest
Train en/en en/es en/ar es/es ar/ar

en/en 0.69 0.68 0.67 0.69 0.71
en/es 0.22 0.27 0.30 0.22 0.24
en/ar 0.72 0.72 0.73 0.71 0.72
es/es 0.63 0.60 0.63 0.65 0.66
ar/ar 0.71 0.72 0.75 0.70 0.71

Table 3: Training results with one source in ad-
dition to in-language data (Pearson correlations).
Columns indicate added training language pairs,
and rows indicate testing language pairs. Bold
numbers indicate best results per row.

including any Spanish training data yields compa-
rable performance to including it (Table 4).

PPPPPPPPPTest
Ablated en/en es/es ar/ar none

en/en 0.60 0.69 0.69 0.65
es/es 0.64 0.64 0.67 0.60
ar/ar 0.68 0.66 0.58 0.72

Table 4: Ablation results (Pearson correlations).
Columns indicate ablated language pairs, and rows
indicate testing language pairs. The none column
indicates no ablation, i.e., training on all three
monolingual pairs. Bold indicates results when
not training on the language pair evaluated on.

4.3 Shared Task Test Results

The results from the official SemEval-2017 eval-
uation are shown in Table 5. Although our re-
sults for Spanish/Spanish and English/English are
in line with our development results, the results
for all other language pairs are far lower than ex-
pected. This might be explained by overfitting to
the training/dev sets we use. After the official eval-
uation period ended, we also attempted to perform
a sanity check. We allowed our model to tune on
the gold data, which surprisingly did not increase
performance particularly much. We therefore sus-
pect that the poor system performance we observe,
may be partially owed to two factors: i) overfitting
on the tracks involving Arabic, as we did not ap-
ply any type of pre-processing, and our vector set
was tuned on relatively little Arabic data; ii) dis-
crepancies between the mix of training-data (and
possibly annotators) from previous editions of the

156



Primary ar/ar ar/en es/es es/en es/en (wmt) en/en en/tr

Single-source 0.3148 0.2892 0.1045 0.6613 0.2389 0.0305 0.6906 0.1884
Multi-source 0.2938 0.3120 0.1288 0.6920 0.1002 0.0162 0.6877 0.1195
Ablation 0.2145 0.0033 0.1098 0.5465 0.2262 0.0199 0.5057 0.0902

Table 5: Results on SemEval-2017 Shared Task Test sets.

shared task, and test data in this year’s edition.
An interesting option to attempt to solve part of

this problem, would be to frame this as a multi-
task learning problem. This could be done by as-
signing each year’s data set a separate output layer.
Should annotator conventions differ, e.g., if a score
of 2.5 in 2015 is equivalent to a score of 3.5 in
2016, the network should be able to learn this and
compensate for such effects.

5 Discussion

In all cases, training on the target language pair is
beneficial. We also observe that using multilingual
embeddings is crucial for multilingual approaches,
as monolingual embeddings naturally only yield
on-par results in monolingual settings. This is due
to the fact that using the shared language-agnostic
input representation allows us to take advantage
of linguistic regularities across languages, which
we obtain solely from observing distributions be-
tween languages in parallel text. Using monolin-
gual word representations, however, there is no
similarity between, e.g., dog and perro to rely on
to guide learning.

For the single-source training, we in one case
observe somewhat better performance using other
training sets than the in-language one: training
on English/Arabic outperforms training on Ara-
bic/Arabic, when evaluating on Arabic/Arabic.
We expected this to be due to differing data set
sizes (English/Arabic is about twice as big). Con-
trolling for this does, indeed, bring the perfor-
mance of training on English/Arabic close to train-
ing on Arabic/Arabic. However, combining these
datasets increases performance further (Table 3).

In single-source training, we also observe that
certain source languages do not offer any gener-
alisation over certain target languages. Interest-
ingly, certain combinations of training/testing lan-
guage pairs yield very poor results. For instance,
training on English/English yields very poor re-
sults when evaluating on English/Arabic, and vice
versa. The same is observed for the combination

Spanish/Spanish and English/Arabic. This may be
explained by domain differences in training and
evaluation data. A general trend appears to be
that either monolingual training pairs and evalua-
tion pairs, or cross-lingual pairs with some overlap
(e.g. English/Arabic, Arabic/Arabic) is beneficial.

The positive results on pairings without any lan-
guage overlap are particularly promising. Train-
ing on English/English yields results not too
far from training on the source language pairs,
for Spanish/Spanish and Arabic/Arabic. Simi-
lar results are observed when training on Span-
ish/Spanish and evaluating on English/English and
Arabic/Arabic, as well as when training on Ara-
bic/Arabic and evaluating on English/English and
Spanish/Spanish. This indicates that we can esti-
mate STS relatively reliably, even without assum-
ing any existing STS data for a given language.

6 Conclusions and Future Work

Multilingual word representations allow us to
leverage more available data for multilingual
learning of semantic textual similarity. We have
shown that relatively high STS performance can
be achieved for languages without assuming exist-
ing STS annotation, and relying solely on paral-
lel texts. An interesting direction for future work
is to investigate how multilingual character-level
representations can be included, perhaps learn-
ing morpheme-level representations and mappings
between these across languages. Leveraging ap-
proaches to learning multilingual word represen-
tations from smaller data sets would also be in-
teresting. For instance, learning such representa-
tions from only the New Testament, would allow
for STS estimation for more than 1,000 languages.

Acknowledgments

We would like to thank the Center for Informa-
tion Technology of the University of Groningen
for providing access to the Peregrine high perfor-
mance computing cluster.

157



References
Hanan Aldarmaki and Mona Diab. 2016. GWU NLP at

SemEval-2016 Shared Task 1: Matrix factorization
for crosslingual STS. In Proceedings of SemEval
2016. pages 663–667.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017). Association for Computa-
tional Linguistics, Vancouver, Canada, pages 1–5.
http://www.aclweb.org/anthology/S17-2001.

John R Firth. 1957. A synopsis of linguistic theory,
1930-1955. Blackwell.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Aistats. volume 9, pages 249–256.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A representation learn-
ing framework for multi-source transfer parsing. In
Proc. of AAAI.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit.. Phuket, Thailand.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In ACL. pages 302–308.

Chi-kiu Lo, Cyril Goutte, and Michel Simard. 2016.
Cnrc at semeval-2016 task 1: Experiments in
crosslingual semantic textual similarity. Proceed-
ings of SemEval 2016 pages 668–673.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Robert Östling and Jörg Tiedemann. 2016. Efficient
word alignment with markov chain monte carlo.
The Prague Bulletin of Mathematical Linguistics
106(1):125–146.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.

Micha Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel cor-
pus v1.0. In Nicoletta Calzolari, Khalid Choukri,
Thierry Declerck, Sara Goggi, Marko Grobelnik,
Bente Maegaard, Joseph Mariani, Helene Mazo,
Asuncion Moreno, Jan Odijk, and Stelios Piperidis,

editors, Proceedings of the Tenth International Con-
ference on Language Resources and Evaluation
(LREC 2016). European Language Resources Asso-
ciation (ELRA), Paris, France.

158


