683

Coling 2010: Poster Volume, pages 683–691,

Beijing, August 2010

Linguistic Cues for Distinguishing Literal and Non-Literal Usages

Linlin Li and Caroline Sporleder

Department of Computational Linguistics

Saarland University

{linlin, csporled}@coli.uni-saarland.de

Abstract

We investigate the effectiveness of differ-
ent linguistic cues for distinguishing lit-
eral and non-literal usages of potentially
idiomatic expressions. We focus specif-
ically on features that generalize across
different target expressions. While id-
ioms on the whole are frequent, instances
of each particular expression can be rela-
tively infrequent and it will often not be
feasible to extract and annotate a sufﬁ-
cient number of examples for each expres-
sion one might want to disambiguate. We
experimented with a number of different
features and found that features encoding
lexical cohesion as well as some syntac-
tic features can generalize well across id-
ioms.

Introduction

1
Nonliteral expressions are a major challenge in
NLP because they are (i) fairly frequent and (ii)
often behave idiosyncratically. Apart from typi-
cally being semantically more or less opaque, they
can also disobey grammatical constraints (e.g., by
and large, lie in wait). Hence, idiomatic expres-
sions are not only a problem for semantic anal-
ysis but can also have a negative effect on other
NLP applications (Sag et al., 2001), such as pars-
ing (Baldwin et al., 2004).

To process non-literal language correctly, NLP
systems need to recognise such expressions au-
tomatically. While there has been a signiﬁcant
body of work on idiom (and more generally multi-
word expression) detection (see Section 2), un-
til recently most approaches have focused on
a type-based classiﬁcation, dividing expressions
into “idiomatic” or “not idiomatic” irrespective of
their actual use in a discourse context. However,

while some expressions, such as by and large, al-
ways have a non-compositional, idiomatic mean-
ing, many other expressions, such as break the ice
or spill the beans, can be used literally as well as
idiomatically and for some expressions, such as
drop the ball, the literal usage can even dominate
in some domains. Consequently, those expres-
sions have to be disambiguated in context (token-
based classiﬁcation).

We investigate how well models for distin-
guishing literal and non-literal use can be learned
from annotated examples. We explore different
types of features, such as the local and global con-
text, syntactic properties of the local context, the
form of the expression itself and properties re-
lating to the cohesive structure of the discourse.
We show that several feature types work well for
this task. However, some features can generalize
across speciﬁc idioms, for instance features which
compute how well an idiom “ﬁts” its surrounding
context under a literal or non-literal interpretation.
This property is an advantage because such fea-
tures are not restricted to training data for a spe-
ciﬁc target expression but can also beneﬁt from
data for other idioms. This is important because,
while idioms as a general linguistic class are rela-
tively frequent, instances of each particular idiom
are much more difﬁcult to ﬁnd in sufﬁcient num-
bers. The situation is exacerbated by the fact the
distributions of literal vs. non-literal usage tend
to be highly skewed, with one usage (often the
non-literal one) being much more frequent than
the other. Finding sufﬁcient examples of the mi-
nority class can then be difﬁcult, even if instances
are extracted from large corpora. Furthermore, for
highly skewed distributions, many more majority
class examples have to be annotated to obtain an
acceptable number of minority class instances.

We show that it is possible to circumvent this
problem by employing a generic feature space that

684

looks at the cohesive ties between the potential id-
iom and its surrounding discourse. Such features
generalize well across different expressions and
lead to acceptable performance even on expres-
sions unseen in the training set.

2 Related Work
Until recently, most studies on idiom classiﬁ-
cation focus on type-based classiﬁcation; sofar
there are only comparably few studies on token-
based classiﬁcation. Among the earliest studies
on token-based classiﬁcation were the ones by
Hashimoto et al. (2006) on Japanese and Katz
and Giesbrecht (2006) on German. Hashimoto et
al. (2006) present a rule-based system in which
lexico-syntactic features of different idioms are
hard-coded in a lexicon and then used to distin-
guish literal and non-literal usages. The features
encode information about the passivisation, argu-
ment movement, and the ability of the target ex-
pression to be negated or modiﬁed. Katz and
Giesbrecht (2006) compute meaning vectors for
literal and non-literal examples in the training set
and then classify test instances based on the close-
ness of their meaning vectors to those of the train-
ing examples. This approach was later extended
by Diab and Krishna (2009), who take a larger
context into account when computing the feature
vectors (e.g., the whole paragraph) and who also
include prepositions and determiners in addition
to content words.

Cook et al. (2007) and Fazly et al. (2009) take a
different approach, which crucially relies on the
concept of canonical form (CForm).
It is as-
sumed that for each idiom there is a ﬁxed form
(or a small set of those) corresponding to the syn-
tactic pattern(s) in which the idiom normally oc-
curs (Riehemann, 2001).The canonical form al-
lows for inﬂectional variation of the head verb but
not for other variations (such as nominal inﬂec-
tion, choice of determiner etc.). It has been ob-
served that if an expression is used idiomatically,
it typically occurs in its canonical form (Riehe-
mann, 2001). Cook et al. exploit this behaviour
and propose an unsupervised method in which an
expression is classiﬁed as idiomatic if it occurs in
canonical form and literal otherwise. Canonical
forms are determined automatically using a statis-

tical, frequency-based measure.

Birke and Sarkar (2006) model literal vs. non-
literal classiﬁcation as a word sense disambigua-
tion task and use a clustering algorithm which
compares test instances to two seed sets (one with
literal and one with non-literal expressions), as-
signing the label of the closest set.

Sporleder and Li (2009) propose another un-
supervised method which detects the presence or
absence of cohesive links between the component
words of the idiom and the surrounding discourse.
If such links can be found the expression is clas-
siﬁed as literal otherwise as non-literal. Li and
Sporleder (2009) later extended this work by com-
bining the unsupervised classiﬁer with a second-
stage supervised classiﬁer.

Hashimoto and Kawahara (2008) present a su-
pervised approach to token-based idiom distinc-
tion for Japanese, in which they implement several
features, such as features known from other word
sense disambiguation tasks (e.g., collocations)
and idiom-speciﬁc features taken from Hashimoto
et al. (2006). Finally, Boukobza and Rappoport
(2009) also experimented with a supervised clas-
siﬁer, which takes into account various surface
features.

In the present work, we also investigate super-
vised models for token-based idiom detection. We
are speciﬁcally interested in which types of fea-
tures (e.g., local context, global context, syntac-
tic properties) perform best on this task and more
speciﬁcally which features generalize across id-
ioms.

3 Data
We used the data set created by Sporleder and Li
(2009), which consists of 13 English expressions
(mainly V+PP or V+NP) that can be used both
literally and idiomatically, such as break the ice
or play with ﬁre.1 To create the data set all in-
stances of the target expressions were extracted
from the Gigaword corpus together with ﬁve para-
graphs of context and then labelled manually as
’literal’ or ’non-literal’. Overall the data set con-
sists of just under 4,000 instances. For most ex-

1We excluded four expressions from the original data set
because their number of literal examples was very small (<
2).

685

pressions the distribution is heavily skewed to-
wards the idiomatic interpretation, however for
some, like drop the ball, the literal reading is more
frequent. The number of instances varies, rang-
ing from 15 for pull the trigger to 903 for drop
the ball. While the instances were extracted from
a news corpus, none of them are domain-speciﬁc
and all expressions also occur in the BNC, which
is a balanced, multi-domain corpus.

To compute the features which we extract in
the next section, all instances in our data sets
were part-of-speech tagged by the MXPOST tag-
ger (Ratnaparkhi, 1996), parsed with the Malt-
Parser2, and named entity tagged with the Stan-
ford NE tagger (Finkel et al., 2005). The lemma-
tization was done by RASP (Briscoe and Carroll,
2006).

4

Indicators of Idiomatic and Literal
Usage

In this study we are particularly interested in
which linguistic indicators work well for the task
of distinguishing literal and idiomatic language
use. The few previous studies have mainly looked
at the lexical context in which and expression
occurs (Katz and Giesbrecht, 2006; Birke and
Sarkar, 2006). However, other properties of the
linguistic context might also be useful. We dis-
tinguish these features into different groups and
discuss them in the following sections.

4.1 Global Lexical Context (glc)
That the lexical context might be a good indica-
tor for the usage of an expression is obvious when
one looks at examples as in (1) and (2), which sug-
gest that literal and non-literal usages of a speciﬁc
idiom co-occur with different sets of words. Non-
literal uses of break the ice (1), for instance, tend
to occur with words like discuss, bilateral or re-
lations, while literal usages (2) predictably occur
with, among others, frozen, cold or water. What
we are looking at here is the global lexical context
of an expression, i.e., taking into account previ-
ous and following sentences. We are speciﬁcally
looking for words which are either semantically
related (in a wide sense) to the literal or the non-

2http://maltparser.org/index.html

literal sense of the target expression. The presence
or absence of such words can be a good indicator
of how the expression is used in a context.

(1)

(2)

”Gujral will meet Sharif on Monday and dis-
cuss bilateral relations,” the Press Trust of India
added. The minister said Sharif and Gujral would
be able to ”break the ice” over Kashmir.

the cold penetrated
Meanwhile in Germany,
Cologne cathedral, where worshippers had to
break the ice on the frozen holy water in the font.

We implemented two sets of features which en-
code the global lexical context: salient words and
related words as described in Li and Sporleder
(2009). The former feature uses a variant of
tf.idf to identify words that are particulary salient
for different usages. The latter feature identiﬁes
words which are most strongly related to the com-
ponent words of the idiom.

We notice that sometimes several idioms co-
occur within the same instance. This is to say that
nonliteral usages may be indicators of each other
since authors may put them in a same context to
convey a speciﬁc opinion (e.g., irony). Due to this,
global lexical context features may also generalize
across idioms to some extend.

4.2 Local Lexical Context (locCont)
In addition to the global context, the local lex-
ical context, i.e., the words preceding and fol-
lowing the target expression, might also provide
important information. One obvious local clue
are words like literally or metaphorically speak-
ing, which when preceding or following an ex-
pression might indicate its usage. Unfortunately,
such clues are not only very rare (we only found
a handful in nearly 4,000 annotated examples) but
also not always reliable. For instance, it is not
difﬁcult to ﬁnd examples like (3) and (4) where
the word literally is used even though the idiom
clearly has a non-literal meaning.

(3)

(4)

literally
In the documentary the producer
spills the beans on the real deal behind the movie
production.

The new philosophy is blatantly permissive and lit-
erally passes the buck to the House’s other com-
mittees.

686

However, there are other local cues. For exam-
ple, we found that the word just before get ones
feet wet tends to indicate non-literal usage as in
(5). Non-literal usage can also be indicated by the
occurrence of the prepositions over or between af-
ter break the ice as in (1) and (6). While such
cues are not perfect they often make one usage
more likely than the other. Unlike the semanti-
cally based global cues, many local clues are more
rooted in syntax, i.e., local cues work because spe-
ciﬁc constructions tend to be more frequent for
one or the other usage.

(5)

(6)

The wiki includes a page of tasks suitable for those
just getting their feet wet.

Would the visit of the minister help break the ice
between India and Pakistan?

For example,

Another type of local cues involves selectional
idiomatic usage is
preferences.
probable if the subject of play with ﬁre is a coun-
try as in (7) or if break the ice is followed by a
with-PP whose NP refers to a person (8).

(7)

(8)

Dudayev repeated his frequent warnings that Rus-
sia was playing with ﬁre.

Edwards usually manages to break the ice with the
taciturn monarch.

Based on those observations, we encode which
words occur in a ten word window around the tar-
get expression, ﬁve pre-target words and ﬁve post-
target words, as the locCont features.

4.3 Discourse Cohesion (dc)
We implemented two features, related score and
discourse connectivity, which take into account
the cohesive structure of an expression in its con-
text as described by Li and Sporleder (2009).
In addition, we also included the prediction of
the cohesion graph proposed by Sporleder and Li
(2009) as an additional feature. These features
look at the lexical cohesion between an expression
and the surrounding discourse, so they are more
likely to generalize across different idioms.

4.4 Syntactic Structure (allSyn)
To capture syntactic effects, we encoded infor-
mation of the head node (heaSyn) of the tar-
get expression in the dependency tree (e.g., break

may:ROOT

visit:SUB

the:NMOD

of:NMOD

break:VMOD

ice:OBJ

minister:PMOD

the:NMOD

the:NMOD between

...

Figure 1: Dependency tree for a nonliteral exam-
ple of break the ice (The visit of the minister may
break the ice between India and Pakistan.)

in the dependency tree in Figure 1). The syn-
tactic features we encoded are the parent node
(parSyn), sibling nodes (sibSyn) and children
nodes (chiSyn) of the head node. These nodes in-
clude the following type of syntactic information:

Dependency Relation of the Verb Phrase The
whole idiomatic expression used as an object of
a preposition can be an indicative factor of id-
iomatic usage (see Example 9). This property is
captured by the heaSyn feature.

(9)

Ross headed back last week to Washington to brief
president Bill Clinton on the Hebron talks after
achieving a breakthrough in breaking the ice in the
Hebron talks by arranging an Arafat-Netanyahu
summit .

Modal Verbs
usually appear in the parent posi-
tion of the head verb (parSyn). Modals can be an
indicator of idiomatic usage such as may in Figure
1. In contrast, the modal had to is indicative that
the same phrase is used literally (Example 10).

(10)

Dad had to break the ice on the chicken troughs.

Subjects
can also provide clues about the usage
of an expression, e.g., if selectional preferences
are disobeyed. For instance, visit as a subject of
the verb phrase break the ice is an indicator of id-
iomatic usage (see Figure 1). Subjects typically
appear in the children position of the head verb
(chiSyn), but sometimes may appear in the sibling
position (sibSyn) as in Figure 1 .

Verb Subcat We also encode the arguments of
the head verb of the target expression. These ar-
guments can be, for example, additional PPs. This
feature encodes syntactic constraints and attempts

687

to model selectional restrictions. The likelihood
of subcategorisation frames may differ for the two
usages of an expression, e.g., non-literal expres-
sions often tend to have a shorter argument list.
For instance, the subcat frame <PP-on, PP-for>
intuitively seems more likely for literal usages of
the expression drop the ball (see Example 11)
than for non-literal ones, for which <PP-on> is
more likely (12). To capture subcategorisation be-
haviour, we encode the children nodes of the head
node (chiSyn).

(11)

US defender Alexi Lalas twice went close to forc-
ing an equaliser , ﬁrst with a glancing equaliser
from a Paul Caligiuri free kick and then from a
Wynalda corner when Prunea dropped the ball [on
the ground] only [for Tibor Selyme to kick fran-
tically clear] .

(12)

“Clinton dropped the ball [on this],” said John
Parachini.

Modiﬁers of the verb can also be indicative of
the usage of the target expression. For example,
in 13, the fact that the phrase get one’s feet wet is
modiﬁed by the adverb just suggest that it is used
idiomatically. Similar to verb subcat, modiﬁers
are often appear in the children position (chiSyn).

(13)

The wiki includes a page of tasks suitable for those
just getting their feet wet.

Coordinated Verb Which verbs are coordi-
nated with the target expression, if any, can also
provide cues for the intended interpretation. For
example, in (14), the fact that break the ice is co-
ordinated with fall suggest that it is used literally.
The coordinated verb can appear at the sibling po-
sition, children position, or some other position of
the head verb depending on the parser. The Malt-
parser tends to put the coordinated verbs in the
children position (chiSyn).

(14)

They may break the ice and fall through.

4.5 Other Features
Named Entities (ne)
can also indicate the us-
age of an expression. For instance, a country
name in the subject position of the target expres-
sion break the ice is a strong indicator of this
phrase being used idiomatically (see Example 7).
Diab and Bhutada (2009) ﬁnd that NE-features
perform best. They used a commercial NE-tagger

with 19 classes. We used the Stanford NE tag-
ger (Finkel et al., 2005), and encoded three named
entity classes (“person”, “location”, “organisza-
tion”) in the feature vector.

Indicative Terms (iTerm) Some words such as
literally, proverbially are also indicative of literal
or idiomatic usages. We encoded the frequencies
of those indicative terms as features.

Scare Quotes (quote) This feature encodes
whether the idiom is marked off by scare quotes,
which often indicates non-literal usage (15).

(15)

Do consider “getting your feet wet” online, using
some of the technology that is now available to us.

5 Experiments

In the previous section we discussed different lin-
guistic cues for idiom usage. To determine which
of these cues work best for the task and which
ones generalize across different idioms, we car-
ried out three experiments. In the ﬁrst one (Sec-
tion 5.1) we trained one model for each idiom (see
Section 3) and tested the predictiveness of each
feature type individually as well as all features to-
gether. In the second experiment (Section 5.2), we
trained one generic model for all idioms and deter-
mined how the performance of this model differs
from the idiom-speciﬁc models. Speciﬁcally we
wanted to know whether the model would bene-
ﬁt from the additional training data available by
combining information from several idioms. Fi-
nally (Section 5.3), we tested the generic model on
unseen idioms to determine whether these could
be classiﬁed based on generic properties even if
training data for the target expressions had not
been seen.

Idiom Speciﬁc Models

5.1
The ﬁrst question we wanted to answer was how
difﬁcult token-based idiom classiﬁcation is and
which of the features we deﬁned in the previous
section work well for this task. We implemented
a speciﬁc classiﬁer for each of the idioms in the
data set. We trained one model for all features
in combination and one for each individual fea-
ture. Because the data set is not very big we de-
cided to run these experiments in 10-fold stratiﬁed

688

cross-validation mode. We used the SVM classi-
ﬁer (SMO) from Weka.3

Table 1 shows the results. We report the pre-
cision (Prec.), recall (Rec.) and F-Score for the
literal class, as well as the accuracy. Note that due
to the imbalance in the data set, accuracy is not a
very informative measure here; a classiﬁer always
predicting the majority class would already obtain
a relatively high accuracy. The literal F-Score ob-
tained for individual idioms varies from 38.10%
for bite one’s tongue to 96.10% for bounce of the
wall. However, the data sets for the different id-
ioms are relatively small and it is impossible to
say whether performance differences on individ-
ual idioms are accidental, or due to differences
in training set size or due to some inherent dif-
ﬁculty of the individual idiom. Thus we chose not
to report the performance of our models on indi-
vidual idioms but on the whole data set for which
the numbers are much more reliable. The ﬁnal
performance confusion matrix is the sum over all
individual idiom confusion matrices.

Avg. literal

feature
all
glc+dc
allSyn
heaSyn
parSyn
chiSyn
sibSyn
locCont
ne
iTerm
quote
Basemaj

Prec.
89.84
90.42
76.30
76.64
76.43
76.49
76.27
76.51
76.49
76.51
76.51
76.71

Rec.
77.06
76.44
86.13
85.77
88.34
88.22
88.34
88.34
88.22
88.34
88.34
88.34

Avg.
F-Score Acc.
93.36
82.96
82.85
93.36
91.48
80.92
91.53
80.95
91.84
81.96
81.94
91.84
91.78
81.86
91.86
82.00
81.94
91.84
91.86
82.00
91.86
82.00
82.00
91.86

Table 1: Performance of idiom-speciﬁc models
(averaged over different idioms), 10-fold stratiﬁed
cross-validation.

The Baseline (Base) is built based on predict-
ing the majority class for each expression. This
means predicting literal for the expressions which
consist of more literal examples and nonliteral for
the expressions consisting of more nonliteral ex-

amples. We notice the baseline gets a fairly high
performance (Acc.=91.86%).

The results show that the expressions can be
classiﬁed relatively reliably by the proposed fea-
tures. The performance beats the majority base-
line statistically signiﬁcantly (p = 0.01, χ2 test).
We noticed that parSyn, chiSyn, locCont, iTerm
and quote features are too sparse. These indi-
vidual features cannot guide the classiﬁer. As
a result, the classiﬁer only predicts the majority
class which results in a performance similar to
the baseline. Some of the syntactic features are
less sparse and they get different results from the
baseline classiﬁer, however, the performances of
these features are actually worse than the baseline.
This may be due to the relatively small training
size in each idiom speciﬁc model. When adding
those features together with statistical-based fea-
tures (glc+dc), the performance of the literal class
can be improved slightly. However, we did not ob-
serve any performance increase on the accuracy.

5.2 Generic Models
Having veriﬁed that literal and idiomatic usages
can be distinguished with some success by train-
ing expression-speciﬁc models, we carried out
a second experiment in which we merged the
data sets for different expressions and trained one
generic model. We wanted to see whether a
generic model, which has access to more training
data, performs better and whether some features,
e.g., the cohesion features proﬁt more from this.
The experiment was again run in 10-fold stratiﬁed
cross-validation mode (using 10% from each id-
iom in the test set in each fold).

Table 2 shows the results. The baseline classi-
ﬁer always predict the majority class ‘nonliteral’.
Note that the result of this baseline is different
from the majority baseline in the idiom speciﬁc
model. In the idiom speciﬁc model, there are three
expressions 4 for which the majority class is ‘lit-
eral’.

Unsurprisingly, the F-Score and accuracy of the
combined feature set drops a bit. However, the
performance still statistically signiﬁcantly beats
the majority baseline classiﬁer (p << 0.01,
χ2 test). Similar to previous observation,
the

3http://www.cs.waikato.ac.nz/ml/weka/

4I.e., bounce off the wall, drop the ball, pull the trigger

689

Avg. literal

feature
all
glc+dc
allSyn
heaSyn
sibSyn
ne
iTerm
Basemaj

Prec.
89.59
82.53
50.83
50.57
33.33
62.45
40.00

–

Rec.
65.77
60.86
59.88
59.88
0.86
20.00
0.25

–

Avg.
F-Score Acc.
89.90
73.22
89.08
70.06
54.99
79.42
79.29
54.83
78.83
1.67
30.30
80.69
78.99
0.49
79.01

–

Table 2: Performance of the generic model (av-
eraged over different idioms), 10-fold stratiﬁed
cross-validation.

statistical-based features (glc+dc) work the best,
while the syntactic features are also helpful. How-
ever, the local context, iTerm, quote features are
very sparse and, as in the idiom-speciﬁc experi-
ments, the performances of these features are sim-
ilar to the majority baseline classiﬁer. We ex-
cluded them from the Table 2.

The numbers show that the syntactic features
help more in this model compared with the idiom-
speciﬁc model. When including these features, lit-
eral F-Score increases by 3.16% while accuracy
increases by 0.9%.
It seems that the syntactic
features beneﬁt from the increased training set.
This is evidence that these features can generalize
across idioms. For instance, the phrase “The US”
on the subject position may be not only indicative
of the idiomatic usage of break the ice, but also of
idiomatic usage of drop the ball.

We found that the indicative terms are rare in
our corpus. This is the reason why the recall rate
of the indicative terms is very low (0.25%). The
indicative terms are not very predictive of literal or
non-literal usage, since the precision rate is also
relatively low (40%), which means those words
can be used in both literal and nonliteral cases.

5.3 Unseen Idioms
In our ﬁnal experiment, we tested whether a
generic model can also be applied to completely
new expressions, i.e., expressions for which no
instances have been seen in the data set. Such a
behaviour would be desireable for practical pur-
poses as it is unrealistic to label training data for

each idiom the model might possibly encounter in
a text. To test whether the generic model does in-
deed generalize to unseen expressions, we test it
on all instances of a given expression while train-
ing on the rest of the expressions in the dataset.
That is, we used a modiﬁed cross-validation set-
ting, in which each fold contains instances from
one expression in the test set. Since our dataset
contains 13 expressions, we run a 13-fold cross
validation. The ﬁnal confusion matrix is the sum
over each confusion matrix in each round.

Avg. literal

feature
all
glc+dc
allSyn
heaSyn
sibSyn
ne
iTerm
Basemaj

Prec.
96.70
96.93
52.54
51.35
55.56
61.89
66.67

–

Rec.
81.65
77.00
58.77
59.47
2.32
19.05
0.7
–

Avg.
F-Score Acc.
95.41
88.54
85.83
94.48
79.52
55.48
78.96
55.11
78.38
4.46
29.13
79.87
78.36
1.38
79.01

–

Table 3: Performance of the generic model on un-
seen idioms (cross validation, instances from each
idiom are chosen as test set for each fold)

The results are shown in Table 3. Similar to the
generic model, we found that the cohesion fea-
tures and syntactic features do generalize across
expressions. Statistical features (glc+dc) perform
well in this experiment. When including more
linguistically orientated features, the performance
can be further increased by almost 1%.
In line
with former observations, the sparse features men-
tioned in the former two experiments also do not
work for this experiments. We also excluded them
from the table.

One interesting ﬁnding about this experiment of
this model is that the F-Score is higher than for the
“generic model”. This is counter-intuitive, since
in the generic model, each idiom in the testing set
has examples in the training set, thus, we might
expect the performance to be better due to the fact
that instances from the same expression appear-
ing in the training set are more informative com-
pared with instances from different idioms. Fur-
ther analysis revealed that there are some expres-
sions for which it may actually be beneﬁcial to

690

train on other expressions, as the evidence of some
features may be misleading.

feature
all
glc+dc
allSyn
heaSyn

literal F-S.
Spe.
Gen.
91.79
86.85
88.84
86.75
85.71
71.94
85.79
71.94

Acc.

Spe.
80.67
80.67
75.28
75.39

Gen.
88.37
84.61
61.13
61.13

Table 4: Comparing the performance of the idiom
drop the ball on the idiom speciﬁc model (Spe.)
and generic model (Gen.)

Table 4 shows the comparison of the perfor-
mance of drop the ball on the idiom speciﬁc
model and the generic model on unseen idioms.
It can be seen that the statistical features (glc+dc)
work better for the model that is trained on the in-
stances from other idioms than the model which
is trained on the instances of the target expression
itself. We found this is due to the fact that drop the
ball is especially difﬁcult to classify with the dis-
course cohesion features (dc). The literal cases are
often found in a context containing words, such
as fault, mistake, fail, and miss, which are used
to describe a scenario in a baseball game,5 while,
on the other hand, those context words are also
closely semantically related to the idiomatic read-
ing of drop the ball. This means the classiﬁer can
be mislead by the cohesion features of the literal
instances of this idiom in the training set, since
they exhibit strong idiomatic cohesive links with
the target expression. When excluding drop the
ball from the training set, the cohesive links in
the training data are less noisy. Thus, the perfor-
mance increases. Unsurprisingly, the performance
of syntactic features works better for the idiom
speciﬁc model compared with the unseen idiom
model.

6 Conclusion
Idioms on the whole are frequent but instances of
each particular idiom can be relatively infrequent
(even for common idioms like “spill the beans”).
The classes can also be fairly imbalanced, with
one class (typically the nonliteral interpretation)

5The corpus contains many sports news text

being much more frequent than the other. This
causes problems for training data generation. For
idiom speciﬁc classiﬁers, it is difﬁcult to obtain
large data sets even when extracting from large
corpora and it is even more difﬁcult to ﬁnd suf-
ﬁcient examples of the minority class.
In order
to address this problem, we looked for features
which can generalize across idioms.

We found that statistical features (glc+dc) work
best for distinguishing literal and nonliteral read-
ings. Certain linguistically motivated features can
further boost the performance. However, those
linguistic features are more likely to suffer from
data sparseness, as a result, they often only predict
the majority class if used on their own. We also
found that some of the features that we designed
generalize well across idioms. The cohesion fea-
tures have the best generalization ability, while
syntactic features can also generalize to some ex-
tent.

Acknowledgments
This work was funded by the DFG within the
Cluster of Excellence MMCI.

References
Baldwin, Timothy, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephen Oepen. 2004. Road-testing
the English resource grammar over the British Na-
tional Corpus.
In Proc. LREC-04, pages 2047–
2050.

Birke, Julia and Anoop Sarkar. 2006. A clustering
approach for the nearly unsupervised recognition of
nonliteral language. In Proceedings of EACL-06.

Boukobza, Ram and Ari Rappoport. 2009. Multi-
word expression identiﬁcation using sentence sur-
face features. In Proceedings of EMNLP-09.

Briscoe, Ted and John Carroll.

2006. Evaluating
the accuracy of an unlexicalized statistical parser
on the PARC DepBank.
In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 41–48.

Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identiﬁcation of idiomatic
expressions in context. In Proceedings of the ACL-
07 Workshop on A Broader Perspective on Multi-
word Expressions.

691

Diab, Mona and Pravin Bhutada. 2009. Verb noun
construction mwe token classiﬁcation. In Proceed-
ings of the Workshop on Multiword Expressions:
Identiﬁcation, Interpretation, Disambiguation and
Applications, pages 17–22.

Diab, Mona T. and Madhav Krishna. 2009. Unsuper-
vised classiﬁcation of verb noun multi-word expres-
sion tokens. In CICLing 2009, pages 98–110.

Fazly, Afsaneh, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identiﬁcation of
idiomatic expressions. Computational Linguistics,
35(1):61–103.

Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling.
In Proceedings of ACL-05, pages 363–
370.

Hashimoto, Chikara and Daisuke Kawahara. 2008.
Construction of an idiom corpus and its application
to idiom identiﬁcation based on WSD incorporating
idiom-speciﬁc features. In Proceedings of EMNLP-
08, pages 992–1001.

Hashimoto, Chikara, Satoshi Sato, and Takehito Ut-
suro. 2006. Japanese idiom recognition: Drawing
a line between literal and idiomatic meanings.
In
Proceedings of COLING/ACL-06, pages 353–360.

Katz, Graham and Eugenie Giesbrecht. 2006. Au-
tomatic identiﬁcation of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the ACL/COLING-06 Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties.

Li, Linlin and Caroline Sporleder. 2009. Contextual
idiom detection without labelled data. In Proceed-
ings of EMNLP-09.

Ratnaparkhi, Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of EMNLP-
96.

Riehemann, Susanne. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. thesis,
Stanford University.

Sag, Ivan A., Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
expressions: a pain in the neck for NLP. In Lecture
Notes in Computer Science.

Sporleder, Caroline and Linlin Li. 2009. Unsuper-
vised recognition of literal and non-literal use of id-
iomatic expressions. In Proceedings of EACL-09.

