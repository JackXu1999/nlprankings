



















































An Imitation Learning Approach to Unsupervised Parsing


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3485–3492
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3485

An Imitation Learning Approach to Unsupervised Parsing

Bowen Li† Lili Mou‡ Frank Keller†
†Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh, UK

‡University of Waterloo, Canada
bowen.li@ed.ac.uk, doublepower.mou@gmail.com

keller@inf.ed.ac.uk

Abstract

Recently, there has been an increasing inter-
est in unsupervised parsers that optimize se-
mantically oriented objectives, typically us-
ing reinforcement learning. Unfortunately, the
learned trees often do not match actual syn-
tax trees well. Shen et al. (2018) propose a
structured attention mechanism for language
modeling (PRPN), which induces better syn-
tactic structures but relies on ad hoc heuristics.
Also, their model lacks interpretability as it is
not grounded in parsing actions. In our work,
we propose an imitation learning approach to
unsupervised parsing, where we transfer the
syntactic knowledge induced by the PRPN to
a Tree-LSTM model with discrete parsing ac-
tions. Its policy is then refined by Gumbel-
Softmax training towards a semantically ori-
ented objective. We evaluate our approach
on the All Natural Language Inference dataset
and show that it achieves a new state of the
art in terms of parsing F -score, outperforming
our base models, including the PRPN.1

1 Introduction

From a linguistic perspective, a natural language
sentence can be thought of as a set of nested con-
stituents in the form of a tree structure (Partee
et al., 2012). When a parser is trained on la-
beled treebanks, the predicted constituency trees
are useful for various natural language processing
(NLP) tasks, including relation extraction (Verga
et al., 2016), text simplification (Narayan and Gar-
dent, 2014), and machine translation (Aharoni and
Goldberg, 2017). However, expensive expert an-
notations are usually required to create treebanks.

Unsupervised parsing (also known as grammar
induction or latent tree learning) aims to learn
syntactic structures without access to a treebank

1Our code can be found at
https://github.com/libowen2121/
Imitation-Learning-for-Unsup-Parsing

during training, with potential uses in low resource
or out-of-domain scenarios. In early approaches,
unsupervised parsers were trained by optimizing
the marginal likelihood of sentences (Klein and
Manning, 2014). More recent deep learning ap-
proaches (Yogatama et al., 2017; Maillard et al.,
2017; Choi et al., 2018) obtain latent tree struc-
tures by reinforcement learning (RL). Typically,
this involves a secondary task, e.g., a language
modeling objective or a semantic task. How-
ever, Williams et al. (2018a) have pointed out that
these methods do not yield linguistically plausible
structures, and have low self-agreement when ran-
domly initialized multiple times.

Recently, Shen et al. (2018) proposed the
parsing-reading-predict network (PRPN), which
performs language modeling with structured at-
tention. The model uses heuristics to induce tree
structures from attention scores, and in a repli-
cation was found to be the first latent tree model
to produce syntactically plausible structures (Htut
et al., 2018). Structured attention in the PRPN
is formalized as differentiable continuous vari-
ables, making the model easy to train. But a ma-
jor drawback is that the PRPN does not model
tree-building operations directly. These operations
need to be stipulated externally, in an ad hoc infer-
ence procedure which is not part of the model and
cannot be trained (see Section 3).

In this paper, we propose an imitation learning
framework that combines the continuous PRPN
with a Tree-LSTM model with discrete parsing ac-
tions, both trained without access to labeled parse
trees. We exploit the advantages of the PRPN
by transferring its knowledge to a discrete parser
which explicitly models tree-building operations.
We accomplish the knowledge transfer by train-
ing the discrete parser to mimic the behavior of
the PRPN. Its policy is then refined using straight-
through Gumbel-Softmax (ST-Gumbel, Jang et al.,

bowen.li@ed.ac.uk
doublepower.mou@gmail.com
keller@inf.ed.ac.uk
https://github.com/libowen2121/Imitation-Learning-for-Unsup-Parsing
https://github.com/libowen2121/Imitation-Learning-for-Unsup-Parsing


3486

2017) trained with a semantic objective, viz., nat-
ural language inference (NLI).

We evaluate our approach on the All Natu-
ral Language Inference dataset and show that it
achieves a new state of the art in terms of parsing
F -score, outperforming our base models, includ-
ing the PRPN. Our work also shows that semantic
objectives can improve unsupervised parsing, con-
trary to earlier claims (Williams et al., 2018a; Htut
et al., 2018).

2 Related Work

Recursive neural networks are a type of neural net-
work which incorporates syntactic structures for
sentence-level understanding tasks. Typically, re-
cursive neural network models assume that an an-
notated treebank or a pretrained syntactic parser is
available (Socher et al., 2013; Tai et al., 2015; Kim
et al., 2019a), but recent work pays more attention
to learning syntactic structures in an unsupervised
manner. Yogatama et al. (2017) propose to use re-
inforcement learning, and Maillard et al. (2017)
introduce the Tree-LSTM to jointly learn sentence
embeddings and syntax trees, later combined with
a Straight-Through Gumbel-Softmax estimator by
Choi et al. (2018). In addition to sentence classifi-
cation tasks, recent research has focused on unsu-
pervised structure learning for language modeling
(Shen et al., 2018, 2019; Drozdov et al., 2019; Kim
et al., 2019b). In our work, we explore the possi-
bility for combining the merits of both sentence
classification and language modeling.

Unsupervised parsing is also related to dif-
ferentiation through discrete variables, where re-
searchers have proposed to use reinforcement
learning with sampling (Williams, 1992), neural
attention for marginalization (Deng et al., 2018),
and proximal gradient methods (Jang et al., 2017;
Peng et al., 2018). Our work follows the frame-
work of Mou et al. (2017), who couple neural and
symbolic systems for table querying by pretrain-
ing an reinforcement learning executor with neu-
ral attention. We extend this idea to syntactic pars-
ing and show the relationship between parsing and
downstream tasks. Such a framework couples di-
verse models at the intermediate output level (la-
tent trees in our case); its flexibility allows us to
make use of heterogeneous models, such as the
PRPN and the Tree-LSTM.

The knowledge transfer between the PRPN and
the Tree-LSTM applies a simple imitation learning

procedure, where an agent learns from a teacher (a
human or a well-trained model) based on demon-
strations (i.e., predictions of the teacher). Typi-
cal approaches to imitation learning include be-
havior cloning (step-by-step supervised learning)
and inverse reinforcement learning (Hussein et al.,
2017). If the environment/simulator is available,
the agent can refine its policy after learning from
demonstrations (Gao et al., 2018). Our work also
adopts a two-step strategy: learning from demon-
strations and refining policy. Policy refinement is
needed in our approach because the teacher is im-
perfect, and experiments show the benefit of pol-
icy refinement in our case.

3 Our Approach

Parsing-reading-predict network (PRPN).
The first ingredient of our approach is the PRPN,
which is trained using a language modeling
objective, i.e., it predicts the next word in the text,
based on previous words.

The PRPN introduces the concept of syntactic
distance dt, defined as the height of the common
ancestor of wt−1 and wt in the tree (t is the posi-
tion index in a sentence w1, ..., wN ). Since gold
standard dt is not available, the PRPN learns the
estimated d̂t end-to-end in an unsupervised man-
ner. The PRPN computes the differences between
d̂t at the current step and all previous steps d̂j for
2 ≤ j < t. The differences are normalized to [0, 1]
and used to compute attention scores right to left.
These scores are applied to reweight another set
of inner-sentence attention scores, which are then
used in a recurrent neural network to predict the
next word. The PRPN is explained in more detail
in Appendix A.

Based on the real-valued syntactic distances in
the PRPN, an external procedure is used to infer
tree structures. The main text of Shen et al. (2018)
suggests using the following intuitive scheme: find
the largest distance d̂i and split the sentence into
two constituents (· · · , wi−1) and (wi, · · · ). This
process is then repeated recursively on the two
new constituents.

The trees inferred by this scheme, however,
yield poor parsing F -scores, and the results re-
ported by Shen et al. (2018) are actually obtained
by a different scheme (evidenced in their supple-
mentary material and code repository): find the
largest syntactic distance d̂i and obtain two con-
stituents (· · · , wi−1) and (wi, · · · ). If the latter



3487

constituent contains two or more words, then it is
further split into (wi) and (wi+1, · · · ), regardless
of the syntactic distance d̂i+1. This scheme intro-
duces a bias for right-branching trees, which pre-
sumably is the reason why it yields good parsing
F -scores for English.

The reliance on this trick illustrates the point we
make in the Introduction: syntactic distance has
the advantage of being a continuous value, which
can be computed as an attention score in a differ-
entiable model. However, this comes at a price:
the PRPN does not model trees or tree-building
operations directly. These operations need to be
stipulated externally in an ad hoc inference proce-
dure. This procedure is not part of the model and
cannot be trained, but yet is crucial for good per-
formance.

Discrete syntactic parser. To address this prob-
lem, we combine the PRPN with a parser which
explicitly models tree-building operations. Specif-
ically, we use the pyramid-shaped, tree-based long
short-term memory (Tree-LSTM, Figure 1a, Choi
et al., 2018), where reinforcement learning (RL)
in this model can be relaxed by Gumbel-Softmax.

Concretely, let w1,w2, · · · ,wN be the embed-
dings of the words in a sentence. The model
tries every possible combination of two consecu-
tive words by the Tree-LSTM, but then uses soft-
max (inN−1 ways) to predict which composition
is appropriate at this step.

Let h(1)1 , · · · ,h
(1)
N−1 be the candidate Tree-

LSTM composition at the bottom layer. With q
being a trainable query vector, the model com-
putes a distribution p:

p
(1)
i = softmax{q

>h
(1)
i } (1)

Assuming the model selects an appropriate com-
position at the current step, we copy all other
words intactly, shown as orange arrows in Fig-
ure 1a. This process is applied recursively, form-
ing the structure in the figure.

The Tree-LSTM model is learned by straight-
through Gumbel-Softmax (detailed in Ap-
pendix B), which resembles RL as it samples
actions from its predicted probabilities, exploring
different regions of the latent space other than
a maximum a posteriori tree. Training involves
doubly stochastic gradient descent (Lei et al.,
2016): the first stochasticity comes from sam-
pling input from the data distribution, and the
second one from sampling actions for each input.

(a) Pyramid 
  Model 

(b) Knowledge
  Transfer 

=  [.6               .4] p(2)

p(1) =  [.3                .5              .2] 

q

q

Jparse

Jtask

t̂(2) =  [1 0] 

=  [0 1 0] t̂(1)
Jparse

Imperfect step-by-step 
target parsing labels 
obtained by soft parser

w1 w2 w3 w4

Figure 1: Overview of our approach. (a) The Tree-
LSTM model of Choi et al. (2018). (b) The model
is first trained with step-by-step supervision, and then
Gumbel-Softmax is applied to refine the policy.

Therefore, ST-Gumbel is difficult to train (similar
to RL), and may be stuck in poor local optima,
resulting in low self-agreement for multiple
random initializations (Williams et al., 2018a).

Imitation learning. Our aim is to combine the
PRPN and its continuous notion of syntactic dis-
tance with a parser that has discrete tree-building
operations. The mapping from the sequence
of Tree-LSTM composition operations to a tree
structure is not injective. Given a parse tree,
we may have multiple different composition se-
quences, e.g., left-to-right or right-to-left. This
ambiguity could confuse the Tree-LSTM during
training. We solve this problem by using the
PRPN’s notion of syntactic distance.

Given a parse tree predicted by the PRPN, if
more than one composition is applicable, we al-
ways group the candidates with the lowest syn-
tactic distance. In this way, we can unambigu-
ously determine the composition order from the
trees inferred by the PRPN. Then, we train the
Tree-LSTM model in a step-by-step (SbS) super-
vised fashion. Let t̂(j) be a one-hot vector for
the jth step of Tree-LSTM composition, where the
hat denotes imperfect target labels induced by the
PRPN’s prediction. The parsing loss is defined as:

Jparse = −
∑

j

∑
i
t̂
(j)
i log p

(j)
i (2)

where p(j) is the probability predicted by the Tree-
LSTM model. The subscript i indexes the ith po-
sition among in 1, · · · , Nj − 1, where Nj is the
number of nodes in the jth composition step.

The overall training objective J is a weighted
combination of the loss of the downstream task



3488

and the parsing loss, i.e., J = Jtask + λJparse. Af-
ter step-by-step training, we perform policy refine-
ment by optimizing Jtask with ST-Gumbel, so that
the Tree-LSTM can improve its policy based on a
semantically oriented task.

It should be emphasized that how the Tree-
LSTM model builds the tree structure differs be-
tween step-by-step training and ST-Gumbel train-
ing. For SbS training, we assume an imperfect
parsing tree is in place; hence the Tree-LSTM
model exploits existing partial structures to predict
the next composition position. For ST-Gumbel,
the tree structure is sampled from its predicted
probability, enabling our model to explore the
space of trees beyond the given imperfect tree.

4 Experiments

We train our model on the AllNLI dataset and
evaluate on the MultiNLI development set, follow-
ing experimental settings in Htut et al. (2018) (for
detailed settings, please see Appendix C).

Table 1 shows the parsing F -scores against
the Stanford Parser. The ST-Gumbel Tree-LSTM
model and the PRPN were run five times with dif-
ferent initializations, each known as a trajectory.
For imitation learning, given a PRPN trajectory,
we perform SbS training once and then policy re-
finement for five runs. Left-/right-branching and
balanced trees are also included as baselines.

Parsing results with punctuation. It is a com-
mon setting to keep all punctuation for evaluation
on the AllNLI dataset (Htut et al., 2018). In such
a setting, we find that the Tree-LSTM, trained by
ST-Gumbel from random initialization, does not
outperform balanced trees, whereas the PRPN out-
performs it by around 30 points. Our PRPN repli-
cation results are consistent with Htut et al. (2018).
Our first stage in imitation learning (SbS train-
ing) is able to successfully transfer the PRPN’s
knowledge to the Tree-LSTM, achieving an F -
score of 52.0, which is clearly higher than the
21.9 achieved by the Tree-LSTM trained with ST-
Gumbel alone, and even slightly higher than the
PRPN itself. The second stage, policy refinement,
achieves a further improvement in unsupervised
parsing, outperforming the PRPN by 2.1 points.

We also evaluate the self-agreement by comput-
ing the mean F -score across 25 runs for policy re-
finement and five runs for other models. We find
that our imitation learning achieves improved self-

agreement in addition to improved parsing perfor-
mance.

Parsing results without punctuation. We are
interested in investigating whether punctuation
make a difference on unsupervised parsing. In the
setting without punctuation, our imitation learning
approach with policy refinement outperforms the
PRPN by a larger margin (7.3 F -score points) than
in the setting with punctuation. But surprisingly,
strictly right-branching trees are a very strong
baseline in this setting, achieving the best pars-
ing performance overall. The PRPN cannot out-
perform the right-branching baseline, even though
it uses a right-branching bias in its tree inference
procedure.

By way of explanation, we assume that the syn-
tactic trees we compare against (given by the Stan-
ford parser) become more right-branching if punc-
tuation is removed. A simple example is the period
at the end of the sentence: this is always attached
to a high-level constituent in the correct tree (of-
ten to Root), while right-branching attaches it to
the most deeply embedded constituent. So this pe-
riod is always incorrectly predicted by the right-
branching baseline, if punctuation is left in.

To further elucidate this issue, we also com-
pute the agreement of various models with a right-
branching baseline. In the setting without punctu-
ation, the PRPN sets an initial policy that agrees
fairly well with right-branching, and this right-
branching bias is reinforced by imitation learn-
ing and policy refinement. However, in the set-
ting with punctuation, the agreement with right-
branching changes in the opposite way. We con-
jecture that right-branching is a reason why our
imitation learning achieves a larger improvement
without punctuation. Right-branching provides
a relatively flat local optimum so that imitation
learning can do further exploring with a low risk
of moving out of it.

Performance across constituent types. We
break down the performance of latent tree in-
duction across constituent types in the setting of
keeping punctuation. We see that, among the six
most common ones, our imitation approach out-
performs the PRPN on four types. However, we
also notice that for the most frequent type (NP),
our approach is worse than the PRPN. This shows
that the strengths of the two approaches comple-
ment each other, and in future work ensemble



3489

w/o Punctuation w/ Punctuation
Model Mean F Self-agreement RB-agreement Mean F Self-agreement RB-agreement
Left-Branching 20.7 - - 18.9 - -
Right-Branching 58.5 - - 18.5 - -
Balanced-Tree 39.5 - - 22.0 - -
ST-Gumbel 36.4 57.0 33.8 21.9 56.8 38.1
PRPN 46.0 48.9 51.2 51.6 65.0 27.4
Imitation (SbS only) 45.9 49.5 62.2 52.0 70.8 20.6
Imitation (SbS + refine) 53.3† 58.2 64.9 53.7† 67.4 21.1

Table 1: Parsing performance with and without punctuation. Mean F indicates mean parsing F -score against the
Stanford Parser (early stopping by F -score). Self-/RB-agreement indicates self-agreement and agreement with the
right-branching baseline across multiple runs. † indicates a statistical difference from the corresponding PRPN
baseline with p < 0.01, paired one-tailed bootstrap test.2

Type # Occur ST-Gumbel PRPN
Imitation

(SbS + refine)
NP 69k 22.6 53.2 49.5
VP 58k 4.9 49.4 57.0
S 42k 44.3 63.9 66.0
PP 29k 13.9 55.4 52.4
SBAR 12k 6.9 38.9 41.4
ADJP 4k 10.6 44.2 46.5

Table 2: Parsing accuracy for six phrase types which
occur more than 2k times in the MultiNLI development
set with keeping punctuation.

methods could be employed to combine them.

Discussion. Our results show the usefulness
of a downstream task for unsupervised parsing.
Specifically, policy refinement with a semantically
oriented objective improves parsing performance
by two F -score points, outperforming the previous
state-of-the-art PRPN model. This provides evi-
dence against previous studies which have claimed
that an external, non-syntactic task such as NLI
does not improve parsing performance (Williams
et al., 2018a; Htut et al., 2018). At the same
time, our results are compatible with findings of
Shi et al. (2018) that a range of different tree
structures yield similar classification accuracy in
NLI: we find that the mean NLI accuracy of the
ST-Gumbel-only model and our imitation learning
model with policy refinement is 69.9% and 69.2%,
respectively, on the MultiNLI development set.
NLI performance seems to be largely unaffected
by the syntactic properties of the induced trees.

An interesting question is why ST-Gumbel im-
proves unsupervised parsing when trained with an
NLI objective. It has been argued that NLI as
currently formulated is not a difficult task (Poliak
et al., 2018); this is presumably why models can

2F -score is not normally distributed. It is therefore ap-
propriate to use the non-parametric bootstrap test.

perform well across a range of different tree struc-
tures, only some of which are syntactically plau-
sible. However, this does not imply that the Tree-
LSTM will learn nothing when trained with NLI.
We can think of its error surface being very rugged
with many local optima; the syntactically correct
tree corresponds to one of them. If the model is
initialized in a meaningful catchment basin, NLI
training is more likely to recover that tree. The
intuition also explains why the Tree-LSTM alone
achieves low parsing performance and low self-
agreement. On a very rugged high-dimensional
error surface, the chance of getting into a partic-
ular local optimum (corresponding to a syntacti-
cally correct tree) is low, especially in RL and ST-
Gumbel, which are doubly stochastic.

We show examples of generated trees in Ap-
pendix D.

5 Conclusion

We proposed a novel imitation learning approach
to unsupervised parsing. We start from the differ-
entiable PRPN model and transfer its knowledge
to a Tree-LSTM by step-by-step imitation learn-
ing. The Tree-LSTM’s policy is then refined to-
wards a semantic objective. We achieve a new
state-of-the-art result of unsupervised parsing on
the NLI dataset. In future work, we would like to
combine more potential parsers—including chart-
style parsing and shift-reduce parsing—and trans-
fer knowledge from one to another in a co-training
setting.

Acknowledgments

We would like to thank Yikang Shen and Zhouhan
Lin at MILA for fruitful discussions. FK was sup-
ported by the Leverhulme Trust through Interna-
tional Academic Fellowship IAF-2017-019.



3490

References
Roee Aharoni and Yoav Goldberg. 2017. Towards

string-to-tree neural machine translation. In ACL,
pages 132–140.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP, pages 632–642.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.
Learning to compose task-specific tree structures. In
AAAI, pages 5094–5101.

Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and
Alexander Rush. 2018. Latent alignment and varia-
tional attention. In NIPS, pages 9712–9724.

Andrew Drozdov, Pat Verga, Mohit Yadav, Mohit
Iyyer, and Andrew McCallum. 2019. Unsupervised
latent tree induction with deep inside-outside recur-
sive autoencoders. In NAACL-HLT.

Yang Gao, Ji Lin, Fisher Yu, Sergey Levine, Trevor
Darrell, et al. 2018. Reinforcement learning
from imperfect demonstrations. arXiv preprint
arXiv:1802.05313.

Phu Mon Htut, Kyunghyun Cho, and Samuel Bow-
man. 2018. Grammar induction with neural lan-
guage models: An unusual replication. In EMNLP,
pages 4998–5003.

Ahmed Hussein, Mohamed Medhat Gaber, Eyad
Elyan, and Chrisina Jayne. 2017. Imitation learn-
ing: A survey of learning methods. ACM Comput.
Surveys, 50(2):21:1–21:35.

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cat-
egorical reparameterization with Gumbel-softmax.
In ICLR.

Taeuk Kim, Jihun Choi, Daniel Edmiston, Sanghwan
Bae, and Sang-goo Lee. 2019a. Dynamic composi-
tionality in recursive neural networks with structure-
aware tag representations. In AAAI.

Yoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kun-
coro, Chris Dyer, and Gábor Melis. 2019b. Un-
supervised recurrent neural network grammars. In
NAACL-HLT.

Dan Klein and Christopher Manning. 2014. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In ACL, pages 479–486.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
Rationalizing neural predictions. In EMNLP, pages
107–117.

Jean Maillard, Stephen Clark, and Dani Yogatama.
2017. Jointly learning sentence embeddings and
syntax with unsupervised Tree-LSTMs. arXiv
preprint arXiv:1705.09189.

Lili Mou, Zhengdong Lu, Hang Li, and Zhi Jin. 2017.
Coupling distributed and symbolic execution for nat-
ural language queries. In ICML, pages 2518–2526.

Shashi Narayan and Claire Gardent. 2014. Hybrid sim-
plification using deep semantics and machine trans-
lation. In ACL, pages 435–445.

Barbara BH Partee, Alice G ter Meulen, and Robert
Wall. 2012. Mathematical Methods in Linguistics,
volume 30. Springer Science & Business Media.

Hao Peng, Sam Thomson, and Noah A. Smith. 2018.
Backpropagating through structured argmax using a
SPIGOT. In ACL, pages 1863–1873.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infer-
ence. In Proc. 7th Joint Conf. Lexical and Compu-
tational Semantics, pages 180–191.

Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and
Aaron Courville. 2018. Neural language modeling
by jointly learning syntax and lexicon. In ICLR.

Yikang Shen, Shawn Tan, Alessandro Sordoni, and
Aaron Courville. 2019. Ordered neurons: Integrat-
ing tree structures into recurrent neural networks. In
ICLR.

Haoyue Shi, Hao Zhou, Jiaze Chen, and Lei Li.
2018. On tree-based neural sentence modeling. In
EMNLP, pages 4631–4641.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP, pages 1631–1642.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In ACL-IJCNLP, pages 1556–1566.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multi-
lingual relation extraction using compositional uni-
versal schema. In NAACL-HLT, pages 886–896.

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018a. Do latent tree learning models
identify meaningful structure in sentences? TACL,
6:253–267.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2018b. A broad-coverage challenge corpus
for sentence understanding through inference. In
NAACL-HLT, pages 1112–1122.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8(3-4):229–256.

https://doi.org/10.18653/v1/P17-2021
https://doi.org/10.18653/v1/P17-2021
https://doi.org/10.18653/v1/D15-1075
https://doi.org/10.18653/v1/D15-1075
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16682/16055
https://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf
https://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf
https://arxiv.org/pdf/1904.02142.pdf
https://arxiv.org/pdf/1904.02142.pdf
https://arxiv.org/pdf/1904.02142.pdf
https://arxiv.org/pdf/1802.05313.pdf
https://arxiv.org/pdf/1802.05313.pdf
http://aclweb.org/anthology/D18-1544
http://aclweb.org/anthology/D18-1544
https://doi.org/10.1145/3054912
https://doi.org/10.1145/3054912
https://openreview.net/forum?id=rkE3y85ee
https://openreview.net/forum?id=rkE3y85ee
https://arxiv.org/pdf/1809.02286.pdf
https://arxiv.org/pdf/1809.02286.pdf
https://arxiv.org/pdf/1809.02286.pdf
https://arxiv.org/pdf/1904.03746.pdf
https://arxiv.org/pdf/1904.03746.pdf
http://aclweb.org/anthology/P04-1061
http://aclweb.org/anthology/P04-1061
http://aclweb.org/anthology/P04-1061
https://doi.org/10.18653/v1/D16-1011
https://arxiv.org/abs/1705.09189
https://arxiv.org/abs/1705.09189
http://proceedings.mlr.press/v70/mou17a.html
http://proceedings.mlr.press/v70/mou17a.html
https://doi.org/10.3115/v1/P14-1041
https://doi.org/10.3115/v1/P14-1041
https://doi.org/10.3115/v1/P14-1041
https://www.aclweb.org/anthology/P18-1173
https://www.aclweb.org/anthology/P18-1173
http://aclweb.org/anthology/S18-2023
http://aclweb.org/anthology/S18-2023
https://openreview.net/pdf?id=rkgOLb-0W
https://openreview.net/pdf?id=rkgOLb-0W
https://openreview.net/pdf?id=B1l6qiR5F7
https://openreview.net/pdf?id=B1l6qiR5F7
http://aclweb.org/anthology/D18-1492"
http://www.aclweb.org/anthology/D13-1170
http://www.aclweb.org/anthology/D13-1170
http://www.aclweb.org/anthology/D13-1170
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.18653/v1/N16-1103
https://doi.org/10.18653/v1/N16-1103
https://doi.org/10.18653/v1/N16-1103
http://aclweb.org/anthology/Q18-1019
http://aclweb.org/anthology/Q18-1019
https://doi.org/10.18653/v1/N18-1101
https://doi.org/10.18653/v1/N18-1101
https://link.springer.com/content/pdf/10.1007/BF00992696.pdf
https://link.springer.com/content/pdf/10.1007/BF00992696.pdf
https://link.springer.com/content/pdf/10.1007/BF00992696.pdf


3491

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2017. Learning to
compose words into sentences with reinforcement
learning. In ICLR.

A Details of the PRPN

We now describe in more detail the parsing-
reading-predict network (PRPN), proposed by
Shen et al. (2018). The PRPN introduces a con-
cept called the syntactic distance, illustrated in
Figure 2. The syntactic distance dt is defined as
the height of the common ancestor of wt−1 and wt
in a tree.

The PRPN uses a two-layer multilayer percep-
tron (MLP) to estimate dt. The input is the em-
beddings of the current word and its left context
wt−L,wt−L+1, · · · ,wt. The output is given by:

d̂t = MLP(wt−L,wt−L+1, · · · ,wt) (3)
In fact, absolute distance values are not required, it
is sufficient to preserve their order. In other words,
if di < dj , then it is desired that d̂i < d̂j . How-

3

2

1

0

Height

w1 w2

w3 w4

w5

321 4 5

2 23 1
Syntactic 
Distance

Composition 
Position

(dummy)

d

Figure 2: A parse tree with syntactic distance values.

sti =
gtiPt�1

i=1 g
t
i

esti

…

w1 w2 wt

Predict next word

Gated-weighted 
attention

wt+1 = ?

s1 sts2

Figure 3: The prediction of the next word in the PRPN
language model.

ever, even the order of dt is not available at training
time, and d̂t is learned end-to-end in an unsuper-
vised manner.

The PRPN computes the difference between the
distance dt at the current step and all previous
steps dj for 2 ≤ j < t. The difference is nor-
malized to the range [0, 1]:

αtj =
hardtanh(τ(d̂t − d̂j)) + 1

2
(4)

where τ is the temperature.
Finally, a soft gate is computed right-to-left in a

multiplicatively cumulative fashion:

gti =
t−1∏

j=i+1

αtj (5)

for 1 ≤ i ≤ t−1. The gates gti are used to reweight
another inner-sentence attention s̃ti, which is com-
puted as:

s̃ti = softmax{h>i (W [ht−1;wt])} (6)
The reweighed inner-sentence attention si then be-
comes:

sti =
gti∑t−1
i=1 g

t
i

s̃ti (7)

and is used to compute the convex combination
of attention candidate vectors, which are incorpo-
rated in a recurrent neural network to predict the
next word, shown in Figure 3.

B Details of Gumbel-Softmax

Gumbel-Softmax can be thought of as a relaxed
version of reinforcement learning. It is used in
the training of the Tree-LSTM model (Choi et al.,
2018), as well as policy refinement in our imitation
learning. In particular, we use the straight-through
Gumbel-Softmax (ST-Gumbel, Jang et al., 2017).

In the forward propagation of ST-Gumbel train-
ing, the model samples an action—in the Tree-
LSTM model, the position of composition—from
the distribution p by the Gumbel trick. The sam-
pled action can be represented as a one-hot vector
a, whose elements take the form:

ai =

{
1, if i = argmaxj{log(pj) + gj}
0, otherwise

(8)

where gi is called the Gumbel noise, given by:

gi = − log(− log(ui)) (9)
ui ∼ Uniform(0, 1) (10)

It can be shown that a is an unbiased sample from
the original distribution p (Jang et al., 2017).

https://openreview.net/forum?id=Skvgqgqxe&noteId=ryNsmAZ4l
https://openreview.net/forum?id=Skvgqgqxe&noteId=ryNsmAZ4l
https://openreview.net/forum?id=Skvgqgqxe&noteId=ryNsmAZ4l


3492

This is a powerful evocativeand museum . This is a powerful evocativeand museum .

He seemed triflea .

Chapter 1 : His namereal was .Leonard Franklin Slye Chapter 1 : His namereal was .Leonard Franklin Slye

embarrassed He seemed triflea .embarrassed

Figure 4: Parse tree examples produced by the PRPN and our model (SbS + refine).

During backpropagation, ST-Gumbel substi-
tutes the selected one-hot action a given by
argmax in Equation (8) with a softmax operation.

p̃i =
exp{(log(pi) + gi)/γ}∑
j exp{(log(pj) + gj)/γ}

(11)

where γ is a temperature parameter that can also
be learned by backpropagation.

The Tree-LSTM model is trained using the loss
in a downstream task (for example, cross-entropy
loss for classification problems). Compared with
reinforcement learning, the ST-Gumbel trick al-
lows more information to be propagated back to
the bottom of the Tree-LSTM in addition to the
selected actions, although it does not follow exact
gradient computation. For prediction (testing), the
model selects the most probable composition ac-
cording to its predicted probabilities.

C Experimental Setup

We conduct experiments on the AllNLI dataset,
the concatenation of the Stanford Natural Lan-
guage Inference Corpus (Bowman et al., 2015)
and the Multi-Genre NLI Corpus (MultiNLI;
Williams et al. 2018b). As the MultiNLI test set
is not publicly available, we follow previous work
(Williams et al., 2018a; Htut et al., 2018) and use
the development set for testing. For early stop-
ping, we remove 10k random sentence pairs from
the AllNLI training set to form a validation set.
Thus, our AllNLI dataset contains 931k, 10k, and
10k sample pairs for training, validation, and test,
respectively.

We build the PRPN model and the Tree-LSTM
parser following the hyperparameters in previous
work (Shen et al., 2018; Choi et al., 2018).3 For

the SbS training stage, we set λ to be 0.03. For the
policy refinement stage, the initial temperature is
manually set to 0.5. The PRPN is trained by a lan-
guage modeling loss on the AllNLI training sen-
tences, whereas the Tree-LSTM model is trained
by a cross-entropy loss for AllNLI classification.

We adopt the standard metric and compute the
unlabeled F -score of the constituents predicted by
our parsing model against those given by the Stan-
ford PCFG Parser (version 3.5.2). Although the
Stanford parser itself may make parsing errors, it
achieves generally high performance and is a rea-
sonable approximation of correct parse trees.

D Parse Tree Examples

In Figure 4, we present a few examples of parse
trees generated by the PRPN and by our model
(SbS + refine).

As can be seen, our model is able to handle the
period correctly in these examples. Although this
could be specified by hand-written rules (Drozdov
et al., 2019), it is in fact learned by our approach in
an unsupervised manner, since punctuation marks
are treated as tokens just like other words, and our
training signal gives no clue regarding how punc-
tuation marks should be processed.

Moreover, our model is able to parse the verb
phrases more accurately than the PRPN, including
is a powerful and evocative museum and seemed
a trifle embarrassed. This is also evidenced by
quantitative results in Table 2.

3The code bases of the PRPN and the Gumbel Tree-
LSTM are available at https://github.com/
yikangshen/PRPN and https://github.com/
nyu-mll/spinn/tree/is-it-syntax-release

https://github.com/yikangshen/PRPN
https://github.com/yikangshen/PRPN
https://github.com/nyu-mll/spinn/tree/is-it-syntax-release
https://github.com/nyu-mll/spinn/tree/is-it-syntax-release

