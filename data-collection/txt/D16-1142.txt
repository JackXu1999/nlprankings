



















































All Fingers are not Equal: Intensity of References in Scientific Articles


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1348–1358,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

All Fingers are not Equal: Intensity of References in Scientific Articles

Tanmoy Chakraborty
Dept. of Computer Science & UMIACS

University of Maryland, College Park, USA
tanchak@umiacs.umd.edu

Ramasuri Narayanam
IBM Research, India

ramasurn@in.ibm.com

Abstract

Research accomplishment is usually measured
by considering all citations with equal impor-
tance, thus ignoring the wide variety of pur-
poses an article is being cited for. Here, we
posit that measuring the intensity of a refer-
ence is crucial not only to perceive better un-
derstanding of research endeavor, but also to
improve the quality of citation-based applica-
tions. To this end, we collect a rich annotated
dataset with references labeled by the inten-
sity, and propose a novel graph-based semi-
supervised model, GraLap to label the in-
tensity of references. Experiments with AAN
datasets show a significant improvement com-
pared to the baselines to achieve the true labels
of the references (46% better correlation). Fi-
nally, we provide four applications to demon-
strate how the knowledge of reference inten-
sity leads to design better real-world applica-
tions.

1 Introduction

With more than one hundred thousand new schol-
arly articles being published each year, there is a
rapid growth in the number of citations for the rel-
evant scientific articles. In this context, we high-
light the following interesting facts about the pro-
cess of citing scientific articles: (i) the most com-
monly cited paper by Gerard Salton, titled “A Vector
Space Model for Information Retrieval” (alleged to
have been published in 1975) does not actually ex-
ist in reality (Dubin, 2004), (ii) the scientific authors
read only 20% of the works they cite (Simkin and
Roychowdhury, 2003), (iii) one third of the refer-

ences in a paper are redundant and 40% are perfunc-
tory (Moravcsik and Murugesan, 1975), (iv) 62.7%
of the references could not be attributed a specific
function (definition, tool etc.) (Teufel et al., 2006).
Despite these facts, the existing bibliographic met-
rics consider that all citations are equally significant.

In this paper, we would emphasize the fact that
all the references of a paper are not equally influ-
ential. For instance, we believe that for our current
paper, (Wan and Liu, 2014) is more influential refer-
ence than (Garfield, 2006), although the former has
received lower citations (9) than the latter (1650) so
far1. Therefore the influence of a cited paper com-
pletely depends upon the context of the citing paper,
not the overall citation count of the cited paper. We
further took the opinion of the original authors of
few selective papers and realized that around 16%
of the references in a paper are highly influential,
and the rest are trivial (Section 4). This motivates us
to design a prediction model, GraLap to automati-
cally label the influence of a cited paper with respect
to a citing paper. Here, we label paper-reference
pairs rather than references alone, because a refer-
ence that is influential for one citing paper may not
be influential with equal extent for another citing pa-
per.

We experiment with ACL Anthology Network
(AAN) dataset and show that GraLap along with
the novel feature set, quite efficiently, predicts the
intensity of references of papers, which achieves
(Pearson) correlation of 0.90 with the human anno-
tations. Finally, we present four interesting appli-

1The statistics are taken from Google Scholar on June 2,
2016.

1348



cations to show the efficacy of considering unequal
intensity of references, compared to the uniform in-
tensity.

The contributions of the paper are four-fold: (i)
we acquire a rich annotated dataset where paper-
reference pairs are labeled based on the influence
scores (Section 4), which is perhaps the first gold-
standard for this kind of task; (ii) we propose a
graph-based label propagation model GraLap for
semi-supervised learning which has tremendous po-
tential for any task where the training set is less
in number and labels are non-uniformly distributed
(Section 3); (iii) we propose a diverse set of features
(Section 3.3); most of them turn out to be quite ef-
fective to fit into the prediction model and yield im-
proved results (Section 5); (iv) we present four ap-
plications to show how incorporating the reference
intensity enhances the performance of several state-
of-the-art systems (Section 6).

2 Defining Intensity of References

All the references of a paper usually do not carry
equal intensity/strength with respect to the citing
paper because some papers have influenced the re-
search more than others. To pin down this intuition,
here we discretize the reference intensity by numer-
ical values within the range of 1 to 5, (5: most in-
fluential, 1: least influential). The appropriate def-
initions of different labels of reference intensity are
presented in Figure 1, which are also the basis of
building the annotated dataset (see Section 4):

Note that “reference intensity” and “reference
similarity” are two different aspects. It might hap-
pen that two similar reference are used with differ-
ent intensity levels in a citing paper – while one is
just mentioned somewhere in the paper and other
is used as a baseline. Here, we address the former
problem as a semi-supervised learning problem with
clues taken from content of the citing and cited pa-
pers.

3 Reference Intensity Prediction Model

In this section, we formally define the problem and
introduce our prediction model.

• Label-1: The reference is related to the citing ar-
ticle with very limited extent and can be removed
without compromising the competence of the refer-
ences (e.g., (Garfield, 2006) for this paper).
• Label-2: The reference is little mentioned in the
citing article and can be replaced by others without
compromising the adequacy of the references (e.g.,
(Zhu et al., 2015) for this paper).
• Label-3: The reference occurs separately in a sen-
tence within the citing article and has no significant
impact on the current problem (e.g., references to
metrics, tools) (e.g., (Porter, 1997) for this paper).
• Label-4: The reference is important and highly
related to the citing article. It is usually mentioned
several times in the article with long reference con-
text (e.g., (Singh et al., 2015) for this paper).
• Label-5: The reference is extremely important
and occurs (is emphasized) multiple times within
the citing article. It generally points to the cited arti-
cle from where the citing article borrows main ideas
(and can be treated as a baseline) (e.g., (Wan and
Liu, 2014) for this paper).

Figure 1: Definitions of the intensity of references.

3.1 Problem Definition
We are given a set of papers P = {P1, P2, ..., PM}
and a sets of references R = {R1, R2, ..., RM},
where Ri corresponds to the set of references (or
cited papers) of Pi. There is a set of papers PL ∈
P whose references RL ∈ R are already labeled
by ` ∈ L = {1, ..., 5} (each reference is labeled
with exactly one value). Our objective is to de-
fine a predictive function f that labels the references
RU ∈ {R \ RL} of the papers PU ∈ {P \ PL}
whose reference intensities are unknown, i.e., f :
(P,R, PL, RL, PU , RL) −→ L.

Since the size of the annotated (labeled) data is
much smaller than unlabeled data (|PL| � |PU |),
we consider it as a semi-supervised learning prob-
lem.

Definition 1. (Semi-supervised Learning) Given
a set of entries X and a set of possible labels YL,
let us assume that (x1, y1), (x2, y2),..., (xl, yl) be
the set of labeled data where xi is a data point
and yi ∈ YL is its corresponding label. We as-
sume that at least one instance of each class label

1349



is present in the labeled dataset. Let (xl+1, yl+1),
(xl+2, yl+2),..., (xl+n, yl+u) be the unlabeled data
points where YU = {yl+1, yl+2, ...yl+u} are un-
known. Each entry x ∈ X is represented by a set
of features {f1, f2, ..., fD}. The problem is to deter-
mine the unknown labels using X and YL.

3.2 GraLap: A Prediction Model
We propose GraLap, a variant of label propagation
(LP) model proposed by (Zhu et al., 2003) where
a node in the graph propagates its associated label
to its neighbors based on the proximity. We intend
to assign same label to the vertices which are closely
connected. However unlike the traditional LP model
where the original values of the labels continue to
fade as the algorithm progresses, we systematically
handle this problem in GraLap. Additionally, we
follow a post-processing in order to handle “class-
imbalance problem”.
Graph Creation. The algorithm starts with the
creation of a fully connected weighted graph G =
(X,E) where nodes are data points and the weight
wij of each edge eij ∈ E is determined by the radial
basis function as follows:

wij = exp

(
−
∑D

d=1(x
d
i − xdj )2
σ2

)
(1)

The weight is controlled by a parameter σ. Later
in this section, we shall discuss how σ is selected.
Each node is allowed to propagate its label to its
neighbors through edges (the more the edge weight,
the easy to propagate).
Transition Matrix. We create a probabilistic transi-
tion matrix T|X|×|X|, where each entry Tij indicates
the probability of jumping from j to i based on the
following: Tij = P (j → i) = wij∑|X|

k=1 wkj
.

Label Matrix. Here, we allow a soft label (in-
terpreted as a distribution of labels) to be associ-
ated with each node. We then define a label matrix
Y|X|×|L|, where ith row indicates the label distribu-
tion for node xi. Initially, Y contains only the values
of the labeled data; others are zero.
Label Propagation Algorithm. This algorithm
works as follows:

After initializing Y and T , the algorithm starts by
disseminating the label from one node to its neigh-
bors (including self-loop) in one step (Step 3). Then
we normalize each entry of Y by the sum of its cor-

1: Initialize T and Y
2: while (Y does not converge) do
3: Y ← TY
4: Normalize rows of Y , yij =

yij∑
k yik

5: Reassign original labels to XL

responding row in order to maintain the interpreta-
tion of label probability (Step 4). Step 5 is crucial;
here we want the labeled sources XL to be persis-
tent. During the iterations, the initial labeled nodes
XL may fade away with other labels. Therefore we
forcefully restore their actual label by setting yil = 1
(if xi ∈ XL is originally labeled as l), and other
entries (∀j 6=lyij) by zero. We keep on “pushing”
the labels from the labeled data points which in turn
pushes the class boundary through high density data
points and settles in low density space. In this way,
our approach intelligently uses the unlabeled data in
the intermediate steps of the learning.
Assigning Final Labels. Once YU is computed, one
may take the most likely label from the label distri-
bution for each unlabeled data. However, this ap-
proach does not guarantee the label proportion ob-
served in the annotated data (which in this case is
not well-separated as shown in Section 4). There-
fore, we adopt a label-based normalization tech-
nique. Assume that the label proportions in the la-
beled data are c1, ..., c|L| (s.t.

∑|L|
i=1 ci = 1). In

case of YU , we try to balance the label proportion
observed in the ground-truth. The label mass is the
column sum of YU , denoted by YU.1 , ..., YU.|L| , each
of which is scaled in such a way that YU.1 : ... :
YU.|L| = c1 : ... : c|L|. The label of an unlabeled
data point is finalized as the label with maximum
value in the row of Y .
Convergence. Here we briefly show that our algo-
rithm is guaranteed to converge. Let us combine
Steps 3 and 4 as Y ← T̂ Y , where T̂ = Tij/

∑
k Tik.

Y is composed of YLl×|L| and YUu×|L| , where YU
never changes because of the reassignment. We can
split T̂ at the boundary of labeled and unlabeled data
as follows:

F̂ =

[
T̂ll T̂lu
T̂ul T̂uu

]

Therefore, YU ← T̂uuYU+ T̂ulYL, which can lead
to YU = limn→∞ T̂nuuY

0 + [
∑n

i=1 T̂
(i−1)
uu ]T̂ulYL,

where Y 0 is the shape of Y at iteration 0. We need

1350



to show T̂nuuijY
0 ← 0. By construction, T̂ij ≥ 0,

and since T̂ is row-normalized, and T̂uu is a part
of T̂ , it leads to the following condition: ∃γ <
1,
∑u

j=1 T̂uuij ≤ γ, ∀i = 1, ..., u. So,
∑

j

T̂nuuij =
∑

j

∑

k

T̂
(n−1)
uuik T̂uukj

=
∑

k

T̂
(n−1)
uuik

∑

j

T̂uuik

≤
∑

k

T̂
(n−1)
uuik γ

≤ γn

Therefore, the sum of each row in T̂nuuij converges
to zero, which indicates T̂nuuijY

0 ← 0.
Selection of σ. Assuming a spatial representation
of data points, we construct a minimum spanning
tree using Kruskal’s algorithm (Kruskal, 1956) with
distance between two nodes measured by Euclidean
distance. Initially, no nodes are connected. We
keep on adding edges in increasing order of distance.
We choose the distance (say, df ) of the first edge
which connects two components with different la-
beled points in them. We consider df as a heuristic
to the minimum distance between two classes, and
arbitrarily set σ = d0/3, following 3σ rule of nor-
mal distribution (Pukelsheim, 1994).

3.3 Features for Learning Model
We use a wide range of features that suitably rep-
resent a paper-reference pair (Pi, Rij), indicating Pi
refers to Pj through reference Rij . These features
can be grouped into six general classes.
3.3.1 Context-based Features (CF)

The “reference context” of Rij in Pi is defined by
three-sentence window (sentence where Rij occurs
and its immediate previous and next sentences). For
multiple occurrences, we calculate its average score.
We refer to “reference sentence” to indicate the sen-
tence where Rij appears.
(i) CF:Alone. It indicates whether Rij is mentioned
alone in the reference context or together with other
references.
(ii) CF:First. When Rij is grouped with others, this
feature indicates whether it is mentioned first (e.g.,
“[2]” is first in “[2,4,6]”).

Next four features are based on the occurrence of
words in the corresponding lists created manually
(see Table 1) to understand different aspects.

(iii) CF:Relevant. It indicates whether Rij is explic-
itly mentioned as relevant in the reference context
(Rel in Table 1).
(iv) CF:Recent. It tells whether the reference con-
text indicates that Rij is new (Rec in Table 1).
(v) CF:Extreme. It implies that Rij is extreme in
some way (Ext in Table 1).
(vi) CF:Comp. It indicates whether the reference
context makes some kind of comparison with Rij
(Comp in Table 1).

Note we do not consider any sentiment-based fea-
tures as suggested by (Zhu et al., 2015).
3.3.2 Similarity-based Features (SF)

It is natural that the high degree of semantic simi-
larity between the contents of Pi and Pj indicates the
influence of Pj in Pi. We assume that although the
full text of Pi is given, we do not have access to the
full text of Pj (may be due to the subscription charge
or the unavailability of the older papers). Therefore,
we consider only the title of Pj as a proxy of its
full text. Then we calculate the cosine-similarity2

between the title (T) of Pj and (i) SF:TTitle. the ti-
tle, (ii) SF:TAbs. the abstract, SF:TIntro. the in-
troduction, (iv) SF:TConcl. the conclusion, and (v)
SF:TRest. the rest of the sections (sections other
than abstract, introduction and conclusion) of Pi.

We further assume that the “reference context”
(RC) of Pj in Pi might provide an alternate way of
summarizing the usage of the reference. Therefore,
we take the same similarity based approach men-
tioned above, but replace the title of Pj with its RC
and obtain five more features: (vi) SF:RCTitle, (vii)
SF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and
(x) SF:RCRest. If a reference appears multiple times
in a citing paper, we consider the aggregation of all
RCs together.

3.3.3 Frequency-based Feature (FF)
The underlying assumption of these features is

that a reference which occurs more frequently in
a citing paper is more influential than a single oc-
currence (Singh et al., 2015). We count the fre-
quency of Rij in (i) FF:Whole. the entire content,
(ii) FF:Intro. the introduction, (iii) FF:Rel. the re-
lated work, (iv) FF:Rest. the rest of the sections (as

2We use the vector space based model (Turney and Pantel,
2010) after stemming the words using Porter stammer (Porter,
1997).

1351



Rel pivotal, comparable, innovative, relevant, relevantly, inspiring, related, relatedly, similar, similarly, applicable, appropriate,
pertinent, influential, influenced, original, originally, useful, suggested, interesting, inspired, likewise
recent, recently, latest, later, late, latest, up-to-date, continuing, continued, upcoming, expected, update, renewed, extended

Rec subsequent, subsequently, initial, initially, sudden, current, currently, future, unexpected, previous, previously, old,
ongoing, imminent, anticipated, unprecedented, proposed, startling, preliminary, ensuing, repeated, reported, new, earlier,
earliest, early, existing, further, revised, improved

Ext greatly, awfully, drastically, intensely, acutely, almighty, exceptionally, excessively, exceedingly, tremendously, importantly
significantly, notably, outstandingly

Comp easy, easier, easiest, vague, vaguer, vaguest, weak, weaker, weakest, strong, stronger, strongest, bogus, unclear

Table 1: Manually curated lists of words collected from analyzing the reference contexts. The lists are
further expanded using the Wordnet:Synonym with different lexical variations. Note that while searching
the occurrence of these words in reference contexts, we use different lexical variations of the words instead
of exact matching.

mentioned in Section 3.3.2) of Pi. We also intro-
duce (v) FF:Sec. to measure the fraction of different
sections of Pi where Rij occurs (assuming that ap-
pearance of Rij in different sections is more influ-
ential). These features are further normalized using
the number of sentences in Pi in order to avoid un-
necessary bias on the size of the paper.

3.3.4 Position-based Features (PF)
Position of a reference in a paper might be a pre-

dictive clue to measure the influence (Zhu et al.,
2015). Intuitively, the earlier the reference appears
in the paper, the more important it seems to us. For
the first two features, we divide the entire paper into
two parts equally based on the sentence count and
then see whether Rij appears (i) PF:Begin. in the
beginning or (ii) PF:End. in the end of Pi. Impor-
tantly, if Rij appears multiple times in Pi, we con-
sider the fraction of times it occurs in each part.

For the other two features, we take the entire pa-
per, consider sentences as atomic units, and measure
position of the sentences where Rij appears, includ-
ing (iii) PF:Mean. mean position of appearance, (iv)
PF:Std. standard deviation of different appearances.
These features are normalized by the total length
(number of sentences) of Pi. , thus ranging from 0
(indicating beginning of Pi) to 1 (indicating the end
of Pi).

3.3.5 Linguistic Features (LF)
The linguistic evidences around the context ofRij

sometimes provide clues to understand the intrinsic
influence of Pj on Pi. Here we consider word level
and structural features.
(i) LF:NGram. Different levels of n-grams (1-
grams, 2-grams and 3-grams) are extracted from the
reference context to see the effect of different word
combination (Athar and Teufel, 2012).

(ii) LF:POS. Part-of-speech (POS) tags of the
words in the reference sentence are used as features
(Jochim and Schütze, 2012).
(iii) LF:Tense. The main verb of the reference sen-
tence is used as a feature (Teufel et al., 2006).
(iv) LF:Modal. The presence of modal verbs (e.g.,
“can”, “may”) often indicates the strength of the
claims. Hence, we check the presence of the modal
verbs in the reference sentence.
(v) LF:MainV. We use the main-verb of the refer-
ence sentence as a direct feature in the model.
(vi) LF:hasBut. We check the presence of conjunc-
tion “but”, which is another clue to show less confi-
dence on the cited paper.
(vii) LF:DepRel. Following (Athar and Teufel,
2012) we use all the dependencies present in the ref-
erence context, as given by the dependency parser
(Marneffe et al., 2006).
(viii) LF:POSP. (Dong and Schfer, 2011) use seven
regular expression patterns of POS tags to capture
syntactic information; then seven boolean features
mark the presence of these patterns. We also utilize
the same regular expressions as shown below 3 with
the examples (the empty parenthesis in each exam-
ple indicates the presence of a reference token Rij
in the corresponding sentence; while few examples
are complete sentences, few are not):

• “.*\\(\\) VV[DPZN].*”: Chen () showed that cohesion is held
in the vast majority of cases for English-French.

• “.*(VHP|VHZ) VV.*”: while Cherry and Lin () have shown it to
be a strong feature for word alignment...

• “.*VH(D|G|N|P|Z) (RB )*VBN.*”: Inducing features for tag-
gers by clustering has been tried by several researchers ().

• “.*MD (RB )*VB(RB )* VVN.*”: For example, the likelihood of
those generative procedures can be accumulated to get the like-
lihood of the phrase pair ().

3The meaning of each POS tag can be found in
http://nlp.stanford.edu/software/tagger.
shtml(Toutanova and Manning, 2000).

1352



• “[ IW.]*VB(D|P|Z) (RB )*VV[ND].*”: Our experimental set-up
is modeled after the human evaluation presented in ().

• “(RB )*PP (RB )*V.*”: We use CRF () to perform this tagging.
• “.*VVG (NP )*(CC )*(NP ).*”: Following (), we provide the an-

notators with only short sentences: those with source sentences
between 10 and 25 tokens long.

These are all considered as Boolean features. For
each feature, we take all the possible evidences from
all paper-reference pairs and prepare a vector. Then
for each pair, we check the presence (absence) of
tokens for the corresponding feature and mark the
vector accordingly (which in turn produces a set of
Boolean features).

3.3.6 Miscellaneous Features (MS)
This group provides other factors to explain why

is a paper being cited. (i) MS:GCount. To answer
whether a highly-cited paper has more academic in-
fluence on the citing paper than the one which is less
cited, we measure the number of other papers (ex-
cept Pi) citing Pj .
(ii) MS:SelfC. To see the effect of self-citation, we
check whether at least one author is common in both
Pi and Pj .
(iii) MG:Time. The fact that older papers are rarely
cited, may not stipulate that these are less influential.
Therefore, we measure the difference of the publica-
tion years of Pi and Pj .
(iv) MG:CoCite. It measures the co-citation counts
of Pi and Pj defined by

|Ri∩Rj |
|Ri∪Rj | , which in turn an-

swers the significance of reference-based similarity
driving the academic influence (Small, 1973).

Following (Witten and Frank, 2005), we further
make one step normalization and divide each feature
by its maximum value in all the entires.

4 Dataset and Annotation

We use the AAN dataset (Radev et al., 2009) which
is an assemblage of papers included in ACL related
venues. The texts are preprocessed where sentences,
paragraphs and sections are properly separated us-
ing different markers. The filtered dataset contains
12,843 papers (on average 6.21 references per paper)
and 11,092 unique authors.

Next we use Parscit (Councill et al., 2008) to
identify the reference contexts from the dataset and
then extract the section headings from all the pa-
pers. Then each section heading is mapped into one

of the following broad categories using the method
proposed by (Liakata et al., 2012): Abstract, Intro-
duction, Related Work, Conclusion and Rest.
Dataset Labeling. The hardest challenge in this
task is that there is no publicly available dataset
where references are annotated with the intensity
value. Therefore, we constructed our own annotated
dataset in two different ways. (i) Expert Annota-
tion: we requested members of our research group4

to participate in this survey. To facilitate the labeling
process, we designed a portal where all the papers
present in our dataset are enlisted in a drop-down
menu. Upon selecting a paper, its corresponding
references were shown with five possible intensity
values. The citing and cited papers are also linked
to the original texts so that the annotators can read
the original papers. A total of 20 researchers partic-
ipated and they were asked to label as many paper-
reference pairs as they could based on the definitions
of the intensity provided in Section 2. The annota-
tion process went on for one month. Out of total
1640 pairs annotated, 1270 pairs were taken such
that each pair was annotated by at least two anno-
tators, and the final intensity value of the pair was
considered to be the average of the scores. The Pear-
son correlation and Kendell’s τ among the annota-
tors are 0.787 and 0.712 respectively. (ii) Author
Annotation: we believe that the authors of a paper
are the best experts to judge the intensity of refer-
ences present in the paper. With this intension, we
launched a survey where we requested the authors
whose papers are present in our dataset with signif-
icant numbers. We designed a web portal in similar
fashion mentioned earlier; but each author was only
shown her own papers in the drop-down menu. Out
of 35 requests, 22 authors responded and total 196
pairs are annotated. This time we made sure that
each paper-reference pair was annotated by only one
author. The percentages of labels in the overall an-
notated dataset are as follows: 1: 9%, 2: 74%, 3:
9%, 4: 3%, 5: 4%.

5 Experimental Results

In this section, we start with analyzing the impor-
tance of the feature sets in predicting the reference

4All were researchers with the age between 25-45 working
on document summarization, sentiment analysis, and text min-
ing in NLP.

1353



FF SF CF PF LF MF
0

0.2

0.4

0.6

Feature

P
ea

rs
o
n

 c
o

rr
ea

lt
io

n

 

 

N
o

n
−

in
cr

ea
si

n
g

 o
rd

er
 o

f 
co

rr
el

at
io

n PF

Rel
Rest
Whole
Intro
Sec

LF

Rest
Whole
Rel
Intro
Sec

FFSF

Alone
Relevant
Extreme
Comp
First
Recent

CF

TIntro
TAbs
TRest
TTitle
TConcl
RCRest
RCIntro
RCAbs
RCTitle
RCConcl

CoCite
Time
SelfC
GCount

(a)

(b)

MF

DepRel
Modal
POS
NGram
POSP
MainV
Tense

Figure 2: Pearson correlation coefficient between
the features and the gold-standard annotations. (a)
Group-wise average correlation, and (b) ranking of
features in each group based on the correlation.

intensity, followed by the detailed results.
Feature Analysis. In order to determine which fea-
tures highly determine the gold-standard labeling,
we measure the Pearson correlation between vari-
ous features and the ground-truth labels. Figure 2(a)
shows the average correlation for each feature group,
and in each group the rank of features based on
the correlation is shown in Figure 2(b). Frequency-
based features (FF) turn out to be the best, among
which FF:Rest is mostly correlated. This set of
features is convenient and can be easily computed.
Both CF and LF seem to be equally important. How-
ever, PF tends to be less important in this task.

Model RMSE ρ R2

Uniform 2.09 -0.05 3.21
SVR+W 1.95 0.54 1.34
SVR+O 1.92 0.56 1.29

C4.5SSL 1.99 0.46 2.46
GLM 1.98 0.52 1.35

(a) Baselines

No. Model RMSE ρ R2

(1) GraLap+ FF 1.10 0.79 1.05
(2) (1) + LF 0.98 0.84 0.95
(3) (2) + CF 0.90 0.87 0.87
(4) (3) + MF 0.95 0.89 0.84
(5) (4) + SF 0.92 0.90 0.82
(6) (5) + PF 0.91 0.90 0.80

(b) Our model

Table 2: Performance of the competing models. The
features are added greedily into the GraLap model.

Results of Predictive Models. For the purpose of
evaluation, we report the average results after 10-
fold cross-validation. Here we consider five base-
lines to compare with GraLap: (i) Uniform: as-
sign 3 to all the references assuming equal inten-
sity, (ii) SVR+W: recently proposed Support Vector
Regression (SVR) with the feature set mentioned
in (Wan and Liu, 2014), (iii) SVR+O: SVR model
with our feature set, (iv) C4.5SSL: C4.5 semi-
supervised algorithm with our feature set (Quinlan,
1993), and (v) GLM: the traditional graph-based LP
model with our feature set (Zhu et al., 2003). Three
metrics are used to compare the results of the com-
peting models with the annotated labels: Root Mean
Square Error (RMSE), Pearson’s correlation coeffi-

cient (ρ), and coefficient of determination (R2)5.
Table 2 shows the performance of the competing

models. We incrementally include each feature set
into GraLap greedily on the basis of ranking shown
in Figure 2(a). We observe that GraLap with only
FF outperforms SVR+O with 41% improvement of
ρ. As expected, the inclusion of PF into the model
improves the model marginally. However, the over-
all performance of GraLap is significantly higher
than any of the baselines (p < 0.01).

6 Applications of Reference Intensity

In this section, we provide four different applica-
tions to show the use of measuring the intensity of
references. To this end, we consider all the labeled
entries for training and run GraLap to predict the
intensity of rest of the paper-reference pairs.

6.1 Discovering Influential Articles
Influential papers in a particular area are often dis-
covered by considering equal weights to all the ci-
tations of a paper. We anticipate that considering
the reference intensity would perhaps return more
meaningful results. To show this, Here we use the
following measures individually to compute the in-
fluence of a paper: (i) RawCite: total number of
citations per paper, (ii) RawPR: we construct a ci-
tation network (nodes: papers, links: citations), and
measure PageRank (Page et al., 1998) of each node
n: PR(n) = 1−qN + q

∑
m∈M(n)

PR(m)
|L(m)| ; where,

q, the damping factor, is set to 0.85, N is the to-
tal number of nodes, M(n) is the set of nodes that
have edges to n, and L(m) is the set of nodes that
m has an edge to, (iii) InfCite: the weighted
version of RawCite, measured by the sum of in-
tensities of all citations of a paper, (iv) InfPR:
the weighted version of RawPR: PR(n) = 1−qN +
q
∑

m∈M(n)
Inf(m→n)PR(m)∑

a∈L(m)Inf(m→a)
, where Inf indicates

the influence of a reference. We rank all the arti-
cles based on these four measures separately. Ta-
ble 3(a) shows the Spearman’s rank correlation be-
tween pair-wise measures. As expected, (i) and
(ii) have high correlation (same for (iii) and (iv)),
whereas across two types of measures the correla-
tion is less. Further, in order to know which mea-

5The less (resp. more) the value of RMSE and R2 (resp.
ρ), the better the performance of the models.

1354



sure is more relevant, we conduct a subjective study
where we select top ten papers from each measure
and invite the experts (not authors) who annotated
the dataset, to make a binary decision whether a rec-
ommended paper is relevant. 6. The average pair-
wise inter-annotator’s agreement (based on Cohen’s
kappa (Cohen, 1960)) is 0.71. Table 3(b) presents
that out of 10 recommendations of InfPR, 7 (5) pa-
pers are marked as influential by majority (all) of the
annotators, which is followed by InfCite. These
results indeed show the utility of measuring refer-
ence intensity for discovering influential papers. Top
three papers based on InfPR from the entire dataset
are shown in Table 4.

RowCite RowPR InfCite InfPR
RowCite 1 0.82 0.61 0.54
RowPR 0.82 1 0.52 0.63
InfCite 0.61 0.52 1 0.84
InfPR 0.54 0.63 0.84 1

(a)

Metric All Majority
RowCite 2 5
RowPR 2 4
InfCite 4 5
InfPR 5 7

(b)

Table 3: (a) Spearman’s rank correlation among in-
fluence measures and (b) expert evaluation of the
ranked results (for top 10 recommendations).

6.2 Identifying Influential Authors

H-index, a measure of impact/influence of an author,
considers each citation with equal weight (Hirsch,
2005). Here we incorporate the notion of reference
intensity into it and define hif-index.

Definition 2. An author A with a set of papers P (A)
has an hif-index equals to h, if h is the largest value
such that |{p ∈ P (A)|Inf(p) ≥ h}| ≥ h; where Inf(p)
is the sum of intensities of all citations of p.

We consider 37 ACL fellows as the list of gold-
standard influential authors. For comparative eval-
uation, we consider the total number of papers
(TotP), total number of citations (TotC) and av-
erage citations per paper (AvgC) as three competing
measures along with h-index and hif-index.
We arrange all the authors in our dataset in de-
creasing order of each measure. Figure 3(a) shows
the Spearman’s rank correlation among the com-
mon elements across pair-wise rankings. Figure 3(b)
shows the Precision@k for five competing mea-
sures at identifying ACL fellows. We observe that
hif-index performs significantly well with an
overall precision of 0.54, followed by AvgC (0.37),

6We choose papers from the area of “sentiment analysis” on
which experts agree on evaluating the papers.

h-index (0.35), TotC (0.32) and TotP (0.34). This
result is an encouraging evidence that the reference-
intensity could improve the identification of the
influential authors. Top three authors based on
hif-index are shown in Table 4.

5 10 15 20 25 30 35
0

0.2

0.4

0.6

0.8

1

k

P
re

ci
si

o
n
@

k

 

 

H−index

Hif−index

TopP

TopC

AvgC

(a)

(b)
H−index

0.34AvgC  1

TopP  1 0.34 0.24 0.21

TopP TopC AvgC H−index

0.24

0.21 0.28 0.38 0.31 1

0.310.32 0.24  1

0.24 0.38

0.27TopC 1 0.35 0.32 0.28

0.27

Hif−index

0.35

Hif−index

Figure 3: (a) Sprearman’s rank correlation among
pair-wise ranks, and (b) the performance of all the
measures.

6.3 Effect on Recommendation System
Here we show the effectiveness of reference-
intensity by applying it to a real paper recommen-
dation system. To this end, we consider FeRoSA7

(Chakraborty et al., 2016), a new (probably the first)
framework of faceted recommendation for scien-
tific articles, where given a query it provides facet-
wise recommendations with each facet representing
the purpose of recommendation (Chakraborty et al.,
2016). The methodology is based on random walk
with restarts (RWR) initiated from a query paper.
The model is built on AAN dataset and considers
both the citation links and the content information
to produce the most relevant results. Instead of us-
ing the unweighted citation network, here we use the
weighted network with each edge labeled by the in-
tensity score. The final recommendation of FeRoSA
is obtained by performing RWR with the transition
probability proportional to the edge-weight (we call
it Inf-FeRoSA). We observe that Inf-FeRoSA
achieves an average precision of 0.81 at top 10 rec-
ommendations, which is 14% higher then FeRoSA
while considering the flat version and 12.34% higher
than FeRoSA while considering the faceted version.

6.4 Detecting Citation Stacking
Recently, Thomson Reuters began screening for
journals that exchange large number of anomalous
citations with other journals in a cartel-like arrange-
ment, often known as “citation stacking” (Jump,
2013; Hardcastle, 2015). This sort of citation stack-
ing is much more pernicious and difficult to detect.

7www.ferosa.org

1355



No Paper Author
1. Lexical semantic techniques for corpus analysis (Pustejovsky et al., 1993) Mark Johnson
2. An unsupervised method for detecting grammatical errors (Chodorow and Leacock, 2000) Christopher D. Manning
3. A maximum entropy approach to natural language processing (Berger et al., 1996) Dan Klein

Table 4: Top three papers and authors based on InfPR and Hif-index respectively.

0 500 1000 1500 2000
0

500

1000

1500

Total citations

T
o
ta

l 
ci

ta
ti

o
n
s 

(e
x
cl

u
d
in

g
 s

el
f−

ci
ta

ti
o
n
s)

 

 

0 1 2 3 4 5
0

1

2

3

4

5

IF

IF
if

 

 

(a) (b)

Figure 4: Correlation between (a) IF and IFif and
(b) number of citations before and after removing
self-journal citations.

We anticipate that this behavior can be detected by
the reference intensity. Since the AAN dataset does
not have journal information, we use DBLP dataset
(Singh et al., 2015) where the complete metadata
information (along with reference contexts and ab-
stract) is available, except the full content of the pa-
per (559,338 papers and 681 journals; more details
in (Chakraborty et al., 2014)). From this dataset,
we extract all the features mentioned in Section 3.3
except the ones that require full text, and run our
model using the existing annotated dataset as train-
ing instances. We measure the traditional impact
factor (IF ) of the journals and impact factor after
considering the reference intensity (IFif ). Figure
4(a) shows that there are few journals whose IFif
significantly deviates (3σ from the mean) from IF ;
out of the suspected journals 70% suffer from the ef-
fect of self-journal citations as well (shown in Figure
4(b)), example including Expert Systems with Appli-
cations (current IF of 2.53). One of the future work
directions would be to predict such journals as early
as possible after their first appearance.

7 Related Work

Although the citation count based metrics are widely
accepted (Garfield, 2006; Hirsch, 2010), the belief
that mere counting of citations is dubious has also
been a subject of study (Chubin and Moitra, 1975).
(Garfield, 1964) was the first who explained the rea-
sons of citing a paper. (Pham and Hoffmann, 2003)
introduced a method for the rapid development of
complex rule bases for classifying text segments.

(Dong and Schfer, 2011) focused on a less man-
ual approach by learning domain-insensitive fea-
tures from textual, physical, and syntactic aspects
To address concerns about h-index, different alterna-
tive measures are proposed (Waltman and van Eck,
2012). However they too could benefit from filtering
or weighting references with a model of influence.
Several research have been proposed to weight ci-
tations based on factors such as the prestige of the
citing journal (Ding, 2011; Yan and Ding, 2010),
prestige of an author (Balaban, 2012), frequency of
citations in citing papers (Hou et al., 2011). Re-
cently, (Wan and Liu, 2014) proposed a SVR based
approach to measure the intensity of citations. Our
methodology differs from this approach in at lease
four significant ways: (i) they used six very shallow
level features; whereas we consider features from
different dimensions, (ii) they labeled the dataset by
the help of independent annotators; here we addi-
tionally ask the authors of the citing papers to iden-
tify the influential references which is very realistic
(Gilbert, 1977); (iii) they adopted SVR for labeling,
which does not perform well for small training in-
stances; here we propose GraLap , designed specif-
ically for small training instances; (iv) four applica-
tions of reference intensity mentioned here are com-
pletely new and can trigger further to reassessing the
existing bibliometrics.

8 Conclusion

We argued that the equal weight of all references
might not be a good idea not only to gauge success
of a research, but also to track follow-up work or rec-
ommending research papers. The annotated dataset
would have tremendous potential to be utilized for
other research. Moreover, GraLap can be used for
any semi-supervised learning problem. Each appli-
cation mentioned here needs separate attention. In
future, we shall look into more linguistic evidences
to improve our model.

1356



References
Awais Athar and Simone Teufel. 2012. Context-

enhanced citation sentiment detection. In NAACL,
pages 597–601, Stroudsburg, PA, USA. ACL.

Alexandru T. Balaban. 2012. Positive and negative as-
pects of citation indices and journal impact factors.
Scientometrics, 92(2):241–247.

Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.,
22(1):39–71, March.

Tanmoy Chakraborty, Suhansanu Kumar, Pawan Goyal,
Niloy Ganguly, and Animesh Mukherjee. 2014. To-
wards a stratified learning approach to predict fu-
ture citation counts. In Proceedings of the 14th
ACM/IEEE-CS Joint Conference on Digital Libraries,
JCDL ’14, pages 351–360, Piscataway, NJ, USA.
IEEE Press.

Tanmoy Chakraborty, Amrith Krishna, Mayank Singh,
Niloy Ganguly, Pawan Goyal, and Animesh Mukher-
jee, 2016. Advances in Knowledge Discovery and
Data Mining: 20th Pacific-Asia Conference, PAKDD
2016, Auckland, New Zealand, April 19-22, 2016, Pro-
ceedings, Part II, chapter FeRoSA: A Faceted Recom-
mendation System for Scientific Articles, pages 528–
541. Springer International Publishing, Cham.

Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
NAACL, pages 140–147, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

D. E. Chubin and S. D. Moitra. 1975. Content-Analysis
of References Adjunct or Alternative to Citation
Counting. Social studies of science, 5(4):423–441.

J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37–41.

Isaac G Councill, C Lee Giles, and Min-Yen Kan. 2008.
Parscit: an open-source crf reference string parsing
package. In LREC, pages 28–30, Marrakech, Mo-
rocco.

Ying Ding. 2011. Applying weighted pagerank to author
citation networks. JASIST, 62(2):236–245.

Cailing Dong and Ulrich Schfer. 2011. Ensemble-style
self-training on citation classification. In IJCNLP,
pages 623–631. ACL, 11.

David Dubin. 2004. The most influential paper gerard
salton never wrote. Library Trends, 52(4):748–764.

Eugene Garfield. 1964. Can citation indexing be au-
tomated? Statistical association methods for mecha-
nized documentation, Symposium proceedings, pages
188–192.

Eugene Garfield. 2006. The History and Meaning of the
Journal Impact Factor. JAMA, 295(1):90–93.

G. N. Gilbert. 1977. Referencing as persuasion. Social
Studies of Science, 7(1):113–122.

James Hardcastle. 2015. Citations, self-citations, and
citation stacking, http://editorresources.
taylorandfrancisgroup.com/
citations-self-citations\
\-and-citation-stacking/.

J. E. Hirsch. 2005. An index to quantify an individual’s
scientific research output. PNAS, 102(46):16569–
16572.

J. E. Hirsch. 2010. An index to quantify an individ-
ual’s scientific research output that takes into account
the effect of multiple coauthorship. Scientometrics,
85(3):741–754, December.

Wen-Ru Hou, Ming Li, and Deng-Ke Niu. 2011. Count-
ing citations in texts rather than reference lists to im-
prove the accuracy of assessing scientific contribution.
BioEssays, 33(10):724–727.

Charles Jochim and Hinrich Schütze. 2012. Towards
a generic and flexible citation classifier based on a
faceted classification scheme. In COLING, pages
1343–1358, Bombay, India.

Paul Jump. 2013. Journal citation
cartels on the rise, https://www.
timeshighereducation.com/news/
journal-citation-cartels-on-the-rise/
2005009.article.

J. B. Kruskal. 1956. On the Shortest Spanning Subtree of
a Graph and the Traveling Salesman Problem. In Pro-
ceedings of the American Mathematical Society, vol-
ume 7, pages 48–50.

Maria Liakata, Shyamasree Saha, Simon Dobnik,
Colin R. Batchelor, and Dietrich Rebholz-Schuhmann.
2012. Automatic recognition of conceptualization
zones in scientific articles and two life science appli-
cations. Bioinformatics, 28(7):991–1000.

M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating typed dependency parses from phrase
structure parses. In LREC, pages 449–454, Genoa,
Italy, May. European Language Resources Association
(ELRA).

M. J. Moravcsik and P. Murugesan. 1975. Some results
on the function and quality of citations. Social studies
of science, 5(1):86–92.

L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. In WWW, pages 161–172, Brisbane, Australia.

Son Bao Pham and Achim Hoffmann. 2003. A new ap-
proach for scientific citation classification using cue
phrases. In Tamas Domonkos Gedeon and Lance
Chun Che Fung, editors, Advances in Artificial Intelli-
gence: 16th Australian Conference on AI, pages 759–
771. Springer Berlin Heidelberg.

1357



M. F. Porter. 1997. Readings in information retrieval.
chapter An Algorithm for Suffix Stripping, pages 313–
316. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.

Friedrich Pukelsheim. 1994. The Three Sigma Rule.
The American Statistician, 48(2):88–91.

James Pustejovsky, Peter Anick, and Sabine Bergler.
1993. Lexical semantic techniques for corpus analy-
sis. Comput. Linguist., 19(2):331–358, June.

J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.

Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The acl anthology network
corpus. In Proceedings of the 2009 Workshop on
Text and Citation Analysis for Scholarly Digital Li-
braries, NLPIR4DL, pages 54–61, Stroudsburg, PA,
USA. ACL.

Mikhail V. Simkin and V. P. Roychowdhury. 2003. Read
Before You Cite! Complex Systems, 14:269–274.

Mayank Singh, Vikas Patidar, Suhansanu Kumar, Tan-
moy Chakraborty, Animesh Mukherjee, and Pawan
Goyal. 2015. The role of citation context in pre-
dicting long-term citation profiles: An experimental
study based on a massive bibliographic text dataset. In
CIKM, pages 1271–1280, New York, NY, USA. ACM.

Henry Small. 1973. Co-citation in the scientific litera-
ture: A new measure of the relationship between two
documents. JASIST, 24(4):265–269.

Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In EMNLP, pages 103–110, Stroudsburg, PA, USA.
ACL.

Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP, pages 63–
70, Stroudsburg, PA, USA. ACL.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Int. Res., 37(1):141–188, January.

Ludo Waltman and Nees Jan van Eck. 2012. The in-
consistency of the h-index. JASIST, 63(2):406–415,
February.

Xiaojun Wan and Fang Liu. 2014. Are all literature ci-
tations equally important? automatic citation strength
estimation and its applications. JASIST, 65(9):1929–
1938.

Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, Second
Edition (Morgan Kaufmann Series in Data Manage-
ment Systems). Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.

Erjia Yan and Ying Ding. 2010. Weighted citation: An
indicator of an article’s prestige. JASIST, 61(8):1635–
1643.

Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In ICML, pages 912–919,
Washington D.C.

Xiaodan Zhu, Peter Turney, Daniel Lemire, and Andr
Vellino. 2015. Measuring academic influence: Not
all citations are equal. JASIST, 66(2):408–427.

1358


