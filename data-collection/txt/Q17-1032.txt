








































Unsupervised Acquisition of Comprehensive Multiword Lexicons
using Competition in an n-gram Lattice

Julian Brooke∗ Jan Šnajder† Timothy Baldwin∗
∗ School of Computing and Information Systems, The University of Melbourne

julian.brooke@gmail.com, tb@ldwin.net
† Faculty of Electrical Engineering and Computing, University of Zagreb

jan.snajder@fer.hr

Abstract

We present a new model for acquiring com-
prehensive multiword lexicons from large cor-
pora based on competition among n-gram
candidates. In contrast to the standard ap-
proach of simple ranking by association mea-
sure, in our model n-grams are arranged in
a lattice structure based on subsumption and
overlap relationships, with nodes inhibiting
other nodes in their vicinity when they are se-
lected as a lexical item. We show how the con-
figuration of such a lattice can be optimized
tractably, and demonstrate using annotations
of sampled n-grams that our method consis-
tently outperforms alternatives by at least 0.05
F-score across several corpora and languages.

1 Introduction

Despite over 25 years of research in computational
linguistics aimed at acquiring multiword lexicons
using corpora statistics, and growing evidence that
speakers process language primarily in terms of
memorized sequences (Wray, 2008), the individual
word nonetheless stubbornly remains the de facto
standard processing unit for most research in mod-
ern NLP. The potential of multiword knowledge to
improve both the automatic processing of language
as well as offer new understanding of human acqui-
sition and usage of language is the primary moti-
vator of this work. Here, we present an effective,
expandable, and tractable new approach to compre-
hensive multiword lexicon acquisition. Our aim is
to find a middle ground between standard MWE ac-
quisition approaches based on association measures

(Ramisch, 2014) and more sophisticated statistical
models (Newman et al., 2012) that do not scale to
large corpora, the main source of the distributional
information in modern NLP systems.

A central challenge in building comprehensive
multiword lexicons is paring down the huge space
of possibilities without imposing restrictions which
disregard a major portion of the multiword vocab-
ulary of a language: allowing for diversity creates
significant redundancy among statistically promis-
ing candidates. The lattice model proposed here ad-
dresses this primarily by having the candidates—
contiguous and non-contiguous n-gram types—
compete with each other based on subsumption and
overlap relations to be selected as the best (i.e., most
parsimonious) explanation for statistical irregulari-
ties. We test this approach across four large corpora
in three languages, including two relatively free-
word-order languages (Croatian and Japanese), and
find that this approach consistency outperforms al-
ternatives, offering scalability and many avenues for
future enhancement.

2 Background and Related Work

In this paper we will refer to the targets of our lexi-
con creation efforts as formulaic sequences, follow-
ing the terminology of Wray (2002; 2008), wherein
a formulaic sequence (FS) is defined as “a sequence,
continuous or discontinuous, of words or other ele-
ments, which is, or appears to be, prefabricated: that
is, stored and retrieved whole from memory at the
time of use, rather than being subject to generation
or analysis by the language grammar.” That is, an

455

Transactions of the Association for Computational Linguistics, vol. 5, pp. 455–470, 2017. Action Editor: Noah Smith.
Submission batch: 1/2017; Revision batch: 5/2017; Published 11/2017.

c©2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.



FS shows signs of being part of a mental lexicon.1

As noted by Wray (2008), formulaic sequence the-
ory is compatible with other highly multiword, lex-
icalized approaches to language structure, in partic-
ular Pattern Grammar (Hunston and Francis, ) and
Construction Grammar (Goldberg, 1995); an impor-
tant distinction, though, is that these sorts of theo-
ries often posit entirely abstract grammatical con-
structions/patterns/frames which do not fit well into
the FS framework. Nevertheless, since many such
constructions are composed of sequences of specific
words, the FS inventory of a language includes many
flexible constructions (e.g., ask ∗ for) along with en-
tirely fixed combinations (e.g., rhetorical question)
not typically of interest to grammarians. Note that
the FS framework allows for individual morphemes
to be part of a formulaic sequence, but for practical
reasons we focus primarily on lemmatized words as
the unit out of which FS are built.

In computational linguistics, the most common
term used to describe multiword lexical units is mul-
tiword expression (“MWE”: Sag et al. (2002), Bald-
win and Kim (2010)), but here we wish to make
a principled distinction between at least somewhat
non-compositional, strongly lexicalized MWEs and
FS, a near superset which includes many MWEs
but also compositional linguistic formulas. This
distinction is not a new one; it exists, for exam-
ple, in the original paper of Sag et al. (2002) in
the distinction between lexicalized and institutional-
ized phrases, and also to some extent in the MWE
annotation of Schneider et al. (2014b), who dis-
tinguish between weak (collocational)2 and strong
(non-compositional) MWEs. It is our contention,
however, that separate, precise terminology is use-
ful for research targeted at either class: we need not
strain the concept of MWE to include items which
do not require special semantics, nor are we inclined
to disregard the larger formulaticity of language sim-
ply because it is not the dominant focus of MWE

1Though by this definition individuals or small groups may
have their own FS, here we are only interested in FS that are
shared by a recognizable language community.

2Here we avoid the term collocation entirely due to confu-
sion with respect to its interpretation. Though some define it
similarly to our definition of FS, it can be applied to any words
that show a statistical tendency to appear in the vicinity of one
another for any reason: for instance, the pair of words doc-
tor/nurse might be considered a collocation (Ramisch, 2014).

Formulaic 
Sequences

Multiword
Expressions

Named Entities

Constructions

Figure 1: Multiword Terminology

research. Many MWE researchers might defensibly
balk at including in their MWE lexicons and cor-
pus annotations (English) FS such as there is some-
thing going on, it is more important than ever to
..., ... do not know what it is like to ..., there is no
shortage of ..., the rise and fall of ..., now is not the
time to ..., etc. as well as tens of thousands of other
such phrases which, along with less compositional
MWEs like be worth ...’s weight in gold, fall under
the FS umbrella. Another reason to introduce a dif-
ferent terminology is that there are classes of phrases
which are typically considered MWEs that do not
fit well into an FS framework, for instance novel
compound nouns whose semantics are accessible by
analogy (e.g., glass limb, analogous to wooden leg).
We also exclude from the definition of both FS and
MWE those named entities which refer to people or
places which are little-known and/or whose surface
form appears derived (e.g., Mrs. Barbara W. Smith
or Smith Garden Supplies Ltd). Figure 1 shows the
conception of the relationship between FS, (multi-
word) constructions, MWE, and (multiword) named
entities that we assume for this paper.

From a practical perspective, the starting point for
multiword lexicon creation has typically been lexi-
cal association measures (Church and Hanks, 1990;
Dunning, 1993; Schone and Jurafsky, 2001; Evert,
2004; Pecina, 2010; Araujo et al., 2011; Kulkarni
and Finlayson, 2011; Ramisch, 2014). When these
methods are used to build a lexicon, particular bi-
nary syntactic patterns are typically chosen. Only
some of these measures generalize tractably beyond
two words, for example PMI (Church and Hanks,
1990), i.e., the log ratio of the joint probability to
the product of the marginal probabilities of the in-
dividual words. Another measure which addresses
sequences of longer than two words is the C-value

456



(Frantzi et al., 2000) which weights term frequency
by the log length of the n-gram while penalizing n-
grams that appear in frequent larger ones. Mutual
expectation (Dias et al., 1999) involves deriving a
normalized statistic that reflects the extent to which a
phrase resists the omission of any constituent word.
Similarly, the lexical predictability ratio (LPR) of
Brooke et al. (2015) is an association measure appli-
cable to any possible syntactic pattern, which is cal-
culated by discounting syntactic predictability from
the overall conditional probability for each word
given the other words in the phrase. Though most
association measures involve only usage statistics
of the phrase and its subparts, the DRUID measure
(Riedl and Biemann, 2015) is an exception which
uses distributional semantics around the phrase to
identify how easily an n-gram could be replaced by
a single word.

Typically multiword lexicons are created by rank-
ing n-grams according to an association measure
and applying a threshold. The algorithm of da Silva
and Lopes (1999) is somewhat more sophisticated,
in that it identifies the local maxima of association
measures across subsuming n-grams within a sen-
tence to identify MWEs of unrestricted length and
syntactic composition; its effectiveness beyond noun
phrases, however, seems relatively limited (Ramisch
et al., 2012). Brooke et al. (2014; 2015) developed a
heuristic method intended for general FS extraction
in larger corpora, first using conditional probabili-
ties to do an initial (single pass) coarse-grained seg-
mentation of the corpus, followed by a pass through
the resulting vocabulary, breaking larger units into
smaller ones based on a tradeoff between marginal
and conditional statistics. The work of Newman
et al. (2012) is an example of an unsupervised ap-
proach which does not use association measures: it
extends the Bayesian word segmentation approach
of Goldwater et al. (2009) to multiword tokeniza-
tion, applying a generative Dirichlet Process model
which jointly constructs a segmentation of the cor-
pus and a corresponding multiword vocabulary.

Other research in MWEs has tended to be rather
focused on particular syntactic patterns such as verb-
noun combinations (Fazly et al., 2009). The system
of Schneider et al. (2014a) distinguishes a full range
of MWE sequences in the English Web Treebank,
including gapped expressions, using a supervised se-

quence tagging model. Though, in theory, automatic
lexical resources could be a useful addition to the
Schneider et al. model, which uses only manual lexi-
cal resources, attempts to do so have achieved mixed
success (Riedl and Biemann, 2016).

The motivations for building lexicons of FS nat-
urally overlap with those for MWE: models of dis-
tributional semantics, in particular, can benefit from
sensitivity to multiword units (Cohen and Widdows,
2009), as can parsing (Constant and Nivre, 2016)
and topic models (Lau et al., 2013). One major mo-
tivation for looking beyond MWEs is the ability to
carry out broader linguistic analyses. Within corpus
linguistics, multiword sequences have been studied
in the form of lexical bundles (Biber et al., 2004),
which are simply n-grams that occur above a cer-
tain frequency threshold. Like FS, lexical bundles
generally involve larger phrasal chunks that would
be missed by traditional MWE extraction, and so
research in this area has tended to focus on how
particular formulaic phrases (e.g., if you look at)
are indicative of particular genres (e.g., university
lectures). Lexical bundles have been applied, in
particular, to learner language: for example, Chen
and Baker (2010) show that non-native student writ-
ers use a severely restricted range of lexical bun-
dle types, and tend to overuse those types, while
Granger and Bestgen (2014) investigate the role of
proficiency, demonstrating that intermediate learn-
ers underuse lower-frequency bigrams and overuse
high-frequency bigrams relative to advanced learn-
ers. Sakaguchi et al. (2016) demonstrate that im-
proving fluency (closely linked to the use of lin-
guistic formulas) is more important than improving
strict grammaticality with respect to native speaker
judgments of non-native productions; Brooke et al.
(2015) explicitly argue for FS lexicons as a way to
identify, track, and improve learner proficiency.

3 Method

Our approach to FS identification involves optimiza-
tion of the total explanatory power of a lattice, where
each node corresponds to an n-gram type. The ex-
planatory power of the whole lattice is defined sim-
ply as a product of the explainedness of the individ-
ual nodes. Each node can be considered either “on”
(is an FS) or “off” (is not an FS). The basis of the

457



calculation of explainedness is the syntax-sensitive
LPR association measure of Brooke et al. (2015), but
it is calculated differently depending on the on/off
status of the node as well as the status of the nodes in
its vicinity. Nodes are linked based on n-gram sub-
sumption and corpus overlap relationships (see Fig-
ure 2), with “on” nodes typically explaining other
nodes. Given these relationships, we iterate over the
nodes and greedily optimize the on/off choice rel-
ative to explainedness in the local neighborhood of
each node, until convergence.

3.1 Collecting statistics

The first step in the process is to derive a set of n-
grams and related statistics from a large, unlabeled
corpus of text. Since our primary association mea-
sure is an adaption of LPR, our approach in this
section mostly follows Brooke et al. (2015) up un-
til the last stage. An initial requirement of any such
method is an n-gram frequency threshold, which we
set to 1 instance per 10 million words, following
Brooke et al. (2015).3

We include gapped or non-contiguous n-grams
in our analysis, in acknowledgment of the fact that
many languages have MWEs where the components
can be “separated”, including verb particle construc-
tions in English (Dehé, 2002), and noun-verb id-
ioms in Japanese (Hashimoto and Kawahara, 2008).
Having said this, there are generally strong syntac-
tic and length restrictions on what can constitute a
gap (Wasow, 2002), which we capture in the form
of a language-specific POS-based regular expres-
sion (see Section 4 for details). This greatly lowers
the number of potentially gapped n-gram types, in-
creasing precision and efficiency for negligible loss
of recall. We also exclude punctuation and lemma-
tize the corpus, and enforce an n-gram count thresh-
old. As long as the count threshold is substantially
above 1, efficient extraction of all n-grams can be
done iteratively: in iteration i, i-grams are filtered
by the frequency threshold, and then pairs of in-
stances of these i-grams with (i− 1) words of over-
lap are found, which derives a set of (i + 1)-grams

3Based on manual analysis using the MWE corpus of
Schneider et al. (2014b), this achieves very good (over 90%)
type-level MWE coverage using the frequency filtered n-gram
statistics from the ICWSM blog corpus (see Section 4) after fil-
tering out proper names.

which necessarily includes all those over the fre-
quency threshold.

Once a set of relevant n-grams is identified
and counted, other statistics required to calculate
the Lexical Predictability Ratio (“LPR”) for each
word in the n-gram are collected. LPR is a measure
of how predictable a word is in a lexical context, as
compared to how predictable it is given only syn-
tactic context (over the same span of words). For-
mally, the LPR for word wi in the context of a word
sequence w1, ..., wi, ..., wn with POS tag sequence
t1, ..., tn is given by:

LPR(wi, w1,n) = max
1≤j<k≤n

p(wi|wj,k)
p(wi|tj,k)

where wj,k denotes the word sequence
wj , ..., wi−1, wi+1, ..., wk excluding wi (simi-
larly for tj,k). Note that the lower bound of LPR
is 1, since the ratio for a word with no context is
trivially 1. We use the same equation for gapped
n-grams, with the caveat that quantities involving
sequences which include the location where the
gap occurs are derived from special gapped n-gram
statistics. Note that the identification of the best
ratio across all possible choices of context, not just
the largest, is important for longer FS, where the
entire POS context alone might uniquely identify
the phrase, resulting in the minimum LPR of 1 even
for entirely formulaic sequences—an undesirable
result.

In the segmentation approach of Brooke et al.
(2015), LPR for an entire span is calculated as a
product of the individual LPRs, but here we will use
the minimum LPR across the words in the sequence:

minLPR(w1,n) = min
1≤i≤n

LPR(wi, w1,n)

Here, minLPR for a particular n-gram does not re-
flect the overall degree to which it holds together,
but rather focuses on the word which is its weakest
link. For example, in the case of be keep ∗ under
wraps (Figure 2), a general statistical metric might
assign it a high score due to the strong association
between keep and under or under and wraps, but
minLPR is focused on the weaker relationship be-
tween be and the rest of the phrase. This makes it
particularly suited to use in a lattice model of com-
peting n-grams, where the choice of be keep ∗ under

458



be keep * under wraps keep everything under wraps keep * under wraps for

be keep * under keep * under wraps under wraps for

be keep keep * under under wraps wraps for

Figure 2: A portion of an n-gram lattice. Solid lines in-
dicate subsumption, dotted lines overlaps

wraps versus keep ∗ under wraps should be based
exactly on the extent to which be is an essential part
of the phrase; the other affinities are, in effect, irrel-
evant, because they occur in the smaller n-gram as
well.

3.2 Node interactions

The n-gram nodes in the lattice are directionally
connected to nodes consisting of (n + 1)-grams
which subsume them and (n− 1)-grams which they
subsume. For example, as detailed in Figure 2, the
(gapped) n-gram keep ∗ under wraps would be con-
nected “upwards” to the node keep everything under
wraps and connected “downwards” to under wraps.
These directional relationships allow for two ba-
sic interactions between nodes in the lattice when
a node is turned on: covering, which inhibits nodes
below (subsumed by) a turned-on node (e.g., if keep
∗ under wraps is on, the model will tend not to
choose under wraps as an FS); and clearing, which
inhibits nodes above a turned-on node (e.g., if keep
∗ under wraps is on, the model would avoid select-
ing keep everything under wraps as an FS). A third,
undirected mechanism is overlapping, where nodes
inhibit each other due to overlaps in the corpus (e.g.,
having both keep ∗ under wraps and be keep ∗ under
as FS will be avoided).

3.2.1 Covering
The most important node interaction is covering,

which corresponds to discounting or entirely exclud-
ing a node due to a node higher in the lattice. Our
model includes two types of covering: hard and soft.

Hard covering is based on the idea that, due to
very similar counts, we can reasonably conclude that
the presence of an n-gram in our statistics is a direct
result of a subsuming (n+i)-gram. In Figure 2, e.g.,

if we have 143 counts of keep ∗ under wraps and 152
counts of under wraps, the presence of keep ∗ un-
der wraps almost completely explains under wraps,
and we should consider these two n-grams as one.
We do this by permanently disabling any hard cov-
ered node, and setting the minLPR of the covering
node to the maximum minLPR among all the nodes
it covers (including itself); this means that longer n-
grams with function words (which often have lower
minLPR) can benefit from the strong statistical rela-
tionships between open-class lexical features in n-
grams that they cover. This is done as a prepro-
cessing step, and greatly improves the tractability of
the iterative optimization of the lattice. Of course,
a threshold for hard covering must be chosen: dur-
ing development we found that a ratio of 2/3 (cor-
responding to a significant majority of the counts
of a lower node corresponding to the higher node)
worked well. We also use the concept of hard cov-
ering to address the issue of pronouns, based on the
observation that specific pronouns often have high
LPR values due to pragmatic biases (Brooke et al.,
2015); for instance, private state verbs like feel tend
to have first person singular subjects. In the lat-
tice, n-grams with pronouns are considered covered
(inactive) unless they cover at least one other node
which does not have a pronoun, which allows us to
limit FS with pronouns without excluding them en-
tirely: they are included only in cases where they are
definitively formulaic.

Soft covering is used in cases when a single n-
gram does not entirely account for another, but a
turned-on n-gram to some extent may explain some
of the statistical irregularity of one lower in the lat-
tice. For instance, in Figure 2 keep ∗ under is not
hard-covered by keep ∗ under wraps (since there are
FS such as keep ∗ under surveillance and keep it un-
der your hat), but if keep ∗ under wraps is tagged
as an FS, we nevertheless want to discount the por-
tion of the keep ∗ under counts that correspond to
occurrences of keep ∗ under wraps, with the idea
that these occurrences have already been explained
by the longer n-gram. If enough subsuming n-grams
are on, then the shorter n-gram will be discounted to
the extent that it will be turned off, preventing re-
dundancy. This effect is accomplished by increasing
the turned-off explainedness of keep ∗ under (and
thus making turning on less desirable) in the follow-

459



ing manner: let c(·) be the count function, yi the
current FS status for node xi (0 if off, 1 if on) and
ab(x) a function which produces the set of indicies
of all nodes above node x in the lattice. Then, the
cover(xt) score for a covered node t is:

cover(xt) = max
(
0,
c(xt)−

∑
i∈ab(xt) yi · c(xi)
c(xt)

)

When applied as an exponent to a minLPR score, it
serves as simple, quick-to-calculate approximation
of a new minLPR with the counts corresponding to
the covering nodes removed from the calculation.
The cover score takes on values in the range 0 to
1, with 1 being the default when no covering occurs.

3.2.2 Clearing
In general, covering prefers turning on longer,

covering n-grams since doing so explains nodes
lower in the lattice. Not surprisingly, it is gener-
ally desirable to have a mechanism working in op-
position, i.e., one which views shorter FS as help-
ing to explain the presence of longer n-grams which
contain them, beyond the FS-neutral syntactic expla-
nation provided by minLPR. Clearing does this by
increasing the explainedness of nodes higher in the
lattice when a lower node is turned-on. The basic
mechanism is similar to covering, except that counts
cannot be made use of in the same way—whereas
it makes sense to explain covered nodes in propor-
tion to the counts of their covering nodes (since the
counts of the covered n-grams can be directly at-
tributed to the covering n-gram), in the reverse di-
rection this logic fails.

A simple but effective solution which avoids ex-
tra hyperparameters is to make use of the minLPR
values of the relevant nodes. In the most common
two-node situation, we increase the explainedness of
the cleared node based on the ratio of the minLPR of
two nodes, though only if the minLPR of the lower
node is higher. Generalized to the (rare) case of mul-
tiple clearing nodes, we define clear(xt) as:

clear(xt) =
∏

i∈bl(xt)
min

(
1,

minLPR(xt)
yi ·minLPR(xi)

)

where bl(xt) produces a set of indicies of nodes be-
low xt in the lattice. We refer to this mechanism as
“clearing” because it tends to clear away a variety of

trivial uses of common FS which may have higher
LPR due to the lexical and syntactic specificity of
the FS. For instance, in Figure 2 if the node keep ∗
under wraps is turned on and has a minLPR of 8,
then, if the minLPR of a node such as keep ∗ un-
der wraps for is 4, clear(xt) will be 0.5. Like cover,
clear takes on values in the range 0 to 1, with 1 be-
ing the default when no clearing occurs. Note that
one major advantage with this particular formulation
of clearing is that low-LPR nodes will be unable to
clear higher LPR nodes above them in the lattice;
otherwise, bad FS like of the might be selected as
FS based purely to increase the explainedness of the
many n-grams they appear in.

3.2.3 Overlap
The third mechanism of node interaction involves

n-grams which overlap in the corpus. In general,
independent FS do not consistently overlap. For ex-
ample, given that be keep ∗ under and keep ∗ un-
der wraps often appear together (overlapping on the
tokens keep ∗ under), we do not want both being
selected as an FS, even in the case that both have
high minLPR. To address this problem, rather than
increasing the explainedness of turned-off nodes,
we decrease the explainedness of the overlapping
turned-on nodes—a penalty rather than an incen-
tive which expresses the model’s confusion at hav-
ing overlapping FS. For non-subsuming nodes xi
and xj , let oc(xi, xj) be the count of instances
of xi which contain at least one non-gap token of
a corresponding instance of xj . For subsuming
nodes, though, overlap is treated asymmetrically,
with oc(xi, xj) equal to c(xj) (the lower count) if
j ∈ ab(xi), but zero if j ∈ bl(xi). Given this defini-
tion of oc, we define overlap(xt) as:

overlap(xt) =
c(xt)

c(xt)−
∑|X|

i=1 yi · oc(xt, xi)
Overlap takes on values in the range 1 to +∞,

also defaulting to 1 when no overlaps exist. The ef-
fect of overlap is hyperbolic: small amounts of over-
lap have little effect, but nodes with significant over-
lap will effectively be forced to turn off.

3.3 Explainedness
The objective function maximized by the model is
then the explainedness (expl) across all the nodes

460



of the lattice X , xi, . . . , xN , which can be defined
in terms of minLPR, the node interaction functions,
and the FS status yi of each node in the lattice:

expl(X) =
|X|∏

i=1

yi · C−overlap(xi)

+ (−yi + 1) ·minLPR(xi)−cover(xi)·clear(xi) (1)

When a node is off, its explainedness is the inverse
of its minLPR, except if there are covering or clear-
ing nodes which explain it by pushing the exponent
of minLPR towards zero. When the node is on, its
explainedness is the inverse of a fixed cost hyperpa-
rameterC, though this cost is increased if it overlaps
with other active nodes. All else being equal, when
minLPR(t) > C, a node will be selected as an FS,
and so, independent of the node interactions, C can
be viewed as the threshold for the minLPR associa-
tion measure under a traditional approach to MWE
identification.

3.4 Optimization

The dependence of the explainedness of nodes on
their neighbors effectively prohibits a global opti-
mization of the lattice. Fortunately, though most of
the nodes in the lattice are part of a single connected
graph, most of the effects of nodes on each other
are relatively local, and effective local optimizations
can be made tractable by applying some simple re-
strictions. The main optimization loop consists of it-
erations over the lattice until complete convergence
(no changes in the final iteration). For each itera-
tion over the main loop, each potentially active node
is examined in order to evaluate whether its current
status is optimal given the current state of the lat-
tice. The order that we perform this has an effect
on the result: among the obvious options (LPR, n-
gram length), in development good results were ob-
tained through ordering nodes by frequency, which
gives an implicit advantage to relatively common n-
grams.

Given the relationships between nodes, it is ob-
viously not sufficient to consider switching only the
present node. If, for instance, one or more of be keep
∗ under wraps, under wraps, or be keep ∗ under has
been turned on, the covering, clearing, or overlap-
ping effects of these other nodes will likely prevent

Algorithm 1 Optimization algorithm. X is an or-
dered list of the nodes in the lattice. Nodes (desig-
nated by x) contain pointers to the nodes immedi-
ately linked to them in the lattice. States (designated
by Y ) indicate whether each node is ON or OFF.
Explainedness values are indicated by e. rev = rele-
vant, aff = affected, curr = current

function LOCALOPT(Ystart , x,Xrev , Xaff )
Ystart ← SET(Ystart , x,ON)
Q← EMPTYQUEUE()
ebest ← 0
Ybest ← NULL
PUSH(Q,Ystart)
repeat

Ycurr ← POP(Q)
ecurr ← CALCEXPL(Ycurr , Xaff )
for xrev in Xrev do

Ynew ← SET(Ycurr , xrev ,OFF)
enew ← CALCEXPL(Ynew , Xaff )
if enew > ecurr then

PUSH(Q,Ynew )
if enew > ebest then:

ebest ← enew
Ybest ← Ynew

until ISEMPTY(Q)
return Ybest

FREQUENCYSORTREVERSE(X)
s← INITIALIZEALLOFF(X)
repeat

Changed ← FALSE
for x in X do

Xrev ← GETRELEVANT(x,X)
Xaff ← GETAFFECTED(Xrev , X)
Ynew ← LOCALOPT(Y, x,Xrev , Xaff )
if Ynew 6= Y then

Y ← Ynew
Changed ← TRUE

until !Changed

a competing node like keep ∗ under wraps from be-
ing correctly activated. Instead, the algorithm iden-
tifies a small set of “relevant” nodes which are the
most important to the status of the node under con-
sideration. Since turned-off nodes have no direct
effect on each other, only turned-on nodes above,

461



below, or overlapping with the current node in the
lattice need be considered. Once the relevant nodes
have been identified, all nodes (including turned-off
nodes) whose explainedness is affected by one or
more of the relevant nodes are identified. Next, a
search is carried out for the optimal configuration
of the relevant nodes, starting from an ‘all-on’ state
and iteratively considering new states with one rel-
evant node turned off; the search continues as long
as there is an improvement in explainedness. Since
the node interactions are roughly cumulative in their
effects, this approach will generally identify the op-
timal state without the need for an exhaustive search.
See Algorithm 1 for details.

Omitted from Algorithm 1 for clarity are various
low-level efficiencies which prevent the algorithm
from reconsidering states already checked or from
recalculating the explainedness of nodes when un-
necessary. We also apply the following efficiency re-
strictions, which significantly reduce the runtime of
the algorithm. In each case, more extreme (less effi-
cient) values were individually tested using a devel-
opment set and found to provide no benefit in terms
of the quality of the output lexicon:
• We limit the total number of relevant nodes to

5. When there are more than 5 nodes turned
on in the vicinity of the target node, the most
relevant nodes are selected by ranking candi-
dates by the absolute difference in explained-
ness across possible configurations of the target
and candidate node considered in isolation;
• To avoid having to deal with storing and pro-

cessing trivial overlaps, we exclude overlaps
with a count of less than 5 from our lattice;
• Many nodes have a minLPR which is slightly

larger than 1 (the lowest possible value). There
is very little chance these nodes will be acti-
vated by the algorithm, and so after applying
hard covering, we do not consider activating
nodes with minLPR < 2.

4 Evaluation

We evaluate our approach across three different lan-
guages, including evaluation sets derived from four
different corpora selected for their size and linguis-
tic diversity. In English, we follow Brooke et al.
(2015) in using a 890M token filtered portion of the

ICWSM blog corpus (Burton et al., 2009) tagged
with the Tree Tagger (Schmid, 1995). To facilitate a
comparison with Newman et al. (2012), which does
not scale up to a corpus as large as the ICWSM, we
also build a lexicon using the 100M token British
National Corpus (Burnard, 2000), using the standard
CLAWS-derived POS tags for the corpus. Lemma-
tization included removing all inflectional marking
from both words and POS tags. For English, gaps
are identified using the same POS regex used in
Brooke et al. (2015), which includes simple nouns
and portions thereof, up to a maximum of 4 words.

The other two languages we include in our eval-
uation are Croatian and Japanese. Relative to En-
glish, both languages have freer word order: we
were interested in probing the challenges associ-
ated with using an n-gram approach to FS identi-
fication in such languages. For Croatian, we used
the 1.2-billion-token fhrWaC corpus (Šnajder et al.,
2013), a filtered version of the Croatian web cor-
pus hrWaC (Ljubešić and Klubička, 2014), which is
POS-tagged and lemmatized using the tools of Agić
et al. (2013). Similar to English, the POS regex
for Croatian includes simple nouns, adjectives and
pronouns, but also other elements that regularly ap-
pear inside FS, including both adverbs and copulas.
For Japanese, we used a subset of the 100M-page
web corpus of Shinzato et al. (2008), which was
roughly the same token length as the English cor-
pus. We segmented and POS-tagged the corpus with
MeCab (Kudo, 2008) using the UNIDIC morpho-
logical dictionary (Den et al., 2007). The POS regex
for Japanese covers the same basic nominal struc-
tures as English, but also includes case markers and
adverbials. Though our processing of Japanese in-
cludes basic lemmatization related to superficial el-
ements like the choice of writing script and polite-
ness markers, many elements (such as case marking)
which are removed by lemmatization in Croatian are
segmented into independent morphological units in
the MeCab output, making the task somewhat dif-
ferent for the two languages.

Brooke et al. (2015) introduced a method for eval-
uating FS extraction without a reference lexicon or
direct annotation of the output of a model. Instead,
n-grams are sampled after applying the frequency
threshold and then annotated as being either an FS
or not. Benefits of this style of evaluation include

462



Contiguous Gapped κ

FS non-FS FS non-FS Pre Post

ICWSM 169 702 29 916 0.52 0.84
BNC 49 403 8 475 0.51 0.84
Croatian 64 382 11 456 0.58 0.87
Japanese 102 337 9 438 0.49 0.82

Table 1: Statistics for test sets

replicability, the diversity of FS, and the ability to
calculate a true F-score. We use the annotation of
2000 n-grams in the ICWSM corpus from that ear-
lier work, and applied the same annotation method-
ology to the other three corpora: after training and
based on written guidelines derived from the defini-
tions of Wray (2008), three native-speaker, educated
annotators judged 500 contiguous n-grams and an-
other 500 gapped n-grams for each corpus.

Other than the inclusion of new languages, our
test sets differ from Brooke et al. (2015) in two
ways. One advantage of a type-based annotation ap-
proach, particularly with regards to annotation with
a known subjective component, is that it is quite sen-
sible to simply discard borderline cases, improving
reliability at the cost of some representativeness. As
such we entirely excluded from our test set n-grams
which just one annotator marked as FS. Table 1 con-
tains the counts for the four test sets after this fil-
tering step and Fleiss’ Kappa scores before (“Pre”)
and after (“Post”). The second change is that for the
main evaluation we collapsed gapped and contigu-
ous n-grams into a single test set. The rationale is
that the number of positive gapped examples is too
low to provide a reliable independent F-score.

Our primary comparison is with the heuristic LPR
model of Brooke et al. (2015), which is scalable to
large corpora and includes gapped n-grams. For the
BNC, we also benchmark against the DP-seg model
of Newman et al. (2012) with recommended set-
tings, and the LocalMaxs algorithm of da Silva and
Lopes (1999) using SCP; neither of these methods
scale to the larger corpora.4 Because these other ap-
proaches only generate sequential multiword units,

4DP-seg is far too slow, and LocalMaxs, though faster, cal-
culates counts for all n-grams in the corpus, which would re-
quire terabytes of RAM for the large corpora.

we use only the sequential part of the BNC test
set for this evaluation. All comparison approaches
have themselves been previously compared against
a wide range of association measures. As such, we
do not repeat all these comparisons here, but we do
consider a lexicon built from ranking n-grams ac-
cording to the measure used in our lattice (minLPR)
as well as PMI and raw frequency. For each of these
association measures we rank all n-grams above the
frequency threshold and build a lexicon equal to the
size of the lexicon produced by our model.

We created small development sets for each cor-
pus and used them to do a thorough testing of pa-
rameter settings. Although it is generally possible
to increase precision by increasing C, we found that
across corpora we always obtained near-optimal re-
sults withC = 4, so to demonstrate the usefulness of
the lattice technique as an entirely off-the-shelf tool,
we present the results using identical settings for all
four corpora. We treat covering as a fundamental
part of the Lattice model, but to investigate the effi-
cacy of other node interactions within the model we
present results with overlap and clearing node inter-
actions turned off.

5 Results

The main results for FS acquisition across the four
corpora are shown in Table 2. As noted in Section 2,
simple statistical association measures like PMI do
poorly when faced with syntactically-unrestricted
n-grams of variable length: minLPR is clearly a
much better statistic for this purpose. The LPRseg
method of Brooke et al. (2015) consistently outper-
forms simple ranking, and the lattice method pro-
posed here does better still, with a margin that is
fairly consistent across the languages. Generally,
clearing and overlap node interactions provide a rel-
atively large increase in precision at the cost of a
smaller drop in recall, though the change is fairly
symmetrical in Croatian. When only covering is
used, the results are fairly similar to Brooke et al.
(2015), which is unsurprising given the extent to
which decomposition and covering are related. The
Japanese and ICWSM corpora have relatively high
precision and low recall, whereas both the BNC and
Croatian corpora have low precision and high recall.

In the contiguous FS test set for the BNC (Ta-

463



English

ICWSM BNC Croatian Japanese

P R F P R F P R F P R F

Countrank 0.13 0.09 0.10 0.08 0.14 0.10 0.08 0.12 0.10 0.11 0.06 0.08
PMIrank 0.24 0.15 0.18 0.13 0.25 0.17 0.21 0.25 0.23 0.18 0.08 0.11
minLPRrank 0.47 0.33 0.38 0.27 0.40 0.32 0.34 0.41 0.37 0.53 0.26 0.35
LPR-seg 0.53 0.42 0.47 0.37 0.44 0.40 0.41 0.47 0.43 0.69 0.43 0.53

Lattice −cl,ovr 0.47 0.53 0.50 0.31 0.60 0.41 0.32 0.66 0.43 0.49 0.61 0.54
Lattice −cl 0.57 0.42 0.49 0.33 0.58 0.42 0.39 0.56 0.46 0.63 0.49 0.55
Lattice −ovr 0.52 0.51 0.51 0.34 0.60 0.44 0.36 0.67 0.47 0.53 0.60 0.56
Lattice 0.67 0.43 0.52 0.40 0.60 0.48 0.44 0.56 0.49 0.78 0.48 0.59

Table 2: Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based
ranking; minLPRrank = ranking with minLPR; LPRseg = the method of Brooke et al. (2015); “−cl” = no clearing;
“−ovr” = no penalization of overlaps; “P” = Precision; “R” = Recall; and “F” = F-score. Bold is best in a given
column. The performance difference of the Lattice model relative to the best baseline for all test sets considered
together is significant at p < 0.01 (based on the permutation test: Yeh (2000)).

P R F

PMIrank 0.20 0.29 0.23
minLPRrank 0.34 0.45 0.39
LPR-seg 0.42 0.45 0.43
LocalMaxs 0.56 0.39 0.46
DP-seg 0.35 0.71 0.47

Lattice 0.47 0.63 0.54

Table 3: Results of FS identification in contiguous BNC
test set; LocalMaxs = method of da Silva and Lopes
(1999); DP-seg = method of Newman et al. (2012)

ble 3), we found that both the LocalMaxs algorithm
and the DP-seg method of Newman et al. (2012)
were able to beat our other baseline methods with
roughly similar F-scores, though both are well below
our Lattice method. Some of the difference seems
attributable to fairly severe precision/recall imbal-
ance, though we were unable to improve the F-score
by changing the parameters from recommended set-
tings for either model.

6 Discussion

Though the results across the four corpora are rea-
sonably similar with respect to overall F-score, there
are some discrepancies. By using the standard UNI-

DIC morpheme representation as the base unit for
Japanese, the model ends up doing an extra layer
of FS identification, one which is provided by word
boundaries in the other languages. The result is that
there are relatively more FS for Japanese: preci-
sion is high, and recall is comparably low. Impor-
tantly, the initial n-gram statistics actually reflect
that Japanese is different: the number of n-gram
types over length 4 is almost twice the number in
the ICWSM corpus. One idea for future work is to
automatically adapt to the input language/corpus in
order to ensure a good balance between precision
and recall.

At the opposite extreme, the low precision of the
BNC is almost certainly due to its relatively small
size: whereas the n-gram threshold we used here re-
sults in minimum counts of roughly 100 for the other
three corpora, the BNC statistics include n-grams
with counts of less than 10. At such low counts,
LPR is less reliable and more noise gets into the lex-
icon: the first column of Table 4 shows that the BNC
is noticeably larger then the other lexicons, and the
higher numbers in columns 2 and 3 (number of POS
types and percentage of gapped expressions, resp.)
are also indicative of increased noise. This could
be resolved by increasing the n-gram threshold. It
might also make sense to simply avoid smaller cor-
pora, though for some applications a smaller corpus

464



By Length (%)

Lexicon Word types POS types Gapped (%) 2 3 4 5+

English ICWSM 202k 16.8k 18.6 47.1 30.9 14.8 7.2
English BNC 367k 29.9k 26.2 38.3 36.3 17.9 7.5
Croatian 169k 6.6k 8.1 56.3 30.8 9.9 3.0
Japanese 219k 29.1k 7.9 39.6 31.0 16.9 12.5

Table 4: Statistics for the lexicons created by our lattice method

may be unavoidable. One idea we are pursing is
modifying the calculation of the LPR metric to use
a more conservative probability estimate than maxi-
mum likelihood in the case of low counts.

We were interested in Croatian and Japanese in
part because of their relatively free word order, and
whether the handling of gaps would help with iden-
tifying FS in these languages. We discovered, how-
ever, that free word order actually results in more of
a tendency towards contiguous FS, not less, a fact
that is reflected in our test sets (Table 1) as well as
the lexicons themselves (Table 4). Strikingly rare
in Croatian, in particular, are expressions where the
content of a gap is an argument which must be filled
to syntactically complete an expression: it is En-
glish whose fixed-word-order constraints often keep
elements of an FS distant from each other. The
gaps that do happen in Croatian are mostly prosody-
driven insertions of other elements into already com-
plete FS. This phenomena highlights a problem with
the current model, in that gapped and contiguous
versions of the same n-gram sequence (e.g., take
away and take ∗ away) are, at present, considered
entirely independently. Alternatives for dealing with
this include collapsing statistics to create a single
node in the lattice, creating a promoting link be-
tween contiguous and gapped versions of the same
n-grams sequence in the lattice model, or switching
to a dependency representation (which, we note, re-
quires very little change to the basic model presented
here, but would narrow its applicability).

The statistics in Table 4 otherwise reflect the
quantity and diversity of FS across the corpora, par-
ticularly in terms of the number of POS patterns rep-
resented in the lexicon. Looking at the most com-
mon POS patterns across languages, only noun-noun
and adjective-noun combinations ever account for

more than 5% of all word types in any of the lex-
icons. Though some of the diversity can of course
be attributed to noise, it is safe to say that most FS
do not fall into the standard two-word syntactic cate-
gories used in MWE work, and therefore identifying
them requires a much more general approach like
the one presented here.

Table 5 contains 10 randomly selected examples
from each of the lexicons produced by our method.
Among the English examples, most of the clear er-
rors are bigrams that reflect particular biases of their
respective corpora: The phrase via slashdot comes
from boilerplate text identifying the source of an ar-
ticle, whereas Maureen (from Maureen says) is a
character in one of the novels included in the BNC.
The longer FS mostly seem sensible, in that they are
plausible lexicalized constructions, though be open
to all * in the from the BNC seems too long and is
likely the result of noise due to insufficient exam-
ples. Some FS are dialectal variants, for instance
license endorsed refers to British traffic violations.
More generally, the FS lexicons created by these two
corpora are quite distinct, sharing less than 50% of
their entries.

One striking thing about the non-English FS is
how poorly they translate: many good FS in these
languages become extremely awkward when trans-
lated into English. This is expected, of course, for
idioms like biti general poslije bitke “be the general
after the battle” (i.e., “hindsight is 20/20”), but it ex-
tends to other relatively compositional constructions
like こう言う ∗が続く “repeat occurrences of ∗
like this” and 前期比 “first half comparison”. This
highlights the potential importance of focusing on
FS when learning a language. Though some of the
errors seem to be the result of extra material added to
a good FS, for instance promet teretnih vozila “good

465



English
(ICWSM)

heart ache, so ∗ have some time, part of the blame, via slashdot, any more questions, protein
expression, work in ∗ bank, al-qaeda terrorist, continue discussions, speak about * issue

English
(BNC)

go into decline, Maureen says, be open to all * in the, Peggy Sue, square ∗ shoulders,
delivery system for, this ∗ also includes, license endorsed, point ∗ finger, highly ∗ asset

Croatian negativno utjecati na “negatively affects on”, jedan od dobrih poznavatelja “one of the best
connoisseurs of”, jasno ∗ je da “it is clear to ∗ that”, promet teretnih vozila “good vehicle
traffic”, odvratiti pozornost “divert attention”, biti general poslije bitke “be the general after
the battle”, popularni internetski “popular internet”, izazvati kaos “cause chaos”, austrijski
investitor “Austrian investor”, ideja o gradnji “the idea of building”

Japanese 高速道路整備 “highway construction”,年次後期 “the second half of the fiscal year”,労
働者派遣事業 “temporary labor agency”,こう言う ∗が続く “repeat occurrences of ∗
like this”,風邪っ匹 “cold sufferer”,ＤＨＣＰサーバー “DHCP server”,前期比 “first
half comparison”, 経営事項審査 “examination of administrative affairs”, 自分の文章
“own writing”,深い味わい “deep flavor”

Table 5: 10 randomly-selected examples from the final FS lexicon from each corpus. Lemmas have been converted to
inflected forms where appropriate for readability.

vehicle traffic”, most, again, are somewhat inexpli-
cable artifacts of the corpus they were built from,
like austrijski investitor “Austrian investor”.

Since Zipfian frequency curves naturally extend
to multiword vocabulary, our lexicons (and type-
based evaluation of them) are of course dominated
by rarer terms. This is not, we would argue, a seri-
ous drawback, since in practical terms there is very
little value in focusing on common FS like of course
which manually-built lexicons already contain; most
of the potential in automatic extraction comes from
the long tail. However, we did investigate the other
end of the Zipfian curve by extracting the 20 most
common MWEs (including both strong and weak)
from the Schneider et al. (2014b) corpus. In the
ICWSM lexicon, our recall for these common terms
was fairly high (0.7), with errors mostly resulting
from longer phrases containing these terms “win-
ning out” (in the lattice) over shorter phrases, which
have relatively low LPR due to extremely common
constituent words; for example, we missed on time,
but had 19 FS which contain it (e.g. right on time,
show up on time, and start on time). In one case
which showed this same problem, waste * time, the
lexicon did have its ungapped version, highlighting
the potential for improved handling of this issue.

In Section 2, we noted that FS is generally a
much broader category than MWE, which we take

as referring to terms which carry significant non-
compositional meaning. We decided to investigate
the distinction at a practical level by annotating the
positive examples in the ICWSM test set for being
MWE or non-MWE FS.5 First, we note that only
28% of our FS types were labeled MWE; this is in
contrast to, for instance, the annotation of Schnei-
der et al. (2014b) where “weak” MWE make up a
small fraction of MWE types. Even without any ex-
plicit representation of compositionality, our model
did much better at identifying MWE FS than non-
MWE FS: 0.7 versus 0.32 recall. This may sim-
ply reflect, however, the fact that a disproportion-
ate number of MWEs were noun-noun compounds,
which are fairly easy for the model to identify.

Due to the lack of spaces between words and
an agglutinative morphology, the standard approach
to tokenization and lemmatization in Japanese in-
volves morphological rather than word segmenta-
tion. In terms of the content of the resulting lexi-
con we believe the effect of this difference on FS
extraction is modest, since much of the extra FS
in Japanese would simply be single words in other
languages (and considered trivially part of the FS
lexicon). However, from a theoretical perspective

5The set was exhaustively annotated by two native-speaker
annotators (κ = 0.73), and conflicts were resolved through dis-
cussion.

466



we might very much prefer to build FS for all lan-
guages starting from morphemes rather than words.
Such a framework could, for instance, capture in-
flectional flexibility versus fixedness directly in the
model, with fixed inflectional morphemes included
as a distinct element of the FS and flexible mor-
phemes becoming gaps. However, for many lan-
guages this would result in a huge blow up in com-
plexity with only modest increases in the scope of
FS identification. Though it is indisputable that in-
flectional fixedness is part of the lexical information
contained in an FS, in practice this sort of informa-
tion can be efficiently derived post hoc from the cor-
pus statistics.

Though we have demonstrated that competition
within a lattice is a powerful method for the pro-
duction of multiword lexicons, its usefulness de-
rives less from the specific choices we have made
in this instantiation of the model, and more from
the flexiblity that such a model provides for future
research. Not only do alternatives like DP-seg and
LocalMaxs fail to scale up to large corpora, there
are few obvious ways to improve on their simple
underlying algorithms without compromising their
elegance and worsening tractability. Fast and func-
tional, the LPR decomp approach is nevertheless al-
gorithmically ungainly, involving multiple layers of
heuristic-driven filtering with no possibility of cor-
recting errors. Our lattice method is aimed at some-
thing between these extremes: a practical, optimiz-
able model, but with various component heuristics
that can be improved upon. For instance, though the
current version of clearing is effective and has prac-
tical advantages relative to simpler options that we
tested, it could be enhanced by more careful investi-
gation of the statistical properties of n-grams which
contain FS.

We can also consider adding new terms to the
exponents of the two parts of our objective func-
tion, analagous to the cover, clear, and overlap func-
tions, based on other relationships between nodes in
the lattice. One which we have considered is cre-
ating new connections between identical or similar
syntactic patterns, which could serve to encourage
the model to generalize. In English, for instance,
it might learn that verb-particle combinations are
generally likely to be FS, whereas verb-determiner
combinations are not. Our initial investigations sug-

gest, however, it may be difficult to apply this idea
without merely amplifying existing undesirable bi-
ases in the LPR measure. Bringing in other infor-
mation such as simple distributional statistics might
help the model identify non-compositional seman-
tics, and could, in combination with the existing lat-
tice competition, focus the model on MWEs which
could provide a reliable basis for generalization.

For all four corpora, the lattice optimization al-
gorithm converged within 10 iterations. Although
the optimization of the lattice is several orders of
magnitude more complex than the decomposition
heuristics of Brooke et al. (2015), the time needed
to build and optimize the lattice is a fraction of the
time required to collect the statistics for LPR cal-
culation, and so the end-to-end runtimes of the two
methods are comparable. In the BNC, the full lat-
tice method was much faster than LocalMaxs and
DP-Seg, though direct runtime comparisons to these
methods are of modest value due to differences in
both scope and implementation.

Finally, though the model was designed specifi-
cally for FS extraction, we note that it could be use-
ful for related tasks such as unsupervised learning
of morphological lexicons, particularly for aggluti-
native languages. Character or phoneme n-grams
could compete in an identically structured lattice to
be chosen as the best morphemes for the language,
with LPR adapted to use phonological predictabil-
ity (i.e., based on vowel/consonant “tags”) instead of
syntactic predictability. It is likely, though, that fur-
ther algorithmic modifications would be necessary
to target morphological phenomena well, which we
leave for future work.

7 Conclusion

We have presented here a new methodology for
acquiring comprehensive multiword lexicons from
large corpora, using competition in an n-gram lat-
tice. Our evaluation using annotations of sampled
n-grams shows that it consistently outperforms al-
ternatives across several corpora and languages. A
tool which implements the method, as well as the ac-
quired lexicons, annotation guidelines, and test sets
have been made available.6

6https://github.com/julianbrooke/
LatticeFS

467



Acknowledgements

The second author was supported by an Endeavour
Research Fellowship from the Australian Govern-
ment, and in part by the Croatian Science Founda-
tion under project UIP-2014-09-7312. We would
also like to thank our English, Japanese, and Croat-
ian annotators, and the TACL reviewers and editors
for helping shape this paper into its current form.

References

Željko Agić, Nikola Ljubešić, and Danijela Merkler.
2013. Lemmatization and morphosyntactic tagging of
Croatian and Serbian. In Proceedings of the 4th Bien-
nial International Workshop on Balto-Slavic Natural
Language Processing, pages 48–57.

Vitor De Araujo, Carlos Ramisch, and Aline Villavicen-
cio. 2011. Fast and flexible MWE candidate genera-
tion with the mwetoolkit. In Proceedings of the ACL
2011 Workshop on Multiword Expressions: from Pars-
ing and Generation to the Real World (MWE 2011),
pages 134–136, Portland, OR.

Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, LA.

Douglas Biber, Susan Conrad, and Viviana Cortes. 2004.
If you look at. . .: Lexical bundles in university teach-
ing and textbooks. Applied Linguistics, 25:371–405.

Julian Brooke, Vivian Tsang, Graeme Hirst, and Fraser
Shein. 2014. Unsupervised multiword segmenta-
tion of large corpora using prediction-driven decom-
position of n-grams. In Proceedings of The 25th In-
ternational Conference on Computational Linguistics
(COLING 2014), pages 753–761, Dublin, Ireland.

Julian Brooke, Adam Hammond, David Jacob, Vivian
Tsang, Graeme Hirst, and Fraser Shein. 2015. Build-
ing a lexicon of formulaic language for language learn-
ers. In Proceedings of the NAACL ’15 Workshop on
Multiword Expressions, pages pp. 96–104, Denver,
CO.

Lou Burnard. 2000. User reference guide for British
National Corpus. Technical report, Oxford University.

Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM ’09), San Jose, CA.

Yu-Hua Chen and Paul Baker. 2010. Lexical bundles in
L1 and L2 academic writing. Language Learning &
Technology, 14(2):30–49.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22–29.

Trevor Cohen and Dominic Widdows. 2009. Empir-
ical distributional semantics: Methods and biomedi-
cal applications. Journal of Biomedical Informatics,
42(2):390–405.

Matthieu Constant and Joakim Nivre. 2016. A
transition-based system for joint lexical and syntactic
analysis. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (ACL
’16), pages 161–171, Berlin, Germany.

Joaquim da Silva and Gabriel Lopes. 1999. A local max-
ima method and a fair dispersion normalization for ex-
tracting multi-word units from corpora. In Proceed-
ings of the Sixth Meeting on Mathematics of Language
(MOL6), Orlando, FL.

Nicole Dehé. 2002. Particle Verbs in English: Syn-
tax, Information, Structure and Intonation. John Ben-
jamins, Amsterdam, Netherlands/Philadelphia, PA.

Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, Atsushi
Yamada, Nobuaki Minematsu, Kiyotaka Uchimoto,
and Hanae Koiso. 2007. The development of an elec-
tronic dictionary for morphological analysis and its ap-
plication to Japanese corpus linguistics (in Japanese).
Japanese Linguistics, 22:101–123.

Gaël Dias, Sylvie Guilloré, and José Gabriel Pereira
Lopes. 1999. Language independent automatic ac-
quisition of rigid multiword units from unrestricted
text corpora. In Proceedings of Conférence Traitement
Automatique des Langues Naturelles (TALN) 1999,
Cargèse, France.

Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tistics, 19(1):61–74.

Stefan Evert. 2004. The statistics of word
cooccurrences–word pairs and collocations. Ph.D.
thesis, University of Stuttgart.

Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61–103.

Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms: the
C-value/NC-value method. International Journal on
Digital Libraries, 3:115–130.

Adele Goldberg. 1995. Constructions: A construction
grammar approach to argument structure. University
of Chicago Press, Chicago/London.

Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112:21–54.

468



Sylviane Granger and Yves Bestgen. 2014. The use of
collocations by intermediate vs. advanced non-native
writers: A bigram-based study. International Review
of Applied Linguistics in Language Teaching, 52:229–
252.

Chikara Hashimoto and Daisuke Kawahara. 2008. Con-
struction of an idiom corpus and its application to id-
iom identification based on WSD incorporating idiom-
specific features. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2008), pages 992–1001, Honolulu,
HI.

Susan Hunston and Gill Francis. Pattern Grammar:
A corpus-driven approach to the lexical grammar
of English. John Benjamins, Amsterdam, Nether-
lands/Philadelphia, PA.

Taku Kudo. 2008. MeCab: Yet another part-of-speech
and morphological analyzer. http://taku910.
github.io/mecab/.

Nidhi Kulkarni and Mark Finlayson. 2011. jMWE: A
Java toolkit for detecting multi-word expressions. In
Proceedings of the ACL 2011 Workshop on Multiword
Expressions: from Parsing and Generation to the Real
World (MWE 2011), pages 122–124, Portland, OR.

Jey Han Lau, Timothy Baldwin, and David Newman.
2013. On collocations and topic models. ACM
Transactions on Speech and Language Processing,
10(3):10:1–10:14.

Nikola Ljubešić and Filip Klubička. 2014.
{bs,hr,sr}WaC – web corpora of Bosnian, Croat-
ian and Serbian. In Proceedings of the 9th Web as
Corpus Workshop (WaC-9), pages 29–35, Gothenburg,
Sweden.

David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmentation
for index term identification and keyphrase extraction.
In Proceedings of the 24th International Conference
on Computational Linguistics (COLING ’12), pages
2077–2092, Mumbai, India.

Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44:137–158.

Carlos Ramisch, Vitor De Araujo, and Aline Villavicen-
cio. 2012. A broad evaluation of techniques for auto-
matic acquisition of multiword expressions. In Pro-
ceedings of ACL 2012 Student Research Workshop,
pages 1–6, Jeju Island, Korea.

Carlos Ramisch. 2014. Multiword Expressions Acquisi-
tion: A Generic and Open Framework. Springer, Dor-
drecht, Netherlands.

Martin Riedl and Chris Biemann. 2015. A single word is
not enough: Ranking multiword expressions using dis-
tributional semantics. In Proceedings of the 2015 Con-

ference on Empirical Methods in Natural Language
Processing, pages 2430–2440, Lisbon, Portugal.

Martin Riedl and Chris Biemann. 2016. Impact of MWE
resources on multiword recognition. In Proceedings
of the 12th Workshop on Multiword Expressions, pages
107–111, Berlin, Germany.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing
’02), pages 1–15, Mexico City, Mexico.

Keisuke Sakaguchi, Courtney Napoles, Matt Post, and
Joel Tetreault. 2016. Reassessing the goals of gram-
matical error correction: Fluency instead of grammat-
icality. Transactions of the Assocation for Computa-
tional Linguistics, 4:169–182.

Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop, pages 47–50,
Dublin, Ireland.

Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014a. Discriminative lexical se-
mantic segmentation with gaps: running the MWE
gamut. Transactions of the Association for Compu-
tational Linguistics, 2:193–206.

Nathan Schneider, Spencer Onuffer, Nora Kazour, Emily
Danchik, Michael T. Mordowanec, Henrietta Conrad,
and Noah A. Smith. 2014b. Comprehensive annota-
tion of multiword expressions in a social web corpus.
In Proceedings of the Ninth International Conference
on Language Resources and Evaluation, pages 455–
461, Reykjavı́k, Iceland.

Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary headwords
a solved problem? In Proceedings of Empirical Meth-
ods in Natural Language Processing (EMNLP ’01),
pages 100–108, Pittsburgh, PA.

Keiji Shinzato, Daisuke Kawahara, Chikara Hashimoto,
and Sadao Kurohashi. 2008. A large-scale web data
collection as a natural language processing infrastruc-
ture. In Proceedings of the 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008), pages 2236–2241, Marrakech, Morocco.

Jan Šnajder, Sebastian Padó, and Željko Agić. 2013.
Building and evaluating a distributional memory for
Croatian. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics,
pages 784–789, Sofia, Bulgaria.

Thomas Wasow. 2002. Postverbal Behavior. CSLI Pub-
lications, Stanford, CA.

Alison Wray. 2002. Formulaic Language and the Lexi-
con. Cambridge University Press, Cambridge, UK.

469



Alison Wray. 2008. Formulaic Language: Pushing the
Boundaries. Oxford University Press, Oxford, UK.

Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics,
pages 947–953, Saarbruecken, Germany.

470


