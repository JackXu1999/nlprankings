




































Unsupervised Deep Structured Semantic Models for Commonsense Reasoning


Proceedings of NAACL-HLT 2019, pages 882–891
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

882

Unsupervised Deep Structured Semantic Models
for Commonsense Reasoning

Shuohang Wang1∗, Sheng Zhang2, Yelong Shen4, Xiaodong Liu3,
Jingjing Liu3, Jianfeng Gao3, Jing Jiang1

1Singapore Management University,2Johns Hopkins University, 3Microsoft, 4Tencent AI Lab
{shwang.2014,jingjiang}@smu.edu.sg, zsheng2@jhu.edu

{xiaodl,jingjl,jfgao}@microsoft.com, yelongshen@tencent.com

Abstract

Commonsense reasoning is fundamental to
natural language understanding. While tra-
ditional methods rely heavily on human-
crafted features and knowledge bases, we ex-
plore learning commonsense knowledge from
a large amount of raw text via unsupervised
learning. We propose two neural network
models based on the Deep Structured Seman-
tic Models (DSSM) framework to tackle two
classic commonsense reasoning tasks, Wino-
grad Schema challenges (WSC) and Pronoun
Disambiguation (PDP). Evaluation shows that
the proposed models effectively capture con-
textual information in the sentence and co-
reference information between pronouns and
nouns, and achieve significant improvement
over previous state-of-the-art approaches.

1 Introduction

Commonsense reasoning is concerned with sim-
ulating the human ability to make presumptions
about the type and essence of ordinary situa-
tions they encounter every day (Davis and Mar-
cus, 2015). It is one of the key challenges in nat-
ural language understanding, and has drawn in-
creasing attention in recent years (Levesque et al.,
2011; Roemmele et al., 2011; Zhang et al., 2017;
Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh
and Le, 2018). However, due to the lack of la-
beled training data or comprehensive hand-crafted
knowledge bases, commonsense reasoning tasks
such as Winograd Schema Challenge (Levesque
et al., 2011) are still far from being solved.

In this work, we propose two effective unsu-
pervised models for commonsense reasoning, and
evaluate them on two classic commonsense rea-
soning tasks: Winograd Schema Challenge (WSC)
and Pronoun Disambiguation Problems (PDP).
Compared to other commonsense reasoning tasks,

∗Work done when the author was at Microsoft

1. The city councilmen refused the demonstrators
a permit because they feared violence.

Who feared violence?
A. The city councilmen B. The demonstra-
tors

2. The city councilmen refused the demonstrators
a permit because they advocated violence.
Who advocated violence?
A. The city councilmen B. The demonstra-
tors

Table 1: Examples from Winograd Schema Challenge
(WSC). The task is to identify the reference of the pro-
noun in bold.

WSC and PDP better approximate real human rea-
soning, and can be more easily solved by native
English-speaking adults (Levesque et al., 2011). In
addition, they are also technically challenging. For
example, the best reported result on WSC is only
20 percentage points better than random guess in
accuracy (Radford et al., 2019).

Table 1 shows two examples from WSC. In or-
der to resolve the co-reference in these two exam-
ples, one cannot predict what “they” refers to un-
less she is equipped with the commonsense knowl-
edge that “demonstrators usually cause violence
and city councilmen usually fear violence”.

As no labeled training data is available for these
tasks, previous approaches are based on either
hand-crafted knowledge bases or large-scale lan-
guage models. For example, Liu et al. (2017) used
existing knowledge bases such as ConceptNet (Liu
and Singh, 2004) and WordNet (Miller, 1995) for
external supervision to train word embeddings and
solve the WSC challenge. Recently, Trinh and Le
(2018) first used raw text from books/news to train
a neural Language Model (LM), and then em-



883

ployed the trained model to compare the proba-
bilities of the sequences, where the pronouns are
replaced by each of the candidate references, and
to pick the candidate that leads to the highest prob-
ability as the answer.

Because none of the existing hand-crafted
knowledge bases is comprehensive enough to
cover all the world knowledge1, we focus on build-
ing unsupervised models that can learn common-
sense knowledge directly from unlimited raw text.
Different from the neural language models, our
models are optimized for co-reference resolution
and achieve much better results on both the PDP
and WSC tasks.

In this work we formulate the two common-
sense reasoning tasks in WSC and PDP as a pair-
wise ranking problem. As the first example in
Table 1, we want to develop a pair-wise scor-
ing model Scoreθ(xi, y) that scores the correct
antecedent-pronoun pair (“councilmen“, “they”)
higher than the incorrect one (“demonstrators“,
“they”). These scores depend to a large degree
upon the contextual information of the pronoun
(e.g., they) and the candidate antecedent (e.g.,
councilmen). In other words, it requires to capture
the semantic meaning of the pronoun and the can-
didate antecedent based on the sentences where
they occur, respectively.

To tackle this issue, we propose two models
based on the framework of Deep Structured Sim-
ilarity Model (DSSM) (Huang et al., 2013), as
shown in Figure 1(a). Formally, let Sx be the sen-
tence containing the candidate antecedent xi and
Sy the sentence containing the pronoun y which
we’re interested in. DSSM measures the semantic
similarity of a pair of inputs (xi, y) by 1) map-
ping xi and y, together with their context infor-
mation, into two vectors in a semantic space us-
ing deep neural networks f1 and f2, parameterized
by θ; and 2) computing cosine similarity2 between
them. In our case, we need to learn a task-specific
semantic space where the distance between two
vectors measures how likely they co-refer. Com-
monsense knowledge such as “demonstrators usu-
ally cause violence” can be implicitly captured
in the semantic space through DSSM, which is

1We don’t believe it is possible to construct such a knowl-
edge base given that the world is changing constantly.

2DSSMs can be applied to a wide range of tasks depend-
ing on the definition of (x, y). For example, (x, y) is a query-
document pair for Web search ranking, a document pair in
recommendation, a question-answer pair in QA, and so on.
See Chapter 2 of (Gao et al., 2018) for a survey.

trained on a large amount of raw text.
DSSM requires labeled pairs for training.Since

there is no labeled data for our tasks, we pro-
pose two unsupervised DSSMs, or UDSSMs. As
shown in Figure 1(b) and 1(c), (Sx,Sy) are en-
coded into contextual representations by deep neu-
ral networks f1 and f2; then we compute pair-wise
their co-reference scores.

In what follows, we will describe two assump-
tions we propose to harvest training data from raw
text. Assumption I: A pronoun refers to one
of its preceding nouns in the same sentence.
The sentences generated by this assumption will
be used for training UDSSM-I. Some examples
will be shown in the “data generation” section.
Assumption II: In a sentence, pronouns of the
same gender and plurality are more likely to re-
fer to the same antecedent than other pronouns.
Similarly, the sentences following the assumption
will be used for training UDSSM-II.

Note that the two models, UDSSM-I and
UDSSM-II are trained on different types of pair-
wise training data, thus the model structures are
different, as illustrated in Figure 1(b) and 1(c),
respectively. Experiments demonstrated that our
methods outperform stat-of-the-art performance
on the tasks of WSC and PDP.

2 Related Work

As a key component of natural language under-
standing, commonsense reasoning has been in-
cluded in an increasing number of tasks for eval-
uation: COPA (Roemmele et al., 2011) assesses
commonsense causal reasoning by selecting an al-
ternative, which has a more plausible causal re-
lation with the given premise. Story Cloze Test
(ROCStories, Mostafazadeh et al. 2016) evaluates
story understanding, story generation, and script
learning by choosing the most sensible ending to
a short story. JOCI (Zhang et al., 2017) general-
izes the natural language inference (NLI) frame-
work (Cooper et al., 1996; Dagan et al., 2006;
Bowman et al., 2015; Williams et al., 2018) and
evaluates commonsense inference by predicting
the ordinal likelihood of a hypothesis given a con-
text. Event2Mind (Rashkin et al., 2018b) mod-
els stereotypical intents and reactions of people,
described in short free-form text. SWAG (Zellers
et al., 2018) frames commonsense inference as
multiple-choice questions for follow-up events
given some context. ReCoRD (Zhang et al., 2018)



884

hx hy

f1 f2

Sx Sy

Similarity measurement 

hx hy

f1 f2

Co-reference Scoring

hx hy

f1 f2

Co-reference Scoring

Contextual
Representation

DNN

Input 
sequences

(a) DSSM (b) UDSSM-I (c) UDSSM-II

Sx Sy Sx Sy

Figure 1: An overview of (a) the general framework of Deep Structured Semantic Model (DSSM) and our two
unsupervised models based on DSSM: (b) UDSSM-I and (c) UDSSM-II. Compared with DSSM, both UDSSM-I
and UDSSM-II compute co-reference scores instead of similarity.

evaluates a machine’s ability of commonsense rea-
soning in reading comprehension.

Among all these commonsense reasoning tasks,
the Winograd Schema Challenge (WSC) and Pro-
noun Disambiguation Problems (PDP) (Levesque
et al., 2011) are known as the most challenging
tasks for commonsense reasoning. Although both
tasks are based on pronoun disambiguation, a sub-
task of coreference resolution (Soon et al., 2001;
Ng and Cardie, 2002; Peng et al., 2016), PDP and
WSC differ from normal pronoun disambiguation
due to their unique properties, which are based
on commonsense, selecting the most likely an-
tecedent from both candidates in the directly pre-
ceding context.

Previous efforts on solving the Winograd
Schema Challenge and Pronoun Disambiguation
Problems mostly rely on human-labeled data, so-
phisticated rules, hand-crafted features, or exter-
nal knowledge bases (Peng et al., 2015; Bailey
et al., 2015; Schüller, 2014). Rahman and Ng
(2012) hired workers to annotate supervised train-
ing data and designed 70K hand-crafted features.
Sharma et al. (2015); Schüller (2014); Bailey et al.
(2015); Liu et al. (2017) utilized expensive knowl-
edge bases in their reasoning processes. Recently,
Trinh and Le 2018 applied neural language models
trained with a massive amount of unlabeled data
to the Winograd Schema Challenge and improved
the performance by a large margin. In contrast,
our unsupervised method based on DSSM sig-
nificantly outperforms the previous state-of-the-
art method, with the advantage of capturing more
contextual information in the data.

3 Approach

As shown in Figure 1, we propose two unsuper-
vised deep structured semantic models (UDSSM-
I and UDSSM-II), which consist of two compo-
nents: DNN encoding and co-reference scoring.
For the model UDSSM-I, the co-referred word
pairs are automatically learned through an atten-
tion mechanism, where the attention weights are
the co-reference scores for word pairs. For the
second model UDSSM-II, we will directly opti-
mize the co-reference score during training. After
all, we will get the co-reference scoring function,
Scoreθ(xi, y), to compare the candidate answers
in the tasks of PDP/WSC. Next, we will show the
details of our models trained in an unsupervised
way.

In the following sections, we will use upper-
case symbols in bold, e.g., Sx, to represent ma-
trices. Lowercase symbols in bold, e.g., hx, repre-
sent vectors. A regular uppercase symbol, e.g., Sx,
represents a lexical sequence. A regular lowercase
symbol, e.g., xi or y, represents a token.

3.1 UDSSM-I Model
This model is developed based on Assumption I.
Its architecture is shown in Figure 2. The sen-
tences generated based on this assumption con-
tain a pronoun y and a set of its preceding nouns
{xi, xj ...}, which includes the referred word by
pronoun. For example, the sentence in Figure 2.
As there is no clear label for the co-referred word
pairs under this assumption, our model will rank
the set of nouns {xi, xj ...} which contains the
noun that the pronoun y refers to higher than the
set which does not. And the co-reference score be-
tween words will not be optimized directly during



885

(a) Positive example (b) Negative example

pilots radio controllers

hk2
x

h i
x h j

x hk
x

overflew their Minneapolis

h 2
y

DNN

Input

Contextual
Representation

Scoring

make it negative

↵j
<latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit> ↵j

<latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit>

pilots radio controllers

hk2
x

h i
x h j

x hk
x

h 2
y

↵j
<latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit> ↵j

<latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit><latexit sha1_base64="fFTRfbxrRiYYvC4t/y6zSBs3auk=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMFYwttKJPtpl272cTdjVBC/4QXDype/T3e/Ddu2xy09cHA470ZZuaFqeDauO63s7S8srq2Xtoob25t7+xW9vbvdZIpynyaiES1QtRMcMl8w41grVQxjEPBmuHweuI3n5jSPJF3ZpSyIMa+5BGnaKzU6qBIB9h96Faqbs2dgiwSryBVKNDoVr46vYRmMZOGCtS67bmpCXJUhlPBxuVOplmKdIh91rZUYsx0kE/vHZNjq/RIlChb0pCp+nsix1jrURzazhjNQM97E/E/r52Z6CLIuUwzwySdLYoyQUxCJs+THleMGjGyBKni9lZCB6iQGhtR2Ybgzb+8SPzT2mXNvT2r1q+KNEpwCEdwAh6cQx1uoAE+UBDwDK/w5jw6L8678zFrXXKKmQP4A+fzB3aPj8g=</latexit>

ĥx
<latexit sha1_base64="e6qXSQNvtYYIQU2L7jV9cxk6EgE=">AAACAXicbVBNS8NAEN34WetX1JN4CRbBU0lEUG9FLx4rGFtoYtlsN83SzSbsTqQlBC/+FS8eVLz6L7z5b9y0PWjrg4HHezPMzAtSzhTY9rexsLi0vLJaWauub2xubZs7u3cqySShLkl4ItsBVpQzQV1gwGk7lRTHAaetYHBV+q0HKhVLxC2MUurHuC9YyAgGLXXNfS/CkHsxhigI86go7j2gQ8iHRdes2XV7DGueOFNSQ1M0u+aX10tIFlMBhGOlOo6dgp9jCYxwWlS9TNEUkwHu046mAsdU+fn4hcI60krPChOpS4A1Vn9P5DhWahQHurO8Vc16pfif18kgPPdzJtIMqCCTRWHGLUisMg+rxyQlwEeaYCKZvtUiEZaYgE6tqkNwZl+eJ+5J/aJu35zWGpfTNCroAB2iY+SgM9RA16iJXETQI3pGr+jNeDJejHfjY9K6YExn9tAfGJ8/MeiYJg==</latexit><latexit sha1_base64="e6qXSQNvtYYIQU2L7jV9cxk6EgE=">AAACAXicbVBNS8NAEN34WetX1JN4CRbBU0lEUG9FLx4rGFtoYtlsN83SzSbsTqQlBC/+FS8eVLz6L7z5b9y0PWjrg4HHezPMzAtSzhTY9rexsLi0vLJaWauub2xubZs7u3cqySShLkl4ItsBVpQzQV1gwGk7lRTHAaetYHBV+q0HKhVLxC2MUurHuC9YyAgGLXXNfS/CkHsxhigI86go7j2gQ8iHRdes2XV7DGueOFNSQ1M0u+aX10tIFlMBhGOlOo6dgp9jCYxwWlS9TNEUkwHu046mAsdU+fn4hcI60krPChOpS4A1Vn9P5DhWahQHurO8Vc16pfif18kgPPdzJtIMqCCTRWHGLUisMg+rxyQlwEeaYCKZvtUiEZaYgE6tqkNwZl+eJ+5J/aJu35zWGpfTNCroAB2iY+SgM9RA16iJXETQI3pGr+jNeDJejHfjY9K6YExn9tAfGJ8/MeiYJg==</latexit><latexit sha1_base64="e6qXSQNvtYYIQU2L7jV9cxk6EgE=">AAACAXicbVBNS8NAEN34WetX1JN4CRbBU0lEUG9FLx4rGFtoYtlsN83SzSbsTqQlBC/+FS8eVLz6L7z5b9y0PWjrg4HHezPMzAtSzhTY9rexsLi0vLJaWauub2xubZs7u3cqySShLkl4ItsBVpQzQV1gwGk7lRTHAaetYHBV+q0HKhVLxC2MUurHuC9YyAgGLXXNfS/CkHsxhigI86go7j2gQ8iHRdes2XV7DGueOFNSQ1M0u+aX10tIFlMBhGOlOo6dgp9jCYxwWlS9TNEUkwHu046mAsdU+fn4hcI60krPChOpS4A1Vn9P5DhWahQHurO8Vc16pfif18kgPPdzJtIMqCCTRWHGLUisMg+rxyQlwEeaYCKZvtUiEZaYgE6tqkNwZl+eJ+5J/aJu35zWGpfTNCroAB2iY+SgM9RA16iJXETQI3pGr+jNeDJejHfjY9K6YExn9tAfGJ8/MeiYJg==</latexit> ĥx

<latexit sha1_base64="e6qXSQNvtYYIQU2L7jV9cxk6EgE=">AAACAXicbVBNS8NAEN34WetX1JN4CRbBU0lEUG9FLx4rGFtoYtlsN83SzSbsTqQlBC/+FS8eVLz6L7z5b9y0PWjrg4HHezPMzAtSzhTY9rexsLi0vLJaWauub2xubZs7u3cqySShLkl4ItsBVpQzQV1gwGk7lRTHAaetYHBV+q0HKhVLxC2MUurHuC9YyAgGLXXNfS/CkHsxhigI86go7j2gQ8iHRdes2XV7DGueOFNSQ1M0u+aX10tIFlMBhGOlOo6dgp9jCYxwWlS9TNEUkwHu046mAsdU+fn4hcI60krPChOpS4A1Vn9P5DhWahQHurO8Vc16pfif18kgPPdzJtIMqCCTRWHGLUisMg+rxyQlwEeaYCKZvtUiEZaYgE6tqkNwZl+eJ+5J/aJu35zWGpfTNCroAB2iY+SgM9RA16iJXETQI3pGr+jNeDJejHfjY9K6YExn9tAfGJ8/MeiYJg==</latexit><latexit sha1_base64="e6qXSQNvtYYIQU2L7jV9cxk6EgE=">AAACAXicbVBNS8NAEN34WetX1JN4CRbBU0lEUG9FLx4rGFtoYtlsN83SzSbsTqQlBC/+FS8eVLz6L7z5b9y0PWjrg4HHezPMzAtSzhTY9rexsLi0vLJaWauub2xubZs7u3cqySShLkl4ItsBVpQzQV1gwGk7lRTHAaetYHBV+q0HKhVLxC2MUurHuC9YyAgGLXXNfS/CkHsxhigI86go7j2gQ8iHRdes2XV7DGueOFNSQ1M0u+aX10tIFlMBhGOlOo6dgp9jCYxwWlS9TNEUkwHu046mAsdU+fn4hcI60krPChOpS4A1Vn9P5DhWahQHurO8Vc16pfif18kgPPdzJtIMqCCTRWHGLUisMg+rxyQlwEeaYCKZvtUiEZaYgE6tqkNwZl+eJ+5J/aJu35zWGpfTNCroAB2iY+SgM9RA16iJXETQI3pGr+jNeDJejHfjY9K6YExn9tAfGJ8/MeiYJg==</latexit><latexit sha1_base64="e6qXSQNvtYYIQU2L7jV9cxk6EgE=">AAACAXicbVBNS8NAEN34WetX1JN4CRbBU0lEUG9FLx4rGFtoYtlsN83SzSbsTqQlBC/+FS8eVLz6L7z5b9y0PWjrg4HHezPMzAtSzhTY9rexsLi0vLJaWauub2xubZs7u3cqySShLkl4ItsBVpQzQV1gwGk7lRTHAaetYHBV+q0HKhVLxC2MUurHuC9YyAgGLXXNfS/CkHsxhigI86go7j2gQ8iHRdes2XV7DGueOFNSQ1M0u+aX10tIFlMBhGOlOo6dgp9jCYxwWlS9TNEUkwHu046mAsdU+fn4hcI60krPChOpS4A1Vn9P5DhWahQHurO8Vc16pfif18kgPPdzJtIMqCCTRWHGLUisMg+rxyQlwEeaYCKZvtUiEZaYgE6tqkNwZl+eJ+5J/aJu35zWGpfTNCroAB2iY+SgM9RA16iJXETQI3pGr+jNeDJejHfjY9K6YExn9tAfGJ8/MeiYJg==</latexit>

hy
<latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit> hy

<latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit>

↵k
<latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit><latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit><latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit><latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit>

↵k
<latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit><latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit><latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit><latexit sha1_base64="zOMfb+tFRXh0xgOR06mYTY3XPgw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKpi20oUy2m3bpZhN3N0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZemAqujet+O6W19Y3NrfJ2ZWd3b/+genjU0kmmKPNpIhLVCVEzwSXzDTeCdVLFMA4Fa4fj25nffmJK80Q+mEnKghiHkkecorFSp4ciHWF/3K/W3Lo7B1klXkFqUKDZr371BgnNYiYNFah113NTE+SoDKeCTSu9TLMU6RiHrGupxJjpIJ/fOyVnVhmQKFG2pCFz9fdEjrHWkzi0nTGakV72ZuJ/Xjcz0XWQc5lmhkm6WBRlgpiEzJ4nA64YNWJiCVLF7a2EjlAhNTaiig3BW355lbQu6p5b9+4va42bIo4ynMApnIMHV9CAO2iCDxQEPMMrvDmPzovz7nwsWktOMXMMf+B8/gAMyI/2</latexit>

Figure 2: The procedure of using UDSSM-I to compute the co-reference scores of a positive example and a negative
example respectively. The positive example is generated from the sentence ‘Two Northwest Airlines pilots failed
to make radio contact with ground controllers for more than an hour and overflew their Minneapolis destination
by 150 miles before discovering the mistake and turning around.”. The negative one replaces the second sequence
with one sequence from different sentence.

(a) Positive example (b) Negative example

hk-1 hk hk+1 hj-1 hj hj+1

hk-1 hk hk+1 hj-1 hj hj+1

<bos> He tried but she did

hk-1 ; hk+1 ] hj-1 ; hj+1 ]

DNN

Input

Contextual
Representation

Scoring

hi-1 hi hj-1 hj hj+1

hi-1 hi hi+1 hj hj+1

call her but she did

hi-1: [ ; hi+1 ] hj-1 ; hj+1 ]: [ : [ : [h
x

<latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="C39OhB+IczRcjLNINXH29e9lt8M=">AAAB2HicbZDNSgMxFIXv1L86Vq1rN8EiuCpTN+pOcOOygmML7VAymTttaCYzJHeEMvQFXLhRfDB3vo3pz0KtBwIf5yTk3hMXSloKgi+vtrW9s7tX3/cPGv7h0XGz8WTz0ggMRa5y04+5RSU1hiRJYb8wyLNYYS+e3i3y3jMaK3P9SLMCo4yPtUyl4OSs7qjZCtrBUmwTOmtowVqj5ucwyUWZoSahuLWDTlBQVHFDUiic+8PSYsHFlI9x4FDzDG1ULcecs3PnJCzNjTua2NL9+aLimbWzLHY3M04T+zdbmP9lg5LS66iSuigJtVh9lJaKUc4WO7NEGhSkZg64MNLNysSEGy7INeO7Djp/N96E8LJ90w4eAqjDKZzBBXTgCm7hHroQgoAEXuDNm3iv3vuqqpq37uwEfsn7+Aap5IoM</latexit><latexit sha1_base64="gzCdqps4E2GCn6bK5ZZ1JDrmXec=">AAAB8HicbVBNS8NAFHypX7VWjXr0EiyCp5J4UW+CF48VjC00sWy2m3bp5oPdF2kI8a948aDir/Hmv3HT9qCtAwvDzHu82QlSwRXa9rdRW1vf2Nyqbzd2mrt7++ZB80ElmaTMpYlIZC8gigkeMxc5CtZLJSNRIFg3mNxUfveJScWT+B7zlPkRGcU85JSglgbmkRcRHAdhMS4fPWRTLKblwGzZbXsGa5U4C9KCBToD88sbJjSLWIxUEKX6jp2iXxCJnApWNrxMsZTQCRmxvqYxiZjyi1n40jrVytAKE6lfjNZM/b1RkEipPAr0ZBVVLXuV+J/XzzC89AsepxmymM4PhZmwMLGqJqwhl4yiyDUhVHKd1aJjIglF3VdDl+Asf3mVuOftq7Z9Z0MdjuEEzsCBC7iGW+iACxRyeIE3eDeejVfjY95WzVjUdgh/YHz+AM5Tk/M=</latexit><latexit sha1_base64="gzCdqps4E2GCn6bK5ZZ1JDrmXec=">AAAB8HicbVBNS8NAFHypX7VWjXr0EiyCp5J4UW+CF48VjC00sWy2m3bp5oPdF2kI8a948aDir/Hmv3HT9qCtAwvDzHu82QlSwRXa9rdRW1vf2Nyqbzd2mrt7++ZB80ElmaTMpYlIZC8gigkeMxc5CtZLJSNRIFg3mNxUfveJScWT+B7zlPkRGcU85JSglgbmkRcRHAdhMS4fPWRTLKblwGzZbXsGa5U4C9KCBToD88sbJjSLWIxUEKX6jp2iXxCJnApWNrxMsZTQCRmxvqYxiZjyi1n40jrVytAKE6lfjNZM/b1RkEipPAr0ZBVVLXuV+J/XzzC89AsepxmymM4PhZmwMLGqJqwhl4yiyDUhVHKd1aJjIglF3VdDl+Asf3mVuOftq7Z9Z0MdjuEEzsCBC7iGW+iACxRyeIE3eDeejVfjY95WzVjUdgh/YHz+AM5Tk/M=</latexit><latexit sha1_base64="m62G6GSypt9BNZT8mzd7Taq34F0=">AAAB+3icbVC7TsMwFHV4lvIKdGSxqJCYqoQF2CpYGItEaKUmVI7rtFadh+wb1CgKv8LCAIiVH2Hjb3DaDNByJEtH59yre3z8RHAFlvVtrKyurW9s1rbq2zu7e/vmweG9ilNJmUNjEcueTxQTPGIOcBCsl0hGQl+wrj+5Lv3uI5OKx9EdZAnzQjKKeMApAS0NzIYbEhj7QT4uHlxgU8inxcBsWi1rBrxM7Io0UYXOwPxyhzFNQxYBFUSpvm0l4OVEAqeCFXU3VSwhdEJGrK9pREKmvHwWvsAnWhniIJb6RYBn6u+NnIRKZaGvJ8uoatErxf+8fgrBhZfzKEmBRXR+KEgFhhiXTeAhl4yCyDQhVHKdFdMxkYSC7quuS7AXv7xMnLPWZcu6tZrtq6qNGjpCx+gU2egctdEN6iAHUZShZ/SK3own48V4Nz7moytGtdNAf2B8/gA6J5VV</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit>hx
<latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="C39OhB+IczRcjLNINXH29e9lt8M=">AAAB2HicbZDNSgMxFIXv1L86Vq1rN8EiuCpTN+pOcOOygmML7VAymTttaCYzJHeEMvQFXLhRfDB3vo3pz0KtBwIf5yTk3hMXSloKgi+vtrW9s7tX3/cPGv7h0XGz8WTz0ggMRa5y04+5RSU1hiRJYb8wyLNYYS+e3i3y3jMaK3P9SLMCo4yPtUyl4OSs7qjZCtrBUmwTOmtowVqj5ucwyUWZoSahuLWDTlBQVHFDUiic+8PSYsHFlI9x4FDzDG1ULcecs3PnJCzNjTua2NL9+aLimbWzLHY3M04T+zdbmP9lg5LS66iSuigJtVh9lJaKUc4WO7NEGhSkZg64MNLNysSEGy7INeO7Djp/N96E8LJ90w4eAqjDKZzBBXTgCm7hHroQgoAEXuDNm3iv3vuqqpq37uwEfsn7+Aap5IoM</latexit><latexit sha1_base64="gzCdqps4E2GCn6bK5ZZ1JDrmXec=">AAAB8HicbVBNS8NAFHypX7VWjXr0EiyCp5J4UW+CF48VjC00sWy2m3bp5oPdF2kI8a948aDir/Hmv3HT9qCtAwvDzHu82QlSwRXa9rdRW1vf2Nyqbzd2mrt7++ZB80ElmaTMpYlIZC8gigkeMxc5CtZLJSNRIFg3mNxUfveJScWT+B7zlPkRGcU85JSglgbmkRcRHAdhMS4fPWRTLKblwGzZbXsGa5U4C9KCBToD88sbJjSLWIxUEKX6jp2iXxCJnApWNrxMsZTQCRmxvqYxiZjyi1n40jrVytAKE6lfjNZM/b1RkEipPAr0ZBVVLXuV+J/XzzC89AsepxmymM4PhZmwMLGqJqwhl4yiyDUhVHKd1aJjIglF3VdDl+Asf3mVuOftq7Z9Z0MdjuEEzsCBC7iGW+iACxRyeIE3eDeejVfjY95WzVjUdgh/YHz+AM5Tk/M=</latexit><latexit sha1_base64="gzCdqps4E2GCn6bK5ZZ1JDrmXec=">AAAB8HicbVBNS8NAFHypX7VWjXr0EiyCp5J4UW+CF48VjC00sWy2m3bp5oPdF2kI8a948aDir/Hmv3HT9qCtAwvDzHu82QlSwRXa9rdRW1vf2Nyqbzd2mrt7++ZB80ElmaTMpYlIZC8gigkeMxc5CtZLJSNRIFg3mNxUfveJScWT+B7zlPkRGcU85JSglgbmkRcRHAdhMS4fPWRTLKblwGzZbXsGa5U4C9KCBToD88sbJjSLWIxUEKX6jp2iXxCJnApWNrxMsZTQCRmxvqYxiZjyi1n40jrVytAKE6lfjNZM/b1RkEipPAr0ZBVVLXuV+J/XzzC89AsepxmymM4PhZmwMLGqJqwhl4yiyDUhVHKd1aJjIglF3VdDl+Asf3mVuOftq7Z9Z0MdjuEEzsCBC7iGW+iACxRyeIE3eDeejVfjY95WzVjUdgh/YHz+AM5Tk/M=</latexit><latexit sha1_base64="m62G6GSypt9BNZT8mzd7Taq34F0=">AAAB+3icbVC7TsMwFHV4lvIKdGSxqJCYqoQF2CpYGItEaKUmVI7rtFadh+wb1CgKv8LCAIiVH2Hjb3DaDNByJEtH59yre3z8RHAFlvVtrKyurW9s1rbq2zu7e/vmweG9ilNJmUNjEcueTxQTPGIOcBCsl0hGQl+wrj+5Lv3uI5OKx9EdZAnzQjKKeMApAS0NzIYbEhj7QT4uHlxgU8inxcBsWi1rBrxM7Io0UYXOwPxyhzFNQxYBFUSpvm0l4OVEAqeCFXU3VSwhdEJGrK9pREKmvHwWvsAnWhniIJb6RYBn6u+NnIRKZaGvJ8uoatErxf+8fgrBhZfzKEmBRXR+KEgFhhiXTeAhl4yCyDQhVHKdFdMxkYSC7quuS7AXv7xMnLPWZcu6tZrtq6qNGjpCx+gU2egctdEN6iAHUZShZ/SK3own48V4Nz7moytGtdNAf2B8/gA6J5VV</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit><latexit sha1_base64="YntzHWVY8zrIDQ/SieJ4Xw31Xik=">AAAB+3icbVBNS8NAFNzUr1q/oj16CRbBU0lEUG9FLx4rGFtoY9lsN+3SzSbsvkhDiH/FiwcVr/4Rb/4bN20O2jqwMMy8x5sdP+ZMgW1/G5WV1bX1jepmbWt7Z3fP3D+4V1EiCXVJxCPZ9bGinAnqAgNOu7GkOPQ57fiT68LvPFKpWCTuII2pF+KRYAEjGLQ0MOv9EMPYD7Jx/tAHOoVsmg/Mht20Z7CWiVOSBirRHphf/WFEkpAKIBwr1XPsGLwMS2CE07zWTxSNMZngEe1pKnBIlZfNwufWsVaGVhBJ/QRYM/X3RoZDpdLQ15NFVLXoFeJ/Xi+B4MLLmIgToILMDwUJtyCyiiasIZOUAE81wUQyndUiYywxAd1XTZfgLH55mbinzcumfXvWaF2VbVTRITpCJ8hB56iFblAbuYigFD2jV/RmPBkvxrvxMR+tGOVOHf2B8fkDO2eVWQ==</latexit>

hy
<latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit>hy

<latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit><latexit sha1_base64="G5n/1Fghu52Ax5DmEDM5lT1gJPI=">AAAB+3icbVBNS8NAFNz4WetXtEcvwSJ4KokI6q3oxWMFYwtNLJvtpl262YTdFzGE+Fe8eFDx6h/x5r9x0+agrQMLw8x7vNkJEs4U2Pa3sbS8srq2Xtuob25t7+yae/t3Kk4loS6JeSx7AVaUM0FdYMBpL5EURwGn3WByVfrdByoVi8UtZAn1IzwSLGQEg5YGZsOLMIyDMB8X9x7QR8izYmA27ZY9hbVInIo0UYXOwPzyhjFJIyqAcKxU37ET8HMsgRFOi7qXKppgMsEj2tdU4IgqP5+GL6wjrQytMJb6CbCm6u+NHEdKZVGgJ8uoat4rxf+8fgrhuZ8zkaRABZkdClNuQWyVTVhDJikBnmmCiWQ6q0XGWGICuq+6LsGZ//IicU9aFy375rTZvqzaqKEDdIiOkYPOUBtdow5yEUEZekav6M14Ml6Md+NjNrpkVDsN9AfG5w8865Va</latexit>

Figure 3: The procedure of using UDSSM-II to compute the co-reference scores of a positive example and a
negative example respectively. Both examples are generated from the sentence “He tried twice to call her but she
did not answer the phone”.

training, but is learned indirectly through the atten-
tion mechanism. We will describe in turn how the
training data is generated from raw text, the model
architecture, and the co-reference scoring function
for the final prediction on the tasks of PDP/WSC.

3.1.1 Data Generation
The main challenge of PDP/WSC tasks is that it
has no labeled training data. Here we introduce
a simple method to collect unsupervised train-
ing data by leveraging some linguistic patterns.
Following Assumption 1, the first hypothesis we
make is that “the pronoun refers to one of the pre-
ceding nouns”, which is a common phenomenon
in well-written stories or news. In this way, we
generate (Sx, Sy) pairs from raw text as follows:

• Parse the sentences in the raw text to obtain en-
tity names, nouns and pronouns.

• Pick sentences that contain at least one pronoun
and multiple nouns preceding it.
• Split each sentence into two sub-sentences to

form a positive pair (Sx, Sy), where Sx is the
first sub-sentence with identified nouns and en-
tity names while Sy is the second sub-sentence
with a pronoun.
• One or more negative pairs are generated from

(Sx, Sy) by replacing Sy with Syneg randomly
sampled from other positive pairs.

We split the sentence with pronouns and nouns
into two sub-sequences separated by the previous
word of the pronoun. Therefore, the example sen-
tence in the Figure 2 can be split into two sub-
sentences as shown below:

• Sx: “ Two Northwest Airlines pilots failed to
make radio contact with ground controllers



886

for more than an hour and”
• Sy: “overflew their Minneapolis destination by

150 miles before discovering the mistake and
turning around”.

As the sentences are collected from raw text, the
co-reference words are not given. Our proposed
UDSSM-I model will learn the co-reference scor-
ing function through attention mechanism based
on the generated sequence pairs. Next, we will in-
troduce the details of this model.

3.1.2 Model Architecture
This method takes the pair of sequences, (Sx, Sy),
as inputs, and computes similarity between the
sequences collected from the same sentence. As
we hypothesize that one of the nouns in the first
sequence and the pronoun in the second are co-
referred, we only use the contextual representa-
tions of nouns and pronoun to represent the se-
quences. To obtain the contextual representation,
we first use a bi-directional LSTM to process these
sequences 3:

Hx = Bi-LSTM(Sx),Hy = Bi-LSTM(Sy), (1)

where Sx ∈ Rd×X , Sy ∈ Rd×Y are the word em-
beddings of the two sequences. d is the dimension
of the word embeddings. X,Y are the lengths of
the two sequences. Hx ∈ Rl×X and Hy ∈ Rl×Y
are the hidden states of bi-directional LSTM. Our
model is task-specifically constructed, so we di-
rectly use the hidden state of the first pronoun in
the second sequence as its representation:

f2(S
y) = hy = h

y
2, (2)

where hy2 ∈ Rl is the second4 vector from Hy and
it represents the contextual information of the pro-
noun. Next, we will get the representation of the
first sequence. As there are multiple nouns in the
first sequence and the pronoun usually refers to
only one of them, we use the weighted sum of all
the LSTM hidden states of the nouns to represent
the sequence, ĥx ∈ Rl, as follows:

Hn = [hxi ;h
x
j ; ...]

α = SoftMax
(
(WgHn + bg ⊗ eN )Thy

)
,

f1(S
x) = ĥx = Hnα, (3)

3We use two different LSTMs to process the sequences
Sx and SY here. This is to make the negative sampling in
Eqn. (4) more efficient, so that we can directly use the other
representations in the same batch as negative ones.

4We assign the word just before the pronoun to the sec-
ond sequence, so the pronoun always appears in the second
position of the sequence.

where i, j... are the positions of the nouns in the
sequence Sx and [; ] is the concatenation of two
vectors. Hn ∈ Rl×N are all the hidden states of
the nouns5 in Hx in the sequence.N is the number
of nouns in the sequence. α ∈ RN is the weights
assigned for the different nouns and ĥx ∈ Rl is
the weighted sum of all the hidden states of the
nouns. Wg ∈ Rl×l and bg ∈ Rl are the parame-
ters to learn; eN ∈ RN is a vector of all 1s and it
is used to repeat the bias vector N times into the
matrix. Then we will maximize the similarity of
the contextual representations of (ĥx,hy). Mean-
while, we also need some negative samples h

yneg
k

for ĥx. Then our loss function for this method is
constructed:

L = − log

 exp
(
ĥxhy

)
exp

(
ĥxhy

)
+
∑K

k exp
(
ĥxh

yneg
k

)
 ,

(4)
where h

yneg
k ∈ R

l is the randomly sampled hid-
den state of pronoun from the sequences not in the
same sentence with Sy.

3.1.3 Co-reference Scoring Function
Overall, the model tries to make the co-reference
states similar to each other. The co-reference scor-
ing function is defined:

Scoreθ(xi, y) = g(hxi ,h
y) = (Wghxi + b

g)Thy,
(5)

where the candidate located at the i-th position is
represented by its LSTM hidden state hxi and the
pronoun in the snippet is represented by hy. And
the output value of this function for each candidate
will be used for the final prediction. Next, we will
introduce the other unsupervised method.

3.2 UDSSM-II Model
This model is developed based on Assumption II.
Its architecture is shown in Figure 3. As the model
is similar to the previous one, we will introduce
the details in a similar way.

3.2.1 Data Generation
The second assumption is that “the pronoun pairs
in a single sentence are co-reference words if they
are of the same gender and plurality; otherwise
they are not.” Based on this assumption, we can
directly construct the co-reference training pairs as
follows:

5We use the toolkit of spaCy in Python for POS and NER,
and we will remove the sequences that contain less than 2
nouns.



887

• Parse the raw sentences to identify pronouns.
• Pick sentences that contain at least two pro-

nouns.

• The sub-sequence pair with pronouns of the
same gender and plurality is labeled as a posi-
tive pair; otherwise it is labeled as negative.

• Replace the corresponding pronoun pairs with a
special token “@Ponoun”.

Take the following sentence as an example: “He
tried twice to call her but she did not answer the
phone.” There are three pronouns detected in the
sentence, and we assume that the words her and
she are co-reference words, while pairs (she,He)
and (her,He) are not. Thus we can obtain three
training examples from the given sentence. How-
ever, in the PDP and WSC tasks, models are asked
to compute the co-reference scores between pro-
noun and candidate nouns, instead of two pro-
nouns. Therefore, we replace the first pronoun in
the sentence with a place holder; i.e., a negative
training pair is generated by splitting the raw sen-
tence into the following two sub-sequences:

• Sx: “ @Ponoun tried twice to call her”
• Sy: “but she did not answer the phone.”
• label: Negative

and the positive training pair can be generated by
the same way:

• Sx: “ He tried twice to call @Ponoun”
• Sy: “but she did not answer the phone.”
• label: Positive

Thus, we could directly train the encoder and co-
reference scoring components through the gener-
ated training pairs.

3.2.2 Model Architecture
The previous method, UDSSM-I, follows the task
setting of PDP/WSC, and builds the model based
on the similarity of the representations between
nouns and the pronoun. As there is no signal in-
dicating the exact alignment between co-reference
words, the model tries to learn it based on the
co-occurrence information from large scale unla-
belled corpus. For the method of UDSSM-II, each
representation pair (hx,hy) has a clear signal, r,
indicating whether they are co-referred or not. For
simplicity, we do not have to split the sentence into

two parts. We first use LSTM to process the sen-
tence as follows:

−→
H =

−−−−→
LSTM([Sx;Sy]),

←−
H =

←−−−−
LSTM([Sx;Sy]), (6)

where we can concatenate the word embeddings,
[Sx;Sy], of two sequences collected under As-
sumption II.

−−−−→
LSTM and

←−−−−
LSTM are built in differ-

ent directions, and
−→
H,
←−
H are the hidden states of

the corresponding LSTM. Suppose that the pro-
noun pair in the sentence are located at the i-th
and j-th positions as shown in the bottom part
of Figure 3(a). We use the hidden states around
the pronouns as their contextual representations as
follows:

f1(S
x) = hx =

[−−→
hi−1←−−
hi+1

]
, f2(S

y) = hy =

[−−→
hj−1←−−
hj+1

]
,

(7)

where
[
·
·

]
is the concatenation of all the vectors

inside it. Then we further concatenate these repre-
sentation pair:

hc =

[
hx

hy

]
, (8)

where hc ∈ R4l, and it will be the input of loss
function with cross entropy as follows:

L = −r log
(

exp(wphc)

exp(wphc) + exp(wnhc)

)
− (1− r) log

(
exp(wnhc)

exp(wphc) + exp(wnhc)

)
,

where r ∈ {0, 1} indicates whether the pronouns
at the m-th and n-th positions should be consid-
ered co-reference or not. wp ∈ R4l and wn ∈ R4l
are the parameters to learn.

3.2.3 Co-reference Scoring Function
Similar to the Eqn.(5), for each candidate, we use
co-reference scoring function Scoreθ(xi, y) for the
answer selection:

Scoreθ(xi, y) = g(hxi ,h
y) = wp


−−→
hi−1←−−
hi+1−−→
hj−1←−−
hj+1

 , (9)
where i is the position of the candidate in the sen-
tence and j is the position of the pronoun.



888

PDP WSC
Co-reference Resolution Tool 41.7% 50.5
Patric Dhondt (WS Challenge 2016) 45.0% -
Nicos Issak (WS Challenge 2016) 48.3% -
Quan Liu (WS Challenge 2016 - winner) 58.3% -
Unsupervised Semantic Similarity Method (USSM) 48.3% -
Neural Knowledge Activated Method (NKAM) 51.7% -
USSM + Cause-Effect Knowledge Base 55.0% 52.0%
USSM + Cause-Effect + WordNet + ConceptNet Knowledge Bases 56.7% 52.8%
USSM + NKAM 53.3%
USSM + NKAM + 3 Knowledge Bases 66.7% 52.8%

ELMo 56.7% 51.5%
Google Language Model (Trinh and Le, 2018) 60.0% 56.4%
UDSSM-I 75.0% 54.5%
UDSSM-II 75.0% 59.2%

Google Language Model (ensemble) 70.0% 61.5%
UDSSM-I (ensemble) 76.7% 57.1%
UDSSM-II (ensemble) 78.3% 62.4%

Table 2: The experiment results on PDP and WSC datasets. We compare our models to Goolge LM trained on the
same corpus 6.

4 Experiments

In this section, we will introduce the datasets to
train and evaluate our models for commonsense
reasoning, the hyper-parameters of our model, and
the analysis of our results.

4.1 Datasets

Training Corpus We make use of the raw text
from Gutenberg 7, a corpus offerring over 57,000
free eBooks, and 1 Billion Word 8, a corpus of
news, to train our model. We first ignore the sen-
tences that contain less than 10 tokens or longer
than 50 tokens. Then, for the model UDSSM-I, we
collect all the sentences with the pronoun before
which there’re at least two nouns. For UDSSM-
II, we collect all the sentences with at least 2
pronouns. In total, we collect around 4 million
training pairs from each corpus for our proposed
method respectively, and we split 5% as validation
set.

Evaluation Dataset We evaluate our model on
the commonsense reasoning datasets, Pronoun

6The best models reported in the works of Radford et al.
(2019) and Trinh and Le (2018) are trained on a much larger
corpus from Common Crawl.

7http://www.gutenberg.org
8https://github.com/ciprian-chelba/

1-billion-word-language-modeling-benchmark

Disambiguation Problems (PDP) 9 and Winograd
Schema challenges (WSC) 10, which include 60
and 285 questions respectively. Both of the tasks
are constructed for testing commonsense reason-
ing and all the questions from these challenges are
obvious for human beings to solve with common-
sense knowledge, but hard for machines to solve
with statistical techniques.

4.2 Experimental Setting

We use the same setting for both our models.
The hidden state dimension of a single-directional
LSTM is set to be 300. We use 300 dimensional
GloVe embeddings 11 for initialization. We use
Adamax to optimise the model, set learning rate to
be 0.002, dropout rate on all layers are tuned from
[0, 0.1, 0.2] and the batch size from [30, 50, 100,
200]. For the model UDSSM-I, in one batch, we
treat all sequence pairs not from the same sentence
as negative cases. And it takes around 30 hours on
a single K40 GPU to train our models, which are
much faster than training a large LM (Jozefowicz
et al., 2016) taking weeks on multiple GPUs.

9https://cs.nyu.edu/faculty/davise/
papers/WinogradSchemas/PDPChallenge2016.
xml

10https://cs.nyu.edu/faculty/davise/
papers/WinogradSchemas/WSCollection.xml

11https://github.com/stanfordnlp/GloVe



889

4.3 Experimental Results

The experiment results are shown in Table 2. Most
of the performance in the top of the Table 2 are
the models trained with external knowledge bases,
such as Cause-Effect (Liu et al., 2016), Word-
Net (Miller, 1995), ConceptNet (Liu and Singh,
2004) knowledge bases. Unsupervised Semantic
Similarity Method (USSM) (Liu et al., 2017) is
based on the skip-gram model (Mikolov et al.,
2013) to train word embeddings and the embed-
dings of all the words connected by knowledge
bases are optimized to be closer. Neural Knowl-
edge Activated Method (NKAM) (Liu et al., 2017)
trained a binary classification model based on
whether the word pairs appear in the knowledge
base. One limitation of these methods is that they
rely heavily on the external knowledge bases. An-
other limitation is that they just linearly aggre-
gate the embeddings of the words in the context,
and that’s hard to integrate the word order infor-
mation. Instead, our model with LSTM can better
represent the contextual information. Besides, our
model don’t need any external knowledge bases,
and achieve a significant improvement on both of
the datasets.

We further compare our models with the un-
supervised baselines, ELMo (Peters et al., 2018)
which selects the candidate based on the co-
sine similarity of the hidden states of noun
and pronoun. Another unsupervised baseline,
Google Language Model for commonsense rea-
soning (Trinh and Le, 2018), which compares the
perplexities of the new sentences by replacing the
pronoun with candidates. To make a fair com-
parison to Trinh and Le (2018)’s work, we also
train our single model on the corpus of Guten-
berg only. We can see that both of our methods get
significant improvement on the PDP dataset, and
our UDSSM-II can achieve much better perfor-
mance on the WSC dataset. We also report our en-
semble model (nine models with different hyper-
parameters) trained with both corpus of Gutenberg
and 1 Billion Word, and it also achieve better per-
formance than Google Language Model trained
with the same corpus.

Finally, we also compare to the pre-trained
Coreference Resolution Tool (Clark and Manning,
2016a,b)12, and we can see that it doesn’t adapt
to our commonsense reasoning tasks and can’t tell

12https://github.com/huggingface/
neuralcoref

the difference between each pair of sentences from
WSC. In this way, our model can get much better
performance.

4.4 Analysis

WSC 1: Paul tried to call George on the
phone, but he wasn’t successful.

Ours 1: He tried to call 911 using her cell
phone but that he could n’t get the
phone to work.

WSC 2: Paul tried to call George on the
phone, but he was n’t available .

Ours 2: He tried twice to call her but she did
not answer the phone .

Table 3: Comparison of the data from WSC and our
training data. Our sentences are retrieved from the
UDSSM-II training dataset based on the BM25 value
for analysis. The pseudo labels in our training data can
help identify the co-references in WSC.

In this subsection, we will conduct further anal-
ysis on the reason that our models work, the bene-
fit of our models comparing to a baseline, and the
limitation of our proposed models.

We have a further analysis on the pair-wise sen-
tences, which we collected for training, to check
how our model can work. We find that some rea-
soning problems can somehow be converted to the
paraphrase problem. For example, in Table 3, we
make use of Lucene Index13 with BM25 to retrieve
the similar sentences to the WSC sentences from
our training dataset, and make a comparison. We
can see that these pairs are somehow paraphrased
each other respectively. For the first pair, the con-
textual representations of “Paul” and “he” in WSC
could be similar to the contextual representations
of ”he” in our training sentence. As these represen-
tations are used to compute the co-reference score,
the final scores would be similar. The pseudo la-
bel “positive” for our first sentence will make the
positive probability of the golden co-references
“Paul” and “he” in WSC higher. And for the sec-
ond pair in Table 3, the pseudo label of positive in
our second sentence will make the positive prob-
ability of the golden co-references “George” and
“he” in WSC 2 higher. In this way, these kinds
of co-reference patterns from training data can be
directly mapped to solve the Winograd Schema
Challenges.

13http://lucene.apache.org/pylucene/



890

Here’s another example from PDP demonstrat-
ing the benefit of our method: “Always before,
Larry had helped Dad with his work. But he could
not help him now, for Dad said that ”. Trinh and Le
(2018) failed on this one, probably because lan-
guage models are not good at solving long dis-
tance dependence, and tends to predict that “he ”
refers to “his” in the near context rather the cor-
rect answer “Larry”. And our model can give the
correct prediction.

We further analysis the predictions of our
model. We find that some specific commonsense
knowledge are still hard to learn, such as the fol-
lowing pairs:

• The trophy doesn’t fit into the brown suitcase
because it is too small.

• The trophy doesn’t fit into the brown suitcase
because it is too large.

To solve this problem, the model should learn
the knowledge to compare the size of the objects.
However, all of our models trained with differ-
ent hyper-parameters select the same candidate as
the co-referred word for “it” in both sentences. To
solve the problem, broader data need to collect for
learning more commonsense knowledge.

5 Conclusion

In conclusion, to overcome the lack of human la-
beled data, we proposed two unsupervised deep
structured semantic models (UDSSM) for com-
monsense reasoning. We evaluated our models
on the commonsense reasoning tasks of Pronoun
Disambiguation Problems (PDP) and Winograd
Schema Challenge (Levesque et al., 2011), where
the questions are quite easy for human to answer,
but quite challenging for the machine. Without
using any hand-craft knowledge base, our model
achieved stat-of-the-art performance on the two
tasks.

In the future work, we will use Transformer,
which is proved to be more powerful than LSTM,
as the encoder of our unsupervised deep structured
semantic models, and we will collect a larger cor-
pus from Common Crawl to train our model.

References
Daniel Bailey, Amelia Harrison, Yuliya Lierler,

Vladimir Lifschitz, and Julian Michael. 2015. The
winograd schema challenge and reasoning about

correlation. In Knowledge Representation; Coref-
erence Resolution; Reasoning.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.

Kevin Clark and Christopher D Manning. 2016a. Deep
reinforcement learning for mention-ranking corefer-
ence models. Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.

Kevin Clark and Christopher D Manning. 2016b. Im-
proving coreference resolution by learning entity-
level distributed representations. Proceedings of the
Association for Computational Linguistics.

Robin Cooper, Dick Crouch, Jan Van Eijck, Chris
Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp,
David Milward, Manfred Pinkal, Massimo Poesio,
and Steve Pulman. 1996. Using the framework.
Technical report, Technical Report LRE 62-051 D-
16, The FraCaS Consortium.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine Learning Challenges. Evalu-
ating Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment.

Ernest Davis and Gary Marcus. 2015. Commonsense
reasoning and commonsense knowledge in artificial
intelligence. Communications of the ACM.

Jianfeng Gao, Michel Galley, and Lihong Li. 2018.
Neural approaches to conversational ai. arXiv
preprint arXiv:1809.08267.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion & knowledge management.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the
limits of language modeling. arXiv preprint
arXiv:1602.02410.

Hector J Levesque, Ernest Davis, and Leora Morgen-
stern. 2011. The winograd schema challenge. In
AAAI spring symposium: Logical formalizations of
commonsense reasoning.

Hugo Liu and Push Singh. 2004. Conceptneta practi-
cal commonsense reasoning tool-kit. BT technology
journal, 22(4):211–226.

Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua
Ling, Xiaodan Zhu, Si Wei, and Yu Hu. 2016. Prob-
abilistic reasoning via deep learning: Neural associ-
ation models. arXiv preprint arXiv:1603.07704.



891

Quan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu,
Si Wei, and Yu Hu. 2017. Combing context and
commonsense knowledge through neural networks
for solving winograd schema problems. In AAAI
Spring Symposium Series.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.

Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.

Haoruo Peng, Daniel Khashabi, and Dan Roth. 2015.
Solving hard coreference problems. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies.

Haoruo Peng, Yangqiu Song, and Dan Roth. 2016.
Event detection and co-reference with minimal su-
pervision. In Proceedings of the conference on em-
pirical methods in natural language processing.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Altaf Rahman and Vincent Ng. 2012. Resolving
complex cases of definite pronouns: The winograd
schema challenge. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.

Hannah Rashkin, Antoine Bosselut, Maarten Sap,
Kevin Knight, and Yejin Choi. 2018a. Modeling
naive psychology of characters in simple common-
sense stories. arXiv preprint arXiv:1805.06533.

Hannah Rashkin, Maarten Sap, Emily Allaway,
Noah A. Smith, and Yejin Choi. 2018b.
Event2mind: Commonsense inference on events,
intents, and reactions. In Proceedings of the
Association for Computational Linguistics.

Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S Gordon. 2011. Choice of plausible alterna-
tives: An evaluation of commonsense causal reason-
ing. In AAAI Spring Symposium: Logical Formal-
izations of Commonsense Reasoning.

Peter Schüller. 2014. Tackling winograd schemas by
formalizing relevance theory in knowledge graphs.
In Knowledge Representation and Reasoning Con-
ference.

Arpit Sharma, Nguyen H. Vo, Somak Aditya, and
Chitta Baral. 2015. Towards addressing the wino-
grad schema challenge: Building and using a seman-
tic parser and a knowledge hunting module. In Pro-
ceedings of the International Conference on Artifi-
cial Intelligence.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational linguistics.

Trieu H Trinh and Quoc V Le. 2018. A simple
method for commonsense reasoning. arXiv preprint
arXiv:1806.02847.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies).

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi. 2018. Swag: A large-scale adversarial dataset
for grounded commonsense inference. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.

Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng
Gao, Kevin Duh, and Benjamin Van Durme. 2018.
ReCoRD: Bridging the Gap between Human and
Machine Commonsense Reading Comprehension.
arXiv preprint arXiv:1810.12885.

Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben-
jamin Van Durme. 2017. Ordinal common-sense in-
ference. Transactions of the Association for Com-
putational Linguistics.


