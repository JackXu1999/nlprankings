



















































community2vec: Vector representations of online communities encode semantic relationships


Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 27–31,
Vancouver, Canada, August 3, 2017. c©2017 Association for Computational Linguistics

community2vec: Vector representations of online communities encode
semantic relationships

Trevor Martin
Department of Biology, Stanford University

Stanford, CA 94035
trevorm@stanford.edu

Abstract

Vector embeddings of words have been
shown to encode meaningful semantic re-
lationships that enable solving of complex
analogies. This vector embedding concept
has been extended successfully to many
different domains and in this paper we
both create and visualize vector represen-
tations of an unstructured collection of on-
line communities based on user participa-
tion. Further, we quantitatively and qual-
itatively show that these representations
allow solving of semantically meaningful
community analogies and also other more
general types of relationships. These re-
sults could help improve community rec-
ommendation engines and also serve as a
tool for sociological studies of community
relatedness.

1 Introduction

Social media usage and participation in online
communities has grown steadily over the last
decade (Perrin, 2015). As we increasingly live our
lives online, it is important to characterize the on-
line communities we inhabit and understand the
relationships between them. Our expanding re-
liance on online communities also represents an
exciting opportunity to understand the links be-
tween different interests and hobbies, as candid
participation across online communities is more
immediately and scalably measurable compared to
offline communities.

Recent work has shown that vector represen-
tations and embeddings of entities are a power-
ful tool across a range of applications from words
(Mikolov et al., 2013a) to DNA sequences (As-
gari and Mofrad, 2015). In particular, the co-
occurrence based embeddings of words in a cor-

pus has been demonstrated to encode meaning-
ful semantic relationships between them (Mikolov
et al., 2013b). In this paper we extend the concept
of vector embeddings to represent an unstructured
collection of online communities and show that
the co-occurrence of users across online commu-
nities also embeds the semantic relations between
them. Further downstream applications of these
results could include improved community recom-
mendation engines and advertisement targeting.

We focus our analysis on the social sharing site
Reddit, the 4th most popular website in the US
(Alexa, 2017), which has user created and man-
aged communities called subreddits.1 Subreddits
are communities centered around particular top-
ics and interests where users can post articles and
comments while also voting content up or down to
make it more or less visible. To our knowledge this
paper represents the first use of vector based repre-
sentations of such communities to solve analogies
and perform semantically meaningful calculations
of relationships.

2 Related Work

Reddit is relatively understudied compared to
other social networks such as Facebook, but an in-
creasing body of work has used its data to look at
topics ranging from online user behavior (Hamil-
ton et al., 2017) to user migration across social me-
dia platforms (Newell et al., 2016). A map of Red-
dit using commenter co-occurrences has also been
previously created using a much smaller sample of
comment data (Olsen and Neal, 2015) by treating
the co-occurrence matrix as a weighted graph and
extracting the network backbone. Relatedly, there
has been interest in developing vector representa-
tions of graph structures as shown by techniques

1Subreddits are typically denoted with a leading /r/, for
example /r/dataisbeautiful is the “dataisbeautiful”
subreddit.

27



like DeepWalk (Perozzi et al., 2014) and node2vec
(Grover and Leskovec, 2016), which we could po-
tentially use to create additional vector represen-
tations to test below. Reddit communities do not
have a built-in explicit graph structure though, as
there are not defined links between communities in
the same manner as users can be linked by friend-
ship requests on sites like Facebook. In this pa-
per we show that semantically meaningful maps of
communities can be created using the NLP tool-
box originally created for mapping the semantic
similarity of words, without a need for defining an
explicit graph.

3 Method

Our method for uncovering semantic relationships
between online communities begins by creating
vector representations of each community based
on how often users comment across communities
using one of the three methods outlined below.
Broadly, we follow the general framework of Levy
et al. (2015), where in our modified framework
communities take on the role of words and user
co-occurrence the role of word co-occurrence. We
then simply add and subtract these community
vectors to evaluate semantic correctness. Here, we
use a publicly available corpus of all Reddit com-
ments from January 1st, 2015 through April 30th,
2017 as the input to each technique. This data set
consists of roughly 1.8 billion comments across
60,978 subreddit communities.2

3.1 Subreddit Vectors

We first create a symmetric matrix of community-
community user co-occurrences X, whose entries
Xij indicate the number of unique users who com-
mented 10 times or more in each subreddit.

Explicit: Our explicit subreddit representation
first simply subsets the co-occurrence matrix X
to include only the subreddits with unique author
ranks between 200 and 2,201 as context subred-
dits (columns of X). The choice of rank cutoff
here is arbitrary but based on the idea that per-
formance can be increased by adjusting the num-
ber of context tokens (Bullinaria and Levy, 2007).
We choose the subreddits with the most unique au-
thors because these are likely to encode the most
useful information and drop the top 200 subred-

2Reddit data available at: https://bigquery.
cloud.google.com/table/fh-bigquery:
reddit_comments.all_starting_201501

dits because many of these are “default” subred-
dits that all Reddit users are subscribed to and
thus are unlikely to have as rich co-occurrence in-
formation. Then we transform this new matrix
X:,201:2200 using the positive pointwise mutual
information metric to weigh each count by its in-
formativeness, where p(i, j) is the joint probabil-
ity of seeing authors in both subreddits i and j and
p(i) and p(j) are the probabilities of seeing an au-
thor in each subreddit respectively:

PMI(i, j) ≡ log p(i, j)
p(i)p(j)

PPMI(i, j) =

{
0, if PMI(i, j) < 0
PMI(i, j), otherwise

The subreddit vectors (rows) of the resulting
PPMI matrix are then scaled to unit length.
PCA: We also create a dense vector representation
of subreddits by calculating the principal com-
ponents of the PPMI transformation above ap-
plied to the matrix X:,1:5000, which is X subset
to the top 5,000 context subreddits by unique au-
thor ranks. We extract the top 100 principal com-
ponents and scale each subreddit vector to unit
length.
GloVe: Finally, we create a second dense vector
representation of subreddits by running the GloVe
algorithm (Pennington et al., 2014), originally de-
veloped to create embeddings for word-word co-
occurrence matrices, on the raw co-occurrence
matrix X. The resulting size 100 GloVe subred-
dit vectors are again scaled to unit length.

3.2 Subreddit Algebra

Combinations of subreddit representations (sub-
reddit algebra) are performed through standard
vector addition and subtraction. The similarity be-
tween two subreddits is defined here as the cosine
similarity, given by:

cosine similarity( ~A, ~B) =
~A · ~B
‖ ~A‖‖ ~B‖

Where ~A and ~B are the vector representations
of subreddit A and B respectively. Subreddits are
ranked in similarity by ordering from largest co-
sine similarity to smallest.

28



(a) View of subreddits representing medical interests and health
conscious lifestyles.

(b) View of subreddits representing music genres and perform-
ing groups.

Figure 1: Examples of semantically meaningful clusters in t-SNE visualization of GloVe subreddit vec-
tors. Zoomed-in region of t-SNE visualization indicated in red on figure insets.

4 Evaluation

We quantitatively evaluate the efficacy of subred-
dit algebra by assessing its ability to identify lo-
cal sports team subreddits from combinations of
league and geography subreddits. Additionally,
we qualitatively evaluate our the results by iden-
tifying specific interesting subreddit relationships
and visualizing the subreddit vector space as a
whole.

4.1 tSNE Clustering
To check that our vector representations of sub-
reddit communities are reasonable, we used t-SNE
(Maaten and Hinton, 2008) to project the high-
dimensional vector representations of each sub-
reddit into two dimensions for visualization. Ex-
amples of typical semantically meaningful clusters
that we can observe in these t-SNE projections are
given in Figure 1. Figure 1a shows that medi-
cal and health related subreddits cluster together
and Figure 1b shows the dense clustering of music
and band related subreddits and clustering within
this larger group by music genre.3 These natural
groupings suggest that our vector representations
are reasonable and are encoding semantically rel-
evant information about each subreddit.

3To aid in visualization, we only project the top 5,000 and
2,500 subreddits by unique author count for the medical and
music GloVe based clusters respectively.

4.2 Automated Semantic Relationship Test

In order to quantitatively evaluate the ability of the
subreddit vectors to encode semantic relations, we
created a list of subreddit combinations where we
have a strong expectation for the outcome subred-
dit. Conveniently, sport, location, and team sub-
reddits have a natural analogy structure. Specifi-
cally, for the NBA, NFL, and NHL sports leagues
we created a list of geographic location subreddits
(e.g. /r/sanfrancisco) that when combined
with a league subreddit (e.g. /r/nba) should re-
sult in that location’s local league affiliate (e.g.
/r/warriors).4 Performance on this task for
an individual league-location pair is assessed by
calculating:

median(SR(~S, ~T ), SR(~L, ~T ))− SR(~S + ~L, ~T )

Where ~S is the league subreddit, ~L is the lo-
cation subreddit, and ~T is the target subreddit.
SR( ~A, ~B) is the rank of the subreddit B when all
subreddits are ordered by decreasing cosine simi-
larity to subreddit A.

The decrease in similarity ranking for each
sports league across each of the three vector repre-
sentations was then evaluated for significance by

4In total we use 92 league-location combinations.

29



Figure 2: Comparison of different vector representation’s performance for identifying local sports teams
in each league.

Method League ~S + ~L: ~T Median Rank Median Rank Diff. p-value

Explicit
NBA 7 365.5 1.9e-9
NFL 5 170.8 8.3e-7
NHL 4 87.5 1.9e-9

PCA
NBA 212 976.3 4.7e-8
NFL 13 320.1 9.3e-10
NHL 41.5 330 3.4e-4

GloVe
NBA 7 46.5 1.8e-6
NFL 2 25 1.5e-5
NHL 1 16.5 1.8e-6

Table 1: Results of automated testing of subreddit vector representation semantic encodings.

a two-sided Wilcoxon signed-rank test for sym-
metry of the rank changes around 0. The me-
dian decrease in target subreddit rank between
SR(~S + ~L, ~T ) and median(SR(~S, ~T ), SR(~L, ~T ))
for each sports league-vector representation pair is
shown in Figure 2.5 Interestingly, both the explicit
and PCA vector representations appear to perform
best, but all three methods show significant perfor-
mance on the task as indicated in Table 1.

Closer inspection of the results reveals though
that while the PCA method has the largest im-
provement in target subreddit rank (Median Rank
Diff. in Table 1), it also has the highest median
subreddit ranks for the target subreddits after per-
forming subreddit algebra of the three methods
(~S+~L: ~T Median Rank in Table 1). This observa-
tion suggests that while the PCA representations
benefit the most from algebra they also have the
least accuracy for identifying the target subreddit

5More specifically the Hodges-Lehmann pseudomedian,
with 95% CI

overall.6 In contrast, for algebra using either the
explicit or GloVe vector representations, the target
subreddit is often the most similar result.

4.3 Selected Semantic Examples

In addition to the automated test, we also identi-
fied several interesting analogy tasks to run using
subreddit algebra.7 Because we do not necessarily
have subreddits for representing concepts such as
“man” or “woman” we cannot reproduce exactly
classic cases like king−man+woman = queen,
but for the cases where we could form robust
analogies the results are encouraging, as shown in
Figure 3.

Of note is that we can reproduce country:capital
relationships similar to those found in word em-
beddings using community participation across
subreddits and also can reproduce analogies that

6Also, PCA based representations do not necessarily have
the linear substructure seen in GloVe embeddings.

7We use the explicit representations here.

30



Figure 3: Selected semantic algebra examples.

subtract a component (Chicago) of a whole
(Chicago Bulls NBA team) and add a different
location (Minnesota) to get that locality’s NBA
team (Minnesota Timberwolves). We can also find
communities specific to medium-genre combina-
tions such as the historical fiction book commu-
nity /r/HFnovels. Finally, we see some sur-
prising examples, such as subtracting the commu-
nity for frugality from the community for man-
aging personal finances results in the commu-
nity for taking extreme risks on the stock market,
/r/wallstreetbets.

5 Conclusions

Our work here shows that vector representations
of communities can encode meaningful analogies
and semantic relationships in the same way as
has been previously seen for words. Notably, the
explicit vector representations perform competi-
tively with the GloVe embeddings on the semantic
task we tested, suggesting that the semantic mean-
ings are present in the raw vectors and are simply
preserved through the embedding process. Future
directions we are pursuing involve supplementing
the vector representations with data on comment
voting scores, using posts or views in lieu of or
supplementally to comments and looking at di-
achronic subreddit embeddings to analyze the pat-
terns of subreddit relationships over time.

Acknowledgments

We would like to thank Will Hamilton for his valu-
able comments and suggestions on the manuscript.

References
Alexa. 2017. http://www.alexa.com/siteinfo/reddit.com.

Alexa Rankings .

Ehsaneddin Asgari and Mohammad R. K. Mofrad.
2015. Continuous distributed representation
of biological sequences for deep proteomics
and genomics. PLOS ONE 10(11):1–15.
https://doi.org/10.1371/journal.pone.0141287.

John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word
co-occurrence statistics: A computational study.
Behavior Research Methods 39(3):510–526.
https://doi.org/10.3758/BF03193020.

Aditya Grover and Jure Leskovec. 2016. node2vec:
Scalable feature learning for networks. CoRR
abs/1607.00653. http://arxiv.org/abs/1607.00653.

William L. Hamilton, Justine Zhang, Cristian Danescu-
Niculescu-Mizil, Dan Jurafsky, and Jure Leskovec.
2017. Loyalty in online communities. CoRR
abs/1703.03386. http://arxiv.org/abs/1703.03386.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Asso-
ciation for Computational Linguistics 3:211–225.
https://transacl.org/ojs/index.php/tacl/article/view/570.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. CoRR abs/1301.3781.
http://arxiv.org/abs/1301.3781.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Hlt-naacl. volume 13,
pages 746–751.

Edward Newell, David Jurgens, Haji Saleem, Hardik
Vala, Jad Sassine, Caitrin Armstrong, and Derek
Ruths. 2016. User migration in online social net-
works: A case study on reddit during a period of
community unrest.

Randal Olsen and Zachary Neal. 2015. Navigating the
massive world of reddit: using backbone networks to
map user interests in social media. PeerJ Computer
Science .

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543. http://www.aclweb.org/anthology/D14-1162.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: Online learning of so-
cial representations. CoRR abs/1403.6652.
http://arxiv.org/abs/1403.6652.

Andrew Perrin. 2015. Social media usage: 2005-2015.
PewResearchCenter .

A Supplemental Material

All code and league-location-team combina-
tions are available at https://github.com/
trevormartin/papers.

31


