



















































Sentence Rewriting for Semantic Parsing


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 766–777,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Sentence Rewriting for Semantic Parsing

Bo Chen Le Sun Xianpei Han Bo An
State Key Laboratory of Computer Sciences

Institute of Software, Chinese Academy of Sciences, China.
{chenbo, sunle, xianpei, anbo}@nfs.iscas.ac.cn

Abstract

A major challenge of semantic parsing
is the vocabulary mismatch problem be-
tween natural language and target ontol-
ogy. In this paper, we propose a sen-
tence rewriting based semantic parsing
method, which can effectively resolve the
mismatch problem by rewriting a sentence
into a new form which has the same struc-
ture with its target logical form. Specifi-
cally, we propose two sentence-rewriting
methods for two common types of mis-
match: a dictionary-based method for 1-
N mismatch and a template-based method
for N-1 mismatch. We evaluate our sen-
tence rewriting based semantic parser on
the benchmark semantic parsing dataset –
WEBQUESTIONS. Experimental results
show that our system outperforms the base
system with a 3.4% gain in F1, and gen-
erates logical forms more accurately and
parses sentences more robustly.

1 Introduction

Semantic parsing is the task of mapping natu-
ral language sentences into logical forms which
can be executed on a knowledge base (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Kate and Mooney, 2006; Wong and Mooney,
2007; Lu et al., 2008; Kwiatkowksi et al., 2010).
Figure 1 shows an example of semantic parsing.
Semantic parsing is a fundamental technique of
natural language understanding, and has been used
in many applications, such as question answering
(Liang et al., 2011; He et al., 2014; Zhang et al.,
2016) and information extraction (Krishnamurthy
and Mitchell, 2012; Choi et al., 2015; Parikh et al.,
2015).

Semantic parsing, however, is a challenging

Sentence:     What is the capital of Germany? 

 

 

Logical form: λx.capital(Germany,x) 

 

 

Result:             {Berlin} 

 

(Semantic parsing) 

KB

(Execution) 

Figure 1: An example of semantic parsing.

task. Due to the variety of natural language ex-
pressions, the same meaning can be expressed us-
ing different sentences. Furthermore, because log-
ical forms depend on the vocabulary of target-
ontology, a sentence will be parsed into different
logical forms when using different ontologies. For
example, in below the two sentences s1 and s2
express the same meaning, and they both can be
parsed into the two different logical forms lf1 and
lf2 using different ontologies.

s1 What is the population of Berlin?
s2 How many people live in Berlin?
lf1 λx.population(Berlin,x)
lf2 count(λx.person(x)∧live(x,Berlin))

Based on the above observations, one major
challenge of semantic parsing is the structural mis-
match between a natural language sentence and
its target logical form, which are mainly raised
by the vocabulary mismatch between natural lan-
guage and ontologies. Intuitively, if a sentence has
the same structure with its target logical form, it is
easy to get the correct parse, e.g., a semantic parser
can easily parse s1 into lf1 and s2 into lf2. On
the contrary, it is difficult to parse a sentence into
its logic form when they have different structures,
e.g., s1 → lf2 or s2 → lf1.

To resolve the vocabulary mismatch problem,

766



(a) An example using traditional method
s0 : What is the name of Sonia Gandhis daughter?
l0 : λx.child(S.G.,x)
r0 : {Rahul Gandhi (Wrong answer), Priyanka Vadra}

(b) An example using our method
s0 : What is the name of Sonia Gandhis daughter?
s1 : What is the name of Sonia Gandhis female child?
l1 : λx.child(S.G.,x)∧gender(x,female)
r1 : {Priyanka Vadra}

Table 1: Examples of (a) sentences s0, possible
logical form l0 from traditional semantic parser,
result r0 for the logical form l0; (b) possible sen-
tence s1 from rewriting for the original sentence
s0, possible logical form l1 for sentence s1, result
r1 for l1. Rahul Gandhi is a wrong answer, as he
is the son of Sonia Gandhi.

this paper proposes a sentence rewriting approach
for semantic parsing, which can rewrite a sen-
tence into a form which will have the same struc-
ture with its target logical form. Table 1 gives
an example of our rewriting-based semantic pars-
ing method. In this example, instead of parsing
the sentence “What is the name of Sonia Gand-
his daughter?” into its structurally different log-
ical form childOf.S.G.∧gender.female
directly, our method will first rewrite the sentence
into the form “What is the name of Sonia Gand-
his female child?”, which has the same structure
with its logical form, then our method will get
the logical form by parsing this new form. In
this way, the semantic parser can get the correct
parse more easily. For example, the parse obtained
through traditional method will result in the wrong
answer “Rahul Gandhi”, because it cannot iden-
tify the vocabulary mismatch between “daughter”
and child∧female1. By contrast, by rewriting
“daughter” into “female child”, our method can
resolve this vocabulary mismatch.

Specifically, we identify two common types of
vocabulary mismatch in semantic parsing:

1. 1-N mismatch: a simple word may corre-
spond to a compound formula. For example,
the word “daughter” may correspond to the
compound formula child∧female.

2. N-1 mismatch: a logical constant may cor-
respond to a complicated natural language
expression, e.g., the formula population
can be expressed using many phrases such as
“how many people” and “live in”.

1In this paper, we may simplify logical forms for readabil-
ity, e.g., female for gender.female.

To resolve the above two vocabulary mismatch
problems, this paper proposes two sentence rewrit-
ing algorithms: One is a dictionary-based sen-
tence rewriting algorithm, which can resolve the
1-N mismatch problem by rewriting a word us-
ing its explanation in a dictionary. The other
is a template-based sentence rewriting algorithm,
which can resolve the N-1 mismatch problem
by rewriting complicated expressions using para-
phrase template pairs.

Given the generated rewritings of a sentence,
we propose a ranking function to jointly choose
the optimal rewriting and the correct logical form,
by taking both the rewriting features and the se-
mantic parsing features into consideration.

We conduct experiments on the benchmark
WEBQUESTIONS dataset (Berant et al., 2013).
Experimental results show that our method can ef-
fectively resolve the vocabulary mismatch prob-
lem and achieve accurate and robust performance.

The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our sentence rewriting method for seman-
tic parsing. Section 4 presents the scoring func-
tion which can jointly ranks rewritings and logical
forms. Section 5 discusses experimental results.
Section 6 concludes this paper.

2 Related Work

Semantic parsing has attracted considerable re-
search attention in recent years. Generally, se-
mantic parsing methods can be categorized into
synchronous context free grammars (SCFG) based
methods (Wong and Mooney, 2007; Arthur et al.,
2015; Li et al., 2015), syntactic structure based
methods (Ge and Mooney, 2009; Reddy et al.,
2014; Reddy et al., 2016), combinatory categor-
ical grammars (CCG) based methods (Zettle-
moyer and Collins, 2007; Kwiatkowksi et al.,
2010; Kwiatkowski et al., 2011; Krishnamurthy
and Mitchell, 2014; Wang et al., 2014; Artzi et al.,
2015), and dependency-based compositional se-
mantics (DCS) based methods (Liang et al., 2011;
Berant et al., 2013; Berant and Liang, 2014; Be-
rant and Liang, 2015; Pasupat and Liang, 2015;
Wang et al., 2015).

One major challenge of semantic parsing is how
to scale to open-domain situation like Freebase
and Web. A possible solution is to learn lexicons
from large amount of web text and a knowledge
base using a distant supervised method (Krishna-

767



murthy and Mitchell, 2012; Cai and Yates, 2013a;
Berant et al., 2013). Another challenge is how to
alleviate the burden of annotation. A possible so-
lution is to employ distant-supervised techniques
(Clarke et al., 2010; Liang et al., 2011; Cai and
Yates, 2013b; Artzi and Zettlemoyer, 2013), or
unsupervised techniques (Poon and Domingos,
2009; Goldwasser et al., 2011; Poon, 2013).

There were also several approaches focused
on the mismatch problem. Kwiatkowski et al.
(2013) addressed the ontology mismatch prob-
lem (i.e., two ontologies using different vocabu-
laries) by first parsing a sentence into a domain-
independent underspecified logical form, and then
using an ontology matching model to transform
this underspecified logical form to the target on-
tology. However, their method is still hard to
deal with the 1-N and the N-1 mismatch prob-
lems between natural language and target ontolo-
gies. Berant and Liang (2014) addressed the struc-
ture mismatch problem between natural language
and ontology by generating a set of canonical ut-
terances for each candidate logical form, and then
using a paraphrasing model to rerank the candi-
date logical forms. Their method addresses mis-
match problem in the reranking stage, cannot re-
solve the mismatch problem when constructing
candidate logical forms. Compared with these
two methods, we approach the mismatch prob-
lem in the parsing stage, which can greatly reduce
the difficulty of constructing the correct logical
form, through rewriting sentences into the forms
which will be structurally consistent with their tar-
get logic forms.

Sentence rewriting (or paraphrase generation)
is the task of generating new sentences that have
the same meaning as the original one. Sentence
rewriting has been used in many different tasks,
e.g., used in statistical machine translation to re-
solve the word order mismatch problem (Collins
et al., 2005; He et al., 2015). To our best knowl-
edge, this paper is the first work to apply sentence
rewriting for vocabulary mismatch problem in se-
mantic parsing.

3 Sentence Rewriting for Semantic
Parsing

As discussed before, the vocabulary mismatch be-
tween natural language and target ontology is a
big challenge in semantic parsing. In this section,
we describe our sentence rewriting algorithm for

Word Logical Form WiktionaryExplanation
son child∧male male child

actress actor∧female female actor
father parent∧male male parent

grandaprent parent∧parent parent of one’sparent
brother sibling∧male male sibling

Table 2: Several examples of words, their logical
forms and their explanations in Wiktionary.

solving the mismatch problem. Specifically, we
solve the 1-N mismatch problem by dictionary-
based rewriting and solve the N-1 mismatch prob-
lem by template-based rewriting. The details are
as follows.

3.1 Dictionary-based Rewriting

In the 1-N mismatch case, a word will correspond
to a compound formula, e.g., the target logical
form of the word “daughter” is child∧female
(Table 2 has more examples).

To resolve the 1-N mismatch problem, we
rewrite the original word (“daughter”) into an
expression (“female child”) which will have
the same structure with its target logical form
(child∧female). In this paper, we rewrite
words using their explanations in a dictionary.
This is because each word in a dictionary will
be defined by a detailed explanation using sim-
ple words, which often will have the same struc-
ture with its target formula. Table 2 shows how
the vocabulary mismatch between a word and its
logical form can be resolved using its dictionary
explanation. For instance, the word “daughter” is
explained as “female child” in Wiktionary, which
has the same structure as child∧female.

In most cases, only common nouns will result
in the 1-N mismatch problem. Therefore, in order
to control the size of rewritings, this paper only
rewrite the common nouns in a sentence by replac-
ing them with their dictionary explanations. Be-
cause a sentence usually will not contain too many
common nouns, the size of candidate rewritings
is thus controllable. Given the generated rewrit-
ings of a sentence, we propose a sentence selection
model to choose the best rewriting using multiple
features (See details in Section 4).

Table 3 shows an example of the dictionary-
based rewriting. In Table 3, the example sen-
tence s contains two common nouns (“name”
and “daughter”), therefore we will generate three
rewritings r1, r2 and r3. Among these rewritings,

768



s : What is the name of Sonia Gandhis daughter?
r1: What is the reputation of Sonia Gandhis daughter?
r2: What is the name of Sonia Gandhis female child?
r3: What is the reputation of Sonia Gandhis female child?

Table 3: An example of the dictionary-based sen-
tence rewriting.

the candidate rewriting r2 is what we expected,
as it has the same structure with the target logical
form and doesn’t bring extra noise (i.e., replacing
“name” with its explanation “reputation”).

For the dictionary used in rewriting, this paper
uses Wiktionary. Specifically, given a word, we
use its “Translations” part in the Wiktionary as its
explanation. Because most of the 1-N mismatch
are caused by common nouns, we only collect the
explanations of common nouns. Furthermore, for
polysomic words which have several explanations,
we only use their most common explanations. Be-
sides, we ignore explanations whose length are
longer than 5.

3.2 Template-based Rewriting

In the N-1 mismatch case, a complicated natu-
ral language expression will be mapped to a sin-
gle logical constant. For example, considering the
following mapping from the natural language sen-
tence s to its logical form lf based on Freebase
ontology:

s: How many people live in Berlin?
lf : λx.population(Berlin,x)

where the three words: “how many” (count),
“people” (people) and “live in” (live) will
map to the predicate population together. Ta-
ble 4 shows more N-1 examples.

Expression Logical constant
how many, people, live in population

how many, people, visit, annually annual-visit
what money, use currency

what school, go to education

what language, speak, officially official-
language

Table 4: Several N-1 mismatch examples.

To resolve the N-1 mismatch problem, we pro-
pose a template rewriting algorithm, which can
rewrite a complicated expression into its sim-
pler form. Specifically, we rewrite sentences
based on a set of paraphrase template pairs P =
{(ti1, ti2)|i = 1, 2, ..., n}, where each template t

Template 1 Template 2

How many people live in $y What is the population of$y
What money in $y is used What is the currency of $y

What school did $y go to What is the education of$y
What language does $y

speak officially
What is the official

language of $y

Table 5: Several examples of paraphrase template
pairs.

is a sentence with an argument slot $y, and ti1 and
ti2 are paraphrases. In this paper, we only con-
sider single-slot templates. Table 5 shows several
paraphrase template pairs.
Given the template pair database and a sentence,
our template-based rewriting algorithm works as
follows:

1. Firstly, we generate a set of candidate tem-
plates ST = {st1, st2, ..., stn} of the sen-
tence by replacing each named entity within
it by “$y”. For example, we will gener-
ate template “How many people live in $y”
from the sentence “How many people live in
Berlin”.

2. Secondly, using the paraphrase template pair
database, we retrieve all possible rewriting
template pairs (t1, t2) with t1 ∈ ST , e.g., we
can retrieve template pair (“How many peo-
ple live there in $y”, “What is the population
of $y” for t2) using the above ST .

3. Finally, we get the rewritings by replacing
the argument slot “$y” in template t2 with
the corresponding named entity. For exam-
ple, we get a new candidate sentence “What
is the population of Berlin” by replacing
“$y” in t2 with Berlin. In this way we
can get the rewriting we expected, since this
rewriting will match its target logical form
population(Berlin).

To control the size and measure the quality of
rewritings using a specific template pair, we also
define several features and the similarity between
template pairs (See Section 4 for details).

To build the paraphrase template pair database,
we employ the method described in Fader et al.
(2014) to automatically collect paraphrase tem-
plate pairs. Specifically, we use the WikiAnswers
paraphrase corpus (Fader et al., 2013), which con-
tains 23 million question-clusters, and all ques-

769



How many people live in chembakolli?
How many people is in chembakolli?
How many people live in chembakolli india?
How many people live there chembakolli?
How many people live there in chembakolli?
What is the population of Chembakolli india?
What currency is used on St Lucia?
What is st lucia money?
What is the money used in st lucia?
What kind of money did st lucia have?
What money do st Lucia use?
Which money is used in St Lucia?

Table 6: Two paraphrase clusters from the
WikiAnswers corpus.

tions in the same cluster express the same mean-
ing. Table 6 shows two paraphrase clusters from
the WikiAnswers corpus. To build paraphrase
template pairs, we first replace the shared noun
words in each cluster with the placeholder “$y”,
then each two templates in a cluster will form a
paraphrase template pair. To filter out noisy tem-
plate pairs, we only retain salient paraphrase tem-
plate pairs whose co-occurrence count is larger
than 3.

4 Sentence Rewriting based Semantic
Parsing

In this section we describe our semantic rewriting
based semantic parsing system. Figure 2 presents
the framework of our system. Given a sentence,
we first rewrite it into a set of new sentences, then
we generate candidate logical forms for each new
sentence using a base semantic parser, finally we
score all logical forms using a scoring function
and output the best logical form as the final result.
In following, we first introduce the used base se-
mantic parser, then we describe the proposed scor-
ing function.

4.1 Base Semantic Parser

In this paper, we produce logical forms for each
sentence rewritings using an agenda-based seman-
tic parser (Berant and Liang, 2015), which is
based on the lambda-DCS proposed by Liang
(2013). For parsing, we use the lexicons and the
grammars released by Berant et al. (2013), where
lexicons are used to trigger unary and binary pred-
icates, and grammars are used to conduct logical
forms. The only difference is that we also use the
composition rule to make the parser can handle
complicated questions involving two binary pred-
icates, e.g., child.obama∧gender.female.

Original sentence

New sentences

Logical forms

Results

(Sentence rewriting)

(Semantic parsing)

(Executing)

Figure 2: The framework of our sentence rewriting
based semantic parsing.

For model learning and sentence parsing, the
base semantic parser learned a scoring function
by modeling the policy as a log-linear distribution
over (partial) agenda derivations Q:

pθ(a|s) = exp{φ(a)
T θ)}∑

a′∈A exp{φ(a′)T θ)}
(1)

The policy parameters are updated as follows:

θ ← θ + ηR(htarget)
∑T

t=1
δ(htarget) (2)

δt(h) = ∇θ log pθ(at|st)
= φ(at)− Epθ(a′t|st)[φ(a′t)]

(3)

The reward function R(h) measures the compati-
bility of the resulting derivation, and η is the learn-
ing rate which is set using the AdaGrad algorithm
(Duchi et al., 2011). The target history htarget is
generated from the root derivation d∗ with highest
reward out of the K (beam size) root derivations,
using local reweighting and history compression.

4.2 Scoring Function
To select the best semantic parse, we propose a
scoring function which can take both sentence
rewriting features and semantic parsing features
into consideration. Given a sentence x, a gener-
ated rewriting x′ and the derivation d of x′, we
score them using follow function:

score(x, x′, d) = θ · φ(x, x′, d)
= θ1 · φ(x, x′) + θ2 · φ(x′, d)

This scoring function is decomposed into two
parts: one for sentence rewriting – θ1 · φ(x, x′)
and the other for semantic parsing – θ2 · φ(x′, d).
Following Berant and Liang (2015), we update the
parameters θ2 of semantic parsing features as the

770



Input: Q/A pairs {(xi, yi) : i = 1...n}; Knowledge
baseK; Number of sentencesN ; Number of iterations T .
Definitions: The function REWRITING(xi) returns
a set of candidate sentences by applying sentence
rewriting on sentence x; PARSE(pθ, x) parses the
sentence x based on current parameters θ, using agenda-
based parsing; CHOOSEORACLE(h0) chooses
the derivation with highest reward from the root of
h0; CHOOSEORACLE(Htarget) chooses the
derivation with highest reward from a set of derivations.
CHOOSEORACLE(h∗target) chooses the new sen-
tence that results in derivation with highest reward.

Algorithm:
θ1 ← 0, θ2 ← 0
for t = 1...T , i = 1...N :
X = REWRITING(xi)
for each x′i ∈ X :
h0 ← PARSE(pθ, x′i)
d∗ ← CHOOSEORACLE(h0)
htarget ← PARSE(p+cwθ , x′i)

h∗target ← CHOOSEORACLE(Htarget)
x′∗i ← CHOOSEORACLE(h∗target)
θ2 ← θ2 + ηR(h∗target)

∑T
t=1 δ(h

∗
target)

θ1 ← θ1 + ηR(h∗target)δ(xi, x′∗i )
Output: Estimated parameters θ1 and θ2.

Table 7: Our learning algorithm for parameter es-
timation from question-answer pairs.

same as (2). Similarly, the parameters θ1 of sen-
tence rewriting features are updated as follows:

θ1 ← θ1 + ηR(h∗target)δ(x, x′∗)
δ(x, x′∗) = ∇ log pθ1(x′∗|x)

= φ(x, x′∗)− Epθ1 (x′|x)[φ(x, x
′)]

where the learning rate η is set using the same al-
gorithm in Formula (2).

4.3 Parameter Learning Algorithm

To estimate the parameters θ1 and θ2, our learn-
ing algorithm uses a set of question-answer pairs
(xi, yi). Following Berant and Liang (2015), our
updates for θ1 and θ2 do not maximize reward nor
the log-likelihood. However, the reward provides
a way to modulate the magnitude of the updates.
Specifically, after each update, our model results
in making the derivation, which has the highest re-
ward, to get a bigger score. Table 7 presents our
learning algorithm.

4.4 Features

As described in Section 4.3, our model uses two
kinds of features. One for the semantic parsing
module which are simply the same features de-
scribed in Berant and Liang (2015). One for the

sentence rewriting module these features are de-
fined over the original sentence, the generated sen-
tence rewritings and the final derivations:
Features for dictionary-based rewriting. Given
a sentence s0, when the new sentence s1 is gener-
ated by replacing a word to its explanation w →
ex, we will generate four features: The first fea-
ture indicates the word replaced. The second fea-
ture indicates the replacement w → ex we used.
The final two features are the POS tags of the left
word and the right word of w in s0.
Features for template-based rewriting. Given a
sentence s0, when the new sentence s1 is gener-
ated through a template based rewriting t1 → t2,
we generate four features: The first feature indi-
cates the template pair (t1, t2) we used. The sec-
ond feature is the similarity between the sentence
s0 and the template t1, which is calculated using
the word overlap between s0 and t1. The third
feature is the compatibility of the template pair,
which is the pointwise mutual information (PMI)
between t1 and t2 in the WikiAnswers corpus. The
final feature is triggered when the target logical
form only contains an atomic formula (or predi-
cate), and this feature indicates the mapping from
template t2 to the predicate p.

5 Experiments

In this section, we assess our method and compare
it with other methods.

5.1 Experimental Settings

Dataset: We evaluate all systems on the bench-
mark WEBQUESTIONS dataset (Berant et al.,
2013), which contains 5,810 question-answer
pairs. All questions are collected by crawling
the Google Suggest API, and their answers are
obtained using Amazon Mechanical Turk. This
dataset covers several popular topics and its ques-
tions are commonly asked on the web. According
to Yao (2015), 85% of questions can be answered
by predicting a single binary relation. In our ex-
periments, we use the standard train-test split (Be-
rant et al., 2013), i.e., 3,778 questions (65%) for
training and 2,032 questions (35%) for testing, and
divide the training set into 3 random 80%-20%
splits for development.

Furthermore, to verify the effectiveness of our
method on solving the vocabulary mismatch prob-
lem, we manually select 50 mismatch test exam-
ples from the WEBQUESTIONS dataset, where

771



all sentences have different structure with their tar-
get logical forms, e.g., “Who is keyshia cole dad?”
and “What countries have german as the official
language?”.
System Settings: In our experiments, we use
the Freebase Search API for entity lookup. We
load Freebase using Virtuoso, and execute logical
forms by converting them to SPARQL and query-
ing using Virtuoso. We learn the parameters of our
system by making three passes over the training
dataset, with the beam size K = 200, the dictio-
nary rewriting size KD = 100, and the template
rewriting size KT = 100.
Baselines: We compare our method with sev-
eral traditional systems, including semantic pars-
ing based systems (Berant et al., 2013; Berant and
Liang, 2014; Berant and Liang, 2015; Yih et al.,
2015), information extraction based systems (Yao
and Van Durme, 2014; Yao, 2015), machine trans-
lation based systems (Bao et al., 2014), embed-
ding based systems (Bordes et al., 2014; Yang et
al., 2014), and QA based system (Bast and Hauss-
mann, 2015).
Evaluation: Following previous work (Berant et
al., 2013), we evaluate different systems using the
fraction of correctly answered questions. Because
golden answers may have multiple values, we use
the average F1 score as the main evaluation metric.

5.2 Experimental Results

Table 8 provides the performance of all base-lines
and our method. We can see that:

1. Our method achieved competitive perfor-
mance: Our system outperforms all baselines
and get the best F1-measure of 53.1 on WE-
BQUESTIONS dataset.

2. Sentence rewriting is a promising technique
for semantic parsing: By employing sen-
tence rewriting, our system gains a 3.4% F1
improvement over the base system we used
(Berant and Liang, 2015).

3. Compared to all baselines, our system gets
the highest precision. This result indicates
that our parser can generate more-accurate
logical forms by sentence rewriting. Our sys-
tem also achieves the second highest recall,
which is a competitive performance. Interest-
ingly, both the two systems with the highest
recall (Bast and Haussmann, 2015; Yih et al.,

System Prec. Rec. F1 (avg)
Berant et al., 2013 48.0 41.3 35.7

Yao and Van-Durme, 2014 51.7 45.8 33.0
Berant and Liang, 2014 40.5 46.6 39.9

Bao et al., 2014 – – 37.5
Bordes et al., 2014a – – 39.2

Yang et al., 2014 – – 41.3
Bast and Haussmann, 2015 49.8 60.4 49.4

Yao, 2015 52.6 54.5 44.3
Berant and Liang, 2015 50.5 55.7 49.7

Yih et al., 2015 52.8 60.7 52.5
Our approach 53.7 60.0 53.1

Table 8: The results of our system and recently
published systems. The results of other systems
are from either original papers or the standard
evaluation web.

2015) rely on extra-techniques such as entity
linking and relation matching.

The effectiveness on mismatch problem. To an-
alyze the commonness of mismatch problem in
semantic parsing, we randomly sample 500 ques-
tions from the training data and do manually anal-
ysis, we found that 12.2% out of the sampled ques-
tions have mismatch problems: 3.8% out of them
have 1-N mismatch problem and 8.4% out of them
have N-1 mismatch problem.

To verify the effectiveness of our method on
solving the mismatch problem, we conduct experi-
ments on the 50 mismatch test examples and Table
9 shows the performance. We can see that our sys-
tem can effectively resolve the mismatch between
natural language and target ontology: compared to
the base system, our system achieves a significant
54.5% F1 im-provement.

System Prec. Rec. F1 (avg)
Base system 31.4 43.9 29.4
Our system 83.3 92.3 83.9

Table 9: The results on the 50 mismatch test
dataset.

When scaling a semantic parser to open-domain
situation or web situation, the mismatch problem
will be more common as the ontology and lan-
guage complexity increases (Kwiatkowski et al.,
2013). Therefore we believe the sentence rewrit-
ing method proposed in this paper is an important
technique for the scalability of semantic parser.
The effect of different rewriting algorithms.
To analyze the contribution of different rewriting
methods, we perform experiments using different
sentence rewriting methods and the results are pre-
sented in Table 10. We can see that:

772



Method Prec. Rec. F1 (avg)
base 49.8 55.3 49.1

+ dictionary SR (only) 51.6 57.5 50.9
+ template SR (only) 52.9 59.0 52.3

+ both 53.7 60.0 53.1

Table 10: The results of the base system and our
systems on the 2032 test questions.

1. Both sentence rewriting methods improved
the parsing performance, they resulted in
1.8% and 3.2% F1 improvements respec-
tively2.

2. Compared with the dictionary-based rewrit-
ing method, the template-based rewriting
method can achieve higher performance im-
provement. We believe this is because N-1
mismatch problem is more common in the
WEBQUESTIONS dataset.

3. The two rewriting methods are good comple-
mentary of each other. The semantic parser
can achieve a higher performance improve-
ment when using these two rewriting meth-
ods together.

The effect on improving robustness. We found
that the template-based rewriting method can
greatly improve the robustness of the base se-
mantic parser. Specially, the template-based
method can rewrite similar sentences into a
uniform template, and the (template, predi-
cate) feature can provide additional informa-
tion to reduce the uncertainty during parsing.
For example, using only the uncertain align-
ments from the words “people” and “speak”
to the two predicates official language
and language spoken, the base parser will
parse the sentence “What does jamaican peo-
ple speak?” into the incorrect logical form
official language.jamaican in our ex-
periments, rather than into the correct form
language spoken.jamaican (See the final
example in Table 11). By exploiting the alignment
from the template “what language does $y people
speak” to the predicate , our system can parse the
above sentence correctly.
The effect on OOV problem. We found that the
sentence rewriting method can also provide extra

2Our base system yields a slight drop in accuracy com-
pared to the original system (Berant and Liang, 2015), as we
parallelize the learning algorithm, and the order of the data
for updating the parameter is different to theirs.

O Who is willow smith mom name?
R Who is willow smith female parent name?
LF parentOf.willow smith∧gender.female
O Who was king henry viii son?
R Who was king henry viii male child?
LF childOf.king henry∧gender.male
O What are some of the traditions of islam?
R What is of the religion of islam?
LF religionOf.islam
O What does jamaican people speak?
R What language does jamaican people speak?
LF language spoken.jamaica

Table 11: Examples which our system generates
more accurate logical form than the base seman-
tic parser. O is the original sentence; R is the
generated sentence from sentence rewriting (with
the highest score for the model, including rewrit-
ing part and parsing part); LF is the target logical
form.

profit for solving the OOV problem. Traditionally,
if a sentence contains a word which is not covered
by the lexicon, it will cannot be correctly parsed.
However, with the help of sentence rewriting, we
may rewrite the OOV words into the words which
are covered by our lexicons. For example, in Table
11 the 3rd question “What are some of the tradi-
tions of islam?” cannot be correctly parsed as the
lexicons dont cover the word “tradition”. Through
sentence rewriting, we can generate a new sen-
tence “What is of the religion of islam?”, where
all words are covered by the lexicons, in this way
the sentence can be correctly parsed.

5.3 Error Analysis

To better understand our system, we conduct er-
ror analysis on the parse results. Specifically, we
randomly choose 100 questions which are not cor-
rectly answered by our system. We found that the
errors are mainly raised by following four reasons
(See Table 12 for detail):

Reason #(Ratio) Sample Example
Label issue 38 What band was george clintonin?
N-ary predi-
cate(n > 2) 31

What year did the seahawks
win the superbowl?

Temporal
clause 15

Who was the leader of the us
during wwii?

Superlative 8 Who was the first governor ofcolonial south carolina?
Others 8

What is arkansas state
capitol?

Table 12: The main reasons of parsing errors, the
ratio and an example for each reason are also pro-
vided.

773



The first reason is the label issue. The main la-
bel issue is incompleteness, i.e., the answers of a
question may not be labeled completely. For ex-
ample, for the question “Who does nolan ryan play
for?”, our system returns 4 correct teams but the
golden answer only contain 2 teams. One another
label issue is the error labels. For example, the
gold answer of the question “What state is barack
obama from?” is labeled as “Illinois”, however,
the correct answer is “Hawaii”.

The second reason is the n-ary predicate prob-
lem (n > 2). Currently, it is hard for a parser
to conduct the correct logical form of n-ary pred-
icates. For example, the question “What year did
the seahawks win the superbowl?” describes an n-
ary championship event, which gives the champi-
onship and the champion of the event, and expects
the season. We believe that more research atten-
tions should be given on complicated cases, such
as the n-ary predicates parsing.

The third reason is temporal clause. For ex-
ample, the question “Who did nasri play for be-
fore arsenal?” contains a temporal clause “be-
fore”. We found temporal clause is complicated
and makes it strenuous for the parser to understand
the sentence.

The fourth reason is superlative case, which is
a hard problem in semantic parsing. For example,
to answer “What was the name of henry viii first
wife?”, we should choose the first one from a list
ordering by time. Unfortunately, it is difficult for
the current parser to decide what to be ordered and
how to order.

There are also many other miscellaneous error
cases, such as spelling error in the question, e.g.,
“capitol” for “capital”, “mary” for “marry”.

6 Conclusions

In this paper, we present a novel semantic pars-
ing method, which can effectively deal with the
mismatch between natural language and target on-
tology using sentence rewriting. We resolve two
common types of mismatch (i) one word in natu-
ral language sentence vs one compound formula in
target ontology (1-N), (ii) one complicated expres-
sion in natural language sentence vs one formula
in target ontology (N-1). Then we present two sen-
tence rewriting methods, dictionary-based method
for 1-N mismatch and template-based method for
N-1 mismatch. The resulting system significantly
outperforms the base system on the WEBQUES-

TIONS dataset.
Currently, our approach only leverages sim-

ple sentence rewriting methods. In future work,
we will explore more advanced sentence rewrit-
ing methods. Furthermore, we also want to em-
ploy sentence rewriting techniques for other chal-
lenges in semantic parsing, such as the sponta-
neous, unedited natural language input, etc.

Acknowledgments

We sincerely thank the reviewers for their valu-
able comments and suggestions. This work is sup-
ported by the National High Technology Devel-
opment 863 Program of China under Grants no.
2015AA015405, and the National Natural Science
Foundation of China under Grants no. 61433015,
612722324 and 61572477.

References
Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki

Toda, and Satoshi Nakamura. 2015. Semantic pars-
ing of ambiguous input through paraphrasing and
verification. Transactions of the Association for
Computational Linguistics, 3:571–584.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49–62.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage ccg semantic parsing with amr. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
1699–1710, Lisbon, Portugal, September. Associa-
tion for Computational Linguistics.

Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 967–
976, Baltimore, Maryland, June. Association for
Computational Linguistics.

Hannah Bast and Elmar Haussmann. 2015. More ac-
curate question answering on freebase. In Proceed-
ings of the 24th ACM International on Conference
on Information and Knowledge Management, CIKM
2015, Melbourne, VIC, Australia, October 19 - 23,
2015, pages 1431–1440.

Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1415–1425, Baltimore, Maryland, June. Association
for Computational Linguistics.

774



Jonathan Berant and Percy Liang. 2015. Imitation
learning of agenda-based semantic parsers. Trans-
actions of the Association for Computational Lin-
guistics, 3:545–558.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 615–620, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.

Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 423–433,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Qingqing Cai and Alexander Yates. 2013b. Seman-
tic parsing freebase: Towards open-domain seman-
tic parsing. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 1:
Proceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 328–338,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.

Eunsol Choi, Tom Kwiatkowski, and Luke Zettle-
moyer. 2015. Scalable semantic parsing with partial
ontologies. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1311–1320, Beijing, China, July. Association
for Computational Linguistics.

James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 18–27, Uppsala, Sweden,
July. Association for Computational Linguistics.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 531–540, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.

Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1608–1618,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In Proceedings of the
20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’14,
pages 1156–1165, New York, NY, USA. ACM.

Ruifang Ge and Raymond Mooney. 2009. Learning
a compositional semantic parser using an existing
syntactic parser. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 611–619,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.

Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1486–1495, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.

Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu, and
Jun Zhao. 2014. Question answering over linked
data using first-order logic. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1092–
1103, Doha, Qatar, October. Association for Com-
putational Linguistics.

He He, Alvin Grissom II, John Morgan, Jordan Boyd-
Graber, and Hal Daumé III. 2015. Syntax-based
rewriting for simultaneous machine translation. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
55–64, Lisbon, Portugal, September. Association for
Computational Linguistics.

Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 913–920, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.

Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
754–765, Jeju Island, Korea, July. Association for
Computational Linguistics.

775



Jayant Krishnamurthy and Tom M. Mitchell. 2014.
Joint syntactic and semantic parsing with combi-
natory categorial grammar. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1188–1198, Baltimore, Maryland, June. Association
for Computational Linguistics.

Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223–1233, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in ccg grammar induction for semantic parsing.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1512–1523, Edinburgh, Scotland, UK., July. Asso-
ciation for Computational Linguistics.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545–1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.

Junhui Li, Muhua Zhu, Wei Lu, and Guodong Zhou.
2015. Improving semantic parsing with enriched
synchronous context-free grammar. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 1455–1465,
Lisbon, Portugal, September. Association for Com-
putational Linguistics.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590–599, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

P. Liang. 2013. Lambda dependency-based composi-
tional semantics. arXiv preprint arXiv:1309.4408.

Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
783–792, Honolulu, Hawaii, October. Association
for Computational Linguistics.

Ankur P. Parikh, Hoifung Poon, and Kristina
Toutanova. 2015. Grounded semantic parsing for
complex knowledge extraction. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 756–

766, Denver, Colorado, May–June. Association for
Computational Linguistics.

P. Pasupat and P. Liang. 2015. Compositional semantic
parsing on semi-structured tables. In Association for
Computational Linguistics (ACL).

Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10, Singapore, August.
Association for Computational Linguistics.

Hoifung Poon. 2013. Grounded unsupervised se-
mantic parsing. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 933–943,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Siva Reddy, Mirella Lapata, and Mark Steedman.
2014. Large-scale semantic parsing without
question-answer pairs. Transactions of the Associ-
ation for Computational Linguistics, 2:377–392.

Siva Reddy, Oscar Täckström, Michael Collins, Tom
Kwiatkowski, Dipanjan Das, Mark Steedman, and
Mirella Lapata. 2016. Transforming Dependency
Structures to Logical Forms for Semantic Parsing.
Transactions of the Association for Computational
Linguistics, 4:127–140.

Adrienne Wang, Tom Kwiatkowski, and Luke Zettle-
moyer. 2014. Morpho-syntactic lexical generaliza-
tion for ccg semantic parsing. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1284–
1295, Doha, Qatar, October. Association for Com-
putational Linguistics.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1332–1342,
Beijing, China, July. Association for Computational
Linguistics.

Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 960–967, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.

Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-
Chang Rim. 2014. Joint relational embeddings for
knowledge-based question answering. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
645–650, Doha, Qatar, October. Association for
Computational Linguistics.

776



Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
956–966, Baltimore, Maryland, June. Association
for Computational Linguistics.

Xuchen Yao. 2015. Lean question answering over
freebase from scratch. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Demon-
strations, pages 66–70, Denver, Colorado, June. As-
sociation for Computational Linguistics.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 1321–1331, Beijing, China, July. As-
sociation for Computational Linguistics.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, pages 1050–1055,
Portland, OR, August. AAAI Press/MIT Press.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI ’05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence, Ed-
inburgh, Scotland, July 26-29, 2005, pages 658–666.

Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 678–687,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.

Yuanzhe Zhang, Shizhu He, Kang Liu, and Jun Zhao.
2016. A joint model for question answering over
multiple knowledge bases. In Proceedings of the
Thirtieth AAAI Conference on Artificial Intelligence,
February 12-17, 2016, Phoenix, Arizona, USA.,
pages 3094–3100.

777


