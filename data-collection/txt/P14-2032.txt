



















































Two Knives Cut Better Than One: Chinese Word Segmentation with Dual Decomposition


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193–198,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Two Knives Cut Better Than One:
Chinese Word Segmentation with Dual Decomposition

Mengqiu Wang
Computer Science Department

Stanford University
Stanford, CA 94305

Rob Voigt
Linguistics Department

Stanford University
Stanford, CA 94305

{mengqiu,manning}@cs.stanford.edu robvoigt@stanford.edu

Christopher D. Manning
Computer Science Department

Stanford University
Stanford, CA 94305

Abstract

There are two dominant approaches to
Chinese word segmentation: word-based
and character-based models, each with re-
spective strengths. Prior work has shown
that gains in segmentation performance
can be achieved from combining these
two types of models; however, past efforts
have not provided a practical technique
to allow mainstream adoption. We pro-
pose a method that effectively combines
the strength of both segmentation schemes
using an efficient dual-decomposition al-
gorithm for joint inference. Our method
is simple and easy to implement. Ex-
periments on SIGHAN 2003 and 2005
evaluation datasets show that our method
achieves the best reported results to date
on 6 out of 7 datasets.

1 Introduction

Chinese text is written without delimiters between
words; as a result, Chinese word segmentation
(CWS) is an essential foundational step for many
tasks in Chinese natural language processing. As
demonstrated by (Shi and Wang, 2007; Bai et
al., 2008; Chang et al., 2008; Kummerfeld et al.,
2013), the quality and consistency of segmentation
has important downstream impacts on system per-
formance in machine translation, POS tagging and
parsing.

State-of-the-art performance in CWS is high,
with F-scores in the upper 90s. Still, challenges
remain. Unknown words, also known as out-of-
vocabulary (OOV) words, lead to difficulties for
word- or dictionary-based approaches. Ambiguity
can cause errors when the appropriate segmenta-
tion is determined contextually, such as才能 (“tal-
ent”) and才 /能 (“just able”) (Gao et al., 2003).

There are two primary classes of models:
character-based, where the foundational units for

processing are individual Chinese characters (Xue,
2003; Tseng et al., 2005; Zhang et al., 2006;
Wang et al., 2010), and word-based, where the
units are full words based on some dictionary or
training lexicon (Andrew, 2006; Zhang and Clark,
2007). Sun (2010) details their respective theo-
retical strengths: character-based approaches bet-
ter model the internal compositional structure of
words and are therefore more effective at inducing
new OOV words; word-based approaches are bet-
ter at reproducing the words of the training lexi-
con and can capture information from significantly
larger contextual spans. Prior work has shown per-
formance gains from combining these two types
of models to exploit their respective strengths, but
such approaches are often complex to implement
and computationally expensive.

In this work, we propose a simple and prin-
cipled joint decoding method for combining
character-based and word-based segmenters based
on dual decomposition. This method has strong
optimality guarantees and works very well empir-
ically. It is easy to implement and does not re-
quire retraining of existing character- and word-
based segmenters. Perhaps most importantly, this
work presents a much more practical and usable
form of classifier combination in the CWS context
than existing methods offer.

Experimental results on standard SIGHAN
2003 and 2005 bake-off evaluations show that our
model outperforms the character and word base-
lines by a significant margin. In particular, out
approach improves OOV recall rates and segmen-
tation consistency, and gives the best reported re-
sults to date on 6 out of 7 datasets.

2 Models for CWS

Here we describe the character-based and word-
based models we use as baselines, review existing
approaches to combination, and describe our algo-
rithm for joint decoding with dual decomposition.

193



2.1 Character-based Models

In the most commonly used contemporary ap-
proach to character-based segmentation, first pro-
posed by (Xue, 2003), CWS is seen as a charac-
ter sequence tagging task, where each character
is tagged on whether it is at the beginning, mid-
dle, or end of a word. Conditional random fields
(CRF) (Lafferty et al., 2001) have been widely
adopted for this task, and give state-of-the-art re-
sults (Tseng et al., 2005). In a first-order linear-
chain CRF model, the conditional probability of a
label sequence y given a word sequence x is de-
fined as:

P (y|x) = 1
Z

|y|∑
t=1

exp (θ · f(x, yt, yt+1))

f(x, yt, yt−1) are feature functions that typically
include surrounding character n-gram and mor-
phological suffix/prefix features. These types of
features capture the compositional properties of
characters and are likely to generalize well to un-
known words. However, the Markov assumption
in CRF limits the context of such features; it is
difficult to capture long-range word features in this
model.

2.2 Word-based Models

Word-based models search through lists of word
candidates using scoring functions that directly
assign scores to each. Early word-based seg-
mentation work employed simple heuristics like
dictionary-lookup maximum matching (Chen and
Liu, 1992). More recently, Zhang and Clark
(2007) reported success using a linear model
trained with the average perceptron algorithm
(Collins, 2002). Formally, given input x, their
model seeks a segmentation y such that:

F (y|x) = max
y∈GEN(x)

(α · φ(y))

F (y|x) is the score of segmentation result y.
Searching through the entire GEN(x) space is
intractable even with a local model, so a beam-
search algorithm is used. The search algorithm
consumes one character input token at a time, and
iterates through the existing beams to score two
new alternative hypotheses by either appending
the new character to the last word in the beam, or
starting a new word at the current position.

Algorithm 1 Dual decomposition inference algo-
rithm, and modified Viterbi and beam-search algo-
rithms.
∀i ∈ {1 to |x|} : ∀k ∈ {0, 1} : ui(k) = 0
for t← 1 to T do

yc∗ = argmax
y

P (yc|x) + ∑
i∈|x|

ui(y
c
i)

yw∗ = argmax
y∈GEN(x)

F (yw|x)− ∑
j∈|x|

uj(y
w
j )

if yc∗ = yw∗ then
return (yc∗,yw∗)

end if
for all i ∈ {1 to |x|} do
∀k ∈ {0, 1} : ui(k) = ui(k) + αt(2k − 1)(yw∗i −
yc∗i )

end for
end for
return (yc∗,yw∗)

Viterbi:
V1(1) = 1, V1(0) = 0
for i = 2 to |x| do
∀k ∈ {0, 1} : Vi(k) = argmax

k′
Pi(k|k′)Vi−1k′ +

ui(k)
end for

Beam-Search:
for i = 1 to |x| do

for item v = {w0, · · · , wj} in beam(i) do
append xi to wj , score(v)

+
= ui(0)

v = {w0, · · · , wj , xi}, score(v) += ui(1)
end for

end for

2.3 Combining Models with Dual
Decomposition

Various mixing approaches have been proposed to
combine the above two approaches (Wang et al.,
2006; Lin, 2009; Sun et al., 2009; Sun, 2010;
Wang et al., 2010). These mixing models perform
well on standard datasets, but are not in wide use
because of their high computational costs and dif-
ficulty of implementation.

Dual decomposition (DD) (Rush et al., 2010)
offers an attractive framework for combining these
two types of models without incurring high costs
in model complexity (in contrast to (Sun et al.,
2009)) or decoding efficiency (in contrast to bag-
ging in (Wang et al., 2006; Sun, 2010)). DD has
been successfully applied to similar situations for
combining local with global models; for example,
in dependency parsing (Koo et al., 2010), bilingual
sequence tagging (Wang et al., 2013) and word
alignment (DeNero and Macherey, 2011).

The idea is that jointly modelling both
character-sequence and word information can be
computationally challenging, so instead we can try
to find outputs that the two models are most likely

194



Academia Sinica Peking Univ.
R P F1 Roov C R P F1 Roov C

Char-based CRF 95.2 93.6 94.4 58.9 0.064 94.6 95.3 94.9 77.8 0.089
Word-based Perceptron 95.8 95.0 95.4 69.5 0.060 94.1 95.5 94.8 76.7 0.099

Dual-decomp 95.9 94.9 95.4 67.7 0.055 94.8 95.7 95.3 78.7 0.086

City Univ. of Hong Kong Microsoft Research
R P F1 Roov C R P F1 Roov C

Char-based CRF 94.7 94.0 94.3 76.1 0.065 96.4 96.6 96.5 71.3 0.074
Word-based Perceptron 94.3 94.0 94.2 71.7 0.073 97.0 97.2 97.1 74.6 0.063

Dual-decomp 95.0 94.4 94.7 75.3 0.062 97.3 97.4 97.4 76.0 0.055

Table 1: Results on SIGHAN 2005 datasets. Roov denotes OOV recall, and C denotes segmentation
consistency. Best number in each column is highlighted in bold.

to agree on. Formally, the objective of DD is:

max
yc,yw

P (yc|x) + F (yw|x) s.t. yc = yw (1)

where yc is the output of character-based CRF, yw

is the output of word-based perceptron, and the
agreements are expressed as constraints. s.t. is
a shorthand for “such that”.

Solving this constrained optimization problem
directly is difficult. Instead, we take the La-
grangian relaxation of this term as:

L (yc,yw,U) = (2)

P (yc|x) + F (yw|x) +
∑
i∈|x|

ui(yci − ywi )

where U is the set of Lagrangian multipliers that
consists of a multiplier ui at each word position i.

We can rewrite the original objective with the
Lagrangian relaxation as:

max
yc,yw

min
U

L (yc,yw,U) (3)

We can then form the dual of this problem by
taking the min outside of the max, which is an up-
per bound on the original problem. The dual form
can then be decomposed into two sub-components
(the two max problems in Eq. 4), each of which is
local with respect to the set of Lagrangian multi-
pliers:

min
U

(
max

yc

P (yc|x) + ∑
i∈|x|

ui(yci )

 (4)
+ max

yw

F (yw|x)−∑
j∈|x|

uj(ywj )

)

This method is called dual decomposition (DD)
(Rush et al., 2010). Similar to previous work

(Rush and Collins, 2012), we solve this DD prob-
lem by iteratively updating the sub-gradient as de-
picted in Algorithm 1.1 In each iteration, if the
best segmentations provided by the two models do
not agree, then the two models will receive penal-
ties for the decisions they made that differ from the
other. This penalty exchange is similar to message
passing, and as the penalty accumulates over itera-
tions, the two models are pushed towards agreeing
with each other. We also give an updated Viterbi
decoding algorithm for CRF and a modified beam-
search algorithm for perceptron in Algorithm 1. T
is the maximum number of iterations before early
stopping, and αt is the learning rate at time t. We
adopt a learning rate update rule from Koo et al.
(2010) where αt is defined as 1N , where N is the
number of times we observed a consecutive dual
value increase from iteration 1 to t.

3 Experiments

We conduct experiments on the SIGHAN 2003
(Sproat and Emerson, 2003) and 2005 (Emer-
son, 2005) bake-off datasets to evaluate the ef-
fectiveness of the proposed dual decomposition
algorithm. We use the publicly available Stan-
ford CRF segmenter (Tseng et al., 2005)2 as our
character-based baseline model, and reproduce
the perceptron-based segmenter from Zhang and
Clark (2007) as our word-based baseline model.

We adopted the development setting from
(Zhang and Clark, 2007), and used CTB sections
1-270 for training and sections 400-931 for devel-
opment in hyper-parameter setting; for all results
given in tables, the models are trained and eval-
uated on the standard train/test split for the given
dataset. The optimized hyper-parameters used are:

1See Rush and Collins (2012) for a full introduction to
DD.

2http://nlp.stanford.edu/software/segmenter.shtml

195



`2 regularization parameter λ in CRF is set to
3; the perceptron is trained for 10 iterations with
beam size 200; dual decomposition is run to max
iteration of 100 (T in Algo. 1) with step size 0.1
(αt in Algo. 1).

Beyond standard precision (P), recall (R) and
F1 scores, we also evaluate segmentation consis-
tency as proposed by (Chang et al., 2008), who
have shown that increased segmentation consis-
tency is correlated with better machine transla-
tion performance. The consistency measure cal-
culates the entropy of segmentation variations —
the lower the score the better. We also report
out-of-vocabulary recall (Roov) as an estimation of
the model’s generalizability to previously unseen
words.

4 Results

Table 1 shows our empirical results on SIGHAN
2005 dataset. Our dual decomposition method
outperforms both the word-based and character-
based baselines consistently across all four sub-
sets in both F1 and OOV recall (Roov). Our
method demonstrates a robustness across domains
and segmentation standards regardless of which
baseline model was stronger. Of particular note
is DD’s is much more robust in Roov, where the
two baselines swing a lot. This is an important
property for downstream applications such as en-
tity recognition. The DD algorithm is also more
consistent, which would likely lead to improve-
ments in applications such as machine translation
(Chang et al., 2008).

The improvement over our word- and character-
based baselines is also seen in our results on the
earlier SIGHAN 2003 dataset. Table 2 puts our
method in the context of earlier systems for CWS.
Our method achieves the best reported score on 6
out of 7 datasets.

5 Discussion and Error Analysis

On the whole, dual decomposition produces state-
of-the-art segmentations that are more accurate,
more consistent, and more successful at induc-
ing OOV words than the baseline systems that it
combines. On the SIGHAN 2005 test set, in
over 99.1% of cases the DD algorithm converged
within 100 iterations, which gives an optimality
guarantee. In 77.4% of the cases, DD converged
in the first iteration. The number of iterations to
convergence histogram is plotted in Figure 1.

SIGHAN 2005
AS PU CU MSR

Best 05 95.2 95.0 94.3 96.4
Zhang et al. 06 94.7 94.5 94.6 96.4
Z&C 07 94.6 94.5 95.1 97.2
Sun et al. 09 - 95.2 94.6 97.3
Sun 10 95.2 95.2 95.6 96.9
Dual-decomp 95.4 95.3 94.7 97.4

SIGHAN 2003
Best 03 96.1 95.1 94.0
Peng et al. 04 95.6 94.1 92.8
Z&C 07 96.5 94.0 94.6
Dual-decomp 97.1 95.4 94.9

Table 2: Performance of dual decomposition in
comparison to past published results on SIGHAN
2003 and 2005 datasets. Best reported F1 score
for each dataset is highlighted in bold. Z&C 07
refers to Zhang and Clark (2007). Best 03, 05 are
results of the winning systems for each dataset in
the respective shared tasks.

Error analysis In many cases the relative con-
fidence of each model means that dual decom-
position is capable of using information from
both sources to generate a series of correct
segmentations better than either baseline model
alone. The example below shows a difficult-to-
segment proper name comprised of common char-
acters, which results in undersegmentation by the
character-based CRF and oversegmentation by the
word-based perceptron, but our method achieves
the correct middle ground.

Gloss Tian Yage / ’s / creations
Gold 田雅各 /的 /创作
CRF 田雅各的 /创作
PCPT 田雅 /各 /的 /创作
DD 田雅各 /的 /创作

A powerful feature of the dual decomposition
approach is that it can generate correct segmenta-
tion decisions in cases where a voting or product-
of-experts model could not, since joint decod-
ing allows the sharing of information at decod-
ing time. In the following example, both baseline
models miss the contextually clear use of the word
点心 (“sweets / snack food”) and instead attach点
to the prior word to produce the otherwise com-
mon compound 一点点 (“a little bit”); dual de-
composition allows the model to generate the cor-
rect segmentation.

Gloss Enjoy / a bit of / snack food / , ...
Gold 享受 /一点 /点心 /，
CRF 享受 /一点点 /心 /，
PCPT 享受 /一点点 /心 /，
DD 享受 /一点 /点心 /，

196



Figure 1: No. of iterations till DD convergence.

We found more than 400 such surprisingly ac-
curate instances in our dual decomposition output.

Finally, since dual decomposition is a method of
joint decoding, it is still liable to reproduce errors
made by the constituent systems.

6 Conclusion

In this paper we presented an approach to Chinese
word segmentation using dual decomposition for
system combination. We demonstrated that this
method allows for joint decoding of existing CWS
systems that is more accurate and consistent than
either system alone, and further achieves the best
performance reported to date on standard datasets
for the task. Perhaps most importantly, our ap-
proach is straightforward to implement and does
not require retraining of the underlying segmenta-
tion models used. This suggests its potential for
broader applicability in real-world settings than
existing approaches to combining character-based
and word-based models for Chinese word segmen-
tation.

Acknowledgements

We gratefully acknowledge the support of the U.S.
Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of DARPA, or
the US government.

References
Galen Andrew. 2006. A hybrid Markov/semi-Markov

conditional random field for sequence segmentation.
In Proceedings of EMNLP.

Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting chi-
nese word segmentation. In Proceedings of the third
International Joint Conference on Natural Lan-
guage Processing (IJCNLP).

Pichuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In Proceedings of
the ACL Workshop on Statistical Machine Transla-
tion.

Keh-Jiann Chen and Shing-Huan Liu. 1992. Word
identification for mandarin chinese sentences. In
Proceedings of COLING.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.

John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL.

Thomas Emerson. 2005. The second international
Chinese word segmentation bakeoff. In Proceed-
ings of the fourth SIGHAN workshop on Chinese
language Processing.

Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003.
Improved source-channel models for Chinese word
segmentation. In Proceedings of ACL.

Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP.

Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-
ran, and Dan Klein. 2013. An empirical examina-
tion of challenges in chinese parsing. In Proceed-
ings of ACL-Short.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of 18th International
Conference on Machine Learning (ICML).

Dekang Lin. 2009. Combining language modeling and
discriminative classification for word segmentation.
In Proceedings of the 10th International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing).

Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and Lagrangian relax-
ation for inference in natural language processing.
JAIR, 45:305–362.

197



Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.

Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of
Joint Conferences on Artificial Intelligence (IJCAI).

Richard Sproat and Thomas Emerson. 2003. The
first international Chinese word segmentation bake-
off. In Proceedings of the second SIGHAN work-
shop on Chinese language Processing.

Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of HLT-NAACL.

Weiwei Sun. 2010. Word-based and character-
basedword segmentation models: Comparison and
combination. In Proceedings of COLING.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurasfky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing.

Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop on Chi-
nese language Processing.

Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLING.

Mengqiu Wang, Wanxiang Che, and Christopher D.
Manning. 2013. Joint word alignment and bilingual
named entity recognition using dual decomposition.
In Proceedings of ACL.

Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, pages 29–48.

Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of ACL.

Ruiqiang Zhang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based tagging by condi-
tional random fields for Chinese word segmentation.
In Proceedings of HLT-NAACL.

198


