454

Coling 2010: Poster Volume, pages 454–462,

Beijing, August 2010

Dimensions of Sense Disambiguation for an Interesting Word Class

What’s in a Preposition?

Dirk Hovy, Stephen Tratz, and Eduard Hovy

Information Sciences Institute

University of Southern California
{dirkh, stratz, hovy}@isi.edu

Abstract

Choosing the right parameters for a word
sense disambiguation task is critical to
the success of the experiments. We ex-
plore this idea for prepositions, an of-
ten overlooked word class. We examine
the parameters that must be considered in
preposition disambiguation, namely con-
text, features, and granularity. Doing
so delivers an increased performance that
signiﬁcantly improves over two state-of-
the-art systems, and shows potential for
improving other word sense disambigua-
tion tasks. We report accuracies of 91.8%
and 84.8% for coarse and ﬁne-grained
preposition sense disambiguation, respec-
tively.

Introduction

1
Ambiguity is one of the central topics in NLP. A
substantial amount of work has been devoted to
disambiguating prepositional attachment, words,
and names. Prepositions, as with most other word
types, are ambiguous. For example, the word in
can assume both temporal (“in May”) and spatial
(“in the US”) meanings, as well as others, less
easily classiﬁable (“in that vein”). Prepositions
typically have more senses than nouns or verbs
(Litkowski and Hargraves, 2005), making them
difﬁcult to disambiguate.

Preposition sense disambiguation (PSD) has
many potential uses. For example, due to the
relational nature of prepositions, disambiguating
their senses can help with all-word sense disam-
biguation. In machine translation, different senses
of the same English preposition often correspond

to different translations in the foreign language.
Thus, disambiguating prepositions correctly may
help improve translation quality.1 Coarse-grained
PSD can also be valuable for information extrac-
tion, where the sense acts as a label. In a recent
study, Hwang et al. (2010) identiﬁed preposition
related features, among them the coarse-grained
PP labels used here, as the most informative fea-
ture in identifying caused-motion constructions.
Understanding the constraints that hold for prepo-
sitional constructions could help improve PP at-
tachment in parsing, one of the most frequent
sources of parse errors.

Several papers have successfully addressed
PSD with a variety of different approaches (Rudz-
icz and Mokhov, 2003; O’Hara and Wiebe, 2003;
Ye and Baldwin, 2007; O’Hara and Wiebe, 2009;
Tratz and Hovy, 2009). However, while it is often
possible to increase accuracy by using a differ-
ent classiﬁer and/or more features, adding more
features creates two problems: a) it can lead to
overﬁtting, and b) while possibly improving ac-
curacy, it is not always clear where this improve-
ment comes from and which features are actually
informative. While parameter studies exist for
general word sense disambiguation (WSD) tasks
(Yarowsky and Florian, 2002), and PSD accuracy
has been steadily increasing, there has been no
exploration of the parameters of prepositions to
guide engineering decisions.

We go beyond simply improving accuracy to
analyze various parameters in order to determine
which ones are actually informative. We explore
the different options for context and feature se-
1See (Chan et al., 2007) for the relevance of word sense
disambiguation and (Chiang et al., 2009) for the role of
prepositions in MT.

455

lection, the inﬂuence of different preprocessing
methods, and different levels of sense granular-
ity. Using the resulting parameters in a Maximum
Entropy classiﬁer, we are able to improve signif-
icantly over existing results. The general outline
we present can potentially be extended to other
word classes and improve WSD in general.

2 Related Work
Rudzicz and Mokhov (2003) use syntactic and
lexical features from the governor and the preposi-
tion itself in coarse-grained PP classiﬁcation with
decision heuristics. They reach an average F-
measure of 89% for four classes. This shows that
using a very small context can be effective. How-
ever, they did not include the object of the prepo-
sition and used only lexical features for classiﬁ-
cation. Their results vary widely for the different
classes.

O’Hara and Wiebe (2003) made use of a win-
dow size of ﬁve words and features from the
Penn Treebank (PTB) (Marcus et al., 1993) and
FrameNet (Baker et al., 1998) to classify prepo-
sitions. They show that using high level fea-
tures, such as semantic roles, signiﬁcantly aid dis-
ambiguation. They caution that using colloca-
tions and neighboring words indiscriminately may
yield high accuracy, but has the risk of overﬁt-
ting. O’Hara and Wiebe (2009) show compar-
isons of various semantic repositories as labels for
PSD approaches. They also provide some results
for PTB-based coarse-grained senses, using a ﬁve-
word window for lexical and hypernym features in
a decision tree classiﬁer.

SemEval 2007 (Litkowski and Hargraves,
2007) included a task for ﬁne-grained PSD (more
than 290 senses). The best participating system,
that of Ye and Baldwin (2007), extracted part-of-
speech and WordNet (Fellbaum, 1998) features
using a word window of seven words in a Max-
imum Entropy classiﬁer. Tratz and Hovy (2009)
present a higher-performing system using a set of
20 positions that are syntactically related to the
preposition instead of a ﬁxed window size.

Though using a variety of different extraction
methods, contexts, and feature words, none of
these approaches explores the optimal conﬁgura-
tions for PSD.

3 Theoretical Background

The following parameters are applicable to other
word classes as well. We will demonstrate their
effectiveness for prepositions.

Analyzing the syntactic elements of preposi-
tional phrases, one discovers three recurring ele-
ments that exhibit syntactic dependencies and de-
ﬁne a prepositional phrase. The ﬁrst one is the
governing word (usually a noun, verb, or adjec-
tive)2, the preposition itself, and the object of the
preposition.

Prepositional phrases can be fronted (“In May,
prices dropped by 5%”), so that the governor (in
this case the verb “drop”) occurs later in the sen-
tence. Similarly, the object can be fronted (con-
sider “a dessert to die for”).

In the simplest version, we can do classiﬁcation
based only on the preposition and the governor or
object alone.3 Furthermore, directly neighboring
words can inﬂuence the preposition, mostly two-
word prepositions such as “out of” or “because
of”.

To extract the words discussed above, one can
either employ a ﬁxed window size, (which has
to be large enough to capture the words), or se-
lect them based on heuristics or parsing informa-
tion. The governor and object can be hard to ex-
tract if they are fronted, since they do not occur in
their unusual positions relative to the preposition.
While syntactically related words improve over
ﬁxed-window-size approaches (Tratz and Hovy,
2009), it is not clear which words contribute most.
There should be an optimal context, i.e., the small-
est set of words that achieves the best accuracy. It
has to be large enough to capture all relevant infor-
mation, but small enough to avoid noise words.4
We surmise that earlier approaches were not uti-
lizing that optimal context, but rather include a lot
of noise.

Depending on the task, different levels of sense
granularity may be used. Fewer senses increase
the likelihood of correct classiﬁcation, but may in-

2We will refer to the governing word, irrespective of

class, as governor.

3Basing classiﬁcation on the preposition alone is not fea-

sible, because of the very polysemy we try to resolve.

4It is not obvious how much information a sister-PP can

provide, or the subject of the superordinate clause.

456

correctly conﬂate prepositions. A ﬁner granular-
ity can help distinguish nuances and better ﬁt the
different contexts. However, it might suffer from
sparse data.

4 Experimental Setup

We explore the different context types (ﬁxed win-
dow size vs. selective), the inﬂuence of the words
in that context, and the preprocessing method
(heuristics vs. parsing) on both coarse and ﬁne-
grained disambiguation. We use a most-frequent-
sense baseline.
In addition, we compare to the
state-of-the-art systems for both types of granu-
larity (O’Hara and Wiebe, 2009; Tratz and Hovy,
2009). Their results show what has been achieved
so far in terms of accuracy, and serve as a second
measure for comparison beyond the baseline.

4.1 Model
We use the MALLET implementation (McCal-
lum, 2002) of a Maximum Entropy classiﬁer
(Berger et al., 1996) to construct our models. This
classiﬁer was also used by two state-of-the-art
systems (Ye and Baldwin, 2007; Tratz and Hovy,
2009). For ﬁne-grained PSD, we train a separate
model for each preposition due to the high num-
ber of possible classes for each individual prepo-
sition. For coarse-grained PSD, we use a single
model for all prepositions, because they all share
the same classes.

4.2 Data
We use two different data sets from existing re-
sources for coarse and ﬁne-grained PSD to make
our results as comparable to previous work as pos-
sible.

For the coarse-grained disambiguation, we use
data from the POS tagged version of the Wall
Street Journal (WSJ) section of the Penn Tree-
Bank. A subset of the prepositional phrases in
this corpus is labelled with a set of seven classes:
beneﬁcial (BNF), direction (DIR), extent (EXT),
location (LOC), manner (MNR), purpose (PRP),
and temporal (TMP). We extract only those prepo-
sitions that head a PP labelled with such a class
(N = 35, 917). The distribution of classes is
highly skewed (cf. Figure 1). We compare the

Figure 1: Distribution of Class Labels in the WSJ
Section of the Penn TreeBank.

results of this task to the ﬁndings of O’Hara and
Wiebe (2009).

For the ﬁne-grained task, we use data from
the SemEval 2007 workshop (Litkowski and Har-
graves, 2007), separate XML ﬁles for the 34 most
frequent English prepositions, comprising 16, 557
training and 8096 test sentences, each instance
containing one example of the respective prepo-
sition. Each preposition has between two and 25
senses (9.76 on average) as deﬁned by The Prepo-
sition Project (Litkowski and Hargraves, 2005).
We compare our results directly to the ﬁndings
from Tratz and Hovy (2009). As in the original
workshop task, we train and test on separate sets.

5 Results
In this section we show experimental results for
the inﬂuence of word extraction method (parsing
vs. POS-based heuristics), context, and feature se-
lection on accuracy. Each section compares the
results for both coarse and ﬁne-grained granular-
ity. Accuracy for the coarse-grained task is in all
experiments higher than for the ﬁne-grained one.

5.1 Word Extraction
In order to analyze the impact of the extraction
method, we compare parsing versus POS-based
heuristics for word extraction.

Both O’Hara and Wiebe (2009) and Tratz and
Hovy (2009) use constituency parsers to prepro-
cess the data. However, parsing accuracy varies,

18000 16995

PTB class distrib

y
c
n
e
u
q
e
r
f

16000

14000

12000

10000

8000

6000

4000

2000

0

10332

5414

1781

1071

280

44

LOC TMP DIR MNR PRP EXT BNF

classes

457

and the problem of PP attachment ambiguity in-
creases the likelihood of wrong extractions. This
is especially troublesome in the present case,
where we focus on prepositions.5 We use the
MALT parser (Nivre et al., 2007), a state-of-the-
art dependency parser, to extract the governor and
object.

The alternative is a POS-based heuristics ap-
proach. The only preprocessing step needed is
POS tagging of the data, for which we used the
system of Shen et al. (2007). We then use simple
heuristics to locate the prepositions and their re-
lated words. In order to determine the governor
in the absence of constituent phrases, we consider
the possible governing noun, verb, and adjective.
The object of the preposition is extracted as ﬁrst
noun phrase head to the right. This approach is
faster than parsing, but has problems with long-
range dependencies and fronting of the PP (e.g.,
the PP appearing earlier in the sentence than its
governor).

Table 1: Accuracies (%) for Word-Extraction Us-
ing MALT Parser or Heuristics.

Interestingly, the extraction method does not
signiﬁcantly affect the ﬁnal score for ﬁne-grained
PSD (see Table 1). The high score achieved when
using the MALT parse for coarse-grained PSD
can be explained by the fact that the parser was
originally trained on that data set. The good re-
sults we see when using heuristics-based extrac-
tion only, however, means we can achieve high-
accuracy PSD even without parsing.

5.2 Context
We compare the effects of ﬁxed window size ver-
sus syntactically related words as context. Table 2
shows the results for the different types and sizes
of contexts.6

5Rudzicz and Mokhov (2003) actually motivate their
work as a means to achieve better PP attachment resolution.
6See also (Yarowsky and Florian, 2002) for experiments

on the effect of varying window size for WSD.

Table 2: Accuracies (%) for Different Context
Types and Sizes

The results show that the approach using both
governor and object is the most accurate one. Of
the ﬁxed-window-size approaches, three words to
either side works best. This does not necessarily
reﬂect a general property of that window size, but
can be explained by the fact that most governors
and objects occur within this window size.7 This
distance can vary from corpus to corpus, so win-
dow size would have to be determined individu-
ally for each task. The difference between using
governor and preposition versus preposition and
object between coarse and ﬁne-grained classiﬁca-
tion might reﬂect the annotation process: while
Litkowski and Hargraves (2007) selected exam-
ples based on a search for governors8, most anno-
tators in the PTB may have based their decision
of the PP label on the object that occurs in it. We
conclude that syntactically related words present a
better context for classiﬁcation than ﬁxed window
sizes.

5.3 Features
Having established the context we want to use, we
now turn to the details of extracting the feature
words from that context.9 Using higher-level fea-
tures instead of lexical ones helps accounting for
sparse training data (given an inﬁnite amount of
data, we would not need to take any higher-level

7Based on such statistics, O’Hara and Wiebe (2003) ac-

tually set their window size to 5.

8Personal communication.
9As one reviewer pointed out, these two dimensions are
highly interrelated and inﬂuence each other. To examine the
effects, we keep one dimension constant while varying the
other.

word selection

extraction method

fine

coarse

MALT

Heuristics

MALT + Heuristics

84.4

84.8

84.8

94.0

90.9

91.8

context

Context

coarse

fine

2-word window

3-word window

4-word window

5-word window

Governor, prep

Prep, object

Governor, prep, object

91.6

92.0

91.6

91.0

80.7

94.2

94.0

80.4

81.4

79.8

78.7

78.9

56.9

84.8

458

features into account, since every case would be
covered). Compare O’Hara and Wiebe (2009).

Following the prepocessing, we use a set of
rules to select the feature words, and then gen-
erate feature values from them using a variety
of feature-generating functions.10
The word-
selection rules are listed below.

Word-Selection Rules

sition

verb/noun/adjective to the left)

• Governor from the MALT parse
• Object from the MALT parse
• Heuristically determined object of the prepo-
• First verb to the left of the preposition
• First verb/noun/adjective to the left of the
preposition
• Union of
(First verb to the left, First
• First word to the left
The feature-generating functions, many of
which utilize WordNet (Fellbaum, 1998), are
listed below. To conserve space, curly braces are
used to represent multiple functions in a single
line. The name of each feature is the combination
of the word-selection rule and the output from the
feature-generating function.

WordNet-based Features

word

sense(s) of the word

associated with the word

• {Hypernyms, Synonyms} for {1st, all}
• All terms in the deﬁnitions (‘glosses’) of the
• Lexicographer ﬁle names for the word
• Lists of all link types (e.g., meronym links)
• Part-of-speech indicators for the existence of
NN/VB/JJ/RB entries for the word
• All sentence frames for the word
• All {part, member, substance}-of holonyms
• All sentence frames for the word

for the word

Other Features

• Indicator that the word-ﬁnding rule found a

word

10Some words may be selected by multiple word-selection
rules. For example, the governor of the preposition may
be identiﬁed by the Governor from MALT parse rule, ﬁrst
noun/verb/adjective to left, and the ﬁrst word to the left rule.

• Capitalization indicator
• {Lemma, surface form} of the word
• Part-of-speech tag for the word
• General POS tag for the word (e.g. NNS →
• The {ﬁrst, last} {two, three} letters of each
de-
• Indicators
adjectival,
[non]agentive,
de-verbal [non]agentive)

NN, VBZ → VB)
word

sufﬁx types

de-nominal

(e.g.,

for

• Indicators for a wide variety of other afﬁxes
including those related to degree, number, or-
der, etc. (e.g., ultra-, poly-, post-)

• Roget’s Thesaurus divisions for the word
To establish the impact of each feature word on
the outcome, we use leave-one-out and only-one
evaluation.11 The results can be found in Table 3.
A word that does not perform well as the only at-
tribute may still be important in conjunction with
others. Conversely, leaving out a word may not
hurt performance, despite being a good single at-
tribute.

(%)

Accuracies

Table 3:
for Leave-One-
Out (LOO) and Only-One Word-Extraction-Rule
Evaluation. none includes all words and serves for
comparison. Important words reduce accuracy for
LOO, but rank high when used as only rule.

Independent of the extraction method (MALT
parser or POS-based heuristics), the governor is
the most informative word. Combining several
heuristics to locate the governor is the best sin-
gle feature for ﬁne-grained classiﬁcation. The rule
looking only for a governing verb fails to account
11Since the feature words are not independent of one an-

other, neither of the two measures is decisive on its own.

word selection

Word

LOO

Only

LOO

Only

coarse

fine

MALT governor

MALT object

Heuristics VB to left

Heur. NN/VB/ADJ to left

Heur. Governor Union

Heuristics word to left

Heuristics object

none

92.1

93.4

92.0

92.1

92.1

92.0

91.9

91.8

80.1

94.2

77.9

78.7

78.4

78.8

93.0

–

84.3

84.9

85.0

84.3

84.5

84.4

84.9

84.8

78.9

56.3

62.1

78.5

81.0

77.2

56.8

–

459

Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by
preposition.

for noun governors, which consequently leads to
a slight improvement when left out.

creases accuracy, which implies that their infor-
mation can be covered by other words.

Curiously, the word directly to the left is a bet-
ter single feature than the object (for ﬁne-grained
classiﬁcation). Leaving either of them out in-

fine

coarse

fine

coarse

Prep

Total

Acc

Total

Acc

Prep

Total

Acc

Total

Acc

full both

6 100.0

125

90.4

aboard

about

above

across

after

against

along

alongside

amid

among

amongst

around

as

astride

at

atop

because

before

behind

below

beneath

beside

besides

between

beyond

by

down

during

except

for

from

in

inside

into

–

94.0

69.6

96.7

79.2

92.4

96.0

–

–

–

5

78

87

841

16

45

5

58

1

107

232

2

80.0

65.4

79.3

92.5

43.8

71.1

80.0

70.7

93.9

0.0

86.0

84.5

50.0

92.0

50

80.0

358

155

69.0

84 100.0

–

–

367

86.4

3078

–

–

90.0

77.9

–

20

68

28

78.6

5 100.0

420

384

65

94

11

91.7

83.3

87.7

71.3

72.7

102

94.1

–

–

1

98

45

–

364

23

151

53

92

173

–

–

–

–

–

–

–

–

248

153

39

–

478

578

688

38

297

like

near

nearest

next

of

off

on

onto

out

outside

over

past

per

round

since

than

–

–

–

1478

76

441

58

–

–

–

–

–

–

–

–

–

87.9

84.2

81.4

91.4

–

–

–

–

–

–

98

79.6

417

82

65.9

–

–

53

74

1

7

71

28

2287

15

90

62

47.2

93.2

0.0

71.4

64.8

75.0

90.8

53.3

68.9

90.3

89.4

83.3

6

3 100.0

449

94.4

2

364

62

0.0

69.0

93.5

3 100.0

97.5

65.5

through

208

48.1

throughout

till

to

–

–

–

–

572

89.7

3166

toward

–

–

55

29 100.0

4 100.0

towards

102

97.1

2 100.0

0.0

84.7

64.4

87.5

56.2

92.1

0.0

84.5

90.5

95.0

91.7

80.0

under

underneath

until

up

upon

via

whether

while

with

within

without

–

–

–

–

–

–

–

–

–

–

–

–

–

–

–

–

578

84.4

–

–

–

–

604

2

208

20

23

22

91.4

50.0

94.2

75.0

73.9

40.9

1 100.0

3

272

213

69

33.3

69.5

96.2

63.8

Overall

8096

84.8 35917

91.8

88.3

1341

81.7

87.2

–

16

547

1

82.4

1455

85.5

1712

77.0 15706

73.7

86.2

24

415

460

Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classiﬁcation. Comparison to O’Hara
and Wiebe (2009). Classes ordered by frequency

5.4 Comparison with Related Work
To situate our experimental results within the
body of work on PSD, we compare them to both
a most-frequent-sense baseline and existing work
for both granularities (see Table 6). The results
use a syntactically selective context of preposi-
tion, governor, object, and word to the left as
determined by combined extraction information
(POS tagging and parsing).

Table 6: Accuracies (%) for Different Classiﬁ-
cations. Comparison with O’Hara and Wiebe
(2009)*, and Tratz and Hovy (2009)**.

Our system easily exceeds the baseline for both
coarse and ﬁne-grained PSD (see Table 6). Com-
parison with related work shows that we achieve
an improvement of 6.5% over Tratz and Hovy
(2009), which is signiﬁcant at p < .0001, and
of 4.5% over O’Hara and Wiebe (2009), which is
signiﬁcant at p < .0001.

A detailed overview over all prepositions for
frequencies and accuracies of both coarse and
ﬁne-grained PSD can be found in Table 4.

In addition to overall accuracy, O’Hara and
Wiebe (2009) also measure precision, recall and
F-measure for the different classes. They omitted
BNF because it is so infrequent. Due to different
training data and models, the two systems are not

strictly comparable, yet they provide a sense of
the general task difﬁculty. See Table 5. We note
that both systems perform better than the most-
frequent-sense baseline. DIR is reliably classiﬁed
using the baseline, while EXT and BNF are never
selected for any preposition. Our method adds
considerably to the scores for most classes. The
low score for BNF is mainly due to the low num-
ber of instances in the data, which is why it was
excluded by O’Hara and Wiebe (2009).

6 Conclusion
To get maximal accuracy in disambiguating
prepositions—and also other word classes—one
needs to consider context, features, and granular-
ity. We presented an evaluation of these parame-
ters for preposition sense disambiguation (PSD).
We ﬁnd that selective context is better than
ﬁxed window size. Within the context for prepo-
sitions, the governor (head of the NP or VP gov-
erning the preposition), the object of the prepo-
sition (i.e., head of the NP to the right), and the
word directly to the left of the preposition have
the highest inﬂuence.12 This corroborates the lin-
guistic intuition that close mutual constraints hold
between the elements of the PP. Each word syn-
tactically and semantically restricts the choice of
the other elements. Combining different extrac-
tion methods (POS-based heuristics and depen-
dency parsing) works better than either one in iso-
lation, though high accuracy can be achieved just
using heuristics. The impact of context and fea-
tures varies somewhat for different granularities.

12These will likely differ for other word classes.

coarse both 2009

Most Frequent Sense

O'Hara/Wiebe 2009

10-fold CV

Class prec

rec

f1

prec

rec

f1

prec

rec

f1

LOC

TMP

DIR

MNR

PRP

EXT

BNF

71.8

77.5

91.6

69.9

78.2

0.0

0.0

97.4

39.4

94.2

43.2

48.8

0.0

0.0

82.6

52.3

92.8

53.4

60.1

0.0

0.0

90.8

84.5

95.6

82.6

79.3

81.7

–

93.2

85.2

96.5

55.8

70.1

84.6

–

92.0

84.8

96.1

66.1

74.4

82.9

–

94.7

94.6

94.6

83.3

90.6

87.5

75.0

96.4

94.6

94.5

75.0

83.8

82.1

34.1

95.6

94.6

94.5

78.9

87.1

84.7

46.9

accuracies

Baseline

Related Work

Our system

coarse

75.8 

89.3*

93.9 

fine

39.6  

78.3**

84.8  

461

Not surprisingly, we see higher scores for coarser
granularity than for the more ﬁne-grained one.

We measured success in accuracy, precision, re-
call, and F-measure, and compared our results to
a most-frequent-sense baseline and existing work.
We were able to improve over state-of-the-art sys-
tems in both coarse and ﬁne-grained PSD, achiev-
ing accuracies of 91.8% and 84.8% respectively.

Acknowledgements

The authors would like to thank Steve DeNeefe,
Victoria Fossum, and Zornitsa Kozareva for com-
ments and suggestions. StephenTratz is supported
by a National Defense Science and Engineering
fellowship.

References
Baker, C.F., C.J. Fillmore, and J.B. Lowe.

1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86–90. Association for
Computational Linguistics Morristown, NJ, USA.

Berger, A.L., V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.

Chan, Y.S., H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In Annual Meeting – Association For Com-
putational Linguistics, volume 45, pages 33–40.

Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 218–226, Boulder, Colorado, June.
Association for Computational Linguistics.

Fellbaum, C. 1998. WordNet: an electronic lexical

database. MIT Press USA.

Hwang, J. D., R. D. Nielsen, and M. Palmer. 2010.
Towards a domain independent semantics: Enhanc-
ing semantic representation with construction gram-
mar. In Proceedings of the NAACL HLT Workshop
on Extracting and Using Constructions in Computa-
tional Linguistics, pages 1–8, Los Angeles, Califor-
nia, June. Association for Computational Linguis-
tics.

Litkowski, K. and O. Hargraves. 2005. The preposi-
tion project. ACL-SIGSEM Workshop on “The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions”, pages 171–179.

Litkowski, K. and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation of Preposi-
tions. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.

Marcus, M.P., M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn TreeBank. Computational Linguis-
tics, 19(2):313–330.

McCallum, A.K. 2002. MALLET: A Machine Learn-
ing for Language Toolkit. 2002. http://mallet. cs.
umass. edu.

Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95–135.

O’Hara, T. and J. Wiebe. 2003. Preposition semantic
classiﬁcation via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79–86.

O’Hara, T. and J. Wiebe. 2009. Exploiting seman-
tic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151–184.

a

heuristic

Rudzicz, F. and S. A. Mokhov.

To-
prepo-
in
english with word-
report, Cornell University,

wards
sitional
net.
arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.

phrases
Technical

categorization

2003.
of

Shen, L., G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classiﬁcation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, volume 45, pages
760–767.

Tratz, S. and D. Hovy.

2009. Disambiguation of
preposition sense using linguistically motivated fea-
tures.
In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Student Re-
search Workshop and Doctoral Consortium, pages
96–100, Boulder, Colorado, June. Association for
Computational Linguistics.

Yarowsky, D. and R. Florian. 2002. Evaluating sense
disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293–310.

462

Ye, P. and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Fea-
tures. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.

