

















































Leveraging Dependency Forest for Neural Medical Relation Extraction


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 208–218,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

208

Leveraging Dependency Forest for Neural Medical Relation Extraction

Linfeng Song1,2, Yue Zhang3,4⇤, Daniel Gildea1, Mo Yu5, Zhiguo Wang6 and Jinsong Su7
1University of Rochester, Rochester, NY, USA

2Tencent AI Lab, Bellevue, WA, USA
3Institute of Advanced Technology, Westlake Institute for Advanced Study

4School of Engineering, Westlake Universtiy
5IBM T.J. Watson Research, Yorktown Heights, NY, USA

6Amazon AWS, New York, NY, USA
7Xiamen University, Xiamen, China

Abstract

Medical relation extraction discovers relations
between entity mentions in text, such as re-
search articles. For this task, dependency syn-
tax has been recognized as a crucial source
of features. Yet in the medical domain, 1-
best parse trees suffer from relatively low ac-
curacies, diminishing their usefulness. We
investigate a method to alleviate this prob-
lem by utilizing dependency forests. Forests
contain many possible decisions and therefore
have higher recall but more noise compared
with 1-best outputs. A graph neural network
is used to represent the forests, automatically
distinguishing the useful syntactic information
from parsing noise. Results on two biomedi-
cal benchmarks show that our method outper-
forms the standard tree-based methods, giving
the state-of-the-art results in the literature.

1 Introduction

The sheer amount of medical articles and their
rapid growth prevent researchers from receiv-
ing comprehensive literature knowledge by direct
reading. This can hamper both medical research
and clinical diagnosis. NLP techniques have been
used for automating the knowledge extraction pro-
cess from the medical literature (Friedman et al.,
2001; Yu and Agichtein, 2003; Hirschman et al.,
2005; Xu et al., 2010; Sondhi et al., 2010; Abacha
and Zweigenbaum, 2011). Along this line of work,
a long-standing task is relation extraction, which
mines factual knowledge from free text by labeling
relations between entity mentions. As shown in
Figure 1, the sub-clause “previously observed cy-
tochrome P450 3A4 ( CYP3A4 ) interaction of the
dual orexin receptor antagonist almorexant” con-
tains two entities, namely “orexin receptor” and
“almorexant”. There is an “adversary” relation be-
tween these two entities, denoted as“CPR:6”.

⇤Yue Zhang is the corresponding author

... observed ... interaction  of  orexin  receptor  antagonist  almorexant

amod

nmod

comp

comp

comp

(a)

... observed ... interaction  of  orexin  receptor  antagonist  almorexant

amod

nmod

comp

comp

comp

nmod
comp

(b)

Figure 1: (a) 1-best dependency tree and (b) depen-
dency forest for a medical-domain sentence, where
edge label “comp” represents “compound”. Associated
mentions are in different colors. Some irrelevant words
and edges are omitted for simplicity.

Previous work has shown that dependency
syntax is important for guiding relation extrac-
tion (Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Liu et al., 2015; Gormley et al.,
2015; Xu et al., 2015a,b; Miwa and Bansal, 2016;
Zhang et al., 2018b), especially in biological and
medical domains (Quirk and Poon, 2017; Peng
et al., 2017; Song et al., 2018b). Compared with
sequential surface-level structures, such as POS
tags, dependency trees help to model word-to-
word relations more easily by drawing direct con-
nections between distant words that are syntacti-
cally correlated. Take the phrase “effect on the
medicine” for example; “effect” and “medicine”
are directly connected in a dependency tree, re-
gardless of how many modifiers are added in be-
tween.

Dependency parsing has achieved an accuracy
over 96% in the news domain (Liu and Zhang,
2017; Kitaev and Klein, 2018). However, for the
medical literature domain, parsing accuracies can
drop significantly (Lease and Charniak, 2005; Mc-
Closky and Charniak, 2008; Sagae et al., 2008;
Candito et al., 2011). This can lead to severe er-



209

ror propagation in downstream relation extraction
tasks, offsetting much of the benefit that relation
extraction models can obtain by exploiting depen-
dency trees as a source of external features.

We address the low-accuracy issue in biomed-
ical dependency parsing by considering depen-
dency forests as external features. Instead of 1-
best trees, dependency forests consist of depen-
dency arcs and labels that a parser is relatively
confident about, therefore having better recall of
gold-standard arcs by offering more candidate
choices with noise. Our main idea is to let a re-
lation extraction system learn automatically from
a forest which arcs are the most relevant through
end-task training, rather than relying solely on the
decisions of a noisy syntactic parser. To this end, a
graph neural network is used for encoding a forest,
which in turn provides features for relation extrac-
tion. Back-propagation passes loss gradients from
the relation extraction layer to the graph encoder,
so that the more relevant edges can be chosen au-
tomatically for better relation extraction.

Results on BioCreative VI ChemProt (CPR)
(Krallinger et al., 2017) and a recent dataset fo-
cused on phenotype-gene relations (PGR) (Sousa
et al., 2019) show that our method outperforms a
strong baseline that uses 1-best dependency trees
as features, giving the state-of-the-art accuracies
in the literature. To our knowledge, we are the
first to study dependency forests for medical in-
formation extraction, showing their advantages
over 1-best tree structures. Our code is available
at http://github.com/freesunshine/
dep-forest-re.

2 Related work

Syntactic forests There have been previous
studies leveraging constituent forests for machine
translation (Mi et al., 2008; Ma et al., 2018; Zare-
moodi and Haffari, 2018), sentiment analysis (Le
and Zuidema, 2015) and text generation (Lu and
Ng, 2011). However, the usefulness of depen-
dency forests is relatively rarely studied, with one
exception being Tu et al. (2010), who use depen-
dency forests to enhance long-range word-to-word
dependencies for statistical machine translation.
To our knowledge, we are the first to study the
usefulness of dependency forests for relation ex-
traction under a strong neural framework.

Graph neural network Graph neural net-
works (GNNs) have been successful in encoding

dependency trees for downstream tasks, such as
semantic role labeling (Marcheggiani and Titov,
2017), semantic parsing (Xu et al., 2018), ma-
chine translation (Song et al., 2019; Bastings et al.,
2017), relation extraction (Song et al., 2018b) and
sentence ordering (Yin et al., 2019). In particular,
Song et al. (2018b) showed that GNNs are more
effective than DAG networks (Peng et al., 2017)
for modeling syntactic trees in relation extraction,
which cause loss of important structural informa-
tion. We are the first to exploit GNNs for encoding
search spaces in the form of dependency forests.

3 Task

Formally, the input to our task is a sentence s =
w1, w2, . . . , wN , where N is the number of words
in the sentence and wi represents the i-th input
word. s is annotated with boundary information
(⇠1 : ⇠2 and ⇣1 : ⇣2) of target entity mentions (⇠
and ⇣). We focus on the classic binary relation
extraction setting (Quirk and Poon, 2017), where
the number of associated mentions is two. The
output is a relation from a predefined relation set
R = (r1, . . . , rM ,None), where “None” means
that no relation holds for the entities.

Two steps are taken for predicting the correct re-
lation given an input sentence. First, a dependency
parser is used to label the syntactic structure of the
input. Here our baseline system takes the standard
approach, using the 1-best parser output tree DT
as features. In contrast, our proposed model uses
the most confident parser forest DF as features.
Given DT or DF , the second step is to encode
both s and DT /DF using a neural network, be-
fore making a prediction.

We make use of the same graph neural network
encoder structure to represent dependency syntax
information for both the baseline and our model.
In particular, a graph recurrent neural network ar-
chitecture (Beck et al., 2018; Song et al., 2018a;
Zhang et al., 2018a) is used, which has been shown
effective in encoding graph structures (Song et al.,
2019), giving competitive results with alternative
graph networks such as graph convolutional neu-
ral networks (Marcheggiani and Titov, 2017; Bast-
ings et al., 2017).

4 Baseline: DEPTREE

As shown in Figure 2, our baseline model stacks a
bidirectional LSTM layer to encode an input sen-
tence w1, . . . , wN with a graph recurrent network



210

word
embeddings ...

Bi-LSTM

...

Graph recurrent network

...

mean pooling
& concatenation

Tree structure

Figure 2: Framework of our baseline and model.

(GRN) to encode a 1-best dependency tree, which
extracts features from the sentence and the de-
pendency tree DT , respectively. Similar model
frameworks have shown highly competitive per-
formances in previous relation extraction studies
(Peng et al., 2017; Song et al., 2018b).

4.1 Bi-LSTM layer
Given the input sentence w1, w2, . . . , wN , we rep-
resent each word with its embedding to generate
a sequence of embeddings e1, e2, . . . , eN . A Bi-
LSTM layer is used to encode the sentences:

 �
h (0)i = LSTMl(

 �
h (0)i+1, ei) (1)

�!
h (0)i = LSTMr(

�!
h (0)i�1, ei), (2)

where the state of each word wi is generated by
concatenating the states of both directions:

h(0)i = [
 �
h (0)i ;

�!
h (0)i ] (3)

4.2 GRN layer
A 1-best dependency tree can be represented
as a directed graph DT = hV ,Ei, where V
includes all words w1, w2, . . . , wN and E =
{(wj , l, wi)}|wj2V,wi2V represents all dependency
edges (Marcheggiani and Titov, 2017). Each triple
(wj , l, wi) corresponds to a dependency edge,
where wj modifies wi with an arc label l. Each
word wi is associated with a hidden state that is
initialized with the Bi-LSTM output h(0)i . The
state representation of the entire tree consists of
all word states:

h(0) = {h(0)i }wi2V (4)

In order to capture non-local interactions be-
tween words, the GRN layer adopts a message
passing framework that performs iterative in-
formation exchange between directly connected
words. As a result, each word state is updated by
absorbing larger contextual information through
the message passing process, and a sequence of
state transitions h(0),h(1), . . . is generated for the
entire tree. The final state h(T ) = GRN(h(0), T ),
where T is a hyperparameter representing the
number of state transitions.

Message passing The message passing frame-
work takes two main steps within each iteration:
message calculation and state update. Take wi and
iteration t as the example. In the first step, separate
messages m"i and m

#
i are calculated by summing

up the messages of its children and parent in the
dependency tree, respectively:

m"i =
X

(wj ,l,wi)2E(·,·,i)

[h(t�1)j ; el] (5)

m#i =
X

(wi,l,wk)2E(i,·,·)

[h(t�1)k ; elrev ], (6)

where E(·,·,i) and E(i,·,·) represent all edges with
a head word wi and a modifier word wi, respec-
tively, and elrev represents the embedding of label
lrev, the reverse version of original label l (such
as “amod-rev” is the reverse version of “amod”).
The message from a child or a parent is obtained
by simply concatenating its hidden state with the
corresponding edge label embedding.

In the second step, GRN uses standard gated op-
erations of LSTM (Hochreiter and Schmidhuber,
1997) to update hidden state h(t�1)i with the previ-
ously integrated message. In particular, a cell c(t)i
is taken to record memory for h(t)i ; an input gate
i(t)i , an output gate o

(t)
i and a forget gate f

(t)
i are

used to control information flow from the inputs
and to the output h(t)i :

i(t)i = �(W
"
1m

"
i +W

#
1m

#
i + b1)

o(t)i = �(W
"
2m

"
i +W

#
2m

#
i + b2)

f (t)i = �(W
"
3m

"
i +W

#
3m

#
i + b3)

u(t)i = tanh(W
"
4m

"
i +W

#
4m

#
i + b4)

c(t)i = f
(t)
i � c

(t�1)
i + i

(t)
i � u

(t)
i

h(t)i = o
(t)
i � tanh(c

(t)
i ),

(7)

where W "x , W #x , and bx (x 2 {1, 2, 3, 4}) are



211

model parameters, and c(0)i is initialized as a vec-
tor of zeros.

The same process repeats for T iterations. Start-
ing from h(0) of the Bi-LSTM layer, increasingly
more informed hidden states h(t) are obtained as
the iteration increases, and h(T ) is used as the final
representation of each word.

4.3 Relation prediction
Given h(T ) of the GRN encoding, we calculate
the representation vector of the two related en-
tity mentions ⇠ and ⇣ (such as “almorexant” and
“orexin receptor” in Figure 1) with mean pooling:

h⇠ = fmean(h
(T )
⇠1:⇠2

) (8)

h⇣ = fmean(h
(T )
⇣1:⇣2

) (9)

where ⇠1 : ⇠2 and ⇣1 : ⇣2 represent the span of ⇠
and ⇣, respectively, and fmean is the mean-pooling
function.

Finally, the representations of both mentions are
concatenated to be the input of a logistic regres-
sion classifier:

y = softmax(W5[h⇠;h⇣ ] + b5), (10)

where W5 and b5 are model parameters.

5 Model

In this section, we first discuss how to generate
high-quality dependency forests, before showing
how to adapt GRN to consider the parser proba-
bility of each dependency edge.

5.1 Forest generation
Given a dependency parser, generating depen-
dency forests with high recall and low noise is a
non-trivial problem. On the one hand, keeping the
whole search space gives 100% recall, but intro-
duces maximum noise. On the other hand, using
the 1-best dependency tree can result in low recall
given an imperfect parser. We investigate two al-
gorithms to generate high-quality forests by judg-
ing “quality” from different perspectives: one fo-
cusing on arcs, and the other focusing on trees.

EDGEWISE This algorithm focuses on the lo-
cal relation of each individual edge and uses parser
probabilities as confidence scores to assess edge
qualities. Starting from the whole parser search
space, it keeps all the edges with scores greater

than a threshold �. The time complexity is O(N2),
where N represents the sentence length.1

KBESTEISNER This algorithm extends the
Eisner algorithm (Eisner, 1996) with cube prun-
ing (Huang and Chiang, 2005) for finding K
highest-scored tree structures. The Eisner algo-
rithm is a standard method for decoding 1-best
trees for graph-based dependency parsing. Based
on bottom-up dynamic programming, it stores the
1-best subtree for each span and takes O(N3) time
complexity for decoding a sentence of N words.

KBESTEISNER keeps a sorted list of K-best
hypotheses for each span. Cube pruning (Huang
and Chiang, 2005) is adopted to generate the K-
best list for each larger span from the K-best lists
of its sub-spans. After the bottom-up decoding,
we merge the final K-bests by combining identical
dependency edges to make the forest. As a result,
KBESTEISNER takes O(N3K logK) time.

Discussions EDGEWISE is much simpler and
faster than KBESTEISNER. Compared with the
O(N3K logK) time complexity of KBESTEIS-
NER, EDGEWISE only takes O(N2) running time,
and each step (storing an edge) runs faster than
KBESTEISNER (making a new hypothesis by
combining two from sub-spans). Besides, the
forests of EDGEWISE can be denser and provide
richer information than those from KBESTEIS-
NER. This is because KBESTEISNER only merges
K trees, where many edges are shared among
them. Also, K cannot be set to a large number
(such as 100), because that will cause a dramatic
increase of running time.

Compared with KBESTEISNER, EDGEWISE
suffers from two potential problems. First, EDGE-
WISE does not guarantee to produce a 1-best tree
in a generated forest, as it makes decisions by
considering the individual edges. Second, it does
not guarantee to generate spanning forests, which
can happen when the threshold � is high. On
the other hand, no previous work has shown that
the information from the whole tree is crucial for
relation extraction. In fact, many previous stud-
ies use only the dependency path between the tar-
get entity mentions (Bunescu and Mooney, 2005;
Airola et al., 2008; Chowdhury et al., 2011; Gorm-
ley et al., 2015; Mehryary et al., 2016). We study

1More accurately, it is O(N2L) and L s a constant factor,
denoting the number of distinct dependency labels. We omit
it for simplicity.



212

the effectiveness of both algorithms in our experi-
ments.

5.2 GRN encoding with parser confidence
As illustrated by Figure 1(b), our dependency
forests are directed graphs that can be consumed
by GRN without any structural changes. For fair
comparison, we use the same model as the base-
line to encode sentences and forests. Thus our
model uses the same number of parameters as
our baseline taking 1-best trees.

Since forests contain more than one tree, it is
intuitive to consider parser confidence scores for
potentially better feature extraction. To this end,
we slightly adjust the GRN encoding process with-
out introducing additional parameters. In particu-
lar, we enhance the original message sum function
(Equations 5 and 6) by applying the edge proba-
bilities in calculating weighted message sums:

m"i =
X

✏2E(·,·,i)

p✏[h
(t�1)
j ; el] (11)

m#i =
X

✏2E(i,·,·)

p✏[h
(t�1)
k ; elrev ], (12)

where ✏ (instead of a triple) is used to represent an
edge for simplicity, and p✏ is the parser probability
for edge ✏. The edge probabilities are not adjusted
during end-task training.

6 Training

Relation loss Given a set of training instances,
each containing a sentence s with two target men-
tions ⇠ and ⇣, and a dependency structure D
(tree or forest), we train our models with a cross-
entropy loss between the gold-standard relations r
and model distribution:

lR = � log p(r|s, ⇠, ⇣,D;✓), (13)

where ✓ represents the model parameters.

Using additional NER loss For training on
BioCreative VI CPR, we follow previous work
(Liu et al., 2017; Verga et al., 2018) to take NER
loss as additional supervision, though the mention
boundaries are known during testing.

lNER = �
1

N

NX

n=1

log p(tn|s,D;✓), (14)

where tn is the gold NE tag of wn with the
“BIO” scheme. Both losses are conditionally in-
dependent given the deep features produced by our

model, and the final loss for BioCreative VI CPR
training is l = lR + lNER.

7 Experiments

We conduct experiments on two medical bench-
marks to test the usefulness of dependency forest.

7.1 Data
BioCreative VI CPR (Krallinger et al., 2017)

This task2 focuses on the relations between chem-
ical compounds (such as drugs) and proteins (such
as genes). The full corpus contains 1020, 612 and
800 extracted PubMed3 abstracts for training, de-
velopment and testing, respectively. All abstracts
are manually annotated with the boundaries of en-
tity mentions and the relations. The data provides
three types of NEs: “CHEMICAL”, “GENE-Y”
and “GENE-N”, and the relation set R contains
5 regular relations (“CPR:3”, “CPR:4”, “CPR:5”,
“CPR:6” and “CPR:9”) and the “None” relation.

For efficient generation of dependency struc-
tures, we segment each abstract into sentences,
keeping only the sentences that contain at least a
chemical mention and a protein mention. For any
sentence containing several chemical mentions or
protein mentions, we keep multiple copies of it
with each copy having different target mention
pairs. As a result, we only consider the relations
of mentions in the same sentence, assigning all
cross-sentence chemical-protein pairs as “None”
relation. By doing this, we effectively sacrifice
cross-sentence relations, which has a negative ef-
fect on our systems; but this is necessary for ef-
ficient generation of dependency structures since
directly parsing a short paragraph is slow and er-
roneous.4 In general, we obtain 16,107 training,
10,030 development and 14,269 testing instances,
in which around 23% have regular relations. The
highest recalls for relations on our development
and test sets are 92.25 and 92.54, respectively, be-
cause of the exclusion of cross-sentence relations
in preprocessing. We report F1 scores of the full
test set for a fair comparison, using all gold regular
relations to calculate recalls.

Phenotype-Gene relation (PGR) (Sousa et al.,
2019) This dataset concerns the relations between

2https://biocreative.bioinformatics.udel.edu/tasks/biocreative-
vi/track-5/

3https://www.ncbi.nlm.nih.gov/pubmed/
4Peng et al. (2017) describe a solution for cross-sentence

cases, which joins different dependency structures by con-
necting their roots. We leave it for future work.



213

human phenotypes (such as diseases) with human
genes, where the relation set is a binary class
on whether a phenotype is related to a gene. It
has 18,451 silver training instances and 220 high-
quality test instances, with each containing men-
tion boundary annotations. We separate the first
15% training instances as our development set.
Unlike BioCreative VI CPR, almost every relation
of PGR is within a single sentence.

7.2 Models
We compare the following models:

• TEXTONLY: It does not take dependency
structures and directly uses the Bi-LSTM
outputs (h(0) in Eq. 3) to make predictions.

• DEPTREE: Our baseline using 1-best depen-
dency trees, as shown in Section 4.

• EDGEWISEPS and EDGEWISE: Our mod-
els using the forests generated by our
EDGEWISE algorithm with or without parser
scores.

• KBESTEISNERPS and KBESTEISNER: Our
model using the forests generated by our
KBESTEISNER algorithm with or without
parser scores, respectively.

7.3 Settings
We take a state-of-the-art deep biaffine parser
(Dozat and Manning, 2017), trained on the Penn
Treebank (PTB) (Marcus and Marcinkiewicz,
1993) converted to Universal Dependency, to ob-
tain 1-best trees and full search spaces for generat-
ing forests. Using standard PTB data split (02–21
for training, 22 for development and 23 for test-
ing), it gives UAS and LAS scores of 95.7 and
94.6, respectively.

For the other hyper-parameters, word embed-
dings are initialized with the 200-dimensional
BioASQ vectors5, pretrained on 10M abstracts of
biomedical articles, and are fixed during train-
ing. The dimension of hidden vectors in Bi-LSTM
is set to 200, and the number of message pass-
ing steps T is set to 2 based on Zhang et al.
(2018b). We use Adam (Kingma and Ba, 2014),
with a learning rate of 0.001, as the optimizer.
The batch size, coefficient for l2 normalization
loss and dropout rate are 20, 10�8 and 0.1, respec-
tively.

5http://bioasq.lip6.fr/tools/BioASQword2vec/

� #Edge/#Node LAS Conn. Ratio(%)

0.05 2.09 92.5 100.0
0.1 1.57 91.2 99.5
0.2 1.34 90.5 94.2
0.3 1.04 88.0 77.6

K #Edge/#Node LAS Conn. Ratio(%)

1 1.00 86.4 100.0
2 1.03 87.3 100.0
5 1.09 89.1 100.0
10 1.14 89.8 100.0

Table 1: Statistics on forests generated with various �
(upper half) and K (lower half) on the development set.

7.4 Analyses of generated forests

Table 1 demonstrates several characteristics of
the generated forests of both the EDGEWISE and
KBESTEISNER algorithms in Section 5.1, where
“#Edge/#Sent” measures the forest density with
the number of edges divided by the sentence
length, “LAS” represents the oracle LAS score
on 100 biomedical sentences with manually an-
notated dependency trees, and “Conn. Ratio (%)”
shows the percentage of forests where both related
entity mentions are connected.

Regarding the forest density, forests produced
by EDGEWISE generally contain more edges than
those from KBESTEISNER. Due to the combi-
natorial property of forests, EDGEWISE can give
much more candidate trees (and sub-trees) for the
whole sentence (and each sub-span). This coin-
cides with the fact that the forests generated by
EDGEWISE have higher oracle scores than these
generated by KBESTEISNER.

For connectivity, KBESTEISNER guarantees to
generate spanning forests. On the other hand,
the connectivity ratio for the forests produced by
EDGEWISE drops when increasing the threshold
�. We can have more than 94% being connected
with �  0.2. Later we will show that good end-
task performance can still be achieved with the
94% connectivity ratio. This indicates that losing
connectivity for a small potion of the data may not
hurt the overall performance.

7.5 Development results

Figure 3 shows the development experiments
for our forest generation algorithms, where both
EDGEWISE and KBESTEISNER give consistent



214

(a) EDGEWISE

(b) KBESTEISNER

Figure 3: Development results (F1 score) for our forest
generation methods.

improvements over DEPTREE and TEXTONLY.
Generally, EDGEWISE gives more improvements
than KBESTEISNER. The main reason may be
that EDGEWISE generates denser forests, pro-
viding richer features. On the other hand,
KBESTEISNER shows a marginal improvement by
increasing K from 5 to 10. This indicates that
only merging 10-best trees may be far from suf-
ficient. However, using a much larger K (such
as 100) is not practical due to dramatically in-
creased computation time. In particular, the run-
ning time of KBESTEISNER with K = 10 is al-
ready much longer than that of EDGEWISE. As a
result, EDGEWISE better serves our goal compared
to KBESTEISNER. This may sound surprising, as
EDGEWISE does not consider tree-level scores. It
suggests that relation extraction may not require
full dependency tree features. This coincides with
previous relation extraction research (Bunescu and
Mooney, 2005; Airola et al., 2008), which utilizes
the shortest path connecting the two candidate en-
tities in the dependency tree.

Leveraging parser confidence scores also con-
sistently helps both methods. It is especially ef-
fective for EDGEWISE when � = 0.05. This
is likely because the parser confidence scores are
useful for distinguishing some erroneous depen-
dency arcs, when noise is large (e.g. when � is
too small). Following the development results, we

Model F1 score

GRU+Attn (Liu et al., 2017)† 49.5
Bran (Verga et al., 2018)† 50.8

TEXTONLY 50.6
DEPTREE 51.4

KBESTEISNERPS 52.4**
EDGEWISEPS 53.4**

Table 2: Test results of Biocreative VI CPR. † indi-
cates previously reported numbers. ** means signifi-
cant over DEPTREE at p < 0.01 with 1000 bootstrap
tests (Efron and Tibshirani, 1994).

directly report the performances of EDGEWISEPS
and KBESTEISNERPS, setting � and K to 0.2 and
10, respectively, in our remaining experiments.

7.6 Main results on BioCreative VI CPR
Table 2 shows the main comparison results on
the BioCreative CPR testset, with comparisons
to the previous state-of-the-art and our baselines.
GRU+Attn (Liu et al., 2017) stacks a self-attention
layer on top of GRU (Cho et al., 2014) and em-
bedding layers; Bran (Verga et al., 2018) adopts a
biaffine self-attention model to simultaneously ex-
tract the relations of all mention pairs. Both meth-
ods use only textual knowledge.

TEXTONLY gives a performance comparable
with Bran. With 1-best dependency trees, our
DEPTREE baseline gives better performances than
the previous state of the art. This confirms
the usefulness of dependency structures and the
effectiveness of GRN on encoding these struc-
tures. Using dependency forests and parser confi-
dence scores, both KBESTEISNERPS and EDGE-
WISEPS obtain significantly higher numbers than
DEPTREE. Consistent with the development ex-
periments, EDGEWISEPS has a higher testset per-
formance than KBESTEISNERPS.

7.7 Analysis
Effectiveness on parsing accuracy We have

shown in Sections 7.5 and 7.6 that a dependency
parser trained using a domain-general treebank
can produce high-quality dependency forests in a
target domain (biomedical) for helping relation ex-
traction. This is based on the assumption of there
being a high-quality treebank in a descent scale,
which may not be true for low-resource languages.
We simulate this low-resource effect by training
our parser in much smaller treebanks of 1K or 5K



215

Figure 4: DEV results of BioCreative CPR regarding
the dependency parsers trained on different number
(1K, 5K or Full) of dependency trees.

dependency trees, respectively. The LAS scores
for the resulting parsers on our 100 manually an-
notated biomedical dependency trees are 79.3 and
84.2, respectively, while the LAS score for the
parser trained with the full treebank is 86.4, as
shown in Table 1.

Figure 4 shows the results on the Biocreative
CPR development set, where the performance of
TEXTONLY is 51.6. DEPTREE fails to outperform
TEXTONLY when only 1K or 5K dependency
trees are available for training our parser. This is
due to the low parsing recall and subsequent noise
caused by the weak parsers. It confirms the pre-
vious conclusion that dependency structures are
highly influential to the performance of relation
extraction. Both EDGEWISEPS and KBESTEIS-
NERPS are still more effective than DEPTREE.
In particular, KBESTEISNERPS significantly im-
proves TEXTONLY with 5K dependency trees, and
EDGEWISEPS is helpful even with 1K depen-
dency trees.

KBESTEISNER shows relatively smaller gaps
than EDGEWISE when only a limited number of
dependency trees are available. This is probably
because considering whole-tree quality helps to
better eliminate noise.

Case study Figure 5 illustrates two major
types of errors in BioCreative CPR, which are
caused by inaccurate 1-best dependency trees. As
shown in Figure 5(a), the baseline system mistak-
enly predicts a “None” relation for that instance.
This is mainly because “STAT3” is incorrectly
linked to the main verb “inhibited” with a “punct”
relation, but it should be linked to “AKT”. In con-
trast, our forest contains the correct relation and
with a probability of 0.18. This is possibly be-
cause “AKT and STAT3” fits the common pattern
of “A and B” that conjunct two nouns.

Figure 5(b) shows another type of parsing er-

ATO  inhibited  phosphorylation  and  activation  of  AKT  and  STAT3

nsubj

(a)

Role  of  the  calcium  modulated  cyclases   in  ...  retinal  projection  ...

nmod

(b)

obj conj

punct
nmod

conj

nsubj

obj
nmod

amod

nmod

amod
comp

Figure 5: Two representative cases in BioCreative
CPR, contrasting 1-best trees and forests, where irrele-
vant content and arcs are omitted for simplicity.

Model F1 score

BO-LSTM (Lamurias et al., 2019)† 52.3
BioBERT (Lee et al., 2019)† 67.2

TEXTONLY 76.0
DEPTREE 78.9

KBESTEISNERPS 83.6*
EDGEWISEPS 85.7**

Table 3: Main results on PGR testest. † denotes previ-
ous numbers rounded into 3 significant digits. * and **
indicate significance over DEPTREE at p < 0.05 and
p < 0.01 with 1000 bootstrap tests.

rors that cause end-task mistakes. In this exam-
ple, the multi-token mention “calcium modulated
cyclases” is incorrectly segmented in the 1-best
dependency tree, where “modulated” is used as
the main verb of the whole sentence, leaving “cy-
clases” and “calcium” as the object and the mod-
ifier of the subject, respectively. However, this
mention ought to be a noun phrase with “cyclases”
being the head. Our forest helps in this case by
providing a more reasonable structure (shown as
the yellow dashed arcs), where both “calcium” and
“modulated” modify “cyclases”. This is likely be-
cause “modulated” can be interpreted as an adjec-
tive in addition to being a verb. It shows the advan-
tage of keeping multiple candidate syntactic arcs.

7.8 Main results on PGR
Table 3 shows the comparison with previous work
on the PGR testset, where our models are sig-
nificantly better than the existing models. This
is likely because the previous models do not uti-
lize all the information from inputs: BO-LSTM
only takes the words (without arc labels) along the
shortest dependency path between the target men-
tions; the pretrained weights of BioBERT are kept
constant during training for relation extraction.

With 1-best trees, DEPTREE is 2.9 points bet-



216

Model F1 score

C-GCN (Zhang et al., 2018b)† 84.8
C-AGGCN (Guo et al., 2019)† 85.7

DEPTREE 84.6

KBESTEISNERPS 85.8
EDGEWISEPS 86.3

Table 4: Main results on SemEval-2010 task 8 testest.
† denotes previous numbers.

ter than TEXTONLY, confirming the usefulness
of dependency structures. Leveraging depen-
dency forests, both KBESTEISNERPS and EDGE-
WISEPS significantly outperform DEPTREE with
p-values of 0.003 and 0.024, respectively. This
further confirms the usefulness of dependency
forests for medical relation extraction.

7.9 Main results on SemEval-2010 task 8
In addition to the biomedical domain, leveraging
dependency forests applies to other domains as
well. As shown in Table 4, we conduct a prelim-
inary study on SemEval-2010 task 8 (Hendrickx
et al., 2009), a widely used benchmark for news-
domain relation extraction. It is a public dataset,
containing 10,717 instances (8000 for training and
development, 2717 for testing) with 19 relations:
9 directed relations and a special “Other” class.
Both C-GCN and C-AGGCN take a similar net-
work as ours by stacking a graph neural network
for encoding trees on top of a Bi-LSTM layer for
encoding sentences.

DEPTREE achieves similar performance as C-
GCN and is slightly worse than C-AGGCN, with
one potential reason being that C-AGGCN takes
more parameters. Using forests, both KBESTEIS-
NERPS and EDGEWISEPS outperform DEPTREE
with the same number of parameters, and they
show comparable and slightly better performances
than C-AGGCN. Again, EDGEWISEPS is better
than KBESTEISNERPS, showing that the former
is a better way for generating forests.

8 Conclusion

We have proposed two algorithms for generating
high-quality dependency forests for relation ex-
traction, and studied a graph recurrent network
for effectively distinguishing useful features from
noise in parsing forests. Experiments on two
biomedical relation extraction benchmarks show
the superiority of forests versus tree structures,

without introducing any additional model param-
eters. Our deep analyses indicate that the main
advantage comes from alleviating out-of-domain
parsing errors.

Acknowledgments Research supported by NSF
award IIS-1813823.

References
Asma Ben Abacha and Pierre Zweigenbaum. 2011.

Automatic extraction of semantic relations between
medical entities: a rule based approach. Journal of
biomedical semantics, 2(5).

Antti Airola, Sampo Pyysalo, Jari Björne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interac-
tion extraction with evaluation of cross-corpus learn-
ing. BMC bioinformatics, 9(11):S2.

Joost Bastings, Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Simaan. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing.

Daniel Beck, Gholamreza Haffari, and Trevor Cohn.
2018. Graph-to-sequence learning using gated
graph neural networks. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers).

Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on human
language technology and empirical methods in nat-
ural language processing.

Marie Candito, Enrique Henestroza Anguiano, and
Djamé Seddah. 2011. A word clustering approach
to domain adaptation: Effective parsing of biomed-
ical texts. In Proceedings of the 12th International
Conference on Parsing Technologies.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).

Faisal Md. Chowdhury, Alberto Lavelli, and Alessan-
dro Moschitti. 2011. A study on dependency tree
kernels for automatic extraction of protein-protein
interaction. In Proceedings of BioNLP 2011 Work-
shop.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL’04), Main Volume.



217

Timothy Dozat and Christopher D Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In Proceedings of International Conference on
Learning Representations.

Bradley Efron and Robert J Tibshirani. 1994. An intro-
duction to the bootstrap. CRC press.

Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1.

Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. Genies:
a natural-language processing system for the extrac-
tion of molecular pathways from journal articles.
Bioinformatics, 17(1).

Matthew R Gormley, Mo Yu, and Mark Dredze. 2015.
Improved relation extraction with feature-rich com-
positional embedding models. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1774–1784.

Zhijiang Guo, Yan Zhang, and Wei Lu. 2019. Atten-
tion guided graph convolutional networks for rela-
tion extraction. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics, pages 241–251.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of Se-
mEval, pages 94–99.

Lynette Hirschman, Alexander Yeh, Christian
Blaschke, and Alfonso Valencia. 2005. Overview
of biocreative: critical assessment of information
extraction for biology.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8).

Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Nikita Kitaev and Dan Klein. 2018. Constituency pars-
ing with a self-attentive encoder. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).

Martin Krallinger, Obdulia Rabal, Saber A Akhondi,
et al. 2017. Overview of the biocreative vi chemical-
protein interaction track. In Proceedings of the VI
BioCreative challenge evaluation workshop.

Andre Lamurias, Diana Sousa, Luka A Clarke, and
Francisco M Couto. 2019. BO-LSTM: classify-
ing relations via long short-term memory networks
along biomedical ontologies. BMC bioinformatics,
20(1):10.

Phong Le and Willem Zuidema. 2015. The forest con-
volutional network: Compositional distributional se-
mantics with a neural chart and without binarization.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.

Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In International Conference
on Natural Language Processing.

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So, and
Jaewoo Kang. 2019. Biobert: pre-trained biomed-
ical language representation model for biomedical
text mining. arXiv preprint arXiv:1901.08746.

Jiangming Liu and Yue Zhang. 2017. In-order
transition-based constituent parsing. Transactions
of the Association for Computational Linguistics, 5.

Sijia Liu, Feichen Shen, Yanshan Wang, Majid
Rastegar-Mojarad, Ravikumar Komandur Elayav-
illi, Vipin Chaudhary, and Hongfang Liu. 2017.
Attention-based neural networks for chemical pro-
tein relation extraction. In Proceedings of the
BioCreative VI Workshop.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng WANG. 2015. A dependency-based
neural network for relation classification. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers).

Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.

Chunpeng Ma, Akihiro Tamura, Masao Utiyama,
Tiejun Zhao, and Eiichiro Sumita. 2018. Forest-
based neural machine translation. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for se-
mantic role labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing.

Mitchell P Marcus and Mary Ann Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19(2).



218

David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics.

Farrokh Mehryary, Jari Björne, Sampo Pyysalo, Tapio
Salakoski, and Filip Ginter. 2016. Deep learning
with minimal training data: TurkuNLP entry in the
BioNLP shared task 2016. In Proceedings of the 4th
BioNLP Shared Task Workshop.

Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers).

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph LSTMs. Trans-
actions of the Association for Computational Lin-
guistics, 5:101–115.

Chris Quirk and Hoifung Poon. 2017. Distant super-
vision for relation extraction beyond the sentence
boundary. In Proceedings of the 15th Conference
of the European Chapter of the ACL (EACL-17).

Kenji Sagae, Yusuke Miyao, Rune Sætre, and Jun’ichi
Tsujii. 2008. Evaluating the effects of treebank
size in a practical application for parsing. In Soft-
ware Engineering, Testing, and Quality Assurance
for Natural Language Processing.

Parikshit Sondhi, Manish Gupta, ChengXiang Zhai,
and Julia Hockenmaier. 2010. Shallow information
extraction from medical forum data. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters.

Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo
Wang, and Jinsong Su. 2019. Semantic neural ma-
chine translation using amr. Transactions of the As-
sociation for Computational Linguistics, 7:19–31.

Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel
Gildea. 2018a. A graph-to-sequence model for amr-
to-text generation. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1616–
1626.

Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel
Gildea. 2018b. N-ary relation extraction using
graph-state lstm. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2226–2235.

Diana Sousa, André Lamúrias, and Francisco M Couto.
2019. A silver standard corpus of human phenotype-
gene relations. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies.

Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency forest
for statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010).

Patrick Verga, Emma Strubell, and Andrew McCallum.
2018. Simultaneously self-attending to all mentions
for full-abstract biological relation extraction. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers).

Hua Xu, Shane P Stenner, Son Doan, Kevin B John-
son, Lemuel R Waitman, and Joshua C Denny. 2010.
Medex: a medication information extraction sys-
tem for clinical narratives. Journal of the American
Medical Informatics Association, 17(1).

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation classifica-
tion via convolutional neural networks with simple
negative sampling. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing.

Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Li-
wei Chen, and Vadim Sheinin. 2018. Exploiting
rich syntactic information for semantic parsing with
graph-to-sequence model. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015b. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing.

Yongjing Yin, Linfeng Song, Jinsong Su, Jiali Zeng,
Chulun Zhou, and Jiebo Luo. 2019. Graph-based
neural sentence ordering. In Proceedings of IJCAI.

Hong Yu and Eugene Agichtein. 2003. Extracting syn-
onymous gene and protein terms from biological lit-
erature. Bioinformatics, 19.

Poorya Zaremoodi and Gholamreza Haffari. 2018. In-
corporating syntactic uncertainty in neural machine
translation with a forest-to-sequence model. In Pro-
ceedings of the 27th International Conference on
Computational Linguistics, pages 1421–1429.

Yue Zhang, Qi Liu, and Linfeng Song. 2018a.
Sentence-state lstm for text representation. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 317–327.

Yuhao Zhang, Peng Qi, and Christopher D. Manning.
2018b. Graph convolution over pruned dependency
trees improves relation extraction. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing.


