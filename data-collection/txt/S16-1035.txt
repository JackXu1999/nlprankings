



















































PotTS at SemEval-2016 Task 4: Sentiment Analysis of Twitter Using Character-level Convolutional Neural Networks.


Proceedings of SemEval-2016, pages 230–237,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

PotTS at SemEval-2016 Task 4: Sentiment Analysis of Twitter Using
Character-level Convolutional Neural Networks.

Uladzimir Sidarenka
Applied Computational Linguistics/FSP Cognitive Science

University of Potsdam
Karl-Liebknecht Straße 24-25

14476 Potsdam
sidarenk@uni-potsdam.de

Abstract

This paper presents an alternative approach to
polarity and intensity classification of senti-
ments in microblogs. In contrast to previous
works, which either relied on carefully de-
signed hand-crafted feature sets or automat-
ically derived neural embeddings for words,
our method harnesses character embeddings
as its main input units. We obtain task-specific
vector representations of characters by train-
ing a deep multi-layer convolutional neural
network on the labeled dataset provided to
the participants of the SemEval-2016 Shared
Task 4 (Sentiment Analysis in Twitter; Nakov
et al., 2016b) and subsequently evaluate our
classifiers on subtasks B (two-way polarity
classification) and C (joint five-way prediction
of polarity and intensity) of this competition.
Our first system, which uses three manifold
convolution sets followed by four non-linear
layers, ranks 16 in the former track; while
our second network, which consists of a sin-
gle convolutional filter set followed by a high-
way layer and three non-linearities with linear
mappings in-between, attains the 10-th place
on subtask C.1

1 Introduction

Sentiment analysis (SA) – a field of knowledge
which deals with the analysis of people’s opinions,
sentiments, evaluations, appraisals, attitudes, and
emotions towards particular entities mentioned in
discourse (Liu, 2012) – is commonly considered to
be one of the most challenging, competitive, but at

1The source code of our implementation is freely avail-
able online at https://github.com/WladimirSidorenko/
SemEval-2016/

the same time utmost necessary areas of research in
modern computational linguistics.

Unfortunately, despite numerous notable ad-
vances in recent years (e.g., Breck et al., 2007;
Yessenalina and Cardie, 2011; Socher et al., 2011),
many of the challenges in the opinion mining field,
such as domain adaptation or analysis of noisy texts,
still pose considerable difficulties to researchers. In
this respect, rapidly evaluating and comparing dif-
ferent approaches to solving these problems in a
controlled environment – like the one provided for
the SemEval task (Nakov et al., 2016b) – is of cru-
cial importance for finding the best possible way of
mastering them.

We also pursue this goal in the present paper by
investigating whether one of the newest machine
learning trends – the use of deep neural networks
(DNNs) with small receptive fields – would be a vi-
able solution for improving state-of-the-art results in
sentiment analysis of Twitter.

After a brief summary of related work in Section
2, we present the architectures of our networks and
describe the training procedure we used for them
in Section 3. Since we applied two different DNN
topologies to subtasks B and C, we make a cross-
comparison of both systems and evaluate the role of
the preprocessing steps in the next-to-last section.
Finally, in Section 5, we draw conclusions from our
experiments and make further suggestions for future
research.

2 Related Work

Since its presumably first official mention by Na-
sukawa and Yi in 2003 (cf. Liu, 2012), sentiment
analysis has constantly attracted the attention of re-

230



searchers. Though earlier works on opinion mining
were primarily concerned with the analysis of narra-
tives (Wiebe, 1994) or newspaper articles (Wiebe et
al., 2003), the explosive emergence of social media
(SM) services in the mid-2000s has brought about a
dramatic focus change in this field.

A particularly important role in this regard was
played by Twitter – a popular microblogging service
first introduced by Jack Dorsey in 2006 (Dorsey,
2006). The sudden availability of huge amounts
of data combined with the presence of all possible
social and national groups on this stream rapidly
gave rise to a plethora of scientific studies. No-
table examples of these were the works conducted
by Go et al. (2009) and Pak and Paroubek (2010),
who obtained their corpora using distant supervi-
sion and subsequently trained several classifiers on
these data; Kouloumpis et al. (2011), who trained an
AdaBoost system on the Edinburgh Twitter corpus2;
and Agarwal et al. (2011), who proposed tree-kernel
methods for doing message-level sentiment analysis
of tweets.

Eventually, with the introduction of the SemEval
corpus (Nakov et al., 2013), a great deal of automatic
systems and resources have appeared on the scene.
Though most of these systems typically rely on tra-
ditional supervised classification methods, such as
SVM (Mohammad et al., 2013; Becker et al., 2013)
or logistic regression (Hamdan et al., 2015; Plot-
nikova et al., 2015), in recent years, the deep learn-
ing (DL) tsunami (Manning, 2015) has also started
hitting the shores of this “battlefield”.

In this paper we investigate whether one of the
newest lines of research in DL – the use of character-
level deep neural networks (charDNNs) – would be
a perspective way for addressing the sentiment anal-
ysis task on Twitter as well.

Introduced by Sutskever et al. (2011), char-DNNs
have already proved their efficiency for a variety
of NLP applications, including part-of-speech tag-
ging (dos Santos and Zadrozny, 2014), named-entity
recognition (dos Santos and Guimarães, 2015),
and general language modeling (Kim et al., 2015;
Józefowicz et al., 2016). We hypothesized that the
reduced feature sparsity of this approach, its lower
susceptibility to informal spellings, and the shift of

2http://demeter.inf.ed.ac.uk

the main discriminative classification power from
input units to transformation layers would make it
suitable for doing opinion mining on Twitter as well.

3 Method

To test our conjectures, we downloaded the train-
ing and development data provided by the organiz-
ers of the SemEval-2016 Task 4 (Sentiment Anal-
ysis in Twitter; Nakov et al., 2016b). Due to dy-
namic changes of this content, we were only able
to retrieve a total of 5,178 messages for subtasks
B and D (two-way polarity classification) and 7,335
microblogs for subtasks C and E (joint five-way pre-
diction of polarity and intensity).

We deliberately refused to do any heavy-weight
NLP preprocessing of these data to check whether
the applied DL method alone would suffice to get
acceptable results. In order to facilitate the train-
ing and reduce the variance of the learned weights
though, we applied a shallow normalization of the
input by lower-casing messages’ strings and filter-
ing out stop words before passing the tweets to the
classifiers.

As stop words we considered all auxiliary verbs
(e.g., be, have, do) and auxiliary parts of speech
(prepositions, articles, particles, and conjunctions)
up to a few exceptions – we kept the negations and
words that potentially could inverse the polarity of
opinions, e.g., without, but, and however. Further-
more, we also removed hyperlinks, digits, retweets,
@-mentions, common temporal expressions, and
mentions of tweets’ topics, since all of these ele-
ments were a priori guaranteed to be objective. An
example of such preprocessed microblog is provided
below:

EXAMPLE 3.1.
Original: Going to MetLife tomorrow but not to
see the boys is a weird feeling
Normalized: but not see boys weird feeling

3.1 Adversarial Convolutional Networks
(Subtasks B and D)

We then defined a multi-layer deep convolutional
network for subtasks B and D as follows:

At the initial step, we map the input characters
to their appropriate embeddings, obtaining an input
matrix E ∈ Rn×m, where n stands for the length of

231



the input instance, andm denotes the dimensionality
of the embedding space (specifically, we use m =
32).

Next, three sets of convolutional filters – positive
(+), negative (−), and shifter (x) convolutions – are
applied to the input matrix E. Each of these sets in
turn consists of three subsets: one subset with 4 fil-
ters of width 3, another subset comprising 8 filters of
width 4, and, finally, a third subset having 12 filters
of width 5.3

Each subset filter F forms a matrix Rw×m with
the number of rows w corresponding to the filter
width and the number of columns m being equal to
the embedding dimensionality as above. A subset of
filters Spw for p ∈ {+,−, x} is then naturally repre-
sented as a tensor Rc×w×m, where c is the number
of filters with the given width w.

We apply the usual convolution operation with
max-pooling over time for each filter, getting an out-
put vector ~vSpw ∈ Rc for each subset. All output
vectors ~vSp∗ of the same subset are then concate-
nated into one vector ~vSp = [~vSp3 , ~vSp4 , ~vSp5 ] of size
4 + 8 + 12 = 24.

The results of the three sets are subsequently
joined using the following equation:

~vconv = sig(~vS+ − ~vS−)� tanh(~vSx),

where ~vS+ , ~vS− , and ~vSx mean the output vectors for
the positive, negative, and shifter sets respectively,
and � denotes the Hadamard product.

The motivation behind this choice of unification
function is that we first want to obtain the difference
between the positive and negative predictions (thus
~vS+ − ~vS−), then map this difference to the range
[0, 1] (therefore the sigmoid), and finally either in-
verse or dampen these results depending on the out-
put of the shifter layer, whose values are guaranteed
to be in the range [−1, 1] thanks to tanh. Since we
simultaneously apply competing convolutions to the
same input, we call this layer “adversarial” as all of
its components have different opinions regarding the
final outcome.

After obtaining ~vconv, we consecutively use three
non-linear transformations (linear rectification, hy-

3By simultaneously applying multiple filter sets of different
width to the same input, we hoped to improve the precision-
recall trade-off, getting more accurate outputs from wider filters
while reducing their sparsity with narrower kernels.

perbolic tangent, and sigmoid function) with linear
modifications in-between:

~vrelu = relu(~vconv ·Mrelu +~brelu),
~vtanh = tanh(~vrelu ·Mtanh +~btanh),
~vsig = sig(~vtanh ·Msig +~bsig).

In this equation, Mrelu,Mtanh, and Msig ∈
R24×24 stand for the linear transform matrices, and
~brelu,~btanh,~bsig ∈ R24 represent the usual bias
terms. With this combination, we hope to first prune
unreliable input signals by using a hard rectifying
linear unit (Jarrett et al., 2009) and then gain more
discriminative power by successively applying tanh
and sig, thus funneling the input to increasingly
smaller ranges: [−1, 1] in the case of tanh, and [0, 1]
in the case of sigmoid.

At the last stage, after applying a binomial
dropout mask with p = 0.5 to the ~vsig vector (Sri-
vastava et al., 2014), we compute the final prediction
as:

y′ =

{
1, if sig(

∑
~vsig ·Mpred +~bpred) ≥ 0.5

0, otherwise,
(1)

where Mpred ∈ R24×2 and ~bpred ∈ R2 stand for
the transformation matrix and bias term respectively,
and the summation runs over the two elements of the
resulting R2 vector.

To train our classifier, we normally define the cost
function as:

cost =
∑

i

yi ∗ (1− y′i) + (1− yi) ∗ y′i, (2)

where yi denotes the gold category of the i-th train-
ing instance and y′i stands for its predicted class,
and optimize this function using RMSProp (Tiele-
man and Hinton, 2012).

3.2 Highway Convolutional Networks
(Subtasks C and E)

A slightly different model was used for subtasks C
and E:

In contrast to the previous two-way classification
network, we only use one set of convolutions with
4 filters of width 3, 16 filters of width 4, and 24 fil-
ters of width 5, and the number of dimensions of the

232



(a) Adversarial network used for subtasks B and D. (b) Highway network used for subtasks C and E.

Figure 1: Network architectures.

resulting ~vconv vector being equal to 44 instead of
24.

After normally computing and max-pooling the
convolutions, we pass the output convolution vec-
tor through a highway layer (Srivastava et al., 2015)
in addition to using relu, i.e.:

~vhwtrans = sig(~vconv ·Mhwtrans +~bhwtrans),
~vhwcarry = ~vconv � (1− ~vhwtrans),
~vrelu′ = relu(~vconv ·Mconv′ +~bconv′),
~vrelu = sig(~vrelu′ � ~vhwtrans + ~vhwcarry).

The rest of the network is organized the same way
as in the previous model, up to the final layer. Since
this task involves multivariate classification, instead
of computing the sigmoid of the sum as in Equa-
tion 1, we obtain a softmax vector ~vσ ∈ R5 and
consider the argmax value of this vector as the pre-
dicted class:

~vσ = σ(~vsig ·Mσ +~bσ)
y′ = argmax(~vσ)

The corresponding cost function is appropriately de-
fined as:

cost =
∑

i

− ln~vσ[yi]+`2∗
∑

p

‖p‖2+`3∗(yi−y′i)2,

where ~vσ[yi] means the probability value for the
gold class in the ~vσ vector, `2 and `3 are constants
(we use `2 = 1e−5 and `3 = 3e−4), p’s denote
the training parameters of the model, and (yi − y′i)2
stand for the squared difference between the numer-
ical values of the predicted and gold classes.

In this task, we opted for the L2 regularization in-
stead of using dropout, since we found it working
slightly better on the development set, though the
differences between the two methods were not very
big, and the derivative computation with dropout
was significantly faster.

3.3 Initialization and Training

Because initialization has a crucial impact on the re-
sults of deep learning approaches (Sutskever et al.,
2011), we did not rely on purely random weights but
used the uniform He method (He et al., 2015) for ini-
tially setting the embeddings, convolutional filters,
and bias terms instead. The inter-layer transforma-
tions were set to orthogonal matrices to ensure their
full rank.

Additionally, to guarantee that each preceding
network stage came maximally prepared and pro-
vided best possible output to its successors, after

233



adding each new intermediate layer, we temporarily
short-circuited it to the final output node(s) and pre-
trained this abridged network for 5 epochs, remov-
ing the short-circuit connections afterwards. The fi-
nal training then took 50 epochs with each epoch
lasting for 35 iterations over the provided training
data.

Since our models appeared to be very susceptible
to imbalanced classes, we subsampled the training
data by getting min(1.1∗nmin, nc) samples for each
distinct gold category c, where nmin is the number
of instances of the rarest class in the corpus, and nc
denotes the number of training examples belonging
to the c-th class. This subset was resampled anew
for each subsequent training epoch.

Finally, to steer our networks towards recogniz-
ing correct features, we randomly added additional
training instances from two established sentiment
lexica: Subjectivity Clues (Wilson et al., 2005) and
NRC Hashtag Affirmative Context Sentiment Lex-
icon (Kiritchenko et al., 2014). To that end, we
drew n binary random numbers for each polarity
class in the corpus from a binomial distribution
B(n, 0.1), where n stands for the total size of the
generated training set, and added a uniformly cho-
sen term from either lexica whenever the sampled
value was equal to one. In the same way, we ran-
domly (with the probability B(m, 0.15), where m
means the number of matches) replaced occurrences
of terms from the lexica in the training tweets with
other uniformly drawn lexicon items.

4 Evaluation

To train our final model, we used both training and
development data provided by the organizers, setting
aside 15 percent of the samples drawn in each epoch
for evaluation and using the remaining 85 percent
for optimizing the networks’ weights.

We obtained the final classifier by choosing the
network state that produced the best task-specific
score on the set-aside part of the corpus during the
training. For this purpose, in each training iteration,
we estimated the macroaveraged recall ρPN on the
evaluation set for subtask B:

ρPN = ρ
Pos+ρNeg

2 ,

and computed the macroaveraged mean absolute er-
ror measure MAEM (cf. Nakov et al., 2016a) to

select a model for track C :

MAEM (h, Te) = 1|C|
|C|∑
j=1

1
|Tej |

∑
x∈Tej

|h(xi)− yi|

The resulting models were then used in both classi-
fication and quantification subtasks of the SemEval
competition, i.e., we used the adversarial network
with the maximum ρPN score observed during the
training to generate the output for tracks B and D
and applied the highway classifier with the mini-
mum achieved MAEM rate to get predictions for
subtasks C and E.4 The scores of the final evaluation
on the official test set are shown in Table 1.

Since many of our parameter and design choices
were made empirically by analyzing systems’ errors
at each development step, we decided to recheck
whether these decisions were still optimal for the
final configuration. To that end, we re-evaluated
the effects of the preprocessing steps by temporar-
ily switching off lower-casing and stop word filter-
ing, and also estimated the impact of the network
structure by applying the model architecture used
for subtask B to the five-way prediction task, and
vice versa using the highway network for the binary
classification. The output layers, costs, and regular-
ization functions of these two approaches were also
swapped in these experiments when applied to dif-
ferent objectives.

Because re-running the complete training from
scratch was relatively expensive (taking eight to ten
hours on our machine), we reduced the number of
training epochs by a factor of five, but tested each
configuration thrice in order to overcome the ran-
dom factors in the He initialization. The arithmetic
mean and standard deviation (with N = 2) of these
three outcomes for each setting are also provided in
the table.

As can be seen from the results, running fewer
training epochs does not notably harm the final pre-
diction quality for the binary task. On the contrary,
it might even lead to some improvements for the ad-
versarial network. We explain this effect by the fact
that the model selected during the shorter training
had a lower score on the evaluation set than the net-
work state chosen during 50 epochs. Nevertheless,

4We used the official aggregating scripts to generate the re-
sults for the quantification tasks.

234



Training
Configuration

ρPN∧
(Subtask B)

MAEM∨
(Subtask C)

Adversarial1/5,cs 61.34±1.24 1.3±0.05

Adversarial1/5,sw 58.64±0.8 1.3±0.05

Adversarial1/5 61.9±0.66 1.37±0.03

Adversarial 61.8 n/a

Highway1/5,cs 59.87±0.79 1.26±0.01

Highway1/5,sw 60.35±1.5 1.23±0.05

Highway1/5 62.05±0.75 1.3±0.04

Highway n/a 1.24

Table 1: Results of the adversarial and highway networks with
different preprocessing steps on Subtasks B and C. (∧ – higher
is better; ∨ – lower is better; 1/5 – using 1/5 of training epochs;
cs – preserving the character case; sw – keeping stop words)

despite its worse evaluation results, this first config-
uration was more able to fit the test data than the
second system, which apparently overfitted the set-
aside part of the corpus.

Furthermore, we also can observe a mixed effect
of the normalization on the two tasks: while keep-
ing stop words and preserving character case deteri-
orates the results for the binary classification, aban-
doning any preprocessing steps turns out to be a
more favorable solution when doing five-way pre-
diction. The reasons for such different behavior are
presumably twofold: a) the character case by itself
might serve as a good indicator of sentiment inten-
sity but be rather irrelevant to expressing its polar-
ity, and b) the number of training instances might
have become scarce as the number of possible gold
classes in the corpus increased.

Finally, one also can see that the highway network
performs slightly better on both subtasks (two- and
five-way) than its adversarial counterpart when used
with shorter training. In this case, we assume that
the swapping of the regularization and cost functions
has hidden the distinctions of the two networks at
their initial layers, since, in our earlier experiments,
we did observe better results for the two-way classi-
fication with the adversarial structure.

5 Discussion and Conclusion

Unfortunately, despite our seemingly sound theoret-
ical assumptions set forth at the beginning, relying
on character embeddings as input did not work out in
practice at the end. Our adversarial system was only
ranked fourth to last on subtask B, and the highway

network attained the second to last place in track C.
However, knowing this outcome in advance was not
possible without trying out these approaches first.

In order to make a retrospective error analysis, we
computed the correlation coefficients between the
character n-grams occurring in the training data and
their gold classes, also comparing these figures with
the corresponding numbers obtained on the test set.
The results of this comparison are shown in Table 2.

3
chars

ρtrain ρtest 4
chars

ρtrain ρtest 5
chars

ρtrain ρtest

urk 0.128 0.039 turk 0.14 0.036 turk 0.127 0.036
pol 0.125 0.069 fail 0.124 0.038 trum 0.122 0.055
why 0.112 0.083 rkey 0.112 0.036 urkey 0.117 0.036
ion 0.106 0.024 rump 0.112 0.067 turke 0.117 0.036
no 0.105 0.109 urke 0.108 0.036 trump 0.112 0.067

tio 0.104 0.006 ame 0.107 0.047 rump 0.103 0.059
ate 0.104 0.031 pol 0.105 0.063 rkey 0.101 0.036
hy 0.103 0.9 why 0.105 0.085 not 0.097 0.1
ot 0.102 0.071 trum 0.104 0.054 poll 0.096 0.026
isi 0.097 0.075 tion 0.104 0.006 amend 0.096 0.062

Table 2: Top-10 character n-grams from the training data and
their correlation coefficients with the negative class on the train-

ing (ρtrain) and test sets (ρtest) of subtask B.

As can be seen from the table, the most reliable
classification traits that could have been learned dur-
ing the training are very specific to their respective
topics – in particular, Trump and Turkey appear to be
very negatively biased terms. This effect becomes
even more evident as the length of the character n-
grams increases. The reason why we did not pre-
filter these substrings in the preprocessing was that
the respective topics of these messages were speci-
fied as donald trump and erdogan, but we only re-
moved exact topic matches from tweets.

Due to this evident topic susceptibility, as a possi-
ble way to improve our results, we could imagine the
inclusion of more training data. Applying ensemble
approaches, as it was done by the top-scoring sys-
tems this year, could also be a perspective direction
to go. We would, however, advise the reader from
further experimenting with network architectures (at
least when training on the original SemEval dataset
only), since both the recursive (RNTN, Socher et
al., 2012) and recurrent variants (LSTM, Hochreiter
and Schmidhuber, 1997) of neural classifiers were
found to perform worse in our experiments than the
feed-forward structure we described.

235



References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,

and Rebecca Passonneau. 2011. Sentiment Analysis
of Twitter Data. In Proceedings of the Workshop on
Languages in Social Media, LSM ’11, pages 30–38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Lee Becker, George Erhart, David Skiba, and Valentine
Matula. 2013. AVAYA: Sentiment Analysis on Twitter
with Self-Training and Polarity Lexicon Expansion.
In Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 333–340, Atlanta,
Georgia, USA, June. Association for Computational
Linguistics.

Eric Breck, Yejin Choi, and Claire Cardie. 2007.
Identifying expressions of opinion in context. In
Manuela M. Veloso, editor, IJCAI 2007, Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, Hyderabad, India, January 6-12, 2007,
pages 2683–2688.

Jack Dorsey. 2006. just setting up my twttr.
Cı́cero Nogueira dos Santos and Victor Guimarães. 2015.

Boosting named entity recognition with neural charac-
ter embeddings. CoRR, abs/1505.05008.

Cı́cero Nogueira dos Santos and Bianca Zadrozny. 2014.
Learning character-level representations for part-of-
speech tagging. In Proceedings of the 31th Interna-
tional Conference on Machine Learning, ICML 2014,
Beijing, China, 21-26 June 2014, volume 32 of JMLR
Proceedings, pages 1818–1826. JMLR.org.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter Sentiment Classification using Distant Supervision.
Technical report, pages 1–6.

Hussam Hamdan, Patrice Bellot, and Frederic Bechet.
2015. Lsislif: Feature Extraction and Label Weight-
ing for Sentiment Analysis in Twitter. In Proceedings
of the 9th International Workshop on Semantic Eval-
uation (SemEval 2015), pages 568–573, Denver, Col-
orado, June. Association for Computational Linguis-
tics.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.
CoRR, abs/1502.01852.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation, 9(8):1735–
1780.

Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ran-
zato, and Yann LeCun. 2009. What is the best multi-
stage architecture for object recognition? In IEEE
12th International Conference on Computer Vision,

ICCV 2009, Kyoto, Japan, September 27 - October 4,
2009, pages 2146–2153. IEEE.

Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the limits
of language modeling. CoRR, abs/1602.02410.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2015. Character-aware neural language
models. CoRR, abs/1508.06615.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Moham-
mad. 2014. Sentiment analysis of short informal texts.
J. Artif. Intell. Res. (JAIR), 50:723–762.

Efthymios Kouloumpis, Theresa Wilson, and Johanna D.
Moore. 2011. Twitter Sentiment Analysis: The Good
the Bad and the OMG! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, Proceedings
of the Fifth International Conference on Weblogs and
Social Media, Barcelona, Catalonia, Spain, July 17-
21, 2011. The AAAI Press.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Christopher D. Manning. 2015. Computational linguis-
tics and deep learning. Computational Linguistics,
41(4):701–707.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. CoRR,
abs/1308.6242.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 312–320,
Atlanta, Georgia, USA, June. Association for Compu-
tational Linguistics.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Se-
bastiani, and Veselin Stoyanov. 2016a. Evaluation
measures for the SemEval-2016 task 4: ”sentiment
analysis in Twitter”.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Veselin Stoy-
anov, and Fabrizio Sebastiani. 2016b. SemEval-2016
task 4: Sentiment analysis in Twitter. In Proceedings
of the 10th International Workshop on Semantic Eval-
uation (SemEval 2016), San Diego, California, June.
Association for Computational Linguistics.

Alexander Pak and Patrick Paroubek. 2010. Twitter as a
Corpus for Sentiment Analysis and Opinion Mining.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the International Conference on Language Re-
sources and Evaluation, LREC 2010, 17-23 May 2010,

236



Valletta, Malta. European Language Resources Asso-
ciation.

Nataliia Plotnikova, Micha Kohl, Kevin Volkert, Stefan
Evert, Andreas Lerner, Natalie Dykes, and Heiko Er-
mer. 2015. KLUEless: Polarity Classification and
Association. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015),
pages 619–625, Denver, Colorado, June. Association
for Computational Linguistics.

Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2011, 27-31 July 2011,
John McIntyre Conference Centre, Edinburgh, UK, A
meeting of SIGDAT, a Special Interest Group of the
ACL, pages 151–161. ACL.

Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Jun’ichi
Tsujii, James Henderson, and Marius Pasca, editors,
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL 2012, July 12-14, 2012, Jeju Island, Korea,
pages 1201–1211. ACL.

Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. CoRR,
abs/1505.00387.

Ilya Sutskever, James Martens, and Geoffrey E. Hinton.
2011. Generating text with recurrent neural networks.
In Lise Getoor and Tobias Scheffer, editors, Proceed-
ings of the 28th International Conference on Machine
Learning, ICML 2011, Bellevue, Washington, USA,
June 28 - July 2, 2011, pages 1017–1024. Omnipress.

T. Tieleman and G. Hinton. 2012. Lecture 6.5—
RmsProp: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural Networks
for Machine Learning.

Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie,
Paul Davis, Bruce Fraser, Diane J. Litman, David R.
Pierce, Ellen Riloff, Theresa Wilson, David S. Day,
and Mark T. Maybury. 2003. Recognizing and or-
ganizing opinions expressed in the world press. In
Mark T. Maybury, editor, New Directions in Question
Answering, Papers from 2003 AAAI Spring Sympo-
sium, Stanford University, Stanford, CA, USA, pages
12–19. AAAI Press.

Janyce Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233–287.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP 2005, Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
Proceedings of the Conference, 6-8 October 2005,
Vancouver, British Columbia, Canada. The Associa-
tion for Computational Linguistics.

Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
2011, 27-31 July 2011, John McIntyre Conference
Centre, Edinburgh, UK, A meeting of SIGDAT, a Spe-
cial Interest Group of the ACL, pages 172–182.

237


