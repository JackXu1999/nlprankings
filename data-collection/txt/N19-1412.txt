



















































Benchmarking Hierarchical Script Knowledge


Proceedings of NAACL-HLT 2019, pages 4077–4085
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

4077

Benchmarking Hierarchical Script Knowledge

Yonatan Bisk1 Jan Buys1 Karl Pichotta∗ Yejin Choi1,2
1Paul G. Allen School of Computer Science and Engineering, University of Washington

2Allen Institute for Artificial Intelligence
{ybisk, jbuys}@cs.washington.edu

Abstract

Understanding procedural language requires
reasoning about both hierarchical and tempo-
ral relations between events. For example,
“boiling pasta” is a sub-event of “making a
pasta dish”, typically happens before “draining
pasta,” and requires the use of omitted tools
(e.g. a strainer, sink...). While people are able
to choose when and how to use abstract ver-
sus concrete instructions, the NLP community
lacks corpora and tasks for evaluating if our
models can do the same. In this paper, we in-
troduce KIDSCOOK, a parallel script corpus,
as well as a cloze task which matches video
captions with missing procedural details. Ex-
perimental results show that state-of-the-art
models struggle at this task, which requires
inducing functional commonsense knowledge
not explicitly stated in text.

1 Introduction

The level of detail used in natural language com-
munication varies: descriptive or instructive text
for experts may elide over details the reader can
seamlessly infer, while text for more novice audi-
ences may be more verbose. A given document
typically adheres to a single level of verbosity
suited to its presumed audience (Grice, 1975),
so learning correspondences between abstract and
detailed descriptions of similar concepts from text
is a challenging problem.

Commonsense knowledge of how complex
events decompose into stereotypical sequences of
simpler events is a necessary component of a sys-
tem that can automatically understand and rea-
son about different types of discourse. Hierarchi-
cal correspondences between abstract and detailed
representations of concepts and events were an im-
portant aspect of the original formulation of scripts
for natural language understanding (Schank and

∗Author now at Google. Work done while unaffiliated.

1. Take the strainer with the 
pasta and pour the pasta 
into the sauce.

2. Stir the pasta into sauce while it is in the 
pan.

3. Let the pasta and sauce simmer for a few minutes.

Add pasta to the sauce t2<latexit sha1_base64="P4+gfywhwn2dG1TBnMWEEbekCJE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI9FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKDzioDcoVt+ouQNaJl5MK5GgOyl/9YczSiCtkkhrT89wE/YxqFEzyWamfGp5QNqEj3rNU0YgbP1ucOiMXVhmSMNa2FJKF+nsio5Ex0yiwnRHFsVn15uJ/Xi/F8NrPhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+1a1XOr3v1VpXGTx1GEMziHS/CgDg24gya0gMEInuEV3hzpvDjvzseyteDkM6fwB87nDwccjZ0=</latexit><latexit sha1_base64="P4+gfywhwn2dG1TBnMWEEbekCJE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI9FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKDzioDcoVt+ouQNaJl5MK5GgOyl/9YczSiCtkkhrT89wE/YxqFEzyWamfGp5QNqEj3rNU0YgbP1ucOiMXVhmSMNa2FJKF+nsio5Ex0yiwnRHFsVn15uJ/Xi/F8NrPhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+1a1XOr3v1VpXGTx1GEMziHS/CgDg24gya0gMEInuEV3hzpvDjvzseyteDkM6fwB87nDwccjZ0=</latexit><latexit sha1_base64="P4+gfywhwn2dG1TBnMWEEbekCJE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI9FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKDzioDcoVt+ouQNaJl5MK5GgOyl/9YczSiCtkkhrT89wE/YxqFEzyWamfGp5QNqEj3rNU0YgbP1ucOiMXVhmSMNa2FJKF+nsio5Ex0yiwnRHFsVn15uJ/Xi/F8NrPhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+1a1XOr3v1VpXGTx1GEMziHS/CgDg24gya0gMEInuEV3hzpvDjvzseyteDkM6fwB87nDwccjZ0=</latexit><latexit sha1_base64="P4+gfywhwn2dG1TBnMWEEbekCJE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI9FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKDzioDcoVt+ouQNaJl5MK5GgOyl/9YczSiCtkkhrT89wE/YxqFEzyWamfGp5QNqEj3rNU0YgbP1ucOiMXVhmSMNa2FJKF+nsio5Ex0yiwnRHFsVn15uJ/Xi/F8NrPhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+1a1XOr3v1VpXGTx1GEMziHS/CgDg24gya0gMEInuEV3hzpvDjvzseyteDkM6fwB87nDwccjZ0=</latexit>

1. Put a large pot half full 
of water on the stove.

2. Turn the heat on under the pot 
and wait for the water to boil hard.

3. Pour the pasta into the boiling water.

Cook the pastat0<latexit sha1_base64="apCCl0QQ/pUAKLQ3uBGKSWYytus=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAfte3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAQUjZs=</latexit><latexit sha1_base64="apCCl0QQ/pUAKLQ3uBGKSWYytus=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAfte3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAQUjZs=</latexit><latexit sha1_base64="apCCl0QQ/pUAKLQ3uBGKSWYytus=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAfte3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAQUjZs=</latexit><latexit sha1_base64="apCCl0QQ/pUAKLQ3uBGKSWYytus=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAfte3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAQUjZs=</latexit>

… …

Drain the pasta

1. Put the strainer in the sink. 
2. Once the pot with pasta is cool 

enough, grab it by the handles. 
3. Pour the pasta and water into the strainer 

in the sink.
4. Pick up the strainer and shake it a little bit so 

more water comes out.

t1<latexit sha1_base64="FVaQdjUWZVLyUUalbbTR1NMDoKM=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAft+3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAWYjZw=</latexit><latexit sha1_base64="FVaQdjUWZVLyUUalbbTR1NMDoKM=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAft+3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAWYjZw=</latexit><latexit sha1_base64="FVaQdjUWZVLyUUalbbTR1NMDoKM=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAft+3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAWYjZw=</latexit><latexit sha1_base64="FVaQdjUWZVLyUUalbbTR1NMDoKM=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpAft+3616NW8Oskr8glShQKPvfvUGCctirpBJakzX91IMcqpRMMmnlV5meErZmA5511JFY26CfH7qlJxZZUCiRNtSSObq74mcxsZM4tB2xhRHZtmbif953Qyj6yAXKs2QK7ZYFGWSYEJmf5OB0JyhnFhCmRb2VsJGVFOGNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzZHOi/PufCxaS04xcwx/4Hz+AAWYjZw=</latexit>

Figure 1: An example KIDSCOOK sequence with mul-
tiple types of hierarchy and abstraction: the example
contains sequences of complex instructions, given both
as sentences and sequences of simpler instructions.

Abelson, 1977; DeJong, 1981) but required hand-
written data structures encoding world knowledge.
However, the automatic induction of such com-
monsense knowledge from open-domain noisy
text corpora remains an open problem (Chambers,
2013; Weber et al., 2018; Zellers et al., 2018). As a
step towards solving this problem we consider tex-
tual descriptions of actions in a cooking domain.

We introduce a dataset, KIDSCOOK, targeted at
exploring the automatic acquisition of correspon-
dences between abstract and concrete descriptions
of actions. The dataset consists of higher-level
single-sentence imperative descriptions paired
with lower-level descriptions with elided details
included. Descriptions come from real grounded
actions, built on top of the YouCookII video cap-
tion dataset (Zhou et al., 2017).

Figure 1 gives an example annotation from the
dataset: the phrase “drain the pasta,” presented
to an annotator with its corresponding video clip,
was annotated as corresponding to four constituent
steps appropriate as instruction for a child. The
constituent steps are “simpler” in the sense that
they correspond to more atomic actions, but not
necessarily in their linguistic complexity. We
identify over 1,500 procedures and tools which
KIDSCOOK makes explicit but are assumed as



4078

commonsense world knowledge by YouCookII.
The KIDSCOOK dataset allows us to learn

mappings between abstract and concrete descrip-
tions via sequence-to-sequence prediction. We ap-
ply several standard neural sequence-to-sequence
models; however, since these models do not ex-
pose explicit, interpretable correspondences be-
tween abstract and concrete descriptions, we also
propose the application of neural transduction
models which capture correspondences with latent
hard alignment variables. We define a cloze-style
evaluation to complement our dataset, in which
models must predict the values of held-out tokens
which target knowledge of tool usage, temporal
ordering, and kitchen commonsense. We find that
our neural transduction models are able to match
the predictive power of traditional neural sequence
models while providing interpretable alignments
between abstract and concrete subsequences use-
ful for our primary goal of analysis of implicit hi-
erarchical script knowledge.

2 Data & Task

Our approach situates script learning as a case
of grounding. For simplicity of exposition, let
us assume there are three levels of abstraction to
grounding: abstract → concrete → motor con-
trol. Most prior work in grounding treats language
monolithically1 and ignores the issue of audience.
In practice, this means the task formulation or ex-
posed API may implicitly bias the language to be
more concrete. By viewing the task as purely lin-
guistic, we have no API or robot that constrains
our language; instead, we define our audience as
children. By eliciting child-directed instructions,
we collect concrete language capturing otherwise
implicit world knowledge that a child would not
know. Because annotators assume a smart and ca-
pable but uninformed listener, we posit this lan-
guage corresponds closely to the most “concrete”
form in which language naturally occurs.

2.1 Data Collection
We construct a task on Amazon’s Mechanical
Turk, where workers are asked to explain a video
action caption to a child.2 Every instruction
is paired with the original YouTube video and
YouCook caption so the annotator could see how

1Notable exceptions include the hierarchical instructions
of (Regneri et al., 2013) and (Bisk et al., 2016).

2Pay was calculated based on $15/hr and assuming work-
ers took 1.5x as long to complete a task as the experimenters.

Avg Len
Seqs Tokens Vocab YC KC

Train 8,125 307,573 3,573 10.0 37.9
Valid 1,014 36,830 1,479 8.8 36.3
Test 1,020 37,156 1,489 8.8 36.4

Table 1: KIDSCOOK corpus statistics

the action was performed, rather than hallucinat-
ing additional details. All captions received three
simplifications. The instructions ask users to fo-
cus on missing information and allow them up to
five steps. Finally, we explicitly asked annotators
to simplify complex actions (e.g. dice) that can be
defined by a series of more basic actions (e.g. cut).

Our KIDSCOOK corpus statistics are shown in
Table 1. In total we collected over 10K action se-
quences (∼400K tokens). The average caption is
approximately 4x longer than a YouCook caption.
Most importantly 1,536 lemmas and 2,316 lexical
types from KIDSCOOK’s vocabulary do not ap-
pear in any of the original captions. This indicates
that there are over 1,500 new concepts, tools, and
procedures that were assumed by YouCookII but
are now explicit in KIDSCOOK.

2.2 Cloze Task

To investigate what new knowledge is being in-
troduced and whether a model has captured it,
we construct a cloze-style slot-filling task (Cham-
bers, 2017; Hermann et al., 2015). We drop key
content words from the concrete realization of an
abstract instruction and ask the model to predict
them. Several examples from the validation set are
shown in Table 2. Correctly predicting the missing
words requires knowledge of the manner of exe-
cuting a task and the tools required.

To choose candidate words to drop, we only
allow words that occur primarily in the concrete
instructions. Additionally, we do not drop stop
words, numbers, or words occurring fewer than
five times. We do, however, drop units of mea-
sure (cup, minute, etc.). This ensures we create
blanks whose answers are previously omitted con-
crete details. Relatedly, under this filter the answer
to a blank is very rarely an ingredient, as our goal
is not to memorize recipes, but to infer the tool
knowledge necessary to execute them. In total, we
whitelist∼1,000 words that can be dropped to cre-
ate blanks. We prefer longer blanks when avail-
able to give preference to compound nouns (e.g.
wire whisk). Finally, we do not drop any words



4079

ABS chop garlic into small pieces .
CON put garlic on cutting board. press on back of knife with hand, cutting into small pieces.

ABS add some parmesan cheese into the bowl and mix them well.
CON use a grater to grate some parmesan cheese into the bowl. use a wire whisk to stir the cheese in.

ABS add the tofu to the wok.
CON drain the water from the tofu using a strainer. add the tofu into the pan. use a spoon to stir the tofu in the mixture.

Table 2: Example abstract/concrete pairs with blanks (red) where predictions and surprisal are computed.

from the concrete sentence if they occur in the ab-
stract description. This restriction eliminates any
benefits that might have been achieved via mod-
els with copy mechanisms. Examples that do not
meet our criteria are removed from the corpus.

3 Models

We investigate the utility of sequence-to-sequence
models with attention (Bahdanau et al., 2015) to
generate concrete realizations of abstract task de-
scriptions. We hypothesize that models that learn
explicit alignments are particularly amenable to
interpretable analysis on the task. Therefore, in
addition to using the global attention model of
(Luong et al., 2015), we adapt the transducer
model proposed by Yu et al. (2016), which uses
learned latent discrete variables to model phrase-
to-phrase alignments. In contrast to many standard
neural models, this approach enables us to incor-
porate prior knowledge about the alignment struc-
ture, and to extract interpretable alignments be-
tween task phrases. Closely related architectures
have been proposed for segmental sequence mod-
eling (Wang et al., 2017) and phrase-based neural
machine translation (Huang et al., 2018).

We train the transducer models using Viterbi
EM (after doing marginal likelihood training for
the initial iterations), as we found it gave higher
predictive accuracy than marginal likelihood train-
ing only. Following Yu et al. (2016) we experi-
ment with both a fixed alignment transition prob-
ability model and a transition model with a neu-
ral parameterization. Cloze task prediction is per-
formed greedily.3 At each slot the Viterbi align-
ment of the prefix of the sequence up to that slot
is computed. See appendix 7 for model details.4

We also evaluate the performance of a language
modelling baseline and a seq2seq model without
attention (Sutskever et al., 2014), to compare the

3During preliminary experiments beam search did not im-
prove performance.

4All code and data is available at https://github.
com/janmbuys/ScriptTransduction.

effect of not modeling alignment at all.
We expect all the models to implicitly capture

aspects of world knowledge. However, the dis-
crete latent variable models provide Viterbi align-
ments over the training data, from which we can
compile a look-up table with the extracted knowl-
edge. In neural attention models, this knowledge
is only weakly recoverable: extracting information
requires hand tuning attention thresholds and there
is no direct way to extract contiguous alignments
for multi-word phrases.

4 Results

4.1 Evaluation Metrics
During generation, we provide the model with the
number of words in each blank to be predicted. We
consider two setups for evaluating examples with
multiple blanks, both assuming that predictions
are made left-to-right: Oracle, where the gold pre-
diction of each blank is fed into the model to con-
dition on for future predictions, and Greedy, where
the model prediction is used for future predictions.
We compute the proportion of exact word matches
over each blank and the precision of the top k = 5
predictions for both setups. Additionally we com-
pute the average surprisal of the gold prediction
(conditioning on oracle predictions). The sur-
prisal of a word (Attneave, 1959; Hale, 2001)
is its negative log probability under the model:
−log(P (wi|w1:i−1)). The higher the probability
of the ground truth, the lower the model’s “sur-
prise” at seeing it in that context.

Finally, as a quantitative proxy for interpretabil-
ity, we report the length of the transducer mod-
els’ average Viterbi alignment span: our goal is
a model which balances low average alignment
lengths and high matching or ranking scores.

4.2 Cloze Task Results
We report results on the prediction task in Table
4. First we consider models trained only on our
dataset: All the models that incorporate a notion of
alignment do substantially better than those who

https://github.com/janmbuys/ScriptTransduction
https://github.com/janmbuys/ScriptTransduction


4080

abstract→ concrete concrete→ abstract

parmesan sprinkle grated, grate, hold a grater, ... whisk eggs, mayonnaise, milk, combine, pour, stir, ...
macaroni stove on high heat, large pot, bowl, ... spatula colors, thickens, coated, simmer, grill, ...
egg place the boiled, gently crack the, crack, ... tongs shrimp, bratwurst, turn, bun, marinate, ...
sauce stir hot, pour gravy, lower setting, find a spoon, ... cutting board onions, bell pepper, meat, bok choy, ...
oil spray cooking, splashing, slowly pour, ... preheat oven, broil, medium, degrees, ...

Table 3: Example Viterbi Alignments. For concrete to abstract, we match any phrase containing the word(s).

Oracle Greedy

Model Match Top-5 Match Top-5 Surp

Language Model 21.59 52.32 21.72 43.33 3.970
Seq2seq 23.57 53.38 23.44 45.76 3.755
+Att 24.52 53.98 24.57 47.34 3.780

Transducer 24.72 55.09 24.91 47.80 4.780
+ParamTran 23.81 53.98 24.00 46.19 3.547

OpenAI GPT 23.19 42.43 15.55 32.86 4.781
+ fine-tuning 38.01 63.69 31.05 57.05 3.151

Table 4: Results on the Cloze prediction task (Match
= Exact Match, Top-5 = Precision of Top 5 predic-
tions, Surp = Surprisal). Transducer results are re-
ported for models with unparameterized and parame-
terized (+ParamTran) alignment transition models. The
best and second best results are emphasized.

do not. We see that our transducer model with
fixed alignment transition probabilities performs
best in terms of predictive accuracy (exact match
and top-5 precision), while the seqseq model with
attention is the next best in most comparisons. The
model with parameterized transitions has the low-
est surprisal though, as it is more confident about
the alignment predictions it is making.

Using average alignment length to quantify
whether the phrase alignments exhibit desirable
structure, we see that the alignments found by the
unparameterized transition model (average length
6.18) are significantly shorter than those of the
parameterized model (average length 16.61). In-
vestigation shows that the paramaterized model
mostly learns degenerate alignments, aligning
most of the concrete sequence to either the start
or end of the abstract sentence. In contrast, qual-
itative analysis of the unparameterized transition
model show that its alignments learn desirable cor-
respondences (see Figure 2). Therefore among
our proposed models (trained on in-domain data
only) the transducer with unparameterized tran-
sitions satisfies our desiderata of displaying both
good predictive power for word generation, and
learning interpretable alignments.

Given the recent success of massively pre-

cut

the

dough

in

half

and

shape

each

half

into

a

ball

.

<S>

place

the

dough

on

a

cutting

board

. 

using

a

sharp

tool

such

as

a 

knife 

, 

cut 

the 

dough 

in 

half 

. 

shape 

each 

half 

of 

the 

dough 

into 

a 

ball 

.

weigh

the 

cabbage 

and 

add 

salt 

.

<S> 

pour

the 

cabbage 

into 

the 

big 

bowl 

. 

use 

a 

spoon 

and 

scoop 

some 

salt 

into 

the 

bowl

place

the

bread

over

high

flame

.

<S>

take

the

tongs

and

pick

up

the

bread

. 

set

the

bread

on

a

grill

. 

put 

on

the 

grill 

over 

the 

high 

flame 

.

...<latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit>

e0
<latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit><latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit><latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit><latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit>

e6
<latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit><latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit><latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit><latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit>

...<latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit><latexit sha1_base64="Ul1zjHMk9xZA8oyRXJLJOxn4TQw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0hE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z+oHh61TJJpxpsskYnuhNRwKRRvokDJO6nmNA4lb4fj25nffuLaiEQ94iTlQUyHSkSCUbTSg+u6/WrNc705yCrxC1KDAo1+9as3SFgWc4VMUmO6vpdikFONgkk+rfQyw1PKxnTIu5YqGnMT5PNTp+TMKgMSJdqWQjJXf0/kNDZmEoe2M6Y4MsveTPzP62YYXQe5UGmGXLHFoiiTBBMy+5sMhOYM5cQSyrSwtxI2opoytOlUbAj+8surpHXh+p7r31/W6jdFHGU4gVM4Bx+uoA530IAmMBjCM7zCmyOdF+fd+Vi0lpxi5hj+wPn8AUvkjSI=</latexit>

e0
<latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit><latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit><latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit><latexit sha1_base64="1NSVA7XC5wla7NGfXitfHcXtZN8=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTAXXxvO+ndLa+sbmVnm7srO7t3/gHh61dJIphk2WiER1QqpRcIlNw43ATqqQxqHAdji+nfntJ1SaJ/LRTFIMYjqUPOKMGis9YN/ru1Wv5s1BVolfkCoUaPTdr94gYVmM0jBBte76XmqCnCrDmcBppZdpTCkb0yF2LZU0Rh3k81On5MwqAxIlypY0ZK7+nshprPUkDm1nTM1IL3sz8T+vm5noOsi5TDODki0WRZkgJiGzv8mAK2RGTCyhTHF7K2EjqigzNp2KDcFffnmVtC5qvlfz7y+r9ZsijjKcwCmcgw9XUIc7aEATGAzhGV7hzRHOi/PufCxaS04xcwx/4Hz+AO0rjYw=</latexit>

e6
<latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit><latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit><latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit><latexit sha1_base64="12MfjmJDqM21J4rd2GarPcwFAso=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit>

Figure 2: Example Viterbi alignments

trained language models (Peters et al., 2018), we
are interested if these approaches transfer to our
cloze task. We evaluate the OpenAI GPT trans-
former language model (Radford et al., 2018) with
and without fine-tuning.Without fine-tuning this
model does slightly worse than our best domain-
specific model. With fine-tuning, its accuracy is
substantially higher, but it still suffers from the
same fundamental limitations as our other mod-
els (see Table 5). The transformer (Vaswani et al.,
2017) attention is multi-headed and multi-layered
which prohibits direct interpretability.

5 Qualitative Analysis

We visualize alignments of our transduction model
over two partial sequences in Fig. 2. This shows
which hidden vector of the abstract sentence
aligned to every region of the concrete sequence.
Specifically, we see how tools like the big bowl,
spoon, and tongs are introduced to facilitate the
actions. There are also implications, e.g. that high
indicates grill. For further analysis we extract
alignments over the training corpus, linking each
decoded phrase with the word from the encoding it
used during generation. We then aggregate these
tuples into a table which we can filter (based on
our whitelist) and sort (with PMI). This process is
imprecise as it discards the context in which the
alignment occurs, but it nonetheless extracts many



4081

Abs shape each dough ball into a circle and add tomato sauce .
Pred flatten out your dough into a flat circle using your hands. take a knife to add tomato sauce to the center of your dough.

use the back side of the knife to cut the sauce out. make sure you keep the sauce about an inch from the edges.
Gold flatten out your dough into a flat circle using your hands. take a spoon to add tomato sauce to the center of your dough.

use the back side of the spoon to spread the sauce out. make sure you keep the sauce about an inch from the edges.

Abs place the kale cucumber bell peppers carrots and radishes on the wrapper .
Pred put the cut on a cutting . put a cutting amount of kale on the cutting . add a cut amount of cucumber ...
Gold put the wrap on a plate . put a small amount of kale on the wrap . add a small amount of cucumber ...

Abs wrap the pizza .
Pred find a large piece to put the pizza om . place the pizza in the center for it not to stick around . grab the plastic wrap and

start wrapping the entire thing and pizza . wrap all around until completely covered on all corners . put in freezer
on a cold water and freeze overnight

Gold find a hard surface to put the pizza om . place the pizza in the center for it not to slide around . grab the plastic wrap
and start wrapping the hard surface and pizza . wrap all around until fully covered on all corners . put in freezer on
a flat surface and freeze overnight

Table 5: Above is the output of OpenAI GPT when forced to greedily decode answers to blanks in validation.

of the phenomena we would hope to see (Table 3).
The left-hand side of the table shows words

from the abstract YouCook annotations and corre-
sponding phrases in the concrete annotation. For
the righthand side we searched for common con-
crete terms that may be preceded or followed by
other terms, and present the abstract terms they
were most often generated by.

Finally, Table 5 shows three randomly chosen
examples (from the validation set) of greedy de-
codings for slot filling with GPT fine-tuned on our
dataset. These examples demonstrate that, first,
there are cases where GPT is successful or pro-
duces a semantically valid answer (e.g. fully vs
completely). Second, as is common with greedy
decoding, the model can get stuck in a loop (e.g.
cut, cutting, cutting, ...). Finally, note there are
nonsensical cases where the model appears to have
discarded the abstract context (e.g. knife to add
tomato sauce or freezer on a cold water).

6 Related Work

Many script learning systems are based on event
co-occurrence and language modeling in large text
corpora, and can infer implicit events without cre-
ating explicit situation-specific frame structures
(Chambers and Jurafsky, 2008; Rudinger et al.,
2015; Pichotta and Mooney, 2016). Other systems
induce situation-specific frames from text (Che-
ung et al., 2013; Balasubramanian et al., 2013).
However, these methods do not explicitly target
the commonsense correspondence between differ-
ing levels of detail of complex events.

Most relevant to this paper is the pioneering
work of Regneri et al. (2013) as extended by
Senina et al. (2014) and Rohrbach et al. (2014).

These papers present the TACOS corpus, consist-
ing of natural language descriptions of activities in
videos paired with low-level activity labels. Sen-
ina et al. (2014) collect an additional level of
multi-sentence annotations on the corpus, which
allowing for video caption generation at multiple
levels of detail. Rohrbach et al. (2014) describe
a similar corpus of natural descriptions of com-
posite actions, useful for activity recognition in
video. These corpora differ in a number of impor-
tant ways from KIDSCOOK; in particular, the lan-
guage has somewhat limited complexity and “nat-
uralness” when describing complex scenarios, a
phenomenon also observed in the robotics litera-
ture (Scalise et al., 2018). Our data collection pro-
cess avoids more formulaic language by eliciting
“child-directed” descriptions.

7 Conclusion

We introduce a new hierarchical script learning
dataset and cloze task in which models must learn
commonsense world knowledge about tools, pro-
cedures and even basic physics to perform well.
Our aim is to begin a conversation about abstrac-
tion in language, how it is modeled, and what is
implicitly hidden. Our abstract and concrete in-
structions are grounded in the same videos yet dif-
fer dramatically due to their assumed audiences.
We show that a neural transduction model pro-
duces interpretable alignments for analyzing these
otherwise latent correlations and phenomena.

Acknowledgements

This work was supported in part by NSF (IIS-
1524371 & 1703166) and through DARPA’s CwC
program through ARO (W911NF-15-1-0543).



4082

References
Fred Attneave. 1959. Applications of information the-

ory to psychology: A summary of basic concepts,
methods, and results. Henry Holt.

Dzmitry Bahdanau, KyungHyun Cho, and Yoshua
Bengio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In EMNLP.

Yonatan Bisk, Daniel Marcu, and William Wong. 2016.
Towards a dataset for human computer communica-
tion via grounded language acquisition. In Proceed-
ings of the AAAI 2016 Workshop on Symbiotic Cog-
nitive Systems, pages 729–732, Phoenix, AZ.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. CL, 19(2):263–311.

Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In EMNLP.

Nathanael Chambers. 2017. Behind the scenes of an
evolving event cloze test. In LSDSem.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In ACL.

Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
NAACL.

Gerald F. DeJong. 1981. Generalizations based on ex-
planations. In IJCAI.

Arthur P Dempster, Nan M Laird, and Donald B Rubin.
1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the royal statistical
society., pages 1–38.

Jason Eisner. 2016. Inside-outside and forward-
backward algorithms are just backprop (tutorial pa-
per). In Workshop on Structured Prediction for NLP.

H. Paul Grice. 1975. Logic and conversation. Syntax
and Semantics 3: Speech Acts, pages 41–58.

John Hale. 2001. A probabilistic earley parser as a psy-
cholinguistic model. In Proceedings of the second
meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
technologies, pages 1–8. Association for Computa-
tional Linguistics.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In NIPS.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong
Zhou, and Li Deng. 2018. Towards neural phrase-
based machine translation. In ICLR.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In EMNLP.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL.

Karl Pichotta and Raymond J Mooney. 2016. Learning
statistical scripts with LSTM recurrent neural net-
works. In AAAI.

Alex Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. Technical re-
port, OpenAI.

Mechaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. TACL, 1.

Anna Rohrbach, Marcus Rohrbach, Wei Qiu, An-
nemarie Friedrich, Manfred Pinkal, and Bernt
Schiele. 2014. Coherent multi-sentence video de-
scription with variable level of detail. In GCPR.

Rachel Rudinger, Vera Demberg, Ashutosh Modi,
Benjamin Van Durme, and Manfred Pinkal. 2015.
Learning to predict script events from domain-
specific text. In *Sem.

Rosario Scalise, Yonatan Bisk, Maxwell Forbes,
Daqing Yi, Yejin Choi, and Siddhartha Srini-
vasa. 2018. Balancing shared autonomy with
human-robot communication. arXiv preprint
arXiv:1805.07719.

Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: An Inquiry into
Human Knowledge Structures. LEA.

Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie
Friedrich, Sikandar Amin, Mykhaylo Andriluka,
Manfred Pinkal, and Bernt Schiele. 2014. Coherent
multi-sentence video description with variable level
of detail. arXiv preprint arXiv:1403.6173.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel
Jurafsky. 2011. Lateen EM: Unsupervised train-
ing with multiple objectives, applied to dependency
grammar induction. In EMNLP.

Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D Manning. 2010. Viterbi train-
ing improves unsupervised dependency parsing. In
CoNLL.



4083

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Chong Wang, Yining Wang, Po-Sen Huang, Abdel-
rahman Mohamed, Dengyong Zhou, and Li Deng.
2017. Sequence modeling via segmentations. In
ICML.

Noah Weber, Leena Shekhar, Niranjan Balasubrama-
nian, and Nathanael Chambers. 2018. Hierarchical
quantized representations for script generation. In
EMNLP.

Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-
stette, and Tomas Kocisky. 2017. The neural noisy
channel. In ICLR.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online
segment to segment neural transduction. In EMNLP.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi. 2018. Swag: A large-scale adversarial dataset
for grounded commonsense inference. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing (EMNLP).

Luowei Zhou, Chenliang Xu, and Jason J Corso.
2017. Towards automatic learning of procedures
from web instructional videos. arXiv preprint
arXiv:1703.09788.



4084

A Transducer Model

We briefly describe the model of Yu, Buys, and
Blunsom (2016) and our minor modifications
thereto.

A.1 Alignment with Latent Variables
We model the conditional probability of a concrete
sequence y given abstract sequence x through a la-
tent alignment variable a between x and y, which
is a sequence of variables aj , with aj = i signify-
ing that yj is aligned to xi. The marginal probabil-
ity of y given x is

p(y|x) =
∑
a

p(y, a|x). (1)

In the following, we use m to denote the length
of x and n to denote the length of y. The model
formulation restricts alignments to be monotonic,
i.e. aj+1 ≥ aj for all j.

The model factorizes over timesteps into align-
ment and word prediction probabilities, such that
the word prediction at each timestep is informed
by its alignment:

p(y, a|x) =
∏
j

p(aj |aj−1, x1:aj−1 , y1:j−1)

× p(yj |aj , x1:aj , y1:j−1) (2)

The abstract and concrete sequences are both en-
coded with LSTM Recurrent Neural Networks
(Hochreiter and Schmidhuber, 1997). In contrast
to standard attention-based models, the aligned
encoder representation is not fed into the decoder
RNN state, but only used to make next word pre-
dictions. Due to the small size of the training data,
words in both sequences are embedded using fixed
GloVe embeddings (Pennington et al., 2014). The
word emission probability is then defined as

p(yj |aj , x1:aj , y1:j−1) = softmax(MLP(eaj , dj)) (3)

with e the encoder hidden states and d the decoder
hidden states.

The alignment probability factorizes into shift
and emit probabilities, where a shift action incre-
ments the alignment to the next word in the input
sequence, and an emit action generates the next
output word. We refer to these as transition prob-
abilities. This formulation enables us to restrict
the hard alignment to be monotonic.

We consider two parameterizations of this dis-
tribution. In the first, the probabilities are param-
eterized by the neural network, using the encoder

and decoder hidden state in a similar manner to
how the word emission probability was computed.
The alignment probability at a given timestep is
therefore parameterized as

p(aj |aj−1, x1:aj−1 ,y1:j−1) = p(emit|aj , x1:aj , y1:j−1)

×
aj−1∏

i=aj−1

p(shift|i, x1:i, y1:j−1), (4)

where

p(shift|i, x1:i, y1:j−1) = σ(MLP (ei, dj)), (5)
p(emit|i, x1:i, y1:j−1) = 1− p(shift|i, x1:i, y1:j−1). (6)

We also consider using the simpler, fixed align-
ment parameterization in Yu, Buys, and Blunsom
(2016), where the transition probability is condi-
tioned only on sequence length, not on x or y, and
can therefore be estimated using the ratio between
input and output sentence lengths. The alignment
probabilities are not updated during training, and
consequently the posterior distribution over the
alignments is biased towards this prior, favoring
alignments close to the diagonal.

The parameterized alignment model contains
as special cases two degenerate solutions: (1) an
unconditional language model and (2) a seq2seq
model. These occur if the model performs all
emits before shifting or all shifts before emitting,
respectively. To prevent the creation of a language
model we force the last output word to be aligned
to the last word in the abstract sequence, similar to
Yu et al. (2017). However, the parameterized tran-
sition model could still in practice revert to a pure
sequence-to-sequence model.

A.2 Marginalization
Next we briefly describe the dynamic program
used to marginalize over alignments during train-
ing and to find the most likely alignments of a
given alignment during inference; we refer the
reader to Yu, Buys, and Blunsom (2016) for a
more thorough treatment.

The forward variable αi(j) representing
p(y1:j , aj = i|x1:i) is recursively as

αi(j) = p(yj |i, x1:i, y1:j−1)

×
i∑

k=1

αk(j − 1)p(aj = i|k, x1:k, y1:j−1). (7)

The marginal likelihood objective is to train
the model to optimize αm(n) = p(y1:n, an =



4085

m|x1:m). The gradients are computed with au-
tomatic differentiation; as this is is equivalent to
using the forward-backward algorithm to estimate
the gradients (Eisner, 2016), only the forward al-
gorithm has to be implemented.

To make the implementation GPU-efficient, we
vectorize the computation of α. The computa-
tion iterates through decoding steps, each of which
can be generated from an alignment to any of the
encoder tokens. We can efficiently construct a
transition matrix T , corresponding to all possible
encoder states performing all possible shifts, and
emission matrix Ej which is a gather by word in-
dex j.

To compute the forward probabilities at each
timestep, the current forward probabilities are first
multiplied by all possible transitions. A sum
in logspace collapses all paths, and the emis-
sion (word generation) probabilities are multiplied
to obtain the new forward probabilities. When
fixed transition probabilities are used, T is pre-
computed.

A.3 Viterbi EM Training

Latent variable models can be trained either
through directly optimizing the likelihood objec-
tive through gradient descent (as described above),
or with the Expectation Maximization (EM) algo-
rithm (Dempster et al., 1977), which alternates be-
tween calculating expectations over the values of
the latent variables given the current parameters,
and maximizing the expected complete data log
likelihood given those expectations. We consider
training our alignment model with Viterbi EM
(Brown et al., 1993), also known as “hard” EM,
where at each iteration the most likely assignment
of the hidden variables (alignments) are found and
the parameters are updated to optimize the log
likelihood given those alignments. Viterbi EM
has been shown to give superior performance to
standard EM on unsupervised parsing (Spitkovsky
et al., 2010), due to better convergence proper-
ties in practice by making the distribution more
peaked.

We perform batched Viterbi EM training by
computing the Viterbi alignments for a batch, and
then performing a gradient step based on treating
those alignments as observations.

We follow a two-stage training procedure: we
first directly optimize the marginal likelihood with
batched SGD to find a reasonable initial distribu-

tion over alignments, before switching to Viterbi
EM training. Such a strategy has been shown to
reduce the chance that the model will get stuck in
local optima (Spitkovsky et al., 2011).

A.4 Inference
We apply the trained models to multiple inference
problems to evaluate how well they are capturing
script knowledge. The first is finding the most
likely alignment given a pair of abstract and con-
crete sequences. We use the standard Viterbi al-
gorithm, in which we replace the sum in equation
(7) with max, and keep track of the index corre-
sponding to each value of α during the forward
computation. The most likely alignment can then
be traced back from an = m.

The second inference problem is slot-filling, for
application to the cloze task. Given an abstract
sentence and a partially-filled concrete sequence,
we want to use the model to predict words to fill
the given blanks. To make the prediction, we sam-
ple 5 candidate sequences by predicting words for
each slot, in left-to-right order, and then choosing
the sequence with the highest overall probability.
Words are predicted by sampling with temperature
0.1, in order to peak the distribution while still al-
lowing some diversity in the samples. The moti-
vation for selecting the final output from multiple
samples is that the original samples are biased, as
they are only conditioned on the left context.

At the start of the prediction for each slot, the
Viterbi alignment of the prefix of the sequence up
to the start of that slot is re-predicted, indepen-
dent of previous alignment predictions. Conse-
quently alignment decisions can be revised, and
the slot alignments are no longer constrained to
be monotonic, which makes the slot prediction
model more flexible. For the parameterized transi-
tion model, the slot alignment is predicted greedily
by incrementing the last predicted alignment while
the shift probability is greater than 0.5. The fixed
transition model assumes that the alignment of the
word preceding the slot is shared across the slot.


