




































Naver Labs Europe's Systems for the WMT19 Machine Translation Robustness Task


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 526‚Äì532
Florence, Italy, August 1-2, 2019. c¬©2019 Association for Computational Linguistics

526

Naver Labs Europe‚Äôs Systems for the WMT19
Machine Translation Robustness Task

Alexandre B√©rard Ioan Calapodescu Claude Roux
Naver Labs Europe

first.last@naverlabs.com

Abstract

This paper describes the systems that we sub-
mitted to the WMT19 Machine Translation
robustness task. This task aims to improve
MT‚Äôs robustness to noise found on social me-
dia, like informal language, spelling mistakes
and other orthographic variations. The orga-
nizers provide parallel data extracted from a
social media website1 in two language pairs:
French-English and Japanese-English (in both
translation directions). The goal is to ob-
tain the best scores on unseen test sets from
the same source, according to automatic met-
rics (BLEU) and human evaluation. We pro-
posed one single and one ensemble system for
each translation direction. Our ensemble mod-
els ranked first in all language pairs, accord-
ing to BLEU evaluation. We discuss the pre-
processing choices that we made, and present
our solutions for robustness to noise and do-
main adaptation.

1 Introduction

Neural Machine Translation (NMT) has achieved
impressive results in recent years, especially on
high-resource language pairs (Vaswani et al., 2017;
Edunov et al., 2018), and has even lead to some
claims of human parity (Hassan et al., 2018).2

However, Belinkov and Bisk (2018) show that
NMT is brittle, and very sensitive to simple
character-level perturbations like letter swaps or
keyboard typos. They show that one can make
an MT system more robust to these types of syn-
thetic noise, by introducing similar noise on the
source side of the training corpus. Sperber et al.
(2017) do similar data augmentation, but at the
word level and so as to make an MT model more
robust to Automatic Speech Recognition errors
(within a speech translation pipeline). Cheng et al.
(2018) propose an adversarial training approach

1https://www.reddit.com
2These claims were discussed at WMT by Toral et al.

(2018).

to make an encoder invariant to word-level noise.
Karpukhin et al. (2019) propose to inject aggres-
sive synthetic noise on the source side of training
corpora (with random char-level operations: dele-
tion, insertion, substitution and swap), and show
that this is helpful to deal with natural errors found
in Wikipedia edit logs, in several language pairs.

Michel and Neubig (2018) release MTNT, a
real-world noisy corpus, to help researchers de-
velop MT systems that are robust to natural noise
found on social media. The same authors co-
organized this task (Li et al., 2019), in which
MTNT is the primary resource. Vaibhav et al.
(2019) show that back-translation (with a model
trained on MTNT) and synthetic noise (that em-
ulates errors found in MTNT) are useful to make
NMT models more robust to MTNT noise.

This task aims at improving MT‚Äôs robustness
to noise found on social media, like informal lan-
guage, spelling mistakes and other orthographic
variations. We present the task in more detail in
Section 2. Then, we describe our baseline models
and pre-processing in Section 3. We extend these
baseline models with robustness and domain adap-
tation techniques that are presented in Section 4.
Finally, in Section 5, we present and discuss the
results of our systems on this task.

2 Task description

The goal of the task is to make NMT systems that
are robust to noisy text found on Reddit, a social
media, in two language pairs (French-English and
Japanese-English) and both translation directions.
The evaluation will be performed on a blind test
set (obtained from the same source), using auto-
matic metrics and human evaluation. We present
our final BLEU scores in Section 5, while the hu-
man evaluation results are given in the shared task
overview paper (Li et al., 2019).

MTNT Michel and Neubig (2018) crawled
monolingual data from Reddit in three languages:

https://www.reddit.com


527

English, French and Japanese, which they filtered
to keep only the ‚Äúnoisiest‚Äù comments (containing
unknown words or with low LM scores).

Then, they tasked professional translators to
translate part of the English data to French, and
part of it to Japanese. The Japanese and French
data was translated to English. The resulting par-
allel corpora were split into train, valid and test sets
(see Table 1). The test sets were manually filtered
so as to keep only good quality translations. The
data that was not translated is made available as
monolingual corpora (see Table 3).

Other data In addition to the provided in-
domain training and evaluation data, we are al-
lowed to use larger parallel and monolingual cor-
pora (see Tables 2 and 3). For FR‚ÜîEN, any par-
allel or monolingual data from the WMT15 news
translation task3 is authorized. For JA‚ÜîEN, we
are allowed the same data that was used by Michel
and Neubig (2018): KFTT, TED and JESC.

Challenges Michel and Neubig (2018) identified
a number of challenges for Machine Translation of
MTNT data, which warrant the study of MT ro-
bustness. Here is an abbreviated version of their
taxonomy:

‚Ä¢ Spelling and grammar mistakes: e.g.,
their/they‚Äôre, have/of.

‚Ä¢ Spoken language and internet slang: e.g., lol,
mdr, lmao, etc.

‚Ä¢ Named entities: many Reddit posts link to
recent news articles and evoke celebrities or
politicians. There are also many references
to movies, TV shows and video games.

‚Ä¢ Code switching: for instance, Japanese text
on Reddit contains many English words.

‚Ä¢ Reddit jargon: words like ‚Äúdownvote‚Äù, ‚Äúup-
vote‚Äù and ‚Äúcross-post‚Äù,4 and many acronyms
like TIL (Today I Learned), OP (Original
Poster), etc.

‚Ä¢ Reddit markdown: characters like ‚Äú‚àº‚Äù, ‚Äú*‚Äù
and ‚Äú^‚Äù are extensively used for formatting.
‚Äú!‚Äù is used to call macros.

‚Ä¢ Emojis (üòâ) and emoticons (‚Äú;-)‚Äù).
3http://www.statmt.org/wmt15/

translation-task.html
4The French-speaking Reddit community sometimes uses

funny literal translations of these: ‚Äúbas-vote‚Äù, ‚Äúhaut-vote‚Äù and
‚Äúcroix-poteau‚Äù.

Lang pair Lines WordsSource Target
JA‚ÜíEN 6 506 160k 155k
EN‚ÜíJA 5 775 339k 493k
FR‚ÜíEN 19 161 794k 763k
EN‚ÜíFR 36 058 1 014k 1 152k

Table 1: Size of the MTNT training corpora. Word
counts by Moses (fr/en) and Kytea (ja) tokenizers.

Lang pair Lines WordsSource Target
JA‚ÜîEN 3.90M 48.42M 42.63M
FR‚ÜîEN 40.86M 1 392M 1 172M

Table 2: Size of the authorized out-of-domain parallel
corpora in constrained submissions.

‚Ä¢ Inconsistent capitalization: missing capital
letters on proper names, capitalization for em-
phasis or ‚Äúshouting‚Äù, etc.

‚Ä¢ Inconsistent punctuation.

Evaluation Automatic evaluation is performed
with cased BLEU (Papineni et al., 2002), us-
ing SacreBLEU (Post, 2018).5 For English and
French, the latter takes as input the detokenized
MT outputs and the untokenized reference data.
For Japanese, the MT output and reference are first
tokenized with Kytea6 (Neubig et al., 2011) before
being processed by SacreBLEU (because it does
not know how to tokenize Japanese). The organiz-
ers will also collect subjective judgments from hu-
man annotators, and rank the participants accord-
ingly.

Language Corpus Lines
Japanese MTNT 32 042

French
MTNT 26 485

news-discuss 3.84M
news-crawl 42.1M

English
MTNT 81 631

news-discuss 57.8M
news-crawl 118.3M

Table 3: Authorized monolingual data.

5BLEU+case.mixed+numrefs.1+smooth.exp
+tok.13a+version.1.3.1

6kytea -model share/kytea/model.bin -out
tok (v0.4.7)

http://www.statmt.org/wmt15/translation-task.html
http://www.statmt.org/wmt15/translation-task.html


528

3 Baseline models
This section describes the pre-processing and hy-
per parameters of our baseline models. We will
then detail the techniques that we applied for ro-
bustness and domain adaptation.

3.1 Pre-processing
CommonCrawl filtering We first spent efforts
on filtering and cleaning the WMT data (in partic-
ular CommonCrawl).

We observed two types of catastrophic failures
when training FR‚ÜíEN models: source sentence
copy, and total hallucinations.

The first type of error (copy) is due to having
sentence pairs in the training data whose refer-
ence ‚Äútranslation‚Äù is a copy of the source sentence.
Khayrallah and Koehn (2018) show that even a
small amount of this type of noise can have catas-
trophic effects on BLEU. We solve this problem by
using a language identifier (langid.py, Lui and
Baldwin, 2012) to remove any sentence pair whose
source or target language is not right.

Then, we observed that most of the hallucina-
tions produced by our models were variants of the
same phrases (see Table 5 for an example). We
looked for the origin of these phrases in the train-
ing data, and found that they all come from Com-
monCrawl (Smith et al., 2013).

We tried several approaches to eliminate hallu-
cinations, whose corresponding scores are shown
in Table 4:

1. Length filtering (removing any sentence pair
whose length ratio is greater than 1.8, or 1.5 for
CommonCrawl): removes most hallucinations
and gives the best BLEU score (when combined
with LID filtering). This type of filtering is
common in MT pipelines (Koehn et al., 2007).

2. Excluding CommonCrawl from the training
data: removes all hallucinations, but gives
worse BLEU scores, suggesting that, albeit
noisy, CommonCrawl is useful to this task.7

3. Attention-based filtering: we observed that
when hallucinating, an NMT model produces a
peculiar attention matrix (see Figure 1), where
almost all the probability mass is concentrated
on the source EOS token. A similar matrix is
produced during the forward pass of training
when facing a misaligned sentence pair. We
7And yet, CommonCrawl represents only 7.9% of all lines

and 6.5% of all words in WMT.

LID Len CC Att FR Hallu. BLEU
‚úì 126 46 34.4

0 12 34.8
‚úì ‚úì 0 0 35.2
‚úì ‚úì 0 29 37.7
‚úì ‚úì ‚úì ‚úì 0 0 38.7
‚úì ‚úì ‚úì 0 10 39.6

Table 4: Number of hallucinations and French-
language outputs (according to langid.py) when
translating MTNT-test (FR‚ÜíEN). LID: language iden-
tifier, Len: length filtering, CC: training data includes
CommonCrawl, Att: attention-based filtering.

SRC T‚Äôas trouv√© un champion on dirait !
REF You got yourself a champion it seems !
MT I‚Äôve never seen videos that SEXY !!!

Table 5: Example of hallucination by a FR‚ÜíEN Trans-
former trained on WMT15 data without filtering.

filtered CommonCrawl as follows: we trained
a baseline FR‚ÜíEN model on WMT without
filtering, then translated CommonCrawl while
forcing the MT output to be the actual reference,
and extracted the corresponding attention ma-
trices. We computed statistics on these atten-
tion matrices: their entropy and proportion of
French words with a total attention mass lower
than 0.2, 0.3, 0.4 and 0.5. Then, we manu-
ally looked for thresholds to filter out most of
the misalignments, while removing as little cor-
rectly aligned data as possible.

A combination of LID, length-based and
attention-based filtering removed all hallucina-
tions in the MT outputs, while obtaining excellent
BLEU scores. The resulting corpus has 12%
fewer lines.8 We use this filtered data for both
FR‚ÜíEN and EN‚ÜíFR. As the JA‚ÜîEN training
data seemed much cleaner, we only did a LID
filtering step.

SentencePiece We use SentencePiece (Kudo
and Richardson, 2018) for segmentation into sub-
word units.

An advantage of SentencePiece is that it does
not require a prior tokenization step (it does its
own coarse tokenization, based on whitespaces and
changes of unicode categories). It also escapes
all whitespaces (by replacing them with a meta

8LID: -5%, length filtering: -6.7%, attention filtering: -
0.5%.



529

I ' ve n
ev

er

se
en

vi
de

os

th
at

S EX Y ! ! ! EO
S

T

'

as

trouv√©

un

champion

on

di

rait

!

EOS

11

0

1

1

0

1

1

0

0

1

1

76

1

1

0

1

0

0

0

0

0

0

0

90

0

0

1

0

0

0

0

0

0

0

0

94

4

0

1

12

0

2

0

1

1

0

0

71

3

0

0

2

0

1

1

1

0

0

0

85

3

0

0

0

0

1

0

0

0

0

1

92

0

0

0

0

0

0

0

0

0

0

2

95

0

0

0

0

0

0

0

0

0

0

0

96

0

0

0

0

0

0

0

0

0

0

0

97

0

0

0

0

0

0

0

0

0

0

0

97

0

0

0

0

0

0

0

0

0

1

3

93

0

0

0

0

0

0

0

0

0

0

8

89

0

0

0

0

0

0

0

0

0

0

5

92

0

0

0

0

0

0

0

0

0

0

5

91

1

0

0

0

0

0

0

0

0

0

0

94

Figure 1: Attention matrix of a French (left) ‚Üí English
(top) Transformer when hallucinating. This is the aver-
age of the attention heads of the last decoder layer over
the last encoder layer.

symbol), so that its tokenization is fully reversible.
This is convenient for emoticons (e.g., ‚Äò:-(‚Äô), which
Moses-style tokenization tends to break apart irre-
versibly.

SentencePiece also normalizes unicode charac-
ters using the NFKC rules (e.g., ¬Ω ‚Üí 1/2). It is
useful for Japanese, which sometimes uses double-
width variants of the ASCII punctuation symbols
(e.g., ‚Äúfullwidth question mark‚Äù in unicode table).

We tried different settings of SentencePiece, and
settled with the BPE algorithm (Sennrich et al.,
2016b),9 with a joined model of 32k tokens for
FR‚ÜîEN (with a vocabulary threshold of 100), and
two separate models of size 16k for JA‚ÜîEN.

Japanese tokenization SentencePiece‚Äôs tok-
enization is based mostly on whitespaces, which
are very rare in Japanese. For this reason, a
pre-tokenization step may be useful (as a way to
enforce some linguistic bias and consistency in
the BPE segmentation).

We tested several tokenizers for Japanese:
MeCab (with IPA and Juman dictionaries),10 Ju-
man++,11 and Kytea.12 MeCab and KyTea gave
comparable results, slightly better than when us-
ing no pre-tokenization (especially when Japanese
is the target language), and Juman++ gave worse
results. We settled with Kytea, which is the offi-
cial tokenizer used on the EN‚ÜíJA task.13

9SentencePiece also implements ULM (Kudo, 2018).
10http://taku910.github.io/mecab/
11https://github.com/ku-nlp/jumanpp
12http://www.phontron.com/kytea/
13We use the default model shipped with KyTea.

3.2 Model and hyper-parameters
We use Transformer Big for FR‚ÜîEN and JA‚ÜíEN,
and Transformer Base for EN‚ÜíJA. We work
with Fairseq, with essentially the same hyper-
parameters as Ott et al. (2018).

For FR‚ÜîEN, we fit up to 3500 tokens in each
batch, while training on 8 GPUs (with synchronous
SGD). We accumulate gradients over 10 batches
before updating the weights. This gives a theoret-
ical maximum batch size of 280k tokens. These
models are trained for 15 epochs, with a check-
point every 2500 updates. We set the dropout rate
to 0.1. The source and target embedding matrices
are shared and tied with the last layer.

For JA‚ÜîEN, we fit 4000 tokens in each batch,
and train on 8 GPUs without delayed updates, for
100 epochs with one checkpoint every epoch. We
set the dropout rate to 0.3.

For both language pairs, we train with Adam
(Kingma and Ba, 2015), with a max learning rate
of 0.0005, and the same learning rate schedule as
Ott et al. (2018); Vaswani et al. (2017). We also do
label smoothing with a 0.1 weight. We average the
5 best checkpoints of each model according to their
perplexity on the validation set. We do half preci-
sion training, resulting in a 3√ó speedup on V100
GPUs (Ott et al., 2018).

4 Robustness techniques
We now describe the techniques that we applied to
our baseline models to make them more robust to
the noise found in MTNT.

4.1 Case handling
One of the sources of noise in the MTNT data is
capital letters. On the Web, capital letters are often
used for emphasis (to stress one particular word,
or for ‚Äúshouting‚Äù). However, NMT models treat
uppercase words or subwords as completely differ-
ent entities than their lowercase counterparts. BPE
even tends to over-segment capitalized words that
were not seen in its training data.

One solution, used by Levin et al. (2017) is to do
factored machine translation (Sennrich and Had-
dow, 2016; Garcia-Martinez et al., 2016), where
words (or subwords) are set to lowercase and their
case is considered as an additional feature.

In this work, we use a simpler technique that we
call ‚Äúinline casing‚Äù, which consists in using special
tokens to denote uppercase (<U>) or title case sub-
words (<T>), and including these tokens within the

http://taku910.github.io/mecab/
https://github.com/ku-nlp/jumanpp
http://www.phontron.com/kytea/


530

sequence right after the corresponding (lowercase)
subword. For instance, ‚ÄùThey were SO TASTY!!‚Äù
‚Üí ‚Äùthey <T> _were _so <U> _tas <U> ty <U>
!!‚Äù. SentencePiece is trained and applied on low-
ercase text and the case tokens are added after the
BPE segmentation. We also force SentencePiece
to split mixed-case words (e.g., MacDonalds ‚Üí
_mac <T> donalds <T>)

4.2 Placeholders
MTNT contains emojis, which our baseline MT
models cannot handle (unicode defines over 3 000
unique emojis). We simply replace all emojis in
the training and test data with a special <emoji>
token. Models trained with this data are able to
recopy <emoji> placeholders at the correct posi-
tion.14 At test time, we replace target-side place-
holders with source-side emojis in the same order.

We use the same solution to deal with Reddit
user names (e.g., /u/frenchperson) and subred-
dit names (e.g., /r/france). MT models some-
times fail to recopy them (e.g., /u/fran√ßais).
For this reason, we identify such names with regu-
lar expressions (robust to small variations: without
leading / or with extra spaces), and replace them
with <user> and <reddit> placeholders.

4.3 Natural noise
We extract noisy variants of known words from
the MTNT monolingual data, thanks to French
and English lexicons and an extended edit dis-
tance (allowing letter swaps and letter repetitions).
We also manually build a list of noise rules,
with the most common spelling errors in English
(e.g., your/you‚Äôre, it/it‚Äôs) and French (e.g., √ßa/sa,
√†/a), punctuation substitutions, letter swaps, spaces
around punctuation and accent removal. Then
we randomly replace words with noisy variants
and apply these noise rules on the source side of
MTNT-train, CommonCrawl and News Commen-
tary (MTNT-train, TED and KFTT for EN‚ÜíJA),
and concatenate these noised versions to the clean
training corpus.

4.4 Back-translation
Back-translation (Sennrich et al., 2016a; Edunov
et al., 2018) is a way to take advantage of large
amounts of monolingual data. This is particularly
useful for domain adaptation (when the parallel

14We ensure that there is always the same number of place-
holders on both sides of the training corpus.

data is not in the right domain), or for low-resource
MT (when parallel data is scarce).

In this task, we hope that back-translation can
help on JA‚ÜíEN, where we have less parallel data,
and on FR‚ÜîEN to expand vocabulary coverage (in
particular w.r.t. recent named entities and news
topics which are often evoked on Reddit).

Table 3 describes the monolingual data which
is available for constrained submissions. News-
discuss (user comments on the Web about news
articles) is probably more useful than news-crawl
as it is closer to the domain. We use our baseline
models presented in Section 3 to back-translate the
monolingual data. Following Edunov et al. (2018),
we do sampling instead of beam search, with a soft-
max temperature of 10.9 .

In all language pairs, we back-translate the tar-
get language MTNT monolingual data, with one
different sampling for each epoch. We also back-
translate the following data:

‚Ä¢ JA‚ÜíEN: 120
th of news-discuss.en per epoch

(with rotation at the 21th epoch).
‚Ä¢ FR‚ÜíEN: 15

th of news-discuss.en per epoch
(with rotation at the 6th epoch).

‚Ä¢ EN‚ÜíFR: news-discuss.fr with one different
sampling for each epoch and 15

th of news-
crawl.fr (with rotation at the 6th epoch).

4.5 Tags
We insert a tag at the beginning of each source
sentence, specifying its type: <BT> for back-
translations, <noise> for natural noise, <real>
for real data, and <rev> for MTNT data in the
reverse direction (e.g., for JA‚ÜíEN MT, we con-
catenate MTNT JA‚ÜíEN and ‚Äúreversed‚Äù MTNT
EN‚ÜíJA). Like Vaibhav et al. (2019), we found that
‚Äúisolating‚Äù the back-translated data with a different
source-side tag gave better BLEU scores. At test
time, we always use the <real> tag.

Like Kobus et al. (2017), we also use tags
for domain adaptation. We prepend a tag to all
source sentences specifying their corpus. For in-
stance, sentences from MTNT get the <MTNT> tag
and those from Europarl get the <europarl> tag.
These ‚Äúcorpus‚Äù tags are used in conjunction with
the ‚Äútype‚Äù tags (e.g., MTNT back-translated sen-
tences begin with <MTNT> <BT>). At test time, we
use <MTNT> to translate MTNT-domain text, and
no corpus tag at all to translate out-of-domain text.

We found that this method is roughly as good
for domain adaptation as fine-tuning. We settle



531

Model Test Valid Blind
MTNT 6.7‚Ä† ‚Äì 5.8
MTNT fine-tuned 9.8‚Ä† ‚Äì ‚Äì
Transformer base + tags 13.5 11.2 13.7
+ Back-Translation (BT) 15.0 12.8 14.1
+ Trans. big architecture ‚àó‚àó 15.5 12.4 14.0
+ Ensemble of 4 ‚àó 16.6 13.7 15.5

Table 6: BLEU scores of the JA‚ÜíEN models on
MTNT-test, MTNT-valid and MTNT-blind.

Model Test Valid Blind
MTNT 9.0‚Ä† ‚Äì 8.4
MTNT fine-tuned 12.5‚Ä† ‚Äì ‚Äì
Transformer base + tags 19.5 19.0 16.6
+ BT + natural noise ‚àó‚àó 19.4 19.4 16.8
+ Ensemble of 6 ‚àó 20.7 21.2 17.9

Table 7: BLEU scores of the EN‚ÜíJA models.

with corpus tags (rather than fine-tuning), as it is
more flexible, less tricky to configure and has bet-
ter properties on out-of-domain text.

5 Results

Tables 6, 7, 8 and 9 give the BLEU scores of
our models on the MTNT-valid, MTNT-test and
MTNT-blind sets (i.e., final results of the task). For
FR‚ÜîEN we also give BLEU scores on news-test
2014, to compare with the literature, and to mea-
sure general-domain translation quality after do-
main adaptation. For news-test, we use Moses‚Äô
normalize-punctuation.perl on the MT out-
puts before evaluation.

‚ÄúMTNT‚Äù and ‚ÄúMTNT fine-tuned‚Äù are the base-
line models of the task organizers (Michel and
Neubig, 2018). The models marked ‚àó and ‚àó‚àó were
submitted respectively to the competition as pri-
mary and secondary systems. Our primary ensem-
ble models ranked first in all translation directions

Model Test News Blind
MTNT 23.3‚Ä† ‚Äì 25.6
MTNT fine-tuned 30.3‚Ä† ‚Äì ‚Äì
Transformer big 39.1 39.3 40.9
+ MTNT + tags 43.1 39.2 45.0
+ BT + natural noise ‚àó‚àó 44.3 40.2 47.0
+ Ensemble of 4 ‚àó 45.7 40.9 47.9

Table 8: BLEU scores of the FR‚ÜíEN models on
MTNT-test, news-test 2014 and MTNT-blind.

Model Test News Blind
MTNT 21.8‚Ä† ‚Äì 22.1
MTNT fine-tuned 29.7‚Ä† ‚Äì ‚Äì
Transformer big 33.1 40.7 37.0
+ MTNT + tags 38.8 40.2 39.0
+ BT + natural noise ‚àó‚àó 40.5 42.3 41.0
+ Ensemble of 4 ‚àó 41.0 42.9 41.4

Table 9: BLEU scores of the EN‚ÜíFR models.

(with +0.7 up to +3.1 BLEU compared to the next
best result). ‚Ä† means that different SacreBLEU pa-
rameters were used (namely ‚Äúintl‚Äù tokenization).

The ‚Äúrobustness‚Äù techniques like inline cas-
ing, emoji/Reddit placeholders and natural noise
had little to no impact on BLEU scores. They
solve problems that are too rare to be accurately
measured by BLEU. For instance, we counted 5
emojis and 36 ‚Äúexceptionally‚Äù capitalized words
in MTNT-test. Improvements could be mea-
sured with BLEU on test sets where these phe-
nomena have been artificially increased: e.g.,
an all-uppercase test set, or the natural noise of
Karpukhin et al. (2019).

Most of the BLEU gains were obtained thanks
to careful data filtering and pre-processing, and
thanks to domain adaptation: back-translation and
integration of in-domain data with corpus tags.
Punctuation fixes We looked at the translation
samples on the submission website, and observed
that the French references used apostrophes (‚Äô) and
angle quotes (¬´ and ¬ª). This is inconsistent with the
training data (including MTNT), which contains
mostly single quotes (') and double quotes ("). A
simple post-processing step to replace quotes led
to a BLEU increase of 5 points for EN‚ÜíFR.15

6 Conclusion
We presented our submissions to the WMT Ro-
bustness Task. The goal of this task was to
build Machine Translation systems that are robust
to the types of noise found on social media, in
two language pairs (French-English and Japanese-
English). Thanks to careful pre-processing and
data filtering, and to a combination of several do-
main adaptation and robustness techniques (spe-
cial handling of capital letters and emojis, natural
noise injection, corpus tags and back-translation),
our systems ranked first in the BLEU evaluation in
all translation directions.

15The organizers and participants were informed of this.



532

References
Yonatan Belinkov and Yonatan Bisk. 2018. Syn-

thetic and Natural Noise Both Break Neural Machine
Translation. In ICLR.

Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie
Zhai, and Yang Liu. 2018. Towards Robust Neural
Machine Translation. In ACL.

Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding Back-Translation at
Scale. In EMNLP.

Mercedes Garcia-Martinez, Loic Barrault, and Fethi
Bougares. 2016. Factored Neural Machine Transla-
tion Architectures. In IWSLT.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu,
Renqian Luo, Arul Menezes, Tao Qin, Frank Seide,
Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce
Xia, Dongdong Zhang, Zhirui Zhang, and Ming
Zhou. 2018. Achieving Human Parity on Automatic
Chinese to English News Translation. arXiv.

Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, and
Marjan Ghazvininejad. 2019. Training on Synthetic
Noise Improves Robustness to Natural Noise in Ma-
chine Translation. arXiv.

Huda Khayrallah and Philipp Koehn. 2018. On the Im-
pact of Various Types of Noise on Neural Machine
Translation. In Proceedings of the Second Work-
shop on Neural Machine Translation and Generation
(WNMT).

Diederik Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In ICLR.

Catherine Kobus, Josep Crego, and Jean Senellart.
2017. Domain Control for Neural Machine Trans-
lation. In RANLP.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond≈ôej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL.

Taku Kudo. 2018. Subword Regularization: Improv-
ing Neural Network Translation Models with Multi-
ple Subword Candidates. In ACL.

Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for Neural Text Processing.
In EMNLP.

Pavel Levin, Nishikant Dhanuka, and Maxim Khalilov.
2017. Machine Translation at Booking.com: Jour-
ney and Lessons Learned. In EAMT.

Xian Li, Paul Michel, Antonios Anastasopoulos,
Yonatan Belinkov, Nadir K. Durrani, Orhan Firat,
Philipp Koehn, Graham Neubig, Juan M. Pino, and
Hassan Sajjad. 2019. Findings of the First Shared
Task on Machine Translation Robustness. In WMT.

Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In ACL.

Paul Michel and Graham Neubig. 2018. MTNT: A
Testbed for Machine Translation of Noisy Text. In
EMNLP.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise Prediction for Robust, Adaptable
Japanese Morphological Analysis. In ACL-HLT.

Myle Ott, Sergey Edunov, David Grangier, and Michael
Auli. 2018. Scaling Neural Machine Translation. In
WMT.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wj Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL.

Matt Post. 2018. A Call for Clarity in Reporting BLEU
Scores. In WMT.

Rico Sennrich and Barry Haddow. 2016. Linguistic In-
put Features Improve Neural Machine Translation.
In WMT.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving Neural Machine Translation Mod-
els with Monolingual Data. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural Machine Translation of Rare Words
with Subword Units. In ACL.

Jason R Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt Cheap Web-Scale Parallel
Text from the Common Crawl. In ACL.

Matthias Sperber, Jan Niehues, and Alex Waibel.
2017. Toward Robust Neural Machine Translation
for Noisy Input Sequences. In IWSLT.

Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way.
2018. Attaining the Unattainable? Reassessing
Claims of Human Parity in Neural Machine Trans-
lation. In WMT.

Vaibhav, Sumeet Singh, Craig Stewart, and Graham
Neubig. 2019. Improving Robustness of Machine
Translation with Synthetic Noise. In NAACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. In NIPS.

http://arxiv.org/abs/1711.02173
http://arxiv.org/abs/1711.02173
http://arxiv.org/abs/1711.02173
http://arxiv.org/abs/1805.06130
http://arxiv.org/abs/1805.06130
http://arxiv.org/abs/1808.09381
http://arxiv.org/abs/1808.09381
https://arxiv.org/abs/1609.04621
https://arxiv.org/abs/1609.04621
http://arxiv.org/abs/1803.05567
http://arxiv.org/abs/1803.05567
http://arxiv.org/abs/1902.01509
http://arxiv.org/abs/1902.01509
http://arxiv.org/abs/1902.01509
http://arxiv.org/abs/1805.12282
http://arxiv.org/abs/1805.12282
http://arxiv.org/abs/1805.12282
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1612.06140
http://arxiv.org/abs/1612.06140
https://aclweb.org/anthology/papers/P/P07/P07-2045/
https://aclweb.org/anthology/papers/P/P07/P07-2045/
http://arxiv.org/abs/1804.10959
http://arxiv.org/abs/1804.10959
http://arxiv.org/abs/1804.10959
http://arxiv.org/abs/1808.06226
http://arxiv.org/abs/1808.06226
http://arxiv.org/abs/1808.06226
http://arxiv.org/abs/1707.07911
http://arxiv.org/abs/1707.07911
http://www.statmt.org/wmt19/robustness.html
http://www.statmt.org/wmt19/robustness.html
https://aclweb.org/anthology/papers/P/P12/P12-3005/
https://aclweb.org/anthology/papers/P/P12/P12-3005/
http://arxiv.org/abs/1809.00388
http://arxiv.org/abs/1809.00388
https://aclweb.org/anthology/papers/P/P11/P11-2093/
https://aclweb.org/anthology/papers/P/P11/P11-2093/
http://arxiv.org/abs/1806.00187
https://aclanthology.info/papers/P02-1040/p02-1040
https://aclanthology.info/papers/P02-1040/p02-1040
http://arxiv.org/abs/1804.08771
http://arxiv.org/abs/1804.08771
https://arxiv.org/abs/1606.02892
https://arxiv.org/abs/1606.02892
https://arxiv.org/abs/1511.06709
https://arxiv.org/abs/1511.06709
https://arxiv.org/abs/1508.07909
https://arxiv.org/abs/1508.07909
https://aclweb.org/anthology/papers/P/P13/P13-1135/
https://aclweb.org/anthology/papers/P/P13/P13-1135/
https://aclweb.org/anthology/papers/P/P18/P18-1163/
https://aclweb.org/anthology/papers/P/P18/P18-1163/
https://aclweb.org/anthology/papers/W/W18/W18-6312/
https://aclweb.org/anthology/papers/W/W18/W18-6312/
https://aclweb.org/anthology/papers/W/W18/W18-6312/
http://arxiv.org/abs/1902.09508
http://arxiv.org/abs/1902.09508
https://arxiv.org/abs/1706.03762
https://arxiv.org/abs/1706.03762

