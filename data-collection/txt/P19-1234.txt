



















































Unsupervised Learning of PCFGs with Normalizing Flow


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2442–2452
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2442

Unsupervised Learning of PCFGs with Normalizing Flow
Lifeng Jin

Department of Linguistics
The Ohio State University
jin.544@osu.edu

Finale Doshi-Velez
Harvard University

finale@seas.harvard.edu

Timothy Miller
Boston Children’s Hospital &

Harvard Medical School
timothy.miller@childrens.harvard.edu

William Schuler
Department of Linguistics
The Ohio State University
schuler@ling.osu.edu

Lane Schwartz
Department of Linguistics

University of Illinois at Urbana-Champaign
lanes@illinois.edu

Abstract

Unsupervised PCFG inducers hypothesize sets
of compact context-free rules as explanations
for sentences. These models not only pro-
vide tools for low-resource languages, but also
play an important role in modeling language
acquisition (Bannard et al., 2009; Abend
et al., 2017). However, current PCFG induc-
tion models, using word tokens as input, are
unable to incorporate semantics and morphol-
ogy into induction, and may encounter issues
of sparse vocabulary when facing morpholog-
ically rich languages. This paper describes a
neural PCFG inducer which employs context
embeddings (Peters et al., 2018) in a normal-
izing flow model (Dinh et al., 2015) to ex-
tend PCFG induction to use semantic and mor-
phological information1. Linguistically moti-
vated similarity penalty and categorical dis-
tance constraints are imposed on the inducer
as regularization. Experiments show that the
PCFG induction model with normalizing flow
produces grammars with state-of-the-art accu-
racy on a variety of different languages. Abla-
tion further shows a positive effect of normal-
izing flow, context embeddings and proposed
regularizers.

1 Introduction

Unsupervised PCFG inducers (Jin et al., 2018b)
automatically bracket sentences into nested spans,
and label these spans with consistent, linguisti-
cally relevant syntactic categories, which may be
useful in downstream applications or linguistic re-
search on under-resourced languages. Their suc-
cess also provides evidence for learnability of
grammar in absence of strong linguistic univer-
sals (MacWhinney and Bates, 1993; Plunkett and
Wood, 2004; Bannard et al., 2009). However, cur-
rent PCFG induction models, using word tokens

1The code can be found at https://github.com/
lifengjin/acl_flow

as input, are unable to incorporate semantics and
morphology into induction, and may encounter is-
sues of sparse vocabulary when facing morpholog-
ically rich languages.

This paper describes a PCFG induction model
which exploits recent advances in deep generative
models and context embeddings to generalize over
rare, morphologically rich forms. We contextual-
ize a PCFG’s terminal emission rules with con-
text embeddings (Peters et al., 2018) as observa-
tions, in order to bring context and subword in-
formation into the model. Probabilities for these
contextualized terminal emission rules are mod-
eled by transforming distributions with normal-
izing flow (Rezende and Mohamed, 2015; Dinh
et al., 2015; He et al., 2018). Through invert-
ible transformations, flow models transform sim-
ple distributions (e.g. Gaussian) into complex and
potentially multi-modal distributions over obser-
vation vectors. These improvements help increase
the expressivity of the induction model and give
the model the ability to generalize over rare words,
but still preserve the tractability of marginal like-
lihood computation so that inference is possible
with marginal likelihood maximization.

Experiments described in this paper show that
the model is able to achieve state-of-the-art or
competitive results on multiple languages com-
pared with existing PCFG induction and unla-
beled tree induction models, especially on lan-
guages where complex morphology may cause in-
duction models with discrete observations to suc-
cumb to data sparsity. Further analyses show (1)
that the flow-based inducer is able to use morpho-
logical and semantic information in embeddings
for grammar induction, (2) that the model pro-
duces consistent and meaningful labels at phrasal
and lexical levels, and (3) that both the normal-
izing flow and the linguistically-motivated regu-
larization terms make substantial improvements to

https://github.com/lifengjin/acl_flow
https://github.com/lifengjin/acl_flow


2443

parsing accuracy.

2 PCFGs with vector terminals

We first consider factoring the Chomsky nor-
mal form PCFG with C non-terminal categories
into two separate parts: binary-branching non-
terminal expansion rule2 probabilities, and unary-
branching terminal emission rule probabilities.
Given a tree as a set τ of nodes η undergoing
non-terminal expansions cη → cη1 cη2 (where
η ∈ {1, 2}∗ is a Gorn address specifying a path of
left or right branches from the root), and a set τ′ of
nodes η undergoing terminal emissions cη → xη
(where xη is an embedding for the word at node
η), the marginal probability of a sentence σi can
be computed as:

P(σi) =
∑
τ,τ′

∏
η∈τ

P(cη → cη1 cη2) ·
∏
η∈τ′

P(cη → xη)

(1)

We first define a set of Bernoulli distributions
that distribute probability mass between these two
sets of rules:

P(Term = 1 | cη) =
1

1 + exp(−δ>cηd)
, (2)

where cη is a non-terminal category, δcη is a Kro-
necker delta function – a vector with value one at
index cη and zeros everywhere else – and δ>cηd is a
parameter for the Bernoulli distribution of cη with
d ∈ RC .

Binary-branching non-terminal expansion rule
probabilities for a non-terminal category cη are de-
fined as:

P(cη → cη1 cη2) =

P(Term = 0 | cη) ·
exp(δ>cηN)(δcη1 ⊗ δcη2)

exp(δ>cηN)1
(3)

where ⊗ is a Kronecker product, cη1 is the cate-
gory of the left child, cη2 is the category of the
right child, and δ>cηN is a parameter vector for the
multinomial distribution of the category cη with
N ∈ RC×C2 .

The contextualized unary-branching terminal
emission rule probabilities for a preterminal cat-
egory cη are defined as:

P(cη → xη) = P(Term = 1 | cη) · fcη(xη; δ>cηL) (4)
2They include the expansion rules generating the top node

in the tree.

where the terminal at node η is an observed word
token, xη ∈ RD is the vectorial representation of
that token, fcη is a probability density or mass
function, and δ>cηL is a parameter vector for the
probability function of the category cη. We can re-
cover the multinomial PCFG formulation by set-
ting xη to be a one-hot word representation and the
probability function fcη to be a multinomial distri-
bution parameterized by δ>cηL. We can also set xη
to be a word embedding and fcη to be Gaussian
distributions parameterized by δ>cηL, giving us a
PCFG with Gaussian emission.

In order to incorporate more information into
the induction model, context embeddings (Peters
et al., 2018) can be used here for xη. The ELMo
model combines learned word embeddings with
character embeddings through CNN encoders, and
composes contextualized embeddings with bidi-
rectional LSTMs over the combined representa-
tions. The output from the BiLSTM contains both
subword information, word information and con-
text information and is used as contextualized em-
beddings for words. While simple D-dimensional
multivariate Gaussians can be used as the emis-
sion density f , it is unrealistic to assume that such
embeddings follow simple Gaussian distributions.
This work explores more complex transformed
distributions using normalizing flows.

3 Normalizing flows

Flow models (Dinh et al., 2015, 2017; Kingma
and Dhariwal, 2018) are a class of deep genera-
tive models that model unknown yet complex dis-
tributions by transforming the observation through
a series of invertible transformations to create la-
tent representations to be used with known distri-
butions like Gaussians. For PCFG induction with
embeddings, we first consider the generative story
for the observed embeddings. Let cη be a category
label at the node η. M ∈ RC×D is the matrix of
the means of the Gaussian distributions for the la-
tent representations, and S ∈ RC×D the diagonal
covariances with L = [M; S]. A probability model
over trees may be defined as follows:

1. Sample an expansion decision Term ∼
Bernoulli

(
1

1+exp(−δcη>d)

)
to expand node ηwith

category cη to a lexical item, or to a binary
branch.

2. If expanded as a binary branch (Term=0),
given the category of the node cη,



2444

sample a non-terminal expansion,

cη1 cη2 ∼ Mult
(

exp(δcη
>N)

exp(δcη
>N)1

)
.

3. If lexically expanded (Term = 1), sam-
ple from Gaussian with diagonal covari-
ance over latent representations: hη ∼
N(δcη>M, diag(δcη>S)).

4. Again, if Term=1, transform the latent rep-
resentation deterministically to generate the
observed embedding xη for the token at η:
xη = g(hη).

In order to compute the likelihood given the ob-
servation, we need to invert this process. If we
integrate over x′η = g(hη), with the change-of-
variable formula, we have:

fcη(xη; δ
>
cηL) =

∫
P(cη → hη) δ(xη − g(hη)) dhη

=

∫
P(cη → g−1(x′η)) δ(xη − x′η)

∣∣∣∣∣det∂g−1∂x′η
∣∣∣∣∣ dx′η

= P(cη → g−1(xη)) ·
∣∣∣∣∣det∂g−1∂xη

∣∣∣∣∣, (5)
where δ here is the Dirac delta function. This can
be used to directly compute the likelihood of the
observed embedding exactly given a category. In
order to make this calculation tractable, the re-
quirements on g−1 are usually (1) that it is invert-
ible, and (2) that computing the log Jacobian de-
terminant is possible without calculating the full
Jacobian matrix or its full determinant. Note that
g need not be explicitly constructed as it is usually
only used in generation, not in inference.

There have been many proposed invertible func-
tions that can be used as g−1. The volume preserv-
ing invertible transformation is first proposed by
Dinh et al. (2015) in the NICE model and later
used in unsupervised learning (He et al., 2018).
Because of the volume preserving property, the
log Jacobian determinant is always 0. This prop-
erty may allow the structural features of the orig-
inal embedding space to be better preserved than
other, less restrictive, invertible functions.

The invertible transformation g−1 consists of I
stacked-up coupling layers. The input x to it is di-
vided into two equal parts h(0)1 ,h

(0)
2 :

g−1
( h(0)1h(0)2

 ) = h(I)1h(I)2
 , (6)

and the coupling layers in g−1 transform the two
parts at alternating layers:

h(i−1)1h(i−1)2
 = h(i−2)1h(i−2)2 + q(i−1)(h(i−2)1 )

 ;h(i)1h(i)2
 = h(i−1)1 + q(i)(h(i−1)2 )h(i−1)2

 . (7)
The volume-preserving restriction is removed in
the coupling layer in the Real NVP model (Dinh
et al., 2017), in which the coupling layers trans-
form the inputs as follows:

h(i−1)1h(i−1)2
 =h(i−2)1h(i−2)2 � exp(q(i−1)1 (h(i−2)1 )) + q(i−1)2 (h(i−2)1 )

 ;h(i)1h(i)2
 = h(i−1)1 � exp(q(i)1 (h(i−1)2 )) + q(i)2 (h(i−1)2 )h(i−1)2

 ,
(8)

where � is a Hadamard product. All q : RD/2 →
RD/2 in both models can be arbitrary nonlinear
transformations. For Real NVP, the log Jacobian
determinant is:

I/2∑
i=1

(
q(2i−1)1 (h

(2i−2)
1 ) + q

(2i)
1 (h

(2i−1)
2 )

)>
1. (9)

4 Regularization

In order to avoid undesirable yet possible gram-
mars, we impose two linguistically-motivated reg-
ularization terms onto the model. In experiments
described in this paper, for the emission parame-
ters, we want to discourage the model from find-
ing a solution in which all words are equally likely
to be generated by any category, so we impose
a regularization term on the model to encourage
the rows of M to be far apart. The flow models
can learn arbitrary transformations over the pre-
trained context embeddings. Because each token
in the corpus has an embedding, the flow models
may learn transformations that cue off arbitrary in-
formation in those embeddings, effectively mak-
ing changes to observations. A Euclidean distance
penalty is put between the output of the flow trans-
formation g−1(xη) and the input embedding xη to
penalize the output drifting too far from the input



2445

embedding. The final objective to maximize is:

L(σ) =
1
|σ|

|σ|∑
i=0

log P(σi) + λ1
∑
d,e

‖δ>d M − δ>e M‖2

− λ2
∑
η∈σi
‖g−1(xη) − xη‖2, (10)

where σ is a minibatch of sentences, a, b, c, d, e
are all category labels, λ1 and λ2 are the weights
for the two regularization terms and ‖ . . . ‖n is the
n-norm.

5 Experiments

We report results of labeled parsing evaluation
and unlabeled parsing evaluation against exist-
ing grammar induction and unsupervised parsing
models. We evaluate our models on full English
(The Penn Treebank; Marcus et al., 1993), Chi-
nese (The Chinese Treebank 5.0; Xia et al., 2000)
and German (NEGRA 2.0; Skut et al., 1998) con-
stituency treebanks and the 20-or-fewer-word sub-
sets for labeled parsing performance.3 For unla-
beled parsing evaluation, we first report results on
a set of languages with complex morphology cho-
sen prior to evaluation. This set includes Czech
and Russian, which are fusional languages, Ko-
rean and Uyghur, which are agglutinative lan-
guages, and Finnish, which has elements of both
types. Dependency trees from the Universal De-
pendency Treebank (Nivre et al., 2016) of these
languages are converted into constituency trees
(Collins et al., 1999) by keeping constituents that
have a single incoming and no outgoing depen-
dency arc. For example, constituents like noun
phrases that are kept in conversion may only have
one incoming arc from the main verb, and no out-
going arc to any modifier. Each dataset has 15,000
sentences randomly sampled from the dependency
treebank (if the treebank has enough sentences),
or is augmented with sentences randomly sampled
from Wikipedia (if the treebank has fewer sen-
tences). Finally, unlabeled parsing experiments on
the three constituency treebanks are reported, one
following Jin et al. (2018a) and one following Htut
et al. (2018).

The hyperparameters of the model for all ex-
periments are tuned on the Brown Corpus portion
of the Penn Treebank. We set the number of cate-
gories C to 30, the categorical distance constraint
strength λ1 to be 0.0001, and the drifting penalty

3WSJ20test is the second half of WSJ20.

λ2 to be 10. Function g−1 is set to have 4 coupling
layers with q(i) being a feed-forward network with
one hidden layer for both NICE and Real NVP,
following He et al. (2018). We train the system
until the marginal likelihood over the whole train-
ing set starts to oscillate, around 10,000 batches
for smaller corpora and around 20,000 for larger
corpora. Because the inside algorithm is quadratic
on the length of the sentences, the batch size for
training gets quadratically smaller from 400 to 1 as
sentences get longer. We use the Adam optimizer
(Kingma and Ba, 2015), initialized with learning
rates 0.1 for d and N, and 0.001 for L and parame-
ters in g−1. Means and standard deviations of eval-
uation metrics are reported in tables with 10 runs
of the proposed system.

We use ELMo embeddings (Peters et al., 2018)
with 1024 dimensions from averaging represen-
tations from two BiLSTM layers and the word
encoder in ELMo for all languages (Che et al.,
2018).4 These embeddings are each trained with
20 million words from Wikipedia and Common
Crawl. We initialize d and N with multinomials
drawn from a Dirichlet distribution with 0.2 as the
concentration parameter, following PCFG induc-
tion work with Bayesian models (Jin et al., 2018b).
We assign the same diagonal variance matrix to
all latent Gaussian distributions, calculated empir-
ically from embeddings from 5000 randomly sam-
pled sentences. M is initialized with the empirical
mean of the same sampled embeddings, but with
random Gaussian noise added to each row. The
parameters of the normalizing flow g−1 are initial-
ized from a uniform distribution with 0 mean and
a standard deviation of

√
1/D.

For labeled constituency evaluation, we com-
pare against the state-of-the-art PCFG induction
system DIMI (D2K15: depth bounded at 2 and 15
categories; Jin et al., 2018a) which takes word to-
kens as input and produces labeled trees.5 For un-
labeled constituency evaluation, results from other
unsupervised systems are used for comparison, in-
cluding CCL (Seginer, 2007), UPPARSE (Pon-
vert et al., 2011), PRPN (Shen et al., 2018), as
well as systems which use gold part-of-speech
tags: DMV+CCM (Klein and Manning, 2002) and
UML-DOP (Bod, 2006).

4https://github.com/HIT-SCIR/
ELMoForManyLangs.

5The DB-PCFG system (Jin et al., 2018b) is formally
equivalent to the DIMI system.

https://github.com/HIT-SCIR/ELMoForManyLangs
https://github.com/HIT-SCIR/ELMoForManyLangs


2446

Model WSJ20test WSJ CTB20 CTB NEGRA20 NEGRA

µ(σ) max µ(σ) max µ(σ) max µ(σ) max µ(σ) max µ(σ) max

DIMI 23.0(6.5) 34.1 - - 15.4(4.4) 20.7 - - 13.6(1.6) 17.5 - -
this work 22.8(6.0) 24.0 22.2(3.8) 27.0 19.7(1.9) 24.0 13.8(3.4) 20.2 26.2(2.8) 30.4 24.5(2.7) 29.1

Table 1: Recall-V-Measure scores for labeled grammar induction models trained on the listed treebanks with punc-
tuation. For all tables, µ (σ) means the mean (standard deviation) of the reported scores.

5.1 Labeled parsing evaluation
Metric: Labeled trees induced by DIMI (Jin et al.,
2018a) and the flow-based system are evaluated
on six different datasets. In this evaluation, pre-
dicted labels of induced constituents that are in
gold trees are compared against gold labels of
these constituents6 using V-Measure (Rosenberg
and Hirschberg, 2007). Recall of the induced trees
is used to weight these V-Measure scores. The fi-
nal Recall-V-Measure (RVM) score is computed
as the product of these two measures. RVM can
be maximized when gold constituents are included
in induced trees and their clustering is consistent
with gold annotation. RVM is equal to unlabeled
recall when the matching constituents have the
same clustering of labels as the gold annotation.

Results: Left- and right-branching baselines are
constructed by assigning 21 random labels7 to
constituents in purely left- and right-branching
trees. However, both branching baselines perform
poorly in this evaluation, due to the fact that there
is no straightforward way to assign labels to con-
stituent spans that may correspond to how gold la-
bels are organized. VM scores for both baselines
are close to 0, leading to RVM scores close to
0. Table 1 shows RVM scores for both the DIMI
system and the flow-based system. For the labeled
grammar induction systems, results show that the
flow-based model outperforms DIMI on two of the
three test datasets. Table 3 shows only the per-
formance of the systems on bracketing. Although
DIMI performs much better than the flow-based
system in terms of bracketing F1 on WSJ20test,
the flow-based system’s performance on average
RVM is much closer to DIMI, which indicates that
the flow-based system assigns more consistent la-
bels to constituents than DIMI. On CTB20 and
NEGRA20, where the bracketing performance of
the flow-based system is better, this system out-

6The maximal projection category is used when a span
is labeled with several categories in the gold annotation. All
functional tags are removed.

7There are 21 phrase level tags in the Penn Treebank II
tag set.

performs DIMI by a large margin on RVM. Also,
runs with the highest performance on bracketing
are not the highest on RVM in general, showing
that for labeled induction models, bracketing ac-
curacy may be traded for labeling accuracy.

Confusion matrix: Figure 1 shows the gold
constituent recall on NEGRA20 for the two la-
beled grammar induction systems. We show 5
main phrasal categories in gold annotation and in
a run of predicted trees. Grammars from DIMI are
prone to category collapse in which only a few
categories are active as non-terminals. Figure 1a
shows that categories 8 and 3 are the main ac-
tive categories containing the majority of all con-
stituents, with category 8 covering 78% of all S
categories, 23% of NPs, and many others. In Fig-
ure 1b, the clear diagonal pattern for the flow-
based model shows that the gold categories do
have separate corresponding predicted categories.
For example, VP is almost exclusively in cate-
gory 1 if appears in the predicted trees and PP
is predominately in category 27. NP has a wider
spread across predicted categories, but category 8
is mostly used to represent it.

5.2 Unlabeled parsing evaluation

We additionally perform three unlabeled parsing
evaluations against baseline systems. The first ex-
periment uses a set of dependency-derived tree-
banks in morphologically rich languages to exam-
ine how morphology is used by the proposed sys-
tem. The second experiment induces on datasets
used in Jin et al. (2018a) and the final experiment
uses the WSJ, CTB and NEGRA datasets without
any punctuation for evaluation against published
results by Htut et al. (2018).

Morphologically rich languages: Table 2
shows unlabeled parsing performance on the mor-
phologically rich languages described at the be-
ginning of this section, compared against branch-
ing baselines and DIMI. There is a substantial per-
formance improvement observed across all lan-
guages when context embeddings are used as ob-



2447

8 3 5 7 1
Other

NotInPred

S

NP

VP

PP

AP

Other

0.78 0.00 0.00 0.00 0.00 0.00 0.21

0.23 0.35 0.01 0.01 0.00 0.00 0.41

0.04 0.02 0.00 0.00 0.00 0.00 0.93

0.13 0.38 0.00 0.02 0.00 0.00 0.47

0.07 0.03 0.01 0.08 0.00 0.00 0.81

0.25 0.09 0.01 0.01 0.00 0.00 0.63
0.0

0.2

0.4

0.6

0.8

1.0

Labeled Constituent Recall

(a) the DIMI system.

18 8 1 27
Other 21

NotInPred

S

NP

VP

PP

AP

Other

0.67 0.00 0.01 0.00 0.06 0.07 0.18

0.02 0.29 0.00 0.14 0.11 0.17 0.26

0.01 0.00 0.19 0.00 0.02 0.01 0.77

0.01 0.06 0.02 0.44 0.08 0.08 0.32

0.00 0.01 0.04 0.01 0.36 0.01 0.57

0.16 0.08 0.08 0.02 0.23 0.05 0.39
0.2

0.4

0.6

0.8

1.0

Labeled Constituent Recall

(b) the flow-based system.

Figure 1: The confusion matrices for DIMI and the
flow-based system on the constituents in NEGRA20.
The runs with best RVM scores are chosen for plotting.
NotInPred means the proportion of gold constituents
not in predicted trees.

servations. Korean and Uyghur both have very
sparse vocabulary, leading to poor performance of
the DIMI system.

Constituency treebanks: We also compare the
flow-based system to published unlabeled parsing
results from previous work. Table 3 shows the un-
labeled parsing F1 scores for several grammar in-
duction systems on the WSJ20test, CTB20 and
NEGRA20 datasets reported in Jin et al. (2018a).
Posterior inference on constituents (PIoC) pro-
posed in Jin et al. (2018a) is also used with parse
trees from 10 runs of the flow-based system. The
flow-based system is able to produce more accu-
rate trees on the CTB20 and NEGRA20 datasets
despite not being depth-bounded. However, its
performance is subpar on the WSJ20test dataset.

Finally, the flow-based model is compared
against other unsupervised parsing models on the

Lang. LB RB DIMI this work
µ (σ) µ (σ)

Czech 24.8 50.3 49.3 (8.5) 52.9 (4.7)
Finnish 30.5 52.1 49.0 (5.0) 52.5 (5.2)
Korean 40.4 20.2 22.6 (2.1) 51.1 (2.6)
Russian 45.5 28.7 50.2 (8.1) 58.0 (4.7)
Uyghur 45.8 24.6 33.0 (3.2) 54.1 (1.4)

Table 2: Unlabeled recall scores on a set of morpholog-
ically rich languages for the proposed system, DIMI
and the left- and right-branching baselines.

System WSJ20test CTB20 NEGRA20

CCL 60.9 37.1 33.7
UPPARSE 43.9 38.2 47.7

DB-PCFG 60.5 - -
DIMI 63.1 38.9 40.8
this work 51.7 43.5 48.2

Table 3: Unlabeled parsing F1 scores for different
grammar induction systems trained on only the 20
words or less subsets of the three constituency tree-
banks as in Jin et al. (2018a).

three full constituency treebanks and their 10-or-
fewer-word subsets, trained with sentences with-
out punctuation in training, following Htut et al.
(2018). The results are shown in Table 4. First, the
flow-based system performs better than reported
results from all systems, using raw text only, on
both NEGRA and CTB, showing that the system
is able to accurately generate structure. Second,
there is a smaller performance gap between the
flow-based system and the best-performing one on
WSJ than on WSJ10.

The fact that the flow-based model underper-
forms on English may be due to the fact that

10 15 20 25 30 35
Embedding Distance

0

20

Re
ca

ll 
Di

ffe
re

nc
e

Finnish
WSJ20test

CTB20
Czech

Korean

Russian
Uyghur

NEGRA20

Figure 2: Correlation between recall difference of the
flow-based system and DIMI and the average distance
between ELMo embeddings.



2448

Model WSJ10 WSJ CTB10 CTB NEGRA10 NEGRA

µ(σ) max µ(σ) max µ(σ) max µ(σ) max µ(σ) max µ(σ) max

CCL 67.3(0.0) 67.3 44.9(0.0) 44.9 47.8(0.0) 47.8 21.1(0.0) 21.1 48.0(0.0) 48.0 27.6(0.0) 27.6
UPPARSE 44.8(0.0) 44.8 23.6(0.0) 23.6 44.7(0.0) 44.7 24.2(0.0) 24.2 53.4(0.0) 53.4 33.4(0.0) 33.4
PRPN-UP 62.2(3.9) 70.3 26.0(2.3) 32.8 - - - - - - - -
PRPN-LM 70.5(0.4) 71.3 37.4(0.3) 38.1 - - - - - - - -

DIMI 49.0(4.8) 55.8 - - 41.1(2.9) 45.9 - - 47.5 (2.7) 54.1 - -
this work 56.0(6.1) 63.6 38.5(3.9) 42.7 49.4(1.3) 50.7 29.2(2.1) 31.9 51.8 (3.1) 58.5 37.1(2.5) 41.2

RB 61.7(0.0) 61.7 39.5(0.0) 39.5 50.4(0.0) 50.4 21.8(0.0) 21.8 43.3(0.0) 43.3 22.8(0.0) 22.8
LB 28.7(0.0) 28.7 11.6(0.0) 11.6 35.8(0.0) 35.8 11.7(0.0) 11.7 35.1(0.0) 35.1 16.9(0.0) 16.9

DMV+CCM 77.6(0.0) 77.6 - - - - - - 63.9(0.0) 63.9 - -
UML-DOP 82.9(0.0) 82.9 - - - - - - 67.0(0.0) 67.0 - -

Table 4: Unlabeled parsing F1 scores for different constituency grammar induction systems trained on the full set
of the treebanks where punctuation is removed from all data in training and evaluation with results reported in Htut
et al. (2018). PRPN models train and test on different subsets of the corpora, whereas other models use the full
corpora to train and evaluate. All models except DIMI and this work produce unlabeled trees. DMV+CCM and
UML-DOP use gold POS tags as observations for induction, listing here for reference.

the English vocabulary contains a relatively large
number of high frequency words, which makes
contexts for words similar, showing up as similar-
ities between the context embeddings for differ-
ent words. This confuses the model because it re-
lies on the observed embeddings being distinct and
representative for induction. Figure 2 shows aver-
age Euclidean distances for 50,000 pairs of ELMo
embeddings of different words randomly sampled
from each dataset. The averaged distance between
the embeddings is positively correlated with the
gain of the flow-based system over DIMI, indicat-
ing the importance of varied contexts for grammar
induction.

5.3 Induced interpretable categories
PCFG induction systems usually create syntac-
tic categories that correspond to coarse-grained
linguistic classes like nouns and verbs using co-
occurrence statistics. However the flow-based sys-
tem also creates classes that are morphological or
semantic in nature. The ability of the system to use
morphological and semantic information to help
grammar induction is shown in Table 5.

Grammars induced on Korean from the flow-
based system are greatly improved over baselines
which use words only as input. Korean is an agglu-
tinative language with many morphemes per to-
ken, so approaches that treat tokens as words must
address severe sparsity issues. As ELMo embed-
dings include subword information from Korean
characters, they may contain information useful
for understanding morphology – the nominative
clitics이 or가 and the accusative clitics을 or를,

Cat. Interp. Most common words

Korean

3 ADJ 큰 (big),많은 (many)
새로운 (new),중요한 (important)

11 N-NOM 사람이 (person),문제가 (problem)
사람들이 (people),일이 (work)

12 N-ACC 사실을 (fact),영향을 (influence)
일을 (work),의미를 (meaning)

German
7 DAT den, dem, einem, diesem, ihren
8 GEN der, des, einer, dieser, seiner, eines
20 NOM/ACC die, das, der, ein, eine, ihre, keine

Chinese

1 V-TRANS 提供(provide),进行(carry out)
举行(hold),利用(utilize)

14 V-MODAL 要(would like),会(will)
能(can),可以(be able)

28 V-SCOMP 说(say),希望(hope)
认为(think),指出(point out)

Table 5: Analysis of predicted syntactic categories
(Cat.) and their interpreted syntactic categories (In-
terp.) in runs with highest RVM scores for Korean, Ger-
man and Chinese. The most common words in each
predicted category are listed.

for example, may encode strong biases towards a
word token being a noun along with its case.

Categories like 11 and 12 in Table 5 reliably
capture nouns in the nominative and accusative
cases, respectively, even though in both cases the
marking clitic differs depending on whether the
noun preceding it ends in a vowel or consonant.
Similarly, category 3 shows noun-preceding adjec-
tives, which in Korean are formed by verb stems
plusㄴ or은, and the inducer is again able to clus-
ter words with both endings together.



2449

Model setup RVM
µ (σ) max

Multi 18.9 (1.6) 21.0

Gauss +Fasttext 17.5 (1.5) 19.4
+ELMo 23.4 (2.0) 26.7

NICE +ELMo 13.9 (4.6) 22.3
+ELMo+sim 25.7 (2.2) 28.7
+ELMo+sim+µDist 26.2 (2.8) 30.3

RNVP +ELMo+sim+µDist 24.1 (3.2) 27.9

Table 6: Parsing performance on the NEGRA20 dataset
with different configurations of the model. NICE and
RNVP are the NICE and RealNVP models used for
modeling emission. Sim and µDist are the similarity
penalty and category distance regularizers respectively.

For German, the cased articles also have sim-
ilar endings. The dative articles usually end with
-en or -em, and the genitive articles usually end
with -er or -es. Having access to the subword in-
formation, the flow-based system is able to come
up with these distinctions with no supervision, be-
cause the cases may provide important clues to
relative positions of the following nouns to verbs
or prepositions. Contextual information also helps
greatly, seen here when the system distinguishes
the genitive der in category 8 and the nominative
or accusative der in category 20 in the phrases like
der(20) Pächter der(8) Junkerstube (the lessee of
the junkerstube).

Finally, for languages like Chinese where there
are few morphological markings, semantic infor-
mation may help the system induce syntactic cat-
egories. Category 28 is a category of verbs related
to cognition and expression, which also character-
istically accepts sentential complements (Vendler,
1972; Fisher et al., 1991). Syntactic categories like
these are not seen in systems inducing with words
only. This indicates that the semantics of these
verbs may play a role here, especially since Chi-
nese has no complementizer to signal an upcom-
ing sentential complement.

5.4 Ablation experiments
Table 6 shows the ablation and comparison ex-
periments on NEGRA20. ELMo embeddings pro-
vide a large performance boost with the Gaussian
emission model over both the multinomial emis-
sion model, which has no access to contextual and
subword information, and the Gaussian emission
model with Fasttext embeddings based on charac-
ter n-grams (Joulin et al., 2016), showing that both
context and subword information helps grammar

induction. The two linguistically-motivated regu-
larization terms help the flow-based model per-
form even better. Most notably, the similarity per-
formance helps the flow models greatly by re-
stricting the freedom that the flow models have
to change the context embeddings, indicating that
the information in context embeddings is valu-
able for induction. The Real NVP model produces
higher data likelihood but its performance is lower
than other NICE-based models, indicating that the
volume-preserving property of NICE is important
for preventing overfitting.

6 Related work

Earlier work on PCFG induction (Carroll and
Charniak, 1992; Johnson et al., 2007; Liang et al.,
2009; Tu, 2012) shows that directly inducing
PCFGs from raw text is difficult. Recent work
(Shain et al., 2016; Jin et al., 2018b,a) shows
that inducing PCFGs from raw text is possi-
ble, and cognitive constraints are useful for help-
ing the induction model to find good grammars.
Closely related to PCFG induction is the task of
unsupervised constituency parsing from raw text
where trees are unlabeled. Earlier work by Seginer
(2007) and Ponvert et al. (2011) induces unla-
beled trees and achieves good results. More re-
cent work (Shen et al., 2018) utilizes complex
neural architectures for unsupervised parsing and
language modeling and also shows good results
on English. Although unlabeled parsing evaluation
is common, other work (Bisk and Hockenmaier,
2015) has argued for labeled parsing evaluation for
grammar induction.

Early unsupervised dependency grammars and
part-of-speech induction models (Klein and Man-
ning, 2004; Christodoulopoulos and Steedman,
2010) have been similarly augmented with neu-
ral networks and word embeddings (Tran et al.,
2016; Jiang et al., 2016). Neural networks pro-
vide flexible ways to parameterize distributions,
and word embeddings (Mikolov et al., 2013; Pen-
nington et al., 2014) allow these models to use se-
mantic information in these distributed representa-
tions. Results show that these improvements pro-
duce more accurate dependencies and POS assign-
ments, but these improvements have not been ap-
plied to PCFG induction.

Normalizing flows have been shown to be pow-
erful models for complex densities (Dinh et al.,
2015, 2017; Rezende and Mohamed, 2015; Papa-



2450

makarios et al., 2017). He et al. (2018) showed
improved performance on POS induction and de-
pendency induction by incorporating normalizing
flows into baseline models (Klein and Manning,
2004; Lin et al., 2015).

7 Conclusion

This work proposes a neural PCFG inducer which
employs context embeddings (Peters et al., 2018)
in a normalizing flow model (Dinh et al., 2015) to
extend PCFG induction to use semantic and mor-
phological information. Linguistically motivated
similarity penalty and categorical distance con-
straints are also imposed on the inducer as regular-
ization. Labeled and unlabeled evaluation shows
that the PCFG induction model with normaliz-
ing flow and context embeddings produces gram-
mars with state-of-the-art accuracy on a variety of
different languages. Results show consistent and
meaningful use of labels at phrasal and lexical
levels by the flow-based model. Ablation further
shows a positive effect of normalizing flow, con-
text embeddings and proposed regularizers.

Acknowledgments

The authors would like to thank the anonymous
reviewers for their helpful comments. Compu-
tations for this project were partly run on the
Ohio Supercomputer Center (1987). This research
was funded by the Defense Advanced Research
Projects Agency award HR0011-15-2-0022. The
content of the information does not necessarily re-
flect the position or the policy of the Government,
and no official endorsement should be inferred.
This work was also supported by the National
Science Foundation grant 1816891. All views ex-
pressed are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.

References
Omri Abend, Tom Kwiatkowski, Nathaniel J. Smith,

Sharon Goldwater, and Mark Steedman. 2017.
Bootstrapping language acquisition. In Cognition,
volume 164, pages 116–143. Elsevier B.V.

Colin Bannard, Elena Lieven, and Michael Tomasello.
2009. Modeling children’s early grammatical
knowledge. Proceedings of the National Academy
of Sciences of the United States of America,
106(41):17284–9.

Yonatan Bisk and Julia Hockenmaier. 2015. Prob-
ing the linguistic strengths and limitations of unsu-
pervised grammar induction. ACL-IJCNLP 2015 -
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing,
Proceedings of the Conference, 1:1395–1404.

Rens Bod. 2006. Unsupervised parsing with U-DOP.
In Proceedings of the Conference on Computational
Natural Language Learning, pages 85–92.

Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Working Notes of the Workshop
on Statistically-Based NLP Techniques, (March):1–
13.

Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng,
and Ting Liu. 2018. Towards Better UD Parsing:
Deep Contextualized Word Embeddings, Ensemble,
and Treebank Concatenation. In Proceedings of
the CoNLL 2018 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages
55–64, Brussels, Belgium. Association for Compu-
tational Linguistics.

Christos Christodoulopoulos and Mark Steedman.
2010. Two Decades of Unsupervised POS induc-
tion: How far have we come? 2010 Conference on
Empirical Methods in Natural Language Process-
ing, (October):575–584.

Michael Collins, Lance Ramshaw, Jan Hajič, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, pages
505–512.

Laurent Dinh, David Krueger, and Yoshua Bengio.
2015. NICE: Non-Linear Independent Components
Estimation. In ICLR Workshop.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-
gio. 2017. Density estimation using Real NVP.
ICLR.

Cynthia Fisher, Henry Gleitman, and Lila R Gleitman.
1991. On the semantic content of subcategorization
frames. Cognitive Psychology, 23(3):331–392.

Junxian He, Graham Neubig, and Taylor Berg-
Kirkpatrick. 2018. Unsupervised Learning of Syn-
tactic Structure with Invertible Neural Projections.
In EMNLP, pages 1292–1302. Association for Com-
putational Linguistics.

Phu Mon Htut, Kyunghyun Cho, and Samuel R Bow-
man. 2018. Grammar Induction with Neural Lan-
guage Models: An Unusual Replication. In EMNLP,
pages 4998–5003.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Unsu-
pervised neural dependency parsing. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, 61503248, pages 763–771.

https://doi.org/10.1016/j.cognition.2017.02.009
https://doi.org/10.1073/pnas.0905638106
https://doi.org/10.1073/pnas.0905638106
http://nlp.cs.illinois.edu/HockenmaierGroup/Papers/ACL2015/Analysis.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84943743437&partnerID=tZOtx3y1
http://nlp.cs.illinois.edu/HockenmaierGroup/Papers/ACL2015/Analysis.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84943743437&partnerID=tZOtx3y1
http://nlp.cs.illinois.edu/HockenmaierGroup/Papers/ACL2015/Analysis.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84943743437&partnerID=tZOtx3y1
https://doi.org/10.3115/1596276.1596293
https://pdfs.semanticscholar.org/eb34/c9981c50bde33d165a7f5faeb72018aa4d09.pdf
https://pdfs.semanticscholar.org/eb34/c9981c50bde33d165a7f5faeb72018aa4d09.pdf
https://pdfs.semanticscholar.org/eb34/c9981c50bde33d165a7f5faeb72018aa4d09.pdf
http://www.aclweb.org/anthology/K18-2005
http://www.aclweb.org/anthology/K18-2005
http://www.aclweb.org/anthology/K18-2005
http://homepages.inf.ed.ac.uk/sgwater/papers/emnlp10-20yrsPOS.pdf http://dl.acm.org/citation.cfm?id=1870658.1870714
http://homepages.inf.ed.ac.uk/sgwater/papers/emnlp10-20yrsPOS.pdf http://dl.acm.org/citation.cfm?id=1870658.1870714
https://doi.org/10.3115/1034678.1034754
https://doi.org/10.3115/1034678.1034754
https://arxiv.org/pdf/1410.8516.pdf
https://arxiv.org/pdf/1410.8516.pdf
https://doi.org/1605.08803
https://doi.org/10.1016/0010-0285(91)90013-E
https://doi.org/10.1016/0010-0285(91)90013-E
https://github.com/jxhe/struct- http://arxiv.org/abs/1808.09111
https://github.com/jxhe/struct- http://arxiv.org/abs/1808.09111
https://doi.org/10.1109/20.706375
https://doi.org/10.1109/20.706375
https://aclweb.org/anthology/D16-1073
https://aclweb.org/anthology/D16-1073


2451

Lifeng Jin, Finale Doshi-Velez, Timothy A Miller,
William Schuler, and Lane Schwartz. 2018a. Depth-
bounding is effective: Improvements and evaluation
of unsupervised PCFG induction. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.

Lifeng Jin, Finale Doshi-Velez, Timothy A Miller,
William Schuler, and Lane Schwartz. 2018b. Un-
supervised Grammar Induction with Depth-bounded
PCFG. Transactions of the Association for Compu-
tational Linguistics.

Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Bayesian Inference for PCFGs via
Markov chain Monte Carlo. Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
pages 139–146.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of Tricks for Efficient
Text Classification.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In ICLR.

Diederik P Kingma and Prafulla Dhariwal. 2018.
Glow: Generative Flow with Invertible 1x1 Convo-
lutions. NIPS.

Dan Klein and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 128–135.

Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
Annual Meeting on Association for Computational
Linguistics, volume 1, pages 478–485.

Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 1 - ACL-IJCNLP
’09, volume 1, page 91.

Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2015. Unsupervised POS Induction with
Word Embeddings. In Proceedings of Human Lan-
guage Technologies: The 2015 Annual Conference
of the North American Chapter of the ACL, pages
1311–1316.

Brian MacWhinney and Elizabeth Bates. 1993. The
Crosslinguistic Study of Sentence Processing. Cam-
bridge University Press, New York.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. CoRR, abs/1301.3:1–12.

Joakim Nivre, Marie-Catherine De Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajič, Christopher D
Manning, Ryan Mcdonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel
Zeman. 2016. Universal Dependencies v1: A Multi-
lingual Treebank Collection. In Proceedings of Lan-
guage Resources and Evaluation Conference.

The Ohio Supercomputer Center.
1987. Ohio Supercomputer Center.
\url{http://osc.edu/ark:/19495/f5s1ph73}.

George Papamakarios, Theo Pavlakou, and Iain Mur-
ray. 2017. Masked Autoregressive Flow for Density
Estimation. In Advances in Neural Information Pro-
cessing Systems, pages 2338–2347.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global Vectors for Word
Representation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2014), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL.

Kim Plunkett and Clair Wood. 2004. The development
of children’s understanding of grammar. Cogni-
tive and language development in children. Oxford:
Blackwell, pages 163–204.

Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw
text with cascaded finite state models. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics, pages 1077–1086.

Danilo Jimenez Rezende and Shakir Mohamed. 2015.
Variational Inference with Normalizing Flows. In
Proceedings of the 32nd International Conference
on Machine Learning.

Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
joint conference on empirical methods in natural
language processing and computational natural lan-
guage learning (EMNLP-CoNLL).

Yoav Seginer. 2007. Fast Unsupervised Incremental
Parsing. In Proceedings of the Annual Meeting of
the Association of Computational Linguistics, pages
384–391.

Cory Shain, William Bryce, Lifeng Jin, Victo-
ria Krakovna, Finale Doshi-Velez, Timothy
Miller, William Schuler, and Lane Schwartz.
2016. Memory-bounded left-corner unsupervised
grammar induction on child-directed input. In
Proceedings of the International Conference on
Computational Linguistics, pages 964–975.

https://github.com/lifengjin/dimi_emnlp18
https://github.com/lifengjin/dimi_emnlp18
https://github.com/lifengjin/dimi_emnlp18
http://homepages.inf.ed.ac.uk/sgwater/papers/naacl07-mcmc-pcfg.pdf
http://homepages.inf.ed.ac.uk/sgwater/papers/naacl07-mcmc-pcfg.pdf
https://doi.org/1511.09249v1
https://doi.org/1511.09249v1
https://doi.org/10.1063/1.4902458
https://doi.org/10.1063/1.4902458
https://doi.org/10.1097/PHM.0b013e318038d39c
https://doi.org/10.1097/PHM.0b013e318038d39c
https://doi.org/10.3115/1218955.1219016
https://doi.org/10.3115/1218955.1219016
https://doi.org/10.3115/1218955.1219016
https://doi.org/10.3115/1687878.1687893
https://doi.org/10.3115/1687878.1687893
http://www.aclweb.org/anthology/N15-1144 http://arxiv.org/abs/1503.06760
http://www.aclweb.org/anthology/N15-1144 http://arxiv.org/abs/1503.06760
http://dl.acm.org/citation.cfm?id=972470.972475
http://dl.acm.org/citation.cfm?id=972470.972475
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1301.3781
http://universaldependencies.org.
http://universaldependencies.org.
https://doi.org/10.1080/0955300001003760
https://doi.org/10.1080/0955300001003760
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://arxiv.org/pdf/1802.05365.pdf
https://arxiv.org/pdf/1802.05365.pdf
https://arxiv.org/pdf/1505.05770.pdf http://arxiv.org/abs/1505.05770
http://www.aclweb.org/anthology/P07-1049
http://www.aclweb.org/anthology/P07-1049
https://www.aclweb.org/anthology/C/C16/C16-1092.pdf
https://www.aclweb.org/anthology/C/C16/C16-1092.pdf


2452

Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and
Aaron Courville. 2018. Neural Language Modeling
by Jointly Learning Syntax and Lexicon. In ICLR.

Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A Linguistically Interpreted
Corpus of German Newspaper Text. In Proceedings
of the ESSLLI Workshop on Recent Advances in Cor-
pus Annotation., page 7.

Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu,
and Kevin Knight. 2016. Unsupervised Neural Hid-
den Markov Models. In Proceedings of the Work-
shop on Structured Prediction for NLP.

Kewei Tu. 2012. Unsupervised learning of probabilis-
tic grammars. Ph.D. thesis.

Zeno Vendler. 1972. Res cogitans: An essay in rational
psychology.

Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Ocurowski, John Kovarik, Fu-Dong Chiou, Shizhe
Huang, Tony Kroch, and Mitch Marcus. 2000. De-
veloping Guidelines and Ensuring Consistency for
Chinese Text Annotation. In Proceedings of the
Second Language Resources and Evaluation Con-
ference.

https://arxiv.org/pdf/1711.02013.pdf http://arxiv.org/abs/1711.02013
https://arxiv.org/pdf/1711.02013.pdf http://arxiv.org/abs/1711.02013
http://arxiv.org/abs/cmp-lg/9807008
http://arxiv.org/abs/cmp-lg/9807008
https://doi.org/10.18653/v1/W16-5907
https://doi.org/10.18653/v1/W16-5907
http://lib.dr.iastate.edu/etd http://lib.dr.iastate.edu/etd/12488
http://lib.dr.iastate.edu/etd http://lib.dr.iastate.edu/etd/12488
https://pdfs.semanticscholar.org/25d1/59d611a2b7ea1c09a629e87fea3696f7f913.pdf http://www.cis.upenn.edu/~chinese/ctb.html
https://pdfs.semanticscholar.org/25d1/59d611a2b7ea1c09a629e87fea3696f7f913.pdf http://www.cis.upenn.edu/~chinese/ctb.html
https://pdfs.semanticscholar.org/25d1/59d611a2b7ea1c09a629e87fea3696f7f913.pdf http://www.cis.upenn.edu/~chinese/ctb.html

