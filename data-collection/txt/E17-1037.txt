



















































Enumeration of Extractive Oracle Summaries


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 386–396,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Enumeration of Extractive Oracle Summaries

Tsutomu Hirao and Masaaki Nishino and Jun Suzuki and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,nishino.masaaki}@lab.ntt.co.jp
{suzuki.jun,nagata.masaaki}@lab.ntt.co.jp

Abstract

To analyze the limitations and the future
directions of the extractive summarization
paradigm, this paper proposes an Inte-
ger Linear Programming (ILP) formula-
tion to obtain extractive oracle summaries
in terms of ROUGEn. We also propose an
algorithm that enumerates all of the ora-
cle summaries for a set of reference sum-
maries to exploit F-measures that evalu-
ate which system summaries contain how
many sentences that are extracted as an or-
acle summary. Our experimental results
obtained from Document Understanding
Conference (DUC) corpora demonstrated
the following: (1) room still exists to
improve the performance of extractive
summarization; (2) the F-measures de-
rived from the enumerated oracle sum-
maries have significantly stronger correla-
tions with human judgment than those de-
rived from single oracle summaries.

1 Introduction

Recently, compressive and abstractive summariza-
tion are attracting attention (e.g., Almeida and
Martins (2013), Qian and Liu (2013), Yao et al.
(2015), Banerjee et al. (2015), Bing et al. (2015)).
However, extractive summarization remains a pri-
mary research topic because the linguistic qual-
ity of the resultant summaries is guaranteed, at
least at the sentence level, which is a key require-
ment for practical use (e.g., Hong and Nenkova
(2014), Hong et al. (2015), Yogatama et al. (2015),
Parveen et al. (2015)).

The summarization research community is ex-
periencing a paradigm shift from extractive to
compressive or abstractive summarization. Cur-
rently our question is: “Is extractive summariza-

tion still useful research?” To answer it, the ul-
timate limitations of the extractive summarization
paradigm must be comprehended; that is, we have
to determine its upper bound and compare it with
the performance of the state-of-the-art summariza-
tion methods. Since ROUGEn is the de-facto auto-
matic evaluation method and is employed in many
text summarization studies, an oracle summary is
defined as a set of sentences that have a maximum
ROUGEn score. If the ROUGEn score of an or-
acle summary outperforms that of a system that
employs another summarization approach, the ex-
tractive summarization paradigm is worthwhile to
leverage research resources.

As another benefit, identifying an oracle sum-
mary for a set of reference summaries allows us to
utilize yet another evaluation measure. Since both
oracle and extractive summaries are sets of sen-
tences, it is easy to check whether a system sum-
mary contains sentences in the oracle summary.
As a result, F-measures, which are available to
evaluate a system summary, are useful for evaluat-
ing classification-based extractive summarization
(Mani and Bloedorn, 1998; Osborne, 2002; Hi-
rao et al., 2002). Since ROUGEn evaluation does
not identify which sentence is important, an F-
measure conveys useful information in terms of
“important sentence extraction.” Thus, combining
ROUGEn and an F-measure allows us to scrutinize
the failure analysis of systems.

Note that more than one oracle summary might
exist for a set of reference summaries because
ROUGEn scores are based on the unweighted
counting of n-grams. As a result, an F-measure
might not be identical among multiple oracle sum-
maries. Thus, we need to enumerate the oracle
summaries for a set of reference summaries and
compute the F-measures based on them.

In this paper, we first derive an Integer Linear
Programming (ILP) problem to extract an oracle

386



summary from a set of reference summaries and
a source document(s). To the best of our knowl-
edge, this is the first ILP formulation that extracts
oracle summaries. Second, since it is difficult
to enumerate oracle summaries for a set of ref-
erence summaries using ILP solvers, we propose
an algorithm that efficiently enumerates all ora-
cle summaries by exploiting the branch and bound
technique. Our experimental results on the Doc-
ument Understanding Conference (DUC) corpora
showed the following:

1. Room still exists for the further improvement
of extractive summarization, i.e., where the
ROUGEn scores of the oracle summaries are
significantly higher than those of the state-of-
the-art summarization systems.

2. The F-measures derived from multiple oracle
summaries obtain significantly stronger cor-
relations with human judgment than those de-
rived from single oracle summaries.

2 Definition of Extractive Oracle
Summaries

We first briefly describe ROUGEn. Given set of
reference summaries R and system summary S,
ROUGEn is defined as follows:

ROUGEn(R, S) =
|R|∑
k=1

|U(Rk)|∑
j=1

min{N(gnj ,Rk), N(gnj ,S)}

|R|∑
k=1

|U(Rk)|∑
j=1

N(gnj ,Rk)
.

(1)

Rk denotes the multiple set of n-grams that oc-
cur in k-th reference summary Rk, and S de-
notes the multiple set of n-grams that appear in
system-generated summary S (a set of sentences).
N(gnj ,Rk) and N(gnj ,S) return the number of
occurrences of n-gram gnj in the k-th reference
and system summaries, respectively. Function
U(·) transforms a multiple set into a normal set.
ROUGEn takes values in the range of [0, 1], and
when the n-gram occurrences of the system sum-
mary agree with those of the reference summary,
the value is 1.

In this paper, we focus on extractive summariza-
tion, employ ROUGEn as an evaluation measure,

and define the oracle summaries as follows:

O = arg max
S⊆D

ROUGEn(R, S)

s.t. `(S) ≤ Lmax.
(2)

D is the set of all the sentences contained in the
input document(s), and Lmax is the length limi-
tation of the oracle summary. `(S) indicates the
number of words in the system summary. Eq. (2)
is an NP-hard combinatorial optimization prob-
lem, and no polynomial time algorithms exist that
can attain an optimal solution.

3 Related Work

Lin and Hovy (2003) utilized a naive exhaustive
search method to obtain oracle summaries in terms
of ROUGEn and exploited them to understand the
limitations of extractive summarization systems.
Ceylan et al. (2010) proposed another naive ex-
haustive search method to derive a probability
density function from the ROUGEn scores of or-
acle summaries for the domains to which source
documents belong. The computational complex-
ity of naive exhaustive methods is exponential to
the size of the sentence set. Thus, it may be pos-
sible to apply them to single document summa-
rization tasks involving a dozen sentences, but it
is infeasible to apply them to multiple document
summarization tasks that involve several hundred
sentences.

To describe the difference between the ROUGEn
scores of oracle and system summaries in multiple
document summarization tasks, Riedhammer et al.
(2008) proposed an approximate algorithm with a
genetic algorithm (GA) to find oracle summaries.
Moen et al. (2014) utilized a greedy algorithm for
the same purpose. Although GA or greedy algo-
rithms are widely used to solve NP-hard combi-
natorial optimization problems, the solutions are
not always optimal. Thus, the summary does not
always have a maximum ROUGEn score for the
set of reference summaries. Both works called the
summary found by their methods the oracle, but it
differs from the definition in our paper.

Since summarization systems cannot reproduce
human-made reference summaries in most cases,
oracle summaries, which can be reproduced by
summarization systems, have been used as train-
ing data to tune the parameters of summarization
systems. For example, Kulesza and Tasker (2011)
and Sipos et al. (2012) trained their summarizers

387



with oracle summaries found by a greedy algo-
rithm. Peyrard and Eckle-Kohler (2016) proposed
a method to find a summary that approximates a
ROUGE score based on the ROUGE scores of in-
dividual sentences and exploited the framework
to train their summarizer. As mentioned above,
such summaries do not always agree with the or-
acle summaries defined in our paper. Thus, the
quality of the training data is suspect. Moreover,
since these studies fail to consider that a set of ref-
erence summaries has multiple oracle summaries,
the score of the loss function defined between their
oracle and system summaries is not appropriate in
most cases.

As mentioned above, no known efficient algo-
rithm can extract “exact” oracle summaries, as de-
fined in Eq. (2), i.e., because only a naive exhaus-
tive search is available. Thus, such approximate
algorithms as a greedy algorithm are mainly em-
ployed to obtain them.

4 Oracle Summary Extraction as an
Integer Linear Programming (ILP)
Problem

To extract an oracle summary from document(s)
and a given set of reference summaries, we start
by deriving an Integer Linear Programming (ILP)
problem. Since the denominator of Eq. (1) is con-
stant for a given set of reference summaries, we
can find an oracle summary by maximizing the nu-
merator of Eq. (1). Thus, the ILP formulation is
defined as follows:

maximize
z

|R|∑
k=1

|U(Rk)|∑
j=1

zkj (3)

s.t.

|D|∑
i=1

`(si)xi ≤ Lmax (4)

∀j :
|D|∑
i=1

N(gnj , si)xi ≥ zkj (5)

∀j : N(gnj ,Rk) ≥ zkj (6)
∀i : xi ∈ {0, 1} (7)
∀j : zkj ∈ Z+. (8)

Here, zkj is the count of the j-th n-gram of
the k-th reference summary in the oracle sum-
mary, i.e., zkj = min{N(gnj ,Rk), N(gnj ,S)}.
`(·) returns the number of words in the sen-
tence, xi is a binary indicator, and xi = 1
denotes that the i-th sentence si is included in

Root 

Figure 1: Example of a search tree

the oracle summary. N(gnj , si) returns the num-
ber of occurrences of n-gram gnj in the i-th sen-
tence. Constraints (5) and (6) ensure that zkj =
min{N(gnj ,Rk), N(gnj ,S)}.
5 Branch and Bound Technique for

Enumerating Oracle Summaries

Since enumerating oracle summaries with an ILP
solver is difficult, we extend the exhaustive search
approach by introducing a search and prune tech-
nique to enumerate the oracle summaries. The
search pruning decision is made by comparing the
current upper bound of the ROUGEn score with the
maximum ROUGEn score in the search history.

5.1 ROUGEn Score for Two Distinct Sets of
Sentences

The enumeration of oracle summaries can be re-
garded as a depth-first search on a tree whose
nodes represent sentences. Fig. 1 shows an ex-
ample of a search tree created in a naive exhaus-
tive search. The nodes represent sentences and the
path from the root node to an arbitrary node repre-
sents a summary. For example, the red path in Fig.
1 from the root node to node s2 represents a sum-
mary consisting of sentences s1, s2. By utilizing
the tree, we can enumerate oracle summaries by
exploiting depth-first searches while excluding the
summaries that violate length constraints. How-
ever, this naive exhaustive search approach is im-
practical for large data sets because the number of
nodes inside the tree is 2|D|.

If we prune the unwarranted subtrees in each
step of the depth-first search, we can make the
search more efficient. The decision to search or
prune is made by comparing the current upper

388



bound of the ROUGEn score with the maximum
ROUGEn score in the search history. For instance,
in Fig. 1, we reach node s2 by following this path:
“Root→ s1,→ s2”. If we estimate the maximum
ROUGEn score (upper bound) obtained by search-
ing for the descendant of s2 (the subtree in the
blue rectangle), we can decide whether the depth-
first search should be continued. When the upper
bound of the ROUGEn score exceeds the current
maximum ROUGEn in the search history, we have
to continue. When the upper bound is smaller than
the current maximum ROUGEn score, no summary
is optimal that contains s1, s2, so we can skip sub-
sequent search activity on the subtree and proceed
to check the next branch: “Root→ s1→ s3”.

To estimate the upper bound of the ROUGEn
score, we re-define it for two distinct sets of sen-
tences, V and W , i.e., V ∩W = φ, as follows:

ROUGEn(R, V ∪W ) = ROUGEn(R, V )
+ ROUGE′n(R, V,W ).

(9)

Here ROUGE′n is defined as follows:

ROUGE′n(R, V,W ) =
|R|∑
k=1

∑
tn∈U(Rk)

min{N(tn,Rk \ V), N(tn,W)}

|R|∑
k=1

∑
tn∈U(Rk))

N(tn,Rk)
.

(10)

V,W are the multiple sets of n-grams found in the
sets of sentences V and W , respectively.

Theorem 1. Eq. (9) is correct.

We omit the proof of Theorem 1 due to space
limitations.

5.2 Upper Bound of ROUGEn
Let V be the set of sentences on the path from the
current node to the root node in the search tree, and
let W be the set of sentences that are the descen-
dants of the current node. In Fig. 1, V={s1, s2}
andW={s3, s4, s5, s6}. According to Theorem 1,
the upper bound of the ROUGEn score is defined
as:

̂ROUGEn(R, V ) = ROUGEn(R, V ) +
max
Ω⊆W

{ROUGE′n(R, V,Ω):`(Ω)≤Lmax−`(V )}.(11)

Algorithm 1 Algorithm to Find Upper Bound of
ROUGEn
1: Function: ̂ROUGEn(R, V )
2: W ← descendant(last(V )), W ′ ← φ
3: U ← ROUGE(R, V )
4: for each w ∈W do
5: append(W ′, ROUGE

′
n(R,V,{w})
`(w)

)
6: end for
7: sort(W ′, ’descend’)
8: for each w ∈W ′ do
9: if Lmax − `({w}) ≥ 0 then

10: U ← U + ROUGE′n(R, V, {w})
11: Lmax ← Lmax − `({w})
12: else
13: U ← U + ROUGE

′
n(R, V, {w})
`({w}) × Lmax

14: break the loop
15: end if
16: end for
17: return U
18: end

Since the second term on the right side
in Eq. (11) is an NP-hard problem, we
turn to the following relation by intro-
ducing inequality, ROUGE′n(R, V,Ω) ≤∑

ω∈Ω ROUGE
′
n(R, V, {ω}),

max
Ω⊆W

{
ROUGE′n(R, V,Ω):`(Ω)≤Lmax−`(V )

}
≤max

x

{∑|W |
i=1 ROUGE

′
n(R, V, {wi})xi:∑|W |

i=1`({wi})xi≤Lmax−`(V )
}
. (12)

Here, x = (x1, . . . , x|W |) and xi ∈ {0, 1}. The
right side of Eq. (12) is a knapsack problem, i.e., a
0-1 ILP problem. Although we can obtain the op-
timal solution for it using dynamic programming
or ILP solvers, we solve its linear programming
relaxation version by applying a greedy algorithm
for greater computation efficiency. The solution
output by the greedy algorithm is optimal for the
relaxed problem. Since the optimal solution of
the relaxed problem is always larger than that of
the original problem, the relaxed problem solution
can be utilized as the upper bound. Algorithm 1
shows the pseudocode that attains the upper bound
of ROUGEn. In the algorithm, U indicates the up-
per bound score of ROUGEn. We first set the initial
score of upper bound U to ROUGEn(R, V ) (line
3). Then we compute the density of the ROUGE′n
scores (ROUGE′n(R, V, {w})/`(w)) for each sen-
tence w in W and sort them in descending or-
der (lines 4 to 6). When we have room to add
w to the summary, we update U by adding the
ROUGE′n(R, V, {w}) (line 10) and update length

389



Algorithm 2 Greedy algorithm to obtain initial
score
1: Function: GREEDY(R, D, Lmax)
2: L← 0, S ← φ,E ← D
3: while E 6= φ do
4: s∗← arg max

s∈E

{
ROUGEn(R, S ∪ {s})−ROUGEn(R, S)

`({s})

}
5: L← L+ `({s∗})
6: if L ≤ Lmax then
7: S ← S ∪ {s∗}
8: end if
9: E ← E \ {s∗}

10: end while
11: i∗ ← arg max

i∈D,`({i})≤Lmax
ROUGEn(R, {i})

12: S∗ ← arg max
K∈{{i∗},S}

ROUGEn(R,K)

13: return ROUGEn(R, S∗)
14: end

constraint Lmax (line 11). When we do not have
room to add w, we update U by adding the score
obtained by multiplying the density ofw by the re-
maining length, Lmax (line 13), and exit the while
loop.

5.3 Initial Score for Search

Since the branch and bound technique prunes
the search by comparing the best solution found so
far with the upper bounds, obtaining a good solu-
tion in the early stage is critical for raising search
efficiency.

Since ROUGEn is a monotone submodular func-
tion (Lin and Bilmes, 2011), we can obtain a
good approximate solution by a greedy algorithm
(Khuller et al., 1999). It is guaranteed that the
score of the obtained approximate solution is
larger than 12(1− 1e )OPT, where OPT is the score
of the optimal solution. We employ the solution as
the initial ROUGEn score of the candidate oracle
summary.

Algorithm 2 shows the greedy algorithm. In it,
S denotes a summary and D denotes a set of sen-
tences. The algorithm iteratively adds sentence s∗

that yields the largest gain in the ROUGEn score
to current summary S, provided the length of the
summary does not violate length constraint Lmax
(line 4). After the while loop, the algorithm com-
pares the ROUGEn score of S with the maximum
ROUGEn score of the single sentence and outputs
the larger of the two scores (lines 11 to 13).

5.4 Enumeration of Oracle summaries

By introducing threshold τ as the best ROUGEn
score in the search history, pruning decisions in-
volve the following three conditions:

Algorithm 3 Branch and bound technique to enu-
merate oracle summaries
1: Read R,D,Lmax
2: τ ← GREEDY(R,D,Lmax),Oτ ← φ
3: for each s ∈ D do
4: append(S,〈ROUGEn(R, {s}), s〉)
5: end for
6: sort(S,’descend’)
7: call FINDORACLE(S,C)
8: output Oτ
9: Procedure: FINDORACLE(Q,V )

10: while Q 6= φ do
11: s←shift(Q)
12: append(V, s)
13: if Lmax − `(V ) ≥ 0 then
14: if ROUGEn(R, V ) ≥ τ then
15: τ ← ROUGEn(R, V )
16: append(Oτ , V )
17: call FINDORACLE(Q,V )
18: else if ̂ROUGEn(R, V ) ≥ τ then
19: call FINDORACLE(Q,V )
20: end if
21: end if
22: pop(V )
23: end while
24: end

1. ROUGEn(R, V ) ≥ τ ;
2. ROUGEn(R, V ) < τ , ̂ROUGEn(R, V ) < τ ;
3. ROUGEn(R, V ) < τ , ̂ROUGEn(R, V ) ≥ τ .

With case 1, we update the oracle summary
as V and continue the search. With case 2, be-
cause both ROUGEn(R, V ) and ̂ROUGEn(R, V )
are smaller than τ , the subtree whose root node
is the current node (last visited node) is pruned
from the search space, and we continue the depth-
first search from the neighbor node. With case 3,
we do not update oracle summary as V because
ROUGEn(R, V ) is less than τ . However, we might
obtain a better oracle summary by continuing the
depth-first search because the upper bound of the
ROUGEn score exceeds τ . Thus, we continue to
search for the descendants of the current node.

Algorithm 3 shows the pseudocode that enu-
merates the oracle summaries. The algorithm
reads a set of reference summaries R, length lim-
itation Lmax, and set of sentences D (line 1) and
initializes threshold τ as the ROUGEn score ob-
tained by the greedy algorithm (Algorithm 2).
It also initializes Oτ , which stores oracle sum-
maries whose ROUGEn scores are τ , and priority
queueC, which stores the history of the depth-first
search (line 2). Next, the algorithm computes the
ROUGEn score for each sentence and stores S af-
ter sorting them in descending order. After that,
we start a depth-first search by recursively call-

390



Year Topics Docs. Sents. Words Refs. Length
01 30 10 365 7706 89 100
02 59 10 238 4822 116 100
03 30 10 245 5711 120 100
04 50 10 218 4870 200 100
05 50 29.5 885 18273.5 300 250
06 50 25 732.5 15997.5 200 250
07 45 25 516 11427 180 250

Table 1: Statistics of data set

ing procedure FINDORACLE. In the procedure,
we extract the top sentence from priority queue
Q and append it to priority queue V (lines 11 to
12). When the length of V is less than Lmax, if
ROUGEn(R, V ) is larger than threshold τ (case 1),
we update τ as the score and append current V to
Oτ . Then we continue the depth-first search by
calling the procedure the FINDORACLE (lines 15
to 17). If ̂ROUGEn(R, V ) is larger than τ (case 3),
we do not update τ and Oτ but reenter the depth-
first search by calling the procedure again (lines
18 to 19). If neither case 1 nor case 3 is true, we
delete the last visited sentence from V and return
to the top of the recurrence.

6 Experiments

6.1 Experimental Setting

We conducted experiments on the corpora devel-
oped for a multiple document summarization task
in DUC 2001 to 2007. Table 1 show the statistics
of the data. In particular, the DUC-2005 to -2007
data sets not only have very large numbers of sen-
tences and words but also a long target length (the
reference summary length) of 250 words.

All the words in the documents were stemmed
by Porter’s stemmer (Porter, 1980). We computed
ROUGE1 scores, excluding stopwords, and com-
puted ROUGE2 scores, keeping them. Owczarzak
et al. (2012) suggested using ROUGE1 and keeping
stopwords. However, as Takamura et al. argued
(Takamura and Okumura, 2009), the summaries
optimized with non-content words failed to con-
sider the actual quality. Thus, we excluded stop-
words for computing the ROUGE1 scores.

We enumerated the following two types of ora-
cle summaries: those for a set of references for a
given topic and those for each reference in the set
of references.

6.2 Results and Discussion

6.2.1 Impact of Oracle ROUGEn scores
Table 2 shows the average ROUGE1,2 scores of the
oracle summaries obtained from both a set of ref-
erences and each reference in the set (“multi” and
“single”), those of the best conventional system
(Peer), and those obtained from summaries pro-
duced by a greedy algorithm (Algorithm 2).

Oracle (single) obtained better ROUGE1,2
scores than Oracle (multi). The results imply that
it is easier to optimize a reference summary than a
set of reference summaries. On the other hand,
the ROUGE1,2 scores of these oracle summaries
are significantly higher than those of the best sys-
tems. The best systems obtained ROUGE1 scores
from 60% to 70% in “multi” and from 50% to 60%
in “single” as well as ROUGE2 scores from 40% to
55% in “multi” and from 30% to 40% in “single”
for their oracle summaries.

Since the systems in Table 2 were developed
over many years, we compared the ROUGEn
scores of the oracle summaries with those of the
current state-of-the-art systems using the DUC-
2004 corpus and obtained summaries generated by
different systems from a public repository1 (Hong
et al., 2014). The repository includes summaries
produced by the following seven state-of-the-art
summarization systems: CLASSY04 (Conroy et
al., 2004), CLASSY11 (Conroy et al., 2011), Sub-
modular (Lin and Bilmes, 2012), DPP (Kulesza
and Tasker, 2011), RegSum (Hong and Nenkova,
2014), OCCAMS V (Davie et al., 2012; Conroy
et al., 2013), and ICSISumm (Gillick and Favre,
2009; Gillick et al., 2009). Table 3 shows the re-
sults.

Based on the results, RegSum (Hong
and Nenkova, 2014) achieved the best
ROUGE1=0.331 result, while ICSISumm (Gillick
and Favre, 2009; Gillick et al., 2009) (a compres-
sive summarizer) achieved the best result with
ROUGE2=0.098. These systems outperformed
the best systems (Peers 65 and 67 in Table 2), but
the differences in the ROUGEn scores between the
systems and the oracle summaries are still large.
More recently, Hong et al. (2015) demonstrated
that their system’s combination approach achieved
the current best ROUGE2 score, 0.105, for the
DUC-2004 corpus. However, a large difference
remains between the ROUGE2 score of oracle and

1http://www.cis.upenn.edu/˜nlp/
corpora/sumrepo.html

391



01 02 03 04 05 06 07
R1 R2 R1 R2 R1 R2 R1 R2 R1 R2 R1 R2 R1 R2

Oracle (multi) .400 .164 .452 .186 .434 .185 .427 .162 .445 .177 .491 .211 .506 .236
Oracle (single) .500 .226 .515 .225 .525 .258 .519 .228 .574 .279 .607 .303 .622 .330
Greedy .387 .161 .438 .184 .424 .182 .412 .157 .430 .173 .473 .206 .495 .234
Peer .251 .080 .269 .080 .295 .094 .305 .092 .262 .073 .305 .095 .363 .117
ID T T 19 19 26 13 67 65 10 15 23 24 29 15

Table 2: ROUGE1,2 scores of oracle summaries, greedy summaries, and system summaries for each data
set

System ROUGE1 ROUGE2
Oracle (multi) .427 .162
Oracle (single) .519 .228
CLASSY04 .305 .0897
CLASSY11 .286 .0919
Submodular .300 .0933
DPP .309 .0960
RegSum .331 .0974
OCCAMS V .300 .0974
ICSISumm .310 .0980

Table 3: ROUGE1,2 scores for state-of-the-art sum-
marization systems on DUC-2004 corpus

their summaries.
In short, the ROUGEn scores of the oracle sum-

maries are significantly higher than those of the
current state-of-the-art summarization systems,
both extractive and compressive summarization.
These results imply that further improvement of
the performance of extractive summarization is
possible.

On the other hand, the ROUGEn scores of the or-
acle summaries are far from ROUGEn = 1. We be-
lieve that the results are related to the summary’s
compression rate. The data set’s compression rate
was only 1 to 2%. Thus, under tight length con-
straints, extractive summarization basically fails to
cover large numbers of n-grams in the reference
summary. This reveals the limitation of the extrac-
tive summarization paradigm and suggests that we
need another direction, compressive or abstractive
summarization, to overcome the limitation.

6.2.2 ROUGE Scores of Summaries Obtained
from Greedy Algorithm

Table 2 also shows the ROUGE1,2 scores of the
summaries obtained from the greedy algorithm
(greedy summaries). Although there are statisti-
cally significant differences between the ROUGE

single multi
ROUGE1 .451 .419
ROUGE2 .536 .530

Table 4: Jaccard Index between both oracle and
greedy summaries

scores of the oracle summaries and greedy sum-
maries, those obtained from the greedy summaries
achieved near optimal scores, i.e., approximation
ratio of them are close to 0.9. These results are
surprising since the algorithm’s theoretical lower
bound is 12(1− 1e )(' 0.32)OPT.

On the other hand, the results do not support
that the differences between them are small at the
sentence-level. Table 4 shows the average Jaccard
Index between the oracle summaries and the cor-
responding greedy summaries for the DUC-2004
corpus. The results demonstrate that the oracle
summaries are much less similar to the greedy
summaries at the sentence-level. Thus, it might
not be appropriate to use greedy summaries as
training data for learning-based extractive summa-
rization systems.

6.2.3 Impact of Enumeration
Table 5 shows the median number of oracle sum-
maries and the rates of the reference summaries
that have multiple oracle summaries for each data
set. Over 80% of the reference summaries and
about 60% to 90% of the topics have multiple or-
acle summaries. Since the ROUGEn scores are
based on the unweighted counting of n-grams,
when many sentences have similar meanings, i.e.,
many redundant sentences, the number of oracle
summaries that have the same ROUGEn scores in-
creases. The source documents of multiple docu-
ment summarization tasks are prone to have many
such redundant sentences, and the amount of ora-
cle summaries is large.

392



Median Rate
single multi single multi

ROUGE1 ROUGE2 ROUGE1 ROUGE2 ROUGE1 ROUGE2 ROUGE1 ROUGE2
01 8 9 4 5 .854 .787 .833 .733
02 7.5 5.5 4 4 .897 .836 .814 .780
03 8 10.5 3.5 4 .833 .858 .800 .900
04 8 8 3.5 3 .865 .865 .780 .760
05 35 35.5 2 3 .916 .907 .580 .660
06 28 22 2.5 3 .877 .880 .700 .720
07 23 16 4 2 .910 .878 .733 711

Table 5: Median number of oracle summaries and rates of reference summaries and topics with multiple
oracle summaries for each data set

The oracle summaries offer significant bene-
fit with respect to evaluating the extracted sen-
tences. Since both the oracle and system sum-
maries are sets of sentences, it is easy to check
whether each sentence in the system summary is
contained in one of the oracle summaries. Thus,
we can exploit the F-measures, which are use-
ful for evaluating classification-based extractive
summarization (Mani and Bloedorn, 1998; Os-
borne, 2002; Hirao et al., 2002). Here, we have
to consider that the oracle summaries, obtained
from a reference summary or a set of reference
summaries, are not identical at the sentence-level
(e.g., the average Jaccard Index between the ora-
cle summaries for the DUC-2004 corpus is around
0.5). The F-measures are varied with the ora-
cle summaries that are used for such computa-
tion. For example, assume that we have sys-
tem summary S={s1, s2, s3, s4} and oracle sum-
maries O1={s1, s2, s5, s6} and O2={s1, s2, s3}.
The precision for O1 is 0.5, while that for O2 is
0.75; the recall for O1 is 0.5, while that for O2 is
1; the F-measure for O1 is 0.5, while that for O2 is
0.86.

Thus, we employ the scores gained by averag-
ing all of the oracle summaries as evaluation mea-
sures. Precision, recall, and F-measure are defined
as follows: P={∑O∈Oall |O ∩ S|/|S|}/|Oall|,
R={∑O∈Oall |O ∩ S|/|O|}/|Oall|,
F-measure=2PR/(P +R).

To demonstrate F-measure’s effectiveness, we
investigated the correlation between an F-measure
and human judgment based on the evaluation re-
sults obtained from the DUC-2004 corpus. The re-
sults include summaries generated by 17 systems,
each of which has a mean coverage score assigned
by a human subject. We computed the correla-

tion coefficients between the average F-measure
and the average mean coverage score for 50 topics.
Table 6 shows Pearson’s r and Spearman’s ρ. In
the table, “F-measure (R1)” and “F-measure (R2)”
indicate the F-measures calculated using oracle
summaries optimized to ROUGE1 and ROUGE2,
respectively. “M” indicates the F-measure calcu-
lated using multiple oracle summaries, and “S” in-
dicates F-measures calculated using randomly se-
lected oracle summaries. “multi” indicates oracle
summaries obtained from a set of references, and
“single” indicates oracle summaries obtained from
a reference summary in the set. For “S,” we ran-
domly selected a single oracle summary and cal-
culated the F-measure 100 times and took the av-
erage value with the 95% confidence interval of
the F-measures by bootstrap resampling.

The results demonstrate that the F-measures are
strongly correlated with human judgment. Their
values are comparable with those of ROUGE1,2.
In particular, F-measure (R1) (single-M) achieved
the best Spearman’s ρ result. When comparing
“single” with “multi,” Pearson’s r of “multi” was
slightly lower than that of “single,” and the Spear-
man’s r of “multi” was almost the same as those of
“single.” “M” has significantly better performance
than “S.” These results imply that F-measures
based on oracle summaries are a good evaluation
measure and that oracle summaries have the po-
tential to be an alternative to human-made ref-
erence summaries in terms of automatic evalua-
tion. Moreover, the enumeration of the oracle
summaries for a given reference summary or a set
of reference summaries is essential for automatic
evaluation.

393



Metric r ρ
ROUGE1 .861 .760
ROUGE2 .907 .831
F-measure (R1) (single-M) .857 .855
F-measure (R1) (single-S) .815-.830 .811-.830
F-measure (R2) (single-M) .904 .826
F-measure (R2) (single-S) .855-.865 .740-.760
F-measure (R1) (multi-M) .814 .841
F-measure (R1) (multi-S) .794-.802 .803-.813
F-measure (R2) (multi-M) .824 .846
F-measure (R2) (multi-S) .806-.816 .797-.817

Table 6: Correlation coefficients between auto-
matic evaluations and human judgments on DUC-
2004 corpus

6.2.4 Search Efficiency
To demonstrate the efficiency of our search algo-
rithm against the naive exhaustive search method,
we compared the number of feasible solutions
(sets of sentences that satisfy the length constraint)
with the number of summaries that were checked
in our search algorithm.

Table 7 shows the median number of feasible
solutions and checked summaries yielded by our
method for each data set (in the case of “sin-
gle”). The differences in the number of feasible
solutions between ROUGE1 and ROUGE2 are very
large. Input set (|D|) of ROUGE1 is much larger
than ROUGE1. On the other hand, the differences
between ROUGE1 and ROUGE2 in our method are
of the order of 10 to 102. When comparing our
method with naive exhaustive searches, its search
space is significantly smaller. The differences are
of the order of 107 to 1030 with ROUGE1 and 104

to 1017 with ROUGE2. These results demonstrate
the efficiency of our branch and bound technique.

In addition, we show an example of the pro-
cessing time for extracting one oracle summary
and enumerating all of the oracle summaries for
the reference summaries in the DUC-2004 cor-
pus with a Linux machine (CPU: Intel R© Xeon R©
X5675 (3.07GHz)) with 192 GB of RAM. We
utilized CPLEX 12.1 to solve the ILP problem.
Our algorithm was implemented in C++ and com-
plied with GCC version 4.4.7. The results show
that we needed 0.026 and 0.021 sec. to extract
one oracle summary per reference summary and
0.047 and 0.031 sec. to extract one oracle sum-
mary per set of reference summaries for ROUGE1
and ROUGE2, respectively. We needed 11.90 and
1.40 sec. to enumerate the oracle summaries
per reference summary and 102.94 and 3.65 sec.
per set of reference summaries for ROUGE1 and

ROUGE1 ROUGE2
Naive Proposed Naive Proposed

01 3.66×1013 5.75×103 3.32×107 1.00×103
02 1.12×1012 4.64×103 1.34×107 8.87×102
03 1.62×1011 3.65×103 6.37×106 8.19×102
04 9.65×1010 4.47×103 6.90×106 9.83×102
05 5.48×1036 2.32×106 3.48×1021 7.03×104
06 1.94×1032 1.97×106 2.11×1020 5.08×104
07 4.14×1028 1.40×106 1.81×1019 2.60×104

Table 7: Median number of summaries checked by
each search method

ROUGE2, respectively. The extraction of one or-
acle summary for a reference summary can be
achieved with the ILP solver in practical time and
the enumeration of oracle summaries is also effi-
cient. However, to enumerate oracle summaries,
we needed several weeks for some topics in DUCs
2005 to 2007 since they hold a huge number of
source sentences.

7 Conclusions

To analyze the limitations and the future direction
of extractive summarization, this paper proposed
(1) Integer Linear Programming (ILP) formulation
to obtain extractive oracle summaries in terms of
ROUGEn scores and (2) an algorithm that enumer-
ates all oracle summaries to exploit F-measures
that evaluate the sentences extracted by systems.

The evaluation results obtained from the cor-
pora of DUCs 2001 to 2007 identified the follow-
ing: (1) room still exists to improve the ROUGEn
scores of extractive summarization systems even
though the ROUGEn scores of the oracle sum-
maries fell below the theoretical upper bound
ROUGEn=1. (2) Over 80% of the reference sum-
maries and from 60% to 90% of the sets of refer-
ence summaries have multiple oracle summaries,
and the F-measures computed by utilizing the enu-
merated oracle summaries showed stronger corre-
lation with human judgment than those computed
from single oracle summaries.

Acknowledgments

The authors thank three anonymous reviewers for
their valuable comments and suggestions to im-
prove the quality of the paper.

394



References
Miguel B. Almeida and André F.T. Martins. 2013. Fast

and robust compressive summarization with dual de-
composition and multi-task learning. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 196–206.

Soddhartha Banerjee, Prasenjit Mitra, and Kazunari
Sugiyama. 2015. Multi-document abstractive sum-
marization using ILP based multi-sentence compres-
sion. In Proc. of the 24th International Joint Confer-
ence on Artificial Intelligence (IJCAI 2015), pages
1208–1214.

Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo,
and Rebecca J. Passonneau. 2015. Abstractive
multi-document summarization via phrase selection
and merging. In Proc. of the 53rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1587–1597.

Hakan Ceylan, Rada Mihalcea, Umut Özertem, Elena
Lloret, and Manuel Palomar. 2010. Quantifying the
limits and success of extractive summarization sys-
tems across domains. In Proc. of the Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 903–911.

John M. Conroy, Jade Goldstein, Judith D. Schlesinger,
and Dianne P. O’Leary. 2004. Left-brain/right-
brain multi-document summarization. In Proc. of
the Document Understanding Conference (DUC).

John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O’Leary. 2011.
Classy 2011 at TAC: Guided and multi-lingual sum-
maries and evaluation metrics. In Proc. of the Text
Analysis Conference (TAC).

John M. Conroy, Sashka T. Davis, Jeff Kubina, Yi-Kai
Liu, Dianne P. O’Leary, and Judith D Schlesinger.
2013. Multilingual summarization: Dimensionality
reduction and a step towards optimal term coverage.
In Proc. of the MultiLing 2013 Workshop on Mul-
tilingual Multi-document Summarization, pages 55–
63.

Sashka T. Davie, John M. Conroy, and Judith D.
Schlesinger. 2012. OCCAMS - an optimal com-
binatorial covering algorithm for multi-document
summarization. In Proc. of the 12th IEEE Inter-
national Conference on Data Mining Workshops,
ICDM Workshops, pages 454–463.

Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proc. of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 10–18.

Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
ICSI/UTD summarization system at TAC 2009. In
Proc. of the Text Analysis Conference (TAC).

Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and
Yuji Matsumoto. 2002. Extracting import sen-
tences with support vector machines. In Proc. of
the 19th International Conference on Computational
Linguistics (COLING), pages 342–348.

Kai Hong and Ani Nenkova. 2014. Improving
the estimation of word importance for news multi-
document summarization. In Proc. of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 712–721.

Kai Hong, John Conroy, Benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A repository
of state of the art and competitive baseline sum-
maries for generic news summarization. In Proc.
of the Ninth International Conference on Language
Resources and Evaluation (LREC’14), pages 1608–
1616.

Kai Hong, Mitchell Marcus, and Ani Nenkova. 2015.
System combination for multi-document summa-
rization. In Proc. of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 107–117.

Samir Khuller, Anna Moss, and Joseph Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39–45.

Alex Kulesza and Ben Tasker. 2011. Learning deter-
minantal point process. In Proc. of the 27th Confer-
ence on Uncertainty in Artificial Intelligence.

Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proc. of
the 49th Association for Computational Linguistics:
Human Language Technologies, pages 510–520.

Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In Proc. of the 28th Conference on
Uncertainty in Artificial Intelligence (UAI2012).

Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proc. of the HLT-NAACL 03 Text
Summarization Workshop, pages 73–80.

Inderjeet Mani and Eric Bloedorn. 1998. Machine
learning of generic and user-focused summarization.
In Proceedings of the Fifteenth National/Tenth Con-
ference on Artificial Intelligence/Innovative Appli-
cations of Artificial Intelligence, pages 820–826.

Hans Moen, Juho Heimonen, Laura-Maria Murtola,
Antti Airola, Tapio Pahikkala, Virpi Terv, Ri-
itta Danielsson-Ojala, Tapio Salakoski, and Sanna
Salanter. 2014. On evaluation of automatically
generated clinical discharge summaries. In Proc. of
the 2nd European Workshop on Practical Aspects of
Health Informatics, pages 101–114.

Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization, pages 1–8.

395



Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proc. of Workshop on Evaluation Metrics
and System Comparison for Automatic Summariza-
tion, pages 1–9, June.

Daraksha Parveen, Hans-Martin Ramsl, and Michael
Strube. 2015. Topical coherence for graph-based
extractive summarization. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1949–1954, Lisbon,
Portugal, September. Association for Computational
Linguistics.

Maxime Peyrard and Judith Eckle-Kohler. 2016. Op-
timizing an approximation of rouge - a problem-
reduction approach to extractive multi-document
summarization. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1825–
1836, Berlin, Germany, August. Association for
Computational Linguistics.

Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.

Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proc. of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1492–1502.

Korbinian Riedhammer, Dan Gillick, Benoit Favre, and
Dilek Hakkani-Tür. 2008. Packing the meeting
summarization knapsack. In Proc. of the 9th Annual
Conference of the International Speech Communi-
cation Association, pages 2434–2437.

Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin learning of submod-
ular summarization models. In Proc. of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 224–233.

Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proc. of the 12th Confer-
ence of the European of the Association for Compu-
tational Linguistics, pages 781–789.

Jin-ge Yao, Xiaojun Wan, and Jianguo Xiao. 2015.
Compressive document summarization via sparse
optimization. In Proc. of the 24th International
Joint Conference on Artificial Intelligence (IJCAI
2015), pages 1376–1382.

Dani Yogatama, Fei Liu, and Noah A. Smith. 2015.
Extractive summarization by maximizing semantic
volume. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1961–1966, Lisbon, Portugal, September.
Association for Computational Linguistics.

396


