



















































Proceedings of the...


S Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 283–289,
Kolkata, India. December 2017. c©2016 NLP Association of India (NLPAI)

Correcting General Purpose ASR Errors using Posteriors

Sunil Kumar Kopparapu
TCS Research and Innovation - Mumbai
SunilKumar.Kopparapu@TCS.Com

C Anantaram
TCS Research and Innovation - Delhi

C.Anantaram@TCS.Com

Abstract

Speech based interfaces have gained pop-
ularity because of the advances in auto-
matic speech recognition (ASR) technol-
ogy, more recently, triggered by the use
of deep neural networks for acoustic and
language modeling. However, the per-
formance of any general purpose ASR
(gpASR) is poor especially for low re-
source languages and the performance de-
teriorate further for natural conversational
speech. In this paper, we propose a statisti-
cal approach to learn the corrections to en-
able the use of a general purpose ASR for
domain specific use. The proposed idea
is based on the observation that there are
three types of errors that occur in an ASR
output, namely, insertion, deletion or sub-
stitution of a word or a phoneme. We pro-
pose to model these errors statistically and
use these posteriors to develop a scheme
that is able to repair the ASR output. The
proposed system is able to handle ASR er-
rors spread across lexical words also.

1 Introduction

Speech is the most natural mode of communica-
tion to query for answers (Kopparapu, 2014). Use
of natural language speech to query for informa-
tion is gaining practical applicability in our day to
day activities. It is also believed that in the next
ten years there will be a ten times increase in the
number of speech interface we will be facing in
our day to day life (Fuhrmann et al., 2017). How-
ever recognition of natural language speech is not
always 100% accurate even when a state of the art
ASR engine is employed for a resource rich lan-
guage. There are several reasons, the significant
among them are the mismatch in the train and the

test conditions in terms of the acoustic modeling,
the accent, the language, the environment etc.

There have been several attempts to improve the
speech recognition accuracies (a) by fine tuning,
adapting or learning the acoustic models (AM)
to handle the train-test mismatch condition (Mo-
hamed et al., 2012); and (b) by configuring the
statistical language models (SLM) so that the per-
plexity of the search space can be reduced (Kom-
brink et al., 2012). However, these attempts are
either far from producing accurate speech to text
conversion especially for natural language speech
or make the speech recognition engine constrained
to perform for a very specific task or domain. Sub-
sequently, there have been several attempts to post
process (Ainsworth and Pratt, 1992; Nishizaki and
Sekiguchi, 2006; Bassil and Alwani, 2012) the
output of the speech recognition engine to identify
and correct the erroneous output.

Most work on ASR error detection and correc-
tion has focused on using confidence measures,
generally called the log-likelihood score, provided
by the speech recognition engine; the text with
lower confidence is assumed to be incorrect and
subjected to correction (Shi, 2008; Zhou et al.,
2005). Such confidence based methods are use-
ful only when we have access to the internals of
a speech recognition engine built for a specific
domain. As mentioned earlier, use of domain-
specific engine requires one to rebuild the inter-
face every time the domain is updated, or a new
domain is introduced. As mentioned earlier, our
focus is to avoid rebuilding the interface each time
the domain changes by using an existing ASR. As
such our method is specifically a post-ASR sys-
tem. A post-ASR system provides greater flexi-
bility in terms of absorbing domain variations and
adapting the output of ASR in ways that are not
possible during training a domain-specific ASR
system (Ringger and Allen, 1996). More recently,283



there have been attempts to use a general purpose
speech recognition engine and then correct the
ASR output using bio-inspired evo-devo (Anan-
taram et al., 2015b; Anantaram et al., 2015a) and
statistical techniques (Anantaram and Kopparapu,
2017) based on features extracted from the refer-
ence and the ASR output text. The machine learn-
ing based system described in (Anantaram and
Kopparapu, 2017) is along the lines of (Jeong et
al., 2004) but differs in the sense that they use of
multiple features for training the Naive Bayes clas-
sifier instead of a single feature (syllable count) for
training used in (Jeong et al., 2004) in addition to
not using manual alignment between the ASR and
reference text. In (Twiefel et al., 2014) the au-
thors address the post correction of a general pur-
pose ASR by using (a) the closeness of phoneme
depending on the place and manner of articula-
tion to identify the confusability between the ac-
tual and the recognized phoneme and (b) using the
front-end and the linguist module of Sphinx. How-
ever their experiments seem to suggest that their
task is that of aligning the gpASR output text with
another sentence from a known finite set of sen-
tences.

In this paper, we use a novel approach to repair
the errors produced by a gpASR engine. We do not
assume, unlike (Twiefel et al., 2014) the availabil-
ity of reference pre defined sentences. In a very
broad sense we try to model the performance of
the gpASR engine for a certain environment and
domain which is then used to repair the ASR out-
put for all speech utterances coming from the same
environment and domain. The rest of the paper is
organized as follows. In Section 2 we formulate
the problem and describe an approach to model the
ASR for a specific domain and environment and in
Section 3 we describe the dataset used to evaluate
the proposed approach and give some preliminary
results. We conclude in Section 4.

2 Problem Formulation

Let
~R = /r1r2r3 · · · rm/

be a spoken sentence consisting ofM words that is
input to a speech recognition engine. For example,

~R =

{/who is the accountable
person for manufacturing
solutions/

Let the general purpose automatic speech

recognition (gpASR) output,

~O = ”o1o2o3 · · · on”

consisting of say N words such that ~O 6= ~R. For
example,

~O =

{
"who is accountable boston
for the men affecting
solutions"

which was obtained using Kaldi (Povey et al.,
2011) with Fisher acoustic models (gpAM) and
language models (gpLM) (Kaldi, 2015). Namely,

~R −→ gpASR
︸ ︷︷ ︸x

gpLM, gpAM

−→ ~O

Observe that M can be ≶ N . However, there
are only three type of operations that are possible
to transform ~O to ~R, namely, a word in ~O is either
deleted or is inserted or is substituted so that ~O can
become identical to ~R. Clearly, one insertion, two
deletions and two substitutions to ~O could make it
identical to ~R, namely,

"who is (φ
ins→the) accountable

(boston
sub→person) for

(the
del→ φ) (men del→ φ)

(affecting
sub→manufacturing)

solutions"

~Op ←− word2phoneme︸ ︷︷ ︸x
gpLexicon

←− ~O

Let each word in ~R and ~O be represented by its
phonetic equivalent so that

~Rp = r1r2 · · · rM
and

~Op = o1o2o3 · · · oN
such that {ri, oi} ∈ IP where IP is the set of
phonemes. Note that r1 in ~Rp is a phoneme while
r1 in ~R is a lexical word. In general there are 39
phones in IP , namely,

IP =





a, ae, ah, ao, aw, ay, b, ch,
d, dh, eh, er, ey, f, g, hh, ih,
iy, jh, k, l, m, n, ng, ow, oy,
p, r, s, sh, t, th, uh, uw, v,
w, y, z, zh



284



Subsequently, we can write

~Op =





hh uw ih z dh iy ah k aw n t ah b

ah l p er s ah n f r er m ae n y

ah f ae k ch er ih ng s ah l uw sh

ah n z
(1)

and

~Rp =

{hh uw ih z ah k aw n t ah b ah l b
ao s t ah n f r er dh iy m eh n ah

f eh k t ih ng s ah l uw sh ah n z.
(2)

Notice that even as a phonemic string, ~Op can be
transformed into ~Rp through one of the three oper-
ations, namely, deletion, insertion or substitution.

2.1 Computing Posteriors
We define an extension IP ′ = {IP, φ}, where the
element φ represents a null-phoneme. Given a cor-
pus of { ~Oip, ~Rip}Ki=1 pairs (K is large). Note that
the elements of ~Op and elements of ~Rp ∈ IP and
can take one of the 39 unique phones. Represent o
and r as p ∈ IP . Now we compute for all pi ∈ IP

Psub = P (pi
sub→ pj) =

#((pi ∈ ~Op)&(pj ∈ ~Rp))
#(pi ∈ ~Op)

(3)
where (a) #(pi ∈ ~Op) is the count of the
phone pi occurring in { ~Oip}Ki=1 and (b)
#((pi ∈ ~Op)&(pj ∈ ~Rp)) is the count of the
phone pi which occurs in { ~Oip}Ki=1 when pj
occurs in {~Rip}Ki=1. Similarly, we find

Pins = P (φ
sub→ pj) (4)

where pj occurs in ~Rp but does not occur in ~Op
and

Pdel = P (pi
sub→ φ) (5)

where pi occurs in ~Op but does not occur in ~Rp.

Note that we can compute P∗(pi
sub→ pj) for all

pi, pj ∈ IP ′, clearly P∗ is a 40 × 40 matrix. The
last column corresponds to Pdel while the last row
corresponds to Pins. We conjecture that these pos-
terior probabilities P∗(pi

sub→ pj) can be used to
model the ASR engine (see Figure 1) which in turn
can be used to repair the ASR output, namely,

~Op −→ repair︸ ︷︷ ︸x
P∗

−→ ~Rp

Figure 1: P∗ for Kaldi (Povey et al., 2011) with
Fischer acoustic and language models. Note that
there are higher Pins.

2.2 Repair Approach
Now given an general purpose ASR output,
say, O1O2O3O4O5 · · ·ON identify the words Oi
which are not part of the domain along the lines
of (Anantaram et al., 2015a), using a domain on-
tology. The domain ontology (dLexicon) helps in
identifying all the Oi’s such that Oi /∈ dLexicon.

Then using the domain lexicon we con-
struct for each Oi the phoneme sequence
oi1, oi2, oi3, oi4, oi5, · · · , oim where each oij ∈ IP .
We then expand the phonemes that require correc-
tion as

expand(oi, φ) = φ, oi1, φ, oi2φ, oi3, φ, oi4,

φ, oi5, φ, · · · , φ, oim, φ (6)
by inserting φ between the phonemic representa-
tion of Oi. This extended phoneme string is cor-
rected using the posterior (P∗). Now for all iden-
tified oij and φ ∈ �, find all pk ∈ IP ′ such that the
posterior

P∗(oj
sub→ pk) > τ and P∗(φ sub→ pk) > τ. (7)

We form a lattice (L) using pk obtained in (7) ,
namely,

� −→ P∗ −→ L
We search through this lattice, L, to identify the
best set of pk that results in the highest probability
score such that the resulting phoneme string rep-
resents a word in the domain, namely,

L −→ search, phoneme2word
︸ ︷︷ ︸x

dLexicon

−→ O′

285



where O′ is the corrected word corresponding to
Oi. Algorithm 1 captures this approach in greater
details.

Algorithm 1 Repair ~O.
Given ~O = O1O2O3O4O5 · · ·ON
Given τ , P∗/* Posterior */
Given cnt = 0, dLexicon /* Domain Ontology
*/

for k = 1, 2 · · ·N do
if Ok /∈ dLexicon then
α[cnt++]←− Ok
/* α contains all words /∈ dLexicon */

end if
end for

for l = 1, 2, · · · , cnt do
~o← Ol /* Using gpLexicon */
~ol ← expand(~o, φ) /* Expand using (6)*/
for pj ∈ ~ol do
knt = 0
for (pk ∈ IP ′) do

if (P (pj
sub→ pk) > τ) then

Lj,knt++ =
{
pk, P (pj

sub→ pk)
}

end if
end for
/* L is the lattice */

end for

Search through L to find the repaired ~o ′l
end for

O′l ← ~o ′l /* Using Lexicon */
/* O′l is the corrected word */

3 Experimental Results

3.1 Data setup
Experiments were carried out on a database of 700
spoken utterances by 7 different people, each of
them speaking 100 sentences. Each of the 7 speak-
ers were given a set of 100 different queries which
they were required to speak into a data collection
application built in-house. Each of them spoke
10 queries in a session and the 10 sessions were
recorded over a period of one or two days. The
spoken utterance was recorded in the wave for-
mat with 16 bits resolution and a sampling rate of
16 kHz. These 700 spoken utterances were first

converted to text using a general purpose speech
recognition engine, in our case Kaldi (Povey et al.,
2011) using the Fischer acoustic (Kaldi, 2015) and
Fischer language models. We built the gpLexicon
using the 700 decoded text outputs using an online
tool (CMU, 2017). Note that the general purpose
ASR (Kaldi) can output words which need not al-
ways be part of the domain Ontology (in our case
the domain was related to software industry). In
all there were 1426 unique words in the 700 ASR
text output (gpLexicon) and there were 372 unique
words in the domain lexicon (dLexicon). We di-
vided the 700 into 5 sets of 140 utterances each
and carried out a 5 fold validation, namely we used
4 sets, consisting of 560 sentences to compute the
posterior and 1 set consisting of 140 sentences to
test. In this paper, we present a sample example to
demonstrate the effectiveness of our approach.

3.2 Construction of P∗
We constructed the posteriors using the decoded
text (example, (1)) and the actual spoken text (ex-
ample, (2)) after converting both the text strings
into phonemes using a phonetic lexicon (we used
gpLexicon for ASR output text and dLexicon for
the actual spoken text). We aligned the two pho-
netic strings using edit distance algorithm which
came with (Povey et al., 2011). For (1) and (2) we
get
”hh, uw, ih, z, dh, (aesub→ah), (t del→ φ), ah, k, aw,
n, t, ah, b, ah, l, (bsub→p), (aasub→er), s, (t del→ φ),
ah, n, f, ao, r, (dh del→ φ), (ah del→ φ), m, (ehsub→ae),
n, (φ ins→y), ah, f, (ehsub→ae), k, (tsub→ch), (ihsub→er),
(φ ins→ih), ng, s, ah, l, uw, sh, ah, n, z”
which are used to compute the posterior as men-
tioned earlier. As seen there are 7 substitutions,
3 deletions and 2 insertions. A sample posterior
P∗ is shown in Figure 1 as a contour plot. The x-
axis represents the phonemes (from the actual text)
while the y-axis is the phonemes (in the words rec-
ognized by gpASR).

3.3 Sample Repair
As an example the general purpose ASR returned

"who is that accountable
boston for the men affecting
solutions"

when
/who is the accountable
person for manufacturing
solutions/

286



”boston"
b ao s t ah n

1 φ b φ ao φ s φ t φ ah φ n φ
2 ah p ah ah ah sh ah φ ah φ ah φ ah
3 ih φ ih φ ih φ ih r ih er ih l ih
4 k r k er k z k ah k r k ah k
5 t w t aa t ah t er t aa t k t
6 l l l ow l er l l l ae l r l
7 r ah r iy r ey r d r ih r ae r
8 s dh s sh s r s th s eh s ih s
9 d d d p d ih d dh d ey d m d
10 w t w v w ch w eh w uw w ng w

p er s ah n
”person"

Table 1: Lattice L constructed using the posterior P∗, used to correct ’b ao s t ah n’ to ’p er s
ah n’. The phones marked in bold are the ones that are picked during the lattice search.

was given as the audio input to the gpASR. The
output text ”boston" and ”men affecting"
are not part of the domain ontology (dLexicon).

We first converted the word ”boston" into
its phoneme equivalent using the general pur-
pose phonetic lexicon (gpLexicon), namely ’b
ao s t ah n’, then using the posterior P∗ we
constructed the lattice L (see Table 1) which
shows the top 10 phonemes that could replace the
phoneme output by the ASR based on the poste-
riorP∗. For example, the column associated with b
shows that P (b sub→ p) ≥ P (b del→ φ) ≥ P (b sub→ r)
etc. Note that we have captured the possible inser-
tion by appending φ between the phones, namely,
φ b φ ao φ s φ t φ ah φ n (6). Now traversing the
lattice L in Table 1 from left to right we can get φ
p φ er φ s φ φ φ ah φ n φ which is nothing but ’p
er s ah n’ which is part of our domain.

A similar repair mechanism described in Algo-
rithm 1 helps in correcting ”men affecting"
to the domain word ”manufacturing" as
shown in Figure 2. As a result the general purpose
ASR output

"who is that accountable
boston for the men affecting
solutions"

is corrected to

"who is that accountable
person for the manufacturing
solutions"

Note that this is not exactly what was spoken,

namely,

/who is the accountable
person for manufacturing
solutions/

Notice that the repair mechanism is unable to cor-
rect ”that" to ”the" and delete ”the" however
the repair mechanism is able to correct the non-
domain words by identifying phonetically equiva-
lent words in the domain. In this example and all
our experiments we choose τ in a manner that we
had the top 10 phonemes to form the lattice L.

Also notice that because the repair mech-
anism operates in the phoneme space it is
able to operate even if the error is spread
across words (example, ”men affecting"
−→ ”manufacturing"). This example sen-
tence clearly demonstrates the ability of the pro-
posed repair approach to correct the output of a
general purpose automatic speech recognition en-
gine based on the computed posteriors.

We measured the accuracy of improvement in
the ASR output, by looking at the edit distance
between ( ~O, ~R) and the edit distance between ( ~O′,
~R). Here ~R is the reference out (perfect ASR out-
put), while ~O is the output of the general purpose
ASR (in our case Kaldi ASR output) and ~O′ is the
repaired output based on Algorithm 1. It is ob-
served that the dis( ~O′, ~R)≤ dis( ~O, ~R). Namely,
the repaired output was always closer to the refer-
ence that the output of the general purpose ASR
engine ( ~O).287



”men" ”affecting"
m eh n ah f eh k t ih ng

φ m φ eh φ n φ ah φ f φ eh φ k φ t φ ih φ ng φ
ah φ ah ah ah φ ah φ ah v ah ah ah φ ah φ ah ey ah n ah
ih n ih ae ih l ih er ih ah ih ae ih t ih r ih ah ih φ ih
k l k φ k ah k r k φ k φ k ah k ah k eh k d k
t ih t ih t k t aa t p t ih t f t er t φ t v t

m ae n ah f ae k er ih ng
”manufacturing"

Table 2: Lattice L constructed using the posterior P∗, used to correct ”men affecting".

4 Conclusions

Speech based applications are being increasingly
deployed for self help in enterprises. However
for resource deficient languages (including Indian
English) the performance of ASR engine is poor
especially for natural spoken utterances. While
there are two ways of getting over the poor per-
formance of ASR engine, namely (a) fine tuning
the AR engine in terms of acoustic and language
models; thereby making the ASR engine domain
specific or (b) using a general purpose ASR engine
and then correcting the ASR output using domain
ontology and in some way modeling the ASR be-
havior. The advantage of the second approach is
that of being able to use a readily available state
of the art ASR engine without having to build a
unique speech recognition engine for every appli-
cation. In this paper, we approach the problem of
correcting the general purpose ASR output using
posteriors. The main contribution of this paper is
the formulation of a posterior approach to repair
the output of a general purpose ASR engine as de-
picted in detail in Algorithm 1. We also showed
that the approach is able to repair the errors that
might be spread across lexical words.

5 Acknowledgements

The authors would like to thank Chirag Patel who
assisted in performing some experiments in the
earlier stages of this work.

References
W.A. Ainsworth and S.R. Pratt. 1992. Feedback strate-

gies for error correction in speech recognition sys-
tems. International Journal of Man-Machine Stud-
ies, 36(6):833 – 842.

C. Anantaram and Sunil Kumar Kopparapu. 2017.
Adapting general-purpose speech recognition en-

gine output for domain-specific natural language
question answering. CoRR, abs/1710.06923.

C. Anantaram, Rishabh Gupta, Nikhil Kini, and
Sunil Kumar Kopparapu. 2015a. Adapting
general-purpose speech recognition engine output
for domain-specific natural language question an-
swering. In Workshop on Replicability and Repro-
ducibility in Natural Language Processing: adap-
tive methods, resources and software at IJCAI 2015,
Buenous Aires.

C Anantaram, Nikhil Kini, Chirag Patel, and Sunil
Kopparapu. 2015b. Improving ASR recognized
speech output for effective NLP. In The Ninth Inter-
national Conference on Digital Society ICDS 2015,
pages 17–21, Lisbon, Portugal.

Youssef Bassil and Mohammad Alwani. 2012. Post-
editing error correction algorithm for speech recog-
nition using bing spelling suggestion. CoRR,
abs/1203.5255.

CMU. 2017. The CMU pronouncing dictio-
nary. http://www.speech.cs.cmu.edu/
cgi-bin/cmudict.

Ferdinand Fuhrmann, Anna Maly, Christina Leitner,
and Franz Graf. 2017. Three experiments on the
application of automatic speech recognition in in-
dustrial environments. In Nathalie Camelin, Yan-
nick Estève, and Carlos Martı́n-Vide, editors, Statis-
tical Language and Speech Processing - 5th Inter-
national Conference, SLSP 2017, Le Mans, France,
October 23-25, 2017, Proceedings, volume 10583 of
Lecture Notes in Computer Science, pages 109–118.
Springer.

Minwoo Jeong, Byeongchang Kim, and G Lee. 2004.
Using higher-level linguistic knowledge for speech
recognition error correction in a spoken q/a dialog.
In HLT-NAACL special workshop on Higher-Level
Linguistic Information for Speech Processing, pages
48–55.

Kaldi. 2015. Kaldi fisher english. http:
//kaldi-asr.org/downloads/build/2/
sandbox/online/egs/fisher_english/.288



Stefan Kombrink, Tomas Mikolov, Martin Karafiát,
and Lukás Burget. 2012. Improving language mod-
els for ASR using translated in-domain data. In
2012 IEEE International Conference on Acoustics,
Speech and Signal Processing, ICASSP 2012, Ky-
oto, Japan, March 25-30, 2012, pages 4405–4408.
IEEE.

S.K. Kopparapu. 2014. Non-Linguistic Analysis of
Call Center Conversations. SpringerBriefs in Elec-
trical and Computer Engineering. Springer Interna-
tional Publishing.

A. Mohamed, G.E. Dahl, and G. Hinton. 2012. Acous-
tic modeling using deep belief networks. Audio,
Speech, and Language Processing, IEEE Transac-
tions on, 20(1):14 –22, jan.

Hiromitsu Nishizaki and Yoshihiro Sekiguchi. 2006.
Word error correction of continuous speech recog-
nition using web documents for spoken document
indexing. In Yuji Matsumoto, RichardW. Sproat,
Kam-Fai Wong, and Min Zhang, editors, Computer
Processing of Oriental Languages. Beyond the Ori-
ent: The Research Challenges Ahead, volume 4285
of Lecture Notes in Computer Science, pages 213–
221. Springer Berlin Heidelberg.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, Jan Silovsky, Georg Stemmer, and Karel
Vesely. 2011. The kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, number
Idiap-RR-04-2012, Rue Marconi 19, Martigny, De-
cember. IEEE Signal Processing Society. IEEE Cat-
alog No.: CFP11SRW-USB.

E.K. Ringger and J.F. Allen. 1996. Error correction
via a post-processor for continuous speech recogni-
tion. In Acoustics, Speech, and Signal Processing,
1996. ICASSP-96. Conference Proceedings., 1996
IEEE International Conference on, volume 1, pages
427–430 vol. 1, May.

Yongmei Shi. 2008. An Investigation of Linguistic
Information for Speech Recognition Error Detec-
tion. Ph.D. thesis, University of Maryland, Balti-
more County, October.

Johannes Twiefel, Timo Baumann, Stefan Heinrich,
and Stefan Wermter. 2014. Improving domain-
independent cloud-based speech recognition with
domain-dependent phonetic post-processing. In
Proceedings of the Twenty-Eighth AAAI Conference
on Artificial Intelligence, AAAI’14, pages 1529–
1535. AAAI Press.

Lina Zhou, Jinjuan Feng, A. Sears, and Yongmei Shi.
2005. Applying the naive bayes classifier to assist
users in detecting speech recognition errors. In Sys-
tem Sciences, 2005. HICSS ’05. Proceedings of the
38th Annual Hawaii International Conference on,
pages 183b–183b, Jan.

289


