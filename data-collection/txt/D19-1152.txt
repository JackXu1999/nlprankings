



















































Improving Generative Visual Dialog by Answering Diverse Questions


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1449–1454,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1449

Improving Generative Visual Dialog by Answering Diverse Questions

Vishvak Murahari1 Prithvijit Chattopadhyay1

Dhruv Batra1,2 Devi Parikh1,2 Abhishek Das1

1Georgia Tech 2Facebook AI Research

Abstract

Prior work on training generative Visual Dia-
log models with reinforcement learning (Das
et al., 2017b) has explored a Q-BOT-A-
BOT image-guessing game and shown that
this ‘self-talk’ approach can lead to im-
proved performance at the downstream dialog-
conditioned image-guessing task. However,
this improvement saturates and starts degrad-
ing after a few rounds of interaction, and does
not lead to a better Visual Dialog model. We
find that this is due in part to repeated interac-
tions between Q-BOT and A-BOT during self-
talk, which are not informative with respect to
the image. To improve this, we devise a sim-
ple auxiliary objective that incentivizes Q-BOT
to ask diverse questions, thus reducing repeti-
tions and in turn enabling A-BOT to explore a
larger state space during RL i.e. be exposed to
more visual concepts to talk about, and varied
questions to answer. We evaluate our approach
via a host of automatic metrics and human
studies, and demonstrate that it leads to bet-
ter dialog, i.e. dialog that is more diverse (i.e.
less repetitive), consistent (i.e. has fewer con-
flicting exchanges), fluent (i.e. more human-
like), and detailed, while still being compara-
bly image-relevant as prior work and ablations.

1 Introduction

Our goal is to build agents that can see and talk i.e.
agents that can perceive the visual world and com-
municate this understanding in natural language
conversations in English. To this end, Das et al.
(2017a); de Vries et al. (2017) proposed the task
of Visual Dialog – given an image, dialog history
consisting of a sequence of question-answer pairs,
and a follow-up question about the image, predict
a free-form natural language answer to the ques-
tion – along with a dataset and evaluation metrics1.
Posing Visual Dialog as a supervised learning
problem is unnatural. This is because at every

1visualdialog.org, guesswhat.ai

round of dialog, the agent’s answer prediction is
thrown away and it gets access to ground-truth di-
alog history, thus disabling it from steering conver-
sations during training. This leads to compound-
ing errors over long-range sequences at test time,
a problem also common in training recurrent neu-
ral networks for language modeling (Bengio et al.,
2015; Ross et al., 2011; Ranzato et al., 2016).
To overcome this, Das et al. (2017b) devised a
goal-driven approach for training Visual Dialog
agents with deep reinforcement learning. This is
formulated as a game between two agents – Q-
BOT (that asks questions) and A-BOT (that an-
swers questions). Q-BOT is shown a one-line de-
scription of an unseen image that A-BOT has ac-
cess to and Q-BOT is allowed to ask questions (in
natural language) to A-BOT for a fixed number
of rounds and simultaneously make predictions of
the unseen image. Both agents are rewarded for
Q-BOT’s image-guessing performance and trained
with REINFORCE (Williams, 1992) to optimize
this reward. Thus, there is incentive for Q-BOT
to ask questions informative of the hidden image,
and for A-BOT to provide meaningful answers.
While this reinforcement learning approach leads
to improved performance on the image-guessing
task than supervised learned agents, it has a few
shortcomings – 1) image-guessing performance
degrades after a few rounds of dialog (Fig. 2e),
and 2) these improvements over supervised learn-
ing do not translate to an improved A-BOT, i.e.
responses from this Visual Dialog agent are not
necessarily better (on automatic metrics or human
judgements), just that the Q-BOT-A-BOT pair is
sufficiently in sync to do well at image-guessing.
We begin by understanding why this is the case,
and find that Q-BOT-A-BOT dialog during ‘self-
talk’ often tends to be repetitive i.e. the same
question-answer pairs get repeated across rounds
(Fig. 1a left). Since repeated interactions convey
no additional information, image-guessing perfor-

https://visualdialog.org
https://guesswhat.ai


1450

(a) Left. Prior work on training generative Visual Dialog models with RL on an image-guessing task between
Q-BOT and A-BOT (Das et al., 2017b) leads to repetitive dialog. Right. We devise an auxiliary objective that
incentivizes Q-BOT to ask diverse questions, thus reducing repetitions and enabling A-BOT to be exposed to more
varied questions during RL, overall leading to better dialog, as measured by automatic metrics and human studies.

(b) Cosine similarity of successive dia-
log state embeddings within Q-BOT. Prior
work (Das et al., 2017b) has high similarity.
Our approach explicitly minimizes this sim-
ilarity leading to more diverse dialog.

Figure 1

mance saturates, and even starts to degrade as the
agent forgets useful context from the past.
These repetitions are due to high similarity in Q-
BOT’s context vectors of successive rounds driving
question generation (Fig. 1b). To address this, we
devise a smooth-L1 penalty that penalizes similar-
ity in successive state vectors (Section 3). This
incentivizes Q-BOT to ask diverse questions, thus
reducing repetitions and in turn enabling A-BOT
to explore a larger part of the state space during
RL i.e. be exposed to more visual concepts to talk
about, and varied questions to answer (Section 6).
Note that a trivial failure mode with this penalty
is for Q-BOT to start generating diverse but totally
image-irrelevant questions, which are not useful
for the image-guessing task. A good balance be-
tween diversity and image-relevance in Q-BOT’s
questions is necessary to improve at this task.
We extensively evaluate each component of our
approach against prior work and baselines:

• Q-BOT on diversity and image-relevance of gen-
erated questions during Q-BOT-A-BOT self-talk.
We find that diverse-Q-BOT asks more novel
questions while still being image-relevant.

• Q-BOT-A-BOT self-talk on consistency, flu-
ency, level of detail, and human-interpretability,
through automatic metrics and human studies.
We find that diverse-Q-BOT-A-BOT dialog after
RL is more consistent, fluent, and detailed.

• A-BOT on precision and recall of generated an-
swers on the VisDial dataset (Das et al., 2017a)
i.e. quality of answers to human questions.
Training diverse-Q-BOT` A-BOT with RL does
not lead to a drop in accuracy on VisDial.

2 Preliminaries

We operate in the same setting as Das et al.
(2017b) – an image-guessing task between a ques-
tioner (Q-BOT) and an answerer (A-BOT) – where
Q-BOT has to guess the image A-BOT has access
to by asking questions in multi-round dialog.
We adopt the same training paradigm2 consisting
of 1) a supervised pre-training stage where Q-BOT
and A-BOT are trained with Maximum Likelihood
Estimation objectives on the VisDial dataset (Das
et al., 2017a), and 2) a self-talk RL finetuning
stage where Q-BOT and A-BOT interact with each
other and the agents are rewarded for each suc-
cessive exchange based on incremental improve-
ments in guessing the unseen image. We learn pa-
rameterized policies πθQpqt|s

Q
t´1q and πθApat|sAt q

for Q-BOT and A-BOT respectively which decide
what tokens to utter (actions: question qt and an-
swer at at every dialog round t) conditioned on
the context available to the agent (state represen-
tations: sQt´1, s

A
t ). Q-BOT additionally makes an

image feature prediction ŷt at every dialog round,
and the reward is rt “ ||ygt´ŷt´1||22´||ygt´ŷt||22,
i.e. change in distance to the true representation
ygt before and after a dialog round. We use RE-
INFORCE (Williams, 1992) to update agent pa-
rameters, i.e. Q-BOT and A-BOT are respectively
updated with EπQ,πArrt∇θQ log πQpqt|s

Q
t´1qs and

EπQ,πArrt∇θA log πApat|sAt qs as gradients.
Our transition from supervised to RL is gradual
– we supervise for N rounds and have policy-
gradient updates for the remaining 10´N , starting
from N “ 9 till N “ 4, one round at a time. Af-
ter reaching N “ 4, repeating this procedure from
N “ 9 led to further (marginal) improvements.

2github.com/batra-mlp-lab/visdial-rl

https://github.com/batra-mlp-lab/visdial-rl


1451

Both Q-BOT and A-BOT are modeled as Hi-
erarchical Recurrent Encoder-Decoder architec-
tures (Serban et al., 2016). Q-BOT’s fc7 (Si-
monyan and Zisserman, 2015) feature prediction
of the unseen image (ŷ) is conditioned on the dia-
log history so far (sQt´1) via a regression head

3.

3 Smooth-L1 Penalty on Question
Repetition

Our goal is to encourage Q-BOT to ask a diverse
set of questions so that when A-BOT is exposed to
the same during RL finetuning, it is better able to
explore its state space4. Furthermore, asking di-
verse questions allows Q-BOT-A-BOT exchanges
across rounds to be more informative of the im-
age, thus more useful for the image-guessing task.
We observe that agents trained using the paradigm
proposed by Das et al. (2017b) suffer from repe-
tition of context across multiple rounds of dialog
– similar dialog state embeddings across multi-
ple rounds leading to repeated utterances and sim-
ilar predicted image representations, which con-
sequently further increases similarity in state em-
beddings. Fig. 1b shows increasing cospsQt´1, s

Q
t q

across dialog rounds for Das et al. (2017b).
To encourage Q-BOT to ask diverse questions, we
propose a simple auxiliary loss that penalizes sim-
ilar dialog state embeddings. Specifically, given
Q-BOT states – sQt´1, s

Q
t – in addition to maximiz-

ing likelihood of question (during supervised pre-
training), or image-guessing reward (during self-
talk RL finetuning), we maximize a smooth-L1
penalty on ∆t “ absp

∥∥∥sQt´1∥∥∥
2
´
∥∥∥sQt ∥∥∥

2
q,

fp∆tq “
#

0.5∆2t if ∆t ă 0.1
0.1p∆t ´ 0.05q otherwise

(1)

resulting in
řN
t“2 fp∆tq as an additional term in

the overall objective (N “ no. of dialog rounds).
Note that in order to maximize this penalty, Q-
BOT has to push sQt´1 and s

Q
t further apart,

which can only happen if sQt´1 is updated using a
question-answer pair that is different from the pre-
vious exchange, thus overall forcing Q-BOT to ask
different questions in successive dialog rounds.
Similar diversity objectives have also been ex-
plored in Li et al. (2016b) as reward heuristics.

3Please refer to appendix for architecture details.
4A-BOT’s state-space is characterized by a representation

of the question, image, and the dialog history so far.

Before arriving at (1), and following Fig. 1b, we
also experimented with directly minimizing cosine
similarity, cospsQt´1, s

Q
t q. This led to the network

learning large biases to flip the direction of succes-
sive sQt´1 vectors (without affecting norms), lead-
ing to question repetitions in alternating rounds.

4 Experiments

Baselines and ablations. To understand the ef-
fect of the proposed penalty, we compare our full
approach – ‘RL: Diverse-Q-bot + A-bot’ – with
the baseline setup in Das et al. (2017b), as well as
several ablations – 1) ‘SL: Q-bot + A-bot’: su-
pervised agents (i.e. trained on VisDial data under
MLE, no RL, no smooth-L1 penalty). Compar-
ing to this quantifies how much our penalty + RL
helps. 2) ‘SL: Diverse-Q-bot + A-bot’: supervised
agents where Q-BOT is trained with the smooth-
L1 penalty. This quantifies gains from RL.
Automatic Metrics. To evaluate Q-BOT’s diver-
sity (Table 1), we generate Q-BOT-A-BOT dialogs
(with beam size = 5) for 10 rounds on VisDial v1.0
val and compute 1) Novel Questions: the number
of new questions (via string matching) in the gen-
erated dialog not seen during training, 2) Unique
Questions: no. of unique questions per dialog in-
stance (soď 10), 3) Dist-n and Ent-n (Zhang et al.,
2018; Li et al., 2016a): the number and entropy of
unique n-grams in the generated questions normal-
ized by the total number of tokens, and 4) Mutual
Overlap (Deshpande et al., 2019): BLEU-4 over-
lap of every question in the generated 10-round di-
alog with the other 9 questions, followed by aver-
aging these 10 numbers. To measure Q-BOT’s rel-
evance, we report the negative log-likelihood un-
der the model of human questions from VisDial.
We evaluate A-BOT’s answers to human questions
from the VisDial dataset on the retrieval metrics
introduced by Das et al. (2017a) (Table 2). Fi-
nally, we also evaluate performance of the Q-BOT-
A-BOT pair at image-guessing (Fig. 2e), which is
the downstream task they are trained with RL for.
Human Studies. To evaluate how human-
understandable Q-BOT-A-BOT dialogs are, we
conducted a study where we showed humans these
dialogs (from our agents as well as baselines),
along with a pool of 16 images from the VisDial
v1.0 test-std split – consisting of the unseen im-
age, 5 nearest neighbors (in fc7 space), and 10 ran-
dom images – and asked humans to pick their top-
5 guesses for the unseen image. Our hypothesis



1452

Diversity Relevance

# Novel questions Ò # Unique questions Ò Mutual overlap Ó Ent-1 Ò Ent-2 Ò Dist-1 Ò Dist-2 Ò Negative log likelihood Ó

Baseline: Das et al. (2017b) 71 6.70 ˘ 0.07 0.58 ˘ 0.01 2.72 ˘ 0.01 3.03 ˘ 0.02 0.35 ˘ 0.0 0.43 ˘ 0.0 9.94
SL: Q-BOT ` A-BOT 51 6.57 ˘ 0.07 0.60 ˘ 0.01 2.70 ˘ 0.01 3.00 ˘ 0.02 0.34 ˘ 0.0 0.42 ˘ 0.0 10.05
SL: Diverse-Q-BOT ` A-BOT 146 7.45 ˘ 0.07 0.51 ˘ 0.01 2.82 ˘ 0.01 3.18 ˘ 0.01 0.38 ˘ 0.0 0.48 ˘ 0.0 10.10
RL: Diverse-Q-BOT ` A-BOT 449 8.19 ˘ 0.06 0.41 ˘ 0.01 2.90 ˘ 0.01 3.31 ˘ 0.01 0.40 ˘ 0.0 0.53 ˘ 0.0 10.80

Table 1: Q-BOT diversity and relevance on v1.0 val. Ò indicates higher is better. Ó indicates lower is better.
v1.0 val v1.0 test-std

NDCG Ò MRR Ò R@1 Ò R@5 Ò R@10 Ò Mean Rank Ó NDCG Ò MRR Ò R@1 Ò R@5 Ò R@10 Ò Mean Rank Ó

Baseline: Das et al. (2017b) 53.76 46.35 36.22 56.15 62.41 19.34 51.60 45.67 35.05 56.30 63.25 19.15
SL: A-BOT 53.10 46.21 36.11 55.82 62.22 19.58 51.18 45.43 34.88 55.65 63.20 19.16
RL: A-BOT (finetuned with Diverse-Q-BOT) 53.91 46.46 36.31 56.26 62.53 19.35 51.67 45.64 34.85 56.55 63.43 18.96

Table 2: A-BOT performance on VisDial v1.0 (Das et al., 2017a). Ò indicates higher is better. Ó indicates lower is better.

(a) Consistency Ò (b) Fluency Ò (c) Detail Ò (d) Top-1 Acc. Ò

(e) Q-BOT-A-BOT image-guessing task performance

Figure 2: (a-d): Human evaluation of Q-BOT-A-BOT dialog over 50 images and 200 human subjects for each model variant.
(e): Percentile rank (higher is better) of the true image (shown to A-BOT) as retrieved using fc7 feature predictions from Q-BOT.

was that if questions are more diverse, the dialog
will be more image-informative, and so humans
should be able to better guess which image was be-
ing talked about. We report top-1 accuracy of true
image in human guesses. We also asked humans
to rate Q-BOT-A-BOT dialog on consistency, flu-
ency, and level of detail on a 5-point Likert scale.

5 Implementation Details

We used beam search with a beam size of 5 dur-
ing self-talk between all Q-BOT-A-BOT variants.
NDCG scores on the v1.0 val split and the to-
tal SL loss (on the same split) were used to se-
lect the best SL A-BOT and Q-BOT checkpoints
respectively. We used a dropout rate of 0.5 for
all SL-pretraining experiments and no dropout for
RL-finetuning. We used Adam (Kingma and Ba,
2015) with a learning rate of 10´3 decayed by
„0.25 every epoch, upto a minimum of 5ˆ 10´5.
The objective for training Diverse-Q-BOT was a
sum of the smooth-L1 penalty (introduced in Sec-
tion 3), cross entropy loss, and L2 loss between the
regression head output and the fc7 (Simonyan and
Zisserman, 2015) embedding of the image. We
observed that coefficients in the range of 1e´3 to
1e´5 worked best for the smooth-L1 penalty. We
also observed that training for a large number of
epochs („80) with the above mentioned range of
coefficient values led to the best results.

6 Results

• Q-BOT’s diversity (Table 1): The question-
repetition penalty consistently increases diver-
sity (in both SL and RL) over the baseline! RL:
Diverse-Q-bot asks ~1.5 more unique questions
on average than Das et al. (2017b) (6.70 Ñ
8.19) for every 10-round dialog, ~6.3x more
novel questions (71 Ñ 449), and a higher frac-
tion and entropy of unique generated n-grams,
while still staying comparably relevant (NLL).

• A-BOT on VisDial (Table. 2): RL: A-bot out-
performs SL: A-bot, but does not statistically
improve over the baseline on answering human
questions from VisDial5 (on v1.0 val & test-std).

• Image-guessing task (Fig. 2e): Diverse-Q-bot
+ A-bot (SL and RL) significantly outperform
the baseline on percentile rank of ground-truth
image as retrieved using Q-BOT’s fc7 predic-
tion. Thus, the question-repetition penalty leads
to a more informative communication protocol.

• Human studies (Fig. 2): Humans judged RL:
Diverse-Q-bot + A-bot dialog significantly
more consistent (fewer conflicting exchanges),
fluent (fewer grammatical errors), and detailed
(more image-informative) over the baseline and
supervised learning. This is an important re-
sult. Performance on GuessWhich, together

5This is consistent with trends in Das et al. (2017b).



1453

with these dialog quality judgements from hu-
mans show that agents trained with our approach
develop a more effective communication pro-
tocol for the downstream image-guessing task,
while still not deviating off English, which is a
common pitfall when training dialog agents with
RL (Kottur et al., 2017; Lewis et al., 2017).

Note that since our penalty (Eqn. 1) is structured
to avoid repetition across successive rounds, one
possible failure mode is that Q-BOT learns to ask
the same question every alternate dialog round (at
t and t`2). Empirically, we find that this happens
„15% of times (2490 times out of „16.5k ques-
tion pairs) on v1.0 val for RL: Diverse Q-BOT +
A-BOT compared to „22% for SL: Q-BOT + A-
BOT. This observation, combined with the fact
that Diverse-Q-BOT asks „1.6 more unique ques-
tions relative to SL: Q-BOT across 10 rounds sug-
gests that simply incentivizing diversity in succes-
sive rounds works well empirically. We hypothe-
size that this is because repeating questions every
other round or other such strategies to game our
repetition penalty is fairly specific behavior that
is likely hard for models to learn given the large
space of questions Q-BOT could potentially ask.

7 Related Work

Our work is related to prior work in visual di-
alog (Das et al., 2017a; de Vries et al., 2017)
and modeling diversity in text-only dialog (Zhang
et al., 2018; Li et al., 2016a,b).
Closest to our setting is work on using conditional
variational autoencoders for self-talk in visual di-
alog (Massiceti et al., 2018), where diversity is
not explicitly modeled but is measured via metrics
specific to the proposed architecture.
Adding constraints to generate a diverse set of
natural language dialog responses have previously
been explored in Zhang et al. (2018) via adversar-
ial information maximization, in Gao et al. (2019)
by jointly modeling diversity and relevance in a
shared latent space, and in Li et al. (2016a) us-
ing a maximum mutual information criterion. In
contrast, we are interested in diversity at the level
of the entire dialog (instead of a single round)
– reducing repetitions in QA pairs across multi-
ple rounds. Our repetition penalty is partly in-
spired by the ‘Information Flow’ constraint in Li
et al. (2016b). As detailed in Section 2, we ex-
perimented with similar forms of the penalty and
eventually settled on smooth-L1. To the best of

our knowledge, we are the first to explicitly model
diversity as a constraint in visual dialog.

8 Conclusions & Future Work

We devised an auxiliary objective for training gen-
erative Visual Dialog agents with RL, that incen-
tivizes Q-BOT to ask diverse questions. This re-
duces repetitions in Q-BOT-A-BOT dialog during
RL self-talk, and in turn enables A-BOT to be
exposed to a larger state space. Through exten-
sive evaluations, we demonstrate that our Q-BOT-
A-BOT pair has significantly more diverse dia-
log while still being image-relevant, better down-
stream task performance, and higher consistency,
fluency, and level of detail than baselines. Our
code will be publicly released, and we hope this
will serve as a robust base for furthering progress
on training visual dialog agents with RL for other
multi-agent grounded language games, adapting to
learn to talk about novel visual domains, etc.

9 Acknowledgements

We thank Nirbhay Modhe and Viraj Prabhu for
the PyTorch implementation (Modhe et al., 2018)
of Das et al. (2017b) that we built on, and Jiasen
Lu for helpful discussions. The Georgia Tech ef-
fort is supported in part by NSF, AFRL, DARPA,
ONR YIPs, ARO PECASE. AD is supported in
part by fellowships from Facebook, Adobe, and
Snap Inc. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of the US Government, or any sponsor.

References

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks. In
NIPS. 1

Abhishek Das, Satwik Kottur, Khushi Gupta, Avi
Singh, Deshraj Yadav, José M.F. Moura, Devi Parikh,
and Dhruv Batra. 2017a. Visual Dialog. In CVPR. 1,
2, 3, 4, 5

Abhishek Das, Satwik Kottur, José M.F. Moura, Ste-
fan Lee, and Dhruv Batra. 2017b. Learning Cooper-
ative Visual Dialog Agents with Deep Reinforcement
Learning. In ICCV. 1, 2, 3, 4, 5

Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexan-
der G Schwing, and David Forsyth. 2019. Fast, di-



1454

verse and accurate image captioning guided by part-
of-speech. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
10695–10704. 3

Xiang Gao, Sungjin Lee, Yizhe Zhang, Chris Brock-
ett, Michel Galley, Jianfeng Gao, and Bill Dolan. 2019.
Jointly optimizing diversity and relevance in neural re-
sponse generation. arXiv preprint arXiv:1902.11205.
5

Diederik Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In ICLR. 4

Satwik Kottur, José MF Moura, Stefan Lee, and Dhruv
Batra. 2017. Natural language does not emerge ‘natu-
rally’ in multi-agent dialog. In EMNLP. 5

Mike Lewis, Denis Yarats, Yann N Dauphin, Devi
Parikh, and Dhruv Batra. 2017. Deal or no deal? end-
to-end learning for negotiation dialogues. In EMNLP.
5

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting objec-
tive function for neural conversation models. 3, 5

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016b. Deep Re-
inforcement Learning for Dialogue Generation. In
EMNLP. 3, 5

Daniela Massiceti, N Siddharth, Puneet K Dokania,
and Philip HS Torr. 2018. Flipdial: A generative model
for two-way visual dialogue. In CVPR. 5

Nirbhay Modhe, Viraj Prabhu, Michael Cogswell,
Satwik Kottur, Abhishek Das, Stefan Lee, Devi Parikh,
and Dhruv Batra. 2018. VisDial-RL-PyTorch. 5

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level training
with recurrent neural networks. In ICLR. 1

Stéphane Ross, Geoffrey Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and structured
prediction to no-regret online learning. In AISTATS. 1

Iulian Vlad Serban, Alberto García-Durán, Çaglar
Gülçehre, Sungjin Ahn, Sarath Chandar, Aaron C.
Courville, and Yoshua Bengio. 2016. Generating Fac-
toid Questions With Recurrent Neural Networks: The
30M Factoid Question-Answer Corpus. In ACL. 3

Karen Simonyan and Andrew Zisserman. 2015. Very
deep convolutional networks for large-scale image
recognition. In ICLR. 3, 4

Harm de Vries, Florian Strub, Sarath Chandar, Olivier
Pietquin, Hugo Larochelle, and Aaron Courville. 2017.
GuessWhat?! visual object discovery through multi-
modal dialogue. In CVPR. 1, 5

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256. 1, 2

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,
Xiujun Li, Chris Brockett, and Bill Dolan. 2018.
Generating informative and diverse conversational re-
sponses via adversarial information maximization. In
NIPS. 3, 5


