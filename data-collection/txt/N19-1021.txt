




































Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing


Proceedings of NAACL-HLT 2019, pages 240–250
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

240

Cyclical Annealing Schedule:
A Simple Approach to Mitigating KL Vanishing

Hao Fu1†, Chunyuan Li2†⇤, Xiaodong Liu2
Jianfeng Gao2, Asli Celikyilmaz2, Lawrence Carin1
1Duke University 2Microsoft Research, Redmond

{chunyl, xiaodl, jfgao, aslicel}@microsoft.com
{hao.fu, lcarin}@duke.edu

Abstract

Variational autoencoders (VAEs) with an auto-
regressive decoder have been applied for many
natural language processing (NLP) tasks. The
VAE objective consists of two terms, (i) re-
construction and (ii) KL regularization, bal-
anced by a weighting hyper-parameter �. One
notorious training difficulty is that the KL
term tends to vanish. In this paper we study
scheduling schemes for �, and show that KL
vanishing is caused by the lack of good latent
codes in training the decoder at the beginning
of optimization. To remedy this, we propose a
cyclical annealing schedule, which repeats the
process of increasing � multiple times. This
new procedure allows the progressive learning
of more meaningful latent codes, by leverag-
ing the informative representations of previ-
ous cycles as warm re-starts. The effectiveness
of cyclical annealing is validated on a broad
range of NLP tasks, including language mod-
eling, dialog response generation and unsuper-
vised language pre-training.

1 Introduction

Variational autoencoders (VAEs) (Kingma and
Welling, 2013; Rezende et al., 2014) have been ap-
plied in many NLP tasks, including language mod-
eling (Bowman et al., 2015; Miao et al., 2016),
dialog response generation (Zhao et al., 2017;
Wen et al., 2017), semi-supervised text classifi-
cation (Xu et al., 2017), controllable text genera-
tion (Hu et al., 2017), and text compression (Miao
and Blunsom, 2016). A prominent component of a
VAE is the distribution-based latent representation
for text sequence observations. This flexible repre-
sentation allows the VAE to explicitly model holis-
tic properties of sentences, such as style, topic, and
high-level linguistic and semantic features. Sam-
ples from the prior latent distribution can produce

⇤Corresponding author †Equal Contribution

diverse and well-formed sentences through simple
deterministic decoding (Bowman et al., 2015).

Due to the sequential nature of text, an auto-
regressive decoder is typically employed in the
VAE. This is often implemented with a recurrent
neural network (RNN); the long short-term mem-
ory (LSTM) (Hochreiter and Schmidhuber, 1997)
RNN is used widely. This introduces one notori-
ous issue when a VAE is trained using traditional
methods: the decoder ignores the latent variable,
yielding what is termed the KL vanishing problem.

Several attempts have been made to ameliorate
this issue (Yang et al., 2017; Dieng et al., 2018;
Zhao et al., 2017; Kim et al., 2018). Among them,
perhaps the simplest solution is monotonic KL an-
nealing, where the weight of the KL penalty term
is scheduled to gradually increase during train-
ing (Bowman et al., 2015). While these techniques
can effectively alleviate the KL-vanishing issue,
a proper unified theoretical interpretation is still
lacking, even for the simple annealing scheme.

In this paper, we analyze the variable depen-
dency in a VAE, and point out that the auto-
regressive decoder has two paths (formally defined
in Section 3.1) that work together to generate text
sequences. One path is conditioned on the latent
codes, and the other path is conditioned on previ-
ously generated words. KL vanishing happens be-
cause (i) the first path can easily get blocked, due
to the lack of good latent codes at the beginning of
decoder training; (ii) the easiest solution that an
expressive decoder can learn is to ignore the latent
code, and relies on the other path only for decod-
ing. To remedy this issue, a promising approach is
to remove the blockage in the first path, and feed
meaningful latent codes in training the decoder, so
that the decoder can easily adopt them to generate
controllable observations (Bowman et al., 2015).

This paper makes the following contributions:
(i) We provide a novel explanation for the KL-



241

vanishing issue, and develop an understanding of
the strengths and weaknesses of existing schedul-
ing methods (e.g., constant or monotonic anneal-
ing schedules). (ii) Based on our explanation,
we propose a cyclical annealing schedule. It re-
peats the annealing process multiple times, and
can be considered as an inexpensive approach to
leveraging good latent codes learned in the pre-
vious cycle, as a warm restart, to train the de-
coder in the next cycle. (iii) We demonstrate that
the proposed cyclical annealing schedule for VAE
training improves performance on a large range of
tasks (with negligible extra computational cost),
including text modeling, dialog response genera-
tion, and unsupervised language pre-training.

2 Preliminaries

2.1 The VAE model
To generate a text sequence of length T , x =
[x1, · · · , xT ], neural language models (Mikolov
et al., 2010) generate every token x

t

conditioned
on the previously generated tokens:

p(x) =
TY

t=1

p(x
t

|x
<t

),

where x
<t

indicates all tokens before t.
The VAE model for text consists of two parts,

generation and inference (Kingma and Welling,
2013; Rezende et al., 2014; Bowman et al., 2015).
The generative model (decoder) draws a continu-
ous latent vector z from prior p(z), and generates
the text sequence x from a conditional distribution
p
✓

(x|z); p(z) is typically assumed a multivariate
Gaussian, and ✓ represents the neural network pa-
rameters. The following auto-regressive decoding
process is usually used:

p
✓

(x|z) =
TY

t=1

p
✓

(x
t

|x
<t

, z). (1)

Parameters ✓ are typically learned by maximiz-
ing the marginal log likelihood log p

✓

(x) =

log

R
p(z)p

✓

(x|z)dz. However, this marginal
term is intractable to compute for many decoder
choices. Thus, variational inference is considered,
and the true posterior p

✓

(z|x) / p
✓

(x|z)p(z)
is approximated via the variational distribution
q
�

(z|x) is (often known as the inference model
or encoder), implemented via a �-parameterized
neural network. It yields the evidence lower bound

(ELBO) as an objective:

log p
✓

(x) � LELBO = (2)
E
q�(z|x)

⇥
log p

✓

(x|z)
⇤
� KL(q

�

(z|x)||p(z))

Typically, q
�

(z|x) is modeled as a Gaussian dis-
tribution, and the re-parametrization trick is used
for efficient learning (Kingma and Welling, 2013).

2.2 Training Schedules and KL Vanishing
There is an alternative interpretation of the ELBO:
the VAE objective can be viewed as a regular-
ized version of the autoencoder (AE) (Goodfel-
low et al., 2016). It is thus natural to extend the
negative of LELBO in (2) by introducing a hyper-
parameter � to control the strength of regulariza-
tion:

L
�

= L
E

+ �L
R

, with (3)
L
E

= �E
q�(z|x)

⇥
log p

✓

(x|z)
⇤

(4)

L
R

= KL(q
�

(z|x)||p(z)) (5)

where L
E

is the reconstruction error (or negative
log-likelihood (NLL)), and L

R

is a KL regularizer.
The cost function L

�

provides a unified per-
spective for understanding various autoencoder
variants and training methods. When � = 1,
we recover the VAE in (2). When � = 0, and
q
�

(z|x) is a delta distribution, we recover the AE.
In other words, the AE does not regularize the
variational distribution toward a prior distribution,
and there is only a point-estimate to represent the
text sequence’s latent feature. In practice, it has
been found that learning with an AE is prone to
overfitting (Bowman et al., 2015), or generating
plain dialog responses (Zhao et al., 2017). Hence,
it is desirable to retain meaningful posteriors in
real applications. Two different schedules for �
have been commonly used for a text VAE.

Constant Schedule The standard approach is to
keep � = 1 fixed during the entire training proce-
dure, as it corresponds to optimizing the true VAE
objective. Unfortunately, instability on text anal-
ysis has been witnessed, in that the KL term L

R

becomes vanishingly small during training (Bow-
man et al., 2015). This issue causes two undesir-
able outcomes: (i) an encoder that produces poste-
riors almost identical to the Gaussian prior, for all
observations (rather than a more interesting pos-
terior); and (ii) a decoder that completely ignores
the latent variable z, and a learned model that re-
duces to a simpler language model. This is known
as the KL vanishing issue in text VAEs.



242

xzx �
�

(a) Traditional VAE

z

x<t

xtx

t=1,· · ·,T

Path A

Path B

��

�

(b) VAE with an auto-regressive decoder
Figure 1: Illustration of learning parameters {�,✓} in
the two different paradigms. Starting from the obser-
vation x in blue circle, a VAE infers its latent code z
in the green circle, and further generates its reconstruc-
tion in the red circle. (a) Standard VAE learning, with
only one path via {�,✓} from x to its reconstruction;
(b) VAE learning with an auto-regressive decoder. Two
paths are considered from x to its reconstruction: Path
A via {�,✓} and Path B via ✓.

Monotonic Annealing Schedule. A simple
remedy has been proposed in (Bowman et al.,
2015) to alleviate KL collapse. It sets � = 0 at
the beginning of training, and gradually increases
� until � = 1 is reached. In this setting, we do
not optimize the proper lower bound in (2) dur-
ing the early stages of training, but nonetheless
improvements on the value of that bound are ob-
served at convergence in previous work (Bowman
et al., 2015; Zhao et al., 2017).

The monotonic annealing schedule has become
the de facto standard in training text VAEs, and has
been widely adopted in many NLP tasks. Though
simple and often effective, this heuristic still lacks
a proper justification. Further, how to best sched-
ule � is largely unexplored.

3 Cyclical Annealing Schedule

3.1 Identifying Sources of KL Vanishing
In the traditional VAE (Kingma and Welling,
2013), z generates x directly, and the reconstruc-
tion depends only on one path of {�,✓} passing
through z, as shown in Figure 1(a). Hence, z
can largely determine the reconstructed x. In con-
trast, when an auto-regressive decoder is used in
a text VAE (Bowman et al., 2015), there are two
paths from x to its reconstruction, as shown in
Figure 1(b). Path A is the same as that in the
standard VAE, where z is the global representa-
tion that controls the generation of x; Path B leaks

the partial ground-truth information of x at every
time step of the sequential decoding. It generates
x
t

conditioned on x
<t

. Therefore, Path B can po-
tentially bypass Path A to generate x, leading to
KL vanishing.

From this perspective, we hypothesize that the
model-collapse problem is related to the low qual-
ity of z at the beginning phase of decoder training.
A lower quality z introduces more difficulties in
reconstructing x via Path A. As a result, the model
is forced to learn an easier solution to decoding:
generating x via Path B only.

We argue that this phenomenon can be easily
observed due to the powerful representation capa-
bility of the auto-regressive decoder. It has been
shown empirically that auto-regressive decoders
are able to capture highly-complex distributions,
such as natural language sentences (Mikolov et al.,
2010). This means that Path B alone has enough
capacity to model x, even though the decoder
takes {x

<t

, z} as input to produce x
t

. Zhang
et al. (2017a) has shown that flexible deep neural
networks can easily fit randomly labeled training
data, and here the decoder can learn to rely solely
on x

<t

for generation, when z is of low quality.
We use our hypothesis to explain the learning

behavior of different scheduling schemes for � as
follows.

Constant Schedule The two loss terms in (2)
are weighted equally in the constant schedule. At
the early stage of optimization, {�,✓} are ran-
domly initialized and the latent codes z are of low
quality. The KL term L

R

pushes q
�

(z|x) close
to an uninformative prior p(z): the posterior be-
comes more like an isotropic Gaussian noise, and
less representative of their corresponding observa-
tions. In other words, L

R

blocks Path A, and thus
z remains uninformative during the entire training
process: it starts with random initialization and
then is regularized towards a random noise. Al-
though the reconstruction term L

E

can be satisfied
via two paths, since z is noisy, the decoder learns
to discard Path A (i.e., ignores z), and chooses
Path B to generate the sentence word-by-word.

Monotonic Annealing Schedule The mono-
tonic schedule sets � close to 0 in the early stage of
training, which effectively removes the blockage
L
R

on Path A, and the model reduces to a denois-
ing autoencoder 1. L

E

becomes the only objective,
1The Gaussian sampling remains for q�(z|x)



243

which can be reached by both paths. Though ran-
domly initialized, z is learned to capture useful in-
formation for reconstruction of x during training.
At the time when the full VAE objective is con-
sidered (� = 1), z learned earlier can be viewed
as the VAE initialization; such latent variables are
much more informative than random, and thus are
ready for the decoder to use.

To mitigate the KL-vanishing issue, it is key to
have meaningful latent codes z at the beginning
of training the decoder, so that z can be utilized.
The monotonic schedule under-weights the prior
regularization, and the learned q

�

(z|x) tends to
collapse into a point estimate (i.e., the VAE re-
duces to an AE). This underestimate can result in
sub-optimal decoder learning. A natural question
concerns how one can get a better distribution es-
timate for z as initialization, while retaining low
computational cost.

3.2 Cyclical Annealing Schedule

Our proposal is to use z ⇠ q
�

(z|x), which has
been trained under the full VAE objective, as ini-
tialization. To learn to progressively improve la-
tent representation z, we propose a cyclic anneal-
ing schedule. We start with � = 0, increase � at
a fast pace, and then stay at � = 1 for subsequent
learning iterations. This encourages the model to
converge towards the VAE objective, and infers its
first raw full latent distribution.

Unfortunately, Path A is blocked at � = 1. The
optimization is then continued at � = 0 again,
which perturbs the VAE objective, dislodges it
from the convergence, and reopens Path A. Impor-
tantly, the decoder is now trained with the latent
code from a full distribution z ⇠ q

�

(z|x), and
both paths are considered. We repeat this process
several times to achieve better convergences.

Formally, � has the form:

�
t

=

⇢
f(⌧), ⌧  R

1, ⌧ > R
with (6)

⌧ =
mod(t � 1, dT/Me)

T/M
, (7)

where t is the iteration number, T is the total num-
ber of training iterations, f is a monotonically
increasing function, and we introduce two new
hyper-parameters associated with the cyclical an-
nealing schedule:

• M : number of cycles (default M = 4);

0

0.5

1

�

Monotonic

0 5K 10K 15K 20K 25K 30K 35K 40K
Iteration

0

0.5

1

�

Cyclical

Figure 2: Comparison between (a) traditional mono-
tonic and (b) proposed cyclical annealing schedules.In
this figure, M = 4 cycles are illustrated, R = 0.5 is
used for increasing within each cycle.

• R: proportion used to increase � within a cy-
cle (default R = 0.5).

In other words, we split the training process into
M cycles, each starting with � = 0 and ending
with � = 1. We provide an example of a cyclical
schedule in Figure 2(b), compared with the mono-
tonic schedule in Figure 2(a). Within one cycle,
there are two consecutive stages (divided by R):

• Annealing. � is annealed from 0 to 1 in the
first R dT/Me training steps over the course
of a cycle. For example, the steps [1, 5K]
in the Figure 2(b). � = f(0) = 0 forces
the model to learn representative z to recon-
struct x. As depicted in Figure 1(b), there is
no interruption from the prior on Path A, z is
forced to learn the global representation of x.
By gradually increasing � towards f(R) = 1,
q(z|x) is regularized to transit from a point
estimate to a distribution estimate, spreading
out to match the prior.

• Fixing. As our ultimate goal is to learn a
VAE model, we fix � = 1 for the rest of
training steps within one cycle, e.g., the steps
[5K, 10K] in Figure 2(b). This drives the
model to optimize the full VAE objective un-
til convergence.

As illustrated in Figure 2, the monotonic sched-
ule increasingly anneals � from 0 to 1 once, and
fixes � = 1 during the rest of training. The cycli-
cal schedules alternatively repeats the annealing
and fixing stages multiple times.

A Practical Recipe The existing schedules can
be viewed as special cases of the proposed cycli-
cal schedule. The cyclical schedule reduces to the



244

constant schedule when R = 0, and it reduces to
an monotonic schedule when M = 1 and R is
relatively small 2. In theory, any monotonically
increasing function f can be adopted for the cycli-
cal schedule, as long as f(0) = 0 and f(R) = 1.
In practice, we suggest to build the cyclical sched-
ule upon the success of monotonic schedules: we
adopt the same f , and modify it by setting M and
R (as default). Three widely used increasing func-
tions for f are linear (Fraccaro et al., 2016; Goyal
et al., 2017), Sigmoid (Bowman et al., 2015) and
Consine (Lai et al., 2018). We present the compar-
ative results using the linear function f(⌧) = ⌧/R
in Figure 2, and show the complete comparison for
other functions in Figure 7 of the Supplementary
Material (SM).

3.3 On the impact of �
This section derives a bound for the training
objective to rigorously study the impact of �;
the proof details are included in SM. For nota-
tional convenience, we identify each data sam-
ple with a unique integer index n ⇠ q(n), drawn
from a uniform random variable on {1, 2, · · · , N}.
Further we define q(z|n) = q

�

(z|x
n

) and
q(z, n) = q(z|n)q(n) = q(z|n) 1

N

. Follow-
ing (Makhzani et al., 2016), we refer to q(z) =P

N

n=1 q(z|n)q(n) as the aggregated posterior.
This marginal distribution captures the aggregated
z over the entire dataset. The KL term in (5) can
be decomposed into two refined terms (Chen et al.,
2018; Hoffman and Johnson, 2016):

F
R

= E
q(n)[KL(q(z|n)||p(z))]

= I
q

(z, n)
| {z }

F1: Mutual Info.

+ KL(q(z)||p(z))| {z }
F2: Marginal KL

(8)

where F1 is the mutual information (MI) mea-
sured by q. Higher MI can lead to a higher cor-
relation between the latent variable and data vari-
able, and encourages a reduction in the degree of
KL vanishing. The marginal KL is represented by
F2, and it measures the fitness of the aggregated
posterior to the prior distribution.

The reconstruction term in (5) provides a lower
bound for MI measured by q, based on Corollary
3 in (Li et al., 2017):

F
E

= E
q(n),z⇠q(z|n)(log p(n|z))] + Hq(n)

 I
q

(z, n) (9)

where H(n) is a constant.
2In practice, the monotonic schedule usually anneals in

a very fast pace, thus R is small compared with the entire
training procedure.

Analysis of � When scheduled with �, the train-
ing objective over the dataset can be written as:

F = �F
E

+ �F
R

(10)
� (� � 1)I

q

(z, n) + �KL(q(z)||p(z)) (11)

To reduce KL vanishing, we desire an increase
in the MI term I(z, n), which appears in both F

E

and F
R

, modulated by �. It shows that reduc-
ing KL vanishing is inversely proportional with �.
When � = 0, the model fully focuses on maximiz-
ing the MI. As � increases, the model gradually
transits towards fitting the aggregated latent codes
to the given prior. When � = 1, the implemen-
tation of MI becomes implicit in KL(q(z)||p(z)).
It is determined by the amortized inference reg-
ularization (implied by the encoder’s expressiv-
ity) (Shu et al., 2018), which further affects the
performance of the generative density estimator.

4 Visualization of Latent Space

We compare different schedule methods by visu-
alizing the learning processes on an illustrative
problem. Consider a dataset consisting of 10 se-
quences, each of which is a 10-dimensional one-
hot vector with the value 1 appearing in differ-
ent positions. A 2-dimensional latent space is
used for the convenience of visualization. Both
the encoder and decoder are implemented using
a 2-layer LSTM with 64 hidden units each. We
use T = 40K total iterations, and the scheduling
schemes in Figure 2.

The learning curves for the ELBO, reconstruc-
tion error, and KL term are shown in Figure 3. The
three schedules share very similar values. How-
ever, the cyclical schedule provides substantially
lower reconstruction error and higher KL diver-
gence. Interestingly, the cyclical schedule im-
proves the performance progressively: it becomes
better than the previous cycle, and there are clear
periodic patterns across different cycles. This sug-
gests that the cyclical schedule allows the model
to use the previously learned results as a warm-
restart to achieve further improvement.

We visualize the resulting division of the la-
tent space for different training steps in Figure 4,
where each color corresponds to z ⇠ q(z|n), for
n = 1, · · · , 10. We observe that the constant
schedule produces heavily mixed latent codes z
for different sequences throughout the entire train-
ing process. The monotonic schedule starts with a
mixed z, but soon divides the space into a mixture



245

(a) ELBO (b) Reconstruction Error (c) KL term
Figure 3: Comparison of the learning curves for the three schedules on an illustrative problem.

Figure 4: Visualization of the latent space along the learning dynamics on an illustrative problem.

of 10 cluttered Gaussians in the annealing process
(the division remains cluttered in the rest of train-
ing). The cyclical schedule behaves similarly to
the monotonic schedule in the first 10K steps (the
first cycle). But, starting from the 2nd cycle, much
more divided clusters are shown when learning on
top of the 1st cycle results. However, � < 1 leads
to some holes between different clusters, making
q(z) violate the constraint of p(z). This is allevi-
ated at the end of the 2nd cycle, as the model is
trained with � = 1. As the process repeats, we
see clearer patterns in the 4th cycle than the 2nd
cycle for both � < 0 and � = 1. It shows that
more structured information is captured in z using
the cyclical schedule, which is beneficial in down-
stream applications as shown in the experiments.

5 Related Work

Solutions to KL vanishing Several techniques
have been proposed to mitigate the KL vanish-
ing issue. The proposed method is most closely
related to the monotonic KL annealing technique
in (Bowman et al., 2015). In addition to intro-
ducing a specific algorithm, we have comprehen-
sively studied the impact of � and its scheduling
schemes. Our explanations can be used to inter-
pret other techniques, which can be broadly cate-
gorized into two classes.

The first category attempts to weaken Path B,
and force the decoder to use Path A. Word drop
decoding (Bowman et al., 2015) sets a certain per-
centage of the target words to zero. It has shown
that it may degrade the performance when the drop
rate is too high. The dilated CNN was considered
in (Yang et al., 2017) as a new type of decoder
to replace the LSTM. By changing the decoder’s
dilation architecture, one can control Path B: the
effective context from x

<t

.
The second category of techniques improves the

dependency in Path A, so that the decoder uses la-
tent codes more easily. Skip connections were de-
veloped in (Dieng et al., 2018) to shorten the paths
from z to x in the decoder. Zhao et al. (2017)
introduced an auxiliary loss that requires the de-
coder to predict the bag-of-words in the dialog re-
sponse (Zhao et al., 2017). The decoder is thus
forced to capture global information about the tar-
get response. Zhao et al. (2019) enhanced Path
A via mutual information. Concurrent with our
work, He et al. (2019) proposed to update encoder
multiple times to achieve better latent code before
updating decoder. Semi-amortized training (Kim
et al., 2018) was proposed to perform stochastic
variational inference (SVI) (Hoffman et al., 2013)
on top of the amortized inference in VAE. It shares
a similar motivation with the proposed approach,



246

in that better latent codes can reduce KL vanish-
ing. However, the computational cost to run SVI
is high, while our monotonic schedule does not re-
quire any additional compute overhead. The KL
scheduling methods are complementary to these
techniques. As shown in experiments, the pro-
posed cyclical schedule can further improve them.

�-VAE The VAE has been extended to �-
regularized versions in a growing body of
work (Higgins et al., 2017; Alemi et al., 2018).
Perhaps the seminal work is �-VAE (Higgins
et al., 2017), which was extended in (Kim and
Mnih, 2018; Chen et al., 2018) to consider � on
the refined terms in the KL decomposition. Their
primary goal is to learn disentangled latent rep-
resentations to explain the data, by setting � >
1. From an information-theoretic point of view,
(Alemi et al., 2018) suggests a simple method to
set � < 1 to ensure that latent-variable models
with powerful stochastic decoders do not ignore
their latent code. However, � 6= 1 results in an
improper statistical model. Further, � is static in
their work; we consider dynamically scheduled �
and find it more effective.

Cyclical schedules Warm-restart techniques are
common in optimization to deal with multimodal
functions. The cyclical schedule has been used to
train deep neural networks (Smith, 2017), warm
restart stochastic gradient descent (Loshchilov and
Hutter, 2017), improve convergence rates (Smith
and Topin, 2017), obtain model ensembles (Huang
et al., 2017) and explore multimodal distributions
in MCMC sampling (Zhang et al., 2019). All these
works applied cyclical schedules to the learning
rate. In contrast, this paper represents the first
to consider the cyclical schedule for � in VAE.
Though the techniques seem simple and similar,
our motivation is different: we use the cyclical
schedule to re-open Path A in Figure 1(b) and
provide the opportunity to train the decoder with
high-quality z.

6 Experiments

The source code to reproduce the experimental re-
sults will be made publicly available on GitHub3.
For a fair comparison, we follow the practical
recipe described in Section 3.2, where the mono-
tonic schedule is treated as a special case of cycli-

3
https://github.com/haofuml/cyclical_

annealing

Schedule Rec KL ELBO PPL

VAE M 101.73 0.907 -102.63 108.09C 100.51 1.955 -102.46 107.25

SA-VAE
M⇤ 100.75 1.796 -102.54 107.64
M 101.83 1.053 -102.89 109.33
C 100.50 2.261 -102.76 108.71

�
m

=0.5
M 97.36 9.605 -106.96 132.57
C 95.22 9.484 -104.70 118.78

Table 1: Comparison of language modeling on Penn
Tree Bank (PTB). The last iteration results are reported.
Since SA-VAE tends to overfit, we report its best re-
sults in row M⇤.

cal schedule (while keeping all other settings the
same). The default hyper-parameters of the cycli-
cal schedule are used in all cases unless stated oth-
erwise. We study the impact of hyper-parameters
in the SM, and show that larger M can provide
higher performance for various R. We show the
major results in this section, and put more details
in the SM. The monotonic and cyclical schedules
are denoted as M and C, respectively.

6.1 Language Modeling
We first consider language modeling on the Penn
Tree Bank (PTB) dataset (Marcus et al., 1993).
Language modeling with VAEs has been a chal-
lenging problem, and few approaches have been
shown to produce rich generative models that do
not collapse to standard language models. Ide-
ally a deep generative model trained with varia-
tional inference would pursue higher ELBO, mak-
ing use of the latent space (i.e., maintain a nonzero
KL term) while accurately modeling the underly-
ing distribution (i.e., lower reconstruction errors).
We implemented different schedules based on the
code4 published by Kim et al. (2018).

The latent variable is 32-dimensional, and 40
epochs are used. We compare the proposed cycli-
cal annealing schedule with the monotonic sched-
ule baseline that, following (Bowman et al., 2015),
anneals linearly from 0 to 1.0 over 10 epochs.
We also compare with semi-amortized (SA) train-
ing (Kim et al., 2018), which is considered as the
state-of-the-art technique in preventing KL van-
ishing. We set SVI steps to 10.

Results are shown in Table 1. The perplexity
is reported in column PPL. The cyclical schedule
outperforms the monotonic schedule for both stan-
dard VAE and SA-VAE training. SA-VAE training

4
https://github.com/harvardnlp/sa-vae



247

(a) ELBO (b) Reconstruction error (c) KL

Figure 5: Learning curves of VAE and SA-VAE on PTB. Under similar ELBO, the cyclical schedule provides
lower reconstruction errors and higher KL values than the monotonic schedule.

Context Alice: yeah you know its interesting especially when my experience has always been at a public university.
Topic: Choose a College Target Bob (statement): yeah that’s right

C

1. yes

M

1. i’m not sure
2. oh really 2. and i’m not sure
3. and there’s a lot of <unk> there’s a lot of people 3. and i’m not sure
4. yeah 4. i’m not sure
5. and i think that’s probably the biggest problem i’ve ever seen in the past 5. i’m not sure

Table 2: Generated dialog responses from the cyclical and monotonic schedules.

can effectively reduce KL vanishing, it takes 472s
per epoch. However, this is significantly more
expensive than the standard VAE training which
takes 30s per epoch. The proposed cyclical sched-
ule adds almost zero cost.

We show the learning curves for VAE and SA-
VAE in Figure 5. Interestingly, the cyclical sched-
ule exhibits periodical learning behaviours. The
performance of the cyclical schedule gets better
progressively, after each cycle. While ELBO and
PPL ar similar, the cyclical schedule improves the
reconstruction ability and KL values for both VAE
and SA-VAE. We observe clear over-fitting is-
sues for the SA-VAE with the monotonic schedule,
while this issue is less severe for SA-VAE with the
cyclical schedule.

Finally, we further investigate whether our im-
provements are from simply having a lower �,
rather than from the cyclical schedule re-opening
Path A for better learning. To test this, we use a
monotonic schedule with maximum � = 0.5. We
observe that the reconstruction and KL terms per-
form better individually, but the ELBO is substan-
tially worse than � = 1, because � = 0.5 yields
an improper model. Even so, the cyclical schedule
improves its performance.

6.2 Conditional VAE for Dialog
We use a cyclical schedule to improve the la-
tent codes in (Zhao et al., 2017), which are key
to diverse dialog-response generation. Follow-

Model CVAE CVAE+BoW
Schedule M C M C
Rec-P # 36.16 29.77 18.44 16.74
KL Loss " 0.265 4.104 14.06 15.55
B4 prec 0.185 0.234 0.211 0.219
B4 recall 0.122 0.220 0.210 0.219
A-bow prec 0.957 0.961 0.958 0.961
A-bow recall 0.911 0.941 0.938 0.940
E-bow prec 0.867 0.833 0.830 0.828
E-bow recall 0.784 0.808 0.808 0.805

Table 3: Comparison on dialog response genera-
tion. Reconstruction perplexity (Rec-P) and BLEU (B)
scores are used for evaluation.

ing (Zhao et al., 2017), Switchboard (SW) Cor-
pus (Godfrey and Holliman, 1997) is used, which
has 2400 two-sided telephone conversations.

Two latent variable models are considered. The
first one is the Conditional VAE (CVAE), which
has been shown better than the encoder-decoder
neural dialog (Serban et al., 2016). The second is
to augment VAE with a bag-of-word (BoW) loss
to tackle the KL vanishing problem, as proposed
in (Zhao et al., 2017).

Table 2 shows the sample outputs generated
from the two schedules using CVAE. Caller Alice
begins with an open-ended statement on choos-
ing a college, and the model learns to generate re-
sponses from Caller Bob. The cyclical schedule
generated highly diverse answers that cover multi-



248

ple plausible dialog acts. On the contrary, the re-
sponses from the monotonic schedule are limited
to repeat plain responses, i.e., “i’m not sure”.

Quantitative results are shown in Table 3, using
the evaluation metrics from (Zhao et al., 2017).
(i) Smoothed Sentence-level BLEU (Chen and
Cherry, 2014): BLEU is a popular metric that
measures the geometric mean of modified n-gram
precision with a length penalty. We use BLEU-1
to 4 as our lexical similarity metric and normalize
the score to 0 to 1 scale. (ii) Cosine Distance of
Bag-of-word Embedding (Liu et al., 2016): a sim-
ple method to obtain sentence embeddings is to
take the average or extreme of all the word embed-
dings in the sentences. We used Glove embedding
and denote the average method as A�bow and ex-
treme method as E�bow. The score is normalized
to [0, 1]. Higher values indicate more plausible re-
sponses.

The BoW indeed reduces the KL vanishing is-
sue, as indicated by the increased KL and de-
creased reconstruction perplexity. When applying
the proposed cyclical schedule to CVAE, we also
see a reduced KL vanishing issue. Interestingly,
it also yields the highest BLEU scores. This sug-
gests that the cyclical schedule can generate dialog
responses of higher fidelity with lower cost, as the
auxiliary BoW loss is not necessary. Further, BoW
can be improved when integrated with the cyclical
schedule, as shown in the last column of Table 3.

6.3 Unsupervised Language Pre-training
We consider the Yelp dataset, as pre-processed
in (Shen et al., 2017) for unsupervised language
pre-training. Text features are extracted as the
latent codes z of VAE models, pre-trained with
monotonic and cyclical schedules. The AE is
used as the baseline. A good VAE can learn
to cluster data into meaningful groups (Kingma
and Welling, 2013), indicating that well-structured
z are highly informative features, which usually
leads to higher classification performance. To
clearly compare the quality of z, we build a simple
one-layer classifier on z, and fine-tune the model
on different proportions of labelled data (Zhang
et al., 2017b).

The results are shown in Figure 6. The cyclical
schedule consistently yields the highest accuracy
relative to other methods. We visualize the tSNE
embeddings (Maaten and Hinton, 2008) of z in
Figure 9 of the SM, and observe that the cyclical
schedule exhibits clearer clustered patterns.

Figure 6: Accuracy of fine-tuning on the unsupervised pre-
trained models on the Yelp dataset.

Schedule Rec KL ELBO
Cyc � + Const ⌘ 101.30 1.457 -102.76
Mon � + Const ⌘ 101.93 0.858 -102.78
Cyc � + Cyc ⌘ 100.61 1.897 -102.51
Mon � + Cyc ⌘ 101.74 0.748 -102.49

Table 4: Comparison of cyclical schedules on � and ⌘,
tested with language modeling on PTB.

6.4 Ablation Study
To enhance the performance, we propose to apply
the cyclical schedule to the learning rate ⌘ on real
tasks. It ensures that the optimizer has the same
length of optimization trajectory for each � cycle
(so that each cycle can fully converge). To inves-
tigate the impact of cyclical on ⌘, we perform two
more ablation experiments: (i) We make only �
cyclical, keep ⌘ constant. (ii) We make only ⌘
cyclical, keep � monotonic. The last epoch num-
bers are shown in Table 4, and the learning curves
on shown in Figure 10 in SM. Compared with the
baseline, we see that it is the cyclical � rather than
cyclical ⌘ that contributes to the improved perfor-
mance.

7 Conclusions

We provide a novel two-path interpretation to ex-
plain the KL vanishing issue, and identify its
source as a lack of good latent codes at the be-
ginning of decoder training. This provides an un-
derstanding of various � scheduling schemes, and
motivates the proposed cyclical schedule. By re-
opening the path at � = 0, the cyclical sched-
ule can progressively improve the performance, by
leveraging good latent codes learned in the previ-
ous cycles as warm re-starts. We demonstrate the
effectiveness of the proposed approach on three
NLP tasks, and show that it is superior to or com-
plementary to other techniques.



249

References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dil-

lon, Rif A Saurous, and Kevin Murphy. 2018. Fix-
ing a broken ELBO. In ICML.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349.

Boxing Chen and Colin Cherry. 2014. A systematic
comparison of smoothing techniques for sentence-
level bleu. In Proceedings of the Ninth Workshop on
Statistical Machine Translation, pages 362–367.

Ricky TQ Chen, Xuechen Li, Roger Grosse, and David
Duvenaud. 2018. Isolating sources of disentangle-
ment in VAEs. NIPS.

Adji B Dieng, Yoon Kim, Alexander M Rush, and
David M Blei. 2018. Avoiding latent variable col-
lapse with generative skip models. arXiv preprint
arXiv:1807.04863.

Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet,
and Ole Winther. 2016. Sequential neural models
with stochastic layers. In NIPS.

J Godfrey and E Holliman. 1997. Switchboard-1 re-
lease 2: Linguistic data consortium. SWITCH-
BOARD: A User’s Manual.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep learning, volume 1. MIT press Cam-
bridge.

Anirudh Goyal Alias Parth Goyal, Alessandro Sor-
doni, Marc-Alexandre Côté, Nan Rosemary Ke, and
Yoshua Bengio. 2017. Z-forcing: Training stochas-
tic recurrent networks. In NIPS.

Junxian He, Daniel Spokoyny, Graham Neubig, and
Taylor Berg-Kirkpatrick. 2019. Lagging inference
networks and posterior collapse in variational au-
toencoders. ICLR.

Irina Higgins, Loic Matthey, Arka Pal, Christopher
Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. 2017. beta-vae:
Learning basic visual concepts with a constrained
variational framework. ICLR.

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long
short-term memory. Neural computation.

Matthew D Hoffman, David M Blei, Chong Wang, and
John Paisley. 2013. Stochastic variational inference.
The Journal of Machine Learning Research.

Matthew D Hoffman and Matthew J Johnson. 2016.
Elbo surgery: yet another way to carve up the vari-
ational evidence lower bound. In Workshop in Ad-
vances in Approximate Bayesian Inference, NIPS.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Toward con-
trolled generation of text. ICML.

Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu,
John E Hopcroft, and Kilian Q Weinberger. 2017.
Snapshot ensembles: Train 1, get m for free. ICLR.

Hyunjik Kim and Andriy Mnih. 2018. Disentangling
by factorising. ICML.

Yoon Kim, Sam Wiseman, Andrew C Miller, David
Sontag, and Alexander M Rush. 2018. Semi-
amortized variational autoencoders. ICML.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. ICLR.

Guokun Lai, Bohan Li, Guoqing Zheng, and Yiming
Yang. 2018. Stochastic wavenet: A generative latent
variable model for sequential data. ICML workshop.

Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu,
Liqun Chen, Ricardo Henao, and Lawrence Carin.
2017. ALICE: Towards understanding adversarial
learning for joint distribution matching. In NIPS.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

Ilya Loshchilov and Frank Hutter. 2017. Sgdr:
Stochastic gradient descent with warm restarts.
ICLR.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579–2605.

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly,
Ian Goodfellow, and Brendan Frey. 2016. Adver-
sarial autoencoders. ICLR workshop.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics.

Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. EMNLP.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural
variational inference for text processing. In ICML.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
Eleventh Annual Conference of the International
Speech Communication Association.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and
approximate inference in deep generative models.
ICML.



250

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In AAAI.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In NIPS.

Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochen-
derfer, and Stefano Ermon. 2018. Amortized infer-
ence regularization. NIPS.

Leslie N Smith. 2017. Cyclical learning rates for train-
ing neural networks. In WACV. IEEE.

Leslie N Smith and Nicholay Topin. 2017. Super-
convergence: Very fast training of residual net-
works using large learning rates. arXiv preprint
arXiv:1708.07120.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017. Latent intention dialogue mod-
els. ICML.

Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan.
2017. Variational autoencoder for semi-supervised
text classification. In AAAI.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. ICML.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-
jamin Recht, and Oriol Vinyals. 2017a. Understand-
ing deep learning requires rethinking generalization.
ICLR.

Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou
Chen, and Andrew Gordon Wilson. 2019. Cyclical
stochastic gradient mcmc for bayesian deep learn-
ing. arXiv preprint arXiv:1902.03932.

Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan,
Ricardo Henao, and Lawrence Carin. 2017b. De-
convolutional paragraph representation learning. In
NIPS.

Shengjia Zhao, Jiaming Song, and Stefano Ermon.
2019. InfoVAE: Information maximizing varia-
tional autoencoders. AAAI.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. ACL.


