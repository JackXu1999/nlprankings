















































Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance


Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 8–17
Minneapolis, USA, June 6, 2019. c©2019 Association for Computational Linguistics

8

Characterizing the Impact of Geometric Properties
of Word Embeddings on Task Performance

Brendan Whitaker1∗, Denis Newman-Griffis1∗, Aparajita Haldar2∗,
Hakan Ferhatosmanoglu2, Eric Fosler-Lussier1

1The Ohio State University, Columbus, OH, USA
2University of Warwick, Coventry, UK

{whitaker.213, newman-griffis.1, fosler-lussier.1}@osu.edu
{aparajita.haldar, h.ferhatosmanoglu}@warwick.ac.uk

Abstract

Analysis of word embedding properties to
inform their use in downstream NLP tasks
has largely been studied by assessing near-
est neighbors. However, geometric prop-
erties of the continuous feature space con-
tribute directly to the use of embedding fea-
tures in downstream models, and are largely
unexplored. We consider four properties of
word embedding geometry, namely: posi-
tion relative to the origin, distribution of fea-
tures in the vector space, global pairwise dis-
tances, and local pairwise distances. We de-
fine a sequence of transformations to gener-
ate new embeddings that expose subsets of
these properties to downstream models and
evaluate change in task performance to under-
stand the contribution of each property to NLP
models. We transform publicly available pre-
trained embeddings from three popular toolk-
its (word2vec, GloVe, and FastText) and evalu-
ate on a variety of intrinsic tasks, which model
linguistic information in the vector space, and
extrinsic tasks, which use vectors as input to
machine learning models. We find that intrin-
sic evaluations are highly sensitive to absolute
position, while extrinsic tasks rely primarily
on local similarity. Our findings suggest that
future embedding models and post-processing
techniques should focus primarily on similar-
ity to nearby points in vector space.

1 Introduction

Learned vector representations of words, known
as word embeddings, have become ubiquitous
throughout natural language processing (NLP) ap-
plications. As a result, analysis of embedding
spaces to understand their utility as input fea-
tures has emerged as an important avenue of in-
quiry, in order to facilitate proper use of embed-
dings in downstream NLP tasks. Many analyses
have focused on nearest neighborhoods, as a vi-
able proxy for semantic information (Rogers et al.,

∗These authors contributed equally to this work.

2018; Pierrejean and Tanguy, 2018). However,
neighborhood-based analysis is limited by the un-
reliability of nearest neighborhoods (Wendlandt
et al., 2018). Further, it is intended to evaluate the
semantic content of embedding spaces, as opposed
to characteristics of the feature space itself.

Geometric analysis offers another recent an-
gle from which to understand the properties of
word embeddings, both in terms of their distri-
bution (Mimno and Thompson, 2017) and corre-
lation with downstream performance (Chandrahas
et al., 2018). Through such geometric investiga-
tions, neighborhood-based semantic characteriza-
tions are augmented with information about the
continuous feature space of an embedding. Ge-
ometric features offer a more direct connection
to the assumptions made by neural models about
continuity in input spaces (Szegedy et al., 2014),
as well as the use of recent contextualized rep-
resentation methods using continuous language
models (Peters et al., 2018; Devlin et al., 2018).

In this work, we aim to bridge the gap between
neighborhood-based semantic analysis and geo-
metric performance analysis. We consider four
components of the geometry of word embeddings,
and transform pretrained embeddings to expose
only subsets of these components to downstream
models. We transform three popular sets of em-
beddings, trained using word2vec (Mikolov et al.,
2013),1 GloVe (Pennington et al., 2014),2 and
FastText (Bojanowski et al., 2017),3 and use the
resulting embeddings in a battery of standard eval-
uations to measure changes in task performance.

We find that intrinsic evaluations, which model
linguistic information directly in the vector space,

13M 300-d GoogleNews vectors from https://code.
google.com/archive/p/word2vec/

22M 300-d 840B Common Crawl vectors from https:
//nlp.stanford.edu/projects/glove/

31M 300-d WikiNews vectors with subword infor-
mation from https://fasttext.cc/docs/en/
english-vectors



9

Original Affine CDE NNE

Translation

Reflection

Rotation

Dilation
Thresholded

Weighted

Unweighted

Random

Homothety

Figure 1: Sequence of transformations applied to word embeddings, including transformation variants. Note that
each transformation is applied independently to source word embeddings. Transformations are presented in order
of decreasing geometric information retained about the original vectors.

are highly sensitive to absolute position in pre-
trained embeddings; while extrinsic tasks, in
which word embeddings are passed as input fea-
tures to a trained model, are more robust and rely
primarily on information about local similarity be-
tween word vectors. Our findings, including evi-
dence that global organization of word vectors is
often a major source of noise, suggest that fur-
ther development of embedding learning and tun-
ing methods should focus explicitly on local sim-
ilarity, and help to explain the success of several
recent methods.

2 Related Work

Word embedding models and outputs have been
analyzed from several angles. In terms of per-
formance, evaluating the “quality” of word em-
bedding models has long been a thorny problem.
While intrinsic evaluations such as word similar-
ity and analogy completion are intuitive and easy
to compute, they are limited by both confounding
geometric factors (Linzen, 2016) and task-specific
factors (Faruqui et al., 2016; Rogers et al., 2017).
Chiu et al. (2016) show that these tasks, while cor-
related with some semantic content, do not always
predict downstream performance. Thus, it is nec-
essary to use a more comprehensive set of intrinsic
and extrinsic evaluations for embeddings.

Nearest neighbors in sets of embeddings are
commonly used as a proxy for qualitative seman-
tic information. However, their instability across
embedding samples (Wendlandt et al., 2018) is a
limiting factor, and they do not necessarily corre-
late with linguistic analyses (Hellrich and Hahn,
2016). Modeling neighborhoods as a graph struc-
ture offers an alternative analysis method (Cuba
Gyllensten and Sahlgren, 2015), as does 2-D or
3-D visualization (Heimerl and Gleicher, 2018).
However, both of these methods provide qualita-
tive insights only. By systematically analyzing ge-
ometric information with a wide variety of eval-

uations, we provide a quantitative counterpart to
these understandings of embedding spaces.

3 Methods

In order to investigate how different geomet-
ric properties of word embeddings contribute to
model performance on intrinsic and extrinsic eval-
uations, we consider the following attributes of
word embedding geometry:
• position relative to the origin;
• distribution of feature values in Rd;
• global pairwise distances, i.e. distances be-

tween any pair of vectors;
• local pairwise distances, i.e. distances be-

tween nearby pairs of vectors.
Using each of our sets of pretrained word em-

beddings, we apply a variety of transformations to
induce new embeddings that only expose subsets
of these attributes to downstream models. These
are: affine transformation, which obfuscates the
original position of the origin; cosine distance en-
coding, which obfuscates the original distribution
of feature values in Rd; nearest neighbor encod-
ing, which obfuscates global pairwise distances;
and random encoding. This sequence is illustrated
in Figure 1, and the individual transformations are
discussed in the following subsections.

General notation for defining our transforma-
tions is as follows. Let W be our vocabulary of
words taken from some source corpus. We asso-
ciate with each word w ∈ W a vector v ∈ Rd
resulting from training via one of our embedding
generation algorithms, where d is an arbitrary di-
mensionality for the embedding space. We define
V to be the set of all pretrained word vectors v for
a given corpus, embedding algorithm, and param-
eters. The matrix of embeddings MV associated
with this set then has shape |V | × d. For simplic-
ity, we restrict our analysis to transformed embed-
dings of the same dimensionality d as the original
vectors.



10

3.1 Affine transformations
Affine transformations have been previously uti-
lized for post-processing of word embeddings. For
example, Artetxe et al. (2016) learn a matrix trans-
form to align multilingual embedding spaces, and
Faruqui et al. (2015) use a linear sparsification to
better capture lexical semantics. In addition, the
simplicity of affine functions in machine learn-
ing contexts (Hofmann et al., 2008) makes them
a good starting point for our analysis.

Given a set of embeddings in Rd, referred to as
an embedding space, affine transformations

faffine : Rd → Rd

change positions of points relative to the origin.
While prior work has typically focused on linear

transformations, which fix the origin, we consider
the broader class of affine transformations, which
do not. Thus, affine transformations such as trans-
lation cannot in general be represented as a square
matrix for finite-dimensional spaces.

We use the following affine transformations:

• translations;
• reflections over a hyperplane;
• rotations about a subspace;
• homotheties.

We give brief definitions of each transformation.

Definition 1. A translation is a function Tx :
Rd → Rd given by

Tx(v) = v + x (3.1)

where x ∈ Rd.
Definition 2. For every a ∈ Rd, we call the map
Refla : Rd → Rd given by

Refla(v) = v − 2
v · a
a · a

a (3.2)

the reflection over the hyperplane through the ori-
gin orthogonal to a.

Definition 3. A rotation through the span of vec-
tors u,x by angle θ is a map Rotu,x : Rd → Rd
given by

Rotu,x(v) = Av (3.3)

where

A = I + sin θ(xuT − uxT )
+ (cos θ − 1)(uuT + xxT )

(3.4)

and I ∈ Matd,d(R) is the identity matrix.

Definition 4. For every a ∈ Rd and λ ∈ R \ { 0 },
we call the map Ha,λ : Rd → Rd given by

Ha,λ(v) = a + λ(v − a) (3.5)

a homothety of center a and ratio λ. A homothety
centered at the origin is called a dilation.

Parameters used in our analysis for each of these
transformations are provided in Appendix A.

3.2 Cosine distance encoding (CDE)
Our cosine distance encoding transformation

fCDE : Rd → R|V |

obfuscates the distribution of features in Rd by
representing a set of word vectors as a pairwise
distance matrix. Such a transformation might be
used to avoid the non-interpretability of embed-
ding features (Fyshe et al., 2015) and compare em-
beddings based on relative organization alone.

Definition 5. Let a,b ∈ Rd. Then their cosine
distance dcos : Rd × Rd → [0, 2] is given by

dcos(a,b) = 1−
a · b
||a||||b||

(3.6)

where the second term is the cosine similarity.
As all three sets of embeddings evaluated in this

study have vocabulary size on the order of 106, use
of the full distance matrix is impractical. We use
a subset consisting of the distance from each point
to the embeddings of the 10K most frequent words
from each embedding set, yielding

fCDE : Rd → R10
4

This is not dissimilar to the global frequency-
based negative sampling approach of word2vec
(Mikolov et al., 2013). We then use an autoen-
coder to map this back to Rd for comparability.
Definition 6. Let v ∈ R|V |,W1,W2 ∈ R|V |×d.
Then an autoencoder over R|V | is defined as

h = ϕ(vW1) (3.7)

v̂ = ϕ(W2
Th) (3.8)

Vector h ∈ Rd is then used as the compressed rep-
resentation of v.

In our experiments, we use ReLU as our activa-
tion function ϕ, and train the autoencoder for 50
epochs to minimize L2 distance between v and v̂.



11

We recognize that low-rank compression us-
ing an autoencoder is likely to be noisy, thus po-
tentially inducing additional loss in evaluations.
However, precedent for capturing geometric struc-
ture with autoencoders (Li et al., 2017b) suggests
that this is a viable model for our analysis.

3.3 Nearest neighbor encoding (NNE)
Our nearest neighbor encoding transformation

fNNE : Rd → R|V |

discards the majority of the global pairwise dis-
tance information modeled in CDE, and retains
only information about nearest neighborhoods.
The output of fNNE(v) is a sparse vector.

This transformation relates to the common use
of nearest neighborhoods as a proxy for seman-
tic information (Wendlandt et al., 2018; Pierre-
jean and Tanguy, 2018). We take the previously
proposed approach of combining the output of
fNNE(v) for each v ∈ V to form a sparse adja-
cency matrix, which describes a directed nearest
neighbor graph (Cuba Gyllensten and Sahlgren,
2015; Newman-Griffis and Fosler-Lussier, 2017),
using three versions of fNNE defined below.

Thresholded The set of non-zero indices in
fNNE(v) correspond to word vectors ṽ such that
the cosine similarity of v and ṽ is greater than or
equal to an arbitrary threshold t. In order to en-
sure that every word has non-zero out degree in the
graph, we also include the k nearest neighbors by
cosine similarity for every word vector. Non-zero
values in fNNE(v) are set to the cosine similarity
of v and the relevant neighbor vector.

Weighted The set of non-zero indices in
fNNE(v) corresponds to only the set of k nearest
neighbors to v by cosine similarity. Cosine simi-
larity values are used for edge weights.

Unweighted As in the previous case, only k
nearest neighbors are included in the adjacency
matrix. All edges are weighted equally, regardless
of cosine similarity.

We report results using k = 5 and t = 0.05;
other settings are discussed in Appendix B.

Finally, much like the CDE method, we use a
second mapping function

ψ : R|V | → Rd

to transform the nearest neighbor graph back to
d-dimensional vectors for evaluation. Following
Newman-Griffis and Fosler-Lussier (2017), we

use node2vec (Grover and Leskovec, 2016) with
default parameters to learn this mapping. Like the
autoencoder, this is a noisy map, but the intent of
node2vec to capture patterns in local graph struc-
ture makes it a good fit for our analysis.

3.4 Random encoding
Finally, as a baseline, we use a random encoding

fRand : Rd → Rd

that discards original vectors entirely.
While intrinsic evaluations rely only on input

embeddings, and thus lose all source informa-
tion in this case, extrinsic tasks learn a model to
transform input features, making even randomly-
initialized vectors a common baseline (Lample
et al., 2016; Kim, 2014). For fair comparison, we
generate one set of random baselines for each em-
bedding set and re-use these across all tasks.

3.5 Other transformations
Many other transformations of a word embedding
space could be included in our analysis, such as
arbitrary vector-valued polynomial functions, ra-
tional vector-valued functions, or common decom-
position methods such as principal components
analysis (PCA) or singular value decomposition
(SVD). Additionally, though they cannot be effec-
tively applied to the unordered set of word vectors
in a raw embedding space, transformations for se-
quential data such as discrete Fourier transforms
or discrete wavelet transforms could be used for
word sequences in specific text corpora.

For this study, we limit our scope to the transfor-
mations listed above. These transformations align
with prior work on analyzing and post-processing
embeddings for specific tasks, and are highly in-
terpretable with respect to the original embedding
space. However, other complex transformations
represent an intriguing area of future work.

4 Evaluation

In order to measure the contributions of each geo-
metric aspect described in Section 3 to the utility
of word embeddings as input features, we evalu-
ate embeddings transformed using our sequence
of operations on a battery of standard intrinsic
evaluations, which model linguistic information
directly in the vector space; and extrinsic eval-
uations, which use the embeddings as input to
learned models for downstream applications Our
intrinsic evaluations include:



12

(a) Results of intrinsic evaluations

(b) Results of extrinsic evaluations

Figure 2: Performance metrics on intrinsic and extrinsic tasks, comparing across different transformations applied
to each set of word embeddings. Dotted lines are for visual aid in tracking performance on individual tasks, and
do not indicate continuous transformations. Transformations are presented in order of decreasing geometric infor-
mation about the original vectors, and are applied independent of one another to the original source embedding.

• Word similarity and relatedness, using co-
sine similarity: WordSim-353 (Finkelstein
et al., 2001), SimLex-999 (Hill et al., 2015),
RareWords (Luong et al., 2013), RG65
(Rubenstein and Goodenough, 1965), MEN
(Bruni et al., 2014), and MTURK (Radinsky
et al., 2011).4

• Word categorization, using an oracle combi-
nation of agglomerative and k-means clus-
tering: AP (Almuhareb and Poesio, 2005),
BLESS (Baroni and Lenci, 2011), Battig
(Battig and Montague, 1969), and the ESS-
LLI 2008 shared task (Baroni et al. (2008),
performance averaged across nouns, verbs,

4https://github.com/kudkudak/
word-embeddings-benchmarks using single-word
datasets only. For brevity, we omit the Sim/Rel splits of
WordSim-353 (Agirre et al., 2009), which showed the same
trends as the full dataset.

and concrete nouns).5

Given the well-documented issues with using
vector arithmetic-based analogy completion as an
intrinsic evaluation (Linzen, 2016; Rogers et al.,
2017; Newman-Griffis et al., 2017), we do not in-
clude it in our analysis.

We follow Rogers et al. (2018) in evaluating on
a set of five extrinsic tasks:5

• Relation classification: SemEval-2010 Task
8 (Hendrickx et al., 2010), using a CNN with
word and distance embeddings (Zeng et al.,
2014).
• Sentence-level sentiment polarity classifica-

tion: MR movie reviews (Pang and Lee,
2005), with a simplified CNN model from
(Kim, 2014).

5https://github.com/drgriffis/
Extrinsic-Evaluation-tasks



13

• Sentiment classification: IMDB movie re-
views (Maas et al., 2011), with a single 100-d
LSTM.
• Subjectivity/objectivity classification: Rotten

Tomato snippets (Pang and Lee, 2004), using
a logistic regression over summed word em-
beddings (Li et al., 2017a).
• Natural language inference: SNLI (Bow-

man et al., 2015), using separate LSTMs for
premise and hypothesis, combined with a
feed-forward classifier.

5 Analysis and Discussion

Figure 2 presents the results of each intrinsic and
extrinsic evaluation on the transformed versions of
our three sets of word embeddings.6 The largest
drops in performance across all three sets for in-
trinsic tasks occur when explicit embedding fea-
tures are removed with the CDE transformation.
While some cases of NNE-transformed embed-
dings recover a measure of this performance, they
remain far under affine-transformed embeddings.
Extrinsic tasks are similarly affected by the CDE
transformation; however, NNE-transformed em-
beddings recover the majority of performance.

Comparing within the set of affine transforma-
tions, the innocuous effect of rotations, dilations,
and reflections on both intrinsic and extrinsic tasks
suggests that the models used are robust to simple
linear transformations. Extrinsic evaluations are
also relatively insensitive to translations, which
can be modeled with bias terms, though the lack
of learned models and reliance on cosine similar-
ity for the intrinsic tasks makes them more sensi-
tive to shifts relative to the origin. Interestingly,
homothety, which effectively combines a transla-
tion and a dilation, leads to a noticeable drop in
performance across all tasks. Intuitively, this re-
sult makes sense: by both shifting points rela-
tive to the origin and changing their distribution
in the space, angular similarity values used for in-
trinsic tasks can be changed significantly, and the
zero mean feature distribution preferred by neu-
ral models (Clevert et al., 2016) becomes harder
to achieve. This suggests that methods for tuning
embeddings should attempt to preserve the origin
whenever possible.

The large drops in performance observed when
using the CDE transformation is likely to relate

6Due to their large vocabulary size, we were unable to run
Thresholded-NNE experiments with word2vec embeddings.

to the instability of nearest neighborhoods and
the importance of locality in embedding learn-
ing (Wendlandt et al., 2018), although the effects
of the autoencoder component also bear further
investigation. By effectively increasing the size
of the neighborhood considered, CDE adds ad-
ditional sources of semantic noise. The similar
drops from thresholded-NNE transformations, by
the same token, is likely related to observations
of the relationship between the frequency ranks of
a word and its nearest neighbors (Faruqui et al.,
2016). With thresholded-NNE, we find that the
words with highest out degree in the nearest neigh-
bor graph are rare words (e.g., “Chanterelle” and
“Courtier” in FastText, “Tiegel” and “demangler”
in GloVe), which link to other rare words. Thus,
node2vec’s random walk method is more likely
to traverse these dense subgraphs of rare words,
adding noise to the output embeddings.

Finally, we note that Melamud et al. (2016)
showed significant variability in downstream task
performance when using different embedding di-
mensionalities. While we fixed vector dimension-
ality for the purposes of this study, varying d in
future work represents a valuable follow-up.

Our findings suggest that methods for train-
ing and tuning embeddings, especially for down-
stream tasks, should explicitly focus on local geo-
metric structure in the vector space. One concrete
example of this comes from Chen et al. (2018),
who demonstrate empirical gains when changing
the negative sampling approach of word2vec to
choose negative samples that are currently near to
the target word in vector space, instead of the orig-
inal frequency-based sampling (which ignores ge-
ometric structure). Similarly, successful methods
for tuning word embeddings for specific tasks have
often focused on enforcing a specific neighbor-
hood structure (Faruqui et al., 2015). We demon-
strate that by doing so, they align qualitative se-
mantic judgments with the primary geometric in-
formation that downstream models learn from.

6 Conclusion

Analysis of word embeddings has largely fo-
cused on qualitative characteristics such as near-
est neighborhoods or relative distribution. In this
work, we take a quantitative approach analyzing
geometric attributes of embeddings in Rd, in order
to understand the impact of geometric properties
on downstream task performance. We character-



14

ized word embedding geometry in terms of abso-
lute position, vector features, global pairwise dis-
tances, and local pairwise distances, and generated
new embedding matrices by removing these at-
tributes from pretrained embeddings. By evaluat-
ing the performance of these transformed embed-
dings on a variety of intrinsic and extrinsic tasks,
we find that while intrinsic evaluations are sensi-
tive to absolute position, downstream models rely
primarily on information about local similarity.

As embeddings are used for increasingly
specialized applications, and as recent contextual-
ized embedding methods such as ELMo (Peters
et al., 2018) and BERT (Devlin et al., 2018)
allow for dynamic generation of embeddings
from specific contexts, our findings suggest that
work on tuning and improving these embeddings
should focus explicitly on local geometric struc-
ture in sampling and evaluation methods. The
source code for our transformations and com-
plete tables of our results are available online at
https://github.com/OSU-slatelab/
geometric-embedding-properties.

Acknowledgments

We gratefully acknowledge the use of Ohio Su-
percomputer Center (Ohio Supercomputer Cen-
ter, 1987) resources for this work, and thank our
anonymous reviewers for their insightful com-
ments. Denis is supported via a Pre-Doctoral Fel-
lowship from the National Institutes of Health,
Clinical Center. Aparajita is supported via a Feuer
International Scholarship in Artificial Intelligence.

References
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.

2009. Knowledge-based wsd on specific domains:
Performing better than generic supervised WSD. IJ-
CAI International Joint Conference on Artificial In-
telligence, pages 1501–1506.

A Almuhareb and M Poesio. 2005. Concept Learning
and Categorization from the Web. Proceedings of
the Annual Meeting of the Cognitive Science Society,
27.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294, Austin, Texas. Association for Compu-
tational Linguistics.

Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-
itors. 2008. Proceedings of the ESSLLI Workshop on
Distributional Lexical Semantics: Bridging the gap
between semantic theory and computational simula-
tions. Hamburg, Germany.

Marco Baroni and Alessandro Lenci. 2011. How we
BLESSed distributional semantic evaluation. GEMS
’11 Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 1–10.

William F Battig and William E Montague. 1969. Cat-
egory norms of verbal items in 56 categories A repli-
cation and extension of the Connecticut category
norms. Journal of Experimental Psychology, 80(3,
Pt.2):1–46.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching Word Vectors with
Subword Information. Transactions of the ACL,
5:135–146.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal Distributional Semantics. Journal of
Artificial Intelligence Research, 49(1):1–47.

Chandrahas, Aditya Sharma, and Partha Talukdar.
2018. Towards Understanding the Geometry of
Knowledge Graph Embeddings. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 122–131. Association for Computational Lin-
guistics.

Long Chen, Fajie Yuan, Joemon M. Jose, and Weinan
Zhang. 2018. Improving negative sampling for
word representation using self-embedded features.
In Proceedings of the Eleventh ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ’18, pages 99–107, New York, NY, USA.
ACM.

Billy Chiu, Anna Korhonen, and Sampo Pyysalo. 2016.
Intrinsic Evaluation of Word Vectors Fails to Pre-
dict Extrinsic Performance. Proceedings of the 1st
Workshop on Evaluating Vector Space Representa-
tions for NLP, pages 1–6.

Djork-Arne Clevert, Thomas Unterthiner, and Sepp
Hochreiter. 2016. Fast and Accurate Deep Net-
work Learning by Exponential Linear Units (ELUs).
In Proceedings of the International Conference on
Learning Representations (ICLR).

Amaru Cuba Gyllensten and Magnus Sahlgren. 2015.
Navigating the Semantic Horizon using Relative
Neighborhood Graphs. In Proceedings of the 2015



15

Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2451–2460. Association
for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. In arXiv preprint arXiv:1810.04805v1.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems With Evaluation
of Word Embeddings Using Word Similarity Tasks.
In Proceedings of the 1st Workshop on Evaluating
Vector-Space Representations for NLP, pages 30–
35, Berlin, Germany. Association for Computational
Linguistics.

Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris
Dyer, and Noah A Smith. 2015. Sparse Overcom-
plete Word Vector Representations. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 1491–1500, Bei-
jing, China. Association for Computational Linguis-
tics.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing Search in Context: The
Concept Revisited. In Proceedings of the 10th In-
ternational Conference on World Wide Web, WWW
’01, pages 406–414, Hong Kong. ACM.

Alona Fyshe, Leila Wehbe, Partha P Talukdar, Brian
Murphy, and Tom M Mitchell. 2015. A Compo-
sitional and Interpretable Semantic Space. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
32–41, Denver, Colorado. Association for Computa-
tional Linguistics.

Aditya Grover and Jure Leskovec. 2016. Node2Vec:
Scalable Feature Learning for Networks. In Pro-
ceedings of the 22Nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ’16, pages 855–864, New York, NY, USA.
ACM.

F Heimerl and M Gleicher. 2018. Interactive Analysis
of Word Vector Embeddings. Computer Graphics
Forum, 37(3):253–265.

Johannes Hellrich and Udo Hahn. 2016. Bad
Company—Neighborhoods in Neural Embedding
Spaces Considered Harmful. In Proceedings of
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers,
pages 2785–2796. The COLING 2016 Organizing
Committee.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ’O S ’eaghdha, Sebastian
Pad ’o, Marco Pennacchiotti, Lorenza Romano, and

Stan Szpakowicz. 2010. SemEval-2010 Task 8:
Multi-Way Classification of Semantic Relations be-
tween Pairs of Nominals. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 33–38. Association for Computational Lin-
guistics.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
SimLex-999: Evaluating Semantic Models with
(Genuine) Similarity Estimation. Computational
Linguistics, 41(4):665–695.

Thomas Hofmann, Bernhard Schölkopf, and Alexan-
der J. Smola. 2008. Kernel methods in machine
learning. The Annals of Statistics, 36(3):1171–1220.

Yoon Kim. 2014. Convolutional Neural Networks for
Sentence Classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751. As-
sociation for Computational Linguistics.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Bofang Li, Tao Liu, Zhe Zhao, Buzhou Tang, Alek-
sandr Drozd, Anna Rogers, and Xiaoyong Du.
2017a. Investigating Different Syntactic Context
Types and Context Representations for Learning
Word Embeddings. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2421–2431. Association for Com-
putational Linguistics.

Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer,
Hao Zhang, and Leonidas Guibas. 2017b. Grass:
Generative recursive autoencoders for shape struc-
tures. ACM Trans. Graph., 36(4):52:1–52:14.

Tal Linzen. 2016. Issues in evaluating semantic spaces
using word analogies. In Proceedings of the 1st
Workshop on Evaluating Vector-Space Representa-
tions for NLP, pages 13–18.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better Word Representations with Re-
cursive Neural Networks for Morphology. In Pro-
ceedings of the Seventeenth Conference on Com-
putational Natural Language Learning, pages 104–
113, Sofia, Bulgaria. Association for Computational
Linguistics.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning Word Vectors for Sentiment Analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 142–150. Association for
Computational Linguistics.

Oren Melamud, David McClosky, Siddharth Patward-
han, and Mohit Bansal. 2016. The Role of Context
Types and Dimensionality in Learning Word Em-
beddings. In Proceedings of the 2016 Conference of



16

the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT ’16, pages 1030–1040, San
Diego, CA. Association for Computational Linguis-
tics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

David Mimno and Laure Thompson. 2017. The strange
geometry of skip-gram with negative sampling. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
2873–2878, Copenhagen, Denmark. Association for
Computational Linguistics.

Denis Newman-Griffis and Eric Fosler-Lussier. 2017.
Second-order word embeddings from nearest
neighbor topological features. arXiv preprint
arXiv:1705.08488.

Denis Newman-Griffis, Albert M Lai, and Eric Fosler-
Lussier. 2017. Insights into Analogy Completion
from the Biomedical Domain. In BioNLP 2017,
pages 19–28, Vancouver, Canada. Association for
Computational Linguistics.

Ohio Supercomputer Center. 1987. Ohio su-
percomputer center. http://osc.edu/ark:
/19495/f5s1ph73.

Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Sum-
marization Based on Minimum Cuts. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL-04).

Bo Pang and Lillian Lee. 2005. Seeing Stars: Ex-
ploiting Class Relationships for Sentiment Catego-
rization with Respect to Rating Scales. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL’05), pages 115–
124. Association for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global Vectors for
Word Representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1532–1543, Doha, Qatar.
Association for Computational Linguistics.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep Contextualized Word Rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237, New Orleans, Louisiana. Association
for Computational Linguistics.

Benedicte Pierrejean and Ludovic Tanguy. 2018. To-
wards Qualitative Word Embeddings Evaluation:
Measuring Neighbors Variation. In Proceedings of

the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Student Research Workshop, pages 32–39. Associa-
tion for Computational Linguistics.

Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
Word at a Time: Computing Word Relatedness
Using Temporal Semantic Analysis. In Proceedings
of the 20th International Conference on World Wide
Web, WWW ’11, pages 337–346, New York, NY,
USA. ACM.

Anna Rogers, Aleksandr Drozd, and Bofang Li. 2017.
The (too Many) Problems of Analogical Reasoning
with Word Vectors. Proceedings of the 6th Joint
Conference on Lexical and Computational Seman-
tics (*SEM 2017), pages 135–148.

Anna Rogers, Shashwath Hosur Ananthakrishna, and
Anna Rumshisky. 2018. What’s in Your Embed-
ding, And How It Predicts Task Performance. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 2690–2703, Santa
Fe, NM, USA. Association for Computational Lin-
guistics.

Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neural
networks. In International Conference on Learning
Representations.

Laura Wendlandt, Jonathan K Kummerfeld, and Rada
Mihalcea. 2018. Factors Influencing the Surprising
Instability of Word Embeddings. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 2092–2102. Association for Computa-
tional Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation Classification via
Convolutional Deep Neural Network. In Proceed-
ings of COLING 2014, the 25th International Con-
ference on Computational Linguistics: Technical
Papers, pages 2335–2344. Dublin City University
and Association for Computational Linguistics.



17

Appendix A Parameters

We give the following library of vectors in Rd used
as parameter values:

vdiag =


1√
d
...
1√
d

 ;

vdiagNeg =


− 1√

d
1√
d
...
1√
d

 .
(A.1)

Transform Parameter Value

Translation Direction: 0
Magnitude: 1

Dilation Magnitude: 2

Homothety Center: vdiag
Magnitude: 0.25

Reflection Hyperplane Vector: vdiag

2-D Rotation Basis Vector 1: vdiag
Basis Vector 2: vdiagNeg
Angle: π/4

Table 1: Transform parameters.

Appendix B NNE settings

We experimented with k ∈ {5, 10, 15} for our
weighted and unweighted NNE transformations.
For thresholded NNE, in order to best evaluate the
impact of thresholding over uniform k, we used
the minimum k = 5 and experimented with t ∈
{0.01, 0.05, 0.075}; higher values of t increased
graph size sufficiently to be impractical. We report
using k = 5 for weighted and unweighted settings
in our main results for fairer comparison with the
thresholded setting.

The effect of thresholding on nearest neigh-
bor graphs was a strongly right-tailed increase in
out degree for a small portion of nodes. Our re-
ported value of t = 0.05 increased the out de-
gree of 20,229 nodes for FastText (out of 1M to-
tal nodes), with the maximum increase being 819
(“Chanterelle”), and 1,354 nodes increasing out
degree by only 1. For GloVe, 7,533 nodes in-
creased in out degree (out of 2M total), with max-
imum increase 240 (“Tiegel”), and 372 nodes in-
creasing out degree by only 1.

Table 2 compares averaged performance values
across all intrinsic tasks for these settings, and
Table 3 compares average extrinsic task perfor-
mance.

NNE params FastText word2vec GloVe
Thresholded
k = 5, t = 0.01 0.160 – 0.106
k = 5, t = 0.05 0.129 – 0.130

k = 5, t = 0.075 0.150 – 0.132
Weighted

k = 5 0.320 0.419 0.426
k = 10 0.342 0.363 0.460
k = 15 0.346 0.376 0.448

Unweighted
k = 5 0.330 0.428 0.435

k = 10 0.351 0.396 0.463
k = 15 0.341 0.365 0.432

Table 2: Mean performance on intrinsic tasks under
different NNE settings.

NNE params FastText word2vec GloVe
Thresholded
k = 5, t = 0.01 0.642 – 0.666
k = 5, t = 0.05 0.650 – 0.664

k = 5, t = 0.075 0.649 – 0.663
Weighted

k = 5 0.721 0.720 0.738
k = 10 0.728 0.713 0.740
k = 15 0.725 0.713 0.739

Unweighted
k = 5 0.720 0.717 0.732

k = 10 0.724 0.712 0.738
k = 15 0.729 0.708 0.725

Table 3: Mean performance on extrinsic tasks under
different NNE settings.


