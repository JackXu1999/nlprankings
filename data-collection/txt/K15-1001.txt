



















































A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation


Proceedings of the 19th Conference on Computational Language Learning, pages 1–11,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics

A Coactive Learning View of Online Structured Prediction
in Statistical Machine Translation

Artem Sokolov and Stefan Riezler∗
Computational Linguistics & IWR∗

69120 Heidelberg, Germany
{sokolov,riezler}@cl.uni-heidelberg.de

Shay B. Cohen
University of Edinburgh
Edinburgh EH8 9LE, UK
scohen@inf.ed.ac.uk

Abstract

We present a theoretical analysis of online
parameter tuning in statistical machine
translation (SMT) from a coactive learn-
ing view. This perspective allows us to
give regret and generalization bounds for
latent perceptron algorithms that are com-
mon in SMT, but fall outside of the stan-
dard convex optimization scenario. Coac-
tive learning also introduces the concept of
weak feedback, which we apply in a proof-
of-concept experiment to SMT, showing
that learning from feedback that consists
of slight improvements over predictions
leads to convergence in regret and transla-
tion error rate. This suggests that coactive
learning might be a viable framework for
interactive machine translation. Further-
more, we find that surrogate translations
replacing references that are unreachable
in the decoder search space can be inter-
preted as weak feedback and lead to con-
vergence in learning, if they admit an un-
derlying linear model.

1 Introduction

Online learning has become the tool of choice for
large scale machine learning scenarios. Compared
to batch learning, its advantages include memory
efficiency, due to parameter updates being per-
formed on the basis of single examples, and run-
time efficiency, where a constant number of passes
over the training sample is sufficient for conver-
gence (Bottou and Bousquet, 2004). Statistical
Machine Translation (SMT) has embraced the po-
tential of online learning, both to handle millions
of features and/or millions of data in parameter

tuning via online structured prediction (see Liang
et al. (2006) for seminal early work), and in in-
teractive learning from user post-edits (see Cesa-
Bianchi et al. (2008) for pioneering work on on-
line computer-assisted translation). Online learn-
ing algorithms can be given a theoretical analy-
sis in the framework of online convex optimiza-
tion (Shalev-Shwartz, 2012), however, the appli-
cation of online learning techniques to SMT sac-
rifices convexity because of latent derivation vari-
ables, and because of surrogate translations replac-
ing human references that are unreachable in the
decoder search space. For example, the objective
function actually optimized in Liang et al.’s (2006)
application of Collins’ (2002) structure perceptron
has been analyzed by Gimpel and Smith (2012)
as a non-convex ramp loss function (McAllester
and Keshet, 2011; Do et al., 2008; Collobert et al.,
2006). Since online convex optimization does not
provide convergence guarantees for the algorithm
of Liang et al. (2006), Gimpel and Smith (2012)
recommend CCCP (Yuille and Rangarajan, 2003)
instead for optimization, but fail to provide a the-
oretical analysis of Liang et al.’s (2006) actual al-
gorithm under the new objective.

The goal of this paper is to present an alternative
theoretical analysis of online learning algorithms
for SMT from the viewpoint of coactive learning
(Shivaswamy and Joachims, 2012). This frame-
work allows us to make three main contributions:
• Firstly, the proof techniques of Shivaswamy

and Joachims (2012) are a simple and elegant tool
for a theoretical analysis of perceptron-style al-
gorithms that date back to the perceptron mistake
bound of Novikoff (1962). These techniques pro-
vide an alternative to an online gradient descent
view of perceptron-style algorithms, and can eas-
ily be extended to obtain regret bounds for a la-

1



tent perceptron algorithm at a rate ofO
(

1√
T

)
, with

possible improvements by using re-scaling. This
bound can be directly used to derive generalization
guarantees for online and online-to-batch conver-
sions of the algorithm, based on well-known con-
centration inequalities. Our analysis covers the ap-
proach of Liang et al. (2006) and supersedes Sun
et al. (2013)’s analysis of the latent perceptron by
providing simpler proofs and by adding a general-
ization analysis. Furthermore, an online learning
framework such as coactive learning covers prob-
lems such as changing n-best lists after each up-
date that were explicitly excluded from the batch
analysis of Gimpel and Smith (2012) and consid-
ered fixed in the analysis of Sun et al. (2013).

• Our second contribution is an extension of
the online learning scenario in SMT to include a
notion of “weak feedback” for the latent percep-
tron: Coactive learning follows an online learning
protocol, where at each round t, the learner pre-
dicts a structured object yt for an input xt, and
the user corrects the learner by responding with
an improved, but not necessarily optimal, object
ȳt with respect to a utility function U . The key as-
set of coactive learning is the ability of the learner
to converge to predictions that are close to opti-
mal structures y∗t , although the utility function is
unknown to the learner, and only weak feedback
in form of slightly improved structures ȳt is seen
in training. We present a proof-of-concept ex-
periment in which translation feedback of varying
grades is chosen from the n-best list of an “opti-
mal” model that has access to full information. We
show that weak feedback structures correspond to
improvements in TER (Snover et al., 2006) over
predicted structures, and that learning from weak
feedback minimizes regret and TER.

• Our third contribution is to show that cer-
tain practices of computing surrogate references
actually can be understood as a form of weak
feedback. Coactive learning decouples the learner
(performing prediction and updates) from the user
(providing feedback in form of an improved trans-
lation) so that we can compare different surro-
gacy modes as different ways of approximate util-
ity maximization. We show experimentally that
learning from surrogate “hope” derivations (Chi-
ang, 2012) minimizes regret and TER, thus fa-
voring surrogacy modes that admit an underly-
ing linear model, over “local” updates (Liang et
al., 2006) or “oracle” derivations (Sokolov et al.,

2013), for which learning does not converge.
It is important to note that the goal of our ex-

periments is not to present improvements of coac-
tive learning over the “optimal” full-information
model in terms of standard SMT performance. In-
stead, our goal is to present experiments that serve
as a proof-of-concept of the feasibility of coactive
learning from weak feedback for SMT, and to pro-
pose a new perspective on standard practices of
learning from surrogate translations. The rest of
this paper is organized as follows. After a review
of related work (Section 2), we present a latent
percpetron algorithm and analyze its convergence
and generalization properties (Section 3). Our first
set of experiments (Section 4.1) confirms our the-
oretical analysis by showing convergence in regret
and TER for learning from weak and strong feed-
back. Our second set of experiments (Section 4.2)
analyzes the relation of different surrogacy modes
to minimization of regret and TER.

2 Related Work

Our work builds on the framework of coactive
learning, introduced by Shivaswamy and Joachims
(2012). We extend their algorithms and proofs to
the area of SMT where latent variable models are
appropriate, and additionally present generaliza-
tion guarantees and an online-to-batch conversion.
Our theoretical analysis is easily extendable to the
full information case of Sun et al. (2013). We
also extend our own previous work (Sokolov et al.,
2015) with theory and experiments for online-to-
batch conversion, and with experiments on coac-
tive learning from surrogate translations.

Online learning has been applied for discrimi-
native training in SMT, based on perceptron-type
algorithms (Shen et al. (2004), Watanabe et al.
(2006), Liang et al. (2006), Yu et al. (2013), inter
alia), or large-margin approaches (Tillmann and
Zhang (2006), Watanabe et al. (2007), Chiang et
al. (2008), Chiang et al. (2009), Chiang (2012), in-
ter alia). The latest incarnations are able to handle
millions of features and millions of parallel sen-
tences (Simianer et al. (2012), Eidelmann (2012),
Watanabe (2012), Green et al. (2013), inter alia).
Most approaches rely on hidden derivation vari-
ables, use some form of surrogate references, and
involve n-best lists that change after each update.

Online learning from post-edits has mostly been
confined to “simulated post-editing” where inde-
pendently created human reference translations,

2



or post-edits on the output from similar SMT
systems, are used as for online learning (Cesa-
Bianchi et al. (2008), López-Salcedo et al. (2012),
Martı́nez-Gómez et al. (2012), Saluja et al. (2012),
Saluja and Zhang (2014), inter alia). Recent
approaches extend online parameter updating by
online phrase extraction (Wäschle et al. (2013),
Bertoldi et al. (2014), Denkowski et al. (2014),
Green et al. (2014), inter alia). We exclude dy-
namic phrase table extension, which has shown to
be important in online learning for post-editing, in
our theoretical analysis (Denkowski et al., 2014).

Learning from weak feedback is related to bi-
nary response-based learning where a meaning
representation is “tried out” by iteratively generat-
ing system outputs, receiving feedback from world
interaction, and updating the model parameters.
Such world interaction consists of database access
in semantic parsing (Kwiatowski et al. (2013), Be-
rant et al. (2013), or Goldwasser and Roth (2013),
inter alia). Feedback in response-based learning
is given by a user accepting or rejecting system
predictions, but not by user corrections.

Lastly, feedback in form of numerical utility
values for actions is studied in the frameworks of
reinforcement learning (Sutton and Barto, 1998)
or in online learning with limited feedback, e.g.,
multi-armed bandit models (Cesa-Bianchi and Lu-
gosi, 2006). Our framework replaces quantitative
feedback with immediate qualitative feedback in
form of a structured object that improves upon the
utility of the prediction.

3 Coactive Learning for Online Latent
Structured Prediction

3.1 Notation and Background

Let X denote a set of input examples, e.g.,
sentences, and let Y(x) denote a set of structured
outputs for x ∈ X , e.g., translations. We define
Y = ∪xY(x). Furthermore, by H(x, y) we
denote a set of possible hidden derivations for a
structured output y ∈ Y(x), e.g., for phrase-based
SMT, the hidden derivation is determined by a
phrase segmentation and a phrase alignment be-
tween source and target sentences. Every hidden
derivation h ∈ H(x, y) deterministically identifies
an output y ∈ Y(x). We defineH = ∪x,yH(x, y).
Let φ : X ×Y×H → Rd denote a feature function
that maps a triplet (x, y, h) to a d-dimensional
vector. For phrase-based SMT, we use 14 fea-
tures, defined by phrase translation probabilities,

Algorithm 1 Feedback-based Latent Perceptron
1: Initialize w ← 0
2: for t = 1, . . . , T do
3: Observe xt
4: (yt, ht)← arg max(y,h) w>t φ(xt, y, h)
5: Obtain weak feedback ȳt
6: if yt 6= ȳt then
7: h̄t ← arg maxh w>t φ(xt, ȳt, h)
8: wt+1 ← wt+∆h̄t,ht

(
φ(xt, ȳt, h̄t)−φ(xt, yt, ht)

)

language model probability, distance-based and
lexicalized reordering probabilities, and word
and phrase penalty. We assume that the fea-
ture function has a bounded radius, i.e. that
‖φ(x, y, h)‖ ≤ R for all x, y, h. By ∆h,h′ we
denote a distance function that is defined for any
h, h′ ∈ H, and is used to scale the step size of
updates during learning. In our experiments, we
use the ordinary Euclidean distance between the
feature vectors of derivations. We assume a linear
model with fixed parameters w∗ such that each
input example is mapped to its correct deriva-
tion and structured output by using (y∗, h∗) =
arg maxy∈Y(x),h∈H(x,y)w∗>φ(x, y, h). We define
for each given input x, its highest scoring deriva-
tion over all outputs Y(x) such that h(x;w) =
arg maxh′∈H(x,y) maxy∈Y(x)w>φ(x, y, h′)
and the highest scoring derivation for
a given output y ∈ Y(x) such that
h(x|y;w) = arg maxh′∈H(x,y)w>φ(x, y, h′). In
the following theoretical exposition we assume
that the arg max operation can be computed
exactly.

3.2 Feedback-based Latent Perceptron

We assume an online setting, in which examples
are presented one-by-one. The learner observes
an input xt, predicts an output structure yt, and
is presented with feedback ȳt about its prediction,
which is used to make an update to an existing pa-
rameter vector. Algorithm 1 is called ”Feedback-
based Latent Perceptron” to stress the fact that
it only uses weak feedback to its predictions for
learning, but does not necessarily observe optimal
structures as in the full information case (Sun et
al., 2013). Learning from full information can be
recovered by setting the informativeness parame-
ter α to 1 in Equation (2) below, in which case
the feedback structure ȳt equals the optimal struc-
ture y∗t . Algorithm 1 differs from the algorithm
of Shivaswamy and Joachims (2012) by a joint
maximization over output structures y and hid-

3



den derivations h in prediction (line 4), by choos-
ing a hidden derivation h̄ for the feedback struc-
ture ȳ (line 7), and by the use of the re-scaling
factor ∆h̄t,ht in the update (line 8), where h̄t =
h(xt|ȳt;wt) and ht = h(xt;wt) are the deriva-
tions of the feedback structure and the prediction
at time t, respectively. In our theoretical exposi-
tion, we assume that ȳt is reachable in the search
space of possible outputs, that is, ȳt ∈ Y(xt).
3.3 Feedback of Graded Utility
The key in the theoretical analysis in Shivaswamy
and Joachims (2012) is the notion of a linear utility
function, determined by parameter vector w∗, that
is unknown to the learner:

Uh(x, y) = w∗>φ(x, y, h).

Upon a system prediction, the user approximately
maximizes utility, and returns an improved object
ȳt that has higher utility than the predicted yt s.t.

U(xt, ȳt) > U(xt, yt)

where for given x ∈ X , y ∈ Y(x), and h∗ =
arg maxh∈H(x,y) Uh(x, y), we define U(x, y) =
Uh∗(x, y) and drop the subscript unless h 6= h∗.
Importantly, the feedback is typically not the opti-
mal structure y∗t that is defined as

y∗t = arg max
y∈Y(xt)

U(xt, y).

While not receiving optimal structures in training,
the learning goal is to predict objects with util-
ity close to optimal structures y∗t . The regret that
is suffered by the algorithm when predicting ob-
ject yt instead of the optimal object y∗t is

REGT =
1
T

T∑
t=1

(
U(xt, y∗t )− U(xt, yt)

)
. (1)

To quantify the amount of information in the
weak feedback, Shivaswamy and Joachims (2012)
define a notion of α-informative feedback, which
we generalize as follows for the case of latent
derivations. We assume that there exists a deriva-
tion h̄t for the feedback structure ȳt, such that
for all predictions yt, the (re-scaled) utility of the
weak feedback ȳt is higher than the (re-scaled)
utility of the prediction yt by a fraction α of the
maximum possible utility range (under the given
utility model). Thus ∀t,∃h̄t,∀h and for α ∈ (0, 1]:(
Uh̄t(xt,ȳt)− Uh(xt, yt)

)×∆h̄t,h
≥ α(U(xt, y∗t )− U(xt, yt))− ξt, (2)

where ξt ≥ 0 are slack variables allowing for vio-
lations of (2) for given α. For slack ξt = 0, user
feedback is called strictly α-informative.

3.4 Convergence Analysis
A central theoretical result in learning from weak
feedback is an analysis that shows that Algo-
rithm 1 minimizes an upper bound on the average
regret (1), despite the fact that optimal structures
are not used in learning:
Theorem 1. Let DT =

∑T
t=1 ∆

2
h̄t,ht

. Then the
average regret of the feedback-based latent per-
ceptron can be upper bounded for any α ∈ (0, 1],
for any w∗ ∈ Rd:

REGT ≤ 1
αT

T∑
t=1

ξt +
2R‖w∗‖

α

√
DT
T

.

A proof for Theorem 1 is similar to the proof
of Shivaswamy and Joachims (2012) and the orig-
inal mistake bound for the perceptron of Novikoff
(1962).1 The theorem can be interpreted as fol-
lows: we expect lower average regret for higher
values of α; due to the dominant term T , regret
will approach the minimum of the accumulated
slack (in case feedback structures violate Equa-
tion (2)) or 0 (in case of strictly α-informative
feedback). The main difference between the above
result and the result of Shivaswamy and Joachims
(2012) is the term DT following from the re-
scaled distance of latent derivations. Their anal-
ysis is agnostic of latent derivations, and can be
recovered by setting this scaling factor to 1. This
yields DT = T , and thus recovers the main fac-
tor

√
DT
T =

1√
T

in their regret bound. In our al-
gorithm, penalizing large distances of derivations
can help to move derivations ht closer to h̄t, there-
fore decreasing DT as learning proceeds. Thus in
caseDT < T , our bound is better than the original
bound of Shivaswamy and Joachims (2012) for a
perceptron without re-scaling. As we will show
experimentally, re-scaling leads to a faster conver-
gence in practice.

3.5 Generalization Analysis
Regret bounds measure how good the average pre-
diction of the current model is on the next example
in the given sequence, thus it seems plausible that
a low regret on a sequence of examples should im-
ply good generalization performance on the entire
domain of examples.

1Short proofs are provided in the appendix.

4



Generalization for Online Learning. First we
present a generalization bound for the case of on-
line learning on a sequence of random examples,
based on generalization bounds for expected aver-
age regret as given by Cesa-Bianchi et al. (2004).
Let probabilities P and expectations E be de-
fined with respect to the fixed unknown underly-
ing distribution according to which all examples
are drawn. Furthermore, we bound our loss func-
tion `t = U(xt, y∗t )−U(xt, yt) to [0, 1] by adding
a normalization factor 2R||w∗|| s.t. REGT =
1
T

∑T
t=1 `t. Plugging the bound on REGT of The-

orem 1 directly into Proposition 1 of Cesa-Bianchi
et al. (2004) gives the following theorem:

Theorem 2. Let 0 < δ < 1, and let x1, . . . , xT be
a sequence of examples that Algorithm 1 observes.
Then with probability at least 1− δ,

E[REGT ] ≤ 1
αT

T∑
t=1

ξt +
2R‖w∗‖

α

√
DT
T

+ 2||w∗||R
√

2
T

ln
1
δ
.

The generalization bound tells us how far the
expected average regret E[REGT ] (or average
risk, in terms of Cesa-Bianchi et al. (2004)) is from
the average regret that we actually observe in a
specific instantiation of the algorithm.

Generalization for Online-to-Batch Conver-
sion. In practice, perceptron-type algorithms are
often applied in a batch learning scenario, i.e.,
the algorithm is applied for K epochs to a train-
ing sample of size T and then used for predic-
tion on an unseen test set (Freund and Schapire,
1999; Collins, 2002). The difference to the online
learning scenario is that we treat the multi-epoch
algorithm as an empirical risk minimizer that se-
lects a final weight vector wT,K whose expected
loss on unseen data we would like to bound. We
assume that the algorithm is fed with a sequence
of examples x1, . . . , xT , and at each epoch k =
1, . . . ,K it makes a prediction yt,k. The correct
label is y∗t . For k = 1, . . . ,K and t = 1, . . . , T ,
let `t,k = U(xt, y∗t ) − U(xt, yt,k), and denote by
∆t,k and ξt,k the distance at epoch k for example
t, and the slack at epoch k for example t, respec-
tively. Finally, we denote by DT,K =

∑T
t=1 ∆

2
t,K ,

and by wT,K the final weight vector returned after
K epochs. We state a condition of convergence2:

2This condition is too strong for large datasets. However,
we believe that a weaker condition based on ideas from the

Condition 1. Algorithm 1 has converged on train-
ing instances x1, . . . , xT after K epochs if the
predictions on x1, . . . , xT using the final weight
vector wT,K are the same as the predictions on
x1, . . . , xT in the Kth epoch.

Denote by EX(`(x)) the expected loss on
unseen data when using wT,K where `(x) =
U(x, y∗) − U(x, y′), y∗ = arg maxy U(x, y) and
y′ = arg maxy maxhw>T,Kφ(x, y, h). We can
now state the following result:

Theorem 3. Let 0 < δ < 1, and let x1, . . . , xT
be a sample for the multiple-epoch perceptron al-
gorithm such that the algorithm converged on it
(Condition 1). Then, with probability at least 1−δ,
the expected loss of the feedback-based latent per-
ceptron satisfies:

EX(`(x)) ≤ 1
αT

T∑
t=1

ξt,K +
2R‖w∗‖

α

√
DT,K

T

+R‖w∗‖
√

8 ln 2δ
T

.

The theorem can be interpreted as a bound on
the generalization error (lefthand-side) by the em-
pirical error (the first two righthand-side terms)
and the variance caused by the finite sample (the
third term in the theorem). The result follows di-
rectly from McDiarmid’s concentration inequality.

4 Experiments

We used the LIG corpus3 which consists of 10,881
tuples of French-English post-edits (Potet et al.,
2012). The corpus is a subset of the news-
commentary dataset provided at WMT4 and con-
tains input French sentences, MT outputs, post-
edited outputs and English references. To prepare
SMT outputs for post-editing, the creators of the
corpus used their own WMT10 system (Potet et
al., 2010), based on the Moses phrase-based de-
coder (Koehn et al., 2007) with dense features.
We replicated a similar Moses system using the
same monolingual and parallel data: a 5-gram
language model was estimated with the KenLM
toolkit (Heafield, 2011) on news.en data (48.65M
sentences, 1.13B tokens), pre-processed with the
tools from the cdec toolkit (Dyer et al., 2010).

perceptron cycling theorem (Block and Levin, 1970; Gelfand
et al., 2010) should suffice to show a similar bound.

3
http://www-clips.imag.fr/geod/User/marion.potet/

index.php?page=download
4
http://statmt.org/wmt10/translation-task.html

5



0.00

0.10

0.20

0.30

0.40

0.50

0.60

0.70

0.80

0.90

0 4000 8000 12000 16000 20000

re
gr

et

iterations

α = 0.1
α = 0.5
α = 1.0

0 4000 8000 12000 16000 20000
0.29

0.30

0.31

0.32

T
E

R

iterations

α = 0.1
α = 0.5
α = 1.0

0.00

0.10

0.20

0.30

0.40

0.50

0.60

0.70

0.80

0.90

0 4000 8000 12000 16000 20000

re
gr

et

iterations

scaled; α = 0.1
scaled; α = 0.5
scaled; α = 1.0

0 4000 8000 12000 16000 20000
0.29

0.30

0.31

0.32

T
E

R

iterations

scaled; α = 0.1
scaled; α = 0.5
scaled; α = 1.0

Figure 1: Regret and TER vs. iterations for α-informative feedback ranging from weak (α = 0.1) to
strong (α = 1.0) informativeness, with (lower part) and without re-scaling (upper part).

Parallel data (europarl+news-comm, 1.64M sen-
tences) were similarly pre-processed and aligned
with fast align (Dyer et al., 2013). In all ex-
periments, training is started with the Moses de-
fault weights. The size of the n-best list, where
used, was set to 1,000. Irrespective of the use of
re-scaling in perceptron training, a constant learn-
ing rate of 10−5 was used for learning from simu-
lated feedback, and 10−4 for learning from surro-
gate translations.

Our experiments on online learning require
a random sequence of examples for learning.
Following the techniques described in Bertsekas
(2011) to generate random sequences for incre-
mental optimization, we compared cyclic order (K
epochs of T examples in fixed order), randomized
order (sampling datapoints with replacement), and
random shuffling of datapoints after each cycle,
and found nearly identical regret curves for all
three scenarios. In the following, all figures are
shown for sequences in the cyclic order, with re-
decoding after each update. Furthermore note that
in all three definitions of sequence, we never see
the fixed optimal feedback y∗t in training, but in-
stead in general a different feedback structure ȳt
(and a different prediction yt) every time we see
the same input xt.

4.1 Idealized Weak and Strong Feedback

In a first experiment, we apply Algorithm 1 to
user feedback of varying utility grade. The goal of

strict (ξt = 0) slack (ξt > 0)

# datapoints 5,725 1,155

TER(ȳt) < TER(yt) 52.17% 32.55%
TER(ȳt) = TER(yt) 23.95% 20.52%
TER(ȳt) > TER(yt) 23.88% 46.93%

Table 1: Improved utility vs. improved TER dis-
tance to human post-edits for α-informative feed-
back ȳt compared to prediction yt using default
weights at α = 0.1.

this experiment is to confirm our theoretical anal-
ysis by showing convergence in regret for learn-
ing from weak and strong feedback. We select
feedback of varying grade by directly inspecting
the optimal w∗, thus this feedback is idealized.
However, the experiment also has a realistic back-
ground since we show that α-informative feedback
corresponds to improvements under standard eval-
uation metrics such as lowercased and tokenized
TER, and that learning from weak and strong feed-
back leads to convergence in TER on test data.

For this experiment, the post-edit data from the
LIG corpus were randomly split into 3 subsets:
PE-train (6,881 sentences), PE-dev, and PE-test
(2,000 sentences each). PE-train was used for
our online learning experiments. PE-test was held
out for testing the algorithms’ progress on unseen
data. PE-dev was used to obtain w∗ to define the
utility model. This was done by MERT optimiza-
tion (Och, 2003) towards post-edits under the TER
target metric. Note that the goal of our experi-

6



% strictly α-informative

local 39.46%
filtered 47.73%
hope 83.30%

Table 2: α-informativeness of surrogacy modes.

ments is not to improve SMT performance over
any algorithm that has access to full information to
compute w∗. Rather, we want to show that learn-
ing from weak feedback leads to convergence in
regret with respect to the optimal model, albeit
at a slower rate than learning from strong feed-
back. The feedback data in this experiment were
generated by searching the n-best list for transla-
tions that are α-informative at α ∈ {0.1, 0.5, 1.0}
(with possible non-zero slack). This is achieved
by scanning the n-best list output for every input
xt and returning the first ȳt 6= yt that satisfies
Equation (2).5 This setting can be thought of as an
idealized scenario where a user picks translations
from the n-best list that are considered improve-
ments under the optimal w∗.

In order to verify that our notion of graded util-
ity corresponds to a realistic concept of graded
translation quality, we compared improvements in
utility to improved TER distance to human post-
edits. Table 1 shows that for predictions under
default weights, we obtain strictly α-informative
(for α = 0.1) feedback for 5,725 out of 6,881
datapoints in PE-train. These feedback structures
improve utility per definition, and they also yield
better TER distance to post-edits in the majority
of cases. A non-negative slack has to be used in
1,155 datapoins. Here the majority of feedback
structures do not improve TER distance.

Convergence results for different learning sce-
narios are shown in Figure 1. The left upper part
of Figure 1 shows average utility regret against
iterations for a setup without re-scaling, i.e., set-
ting ∆h̄,h = 1 in the definition of α-informative
feedback (Equation (2)) and in the update of Al-
gorithm 1 (line 8). As predicted by our regret
analysis, higher α leads to faster convergence, but
all three curves converge towards a minimal re-
gret. Also, the difference between the curves for

5Note that feedback provided in this way might be
stronger than required at a particular value of α since for all
β ≥ α, strictly β-informative feedback is also strictly α-
informative. On the other hand, because of the limited size of
the n-best list, we cannot assume strictly α-informative user
feedback with zero slack ξt. In experiments where updates
are only done if feedback is strictly α-informative we found
similar convergence behavior.

0.05

0.10

0.15

0.20

0.25

1 2 3 4 5 6 7 8 9 10

av
er

ag
e

lo
ss
` t

epochs

test, α = 0.1
test, α = 0.5
test, α = 1.0

train, α = 0.1
train, α = 0.5
train, α = 1.0

Figure 3: Average loss `t on heldout and train data.

α = 0.1 and α = 1.0 is much smaller than a fac-
tor of ten. As expected from the correspondence of
α-informative feedback to improvements in TER,
similar relations are obtained when plotting TER
scores on test data for training from weak feed-
back at different utility grades. This is shown in
the right upper part of Figure 1.

The left lower part of Figure 1 shows average
utility regret plotted against iterations for a setup
that uses re-scaling. We define ∆h̄t,h by the `2-
distance between the feature vectors φ(xt, ȳt, h̄t)
of the derivation of the feedback structure and the
feature vector φ(xt, yt, ht) of the derivation of the
predicted structure. We see that the curves for all
grades of feedback converge faster than the corre-
sponding curves for un-scaled feedback shown in
the upper part Figure 1. Furthermore, as shown in
the right lower part of Figure 1, TER is decreased
on test data as well at a faster rate.6

Lastly, we present an experimental validation of
the online-to-batch application of our algorithm.
That is, we would like to evaluate predictions that
use the final weight vector wT,K by comparing the
generalization error with the empirical error stated
in Theorem 3. The standard way to do this is to
compare the average loss on heldout data with the
the average loss on the training sequence. Fig-
ure 3 shows these results for models trained on
α-informative feedback of α ∈ {0.1, 0.5, 1.0} for
10 epochs. Similar to the online learning setup,
higher α results in faster convergence. Further-
more, curves for training and heldout evaluation
converge at the same rate.

4.2 Feedback from Surrogate Translations
In this section, we present experiments on learn-
ing from real human post-edits. The goal of
this experiment is to investigate whether the stan-

6We also conducted online-to-batch experiments for sim-
ulated feedback at α ∈ {0.1, 0.5, 1.0}. Similar to the online
learning setup, higher α results in faster convergence.

7



0.00

0.20

0.40

0.60

0.80

1.00

1.20

1.40

0 4000 8000 12000 16000 20000

re
gr

et

iterations

α = 0.1
α = 1.0

oracles
local

filtered
hope

0 4000 8000 12000 16000 20000
0.29

0.30

0.31

0.32

0.33

0.34

0.35

T
E

R

iterations

α = 0.1
α = 1.0

oracles
local

filtered
hope

Figure 2: Regret and TER for online learning from oracles, local, filtered, and hope surrogates.

dard practices for extracting feedback from ob-
served user post-edits for discriminative SMT can
be matched with the modeling assumptions of
the coactive learning framework. The custom-
ary practice in discriminative learning for SMT is
to replace observed user translations by surrogate
translations since the former are often not reach-
able in the search space of the SMT decoder. In
our case, only 29% of the post-edits in the LIG-
corpus were reachable by the decoder. We com-
pare four heuristics of generating surrogate trans-
lations: oracles are generated using the lattice or-
acle approach of Sokolov et al. (2013) which re-
turns the closest path in the decoder search graph
as reachable surrogate translation.7 A local sur-
rogate ỹ is chosen from the n-best list of the
linear model as the translation that achieves the
best TER score with respect to the actual post-
edit y: ỹ = arg miny′∈n-best(xt;wt) TER(y

′, y).
This corresponds to the local update mode of
Liang et al. (2006). A filtered surrogate trans-
lation ỹ is found by scanning down the n-best
list, and accepting the first translation as feed-
back that improves TER score with respect to the
human post-edit y over the 1-best prediction yt
of the linear model: TER(ỹ, y) < TER(yt, y).
Finally, a hope surrogate is chosen from the n-
best list as the translation that jointly maximizes
model score under the linear model and nega-
tive TER score with respect to the human post-
edit: ỹ = arg maxy′∈n-best(xt;wt)(−TER(y′, y) +
w>t φ(xt, y′, h)). This corresponds to what Chi-
ang (2012) termed “hope derivations”. Informally,
oracles are model-agnostic, as they can pick a
surrogate even from outside of the n-best list;
local is constrained to the n-best list, though
still ignoring the ordering according to the linear

7While the original algorithm is designed to maximize the
BLEU score of the returned path, we tuned its two free pa-
rameters to maximize TER.

model; finally, filtered and hope represent dif-
ferent ways of letting the model score influence
the selected surrogate.

As shown in Figure 2, regret and TER de-
crease with the increased amount of information
about the assumed linear model that is induced by
the surrogate translations: Learning from oracle
surrogates does not converge in regret and TER.
The local surrogates extracted from 1,000-best
lists still do not make effective use of the linear
model, while filtered surrogates enforce an im-
provement over the prediction under TER towards
the human post-edit, and improve convergence in
learning. Empirically, convergence is achieved
only for hope surrogates that jointly maximize
negative TER and linear model score, with a con-
vergence behavior that is very similar to learning
from weak α-informative feedback at α ' 0.1.
We quantify this in Table 2 where we see that the
improvement in TER over the prediction that holds
for any hope derivation, corresponds to an im-
provement in α-informativeness: hope surrogates
are strictly α-informative in 83.3% of the cases
in our experiment, whereas we find a correspon-
dence to strict α-informativeness only in 45.74%
or 39.46% of the cases for filtered and local
surrogates, respectively.

5 Discussion

We presented a theoretical analysis of online
learning for SMT from a coactive learning per-
spective. This viewpoint allowed us to give regret
and generalization bounds for perceptron-style on-
line learners that fall outside the convex opti-
mization scenario because of latent variables and
changing feedback structures. We introduced the
concept of weak feedback into online learning for
SMT, and provided proof-of-concept experiments
whose goal was to show that learning from weak
feedback converges to minimal regret, albeit at a

8



slower rate than learning from strong feedback.
Furthermore, we showed that the SMT standard
of learning from surrogate hope derivations can to
be interpreted as a search for weak improvements
under the assumed linear model. This justifies
the importance of admitting an underlying linear
model in computing surrogate derivations from a
coactive learning perspective.

Finally, we hope that our analysis motivates fur-
ther work in which the idea of learning from weak
feedback is taken a step further. For example,
our results could perhaps be strengthened by ap-
plying richer feature sets or dynamic phrase table
extension in experiments on interactive SMT. Our
theory would support a new post-editing scenario
where users pick translations from the n-best list
that they consider improvements over the predic-
tion. Furthermore, it would be interesting to see if
“light” post-edits that are better reachable and eas-
ier elicitable than “full” post-edits provide a strong
enough signal for learning.

Acknowledgments

This research was supported in part by DFG
grant RI-2221/2-1 “Grounding Statistical Machine
Translation in Perception and Action.”

Appendix: Proofs of Theorems

Proof of Theorem 1
Proof. First we bound w>T+1wT+1 from above:

w>T+1wT+1 = w
>
T wT

+ 2w>T
(
φ(xT , ȳT , h̄T )− φ(xT , yT , hT )

)
∆h̄T ,hT

+
(
φ(xT , ȳT , h̄T )− φ(xT , yT , hT )

)>
∆h̄T ,hT(

φ(xT , ȳT , h̄T )− φ(xT , yT , hT )
)
∆h̄T ,hT

≤ w>T wT + 4R2∆2h̄T ,hT ≤ 4R
2DT . (3)

The first equality uses the update rule from Algorithm
1. The second uses the fact that w>T (φ(xT , ȳT , h̄T ) −
φ(xT , yT , hT )) ≤ 0 by definition of (yT , hT ) in Algo-
rithm 1. By assumption ‖φ(x, y, h)‖ ≤ R, ∀x, y, h and
by the triangle inequality, ‖φ(x, y, h) − φ(x, y′, h′)‖ ≤
‖φ(x, y, h)‖ + ‖φ(x, y′, h′)‖ ≤ 2R. Finally, DT =∑T

t=1 ∆
2
h̄t,ht

by definition, and the last inequality follows
by induction.

The connection to average regret is as follows:

w>T+1w∗ = w
>
T w∗

+ ∆h̄T ,hT
(
φ(xT , ȳT , h̄T ))− φ(xT , yT , hT )

)>
w∗

=

T∑
t=1

∆h̄t,ht
(
φ(xt, ȳt, h̄t)− φ(xt, yt, ht)

)>
w∗

=

T∑
t=1

∆h̄t,ht
(
Uh̄t(xt, ȳt)− Uht(xt, yt)

)
. (4)

The first equality again uses the update rule from Algorithm
1. The second follows by induction. The last equality applies
the definition of utility.

Next we upper bound the utility difference:

T∑
t=1

∆h̄t,ht
(
Uh̄t(xt, ȳt)− Uht(xt, yt)

)
≤ ‖w∗‖‖wT+1‖ ≤ ‖w∗‖2R

√
DT . (5)

The first inequality follows from applying the Cauchy-
Schwartz inequality w>T+1w∗ ≤ ‖w∗‖‖wT+1‖ to Equa-
tion (4). The seond follows from applying Equation (3) to

‖wT+1‖ =
√
w>T+1wT+1.

The final result is obtained simply by lower bounding
Equation (5) using the assumption in Equation (2).

‖w∗‖2R
√
DT

≥
T∑

t=1

∆h̄t,ht
(
Uh̄t(xt, ȳt)− Uht(xt, yt)

)
≥ α

T∑
t=1

(
U(xt, y

∗
t )− U(xt, yt)

)− T∑
t=1

ξt

= α T REGT −
T∑

t=1

ξt.

Proof of Theorem 3
Proof. The theorem can be shown by an application of Mc-
Diarmid’s concentration inequality:

Theorem 4 (McDiarmid, 1989). Let Z1, . . . , Zm be a set
of random variables taking value in a set Z . Further, let
f : Zm → R be a function that satisfies for all i and
z1, . . . , zm, z

′
i ∈ Z:
|f(z1, . . . , zi, . . . , zm)

− f(z1, . . . , z′i, . . . , zm)| ≤ c, (6)
for some c. Then for all � > 0,

P(|f − E(f)| > �) ≤ 2 exp(− 2�
2

mc2
). (7)

Let f be the average loss for predicting yt on example xt
in epoch K: f(x1, . . . , xT ) = REGT,K = 1T

∑T
t=1 `t,K .

Because of the convergence condition (Condition 1), `t,K =
`(xt). The expectation of f is E(f) = 1T

∑T
t=1 E[`t,k] =

1
T

∑T
t=1 E[`(xt)] = EX(`(x)).

The first and second term on the righthand-side of Theo-
rem 3 follow from upper bounding REGT in theKth epochs,
using Theorem 1. The third term is derived by calculating c
in Equation (6) as follows:

|f(x1, . . . , xt, . . . , xT )− f(x1, . . . , x′t, . . . , xT )|

= | 1
T

T∑
t=1

`t,K − 1
T

T∑
t=1

`′t,K | = | 1
T

T∑
t=1

(
`t,K − `′t,K

) |
≤ 1
T

T∑
t=1

(|`t,k|+ |`′t,K |) ≤ 4R‖w∗‖
T

= c.

The first inequality uses the triangle inequality; the sec-
ond uses the upper bound |`t,k| ≤ 2R||w∗||. Setting the
righthand-side of Equation (7) to at least δ and solving for �,
using c, concludes the proof.

9



References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy

Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, Seattle, WA.

Nicola Bertoldi, Patrick Simianer, Mauro Cettolo,
Katharina Wäschle, Marcello Federico, and Stefan
Riezler. 2014. Online adaptation to post-edits for
phrase-based statistical machine translation. Ma-
chine Translation, 29:309–339.

Dimitri P. Bertsekas. 2011. Incremental gradient,
subgradient, and proximal methods for convex op-
timization: A survey. In Suvrit Sra, Sebastian
Nowozin, and Stephen J. Wright, editors, Optimiza-
tion for Machine Learning. MIT Press.

Henry D. Block and Simon A. Levin. 1970. On the
boundedness of an iterative procedure for solving
a system of linear inequalities. Proceedings of the
American Mathematical Society, 26(2):229–235.

Leon Bottou and Olivier Bousquet. 2004. Large scale
online learning. In NIPS, Vancouver, Canada.

Nicolò Cesa-Bianchi and Gàbor Lugosi. 2006. Predic-
tion, Learning, and Games. Cambridge University
Press.

Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gen-
tile. 2004. On the generalization ablility of on-line
learning algorithms. IEEE Transactions on Infor-
mation Theory, 50(9):2050–2057.

Nicolò Cesa-Bianchi, Gabriele Reverberi, and San-
dor Szedmak. 2008. Online learning algorithms
for computer-assisted translation. Technical report,
SMART (www.smart-project.eu).

David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, Waikiki, HA.

David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, Boulder, CO.

David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 12:1159–1187.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and exper-
iments with perceptron algorithms. In EMNLP,
Philadelphia, PA.

Ronan Collobert, Fabian Sinz, Jason Weston, and Leon
Bottou. 2006. Trading convexity for scalability. In
ICML, Pittsburgh, PA.

Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
EACL, Gothenburg, Sweden.

Chuong B. Do, Quoc Le, and Choon Hui Teo. 2008.
Tighter bounds for structured estimation. In NIPS,
Vancouver, Canada.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Türe, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL, Uppsala, Sweden.

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In NAACL, Atlanta, GA.

Vladimir Eidelmann. 2012. Optimization strategies
for online large-margin learning in machine transla-
tion. In WMT, Montreal, Canada.

Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Journal of Machine Learning Research, 37:277–
296.

Andrew E. Gelfand, Yutian Chen, Max Welling, and
Laurens van der Maaten. 2010. On herding and the
perceptron cycling theorem. In NIPS, Vancouver,
Canada.

Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
NAACL, Montreal, Canada.

Dan Goldwasser and Dan Roth. 2013. Learning from
natural instructions. Machine Learning, 94(2):205–
232.

Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In CHI, Paris, France.

Spence Green, Sida I. Wang, Jason Chuang, Jeffrey
Heer, Sebastian Schuster, and Christopher D. Man-
ning. 2014. Human effort and machine learnabil-
ity in computer aided translation. In EMNLP, Doha,
Qatar.

Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT, Edinburgh, UK.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Prague, Czech Republic.

Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In EMNLP, Seattle,
WA.

Percy Liang, Alexandre Bouchard-Côté, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL, Sydney, Australia.

10



Francisco-Javier López-Salcedo, Germán Sanchis-
Trilles, and Francisco Casacuberta. 2012. Online
learning of log-linear weights in interactive machine
translation. In IberSpeech, Madrid, Spain.

Pascual Martı́nez-Gómez, Germán Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193–
3202.

David McAllester and Joseph Keshet. 2011. General-
ization bounds and consistency for latent structural
probit and ramp loss. In NIPS, Granada, Spain.

Colin McDiarmid. 1989. On the method of bounded
differences. Surveys in combinatorics, 141(1):148–
188.

Albert B.J. Novikoff. 1962. On convergence proofs on
perceptrons. Symposium on the Mathematical The-
ory of Automata, 12:615–622.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In NAACL, Edmon-
ton, Canada.

Marion Potet, Laurent Besacier, and Hervé Blanchon.
2010. The LIG machine translation system for
WMT 2010. In WMT, Upsala, Sweden.

Marion Potet, Emanuelle Esperança-Rodier, Laurent
Besacier, and Hervé Blanchon. 2012. Collection
of a large database of French-English SMT output
corrections. In LREC, Istanbul, Turkey.

Avneesh Saluja and Ying Zhang. 2014. On-
line discriminative learning for machine translation
with binary-valued feedback. Machine Translation,
28:69–90.

Avneesh Saluja, Ian Lane, and Ying Zhang. 2012.
Machine translation with binary feedback: A large-
margin approach. In AMTA, San Diego, CA.

Shai Shalev-Shwartz. 2012. Online learning and on-
line convex optimization. Foundations and Trends
in Machine Learning, 4(2):107–194.

Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
NAACL, Boston, MA.

Pannaga Shivaswamy and Thorsten Joachims. 2012.
Online structured prediction via coactive learning.
In ICML, Edinburgh, UK.

Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT.
In ACL, Jeju, Korea.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, Cambridge, MA.

Artem Sokolov, Guillaume Wisniewski, and François
Yvon. 2013. Lattice BLEU oracles in machine
translation. Transactions on Speech and Language
Processing, 10(4):18.

Artem Sokolov, Stefan Riezler, and Shay B. Cohen.
2015. Coactive learning for interactive machine
translation. In ICML Workshop on Machine Learn-
ing for Interactive Systems (MLIS), Lille, France.

Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013.
Latent structured perceptrons for large scale learn-
ing with hidden information. IEEE Transactions
on Knowledge and Data Engineering, 25(9):2064–
2075.

Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning. An Introduction. The MIT
Press.

Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In COLING-ACL, Sydney, Australia.

Katharina Wäschle, Patrick Simianer, Nicola Bertoldi,
Stefan Riezler, and Marcello Federico. 2013. Gen-
erative and discriminative methods for online adap-
tation in SMT. In MT Summit, Nice, France.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2006. NTT statistical machine
translation for IWSLT 2006. In IWSLT, Kyoto,
Japan.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In EMNLP,
Prague, Czech Republic.

Taro Watanabe. 2012. Optimized online rank learn-
ing for machine translation. In NAACL, Montreal,
Canada.

Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In EMNLP, Seattle,
WA.

Alan Yuille and Anand Rangarajan. 2003. The
concave-convex procedure. Neural Computation,
15:915–936.

11


