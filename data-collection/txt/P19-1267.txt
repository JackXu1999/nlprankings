




































We Need to Talk about Standard Splits


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2786–2791
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2786

We need to talk about standard splits

Kyle Gorman
City University of New York

kgorman@gc.cuny.edu

Steven Bedrick
Oregon Health & Science University

bedricks@ohsu.edu

Abstract

It is standard practice in speech & language
technology to rank systems according to per-
formance on a test set held out for evalua-
tion. However, few researchers apply statis-
tical tests to determine whether differences in
performance are likely to arise by chance, and
few examine the stability of system ranking
across multiple training-testing splits. We con-
duct replication and reproduction experiments
with nine part-of-speech taggers published be-
tween 2000 and 2018, each of which reports
state-of-the-art performance on a widely-used
“standard split”. We fail to reliably repro-
duce some rankings using randomly generated
splits. We suggest that randomly generated
splits should be used in system comparison.

1 Introduction

Evaluation with a held-out test set is one of the few
methodological practices shared across nearly all
areas of speech and language processing. In this
study we argue that one common instantiation of
this procedure—evaluation with a standard split—
is insufficient for system comparison, and propose
an alternative based on multiple random splits.
Standard split evaluation can be formalized as

follows. Let G be a set of ground truth data, parti-
tioned into a training set Gtrain, a development set
Gdev and a test (evaluation) set Gtest. Let S be a
system with arbitrary parameters and hyperparam-
eters, and letM be an evaluation metric. Without
loss of generality, we assume that M is a func-
tion with domain G × S and that higher values
of M indicate better performance. Furthermore,
we assume a supervised training scenario in which
the free parameters of S are set so as to maximize
M(Gtrain, S), optionally tuning hyperparameters
so as to maximize M(Gdev, S). Then, if S1 and S2
are competing systems so trained, we prefer S1 to
S2 if and only ifM(Gtest, S1) > M(Gtest, S2).

1.1 Hypothesis testing for system comparison

One major concern with this procedure is that it
treatsM(Gtest, S1) andM(Gtest, S2) as exact quan-
tities when they are better seen as estimates of ran-
dom variables corresponding to true system perfor-
mance. In fact many widely used evaluation met-
rics, including accuracy and F-score, have known
statistical distributions, allowing hypothesis test-
ing to be used for system comparison.
For instance, consider the comparison of two

systems S1 and S2 trained and tuned to maximize
accuracy. The difference in test accuracy, δ̂ =
M(Gtest, S1) − M(Gtest, S2), can be thought of
as estimate of some latent variable δ representing
the true difference in system performance. While
the distribution of δ̂ is not obvious, the probabil-
ity that there is no population-level difference in
system performance (i.e., δ = 0) can be com-
puted indirectly usingMcNemar’s test (Gillick and
Cox, 1989). Let n1>2 be the number of samples in
Gtest which S1 correctly classifies but S2 misclas-
sifies, and n2>1 be the number of samples which
S1 misclassifies but S2 correctly classifies. When
δ = 0, roughly half of the disagreements should fa-
vor S1 and the other half should favor S2. Thus, un-
der the null hypothesis, n1>2 ∼ Bin(n, .5) where
n = n1>2 + n2>1. And, the (one-sided) probabil-
ity of the null hypothesis is the probability of sam-
pling n1>2 from this distribution. Similar methods
can be used for other evaluation metrics, or a refer-
ence distribution can be estimated with bootstrap
resampling (Efron, 1981).
Despite this, few recent studies make use of sta-

tistical system comparison. Dror et al. (2018) sur-
vey statistical practices in all long papers presented
at the 2017 meeting of the Association for Com-
putational Linguistics (ACL), and all articles pub-
lished in the 2017 volume of the Transactions of
the ACL. They find that themajority of these works



2787

do not use appropriate statistical tests for system
comparison, and many others do not report which
test(s) were used. We hypothesize that the lack of
hypothesis testing for system comparisonmay lead
to type I error, the error of rejecting a true null hy-
pothesis. As it is rarely possible to perform the nec-
essary hypothesis tests from published results, we
evaluate this risk using a replication experiment.

1.2 Standard vs. random splits
Furthermore, we hypothesize that standard split
methodology may be insufficient for system eval-
uation. While evaluations based on standard splits
are an entrenched practice in many areas of natu-
ral language processing, the static nature of stan-
dard splits may lead researchers to unconsciously
“overfit” to the vagaries of the training and test sets,
producing poor generalization. This tendency may
also be amplified by publication bias in the sense
of Scargle (2000). The field has chosen to define
“state of the art” performance as “the best perfor-
mance on a standard split”, and few experiments
which do not report improvements on a standard
split are ultimately published. This effect is likely
to be particularly pronounced on highly-saturated
tasks for which system performance is near ceiling,
as this increases the prior probability of the null hy-
pothesis (i.e., of no difference). We evaluate this
risk using a series of reproductions.

1.3 Replication and reproduction
In this study we perform a replication and a series
of reproductions. These techniques were until re-
cently quite rare in this field, despite the inherently
repeatable nature of most natural language pro-
cessing experiments. Researchers attempting repli-
cations or reproductions have reported problems
with availability of data (Mieskes, 2017; Wieling
et al., 2018) and software (Pedersen, 2008), and
various details of implementation (Fokkens et al.,
2013; Reimers and Gurevych, 2017; Schluter and
Varab, 2018). While we cannot completely avoid
these pitfalls, we select a task—English part-of-
speech tagging—for which both data and software
are abundantly available. This task has two other
important affordances for our purposes. First, it is
face-valid, both in the sense that the equivalence
classes defined by POS tags reflect genuine lin-
guistic insights and that standard evaluation met-
rics such as token and sentence accuracy directly
measure the underlying construct. Secondly, POS
tagging is useful both in zero-shot settings (e.g.,

Elkahky et al., 2018; Trask et al., 2015) and as a
source of features for many downstream tasks, and
in both settings, tagging errors are likely to propa-
gate. We release the underlying software under a
permissive license.1

2 Materials & Methods

2.1 Data

The Wall St. Journal (WSJ) portion of Penn
Treebank-3 (LDC99T42; Marcus et al., 1993) is
commonly used to evaluate English part-of-speech
taggers. In experiment 1, we also use a portion
of OntoNotes 5 (LDC2013T19; Weischedel et al.,
2011), a substantial subset of the Penn Treebank
WSJ data re-annotated for quality assurance.

2.2 Models

We attempted to choose a set of taggers claim-
ing state-of-the-art performance at time of publi-
cation. We first identified candidate taggers using
the “State of the Art” page for part-of-speech tag-
ging on the ACL Wiki.2 We then selected nine
taggers for which all needed software and exter-
nal data was available at time of writing. These
taggers are described in more detail below.

2.3 Metrics

Our primarily evaluation metric is token accu-
racy, the percentage of tokens which are correctly
tagged with respect to the gold data. We compute
95% Wilson (1927) score confidence intervals for
accuracies, and use the two-sided mid-p variant
(Fagerland et al., 2013) of McNemar’s test for sys-
tem comparison. We also report out-of-vocabulary
(OOV) accuracy—that is, token accuracy limited
to tokens not present in the training data—and sen-
tence accuracy, the percentage of sentences for
which there are no tagging errors.

3 Results

Table 1 reports statistics for the standard split. The
OntoNotes sample is slightly smaller as it omits
sentences on financial news, most of which is
highly redundant and idiosyncratic. However, the
entire OntoNotes sample was tagged by a single
experienced annotator, eliminating any annotator-
specific biases in the Penn Treebank (e.g., Ratna-
parkhi, 1997, 137f.).

1 http://github.com/kylebgorman/SOTA-taggers
2 http://aclweb.org/aclwiki/State_of_the_art

http://github.com/kylebgorman/SOTA-taggers
http://aclweb.org/aclwiki/State_of_the_art


2788

# Sentences # Tokens

Penn Treebank

Train. 38,219 912,344
Dev. 5,527 131,768
Test. 5,462 129,654

OntoNotes

Train. 28,905 703,955
Dev. 4,051 99,441
Test 4,059 98,277

Table 1: Summary statistics for the standard split.

3.1 Models

Three models—SVMTool (Giménez andMàrquez,
2004), MElt (Denis and Sagot, 2009), and
Morče/COMPOST (Spoustová et al., 2009)—
produced substantial compilation or runtime errors.
However, we were able to perform replication with
the remaining six models:

• TnT (Brants, 2000): a second-order (i.e., tri-
gram) hidden Markov model with a suffix-
based heuristic for unknown words, decoded
with beam search

• Collins (2002) tagger: a linear model, fea-
tures from Ratnaparkhi (1997), perceptron
training with weight averaging, decoded with
the Viterbi algorithm3

• LAPOS (Tsuruoka et al., 2011): a linear
model, features from Tsuruoka et al. (2009)
plus first-order lookahead, perceptron train-
ing with weight averaging, decoded locally

• Stanford tagger (Manning, 2011): a log-
linear bidirectional cyclic dependency net-
work, features from Toutanova et al. (2003)
plus distributional similarity features, op-
timized with OWL-QN, decoded with the
Viterbi algorithm

• NLP4J (Choi, 2016): a linear model, dynam-
ically induced features, a hinge loss objective
optimized with AdaGrad, decoded locally

• Flair (Akbik et al., 2018): a bidirectional
long short-term memory (LSTM) conditional
random fields (CRF) model, contextual string

3We use an implementation by Yarmohammadi (2014).

embedding features, a cross-entropy objec-
tive optimized with stochastic gradient de-
scent, decoded globally

3.2 Experiment 1: Replication

In experiment 1, we adopt the standard split es-
tablished by Collins (2002): sections 00–18 are
used for training, sections 19-21 for development,
and sections 22-24 for testing, roughly a 80%-10%-
10% split. We train and evaluate the six remaining
taggers using this standard split. For each tagger,
we train on the training set and evaluate on the test
set. For taggers which support it, we also perform
automated hyperparameter tuning on the develop-
ment set. Results are shown in Table 2. We ob-
tain exact replications for TnT and LAPOS, and
for the remaining four taggers, our results are quite
close to previously reported numbers. Token accu-
racy, OOV accuracy, and sentence accuracy give
the same ranking, one consistent with published
results. For Penn Treebank, McNemar’s test on to-
ken accuracy is significant for all pairwise compar-
isons at α = .05; for OntoNotes, one comparison is
non-significant: LAPOS vs. Stanford (p = .1366).

3.3 Experiment 2: Reproduction

We now repeat these analyses across twenty ran-
domly generated 80%–10%–10% splits. After
Dror et al. (2017), we use the Bonferroni procedure
to control familywise error rate, the probability of
falsely rejecting at least one true null hypothesis.
This is appropriate insofar as each individual trial
(i.e, evaluation on a random split) has a non-trivial
statistical dependence on other trials. Table 3 re-
ports the number of random splits, out of twenty,
where the McNemar test p-value is significant af-
ter the correction for familywise error rate. This
provides a coarse estimate of how often the second
system would be likely to significantly outperform
the first system given a random partition of similar
size. Most of these pairwise comparisons are sta-
ble across random trials. However, for example,
Stanford tagger is not a significant improvement
over LAPOS for nearly all random trials, and in
some random trials—two for Penn Treebank, four-
teen for OntoNotes—it is in fact worse. Recall also
that the Stanford tagger was also not significantly
better than LAPOS for OntoNotes in experiment 1.
Figure 1 shows token accuracies across the two

experiments. The last row of the figure gives re-
sults for an oracle ensemble which correctly pre-



2789

Penn Treebank OntoNotes

Token OOV Sentence Token

Reported Replicated (95% CIs) Replicated Replicated Reproduced

TnT .9646 .9646 (.9636, .9656) .8591 .4771 .9622
Collins .9711 .9714 (.9704, .9723) .8789 .5441 .9679
LAPOS .9722 .9722 (.9713, .9731) .8874 .5602 .9709
Stanford .9732 .9735 (.9726, .9744) .9060 .5710 .9714
NLP4J .9764 .9742 (.9733, .9750) .9148 .5756 .9742
Flair .9785 .9774 (.9765, .9782) .9287 .6111 .9790

Table 2: Previously reported, and replicated, accuracies for the standard split of the WSJ portion of Penn Treebank;
we also provide token accuracies for a reproduction with the WSJ portion of OntoNotes.

PTB ON

TnT vs. Collins 20 20
Collins vs. LAPOS 20 7
LAPOS vs. Stanford 1 0
Stanford vs. NLP4J 19 20
NLP4J vs. Flair 20 20

Table 3: The number of random trials (out of twenty) for
which the second system has significantly higher token
accuracy than the first after Bonferroni correction. PTB,
Penn Treebank; ON, OntoNotes.

dicts the tag just in case any of the six taggers pre-
dicts the correct tag.

3.4 Error analysis

From experiment 1, we estimate that the last two
decades of POS tagging research has produced a
1.28% absolute reduction in token errors. At the
same time, the best tagger is 1.16% below the ora-
cle ensemble. Thus we were interested in disagree-
ments between taggers. We investigate this by
treating each of the six taggers as separate coders
in a collaborative annotation task. We compute per-
sentence inter-annotator agreement using Krippen-
dorff’s α (Artstein and Poesio, 2008), then man-
ually inspect sentences with the lowest α values,
i.e., with the highest rate of disagreement. By
far the most common source of disagreement are
“headline”-like sentences such as Foreign Bonds.
While these sentences are usually quite short, high
disagreement is also found for some longer head-
lines, as in the example sentence in table 4; the ef-
fect seems to be due more to capitalization than
sentence length. Several taggers lean heavily on
capitalization cues to identify proper nouns, and

Figure 1: A visualization of Penn Treebank token ac-
curacies in the two experiments. The whiskers shows
accuracy and 95% confidence intervals in experiment
1, and shaded region represents the range of accuracies
in experiment 2.

thus capitalized tokens in headline sentences are
frequently misclassified as proper nouns and vice
versa, as are sentence-initial capitalized nouns in
general. Most other sentences with low α have lo-
cal syntactic ambiguities. For example, the word
lining, acting as a common noun (NN) in the con-
text …a silver for the…, is mislabeled as a
gerund (VBG) by two of six taggers.

4 Discussion

We draw attention to two distinctions between the
replication and reproduction experiments. First,
we find that a system judged to be significantly bet-
ter than another on the basis of performance on the



2790

Chicken Chains Ruffled By Loss of Customers

Gold NN NNS VBN IN NN IN NNS

TnT NNP NNP NNP IN NN IN NNS
Collins NNP NNP NNP IN NNP IN NNS
LAPOS NNP NNP NNP NNP NNP IN NNS
Stanford NNP NNS VBN IN NN IN NNS
NLP4J NNP NNPS NNP IN NNP IN NNS
Flair NN NNS VBN IN NN IN NNS

Table 4: Example error analysis for a Penn Treebank sentence; α = .521.

standard split, does not in outperform that system
on re-annotated data or randomly generated splits,
suggesting that it is “overfit to the standard split”
and does not represent a genuine improvement in
performance. Secondly, as can be seen in figure 1,
overall performance is slightly higher on the ran-
dom splits. We posit this to be an effect of ran-
domization at the sentence-level. For example, in
the standard split the word asbestos occurs fifteen
times in a single training set document, but just
once in the test set. Such discrepancies are far less
likely to arise in random splits.
Diversity of languages, data, and tasks are all

highly desirable goals for natural language pro-
cessing. However, nothing about this demonstra-
tion depends on any particularities of the English
language, the WSJ data, or the POS tagging task.
English is a somewhat challenging language for
POS tagging because of its relatively impoverished
inflectional morphology and pervasive noun-verb
ambiguity (Elkahky et al., 2018). It would not do
to use these six taggers for other languages as they
are designed for English text and in some cases de-
pend on English-only external resources for fea-
ture generation. However, random split experi-
ments could, for instance, be performed for the sub-
tasks of the CoNLL-2018 shared task on multilin-
gual parsing (Zeman et al., 2018).
We finally note that repeatedly training the Flair

tagger in experiment 2 required substantial grid
computing resources and may not be feasible for
many researchers at the present time.

5 Conclusions

We demonstrate that standard practices in system
comparison, and in particular, the use of a single
standard split, may result in avoidable Type I er-
ror. We suggest that practitioners who wish to
firmly establish that a new system is truly state-of-

the-art augment their evaluations with Bonferroni-
corrected random split hypothesis testing.
It is said that statistical praxis is of greatest im-

port in those areas of science least informed by the-
ory. While linguistic theory and statistical learn-
ing theory both have much to contribute to part-of-
speech tagging, we still lack a theory of the tagging
task rich enough to guide hypothesis formation. In
the meantime, we must depend on system compar-
ison, backed by statistical best practices and error
analysis, to make forward progress on this task.

Acknowledgments

We thank Mitch Marcus for valuable discussion of
the Wall St. Journal data.
Steven Bedrick was supported by the National

Institute on Deafness and Other Communication
Disorders of the National Institutes of Health un-
der award number R01DC015999. The content is
solely the responsibility of the authors and does not
necessarily represent the official views of the Na-
tional Institutes of Health.

References
AlanAkbik, Duncan Blythe, and RolandVollgraf. 2018.

Contextual string embedding for sequence labeling.
In COLING, pages 1638–1649.

Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.

Thorsten Brants. 2000. TnT: a statistical part-of-speech
tagger. In ANLC, pages 224–231.

Jinho D. Choi. 2016. Dynamic feature induction: The
last gist to the state-of-the-art. In NAACL, pages
271–281.

Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. InEMNLP, pages
1–8.



2791

Pascal Denis and Benoît Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art POS tagging with less human effort.
In Pacific Asia Conference on Language, Informa-
tion and Computation, pages 110–119.

Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi
Reichart. 2017. Replicability analysis for natural lan-
guage processing: testing significance with multiple
datasets. Transactions of the Association for Compu-
tational Linguistics, 5:471–486.

Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-
ichart. 2018. The hitchhiker’s guide to testing statis-
tical significance in natural language processing. In
ACL, pages 1383–1392.

Bradley Efron. 1981. Nonparametric estimates of stan-
dard error: the jackknife, the bootstrap and other
methods. Biometrika, 68(3):589–599.

Ali Elkahky, Kellie Webster, Daniel Andor, and Emily
Pitler. 2018. A challenge set and methods for noun-
verb ambiguity. In EMNLP, pages 2562–2572.

MortenW. Fagerland, Stian Lydersen, and Petter Laake.
2013. The McNemar test for binary matched-pairs
data: mid-p and asymptotic are better than exact
conditional. BMC Medical Research Methodology,
13:91–91.

Antske Fokkens, Marieke van Erp, Marten Postma, Ted
Pedersen, Piek Vossen, and Nuno Freire. 2013. Off-
spring from reproduction problems: What replica-
tion failure teaches us. In ACL, pages 1691–1701.

Larry Gillick and Stephen J. Cox. 1989. Some statis-
tical issues in the comparison of speech recognition
algorithms. In ICASSP, pages 23–26.

Jesús Giménez and Lluís Màrquez. 2004. SVMTool: A
general POS tagger generator based on support vec-
tor machines. In LREC, pages 43–46.

Christopher D. Manning. 2011. Part-of-speech tagging
from 97% to 100%: is it time for some linguistics?
In CICLing, pages 171–189.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.

Margot Mieskes. 2017. A quantitative study of data in
the NLP community. InWorkshop on Ethics in NLP,
pages 23–29.

Ted Pedersen. 2008. Empiricism is not a matter of faith.
Computational Linguistics, 34(3):465–470.

Adwait Ratnaparkhi. 1997. Amaximum entropymodel
for part-of-speech tagging. In EMNLP, pages 133–
142.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: performance
study of LSTM-networks for sequence tagging. In
EMNLP, pages 338–348.

Jeffrey D. Scargle. 2000. Publication bias: the “file-
drawer problem” in scientific inference. Journal of
Scientific Exploration, 14(1):91–106.

Natalie Schluter and Daniel Varab. 2018. When data
permutations are pathological: the case of neural nat-
ural language inference. In EMNLP, pages 4935–
4939.

Drahomíra Spoustová, Jan Hajič, Jan Raab, and
Miroslav Spousta. 2009. Semi-supervised training
for the averaged perceptron POS tagger. In EACL,
pages 763–771.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech taggingwith a cyclic dependency network. In
NAACL, pages 173–180.

Andrew Trask, Phil Michalak, and John Liu. 2015.
sense2vec: A fast and accurate method for word
sense disambiguation in neural word embeddings.
ArXiv preprint arXiv:1511.06388.

Yoshimasa Tsuruoka, Yusuke Miyao, and Jun'ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL, pages 238–246.

Yoshimasa Tsuruoka, Jun'ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for L1-regularized log-linear models with cumula-
tive penalty. In ICNLP-AFNLP, pages 477–485.

Ralph Weischedel, Eduard Hovy, Mitchell P. Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan, …,
and Nianwen Xue. 2011. OntoNotes: a large train-
ing corpus for enhanced processing. In Joseph Olive,
Caitlin Christianson, and John McCarthy, editors,
Handbook of natural language processing and ma-
chine translation, pages 54–63. Springer, New York.

Martijn Wieling, Josine Rawee, and Gertjan van Noord.
2018. Reproducibility in computational linguistics:
are we willing to share? Computational Linguistics,
44(4):641–649.

Edwin B. Wilson. 1927. Probable inference, the law of
succession, and statistical inference. Journal of the
American Statistical Association, 22:209–212.

Mahsa Yarmohammadi. 2014. Discriminative train-
ing with perceptron algorithm for POS tagging task.
Technical Report CSLU-2014-001, Center for Spo-
ken Language Understanding, Oregon Health & Sci-
ence University.

Daniel Zeman, Martin Popel, Milan Straka, Jan Hajič,
Joakim Nivre, Filip Ginter, …, and Josie Li. 2018.
CoNLL 2018 shared task: multilingual parsing from
raw text to Universal Dependencies. In Proceedings
of the CoNLL 2018 shared task: multilingual parsing
from raw text to Universal Dependencies, pages 1–
21.


