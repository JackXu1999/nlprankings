



















































Learning to Generate Product Reviews from Attributes


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 623–632,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Learning to Generate Product Reviews from Attributes

Li Dong†, Shaohan Huang‡, Furu Wei‡, Mirella Lapata†, Ming Zhou‡ and Ke Xuᵀ
†University of Edinburgh, Edinburgh, United Kingdom

‡Microsoft Research, Beijing, China
ᵀBeihang University, Beijing, China

li.dong@ed.ac.uk, {fuwei, mingzhou}@microsoft.com,
buaahsh@gmail.com, mlap@inf.ed.ac.uk, kexu@nlsde.buaa.edu.cn

Abstract

Automatically generating product reviews
is a meaningful, yet not well-studied task
in sentiment analysis. Traditional natu-
ral language generation methods rely ex-
tensively on hand-crafted rules and prede-
fined templates. This paper presents an
attention-enhanced attribute-to-sequence
model to generate product reviews for
given attribute information, such as user,
product, and rating. The attribute en-
coder learns to represent input attributes
as vectors. Then, the sequence decoder
generates reviews by conditioning its out-
put on these vectors. We also introduce
an attention mechanism to jointly gener-
ate reviews and align words with input at-
tributes. The proposed model is trained
end-to-end to maximize the likelihood of
target product reviews given the attributes.
We build a publicly available dataset for
the review generation task by leveraging
the Amazon book reviews and their meta-
data. Experiments on the dataset show that
our approach outperforms baseline meth-
ods and the attention mechanism signif-
icantly improves the performance of our
model.

1 Introduction

Nowadays, there are many popular online review
sites (such as Amazon, and Yelp) that allow users
to read and post reviews about books, electronics,
restaurants, etc. The reviews are used to express
opinions for different aspects of products, and
have a wide variety of writing styles and differ-
ent polarity strengths. As a result, much previous
work has focused on how opinions are expressed
in review data. For example, previous studies on

User

Product

Rating

Attribute 
Encoder

Sequence 
Decoder

LSTM

I loved this 
family story , 

it was 
touching .

Attention Layer

Figure 1: Our model learns to encode attributes
into vectors, and then uses recurrent neural net-
works based on long short-term memory (LSTM)
units to generate reviews by conditioning on the
encoding vectors. An attention layer is used to
learn soft alignments between attributes and gen-
erated words.

sentiment analysis identify and extract subjective
content in review data (Liu, 2015). However, few
studies have explored building data-driven mod-
els that can generate product reviews for the given
products and ratings, which is helpful to under-
stand how a specific user comments for products.
As shown in Figure 1, the input to our model is
a set of attributes (such as user, product, and rat-
ing information), and our goal is to generate user-
and product-specific reviews that agree with the
input rating. These automatically generated re-
views are useful for companies. For example, we
could promote a product to users who have not
bought it, by generating novel and personalized
recommendations. We could also build a review
writing assistant for E-commerce websites. Af-
ter the website generates some candidate reviews
according to the user’s rating score, users could
select one and refine it, which makes the proce-
dure more user-friendly. Moreover, we can gener-
ate novel and personalized recommendations for
every user, which makes the recommendation sys-
tem more interpretable.

This attribute-conditioned review generation

623



problem is very challenging due to the variety of
candidate reviews that satisfy the input attributes.
In other words, apart from the given attributes,
there are other unknown or latent factors that in-
fluence the generated reviews, which renders the
generation process non-deterministic. Moreover,
although some attributes (such as rating) explicitly
determine the usage of sentiment words, others
(e.g., user information) implicitly influence word
usage. So the model needs to handle both explicit
and implicit clues. Additionally, the interactions
between attributes are important to obtain the hid-
den factors used for generation. For example, dif-
ferent users tend to describe different aspects of a
product and use different sentiment words to ex-
press a rating score.

In this paper, we propose a neural network
based attribute-to-sequence model. As shown in
Figure 1, our model contains three parts: attribute
encoder, sequence decoder, and an attention mech-
anism. Specifically, we first use multilayer percep-
trons to encode input attributes into vector repre-
sentations that are used as latent factors for gener-
ating reviews. Next, the encoding vectors are fed
into a coarse-to-fine sequence decoder. The de-
coder is built by stacking multiple layers of recur-
rent neural networks, which can generate words
one by one conditioning on the encoding vec-
tors. Besides, we introduce an attention layer into
the proposed attribute-to-sequence model. The
attention mechanism learns soft alignments be-
tween generated words and attributes, and adap-
tively computes encoder-side context vectors used
to predict the next tokens. In order to evaluate our
method, we build a dataset based on Amazon re-
views and performed experiments on it. The ex-
perimental results show that the proposed model
achieves superior performance against baseline
methods. Moreover, we demonstrate that the at-
tention mechanism significantly improves the per-
formance of our model.

The contributions of this work are three-fold:

• We introduce the task of attribute-
conditioned review generation, which is
valuable for sentiment analysis, but not well
studied previously.

• We propose an attention-enhanced attribute-
to-sequence model in order to generate re-
views conditioned on input attributes.

• We create a dataset based on Amazon book

reviews and present empirical studies to
show the proposed model outperforms sev-
eral baseline methods.

2 Related Work

Sentiment analysis and opinion mining aim to
identify and extract subjective content in text (Liu,
2015). Most previous work focuses on using rule-
based methods or machine learning techniques for
sentiment classification, which classifies reviews
into different sentiment categories. Recently, deep
learning has achieved promising results on sen-
timent analysis (Socher et al., 2011; Dong et
al., 2014; Kim, 2014). Lipton et al. (2015) use
character-level concatenated input recurrent neu-
ral networks as a generative model to predict rat-
ing and category for reviews. In contrast, our
model is mainly evaluated on the review gener-
ation task rather than classification. Moreover,
we use an attention mechanism in our encoder-
decoder model, which has been proved very help-
ful in various tasks (Bahdanau et al., 2015; Xu et
al., 2015), to generate user- and product-specific
reviews. Maqsud (2015) compare latent Dirich-
let allocation, Markov chains, and hidden Markov
models for text generation on review data. How-
ever, we focus on generating product reviews con-
ditioned on input attributes. Park et al. (2015) pro-
pose to retrieve relevant opinion sentences using
product specifications as queries, while we work
on generation instead of retrieval.

Our task definition is also related to concept-to-
text generation (Konstas and Lapata, 2012; Kon-
stas and Lapata, 2013), such as generating weather
forecast or sportscasting from database records. A
typical system contains three main stages: content
planning, sentence planning, and surface realiza-
tion. Mei et al. (2016) treat database records and
output texts as sequences, and use recurrent neu-
ral networks to encode and decode them. In con-
trast, our input is a set of discrete attributes instead
of database records or sequences. In addition, the
contents of database records are strong constraints
on results in concept-to-text generation. However,
in our setting, user and product information im-
plicitly indicates the style of generated reviews,
which makes the results extremely diverse.

Another line of related work is the encoder-
decoder model with neural networks. Specifically,
an encoder is employed to encode input informa-
tion into vectors, and then a decoder learns to

624



predict results by conditioning outputs on the en-
coding vectors. This general framework is flexi-
ble because different neural networks can be used
for encoders and decoders depending on the na-
ture of inputs and outputs, which has been used
to address various tasks. For example, recur-
rent neural networks are used to model sequences,
such as machine translation (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014), syntac-
tic parsing (Vinyals et al., 2015b), and seman-
tic parsing (Dong and Lapata, 2016). Addition-
ally, convolutional neural networks are employed
for image data, such as image caption genera-
tion (Vinyals et al., 2015a), and video description
generation (Donahue et al., 2015; Venugopalan et
al., 2015). Our model employs multilayer per-
ceptron to encode attribute information, and uses
recurrent neural networks to decode product re-
views. In order to better handle alignments be-
tween inputs and outputs, the attention mechanism
is introduced for the encoder-decoder model. The
attention model boosts performance for various
tasks (Bahdanau et al., 2015; Luong et al., 2015;
Xu et al., 2015). In our work, we use the atten-
tion mechanism to learn soft alignments between
input attributes and output sequences, which has
not, to our knowledge, been studied in previous
work. Dosovitskiy et al. (2015) propose to use
generative convolutional neural networks to gen-
erate images of chairs given chair type, viewpoint
and color. Similarly, Yan et al. (2016) use varia-
tional auto-encoders to generate face images con-
ditioned on visual attributes. However, our goal is
to generate texts instead of images. Moreover, we
learn a neural attention model to attend over input
attributes during generation.

3 Modelling Approach

To begin with, we state the product review gener-
ation problem as follows. Given input attributes
a =

(
a1, · · · , a|a|

)
, our goal is to generate a prod-

uct review r =
(
y1, · · · , y|r|

)
maximizing the con-

ditional probability p (r|a). Notice that number of
attributes |a| is fixed, while the review r is consid-
ered a word sequence of variable length. We use
the user ID, product ID, and rating as attributes,
so |a| is set to 3 in our task. The training data are
attributes paired with corresponding reviews. The
model learns to compute the likelihood of gener-
ated reviews given input attributes. This condi-

User

Product

Rating

LSTM
LSTM

<s>

i

LSTM
LSTM

i

loved

LSTM
LSTM

loved

this

LSTM
LSTM

this

family

LSTM
LSTM

it

was

LSTM
LSTM

was

touching

LSTM
LSTM

touching

</s>

Attribute Encoder Sequence Decoder

...

...

Figure 2: Attribute-to-sequence model without at-
tention mechanism.

tional probability p (r|a) is decomposed to:

p (r|a) =
|r|∏
t=1

p (yt|y<t, a) (1)

where y<t = (y1, · · · , yt−1).
Our method consists of three parts, i.e., an at-

tribute encoder, a sequence decoder, and an atten-
tion layer. The attribute encoder employs multi-
layer perceptrons to encode attributes a to vectors.
To be specific, we represent the attributes as vec-
tors. Next, the concatenation of these vectors is
fed into a hidden layer to obtain the encoding vec-
tors. After we obtain the encoding vectors, the
sequence decoder stacks L-layer recurrent neural
networks (RNNs) to generate reviews condition-
ing on these vectors. During decoding, RNNs re-
currently compute n-dimensional hidden vectors
which are used to predict output words for differ-
ent time steps. In order to better utilize encoder-
side information, an attention layer is introduced
to learn soft alignments between attributes and
output words. For every decoding time step, we
use the current hidden vector to compute attention
scores over attribute vectors. Then, a weighted
sum of attribute vectors is used as the context vec-
tor to predict output words.

We first describe the attribute-to-sequence
model without using neural attention in Sec-
tion 3.1 and Section 3.2. Next, we introduce the
attention mechanism in Section 3.3.

3.1 Attribute Encoder
We use multilayer perceptrons with one hidden
layer to encode attribute information into a vector
as shown in Figure 2. At first, input attributes a =(
a1, · · · , a|a|

)
are represented by low-dimensional

vectors. The attribute ai’s vector g (ai) is com-
puted via:

g (ai) = W ai e (ai) (2)

625



where W ai ∈ Rm×|ai| is a parameter matrix,
m is the dimension of embedding, and e (ai) ∈
{0, 1}|ai| is a one-hot vector representing the pres-
ence or absence of ai. Then, these attribute vec-
tors are concatenated and fed into a hidden layer
which outputs the encoding vector. The output of
the hidden layer is computed as:

a = tanh
(
H[g (a1), · · · ,g

(
a|a|
)
] + ba

)
(3)

where [g (a1), · · · ,g
(
a|a|
)
] are concatenated at-

tribute vectors, tanh is a nonlinearity function,
H ∈ RLn×|a|m is a weight matrix, and ba ∈ RLn
is the bias. Next, the vector a is used to initialize
the n-dimensional hidden vectors of the L-layer
recurrent neural networks in the decoder.

3.2 Sequence Decoder
As shown in Figure 2, the decoder is built upon
multilayer recurrent neural networks (RNNs) with
long short-term memory (LSTM) units. RNNs
use vectors to represent information for the cur-
rent time step and recurrently compute the next
hidden states. In our work, we stack multiple lay-
ers of RNNs in our architecture. Additionally, a
long short-term memory (Hochreiter and Schmid-
huber, 1997) unit is employed to better handle long
sequences. The LSTM introduces several gates
and explicit memory cells to memorize or forget
information, which enables networks learn more
complicated patterns. Let hlt ∈ Rn denote an
n-dimensional hidden vector in layer l and time
step t. hlt is computed via:

hlt = f
(
hlt−1,h

l−1
t

)
(4)

where h0t = W
re (yt−1) is the word embedding

of the previous predicted word, W r ∈ Rn×|Vr|
is a parameter matrix, |Vr| is the vocabulary size,
and e (yt−1) is a one-hot vector used to extract
word vector for yt−1. We follow the architecture
of LSTM unit described in Zaremba et al. (2015).
To be specific, the unit is given by:

i
f
o
g

 =


sigm
sigm
sigm
tanh

W l (hl−1thlt−1
)

plt = f � plt−1 + i� g
hlt = o� tanh

(
plt
)

(5)

where tanh, sigm, and � are element-wise oper-
ators, and W l ∈ R4n×2n is a weight matrix for
the l-th layer.

Once the input attributes are encoded to the vec-
tor a ∈ RLn by Equation (3), the encoding vector
is split into L vectors to initialize the hidden vec-
tors of the first time step in decoder. Then, RNNs
compute hidden vectors recurrently and predict
output words using the hidden vectors of the top-
most layer hLt . For the vanilla model without us-
ing an attention mechanism, the predicted distri-
bution of the t-th output word is:

p (yt|y<t, a) = softmaxyt
(
W phLt

)
(6)

where W p ∈ R|Vr|×n is a parameter matrix.

3.3 Attention Mechanism

The attention mechanism is introduced to better
utilize encoder-side information. As indicated in
Equation (6), the vanilla model does not directly
use attribute vectors to generate sequences. In-
tuitively, the model can concentrate on different
parts of encoding information to predict the next
word. Previous work has proved this idea signif-
icantly improves performance especially for long
sequences (Bahdanau et al., 2015; Vinyals et al.,
2015b; Luong et al., 2015).

Figure 3 demonstrates how to compute the
encoder-side context vector and use it to predict
output words. For the t-th time step of the decoder,
we compute the attention score of attribute ai via:

sti = exp
(
tanh

(
W s
[
hLt ,g (ai)

]))
/Z (7)

where the brackets [·, ·] denote concatenation, Z
is a normalization term that ensures

∑|a|
i=1 s

t
i = 1,

and W s ∈ R1×(n+m) is a parameter matrix. Next,
the attention context vector ct is obtained by:

ct =
|a|∑
i=1

sti g (ai) (8)

which is a weighted sum of attribute vectors. We
further employ the vector ct to predict the t-th out-
put token as:

hattt = tanh
(
W1ct + W2hLt

)
(9)

p (yt|y<t, a) = softmaxyt
(
W phattt

)
(10)

where W p ∈ R|Vr|×n, W1 ∈ Rn×m and W2 ∈
Rn×n are three parameter matrices.

626



LSTM

LSTM

LSTMAttribute Encoder

User

Product

Rating

Attention 
Scores

Figure 3: Attention scores are computed by at-
tribute vectors and the current hidden vector of the
decoder. Then, the encoder-side context vector is
obtained in the form of a weighted sum, which is
further used to predict the word distribution.

3.4 Model Training
We aim at maximizing the likelihood of generated
reviews given input attributes for the training data.
So we define the optimization problem as:

maximize
∑

(a,r)∈D
log p (r|a) (11)

whereD is the dataset of all attribute-review train-
ing pairs, and p (r|a) is defined as shown in Equa-
tion (1). In order to avoid overfitting, we insert
dropout layers between different LSTM layers as
suggested in Zaremba et al. (2015). The mini-
batched RMSProp (Tieleman and Hinton, 2012)
algorithm is used to optimize the objective func-
tion.

3.5 Inference
At test time, we first use the encoder to encode in-
put attributes into vectors, and use them to initial-
ize the LSTM units of the decoder. Then, the de-
coder predicts a review r̂ that maximizes the con-
ditional probability defined in Equation (1):

r̂ = arg max
r′

p
(
r′|a) (12)

where r′ is a candidate review. Because we de-
compose this probability as shown in Equation (1),
we can use beam search or greedy search to gen-
erate words, which avoids iterating over all candi-
date reviews. In order to determine the termination
of the generation process, we add a special token
</s> to the end of every output review. The gen-
eration terminates once this token is emitted.

4 Experiments

We first introduce a new dataset for this task and
compare our method with several baseline ap-

proaches. Then we conduct some ablation exper-
iments and present model analysis to help us un-
derstand what the model learns.

4.1 Dataset Description

Our dataset is built upon Amazon product
data (McAuley et al., 2015) that includes reviews
and metadata spanning from May 1996 to July
2014 with duplicates removed. The products of
the book domain are used in our experiments.
Every review is paired with three attributes, i.e.,
user ID, product ID and rating. We filter books
and users which do not occur at least 6 and 15
times, respectively. The reviews whose lengths
are greater than 60 words are filtered. Because
we observe that long reviews mainly describe the
plots of books, while our goal is to generate re-
views expressing opinions. The average review
length is about 35 words, and the average number
of sentences is 3. The dataset contains 937, 033
reviews paired with attributes. Specifically, we
have 80, 256 books, 19, 675 users, and 5 rating
levels. The word vocabulary size is 161K. Then,
the whole dataset is randomly split into TRAIN,
DEV, and TEST (70%/10%/20%). The dataset is
available at https://goo.gl/TFjEH4.

4.2 Settings

We used NLTK (Bird et al., 2009) to tokenize the
reviews, and employed the Wikipedia list of com-
mon misspellings to correct misspelled words. We
kept words that appeared more than 10 times in
our vocabulary. The training hyperparameters are
selected based on the results of the DEV set. The
dimension of attribute vectors is set to 64. The
dimensions of word embeddings and hidden vec-
tors are set to 512 in the sequence decoder. More-
over, we stack two layers of recurrent neural net-
works with LSTM units to generate reviews. All
the parameters are randomly initialized by sam-
pling from a uniform distribution [−0.08, 0.08].
The batch size, smoothing constant and base learn-
ing rate of RMSProp are set to 50, 0.95 and 0.002,
respectively. After 10 epochs, the learning rate is
decreased by a factor of 0.97 at the end of every
epoch as suggested in Karpathy et al. (2016). The
dropout rate is set to 0.2 for regularization. We
also clamp gradient values into the range [−5, 5]
to avoid the exploding gradient problem (Pascanu
et al., 2013). The number of epochs is determined
by early stopping on the DEV set. At test time,

627



Method BLEU-4 (%) BLEU-1 (%)
Rand 0.86 20.36
MELM 1.28 21.59
NN-pr 1.53 22.44
NN-ur 3.61 26.37
Att2Seq 4.51 30.24
Att2Seq+A 5.03∗ 30.48∗

Table 1: Evaluation results on the TEST set of
Amazon data. ∗: significantly better than the sec-
ond best score (p < 0.05).

we use the greedy search algorithm to generate re-
views.

4.3 Evaluation Results
The BLEU (Papineni et al., 2002) score is used
for automatic evaluation, which has been shown
to correlate well with human judgment on many
generation tasks. The BLEU score measures the
precision of n-gram matching by comparing the
generated results with references, and penalizes
length using a brevity penalty term. We compute
BLEU-1 (unigram) and BLEU-4 (up to 4 grams)
in experiments.

4.3.1 Comparison with Baseline Methods
We describe the comparison methods as follows:

Rand. The predicted results are randomly
sampled from all the reviews in the TRAIN set.
This baseline method suggests the expected lower
bound for this task.

MELM. Maximum Entropy Language Model
uses n-gram (up to trigram) features, and the fea-
ture template attribute&n-gram (up to bigram).
The feature hashing technique is employed to re-
duce memory usage in each feature group. Noise
contrastive estimation (Gutmann and Hyvrinen,
2010) is used to accelerate the training by drop-
ping the normalization term, with 20 contrastive
samples in training.

NN-pr. This Nearest Neighbor based method
retrieves the reviews that have the same product ID
and rating as the input attributes in the TRAIN set.
Then we randomly choose a review from them,
and use it as the prediction.

NN-ur. The same method as NN-pr but uses
both user ID and rating to retrieve candidate re-
views.

Att2Seq. Our attribute-to-sequence method de-
scribed in Section 3. Notice that the attention
model is not used.

Method MELM Att2Seq Att2Seq+A
Accuracy (%) 59.00 88.67 93.33∗

Table 2: We manually annotate some polarity la-
bels (positive or negative) for generated reviews
and compute accuracy by comparing them with
the input ratings. ∗: significantly better than the
second best accuracy (p < 0.05).

Att2Seq+A. Our method with an attention
mechanism.

As shown in Table 1, we compute BLEU scores
for these methods. The results of random guess in-
dicate that this task is non-trivial to obtain reason-
able performance. MELM performs worse than
nearest neighbor search due to the sparsity of lex-
icalized features, while our model employs dis-
tributed representations to avoid using sparse in-
dicator features. Then, we evaluate the NN meth-
ods that use different attributes to retrieve reviews,
which is a strong baseline for the generation task.
The results show that our method outperforms the
baseline methods. Moreover, the improvements
of the attention mechanism are significant with
p < 0.05 according to the bootstrap resampling
test (Koehn, 2004). We further show some exam-
ples to analyze the attention model in Section 4.4.

4.3.2 Polarity of Generated Reviews
In order to evaluate whether the polarities of gen-
erated reviews correspond to their input ratings,
we randomly sample some generated reviews and
manually annotate their polarity labels. Specifi-
cally, we regard the rating 1-2 as negative and 4-
5 as positive, and then evaluate performance by
computing their classification accuracy. We ran-
domly sample 150 negative examples and 150 pos-
itive examples for each method. Next, we ask two
graduate students to classify the generated reviews
to positive, negative, and indeterminable/neutral.
About 93% of examples are annotated with the
same labels by two annotators. Table 2 shows
our method significantly outperforms others (p <
0.05). For Att2Seq+A, some generated reviews
are classified to indeterminable/neutral because
they contain mixed opinions towards different as-
pects of books.

4.3.3 Ablation Experiments
In order to evaluate the contributions of model
components in our method, we compare to the
variants of our model. These models are described

628



Method BLEU-4 (%) BLEU-1 (%)
Att2Seq+A 5.01 30.23
AvgEnc 4.07 28.13
NoStack 4.73 29.58
w/o user 4.10 26.87
w/o product 4.13 27.15
w/o rating 4.12 27.98

Table 3: Model ablation results on the DEV set.

as follows:
AvgEnc. This model uses the average of at-

tribute vectors as the encoding vector, rather than
multilayer perceptrons.

NoStack. The method only uses one-layer re-
current neural networks for the sequence decoder.

w/o user/product/rating. This variant does not
use the corresponding attribute as input. These re-
sults indicate the importance of different informa-
tion for our model.

As shown in Table 3, we compute BLEU-4 and
BLEU-1 scores for our full model and the differ-
ent variants on the DEV set. The ablation model
AvgEnc performs worse than Att2Seq+A. This in-
dicates that multilayer perceptrons can better han-
dle interactions between attributes, outperforming
simple averaging of input vectors. Next, we com-
pare to the model without stacking multiple layers
of recurrent neural networks as described in Sec-
tion 3.2. The results demonstrate that deep archi-
tectures can improve generation performance. For
the next group of variants, we find that removing
user, product and rating information harms per-
formance, which indicates that all three attributes
contribute to generating relevant reviews.

4.4 The Attention Mechanism
As described in Section 3.3, the attention mecha-
nism learns soft alignment scores between gener-
ated words and input attributes. These scores are
used to obtain encoder-side context vectors that
can better utilize attribute information to predict
the next word.

Figure 4 shows three generated examples with
different input ratings. The attention scores are
represented by gray scales and are column-wisely
normalized as described in Equation (7). Firstly,
we explain the attention scores over rating infor-
mation. The input rating of the first example is
1. We find that the phrases “n’t expecting much”,
“n’t like” and “a little too much” have larger at-
tention scores on the rating attribute. This demon-

i
re
ad th
e

fir
st

bo
ok an
d i

wa
s n'
t

ex
pe

ct
in
g

m
uc
h . i

di
d n'
t

lik
e

th
e

ch
ar
ac
te
rs

an
d

th
e

en
di
ng wa
s

ju
st a

lit
tle to
o

m
uc
h .

</
s>

User
Product

Rating=1

i
lo
ve

d
th
e

st
or
y

bu
t it

wa
s a

lit
tle

slo
w in th
e

m
id
dl
e

an
d i

di
d

no
t

lik
e

th
e

ch
ar
ac
te
rs .

</
s>

User
Product

Rating=3

re
al
ly

re
al
ly

lo
ve

d
th
is

bo
ok
, i ca n'
t

wa
it fo
r

th
e

ne
xt

on
e . i

re
al
ly

ho
pe sh
e

wr
ite

s
an

ot
he

r
bo

ok
.

</
s>

User
Product

Rating=5

Figure 4: Examples of attention scores (Equa-
tion (7)) over three attributes. Darker color indi-
cates higher attention score.

strates rating information has more effect on gen-
erating these sentiment words. Next, we increase
the rating score to 3 in the second example. The
generated review expresses a mixed opinion for
different aspects of the book. As indicated by
the attention scores, we know that the sentiment
words “loved”, “little slow”, and “not like” attend
more to rating information. The last example is a
positive review with a rating of 5. The attention
scores demonstrate the phrases “loved”, “ca n’t
wait”, and “hope * writes another book” are used
to express polarity. Similarly, the attention scores
over user and product information indicate how
the generated words are aligned with these two
input attributes. For instance, the word “charac-
ters” has higher attention scores over the product
attribute in the first and second example. This in-
dicates that users tend to comment about the char-
acters in this book’s reviews.

4.5 Generated Examples

As shown in Table 4, we sample products and
users to generate some examples with different rat-
ings. The special unknown token is removed from
the vocabulary of the decoder in the generation
process. We keep two attributes fixed and change
the other one in every group to show the effects of
input.

In the first group, we change the rating from
1 to 5 and keep the others unchanged. The re-
sults show that the polarity of generated reviews
changes with the rating. For instance, the words
“nice” and “liked” are used for the rating of 3,
while the words “very good” and “enjoyed” are
employed for the rating of 5. Moreover, both ex-

629



U P R Generated Review

A V 1
i’m sorry to say this was a very boring book. i didn’t finish it. i’m not a new fan of the series,
but this was a disappointment.

A V 3
this was a nice story. i liked the characters and the story line. i’m not sure i’d read another
by this author.

A V 5
this was a very good book. i enjoyed the characters and the story line. i’m looking forward to
reading more in this series.

B W 5 i couldn’t put it down. it was a great love story. i can’t wait to read the next one.

C W 5
enjoyable story that keeps you turning the pages. the characters are well developed and the
plot is excellent. i would recommend this book to anyone who enjoys a good love story.

D W 5
i loved this book. i could not put it down. i loved this story and the characters. i will be
reading the next book.

E X 1
i read this book because i was looking for something to read. this book was just too much like
the others. i thought the author was going to be a good writer, but i was disappointed.

E Y 1
i was disappointed. i read the first chapter and then i was bored. i read the whole thing, but i
just couldn’t get into it.

E Z 1
this book was just too much. i read the whole thing, but i didn’t like the way the author
ended it. i was hoping for a different ending.

Table 4: U: User. P: Product. R: Rating. This table shows some generated examples of the Att2Seq+A
model. In every group, two attributes are kept unchanged, while the other attribute has different values.
For instance, in the first group, we use different ratings ranging from 1 (the lowest score) to 5 (the highest
score) with the same user and product to generate reviews. The users and products are anonymized by
A-E and V-Z.

amples describe “characters” and “story line”, and
are written in the similar styles. This indicates
that user and product information determines the
content and style of generated reviews, while rat-
ing affects the choice of sentiment words. In the
next group, we use different user IDs as input at-
tributes. This book is one of the Fatal Series writ-
ten by Marie Force, which tells a romantic love
story. The first and third examples mention “next
one/book”, and both the first two reviews contain
the phrase “love story”. This demonstrates the
generated reviews agree with the input product in-
formation. In the third group, the attributes, ex-
cept product ID, are kept unchanged. The exam-
ples show our model generates varied reviews for
different products.

5 Conclusion

In this paper, we proposed a novel product re-
view generation task, in which generated reviews
are conditioned on input attributes. For this task,
we formulated a neural network based attribute-to-
sequence model that uses multilayer perceptrons
to encode input attributes and employs recurrent
neural networks to generate reviews. Moreover,
we introduced an attention mechanism to better

utilize input attribute information. Additionally,
we built a dataset of Amazon product reviews to
conduct evaluations. The proposed model consis-
tently outperforms the nearest neighbor search and
maximum entropy language model baselines. Be-
sides, the attention mechanism significantly im-
proves the vanilla attribute-to-sequence model.
This work suggests several interesting directions
for future research. We could use more fine-
grained attributes as the input of our model. For
example, the generated reviews could be condi-
tioned on device specification, brand, user’s gen-
der, product description, or ratings of a product’s
various aspects. Moreover, we could leverage re-
view texts without attributes to improve the se-
quence decoder.

Acknowledgments

The support of the European Research Council un-
der award number 681760 “Translating Multiple
Modalities into Text” is gratefully acknowledged.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly

630



learning to align and translate. In International Con-
ference on Learning Representations.

Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media.

J. Donahue, L. A. Hendricks, S. Guadarrama,
M. Rohrbach, S. Venugopalan, T. Darrell, and
K. Saenko. 2015. Long-term recurrent convolu-
tional networks for visual recognition and descrip-
tion. In 2015 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2625–2634.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany. Association for Computa-
tional Linguistics.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In Proceedings of the Twenty-Eighth AAAI Con-
ference on Artificial Intelligence, AAAI’14, pages
1537–1543. AAAI Press.

A. Dosovitskiy, J. T. Springenberg, and T. Brox. 2015.
Learning to generate chairs with convolutional neu-
ral networks. In 2015 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages
1538–1546.

M. Gutmann and A. Hyvrinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Y.W. Teh
and M. Titterington, editors, Proceedings of the 13th
International Conference on Artificial Intelligence
and Statistics (AISTATS), volume 9 of JMLR WCP,
pages 297–304. Journal of Machine Learning Re-
search - Proceedings Track.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700–1709, Seattle,
Washington, USA. Association for Computational
Linguistics.

Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2016.
Visualizing and understanding recurrent networks.
In International Conference on Learning Represen-
tations.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746–1751,
Doha, Qatar. Association for Computational Lin-
guistics.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain. Association for
Computational Linguistics.

Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 369–378, Jeju Island, Korea.
Association for Computational Linguistics.

Ioannis Konstas and Mirella Lapata. 2013. A global
model for concept-to-text generation. Journal of Ar-
tificial Intelligence Research, 48(1):305–346.

Zachary C. Lipton, Sharad Vikram, and Julian
McAuley. 2015. Capturing meaning in product re-
views with character-level generative text models.
arXiv preprint arXiv:1511.03683.

Bing Liu. 2015. Sentiment Analysis: Mining Opinions,
Sentiments, and Emotions. Cambridge University
Press.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Umar Maqsud. 2015. Synthetic text generation for
sentiment analysis. In Proceedings of the 6th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, pages 156–
161, Lisboa, Portugal. Association for Computa-
tional Linguistics.

Julian McAuley, Rahul Pandey, and Jure Leskovec.
2015. Inferring networks of substitutable and com-
plementary products. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’15, pages
785–794, New York, NY, USA. ACM.

Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-fine alignment. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 720–730, San Diego, California. Association
for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

631



Dae Hoon Park, Hyun Duk Kim, ChengXiang Zhai,
and Lifan Guo. 2015. Retrieval of relevant opinion
sentences for new products. In Proceedings of the
38th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
SIGIR ’15, pages 393–402, New York, NY, USA.
ACM.

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the difficulty of training recurrent
neural networks. In Proceedings of The 30th In-
ternational Conference on Machine Learning, pages
1310–1318.

Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and
Chris Manning. 2011. Parsing natural scenes and
natural language with recursive neural networks. In
Lise Getoor and Tobias Scheffer, editors, Proceed-
ings of the 28th International Conference on Ma-
chine Learning (ICML-11), ICML ’11, pages 129–
136, New York, NY, USA. ACM.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

T. Tieleman and G. Hinton. 2012. Lecture 6.5—
RmsProp: Divide the gradient by a running average
of its recent magnitude. Technical report.

Subhashini Venugopalan, Marcus Rohrbach, Jeffrey
Donahue, Raymond Mooney, Trevor Darrell, and
Kate Saenko. 2015. Sequence to sequence - video
to text. In The IEEE International Conference on
Computer Vision (ICCV), pages 4534–4542.

O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015a.
Show and tell: A neural image caption generator.
In 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 3156–3164.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015b. Gram-
mar as a foreign language. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 2773–2781. Curran Associates,
Inc.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In David Blei and Francis Bach, editors, Pro-
ceedings of the 32nd International Conference on
Machine Learning (ICML-15), pages 2048–2057.
JMLR Workshop and Conference Proceedings.

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2016. Attribute2image: Conditional image
generation from visual attributes. In European Con-
ference on Computer Vision, pages 776–791.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2015. Recurrent neural network regularization. In
International Conference on Learning Representa-
tions.

632


