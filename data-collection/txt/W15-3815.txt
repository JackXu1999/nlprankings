



















































Measuring the readability of medical research journal abstracts


Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015), pages 127–133,
Beijing, China, July 30, 2015. c©2015 Association for Computational Linguistics

Measuring the readability of medical research journal abstracts 
 

Samuel Severance 
University of Colorado 

School of Education 
Institute of Cognitive Science 

Boulder, CO USA 
severans@colorado.edu 

 
K. Bretonnel Cohen 
University of Colorado 

School of Medicine 
Biomedical Text Mining Group 

Aurora, CO USA 
kevin.cohen@gmail.com 

 
 

Abstract 

This study examines whether the readability 
of medical research journal abstracts changed 
from 1960 to 2010. Abstracts from medical 
journals were downloaded from PubMed.org 
in ten-year batches (1960s, 1970s, etc.). Ab-
stracts in each decade underwent processing 
via a custom Python script to determine their 
Coleman-Liau Index (CLI) readability score. 
Analysis using one-way ANOVA found statis-
tically significant differences between the 
mean CLI readability scores of each decade 
(F(4, 6689135) = 12936.91,p<0.0001). Post-
hoc analysis using Tukey’s method also found 
all pairwise comparisons between decades’ 
mean CLI readability scores to be statistically 
significant (p<0.001). Readability scores in-
creased from decade to decade beginning with 
a mean CLI score of 16.0813 in the 1960s and 
ending with a mean CLI score of 16.8617 in 
the 2000s. These results indicate a 0.7804 
grade level increase in the difficulty of read-
ing medical research journal abstracts over 
time and raises questions about the accessibil-
ity of medical research for broader audiences.  

1 Introduction 

A persistent issue in academic research centers on 
whether the knowledge published by researchers 
reaches and is understood by those it could benefit. 
The medical field takes up this issue in its efforts 
to translate research into practice, or the idea of 
“translational research” (Woolf, 2008). Ideally, 

practitioners can access and thoroughly compre-
hend research to better ensure new treatments and 
knowledge reaches patients and that patient care 
revolves around evidence-based practices (Pra-
vikoff, Tanner, & Pierce, 2005; Woolf, 2008). Be-
yond seeking to leverage new research among 
medical practitioners, translational research also 
focuses on supporting patients in becoming more 
active and involved in their healthcare (Woolf, 
2008). With the advent of the information age, pa-
tients and patients’ family members have substan-
tial opportunities to research their own medical 
conditions and their treatment options. Navigating 
and understanding medical research requires that it 
proves accessible in terms of its readability. 

This study is a diachronic analysis of the read-
ability of medical research. Specifically, this study 
seeks to answer whether the readability of medical 
research journal abstracts has changed from the 
1960s to the 2000s. Results from this study may 
have implications for how researchers could com-
municate their findings to patients and how to ad-
dress discrepancies between the reading level of 
medical journals and lay audiences’ reading abili-
ties. 

2 Relevant Literature 

2.1 Readability of health materials in relation 
to patients  

Research on the readability of health materials 
in relation to patients has a strong presence in the 
literature. Health literacy researchers have found 

127



that the vast majority of textual information pa-
tients typically encounter—from informed con-
sents to patient education materials—surpass the 
reading ability of patients (Rudd, Moeykens, & 
Colton, 1999). Such discrepancies may have pro-
found negative influences on patient health out-
comes (Paasche-Orlow & Wolf, 2010). Indeed, 
Baker et al. (1998) found an independent associa-
tion between low health literacy and increased 
hospital admission rates where patients with low 
literacy became hospitalized twice as often as more 
literate patients. Additionally, patients with high 
functional health literacy become more involved in 
their care, including exploring options beyond 
those presented by a doctor, whereas patients with 
low functional health literacy tend to limit deci-
sions regarding their care to only those presented 
to them by doctors (Smith et al. 2009). With impli-
cations for personal and community health, a study 
by Navarra et al. (2014) found that HIV-infected 
youth with below-grade-level reading skills did not 
completely adhere to their antiretroviral therapy. 

Despite growing evidence of the role of health 
literacy in patient outcomes, the readability of 
medical information for patients has not improved 
over time, even for items intended for patients. The 
lack of readability of informed consents, in particu-
lar, has garnered attention in the literature (Mead 
& Howser, 1992; Rudd et al., 1999). An examina-
tion of the readability of informed consents from 
1975 to 1982 at the Veterans Administration Med-
ical Center found them to have a college reading 
level and that their reading difficulty may have 
actually increased over the time period examined 
(Baker & Taub, 1983). Fifteen years later, a study 
of surgical consents from across the US also 
showed similarly difficult reading levels with a 
given consent requiring an average reading level of 
12.6 (Hopper et al., 1998). Beyond informed con-
sents, other materials directly aimed at laymen also 
show readability issues. In an analysis of emergen-
cy first-aid instructions, Temnikova (2012a, 
2012b) found ten separate categories of readabil-
ity/complexity problems. Alamoudi and Hong 
(2015) found the readability of websites related to 
microtia and aural atresia lacking in terms of facili-
tating comprehension.  

2.2 Identifying and addressing readability 
issues 

A significant body of work focuses on addressing 
readability issues in health contexts. It makes the 
significance of the corpus-based study reported 
here clear: it shows that we can address readability 
problems, but first we must know what the reada-
bility issues are.  

Elhadad (2006), for instance, shows which 
terms in a medical journal article a lay reader 
would likely not understood and presents an appli-
cation that finds these terms and mines an appro-
priate definition from the Web.  Achieving usable 
results with a small corpus, Elhadad and Sutaria 
(2007) presented a parallel-corpus-driven method 
for finding technical/lay equivalents of medical 
terms using measures of association. Leroy et al. 
(2010) pointed out that perceived and the actual 
difficulty of text influenced the willingness and 
ability to learn from health information. The re-
searchers manipulated characteristics of health 
texts and measured perceived and actual difficulty, 
and found they could improve the perceived diffi-
culty of text. Their technique also uncovered some 
problems with standard readability formulas. Using 
lexical and grammatical analysis of a medical cor-
pus to develop a new metric to estimate text diffi-
culty called “term familiarity,” Leroy et al. (2012) 
performed an experiment where individuals 
showed slightly improved understanding for sim-
plified documents. An evaluation of a writing as-
sistance tool that assists with automated 
simplification related to term familiarity found that 
simplified text had strong beneficial effects on 
both perceived and actual difficulty, with better 
understanding and more learning after reading 
simplified text than after reading un-simplified text 
(Leroy, Kauchak & Mouradi, 2013). In another 
study, Leroy et al. (2013) examined the effects of 
lexical simplification and coherence enhancement 
on readability and showed that they interact in 
complex ways with both perceived and actual dif-
ficulty. Investigating linguistic features, specifical-
ly discourse features that correlate with the 
readability of texts for adults with intellectual dis-
abilities, Fung et al. (2009) presented a tool for 
rating the readability of texts for these readers. 
Huenefaurth et al. (2009) compared different 
methods for evaluating text readability software for 
adults with intellectual disabilities, finding that 
multiple-choice questions with illustrations proved 
more useful than yes/no questions or Likert scales 
for evaluating simplification programs.  

128



2.3 Work presented in context of relevant 
literature 

Specific research utilizing a diachronic, corpus-
based approach to examining the readability of 
medical journals did not turn up in a review of the 
literature. However, previous studies taking a dia-
chronic approach to the readability of corpus data 
do have precedence. Indeed, the inspiration for this 
study comes from work by Štajner (2011). Štajner 
performed a diachronic analysis of the Brown 
“family” of corpora to examine changes in the 
readability of the English language over time. Sim-
ilar to this study, Štajner utilized the Coleman-Liau 
Index as a measure of the readability of the Brown 
“family” of corpora. 

3 Methodology 

This study occurred in three main phases in order 
to answer the research question: How has the read-
ability of medical journal abstracts changed be-
tween the 1960s and 2000s? 

3.1 Obtaining a medical research corpus 

The first phase of this study involved compiling a 
machine-readable corpus of medical research jour-
nal abstracts. PubMed.org contains a large volume 
of medical research journal abstracts and these 
provided the basis of a corpus. Abstracts were 
downloaded in groups by decade (see Table 1). 

 
This study focused solely on journal abstracts deal-
ing with research on human subjects with the as-
sumption that a human-centered research corpus 
has more meaningful parallels to the potential in-
terests of most patients.  

3.2 Measure the readability of abstracts 

The Coleman-Liau Index measure of readabil-
ity (Coleman & Liau, 1975) formula is as follows:  

𝐶𝐿𝐼 = 5.89
𝑐
𝑤 − 29.5

𝑠
𝑤 − 15.8 

 
In this formula, c is equal to the total number of 
characters in a given text, w is equal to the total 
number of words in a given text, and s is equal to 
the total number of sentences in a given text. The 
CLI outcome measure is given as a grade-level 
readability score. For example, a grade of 10.5 
would correspond to a text at a reading level of 
halfway through 10th grade.  

 
 
 
 
 
 
 
 
 
 
 

 
 
 
Figure 1: Distribution of CLI Scores by decade.  

3.3 Statistical analyses 

The next phase of the study involved creating a 
database and running analyses to determine the 
mean CLI scores for abstracts in each decade and 
whether the differences between these mean scores 
were statistically significant. A statistically signifi-
cant difference in the mean CLI scores for each 
decade would indicate changes in the readability of 
medical journal abstracts over time. 

In order to avoid type 1 errors, the analysis did 
not engage in a series of t-tests to compare the 
mean CLI scores for each decade. Rather a one-
way ANOVA was deemed more appropriate after 
checking that the data met certain assumptions. 
Specifically, ANOVA requires that the data have 
an approximately normal distribution. Evidence for 
normality includes histograms of each decade’s 
CLI scores with each distribution closely following 
a normal curve (see Figure 1 above). A Shapiro-
Wilk test for normality could not be done because 
it has an upper limit of 2,000 to 5000 observations 
(Razali & Wah, 2011), and the data sets in this pa-
per surpass that (see Table 1 above). However, 

Table 1. Number of abstracts by decade. 
Decade range Number of abstracts 
1960-1969 5324 
1970-1979 313053 
1980-1989 1049637 
1990-1999 2017482 
2000-2009 3327954 

129



examination of the quantile-quantile plots (Figure 
2 below) is consistent with the data being approx-
imately normally distributed in each decade with 
only a small fraction of overall observations dis-
playing deviations in the tails of some plots.  

 
Figure 2: Quantile-quantile plots, by decade. 
 

ANOVA also requires very similar variances 
for each group. Levene’s test for homogeneity of 
variance, run on subsets created through random 
sampling of each decade, gave statistically signifi-
cant values, which means the null hypothesis that 
the variances were the same could not be rejected. 
Another assumption for running ANOVA is that 
the data are independent. Although it is possible 
that some research articles may have been repub-
lished or had text cited in different decades, such 
instances likely were rare and not significant given 
the size of the corpus. 

With the above assumptions addressed, the one-
way ANOVA was carried out. Examination of the 
output indicated that a statistically significant dif-
ference did exist between the mean CLI scores for 
each decade. A one-way ANOVA, however, is an 
omnibus test and does not indicate between which 
groups the statistically significant difference exists, 
just that a statistically significant difference exists 

somewhere in the data. To determine between 
which decades there exists a statistically significant 
difference in mean CLI scores, a post hoc analysis 
using Tukey’s method was carried out. 

4 Results  

The mean CLI scores for each decade were calcu-
lated (see Table 2.) The 1960s had a mean CLI 
score of 16.0813 with a 95% confidence interval 
(CI) of 16.00567 to 16.1569. The 1970s had a 
mean CLI score of 16.3123 with a 95% CI of 
16.3024 to 16.32212. The 1980s had a mean CLI 
score of 16.3867 with a 95% CI of 16.38137 to 
16.39194. The 1990s had a mean CLI score of 
16.4302 with a 95% CI of 16.42657 to 16.43385. 
The 2000s had a mean CLI score of 16.8617 with a 
95% CI of 16.85901 to 16.86446. Note that none 
of the 95% CIs overlap between decades. 

A one-way ANOVA indicated a statistically 
significant difference between the mean CLI scores 
for each decade (F(4, 6689135) = 12936.91, 
p<0.0001; see table 3). In determining which pairs 
of mean CLI Scores for each decade had a statisti-
cally significant difference, a pairwise comparison 
of means post hoc analysis using Tukey’s method 
indicated that all possible combinations of CLI 
Scores for each decade had statistically significant 
differences (p<0.001). 

5 Analysis 

Having confirmed the statistical significance of the 
differences between all pairings of the mean CLI 
scores for each decade, we can consider the mean 
CLI scores for each decade statistically distinct 

Table 3. One-way ANOVA results comparing mean CLI scores by decade. 
Source SS df MS F-statistic p-value 
Between groups 353331.527 4 88332.8818   12936.91 <0.0001 
Within groups 45673229.4 6689135 6.82797244   
Total 46026560.9  6689139 6.88079003   

Table 2. Mean CLI scores by decade. 
Decade Mean CLI 

Score 
Number of  
Abstracts 

1960s 16.0813 5324 
1970s 16.3123 313053 
1980s 16.3867 1049637 
1990s 16.4302 2017482 
2000s 16.8617 3327954 

130



from one another. Given this, we can make higher-
level observations based on what patterns the indi-
vidual means reveal as part of a group. More im-
portantly, we can make assertions that allow us to 
answer our research question: How has the reada-
bility of medical journal abstracts changed between 
the 1960s and 2000s?  

According to the results of this study, the aver-
age difficulty in readability of medical research 
journal abstracts increased over time. Specifically, 
readability scores increased from decade to decade 
beginning with a mean CLI score of 16.0813 in the 
1960s and ending with a mean CLI score of 
16.8617 in the 2000s. The mean CLI score, there-
fore, increased 0.7804 grade level units within the 
timespan examined. We should also note the high 
mean CLI scores for each decade. All scores fell 
within the level of readability expected for a grade 
level of 16 or a senior in college. 

6 Future work 

The work reported here discusses only one reada-
bility metric. Fleshing out the data with additional 
readability metrics would prove useful.  Experi-
mental assessment of comprehension by lay read-
ers would be a useful addition to the metrics; for 
example, by asking them to read abstracts and an-
swer questions. Specific subdomains of the bio-
medical literature may have their own readability 
issues, such as formulae and gene names, and iden-
tifying these might have implications for ap-
proaches to addressing specific readability issues. 

7 Conclusion 

This study sought to determine whether the reada-
bility of medical research journal abstracts changed 
between the 1960s and 2000s. The results here in-
dicate an increase in difficulty of 0.7804 grade lev-
els during this time period. Medical journal 
abstracts, we can conclude, have become more and 
more difficult to read.   

For patients attempting to learn more about 
medical conditions or their treatment options 
through the reading primary literature, this task has 
become more difficult to achieve. Importantly, 
however, the high overall mean CLI scores for 
each decade indicate that this task likely has al-
ways proven difficult for patients. Medical journal 
abstracts have had readability scores equivalent to 

grade levels of 16 since the 1960s, well above the 
average American who reads between a 7th and 8th 
grade level (NCES, 2003) and certainly above the 
9th-grade level considered “difficult” (USDHHS, 
2000). This consistent difficulty mirrors other re-
search showing a lack of progress in the readability 
of medical-related text (Rudd et al., 1999). 

From this study’s results and the US Depart-
ment of Health and Human Services recommenda-
tions for the reading levels of medical information 
text, the readability gap between published medical 
research and the average American patient’s read-
ing ability appears equal to 7 grade levels. Bridg-
ing this chasm in accessibility will likely require 
interventions for both the researcher and patient. 
Shoring up the “health literacy” of Americans 
would involve a concerted effort to increase the 
average reading ability of patients. Purposefully 
addressing health literacy in K-12 education set-
tings and Adult Basic Education settings may 
prove beneficial (Nielsen-Bohlman et al., 2004; 
Rudd et al., 1999). Such efforts, however, will 
likely not bridge the 7 grade level gap entirely. 
Instead, the medical research community should 
consider taking steps—for example, developing 
reading guides or parallel publications aimed at lay 
readers—to increase the readability of their re-
search given patients’ information needs and to 
support patient self-advocacy.  

Despite a desire by patients to access and 
comprehend research that would increase their in-
volvement in their own care, members of the med-
ical research and publishing community continue 
to place a premium on complex writing skills put-
ting such research out of the reach of most patients. 
Lakoff (1992) makes a strong case for academics 
in general being rewarded for difficult writing, and 
perhaps even being published for incomprehensi-
ble writing. With typical reading levels of almost 
17, most scientific writing is now beyond the read-
ing level of not only the average patient but also 
most health professionals who typically have a 
bachelor’s degree equivalent to a grade level of 16. 

Acknowledgments 
We thank the participants in LING 5200, Compu-
tational Corpus Linguistics, in Fall 2014 for their 
input into this project. Noemie Elhadad and Gondy 
Leroy provided helpful comments on a late draft of 
the work. 

131



References  
Alamoudi, U., and Hong, P. (2015) Readability 

and quality assessment of websites related to 
microtia and aural atresia. International Jour-
nal of Pediatric Otorhinolaryngology 
79(2):151-156. 

Baker, M. T., & Taub, H. A. (1983). Readability of 
informed consent forms for research in a Vet-
erans Administration medical cen-
ter. Jama, 250(19), 2646-2648. 

Baker, D. W., Parker, R. M., Williams, M. V., & 
Clark, W. S. (1998). Health literacy and the 
risk of hospital admission. Journal of general 
internal medicine,13(12), 791-798. 

Coleman, M., & Liau, T. L. (1975). A computer 
readability formula designed for machine scor-
ing. Journal of Applied Psychology, 60(2), 
283. 

Elhadad, N. (2006) Comprehending technical 
texts: predicting and defining unfamiliar terms. 
American Medical Informatics Association 
Symposium Proceedings, pp. 239-243. 

Elhadad, N., and Sutaria, K. Mining a lexicon of 
technical terms and lay equivalents. BioNLP 
2007, pp. 49-56. 

Feng, L., Elhadad, N., and Huenefauth, M. (2009) 
Cognitively motivated features for readability 
assessment. EACL 2009, pp. 229-237. 

Hartley, J. (2004). Current findings from research 
on structured abstracts.Journal of the Medical 
Library Association, 92(3), 368. 

Hopper, K. D., TenHave, T. R., Tully, D. A., & 
Hall, T. E. (1998). The readability of currently 
used surgical/procedure consent forms in the 
United States.Surgery, 123(5), 496-503. 

Huenefauth, M., Feng, L, and Elhadad, N. (2009) 
Comparing evaluation techniques for text 
readability software for adults with intellectual 
disabilities. ACM SIGACCESS conference on 
computers and accessibility, pp. 3-10. 

Joint Commission. (2007) What did the doctor 
say? Improving health literacy to protect pa-
tient safety. Health Care at the Crossroads se-
ries. http://www.jointcommission.org/nr/rdonl 
yres/d5248b2e-e7e6-4121-887499c7b4888301 
/0/improving_health_literacy.pdf 

Lakoff, R.T. (1992) Talking power: the politics of 
language. Basic Books. 

Leroy, G., Endicott, J. E., Kauchak, D., Mouradi, O., & 
Just, M. (2013). User evaluation of the effects of a 

text simplification algorithm using term familiarity 
on perception, understanding, learning, and infor-
mation retention. Journal of medical Internet re-
search, 15(7). 

Leroy, G., Endicott, J. E., Mouradi, O., Kauchak, D., & 
Just, M. L. (2012). Improving perceived and actual 
text difficulty for health information consumers us-
ing semi-automated methods. In AMIA Annual 
Symposium Proceedings (Vol. 2012, p. 522). Amer-
ican Medical Informatics Association. 

Leroy, G., Helmreich, S., & Cowie, J. R. (2010). The 
influence of text characteristics on perceived and 
actual difficulty of health information. Internation-
al journal of medical informatics, 79(6), 438-449. 

Leroy, G., Kauchak, D., & Mouradi, O. (2013). A user-
study measuring the effects of lexical simplification 
and coherence enhancement on perceived and actu-
al text difficulty. International journal of medical 
informatics, 82(8), 717-730. 

Meade, C. D., & Howser, D. M. (1991, Decem-
ber). Consent forms: how to determine and 
improve their readability. In Oncology nursing 
forum (Vol. 19, No. 10, pp. 1523-1528). 

Navarra, A.M., Neu, N., Toussi, S., Nelson, J., & 
Larson, E.L. (2014) Health literacy and adher-
ence to antiretroviral therapy among HIV-
infected youth. J. Assoc. Nurses AIDS Care. 
25(3):203-213. 

National Center for Education Statistics (2003). 
National Assessment of Adult Literacy 
(NAAL). http://nces.ed.gov/naal.  

Pravikoff, D. S., Tanner, A. B., & Pierce, S. T. 
(2005). Readiness of US nurses for evidence-
based practice: many don’t understand or value 
research and have had little or no training to 
help them find evidence on which to base their 
practice. AJN The American Journal of Nurs-
ing, 105(9), 40-51. 

Razali, N. M., & Wah, Y. B. (2011). Power 
comparisons of Shapiro-Wilk , Kolmogorov-
Smirnov , Lilliefors and Anderson-Darling 
tests. Journal of Statistical Modeling and 
Analytics, 2(1), 21–33. 

Roter, D. L., Rudd, R. E., Keogh, J., & Robinson, 
B. (1986). Worker produced health education 
material for the construction 
trades. International Quarterly of Community 
Health Education, 7(2), 109-121. 

Rudd, R. E., Moeykens, B. A., & Colton, T. C. 
(1999). Health and literacy: a review of medi-
cal and public health literature. Office of Edu-
cational Research and Improvement. 

132



Smith, S. K., Dixon, A., Trevena, L., Nutbeam, D., 
& McCaffery, K. J. (2009). Exploring patient 
involvement in healthcare decision making 
across different education and functional 
health literacy groups. Social science & medi-
cine,69(12), 1805-1812. 

Štajner, S. (2011, September). Towards a Better 
Exploitation of the Brown 'Family’ Corpora in 
Diachronic Studies of British and American 
English Language Varieties. In RANLP Stu-
dent Research Workshop (pp. 17-24).  

Temnikova, I. (2012a) Improving emergency in-
structions.  Communicator (pp. 48-53). 

Temnikova, I. (2012b) Text complexity and text 
simplification in the crisis management do-
main.  University of Wolverhampton doctoral 
thesis. 

United States Department of Health and Human 
Services. (2000) Saying it clearly. 
http://www.talkingquality.gov/docs/section3/3
_4.htm.  

Weiss, B. D., Coyne, C., Michielutte, R., Davis, T. 
C., Meade, C. D., Doak, L. G., ... & Furnas, S. 
(1998). Communicating with patients who 
have limited literacy skills-Report of the Na-
tional Work Group on Literacy and 
Health.Journal of Family Practice, 46(2), 168-
176. 

Woolf, S. H. (2008). The meaning of translational  
research and why it matters. JAMA, 299(2), 
211-213. 

 

 

133


