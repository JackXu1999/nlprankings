



















































TARGER: Neural Argument Mining at Your Fingertips


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 195–200
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

195

TARGER: Neural Argument Mining at Your Fingertips

Artem Chernodub1,2, Oleksiy Oliynyk3, Philipp Heidenreich3, Alexander Bondarenko4,
Matthias Hagen4, Chris Biemann3, and Alexander Panchenko5,3

1Grammarly
2Faculty of Applied Sciences, Ukrainian Catholic University, Lviv, Ukraine

3Language Technology Group, Universität Hamburg, Hamburg, Germany
4Big Data Analytics Group, Martin-Luther Universität Halle-Wittenberg, Halle, Germany

5Skolkovo Institute of Science and Technology, Moscow, Russia

Abstract

We present TARGER, an open source neu-
ral argument mining framework for tag-
ging arguments in free input texts and for
keyword-based retrieval of arguments from
an argument-tagged web-scale corpus. The
currently available models are pre-trained on
three recent argument mining datasets and en-
able the use of neural argument mining with-
out any reproducibility effort on the user’s
side. The open source code ensures portabil-
ity to other domains and use cases, such as an
application to search engine ranking that we
also describe shortly.

1 Introduction

Argumentation is a multi-disciplinary field that ex-
tends from philosophy and psychology to linguis-
tics as well as to artificial intelligence. Recent de-
velopments in argument mining apply natural lan-
guage processing (NLP) methods to argumenta-
tion (Palau and Moens, 2011; Lippi and Torroni,
2016a) and are mostly focused on training classi-
fiers on annotated text fragments to identify argu-
mentative text units, such as claims and premises
(Biran and Rambow, 2011; Habernal et al., 2014;
Rinott et al., 2015). More specifically, current ap-
proaches mainly focus on three tasks: (1) detec-
tion of sentences containing argumentative units,
(2) detection of the argumentative units’ bound-
aries inside sentences, and (3) identifying relations
between argumentative units.

Despite vital research in argument mining, there
is a lack of freely available tools that enable users,
especially non-experts, to make use of the field’s
recent advances. In this paper, we close this gap
by introducing TARGER: a system with a user-
friendly web interface1 that can extract argumen-
tative units in free input texts in real-time using

1ltdemos.informatik.uni-hamburg.de/targer

models trained on recent argument mining corpora
with a highly configurable and efficient neural se-
quence tagger. TARGER’s web interface and API
also allow for very fast keyword-based argument
retrieval from a pre-tagged version of the Common
Crawl-based DepCC (Panchenko et al., 2018).

The native PyTorch implementation underlying
TARGER has no external depencies and is avail-
able as open source software:2 it can easily be in-
corporated into any existing NLP pipeline.

2 Related Work

There are three publicly available systems of-
fering some functionality similar to TARGER.
ArgumenText (Stab et al., 2018) is an argument
search engine that retrieves argumentative sen-
tences from the Common Crawl and labels them
as pro or con given a keyword-based user query.
Similarly, args.me (Wachsmuth et al., 2017) re-
trieves pro and con arguments from 300,000 ar-
guments crawled from debating portals. Finally,
MARGOT (Lippi and Torroni, 2016b) provides ar-
gument tagging for free-text inputs. However, an-
swer times of MARGOT are rather slow for single
input sentences (>5 seconds) and the F1 scores
of 17.5 for claim detection and 16.7 for evidence
detection are slightly worse compared to our ap-
proach (see Section 4.1).
TARGER offers a real-time retrieval functional-

ity similar to ArgumenText and fast real-time free-
text argument tagging with the option of switching
between different pre-trained state-of-the-art mod-
els (MARGOT offers only a single one).

3 Architecture of TARGER

The independent components of the modular and
flexible TARGER framework are shown in Fig-
ure 1. In an offline training step, a neural

2github.com/achernodub/targer

http://ltdemos.informatik.uni-hamburg.de/targer
https://github.com/achernodub/targer


196

TaggerTraining 
Datasets

labeled 
text data

Models
models

DepCC Indexer
text data

Index
text + tags

API
que

ry

texttext + tags

Web UI

text + model /
querytagged text / 

arguments with tags

text + model /
query

tagged text / 
arguments with tags

tex
t + 

tag
s

tag
s

tex
t da

ta

text
 + m

ode
l /

que
ry

tag
ged

 tex
t / 

arg
um

ent
s w

ith t
ags

Offline pre-processing Application

Figure 1: Modular architecture of TARGER. The central API is accessed through the Web UI or directly from any
application; it routes requests either to the tagging models or to the retrieval component. TARGER’s components
communicate via HTTP requests and can be deployed on different servers—in Docker containers or natively.

BiLSTM-CNN-CRF sequence tagger is trained on
different datasets yielding a variety of argument
mining models (details in Section 3.1). As part of
the preprocessing, the trained models are run on
the 14 billion sentences of the DepCC corpus to
tag and store argument unit information as addi-
tional fields in an Elasticsearch BM25F-index of
the DepCC (details in Section 3.2).

The online usage is handled via a Flask-based
web app whose API accepts AJAX requests from
the Web UI component or via API calls (details in
Sections 3.3 and 3.4). The API routes free text in-
puts to the respective selected model to be tagged
with argument information or it routes keyword-
based queries to the index to retrieve sentences in
which the query terms match argument units.

3.1 Neural Sequence Tagger

We implement a BiLSTM-CNN-CRF neural tag-
ger (Ma and Hovy, 2016) for identifying argumen-
tative units and for classifying them as claims or
premises. The BiLSTM-CNN-CRF method is a
popular sequence tagging approach and achieves
(near) state-of-the-art performance for tasks like
named entity recognition and part-of-speech tag-
ging (Ma and Hovy, 2016; Lample et al., 2016);
it has also been used for argument mining be-
fore (Eger et al., 2017). The general method re-
lies on pre-computed word embeddings, a sin-
gle bidirectional-LSTM/GRU recurrent layer, con-
volutional character-level embeddings to capture
out-of-vocabulary words, and a first-order Condi-
tional Random Field (Lafferty et al., 2001) to cap-
ture dependencies between adjacent tags.

Besides the existing BiLSTM-CNN-CRF im-
plementation of Reimers and Gurevych (2017), we

Essays WebD IBM

Claims 22,443 3,670 8,073,589
Premises 67,157 20,906 35,349,501
Major Claims 10,966 - -
Backing - 10,775 -
Refutations - 867 -
Rebuttals - 2,247 -
None 47,619 46,352 3,710,839

Combined 148,185 84,817 47,133,929

Table 1: Number of tokens per category in the training
datasets. Note that the IBM data contains many dupli-
cate claims; it was originally published as a dataset to
identify relevant premises for 150 claims.

also use an own Python 3.6 / PyTorch 1.0 im-
plementation that does not contain any third-party
dependencies, has native vectorized code for effi-
cient training and evaluation, and supports several
input data formats as well as evaluation functions.

The different argument tagging models cur-
rently usable through TARGER’s API are trained
on the persuasive essays (Essays) (Eger et al.,
2017), the web discourse (WebD) (Habernal and
Gurevych, 2017), and the IBM Debater (IBM)
(Levy et al., 2018) datasets (characteristics in
Table 1). The models use GloVe (Pennington
et al., 2014), fastText (Mikolov et al., 2018), or
dependency-based embeddings (Levy and Gold-
berg, 2014) (overview in Table 2).

For training, the following variations were
used for hyperparameter tuning: optimizer [SGD,
Adam], learning rate [0.001, 0.05, 0.01],
dropout [0.1, 0.5], number of hidden units in
recurrent layer [100, 150, 200, 250]. On all
datasets, GloVe embeddings, Adam with learning
rate of 0.001 and dropout rate of 0.5 performed



197

Data Embeddings Tagger

Essays fastText (Reimers and Gurevych, 2017)
Essays Dependency (Reimers and Gurevych, 2017)
Essays GloVe Ours

WebD fastText (Reimers and Gurevych, 2017)
WebD Dependency (Reimers and Gurevych, 2017)
WebD GloVe Ours

IBM fastText (Reimers and Gurevych, 2017)
IBM GloVe Ours

Table 2: Models currently deployed in TARGER.

best (hidden units: 200 on the persuasive essays,
250 on web discourse and IBM data).

3.2 Retrieval Functionality

Our background collection for the retrieval
of argumentative sentences is formed by the
DepCC corpus (Panchenko et al., 2018), a linguis-
tically pre-processed subset of the Common Crawl
containing 14.3 billion unique English sentences
from 365 million web documents.

The trained WebD-GloVe model was run on all
the sentences in the DepCC corpus since it per-
formed best on the web data in a pilot experiment.
The respective argumentative unit spans and labels
were added as additional fields to an Elasticsearch
BM25F-index of the DepCC.

3.3 TARGER API

To keep the TARGER framework modular and
scalable while still allowing access to the models
from external clients, online interaction is handled
via a restful API. Each trained model is associated
with a separate API endpoint accepting raw text
as input. The output is provided as a list of word-
level tokens with IOB-formatted labels for argu-
ment units (premises and claims) and the tagger’s
confidence scores for each label.

3.4 TARGER Web UI

The web interface of TARGER offers two function-
alities: Analyze Text and Search Arguments. On
the analysis tab (cf. Figure 2), the user can choose
one of the deployed models to identify arguments
in a user-provided free text. The result is shown
with colored labels for different types of argumen-
tative units (premises and claims) as well as de-
tected named entities (nested tags for entities in
argumentative units are supported). Once a result
is shown, it is possible to customize the display

Essays Web Discourse
Approach F1 Approach F1

STagBLCC 64.74 SVMhmm 22.90
TARGER (GloVe) 64.54 TARGER (GloVe) 24.20

Table 3: Comparison of TARGER’s performance on
the essays (Eger et al., 2017) and web discourse data
(Habernal and Gurevych, 2017) to the best approaches
from the original publications.

by enabling/disabling different labels without per-
forming additional tagging runs.

On the retrieval tab (cf. Figure 3), the user
can enter a keyword query and choose whether it
should be matched in claims, premises, etc. Every
retrieved result is rendered as a text fragment col-
orized with argument and entity information just
as on the analysis tab. To enable provenance, the
URL of the source document is also provided.

4 Evaluation

To demonstrate that our neural tagger is able to re-
produce the originally published argument mining
performances, we compare the best performing of
our pre-trained models (parameter settings at the
end of Section 3.1) to the best performances from
the original dataset publications. We also report
on a pilot study using TARGER as a subroutine in
runs for the TREC 2018 Common Core track.

4.1 Experimental Results
Table 3 shows a comparison of TARGER’s best
performing models (parameter settings at the end
of Section 3.1) on the Persuasive Essays and the
Web Discourse datasets to the best performance
in the original publications. We apply the exper-
imental settings of the original papers: a fixed
70/20/10 train/dev/test split on the Essays data,
and a 10-fold cross-validation for Web Discourse
(in our case allocating 7 folds for training and 2 for
development in each iteration).

On the Persuasive Essays dataset (paragraph
level), the best TARGER model achieves a span-
based micro-F1 of 64.54 for extracted argu-
ment components matching the best performance
of 64.74±1.97 reported by Eger et al. (2017) for
their STagBLCC approach (BiLSTM-CRF-CNN
approach (BLCC) similar to ours).

On the Web Discourse dataset, TARGER’s best
model’s token-based macro-F1 of 24.20 slightly
improves upon the originally reported best macro-
F1 of 22.90 (Habernal and Gurevych, 2017)



198

Figure 2: Analyze Text: input field, drop-down model selection, colorized labels, and tagged result.

Figure 3: Search Arguments: query box, field selectors, and result with link to the original document.

achieved by a structural support vector machine
model SVMhmm for sequence labeling (Joachims
et al., 2009). The SVMhmm model uses lexi-
cal, structural, and other handcrafted feature types.
In contrast, TARGER just uses word embeddings
since especially for cross-domain scenarios, hand-
crafted features show a strong tendency to overfit
on the topics of the training texts (Habernal and
Gurevych, 2017). Thus, we chose “word embed-
dings only” as a more robust feature type for our
domain-agnostic general-purpose argument min-
ing system (free input text and web data).

We cannot compare TARGER’s performance on
the IBM dataset to originally published perfor-
mances since the tasks are different. Instead of
TARGER’s identification of claims and premises,
Levy et al. (2018) focus on the identification of

relevant premises for a given claim (called “topic”
in the original publication). Still, a large num-
ber of potential general domain premises for the
overall 150 topics (i.e., claims) are contained in
the dataset, such that we transformed the original
entries to a token-level claim and premise anno-
tation. This way, only some 2500 distinct tokens
were labeled as not argumentative (e.g., punctua-
tion) while the vast majority are tokens in claims
and premises (but the only 150 different claims are
heavily duplicated).

Not surprisingly—given the class imbalance
and duplication—, the resulting trained TARGER
models “optimistically” identify some argumenta-
tive units in almost every input text. We still pro-
vide the models as a starting point with the inten-
tion to de-duplicate the data and to add more non-



199

Title / Query BM25F Axiomatic
Re-Ranking

declining middle class in u.s. 0.91 0.98 (+0.07)
euro opposition 0.81 1.00 (+0.19)
airport security 0.52 0.72 (+0.20)
law enforcement, dogs 0.43 0.63 (+0.20)

Table 4: The TREC 2018 Common Core track topics
with argument axiom re-ranked nDCG@10 improve-
ments > 0.05 over a BM25F baseline.

argumentative text passages for a more balanced /
realistic training scenario.

4.2 TARGER @ TREC Common Core Track

As a proof of concept, we used TARGER’s
model pre-trained on essays with dependency-
based embeddings in a TREC 2018 Common
Core track submission (Bondarenko et al., 2018).
The TARGER API served as a subroutine in a
pipeline axiomatically re-ranking (Hagen et al.,
2016) BM25F retrieval results with respect to
their argumentativeness (presence/absence of ar-
guments). For the Washington Post corpus used in
the track, the dependency-based essays model best
tagged argumentative units in a small pilot study.

Out of 25 topics manually labeled as argumen-
tative from the 50 Common Core track topics, the
TARGER-based argumentativeness re-ranking im-
proved the retrieval quality by > 0.05 nDCG@10
for 4 topics (see Table 4). Argumentativeness-
based re-ranking might thus be a viable way to in-
tegrate neural argument mining into the retrieval
process—for instance, using TARGER.

5 Conclusion

We have presented TARGER: an open source sys-
tem for tagging arguments in free text and for re-
trieving arguments from a web-scale corpus. With
the available RESTful API and the web interface,
we make the recent argument mining technolo-
gies more accessible and usable to researchers and
developers as well as the general public. The
different argument mining models can easily be
used to perform manual text analyses or can seam-
lessly be integrated into automatic NLP pipelines.
New taggers can be deployed to TARGER at any
time, so that users can have the state of the art
in argument mining at their fingertips. For future
work, we plan to integrate contextualized embed-
dings with ELMo- and BERT-based models (Pe-
ters et al., 2018; Devlin et al., 2018).

Finally, by looking at our experimental results
as well as tagging examples for free input texts or
the DepCC web data, we noticed that despite the
recent advances in argument mining, there is still
considerable headroom to improve in-domain, but
especially out-of-domain argument tagging per-
formance. Free input texts of different styles or
genres taken from the web are tagged very in-
consistently by the different models. More re-
search on domain adaptation and transfer learning
(Ruder, 2019) in the scenario of argument mining
needs to address this issue—and could then ideally
directly be deployed to TARGER as new models.

Acknowledgments

This work was partially supported by the German
Academic Exchange Service (DAAD) through
the short-term research grant 57314022 “Argu-
ment Mining in Web Documents using Orthog-
onal Simple Recurrent Networks” and by the
Deutsche Forschungsgemeinschaft (DFG) through
the project “ACQuA: Answering Comparative
Questions with Arguments” (grants BI 1544/7-
1 and HA 5851/2-1) as part of the priority pro-
gram “RATIO: Robust Argumentation Machines”
(SPP 1999).

References
Or Biran and Owen Rambow. 2011. Identifying Justifi-

cations in Written Dialogs. In Proceedings of the 5th
IEEE International Conference on Semantic Com-
puting (ICSC 2011), pages 162–168.

Alexander Bondarenko, Michael Völske, Alexander
Panchenko, Chris Biemann, Benno Stein, and
Matthias Hagen. 2018. Webis at TREC 2018: Com-
mon Core Track. In 27th International Text Re-
trieval Conference (TREC 2018).

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. CoRR, abs/1810.04805.

Steffen Eger, Johannes Daxenberger, and Iryna
Gurevych. 2017. Neural end-to-end learning for
computational argumentation mining. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 11–22.

Ivan Habernal, Judith Eckle-Kohler, and Iryna
Gurevych. 2014. Argumentation Mining on the Web
from Information Seeking Perspective. In Proceed-
ings of the Workshop on Frontiers and Connections
between Argumentation Theory and Natural Lan-
guage Processing, pages 26–39.

https://core.ac.uk/download/pdf/161443763.pdf
https://core.ac.uk/download/pdf/161443763.pdf
https://trec.nist.gov/pubs/trec27/papers/Webis-CC.pdf
https://trec.nist.gov/pubs/trec27/papers/Webis-CC.pdf
https://arxiv.org/abs/1810.04805
https://arxiv.org/abs/1810.04805
https://arxiv.org/abs/1810.04805
https://doi.org/10.18653/v1/P17-1002
https://doi.org/10.18653/v1/P17-1002
http://ceur-ws.org/Vol-1341/paper4.pdf
http://ceur-ws.org/Vol-1341/paper4.pdf


200

Ivan Habernal and Iryna Gurevych. 2017. Argumen-
tation Mining in User-Generated Web Discourse.
Computational Linguistics, 43(1):125–179.

Matthias Hagen, Michael Völske, Steve Göring, and
Benno Stein. 2016. Axiomatic Result Re-Ranking.
In 25th ACM International Conference on Infor-
mation and Knowledge Management (CIKM 2016),
pages 721–730.

Thorsten Joachims, Thomas Finley, and Chun-Nam J.
Yu. 2009. Cutting-plane training of structural svms.
Machine Learning, 77(1):27–59.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Label-
ing Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing (ICML 2001), pages 282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
302–308.

Ran Levy, Ben Bogin, Shai Gretz, Ranit Aharonov, and
Noam Slonim. 2018. Towards an Argumentative
Content Search Engine Using Weak Supervision. In
Proceedings of the 27th International Conference on
Computational Linguistics, COLING, pages 2066–
2081.

Marco Lippi and Paolo Torroni. 2016a. Argumenta-
tion Mining: State of the Art and Emerging Trends.
ACM Trans. Internet Techn., 16(2):10:1–10:25.

Marco Lippi and Paolo Torroni. 2016b. MARGOT: A
Web Server for Argumentation Mining. Expert Syst.
Appl., 65:292–303.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional LSTM-CNNs-
CRF. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1064–1074.

Tomas Mikolov, Edouard Grave, Piotr Bojanowski,
Christian Puhrsch, and Armand Joulin. 2018. Ad-
vances in Pre-Training Distributed Word Represen-
tations. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation,
LREC 2018, pages 52–55.

Raquel M. Palau and Marie-Francine Moens. 2011.
Argumentation Mining. Artif. Intell. Law, 19(1):1–
22.

Alexander Panchenko, Eugen Ruppert, Stefano Far-
alli, Simone P. Ponzetto, and Chris Biemann.
2018. Building a Web-Scale Dependency-Parsed
Corpus from CommonCrawl. In Proceedings of the
Eleventh International Conference on Language Re-
sources and Evaluation, LREC 2018, pages 1816–
1823.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of LSTM-networks for sequence tagging. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
338–348.

Ruty Rinott, Lena Dankin, Carlos Alzate Perez,
Mitesh M. Khapra, Ehud Aharoni, and Noam
Slonim. 2015. Show me your evidence - an auto-
matic method for context dependent evidence detec-
tion. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing,
pages 440–450.

Sebastian Ruder. 2019. Neural Transfer Learning for
Natural Language Processing. Ph.D. thesis, Na-
tional University of Ireland, Galway.

Christian Stab, Johannes Daxenberger, Chris Stahlhut,
Tristan Miller, Benjamin Schiller, Christopher
Tauchmann, Steffen Eger, and Iryna Gurevych.
2018. ArgumenText: Searching for arguments in
heterogeneous sources. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Demon-
strations, pages 21–25.

Henning Wachsmuth, Martin Potthast, Khalid Al-
Khatib, Yamen Ajjour, Jana Puschmann, Jiani Qu,
Jonas Dorsch, Viorel Morari, Janek Bevendorff, and
Benno Stein. 2017. Building an argument search en-
gine for the web. In Proceedings of the 4th Work-
shop on Argument Mining, pages 49–59.

https://www.aclweb.org/anthology/J17-1004
https://www.aclweb.org/anthology/J17-1004
https://dl.acm.org/citation.cfm?id=2983704
https://dl.acm.org/citation.cfm?id=1613006
https://dl.acm.org/citation.cfm?id=655813
https://dl.acm.org/citation.cfm?id=655813
https://dl.acm.org/citation.cfm?id=655813
https://doi.org/10.18653/v1/N16-1030
https://doi.org/10.3115/v1/P14-2050
https://doi.org/10.3115/v1/P14-2050
https://www.aclweb.org/anthology/C18-1176
https://www.aclweb.org/anthology/C18-1176
https://dl.acm.org/citation.cfm?id=2850417
https://dl.acm.org/citation.cfm?id=2850417
https://www.sciencedirect.com/science/article/pii/S0957417416304493
https://www.sciencedirect.com/science/article/pii/S0957417416304493
https://doi.org/10.18653/v1/P16-1101
https://doi.org/10.18653/v1/P16-1101
https://doi.org/10.18653/v1/P16-1101
http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf
https://dl.acm.org/citation.cfm?id=1568246
https://www.aclweb.org/anthology/L18-1286
https://www.aclweb.org/anthology/L18-1286
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/D17-1035
https://doi.org/10.18653/v1/D17-1035
https://doi.org/10.18653/v1/D17-1035
https://doi.org/10.18653/v1/D15-1050
https://doi.org/10.18653/v1/D15-1050
https://doi.org/10.18653/v1/D15-1050
http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf
http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf
https://doi.org/10.18653/v1/N18-5005
https://doi.org/10.18653/v1/N18-5005
https://doi.org/10.18653/v1/W17-5106
https://doi.org/10.18653/v1/W17-5106

