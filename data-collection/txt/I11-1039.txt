















































Predicting Opinion Dependency Relations for Opinion Analysis


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 345–353,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Predicting Opinion Dependency Relations for Opinion Analysis 

 
Lun-Wei Ku 

National Yunlin University 
 of Science and Technology 

Douliou, Yunlin 64002, Taiwan 
lwku@yuntech.edu.tw 

Ting-Hao Kenneth Huang
National Taiwan University
No.1, Sec.4, Roosevelt Rd., 

 Taipei, Taiwan 
tinghaoh@andrew.cmu.edu 

Hsin-Hsi Chen 
National Taiwan University
No.1, Sec.4, Roosevelt Rd.,

 Taipei, Taiwan 
hhchen@ntu.edu.tw 

 

Abstract 

Syntactic structures have been good fea-
tures for opinion analysis, but it is not easy 
to use them. To find these features by su-
pervised learning methods, correct syntac-
tic labels are indispensible. Two possible 
sources to acquire syntactic structures are 
parsing trees and dependency trees. For the 
annotation processing, parsing trees are 
more readable for annotators, while depen-
dency trees are easier to use by programs. 
To use syntactic structures as features, this 
paper tried to annotate on human friendly 
materials and transform these annotations 
to the corresponding machine friendly ma-
terials. We annotated the gold answers of 
opinion syntactic structures on the parsing 
tree from Chinese Treebank, and then pro-
posed methods to find their corresponding 
dependency relations on the dependency 
trees generated from the same sentence. 
With these relations, we could train a mod-
el to annotate opinion dependency relations 
automatically to provide an opinion depen-
dency parser, which is language indepen-
dent if language resources are incorporated. 
Experiment results show that the annotated 
syntactic structures and their corresponding 
dependency relations improve at least 8% 
of the performance of opinion analysis. 

1 Introduction 
Opinion analysis has drawn much attention in 
research communities of machine learning and 
natural language processing. In the early stages, 
words in documents were used as the main 
features (Pang et al., 2002). Some opinion dic-
tionaries were created for this demand (Ku et 
al., 2007). However, researchers soon realized 
that word features were not sufficient for ac-
quiring good performances, so they started to 
include syntactic structures and semantic in-

formation (Qiu et al., 2008). Their researches 
showed that linguistic knowledge is helpful in 
determining opinions.  

For various applications related to opinions, 
syntactic structures have become powerful 
tools for extracting useful clues. To find opi-
nions in product reviews, modification rela-
tions were used to identify the product and 
their features (Lu et al., 2009), e.g., a good 
price (feature) of this camera (product). To 
find opinion holders and targets, templates and 
linguistic rules were adopted (Breck et al., 
2007). To find more opinion words, dependen-
cy relations were utilized (Qiu et al., 2011). 
Even when applying the basic negation rule 
that flips opinion polarity over, we need to find 
its modified word first by syntactic clues. 
However, we will show that syntactic relations 
do not directly suggest opinions. 

Syntactic relations are obtained usually from 
all kinds of syntax trees. Parsing trees (phrase 
structured) and dependency trees (grammatical) 
are the most commonly seen ones. Parsing 
trees are in-order trees which keep the order of 
words in sentences, so they are more readable 
for people. Instead, nodes in dependency trees 
are displayed by the head-modifier relations, in 
which the sentence sequence probably is not 
remained. People could find the opinion pas-
sages if they can understand the whole sen-
tence, i.e. from parsing trees. However, when 
the linguistic background is needed, it could be 
difficult for most people to reconstruct the 
whole sentence from the dependency trees in 
order to find the opinion passage. Therefore, if 
we want to find annotators to build a corpus 
which could be used to train an opinion rela-
tion recognizer, parsing trees are the better ma-
terials compared to dependency trees. Howev-
er, compared to relations between words, com-
plicated tree structures are more challenge to 
be utilized by algorithms (Doan et al., 2008).  

345



This paper focuses on extracting opinio-
nated dependency relations from relations gen-
erated by the Stanford parser. We design an 
annotation mechanism on the syntactic struc-
tures on the sentence from Chinese Treebank 
to create an annotation environment with a 
lower entry barrier so that sufficient annota-
tions can be labeled. Then these annotations 
are aligned to the relations in the correspond-
ing dependency trees generated by the same 
parser from the same sentence as the gold 
standard for training the automatic annotator of 
the opinion dependency relations.  We conduct 
experiments on the annotated opinion syntactic 
structures in parsing trees, and on the opinion 
dependency relations corresponding to them. 
The proposed process demonstrates a feasible 
direction toward the development of an opi-
nion dependency parser. 

2 Problem Definition 
Given a set of non-collapsed dependencies 
parsed from a specific sentence by the Stanford 
dependency parser (de Marneffe and Manning, 
2008; Chang et al., 2009), each associated with 
a dependency relation between two words in 
this sentence, our goal is to identify which of 
them are with sentiment, i.e., those which 
reveal a part of opinions or the aroused 
emotions. For example, in the sentence “活动 
取得 了 圆满 成功 (Activities scored le perfect 
success)”, the Stanford dependency parser 
gives three relations: nmod(成功 <success>, 圆
满 <perfect>), nsubj( 取 得  <scored>, 活 动
<activities>), dobj(取得 <scored>, 成功<success>), 
and asp(取得 <scored>, 了<le>). The goal is to 
identify the former three may bear sentiment or 
opinions. The corresponding dependency tree is 
shown in Figure 1. From Figure 1 we can also see 
that it is not easy to read the original sentence 
without the linguistic background. 

 
Figure 1. A sample dependency tree with three 

aligned opinion dependency relations. 

Formally, the collection of the non-
collapsed dependency relations of a sentence S, 

generated by the Stanford dependency parser, 
is denoted by Rdep(S) = {r1, r2, …}, where 
each )(SRdepri   is associate with an opi-
nion judgment of op(r).  

Definition: Dependency Relation The de-
pendency relation r, generated by the Stanford 
parser, is composed of the type of relation rel, 
the head word wh and the modifier word wm in 
the form of rel(wh, wm). wh and wm are two in-
dividual words in S. For example, in one rela-
tion in Figure 1, r = nmod(成功 <success>, 圆满
<perfect>), where rel = nmod, wh =成功 <suc-
cess>, and wm =圆满<perfect>. A list of rel is 
available in Stanford Parser Manual (de Mar-
neffe and Manning, 2008; Chang et al., 2009). 

Definition: Opinion Judgment The opi-
nion judgment op(r), generated by the pro-
posed system, indicates whether the corres-
ponding dependency relation r is opinionated, 
and  falsetruerop ,)(  . For example, when r 
= nmod(成功 <success>, 圆满<perfect>), op(r) = 
true. 

Definition: Gold Opinion Judgment The 
gold opinion judgment, generated by mapping 
from manually annotated data, indicates 
whether the corresponding dependency rela-
tion r is opinionated, and  falsetruergop ,)(  . 

The gold answers come from the annota-
tions on Chinese Treebank 5.1. In a parsing 
tree T of the sentence S, generated by the Stan-
ford parser, an in-ordered set of tree nodes O = 
{o1, o2, …} is used to draw a parsing tree for 
the annotation process, and its corresponding 
order, i.e., its index, is used as the node ID to 
record the annotations. 

The way we annotate an opinion relation on 
a parsing tree is annotating an opinion trio (Ku 
et al., 2009). An opinion trio tri = 

TritoootriID rightleftparent ),,,,( is a structure con-
taining a left node oleft and a right node oright in 
a parsing tree, between them there is a syntac-
tic inter-word relation Rptt , and a nearest 
parent node oparent of these two nodes. Rpt is an 
inter-word relation set where Rpt {Substan-
tive-Modifier, Subjective-Predicate, Verb-
Object, Verb-Complement, Other}. A sample 
parsing tree and opinion trios within the sen-
tence in Figure 1 are shown in Figure 2. The 
literal output of opinion trios are shown in 

346



Figure 3. In the trio tri = (3, NP-OBJ, 圆满, 成功, 
Substantive-Modifier),  triID = 3, oparent  = NP-
OBJ, oleft =圆满 (perfect), oright =成功(success), 
and t = Substantive-Modifier. 

 
Figure 2. A sample parsing tree with trios. 

1, IP, 活动, VP, Subjective-Predicate 
2, VP, 取得, NP-OBJ, Verb-Object 
3, NP-OBJ, 圆满, 成功, Substantive-Modifier 

Figure 3. Opinion trios 

Note that because the annotation of trios is 
on nodes of parsing trees, which appear in-
orderly, oleft will always appear before oright in a 
sentence, and keeping this in mind will help 
understand the meaning of each inter-word 
relation t.  

Now for the sentence S, we have its parsing 
tree T, the annotated opinion trios Tri(S) on it, 
and its dependency relations Rdep(S). The next 
step is to mark the op(r) on Rdep(S) according 
to its corresponding Tri(S). For each trio tri, if 
any descendent of its left node oleft and any 
descendent of its right node oright together build 
a relation )(SRdepr , the opinion judgment 
of gop(r) of the relation r is set to true. Other-
wise, gop(r) is set to false. Now we have gop(r) 
for each r in Rdep(S), our goal is to find good 
methods to generate op(r) so that it can predict 
gop(r) as precisely as possible. We propose 
methods to achieve this goal in Section 3. 

3 Methods 
As mentioned, our goal is to predict opinion 
dependency relations as precisely as possible. 
However, to use more readable materials, opi-
nion trios are first annotated on Chinese Tree-
bank 5.1, and then they are mapped to the cor-
responding dependency relations. Before the 
aligning process, we use the annotated trios for 
training to predict the opinion trios in Section 
3.1. Using these predict trios for opinion anal-
ysis shall show the performance before the 
aligning process. After that, the aligned depen-

dency relations, i.e., the gold opinion depen-
dency relations are adopted for training to pre-
dict the opinion dependency relations in Sec-
tion 3.2. Because the parsing tree and the de-
pendency tree are generated by the same parser, 
we can always align them by the provided 
word ID numbers. 

After prediction, the opinion dependency re-
lations are available, and they can provide ne-
cessary information for many applications. 
However, we go one step further to test wheth-
er they benefit the opinion analysis. To fulfill 
this purpose, a basic method which uses the 
opinion dependency relations to extract opi-
nionated sentences and determine their polari-
ties is proposed in Section 3.3. 

3.1 Predicting Opinion Trios 
We predict the opinion trios by the sequential 
labeling model Conditional Random Field 
(CRF, Lafferty et al., 2001).  In a parsing tree, 
the tag of the internal node is the syntactic 
structure of its sub-tree, and the tag of the leaf 
node contains its part of speech and the content 
word. For each node, tags of its first four 
children (the first level), first four children of 
them (the second level), and their three 
children are used as features of this node. 
Features of its siblings (the window size is five) 
are considered, too. 

The labels l we would like the CRF to pre-
dict labels for each node, which are N or labels 
of the form t-C, where Rptt , },{ RLC  , L 
indicates that the current node is oleft in some 
opinion trio and R indicates oright. The label N 
indicates that the current node does not belong 
to any Tritri . The cardinality of the set Rpt 
is five, so that a total of 11 labels are used in 
CRF. CRF++1 is selected for experiments. 

3.2 Predicting Opinion Dependency Rela-
tions 

After aligning the opinion trios to the depen-
dency relations, we will have gop(r) for each 
one of them. In the previous research, usually 
only some relations were selected for opinion 
analysis. No statistical numbers showed the 
connection between the dependency relations 
and the opinions. We believe that it is because 

                                                 
1 http://crfpp.sourceforge.net/ 

347



the opinion annotation on dependency relations 
is more difficult than on words, sentences, or 
documents. However, because of this align-
ment, we are able to see the distribution of dif-
ferent dependency relations in opinion sen-
tences and opinion segments (opinion trios). 
We then predict opinion dependency relations 
based on these distributions: the op(r) of the 
relation r is set to true when its corresponding 
gop(r) appears massively frequently to be true 
in opinion sentences. To make this method 
reasonable, the assumption that there are no 
opinion trios in non-opinionated sentences 
must hold. A similar assumption that there are 
no opinion segments in non-opinionated sen-
tences was made when annotating the NTCIR 
MOAT corpus, too (Seki et al., 2008). Under 
this assumption, the relation that is in most 
case opinionated in opinion sentences is also in 
most case opinionated in all sentences. We 
believe that this assumption holds because in-
tuitively if there is an opinion segment in one 
sentence, this sentence should be opinionated. 

3.3 Using Syntactic Information for Opi-
nion Extraction 

In the previous research, relations were usually 
extracted automatically and then were used in 
various applications. As these relations are 
available after the prediction (or alignment) 
and as our purpose is to provide easy to use 
opinion dependency relations for further appli-
cations, we simply design rules for these rela-
tions in opinion analysis to show the baseline 
enhancement of using them. 

3.3.1 Using opinion trios 
In the past, Ku et al. (2009) have conducted 
rule based experiments for opinion trios. They 
designed formula for trios of each Rptt . 
Therefore, we adopted their rules on our aug-
mentative experiment materials. We define the 
opinion scoring function S(.), and its output 
opinion score varies with the input variables. 
These rules are shown by trio types as follows.  
 Substantive-Modifier Type: oleft of this trio 

type modifies oright, so that the trio’s opinion 
weight comes from the absolute opinion 
score of oleft, while the opinion polarity is de-
termined by the occurrence of negative oleft 

or oright. If at least one of them is negative, 
the trio is negative, else it is positive.   

)()()( else

)( 1- )( else      

 )(  )( then  )0)(  and  0)(( if      

 then)0)(  and  0)(( if

rightleftrightleft

leftrightleft

leftrightleftrightleft

rightleft

oSoSooS
oSooS

oSooSoSoS
oSoS









(1)

      
 

 Subjective-Predicate Type: oleft of this trio 
type is a subject and oright is the action it per-
forms, so that the action decides the opinion 
score of the trio.  If the action is not an opi-
nion or it is neutral, the subject determines 
the opinion score of this trio. 

)()( else

 )()( then )0)(( if

leftrightleft

rightrightleftright

oSooS
oSooSoS



  (2)
 

 Verb-Object Type: oleft of this trio type acts 
upon oright.  The effect depends not only on 
the action but on the target.  The weight is 
determined by the action, but the polarity is 
the multiplication of the signs of opinion 
scores of oleft and oright.  

)()()( else    

))(())(()()(    then  

)0)(  and  0)(( if

rightleftrightleft

rightleftleftrightleft

rightleft

oSoSooS
oSSIGNoSSIGNoSooS

oSoS







(3)

 

 Verb-Complement Type: The scoring func-
tion for trios of this type is defined the same 
as that of a Subjective-Predicate type in 
Formula (2).  The complement node is the 
deciding factor of the opinion score. 

3.3.2 Using opinion dependency relations 
The usages of opinion dependency relations 
were seen in several researches (Bikel and 
Castelli, 2008). In these researches, rules for a 
small number of major dependency relations 
were proposed in different papers but they 
were not listed together for a better utilization. 
Some rules were not ever mentioned in per-
vious researches. Instead, all relations are ana-
lyzed in this paper. For each relation r of 
which gop(r) equals true (when gold opinion 
relations are used for opinion analysis) or op(r) 
equals true (when predicted opinion relations 
are used for opinion analysis), we calculate its 
opinion score ops(r). Let RM(w) be a function 
to return the dependency relations of word w’s 
modifiers one at a time, n is the total number 
of relations RM(w) returns, and S(.) is also the 
defined opinion scoring function then ops(r) is 
defined as in Formula (4). 

348



 ))((1),,()( mmh wRMopsnwwrelSrops
 (4)

That is, the opinion score of a dependency 
relation is an average of the aggregate scores 
of its descendent dependency relations. In 
practice, we design different rules for calculat-
ing opinion scores by the current relation type 
rel in S(.). Here to simplify the problem, we 
adopted Formula (1) and treated wm as oleft and 
wh as oright in it. 

4 Experiments 
Though there were researches which predicted 
opinion dependency relations, they did not 
predict directly from the parsing results. In-
stead, they predicted from documents or sen-
tences according to the context and a large 
quantity of training instances were needed. 
They did not predict on all dependency rela-
tions either. Therefore, there is no existing da-
taset containing correct opinion labels on de-
pendency relations. In this section, we describe 
how to generate opinionated syntactic dataset 
on parsing trees, and align the annotated labels 
to dependency trees. After that, qualitative and 
quantitative analyses of opinion dependency 
relations are provided. At the end, we discuss 
the evaluation results of the proposed methods. 

4.1 Data Set and Preprocessing 
To use the Stanford parser as our tool to gener-
ate dependency tree for experimental sentences 
and to avoid errors as possible, we adopted 
Chinese Treebank 5.1 as experiment materials. 
Sentences in Chinese Treebank are already 
segmented and part of speech tagged, and its 
tagging set is the same with the one Stanford 
parser uses. Therefore, the Stanford parser can 
take the data from the Chinese Treebank to 
generate more accurate dependency trees.  

The dataset Chinese Treebank 5.1 contains 
507,222 words, 824,983 Hanzi, 18,782 sen-
tences, and 890 data files. For the opinion 
analysis experiments, opinionated labels, i.e., 
opinionated, non-opinionated, positive, neutral, 
negative, were annotated on all sentences in 
Chinese Opinion Treebank. Afterward 57,706 
trios were annotated on the parsing trees of 
gold opinion sentences, i.e., sentences which 
were annotated as opinionated. Methods for 

generating the gold opinion sentences pro-
posed by Ku et al. (2007) were adopted. 

Next, the Stanford parser took all sentences 
in Chinese Treebank as input to generate their 
dependency trees. A total of 416,581 depen-
dency relations were generated, and 284,590 of 
them were in opinion sentences. Then the an-
notated trios were aligned to their correspond-
ing dependency relations, and because trios 
were only annotated on opinionated sentences, 
the gop(r) of these aligned relations were set to 
true. At the end, a total of 54,753 relations 
gop(r) were set to true. 

 Opinion 
Polarity Positive Neutral Negative 

# 6,916 1,824 1,937 
% 64.78 17.08 18.14 

Total # 10,677 
Total % 56.84 

Table 1. Statistics of opinions. 
Rpt Number Percentage 

Substantive-Modifier 21,317 36.94  
Subjective-Predicate 15,860 27.48  

Verb-Object 18,010 31.21  
Verb-Complement 1,208 2.09  

Other 1,311 2.27  
Total 57,706 100.00  

Table 2. Statistics of structural trios. 
Table 1 shows the distribution of the opi-

nion and polarity labels. Table 2 shows the 
statistics of trios. Trios of the Substantive-
Modifier and Verb-Object types are the ma-
jority in opinion sentences, while trios of the 
Verb-Complement type are few. 

Table 3 further shows the distribution of de-
pendency relations. It shows that previously 
the most adopted dependency relations for 
opinion analysis, e.g., amod (adjective modifi-
er) or advmod (adverb modifier), do not cer-
tainly bear opinions or appear in opinion sen-
tences. In Section 4.3, we will further test the 
performance of finding the opinionated rela-
tions with the help of the opinion word dictio-
nary, which was also widely adopted by pre-
vious work (Feng et al., 2009). 

4.2 Evaluation of Opinion Trio Prediction 
In this section, results of predicting opinion 
trios by CRF mentioned in Section 3.1 are 
shown. We first predicted the appearance of 

349



oleft and oright in trios, and then predicted the 
trio type Rptt  for each trio. 

The performance in Table 4 is not promising. 
Therefore, we consider the structure of trios, 
that is, oleft and oright should appear as an or-
dered pair, and otherwise the label was viewed 
as illegal. The performance is shown in Table 
5. Table 5 shows that all predicted trios were 
opinionated, and this tells that some opinion 

trios are of certain structures, but not all of 
them. We observed that the precisions 1.00 
came from the collocations of specific words 
and structures, while the low recalls were from 
other trios which were not identified. However, 
these results still confirmed that we can find 
opinion trios by phrase structures and they may 
benefit in the opinion analysis process. 

A B C D E (%) F (%) A B C D E (%) F (%)
Dvpmod 590 501 413 84.92 82.44 Attr 3,869 2,666 140 68.91 5.25

Pass 560 399 224 71.25 56.14 Pobj 12,285 8,067 322 65.67 3.99
Dobj 32,949 24,294 13,192 73.73 54.30 clmpd 2,343 1,902 69 81.18 3.63

Npsubj 137 84 41 61.31 48.81 tcomp 2,839 1,588 53 55.94 3.34
Ba 757 575 263 75.96 45.74 Nmod 60,335 37,476 1,194 62.11 3.19

Top 2,256 1,458 661 64.63 45.34 Asp 4,176 2,889 79 69.18 2.73
Nsubj 36,902 26,102 11,058 70.73 42.36 numod 14,264 7,643 187 53.58 2.45
Neg 2,982 2,699 1,143 90.51 42.35 Clf 7,998 4,635 94 57.95 2.03

Amod 12,425 8,177 3,376 65.81 41.29 Dvpm 642 544 11 84.74 2.02
Rcmod 14,823 10,452 4,079 70.51 39.03 partmod 1,328 1,039 19 78.24 1.83
Rcomp 1,341 934 306 69.65 32.76 Det 6,021 4,083 74 67.81 1.81

Advmod 34,058 26,184 7,845 76.88 29.96 ordmod 1,220 553 10 45.33 1.81
Mmod 5,752 4,908 1,405 85.33 28.63 prnmod 770 320 5 41.56 1.56
Range 2,816 946 269 33.59 28.44 plmod 3,482 2,381 12 68.38 0.50

Assmod 12,365 9,106 1,669 73.64 18.33 Cc 7,462 5,031 17 67.42 0.34
Ccomp 40,712 31,338 4,377 76.97 13.97 Lobj 6,205 4,126 11 66.49 0.27
Vmod 866 613 79 70.79 12.89 Conj 11,414 6,967 17 61.04 0.24
Dep 17,295 8,222 887 47.54 10.79 Cpm 12,586 9,262 16 73.59 0.17

Xsubj 1,514 1,230 114 81.24 9.27 Assm 12,488 9,182 9 73.53 0.10
Comod 755 533 44 70.60 8.26 tclaus 1,583 1,140 1 72.02 0.09
Lccomp 3,102 2,063 155 66.51 7.51 Etc 1,164 663 0 56.96 0.00

Cop 625 519 37 83.04 7.13 xcomp 114 84 0 73.68 0.00
Prep 16,395 11,001 776 67.10 7.05 acomp 16 11 0 68.75 0.00

Table 3. Distributions of dependency relations and opinion dependency relations. 
(A: type of dependency relations (rel); B: total occurrences in generated dependency trees; C: total occurrence in gen-
erated dependency trees of opinions; D: total occurrence in generated dependency trees of opinions when it bears opi-
nions (gop(r) equals true); E: percentage that this relation appears in generated dependency trees of opinions; F: per-
centage that this relation appears in generated dependency trees of opinions when it bears opinions (gop(r) equals true).) 

4.3 Evaluation of Opinion Dependency 
Relation Prediction 

To predict which dependency relations are 
opinionated, we start with analyzing the distri-
bution of them. Table 3 presented the distribu-
tion of dependency relations. The percentage 
of a relation appearing in dependency trees of 
opinion sentences when bearing opinions 
(gop(r) equals true), i.e., the value in F column, 
is taken as the support value. The support val-
ue indicates that in what degree this relation 
bears opinions. If the support value is high, it 
is confident to say that the relation is opinio-
nated; otherwise, considering the content 
words is necessary. This idea conforms to the 

previous observation in Section 4.2: some of 
the opinions are structural, but not all of them.  

According to the support value, dependency 
relations were divided into four categories. The 
Chinese opinion word dictionary NTUSD (Ku 
et al., 2007) is involved to help identify opi-
nion dependency relations when the support 
value is not high. The selecting criteria are 
listed as follows. 
 Very supportive: with the support value 

above 0.8, e.g., dvpmod. Relations in this 
category are viewed as opinionated and their 
gop(r) are automatically set to true. 

 Supportive: with the support value above 
0.35 but lower than 0.8, e.g., pass, dobj, 
npsubj, ba, top, nsubj, neg, amod, rcmod. 
RH(w) is a function to return the word w’s 

350



head in other relations of the same sentences, 
and RM(w) returns w’s modifier. For each 

},,{ mh wwrelr  in this category: 

.)( 
,)(

,   )(),(,, ofany   

falsergopelse
truergop

NTUSDiniswRMwRHwwif mhmh




 

 Minor supportive: with the support value 
above 0.2, e.g., rcomp, advmod, mmod, 
range. For each },,{ mh wwrelr  in this cat-
egory:    

.)( 
,)(

,   )(),(,, of all  

falsergopelse
truergop

NTUSDiniswRMwRHwwif mhmh




 

 Not supportive: with the support value less 
than 0.2. Relations in this category are 
viewed as non-opinionated and their gop(r) 
are automatically set to false. 

Experiment Settings P R f-Score
Appearance: oleft and oright 0.60 0.52 0.56 
t = Substantive-Modifier 0.59 0.41 0.49 
t = Subjective-Predicate 0.53 0.46 0.49 
t = Verb-Object 0.62 0.65 0.64 
t = Verb-Complement 0.44 0.13 0.20 

Table 4. Prediction of the appearance 
 of children nodes in trios. 

Experiment Settings P R f-Score
t = Substantive-Modifier 1.00 0.25 0.40 
t = Subjective-Predicate 1.00 0.25 0.41 
t = Verb-Object 1.00 0.39 0.56 
t = Verb-Complement 1.00 0.13 0.23 

Table 5. Prediction of trios. 
All dependency relations 

Rel P R f-Score 
Nsubj 0.4507 0.7028 0.5492 

Advmod 0.3675 0.6851 0.4784 
Dobj 0.6097 0.8566 0.7124 

Rcmod 0.4195 0.8509 0.5620 
Amod 0.5031 0.7953 0.6163 
Mmod 0.3289 0.7388 0.4552 
Neg 0.4313 0.6404 0.5155 

Range 0.3630 0.5762 0.4454 
Top 0.4833 0.5461 0.5128 

Rcomp 0.3371 0.5817 0.4269 
Ba 0.4286 0.5817 0.4935 

Dvpmod 0.8244 1.0000 0.9037 
Pass 0.5819 0.7455 0.6536 

Npsubj 0.5000 0.7073 0.5859 
Total 0.4713 0.6178 0.5347 

Modification dependency relations 
Total 0.4070 0.2371 0.2997 
Table 6. Performance of predicting opinion 

dependency relations. 

The results of two experiment settings are 
listed: prediction performed on all dependency 
relations and on only modification-related de-
pendency relations (in the form of lex-mod, 
e.g., amod, rcmod, etc.) The later are the rela-
tions adopted in many previous researches. 
Table 6 shows the performance of predicting 
opinion dependency relations. It indicates that 
if only modification related relations were con-
sidered, the f-score dropped nearly half be-
cause more than half of the opinion dependen-
cy relations were expelled in this case. In other 
word, results show that predicting on all rela-
tions instead of taking only modification-
related dependency relations as clues can cap-
ture more opinion relations, and hence the pre-
diction of opinion relations is necessary. 

4.4 Evaluation of Opinion Extraction Us-
ing Predicted Opinion Trios and De-
pendency Relations 

In this section, predicted opinion trios and pre-
dicted opinion dependency relations were uti-
lized in an opinion extraction system. In order 
to make use of these structural cues, opinion 
analysis methods proposed by Ku et al. (2007) 
were selected. Their methods calculated opi-
nion scores of sentences from characters and 
words accumulatively, so syntactic cues can be 
added in and function jointly. 

Five settings for opinion analysis were expe-
rimented: 
 C+W+N: characters, words, and negations 

were used as cues for calculating opinion 
scores. It was the original method proposed 
by Ku et al. 

 C+W+N+goldTrio: annotated opinion trios 
were utilized additionally. 

 C+W+N+Trio: predicted opinion trios were 
utilized additionally. 

 C+W+N+goldDep: opinion dependency 
relations aligned from the annotated trios 
were utilized additionally. 

 C+W+N+Dep: predicted opinion dependen-
cy relations were utilized additionally. 
The results were shown in Table 7. The per-

formance of the opinion extraction improves 
10.40% (0.7162->0.7993) when utilizing opi-
nion trios and 8.66% (0.7162->0.7782) when 
utilizing opinion dependency relations. These 
results clearly indicate that the syntactic in-
formation benefit opinion analysis. Because of 

351



the possible information loss in the automatic 
alignment process, that the performance of us-
ing trios is a little better than using dependency 
relations matches our expectation. 

Setting f-Score 
C+W+N 0.7162 

C+W+N+goldTrio 0.7922 
C+W+N+Trio 0.7993 

C+W+N+goldDep 0.7784 
C+W+N+Dep 0.7782 

Table 7. Performance of using syntactic infor-
mation for opinion analysis. 

5 Related Work 
For all we know, no previous work has anno-
tated opinion information on all dependency 
relations, or mapped annotated opinionated 
structures to dependency relations on a large 
quantity of documents or sentences. Therefore, 
to the best of our knowledge, no statistically 
analysis of opinion dependency relations in-
volving manually annotations has been con-
ducted. Researchers designed ruled or ex-
tracted dependency relations as features for 
opinion analysis based on their linguistic 
knowledge (Qiu et al., 2011). 

Yet there are still several lines of related 
work, including (i) opinion analysis (ii) opi-
nion corpora (iii) syntactic information.  Sev-
eral dozen papers have been published on the 
topic of opinion analysis. Two general ap-
proaches have been proposed previously. They 
are machine learning approaches and heuristic-
rule approaches. For both approaches, syntac-
tic structures could be utilized. For the former, 
they can be used as features (Abbasi et al., 
2008); for the later, rules can be designed ac-
cording to them (Ku et al., 2009). We can see 
from the previous work that syntactic struc-
tures can help to enhance the performance. 

As to the experimental corpora, some re-
searchers managed to generate annotated mate-
rials and gold standards under constraints.  
Somasundaran (2007) annotated discourse in-
formation from meeting dialogs to train a sen-
timent model. MPQA annotated opinions and 
their sources (Wiebe et al., 2002). NTCIR an-
notated opinions, polarities, sources, and tar-
gets for its multilingual opinion analysis task 
(MOAT, Seki et al., 2008). However, none of 
them were annotated on materials with syntac-

tic structures, and it caused the lack of analysis 
of opinion syntactic structures. 

Researchers have acquired syntactic struc-
tures (Zhou, 2008), but few of them have tried 
to associate syntactic structures with opinions. 
The most similar previous work to ours was 
proposed by Ku et al. (2009). Compared to it, 
the proposed process made the development of 
opinion dependency parser feasible. As depen-
dency relations and the predicted opinion de-
pendency relations are of the same form, no 
extra knowledge or integration is needed for 
the use of them. 

6 Conclusions and Future Work 
The proposed new process is the main contri-
bution of this paper. It annotated opinion syn-
tactic structures on phrase structure trees, 
which are more readable for annotators, and 
aligned these structures to grammatical struc-
tures, which facilitates their usage. Chinese 
Treebank was selected as the source of phrase 
structure trees, and dependency relations as the 
grammatical structures. They are both widely 
used in natural language processing. 

Though the experiments were implemented 
on Chinese materials, this process is language 
independent. It can be applied to materials in 
different languages without modifications. 

By predicting opinion dependency relations, 
we can say that a basic opinion dependency 
parser has been developed. Experiments have 
shown that the predicted opinion dependency 
relations are beneficial for opinion extraction. 
Although we still need a parser to generate 
syntactic structures, parsing is relatively a ma-
ture technique in natural language processing.  
For a comparably new research problem like 
opinion analysis, it is common that tools are 
not handy. The best of the proposed method is 
that it can function in a multilingual environ-
ment by incorporating a domain or language 
specific resources (here, NTUSD for Chinese). 

Through the alignment, we made a large 
quantity of opinion dependency relations 
available. According to their distributions 
shown in this paper, researchers can select 
suitable relations to use according to their di-
verse needs, such as extracting evaluative fea-
tures in product reviews or comments, opi-
nions or their polarities. 

352



References 
Abbasi, A., Chen, H., and Salem, A. 2008. Senti-

ment analysis in multiple languages: Feature se-
lection for opinion classification in Web forums. 
ACM Trans. Inf. Syst. 26, 3 (Jun. 2008), 1-34. 
DOI= http://doi.acm.org/10.1145/1361684. 
1361685 

Bikel, D. M. and Castelli, V. 2008. Event Matching 
Using the Transitive Closure of Dependency Re-
lations. Proceedings of the 46th Annual Meeting 
on Association for Computational Linguistics, 
pages 145-148. 

Breck, E., Choi, Y. and Cardie, C. 2007. Identify-
ing Expressions of Opinion in Context. Proceed-
ings of the 20th International Joint Conferences 
on Artificial Intelligence, pages 2683-2688. 

Chang, P.-C., Tseng, H., Jurafsky, D. and Manning 
C. D. 2009. Discriminative Reordering with 
Chinese Grammatical Relations Features. Pro-
ceedings of the Third Workshop on Syntax and 
Structure in Statistical Translation, pages 51-59. 

Doan, H., Cao, Y., Lin, C.-Y. and Yu, Y. 2008. 
Searching Question by Identifying Question 
Topic and Question Focus. Proceedings of the 
46th Annual Meeting on Association for Compu-
tational Linguistics, pages 156-164. 

Feng, S., Wang, D., Yu, G., Yang, C. and Yang, N. 
2009. Chinese Blog Clustering by Hidden Sen-
timent Factors. ADMA, Vol. 5678, Springer 
(2009), pages 140-151. 

Ku, L.-W. and Chen, H.-H. 2007. Mining Opinions 
from the Web: Beyond Relevance Retrieval. 
Journal of American Society for Information 
Science and Technology, Special Issue on Min-
ing Web Resources for Enhancing Information 
Retrieval, 58(12), 1838-1850. 

Ku, L.-W., Huang, T.-H. and Chen, H.-H. 2009. 
Using Morphological and Syntactic Structures 
for Chinese Opinion Analysis. Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing, pages 1260-1269. 

Ku, L.-W., Lo, Y.-S. and Chen H.-H. 2007. Test 
Collection Selection and Gold Standard Genera-
tion for a Multiply-Annotated Opinion Corpus. 
Proceedings of the 45th Annual Meeting on As-
sociation for Computational Linguistics, pages 
89-92. 

Lafferty, J., McCallum, A. and Pereira, F. 2001. 
Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. 
Proceedings of International Conference on Ma-
chine Learning, pages 282-289. 

Lu, Y., Zhai, C.X. and Sundaresan, N. 2009. Rated 
Aspect Summarization of Short Comments. Pro-
ceedings of 18th International World Wide Web 
Conference, pages 131-140. 

de Marneffe, M.-C. and Manning, C. D. 2008. Stan-
ford typed dependencies manual. Technichal re-
port. http://nlp.stanford.edu/software/depend 
encies_manual.pdf 

Pang, B., Lee, L. and Vaithyanathan, S. 2002. 
Thumbs up? Sentiment classification using ma-
chine learning techniques. Proceedings of the 
2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 79-86. 

Qiu, G., Wang, C., Bu, J., Liu, K. and Chen, C. 
2008. Incorporate the Syntactic. Knowledge in 
Opinion Mining in User-generated Content. Pro-
ceedings of NLPIX’08. 

Qiu, G., Liu, B., Bu, J. and Chen, C. 2011. Opinion 
Word Expansion and Target Extraction through 
Double Propagation. Computational Linguistics, 
March 2011, Vol. 37, No. 1: 9.27 

Seki, Y., D. K. Evans, L.-W. Ku, L. Sun, H.-H. 
Chen and N. Kando. 2008.  Overview of Multi-
lingual Opinion Analysis Task at NTCIR-7. 
Proceedings of the 7th NTCIR Workshop Meet-
ing on Evaluation of Information Access Tech-
nologies:Information Retrieval, Question Ans-
wering, and Cross-Lingual Information Access, 
pages 185-203. 

Somasundaran, S., J. Ruppenhofer and J. Wiebe. 
2007. Detecting arguing and sentiment in meet-
ings. Proceedings of the SIGdial Workshop on 
Discourse and Dialogue 2007. 

Wiebe, J., E. Breck, C. Buckly, C. Cardie, P. Davis, 
B. Fraser, D. Litman, D. Pierce, E. Riloff and T. 
Wilson. 2002. NRRC summer workshop on mul-
ti-perspective question answering, final report.  
ARDA NRRC Summer 2002 Workshop. 

Zhou, Q. 2008. Automatic rule acquisition for Chi-
nese intra-chunk relations. Proceedings of Inter-
national Joint Conference of Natural Language 
Processing (IJCNLP-2008). 

 

353


