



















































MetaMind Neural Machine Translation System for WMT 2016


Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 264–267,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

MetaMind Neural Machine Translation System for WMT 2016

James Bradbury
MetaMind - A Salesforce Company

Palo Alto, CA
james.bradbury@salesforce.com

Richard Socher
MetaMind - A Salesforce Company

Palo Alto, CA
rsocher@salesforce.com

Abstract

Neural Machine Translation (NMT) sys-
tems, introduced only in 2013, have
achieved state of the art results in many
MT tasks. MetaMind’s submissions to
WMT ’16 seek to push the state of the art
in one such task, English→German news-
domain translation. We integrate promis-
ing recent developments in NMT, includ-
ing subword splitting and back-translation
for monolingual data augmentation, and
introduce the Y-LSTM, a novel neural
translation architecture.

1 Introduction

The field of Neural Machine Translation (NMT),
which seeks to use end-to-end neural networks to
translate natural language text, has existed for only
three years. In that time, researchers have explored
architectures ranging from convolutional neural
networks (Kalchbrenner and Blunsom, 2013) to
recurrent neural networks (Chung et al., 2014) to
attentional models (Bahdanau et al., 2015; Lu-
ong et al., 2015) and achieved better performance
than traditional statistical or syntax-based MT
techniques on many language pairs. NMT mod-
els first achieved state-of-the-art performance on
the WMT English→German news-domain task in
2015 (Luong et al., 2015) and subsequent im-
provements have been reported since then (Sen-
nrich et al., 2015a; Li and Jurafsky, 2016).

The problem of machine translation is fun-
damentally a sequence-to-sequence transduction
task, and most approaches have been based on
an encoder-decoder architecture (Sutskever et al.,
2014; Cho et al., 2014). This entails coupled neu-
ral networks that encode the input sentence into
a vector or set of vectors and decode that vector
representation into an output sentence in a differ-

ent language respectively. Recently, a third com-
ponent has been added to many of these models:
an attention mechanism, whereby the decoder can
attend directly to localized information from the
input sentence during the output generation pro-
cess (Bahdanau et al., 2015; Luong et al., 2015).
The encoder and decoder in these models typi-
cally consist of one-layer (Cho et al., 2014) or
multi-layer recurrent neural networks (RNNs); we
use four- and five-layer long short-term memory
(LSTM) RNNs. The attention mechanism in our
four-layer model is what Luong (2015) describes
as “Global attention (dot)”; the mechanism in our
five-layer Y-LSTM model is described in Section
2.1.

Every NMT system must contend with the prob-
lem of unbounded output vocabulary: systems that
restrict possible output words to the most com-
mon 50,000 or 100,000 that can fit comfortably
in a softmax classifier will perform poorly due
to large numbers of “out-of-vocabulary” or “un-
known” outputs. Even models that can produce
every word found in the training corpus for the
target language (Jean et al., 2015) may be un-
able to output words found only in the test cor-
pus. There are three main techniques for achiev-
ing fully open-ended decoder output. Models
may use computed alignments between source and
target sentences to directly copy or transform a
word from the input sentence whose correspond-
ing translation is not present in the vocabulary
(Luong et al., 2015) or they may conduct sen-
tence tokenization at the level of individual char-
acters (Ling et al., 2015) or subword units such
as morphemes (Sennrich et al., 2015b). The latter
techniques allow the decoder to construct words
it has not previously encountered out of known
characters or morphemes; we apply the subword
splitting strategy using Morfessor 2.0, an unsuper-
vised morpheme segmentation model (Virpioja et

264



al., 2013).
Another focus of recent research has been ways

of using monolingual corpus data, available in
much larger quantities, to augment the limited par-
allel corpora used to train translation models. One
way to accomplish this is to train a separate mono-
lingual language model on a large corpus of the
target language, then use this language model as
an additional input to the decoder or for re-ranking
output translations (Gülçehre et al., 2015). More
recently, Sennrich (2015b) introduced the concept
of augmentation through back-translation, where
an entirely separate translation model is trained
on a parallel corpus from the target language to
the source language. This backwards translation
model is then used to machine-translate a mono-
lingual corpus from the target language into the
source language, producing a pseudo-parallel cor-
pus to augment the original parallel training cor-
pus. We extend this back-translation method by
translating a very large monolingual German cor-
pus into English, then concatenating a unique sub-
set of this augmentation corpus to the original par-
allel corpus for each training epoch.

2 Model Description

The model identified as metamind-single
is based on the attention-based encoder-decoder
framework described in Luong (2015), using the
attention mechanism referred to as “Global atten-
tion (dot).” The encoder is a four-layer stacked
LSTM recurrent neural network whose inputs (at
the bottom layer) are vectors wint corresponding to
the subword units in the input sentence and which
saves the topmost output state at each timestep et
as the variable-length encoding matrix E. The
decoder also contains a four-layer stacked LSTM
whose states (c0 and h0 for each layer) are initial-
ized to the last states for each layer of the encoder.
At the first timestep, the decoder LSTM receives
as input an initialization word vector wout0 ; its top-
most output state ht is concatenated with an en-
coder context vector κt computed as:

score(ht, es) = hte
>
s

αst = softmaxall s(score(ht, es))

κt =
∑

s

αstes

This concatenated output is then fed through an
additional neural network layer to produce a final

attentional output vector h̃, which serves as input
to the output softmax:

h̃ = tanh(Watt[ht;κt])

output probabilities = softmax(Wouth̃)

For subsequent timesteps, the decoder LSTM re-
ceives as input the previous word vectorwoutt−1 con-
catenated with the previous output vector h̃.

Decoding is performed using beam search, with
beam width 16. The beam search decoder differs
slightly from Luong (2015) in that we normalize
output sentence probabilities by length, following
Cho (2014), rather than performing ad-hoc adjust-
ments to correct for short output sentences.

2.1 Y-LSTM Model

The model identified as metamind-ylstm uses
a novel attentional framework we call the Y-
LSTM. The encoder is a five-layer stacked LSTM
recurrent neural network language model (RNN-
LM) with subword-vector inputs wint , whose top-
most output state htopt is used as input to a soft-
max layer which predicts the next input token. The
middle (l = 3) layer of this encoder RNN-LM
is connected recurrently to a single-layer LSTM
called the “tracker;” at denotes the set of inputs to
a given LSTM layer:

al 6=3t = [h
l−1
t ;h

l
t−1]

al=3t = [h
l−1
t ;h

l
t−1;h

tracker
t−1 ]

atrackert = [h
tracker
t−1 ;h

3
t ]

The hidden and memory states ctrackert and h
tracker
t

of the tracker LSTM are saved at each timestep
as the variable-length encoding matrices C and
H . The decoder is an analogous RNN-LM with a
tracker LSTM, identical except that the hidden and
memory states of the decoder’s tracker (c̃trackert
and h̃trackert ) are replaced at each timestep with
an attentional sum of the encoder’s saved tracker
states:

score(h̃t, hs) = h̃th
>
s

αst = softmaxall s(score(h̃
tracker
t , h

tracker
s ))

c̃trackert =
∑

s

αstc
tracker
s

h̃trackert =
∑

s

αsth
tracker
s

265



System BLEU-c on newstest2016
Best phrase-based system (uedin-syntax) 30.6
Other NMT systems – single model
NYU/U. Montreal character-based 30.8
U. Edinburgh subword-based (uedin-nmt-single) 31.6
Other NMT systems – ensemble or model combination
U. Edinburgh ensemble of 4 (uedin-nmt-ensemble) 34.2
Our systems – single model
metamind-single 31.6
metamind-ylstm 29.3
Our systems – ensemble
metamind-ensemble 32.3
Ensemble of four checkpoints without Y-LSTM 32.1

Table 1: BLEU results on the official WMT 2016 test set. Only our main ensemble was entered into the
human ranking process, coming in second place behind U. Edinburgh.

The overall network loss is the sum of the lan-
guage model (negative log-likelihood over the out-
put softmax) losses for the encoder and decoder.

3 Experiment Description

Initial tokenization and preprocessing of the WMT
2016 English→German news translation dataset
was performed using the standard scripts provided
with Moses (Koehn et al., 2007). Two further
processing steps were used to create the subword-
based training dataset. First, capitalized characters
were replaced with a sequence of a capitalization
control character (a Unicode private-use charac-
ter) and the corresponding lowercase character, in
order to allow the subword splitting algorithm to
treat capitalized words as either inherently capi-
talized or capitalized versions of lowercase words.
Without this step, much of the limited output soft-
max capacity is taken up with capitalized vari-
ants of common lowercase words; performing this
transformation also allows us to forego “truecas-
ing,” which removes sentence-initial capitalization
in a lossy and sometimes unhelpful way. Second,
the capitalization-transformed training corpus for
each language is ingested by a Morfessor 2.0 in-
stance configured to use a balance between corpus
and vocabulary entropy that produces a vocabulary
of approximately 50,000 subword units.

For all experiments, we used using plain
stochastic gradient descent with learning rate 0.7,
gradient clipping at magnitude 5.0, dropout of 0.2,
and learning rate decay of 50% per epoch after 8
epochs.

Following Sennrich (2015b), we first trained

a non-Y-LSTM model in the reverse direction
(German→English) on the full WMT ’16 train-
ing corpus (4.4 million sentences). This model
was then used simultaneously on 8 GPUs (with
a beam search width of 4 for speed purposes) to
translate 45 million sentences of the 2014 mono-
lingual German news crawl into English. A full
copy of the original training corpus was then con-
catenated with a unique subset of this augmenta-
tion corpus to create a new training corpus for each
epoch from 1 to 10; the corpus for epoch 1 was
then repeated as epoch 11 et cetera.

For metamind-single, we trained a non-
Y-LSTM model using these augmented corpora,
with data-parallel synchronous SGD across four
GPUs enabling a batch size of 384 and training
speed of about 2,500 subword units per second.
The run submitted as metamind-single uses
a single snapshot of this model after 12 total train-
ing epochs.

For metamind-ylstm, we trained a Y-LSTM
model using the same corpora, with data-parallel
synchronous SGD across four GPUs enabling a
batch size of 320 and training speed of about 1,500
subword units per second. The run submitted as
metamind-ylstm uses a single snapshot of this
model after 9 total training epochs.

The run submitted as metamind-ensemble
uses an equally-weighted ensemble of three snap-
shots of the metamind-single model (after
10, 11, and 12 epochs) and a single snapshot of
the metamind-ylstm model after 9 total train-
ing epochs.

266



4 Results

Results for all three runs described above are pre-
sented in Table 1. Only the ensemble was sub-
mitted to the human evaluation process, with a
final ranking of second place (behind U. Edin-
burgh’s ensemble of four independently initial-
ized models). Our best single model matches
the performance of the best model from U. Edin-
burgh, which applies a similar attentional frame-
work, subword splitting, and back-translated aug-
mentation.

The Y-LSTM model underperformed relative to
the model based on Luong (2015), but provided
a small additional boost to the ensemble. The
primary contribution of this model is to demon-
strate that purely attentional NMT is possible: the
only inputs to the decoder are through the attention
mechanism. This may be helpful for using transla-
tion to build general attentional sentence encoding
models, since the representation of the input sen-
tence is entirely in the attentional encoding, not
split between an attentional encoding vector and a
vector representing the last timestep of the multi-
layer encoder hidden state.

Acknowledgements

We would like to thank the developers of Chainer
(Tokui et al., ), which we used for all models
and experiments reported here. We also thank
Stephen Merity, Kai Sheng Tai, and Caiming
Xiong for their helpful feedback, and all partic-
ipants in the manual evaluation campaign. We
thank the Salesforce acquisition and IT teams for
keeping the MetaMind compute cluster up and
running throughout the acquisition process.

References
D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural

machine translation by jointly learning to align and
translate. In ICLR.

K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Ben-
gio. 2014. On the properties of neural machine
translation: Encoder-decoder approaches. CoRR,
abs/1409.1259.

J. Chung, C. Gulcehre, K. Cho, and Y. Bengio.
2014. Empirical evaluation of gated recurrent neu-
ral networks on sequence modeling. arXiv preprint
arXiv:1412.3555.

Çaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loı̈c Barrault, Huei-Chi Lin, Fethi Bougares,

Holger Schwenk, and Yoshua Bengio. 2015. On
using monolingual corpora in neural machine trans-
lation. CoRR, abs/1503.03535.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
ACL.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

Jiwei Li and Daniel Jurafsky. 2016. Mutual informa-
tion and diverse decoding improve neural machine
translation. CoRR, abs/1601.00372.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W.
Black. 2015. Character-based neural machine trans-
lation. CoRR, abs/1511.04586.

M. T. Luong, H. Pham, and C. D. Manning. 2015.
Effective approaches to attention-based neural ma-
chine translation. In EMNLP.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015a. Improving neural machine translation mod-
els with monolingual data. CoRR, abs/1511.06709.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare words
with subword units. CoRR, abs/1508.07909.

I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence
to sequence learning with neural networks. In NIPS.

Seiya Tokui, Kenta Oono, and Shohei Hido. Chainer:
a next-generation open source framework for deep
learning.

Sami Virpioja, Peter Smit, Stig-Arne Grönroos, Mikko
Kurimo, et al. 2013. Morfessor 2.0: Python imple-
mentation and extensions for morfessor baseline.

267


