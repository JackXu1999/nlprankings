










































Minimally Supervised Domain-Adaptive Parse Reranking for Relation Extraction


Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128,
October 5-7, 2011, Dublin City University. c© 2011 Association for Computational Linguistics

Minimally Supervised Domain-Adaptive Parse Reranking for
Relation Extraction

Feiyu Xu, Hong Li, Yi Zhang, Hans Uszkoreit, Sebastian Krause

DFKI, LT-Lab, Germany
{feiyu,lihong,Yi.Zhang,uszkoreit,sebastian.krause}@dfki.de

Abstract

The paper demonstrates how the generic
parser of a minimally supervised informa-
tion extraction framework can be adapted
to a given task and domain for relation ex-
traction (RE). For the experiments a generic
deep-linguistic parser was employed that
works with a largely hand-crafted head-
driven phrase structure grammar (HPSG)
for English. The output of this parser is a
list of n best parses selected and ranked by
a MaxEnt parse-ranking component, which
had been trained on a more or less generic
HPSG treebank. It will be shown how the
estimated confidence of RE rules learned
from the n best parses can be exploited
for parse reranking. The acquired rerank-
ing model improves the performance of RE
in both training and test phases with the
new first parses. The obtained significant
boost of recall does not come from an over-
all gain in parsing performance but from an
application-driven selection of parses that
are best suited for the RE task. Since the
readings best suited for successful rule ex-
traction and instance extraction are often
not the readings favored by a regular parser
evaluation, generic parsing accuracy actu-
ally decreases. The novel method for task-
specific parse reranking does not require any
annotated data beyond the semantic seed,
which is needed anyway for the RE task.

1 Introduction
Domain adaptation is a central research topic for
many language technologies including informa-
tion extraction (IE) and parsing (e.g., (Grishman
and Sundheim, 1996; Muslea, 1999; Hara et al.,
2005; McClosky et al., 2010; Miwa et al., 2010)).
The largest challenge is to develop methods that

exploit domain knowledge with minimal human
effort.

Many IE systems benefit from combining
generic NLP components with task-specific ex-
traction methods. Various machine learning ap-
proaches have been employed for adapting the IE
methods to new domains and extraction tasks (e.g.,
(Yangarber, 2001; Sudo et al., 2003; Greenwood
and Stevenson, 2006)). The IE framework ex-
tended in this paper utilizes minimally supervised
learning of extraction rules for the detection of re-
lation instances (Xu et al., 2007). Since the min-
imally supervised learning starts its bootstrapping
from a few semantic examples, no treebanking or
any other annotation is required for new domains.
In addition to this inherently domain-adaptable
rule-learning component, the framework also em-
ploys two language analysis modules: a named-
entity (NE) recognizer (Drozdzynski et al., 2004)
and a parser (Lin, 1998; de Marneffe and Man-
ning, 2008). NE recognizers are adapted to new
domains–if needed–by adding rules for new NE
types and extending the gazetteers. The employed
generic data-driven dependency parsers or deep-
linguistic handcrafted parsers have not yet been
adapted to IE domains and tasks.

The new work presented here concerns the
adaptation of a generic parser to a given relation
extraction (RE) task and domain without actually
changing the parser itself. For the experiments a
generic deep-linguistic parser was used together
with a hand-crafted HPSG (Pollard and Sag, 1994)
grammar for English (ERG) (Flickinger, 2000).
The output of this parser is a list of n best parses
selected and ranked by a MaxEnt parse-ranking
component (Toutanova et al., 2005b), which had
been trained on a generic HPSG treebank (Oepen
et al., 2002). The parse ranking had attracted our

118



attention because the first RE tests with the hand-
crafted grammar revealed recall problems even for
the parsable relation mentions. Our suspicion to
partially blame the generic parse selection was
confirmed by our experiments.

In this paper we will show how the estimated
confidence of rules learned from the n best parses
can be exploited for task-specific parse reranking.
The acquired reranking model improves the per-
formance of RE both in training and test phases.
The task-driven reranking leads to significantly
better RE recall by boosting readings that are bet-
ter suited for RE rule extraction and rule appli-
cation. The beneficial reranking does not im-
prove the quality of parsing measured by task-
independent performance criteria, not even for
the IE domain. The validation of the adapted
parser using a hand-checked HPSG treebank of in-
domain texts rather shows a deterioration of pars-
ing accuracy. But often the incorrect parses se-
lected over less faulty parses support the correct
detection of instance mentions.

The novel method for task-specific parse
reranking does not require any annotated data be-
yond the semantic seed, needed anyway for the RE
task. Thus it does not require a domain-specific
treebank.

The paper is organized as follows. Section 2
describes the grammar and the associated parse se-
lection model. Section 3 introduces the RE frame-
work. Section 4 explains the new task/domain-
oriented reranking approach. Section 5 presents
the experiments and evaluations. Special empha-
sis is placed on the role of reranking for the per-
formance of the RE system. Section 6 discusses
related work. Finally, Section 7 summarizes the
results and suggests directions for further research.

2 HPSG and Parse Selection Model
Recent progress in parsing has several sources.
The most noticeable trend is the shift from pure
symbolic rule-based approaches toward statistical
parsing. The availability of large-scale treebanks
has enabled the training of powerful data-driven
parsers, some based on constituency others on de-
pendency. Meanwhile, existing hand-crafted pre-
cision oriented linguistic grammars have also ben-
efitted from empirical methods through new dis-
ambiguation models trained on treebanks.

Among the available deep linguistic grammars,
ERG is a good representative of the state of the art.
Its lexicon contains ∼35K entries. The 1004 re-

lease of the grammar we use is accompanied by
a maximum-entropy-based parse disambiguation
model trained on the Redwoods Treebank (Oepen
et al., 2002), a treebank of ∼20K sentences with
mixed genre texts (dialogs, tourist information,
emails, etc). The discriminative log-linear disam-
biguation model scores each parse by the follow-
ing (Toutanova et al., 2005b),

P (t|w) =
exp

∑n
i=1 λifi(t, w)∑

t′∈T (w) exp
∑n

i=1 λifi(t
′, w)

(1)

where w is the given input sentence and t is the
HPSG reading; T (w) is the set of all possible
readings for a given sentence w licensed by the
grammar; 〈f1, . . . , fn〉 and 〈λ1, . . . , λn〉 are fea-
ture functions and their corresponding weights. In
practice, the effective features are defined on the
HPSG derivation trees (without details from the
feature structures), and the best readings are de-
coded efficiently from a packed parse forest with
dynamic programming (Zhang et al., 2007).

Although there are indications that parsers with
hand-written grammars usually suffer less from
the shift of domain than statistical parsers (Zhang
and Wang, 2009; Plank and van Noord, 2010), the
effect can still be observed, say in the preference
of lexical selection. The issue is not that the cor-
rect analysis would be ruled out by the constraints
in the treebank-induced grammar, but rather that
it is not favored by the statistical ranking model,
since the statistical distribution of the syntactic
structures in the training corpus is different from
the target application domain. This issue is re-
cently acknowledged in most parsing systems and
known as the domain adaptation task.

3 DARE and Confidence Estimation
DARE (Xu et al., 2007; Xu, 2007) is a minimally
supervised machine learning system for RE for
free texts consisting of two major parts: 1) rule
learning, 2) relation extraction (RE). DARE pro-
vides a recursive extraction-rule representation,
which can deal with relations of varying com-
plexity. Rule learning and RE feed each other
in a bootstrapping framework. The bootstrap-
ping starts from so-called ”semantic seed” as a
search query, which is a small set of instances of
the target relation. The rules are extracted from
found sentences with annotations of semantic en-
tities and parsing results. RE applies acquired
rules to texts in order to discover more relation in-
stances, which in turn are employed as seed for

119



further iterations. The confidence values of the
newly acquired rules and instances are calculated
in the spirit of the ”Duality principle” (Brin, 1998;
Agichtein and Gravano, 2000; Yangarber, 2001),
i.e., the confidence values of the rules are de-
pendent on the truth value of their extracted in-
stances and on the seed instances from which they
stem. The confidence value of an extracted in-
stance makes use of the confidence value of its
ancestor seed instances. The core system architec-
ture of DARE is depicted in Figure 1. The entire
bootstrapping stops when no new rules or new in-
stances can be detected.

Figure 1: DARE core architecture

Relying entirely on semantic seeds as domain
knowledge, DARE can accommodate new relation
types and domains with minimal effort. Since we
had already reported on experiments applying the
framework to different relation types and corpora
including MUC-6 data in the cited papers, includ-
ing comparisons with other ML approaches to RE
(Xu, 2007; Uszkoreit et al., 2009), we omit a com-
parative discussion here.

For confidence estimation, the method proposed
by Xu et al. (2010) is adopted.1 Actually, in (2)
we propose an extended version of the rule scor-
ing, since the rule scoring in (Xu et al., 2010) did
not consider the case when a learned rule does not
extract any new instances. Thus, given the scoring
of instances, the confidence value of a rule is the
average score of all instances (Iextracted ) extracted
by this rule or the average score of seed instances
(Irule ) from which they are learned. Through the
factor δ we reduce the score of rules that have not
proven yet their potential for extracting instances.

1The actual confidence estimation is slightly more com-
plex because it further improves the scoring by utilizing im-
plicit negative evidence provided by closed-world seeds, a
method proposed by (Xu et al., 2010). As this mechanism is
not relevant in the context of this paper, we omit a descrip-
tion.

confidence(rule) =
∑

i∈Iextracted
score(i)

|Iextracted | if Iextracted 6= φ∑
j∈Irule

score(j)
|Irule | × δ if Iextracted = φ

where Iextracted = getInstances(rule),
Irule = getMotherInstancesOf(rule),
δ = 0.5

(2)

This method allows DARE to estimate the con-
fidence value of a rule according to its extraction
performance or the confidence value of its origin.

4 Domain Adaptive Parse Reranking
4.1 Basic Idea

In our research, we observe that there is a strong
connection between the RE task and the parser via
the learned extraction rules, because these rules
are derived from the parse readings. The confi-
dence values of the extraction rules imply the do-
main appropriateness of the parse readings. There-
fore, the confidence values can be utilized as feed-
back to the parser to help it to rerank its readings.

4.2 Reranking Architecture and Method

Figure 2 depicts the overall architecture of our
experimental system. We utilize the HPSG to
parse our experimental corpus and keep the first
n readings of each sentence (e.g., 256) delivered
by the parser. During bootstrapping DARE tries
to learn extraction rules from all readings of sen-
tences containing a seed instance or newly de-
tected instances. At each iteration the extracted
rules are applied to all readings of all sentences.
When bootstrapping has terminated, the obtained
rules are assigned confidence values based on the
DARE ranking method described in Section 3.

Figure 2: DARE and Parse Reranking

120



The parse reranking component scores the alter-
native parses of each sentence based on the confi-
dence values of the rules matching these parses,
i.e., all rules that could have been extracted from a
parse or successfully applied to it.

For each reading from the HPSG parser, the
reranking model assigns a numeric score by the
following formula:

S(t) =
∑

r∈R(t) (confidence(r)− φconfidence)
ifR(t) 6= φ,

0
ifR(t) = φ.

(3)

R(t) is the set of RE rules matching parse read-
ing t, and φconfidence is the average confidence
score among all rules. The score of the read-
ing will be increased if the matching rule has an
above-average confidence score. And the match-
ing of low-confidence rules will decrease the read-
ing’s reranking score. If a reading has no matching
DARE rule, it will be assigned the lowest score 0,
for no potential relation can be extracted from that
reading.

After the calculation, the top-n readings are
sorted in descending order. In case two or more
readings received the same reranking score (e.g.
by matching the same set of DARE rules), the
original maximum entropy-based disambiguation
scores are used as a tie-breaker. The sort compari-
son function is shown below:

Algorithm 1 compare readings(ri, rj)
if compare(S(ri), S(rj)) 6= 0 then

return compare(S(ri), S(rj))
else # Tie-breaking with MaxEnt scores

return compare(MaxEnt(ri), MaxEnt(rj))
end if

In practice, most readings will have no more
than two matching DARE rules. And many read-
ings from the HPSG parser do not affect the RE
task. A consequence is that the reranking model
can only partially disambiguate and have an effect
only on particular subsets of the readings. As we
are only evaluating RE performance, the remain-
ing ambiguity is not an issue.

5 Experiments and Evaluation
5.1 Experiment and Evaluation Setup

Data For several reasons we decided to conduct
our experiments on the Nobel Prize award corpus
used also in (Xu et al., 2007). Previous results
have shown that

1. not every data collection is suited for the
minimally supervised approach to RE (Xu,
2007);

2. freely available Nobel Prize award corpus ac-
tually has the required properties (Uszkoreit
et al., 2009).

Moreover, the availability of a version of the
corpus in which all relation mentions are labelled
and a treebank for a subset of the corpus have
greatly facilitated the evaluation.

The target relation is prize-awarding event,
namely, a relation among four arguments: WIN-
NER, PRIZE NAME, PRIZE AREA and YEAR. We
take the same seed example as utilized in (Xu et
al., 2007), namely, the 1999 Nobel Chemistry win-
ner Ahmed H Zewail in our experiments2. The
seed looks like an database recond:
〈Ahmed H Zewail, Nobel, Chemistry, 1999〉
The corpus contains 2864 documents from

BBC, CNN and NYT, together 143289 sentences.
ERG covers around 70% sentences of the total cor-
pus. For our experiments we randomly divide the
parsable corpus into two parts: training and test
corpus, each containing the same number of sen-
tences. The average sentence length of the to-
tal corpus is around 20 words. If we look at the
domain relevant sentences, namely, those contain
both person name mentions and prize name men-
tions, they have an average length of around 30.
Among those relevant ones, the average length of
the sentences parsable by ERG is around 25.

Experiments Two phases of experiments are
conducted. In the training phase, we show that
reranking improves RE performance. The test
phase applies the reranking model resulting from
the training phase to the test corpus. In both
phases, two different experiments are conducted
1) Baseline: without reranking; 2) reranking: with
parse reranking. In the baseline experiment, we

2Uszkoreit et al. (2009) show that for the given dataset
the particular choice of the single seed instance does not have
any affect on the performance.

121



keep the first n readings of all sentences and
run DARE for rule learning and RE on top of
these readings. The aim is to observe whether
correct relation instances can also be detected in
lower-ranked readings. In the second experiments,
we aim to investigate whether reranking based
on task-feedback and domain knowledge is use-
ful for better extraction performance. These ex-
periments are conducted only with the best read-
ing after reranking, i. e. the normal setting of RE
application. In none of the experiments, confi-
dence thresholds are employed for improving pre-
cision by filtering out less confident rules or in-
stances. As we are mainly interested in the ef-
fects of reranking on RE recall, we are trying to
avoid any other factors that may influence the re-
call. Thus in our experiments confidence estima-
tion scores are only used for reranking.

Qualitative Analysis Given the experimental
results, we carry out various qualitative analysis
on the results of both parsing and RE. With respect
to parsing, we evaluate the results against the gold-
standard treebank before and after reranking. In
addition we evaluate the quality of the extraction
rules before and after the reranking.

5.2 Experiments

5.2.1 Training

Baseline Figure 4 shows the baseline evaluation
results. In this case, no confidence thresholds are
applied, therefore we have neither reranking nor
filtering. In order to monitor the contribution of
lower-ranked parses to RE, we add readings in log-
arithmic increments. We start with one reading,
namely the best reading proposed by the parser
and then in steps go up to 500. From each read-
ing, DARE tries to learn rules and to extract rela-
tion instances. When DARE only works with the
best reading, the precision is very high, namely,
87.83%, but with a very low recall of 45.18%.
When we increase the number of readings, we ob-
serve that precision drops while recall increases.
This confirms our suspicion that many good read-
ings are among the lower ranked ones in the cur-
rent maximum entropy-based parse model. There-
fore, reranking is important for lifting the good
readings to the top.

Reranking In the training phase, we learn
DARE rules from all 500 readings from all sen-
tences in the training corpus. Given the rules and
their confidence values, we rerank the 500 read-

Figure 4: Training phase (baseline): RE perfor-
mance w.r.t. the increase of readings

ings of each sentence in this corpus.

Reading 0 Precision Recall F1-Measure
Baseline (no reranking) 87.83% 45.18% 59.66%
After reranking 83.87% 56.19% 67.29%

Table 1: Training phase: Comparison of RE per-
formance before and after reranking.

Table 1 compares the RE performance with
just the first reading before reranking (baseline
experiment) and after reranking. As indicated,
the reranking strongly improves the recall value
(56.19% vs. 45.18%) and also yields a signifi-
cantly better F-measure (67.29% vs. 59.66%).

Figure 5 illustrates the behavior of parse read-
ings with respect to the respective frequencies of
matches with extraction rules (indicating their use-
fulness for rule or instance extraction). After
reranking, the number of the higher ranked read-
ings that match with the RE rules is increased sig-
nificantly. This indicates that the higher ranked
readings after reranking are better suited for the
RE task.

Figure 5: Training phase: Distribution of parse
readings from 0 to 255 and their frequency of
matching rules before and after reranking

122



egyp%an	  
egyp%an_a1	  

scien%st	  
scien%st_n1	  

Ahmed	  
generic_proper_ne	  

Zewail	  
generic_proper_ne	  

won	  
win_v1	  

the	  
the_1	  

1999	  
generic_year_ne	  

nobel	  
nobel_n1	  

for	  
for	  

chemistry	  
chemistry_n1	  

aj-hdn_norm_c 

np_hdn_ttl-cpd_c 

np_hdn_nme-cpd_c sb-hd_mc_c 

prize	  
prize_n1	  

np-hdn_cpd_c 

np-hdn_cpd_c 

sp-hd_n_c 

hd_cmp_u_c 

hd_aj_int-unsl_c 

hd_cmp_u_c 

reading r0 

reading r2 

egyp%an	  
egyp%an_a1	  

hd_cmp_u_c 

scien%st	  
scien%st_n1	  

Ahmed	  
generic_proper_ne	  

Zewail	  
generic_proper_ne	  

won	  
win_v1	  

the	  
the_1	  

1999	  
generic_year_ne	  

nobel	  
nobel_n1	  

for	  
for	  

chemistry	  
chemistry_n1	  

aj-hdn_norm_c 

np_hdn_ttl-cpd_c 

np_hdn_nme-cpd_c sb-hd_mc_c 

prize	  
prize_n1	  

np-hdn_cpd_c 

np-hdn_cpd_c 

sp-hd_n_c 

hd_cmp_u_c hd_cmp_u_c 

Figure 3: An example of ambiguous parses with PP attachment

reranking Examples In our experiment, we uti-
lize the syntactic derivation tree of the HPSG anal-
ysis. Figure 3 shows two derivation trees of a sen-
tence (4) from the experimental domain corpus.

(4) Egyptian scientist Ahmed Zewail won the 1999
Nobel Prize for Chemistry

In Figure 3, in the first reading r0 the PP “for
chemistry” is wrongly attached to the verb “win”,
while r2 (the third reading) is more appropriate
since the PP here modifies the noun “prize”. The
DARE rule in Figure 6 is presented as a typed fea-
ture structure, which is learned from HPSG parses.
The value of its feature PATTERN contains the
derivation tree structures relevant for the target re-
lation, while the value of the feature OUTPUT
represents the co-indexing between the semantic
arguments of the target relation and the linguistic
arguments in PATTERN. Since this rule has a high
confidence value and it matches the reading r2, r2
is pushed to the top after reranking.

rule_30 
PATTERN pattern 

HEAD (“win_v1”) 
SB-HD_MC_C sb-hd_mc_c 

HEAD <person> 0 

HD-
CMP_U_C 

hd-cmp_u_c 
HEAD 1 <prize> 
HD-
CMP_U_C 

hd-cmp_u_c_2 
HEAD (“for_prtcl”) 
HD-
CMP_U_C 

hd-cmp_u_c_3 
HEAD 2 <area> 

OUTPUT relation 
area 
winner 
prize 

2 
0 
1 

Figure 6: An example DARE rule derived from
HPSG derivation trees

5.2.2 Testing

In the test phase, we apply the reranking model
trained in the training phase to the parsing of the

test corpus when performing RE. The reranking
model consists of RE rules with their respective
confidence values. These rules work as classi-
fiers that add their confidence values to the ranking
scores of matching readings.

Baseline First, we evaluate the performance of
the baseline system, i.e., parsing the test corpus
without reranking. Similar to the experiments on
the training corpus, we first examine the perfor-
mance of RE on different reading sets. The re-
sults are shown in Figure 7. Similar to the training
phase results, the recall and F-measure values in-
crease when more readings are taken into account.

Figure 7: Test phase (baseline): RE performance
with respect to the increase of readings.

Reranking Table 2 presents the extraction per-
formance after application of the trained reranking
model to the test corpus, using only the highest-
ranked reading. Similar to the training phase re-
sults, both recall and F-measure also improve sig-
nificantly in comparison to the baseline system be-
fore reranking.

123



Reading 0 Precision Recall F1-Measure
Baseline (no reranking) 82.93% 45.37% 58.56%
after reranking 80.33% 53.41% 64.16%

Table 2: Test phase: Comparison of RE perfor-
mance before and after reranking.

5.3 Qualitative Analysis

Experiments in both training and test phases con-
firm that our reranking improves RE recall and
F-measure. A further observation is that the re-
ranked best readings are much more compatible
with the learned extraction rules. Naturally, the
question arises whether reranking also improves
overall parsing accuracy.

5.3.1 Parsing before and after Reranking

Finally, we evaluate the general parsing accu-
racy before and after reranking. More specifi-
cally, we compare the syntactic structures against
a high-quality gold-standard treebank annotated
by the ERG grammar developer Dan Flickinger.
This evaluation indicates the general correctness
of the parser (or in particular the disambiguation
model).3

Table 3 reveals that the general parsing perfor-
mance suffers from reranking both with respect to
full trees and subtrees. To further narrow down the
effect of reranking, we manually marked the re-
gions (sub-strings in sentences) most relevant for
the target relations and calculated the parser scores
within those subtrees.4 The degradation of parser
performance (against gold annotation) is more sig-
nificant within these local regions.

Further error analyses show the breakdown of
the differences: Of the 113 test sentences, 68 show
a difference w.r.t. reranking. The labeled bracket-
ing accuracy (on relevant subtrees) increased for
13 sentences. Among these, 3 are due to better

3Since manual treebanking of HPSG derivation trees is
very expensive, the gold-standard treebank only contains 500
randomly selected domain relevant sentences in which both
persons and prizes are mentioned. Among these 500 sen-
tences, 113 are in the test corpus. Although this treebank
was developed independent from our research approach, the
113 sentences turn out to be useful because they are potential
candidates for RE rules and thus their readings can be more
effected by reranking than sentences which are irrelevant for
the target relation.

4We also evaluated the parsing performance on the sub-
trees selected by the relation extraction rules, whose results
are consistent with the above findings.

Model LBf1(full) LBf1(subtree)
MaxEnt 0.8613 0.8918

Reranked 0.7966 0.8132

Table 3: Labeled bracketing f-score

appositions, 2 to better selection of verb subcat
frames, 6 to better PP attachments. Of the 55
cases of degradation, main causes are: incorrect
compounding in NPs (24 cases), bad coordina-
tions (7 cases), wrong lexical categories (2 cases).

“good” for RE
Before reranking 50%
After reranking 85%

Table 4: “Good” readings for RE among 68 re-
ranked sentences

A careful study has been conducted on these
68 cases with respect to their effect on RE per-
formance. It turns out that after reranking more
of the parses are “good” for RE, i.e., leading to
good rules. A “good” rule is defined by us as
a rule which extracts correct instances. Table 4
shows that after reranking 85% of the 68 have
good parses as opposed to 50% before reranking.

An explanation for the drop of linguistic qual-
ity is that linguistically “wrong” analyses nev-
ertheless lead to consistent extraction of rules
and instances. For example, the gold-standard
bracketing of the compound noun “Nobel Peace
Prize laureate” is ((Nobel (Peace Prize)) laure-
ate). The reranking reading is ((Nobel Peace)
(Prize laureate)), which is wrong. However,
the rule derived from this wrong reading can
be applied to all equally incorrect readings of
similar compound nouns such as “Nobel Chem-
istry/Physics/Economics Prize laureate” to suc-
cessfully extract two arguments of the target rela-
tion, namely, PRIZE NAME and PRIZE AREA. Thus
the increased consistency in the re-ranked parses
does help improve the RE process.

5.3.2 Extraction Rules after Reranking

In the above analysis, we can learn the lessons that
a good reading for RE task is not necessary a lin-
guistically correct parse. The major contribution
of reranking is not the improvement of general lin-

124



guistic parse selection but the improvement of se-
lection of good readings for RE tasks.

Table 5 shows a comparison of the distribution
of the good readings before and after reranking in
test corpus. Bad readings are readings where bad
rules are learned, namely, rules which extract only
incorrect instances. Useless readings are readings
from which useless rules are learned. Useless rules
are rules which do not extract any instance. Ta-
ble 5 clearly demonstrates that the porportion of
good readings increases significantly after rerank-
ing, while the number of bad readings and useless
readings drop.

Good Reading Bad Reading Useless Reading
before reranking 29.2% 1.3% 69.5%
after reranking 42.4% 0.8% 56.8%

Table 5: Test corpus: distribution of good readings
before and after reranking

We also compare the number of the learned
good rules and their extraction productivity. Af-
ter reranking, not only the number of good rules
increases, but also the average number of the in-
stances extracted by each good rule is grown to
4.3 in comparison to 3.5 before reranking. The
growth of good readings and rules and the produc-
tivity of rule extraction performance explains the
recall improvement after the parse reranking.

6 Related Work
Various attempts have been made to improve the
cross-domain performance of statistical parsing
models. McClosky et al. (2006) uses self-training
to improve Charniak’s parser by feeding large
amount of unannotated texts to the parser. Plank
(2009) utilize structural-correspondence learning
to improve the accuracy of the Dutch Alpino
parser on the Wikipedia texts. Rimell and Clark
(2008) show that a small set of annotated in-
domain data can significantly improve the CCG
parser’s performance. Hara et al. (2007) im-
proves the Enju HPSG parser performance in the
biomedical domain by a low-cost retraining of
the lexical disambiguation model. Nearly all ap-
proaches evaluate the parsing quality against a
“gold-standard” treebank. Miwa et al. (2010)
compares five parsers for bio-molecular event ex-
traction to investigate the correlation between the
performance on a gold-stand treebank and the
usefulness in real-world applications. All four
domain-adapted parsers achieve similar IE perfor-

mance and are better than the one not adapted.
The idea of reranking parses for better dis-

ambiguation is not new. Charniak and Johnson
(2005) presents a discriminative model for captur-
ing the linguistically motivated global properties
of the candidate parses proposed by the first-stage
generative parser. As the reranking model operates
on a relatively small set of candidates, it is able
to more accurately find the best reading. In the
same spirit, several applications such as named-
entity extraction (Collins, 2002), semantic pars-
ing (Toutanova et al., 2005a) and semantic label-
ing (Ge and Mooney, 2006) have taken advantage
of reranking approaches based on discriminative
models.

In contrast to the above proposals, our approach
does not need the annotated “gold-standard” data
for domain adaptation or training of the reranking
model. Our system exploits application feedback
for reranking. In a sense, the approach is akin
in spirit to the joint learning of multiple types of
linguistic structures with non-jointly labeled data
(Finkel and Manning, 2010), although in our case
the emphasis is entirely put on the application per-
formance.

7 Conclusion and Future Work
The main contribution of our work is a method
for adapting generic parsers to the tasks and do-
mains of relation extraction by parse reranking.
Our reranking is based on feedback from the ap-
plication. We could show that for one generic
parser/grammar, recall and f-measure could be
considerably improved and hope that this effect
can also be obtained for other generic parsers.
We do not worry much about the collateral de-
crease in precision, because precision will be
tightened again when we employ confidence es-
timation thresholds for filtering out less promising
rules and instances.

A side result of the work was the insight that a
better parse ranking for the purpose of relation ex-
traction does not necessarily correspond to a bet-
ter parse ranking for other purposes or for generic
parsing. This should not be surprising since re-
lation extraction in contrast to text understand-
ing does not need the entire and correct syntac-
tic structure for the detection of relation instances.
The ease and consistency of rule extraction and
rule application counts more than the linguisti-
cally correct analysis. The gained new insight that
the consistency of parse selection is more relevant

125



than parsing accuracy, we consider worth sharing.
The presented results may also be viewed as a

step forward toward making deep linguistic gram-
mars useful for relation extraction, whereas up
to now most minimally supervised approaches to
RE have employed shallower robust parsers. The
hope behind these attempts is to improve precision
without losing too much recall. After reclaiming
recall through our parse reranking, next steps in
this line of research will be dedicated to balanc-
ing off the deficits in coverage by data-driven lex-
icon extension in the spirit of (Zhang et al., 2010)
and by exploiting the chart for partial parses in-
volving the relevant types of named entities. Fur-
thermore, the approach of (Dridan and Baldwin,
2010) to learning a parse selection model in an
unsupervised way by utilizing the constraints of
HSPG grammars might also be interesting for do-
main adaptive parse selection for relation extrac-
tion. At some point we may then be in a position
to conduct a fair empirical comparison between
deep-linguistic parsing with hand-crafted gram-
mars on the one hand and purely statistical parsing
on the other. An error analysis may then indicate
the chances for hybrid approaches. However, be-
fore targeting these medium-term goals we plan to
investigate whether our approach can also be ap-
plied to other parsers with inherent generic parse
ranking and whether the set of learned RE rules
with their confidence values can be directly used
as features in the statistical parse disambiguation
models instead of in the post-processing step by a
separate re-ranker.

Acknowledgements
This research was conducted in the context
of the German DFG Cluster of Excellence on
Multimodal Computing and Interaction (M2CI),
projects Theseus Alexandria and Alexandria for
Media (funded by the German Federal Ministry of
Economy and Technology, contract 01MQ07016),
and project TAKE (funded by the German Fed-
eral Ministry of Education and Research, contract
01IW08003). Many thanks go to Peter Adolphs
and Dan Flickinger for providing us the treebank
of the Nobel Prize award corpus.

References
Eugene Agichtein and Luis Gravano. 2000.

Snowball: Extracting relations from large plain-
text collections. In Proceedings of the 5th ACM
International Conference DL’00, San Antonio,
TX, June.

Sergey Brin. 1998. Extracting patterns and re-
lations from the world wide web. In WebDB
Workshop at EDBT 98.

Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent dis-
criminative reranking. In Proceedings of ACL
05, pages 173–180, Ann Arbor, Michigan, June.
Association for Computational Linguistics.

Michael Collins. 2002. Ranking algorithms
for named-entity extraction: Boosting and the
voted perceptron. In Proceedings of ACL ’02.

Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. The stanford typed dependen-
cies representation. In Proceedings of the work-
shop on Cross-Framework and Cross-Domain
Parser Evaluation at COLING08, Manchester,
UK.

Rebecca Dridan and Timothy Baldwin. 2010. Un-
supervised parse selection for hpsg. In Pro-
ceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing,
EMNLP ’10, pages 694–704, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.

Witold Drozdzynski, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Schäfer, and Feiyu Xu. 2004.
Shallow processing with unification and typed
feature structures — foundations and applica-
tions. Künstliche Intelligenz, 1.

Jenny Rose Finkel and Christopher D. Manning.
2010. Hierarchical joint learning: Improving
joint parsing and named entity recognition with
non-jointly labeled data. In Proceedings of ACL
’10, pages 720–728, Uppsala, Sweden.

Dan Flickinger. 2000. On building a more ef-
ficient grammar by exploiting types. Natural
Language Engineering, 6(1):15–28.

Ruifang Ge and Raymond J Mooney. 2006. Dis-
criminative reranking for semantic parsing. In

126



Proceedings of COLING and ACL 06), pages
263–270. Association for Computational Lin-
guistics.

Mark A. Greenwood and Mark Stevenson. 2006.
Improving semi-supervised acquisition of rela-
tion extraction patterns. In Proceedings of the
Workshop on Information Extraction Beyond
The Document, pages 29–35, Sydney, Australia,
July. Association for Computational Linguis-
tics.

Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference - 6: A brief his-
tory. In Proceedings of COLING 96, Copen-
hagen, June.

Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2005. Adapting a probabilistic disambigua-
tion model of an HPSG parser to a new domain.
In Proceedings of IJCNLP 05.

Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2007. Evaluating impact of re-training a
lexical disambiguation model on domain adap-
tation of an hpsg parser. In Proceedings of
the Tenth International Conference on Pars-
ing Technologies, pages 11–22, Prague, Czech
Republic, June. Association for Computational
Linguistics.

Dekan Lin. 1998. Dependency-based evaluation
of MINIPAR. Workshop on the Evaluation of
Parsing Systems.

David McClosky, Eugene Charniak, and Mark
Johnson. 2006. Reranking and self-training for
parser adaptation. In Proceedings of COLING
and ACL 06, pages 337–344, Sydney, Australia.

David McClosky, Eugene Charniak, and Mark
Johnson. 2010. Automatic domain adapta-
tion for parsing. In Proceedings of HLT and
NAACL ’10, pages 28–36, Los Angeles, Cal-
ifornia, June. Association for Computational
Linguistics.

Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara,
and Jun’ichi Tsujii. 2010. A comparative study
of syntactic parsers for event extraction. In
Proceedings of the 2010 Workshop on Biomedi-
cal Natural Language Processing, pages 37–45,
Uppsala, Sweden, July. Association for Compu-
tational Linguistics.

Ion Muslea. 1999. Extraction patterns for infor-
mation extraction tasks: A survey. In AAAI
Workshop on Machine Learning for Informa-
tion Extraction, Orlando, Florida, July.

Stephan Oepen, Kristina Toutanova, Stuart
Shieber, Christopher Manning, Dan Flickinger,
and Thorsten Brants. 2002. The LinGO Red-
woods treebank: motivation and preliminary
applications. In Proceedings of COLING 2002:
The 17th International Conference on Compu-
tational Linguistics: Project Notes, Taipei, Tai-
wan.

Barbara Plank and Gertjan van Noord. 2010.
Grammar-driven versus data-driven: Which
parsing system is more affected by domain
shifts? In Proceedings of the 2010 Workshop
on NLP and Linguistics: Finding the Common
Ground, Uppsala, Sweden, July.

Barbara Plank. 2009. A comparison of structural
correspondence learning and self-training for
discriminative parse selection. In Proceedings
of the NAACL HLT 2009 Workshop on Semi-
supervised Learning for Natural Language Pro-
cessing, pages 37–42, Boulder, Colorado, June.
Association for Computational Linguistics.

Carl J. Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University
of Chicago Press, Chicago, USA.

Laura Rimell and Stephen Clark. 2008. Adapt-
ing a lexicalized-grammar parser to contrasting
domains. In Proceedings of EMNLP 08, pages
475–484, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.

K. Sudo, S. Sekine, and R. Grishman. 2003.
An improved extraction pattern representation
model for automatic IE pattern acquisition.
Proceedings of ACL 2003.

Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. 2005a. Joint learning im-
proves semantic role labeling. In Proceedings
of ACL 05, page 589 596. Association for Com-
putational Linguistics.

Kristina Toutanova, Christoper D. Manning, Dan
Flickinger, and Stephan Oepen. 2005b.
Stochastic HPSG parse selection using the Red-
woods corpus. Journal of Research on Lan-
guage and Computation, 3(1):83–105.

127



Hans Uszkoreit, Feiyu Xu, and Hong Li. 2009.
Analysis and improvement of minimally super-
vised machine learning for relation extraction.
In 14th International Conference on Applica-
tions of Natural Language to Information Sys-
tems. Springer.

Feiyu Xu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning frame-
work for extracting relations of various com-
plexity. In Proceedings of ACL 2007, Prague,
Czech Republic, 6.

Feiyu Xu, Hans Uszkoreit, Sebastian Krause, and
Hong Li. 2010. Boosting relation extraction
with limited closed-world knowledge. In Pro-
ceedings of COLING ’10, Poster Session. Asso-
ciation for Computational Linguistics.

Feiyu Xu. 2007. Bootstrapping Relation Extrac-
tion from Semantic Seeds. Phd-thesis, Saarland
University.

Roman Yangarber. 2001. Scenarion Customiza-
tion for Information Extraction. Dissertation,
Department of Computer Science, New York
University, New York, USA.

Yi Zhang and Rui Wang. 2009. Cross-domain de-
pendency parsing using a deep linguistic gram-
mar. In Proceedings of the Joint Conference
ACL and AFNLP 09, Suntec, Singapore, Au-
gust.

Yi Zhang, Stephan Oepen, and John Carroll.
2007. Efficiency in unification-based N-best
parsing. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies
(IWPT 2007), pages 48–59, Prague, Czech.

Yi Zhang, Timothy Baldwin, Valia Kordoni, David
Martinez, and Jeremy Nicholson. 2010. Chart
mining-based lexical acquisition with precision
grammars. In Proceedings of HLT and NAACL
’10, HLT ’10, pages 10–18, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.

128


