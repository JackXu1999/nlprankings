



















































Phrase-Based SMT for Finnish with More Data, Better Models and Alternative Alignment and Translation Tools


Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 391–398,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

Phrase-Based SMT for Finnish with More Data, Better Models and
Alternative Alignment and Translation Tools

Jörg Tiedemann
University of Helsinki

Fabienne Cap
Uppsala University

Jenna Kanerva and Filip Ginter
University of Turku

Sara Stymne
Uppsala University

Robert Östling
University of Helsinki

Marion Di Marco
University of Stuttgart

Abstract

This paper summarises the contributions
of the teams at the University of Helsinki,
Uppsala University and the University of
Turku to the news translation tasks for
translating from and to Finnish. Our
models address the problem of treating
morphology and data coverage in various
ways. We introduce a new efficient tool
for word alignment and discuss factori-
sations, gappy language models and re-
inflection techniques for generating proper
Finnish output. The results demonstrate
once again that training data is the most
effective way to increase translation per-
formance.

1 Introduction

In this paper we revisit phrase-based models with
and without factors to translate from and into a
morphologically-rich language, Finnish. We dis-
cuss the impact of training data, the use of fac-
tored models and ideas of re-inflection as post-
processing. We also introduce the framework
of gappy language models within document-level
machine translation (without much success in the
given task). Our efforts prove the importance of
training data once again and demonstrate the use
of noisy and out-of-domain data sets as well as the
possibility of integrating synthetic training data
based on back-translation in phrase-based SMT.

2 Data and Tools

This section discusses data sets and tools that we
applied in our models. We focus on non-standard
resources but also summarise the basic setup of
our training procedures.
Training Data: Our submissions include con-
strained and unconstrained systems. The con-

strained systems apply all the data provided by
WMT and also the English Giga-Word corpus that
is distributed by the LDC. Our best systems in-
clude additional parallel data sets coming from
OPUS (Tiedemann, 2012) and syntactically anal-
ysed monolingual data from the Finnish Internet
Parsebank (Luotolahti et al., 2015). Additional to
the parallel data we used in our submission last
year (Tiedemann et al., 2015a), we include the
new version of the OpenSubtitle corpus (Lison and
Tiedemann, 2016) with its 18.6 million aligned
translation units in English and Finnish. Further-
more, we make use of alternative subtitle transla-
tions that have been aligned monolingually in the
same collection (Tiedemann, 2016). Expanding
the parallel corpus with alternative translations ex-
tends the subtitle corpus by roughly 350,000 trans-
lation units with about 6.8 million tokens (count-
ing both languages together). The contribution is
quite small compared to the original corpus with
its 107 million Finnish tokens and 167 million En-
glish tokens, but, nevertheless, it contributes to
the overall collection especially by providing addi-
tional variation of the translation examples, which
is very valuable for the resulting system. The fi-
nal training corpus contains 27.7 million transla-
tion units comprising 353 million English tokens
and 244 million tokens in the Finnish part.

For Finnish, we also increased the coverage of
our language model by further 4.9 billion tokens
compared to our last year submission. The data
comes from an extensive web-crawl and amounts
to 9.5 billion tokens of text, deduplicated on doc-
ument level. Five-gram language models are
trained using KenLM (Heafield et al., 2013). The
English language model based on the provided
Common Crawl data is limited to trigrams.
Pre-Processing Tools: For processing Finnish,
we apply the Finnish parsing pipeline devel-
oped at the University of Turku (Haverinen et

391



al., 2013). It integrates all the necessary pre-
processing steps including tokenisation, morpho-
logical analyses and part-of-speech tagging, and
produces dependency analyses according to the
Universal Dependencies scheme.1 The morpho-
logical component relies on OMorFi - an open-
source finite-state toolkit with a large-coverage
morphology for modern Finnish (Lindén et al.,
2009). The readings given by OMorFi are com-
bined with predictions of the MarMoT CRF-based
tagger (Mueller et al., 2013), and the data is sub-
sequently parsed using the mate-tools data-driven
dependency parser (Bohnet, 2010). The labeled
attachment score of the parsing pipeline is 82.7%
and the pipeline is robust and reliable even for
large data sets and long sentences (Pyysalo et al.,
2015).

We also apply various pre-processing tools pro-
vided by the Moses toolbox. In particular, we
make use of tokenisers (especially for English),
punctuation and Unicode normalisers.

For the factored models of English, we built
our own pre-processing pipeline mainly adapted
from the Finnish pipeline but adjusted for process-
ing English. They include tools for handling long
sentences and keeping track of sentence alignment
points when parsing parallel data sets. We use the
English models for sentence boundary detection
and tokenisation provided by OpenNLP,2 which
is compatible with the Penn Treebank style of to-
kenisation. This is important for the subsequent
tagging and parsing steps, which we trained on
the Universal Dependencies treebank for English
using MarMoT and mate-tools.
MT Tools: Most of our systems are based on
Moses (Koehn et al., 2007) and common compo-
nents for training and tuning models. We apply
KenLM (Heafield et al., 2013) and SRILM (Stol-
cke, 2002) for estimating language model param-
eters and MERT (Och, 2003) and batch-MIRA
(Cherry and Foster, 2012) for parameter tuning.
Most of our models are based on lowercased train-
ing data. All language models use order five with
modified Kneser-Ney smoothing if not stated oth-
erwise. All MT systems apply the phrase-based
paradigm, some of them with factored representa-
tions and generation models if necessary.

For word alignment we experiment with differ-
ent tools. We apply standard tools like GIZA++

1http://universaldependencies.org
2https://opennlp.apache.org

(Och and Ney, 2003) and fast align (Dyer et al.,
2013) but also the recently proposed Bayesian
word aligner efmaral (Östling, 2015). Efmaral
is an efficient implementation of a Markov-Chain
aligner using Gibbs sampling with a Bayesian
extension of the IBM alignment models. It is
both fast and accurate and works as a straightfor-
ward plug-in replacement for standard tools in the
SMT training pipeline. The aligner is faster than
fast align but more accurate in terms of alignment
error rate in various benchmark tests. The ad-
vantage of using Gibbs sampling rather than the
Expectation-Maximisation algorithm (as do both
fast align and GIZA++) is that inference remains
quadratic with respect to sentence length even
when word order and fertility models are added,
which enables the efficient use of higher-order
models. This is the first time that the performance
of this tool is reported in the setting of statistical
machine translation.

Besides Moses, we also apply another phrase-
based machine translation decoder, Docent (Hard-
meier et al., 2013), which implements a stochas-
tic local search decoder that is able to incor-
porate features with long-distance dependencies
even across sentence boundaries. Docent empha-
sises document-level decoding but includes stan-
dard local features that make the decoder com-
parable with standard phrase-based SMT. The de-
coding algorithm applies randomly selected state-
change operations to complete translation hy-
potheses (covering the whole document) that may
be accepted by a strict hill-climbing procedure or
a simulated annealing schedule. The main motiva-
tion for using Docent in our setup is to introduce
non-local dependencies that may improve, for ex-
ample, agreement problems in morphologically-
rich languages such as Finnish. However, the ex-
periments are very initial and, unfortunately, do
not show the desired effect yet.

3 Translating English into Finnish

Our main efforts went into the development of
translation models for the direction from English
to Finnish. Four types of experiments were con-
ducted: (1) Changing word alignment and data
sets; (2) Factored models with morphological fea-
tures; (3) Re-inflection models with robust gener-
ation from underspecified representations; and (4)
Gappy language models for long-distance depen-
dencies.

392



3.1 Changing Alignment and Adding Data

Our first series of experiments considered three
different word alignment tools that can be used
in the training pipeline of standard phrase-based
SMT. We use the well-known IBM alignment
models (up to model 4) implemented in GIZA++,
the modified IBM model 2 implemented in
fast align and the above introduced Bayesian word
aligner based on fertility-enhanced HMM mod-
els implemented in efmaral. Table 1 summarises
the results when applied in the constrained setup
and tested on the news test set from WMT 2015.
The three models use the same feature weights
and the same symmetrisation and phrase extrac-
tion/scoring parameters to make the scores compa-
rable with each other. The results indicate that ef-
maral is comparable and even better than GIZA++
in this setup even though it is magnitudes faster
than the IBM model 4 training and Viterbi align-
ment. Efmaral is also considerably faster than
fast align, which makes it a valuable drop-in re-
placement of these standard tools. The process-
ing times in Table 1 illustrate the significant gains
when using efmaral making it possible to quickly
align large amounts of bitexts. The advantage over
fast align can mainly be seen in CPU time with
a speed-up of almost a factor of 10. fast align,
however, has the advantage to naturally run mul-
tithreaded over many cores whereas the collapsed
Gibbs sampler of efmaral is not as easily paral-
lelised. This can also be seen in our experiments
which we ran on a 16 core machine with align-
ment in both directions in parallel. GIZA++ is by
far the slowest option and does not lead to better
translations either. The figure also excludes word
clustering which is another time-consuming pro-
cess that is necessary for running IBM model 4.

Part of the experiment is also the inclusion of
additional training data. All those runs use ef-
maral, demonstrating that the software is capable
to cope with large data sets. Note, however, that
memory requirements grows with the size of the
data (

∑
e,f (|e| × |f |)) making it possible to run

efficiently. The results of our experiments show
that the additional data is useful even though it is
coming from inappropriate domains. Especially
striking is the gain by including alternative subtitle
translations – a rather small part of the data. Ap-
parently, those examples introduce necessary vari-
ations to push the quality of the models. Another
impressive improvement can be seen with the in-

time for word align
newstest 2015 BLEU real CPU
GIZA++ 13.65 38,514s –
fast align 13.56 682s 8,344s
efmaral 14.10 370s 895s
+ OPUS 14.81 – –
+ alternatives 15.55 2,630s 6,599s
+ WWW-LM 16.98 – –
retuned 18.11 – –
back-translated 14.78 954s 2,606s
+ OPUS, ... 18.22 2,758s 7,187s

Table 1: Lower-cased BLEU scores for standard-
phrase based SMT on development test data (new-
stest 2015). The first three and the second-to-
last rows represent constrained settings whereas
the other rows refer to systems with additional re-
sources. Efmaral is used in all cases except for the
two models at the top. The last two systems in-
clude back-translated news data. Running time is
given for some aligners in terms of walltime (real)
and CPU time (user+sys).

troduction of the large language model based on a
diverse set of data. This Finnish language model is
estimated on the Finnish Internet Parsebank (Lu-
otolahti et al., 2015), totaling 9.5 billion tokens
of text. The data is obtained from a large-scale
Internet crawl, seeded from all Finnish pages in
CommonCrawl.3 However, actual CommonCrawl
data is only a small fraction of the total, roughly
1.5B tokens, the remainder originating from an
independent crawl. The data is heavily filtered,
only preserving clean, parseable text comprising
of complete sentences.

Even the models with additional data use the
same feature weights and only replace the indi-
cated component to enable comparisons between
them. The system denoted by “retuned”, however,
shows the importance of proper tuning when re-
placing system components.

The final part of Table 1 shows additional re-
sults with back-translated news data in the con-
strained and unconstrained setup. We used our
Finnish-English model to translate approximately
1.25 million sentences of the Finnish shuffled
monolingual news data from 2014 and 2015 to
enhance the parallel training data. The result in
terms of BLEU significantly improves when these
noisy data sets are included in the standard train-

3www.commoncrawl.org

393



ing pipeline. Note that the models are retuned
from scratch in both cases.

3.2 Factored Models

The factored models we developed use features
extracted from dependency trees coming out of
the Finnish and English pre-processing pipelines.
We include separate translation models for trans-
lating between English surface word forms and
Finnish lemmas and for translating morphosyn-
tactic features between the two languages. The
latter includes dependency relations besides part-
of-speech labels (on both sides) and detailed mor-
phological information (in Finnish only). Table 2
summarises the results of these models.

newstest 2015 BLEU
(a) surface form 14.10
(b) morph 5.45
(c) constructions 10.89
combined (a) + (c) 14.17
+back-translated 14.70

Table 2: Lower-cased BLEU scores for factored
SMT models on development test data (newstest
2015). System (a) is the same as the constrained
model in Table 1. System (b) uses a factored
model that translates surface words to target lem-
mas and morphosyntactic features separately. Sys-
tem (c) keeps closed-class words in the transla-
tion table of morphosyntactic features. (b) and
(c) include a generation model trained on large
monolingual parsed training data to generate sur-
face word forms from lemmas and morphosyntac-
tic features.

The morphologically enhanced factored model
underperforms significantly when used in isola-
tion. Therefore, we used a variant of the setup
that replaces morphosyntactic features with sur-
face words for all closed-class words in the train-
ing corpus. The assumption is that there is suf-
ficient evidence for those word types even in
morphologically-rich languages such as Finnish.
Using this type of lexicalisation helps to find
construction-like mappings between the two lan-
guages which seems to be beneficial for the system
according to the scores in our experiments (system
(c) in Table 2). In combination with the surface-
oriented translation model this also leads to a
slight improvement over the non-factored model
(without back-translated news), which is also evi-

dent in the final scores of our submitted systems at
least in the constrained setup (see Table 4).

3.3 Re-inflection Models

Furthermore, we also investigated re-inflection
models. These experiments require a different
representation of the training data for each vari-
ant and are, therefore, not directly comparable
with the other systems. The underlying idea of
what we call re-inflection models in our submis-
sion is that we reduce all Finnish training data
to an underspecified representation, where words
are reduced to their lemmas and noun and ad-
jective compounds are split into their component
parts. Then, we train models and translate from
English into this underspecified representation of
Finnish and in a post-processing step we then
merge compounds and predict morphological fea-
tures for Finnish. This approach has been success-
fully applied to Russian and Arabic (Toutanova
et al., 2008) and to German (Fraser et al. (2012),
Cap et al. (2014)). Note however, that for example
Fraser et al. (2012) relied on German prepositions
to predict case-markers on underspecified German
SMT output. In contrast to many other languages,
Finnish only has a limited number of stand-alone
pre- and postpositions. Instead, the prepositional
meaning is encoded by case-marking. We thus
adapt an approach by Tiedemann et al. (2015b)
and introduce place-holder prepositions in the
Finnish training data, which are likely to corre-
spond to the prepositions used on the English side
and thus improve word alignment quality.
Place-holder Prepositions: In contrast to Tiede-
mann et al. (2015b), we do not apply factored
models (with both, lemmatised and surface forms)
here but strip the case-markers from those words
and only keep the underspecified representation.
Moreover, we apply the approach in the opposite
translation direction, which requires a generation
component. The place-holder prepositions will
not only lead to improved word alignments, but
we will also use them to predict case-markers af-
ter translation. Overall, we follow the processing
pipeline of (Cap et al., 2014): we use a rule-based
morphological analyser (Pirinen, 2015) to split
compounds (using the Finnish parsing pipeline to
disambiguate multiple analyses) and lemmatise all
Finnish training data. Compound modifiers are re-
duced to their lemmas and marked with a symbol
that distinguishes them from other words. Sim-

394



ilar to Tiedemann et al. (2015b), we introduce
place-holder prepositions at the beginning of noun
phrases bearing the corresponding case-marker in
order to support word alignment.

Prediction of Case-Markers After translation,
we apply CRF models to predict the case mark-
ers of Finnish. Besides the occurrences of place-
holder prepositions, these take some more local
context, both on lemma and POS level into ac-
count. Clean-data experiments have shown that
our CRF models for re-inflection are very accu-
rate. We reduce all compounds of the CRF train-
ing data to their heads and train the models on this
representation. As we are using the words and
lemmas as features for the CRFs, the reduction
of compounds to their heads reduces data sparsity
and allows the model to better generalise over all
occurrences. For the translation output we remove
all compound modifiers before case prediction.

Morphological Generation The predicted case-
markers are then fed into the morphological gen-
eration automaton (Pirinen, 2015) in order to get
fully inflected forms. In cases where this genera-
tion failed, we used a supervised machine learning
approach as a backoff (Durrett and DeNero, 2013).

Compound Processing In a final step, we
merge compounds using a POS-matching strategy
(Stymne et al., 2008). We merge the marked com-
pound modifiers with the following word if it is a
noun or adjective, and add hyphens for modifiers
in coordinated compounds. Compounding forms
of modifiers are restored based on corpus frequen-
cies. Like Stymne et al. (2008) and Cap et al.
(2014), we also merge compounds in every iter-
ation of the tuning process before the translations
are scored against the reference.

All re-inflection systems are constrained sys-
tems. We used Europarl and Wikipedia as par-
allel resources and all of the Finnish data avail-
able from WMT to train five-gram language mod-
els with SRILM (Stolcke, 2002) and KENLM
(Heafield, 2011). No particular cleaning or pre-
processing of the data has happened. This makes
the re-inflection systems differ from all other sys-
tems in this paper. Otherwise, we trained a con-
ventional phrase-based Moses system with default
settings, tuned weights using batch-MIRA with
”safe-hope” (Cherry and Foster, 2012) and used
an underspecified representation of the tuning ref-
erence set to derive BLEU scores. The final result
of our system is listed in Table 4.

3.4 Gappy Language Models
Tiedemann (2015) introduces the use of language
models over selected words in the framework
of document-level SMT using Docent applied to
the pronoun-aware translation task of DiscoMT
(Hardmeier et al., 2015). We extended this idea
by developing a general framework for what we
call gappy language models that refer to mono-
lingual or bilingual n-gram language models over
selected words and their alignments. We can use
different factors attached to the source and tar-
get language tokens to filter for word sequences
that we would like to consider. Given word align-
ments are used to establish the link between source
and target tokens. Gappy language models may
cross sentence-boundaries but may also stop at
those borders. Regular expressions can be used
to make the selection more flexible. Multi-word
alignments can be concatenated into single to-
kens and empty alignments can be represented as
a special token to avoid the length-penalising ef-
fect of N-gram models. Word selection based on
the source language also helps as this is given and
fixed. However, word alignment is noisy and may
negatively influence the use of the extracted target
item sequence. Therefore, the selection can also
be done on target language properties only and
an additional penalty feature is then used to con-
trol the length of the generated strings. Bilingual
models add both source and aligned target tokens
whereas monolingual models only use target lan-
guage tokens. Items are always sorted in the order
of the target language.

We experimented with various selections and
bilingual models to see the effect of these ad-
ditional features functions. Five-gram Language
model parameters are estimated using KenLM
(Heafield et al., 2013). Our main selection criteria
are part-of-speech patterns (matching coarse uni-
versal POS labels) and dependency relations:

• nouns and their alignments (sentence-internal
only and even document-wide)
• verbs and their alignments (sentence-internal

only and even document-wide)
• subject-predicate sequences (including nega-

tion particles) and their alignments
• closed-class words and their alignments

Gappy language models are fully integrated in
Docent but one unsolved problem is the tuning of
their weights. Currently, we do not have a stable

395



12.7

12.8

12.9

13.0

13.1

13.2

2
8

2
10

2
12

2
14

2
16

2
18

2
20

2
22

B
L

E
U

search steps

verbs (doc)
nouns (doc)
subj-pred-neg
closed-class

Figure 1: Adding gappy LM features and testing
on development test data (newstest 2015).

framework for finding appropriate values for them
and, hence, we needed to set them to a quite ar-
bitrary value (0.1 in our case). The disappointing
results of our extended models are shown in Fig-
ure 1. In general all of them seem to hurt perfor-
mance in the current setup.

4 Translating Finnish into English

The Finnish–English models re-use the factored
setup with pseudo-tokens that we introduced last
year (Tiedemann et al., 2015a). The main differ-
ences to the previous systems are (i) the use of
completely parsed bitexts even with the extended
data sets (last year we only parsed Europarl from
the constrained data), (ii) the large language model
coming from the provided common crawl data (tri-
gram model), and, (iii) improved compound split-
ting of surface words based on the morpholog-
ical analyses and the analysed lemma informa-
tion. For the latter, we use additional string match-
ing heuristics to properly split compounds even
if modifying components are inflected and can-
not be matched with the lemmatised analyses in
a straightforward way. Furthermore, we also add
morphological information to the modifying com-
pound components by looking up the most fre-
quent analyses of the given form in a large anal-
ysed monolingual corpus. The scores for our fac-
tored models in the constrained and unconstrained
settings are listed in Table 3.

Again, we can see the substantial impact of
additional out-of-domain training data. Alterna-
tive subtitle translations contribute marginally in
this translation direction. The common crawl data
is useful but slows down decoding quite signifi-
cantly.

newstest 2015 BLEU
basic 19.02
+ OPUS 21.42
+ alternatives 21.46
+ CC LM 22.09
basic + CC LM 19.33

Table 3: Lower-cased BLEU scores for factored
SMT models for Finnish-to-English on develop-
ment test data (newstest 2015).

5 Final Results and Discussions

Table 4 summarises the final scores when apply-
ing our models to the news test set from this year’s
evaluation campaign. A major, but not very sur-
prising effect is the reduction of unknown words
when adding more data. The factored model leads
to slight improvements in the constrained setting
but this does not carry over to the unconstrained
setup. A significant difference is the number of
unknown tokens which is much higher in the fac-
tored model. This may look surprising but when
inspecting the data, we could identify the reason
for this difference, which is due to the tokenisa-
tion applied in the factored setup. The models
applied in this approach make different decisions,
for example, when keeping numeric and monetary
expressions together. This increases the number
of unknown units without causing much harm in
most cases. Other cases are clearly tokenisation
errors. Some examples are listed below:
200k|ADJ|JJ|dep
228.89|NUM|CD|num
$22million|NOUN|NN|adpobj
2.5bn|NUM|CD|num
"wrestle|VERB|VB|xcomp
(yet|NOUN|NN|dobj

Note that the re-inflection model uses differ-
ent data pre-processing pipelines and, therefore,
the scores are not comparable with the others.
In a contrastive run we could see modest im-
provements over the baseline models without re-
inflection. Finally, we can also see that Finnish-
English suffers more from unknown tokens even
though we apply proper morphological analyses
and compound splitting. This is something that
we need to address in future work.

References
Bernd Bohnet. 2010. Very high accuracy and fast de-

pendency parsing is not a contradiction. In Proceed-

396



BLEU BLEU TER unknown words
English – Finnish lower cased #tokens #types
constrained - basic 13.3 12.7 0.782 1,582 862
constrained - factored 13.5 12.8 0.784 1,659 1,233
constrained - basic + back-translated 14.2 13.6 0.770 1,024 649
constrained + factored + back-translated 14.3 13.6 0.765 1,103 890
constrained - re-inflection 12.2 11.6 0.793
unconstrained - basic 17.0 16.2 0.746 124 60
unconstrained - factored 16.6 15.7 0.744 804 593
unconstrained - basic + back-translated 17.1 16.4 0.752 544 305

BLEU BLEU TER unknown words
Finnish – English lower cased #tokens #types
constrained - factored 20.5 19.3 0.706 2,655 2,004
unconstrained - factored 23.3 22.1 0.670 1,128 842

Table 4: Official results for the WMT 2016 news test set. The systems including the back-translated
news data were submitted after the deadline and will not be listed as official submissions. The system in
italics are marked for manual evaluation at WMT.

ings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 89–97.
Association for Computational Linguistics.

Fabienne Cap, Alexander Fraser, Marion Weller, and
Aoife Cahill. 2014. How to Produce Unseen
Teddy Bears: Improved Morphological Processing
of Compounds in SMT. In EACL’14: Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 579–587.

Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
HLT-NAACL’12: Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, volume 12, pages 34–35. Association
for Computational Linguistics.

Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
HLT-NAACL, pages 1185–1195.

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proceedings of NAACL,
pages 644–648.

Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word
formation in SMT. In EACL’12: Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 664–
674. Association for Computational Linguistics.

Christian Hardmeier, Sara Stymne, Jörg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine trans-
lation. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics:

System Demonstrations, pages 193–198, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.

Christian Hardmeier, Preslav Nakov, Sara Stymne, Jörg
Tiedemann, Yannick Versley, and Mauro Cettolo.
2015. Pronoun-focused MT and cross-lingual pro-
noun prediction: Findings of the 2015 DiscoMT
shared task on pronoun translation. In Proceedings
of the Second Workshop on Discourse in Machine
Translation, pages 1–16, Lisbon, Portugal, Septem-
ber. Association for Computational Linguistics.

Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missilä,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: The
Turku Dependency Treebank. Language Resources
and Evaluation, pages 1–39. In press. Available on-
line.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of ACL, pages 690–696.

Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ondřej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Proceedings of ACL, pages 177–180.

397



Krister Lindén, Miikka Silfverberg, and Tommi Piri-
nen. 2009. HFST tools for morphology — an effi-
cient open-source package for construction of mor-
phological analyzers. In State of the Art in Com-
putational Morphology, volume 41 of Communica-
tions in Computer and Information Science, pages
28–47. Springer.

Pierre Lison and Jörg Tiedemann. 2016. OpenSub-
titles2015: Extracting large parallel corpora from
movie and TV subtitles. In Proceedings of the 10th
International Conference on Language Resources
and Evaluation (LREC-2016), Portorož, Slovenia.

Juhani Luotolahti, Jenna Kanerva, Veronika Laippala,
Sampo Pyysalo, and Filip Ginter. 2015. Towards
universal web parsebanks. In Proceedings of the In-
ternational Conference on Dependency Linguistics
(Depling’15), pages 211–220. Uppsala University.

Thomas Mueller, Helmut Schmid, and Hinrich
Schütze. 2013. Efficient higher-order CRFs for
morphological tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 322–332. Association for
Computational Linguistics.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–52.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167.

Robert Östling. 2015. Bayesian Models for
Multilingual Word Alignment. Ph.D. the-
sis, Stockholm University. software at
https://github.com/robertostling/efmaral.

Tommi A. Pirinen. 2015. Omorfi —free and open
source morphological lexical database for Finnish.
In Proceedings of the 20th Nordic Conference
of Computational Linguistics (NODALIDA 2015),
pages 313–315.

Sampo Pyysalo, Jenna Kanerva, Anna Missilä,
Veronika Laippala, and Filip Ginter. 2015. Uni-
versal dependencies for Finnish. In Proceedings of
NoDaLiDa 2015, pages 163–172. NEALT.

Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modelling toolkit. In ICSLN’02: Proceedings
of the international conference on spoken language
processing, pages 901–904.

Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation (WMT’08), pages 135–138, Columbus, Ohio,
USA.

Jörg Tiedemann, Filip Ginter, and Jenna Kanerva.
2015a. Morphological segmentation and OPUS

for Finnish-English machine translation. In Pro-
ceedings of the Tenth Workshop on Statistical Ma-
chine Translation, pages 177–183, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

Jörg Tiedemann, Filip Ginter, and Jenna Kanerva.
2015b. Morphological segmentation and OPUS for
Finnish-English machine translation. In WMT’15:
Proceedings of the Tenth Workshop on Statistical
Machine Translation, pages 177–183.

Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC-2012), pages 2214–2218, Istan-
bul, Turkey, May. European Language Resources
Association (ELRA).

Jörg Tiedemann. 2015. Baseline models for pro-
noun prediction and pronoun-aware translation. In
Proceedings of the Second Workshop on Discourse
in Machine Translation, pages 108–114, Lisbon,
Portugal, September. Association for Computational
Linguistics.

Jörg Tiedemann. 2016. Finding alternative translations
in a large corpus of movie subtitles. In Proceedings
of the 10th International Conference on Language
Resources and Evaluation (LREC-2016), Portorož,
Slovenia.

Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In ACL’08: Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 514–522. Association for Computational
Linguistics.

398


