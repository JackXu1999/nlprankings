



















































A Multi-aspect Analysis of Automatic Essay Scoring for Brazilian Portuguese


Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter of the Association for Computational Linguistics,
pages 94–102, Valencia, Spain, April 3-7 2017. c©2017 Association for Computational Linguistics

A Multi-aspect Analysis of Automatic Essay Scoring for Brazilian
Portuguese

Evelin Carvalho Freire de Amorim
Computer Science Department

Universidade Federal de Minas Gerais
Minas Gerais, Brazil

evelin.amorim@dcc.ufmg.br

Adriano Veloso
Computer Science Department

Universidade Federal de Minas Gerais
Minas Gerais, Brazil

adrianov@dcc.ufmg.br

Abstract

While several methods for automatic essay
scoring (AES) for the English language
have been proposed, systems for other lan-
guages are unusual. To this end, we pro-
pose in this paper a multi-aspect AES sys-
tem for Brazilian Portuguese which we ap-
ply to a collection of essays, which human
experts evaluated according to the five as-
pects defined by the Brazilian Govern-
ment for the National High School Exam
(ENEM). These aspects are skills that stu-
dent must master and every skill is as-
sessed separately from one another.

In addition to prediction, we also per-
formed feature analysis for each aspect.
The proposed AES system employs sev-
eral features already used by AES sys-
tems for the English language. Our re-
sults show that predictions for some as-
pects performed well with the employed
features, while predictions for other as-
pects performed poorly.

Furthermore, the detailed feature analy-
sis we performed made it possible to note
their independent impacts on each of the
five aspects. Finally, aside from these con-
tributions, our work reveals some chal-
lenges and directions for future research,
related, for instance, to the fact that the
ENEM has over eight million yearly en-
rollments.

1 Introduction

The goal of automatic essay scoring (AES) sys-
tems is to score a given essay. AES systems are
relevant for educational institutions, since the hu-
man effort to evaluate essays is high and, and stu-
dents need feedback to improve his or her writing

skills. Besides these issues, almost every senior
high school student in Brazil should write an es-
say to the National Exam of High School (ENEM),
which Brazilian government uses to evaluate the
quality of high schooler’s education and that of
their institution.

Although there are thousands of essays written
to ENEM every year, to the best of our knowledge
there is no AES system for Brazilian Portuguese
(BP) language, or an analysis of features in a
multi-aspect essay scoring system for BP. Each as-
pect is a skill that students must master as seniors
in high school. Nonetheless, several AES systems
have been proposed for the English Language. At-
tali and Burstein (Attali and Burstein, 2006) pro-
posed an AES system, called e-rater, that employs
general features of argumentative essays to scor-
ing prediction. The main features used by e-rater
are grouped into the following types: grammar, us-
age, mechanics, and style; organization and devel-
opment; lexical complexity; and prompt-specific
vocabulary usage. e-rater employs multiple re-
gression, and it is task independent AES system,
i.e., its score is independent of the given prompt.

Napoles and Callison-Burch (Napoles and
Callison-Burch, 2015) employ linear regression
to an AES system that intends to assign more
uniform grades than multiple human evaluators.
Similar to our task, Napoles and Callison-Burch
propose the task of multi-aspect classification us-
ing five grading categories. However, the authors
leave unexplained how each of their aspects is af-
fected by their features. We think this is a sig-
nificant contribution since students or professors
can use features as a feedback for better under-
standing essays writing. Besides that, Napoles and
Callison-Burch assume that more than one evalu-
ator is available to train their model, which in the
real world is not always the case.

Larkey (Larkey, 1998) proposed three models

94



that are based on text classification to score essays
applying linear regression. However, Larkey strat-
egy is task-dependent. Chen and He (Chen and
He, 2013) also grouped features into four main
types: lexical features; syntactical features; gram-
mar and fluency features; content and prompt-
specific features. Then, the authors proposed a
rank-based algorithm that maximizes the agree-
ment between human score and machine score.
Zesh et al. (Zesch et al., 2015) developed a tech-
nique that adapts domain in an AES system. The
authors tested their method in an English dataset
and a German dataset. Chali and Hasan (Hasan,
2012) proposed an LSA-based method, that is
task-dependent, and the goal was to establish a
strategy to understand the inner meaning of texts.
Beyond the English language, Kakkonen and Suti-
nen (Kakkonen and Sutinen, 2004) developed an
AES system to the Finnish language also based on
LSA algorithm.

Besides assigning grade score, other researches
proposed to analyze argumentation strength of es-
says (Persing and Ng, 2015), discourse structure
of essays (Song et al., 2015)(Stab and Gurevych,
2014) , and grammar correction in general (Ro-
zovskaya and Roth, 2014)(Lee et al., 2014).

Our research is different from the previous re-
search since we aim to answer the following ques-
tions:

1. How objective features behave in a multi-
aspect automatic essay scoring system?

2. Which features are more relevant for each as-
pect?

In addition to this, we aim to pose some in-
teresting questions for future research. Our es-
says present not only grades but also evaluator’s
comments about the aspects that are considered
in the ENEM. During the exploration of evalua-
tors comments, bias was observed in some evalua-
tions, which we define as being when some human
evaluator seems to disagree or agree with student’s
point of view, which can lead to an improper influ-
ence on the student’s grade. The possibility of bias
of human evaluations raises some questions.

1. Are some topics for essays more prone to re-
sult in biased evaluation?

2. Is it possible to detect if human evaluator is
biased for or against a given student’s point
of view?

3. If it is possible to detect the bias of human
evaluator, is it feasible to measure the quanti-
tative affect on grades?

4. Is there any difference between the words in
biased evaluations that agrees with student
point of view and biased evaluations that dis-
agrees with student’s point of view?

In a nutshell, the availability of evaluator com-
ments allows for a host of issues related to bias
detection, quantification, and resolution, yet as far
as we know these questions are still unanswered.

The paper is organized as follows. The second
section details our dataset and the features we use.
The third section explains the experiments we per-
formed and the results of our experiments. The
fourth section presents the main remarks about our
research and the fifth section point to the future di-
rection for our research.

2 Methodology

We propose a methodology that besides the usual
features employed by popular AES methodolo-
gies (Attali and Burstein, 2006) (Chen and He,
2013) (Zesch et al., 2015), it also takes advantage
of domain features. To test our proposed features,
we used a dataset of nearly 1840 essays. Next sec-
tions describe our dataset and our features.

2.1 Dataset

Our dataset is composed of 1840 essays about
96 topics, which were crawled from UOL Essay
Database website1. The average length in words
are 300.51; the biggest essay has 1293 words, and
the smallest essay has 49 words. Each essay is
evaluated according to the following five aspects:

1. Formal language: Mastering of the formal
Portuguese language.

2. Understanding the task: Understanding of
essay prompt and application of concepts
from different knowledge fields, to develop
the theme in an argumentative dissertation
format.

3. Organization of information: Selecting,
connecting, organizing, and interpreting in-
formation, facts, opinions, arguments to ad-
vocate a point of view.

1http://educacao.uol.com.br/bancoderedacoes

95



Table 1: Score and corresponding levels
Score Level
2.0 Satisfactory
1.5 Good
1.0 Regular
0.5 Weak
0.0 Unsatisfying

Table 2: Average Score for each aspect and final
grade in UOL Dataset

Aspect Average Score
Formal Language 1.1
Understanding the task 0.91
Organization of information 0.93
Knowing argumentation 0.83
Solution proposal 1.08
Final grade 4.86

4. Knowing argumentation: Demonstration of
knowledge of linguistic mechanisms required
to construct arguments.

5. Solution proposal: Formulation of a pro-
posal to the problem presented, respecting
human rights and considering socio-cultural
diversity.

Each aspect is scored according to the scale of
Table 1, and the final score is the sum of all aspects
scores. Table 2 depicts the average score assign
by humans for each aspect and final grade in our
dataset.

Each essay is evaluated by only one human. Al-
though this seems a disadvantage, we think that
this is a real world dataset, since in most high
schools only one teacher scores essay. Also, as we
aim to detect the impact of features in each aspect,
one evaluator per essay is enough.

2.2 Features
Features are divided into two main types, domain
features that are related to ENEM exam or Brazil-
ian Portuguese Language, and general features
that are based on Attali and Burstein research (At-
tali and Burstein, 2006).

1. Domain features: ENEM exam doesn’t al-
low the using of the first person pronouns and
verbs. Therefore, we employ as features the
number of first person pronouns and verbs

and the number of first person pronouns and
verbs per number of tokens. Also, we sug-
gest as feature the number of ênclise, a Por-
tuguese language structure, and the number
of ênclise per number of tokens. Ênclise
is unusual to BP spoken language, then if a
student applies such concept in essay, proba-
bly he or she knows how to use formal lan-
guage better. Also, the excessive number
of demonstrative pronouns is condemned in
written BP (Martins, 2000); then we use the
number of demonstrative pronouns and the
number of demonstrative pronouns per num-
ber of tokens.

2. General: Most of the general features are
based on Attali and Burstein (Attali and
Burstein, 2006) research, which presented
ten features. However, due to lack of
tools for Brazilian Portuguese and time con-
straints, we implemented only six features
and adapted two features from the e-rater
framework. Next, we detailed our feature im-
plementation.

• Grammar and style: Grammar was
checked by CoGrOO (Kinoshita et al.,
2006), which is a Brazilian add-on to
Open Office Writer. Also, for spelling
mistakes we use a Brazilian software
2. Both features were also divided by
the number of tokens in an essay; then
we employed four features for grammar
and spelling errors. To evaluate style in
essays, we applied LanguageTool rules
for Portuguese, but also we added some
rules suggested by a Portuguese manual
of writing (Martins, 2000)3. We em-
ployed the number of style errors and
the number of style of errors per sen-
tence as features.
• Syntactical features: According to

(Martins, 2000), in Portuguese Lan-
guage, sentences longer than 70 char-
acters are long sentences, and therefore
are not recommended. We employ as a
feature, the number of sentences longer
than 70 characters.
• Organization and development: There

2https://github.com/giullianomorroni/JCorretorOrtografico
3Rules can be examined in https://goo.gl/

F32hcC

96



are no tools to evaluate organization and
development in Portuguese language,
then we collected discourse markers in a
Brazilian Portuguese grammar (Jubran
and Koch, 2006). Discourse markers
are linguistic units that establish connec-
tions between sentences to build coher-
ent and knit discourse. We employed as
features the number of discourse mark-
ers and the number of discourse markers
per sentence.
• Lexical complexity: To evaluate lexi-

cal complexity, we used four features.
The first feature is Portuguese version of
Flesh score (Martins et al., 1996); the
second feature is average word length,
which length is the number of syllables;
the third feature is the number of to-
kens in an essay; the fourth feature is the
number of different words in an essay.
• Prompt-specific vocabulary usage: It is

desirable to employ concepts from the
prompt in the essay, therefore for each
essay we compute cosine similarity be-
tween prompt and essay. In this case, the
prompt is a frequency vector of words,
and the essay is also a frequency vector
of words, which are from the prompt vo-
cabulary. We decided for this strategy
since, unlike other works, our dataset
comprises many different topics, each
with few essays. Then, we think that
build a vocabulary for each domain it is
not helpful.

3 Experiments

We performed two types of experiments: one eval-
uating the performance of grade prediction for
each aspect and other evaluating the role of each
feature in grade prediction task. Feature analy-
sis is of particular importance for this task since
computer evaluation of an essay is different from
a human analysis. Therefore, explore which vari-
able is important for which aspect is crucial for the
development of our research.

3.1 Prediction Analysis
Besides ASAP challenge at Kaggle4, several
works employ quadratic weighted kappa as the

4https://www.kaggle.com/c/asap-aes/details/evaluation

Table 3: List of Features grouped into domain type
and general type

Group Feature
#first person of singular of
verbs and pronouns
#first person of singular of
verbs and pronouns / #tokens

Domain #demonstrative pronouns
#demonstrative pronouns / #tokens
#enclise
#enclise / #tokens
#sentences longer than 70 characters
#grammar errors
#grammar errors / #token
#spelling errors
#spelling errors / #token

General #style errors / #sentences
#discourse markers
#discourse markers / #sentence
Flesh score
Average word length (syllables)
#tokens
similarity with prompt
#different words

evaluation metric (Zesch et al., 2015)(Chen and
He, 2013)(Attali and Burstein, 2006), which aims
to measure agreement between human evaluation
and machine scoring. When the value of kappa is
closer to 1, the higher the agreement between eval-
uators, and when the value of kappa is closer to 0,
the lower the agreement between evaluators.

First, we compute a matrix of weights (Equation
1) that are based on the difference between human
evaluation and machine scoring.

wi,j =
(i− j)2
(N − 1)2 (1)

The second step calculates a histogram matrix
called O, where Oi,j is the number of essays that
receive grade i ∈ N by a human evaluator and a
grade j ∈ N by a machine evaluator. After that,
we built another matrix E of expected ratings,
which is the outer product between each rater’s
histogram vector of ratings. Finally, we employ
O,E, and w to compute the quadratic weighted
kappa using Equation 2.

97



Table 4: Kappa values for each grade aspect
Grade Type Kappa
Final Grade 0.3673
Formal Language 0.3147
Understanding the task 0.2678
Organization of Information 0.2305
Knowing argumentation 0.2704
Solution proposal 0.1393

Table 5: Kappa values for each grade aspect after
oversampling (full and general feature set)

Grade Type Full General
Final Grade 0.4245 0.4131
Formal Language 0.3351 0.3249
Understanding the task 0.1817 0.1822
Organization of Information 0.2728 0.2679
Knowing argumentation 0.2668 0.2484
Solution proposal 0.1542 0.1430

κ = 1−
∑

i,j wi,jOi,j∑
i,j wi,jEi,j

(2)

A simple regression is applied to predict the fi-
nal grade of essays, and each of other five aspects.
Also, a simple oversampling strategy is applied
since grade distribution is unbalanced (Figure 1).

In the first step of oversampling strategy, it
searches by the class Gmax that holds the largest
number of instances. Then the strategy randomly
selects instances from every class G 6= Gmax and
replicates such instances into training datasets, un-
til the size of every class G 6= Gmax be equal the
size of Gmax.

Table 4 describes the results using quadratic
weighted kappa before the oversampling. We exe-
cuted cross-validation five times and compute the
average of kappas of all experiments, for each
aspect and final grade, to evaluate oversampling
performance. Results after oversampling are de-
scribed in Table 5.

Considering the lack of tools for processing the
Portuguese language, and the limited performance
of the few existing tools, the multi-aspect classi-
fication performed satisfactorily. However, some
aspects performed poorly probably due to the sub-
jectivity intrinsic to these aspects and objective
variables probably can’t capture all the subjectiv-
ity.

3.2 Feature Analysis

Besides kappa results, we also performed an ex-
periment that investigates the impact of each fea-
ture in each aspect and final grade. The experi-
ments were performed removing each feature and
measuring the resulting kappa. If removing a fea-
ture f lowers the resulting kappa, then that fea-
ture is relevant to the model of that aspect. Ac-
cording to this criterion, the lower the resulting
kappa when removing f from the training model,
the more important is f for this model. Table
3.2 describes the three features that most dimin-
ished kappa value and the three features that most
increased kappa value. The full value in table
present the result with the full set of features de-
scribed earlier.

It is possible to observe that the most relevant
features for the final grade are not necessarily a
mix of relevant features from the aspects. For in-
stance, vocabulary level is one of three most im-
portant features for the final grade, but, while not
irrelevant, it is not in the top three for the as-
pects. To understand better the role of vocabulary
level, we compute in our dataset average vocab-
ulary level for the final grade, and, as expected,
the higher the grade, the higher the number of dif-
ferent words in essays. Besides vocabulary level,
lexical complexity seems to play a significant role
to final grade, since three of the most important
features to final grade prediction affect prediction.

Aspect Understanding the task presented the
lowest kappa value between aspects. However, we
can draw some conclusions from Table 3.2. For
instance, Flesh score affected expressively kappa
value. Also, we observe that current features are
not enough for Understanding the task model,
therefore we will implement new features related
to this aspect.

Organization of information resulted in the sec-
ond highest kappa value between aspects. As sim-
ilarity to prompt was the most relevant features,
we believe that similarity between semantic vec-
tors, as proposed by Zesh et. al (Zesch et al.,
2015), also can improve Organization of Informa-
tion prediction. Another observation is the influ-
ence of style errors upon Organization of Infor-
mation aspect. Perhaps this influence is because
the definition of style we used is related to how
the writer “present” the information, which can be
redundancies or nonexistent language expressions.

With respect to the Knowing argumentation as-

98



Figure 1: Distribution of grades in UOL dataset for each aspect and final grade

99



pect, we believe that style errors affected results
for a similar reason that we mentioned in the anal-
ysis of Organization of information aspect. How-
ever, in regard this aspect we think that perhaps
some argument features ( (Stab and Gurevych,
2014), (Song et al., 2015)) will improve Know-
ing argumentation scoring prediction.

4 Conclusion

We proposed a multi-aspect automatic essay cor-
rection system for Brazilian Portuguese. Our pri-
mary goal is to evaluate if classical features for
AES system for the English language performs
well in a multi-aspect scenario, and assess which
features are important for which aspect. In fact, af-
ter experiments, some features performed well for
some aspects. Nonetheless, each aspect performed
in a different way, which suggests that each aspect
needs an own suitable model. Also, more specific
features for some aspects probably will enhance
subjective aspects.

Academic level, represented by Flesh score, is
extremely relevant in most aspects. A possible
reason for these results is because a high school
student should present advanced skills, like gram-
mar, spelling, argumentation, among others. De-
spite this feature in common, each aspect exhibits
their singularity. Like enclise affecting Under-
standing the task, similarity with prompt influ-
encing Organization of information, and discourse
markers changing Solution proposal. Therefore,
while some of the features enhance results for
some aspects, these same features harm prediction
for other aspects.

5 Future Directions

The following issues are directions we aim to pur-
sue in our further research.

Analysis of evaluators comments. Our dataset
comprises human evaluators comments. We in-
tend to analyze these comments, which is of par-
ticular importance for argumentative essays since
the opinion of human evaluators about a topic can
affect grades. In a sample of 48 essays taken from
our dataset, two linguists detected that 11 essays
presented biased evaluation. Biased evaluation is
a more serious issue if we will think about ENEM
and other tests that are a relevant factor to many
students. Some works were performed in bias lan-
guage, but none of them analyzed bias on evalua-
tions. Also, we can apply the same reasoning for

other types of evaluations, like peer review of pa-
pers. Besides that, we would like to research how
we can minimize bias on automatic scoring pre-
diction.

Composite Classifier. A classifier to predict fi-
nal grades employing predictions of the five as-
pects is a natural step in our research.

Adding new features to Brazilian Portuguese
AES. There are more features to add to Brazilian
Portuguese AES. Some of these features are: POS-
tagging ratio; word length in characters; the num-
ber of commas, quotations or exclamation marks;
average sentence length; average depth of syntac-
tic trees; and topical overlap between adjacent sen-
tences. Also, cohesion features like proposed by
Song et al. (Song et al., 2015) can improve as-
pects like Solution Proposal, which probably de-
mands sophisticated features.

6 Acknowledgements

We would like to thank Marcia and Luana, the
two linguists that have been assisting us on bias
research.

We thank the partial support given by the Brazil-
ian National Institute of Science and Technology
for the Web (grant MCT-CNPq 573871/2008-6),
Project Models, Algorithms and Systems for the
Web (grant FAPEMIG/PRONEX/MASWeb APQ-
01400-14), and authors individual grants and
scholarships from CNPq and CAPES.

References
Yigal Attali and Jill Burstein. 2006. Automated essay

scoring with e-rater R© v. 2. The Journal of Technol-
ogy, Learning and Assessment, 4(3).

Hongbo Chen and Ben He. 2013. Automated essay
scoring by maximizing human-machine agreement.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1741–1752, Seattle, WA, USA.

Yllias Chali Sadid A Hasan. 2012. Automatically as-
sessing free texts. In 24th International Conference
on Computational Linguistics, page 9, Bombay, In-
dia.

Clélia Jubran and Ingedore Koch. 2006. Gramática
do português culto falado no Brasil: construção do
texto falado, volume 1. UNICAMP.

Tuomo Kakkonen and Erkki Sutinen. 2004. Automatic
assessment of the content of essays based on course
materials. In 2nd International Conference on Infor-
mation Technology: Research and Education, pages
126–130, Semarang, Indonesia. IEEE.

100



Jorge Kinoshita, Lais N. Salvador, and Carlos E. D.
Menezes. 2006. Cogroo: a brazilian-portuguese
grammar checker based on cetenfolha. In The fifth
international conference on Language Resources
and Evaluation (LREC), pages 2190–2193, Genova,
Italy.

Leah S Larkey. 1998. Automatic Essay Grading Us-
ing Text Categorization Techniques. In Proceedings
of the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 90–95, Melbourne, Australia.

Lung-Hao Lee, Liang-Chih Yu, Kuei-Ching Lee, Yuen-
Hsien Tseng, Li-Ping Chang, and Hsin-Hsi Chen.
2014. A sentence judgment system for grammatical
error detection. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 67–70,
Dublin, Ireland, August. Dublin City University and
Association for Computational Linguistics.

Teresa B. F. Martins, Claudete M. Ghiraldelo, Maria
G. V. Nunes, and Osvaldo N. Oliveira Junior. 1996.
Readability formulas applied to textbooks in brazil-
ian portuguese. Instituto de Ciłncias Matemticas de
So Carlos-USP, São Carlos, Brazil.

E. Martins. 2000. Manual de redação e estilo. O Es-
tado de São Paulo.

Courtney Napoles and Chris Callison-Burch. 2015.
Automatically scoring freshman writing: A prelim-
inary investigation. In Proceedings of the Tenth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 254–263, Denver,
CO, USA.

Isaac Persing and Vincent Ng. 2015. Modeling ar-
gument strength in student essays. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 543–552, Beijing,
China.

Alla Rozovskaya and Dan Roth. 2014. Building a
state-of-the-art grammatical error correction system.
Transactions of the Association for Computational
Linguistics, 2:419–434.

Wei Song, Ruiji Fu, Lizhen Liu, and Ting Liu. 2015.
Discourse Element Identification in Student Essays
based on Global and Local Cohesion. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 2255–2261,
Lisbon, Portugal.

Christian Stab and Iryna Gurevych. 2014. Identify-
ing argumentative discourse structures in persuasive
essays. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, pages 46–56, Doha, Qatar.

Torsten Zesch, Michael Wojatzki, and Dirk Scholten-
Akoun. 2015. Task-Independent Features for Auto-
mated Essay Grading. pages 224–232, Denver, CO,
USA.

101



Table 6: Kappa results for Feature Analysis
Aspect Feature Category Feature Removed Kappa

Final Grade

Average word Length 0.3890
Most Relevant Features Flesh Score 0.4010

Vocabulary Level 0.4059
Discourse markers per #Sentence 0.4259

Least Relevant Features Count of first Person 0.4262
Count first Person per #Sentence 0.4320
Full feature set 0.4245

Understanding the Task

Flesh Score 0.1452
Most Relevant Features #enclise / #sentences 0.1655

#spelling errors 0.1655
#grammar errors 0.1868

Least Relevant Features #style errors / #sentences 0.1878
#first person use / # sentences 0.1885
Full feature set 0.1817

Organization of Information

Similarity with prompt 0.2496
Most Relevant Features Average word length 0.2581

#style errors / #sentences 0.2605
#long sentences 0.2788

Least Relevant Features #demonstrative pronoun / # sentence 0.2799
#first person use / #sentence 0.2817
Full feature set 0.2728

Knowing Argumentation

#spelling errors / #tokens 0.2438
Most Relevant Features #style errors / #sentences 0.2441

Flesh Score 0.2456
#enclise / #sentences 0.2773

Least Relevant Features Average Word Length 0.2784
#grammar errors 0.2849
Full feature set 0.2668

Solution Proposal

Average word length 0.1048
Most Relevant Features Flesh score 0.1192

#discourse markers 0.1240
#grammar errors / #Tokens 0.1586

Least Relevant Features #tokens 0.1593
#first person use 0.1655
Full feature set 0.1655

Formal Language

Flesh Score 0.3060
Most Relevant Features #grammar errors / #tokens 0.3138

#spelling mistakes 0.3248
#long sentences 0.3396

Least Relevant Features #discourse markers 0.3396
#demonstrative pronouns 0.3429
Full feature set 0.3351

102


