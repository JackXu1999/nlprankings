



















































Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 684–695,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

684

Help, Anna! Visual Navigation with Natural Multimodal Assistance via
Retrospective Curiosity-Encouraging Imitation Learning

Khanh Nguyen� and Hal Daumé III�♥
University of Maryland, College Park�, Microsoft Research, New York♥

{kxnguyen,hal}@umiacs.umd.edu

Abstract
Mobile agents that can leverage help from hu-
mans can potentially accomplish more com-
plex tasks than they could entirely on their
own. We develop “Help, Anna!” (HANNA), an
interactive photo-realistic simulator in which
an agent fulfills object-finding tasks by re-
questing and interpreting natural language-
and-vision assistance. An agent solving tasks
in a HANNA environment can leverage simu-
lated human assistants, called ANNA (Auto-
matic Natural Navigation Assistants), which,
upon request, provide natural language and vi-
sual instructions to direct the agent towards
the goals. To address the HANNA problem,
we develop a memory-augmented neural agent
that hierarchically models multiple levels of
decision-making, and an imitation learning al-
gorithm that teaches the agent to avoid re-
peating past mistakes while simultaneously
predicting its own chances of making future
progress. Empirically, our approach is able to
ask for help more effectively than competitive
baselines and, thus, attains higher task suc-
cess rate on both previously seen and previ-
ously unseen environments. We publicly re-
lease code and data at https://github.
com/khanhptnk/hanna .

1 Introduction

The richness and generalizability of natural lan-
guage makes it an effective medium for directing
mobile agents in navigation tasks, even in environ-
ments they have never encountered before (Ander-
son et al., 2018b; Chen et al., 2019; Misra et al.,
2018; de Vries et al., 2018; Qi et al., 2019). Nev-
ertheless, even with language-based instructions,
such tasks can be overly difficult for agents on
their own, especially in unknown environments.
To accomplish tasks that surpass their knowl-
edge and skill levels, agents must be able to ac-
tively seek for and leverage assistance in the en-
vironment. Humans are rich external knowledge

sources but, unfortunately, they may not be avail-
able all the time to provide guidance, or may be
unwilling to help too frequently. To reduce the
needed effort from human assistants, it is essential
to design research platforms for teaching agents to
request help mindfully.

In natural settings, human assistance is often:
� derived from interpersonal interaction (a lost

tourist asks a local for directions);
� reactive to the situation of the receiver, based

on the assistant’s knowledge (the local may
guide the tourist to the goal, or may redirect
them to a different source of assistance);
� delivered via a multimodal communication

channel (the local uses a combination of lan-
guage, images, maps, gestures, etc.).

We introduce the “Help, Anna!” (HANNA) prob-
lem (§ 3), in which a mobile agent has to navi-
gate (without a map) to an object by interpreting
its first-person visual perception and requesting
help from Automatic Natural Navigation Assis-
tants (ANNA). HANNA models a setting in which
a human is not always available to help, but rather
that human assistants are scattered throughout the
environment and provide help upon request (mod-
eling the interpersonal aspect). The assistants are
not omniscient: they are only familiar with cer-
tain regions of the environment and, upon request,
provide subtasks, expressed in language and im-
ages (modeling the multimodal aspect), for getting
closer to the goal, not necessarily for fully com-
pleting the task (modeling the reactive aspect).

In HANNA, when the agent gets lost and be-
comes unable to make progress, it has the option
of requesting assistance from ANNA. At test time,
the agent must decide where to go and whether
to request help from ANNA without additional su-
pervision. At training time, we leverage imitation
learning to learn an effective agent, both in terms
of navigation, and in terms of being able to decide

https://github.com/khanhptnk/hanna
https://github.com/khanhptnk/hanna


685

A

Find a mug

Help, Anna!

B

Enter the bedroom 
and turn left 

immediately. Walk 
straight to the carpet 
in the living room...

Success!

B

A C

Route 1

Route 2

Agent path

Start location

Enter route

Depart route

Goal location

C
Enter the bedroom and 
turn left immediately. 

Walk straight to the 
carpet in the living room.
Turn right, come to the coffee table.

Figure 1: An example HANNA task. Initially, the agent stands in the bedroom at A and is requested by a human
requester to “find a mug.” The agent begins, but gets lost somewhere in the bathroom. It gets to the start location of
route ( B ) to request help from ANNA. Upon request, ANNA assigns the agent a navigation subtask described by
a natural language instruction that guides the agent to a target location, and an image of the view at that location.
The agents follows the language instruction and arrives at C , where it observes a match between the target image
and the current view, thus decides to depart route . After that, it resumes the main task of finding a mug. From
this point, the agent gets lost one more time and has to query ANNA for another subtask that helps it follow route

and enter the kitchen. The agent successfully fulfills the task it finally stops within � meters of an instance of
the requested object ( ). Here, the ANNA feedback is simulated using two pre-collected language-assisted routes
( and ).

when it is most worthwhile to request assistance.

This paper has two primary contributions:

1. Constructing the HANNA simulator by aug-
menting an indoor photo-realistic simulator
with simulated human assistance, mimicking
a scenario where a mobile agent finds objects
by asking for directions along the way (§3).

2. An effective model and training algorithm for
the HANNA problem, which includes a hi-
erarchical memory-augmented recurrent ar-
chitecture that models human assistance as
sub-goals (§ 5), and introduces an imitation
learning objective that enhances exploration
of the environment and interpretability of the
agent’s help-request decisions. (§4).

We embed the HANNA problem in the photo-
realistic Matterport3D environments (Chang et al.,
2017) with no extra annotation cost by reusing
the pre-existing Room-to-Room dataset (Ander-
son et al., 2018b). Empirical results (§ 7) show
that our agent can effectively learn to request and
interpret language and vision instructions, given a
training set of 51 environments and less than 9,000
language instructions. Even in new environments,
where the scenes and the language instructions are
previously unseen, the agent successfully accom-
plishes 47% of its tasks. Our methods for train-
ing the navigation and help-request policies out-
perform competitive baselines by large margins.

2 Related work

Simulated environments provide an inexpensive
platform for fast prototyping and evaluating new
ideas before deploying them into the real world.
Video-game and physics simulators are standard
benchmarks in reinforcement learning (Todorov
et al., 2012; Mnih et al., 2013; Kempka et al.,
2016; Brockman et al., 2016; Vinyals et al., 2017).
Nevertheless, these environments under-represent
the complexity of the world. Realistic simulators
play an important role in sim-to-real approaches,
in which an agent is trained with arbitrarily many
samples provided by the simulators, then trans-
ferred to real settings using sample-efficient trans-
fer learning techniques (Kalashnikov et al., 2018;
Andrychowicz et al., 2018; Karttunen et al., 2019).
While modern techniques are capable of simulat-
ing images that can convince human perception
(Karras et al., 2017, 2018), simulating language
interaction remains challenging. There are efforts
in building complex interactive text-based worlds
(Côté et al., 2018; Urbanek et al., 2019) but the
lack of a graphical component makes them not
suitable for visually grounded learning. On the
other hand, experimentation on real humans and
robots, despite expensive and time-consuming, are
important for understanding the true complexity of
real-world scenarios (Chai et al., 2018, 2016; Ryb-
ski et al., 2007; Mohan and Laird, 2014; Liu et al.,
2016; She et al., 2014).



686

Recent navigation tasks in photo-realistic simu-
lators have accelerated research on teaching agents
to execute human instructions. Nevertheless, mod-
eling human assistance in these problems remains
simplistic (Table 1): they either do not incorporate
the ability to request additional help while execut-
ing tasks (Misra et al., 2014, 2017; Anderson et al.,
2018b; Chen et al., 2019; Das et al., 2018; Misra
et al., 2018; Wijmans et al., 2019; Qi et al., 2019),
or mimic human verbal assistance with primitive,
highly scripted language (Nguyen et al., 2019;
Chevalier-Boisvert et al., 2019). HANNA im-
proves the realisticity of the VNLA setup (Nguyen
et al., 2019) by using fully natural language in-
structions.

Imitation learning algorithms are a great fit for
training agents in simulated environments: ac-
cess to ground-truth information about the envi-
ronments allows optimal actions to be computed
in many situations. The “teacher” in standard
imitation learning algorithms (Daumé III et al.,
2009; Ross et al., 2011; Ross and Bagnell, 2014;
Chang et al., 2015; Sun et al., 2017; Sharaf and
Daumé III, 2017) does not take into consider-
ation the agent’s capability and behavior. He
et al. (2012) present a coaching method where the
teacher gradually increases the complexity of its
demonstrations over time. Welleck et al. (2019)
propose an ”unlikelihood” objective, which, sim-
ilar to our curiosity-encouraging objective, pe-
nalizes likelihoods of candidate negative actions
to avoid mistake repetition. Our approach takes
into account the agent’s past and future behav-
ior to determine actions that are most and least
beneficial to them, combining the advantages of
both model-based and progress-estimating meth-
ods (Wang et al., 2018; Ma et al., 2019a,b).

3 The HANNA Simulator

Problem. HANNA simulates a scenario where a
human requester asks a mobile agent via language
to find an object in an indoor environment. The
task request is only a high-level command (“find
[object(s)]”), modeling the general case when the
requester does not need know how to accomplish
a task when requesting it. We assume the task is
always feasible: there is at least an instance of the
requested object in the environment.

Figure 1, to which references in this section will
be made, illustrates an example where the agent is
asked to “find a mug.” The agent starts at a ran-

Request Multimodal Simulated
Problem assistance instructions humans

VLN 7 7 7
VNLA 3 7 3
CVDN 3 7 7
HANNA (this work) 3 3 3

Table 1: Comparing HANNA with other photo-realistic
navigation problems. VLN (Anderson et al., 2018b)
does not allow agent to request help. VNLA (Nguyen
et al., 2019) models an advisor who is always present
to help but speaks simple, templated language. CVDN
(Thomason et al., 2019b) contains natural conversa-
tions in which a human assistant aids another human
in navigation tasks but offers limited language interac-
tion simulation, as language assistance is not available
when the agent deviates from the collected trajectories
and tasks. HANNA simulates human assistants that pro-
vide language-and-vision instructions that adapt to the
agent’s current position and goal.

dom location ( A ), is given a task request, and is
allotted a budget of T time steps to complete the
task. The agent succeeds the if its final location
is within �success meters of the location of any in-
stance of the requested object ( ). The agent is
not given any sensors that help determine its lo-
cation or the object’s location and must navigate
only with a monocular camera that captures its
first-person view as an RGB image (e.g., image
in the upper right of Figure 1).

The only source of help the agent can leverage
in the environment is assistants, who are present
at both training and evaluation time. The assis-
tants are not aware of the agent unless it enters
their zones of attention, which include all loca-
tions within �attn meters of their locations. When
the agent is in one of these zones, it has an op-
tion to request help from the corresponding assis-
tant. The assistant helps the agent by giving a sub-
task, described by a natural language instruction
that guides the agent to a specific location, and an
image of the view at that location.

In our example, at B , the assistant says “Enter
the bedroom and turn left immediately. Walk straight to the

carpet in the living room. Turn right, come to the coffee ta-

ble.” and provides an image of the destination in
the living room. Executing the subtask may not
fulfill the main task, but is guaranteed to get the
agent to a location closer to a goal than where it
was before (e.g., C ).

Photo-realistic Navigation Simulator. HANNA
uses the Matterport3D simulator (Chang



687

et al., 2017; Anderson et al., 2018b) to photo-
realistically emulate a first-person view while
navigating in indoor environments. HANNA
features 68 Matterport3D environments, each
of which is a residential building consisting of
multiple rooms and floors. Navigation is modeled
as traversing an undirected graph G = (V,E),
where each location corresponds to a node v ∈ V
with 3D-coordinates xv, and edges are weighted
by their lengths (in meters). The state of the agent
is fully determined by its pose τ = (v, ψ, ω),
where v is its location, ψ ∈ (0, 2π] is its heading
(horizontal camera angle), and ω ∈

[
−π6 ,

π
6

]
is

its elevation (vertical camera angle). The agent
does not know v, and the angles are constrained to
multiples of π6 . In each step, the agent can either
stay at its current location, or it can rotate toward
and go to a location adjacent to it in the graph1.
Every time the agent moves (and thus changes
pose), the simulator recalculates the image to
reflect the new view.

Automatic Natural Navigation Assistants
(ANNA). ANNA is a simulation of human as-
sistants who do not necessarily know themselves
how to optimally accomplish the agent’s goal:
they are only familiar with scenes along certain
paths in the environment, and thus give advice to
help the agent make partial progress. Specifically,
the assistance from ANNA is modeled by a set of
language-assisted routes R = {r1, r2, . . . , r|R|}.
Each route r = (ψr, ωr, pr, lr) is defined by
initial camera angles (ψr, ωr), a path pr in
the environment graph, and a natural language
instruction lr. A route becomes enterable when
its start location is adjacent to and within �attn
meters of the agent’s location. When the agent
enters a route, it first adjusts its camera angles to
(ψr, ωr), then attempts to interpret the language
instructions lr to traverse along pr. At any
time, the agent can depart the route by stopping
following lr. An example of a route in Figure 1 is
the combination of the initial camera angles at B ,
the path , and the language instruction “Enter the
bedroom and turn left immediately. . . ”

The set of all routes starting from a location
simulates a human assistant who can recall scenes
along these routes’ paths. The zone of attention of
the simulated human is the set of all locations from
which the agent can enter one of the routes; when
the agent is in this zone, it may ask the human for

1We use the “panoramic action space” (Fried et al., 2018).

help. Upon receiving a help request, the human
selects a route r? for the agent to enter (e.g., ),
and a location vd on the route where it wants the
agent to depart (e.g., C ). It then replies the agent
with a multimedia message (lr

?
, Ivd), where lr? is

the selected route’s language instruction, and Ivd
is an image of the panoramic view at the departure
location. The message describes a subtask which
requires the agent to follow the direction described
by lr

?
and to stop if it reaches the location refer-

enced by Ivd . The route r? and the departure node
vd are selected to get the agent as close to a goal
location as possible. Concretely, let Rcurr be the
set of all routes associated with the requested hu-
man. The selected route minimizes the distance to
the goal locations among all routes in Rcurr:

r? = argmin
r∈Rcurr

d̄
(
r, Vgoal

)
(1)

where d̄
(
r, Vgoal

) def
= min

g∈Vgoal,v∈pr
d (g, v) (2)

d(., .) returns the (shortest-path) distance between
two locations, and Vgoal is the set of all goal loca-
tions. The departure location minimizes the dis-
tance to the goal locations among all locations on
the selected route:

vd = argmin
g∈Vgoal,v∈pr?

d (g, v)
def
= d̄

(
r?, Vgoal

)
(3)

When the agent chooses to depart the route (not
necessarily at the departure node), the human fur-
ther assists it by providing Ig? , an image of the
panoramic view at the goal location closest to the
departure node:

g? = argmin
g∈Vgoal

d (g, vd) (4)

The way the agent leverages ANNA to accom-
plish tasks is analogous to how humans travel us-
ing public transportation systems (e.g., bus, sub-
way). For example, passengers of a subway sys-
tem utilize fractions of pre-constructed routes to
make progress toward a destination. They exe-
cute travel plans consisting of multiple subtasks,
each of which requires entering a start stop, fol-
lowing a route (typically described by its name
and last stop), and exiting at a departure stop (e.g.,
“Enter the Penn Station, hop on the Red line in the direc-

tion toward the South Ferry, get off at the World Trade Cen-

ter”). Occasionally, users walk short distances
(at a lower speed) to switch routes. Our setup
follows the same principle, but instead of hav-
ing physical vehicles and railways, we employ



688

low-level language-and-vision instructions as the
“high-speed means” to accelerate travel.

Constructing ANNA route system. Given a
photo-realistic simulator, the primary cost for con-
structing the HANNA problem comes from crowd-
sourcing the natural language instructions. Ideally,
we want to collect sufficient instructions to simu-
late humans in any location in the environment.
Let N = |V | be the number of locations in the
environment. Since each simulated human is fa-
miliar with at most N locations, in the worst case,
we need to collect O(N2) instructions to connect
all location pairs. However, we theoretically prove
that, assuming the agent executes instructions per-
fectly, it is possible to guide the agent between
any location pair by collecting only Θ(N logN)
instructions. The key idea is using O(logN) in-
stead of a single instruction to connect each pair,
and reusing an instruction for multiple routes.

Lemma 1. (proof in Appendix A) To guide the
agent between any two locations using O(logN)
instructions, we need to collect instructions for
Θ(N logN) location pairs.

In our experiments, we leverage the pre-existing
Room-to-room dataset (Anderson et al., 2018b)
to construct the route system. This dataset con-
tains 21,567 natural language instructions crowd-
sourced from humans and is originally intended to
be used for the Vision-Language Navigation task
(such as those in Figure 1), where an agent exe-
cutes a language instruction to go to a location.
We exclude instructions of the test split and their
corresponding environments because ground-truth
paths are not given. We use (on average) 211
routes to connect (on overage) 125 locations per
environment. Even though the routes are selected
randomly in the original dataset, our experiments
show that they are sufficient for completing the
tasks (assuming perfect assistance interpretation).

4 Retrospective Curiosity-Encouraging
Imitation Learning

Agent Policies. Let s be a fully-observed state
that contains ground-truth information about the
environment and the agent (e.g., object locations,
environment graph, agent parameters, etc.). Let
os be the corresponding observation given to the
agent, which only encodes the current view, the
current task, and extra information that the agent
keeps track of (e.g., time, action history, etc.). The

Algorithm 1 Task episode, given agent help-
request policy π̂ask and navigation policy π̂nav

1: agent receives task request e
2: initialize the agent mode: m← main task
3: initialize the language instruction: l0 ← e
4: initialize the target image: I tgt0 ← None
5: for t = 1 . . . T do
6: let st be the current state, ot the current observation,

and τt = (vt, ψt, ωt) the current pose
7: agent makes a help-request decision âaskt ∼ π̂ask(ot)
8: carry on task from the previous step:

lt ← lt−1, I tgtt = I
tgt
t−1

9: if âaskt = request help then
10: set mode: m← sub task
11: request help: (r, Idepart, Igoal)← ANNA(st)
12: set the language instruction: lt ← lr
13: set the target image: I tgtt ← Idepart
14: set the navigation action:

ânavt ← (pr0, ψr − ψt, ωr − ωt),
where pr0 is the start location of route r

15: else
16: agent chooses navigation: ânavt ∼ π̂nav(ot)
17: if ânavt = stop then
18: if m = main task then
19: break
20: else
21: set mode: m← main task
22: set the language instruction: lt ← e
23: set the target image: I tgtt ← Igoal
24: set navigation action: ânavt ← (vt, 0, 0)
25: end if
26: end if
27: end if
28: agent executes ânavt to go to the next location
29: end for

agent maintains two stochastic policies: a navi-
gation policy π̂nav and a help-request policy π̂ask.
Each policy maps an observation to a probability
distribution over its action space. Navigation ac-
tions are tuples (v,∆ψ,∆ω), where v is a next
location that is adjacent to the current location and
(∆ψ,∆ω) is the camera angle change. A spe-
cial stop action is added to the set of naviga-
tion actions to signal that the agent wants to ter-
minate the main task or a subtask (by departing a
route). The action space of the help-request pol-
icy contains two actions: request help and
do nothing. The request help action is
only available when the agent is in a zone of atten-
tion. Alg 1 describes the effects of these actions
during a task episode.

Imitation Learning Objective. The agent is
trained with imitation learning to mimic behav-
iors suggested by a navigation teacher π?nav and a
help-request teacher π?ask, who have access to the
fully-observed states. In general, imitation learn-
ing (Daumé III et al., 2009; Ross et al., 2011; Ross



689

and Bagnell, 2014; Chang et al., 2015; Sun et al.,
2017) finds a policy π̂ that minimizes the expected
imitation loss Lwith respect to a teacher policy π?
under the agent-induced state distribution Dπ̂:

min
π̂

Es∼Dπ̂ [L(s, π̂, π
?)] (5)

We frame the HANNA problem as an instance
of Imitation Learning with Indirect Intervention
(I3L) (Nguyen et al., 2019). Under this frame-
work, assistance is viewed as augmenting the cur-
rent environment with new information. Interpret-
ing the assistance is cast as finding the optimal
acting policy in the augmented environment. For-
mally, I3L searches for policies that optimize:

min
π̂ask,π̂nav

Es∼Dstateπ̂nav,E ,E∼Denvπ̂ask
[L(s)] (6)

L(s) = Lnav(s, π̂nav, π?nav) + Lask(s, π̂ask, π?ask)

where Lnav and Lask are the navigation and help-
request loss functions, respectively, Denvπ̂ask is the
environment distribution induced by π̂ask, and
Dstateπ̂nav,E is the state distribution induced by π̂nav in
environment E . A common choice for the loss
functions is the agent-estimated negative log like-
lihood of the reference action:

LNL(s, π̂, π?) = − log π̂(a? | os) (7)

where a? is the reference action suggested by π?.
We introduce novel loss functions that enforce
more complex behaviors than simply mimicking
reference actions.

Reference Actions. The navigation teacher sug-
gests a reference action anav? that takes the agent
to the next location on the shortest path from its lo-
cation to the target location. Here, the target loca-
tion refers to the nearest goal location (if no target
image is available), or the location referenced by
the target image (provided by ANNA). If the agent
is already at the target location, anav? = stop. To
decide whether the agent should request help, the
help-request teacher verifies the following condi-
tions:

1. lost: the agent will not get (strictly) closer
to the target location in the future;

2. uncertain wong: the entropy2 of the nav-
igation action distribution is greater than or
equal to a threshold γ, and the highest-
probability predicted navigation action is not
suggested by the navigation teacher;

2Precisely, we use efficiency, or entropy of base |Anav| =
37, where Anav is the navigation action space.

3. never asked: the agent previously never
requested help at the current location;

If condition (1) or (2), and condition (3) are satis-
fied, we set aask? = request help; otherwise,
aask? = do nothing.

Curiosity-Encouraging Navigation Teacher.
In addition to a reference action, the navigation
teacher returns Anav⊗, the set of all non-reference
actions that the agent took at the current location
while executing the same language instruction:

Anav⊗t =
{
a ∈ Anav : ∃t′ < t, vt = vt′ , (8)

lt = lt′ , a = a
nav
t′ 6= anav?t′

}
where Anav is the navigation action space.

We devise a curiosity-encouraging loss Lcurious,
which minimizes the log likelihoods of actions in
Anav⊗. This loss prevents the agent from repeating
past mistakes and motivates it to explore untried
actions. The navigation loss is:

Lnav(s, π̂nav, π?nav) =
LNL(s,π̂nav,π?nav)︷ ︸︸ ︷

− log π̂nav(anav? | os) (9)

+ α
1

|Anav⊗|
∑

a∈Anav⊗
log π̂nav(a | os)︸ ︷︷ ︸

Lcurious(s,π̂nav,π?nav)

where α ∈ [0,∞) is a weight hyperparameter.

Retrospective Interpretable Help-Request
Teacher. In deciding whether the agent should
ask for help, the help-request teacher must
consider the agent’s future situations. Standard
imitation learning algorithms (e.g., DAgger)
employ an online mode of interaction which
queries the teacher at every time step. This mode
of interaction is not suitable for our problem:
the teacher must be able to predict the agent’s
future actions if it is queried when the episode
is not finished. To overcome this challenge, we
introduce a more efficient retrospective mode of
interaction, which waits until the agent completes
an episode and queries the teacher for reference
actions for all time steps at once. With this
approach, because the future actions at each time
step are now fully observed, they can be taken
into consideration when computing the reference
action. In fact, we prove that the retrospective
teacher is optimal for teaching the agent to
determine the lost condition, which is the only
condition that requires knowing the agent’s future.
Lemma 2. (proof in Appendix B) At any time step,
the retrospective help-request teacher suggests the



690

Text 
attention 

query

Text 
attention

hidden 

Self
Attention

Current
view

Visual
Attention

Local 
time 

Target 
similarity

State
features

Text 
Attention

Current
view

Global 
time

Target
similarity

Visual
attention

Visual
attention

Target
view

Self
Attention State

hidden Softmax

Text
memoryEncoder

Language
instruction

Input 
features

Intermediate 
features

Neural 
module

Previous 
action

Inter-task

Intra-taskEncoder

CosineSim
Attention

Figure 2: Our hierarchical recurrent model architecture (the navigation network). The help-request network is
mostly similar except that the navigation action distribution is fed as an input to compute the “state features”.

action that results in the agent getting closer to
the target location in the future under its current
navigation policy (if such an action exists).

To help the agent better justify its help-request
decisions, we train a reason classifier Φ to predict
which conditions are satisfied. To train this clas-
sifier, the teacher provides a reason vector ρ? ∈
{0, 1}3, where ρ?i = 1 indicates that the i-th condi-
tion is met. We formulate this prediction problem
as multi-label binary classification and employ a
binary logistic loss for each condition. Learning to
predict the conditions helps the agent make more
accurate and interpretable decisions. The help-
request loss is:

Lask(s, π̂ask, π?ask) =

LNL(s,π̂ask,π?ask)︷ ︸︸ ︷
− log π̂ask(aask? | os) (10)

−1
3

3∑
i=1

[ρ?i log ρ̂i + (1− ρ?i ) log(1− ρ̂i)]︸ ︷︷ ︸
Lreason(s,π̂ask,π?ask)

where
(
aask?, ρ?

)
= π?ask(s), and ρ̂ = Φ(os) is the

agent-estimated likelihoods of the conditions.

5 Hierarchical Recurrent Architecture

We model the navigation policy and the help-
request policy as two separate neural networks.
The two networks have similar architectures,
which consists of three main components: the
text-encoding component, the inter-task compo-
nent, and the intra-task component (Figure 2).
We use self-attention instead of recurrent neural
networks to better capture long-term dependency,
and develop novel cosine-similarity attention and
ResNet-based time-encoding. Detail on the com-
putations in each module is in the Appendix.

The text-encoding component computes a text
memory M text, which stores the hidden represen-
tation of the current language instruction. The

Split Environments Tasks ANNA Instructions

Train 51 82,484 8,586
Val SeenEnv 51 5,001 4,287
Val UnseenAll 7 5,017 2,103
Test SeenEnv 51 5,004 4,287
Test Unseen 10 5,012 2,331

Table 2: Data split.

inter-task module computes a vector hintert repre-
senting the state of the current task’s execution.
During the episode, every time the current task
is altered (due to the agent requesting help or de-
parting a route), the agent re-encodes the new lan-
guage instruction to generate a new text memory
and resets the inter-task state to a zero vector. The
intra-task module computes a vector hintrat repre-
senting the state of the entire episode. To compute
this state, we first calculate h̄intrat , a tentative cur-
rent state, and h̃intrat , a weighted combination of
the past states at nearly identical situations. hintrat
is computed as:

hintrat = h̄
intra
t − β · h̃intrat (11)

β = σ(Wgate · [h̄intrat ; h̃intrat ]) (12)

Eq 11 creates an context-sensitive dissimilarity be-
tween the current state and the past states at nearly
identical situations. The scale vector β determines
how large the dissimilarity is based on the in-
puts. This formulation incorporates past related
information into the current state, thus enables the
agent to optimize the curiosity-encouraging loss
effectively. Finally, hintrat is passed through a soft-
max layer to produce an action distribution.

6 Experimental Setup

Dataset. We generate a dataset of object-finding
tasks in the HANNA environments to train and
evaluate our agent. Table 2 summarizes the dataset
split. Our dataset features 289 object types; the



691

SEENENV UNSEENALL

Agent SR ↑ SPL ↑ Nav. ↓ Requests/ SR ↑ SPL ↑ Nav. ↓ Requests/
(%) (%) Err. (m) task ↓ (%) (%) Err. (m) task ↓

Non-learning agents
RANDOMWALK 0.54 0.33 15.38 0.0 0.46 0.23 15.34 0.0
FORWARD10 5.98 4.19 14.61 0.0 6.36 4.78 13.81 0.0

Learning agents
No assistance 17.21 13.76 11.48 0.0 8.10 4.23 13.22 0.0
Learn to interpret assistance (ours) 88.37 63.92 1.33 2.9 47.45 25.50 7.67 5.8

Skylines
SHORTEST 100.00 100.00 0.00 0.0 100.00 100.00 0.00 0.0
Perfect assistance interpretation 90.99 68.87 0.91 2.5 83.56 56.88 1.83 3.2

Table 3: Results on test splits. The agent with “perfect assistance interpretation” uses the teacher navigation policy
(π?nav) to make decisions when executing a subtask from ANNA. Results of our final system are in bold.

language instruction vocabulary contains 2,332
words. The numbers of locations on the shortest
paths to the requested objects are restricted to be
between 5 and 15. With an average edge length
of 2.25 meters, the agent has to travel about 9
to 32 meters to reach its goals. We evaluate the
agent in environments that are seen during train-
ing (SEENENV), and in environments that are not
seen (UNSEENALL). Even in the case of SEE-
NENV, the tasks and the ANNA language instruc-
tions given during evaluation were never given in
the same environments during training.

Hyperparameters. See Appendix.

Baselines and Skylines. We compare our
agent against the following non-learning agents:
1. SHORTEST: uses the navigation teacher pol-
icy to make decisions (this is a skyline); 2. RAN-
DOMWALK: randomly chooses a navigation ac-
tion at every time step; 3. FORWARD10: navigates
to the next location closest to the center of the cur-
rent view to advance for 10 time steps. We com-
pare our learned help-request policy with the fol-
lowing heuristics: 1. NOASK: does not request
help; 2. RANDOMASK: randomly chooses to re-
quest help with a probabilty of 0.2, which is the
average help-request ratio of our learned agent;
3. ASKEVERY5: requests help as soon as walking
at least 5 time steps.

Evaluation metrics. Our main metrics are: suc-
cess rate (SR), the fraction of examples on which
the agent successfully solves the task; naviga-
tion error, the average (shortest-path) distance be-
tween the agent’s final location and the nearest
goal from that location; and SPL (Anderson et al.,
2018a), which weights task success rate by travel

distance as follows:

SPL =
1

N

N∑
i=1

Si
Li

max(Pi, Li)
(13)

where N is the number of tasks, Si indicates
whether task i is successful, Pi is the agent’s travel
distance, and Li is the shortest-path distance to the
goal nearest to the agent’s final location.

7 Results

Main results. From Table 3, we see that our
problem is challenging: simple heuristic-based
baselines such as RANDOMWALK and FOR-
WARD10 attain success rates less than 7%. An
agent that learns to accomplish tasks without
additional assistance from ANNA succeeds only
17.21% of the time on TEST SEENENV, and
8.10% on TEST UNSEENALL. Leveraging help
from ANNA dramatically boosts the success rate
by 71.16% on TEST SEENENV and by 39.35%
on TEST UNSEENALL over not requesting help.
Given the small size of our dataset (e.g., the agent
has fewer than 9,000 subtask instructions to learn
from), it is encouraging that our agent is success-
ful in nearly half of its tasks. On average, the agent
takes paths that are 1.38 and 1.86 times longer than
the optimal paths on TEST SEENENV and TEST
UNSEENALL, respectively. In unseen environ-
ments, it issues on average twice as many requests
to as it does in seen environments. To understand
how well the agent interprets the ANNA instruc-
tions, we also provide results where our agent uses
the optimal navigation policy to make decisions
while executing subtasks. The large gaps on TEST
SEENENV indicate there is still much room for im-
provement in the future, purely in learning to exe-



692

Assistance type SEENENV UNSEENALL

Target image only 84.95 31.88
+ Language instruction 88.37 47.45

Table 4: Success rates (%) of agents on test splits with
different types of assistance.

SEENENV UNSEENALL

π̂ask SR ↑ Requests/ SR ↑ Requests/
(%) task ↓ (%) task ↓

NOASK 17.21 0.0 8.10 0.0
RANDOMASK 82.71 4.3 37.05 6.8
ASKEVERY5 87.39 3.4 34.42 7.1
Learned (ours) 88.37 2.9 47.45 5.8

Table 5: Success rates (%) of different help-request
policies on test splits.

cute language instructions.

Does understanding language improve general-
izability? Our agent is assisted with both lan-
guage and visual instructions; similar to Thoma-
son et al. (2019a), we disentangle the useful-
ness two these two modes of assistance. As
seen in Table 4, the improvement from language
on TEST UNSEENALL (+15.17%) is substantially
more than that on TEST SEENENV (+3.42%),
largely the agent can simply memorize the seen
environments. This confirms that understanding
language-based assistance effectively enhances
the agent’s capability of accomplishing tasks in
novel environments.

Is learning to request help effective? Table 5
compares our learned help-request policies with
baselines. We find that ASKEVERY5 provides a
surprisingly strong baseline for this problem, lead-
ing to an improvement of +26.32% over not re-
questing help on TEST UNSEENALL. Neverthe-
less, our learned policy, with the ability to predict
the future and access to the agent’s uncertainty,
outperforms all baselines by at least 10.40% in
success rate on TEST UNSEENALL, while mak-
ing less help requests. The small gap between
the learned policy and ASKEVERY5 on TEST UN-
SEENALL is expected because, on this split, the
performance is mostly determined by the model’s
memorizing capability and is mostly insensitive to
the help-request strategy.

Is proposed model architecture effective?
We implement an LSTM-based encoder-decoder
model that is based on the architecture proposed

SR ↑ Nav. mistake ↓ Help-request ↓
Model (%) repeat (%) repeat (%)

LSTM-ENCDEC 19.25 31.09 49.37
Our model (α = 0) 43.12 25.00 40.17
Our model (α = 1) 47.45 17.85 21.10

Table 6: Results on TEST UNSEENALL of our model,
trained with and without curiosity-encouraging loss,
and an LSTM-based encoder-decoder model (both
models have about 15M parameters). “Navigation mis-
take repeat” is the fraction of time steps on which the
agent repeats a non-optimal navigation action at a pre-
viously visited location while executing the same task.
“Help-request repeat” is the fraction of help requests
made at a previously visited location while executing
the same task.

by (Wang et al., 2019). To incorporate the target
image, we add an attention layer that uses the im-
age’s vector set as the attention memory. We train
this model with imitation learning using the stan-
dard negative log likelihood loss (Eq 7), without
the curiosity-encouraging and reason-prediction
losses. As seen in Table 6, our hierarchical re-
current model outperforms this model by a large
margin on TEST UNSEENALL (+28.2%).

Does the proposed imitation learning algorithm
achieve its goals? The curiosity-encouraging
training objective is proposed to prevent the agent
from making the same mistakes at previously en-
countered situations. Table 6 shows that training
with the curiosity-encouraging objective reduces
the chance of the agent looping and making the
same decisions repeatedly. As a result, its suc-
cess rate is greatly boosted (+4.33% on TEST UN-
SEENALL) over no curiosity-encouraging.

8 Conclusion

In this work, we present a photo-realistic simula-
tor that mimics primary characteristics of real-life
human assistance. We develop effective imitation
learning techniques for learning to request and in-
terpret the simulated assistance, coupled with a hi-
erarchical neural network model for representing
subtasks. Future work aims to provide more natu-
ral, linguistically realistic interaction between the
agent and humans (e.g., providing the agent the
ability ask a natural question rather than just signal
for help), and to establish a theoretical framework
for modeling human assistance. We are also ex-
ploring ways to deploy and evaluate our methods
on real-world platforms.



693

References
Peter Anderson, Angel Chang, Devendra Singh Chap-

lot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen
Koltun, Jana Kosecka, Jitendra Malik, Roozbeh
Mottaghi, Manolis Savva, et al. 2018a. On evalu-
ation of embodied navigation agents. arXiv preprint
arXiv:1807.06757.

Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,
Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018b. Vision-
and-language navigation: Interpreting visually-
grounded navigation instructions in real environ-
ments. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
3674–3683.

Marcin Andrychowicz, Bowen Baker, Maciek Chociej,
Rafal Jozefowicz, Bob McGrew, Jakub Pachocki,
Arthur Petron, Matthias Plappert, Glenn Powell,
Alex Ray, et al. 2018. Learning dexterous in-hand
manipulation. arXiv preprint arXiv:1808.00177.

Greg Brockman, Vicki Cheung, Ludwig Pettersson,
Jonas Schneider, John Schulman, Jie Tang, and Wo-
jciech Zaremba. 2016. Openai gym. arXiv preprint
arXiv:1606.01540.

Joyce Y Chai, Rui Fang, Changsong Liu, and Lanbo
She. 2016. Collaborative language grounding to-
ward situated human-robot dialogue. AI Magazine,
37(4):32–45.

Joyce Y Chai, Qiaozi Gao, Lanbo She, Shaohua Yang,
Sari Saba-Sadiya, and Guangyue Xu. 2018. Lan-
guage to action: Towards interactive task learning
with physical agents. In International Joint Confer-
ence on Artificial Intelligence.

Angel Chang, Angela Dai, Thomas Funkhouser, Ma-
ciej Halber, Matthias Niessner, Manolis Savva, Shu-
ran Song, Andy Zeng, and Yinda Zhang. 2017. Mat-
terport3D: Learning from RGB-D data in indoor en-
vironments. International Conference on 3D Vision
(3DV).

Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agar-
wal, Hal Daume III, and John Langford. 2015.
Learning to search better than your teacher. In Pro-
ceedings of the International Conference of Machine
Learning.

Howard Chen, Alane Shur, Dipendra Misra, Noah
Snavely, Ian Artzi, Yoav, Stephen Gould, and An-
ton van den Hengel. 2019. Touchdown: Natural
language navigation and spatial reasoning in visual
street environments. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Maxime Chevalier-Boisvert, Dzmitry Bahdanau,
Salem Lahlou, Lucas Willems, Chitwan Saharia,
Thien Huu Nguyen, and Yoshua Bengio. 2019.
Babyai: A platform to study the sample efficiency
of grounded language learning. In Proceedings

of the International Conference on Learning
Representations.

Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan,
Ben Kybartas, Tavian Barnes, Emery Fine, James
Moore, Matthew Hausknecht, Layla El Asri, Mah-
moud Adada, Wendy Tay, and Adam Trischler.
2018. Textworld: A learning environment for text-
based games. In Computer Games Workshop at
ICML/IJCAI.

Abhishek Das, Samyak Datta, Georgia Gkioxari, Ste-
fan Lee, Devi Parikh, and Dhruv Batra. 2018. Em-
bodied question answering. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition.

Hal Daumé III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. In
Machine learning, volume 75, pages 297–325.
Springer.

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower models
for vision-and-language navigation. In Proceedings
of Advances in Neural Information Processing Sys-
tems.

He He, Jason Eisner, and Hal Daumé III. 2012. Imi-
tation learning by coaching. In Proceedings of Ad-
vances in Neural Information Processing Systems,
pages 3149–3157.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Ju-
lian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent
Vanhoucke, et al. 2018. Qt-opt: Scalable deep rein-
forcement learning for vision-based robotic manip-
ulation. In Proceedings of the Conference on Robot
Learning.

Tero Karras, Timo Aila, Samuli Laine, and Jaakko
Lehtinen. 2017. Progressive growing of gans for
improved quality, stability, and variation. In Pro-
ceedings of the International Conference on Learn-
ing Representations.

Tero Karras, Samuli Laine, and Timo Aila. 2018. A
style-based generator architecture for generative ad-
versarial networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion.

Janne Karttunen, Anssi Kanervisto, Ville Hautamäki,
and Ville Kyrki. 2019. From video game to real
robot: The transfer between action spaces. arXiv
preprint arXiv:1905.00741.



694

Michał Kempka, Marek Wydmuch, Grzegorz Runc,
Jakub Toczek, and Wojciech Jaśkowski. 2016. Viz-
doom: A doom-based ai research platform for visual
reinforcement learning. In Computational Intelli-
gence and Games (CIG), 2016 IEEE Conference on,
pages 1–8. IEEE.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Changsong Liu, Shaohua Yang, Sari Saba-Sadiya,
Nishant Shukla, Yunzhong He, Song-Chun Zhu,
and Joyce Chai. 2016. Jointly learning grounded
task structures from language instruction and visual
demonstration. In Proceedings of Emperical Meth-
ods in Natural Language Processing, pages 1482–
1492.

Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-
Regib, Zsolt Kira, Richard Socher, and Caiming
Xiong. 2019a. Self-monitoring navigation agent via
auxiliary progress estimation. In Proceedings of the
International Conference on Learning Representa-
tions.

Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caim-
ing Xiong, and Zsolt Kira. 2019b. The regretful
agent: Heuristic-aided navigation through progress
estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
6732–6740.

Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind
Niklasson, Max Shatkhin, and Yoav Artzi. 2018.
Mapping instructions to actions in 3d environments
with visual goal prediction. In Proceedings of Em-
perical Methods in Natural Language Processing,
pages 2667–2678. Association for Computational
Linguistics.

Dipendra Misra, John Langford, and Yoav Artzi. 2017.
Mapping instructions and visual observations to ac-
tions with reinforcement learning. Proceedings of
Emperical Methods in Natural Language Process-
ing.

Dipendra K Misra, Jaeyong Sung, Kevin Lee, and
Ashutosh Saxena. 2014. Tell me dave: Contextsen-
sitive grounding of natural language to mobile ma-
nipulation instructions. In Robotics: Science and
Systems.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. 2013. Playing atari with
deep reinforcement learning. In NIPS Deep Learn-
ing Workshop.

Shiwali Mohan and John Laird. 2014. Learning goal-
oriented hierarchical tasks from situated interactive
instruction. In Association for the Advancement of
Artificial Intelligence.

Khanh Nguyen, Debadeepta Dey, Chris Brockett, and
Bill Dolan. 2019. Vision-based navigation with
language-based assistance via imitation learning
with indirect intervention. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 12527–12537.

Yuankai Qi, Qi Wu, Peter Anderson, Marco Liu,
Chunhua Shen, and Anton van den Hengel. 2019.
Rerere: Remote embodied referring expressions
in real indoor environments. arXiv preprint
arXiv:1904.10151.

Stephane Ross and J Andrew Bagnell. 2014. Rein-
forcement and imitation learning via interactive no-
regret learning. arXiv preprint arXiv:1406.5979.

Stéphane Ross, Geoffrey Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In
Proceedings of Artificial Intelligence and Statistics,
pages 627–635.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition chal-
lenge. International Journal of Computer Vision,
115(3):211–252.

Paul E Rybski, Kevin Yoon, Jeremy Stolarz, and
Manuela M Veloso. 2007. Interactive robot task
training through dialog and demonstration. In Pro-
ceedings of the ACM/IEEE international conference
on Human-robot interaction, pages 49–56. ACM.

Amr Sharaf and Hal Daumé III. 2017. Structured pre-
diction via learning to search under bandit feedback.
In Proceedings of the 2nd Workshop on Structured
Prediction for Natural Language Processing, pages
17–26.

Lanbo She, Shaohua Yang, Yu Cheng, Yunyi Jia, Joyce
Chai, and Ning Xi. 2014. Back to the blocks world:
Learning new actions through situated human-robot
dialogue. In Proceedings of the 15th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), pages 89–97.

Wen Sun, Arun Venkatraman, Geoffrey J Gordon, By-
ron Boots, and J Andrew Bagnell. 2017. Deeply
aggrevated: Differentiable imitation learning for se-
quential prediction. In Proceedings of the Interna-
tional Conference of Machine Learning.

Jesse Thomason, Daniel Gordan, and Yonatan Bisk.
2019a. Shifting the baseline: Single modality per-
formance on visual navigation & qa. In Conference
of the North American Chapter of the Association
for Computational Linguistics.

Jesse Thomason, Michael Murray, Maya Cakmak, and
Luke Zettlemoyer. 2019b. Vision-and-dialog navi-
gation. In Proceedings of the Conference on Robot
Learning.

http://aclweb.org/anthology/D18-1287
http://aclweb.org/anthology/D18-1287


695

Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012.
Mujoco: A physics engine for model-based con-
trol. In 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems, pages 5026–5033.
IEEE.

Jack Urbanek, Angela Fan, Siddharth Karamcheti,
Saachi Jain, Samuel Humeau, Emily Dinan, Tim
Rocktäschel, Douwe Kiela, Arthur Szlam, and Ja-
son Weston. 2019. Learning to speak and act
in a fantasy text adventure game. arXiv preprint
arXiv:1903.03094.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko
Georgiev, Alexander Sasha Vezhnevets, Michelle
Yeo, Alireza Makhzani, Heinrich Küttler, John Aga-
piou, Julian Schrittwieser, et al. 2017. Starcraft ii:
A new challenge for reinforcement learning. arXiv
preprint arXiv:1708.04782.

Harm de Vries, Kurt Shuster, Dhruv Batra, Devi
Parikh, Jason Weston, and Douwe Kiela. 2018.
Talk the walk: Navigating new york city
through grounded dialogue. arXiv preprint
arXiv:1807.03367.

Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jian-
feng Gao, Dinghan Shen, Yuan-Fang Wang,
William Yang Wang, and Lei Zhang. 2019. Re-
inforced cross-modal matching and self-supervised
imitation learning for vision-language navigation.
In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition.

Xin Wang, Wenhan Xiong, Hongmin Wang, and
William Yang Wang. 2018. Look before you
leap: Bridging model-free and model-based rein-
forcement learning for planned-ahead vision-and-
language navigation. In Proceedings of the Euro-
pean Conference on Computer Vision.

Sean Welleck, Ilia Kulikov, Stephen Roller, Emily
Dinan, Kyunghyun Cho, and Jason Weston. 2019.
Neural text generation with unlikelihood training.
arXiv preprint arXiv:1908.04319.

Erik Wijmans, Samyak Datta, Oleksandr Maksymets,
Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan
Essa, Devi Parikh, and Dhruv Batra. 2019. Em-
bodied question answering in photorealistic environ-
ments with point cloud perception. In Proceedings
of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6659–6668.


