



















































Adapting RNN Sequence Prediction Model to Multi-label Set Prediction


Proceedings of NAACL-HLT 2019, pages 3181–3190
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3181

Adapting RNN Sequence Prediction Model to Multi-label Set Prediction

Kechen Qin Cheng Li Virgil Pavlu Javed A. Aslam
Khoury College of Computer Sciences

Northeastern University
qin.ke@husky.neu.edu {chengli,vip,jaa}@ccs.neu.edu

Abstract
We present an adaptation of RNN sequence
models to the problem of multi-label classifi-
cation for text, where the target is a set of la-
bels, not a sequence. Previous such RNN mod-
els define probabilities for sequences but not
for sets; attempts to obtain a set probability are
after-thoughts of the network design, includ-
ing pre-specifying the label order, or relating
the sequence probability to the set probability
in ad hoc ways.

Our formulation is derived from a princi-
pled notion of set probability, as the sum of
probabilities of corresponding permutation se-
quences for the set. We provide a new training
objective that maximizes this set probability,
and a new prediction objective that finds the
most probable set on a test document. These
new objectives are theoretically appealing be-
cause they give the RNN model freedom to
discover the best label order, which often is the
natural one (but different among documents).

We develop efficient procedures to tackle the
computation difficulties involved in training
and prediction. Experiments on benchmark
datasets demonstrate that we outperform state-
of-the-art methods for this task.

1 Introduction

Multi-label text classification is an important ma-
chine learning task wherein one must predict a
set of labels to associate with a given document;
for example, a news article might be tagged with
labels sport, football, 2018 world cup,
and Russia. Formally, we are given a set of label
candidates L = {1, 2, ..., L}, and we aim to build
a classifier which maps a document x to a set of
labels y ⊂ L. The label set y is typically written
as a binary vector y ∈ {0, 1}L, with each bit y`
indicating the presence or absence of a label.

Naively, one could predict each label indepen-
dently without considering label dependencies.

This approach is called Binary Relevance (Boutell
et al., 2004; Tsoumakas and Katakis, 2007), and
is widely used due to its simplicity, but it of-
ten does not deliver good performance. Intu-
itively, knowing some labels—such as sport
and football—should make it easier to predict
2018 world cup and then Russia. There
are several methods that try to capture label de-
pendencies by building a joint probability esti-
mation over all labels p(y = (y1, y2, ..., yL)|x)
(Ghamrawi and McCallum, 2005; Read et al.,
2009; Dembczynski et al., 2010; Li et al., 2016).
The most popular approach, Probabilistic Clas-
sifier Chain (PCC) (Dembczynski et al., 2010)
learns labels one-by-one in a predefined fixed or-
der: for each label, it uses one classifier to estimate
the probability of that label given all previous la-
bels predictions, p(yl|y1, ..., yl−1, x). PCC’s well
known drawback is that errors in early probability
estimations tend to affect subsequent predictions,
and can become massive when the total number of
label candidates L is large.

Recurrent neural network (RNN) is originally
designed to output a sequential structure, such as a
sentence (Cho et al., 2014). Recently, RNNs have
also been applied to multi-label classification by
mapping the label set to a sequence (Wang et al.,
2016; Zhang et al., 2016; Jin and Nakayama, 2016;
Wang et al., 2017b,a; Chen et al., 2018; Yang et al.,
2018). In contrast to PCC where a binary deci-
sion is made for each label sequentially, RNN only
predicts the positive labels explicitly and therefore
its decision chain length is equal to the number of
positive labels, not the number of all labels. This
makes RNN suffer less from early estimation er-
rors than PCC.

Both PCC and RNN rely heavily on label orders
in training and prediction. In multi-label data, the
labels are given as sets, not necessarily with nat-
ural orders. RNN defines a sequence probability,



3182

while PCC defines set probability. Various ways
of arranging sets as sequences have been explored:
ordering alphabetically, by frequency, based on a
label hierarchy, or according to some label ranking
algorithm (Liu and Tsang, 2015). Previous experi-
mental results show that which order to choose can
have a significant impact on learning and predic-
tion (Vinyals et al., 2016; Nam et al., 2017; Chen
et al., 2018). In the above example, starting label
predictions sequence with Russia, while correct,
would make the other predictions very difficult.

Previous work has shown that it is possible to
train an RNN on multi-label data without spec-
ifying the label order in advance. With special
training objectives, RNN can explore different la-
bel orders and converge to some order automati-
cally (Vinyals et al., 2016). In this paper we fol-
low the same line of study: We consider how to
adapt RNN sequence model to multi-label set pre-
diction without specifying the label order. Specif-
ically, we make the following contributions:

1. We analyze existing RNN models proposed
for multi-label prediction, and show that ex-
isting training and prediction objectives are
not well justified mathematically and have
undesired consequences in practice.

2. We develop efficient approximate training
and prediction methods. We propose new
training and prediction objectives based on a
principled notion of set probability. Our new
formulation avoids the drawbacks of existing
ones and gives the RNN model freedom to
discover the best label order.

3. We crawl two new datasets for multi-label
prediction task, and apply our method to
them. We also test our method on two exist-
ing multi-label datasets. The experimental re-
sults show that our method outperforms state-
of-the-art methods on all datasets. We release
the datasets at http://www.ccis.neu.
edu/home/kechenqin.

2 Mapping Sequences to Sets

In this section, we describe how existing ap-
proaches map sequences to sets, by writing down
their objective functions using consistent nota-
tions. To review RNN designed for sequences,
let s = (s1, s2, ..., sT ) be an input sequence
of outcomes, in a particular order, where st ∈

{1, 2, ..., L}; the order is often critical to the dat-
apoint. An RNN model defines a probability dis-
tribution over all possible output sequences given
the input in the form p(s = (s1, s2, ..., sT )|x) =∏T
t=1 p(st|x, s1, s2, ..., st−1). To train the RNN

model, one maximizes the likelihood of the
ground truth sequence.

At prediction time, one seeks to find the
sequence with the highest probability s∗ =
arg maxs p(s|x), and this is usually implemented
approximately with a beam search procedure
(Lowerre, 1976) (we modified into Algorithm 1).
The sequence history is encoded with an inter-
nal memory vector ht which is updated over time.
RNN is also often equipped with the attention
mechanism (Bahdanau et al., 2014), which in
each timestep t puts different weights on differ-
ent words (features) and thus effectively attends
on a list of important words. The context vector
ct is computed as the weighted average over the
dense representation of important words to cap-
ture information from the document. The con-
text ct, the RNN memory ht at timestep t, and
the encoding of previous label st−1 are all con-
catenated and used to model the label probability
distribution at time t as p(st|x, s1, s2, ..., st−1) ∼
softmax(φ(ct, ht, st−1)), where φ is a non-linear
function, and softmax is the normalized exponen-
tial function.

To apply RNN to multi-label problems, one ap-
proach is to map the given set of labels y to a
sequence s = (s1, s2, ..., sT ), on training docu-
ments. This is usually obtained by writing the la-
bel set in a globally fixed order (e.g. by label fre-
quency), as in PCC. Once the mapping is done,
RNN is trained with the standard maximum likeli-
hood objective (Nam et al., 2017):

maximize
N∑
n=1

log p(s(n)|x(n)) (1)

where x(n) is the n-th document and N is the total
number of documents in the corpus.

Vinyals et al. (2016) proposes to dynamically
choose during training the sequence order deemed
as most probable by the current RNN model:

maximize
N∑
n=1

max
s∈π(y(n))

log p(s|x(n)) (2)

where the π(y(n)) stands for all permutations of
the label set y(n). This eliminates the need to man-
ually specify the label order. However, as noticed

http://www.ccis.neu.edu/home/kechenqin
http://www.ccis.neu.edu/home/kechenqin


3183

Methods Training objectives Prediction objectives
seq2seq-RNN maximize

∑N
n=1 log p(s

(n)|x(n)) ŷ = set(s∗), s∗ = arg maxs p(s|x)
Vinyals-RNN-max maximize

∑N
n=1 maxs∈π(y(n)) log p(s|x(n)) ŷ = set(s∗), s∗ = arg maxs p(s|x)

Vinyals-RNN-uniform maximize
∑N

n=1

∑
s∈π(y(n)) log p(s|x(n)) ŷ = set(s∗), s∗ = arg maxs p(s|x)

Vinyals-RNN-sample maximize
∑N

n=1

∑
s∈π(y(n)) p(s|x(n)) log p(s|x(n)) ŷ = set(s∗), s∗ = arg maxs p(s|x)

set-RNN (ours) maximize
∑N

n=1 log
∑

s∈π(y(n)) p(s|x(n)) ŷ = arg maxy p(y|x)

Table 1: Comparison between previous and our set-RNN training and prediction objectives.

by the authors, this objective cannot be used in the
early training stages: the early order choice (of-
ten random) is reinforced by this objective and can
be stuck upon permanently. To address this issue,
Vinyals et al. (2016) also proposes two smoother
alternative objectives to initialize the model train-
ing:

The authors suggest that one first consider many
random orders for each label set in order to explore
the space:

maximize
N∑
n=1

∑
s∈π(y(n))

log p(s|x(n)) (3)

After that, one can sample sequences following
the model predictive distribution instead of uni-
form distribution:

maximize
N∑
n=1

∑
s∈π(y(n))

p(s|x(n)) log p(s|x(n))

(4)

In training, one needs to schedule the transi-
tion among these objectives, a rather tricky en-
deavor. At prediction time, one needs to find
the most probable set. This is done by (ap-
proximately) finding the most probable sequence
s∗ = arg maxs p(s|x) and treating it as a set
ŷ = set(s∗). With a large number of sequences, it
is quite possible that the argmax has actually a low
probability, which can lead to neglecting impor-
tant information when we ignore sequences other
than the top one.

3 Adapting RNN Sequence Prediction
Model to Multi-label Set Prediction

We propose a new way of adapting RNN to multi-
label set prediction, which we call set-RNN. We
appreciate the RNN model structure (Rumelhart
et al., 1988) (defines a probability distribution
over all possible sequences directly) and intro-
duce training and prediction objectives tailored

for sets that take advantage of it, while making a
clear distinction between the sequence probabil-
ity p(s|x) and the set probability p(y|x). We de-
fine the set probability as the sum of sequences
probabilities for all sequence permutations of the
set, namely p(y|x) =

∑
s∈π(y) p(s|x). Based

on this formulation, an RNN also defines a prob-
ability distribution over all possible sets indi-
rectly since

∑
y p(y|x) =

∑
y

∑
s∈π(y) p(s|x) =∑

s p(s|x) = 1. (For this equation to hold, in the-
ory, we should also consider permutations s with
repeated labels, such as (1, 2, 3, 1). But in prac-
tice, we find it very rare for RNN to actually gen-
erate sequences with repeated labels in our setup,
and whether allowing repetition or not does not
make much difference.)

In standard maximum likelihood training,
one wishes to maximize the likelihood of
given label sets, namely,

∏N
n=1 p(y

(n)|x(n)) =∏N
n=1

∑
s∈π(y(n)) p(s|x(n)), or equivalently,

maximize
N∑
n=1

log
∑

s∈π(y(n))

p(s|x(n)) (5)

3.1 How is our new formulation different?

This training objective (5) looks similar to the ob-
jective (3) considered in previous work (Vinyals
et al., 2016), but in fact they correspond to
very different transformations. Under the max-
imum likelihood framework, our objective (5)
corresponds to the transformation p(y|x) =∑

s∈π(y) p(s|x), while objective (3) corresponds
to the transformation p(y|x) =

∏
s∈π(y) p(s|x).

The latter transformation does not define a valid
probability distribution over y (i.e.,

∑
y p(y|x) 6=

1), and it has an undesired consequence in practi-
cal model training: because of the multiplication
operation, the RNN model has to assign equally
high probabilities to all sequence permutations of
the given label set in order to maximize the set
probability. If only some sequence permutations



3184

receive high probabilities while others receive low
probabilities, the set probability computed as the
product of sequence probabilities will still be low.
In other words, if for each document, RNN finds
one good way of ordering relevant labels (such as
hierarchically) and allocates most of the probabil-
ity mass to the sequence in that order, the model
still assigns low probabilities to the ground truth
label sets and will be penalized heavily. As a con-
sequence the model has little freedom in discover-
ing and concentrating on some natural label order.
In contrast, with our proposed training objective,
in which the multiplication operation is replaced
by the summation operation, it suffices to find only
one reasonable permutation of the labels for each
document. It is worth noting that different docu-
ments can have different label orders; thus our pro-
posed training objective gives the RNN model far
more freedom on label order. The other two objec-
tives (2) and (4) proposed in (Vinyals et al., 2016)
are less restrictive than (3), but they have to work
in conjunction with (3) because of the self rein-
forcement issue. Our proposed training objective
has a natural probabilistic interpretation, and does
not suffer from self reinforcement issue. Thus it
can serve as a stand alone training objective. Also,
using Jensen’s inequality, one can show that objec-
tive (3) is maximizing a lower bound on the log-
likelihood, while objective (5) is maximizing it di-
rectly.

3.2 Training by Maximizing Set Probability
Training an RNN model with the proposed objec-
tive (5) requires summing up sequence (permu-
tation) probabilities for a set y, where |y| is the
cardinality of the set. Thus evaluating this objec-
tive exactly can be intractable. We can approxi-
mate this sum by only considering the topK high-
est probability sequences produced by the RNN
model. We introduce a variant of beam search for
sets with width K and with the search candidates
in each step restricted to only labels in the set (see
Algorithm 1 with ALL = 1). This approximate
inference procedure is carried out repeatedly be-
fore each batch training step, in order to find high-
est probability sequences for all training instances
occurring in that batch. The overall training pro-
cedure is summarized in Algorithm 2.

3.3 Predicting the Most Probable Set
The transformation p(y|x) =

∑
s∈π(y) p(s|x)

also naturally leads to a prediction procedure,

Algorithm 1: Beam Search
Input : Instance x

Subset of labels considered G ⊂ L
Boolean flag ALL: 1 if sequences

must contain all G labels; 0 if partial
sequences are allowed
Output: A list of top sequences and the

associated probabilities
1 Let s1,s2,...,sK be the top K sequences found

so far. Initially, all K sequences are empty.
⊕ means concatenation.

2 while true do
3 // Step 1: Generate Candidate Sequences

from each existing sequence sk ∈ K and
all possible new labels l ∈ G:

4 Expand all non-stopped sequences:
5 C = {sk ⊕ l|l ∈ G,STOP /∈ sk}
6 Include stopped sequences:
7 C = C ∪ {sk|STOP ∈ sk}
8 Terminate non-stopped sequences:
9 if ALL == 0 then

10 C = C ∪ {sk ⊕STOP |STOP /∈ sk}
11 end
12 // Step 2: Select highest probabilities

sequences from candidate set C
13 K = topK-argmaxk{prob[sk]|sk ∈ C}
14 if all top K sequences end with STOP or

contain all labels in G then
15 Terminate the algorithm
16 end
17 end
18 return sequence list s1,s2,...,sK and the

associated probabilities

which is different from the previous standard of
directly using most probable sequence as a set.
We instead aim to find the most likely set ŷ =
arg maxy p(y|x), which involves summing up
probabilities for all of its permutations. To make it
tractable, we propose a two-level beam search pro-
cedure. First we run standard RNN beam search
(Algorithm 1 with ALL = 0) to generate a list
of highest probability sequences. We then con-
sider the label set associated with each label se-
quence. For each set, we evaluate its probabil-
ity using the same approximate summation pro-
cedure as the one used during model training (Al-
gorithm 1 with ALL = 1): we run our modified
beam search to find the top few highest probability
sequences associated with the set and sum up their



3185

Algorithm 2: Training method for set-RNN
Input : Multi-label dataset

(x(n),y(n)), n = 1, 2, ..., N
Output: Trained RNN model parameters

1 foreach batch do
2 foreach (xn,yn) in the batch do
3 Get top K sequences :
4 {sn1 , ..., snK , p(sn1 |xn), ..., p(snK |xn)}=

= Beam Search(xn,yn, ALL = 1)
5 end
6 Update model parameters by maximizing∑

(xn,yn)∈batch
log

∑
s∈{sn1 ,...,snK}

p(s|xn)

7 end

probabilities. Among these sets that we have eval-
uated, we choose the one with the highest proba-
bility as the prediction. The overall prediction pro-
cedure is summarized in Algorithm 3. As we shall
show in case study, the most probable set may not
correspond to the most probable sequence; these
are certainly cases where our method has an ad-
vantage.

Both our method and the competitor state-
of-the-art (Vinyals-RNNs) are at most K times
slower than a vanilla-RNN, due to the time spent
on dealing with K permutations per datapoint.
Our proposed method is about as fast as the
Vinyals-RNN methods, except for the Vinyals-
RNN-uniform which is a bit faster (by a factor of
1.5) because its epochs do not run the additional
forward pass.

4 Results and Analysis

4.1 Experimental Setup
We test our proposed set-RNN method on 4 real-
world datasets, RCV1-v2, Slashdot, TheGuardian,
and Arxiv Academic Paper Dataset (AAPD) (Yang
et al., 2018). We take the public RCV1-v2 re-
lease1 and randomly sample 50,000 documents.
We crawl Slashdot and TheGuardian documents
from their websites2 and treat the official editor
tags as ground truth. We also gather a list of user
tags3 for each document and treat them as ad-
ditional features. For AAPD dataset, we follow

1
http://www.ai.mit.edu/projects/jmlr/papers/

volume5/lewis04a/lyrl2004_rcv1v2_README.htm
2Slashdot: https://slashdot.org/ Note that there is another pub-

lic Slashdot multi-label dataset (Read et al., 2009) but we do not use that one
because it is quite small. TheGuardian: https://www.theguardian.
com

3
www.zubiaga.org/datasets/socialbm0311/

Algorithm 3: Prediction Method for set-RNN
Input : Instance x
Output: Predicted label set ŷ

1 Obtain K highest probability sequences :
2 {s1, ..., sK} = Beam Search(x,L, ALL = 0)
3 Map each sequence sk to the corresponding

set yk and remove duplicate sets (if any)
4 foreach yk do
5 Get K most probable sequences

associated with yk and their
probabilities :

6 {s′1, ..., s′K , p(s′1|x), ..., p(s′K |x)}=
7 = Beam Search(x,yk, ALL = 1)
8 Set probability is approx by summing up :

p(yk|x) ≈
∑

s∈{s′1,...,s′K}
p(s|x)

9 end
10 ŷ = argmaxyk(p(yk|x))

the same train/test split as in (Yang et al., 2018).
Table 2 contains statistics of these four datasets.
Links to document, official editor tags, and user
tags are avaliable at http://www.ccis.neu.
edu/home/kechenqin.

Data #Train #Test Cardinality #Labels Doc length
Slashdot 19,258 4,814 4.15 291 64
RCV1-v2 40,000 10,000 3.17 101 121
TheGuardian 37,638 9,409 7.41 1,527 505
AAPD 53,840 1,000 2.41 54 163

Table 2: Statistics of the datasets.

To process documents, we filter out stopwords
and punctuations. Each document is truncated
to have maximum 500 words for TheGuardian
and AAPD, and 120 for Slashdot and RCV1-v2.
Zero padding is used if the document contains
less words than the maximum number. Num-
bers and out-of-vocabulary words are replaced
with special tokens. Words, user tags and labels
are all encoded as 300-dimensional vectors using
WORD2VEC (Mikolov et al., 2013).

We implement RNNs with attention using
TENSORFLOW-1.4.0 (Abadi et al., 2016). The dy-
namic function for RNNs is chosen to be Gated re-
current units (GRU) with 2 layers and at most 50
units in decoder. The size of the GRU unit is 300.
We set dropout rate to 0.3, and train the model
with Adam optimizer (Kingma and Ba, 2014) with
learning rate 0.0005. Beam size is set to be 12
at both training and inference stages. We adopt
label-F1 (average F1 over labels) and instance-F1

http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm
http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm
https://slashdot.org/
https://www.theguardian.com
https://www.theguardian.com
www.zubiaga.org/datasets/socialbm0311/
http://www.ccis.neu.edu/home/kechenqin
http://www.ccis.neu.edu/home/kechenqin


3186

Methods
Slashdot RCV1-v2 TheGuardian AAPD

label-F1 instance-F1 label-F1 instance-F1 label-F1 instance-F1 label-F1 instance-F1 hamming-loss micro-F1
BR .271 .484 .486 .802 .292 .572 .529 .654 .0230 .685
BR-support .247 .516 .486 .805 .296 .594 .545 .689 .0228 .696
PCC .279 .480 .595 .818 - - .541 .688 .0255 .682
seq2seq-RNN .270 .528 .561 .824 .331 .603 .510 .708 .0254 .701
Vinyals-RNN-uniform .279 .527 .578 .826 .313 .567 .532 .721 .0241 .711
Vinyals-RNN-sample .300 .531 .590 .828 .339 .597 .527 .706 .0259 .697
Vinyals-RNN-max .293 .530 .588 .829 .343 .599 .535 .709 .0256 .700
Vinyals-RNN-max-direct .226 .518 .539 .808 .313 .583 .490 .702 .0257 .694
SGM - - - - - - - - .0245 .710
set-RNN .310 .538 .607 .838 .361 .607 .548 .731 .0241 .720

Table 3: Comparison of different approaches. “-” means result not available. For hamming loss, the lower the
value is, the better the model performs. For all other measures, the higher the better.

Methods
Slashdot RCV1-v2 TheGuardian AAPD

label-F1 instance-F1 label-F1 instance-F1 label-F1 instance-F1 label-F1 instance-F1
seq2seq-RNN .270→.269 .528→.528 .561→.561 .824→.824 .331→.336 .603→.603 .510→.511 .708→.709
Vinyals-RNN-uniform .279→.288 .527→.537 .578→.587 .826→.833 .313→.336 .567→.585 .532→.542 .721→.724
Vinyals-RNN-sample .300→.303 .531→.537 .590→.597 .828→.833 .339→.351 .597→.602 .527→.530 .706→.708
Vinyals-RNN-max .293→.301 .530→.535 .588→.585 .829→.830 .343→.352 .599→.604 .535→.537 .709→.712
Vinyals-RNN-max-direct .226→.228 .518→.519 .539→.538 .808→.808 .313→.316 .583→.584 .490→.490 .702→.701
set-RNN .297→.310 .528→.538 .593→.607 .831→.838 .349→.361 .595→.607 .548→.548 .728→.731

Table 4: Predicting the most probable sequence vs. predicting the most probable set. Numbers before the arrow:
predicting the most probable sequence. Numbers after the arrow: predicting the most probable set. We highlight
scores which get significantly improved in bold (improvement is larger than 0.01).

(average F1 over instances) as our main evaluation
metrics, as defined below:

label-F1 =
1

L

L∑
`=1

2
∑N

n=1 y
(n)
` ŷ

(n)
`∑N

n=1 y
(n)
` +

∑N
n=1 ŷ

(n)
`

instance-F1 =
1

N

N∑
n=1

2
∑L

`=1 y
(n)
` ŷ

(n)
`∑L

`=1 y
(n)
` +

∑L
`=1 ŷ

(n)
`

where for each instance n, y(n)` = 1 if label ` is a
given label in ground truth; ŷ(n)` = 1 if label ` is a
predicted label.

We compare our method with the following
methods:

• Binary Relevance (BR) (Tsoumakas and
Katakis, 2007) with both independent train-
ing and prediction;

• Binary Relevance with support infer-
ence (BR-support) (Wang et al., 2018)
which trains binary classifiers independently
but imposes label constraints at predic-
tion time by only considering label sets
observed during training, namely ŷ =
arg maxobserved y

∏L
`=1 p(y`|x);

• Probabilistic Classifier Chain (PCC)
(Dembczynski et al., 2010) which transforms

the multi-label classification task into a chain
of binary classification problems. Predictions
are made with Beam Search.

• Sequence to Sequence RNN (seq2seq-
RNN) (Nam et al., 2017) which maps each
set to a sequence by decreasing label fre-
quency and solves the multi-label task with
an RNN designed for sequence prediction
(see Table 1).

• Vinyals-RNN-uniform, Vinyals-RNN-
sample, and Vinyals-RNN-max are three
variants of RNNs proposed by (Vinyals
et al., 2016). They are trained with differ-
ent objectives that correspond to different
transformations between sets and sequences.
See Table 1 for a summary of their training
objectives. Following the approach taken
by (Vinyals et al., 2016), Vinyals-RNN-
sample and Vinyals-RNN-max are initialized
by Vinyals-RNN-uniform. We have also
tested training Vinyals-RNN-max directly
without having Vinyals-RNN-uniform
as an initialization, and we name it as
Vinyals-RNN-max-direct.

• Sequence Generation Model (SGM) (Yang
et al., 2018) which trains the RNN model



3187

similar to seq2seq-RNN but uses a new de-
coder structure that computes a weighted
global embedding based on all labels as op-
posed to just the top one at each timestep.

In BR and PCC, logistic regressions with L1
and L2 regularizations are used as the underlying
binary classifiers. seq2seq-RNN, PCC, and SGM
rely on a particular label order. We adopt the de-
creasing label frequency order, which is the most
popular choice.

4.2 Experimental Results

Table 3 shows the performance of different meth-
ods in terms of label-F1 and instance-F1. The
SGM results are taken directly from (Yang et al.,
2018), and are originally reported only on AAPD
dataset in terms of hamming-loss and micro-F1.
Definitions of these two metrics can be found in
(Koyejo et al., 2015).

Our method performs the best in all metrics
on all datasets (except hamming loss on AAPD,
see table 3). In general, RNN based methods
perform better than traditional methods BR, BR-
support and PCC. Among the Vinyals-RNN vari-
ants, Vinyals-RNN-max and Vinyals-sample work
the best and have similar performance. How-
ever, they have to be initialized by Vinyals-RNN-
uniform. Otherwise, the training gets stuck in
early stage and the performance degrades signif-
icantly. One can see the clear degradation by com-
paring the Vinyals-RNN-max row (with initializa-
tion) with the Vinyals-RNN-max-direct row (with-
out initialization). By contrast, our training objec-
tive in set-RNN does not suffer from this issue and
can serve as a stable stand alone training objective.

On TheGuardian dataset, set-RNN performs
slightly better than seq2seq-RNN in terms of
instance-F1, but much better in terms of label-
F1. It is known that instance-F1 is basically deter-
mined by the popular labels’ performance while
label-F1 is also sensitive to the performance on
rare labels. Figure 1 shows that set-RNN predicts
rare labels better than seq2seq-RNN.

Next we analyze how much benefit our new
set prediction strategy brings in. For each RNN-
based method, we test two prediction strategies:
1) finding the sequence with the highest proba-
bility and outputting the corresponding set (this
is the default prediction strategy for all models
except set-RNN); 2) outputting the set with the
highest probability (this is the default prediction

Figure 1: Average F1 over rare labels with the same
frequency on TheGuardian dataset. Blue(∆)=set-RNN,
Red(·)=seq2seq-RNN.

strategy for set-RNN). Table 4 shows how each
method performs with these two prediction strate-
gies. One can see that Vinyals-RNN-uniform
and set-RNN benefit most from predicting the top
set, Vinyals-RNN-sample, Vinyals-RNN-max and
Vinyals-RNN-max-direct benefit less, and seq2seq
RNN does not benefit at all. Intuitively, for the
top-set prediction to be different from the top-
sequence prediction, the model has to spread prob-
ability mass across different sequence permuta-
tions of the same set.

4.3 Analysis: Sequence Probability
Dsitribution

Results in Table 4 motivates us to check how
sharply (or uniformly) distributed the probabili-
ties are over different sequence permutations of
the predicted set. We first normalize these se-
quence probabilities related to the predicted set
and then compute the entropy. To make predic-
tions with different set sizes (and hence differ-
ent number of sequence permutations) compara-
ble, we further divide the entropy by the logarithm
of number of sequences. Smaller entropy values
indicate a sharper distributions. The results are
shown in Figure 2.

seq2seq-RNN trained with fixed label order and
standard RNN objective (1) generates very sharp
sequence distributions. It basically only assigns
probability to one sequence in the given order.
The entropy is close to 0. In this case, predict-
ing the set is no different than predicting the top
sequence (see Table 4). On the other extreme
is Vinyals-RNN-uniform, trained with objective
(3), which spreads probabilities across many se-



3188

Figure 2: Entropy of sequence probability distribution
for each model. Blue(\)=Vinyals-RNN-uniform,
Orange(+)=set-RNN, Green(×)=Vinyals-RNN-max,
Red(·)=seq2seq-RNN.

quences, and leads to the highest entropy among
all models tested (the uniform distribution has the
max entropy of 1). From Table 4, we see that by
summing up sequence probabilities and predict-
ing the most probable set, Vinyals-RNN-uniform’s
performance improves. But as discussed earlier,
training with the objective (3) makes it impos-
sible for the model to discover and concentrate
on a particular natural label order (represented
by a sequence). Overall Vinyals-RNN-uniform is
not competitive even with the set-prediction en-
hancement. Between the above two extremes are
Vinyals-RNN-max and set-RNN (we have omit-
ted Vinyals-RNN-sample and Vinyals-RNN-max-
direct here as they are similar to Vinyals-RNN-
max). Both models are allowed to assign probabil-
ity mass to a subset of sequences. Vinyals-RNN-
max produces sharper sequence distributions than
set-RNN, because Vinyals-RNN-max has the in-
centive to allocate most of the probability mass to
the most probable sequence due to the max opera-
tor in its training objective (2). From Table 4, one
can see that set-RNN clearly benefits from sum-
ming up sequence probabilities and predicting the
most probable set while Vinyals-RNN-max does
not benefit much. Therefore, the sequence proba-
bility summation is best used in both training and
prediction, as in our proposed method.

Comparing 4 datasets in Table 4, we also see
that Slashdot and TheGuardian, which have larger
label cardinalities (therefore more permutations
for one set potentially), benefit more from predict-
ing the most probable set than RCV1 and AAPD,
which have smaller label cardinalities.

5 Case Analysis

We further demonstrate how set-RNN works with
two examples. In the first example from the
RCV1-v2 dataset, the most probable set predicted
by set-RNN (which is also the correct set in this
example) does not come from the most probable
sequence. Top sequences in decreasing probabil-
ity order are listed in Table 5. The correct label
set {forex, markets, equity, money markets, met-
als trading, commodity} has the maximum total
probability of 0.161, but does not match the top
sequence.

PROB SEQUENCE
0.0236 equity, markets, money markets, forex
0.0196 forex, markets, equity, money markets, metals trading, commodity
0.0194 equity, markets, forex, money markets, metals trading, commodity
0.0159 markets, equity, forex, money markets, metals trading, commodity
0.0157 forex, money markets, equity, metals trading, markets, commodity
0.0153 forex, money markets, markets, equity, metals trading, commodity
0.0148 markets, equity, money markets, forex
0.0143 money markets, equity, metals trading, commodity, forex, markets
0.0123 markets, money markets, equity, metals trading, commodity, forex
0.0110 markets, equity, forex, money markets, commodity, metals trading
0.0107 forex, markets, equity, money markets, commodity, metals trading
0.0094 forex, money markets, equity, markets, metals trading, commodity

Table 5: The set-RNN predicted set (also the correct
set) {forex, markets, equity, money markets, metals
trading, commodity} has the max total probability of
0.161, but does not match the top sequence. Sequences
for the correct set are in bold.

Next we demonstrate the issue with prescribing
the sequence order in seq2seq-RNN with a The-
Guardian example4. Figure 3 shows the predic-
tions made by seq2seq-RNN and our method. In
this particular example the top sequence agrees
with the top set in our method’s prediction so we
can just analyze the top sequence. seq2seq-RNN
predicts Tate Modern (incorrect but more pop-
ular label) while we predict Tate Britain
(correct but less popular label). The seq2seq pre-
dicted sequence is in the decreasing label fre-
quency order while our predicted sequence is
not. In the training data, Exhibition is
more frequent than Tate Britain and Tate
Modern. If we arrange labels by decreas-
ing frequency, Exhibition is immediately fol-
lowed by Tate Modern 19 times, and by Tate
Britain only 3 times. So it is far more likely
to have Tate Modern than Tate Britain
after Exhibition. However, at the set level,
Exhibition and Tate Modern co-occurs 22
times while Exhibition and Tate Britain

4This document can be viewed at http://www.guardian.
co.uk/artanddesign/jonathanjonesblog/2009/apr/08/
altermodernism-nicolas-bourriaud

http://www.guardian.co.uk/artanddesign/jonathanjonesblog/2009/apr/08/altermodernism-nicolas-bourriaud
http://www.guardian.co.uk/artanddesign/jonathanjonesblog/2009/apr/08/altermodernism-nicolas-bourriaud
http://www.guardian.co.uk/artanddesign/jonathanjonesblog/2009/apr/08/altermodernism-nicolas-bourriaud


3189

Figure 3: Top: best sequence by seq2seq-RNN; bot-
tom: best sequence by set-RNN. Above models, at each
time, we list the top unigrams selected by attention.

co-occurs 12 times, so the difference is not so dra-
matic. In this case, imposing the sequence order
biases the probability estimation and leads to in-
correct predictions.

6 Conclusion

In this work, we present an adaptation of RNN se-
quence models to the problem of multi-label clas-
sification for text. RNN only directly defines prob-
abilities for sequences, but not for sets. Different
from previous approaches, which either transform
a set to a sequence in some pre-specified order, or
relate the sequence probability to the set probabil-
ity in some ad hoc way, our formulation is derived
from a principled notion of set probability. We
define the set probability as the sum of all corre-
sponding sequence permutation probabilities. We
derive a new training objective that maximizes the
set probability and a new prediction objective that
finds the most probable set. These new objec-
tives are theoretically more appealing than exist-
ing ones, because they give the RNN model more
freedom to automatically discover and utilize the
best label orders.

Acknowledgements

We thank reviewers and Krzysztof Dembczyński
for their helpful comments, Xiaofeng Yang for her
help on writing, and Bingyu Wang for his help
on proofreading. This work has been generously
supported through a grant from the Massachusetts
General Physicians Organization.

References

Martı́n Abadi, Paul Barham, Jianmin Chen,
Zhifeng Chen, Andy Davis, Jeffrey Dean,
Matthieu Devin, Sanjay Ghemawat, Geof-
frey Irving, Michael Isard, Manjunath Kudlur,
Josh Levenberg, Rajat Monga, Sherry Moore,
Derek Gordon Murray, Benoit Steiner, Paul A.
Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016.
Tensorflow: A system for large-scale machine
learning. In 12th USENIX Symposium on Op-
erating Systems Design and Implementation,
OSDI 2016, Savannah, GA, USA, November 2-
4, 2016., pages 265–283.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Matthew R. Boutell, Jiebo Luo, Xipeng Shen, and
Christopher M. Brown. 2004. Learning multi-
label scene classification. Pattern Recognition,
37(9):1757–1771.

Shang-Fu Chen, Yi-Chen Chen, Chih-Kuan Yeh,
and Yu-Chiang Frank Wang. 2018. Order-free
RNN with visual attention for multi-label clas-
sification. In Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, New
Orleans, Louisiana, USA, February 2-7, 2018.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014.
Learning phrase representations using RNN
encoder-decoder for statistical machine transla-
tion. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Pro-
cessing, EMNLP 2014, October 25-29, 2014,
Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1724–1734.

Krzysztof Dembczynski, Weiwei Cheng, and
Eyke Hüllermeier. 2010. Bayes optimal mul-
tilabel classification via probabilistic classifier
chains. In Proceedings of the 27th International
Conference on Machine Learning (ICML-10),
June 21-24, 2010, Haifa, Israel, pages 279–286.

Nadia Ghamrawi and Andrew McCallum. 2005.
Collective multi-label classification. In Pro-
ceedings of the 14th ACM international con-



3190

ference on Information and knowledge manage-
ment, pages 195–200. ACM.

Jiren Jin and Hideki Nakayama. 2016. Annota-
tion order matters: Recurrent image annotator
for arbitrary length image tagging. In 23rd In-
ternational Conference on Pattern Recognition,
ICPR 2016, Cancún, Mexico, December 4-8,
2016, pages 2452–2457.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Oluwasanmi O Koyejo, Nagarajan Natarajan,
Pradeep K Ravikumar, and Inderjit S Dhillon.
2015. Consistent multilabel classification. In
Advances in Neural Information Processing
Systems, pages 3321–3329.

Cheng Li, Bingyu Wang, Virgil Pavlu, and Javed
Aslam. 2016. Conditional bernoulli mixtures
for multi-label classification. In International
Conference on Machine Learning, pages 2482–
2491.

Weiwei Liu and Ivor Tsang. 2015. On the opti-
mality of classifier chain for multi-label clas-
sification. In Advances in Neural Information
Processing Systems, pages 712–720.

Bruce T Lowerre. 1976. The harpy speech recog-
nition system. Technical report, CARNEGIE-
MELLON UNIV PITTSBURGH PA DEPT OF
COMPUTER SCIENCE.

Tomas Mikolov, Kai Chen, Greg Corrado, and
Jeffrey Dean. 2013. Efficient estimation of
word representations in vector space. CoRR,
abs/1301.3781.

Jinseok Nam, Eneldo Loza Mencı́a, Hyunwoo J.
Kim, and Johannes Fürnkranz. 2017. Maxi-
mizing subset accuracy with recurrent neural
networks in multi-label classification. In Ad-
vances in Neural Information Processing Sys-
tems 30: Annual Conference on Neural Infor-
mation Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pages 5419–5429.

Jesse Read, Bernhard Pfahringer, Geoff Holmes,
and Eibe Frank. 2009. Classifier chains for
multi-label classification. In Joint European
Conference on Machine Learning and Knowl-
edge Discovery in Databases, pages 254–269.
Springer.

David E Rumelhart, Geoffrey E Hinton, Ronald J
Williams, et al. 1988. Learning representations
by back-propagating errors. Cognitive model-
ing, 5(3):1.

Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Inter-
national Journal of Data Warehousing and Min-
ing (IJDWM), 3(3):1–13.

Oriol Vinyals, Samy Bengio, and Manjunath Kud-
lur. 2016. Order matters: Sequence to sequence
for sets. CoRR, abs/1511.06391.

Bingyu Wang, Cheng Li, Virgil Pavlu, and Jay
Aslam. 2018. A pipeline for optimizing f1-
measure in multi-label text classification. In
2018 17th IEEE International Conference on
Machine Learning and Applications (ICMLA),
pages 913–918. IEEE.

Jiang Wang, Yi Yang, Junhua Mao, Zhiheng
Huang, Chang Huang, and Wei Xu. 2016.
CNN-RNN: A unified framework for multi-
label image classification. In 2016 IEEE Con-
ference on Computer Vision and Pattern Recog-
nition, CVPR 2016, Las Vegas, NV, USA, June
27-30, 2016, pages 2285–2294.

Jingya Wang, Xiatian Zhu, Shaogang Gong, and
Wei Li. 2017a. Attribute recognition by joint
recurrent learning of context and correlation.
CoRR, abs/1709.08553.

Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia
Xu, and Liang Lin. 2017b. Multi-label image
recognition by recurrently discovering atten-
tional regions. In IEEE International Confer-
ence on Computer Vision, ICCV 2017, Venice,
Italy, October 22-29, 2017, pages 464–472.

Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma,
Wei Wu, and Houfeng Wang. 2018. SGM: se-
quence generation model for multi-label classi-
fication. CoRR, abs/1806.04822.

Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang,
and Jianfeng Lu. 2016. Multi-label image clas-
sification with regional latent semantic depen-
dencies. CoRR, abs/1612.01082.


