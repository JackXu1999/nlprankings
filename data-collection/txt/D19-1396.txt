



















































Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3830–3840,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3830

Leverage Lexical Knowledge for Chinese Named Entity Recognition via
Collaborative Graph Network

Dianbo Sui1,2, Yubo Chen1, Kang Liu1,2, Jun Zhao1,2, Shengping Liu3
1 National Laboratory of Pattern Recognition, Institute of Automation,

Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China

3 Beijing Unisound Information Technology Co., Ltd, Beijing, 100028, China
{dianbo.sui, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn

liushengping@unisound.com

Abstract

The lack of word boundaries information has
been seen as one of the main obstacles to
develop a high performance Chinese named
entity recognition (NER) system. Fortunate-
ly, the automatically constructed lexicon con-
tains rich word boundaries information and
word semantic information. However, inte-
grating lexical knowledge in Chinese NER
tasks still faces challenges when it comes to
self-matched lexical words as well as the near-
est contextual lexical words. We present a Col-
laborative Graph Network to solve these chal-
lenges. Experiments on various datasets show
that our model not only outperforms the state-
of-the-art (SOTA) results, but also achieves a
speed that is six to fifteen times faster than that
of the SOTA model.1

1 Introduction

Named entity recognition (NER) aims to locate
and classify certain occurrences of words or ex-
pressions in unstructured text into predefined se-
mantic categories such as the person names, lo-
cations, organizations, etc. NER is an essen-
tial pre-processing step for many natural language
processing (NLP) applications, such as relation
extraction (Bunescu and Mooney, 2005), even-
t extraction (Chen et al., 2015), question answer-
ing (Mollá et al., 2006) etc. In English NER,
LSTM-CRF models (Lample et al., 2016; Ma and
Hovy, 2016; Chiu and Nichols, 2016; Liu et al.,
2018) leveraging word-level representations and
character-level representations achieve the state-
of-the-art results.

In this paper, we focus on Chinese NER. Com-
pared with English, Chinese has no obvious word
boundaries. Since without word boundaries infor-
mation, it is intuitive to use character information

1The code is available at https://github.com/
DianboWork/Graph4CNER

希 尔 顿 北 京 机 场离 开

离开
leave

北京机场
Beijing airport

I-PER or I-ORG ?

I-LOC
The nearest 
contextual 

lexical words
information

Self matched
lexical words
information

I-PER

I-LOC, B-LOC or O?

了

Sentence：希尔顿离开北京机场了。(Mr. Hilton has left Beijing airport.)

Matched Lexical Words:  希尔(Hill), 希尔顿(Hilton), 离开(leave), 北京(Beijing), 
北京机场(Beijing Airport) 

Figure 1: An example sentence integrating the near-
est contextual lexical words (red line) and self-matched
lexical words (green line)

only for Chinese NER (He and Wang, 2008; Liu
et al., 2010; Li et al., 2014), although such meth-
ods could result in the disregard of word informa-
tion. However, word information is very useful in
Chinese NER, because word boundaries are usu-
ally the same as named entity boundaries. For ex-
ample, as shown in Figure 1, the boundaries of the
word “�¬::” (Beijing airport) are the same as
the boundaries of the named entity “ �¬::”
(Beijing airport). Therefore, making full use of
word information would help to improve the Chi-
nese NER performance.

There are three main ways to incorporate word
information in NER. The first one is the pipeline
method. The way of pipeline method is to apply
Chinese Word Segmentation (CWS) first, and then
to use a word-based NER model. However, the
pipeline method suffers from error propagation, s-
ince the error of CWS may affect the performance
of NER. The second one is to learn CWS and NER
tasks jointly (Xu et al., 2013; Peng and Dredze,
2016; Cao et al., 2018; Wu et al., 2019). How-
ever, the joint models must rely on CWS annota-
tion datasets, which are costly and are annotated
under many diverse segmentation criteria (Chen

https://github.com/DianboWork/Graph4CNER
https://github.com/DianboWork/Graph4CNER


3831

et al., 2017). The third one is to leverage an auto-
matically constructed lexicon, which is pre-trained
on large automatically segmented texts. Lexical
knowledge includes boundaries and semantic in-
formation. Boundaries information is provided by
the lexicon word itself, and semantic information
is provided by pre-trained word embeddings (Ben-
gio et al., 2003; Mikolov et al., 2013). Compared
with joint methods, a lexicon is easy to obtain
and additional annotation CWS datasets are not re-
quired. Recently, Zhang and Yang (2018) propose
a lattice LSTM to integrate lexical knowledge in
NER. However, integrating lexical knowledge in-
to sentences still faces two challenges.

The first challenge is to integrate self-matched
lexical words. A self-matched lexical word of
a character is the lexical word that contains this
character. For instance, “�¬::::” (Beijing Air-
port) and “::::” (Airport) are the self-matched
words of the character “:::” (airplane). “»�”
(leave) is not the self-matched word of the char-
acter “:” (airplane), since “:” (airplane) is not
contained in the word “»�” (leave). The lexical
knowledge of self-matched word is useful in Chi-
nese NER. For example, as shown in Figure 1, the
boundaries and semantic knowledge of the self-
matched word “�¬::” (Beijing Airport) can
help the character “:”(airplane) to predict an “I-
LOC” tag, instead of “O” or “B-LOC” tags. How-
ever, due to the limits of the word-character lattice,
the lattice LSTM (Zhang and Yang, 2018) fails to
integrate the self-matched word “�¬::” (Bei-
jing Airport) into the character “:” (airplane).

The second challenge is to integrate the nearest
contextual lexical words directly. The nearest con-
textual lexical word of a character is the word that
matches the nearest past or future subsequence in
the given sentence of this character. For instance,
the lexical word “»�” (leave) is the nearest con-
textual word of the character “” (-ton), since
the word matches the nearest future subsequence
“»�” of the character, while “�¬” (Beijing)
is not the nearest contextual lexical word of this
character. The nearest contextual lexical words
are beneficial for Chinese NER. For example, as
shown in Figure 1, by directly using the semantic
knowledge of the nearest contextual words “»�”
(leave), an “I-PER” tag can be predicted instead
of an “I-ORG” tag, since “��” (Hilton Hotel-
s) cannot be taken as the subject of the verb “»
�” (leave). However, a lattice model (Zhang and

Yang, 2018) only implicitly integrate the knowl-
edge of the nearest contextual lexical words via
the previous hidden state. The information of the
nearest contextual lexical word may be disturbed
by other information.

To solve the above challenges, we propose a
character-based Collaborative Graph Network, in-
cluding an encoding layer, a graph layer, a fu-
sion layer and a decoding layer. Specifically,
there are three word-character interactive graphs
in the graph layer. The first one is the Contain-
ing graph (C-graph), which is designed for inte-
grating self-matched lexical words. It models the
connection between characters and self-matched
lexical words. The second one is the Transition
graph (T-graph), which builds the direct connec-
tion between characters and the nearest contextual
matched words. It helps to handle the challenge
of integrating the nearest contextual words direct-
ly. The third one is the Lattice graph (L-graph),
which is inspired by the lattice LSTM (Zhang and
Yang, 2018). L-graph captures partial information
of self-matched lexical words and the nearest con-
textual lexical words implicitly by multiple hops.
These graphs are built without external NLP tools,
which can avoid error propagation problem. Be-
sides, these graphs complement each other nicely
and a fusion layer is designed for collaboration be-
tween these graphs.

We test our model with various Chinese NER
datasets. our model not only significantly outper-
forms the existing state-of-the-art (SOTA) model
but also is six to fifteen times faster than the speed
of the SOTA model.

In summary, our main contributions are as fol-
lows:

• We propose a Collaborative Graph Network
to integrate lexical knowledge directly and
efficiently for Chinese NER.

• To solve the challenges of integrating self-
matched lexical words and the nearest con-
textual lexical words, we propose three word-
character interactive graphs. These inter-
active graphs can capture different lexical
knowledge and are built without external
NLP tools.

• We achieve the state-of-the-art results in var-
ious popular Chinese NER datasets, and our
model achieves a 6-15x speedup over the ex-
isting SOTA model.



3832

2 Related Work

NER. There is rich literature on NER. This in-
cludes statistic methods, such as SVM (Isozaki
and Kazawa, 2002), HMMs (Bikel et al., 1997)
and CRF (Lafferty et al., 2001), suffering from
feature engineering. There are also a number of
recent neural network approaches applied to NER,
such as (Collobert et al., 2011; Huang et al., 2015;
Lample et al., 2016; Ma and Hovy, 2016; Chiu
and Nichols, 2016; Liu et al., 2018; Akbik et al.,
2018; Jie et al., 2019; Akbik et al., 2019). Com-
pared with English, Chinese is not featured with
obvious word boundaries, but it is important to
leverage word boundaries and semantic informa-
tion in Chinese NER. Many works use word seg-
mentation information as extra features for Chi-
nese NER, such as (Peng and Dredze, 2015; He
and Sun, 2017a; Zhu and Wang, 2019). Peng and
Dredze (2016), Cao et al. (2018) and Wu et al.
(2019) propose joint models to train NER together
with CWS. Our work is inspired by lattice LST-
M (Zhang and Yang, 2018), which can integrate
lexicon in NER.
Graph convolutional networks. There are a
number of recent graph convolutional network
(GCN) architectures (Kipf and Welling, 2017;
Hamilton et al., 2017; Veličković et al., 2018; Qu
et al., 2019) for learning over graphs. Our work
is closely related to the graph attention network-
s (GAT), introduced by Veličković et al. (2018),
leveraging masked self-attention layers to assign
different importance to neighbouring nodes. In re-
cent years, there is more and more literature about
the application of GCN in NLP (Bastings et al.,
2017; Marcheggiani and Titov, 2017; Zhang et al.,
2018; Yao et al., 2019; Wang et al., 2018; Mishra
et al., 2019; Cao et al., 2019; Zhang et al., 2019).
Cetoli et al. (2017) use GCN to investigate the role
of the dependency tree in English named entity
recognition. However, most of the works (Bast-
ings et al., 2017; Marcheggiani and Titov, 2017;
Cetoli et al., 2017; Zhang et al., 2018) heavily re-
ly on the dependency tree to construct a single
graph, which suffer from error propagation. To
capture different semantic and boundaries infor-
mation, we propose a Collaborative Graph Net-
work consisting of three automatically constructed
graphs, which can avoid error propagation prob-
lem naturally. To our best knowledge, we are
the first to introduce GAT and automatically con-
structed semantic graphs to Chinese NER tasks.

3 Approach

In this section, we first introduce the construction
of graphs to integrate self-matched lexical words
and the nearest contextual lexical words into sen-
tences. We then introduce the architecture of Col-
laborative Graph Network as a core for solving
Chinese NER tasks.

3.1 The Construction of Graphs

To integrate self-matched lexical words and the n-
earest contextual lexical words, we propose three
word-character interactive graphs. The first is
the word-character Containing graph (C-graph),
which is to assist the character to capture the
boundaries and semantic information of self-
matched lexical words. The second is the word-
character Transition graph (T-graph). The func-
tion of T-graph is to assist the character to capture
the semantic information of the nearest contextu-
al lexical words. The third is the Lattice graph
(L-graph). Zhang and Yang (2018) propose a lat-
tice structure, nested in the LSTM (Hochreiter and
Schmidhuber, 1997), to integrate lexical knowl-
edge. We free the lattice structure from the LSTM
and adopt it as the third graph.

These three graphs share the same vertex set,
but the edge sets of the three graphs are completely
different. The vertex set is made up of the charac-
ters in the sentence and the matched lexical words,
for example, as shown in Figure 1, the vertex set is
V={�, �,..., , ��, ��, ..., �¬::}.
To represent the edge set, adjacency matrix needs
to be introduced. The elements of the adjacency
matrix indicate whether pairs of vertices are adja-
cent or not in the graph. Since the edge sets of
the three graphs are totally different, the adjacen-
cy matrices of these three graphs are introduced
below:

希 尔 顿 休 斯 顿 机 场离 开

休斯顿机场
Houston airport

休斯顿
Houston

机场
Airport

希尔顿
Hilton

了

离开
leave

希尔
Hill

Figure 2: Word-Character Containing graph



3833

Word-Character Containing graph
With the C-graph, the characters in the sentence
can capture the boundaries and semantic informa-
tion of self-matched lexical words. As shown in
Figure 2, if a lexical word i contains a character j,
the (i, j)-entry of the C-graph corresponding ad-
jacency matrix AC will be assigned a value of 1.

希 尔 顿 休 斯 顿 机 场离 开

休斯顿机场
Houston airport

休斯顿
Houston

机场
Airport

希尔顿
Hilton

了

离开
leave

希尔
Hill

Figure 3: Word-Character Transition graph

Word-Character Transition graph
The T-graph is to assist the character to capture
the semantic information of the nearest contextual
lexical words. As shown in Figure 3, if a lexi-
cal word i or a character m matches the nearest
preceding or following subsequence of a character
j, the (i, j) or (m, j)-entry of the T-graph corre-
sponding adjacency matrix AT will be assigned a
value of 1. Moreover, for capturing the contex-
t relation between lexical words, if a lexical word
i is the preceding or following context of another
lexical word k, we will assign “ATik = 1”. Note
that the T-graph is the same with the word cutting
graph which is used in Chinese Word Segmenta-
tion.

希 尔 顿 休 斯 顿 机 场离 开

休斯顿机场
Houston airport

休斯顿
Houston

机场
Airport

希尔顿
Hilton

了

离开
leave

希尔
Hill

Figure 4: Word-Character Lattice graph

Word-Character Lattice graph
Zhang and Yang (2018) propose a lattice structure
LSTM to exploit lexical knowledge for Chinese N-
ER. A lattice structure can capture the information

of the nearest contextual lexical words implicitly
and capture some information of self-matched lex-
ical words. As shown in Figure 4, if a character m
is the nearest preceding or following character of
a character j, the (m, j)-entry of the L-graph cor-
responding adjacency matrix AL will be assigned
a value of 1. Moreover, if a character j matches
the lexical word i first character or end character,
we will assign “ALij = 1”.

3.2 Model

A character-based Collaborative Graph Network
includes an encoding layer, a graph layer, a fu-
sion layer, and a decoding layer. The encoding
layer is to capture contextual information of the
sentence and to represent the semantic information
of lexical words. The graph layer is based on GAT
(Veličković et al., 2018) for modeling over three
word-character interactive graphs. A fusion lay-
er is used for fusing different lexical knowledge
captured by these three graphs. Finally, a standard
CRF (Lafferty et al., 2001) model is used for de-
coding labels.

Encoding
The input of the model is a sentence and all lex-
ical words that match consecutive subsequences
of the sentence. We denote the sentence as
s = {c1, c2, ..., cn}, where ci is the i-th char-
acter, and denote the matched lexical words as
l = {l1, l2, ..., lm}. By looking up the embedding
vector from a pre-train character embedding ma-
trix, each character ci is represented as a vector,
which denotes as xi.

xi = ec(ci) (1)

ec is a character embedding lookup table.
To capture contextual information, A bidirec-

tional LSTM (Hochreiter and Schmidhuber, 1997)
is applied to {x1, x2, ..., xn}. By concatenat-
ing the left-to-right and right-to-left LSTM hid-
den states, we obtain the contextual representation
H = {h1,h2, ...,hn}.

~hi =
−−−−→
LSTM(xi, ~hi−1) (2)

~hi =
←−−−−
LSTM(xi, ~hi+1) (3)

hi = ~hi ⊕ ~hi (4)

To represent the semantic information of lexi-
cal words, we look up word embeddings from a



3834

GAT1 GAT2 GAT3

Fusion

希 开 北 京 机 场

希尔
Hill

希尔顿
Mr. Hilton

离开
leave

北京机场
Beijing Airport

机场
Airport

尔 顿 离 了

北京
Beijing

希尔顿
Mr. Hilton

希 开 北 京 机 场

希尔
Hill

希尔顿
Mr. Hilton

离开
leave

北京机场
Beijing Airport

机场
Airport

尔 顿 离 了

北京
Beijing

GAT2 Over 
T-graph

GAT3 Over 
L-graph

GAT1 Over 
C-graph

希 开 北 京 机 场

希尔
Hill

希尔顿
Mr. Hilton

离开
leave

北京机场
Beijing Airport

机场
Airport

尔 顿 离 了

北京
Beijing

LSTM LSTM LSTM

Fusion
Layer

Sentence

Graph
Layer

Decoding
Layer

希 … 了 希尔
Hill

…

Encoding 
Layer

Matched 
Lexicon Words

尔

B-PER I-PER I-PER I-ORGI-ORGI-ORGB-ORGOO O

Sentence:  希尔顿离开北京机场了。
Translation：Mr. Hilton has left Beijing Airport.

… …

… …

Figure 5: Main architecture of a Collaborative Graph Network for integrating lexical knowledge in Chinese NER.
The left side shows the overall architecture, including an encoding layer, a graph layer, a fusion layer, and a
decoding layer. On the right side, we show the details of graph attention networks over three word-character
interactive graphs. We use blue to denote the characters in the sentence and use green to denote the matched
lexicon words.

pre-train word embedding matrix, and each lexi-
cal words li is represented as a semantic vector,
which denotes as wvi.

wvi = ew(li) (5)

ew is a word embedding lookup table. We concate-
nate the contextual representation and the word
embeddings as the output of this layer, denoting
it as Nodef .

Nodef = [h1,h2, ...,hn,wv1,wv2, ...,wvm] (6)

Graph Attention Networks over
Word-Character Interactive Graphs
We use Graph Attention Networks (GAT) to mod-
el over three interactive graphs. In an M-layer
GAT, the input of j-th layer is a set of node fea-
tures, NFj = {f1, f2, ..., fN}, together with an ad-
jacency matrix A , fi ∈ RF , A ∈ RN×N , where
N denotes the number of the nodes and F is the
the dimension of features at j-th layer. The out-
put of j-th layer is a new set of node features,
NF(j+1) = {f′1, f′2, ..., f′N}. A GAT operation with
K independent attention head can be written as :

f′i =
K

‖
k=1

σ( Σ
j∈Ni

αkijW
kfj) (7)

αkij =
exp(LeakyReLU(aT[Wkfi‖WKfj]))

Σk∈Ni exp(LeakyReLU(aT[W
kfi‖WKfk]))

(8)
where ‖ denotes concatenation operation, σ is a
nonlinear activation function, Ni is the neighbor-
hood of node i in the graph, αkij are the attention
coefficients, Wk ∈ RF ′×F , and a ∈ R2F ′ is a
single-layer feed-forward neural network. Note
that, the dimension of the output f′i is KF ′. At
the last layer, averaging will be adopted, and the
dimension of final output features is F ′.

ffinali = σ(
1

K

K
Σ
k=1

Σ
j∈Ni

αkijW
kfj) (9)

To model three totally different word-character
interactive graphs, We build three independen-
t graph attention networks, which are denoted as
GAT1, GAT2, and GAT3. Since three word-
character interactive graphs share the same vertex
set, the input node features of all GAT are matrix
Nodef , which is shown in Equation 6. The output
node features are denoted as G1, G2 and G3,

G1 = GAT1(Nodef , AC) (10)

G2 = GAT2(Nodef , AT ) (11)

G3 = GAT3(Nodef , AL) (12)



3835

Extra Resource Models
Named Entity Named Mention Overall

P(%) R(%) F1(%) P(%) R(%) F1(%) F1(%)
Automatic word seg Peng and Dredze (2015) 74.78 39.81 51.96 71.92 53.03 61.05 56.05

Word Seg Data Peng and Dredze (2016) 66.67 47.22 55.28 74.48 54.55 62.97 58.99
Automatic word seg He and Sun (2017a) 66.93 40.67 50.60 66.46 53.57 59.32 54.82

Other data He and Sun (2017b) 61.68 48.82 54.50 74.13 53.54 62.17 58.32
Word Seg Data Cao et al. (2018) 59.51 50.00 54.43 71.43 47.90 57.53 58.70

Automatic word seg Zhu and Wang (2019) - - 55.38 - - 62.98 59.31
Lexicon Zhang and Yang (2018) - - 53.04 - - 62.25 58.79
Lexicon Ours 67.31 48.61 56.45 75.15 62.63 68.32 63.09

Table 1: Main results on Weibo NER

where Gk ∈ RF
′×(n+m), k ∈ {1, 2, 3}. We keep

the first n columns of these matrices and discard
the last m columns, because only character repre-
sentations are used to decode labels.

Qk = Gk[ : , 0:n], k ∈ {1, 2, 3} (13)

Fusion Layer
A fusion layer is used to fuse different lexical
knowledge captured by word-character interactive
graphs. The input of the fusion layer is the contex-
tual representation H and the output of the graph
layer Qi, i ∈ {1, 2, 3}. The equation of the fusion
layer is introduced below:

R = W1H + W2Q1 + W3Q2 + W4Q3 (14)

where W1, W2, W3 and W4 are trainable matri-
ces. Via a fusion layer, we obtain a matrix R,
R ∈ RF ′×n, which is a new sentence representa-
tion integrating the contextual information as well
as the lexical knowledge of self-matched lexical
words and the nearest contextual lexical words.

Decoding and Training
We use a standard CRF (Lafferty et al., 2001) lay-
er to capture the dependencies between successive
labels. Given a sentence s = {c1, c2, ..., cn}, the
input of the CRF layer is R = {r1, r2, ..., rn}, and
the probability of the ground-truth tag sequence
y = {y1, y2, ..., yn} is

p(y|s) =
exp(

∑
i(W

yiri + T(yi−1,yi)))∑
y′ exp(

∑
i(W

y′iri + T(y′i−1,y′i)))
(15)

Here y′ is an arbitrary label sequence, Wyi is used
for modeling emission potential for the i-th char-
acter in the sentence, and T is the transition ma-
trix storing the score of transferring from one tag
to another. Viterbi algorithm (Viterbi, 1967) is
used to get the label sequence with the highest s-
core. Given a manually annotated training data

Resource Models P(%) R(%) F1(%)
Che et al. (2013) 77.71 72.51 75.02

Wang et al. (2013) 76.43 72.32 74.32
Gold Seg Yang et al. (2016) 65.59 71.84 68.57

Yang et al. (2016) 72.98 80.15 76.40
Zhu and Wang (2019) 75.05 72.29 73.64

Lexicon
Zhang and Yang (2018) 76.35 71.56 73.88

Ours 75.06 74.52 74.79

Table 2: Main results on OntoNotes. Gold seg means
gold-standard segmentation, which is not available in
the real world.

{(si, yi)}|Ni=1, we optimize the model by minimiz-
ing the negative log-likelihood loss with L2 regu-
larization. The loss function is defined as:

L = −
N∑
i=1

log(P (yi|si)) +
λ

2
‖Θ‖2 (16)

where λ denotes the L2 regularization parameter
and Θ is the all trainable parameters set

4 Experiments

In this section, we carry out extensive experiments
to investigate the effectiveness of the Collabora-
tive Graph Network.

4.1 Datasets

We evaluate our model on Weibo NER (Peng and
Dredze, 2015; He and Sun, 2017a), OntoNotes
4 (Weischedel et al., 2011), and MSRA (Levow,
2006), where Weibo NER is in social domain,
OntoNotes and MSRA are in the news domain.
On Weibo NER, we use the same training, devel-
opment and test split as Peng and Dredze (2015).
On OntoNotes, we use the same data split as Che
et al. (2013). Since the MSRA dataset does not
have a development set, we randomly select 10%
samples from the training set as the development
set.



3836

Models P(%) R(%) F1(%)
Chen et al. (2006) 91.22 81.71 86.20
Zhang et al. (2006) 92.20 90.18 91.18
Zhou et al. (2013) 91.86 88.75 90.28
Lu et al. (2016) - - 87.94

Dong et al. (2016) 91.28 90.62 90.95
Cao et al. (2018) 91.73 89.58 90.64

Zhu and Wang (2019) 93.53 92.42 92.97
Zhang and Yang (2018) 93.57 92.79 93.18

Ours 94.01 92.93 93.47

Table 3: Main results on MSRA

4.2 Experimental Settings
In our experiments, We use the same character em-
beddings as Zhang and Yang (2018), which is pre-
trained on Chinese Giga-Word. We use the lex-
icon provided by Li et al. (2018), including 1.3
million Chinese words. We set the dimensionali-
ty of LSTM hidden states to 300 and set the ini-
tial learning rate to 0.001. Since the scale of each
dataset varies, we set different training batch size
for different datasets. Specifically, we set batch
sizes of MSRA, OntoNotes and Weibo NER as
64, 20 and 10. We use stochastic gradient De-
scent (SGD) algorithm to optimize parameters in
OntoNotes and WeiboNER, and use Adam (King-
ma and Ba, 2014) algorithm to optimize parame-
ters in MSRA. We stop the training when we find
the best result in the development set.

4.3 Overall Performance
Weibo NER. Table 1 shows the results on Wei-
bo NER. Zhu and Wang (2019) propose a Con-
volutional Attention Network using segmentation
information, which is the existing state-of-the-art
(SOTA) model. Our model outperforms SOTA
model by 3.78%, 1.07% and 5.34% in F1 score
on Overall, Named Entity, and Nominal Mention.
Zhang and Yang (2018) propose a lattice LSTM
to integrate lexical knowledge. Our model out-
performs the lattice LSTM by 4.3%, 3.41% and
6.07% in F1 score on Overall, Named Entity, and
Nominal Mention.

OntoNotes. Table 2 shows the results on
OntoNotes. Compared with lattice LSTM (Zhang
and Yang, 2018), Our model gains a 0.91% im-
provement in F1 score. Compared with the best re-
sult (Yang et al., 2016), our model doesn’t rely on
gold-standard segmentation, which is not available
in the real world. Note that our model even outper-
forms the model proposed by (Wang et al., 2013;
Yang et al., 2016; Zhu and Wang, 2019), which us-
es the information of gold-standard segmentation.

Dataset Ours(s) Lattice(s) Speedup
MSRA 344 13723 ×15

Training OntoNotes 188 2561 ×13
Weibo NER 64 458 ×7

MSRA 52 344 ×6
Testing OntoNotes 27.1 386 ×14

Weibo NER 2.2 23 ×10

Table 4: The performance of models in training and
testing time. Time is measured in seconds. Lattice
means the lattice LSTM (Zhang and Yang, 2018).

MSRA. Results on the MSRA dataset are
shown in Table 3. By leveraging hand crafted
features (Chen et al., 2006; Zhang et al., 2006;
Zhou et al., 2013) and character embeddings (Lu
et al., 2016), statistical models achieve good re-
sults on MSRA dataset. Dong et al. (2016) inte-
grate LSTM-CRF with radical features and Zhang
and Yang (2018) propose a lattice LSTM to inte-
grate lexical knowledge. Our model outperforms
the lattice LSTM by 0.29% in F1 score on MSRA
datasets.

Speed. As an essential preprocessing NLP tool,
NER tasks require high speeds of both training
and testing. Since aligning word-character lattice
structure for batch training is usually non-trivial,
the lattice LSTM (Zhang and Yang, 2018) suffers
from slow speeds in training and testing. Howev-
er, both LSTM and GAT in our model can compute
efficiently by batch training.

For fair comparison, both the lattice LSTM and
our model are implemented under PyTorch2. By
using a single NVIDIA GeForce GTX 1080 Ti G-
PU, We randomly select 10 training and testing e-
poch as samples. The average time of training and
testing is shown in Table 4. Our model can achieve
a 6-15x speedup over the lattice LSTM.

4.4 Effectiveness of Three Word-Character
Interactive Graphs

We conduct ablation experiments to demonstrate
the effectiveness of these three word-character in-
teractive graphs.

Comparison Setting. We design ablation stud-
ies as follow: 1) w/o C: without word-character
Containing graph(C-graph). 2) w/o T: without
word-character Transition graph (T-graph). 3)
w/o L: without word-character Lattice graph (L-
graph). 4)w/o C & T: without C-graph and T-
graph, only keep L-graph. 5)w/o C & L : without
C-graph and L-graph, only keep T-graph. 6) w/o

2https://pytorch.org/

https://pytorch.org/


3837

Case1

Sentence
&//@5PÑ'f:#é&
&// @Xidian University:#good morning&

Case2

Sentence
~¯TóT�Ñw5���å&
Tencent and Lenovo jointly launched a computer cleaning day&

Matched
lexical word

&5PÑ'f(XidianUniversity)�(Xi�an)�
5PÑ'f(UESTC)�5(An Dian)�5PÑ(Electronics Technology),
é(Good Morning)&

Matched
lexical word

~¯(Tencent),Tó(Lenovo),T�(Joint),
Ñw(Launch),5�(Computer),��(Clean)&

Sentence with
gold label

&// (O)@(O)(B-ORG)(I-ORG)5(I-ORG)P(I-ORG)
Ñ(I-ORG)(I-ORG)'(I-ORG)f(I-ORG):(O)#(O)é(O)(O) &

Sentence with
gold label

~(B-ORG)¯(I-ORG)T(B-ORG)ó(I-ORG)T(O)�(O)
Ñ(O)w(O)5(O)�(O)�(O)�(O)å(O)&

w/o C-graph
predicted label

&// (O)@(O)(B-LOC)(I-LOC)5(B-ORG)P(I-ORG)
Ñ(I-ORG)(I-ORG)'(I-ORG)f(I-ORG):(O)#(O)é(O)(O) &

w/o T-graph
predicted label

~(B-ORG)¯(I-ORG)T(O)ó(O)T(O)�(O)
Ñ(O)w(O)5(O)�(O)�(O)�(O)å(O)&

with C-graph
predicted label

&// (O)@(O)(B-ORG)(I-ORG)5(I-ORG)P(I-ORG)
Ñ(I-ORG)(I-ORG)'(I-ORG)f(I-ORG):(O)#(O)é(O)(O)&

with T-graph
predicted label

~(B-ORG)¯(I-ORG)T(B-ORG)ó(I-ORG)T(O)�(O)
Ñ(O)w(O)5(O)�(O)�(O)�(O)å(O)&

Table 6: Case study. w/o C-graph predicted label means without C-graph predicted label, and w/o T-graph predict-
ed label means without T-graph predicted label. We use green to denote the correct labels and use red to denote
the wrong labels.

Models
Dataset

OntoNotes Weibo NER MSRA
Complete model 74.79 63.09 93.47

w/o C 72.24 60.75 93.35
w/o T 71.57 60.94 93.02
w/o L 72.87 60.69 93.21

w/o C & T 70.53 58.51 92.72
w/o C & L 65.81 58.65 91.98
w/o T & L 71.41 58.72 92.80

BiLSTM+CRF 61.84 52.77 88.05

Table 5: Ablation study on reducing word-character in-
teractive graphs, For example, ”w/o C” means remov-
ing word-character containing graph from the complete
model.

T & L : without T-graph and L-graph, only keep
L-graph. 7) BiLSTM+CRF: baseline model.

Comparison Results. Table 5 shows the re-
sults of ablation experiments. We can clearly see
that removing any graph causes obvious perfor-
mance degradation, but the importance of different
graphs varies from dataset to dataset. Specifically,
on OntoNotes and MSRA, ’w/o T-graph’ obtains
worse performance than others, showing that T-
graph is important. However, T-graph performs
poorly without cooperating with other graphs. We
guess that “T-graph” graph can only capture the
information of the nearest contextual lexical word-
s, and it is not enough to rely solely on T-graph.
On Weibo NER, these graphs show equal impor-
tance. Since dialects slangs and irregular phrases
are very common in social domain, we must rely
on C-graph, T-graph, and L-graph jointly to handle
the informal and complex contexts. In conclusion,
from ablation experiments, we can find that each
graph can be implemented independent of the oth-
er, but together they can achieve the best result,
showing that all these three graphs are essential to
our model.

5 Case Study

To show visually that our model can solve the
challenges when integrating self-matched lexical
words and the nearest contextual lexical words, a
case study comparing without C-graph, without T-
graph and the complete model is shown in Table
6. In the first case, there is an entity “5PÑ
'f”(Xidian University) with nested “”
(Xi’an) and “ 5PÑ'f” (UESTC). These
common entities are all in the lexicon. Without C-
graph, the model can’t integrate the information of
the self-matched lexical word ‘5PÑ'
f” (Xidian University) into the characters “ 5”
and “”. Influenced by another lexical word “5
PÑ'f” (UESTC), the predicted label of the
character “ 5” is “B-ORG”, and the label of the
character “” is predicted to be “I-ORG”, affect
by the lexical word “” (Xi’an). In the second
case, there is an entity “ Tó” (Lenovo), which
can also be a common verb (“Associate”) in Chi-
nese. Without T-graph, the model can’t integrate
the information of the nearest contextual lexical
words “ ~¯” (Tencent) and “T�” (Joint) into
the characters “ T” and “ ó”, so the predicted
labels of the characters “T” and “ ó” are ”O”s.
However, with the help of T-graph, the model can
use the information of the nearest contextual lexi-
cal words “~¯” (Tencent) and “T�” (Joint) to
predict the correct labels.

6 Conclusion

In this paper, we propose a Collaborative Graph
Network for integrating lexical knowledge in Chi-
nese NER. The core of the network is three lexi-
cal word-character interactive graphs. These inter-
active graphs can capture different lexical knowl-
edge and are built without external NLP tools. We
show through various experiments that our model
has complementary strengths to the SOTA model
and these interactive graphs are effective.



3838

Acknowledgments

This work is supported by the National Natu-
ral Science Foundation of China (No.61533018),
the Natural Key R&D Program of China
(No.2017YFB1002101), the National Natural Sci-
ence Foundation of China (No.61806201) and the
independent research project of National Labo-
ratory of Pattern Recognition. This work is al-
so supported by the CCF-Tencent Open Research
Fund. We thank the anonymous reviewers for their
insightful comments. We also thank Xiangrong
Zeng, Pengfei Cao and Yushan Xie for helpful
comments and suggestions.

References
Alan Akbik, Tanja Bergmann, and Roland Vollgraf.

2019. Pooled contextualized embeddings for named
entity recognition. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers), pages 724–728.

Alan Akbik, Duncan Blythe, and Roland Vollgraf.
2018. Contextual string embeddings for sequence
labeling. In COLING 2018: The 27th Internation-
al Conference on Computational Linguistics, pages
1638–1649.

Joost Bastings, Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Simaan. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1957–1967.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Fifth Confer-
ence on Applied Natural Language Processing.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on human
language technology and empirical methods in nat-
ural language processing, pages 724–731.

Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and
Shengping Liu. 2018. Adversarial transfer learn-
ing for Chinese named entity recognition with self-
attention mechanism. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 182–192.

Yu Cao, Meng Fang, and Dacheng Tao. 2019. BAG:
Bi-directional attention entity graph convolutional
network for multi-hop reasoning question answer-
ing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
357–362.

Alberto Cetoli, Stefano Bragaglia, Andrew O’Harney,
and Marc Sloan. 2017. Graph convolutional net-
works for named entity recognition. In Proceedings
of the 16th International Workshop on Treebanks
and Linguistic Theories, pages 37–45.

Wanxiang Che, Mengqiu Wang, Christopher D. Man-
ning, and Ting Liu. 2013. Named entity recogni-
tion with bilingual constraints. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 52–62.

Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun.
2006. Chinese named entity recognition with con-
ditional probabilistic models. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing, pages 173–176.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning for
Chinese word segmentation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1193–1203.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dy-
namic multi-pooling convolutional neural network-
s. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
167–176.

Jason PC Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics, 4:357–370.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of machine learning research,
12(Aug):2493–2537.

Chuanhai Dong, Jiajun Zhang, Chengqing Zong,
Masanori Hattori, and Hui Di. 2016. Character-
based lstm-crf with radical-level features for chi-
nese named entity recognition. In Natural Language
Understanding and Intelligent Applications, pages
239–250.

Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017.
Inductive representation learning on large graphs. In
Advances in Neural Information Processing System-
s, pages 1024–1034.



3839

Hangfeng He and Xu Sun. 2017a. F-score driven max
margin neural network for named entity recognition
in Chinese social media. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers, pages 713–718.

Hangfeng He and Xu Sun. 2017b. A unified model
for cross-domain and semi-supervised named entity
recognition in chinese social media. In Thirty-First
AAAI Conference on Artificial Intelligence.

Jingzhou He and Houfeng Wang. 2008. Chinese
named entity recognition and word segmentation
based on character. In Proceedings of the Sixth
SIGHAN Workshop on Chinese Language Process-
ing.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Hideki Isozaki and Hideto Kazawa. 2002. Efficien-
t support vector classifiers for named entity recog-
nition. In COLING 2002: The 19th International
Conference on Computational Linguistics.

Zhanming Jie, Pengjun Xie, Wei Lu, Ruixue Ding, and
Linlin Li. 2019. Better modeling of incomplete an-
notations for named entity recognition. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 729–734.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Thomas N. Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. In International Conference on Learning
Representations (ICLR).

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270.

Gina-Anne Levow. 2006. The third international chi-
nese language processing bakeoff: Word segmen-
tation and named entity recognition. In Proceed-
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 108–117.

Haibo Li, Masato Hagiwara, Qi Li, and Heng Ji. Ji.
2014. Comparison of the impact of word segmenta-
tion on name tagging for chinese and japanese. In
LREC, page 2532¨C2536.

Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and
Xiaoyong Du. 2018. Analogical reasoning on chi-
nese morphological and semantic relations. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 138–143.

Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Jian
Gui, Huan Peng, and Jiawei. Han. 2018. Empower
sequence labeling with task-aware neural language
model. In Thirty-Second AAAI Conference on Arti-
ficial Intelligence.

Zhangxun Liu, Conghui Zhu, and Tieju Zhao. 2010.
Chinese named entity recognition with a sequence
labeling approach: based on characters, or based on
words? In Advanced intelligent computing theories
and applications. With aspects of artificial intelli-
gence, page 634¨C640.

Yanan Lu, Yue Zhang, and Donghong Ji. 2016. Multi-
prototype Chinese character embedding. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
pages 855–859.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1064–1074.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for se-
mantic role labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1506–1515.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Pushkar Mishra, Marco Del Tredici, Helen Yan-
nakoudakis, and Ekaterina Shutova. 2019. Abusive
Language Detection with Graph Convolutional Net-
works. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
2145–2150.



3840

Diego Mollá, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question an-
swering. In Proceedings of the Australasian Lan-
guage Technology Workshop 2006, pages 51–58,
Sydney, Australia.

Nanyun Peng and Mark Dredze. 2015. Named enti-
ty recognition for Chinese social media with joint-
ly trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 548–554.

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for Chinese social media
with word segmentation representation learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 149–155.

Meng Qu, Yoshua Bengio, and Jian Tang. 2019. Gmn-
n: Graph markov neural networks. In Internation-
al Conference on Machine Learning, pages 5241–
5250.

Petar Veličković, Guillem Cucurull, Arantxa Casano-
va, Adriana Romero, Pietro Liò, and Yoshua Ben-
gio. 2018. Graph Attention Networks. International
Conference on Learning Representations.

Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE transactions on Information Theory,
13(2):260–269.

Mengqiu Wang, Wanxiang Che, and Christopher D
Manning. 2013. Effective bilingual constraints for
semi-supervised learning of named entity recogniz-
ers. In Twenty-Seventh AAAI Conference on Artifi-
cial Intelligence.

Zhichun Wang, Qingsong Lv, Xiaohan Lan, and
Yu Zhang. 2018. Cross-lingual knowledge graph
alignment via graph convolutional networks. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 349–
357.

Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, An-
n Taylor, Craig Greenberg, Eduard Hovy, Robert
Belvin, et al. 2011. Ontonotes release 4.0. LD-
C2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.

Fangzhao Wu, Junxin Liu, Chuhan Wu, Yongfeng
Huang, and Xing Xie. 2019. Neural chinese named
entity recognition via cnn-lstm-crf and joint training
with word segmentation. In The World Wide Web
Conference, WWW ’19, pages 3342–3348.

Yan Xu, Yining Wang, Tianren Liu, Jiahua Liu, Yubo
Fan, Yi Qian, Junichi Tsujii, and Eric I Chang. 2013.
Joint segmentation and named entity recognition us-
ing dual decomposition in chinese discharge sum-
maries. Journal of the American Medical Informat-
ics Association, 21(e1):e84–e92.

Jie Yang, Zhiyang Teng, Meishan Zhang, and Yue
Zhang. 2016. Combining discrete and neural fea-
tures for sequence labeling. In International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics, pages 140–154.

Liang Yao, Chengsheng Mao, and Yuan Luo. 2019.
Graph convolutional networks for text classification.
In Thirty-Third AAAI Conference on Artificial Intel-
ligence.

Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guany-
ing Wang, Xi Chen, Wei Zhang, and Huajun Chen.
2019. Long-tail relation extraction via knowledge
graph embeddings and graph convolution networks.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 3016–
3025.

Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie
Wang. 2006. Word segmentation and named entity
recognition for sighan bakeoff3. In Proceedings of
the Fifth SIGHAN Workshop on Chinese Language
Processing, pages 158–161.

Yue Zhang and Jie Yang. 2018. Chinese NER us-
ing lattice LSTM. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1554–
1564.

Yuhao Zhang, Peng Qi, and Christopher D. Manning.
2018. Graph convolution over pruned dependency
trees improves relation extraction. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2205–2215.

Junsheng Zhou, Weiguang Qu, and Fen Zhang. 2013.
Chinese named entity recognition via joint identifi-
cation and categorization. Chinese journal of elec-
tronics, 22(2):225–230.

Yuying Zhu and Guoxin Wang. 2019. CAN-
NER: Convolutional Attention Network for Chinese
Named Entity Recognition. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers), pages 3384–3393.


