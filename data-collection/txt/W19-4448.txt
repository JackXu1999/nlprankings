



















































Equipping Educational Applications with Domain Knowledge


Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 472–477
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

472

Equipping Educational Applications with Domain Knowledge

Tarek Sakakini* Hongyu Gong* Jong Yoon Lee*
Robert Schloss† Jinjun Xiong† Suma Bhat*
*University of Illinois at Urbana-Champaign, USA

†T. J. Watson Research Center, IBM, USA
*{sakakini, hgong6, jlee642, spbhat2}@illinois.edu

†{rschloss, jinjun}@us.ibm.com

Abstract

One of the challenges of building natural lan-
guage processing (NLP) applications for edu-
cation is finding a large domain-specific cor-
pus for the subject of interest (e.g., history or
science). To address this challenge, we pro-
pose a tool, Dexter, that extracts a subject-
specific corpus from a heterogeneous corpus,
such as Wikipedia, by relying on a small seed
corpus and distributed document representa-
tions. We empirically show the impact of the
generated corpus on language modeling, esti-
mating word embeddings, and consequently,
distractor generation, resulting in a better per-
formance than while using a general domain
corpus, a heuristically constructed domain-
specific corpus, and a corpus generated by a
popular system: BootCaT.

1 Introduction

Educational applications tend to target a specific
subject, in other words, a specific domain, such
as the medical domain in the case of (Jin et al.,
2018). Thus, building these applications with un-
derlying NLP algorithms, would typically require
a large domain-specific corpus. Example uses of
these large corpora are estimating language mod-
els (Rosenfeld, 2000), estimating word embed-
dings (Mikolov et al., 2013), and estimating docu-
ment embeddings (Le and Mikolov, 2014). These
estimations are central to several downstream ap-
plications including automatic speech recognition
(Katz, 1987), machine translation (Koehn et al.,
2003), and text categorization (Tang et al., 2015).

Previous findings, such as (McClosky, 2010),
have shown that training NLP applications on a
domain different from the target domain could
prove detrimental to the performance of these ap-
plications. In order to help educational applica-
tions in specific disciplines such as science and
history create a large, yet domain-specific corpus,

we propose a domain extraction tool, Dexter1, that
extracts a domain-specific corpus from Wikipedia.

The algorithm, elaborated in Section 2, retrieves
a set of documents from Wikipedia that are closest
in discipline to a user-supplied small seed corpus.
The size of this extracted set is a user-defined hy-
perparameter, and thus controls the trade-off be-
tween the specificity of the output corpus and its
size. We empirically determine the favorable con-
figuration of Dexter, demonstrate its benefits to-
wards estimating word embeddings, and conse-
quently distractor generation, as well as language
models. We also show how, on the aformentioned
tasks, Dexter outperforms BootCaT, a popular
toolkit to automatically create an Internet-derived
corpus (Baroni and Bernardini, 2004). Datasets
used in this research are released for public use2.

2 Method

Dexter’s algorithm builds on the assumption that
the distributed representation of two documents
covering similar topics are closer in the vec-
tor space than two documents covering differ-
ent topics. To test this assumption qualitatively,
we embed all Wikipedia articles in R300 using
Doc2Avg3, and then map them to R2 using t-SNE
(Maaten and Hinton, 2008) as shown in Figure 1.
We see that the subset of science Wikipedia arti-
cles form a cluster, thus validating our assump-
tion. Details on how science Wikipedia articles
were identified are provided in Section 3.1. Build-
ing on this observation, Dexter takes a seed set of
articles representing the target domain (e.g., sci-
ence), then sorts Wikipedia articles in increasing
order of distance to seed set, then returns the first
k documents as the extracted in-domain corpus.

1Publicly available at http://bit.ly/dexter-acl
2http://bit.ly/dexter-dataset-acl
3By averaging embeddings of words in document.

http://bit.ly/dexter-acl
http://bit.ly/dexter-dataset-acl


473

Figure 1: Clustering of science (red) Wikipedia articles
among all (blue) Wikipedia articles in 2D.

Multiple design choices need to be considered
carefully and are decided empirically in Section
3.2. These decisions are: The document represen-
tation method, the distance function to the seed
set, and the seed corpus size. For the document
representation, we consider: Doc2Vec (Le and
Mikolov, 2014), Doc2Avg, Doc2wAvg4, TFIDF
(Leskovec et al., 2014), and LSA (Dumais, 2004).
For the distance function, we consider: Mean,
Min, Max, 90th percentile, and 10th percentile
(the last two being robust to outliers). For the seed
corpus size, we experiment with: 10, 102, and 103.

3 Experiments

We perform three main experiments. The first is
an intrinsic evaluation of Dexter, guiding our de-
sign choices. Second, we check the effect of the
domain-specificity of the resulting word embed-
dings on the downstream educational task of dis-
tractor generation for science questions. Third, we
evaluate the effect of Dexter on language model-
ing. Before we delve on the experimental details,
we describe the process of labeling Wikipedia ar-
ticles as science or not, and describe our competi-
tive baseline: BootCaT.

3.1 Experimental Setup

Dataset Preparation: Wikipedia does not clearly
partition its articles into different domains, but in-
stead assigns a set of categories to each article.
Wikipedia also organizes its categories as a di-
rected graph, where, if category b is a subcate-
gory of category a, then there exists an edge (a, b)
(Schönhofen, 2009). Although it might seem nat-
ural to consider all descendants of a category (do-
main in a broad sense) to belong to that category,
upon inspection we found the need for setting a

4The embedding of a document is the average of its
words’ embeddings weighted by a word’s TFIDF score

depth limit. For example, “Stalking” is a third de-
scendant of “Biology”, although “Stalking” is not
considered a “Biology”-related subject. Based on
similar observations, we set the depth limit to two.

To identify science articles, we consider a list of
science root categories, and all their subcategories
up to a depth of 2. All articles labeled with any
category in this list are considered science articles,
amounting to 176,905 articles. This collected sci-
ence corpus will be referred to as CD, while the
general Wikipedia corpus will be referred to as C.

We note that the corpus CD is created using
heuristics on Wikipedia’s taxonomy. In order to
assess the extent to which the articles in CD be-
long to the discipline of science, we do the fol-
lowing. Two annotators assess the quality of CD
by taking a subset of 1000 articles equally spread
across depths: 0, 1, 2, and 3. Each article (depth
anonymized) was given an integer score between
0 and 5 to reflect how much the content of the
article is related to science. The inter-annotator
agreement on the scores had a Pearson’s correla-
tion coefficient of 0.77 suggesting a reasonably
high agreement on the scores. Upon comparing
the average scores of articles at different depths,
we found the score to be inversely proportional to
the depth, with scores 4.35, 3.58, 3.21, and 2.4 for
articles at depth 0, 1, 2, and 3 respectively. This
further justifies our depth limit of 2, below which
the average score suddenly drops below 3.

Also, for the purposes of our experiments, we
split CD (176,905 articles) into three corpora: (1)
Cseed (1,000 articles having a depth of 0), used as
the seed corpus for Dexter and the BootCaT base-
lines, (2) Cho (40,000 articles), which is a held-
out dataset to be used for testing language models,
and (3) Csilver5 (136,905 articles) = CD − Cho,
our training subset of CD to be used in language
modeling experiments. CD reflects the quality
of such a corpus heuristically-constructed using
Wikipedia’s taxonomy.
Baseline: One popular system used by researchers
to extract a domain-specific corpus is BootCaT
(Baroni and Bernardini, 2004), which operates on
the World Wide Web. BootCaT generates queries
to a search engine from user-supplied key phrases
and parses the first n pages retrieved for each
query, where n is set by the user.

Since Dexter requires seed articles instead of

5The term silver is used, rather than gold, since we rely
on heuristics rather than direct human supervision.



474

key phrases, we bridge the gap by a key phrase ex-
traction algorithm (Rose et al., 2010) on the set of
seed articles, by utilizing a publicly available im-
plementation6 with default parameters. We thus
take Cseed and extract the top-100 key phrases
with less than 4 words, then feed them to Boot-
CaT, leading to a domain-specific corpus (referred
to as BootCat-KE). To avoid any possible noise
introduced by the keyword extraction algorithm
we consider another version of the BootCaT base-
line but now with a manual set of 100 key phrases
describing the domain of science. We take a list
of science key phrases available online7, and then
randomly select 100 phrases. The corpus gener-
ated by this algorithm is referred to as BootCat-M.

3.2 Intrinsic Evaluation

Before we analyze Dexter’s performance on
downstream tasks and compare it to BootCaT, we
study the intrinsic performance of Dexter under
several design choices and conditions. Our eval-
uation of Dexter is based on the precision of the
extracted articles averaged over 5 runs. Since we
would be manipulating the seed set size, and to
ensure Dexter’s robustness under randomness, we
artificially construct our seed set by taking a ran-
dom subset of CD instead of using Cseed. That
seed set is then used to algorithmically extract the
rest of CD via Dexter. Accordingly, precision is
calculated as the percentage of articles extracted,
which belong to CD−Cseed. As for recall, it is not
measured since precision is sufficient as a compar-
ison between methods assuming same number of
documents extracted.
Document representation: We vary the docu-
ment representation method while fixing the seed
corpus size at 103, and the distance function as
Mean (c.f. Figure 2a). We observe that LSA and
TFIDF are initially superior, but perform compa-
rably to Doc2Avg and Doc2wAvg as k increases.
LSA is chosen due to its low-dimensionality, and
superiority for modest k values.
Distance function: We vary the distance function
used while fixing the document representation to
LSA and the seed corpus size to 1000 (c.f. Figure
2b). We observe that the 10th percentile distance
function leads to the best precision. We hypothe-
size that this is due to the 10th percentile being ro-
bust to noise, and requiring closeness to only one

6https://github.com/csurfer/rake-nltk
7http://sci2.esa.int/glossary/

subdomain of science rather than all at once.
Seed corpus size: We vary the seed corpus size
while fixing the document representation to LSA
and the distance function as 10th percentile (c.f.
Figure 2c). We observe that a size of 100 was
equally sufficient to 1000. This shows that Dexter
does not require an unfeasibly large seed corpus
size, which would have defeated the purpose.

3.3 Distractor generation

To better assess the impact of the extracted cor-
pus by Dexter, we consider the task of distractor
generation for multiple-choice questions (MCQs)
(Stasaski and Hearst, 2017). Educators spend sig-
nificant amount of time choosing suitable distrac-
tors for MCQs, where the distractors are the incor-
rect choices in an MCQ. Moreover, distractor gen-
eration is an essential task for automatic question
generation. The choice of distractors is critical to
the learning outcomes of students, since a mis-
informed selection of easy distractors could ren-
der questions non-challenging (Araki et al., 2016).
The main aspect of the distractors’ quality is their
semantic similarity to the correct answer. The dis-
tractor cannot be a synonym of the answer and
if too distant, it can be easily eliminated by the
learner.

To automate this process, one considered
methodology is relying on word embeddings to
capture the semantics of the answer, and retrieve
the distractors closest semantically to the answer
(Araki et al., 2016). An essential component to
the quality of these word embeddings is the do-
main of the training corpus, as any shift in domain
would lead to a decrease in performance as noted
in (Bollegala et al., 2015). To illustrate this fur-
ther, we take the corpora C (∼2.3B words) and
CD (∼150M words), as described in Section 3.1,
as well as the science corpus extracted by Dexter
at size 150K (∼187M words) along with the cor-
pora BootCaT-KE (∼986K words), and BootCaT-
M (∼1.2M words). Also, to eliminate any ef-
fects of corpus size between Dexter and the Boot-
CaT baselines, we downsample Dexter’s corpus to
the size of the BootCaT corpora ( 986K words).
We then train six sets of word embeddings us-
ing FastText (Bojanowski et al., 2017), on each
of the six aforementioned corpora. The quality of
these word embeddings at capturing the semantics
of science words is then measured on the task of
distractor generation. Taking all questions from

https://github.com/csurfer/rake-nltk
http://sci2.esa.int/glossary/


475

(a) Effect of Document Representation (b) Effect of Distance Function (c) Effect of Seed Corpus Size

Figure 2: Effect of design choices on accuracy of extracted domain

Corpus Recall Perplexity
C 17.43% 431.78
Csilver N/A 334.57
CD 20.47% N/A
BootCaT-KE 15.28% 3199.30
BootCaT-M 13.82% 4586.80
Dexter-Downsampled 18.86% 1117.34
Dexter 22.71% 294.20

Table 1: Distractor recall@100 for word embeddings
(middle) and perplexity of language models (right)
trained on corpora of varying domain specificity.

three science questions datasets – 7,787 questions
from ARC (Clark et al., 2018), 13,679 questions
from SciQ (Johannes Welbl and Gardner, 2017),
and 5059 questions from AI2-ScienceQuestions
(Allen Institute for AI, 2017) – we check the re-
call of the distractors8 in the top 100 most similar
words to the answer of each question. Results are
reported in Table 1.

As hypothesized, we notice that science-
specific word embeddings trained on CD
(20.47%) perform better than when trained on
all of Wikipedia (17.43%). But it was surprising
to observe that training on the science corpus
generated by Dexter led to an even better per-
formance (22.71%) than CD (20.47%). Since
CD was heuristically constructed from human
categorization, it might be the case that Dexter
was able to capture the language characteristics
of the science seed corpus better than heuristic
methods operating on Wikipedia’s taxonomy. The
same two annotators mentioned in 3.1 manually
labeled the extracted corpus (top 500 articles)
using the same scale. The scores of the two
annotators had a Perason’s correlation coefficient
of 0.66. Indeed the quality of the extracted

8 We checked the recall of only one-word distractors since
there is no straightforward method to retrieve phrases, as dis-
tractors, using distance over word embeddings. This does not
affect our comparison study of different word embeddings.

corpus turned out higher than that of CD, with an
average score of 4.712. A less surprising result
was Dexter’s outperformance (22.71%) of both
BootCaT baselines (15.28% and 13.82%) even
when Dexter was downsampled (18.86%). This is
expected as the automatic scraping of webpages
by BootCaT introduces noisy artifacts into the
corpus.

To qualitatively understand why word embed-
dings trained on domain-specific corpora outper-
form general ones we take a look at examples of
polysemous words, and their word embeddings
when trained on different corpora (c.f. Table
2). For example, the closest neighbors to the
word “Force” when trained on C are: “Forces”,
“Troops”, and “Army”, which reflect the mili-
tary sense to the word “Force”. When trained on
CD, the closest neighbors become: “Deflection”,
“Torque”, and “Gravity”, reflecting the scientific
sense of the word “Force”. Similarly, the closest
neighbors to the word “Field”, when trained on
C, are: “Fields”, “Football”, and “Professional-
sized”, reflecting a sports field sense. Whereas,
when trained on the extracted corpus by Dexter,
neighbors of “Field” become: “Fields”, “Magne-
tobiology”, and “Ambipolar”, reflecting a scien-
tific sense of the word “Field”.

3.4 Language modeling
Similar to the previous experiment, except for us-
ing Csilver instead of CD, we train 6 different tri-
gram language models (Brown et al., 1992) on
each corpus using kenlm (Heafield et al., 2013)
and under default parameters9. We then test the
perplexity of these language models on Cho. The
same pattern of comparative performances (c.f.
Table 1) is noticed in language modeling, which
reflects the impact of the quality of the domain-
specific corpus on the variety of resources (lan-
guage models and word embeddings) trained on

9https://kheafield.com/code/kenlm/



476

Word Neighbors (General) Neighbors (Science)

Force Forces Troops Army Deflection Torque Gravity
Digest Review Guide Supplement Digested Extract Metabolize
Matter Matters Subject Debate Particles Materials Universe

Field Fields Football Professional-sized Fields Magnetobiology Ambipolar
Rock Punk Pop Indie Rocks Shoegazing Screamo

Cellular Cell Signalling Apoptosis Cell Organelle Automata

Table 2: Neighbors of polysemous scientific words when trained on the general Wikipedia (left), trained on CD
(top right), and trained on the extracted science corpus by Dexter bottom right).

Figure 3: Perplexity scores for language models trained
on General, Silver, and Dexter corpora across different
domains.

it. We thus conclude that educational NLP appli-
cations in science can benefit from Dexter if their
algorithm relies on a monolingual corpus. An ex-
ample of an educational NLP application utilizing
language models would be a machine translator
for science webpages.

4 Generalization to other domains

To ensure Dexter’s applicability to other educa-
tional domains, we repeated the experiment in
Section 3.4 for educational domains other than sci-
ence and show the results in Figure 3. We notice
that the extracted corpus by Dexter is capable of
training a significantly better language model than
that trained on either the respective silver corpus
or all of Wikipedia. The ineffectiveness of the gen-
eral corpus is expected as the extracted corpus of-
fers a more domain-specific training data. As for
the more surprising outcome of inefficacy of the
respective silver corpora, the reason seems to be
the size of the silver corpus for most of these do-
mains owing to the lack of articles in Wikipedia.
The extracted corpus does not suffer from this lim-
itation as its size is a hyperparameter set by the

user, 100K in this case. With fewer than 100K in-
domain articles, Dexter continues extracting arti-
cles that are close to but not in the domain leading
to an extrapolated language model combining the
benefits of the specificity of the small in-domain
corpus and the generality of the large corpus.

5 Conclusion

Relying on off-the-shelf resources reduces the
quality of educational NLP applications. To ad-
dress this challenge, we offer to the commu-
nity an aiding tool, Dexter, to extract a domain-
specific corpus from Wikipedia. We show that
our simple method outperforms in-domain corpora
constructed heuristically using Wikipedia’s taxon-
omy, or those constructed using popular systems
scraping the World Wide Web.

Acknowledgment

This work is supported by IBM-ILLINOIS Cen-
ter for Cognitive Computing Systems Research
(C3SR) - a research collaboration as part of the
IBM AI Horizons Network.

References
Allen Institute for AI. 2017. AI2 Science Questions

v2.1. Accessed: 2019-04-16.

Jun Araki, Dheeraj Rajagopal, Sreecharan Sankara-
narayanan, Susan Holm, Yukari Yamakawa, and
Teruko Mitamura. 2016. Generating questions and
multiple-choice answers using semantic analysis of
texts. In Proceedings of COLING 2016, the 26th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 1125–1136, Osaka,
Japan. The COLING 2016 Organizing Committee.

Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
LREC, page 1313.

http://data.allenai.org/ai2-science-questions/
http://data.allenai.org/ai2-science-questions/
https://www.aclweb.org/anthology/C16-1107
https://www.aclweb.org/anthology/C16-1107
https://www.aclweb.org/anthology/C16-1107


477

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Danushka Bollegala, Takanori Maehara, and Ken-ichi
Kawarabayashi. 2015. Unsupervised cross-domain
word representation learning. arXiv preprint
arXiv:1505.07184.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457.

Susan T Dumais. 2004. Latent semantic analysis. An-
nual review of information science and technology,
38(1):188–230.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria.

Lifeng Jin, David King, Amad Hussein, Michael
White, and Douglas Danforth. 2018. Using para-
phrasing and memory-augmented models to combat
data sparsity in question interpretation with a vir-
tual patient dialogue system. In Proceedings of the
Thirteenth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 13–23.

Nelson F. Liu Johannes Welbl and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
In Workshop on Noisy User-generated Text.

Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE transactions on acoustics,
speech, and signal processing, 35(3):400–401.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1188–1196.

Jure Leskovec, Anand Rajaraman, and Jeffrey David
Ullman. 2014. Mining of massive datasets. Cam-
bridge university press.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(Nov):2579–2605.

David McClosky. 2010. Any domain parsing: auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Stuart Rose, Dave Engel, Nick Cramer, and Wendy
Cowley. 2010. Automatic keyword extraction from
individual documents. Text Mining: Applications
and Theory, pages 1–20.

Ronald Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here?
Proceedings of the IEEE, 88(8):1270–1278.

Peter Schönhofen. 2009. Identifying document topics
using the wikipedia category network. Web Intelli-
gence and Agent Systems: An International Journal,
7(2):195–207.

Katherine Stasaski and Marti A. Hearst. 2017. Multi-
ple choice question generation utilizing an ontology.
In Proceedings of the 12th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 303–312, Copenhagen, Denmark. Association
for Computational Linguistics.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classification. In EMNLP, pages 1422–
1432.

https://doi.org/10.18653/v1/W17-5034
https://doi.org/10.18653/v1/W17-5034

