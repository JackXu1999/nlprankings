










































Building Specialized Bilingual Lexicons Using Word Sense Disambiguation


International Joint Conference on Natural Language Processing, pages 952–956,
Nagoya, Japan, 14-18 October 2013.

Building Specialized Bilingual Lexicons Using Word Sense
Disambiguation

Dhouha Bouamor
CEA, LIST, Vision and

Content Engineering Laboratory,
91191 Gif-sur-Yvette CEDEX

France
dhouha.bouamor@cea.fr

Nasredine Semmar
CEA, LIST, Vision and Content

Engineering Laboratory,
91191 Gif-sur-Yvette

CEDEX France
nasredine.semmar@cea.fr

Pierre Zweigenbaum
LIMSI-CNRS,

F-91403 Orsay CEDEX
France

pz@limsi.fr

Abstract
This paper presents an extension of the
standard approach used for bilingual lex-
icon extraction from comparable corpora.
We study the ambiguity problem revealed
by the seed bilingual dictionary used to
translate context vectors and augment the
standard approach by a Word Sense Dis-
ambiguation process. Our aim is to iden-
tify the translations of words that are more
likely to give the best representation of
words in the target language. On two spe-
cialized French-English and Romanian-
English comparable corpora, empirical ex-
perimental results show that the proposed
method consistently outperforms the stan-
dard approach.

1 Introduction

Over the years, bilingual lexicon extraction from
comparable corpora has attracted a wealth of re-
search works (Fung, 1998; Rapp, 1995; Chiao and
Zweigenbaum, 2003). The main work in this re-
search area could be seen as an extension of Har-
ris’s distributional hypothesis (Harris, 1954). It is
based on the simple observation that a word and
its translation are likely to appear in similar con-
texts across languages (Rapp, 1995). Based on
this assumption, the alignment method, known as
the standard approach builds and compares con-
text vectors for each word of the source and target
languages.

A particularity of this approach is that, to enable
the comparison of context vectors, it requires the
existence of a seed bilingual dictionary to translate
source context vectors. The use of the bilingual
dictionary is problematic when a word has sev-
eral translations, whether they are synonymous or

polysemous. For instance, the French word action
can be translated into English as share, stock, law-
suit or deed. In such cases, it is difficult to iden-
tify in flat resources like bilingual dictionaries,
wherein entries are usually unweighted and un-
ordered, which translations are most relevant. The
standard approach considers all available trans-
lations and gives them the same importance in
the resulting translated context vectors indepen-
dently of the domain of interest and word ambigu-
ity. Thus, in the financial domain, translating ac-
tion into deed or lawsuit would probably introduce
noise in context vectors.

In this paper, we present a novel approach
which addresses the word ambiguity problem ne-
glected in the standard approach. We introduce a
use of a WordNet-based semantic similarity mea-
sure permitting the disambiguation of translated
context vectors. The basic intuition behind this
method is that instead of taking all translations
of each seed word to translate a context vector,
we only use the translations that are more likely
to give the best representation of the context vec-
tor in the target language. We test the method
on two comparable corpora specialized on the
Breast Cancer domain, for the French-English and
Romanian-English pair of languages. This choice
allows us to study the behavior of the disambigua-
tion for a pair of languages that are richly repre-
sented and for a pair that includes Romanian, a
language that has fewer associated resources than
French and English.

2 Related Work

Recent improvements of the standard approach are
based on the assumption that the more the con-
text vectors are representative, the better the bilin-
gual lexicon extraction is. Prochasson et al. (2009)

952



used transliterated words and scientific compound
words as ‘anchor points’. Giving these words
higher priority when comparing target vectors im-
proved bilingual lexicon extraction. In addition to
transliteration, Rubino and Linarès (2011) com-
bined the contextual representation within a the-
matic one. The basic intuition of their work is that
a term and its translation share thematic similari-
ties. Hazem and Morin (2012) recently proposed a
method that filters the entries of the bilingual dic-
tionary based upon POS-tagging and domain rel-
evance criteria, but no improvements was demon-
strated.

Gaussier et al. (2004) attempted to solve the
problem of different word ambiguities in the
source and target languages. They investigated a
number of techniques including canonical corre-
lation analysis and multilingual probabilistic la-
tent semantic analysis. The best results, with a
very small improvement were reported for a mixed
method. One important difference with Gaussier
et al. (2004) is that they focus on words ambigu-
ities on source and target languages, whereas we
consider that it is sufficient to disambiguate only
translated source context vectors.

3 Context Vector Disambiguation

The approach we propose augments the standard
approach used for bilingual lexicons mining from
comparable corpora. As it was mentioned in sec-
tion 1, when the lexical extraction applies to a spe-
cific domain, not all translations in the bilingual
dictionary are relevant for the target context vec-
tor representation. For this reason, we introduce
a WordNet-based WSD process that aims at im-
proving the adequacy of context vectors and there-
fore improve the results of the standard approach.

A large number of WSD techniques were pre-
viously proposed in the literature. The most popu-
lar ones are those that compute semantic similarity
with the help of existing thesauri such as Word-
Net (Fellbaum, 1998). This thesaurus has been
applied to many tasks relying on word-based sim-
ilarity, including document (Hwang et al., 2011)
and image (Cho et al., 2007; Choi et al., 2012)
retrieval systems. In this work, we use this re-
source to derive a semantic similarity between lex-
ical units within the same context vector. To the
best of our knowledge, this is the first application
of WordNet to the task of bilingual lexicon extrac-
tion from comparable corpora.

Once translated into the target language, the
context vectors disambiguation process inter-
venes. This process operates locally on each con-
text vector and aims at finding the most promi-
nent translations of polysemous words. For this
purpose, we use monosemic words as a seed set
of disambiguated words to infer the polysemous
word’s translations senses. We hypothesize that a
word is monosemic if it is associated to only one
entry in the bilingual dictionary. We checked this
assumption by probing monosemic entries of the
bilingual dictionary against WordNet and found
that 95% of the entries are monosemic in both re-
sources.

Formally, we derive a semantic similarity value
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and
all monosemic words appearing whithin the same
context vector. There is a relatively large number
of word-to-word similarity metrics that were pre-
viously proposed in the literature, ranging from
path-length measures computed on semantic net-
works, to metrics based on models of distribu-
tional similarity learned from large text collec-
tions. For simplicity, we use in this work, the Wu
and Palmer (1994) (WUP) path-length-based se-
mantic similarity measure. It was demonstrated by
(Lin, 1998) that this metric achieves good perfor-
mances among other measures. WUP computes a
score (equation 1) denoting how similar two word
senses are, based on the depth of the two synsets
(s1 and s2) in the WordNet taxonomy and that of
their Least Common Subsumer (LCS), i.e., the
most specific word that they share as an ancestor.

WupSim(s1, s2) =
2× depth(LCS)

depth(s1) + depth(s2)
(1)

In practice, since a word can belong to more
than one synset in WordNet, we determine the
semantic similarity between two words w1 and
w2 as the maximum WupSim between the synset
or the synsets that include the synsets(w1) and
synsets(w2) according to the following equation:

SemSim(w1, w2) = max{WupSim(s1, s2);
(s1, s2) ∈ synsets(w1)× synsets(w2)} (2)

Then, to identify the most prominent translations
of each polysemous unit wp, an average similarity
is computed for each translation wjp of wp:

Ave Sim(wjp) =

∑N
i=1 SemSim(wi, w

j
p)

N
(3)

953



Corpus French English
396, 524 524, 805

Corpus Romanian English
22,539 322,507

Table 1: Comparable corpora sizes in term of
words.

where N is the total number of monosemic words
and SemSim is the similarity value of w

j
p and the

ith monosemic word. Hence, according to average
relatedness values Ave Sim(wjp), we obtain for
each polysemous word wp an ordered list of trans-
lations w1p . . . w

n
p . This allows us to select trans-

lations of words which are more salient than the
others to represent the word to be translated.

4 Experiments and Results

4.1 Resources

4.1.1 Comparable corpora
We conducted our experiments on two French-
English and Romanian-English comparable
corpora specialized on the breast cancer
domain. Both corpora were extracted from
Wikipedia1. We consider the topic in the source
language (for instance cancer du sein [breast
cancer]) as a query to Wikipedia and extract all
its sub-topics (i.e., sub-categories in Wikipedia)
to construct a domain-specific category tree.
Then, based on the constructed tree, we collect
all Wikipedia pages belonging to one of these
categories and use inter-language links to build
the comparable corpus. Both corpora were
normalized through the following linguistic
preprocessing steps: tokenisation, part-of-speech
tagging, lemmatisation, and function word re-
moval. The resulting corpora2 sizes are given in
Table 1.

4.1.2 Bilingual dictionary
The French-English bilingual dictionary used to
translate context vectors consists of an in-house
manually revised bilingual dictionary which con-
tains about 120,000 entries belonging to the gen-
eral domain. It is important to note that words
has on average 7 translations in the bilingual dic-
tionary. The Romanian-English dictionary con-
sists of translation pairs extracted from Wikipedia.

1http://dumps.wikimedia.org/
2Comparable corpora will be shared publicly

The resulting bilingual dictionary contains about
136,681 entries for Romanian-English with an av-
erage of 1 translation per word.

4.1.3 Evaluation list
In bilingual terminology extraction from compa-
rable corpora, a reference list is required to eval-
uate the performance of the alignment. Such
lists are usually composed of about 100 sin-
gle terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created a refer-
ence list3 for each pair of language. The French-
English list contains 96 terms extracted from the
French-English MESH and the UMLS thesauri4.
The Romanian-English reference list was created
by a native speaker and contains 38 pair of words.
Note that reference terms pairs appear at least five
times in each part of both comparable corpora.

4.2 Experimental setup
Three other parameters need to be set up: (1) the
window size, (2) the association measure and the
(3) similarity measure. To define context vectors,
we use a seven-word window as it approximates
syntactic dependencies. Concerning the rest of the
parameters, we followed Laroche and Langlais
(2010) for their definition. The authors carried out
a complete study of the influence of these param-
eters on the bilingual alignment and showed that
the most effective configuration is to combine the
Discounted Log-Odds ratio (equation 4) with the
cosine similarity. The Discounted Log-Odds ratio
is defined as follows:

Odds-Ratiodisc = log
(O11 +

1
2)(O22 +

1
2)

(O12 +
1
2)(O21 +

1
2)

(4)

where Oij are the cells of the 2 × 2 contingency
matrix of a token s co-occurring with the term S
within a given window size.

4.3 Results and discussion
It is difficult to compare results between different
studies published on bilingual lexicon extraction
from comparable corpora, because of difference
between (1) used corpora (in particular their con-
struction constraints and volume), (2) target do-
mains, and also (3) the coverage and relevance of
linguistic resources used for translation. To the
best of our knowledge, there is no common bench-
mark that can serve as a reference. For this reason,

3Reference lists will be shared publicly
4http://www.nlm.nih.gov/

954



b)
FR

-E
N

Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach(SA) 0.49

Si
ng

le
m

ea
su

re
s WUP 0.48 0.56 0.56 0.54 0.55 0.54 0.55

PATH 0.54 0.54 0.55 0.56 0.57 0.55 0.55
LEACOCK 0.50 0.57 0.55 0.56 0.54 0.55 0.54

LESK 0.46 0.54 0.54 0.59 0.55 0.55 0.54
VECTOR 0.51 0.56 0.53 0.56 0.54 0.56 0.55

b)
R

O
-E

N

Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach(SA) 0.21

Si
ng

le
m

ea
su

re
s WUP 0.18 0.21 0.21 0.21 0.21 0.21 0.21

PATH 0.18 0.21 0.21 0.21 0.21 0.21 0.21
LEACOCK 0.15 0.18 0.18 0.18 0.18 0.18 0.18

LESK 0.21 0.21 0.21 0.21 0.21 0.21 0.21
VECTOR 0.18 0.21 0.21 0.21 0.21 0.21 0.21

Table 2: F-Measure at Top20 for the Breast Cancer domain for the two pairs of languages; In each
column, italics shows best single similarity measure, bold shows best result. Underline shows best result
overall.

we use the results of the standard approach (SA)
as a reference. We evaluate the performance of
both the SA and ours with respect to Top20 F-
Measure which computes the harmonic mean be-
tween precision and recall.

Our method provides a ranked list of transla-
tions for each polysemous word. A question that
arises here is whether we should introduce only
the best ranked translation in the context vector
or consider a larger number of words, especially
when a translations list contain synonyms. For this
reason, we take into account in our experiments
different number of translations, noted WN-Ti,
ranging from the pivot translation (i = 1) to
the seventh word in the translations list. This
choice is motivated by the fact that words in the
French-English corpus have on average 7 trans-
lations in the bilingual dictionary. The baseline
(SA) uses all translations associated to each en-
try in the bilingual dictionary. Table 2a displays
the results obtained for the French-English com-
parable corpus. The first substantial observation
is that our method which consists in disambiguat-
ing polysemous words within context vectors con-
sistently outperforms the standard approach. The
maximum F-measure was obtained by LESK when
for each polysemous word up to four translations
(WN-T4) are considered in context vectors. This
method achieves an improvement of +10% and
over the standard approach.

Concerning the Romanian-English pair of lan-

guage, no improvements have been reported. The
reason being that words in the bilingual dictio-
nary are not heavily polysemous. Each word used
to shape context vectors is associated to only one
translation in the bilingual dictionary.

5 Conclusion

We presented in this paper a novel method that
extends the standard approach used for bilin-
gual lexicon extraction from comparable corpora.
The proposed method disambiguates polysemous
words in context vectors and selects only the trans-
lations that are most relevant to the general con-
text of the corpus. Conducted experiments on a
highly polysemous specialized comparable corpus
show that integrating such process leads to a bet-
ter performance than the standard approach. Al-
though our initial experiments are positive, we be-
lieve that they could be improved in a number of
ways. It would also be interesting to mine much
more larger comparable corpora and focus on their
quality as presented in (Li and Gaussier, 2010).
We want also to test our method on bilingual lexi-
con extraction for a larger panel of specialized cor-
pora, where disambiguation methods are needed
to prune translations that are irrelevant to the do-
main.

References
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.

Looking for candidate translational equivalents in

955



specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ’02, pages 1–5. As-
sociation for Computational Linguistics.

Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397–402, Amsterdam.

Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426–433.
Springer.

Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED’12, pages 83–87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.

Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1–17. Springer.

Éric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Hervé Déjean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526–533.

Z.S. Harris. 1954. Distributional structure. Word.

Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.

Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation
network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845–858.

Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617–625, Beijing,
China, Aug.

Bo Li and Ëric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.

Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing, ICML ’98, pages 296–304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284–291, Ot-
tawa, Ontario, Canada.

Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ’95, pages 320–322. Association for
Computational Linguistics.

Raphaël Rubino and Georges Linarès. 2011. A multi-
view approach for term translation spotting. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
29–40.

Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ’94, pages 133–138. Association
for Computational Linguistics.

956


