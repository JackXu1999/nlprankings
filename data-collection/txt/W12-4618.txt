



















































Idioms and extended transducers


Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 153–161,
Paris, September 2012.

Idioms and extended transducers
Gregory M. Kobele

University of Chicago
kobele@uchicago.edu

Abstract

There is a tension between the idea that id-
ioms can be both listed in the lexicon, and
the idea that they are themselves composed
of the lexical items which seem to inhabit
them in the standard way. In other words, in
order to maintain the insight that idioms ac-
tually contain the words they look like they
contain, we need to derive them syntacti-
cally from these words. However, the en-
tity that should be assigned a special mean-
ing is then a derivation, which is not the
kind of object that can occur in a lexicon
(which is, by definition, the atoms of which
derivations are built), and thus not the kind
of thing that we are able to assign meanings
directly to. Here I will show how to resolve
this tension in an elegant way, one which
bears striking similarities to those proposed
by psychologists and psycholinguists work-
ing on idioms.

1 Introduction

One standard conception of the lexicon is that it is
a set of form-meaning pairs, usually just the bare
minimum needed to derive in a systematic way
all the other form-meaning pairings constitutive
of the language (Di Sciullo and Williams, 1987).
Under this conception of the lexicon, an idiom, as
its meaning is by definition not predictable given
its form, must be a lexical item. Given that id-
ioms are able to occur in syntactic environments
where their parts are separated by arbitrarily large
amounts of material, as in 1, it then follows, under
this conception of the lexicon, that lexical items
must (be able to) have complex internal structure,
of the sort that is amenable to syntactic manipula-
tion.

(1) The cat seems to have been let out of the
bag.

This treatment of idioms is unsatisfying in the
following way. It is natural to think that the
word ‘cat’ occurs in the idiom ‘let the cat out of
the bag’, and not just a synchronically unrelated
string of phonemes /c/, /a/, /t/. However, as id-
ioms are simply entered en masse into the lexicon,

and are therefore not derived objects, they cannot
be said to contain the words they look like they
contain. In other words, the word ‘cat’ occurs in
‘let the cat out of the bag’ under this conception
of the lexicon to the very same degree it occurs in
‘catastrophe’ (i.e. not at all).

In order for the word ‘cat’ to actually occur
in the idiomatic expression ‘let the cat out of the
bag’, the idiomatic expression must have a deriva-
tion that uses the word ‘cat’.1 But then it would
seem that we must assign a non-predictable mean-
ing to a non-lexical item.

There are thus two (mutually incompatible)
roles that a lexicon plays. It is on one hand the set
of syntactic building blocks, and on the other the
repository of form-meaning pairings. In this pa-
per I will show how the transductive (‘two-step’)
approach to grammar (Morawietz, 2003) disen-
tangles these two notions. The transductive ap-
proach to grammar defines the expressions gener-
ated in two steps: first, in terms of a set of deriva-
tion trees, and second, in terms of operations turn-
ing these derivation trees into the objects (sounds,
and meanings) that they are the derivations of. We
propose that the mapping between derivation tree
and derived object be not a simple transduction,
but rather an extended one in the sense of Graehl
et al. (2008). These transducers implement ex-
actly the kind of ‘bounded subderivation’ transla-
tion suggested by Shieber (1994). This ennables
us to dissolve the tension between listing idioms
on the one hand, and allowing them to be derived,
on the other – they are derivationally complex, but
interpretatively atomic.

Compare this approach with the general picture
painted by Chomsky (1995):

A language, in turn, determines an infi-
nite set of linguistic expressions (SDs),

1This is true by definition: an expression A contains all
and only those expressions that its immediate constituents
contain, along with its immediate constituents. (Note that
this implicitly involves identifying an expression with its
derivation; we don’t say of ambiguous expressions that they
contain all the expressions that occur in any of their deriva-
tions, but rather that under one reading, the expression con-
tains one set of expressions, and under another, another.)

153



each a pair 〈π, λ〉 drawn from the inter-
face levels (PF,LF), respectively.

According to the two-step approach, each lan-
guage determines an infinite set of linguistic ex-
pressions d, which are then mapped to inter-
face interpretable elements π and λ by opera-
tions Π and Λ, respectively. Λ will be viewed
as the repository of atomic syntax-meaning pair-
ings, and Π as the repository of atomic syntax-
form pairings, while these may make reference
to syntactic atoms, they are defined over entire
derivation trees, and thus can make reference to
larger chunks than single nodes.

The remainder of this paper is structured as fol-
lows. In section 2 we present some formal prelim-
inaries. Our proposal is presented in more detail
in §3. Section 5 explores linguistic aspects of id-
ioms from this perspective. Section 6 is the con-
clusion.

2 Background

Lambda terms provide a simple and uniform pre-
sentation of structured objects (trees, strings) and
of mappings between them. Given a denumerably
infinite set X of variables, and a set C of con-
stants, the set of lambda terms over C is defined
by the following grammar:

M ::= X |C |(MM) |λX.M

The usual notions of β, η, and α reduction apply
(Barendregt, 1984), and we do not distinguish be-
tween terms which are β, η, or α equivalent.

We are interested in lambda terms which can
be (simply) typed. Given a finite set A (of atomic
types), we define the set T (A) of types as the clo-
sure of A under pairing. When writing elements
of T (A), we treat pairing as associating to the
right; (a, (b, c)) will be written simply as abc. We
write t0 := t and tn+1 := ttn. The order of an
atomic type is 1, and of a function type ab is the
greater of ord(a) + 1 and ord(b). A linear Higher
Order Signature (HOS) is a triple Σ = 〈C,A, τ〉
where C and A are finite sets (of constants and
atomic types, respectively), and τ : C → T (A) is
a function assigning types over A to each c ∈ C.
We adopt the notational convention that a HOS
Σi consists of Ci, Ai, and τi. A HOS is of order n
just in case the highest order type assigned to any
constant is n.

x :α`Σ x :α
Γ`ΣM :αβ ∆`ΣN :α

Γ,∆`Σ(MN) :β

Γ,x :α`ΣM :β
Γ`Σ λx.M :αβ `Σ c : τ(c)

Figure 1: Deriving typing judgments

A typing context Γ is a finite map from vari-
ables to types. We write x :α to indicate the typ-
ing context defined only on x, mapping x to α.
Given typing contexts Γ,∆, their union Γ,∆ is
defined iff they have disjoint domains (i.e. no vari-
able is assigned a type in both Γ and ∆). A typing
judgment Γ`ΣM :α indicates that M has type α
in context Γ, with respect to HOS Σ. A typing
judgment is derivable just in case it is licensed
by the inference system in figure 1.2 The lan-
guage Λα(Σ) at type α of HOS Σ is defined to be
the set of lambda terms which have the type α in
the empty context (Λα(Σ) := {M : `ΣM :α}).
We write Λ(Σ) to denote the set of all well-typed
lambda terms over Σ.

A tree is a first order term (i.e. a term of atomic
type) over a second order HOS (which we will
call a tree signature). Here a constant corresponds
to a node of a tree, and its arguments (which are
themselves trees) to its daughters. A tree con-
text is a second order term over a second order
HOS. A set of trees is regular iff it is the lan-
guage Λα(Σ) at some atomic type α of some tree
signature Σ. The atomic types in this case corre-
spond to states of a tree automaton, and a constant
c of type a1a2 · · · ana corresponds to a production
c(a1, . . . , an) → a. A string is a second order
term over a second order HOS all of whose con-
stants have type αβ for some atomic types α, β
(which we will call a string signature). As an
example, λx.x represents the empty string, and
λx.a(b(x)) the string “ab”. As before, the types
correspond to states of an NFA, and the function
type αβ assigned to a constant c to a transition
fromα to β reading c (without � transitions). A set
of strings is regular iff it is the language Λαβ(Σ)
of some string signature Σ, for α, β atomic types

2Because the rules for variables and constants require
particular typing contexts, lambda bindings are necessar-
ily non-vacuous. Furthermore, because contexts are finite
maps, and context union is only defined over disjoint con-
texts, lambda bindings are necessarily linear.

154



Σ1

Σ2

L

Figure 2: ACGs, graphically

(here α is the start state of the NFA, and β the
(unique) final state).

A linear homomorphism L from HOS Σ1 to
HOS Σ2 is a pair of maps F : A1 → T (A2)
and G : C1 → Λ(Σ2) such that for any c ∈
C1, `Σ2 G(c) : F̂ (τ1(c)), where F̂ is F extended
pointwise over T (A1); this simply demands that
the lambda term a constant is mapped to has a type
which is appropriately related to the type of the
original constant. The order of a linear homomor-
phismL is the maximum order of the image under
it of an atomic type.3

Our proposal is formalized using abstract cat-
egorial grammars (ACGs). An ACG (de Groote,
2001) G = 〈Σ1,Σ2,L, α〉 consists of a pair of
higher order signatures Σ1 (called its abstract vo-
cabulary) and Σ2 (its concrete vocabulary), a lin-
ear homomorphism L (called a lexicon) between
them, and a designated type α inA1. The abstract
language of an ACG G is the set Λα(Σ1), and
its concrete language is the set L(Λα(Σ1)) :=
{L(M) : M ∈ Λα(Σ1)}. It will be convenient
to depict ACGs graphically, as in figure 2.

We write G(m,n) for the set of ACGs G with
m the order of its abstract vocabulary, and n the
order of its lexicon. Thinking of the abstract vo-
cabulary as the ‘derivation structures’, ACGs with
a second order abstract vocabulary (i.e. those in
G(2) := ⋃n∈N G(2, n)) represent grammar for-
malisms with regular derivation tree languages
(de Groote and Pogodalla, 2004). The set of con-
crete tree languages defined by G(2) is exactly the
set of tree languages generated by hyperedge re-
placement grammars (Kanazawa, 2010), and the
concrete string languages defined by G(2) is ex-
actly the set of multiple context free languages
(Salvati, 2007).

3ord(L) := max({ord(L(a)) : a ∈ A1})

3 Proposal

We focus on grammars with regular derivation
tree languages; in particular tree adjoining gram-
mars (Joshi, 1987) and minimalist grammars (Sta-
bler, 1997).

The derived structures generated by a grammar
in both of these grammar formalisms can be given
in terms of a transducer of a particular sort acting
on a regular set of derivation trees – in the case of
tree adjoining grammars the transducer is a simple
macro tree transducer (Shieber, 2006), and in the
case of minimalist grammars it is a linear deter-
ministic multi-bottom up tree transducer (Kobele
et al., 2007; Mönnich, 2007).

Importantly, natural semantic analyses for both
grammar formalisms can be given in terms
of a similar tree transduction over the deriva-
tion (Nesson, 2009; Kobele, 2006), allowing
a ‘synchronous’ representation whereby lexical
items ` are represented as triples of the form
〈h1(`),cat(`), h2(`)〉, where h1, h2 are the de-
rived structure and semantic structure transduc-
tions respectively, and cat(`) is the relevant cat-
egorical information (the state the regular tree au-
tomaton recognizing the well-formed derivation
trees is in upon scanning `).4 The ‘standard’ ap-
proach to idioms in (synchronous) TAG is to enter
them directly into the lexicon (Shieber and Sch-
abes, 1990) – this amounts to introducing a new
atomic lexical item `idiom, with (possibly com-
plex) images under h1 and h2. While idiomatic
expressions have not been handled in published
work on MGs, this is also the obvious approach
here too. While judicious choice of the derived
structures associated with `idiom may allow for
the ability of idiomatic material to be affected by
syntactic operations, it does not capture the in-
tuition that idioms actually contain the words it
seems they do. To do this, we move to extended
transductions (we discuss for simplicity homo-
morphisms: transductions without state). The
kernel of an extended homomorphism is a finite
relation between tree contexts: H ⊂ TΣ(X) ×
T∆(X). This kernel is thus the repository of non-
predictable interpretations of derivation tree con-
texts. Intuitively, H will map kick(the(bucket))
directly to die, as well as, indirectly via its

4The second component, cat(`), is only implicit in the
standard presentations of synchronous TAGs, e.g. (Shieber
and Schabes, 1990) (but is partially indicated by the links).

155



components, to kick(ιx.bucket(x)). Note
that, in the case we are primarily interested in
here in which only the semantic map is ‘non-
compositional’, we have two objects which can
be weighted – the set of lexical items, and the
kernel of the semantic homomorphism. This is
in line with the psycholinguistic findings (more
on which in §5.2) (Titone and Connine, 1999)
that (1) idioms seem to behave as though they are
syntactically complex (weights over the lexicon)
and (2) idioms themselves have different frequen-
cies which subjects are sensitive to (weights over
the extended transducers). In the remainder of
this paper, we use abstract categorial grammars
to work out this approach to idioms.

4 An ACG perspective

Viewing extended transducers from the perspec-
tive of abstract categorial grammars (ACGs) (de
Groote, 2001) gives us a uniform way of visual-
izing this approach to idioms, one which explains
the attraction of treating idioms as complex lexi-
cal items. Simply put, applying an extended trans-
ducer to a term t is the same as applying a non-
extended transducer to the inverse homomorphic
image of t under the map hex : Σ∪∆→ TΣ(X),
which maps elements of Σ to themselves, and el-
ements of ∆ to contexts over Σ. (∆ represents the
idioms as atomic objects.) (Second order) ACGs
provide a uniform notation for the macro and the
multi bottom-up transducers mentioned above –
both are defined in terms of a common set A of
‘abstract λ-terms’ which are mapped via homo-
morphism L to a set of ‘concrete λ-terms’ C. The
differences between macro and multi bottom-up
transductions are cashed out in terms of the homo-
morphisms and the nature of the concrete terms
(de Groote and Pogodalla, 2004). Synchronous
grammars are given in terms of two ACGs sharing
the same abstract language A, but with different
homomorphisms LΠ and LΛ to different concrete
languagesC1 (derived syntax) andC2 (semantics)
respectively as illustrated in figure 3 (cf. (Pogo-
dalla, 2007)).

An ACG representation of an extended trans-
duction τ from the terms of HOS A to those of
HOS C is obtained as follows. First, we cre-
ate a HOS X , whose constants represent the left
hand sides of the extended transducer rules, and
a homomorphism LX expanding constants in X
to the terms over A which they represent. Then

A

C1

LΠ

C2

LΛ

Figure 3: An ACG Perspective on Synchronous Gram-
mars

X

C

rhs(τ)

A

LX

Figure 4: Representing an extended transducer τ

the right hand sides of the extended transducer
rules are implemented via a homomorphism from
X to C. In the degenerate case where τ is
a non-extended transduction, A and X are the
same and the map LX is the identity function.
This is shown in figure 4. As a concrete ex-
ample, consider an extended (linear) bottom-up
transducer t from TΣ to T∆. The HOS A has
a single atomic type o, and constants σ ∈ Σ of
type orank(σ), similarly for C and ∆. The atomic
types of the HOS X are the states of the trans-
ducer. For each rule ρ = q(D[x1, . . . , xn]) →
E[q1(x1), . . . , qn(xn)], we have in X a con-
stant cρ of type q1 · · · qnq, whose image under
LX : X → A is λx1, . . . , xn.E(x1) · · · (xn),
and whose image under L : X → C is
λx1, . . . , xn.D(x1) · · · (xn).

From the synchronous perspective, it is natural
to begin with the non-extended case, as shown in
figure 3. Adding ‘extension’ on the semantic side,
so as to describe idioms, we must introduce a new
abstract language X , which is simply the original
abstract language of derivation terms A with an
extra atomic constant for each idiom. Then the
semantic homomorphism LΛ is defined from X
to C2, and the derived syntactic homomorphism
is the composition of the mapping LX from X to
A, which maps elements of A to themselves, and

156



X

A
LX

C1

LΠ
C2

LΛ

Figure 5: Semantically Extended Synchronous Gram-
mars

X

C1

LΠ ◦ LX

C2

LΛ

Figure 6: An ACG Perspective on Idioms as Lexical
Items

idiomatic constants to the complex (derivational)
objects they appear to be, and the mapping LΠ
from A to C1. Here we see that idioms are both
lexical items (elements of X) and derived (com-
plex objects in A). This is shown in figure 5.

Indeed, the standard approach to idioms,
whereby they are simply lexical items, is obtained
from ours by eliminating the HOS A by compos-
ing the maps LX and LΠ, as shown in figure 6.
Distinguishing between A and X , as we propose
here, does however have two (potential) benefits,
as discussed in sections 4.1, where we propose
a similar approach to morphological suppletion,
and 5.2, where we suggest that our approach has
some psychological plausibility.

4.1 Interface Uniformity
In decompositional (i.e. non-lexicalist) ap-
proaches to syntax, the notion of word-hood is
divorced from the notion of syntactic terminal –
words are conceived of as syntactically complex
objects. In this sort of approach, an additional
interface is needed to mediate between syntax
and morphology, so as to permit suppletive forms

X

Y

C1

LΠ

C2
LΛ

A

LX

LY

Figure 7: A w-shaped ACG

(go + Past ; went). A natural idea is to take
both the mapping from syntax to semantics and
the one from syntax to morphology to be of the
same kind; an extended transduction.

In our ACG setting, this amounts to introducing
a new abstract language Y , and a new lexicon LY
mapping terms in Y to terms in A. The lexicon Π
from A to C1, the concrete language of strings, is
replaced by one from Y to C1. This gives rise to a
new ‘w-shaped’ way of combining ACGs, shown
in figure 7. In the figure, we see that the deriva-
tion tree language is no longer directly mapped to
any derived structure language (and has become
therefore a concrete language). Instead, it serves
only to coordinate the abstract languages X and
Y , making sure that the strings and meanings they
produce are ‘of the same derivation’. Note also
that we are no longer able to eliminate the HOSA
(and recover the standard approach to idioms out-
lined in figure 6) as the relation betweenX andC1
(or between Y and C2) is no longer functional.

Parsing in a w-shaped ACG is a matter of com-
puting the inverse image of LΠ, then its image
under LY , then its inverse image under LX , and
then finally its image under LΛ; in other words,
the composition LΛ ◦ L−1X ◦ LY ◦ L−1Π . Whereas
application of a lambda homomorphism to a rec-
ognizable set of terms does not usually preserve
recognizability, here the mappings LX and LY

157



are both first order (as they map atomic types to
atomic types) and lexicalized (abstract constants
are mapped to non-combinators), and thus do in-
deed preserve recognizability. Hence a regular set
of strings (over the concrete HOS C1) is mapped
to a regular set of trees over the abstract HOS X .

The fact that the maps LX and LY are first or-
der distinguishes our proposal formally from the
related one proposed by Dras (1999) in the con-
text of textual paraphrase, which, from our per-
spective, introduces a sequence of ever more ab-
stract languages, which are related to each other
by maps of the same complexity as the maps LΠ
and LΛ. (There it is described as deriving tree
languages which are then interpreted as derivation
tree languages and so on.)

5 Linguistics

5.1 Constraints on Idiomatic Structure
Linguists have formulated various constraints on
possible idioms, in particular that dependents of
a head can belong to an idiom only if the head
belongs as well (Koopman and Sportiche, 1991;
O’Grady, 1998). This constraint is naturally im-
plemented in the present context by adopting a
TAG-like perspective on derivations, whereby the
operations of the grammar are left implicit, and
the lexical items are treated as constants of higher
rank. Then any first order subterm of a deriva-
tion tree satisfies this constraint. This move addi-
tionally rules out completely unlexicalized idioms
and constructions.5 This perspective on deriva-
tion trees can be easily adapted into the mini-
malist grammar framework, where the rank of
each lexical item is the number of positive selec-
tor features6 it has.7 Although the set of well-
formed minimalist derivation trees is not the al-
gebra freely generated over this signature, it is a
regular subset thereof.

5For example, λx, y.merge(move(y), z) is a first order
term over a HOS for (standard presentations of) minimalist
derivations. It could be the image of some idiom under the
map LS .

6Recall that minimalist grammar categories are finite lists
of syntactic features, which are either positive or negative
versions of either licensing (for the move operation) or se-
lection (for merge) feature types.

7It is also not necessary in the TAG framework, where
one could treat elementary trees as nullary function symbols,
and adjoin and substitute as binary ones.

5.1.1 Syntactic Permeability
Empirical ‘puzzles’ about idioms, such as

their variable permeability to syntactic operations
(Nunberg et al., 1994), must be dealt with by
fine-tuning the syntactic analysis; canonical anal-
yses of voice phenomena in MGs (as in (Kobele,
2006)) treat the voice head as taking a verb phrase
as an argument – an idiom which is not passiviz-
able must include the voice head, whereas one
which is must not.

As an example, consider the idiom “kick the
bucket” (‘die’), which cannot be passivized.8

Adopting the naı̈ve formalization of standard
transformational analyses from Kobele (2006),
we could represent the idiomatic interpreta-
tion of this phrase by means of a unary ab-
stract constant ckick the bucket, which is inter-
preted semantically as die, and derivationally
as λx.act(kick(the(bucket)))(x).9 On the
other hand, idioms like “let the cat out of the bag”
(‘reveal a secret’) can be passivized (and then sub-
ject to raising, and other transformations). While
there is no analysis of prepositions in Kobele
(2006), we can treat let as a raising to object verb,
and adapt a small clause analysis of the following
form:10

[V let [sc[Dthe cat] out [P of the bag]]

We represent this in terms of a
nullary abstract constant clet cat out of bag,
which is semantically interpreted as
λx.some(secret)(λy.reveal(y)(x)), and
which is derivationally interpreted as
let(out(P(of(the(bag))))(the(cat)))

Finally, an idiom like “throw the book at”
(‘punish severely’) illustrates the need for con-
stants corresponding not to subtrees, but to deriva-
tion tree contexts. Adopting again an overly
simplistic approach to prepositions, we repre-
sent this passivizible idiom using the unary ab-
stract constant cthrow book at, which is seman-
tically interpreted as λx.severely(punish(x)),

8This means that sentences like “The bucket was kicked”
can have only a literal interpretation.

9Lexical entries:
〈act,=V +k =d v〉 〈kick,=d V〉
〈the,=n d -k -q〉 〈bucket, n〉

10Lexical entries:
〈let,=sc V〉 〈out,=p =d sc〉
〈of,=d +k +q P〉 〈P,=P p〉
〈bag, n〉 〈cat, n〉

158



and is interpreted as the derivational term
λx.Adj(throw(the(book)))(P(at(x))).11

5.1.2 Transformationalism
In general, treating idioms via extended trans-

ducers (as done here) puts great pressure on the
type system of the grammar formalism (if it is de-
sired to have a single abstract constant represent-
ing the idiom in all of its glory). As one of the
arguments for transformational analysis continues
to be the variety of syntactic contexts which per-
mits idiomatic interpretation, it is perhaps no sur-
prise that the minimalist analyses sketched here
go some ways in achieving the ideal of assign-
ing a single type to the idiomatic abstract con-
stant. However, the particular constraints on id-
iom shape in place here rule out certain otherwise
reasonable transformational analyses, such as the
implementation of the raising analysis of relative
clauses in Kobele (2006).

5.2 Psycholinguistics
Adopting a ‘levels’ perspective on the relation
between grammar and parser (Marr, 1982), a
‘search’ perspective on parsing still needs a way
to order nodes in the agenda. A natural strategy
here is to use weights assigned to lexical entries.
Perhaps the main substantive difference between
our proposal and ‘standard’ treatments of idioms
is the availability of two ‘lexica’ – the HOSs A
(for derivations) and X (for idioms) – for dif-
ferential weighting (see figure 5). The standard
treatment, according to which idioms are lexical
items (as in figure 6), does not have access to the
HOS A. We will need to make a linking theory
precise, but, generally speaking, this setup will
predict that idioms behave in certain respects as
though they were indeed composed of the syntac-
tic atoms it appears they contain. We claim that
the literature on priming is consistent with this
prediction (Cutting and Bock, 1997; Titone and
Connine, 1999; Peterson et al., 2001; Sprenger
et al., 2006; Konopka and Bock, 2009). Priming
is the phenomenon whereby a prior exposure to
some linguistic object token facilitates the com-
prehension of another token of that type, and in-
creases the likelihood of producing a token of that
type.

11Lexical entries:
〈throw,=d V〉 〈Adj,=V =p V〉
〈at,=d +k +q P〉 〈P,=P p〉
〈book, n〉 〈cat, n〉

In order to account for the fact that words
can prime idioms which contain them, and vice
versa, Sprenger et al. (2006) (following Cutting
and Bock (1997)) propose that the mental lexicon
is structured as a graph; the lexicon contains id-
ioms, along with links to their constituent parts.
Priming works by increasing the weights attached
not only to a single node, but to all of its immedi-
ate neighbors. (And thus kick will prime the node
kick as well as its neighbors kick the bucket, kick
the can, . . . ) One defect of this account is that it
does not explain why certain nodes are linked to
others. Still, it bears an obvious resemblance to
the ACG account presented here – its nodes are
the elements of HOSX (which is the HOSAwith
extra constants for idioms), and the links are given
by the lexicon LX .

First, a (sketch of a) naı̈ve linking theory. We
assume that priming effects are to be captured
by updating weights on lexical items, and that
the weights of all and only the lexical items in
a successful parse are increased.12 Under this
view, priming effects come about due to an in-
creased weight of the primed lexical item as a
result of having used it previously in a success-
ful parse. For our preliminary purposes here, we
will say that a derivation d′ primes derivation d
(of sentence s) given grammar G just in case
WeightGd′ (d) > WeightG(d), where Gx is
the result of updating the weights of lexical items
in parse x (i.e. the weight of d is higher in the
reweighted grammar than in the original gram-
mar).

Now, taking the unique derivation d′ of the
sentence “John kicked Mary” as our primer, and
the idiomatic derivation d of the sentence “Susan
will kick the bucket” as our primee, we have that
the weight of d in the ‘idiomatic’ HOS X is the
same as in Xd′ , but that the weight in the ‘deriva-
tional’ HOSA is less than inAd′ (as the weight of
the constant ckick was increased after parsing d′).
Taking the overall weight of a term t in G to be
a monotonic function f of its weight in X and its
weight in A, we have that d is primed.

12This is an ‘off-line’ implementation of priming effects,
which neglects the role played by alternative parses of the
sentence; Slevc and Ferreira (to appear) demonstrate that
priming may occur from failed parses.

159



6 Conclusion

The noncompositional aspect of idiomatic expres-
sions is most naturally dealt with in terms of ex-
tended transducers. The ACG perspective allows
for a generalization to a wide variety of transducer
types, and language families. In addition to being
a principled formal account of idioms, the present
story allows for a natural connection to the psy-
cholinguistic data.

What we have not explained is the intuition
that, in some idioms (such as let the cat out of
the bag), certain parts of the idioms (e.g. the cat)
seem to be associated with certain parts of the id-
iomatic meaning (e.g. the secret). That this intu-
ition may in fact be deserving of an explanation
is suggested by Horn (2003) (building on work by
Nunberg et al. (1994)), who argues that this sort
of ‘semantic transparency’ is a good predictor of
whether the internal structure of an idiom can be
subject to syntactic manipulation.

References
Hendrik Barendregt. 1984. The Lambda Calculus:

Its syntax and semantics, volume 103 of Studies in
Logic and the Foundations of Mathematics. Else-
vier, Amsterdam.

Noam Chomsky. 1995. The Minimalist Program.
MIT Press, Cambridge, Massachusetts.

J. Cooper Cutting and Kathryn Bock. 1997. That’s
the way the cookie bounces: Syntactic and se-
mantic components of experimentally elicited id-
iom blends. Memory and Cognition, 25(1):57–71.

Philippe de Groote and Sylvain Pogodalla. 2004.
On the expressive power of Abstract Catego-
rial Grammars: Representing Context-Free for-
malisms. Journal of Logic, Language and Infor-
mation, 13(4):421–438.

Philippe de Groote. 2001. Towards abstract categorial
grammars. In Association for Computational Lin-
guistics, 39th Annual Meeting and 10th Conference
of the European Chapter, Proceedings of the Con-
ference, pages 148–155.

Anna Maria Di Sciullo and Edwin Williams. 1987.
On the definition of word, volume 14 of Linguistic
Inquiry Monographs. MIT Press.

Mark Dras. 1999. Tree Adjoining Grammar and the
Reluctant Paraphrasing of Text. Ph.D. thesis, Mac-
quarie University.

Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391–427.

George M. Horn. 2003. Idioms, metaphors and syn-
tactic mobility. Journal of Linguistics, 39:245–273.

Aravind K. Joshi. 1987. An introduction to tree ad-
joining grammars. In A. Manaster-Ramer, editor,
Mathematics of Language. John Benjamins, Ams-
terdam.

Makoto Kanazawa. 2010. Second-order abstract cat-
egorial grammars as hyperedge replacement gram-
mars. Journal of Logic, Language and Information,
19:137–161.

Gregory M. Kobele, Christian Retoré, and Sylvain Sal-
vati. 2007. An automata theoretic approach to min-
imalism. In James Rogers and Stephan Kepser, edi-
tors, Proceedings of the Workshop Model-Theoretic
Syntax at 10; ESSLLI ’07, Dublin.

Gregory M. Kobele. 2006. Generating Copies: An in-
vestigation into structural identity in language and
grammar. Ph.D. thesis, University of California,
Los Angeles.

Agnieszka E. Konopka and Kathryn Bock. 2009. Lex-
ical of syntactic control of sentence formulation?
Structural generalizations from idiom production.
Cognitive Psychology, 58:68–101.

Hilda Koopman and Dominique Sportiche. 1991. The
position of subjects. Lingua, 85:211–258.

David Marr. 1982. Vision. W. H. Freeman and Com-
pany, New York.

Uwe Mönnich. 2007. Minimalist syntax, multi-
ple regular tree grammars and direction preserv-
ing tree transductions. In James Rogers and
Stephan Kepser, editors, Proceedings of the Work-
shop Model-Theoretic Syntax at 10; ESSLLI ’07,
Dublin.

Frank Morawietz. 2003. Two-Step Approaches to Nat-
ural Language Formalisms, volume 64 of Studies in
Generative Grammar. Mouton de Gruyter.

Rebecca Nesson. 2009. Synchronous and Multi-
componenet Tree-Adjoining Grammars: Complex-
ity, Algorithms and Linguistic Applications. Ph.D.
thesis, Harvard University.

Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70(3):491–538.

William O’Grady. 1998. The syntax of idioms. Natu-
ral Language and Linguistic Theory, 16:279–312.

Robert R. Peterson, Curt Burgess, Gary S. Dell, and
Kathleen M. Eberhard. 2001. Dissociation be-
tween syntactic and semantic processing during
idiom comprehension. Journal of Experimental
Psychology: Learning, Memory, and Cognition,
27(5):1223–1237.

Sylvain Pogodalla. 2007. Generalizing a proof-
theoretic account of scope ambiguity. In
J. Geertzen, E. Thijsse, H. Bunt, and A. Schiffrin,
editors, Proceedings of the 7th International Work-
shop on Computational Semantics (IWCS), pages
154–165.

Sylvain Salvati. 2007. Encoding second order string
acgs with deterministic tree walking transducers. In
Shuly Wintner, editor, Proceedings of FG 2006: the

160



11th conference on Formal Grammar, pages 143–
156. CSLI Publications.

Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proceedings
of the 13th International Conference on Computa-
tional Linguistics (COLING), volume 3, pages 253–
258, Helsinki,Finland.

Stuart M. Shieber. 1994. Restricting the weak-
generative capacity of synchronous tree-adjoining
grammars. Computational Intelligence, 10(4):371–
385.

Stuart M. Shieber. 2006. Unifying synchronous tree-
adjoining grammars and tree transducers via bi-
morphisms. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association
for Computational Linguistics (EACL-2006), pages
377–384, Trento.

L. Robert Slevc and Victor S. Ferreira. to ap-
pear. To err is human; to structurally prime from
errors is also human. Journal of Experimental
Psychology: Learning, Memory, and Cognition.
doi:10.1037/a0029525.

Simone A. Sprenger, Willem J. M. Levelt, and Gerard
Kempen. 2006. Lexical access during the produc-
tion of idiomatic phrases. Journal of Memory and
Language, 54:161–184.

Edward P. Stabler. 1997. Derivational minimal-
ism. In Christian Retoré, editor, Logical Aspects of
Computational Linguistics, volume 1328 of Lecture
Notes in Computer Science, pages 68–95. Springer-
Verlag, Berlin.

Debra A. Titone and Cynthia M. Connine. 1999.
On the compositional and noncompositional nature
of idiomatic expressions. Journal of Pragmatics,
31:1655–1674.

161


