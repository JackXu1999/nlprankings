










































Finding the Most Probable String and the Consensus String: an Algorithmic Study


Proceedings of the 12th International Conference on Parsing Technologies, pages 26–36,
October 5-7, 2011, Dublin City University. c© 2011 Association for Computational Linguistics

Finding the Most Probable String and the Consensus String: an
Algorithmic Study

Colin de la Higuera∗ and Jose Oncina∗∗
∗Université de Nantes, CNRS, LINA, UMR6241, F-44000, France

cdlh@univ-nantes.fr
∗∗Departamento de Lenguajes y Sistemas Informaticos

Universidad de Alicante, Alicante, Spain
oncina@dlsi.ua.es

Abstract

The problem of finding the most proba-
ble string for a distribution generated by a
weighted finite automaton or a probabilistic
grammar is related to a number of important
questions: computing the distance between
two distributions or finding the best transla-
tion (the most probable one) given a prob-
abilistic finite state transducer. The prob-
lem is undecidable with general weights and
is NP-hard if the automaton is probabilis-
tic. We give a pseudo-polynomial algorithm
which computes the most probable string in
time polynomial in the inverse of the proba-
bility of the most probable string itself, both
for probabilistic finite automata and proba-
bilistic context-free grammars. We also give
a randomised algorithm solving the same
problem.

1 Introduction

When using probabilistic machines to define dis-
tributions over sets of strings, the usual and best
studied problems are those of parsing and of find-
ing the most probable explanation of a given
string (the most probable parse). These problems,
when dealing with probabilistic (generating) finite
state automata, hidden Markov Models (HMMs) or
probabilistic context-free grammars depend on the
ambiguity of the machine: indeed, if there can be
different parses for the same string, then the prob-
ability of the string is obtained by summing over
the different parses.

A more difficult problem we study here is that
of finding the most probable string; this string is
also known as the consensus string.

The problem of finding the most probable string
was first addressed in the computational linguis-
tics community by Sima’an (1996): he proved the
problem to be NP-hard if we consider tree gram-
mars, and as a corollary he gave the same result for

context-free grammars. Goodman (1998) showed
that, in the case of HMMs, the problem of finding
whether the most most probable string of a given
length n is at least p is NP-Complete. Moreover,
he points that his technique cannot be applied to
show the NP-completeness of the problem when
n is not prespecified because the most probable
string can be exponentially long. Casacuberta
and de la Higuera (2000) proved the problem to
be NP-hard, using techniques developed for lin-
guistic decoding (Casacuberta and de la Higuera,
1999): their result holds for probabilistic finite
state automata and for probabilistic transducers
even when these are acyclic: in the transducer case
the related (and possibly more important) ques-
tion is that of finding the most probable transla-
tion. The problem was also addressed with mo-
tivations in bioinformatics by Lyngsø and Peder-
sen (2002). Their technique relies on reductions
from maximal cliques. As an important corol-
lary of their hardness results they prove that the
L1 and L∞ distances between distributions repre-
sented by HMMs are also hard to compute: indeed
being able to compute such distances would en-
able to find (as a side product) the most probable
string. This result was then applied on probabilis-
tic finite automata in (Cortes et al., 2006; Cortes et
al., 2007) and the Lk distance, for each odd k was
proved to be intractable.

An essential consequence of these results is that
finding the most probable translation given some
probabilistic (non deterministic) finite state trans-
ducer is also at least as hard. It can be shown
(Casacuberta and de la Higuera, 1999; Vidal et al.,
2005) that solving this problem consists in finding
the most probable string inside the set of all ac-
ceptable translations, and this set is structured as a
probabilistic finite automaton. Therefore, the most
probable translation problem is also NP-hard.

On the other hand, in the framework of multi-
plicity automata or of accepting probabilistic finite

26



automata (also called Rabin automata), the prob-
lem of the existence of a string whose weight is
above (or under) a specific threshold is known to
be undecidable (Blondel and Canterini, 2003). In
the case where the weight of each individual edge
is between 0 and 1, the score can be interpreted as
a probability. The differences reside in the fact that
in multiplicity automata the sum of the probabili-
ties of all strings does not need to be bounded; this
is also the case for Rabin automata, as each prob-
ability corresponds to the probability for a given
string to belong to the language.

In this paper we attempt to better understand
the status of the problem and provide algorithms
which find a string of probability higher than a
given threshold in time polynomial in the inverse
of this threshold. These algorithms give us prag-
matic answers to the consensus string problem as
it is possible to use the probabilistic machine to de-
fine a threshold and to use our algorithms to find,
in this way, the most probable string.

We will first (Section 2) give the different defi-
nitions concerning automata theory, distributions
over strings and complexity theory. In Section
3 we show that we can compute the most prob-
able string in time polynomial in the inverse of
the probability of this most probable string but in
the bounded case, i.e. when we are looking for
a string of length smaller than some given bound.
In Section 4 we show how we can compute such
bounds. In Section 5 the algorithms are experi-
mentally compared and we conclude in Section 6.

2 Definitions and Notations

2.1 Languages and Distributions

Let [n] denote the set {1, . . . , n} for each n ∈ N.
An alphabet Σ is a finite non-empty set of sym-
bols called letters. A string w over Σ is a fi-
nite sequence w = a1 . . . an of letters. Let |w|
denote the length of w. In this case we have
|w| = |a1 . . . an| = n. The empty string is
denoted by λ. When decomposing a string into
substrings, we will write w = w1 . . . wn where
∀i ∈ [n] wi ∈ Σ⋆.

Letters will be indicated by a, b, c, . . ., and
strings by u, v, . . . , z.

We denote by Σ⋆ the set of all strings, by Σn

the set of those of length n, by Σ<n (respectively
Σ≤n, Σ≥n) the set of those of length less than n
(respectively at most n, at least n).

A probabilistic language D is a probability dis-

tribution over Σ⋆. The probability of a string
x ∈ Σ⋆ under the distribution D is denoted as
PrD(x) and must verify

∑

x∈Σ⋆ PrD(x) = 1.
If the distribution is modelled by some syntactic

machine M, the probability of x according to the
probability distribution defined by M is denoted
PrM(x). The distribution modelled by a machine
M will be denoted by DM and simplified to D if
the context is not ambiguous.

If L is a language (thus a set of strings, included
in Σ⋆), and D a distribution over Σ⋆, PrD(L) =
∑

x∈L PrD(x).

2.2 Probabilistic Finite Automata

The probabilistic finite automata (PFA) (Paz,
1971) are generative devices:

Definition 1. A Probabilistic Finite Automaton
(PFA) is a tuple A = 〈Σ, Q, S, F, δ〉, where:

- Σ is the alphabet;

- Q = {q1,. . . , q|Q|} is a finite set of states;

- S : Q→ R+ ∩ [0, 1] (initial probabilities);

- F : Q→ R+ ∩ [0, 1] (final probabilities);

- δ : Q × (Σ ∪ {λ}) × Q → R+ is a
transition function; the function is complete:
δ(q, a, q′) = 0 can be interpreted as “no
transition from q to q′ labelled with a”.

S, δ and F are functions such that:

∑

q∈Q
S(q) = 1, (1)

and ∀q ∈ Q,

F (q) +
∑

a∈Σ∪{λ}, q′∈Q
δ(q, a, q′) = 1. (2)

Let x ∈ Σ⋆. ΠA(x) is the set of all
paths accepting x: a path is a sequence π =
qi0x1qi1x2 . . . xnqin where x = x1 · · ·xn, xi ∈
Σ ∪ {λ}, and ∀j ≤ n, ∃pj 6= 0 such that
δ(qij−1 , xj , qij ) = pj . The probability of the path
π is

S(qi0) ·
∏

j∈[n]
pj · F (qin)

And the probability of the string x is obtained
by summing over all the paths in ΠA(x). Note
that this may result in an infinite sum because of
λ-transitions (and more problematically λ-cycles.

27



An effective computation can be done by means of
the Forward (or Backward) algorithm (Vidal et al.,
2005).

0.4 : q1

0.6 : q2 : 0.1

q3 : 0.4

q4 : 0.3

a 0.5 a 0.5

b 0.4a 0.5

b 0.2b 0.5

a 0.2 b 0.4

Figure 1: Graphical representation of a PFA.

Alternatively, a PFA (with n states) is given
when the following matrices are known:

• S ∈ R1×n represents the probabilities of
starting at each state. S[i]=S(qi);

• M = {Ma ∈ Rn×n|a ∈ Σ ∪ {λ}} repre-
sents the transition probabilities. Ma[i, j] =
δ(qi, a, qj);

• F ∈ Rn×1 represents the probabilities of
ending in each state. F[i]=F (qi).

Given a string x = a1 · · · ak we compute
PrA(x) as:

PrA(x) = S
|x|
∏

i=1

[M∗λMai ]M
∗
λF (3)

where

M
∗
λ =

∞
∑

i=0

M iλ = (I −Mλ)−1

Then, equations 1 and 2 can be written as:

S1 = 1 (4)
∑

a∈Σ∪{λ}
Ma1 + F = 1 (5)

where 1 ∈ Rn is such that ∀i 1[i] = 1.
Note that

PrA(λ) = SM
∗
λF ∈ [0, 1] (6)

This implies that M∗λ should be a non singular
matrix.

Moreover, in order for PrA to define a distribu-
tion probability over Σ⋆ it is required that:

∑

x∈Σ∗
PrA(x) =

∞
∑

i=0

SM
∗
λM

i
ΣM

∗
λF

= SM∗λ(I −MΣ)−1M∗λF = 1

where I is the identity matrix and MΣ =
∑

a∈Σ Ma. Note that as a consequence of that,
(I −MΣ) is a non singular matrix.

2.3 Hidden Markov Models

Hidden Markov models (HMMs) (Rabiner, 1989;
Jelinek, 1998) are finite state machines defined by
(1) a finite set of states, (2) a probabilistic transi-
tion function, (3) a distribution over initial states,
and (4) an output function.

An HMM generates a string by visiting (in a
hidden way) states and outputting values when in
those states. Typical problems include finding the
most probable path corresponding to a particular
output (usually solved by the Viterbi algorithm).
Here the question of finding the most probable
output has been addressed by Lyngsø and Peder-
sen (2002). In this paper the authors prove that the
hardness of this problem implies that it is also hard
to compute certain distances between two distribu-
tions given by HMMs.

Note that to obtain a distribution over Σ⋆ and
not each Σn the authors introduce a unique final
state in which, once reached, the machine halts.
An alternative often used is to introduce a special
symbol (♯) and to only consider the strings termi-
nating with ♯: the distribution is then over Σ⋆ ♯.

Equivalence results between HMMs and PFA
can be found in (Vidal et al., 2005).

2.4 Probabilistic Context-free Grammars

Definition 2. A probabilistic context-free gram-
mar (PCFG) G is a quintuple < Σ, V,R, P,N >
where Σ is a finite alphabet (of terminal sym-
bols), V is a finite alphabet (of variables or non-
terminals), R ⊂ V × (Σ ∪ V )∗ is a finite set
of production rules, and N (∈ V ) is the axiom.
P : R→ R+ is the probability function.

A PCFG is used to generate strings by rewriting
iteratively the non terminals in the string, start-
ing from the axiom. A string may be obtained
by different derivations. In this case the problem
is called ambiguity. Parsing with a PCFG is usu-
ally done by adapting the Earley or the CKY algo-
rithms.

28



Particularly appealing is a very efficient exten-
sion of the Early algorithm due to Stolcke (1995)
that can compute:

• the probability of a given string x generated
by a PCFG G;

• the single most probable parse for x;

• the probability that x occurs as a prefix of
some string generated byG, which we denote
by PrG(xΣ⋆).

2.5 Probabilistic Transducers

There can be different definitions of probabilistic
transducers. We use the one from (Vidal et al.,
2005):

q1 : 0.2 q2 : 0.3 q3 : 0

λ :: 1, 0.3

b :: 00, 0.2

b :: 0, 0.2

λ :: 1, 0.5

a :: λ, 0.4

a :: 1, 0.3 λ :: 0, 0.6

Figure 2: Transducer.

q[1,λ] q[2,λ] q[3,λ]

q[1,a] q[2,a] q[3,a]

q[1,ab] : 0.2 q[2,ab] : 0.3 q[3,ab]

(λ) 1, 0.3 (λ) 1, 0.5

(λ) 0, 0.6

(a) 1, 0.3

(a)λ, 0.4

(λ) 1, 0.3 (λ) 1, 0.5

(λ) 0, 0.6

(b) 00, 0.2(b) 0, 0.2

(λ) 1, 0.3 (λ) 1, 0.5

(λ) 0, 0.6

Figure 3: Corresponding non normalized PFA for the
translations of ab. Each state indicates which input
prefix has been read. Between the brackets, on the tran-
sitions, the input symbol justifying the transition.

Probabilistic finite-state transducers (PFST) are
similar to PFA, but in this case two different alpha-
bets (source Σ and target Γ) are involved. Each
transition in a PFST has attached a symbol from
the source alphabet (or λ) and a string (possible
empty string) of symbols from the target alphabet.
PFSTs can be viewed as graphs, as for example in
Figure 3.

Definition 3 (Probabilistic transducer). A proba-
bilistic finite state transducer (PFST) is a 6-tuple
〈Q,Σ,Γ, S, E, F 〉 such that:

q1,λ q3,λ q3,λ

q1,a q2,a

q1,ab : 0.4 q2,ab : 1

1, 0.5 1, 1
0, 0.6

1, 0.5

λ, 0.4
1, 1

0, 1

1, 0.6

Figure 4: Corresponding normalized PFA for the trans-
lations of ab. The most probable string (111) has prob-
ability 0.54.

- Q is a finite set of states; these will be la-
belled q1,. . . , q|Q|;

- S : Q→ R+ ∩ [0, 1] (initial probabilities);

- F : Q→ R+ ∩ [0, 1] (halting probabilities);

- E ∈ Q× (Σ∪{λ})×Γ⋆×Q×R+ is the set
of transitions;

S, δ and F are functions such that:
∑

q∈Q
S(q) = 1,

and ∀q ∈ Q,

F (q) +
∑

a∈Σ∪{λ}, q′∈Q
p : (q, a, w, q′, p) ∈ E = 1.

Let x ∈ Σ⋆ and y ∈ Γ⋆. Let ΠT (x, y) be the
set of all paths accepting (x, y): a path is a se-
quence π = qi0(x1, y1)qi1(x2, y2) . . . (xn, yn)qin
where x = x1 · · ·xn and y = y1 · · · yn, with ∀j ∈
[n], xj ∈ Σ∪{λ} and yj ∈ Γ⋆, and ∀j ∈ [n], ∃pij
such that (qij−1 , xj , yj , qij , pij ) ∈ E. The proba-
bility of the path is

S(qi0) ·
∏

j∈[n]
pij · F (qin)

And the probability of the translation pair (x, y)
is obtained by summing over all the paths in
ΠT (x, y).

Note that the probability of y given x (the
probability of y as a translation of x, denoted as
PrT (y|x)) is PrT (x,y)P

z∈Σ⋆ PrT (x,z)
.

Probabilistic finite state transducers are used as
models for the the stochastic translation problem
of a source sentence x ∈ Σ⋆ that can be defined as
the search for a target string y that:

argmax
y

Pr(y | x) = argmax
y

Pr(y, x).

29



The problem of finding this optimal translation
is proved to be a NP-hard by Casacuberta and de
la Higuera (2000).

An approximate solution to the stochastic trans-
lation can be computed in polynomial time by us-
ing an algorithm similar to the Viterbi algorithm
for probabilistic finite-state automata (Casacu-
berta, 1995; Picó and Casacuberta, 2001).

The stochastic translation problem is compu-
tationally tractable in particular cases. If the
PFST T is non-ambiguous in the translation sense
(∀x ∈ Σ⋆ there are not two target sentences
y, y′ ∈ Γ⋆, y 6= y′, such that PrT (x, y) > 0 and
PrT (x, y′) > 0), the translation problem is poly-
nomial. If the PFST T is simply non-ambiguous
(∀x ∈ Σ⋆ there are not two different paths that
deal with (x, y) and with probability different to
zero), the translation problem is also polynomial.
In both cases, the computation can be carried out
using an adequate version of the Viterbi algorithm
(Vidal et al., 2005).

Alternative types of PFSTs have been intro-
duced and applied with success in different areas
of machine translation. In (Mohri, 1997; Mohri
et al., 2000), weighted finite-state transducers are
studied.

2.6 Complexity Classes and Decision
Problems

We only give here some basic definitions and re-
sults from complexity theory. A decision prob-
lem is one whose answer is true or false. A deci-
sion problem is decidable if there is an algorithm
which, given any specific instance, computes cor-
rectly the answer and halts. It is undecidable if
not. A decision problem is in P if there is a poly-
nomial time algorithm that solves it.

A decision problem isNP-complete if it is both
NP-hard and in the class NP: in this case a
polynomial time non-deterministic algorithm ex-
ists that always solves this problem. Alterna-
tively, a problem is in NP if there exists a poly-
nomial certificate for it. A polynomial certificate
for an instance I is a short (polynomial length)
string which when associated to instance I can be
checked in polynomial time to confirm that the in-
stance is indeed positive. A problem is NP-hard
if it is at least as hard as the satisfiability problem
(SAT), or either of the other NP-complete prob-
lems (Garey and Johnson, 1979).

A randomized algorithm makes use of random

bits to solve a problem. It solves a decision prob-
lem with one-sided error if given any value δ and
any instance, the algorithm:

• makes no error on a negative instance of a
problem (it always answers no);

• makes an error in at most δ cases when work-
ing on a positive instance.

If such an algorithm exists, the problem is said to
belong to the class RP . It should be noticed that
by running such a randomized algorithm n times
the error decreases exponentially with n: if a pos-
itive answer is obtained, then the instance had to
be positive, and the probability of not obtaining a
positive answer (for a positive instance) in n tries
is less than δn. A randomized algorithm which
solve a decision problem in the conditions above
is called a Monte Carlo algorithm.

When a decision problem depends on an in-
stance containing integer numbers, the fair (and
logical) encoding is in base 2. If the problem ad-
mits a polynomial algorithm whenever the integers
are encoded in base 1, the problem (and the algo-
rithm) are said to be pseudo-polynomial.

2.7 About Sampling

One advantage of using PFA or similar devices is
that they can be effectively used to develop ran-
domised algorithms. But when generating ran-
dom strings, the fact that the length of these is un-
bounded is an issue. Therefore the termination of
the algorithm might only be true with probability
1: this means that the probability of an infinite run,
even if it cannot be discarded, is of null measure.

In the work of Ben-David et al. (1992) which
extends Levin’s original definitions from (Levin,
1986), a distribution over {0, 1}∗ is considered
samplable if it is generated by a randomized al-
gorithm that runs in time polynomial in the length
of its output.

We will require a stronger condition to be met.
We want a distribution represented by some ma-
chine M to be sampable in a bounded way, ie,
we require that there is a randomized algorithm
which, when given a bound b, will either return
any string w in Σ≤b with probability PrM(w) or
return fail with probability PrM(Σ>b). Further-
more, the algorithm should run in time polynomial
in b.

As we also need parsing to take place in polyno-
mial time, we will say that a machineM is stronly

30



sampable if

• one can parse an input string x by M and
return PrM(x) in time polynomial in |x|;

• one can sample DM in a bounded way.

2.8 The Problem

The question is to find the most probable string in
a probabilistic language. An alternative name to
this string is the consensus string.

Name: Consensus string (CS)
Instance: A probabilistic machine M
Question: Find in Σ⋆ a string x such that
∀y ∈ Σ⋆ PrM(x) ≥ PrM(y).

With the above problem we associate the fol-
lowing decision problem:

Name: Most probable string (MPS)
Instance: A probabilistic machine M, a p ≥ 0
Question: Is there in Σ⋆ a string x such that
PrM(x) ≥ p?

For example, if we consider the PFA from Fig-
ure 1, the most probable string is a.

Note that p is typically encoded as a fraction and
that the complexity of our algorithms is to depend
on the size of the encodings, hence of log 1p .

The problem MPS is known to be NP-hard
(Casacuberta and de la Higuera, 2000). In their
proof the reduction is from SAT and uses only
acyclic PFA. There is a problem with MPS: there
is no bound, in general, over the length of the
most probable string. Indeed, even for regular lan-
guages, this string can be very long. In Section 4.4
such a construction is presented.

Of interest, therefore, is to study the case where
the longest string can be bounded, with a bound
given as a separate argument to the problem:

Name: Bounded most probable string (BMPS)
Instance: A probabilistic machine M, a p ≥ 0,
an integer b
Question: Is there in Σ≤b a string x such that
PrM(x) ≥ p?

In complexity theory, numbers are to be en-
coded in base 2. In BMPS, it is necessary, for the
problem not to be trivially unsolvable, to consider
a unary encoding of b, as strings of length up to b
will have to be built.

3 Solving BMPS

In this section we attempt to solve the bounded
case. We first solve it in a randomised way, then
propose an algorithm that will work each time
the prefix probabilities can be computed. This is
the case for PFA and for probabilistic context free
grammars.

3.1 Solving by Sampling

Let us consider a class of strongly sampable ma-
chines.

Then BMPS, for this class, belongs to RP:
Theorem 1. If a machineM is strongly sampable,
BMPS can be solved by a Monte Carlo algorithm.

Proof. The idea is that any string s whose proba-
bility is at least p, should appear (with high proba-
bility, at least 1−δ) in a sufficiently large randomly
drawn sample (of size m), and have a relative fre-
quency fm of at least

p
2 .

Algorithm 1 therefore draws this large enough
sample in a bounded way and then checks if any
of the more frequent strings (relative frequency fm
of at least p2 ) has real probability at least p.

We use multiplicative Chernov bounds to com-
pute the probability that an arbitrary string whose
probability is at least p has relative frequency fm
of at least p2 :

Pr
( f

m
<
p

2

)

≤ 2e−mp/8

So for a value of δ ≤ 2e−mp/8 it is sufficient
to draw a sample of size m ≥ 8p ln 2δ in order to
be certain (with error δ) that in a sample of size m
any probable string is in the sample with relative
frequency fm of at least

p
2 .

We then only have to parse each string in the
sample which has relative frequency at least p2 to
be sure (within error δ) that s is in the sample.

If there is no string with probability at least p,
the algorithm will return false.

The complexity of the algorithm depends on
that of bounded sampling and of parsing. One can
check that in the case of PFA, the generation is in
O(b · log |Σ|) and the parsing (of a string of length
at most b) is in O(b · |Q|2).

3.2 A Direct Computation in the Case of PFA

When the machine is a probabilistic finite automa-
ton, we can do a bit better by making use of simple
properties concerning probabilistic languages.

31



Data: a machine M, p ≥ 0, b ≥ 0
Result: w ∈ Σ≤b such that PrM(w) ≥ p,

false if there is no such w
begin

Map f;
m = 8p ln

2
δ ;

repeat m times
w = bounded sample(M, b);
f [w]++;

foreach w: f [w] ≥ pm2 do
if PrM(w) ≥ p then

return w;

return false

Algorithm 1: Solving BMPS in the general
case

We are given a p > 0 and a PFA A. Then we
have the following properties:

Property 1. ∀u ∈ Σ⋆, PrA(uΣ⋆) ≥ PrA(u).
Property 2. For each n ≥ 0 there are at most 1p
strings u in Σn such that PrA(uΣ⋆) ≥ p.

Both proofs are straightforward and hold not
only for PFA but for all distributions. Notice that
a stronger version of Property 2 is Property 3:

Property 3. If X is a set of strings such that (1)
∀u ∈ X,PrA(uΣ⋆) ≥ p and (2) no string in X
is a prefix of another different string in X , then
|X| ≤ 1p .

Analysis and complexity of Algorithm 2. The
idea of the algorithm is as follows. For each length
n compute the set of viable prefixes of length n,
and keep those whose probability is at least p. The
process goes on until either there are no more vi-
able prefixes or a valid string has been found. We
use the fact that PrA(uaΣ⋆) and PrA(u) can be
computed from PrA(uΣ⋆) provided we memo-
rize the value in each state (by a standard dynamic
programming technique). Property 2 ensures that
at every moment at most 1p valid prefixes are open.

If all arithmetic operations are in constant time,

the complexity of the algorithm is in O( b|Σ|·|Q|2p ).

3.3 Sampling Vs Exact Computing

BMPS can be solved with a randomized algorithm
(and with error at most δ) or by the direct Algo-
rithm 2. If we compare costs, and assuming that
bounded sampling a string can be done in time
linear in b, and that all arithmetic operations take
constant time we have:

Data: a PFA : A = 〈Σ,S,M,F〉, p ≥ 0,
b ≥ 0

Result: w ∈ Σ≤b such that PrA(u) ≥ p,
false if there is no such w
begin

Queue Q;
pλ = SF;
if pλ ≥ p then

return pλ;

push( Q, (λ, F));
while not empty(Q) do

(w, V) = pop (Q);
foreach a ∈ Σ do

V
′ = VMa;

if V′F ≥ p then
return V′F;

if |w| < b and V′1 ≥ p then
push( Q, (wa, V′));

return false

Algorithm 2: Solving BMPS for automata

• Complexity of (randomized) Algorithm 1 for
PFA is inO(8bp ln 2δ ·log |Σ|) to build the sam-
ple and O(2bp · |Q|2) to check the 2p most fre-
quent strings.

• Complexity of Algorithm 2 is inO( b|Σ|·|Q|2p ).
Therefore,for the randomized algorithm to be
faster, the alphabet has to be very large. Experi-
ments (see Section 5) show that this is rarely the
case.

3.4 Generalising to Other Machines

What is really important in Algorithm 2 is that the
different PrM(uΣ⋆) can be computed. If this is
a case, the algorithm can be generalized and will
work with other types of machines. This is the
case for context-free grammars (Stolcke, 1995).

For classes which are strongly sampable, we
propose the more general Algorithm 3.

4 More about the Bounds

The question we now have to answer is: how do
we choose the bound? We are given some machine
M and a number p ≥ 0. We are looking for a
value np which is the smallest integer such that
PrM(x) ≥ p =⇒ |x| ≤ np. If we can compute
this bound we can run one of the algorithms from
the previous section.

32



Data: a machine M, p ≥ 0, b ≥ 0
Result: w ∈ Σ≤b such that PrM(w) ≥ p,

false if there is no such w
begin

Queue Q;
pw = PrM(λ);
if pw ≥ p then

return pw;

push( Q, λ);
while not empty(Q) do

w = pop (Q);
foreach a ∈ Σ do

if PrM(wa) ≥ p then
return PrM(wa);

if |w| < b and PrM(waΣ∗) ≥ p
then

push( Q, wa);

return false

Algorithm 3: Solving BMPS for general ma-
chines

4.1 Computing Analytically np

If given the machineM we can compute the mean
µ and the variance σ of the length of strings in
DM, we can use Chebychev’s inequality:

PrM
(∣

∣|x| − µ
∣

∣ > kσ
)

<
1

k2

We now choose k = 1√p and rewrite:

PrM
(

|x| > µ+ σ√
p

)

< p

This means that, if we are looking for strings with
a probability bigger than p, it is not necessary to
consider strings longer than µ+ σ√p .

In other words, we can set b = ⌈µ+ σ√p⌉ and run
an algorithm from Section 3 which solves BMPS.

4.2 Computing Analytically np for PFA

We consider the special case where the probabilis-
tic machine is a PFA A. We are interested in
computing the mean and the variance of the string
length. It can be noted that the fact the PFA is de-
terministic or not is not a problem.

The mean string length of the strings generated

by A can be computed as:

µ =
∞

∑

i=0

i PrA(Σ
i)

=
∞

∑

i=0

iSM∗λM
i
ΣM

∗
λF

= SM∗λMΣ(I −MΣ)−2M∗λF

Moreover, taking into account that:

∞
∑

i=0

i2PrA(Σ
i) =

∞
∑

i=0

i2SM∗λM
i
ΣM

∗
λF

= SM∗λMΣ(I + MΣ)(I −MΣ)−3M∗λF

The variance can be computed as:

σ2 =
∞

∑

i=0

(i− µ)2PrA(Σi)

=
∞

∑

i=0

i2PrA(Σ
i)− µ2

= SM∗λMΣ(I + MΣ)(I −MΣ)−3M∗λF
−

[

SM
∗
λMΣ(I −MΣ)−2M∗λF

]2

Then, both values are finite since (I −MΣ) is
non singular.

4.3 Computing np,δ via Sampling

In certain cases we cannot draw an analytically ob-
tained value for the mean and the variance. We
have to resort to sampling in order to compute an
estimation of np.

A sufficiently large sample is built and used by
Lemma 1 to obtain our result. In that case we have
the following:

• If the instance is negative, it is anyhow im-
possible to find a string with high enough
probability, so the answer will always be
false.

• If the instance is positive, the bound returned
by the sampling will be good in all but a small
fraction (less than δ) of cases. When the sam-
pling has gone correctly, then the algorithm
when it halts has checked all the strings up to
length n. And the total weight of the remain-
ing strings is less than p.

The general goal of this section is to com-
pute, given a strogly sampable machine M ca-
pable of generating strings following distribution

33



DM and a positive value p, an integer np,δ such
that PrDM(Σ

np,δ) < p. If we do this by sampling
we will of course have the result depend also on
the value δ covering the case where the sampling
process went abnormally wrong.

Lemma 1. Let D be a distribution over Σ⋆. Then
if we draw, following distribution D, a sample S
of size at least 1p ln

1
δ , given any p > 0 and any

δ > 0, the following holds with probability at least
1-δ: the probability of sampling a string x longer
than any string seen in S is less than p.

Alternatively, if we write nS = max{|y| :
y ∈ S}, then, with probability at least 1 − δ,
PrD(|x| > nS) < p.

Proof. Denote by mp the smallest integer such
that the probability for a randomly drawn string to
be longer than mp is less than p: PrD(Σ>mp) <
p.

We need now to compute a large enough sample
to be sure (with a possible error of at most δ) that
max{|y| : y ∈ S} ≥ mp. For PrD(|x| > mp) <
p to hold, a sufficient condition is that we take a
sample large enough to be nearly sure (i.e. with
probability at least 1−δ) to have at least one string
as long as mp. On the contrary, the probability
of having all (k) strings in S of length less than
mp is at most (1 − p)k. Using the fact that (1 −
p)k > δ implies that k > 1p ln

1
δ , it follows that it

is sufficient, once we have chosen δ, to take np,δ >
1
p ln

1
δ to have a correct value.

Note that in the above, all we ask is that we
are able to sample. This is indeed the case
with HMM, PFA and (well defined) probablistic
context-free grammars, provided these are not ex-
pansive. Lemma 1 therefore holds for any of such
machines.

4.4 The Most Probable String Can Be of
Exponential Length

If the most probable string can be very long, how
long might it be? We show now an automaton for
which the most probable string is of exponential
length with the size of the automaton. The con-
struction is based on (de la Higuera, 1997). Let
us use a value γ > 0 whose exact value we will
compute later.

We first note (Figure 5) how to build an automa-
ton that only gives non null probabilities to strings
whose length are multiples of ψ for any value of ψ

(and of particular interest are the prime numbers).
Here, Pr(akψ) = γ(1− γ)k.

γ

a : 1 a : 1

a : 1

a : 1

a : 1− γ

Figure 5: Automaton for (a5)∗.

We now extend this construction by building
for a set of prime numbers { ψ1, ψ2,. . . , ψz}
the automaton for each ψi and adding an initial
state. When parsing a non empty string, a sub-
automaton will only add to the mass of probabili-
ties if the string is of length multiple of ψi. This
PFA can be constructed as proposed in Figure 6,
and has 1 +

∑i=z
i=1 ψi states.

The probability of string ak with k =
∏i=z
i=1 pi

is
∑i=z

i=1
1
zγ(1− γ)

k
ψi
−1

= γz
∑i=z

i=1(1− γ)
k
ψi
−1

.
First consider a string of length less than k. This

string is not accepted by at least one of the sub-
automata so it’s probability is at most γ z−1z .

On the other hand we prove now that for a good
value of γ, Pr(ak) > γ z−1z .

We simplify by noticing that since kψi − 1 ≤ k,
(1− γ)

k
ψi
−1

> (1− γ)k.
So Pr(ak) > γz

∑i=z
i=1(1− γ)k = γ(1− γ)k.

(1− γ)k > z − 1
z

1− γ > k
√

z − 1
z

γ < 1− k
√

z − 1
z

no shorter string can have higher probability.

5 Experiments

We report here some experiments in which we
compared both algorithms over probabilistic au-
tomata.

In order to have languages where the most prob-
able string is not very short, we generated a set of
random automata with a linear topology, only one
initial state and one final state, and where tran-
sitions were added leading from each state to all

34



γ

a : 1 a : 1

a : 1

a : 1

a : 1− γ

γ

a : 1 a : 1− γ

γ

a : 1

a : 1

a : 1− γ
a : 1

z

a : 1
z

a : 1
z

γ

a : 1
z

γ

a : 1
z

Figure 6: An automaton whose smallest ‘interesting
string’ is of exponential length.

previous states labelled by all the symbols of the
vocabulary.

The probabilities on the edges and the final state
were set assigning to them randomly (uniformly)
distributed numbers in the range [0, 1] and then
normalizing.

Voc size Sampling (s) Exact (s)
2 0.34 0.00
4 13.26 0.00
6 13.80 0.01
8 31.85 0.02

10 169.21 0.09
12 156.58 0.10

Table 1: Execution time of Algorithm 1 (sampling) and
Algorithm 2 (exact) for 4 state automata

In our experiments, the exact algorithm is sys-
tematically faster than the one that uses sampling.

Alternative settings which would be favourable
to the randomized algorithm are still to be found.

6 Conclusion

We have proved the following:

1. There exists a PFA whose most probable
string is not of polynomial length.

2. If we can sample and parse (strongly sam-
pable distribution), then we have a ran-
domised algorithm which solves MPS.

3. If furthermore we can analytically compute
the mean and variance of the distribution,
there is an exact algorithm for MPS. This
means that the problem is decidable for a PFA
or HMMs.

4. In the case of PFA the mean and the variance
are polynomially computable, so MPS can be
solved in time polynomial in the size of the
PFA and in 1p .

5. In the case of PFA, we can use practical algo-
rithms:

(a) randomly draw a sample S of n strings
following distribution DA;

(b) let p = max{p(u) : u ∈ S} and b =
max{|u| : u ∈ S};

(c) run Algorithm 2 using p and b.

Practically, the crucial problem may be CS; A
consensus string can be found by either sampling
to obtain a lower bound to the probability of the
most probable string and solving MPS, or by some
form of binary search.

Further experiments are needed to see in what
cases the sampling algorithm works better, and
also to check its robustness with more complex
models (like probabilistic context-free grammars).

Finally, in Section 4.4 we showed that the length
of the most probable string could be exponential,
but it is unclear if a higher bound to the length can
be obtained.

Acknowledgement

Discussions with Pascal Koiran during earlier
stages of this work were of great help towards
the understanding of the nature of the problem.
The first author also acknowledges partial support
by the Région des Pays de la Loire. The sec-
ond author thanks the Spanish CICyT for partial
support of this work through projects TIN2009-
14205-C04-C1, and the program CONSOLIDER
INGENIO 2010 (CSD2007-00018).

35



References

S. Ben-David, B. Chor, O. Goldreich, and
M. Luby. 1992. On the theory of average case
complexity. Journal of Computer and System
Sciences, 44(2):193—-219.

V. D. Blondel and V. Canterini. 2003. Unde-
cidable problems for probabilistic automata of
fixed dimension. Theory of Computer Systems,
36(3):231–245.

F. Casacuberta and C. de la Higuera. 1999. Op-
timal linguistic decoding is a difficult compu-
tational problem. Pattern Recognition Letters,
20(8):813–821.

F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on proba-
bilistic grammars and transducers. In Proceed-
ings of ICGI 2000, volume 1891 of LNAI, pages
15–24. Springer-Verlag.

F. Casacuberta. 1995. Probabilistic estimation
of stochastic regular syntax-directed translation
schemes. In R. Moreno, editor, VI Spanish Sym-
posium on Pattern Recognition and Image Anal-
ysis, pages 201–297. AERFAI.

C. Cortes, M. Mohri, and A. Rastogi. 2006. On
the computation of some standard distances be-
tween probabilistic automata. In Proceedings
of CIAA 2006, volume 4094 of LNCS, pages
137–149. Springer-Verlag.

C. Cortes, M. Mohri, and A. Rastogi. 2007. lp dis-
tance and equivalence of probabilistic automata.
International Journal of Foundations of Com-
puter Science, 18(4):761–779.

C. de la Higuera. 1997. Characteristic sets for
polynomial grammatical inference. Machine
Learning Journal, 27:125–138.

M. R. Garey and D. S. Johnson. 1979. Computers
and Intractability. Freeman.

Joshua T. Goodman. 1998. Parsing Inside–Out.
Ph.D. thesis, Harvard University.

F. Jelinek. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.

L. Levin. 1986. Average case complete problems.
SIAM Journal on Computing, 15(1):285–286.

R. B. Lyngsø and C. N. S. Pedersen. 2002. The
consensus string problem and the complexity of
comparing hidden markov models. Journal of
Computing and System Science, 65(3):545–569.

M. Mohri, F. C. N. Pereira, and M. Riley. 2000.
The design principles of a weighted finite-state
transducer library. Theoretical Computer Sci-
ence, 231(1):17–32.

M. Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational
Linguistics, 23(3):269–311.

A. Paz. 1971. Introduction to probabilistic au-
tomata. Academic Press, New York.

D. Picó and F. Casacuberta. 2001. Some
statistical-estimation methods for stochastic
finite-state transducers. Machine Learning
Journal, 44(1):121–141.

L. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech
recoginition. Proceedings of the IEEE, 77:257–
286.

K. Sima’an. 1996. Computational complexity of
probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175–1180.

A. Stolcke. 1995. An efficient probablistic
context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165–201.

E. Vidal, F. Thollard, C. de la Higuera, F. Casacu-
berta, and R. C. Carrasco. 2005. Prob-
abilistic finite state automata – part I and
II. Pattern Analysis and Machine Intelligence,
27(7):1013–1039.

36


