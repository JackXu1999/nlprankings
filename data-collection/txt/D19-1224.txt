




































Show Your Work: Improved Reporting of Experimental Results


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2185–2194,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2185

Show Your Work: Improved Reporting of Experimental Results

Jesse Dodge♣ Suchin Gururangan♦ Dallas Card♥ Roy Schwartz♠♦ Noah A. Smith♠♦
♣Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA

♦Allen Institute for Artificial Intelligence, Seattle, WA, USA
♥Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA

♠Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA
{jessed,dcard}@cs.cmu.edu {suching,roys,noah}@allenai.org

Abstract

Research in natural language processing pro-
ceeds, in part, by demonstrating that new mod-
els achieve superior performance (e.g., accu-
racy) on held-out test data, compared to pre-
vious results. In this paper, we demonstrate
that test-set performance scores alone are in-
sufficient for drawing accurate conclusions
about which model performs best. We argue
for reporting additional details, especially per-
formance on validation data obtained during
model development. We present a novel tech-
nique for doing so: expected validation per-
formance of the best-found model as a func-
tion of computation budget (i.e., the number
of hyperparameter search trials or the overall
training time). Using our approach, we find
multiple recent model comparisons where au-
thors would have reached a different conclu-
sion if they had used more (or less) compu-
tation. Our approach also allows us to esti-
mate the amount of computation required to
obtain a given accuracy; applying it to sev-
eral recently published results yields massive
variation across papers, from hours to weeks.
We conclude with a set of best practices for
reporting experimental results which allow for
robust future comparison, and provide code to
allow researchers to use our technique.1

1 Introduction

In NLP and machine learning, improved perfor-
mance on held-out test data is typically used as
an indication of the superiority of one method
over others. But, as the field grows, there is
an increasing gap between the large computa-
tional budgets used for some high-profile exper-
iments and the budgets used in most other work
(Schwartz et al., 2019). This hinders meaning-
ful comparison between experiments, as improve-
ments in performance can, in some cases, be ob-

1https://github.com/allenai/allentune

(Budget,    [accuracy])

Budget that  
favors LR

Budget that  
favors CNN

39.8
32.0
38.8
31.1
39.5

…

LR val. accuracy CNN val. accuracy 

38.9
26.1
26.4
40.5
36.1

…

current practice:

max

max

report corresponding test-set accuracies

H
yp

er
pa

ra
m

et
er

 
as

si
gn

m
en

ts

Figure 1: Current practice when compraing NLP mod-
els is to train multiple instantiations of each, choose
the best model of each type based on validation per-
formance, and compare their performance on test data
(inner box). Under this setup, (assuming test-set re-
sults are similar to validation), one would conclude
from the results above (hyperparameter search for two
models on the 5-way SST classification task) that the
CNN outperforms Logistic Regression (LR). In our
proposed evaluation framework, we instead encourage
practitioners to consider the expected validation accu-
racy (y-axis; shading shows ±1 standard deviation), as
a function of budget (x-axis). Each point on a curve
is the expected value of the best validation accuracy
obtained (y) after evaluating x random hyperparame-
ter values. Note that (1) the better performing model
depends on the computational budget; LR has higher
expected performance for budgets up to 10 hyperpa-
rameter assignments, while the CNN is better for larger
budgets. (2) Given a model and desired accuracy (e.g.,
0.395 for CNN), we can estimate the expected budget
required to reach it (16; dotted lines).



2186

tained purely through more intensive hyperparam-
eter tuning (Melis et al., 2018; Lipton and Stein-
hardt, 2018).2

Moreover, recent investigations into “state-of-
the-art” claims have found competing methods
to only be comparable, without clear superiority,
even against baselines (Reimers and Gurevych,
2017; Lucic et al., 2018; Li and Talwalkar, 2019);
this has exposed the need for reporting more than
a single point estimate of performance.

Echoing calls for more rigorous scientific prac-
tice in machine learning (Lipton and Steinhardt,
2018; Sculley et al., 2018), we draw attention to
the weaknesses in current reporting practices and
propose solutions which would allow for fairer
comparisons and improved reproducibility.

Our primary technical contribution is the intro-
duction of a tool for reporting validation results
in an easily interpretable way: expected valida-
tion performance of the best model under a given
computational budget.3 That is, given a budget
sufficient for training and evaluating n models,
we calculate the expected performance of the best
of these models on validation data. Note that
this differs from the best observed value after n
evaluations. Because the expectation can be esti-
mated from the distribution of N validation per-
formance values, with N ≥ n, and these are ob-
tained during model development,4 our method
does not require additional computation be-
yond hyperparameter search or optimization. We
encourage researchers to report expected valida-
tion performance as a curve, across values of n ∈
{1, . . . , N}.

As we show in §4.3, our approach makes clear
that the expected-best performing model is a func-
tion of the computational budget. In §4.4 we
show how our approach can be used to estimate
the budget that went into obtaining previous re-
sults; in one example, we see a too-small bud-
get for baselines, while in another we estimate a
budget of about 18 GPU days was used (but not
reported). Previous work on reporting validation
performance used the bootstrap to approximate the
mean and variance of the best performing model
(Lucic et al., 2018); in §3.2 we show that our ap-

2Recent work has also called attention to the environmen-
tal cost of intensive model exploration (Strubell et al., 2019).

3We use the term performance as a general evaluation
measure, e.g., accuracy, F1, etc.

4We leave forecasting performance with larger budgets
n > N to future work.

proach computes these values with strictly less er-
ror than the bootstrap.

We conclude by presenting a set of recommen-
dations for researchers that will improve scientific
reporting over current practice. We emphasize
this work is about reporting, not about running
additional experiments (which undoubtedly can
improve evidence in comparisons among mod-
els). Our reporting recommendations aim at repro-
ducibility and improved understanding of sensitiv-
ity to hyperparameters and random initializations.
Some of our recommendations may seem obvious;
however, our empirical analysis shows that out of
fifty EMNLP 2018 papers chosen at random, none
report all items we suggest.

2 Background

Reproducibility Reproducibility in machine
learning is often defined as the ability to pro-
duce the exact same results as reported by the
developers of the model. In this work, we follow
Gundersen and Kjensmo (2018) and use an
extended notion of this concept: when comparing
two methods, two research groups with different
implementations should follow an experimental
procedure which leads to the same conclusion
about which performs better. As illustrated in
Fig. 1, this conclusion often depends on the
amount of computation applied. Thus, to make a
reproducible claim about which model performs
best, we must also take into account the budget
used (e.g., the number of hyperparameter trials).

Notation We use the term model family to refer
to an approach subject to comparison and to hyper-
parameter selection.5 Each model family M re-
quires its own hyperparameter selection, in terms
of a set of k hypermarameters, each of which de-
fines a range of possible values. A hyperparame-
ter value (denoted h) is a k-tuple of specific val-
ues for each hyperparameter. We call the set of all
possible hyperparameter valuesHM.6 GivenHM
and a computational budget sufficient for train-
ing B models, the set of hyperparameter values is
{h1, . . . , hB}, hi ∈ HM. We let mi ∈ M denote
the model trained with hyperparameter value hi.

5Examples include different architectures, but also abla-
tions of the same architecture.

6The hyperparameter value space can also include the ran-
dom seed used to initialize the model, and some specifica-
tions such as the size of the hidden layers in a neural network,
in addition to commonly tuned values such as learning rate.



2187

Hyperparameter value selection There are
many ways of selecting hyperparameter values,
hi. Grid search and uniform sampling are popular
systematic methods; the latter has been shown to
be superior for most search spaces (Bergstra and
Bengio, 2012). Adaptive search strategies such
as Bayesian optimization select hi after evaluat-
ing h1, . . . , hi−1. While these strategies may find
better results quickly, they are generally less repro-
ducible and harder to parallelize (Li et al., 2017).
Manual search, where practitioners use knowl-
edge derived from previous experience to adjust
hyperparameters after each experiment, is a type
of adaptive search that is the least reproducible,
as different practitioners make different decisions.
Regardless of the strategy adopted, we advocate
for detailed reporting of the method used for hy-
perparmeter value selection (§5). We next intro-
duce a technique to visualize results of samples
which are drawn i.i.d. (e.g., random initializations
or uniformly sampled hyperparameter values).

3 Expected Validation Performance
Given Budget

After selecting the best hyperparameter values hi∗
from among {h1, . . . , hB} with actual budget B,
NLP researchers typically evaluate the associated
model mi∗ on the test set and report its perfor-
mance as an estimate of the family M’s ability
to generalize to new data. We propose to make
better use of the intermediately-trained models
m1, . . . ,mB .

For any set of n hyperparmeter values, denote
the validation performance of the best model as

v∗n = maxh∈{h1,...,hn}A(M, h,DT ,DV ), (1)

whereA denotes an algorithm that returns the per-
formance on validation data DV after training a
model from family M with hyperparameter val-
ues h on training data DT .7 We view evaluations
of A as the elementary unit of experimental cost.8

Though not often done in practice, procedure
(1) could be repeated many times with different
hyperparameter values, yielding a distribution of
values for random variable V ∗n . This would allow
us to estimate the expected performance, E[V ∗n |
n] (given n hyperparameter configurations). The

7A captures standard parameter estimation, as well as pro-
cedures that depend on validation data, like early stopping.

8Note that researchers do not always report validation, but
rather test performance, a point we will return to in §5.

key insight used below is that, if we use random
search for hyperparameter selection, then the ef-
fort that goes into a single round of random search
(Eq. 1) suffices to construct a useful estimate of
expected validation performance, without requir-
ing any further experimentation.

Under random search, the n hyperparameter
values h1, . . . , hn are drawn uniformly at random
fromHM, so the values ofA(M, hi,DT ,DV ) are
i.i.d. As a result, the maximum among these is it-
self a random variable. We introduce a diagnostic
that captures information about the computation
used to generate a result: the expectation of max-
imum performance, conditioned on n, the amount
of computation used in the maximization over hy-
perparameters and random initializations:

E
[
maxh∈{h1,...,hn}A(M, h,DT ,DV ) | n

]
. (2)

Reporting this expectation as we vary n ∈
{1, 2, . . . , B} gives more information than the
maximum v∗B (Eq. 1 with n = B); future re-
searchers who use this model will know more
about the computation budget required to achieve
a given performance. We turn to calculating this
expectation, then we compare it to the bootstrap
(§3.2), and discuss estimating variance (§3.3).

3.1 Expected Maximum
We describe how to estimate the expected maxi-
mum validation performance (Eq. 2) given a bud-
get of n hyperparameter values.9

Assume we draw {h1, . . . , hn} uniformly at
random from hyperparameter space HM. Each
evaluation of A(M, h,DT ,DV ) is therefore an
i.i.d. draw of a random variable, denoted Vi, with
observed value vi for hi ∼ HM. Let the maxi-
mum among n i.i.d. draws from an unknown dis-
tribution be

V ∗n = maxi∈{1,...,n} Vi (3)

We seek the expected value of V ∗n given n:

E[V ∗n | n] =
∑

v v · P (V ∗n = v | n) (4)

where P (V ∗n | n) is the probability mass function
(PMF) for the max-random variable.10 For dis-

9Conversion to alternate formulations of budget, such
as GPU hours or cloud-machine rental cost in dollars, is
straightforward in most cases.

10For a finite validation set DV , most performance mea-
sures (e.g., accuracy) only take on a finite number of possible
values, hence the use of a sum instead of an integral in Eq. 4.



2188

crete random variables,

P (V ∗n = v | n) = P (V ∗n ≤ v | n)− P (V ∗n < v | n),
(5)

Using the definition of “max”, and the fact that
the Vi are drawn i.i.d.,

P (V ∗n ≤ v | n) = P
(
maxi∈{1,...,n} Vi ≤ v | n

)
= P (V1 ≤ v, V2 ≤ v, . . . , Vn ≤ v | n)
=
∏n

i=1 P (Vi ≤ v) = P (V ≤ v)n, (6)

and similarly for P (V ∗n < v | n).
P (V ≤ v) and P (V < v) are cumulative distri-

bution functions, which we can estimate using the
empirical distribution, i.e.

P̂ (V ≤ v) = 1n
∑n

i=1 1[Vi≤v] (7)

and similarly for strict inequality.
Thus, our estimate of the expected maximum

validation performance is

Ê[V ∗n | n] =
∑

v v · (P̂ (Vi ≤ v)n − P̂ (Vi < v)n).
(8)

Discussion As we increase the amount of com-
putation for evaluating hyperparameter values (n),
the maximum among the samples will approach
the observed maximum v∗B . Hence the curve of
E[V ∗n | n] as a function of n will appear to asymp-
tote. Our focus here is not on estimating that value,
and we do not make any claims about extrapola-
tion of V ∗ beyond B, the number of hyperparam-
eter values to which A is actually applied.

Two points follow immediately from our deriva-
tion. First, at n = 1, E[V ∗1 | n = 1] is the mean of
v1, . . . , vn. Second, for all n, E[V ∗n | n] ≤ v∗n =
maxi vi, which means the curve is a lower bound
on the selected model’s validation performance.

3.2 Comparison with Bootstrap
Lucic et al. (2018) and Henderson et al. (2018)
have advocated for using the bootstrap to estimate
the mean and variance of the best validation per-
formance. The bootstrap (Efron and Tibshirani,
1994) is a general method which can be used to
estimate statistics that do not have a closed form.
The bootstrap process is as follows: draw N i.i.d.
samples (in our case, N model evaluations). From
these N points, sample n points (with replace-
ment), and compute the statistic of interest (e.g.,
the max). Do this K times (where K is large), and

average the computed statistic. By the law of large
numbers, as K → ∞ this average converges to
the sample expected value (Efron and Tibshirani,
1994).

The bootstrap has two sources of error: the error
from the finite sample of N points, and the error
introduced by resampling these points K times.
Our approach has strictly less error than using the
bootstrap: our calculation of the expected maxi-
mum performance in §3.1 provides a closed-form
solution, and thus contains none of the resampling
error (the finite sample error is the same).

3.3 Variance of V ∗n
Expected performance becomes more useful with
an estimate of variation. When using the boot-
strap, standard practice is to report the standard
deviation of the estimates from the K resamples.
As K →∞, this standard deviation approximates
the sample standard error (Efron and Tibshirani,
1994). We instead calculate this from the distribu-
tion in Eq. 5 using the standard plug-in-estimator.

In most cases, we advocate for reporting a mea-
sure of variability such as the standard deviation
or variance; however, in some cases it might cause
confusion. For example, when the variance is
large, plotting the expected value plus the variance
can go outside of reasonable bounds, such as accu-
racy greater than any observed (even greater than
1). In such situations, we recommend shading
only values within the observed range, such as in
Fig. 4. Additionally, in situations where the vari-
ance is high and variance bands overlap between
model families (e.g., Fig. 1), the mean is still the
most informative statistic.

4 Case Studies

Here we show two clear use cases of our method.
First, we can directly estimate, for a given budget,
which approach has better performance. Second,
we can estimate, given our experimental setup, the
budget for which the reported validation perfor-
mance (V ∗) matches a desired performance level.
We present three examples that demonstrate these
use cases. First, we reproduce previous findings
that compared different models for text classifi-
cation. Second, we explore the time vs. perfor-
mance tradeoff of models that use contextual word
embeddings (Peters et al., 2018). Third, from
two previously published papers, we examine the
budget required for our expected performance to



2189

match their reported performance. We find these
budget estimates vary drastically. Consistently, we
see that the best model is a function of the budget.
We publicly release the search space and training
configurations used for each case study. 11

Note that we do not report test performance in
our experiments, as our purpose is not to establish
a benchmark level for a model, but to demonstrate
the utility of expected validation performance for
model comparison and reproducibility.

4.1 Experimental Details

For each experiment, we document the hyperpa-
rameter search space, hardware, average runtime,
number of samples, and links to model imple-
mentations. We use public implementations for
all models in our experiments, primarily in Al-
lenNLP (Gardner et al., 2018). We use Tune (Liaw
et al., 2018) to run parallel evaluations of uni-
formly sampled hyperparameter values.

4.2 Validating Previous Findings

We start by applying our technique on a text classi-
fication task in order to confirm a well-established
observation (Yogatama and Smith, 2015): logistic
regression has reasonable performance with mini-
mal hyperparameter tuning, but a well-tuned con-
volutional neural network (CNN) can perform bet-
ter.

We experiment with the fine-grained Stan-
ford Sentiment Treebank text classification dataset
(Socher et al., 2013). For the CNN classifier, we
embed the text with 50-dim GloVe vectors (Pen-
nington et al., 2014), feed the vectors to a Con-
vNet encoder, and feed the output representation
into a softmax classification layer. We use the
scikit-learn implementation of logistic regression
with bag-of-word counts and a linear classification
layer. The hyperparameter spaces HCNN and HLR
are detailed in Appendix B. For logistic regres-
sion we used bounds suggested by Yogatama and
Smith (2015), which include term weighting, n-
grams, stopwords, and learning rate. For the CNN
we follow the hyperparameter sensitivity analysis
in Zhang and Wallace (2015).

We run 50 trials of random hyperparameter
search for each classifier. Our results (Fig. 1) con-
firm previous findings (Zhang and Wallace, 2015):
under a budget of fewer than 10 hyperparameter

11https://github.com/allenai/
show-your-work

30min 1h 6h 1d 3d 10d
Training duration

0.76

0.78

0.80

0.82

0.84

0.86

0.88

0.90

0.92

E
xp

ec
te

d 
va

lid
at

io
n 

ac
cu

ra
cy

SST (binary)

GloVe + ELMo (FT)
GloVe + ELMo (FR)
GloVe

Figure 2: Expected maximum performance of a BCN
classifier on SST. We compare three embedding ap-
proaches (GloVe embeddings, GloVe + frozen ELMo,
and GloVe + fine-tuned ELMo). The x-axis is time, on
a log scale. We omit the variance for visual clarity. For
each of the three model families, we sampled 50 hy-
perparameter values, and plot the expected maximum
performance with the x-axis values scaled by the aver-
age training duration. The plot shows that for each ap-
proach (GloVe, ELMo frozen, and ELMo fine-tuned),
there exists a budget for which it is preferable.

search trials, logistic regression achieves a higher
expected validation accuracy than the CNN. As
the budget increases, the CNN gradually improves
to a higher overall expected validation accuracy.
For all budgets, logistic regression has lower vari-
ance, so may be a more suitable approach for fast
prototyping.

4.3 Contextual Representations

We next explore how computational budget affects
the performance of contextual embedding models
(Peters et al., 2018). Recently, Peters et al. (2019)
compared two methods for using contextual rep-
resentations for downstream tasks: feature extrac-
tion, where features are fixed after pretraining and
passed into a task-specific model, or fine-tuning,
where they are updated during task training. Pe-
ters et al. (2019) found that feature extraction is
preferable to fine-tuning ELMo embeddings. Here
we set to explore whether this conclusion depends
on the experimental budget.

Closely following their experimental setup, in
Fig. 2 we show the expected performance of the
biattentive classification network (BCN; McCann
et al., 2017) with three embedding approaches
(GloVe only, GloVe + ELMo frozen, and GloVe



2190

+ ELMo fine-tuned), on the binary Stanford Sen-
timent Treebank task.12

We use time for the budget by scaling the
curves by the average observed training duration
for each model. We observe that as the time bud-
get increases, the expected best-performing model
changes. In particular, we find that our experi-
mental setup leads to the same conclusion as Pe-
ters et al. (2019) given a budget between approxi-
mately 6 hours and 1 day. For larger budgets (e.g.,
10 days) fine-tuning outperforms feature extrac-
tion. Moreover, for smaller budgets (< 2 hours),
using GloVe embeddings is preferable to ELMo
(frozen or fine-tuned).

4.4 Inferring Budgets in Previous Reports

Our method provides another appealing property:
estimating the budget required for the expected
performance to reach a particular level, which we
can compare against previously reported results.
We present two case studies, and show that the
amount of computation required to match the re-
ported results varies drastically.

We note that in the two examples that follow,
the original papers only reported partial experi-
mental information; we made sure to tune the hy-
perparameters they did list in addition to standard
choices (such as the learning rate). In neither case
do they report the method used to tune the hyper-
parameters, and we suspect they tuned them man-
ually. Our experiments here are meant give an
idea of the budget that would be required to re-
produce their results or to apply their models to
other datasets under random hyperparameter value
selection.

SciTail When introducing the SciTail textual en-
tailment dataset, Khot et al. (2018) compared
four models: an n-gram baseline, which mea-
sures word-overlap as an indicator of entailment,
ESIM (Chen et al., 2017), a sequence-based en-
tailment model, DAM (Parikh et al., 2016), a bag-
of-words entailment model, and their proposed
model, DGEM (Khot et al., 2018), a graph-based
structured entailment model. Their conclusion
was that DGEM outperforms the other models.

12Peters et al. (2019) use a BCN with frozen embeddings
and a BiLSTM BCN for fine-tuning. We conducted experi-
ments with both a BCN and a BiLSTM with frozen and fine-
tuned embeddings, and found our conclusions to be consis-
tent. We report the full hyperparameter search space, which
matched Peters et al. (2019) as closely as their reporting al-
lowed, in Appendix C.

5 10 50 100
Hyperparameter assignments

0.625

0.650

0.675

0.700

0.725

0.750

0.775

0.800

0.825

E
xp

ec
te

d 
va

lid
at

io
n 

ac
cu

ra
cy

reported DGEM accuracy

reported DAM accuracy

reported ESIM accuracy

reported n-gram baseline accuracy

SciTail

DGEM
DAM
ESIM
n-gram baseline

Figure 3: Comparing reported accuracies (dashed
lines) on SciTail to expected validation performance
under varying levels of compute (solid lines). The es-
timated budget required for expected performance to
match the reported result differs substantially across
models, and the relative ordering varies with budget.
We omit variance for visual clarity.

We use the same implementations of each of
these models each with a hyperparameter search
space detailed in Appendix D.13 We use a budget
based on trials instead of runtime so as to empha-
size how these models behave when given a com-
parable number of hyperparameter configurations.

13The search space bounds we use are large neighborhoods
around the hyperparameter assignments specified in the pub-
lic implementations of these models. Note that these curves
depend on the specific hyperparameter search space adopted;
as the original paper does not report hyperparameter search or
model selection details, we have chosen what we believe to
be reasonable bounds, and acknowledge that different choices
could result in better or worse expected performance.



2191

8h 1d 3d 10d 18d 1mo
Training duration

0.1

0.2

0.3

0.4

0.5

0.6

0.7
E

xp
ec

te
d 

va
lid

at
io

n 
E

M
reported BIDAF EM

SQuAD

BIDAF

Figure 4: Comparing reported development exact-
match score of BIDAF (dashed line) on SQuAD to
expected performance of the best model with varying
computational budgets (solid line). The shaded area
represents the expected performance ±1 standard de-
viation, within the observed range of values. It takes
about 18 days (55 hyperparameter trials) for the ex-
pected performance to match the reported results.

Our results (Fig. 3) show that the different mod-
els require different budgets to reach their reported
performance in expectation, ranging from 2 (n-
gram) to 20 (DGEM). Moreover, providing a large
budget for each approach improves performance
substantially over reported numbers. Finally, un-
der different computation budgets, the top per-
forming model changes (though the neural models
are similar).

SQuAD Next, we turn our attention to SQuAD
(Rajpurkar et al., 2016) and report performance
of the commonly-used BiDAF model (Seo et al.,
2017). The set of hyperparameters we tune covers
those mentioned in addition to standard choices
(details in Appendix D). We see in Fig. 4 that
we require a budget of 18 GPU days in order
for the expected maximum validation performance
to match the value reported in the original paper.
This suggests that some combination of prior in-
tuition and extensive hyperparameter tuning were
used by the original authors, though neither were
reported.

X For all reported experimental results

� Description of computing infrastructure
� Average runtime for each approach
� Details of train/validation/test splits
� Corresponding validation performance for each

reported test result
� A link to implemented code

X For experiments with hyperparameter search

� Bounds for each hyperparameter
� Hyperparameter configurations for best-

performing models
� Number of hyperparameter search trials
� The method of choosing hyperparameter values

(e.g., uniform sampling, manual tuning, etc.) and
the criterion used to select among them (e.g., ac-
curacy)

� Expected validation performance, as introduced
in §3.1, or another measure of the mean and vari-
ance as a function of the number of hyperparam-
eter trials.

Text Box 1: Experimental results checklist.

5 Recommendations

Experimental results checklist The findings
discussed in this paper and other similar efforts
highlight methodological problems in experimen-
tal NLP. In this section we provide a checklist to
encourage researchers to report more comprehen-
sive experimentation results. Our list, shown in
Text Box 1, builds on the reproducibility checklist
that was introduced for the machine learning com-
munity during NeurIPS 2018 (which is required to
be filled out for each NeurIPS 2019 submission;
Pineau, 2019).

Our focus is on improved reporting of exper-
imental results, thus we include relevant points
from their list in addition to our own. Similar
to other calls for improved reporting in machine
learning (Mitchell et al., 2019; Gebru et al., 2018),
we recommend pairing experimental results with
the information from this checklist in a structured
format (see examples provided in Appendix A).

EMNLP 2018 checklist coverage. To estimate
how commonly this information is reported in the
NLP community, we sample fifty random EMNLP
2018 papers that include experimental results and
evaluate how well they conform to our proposed
reporting guidelines. We find that none of the
papers reported all of the items in our checklist.
However, every paper reported at least one item
in the checklist, and each item is reported by at



2192

least one paper. Of the papers we analyzed, 74%
reported at least some of the best hyperparameter
assignments. By contrast, 10% or fewer papers re-
ported hyperparameter search bounds, the number
of hyperparameter evaluation trials, or measures of
central tendency and variation. We include the full
results of this analysis in Table 1 in the Appendix.

Comparisons with different budgets. We have
argued that claims about relative model perfor-
mance should be qualified by computational ex-
pense. With varying amounts of computation, not
all claims about superiority are valid. If two mod-
els have similar budgets, we can claim one outper-
forms the other (with that budget). Similarly, if a
model with a small budget outperforms a model
with a large budget, increasing the small budget
will not change this conclusion. However, if a
model with a large budget outperforms a model
with a small budget, the difference might be due
to the model or the budget (or both). As a con-
crete example, Melis et al. (2018) report the per-
formance of an LSTM on language modeling the
Penn Treebank after 1,500 rounds of Bayesian op-
timization; if we compare to a new M with a
smaller budget, we can only draw a conclusion if
the new model outperforms the LSTM.14

In a larger sense, there may be no simple way
to make a comparison “fair.” For example, the two
models in Fig. 1 have hyperparameter spaces that
are different, so fixing the same number of hy-
perparameter trials for both models does not im-
ply a fair comparison. In practice, it is often not
possible to measure how much past human experi-
ence has contributed to reducing the hyperparame-
ter bounds for popular models, and there might not
be a way to account for the fact that better under-
stood (or more common) models can have better
spaces to optimize over. Further, the cost of one
application of A might be quite different depend-
ing on the model family. Converting to runtime
is one possible solution, but implementation effort
could still affect comparisons at a fixed x-value.
Because of these considerations, our focus is on
reporting whatever experimental results exist.

6 Discussion: Reproducibility

In NLP, the use of standardized test sets and pub-
lic leaderboards (which limit test evaluations) has

14This is similar to controlling for the amount of training
data, which is an established norm in NLP research.

helped to mitigate the so-called “replication cri-
sis” happening in fields such as psychology and
medicine (Ioannidis, 2005; Gelman and Loken,
2014). Unfortunately, leaderboards can create
additional reproducibility issues (Rogers, 2019).
First, leaderboards obscure the budget that was
used to tune hyperparameters, and thus the amount
of work required to apply a model to a new dataset.
Second, comparing to a model on a leaderboard is
difficult if they only report test scores. For exam-
ple, on the GLUE benchmark (Wang et al., 2018),
the differences in test set performance between the
top performing models can be on the order of a
tenth of a percent, while the difference between
test and validation performance might be one per-
cent or larger. Verifying that a new implemen-
tation matches established performance requires
submitting to the leaderboard, wasting test eval-
uations. Thus, we recommend leaderboards re-
port validation performance for models evaluated
on test sets.

As an example, consider Devlin et al. (2019),
which introduced BERT and reported state-of-the-
art results on the GLUE benchmark. The authors
provide some details about the experimental setup,
but do not report a specific budget. Subsequent
work which extended BERT (Phang et al., 2018)
included distributions of validation results, and we
highlight this as a positive example of how to re-
port experimental results. To achieve comparable
test performance to Devlin et al. (2019), the au-
thors report the best of twenty or one hundred ran-
dom initializations. Their validation performance
reporting not only illuminates the budget required
to fine-tune BERT on such tasks, but also gives
other practitioners results against which they can
compare without submitting to the leaderboard.

7 Related Work

Lipton and Steinhardt (2018) address a number of
problems with the practice of machine learning,
including incorrectly attributing empirical gains
to modeling choices when they came from other
sources such as hyperparameter tuning. Sculley
et al. (2018) list examples of similar evaluation is-
sues, and suggest encouraging stronger standards
for empirical evaluation. They recommend de-
tailing experimental results found throughout the
research process in a time-stamped document, as
is done in other experimental science fields. Our
work formalizes these issues and provides an ac-



2193

tionable set of recommendations to address them.
Reproducibility issues relating to standard data

splits (Schwartz et al., 2011; Gorman and Bedrick,
2019; Recht et al., 2019a,b) have surfaced in a
number of areas. Shuffling standard training, val-
idation, and test set splits led to a drop in perfor-
mance, and in a number of cases the inability to
reproduce rankings of models. Dror et al. (2017)
studied reproducibility in the context of consis-
tency among multiple comparisons.

Limited community standards exist for docu-
menting datasets and models. To address this, Ge-
bru et al. (2018) recommend pairing new datasets
with a “datasheet” which includes information
such as how the data was collected, how it was
cleaned, and the motivation behind building the
dataset. Similarly, Mitchell et al. (2019) advocate
for including a “model card” with trained mod-
els which document training data, model assump-
tions, and intended use, among other things. Our
recommendations in §5 are meant to document rel-
evant information for experimental results.

8 Conclusion

We have shown how current practice in experi-
mental NLP fails to support a simple standard of
reproducibility. We introduce a new technique for
estimating the expected validation performance of
a method, as a function of computation budget,
and present a set of recommendations for report-
ing experimental findings.

Acknowledgments

This work was completed while the first author
was an intern at the Allen Institute for Artificial
Intelligence. The authors thank Kevin Jamieson,
Samuel Ainsworth, and the anonymous reviewers
for helpful feedback.

References
James Bergstra and Yoshua Bengio. 2012. Random

search for hyper-parameter optimization. JMLR,
13:281–305.

Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017. Enhanced
LSTM for natural language inference. In Proc. of
ACL.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proc. of NAACL.

Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi
Reichart. 2017. Replicability analysis for natural
language processing: Testing significance with mul-
tiple datasets. TACL, 5:471–486.

Bradley Efron and Robert Tibshirani. 1994. An Intro-
duction to the Bootstrap. CRC Press.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E.
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2018. AllenNLP: A deep semantic natural language
processing platform. In Proc. of NLP-OSS.

Timnit Gebru, Jamie H. Morgenstern, Briana Vec-
chione, Jennifer Wortman Vaughan, Hanna M.
Wallach, Hal Daumé, and Kate Crawford. 2018.
Datasheets for datasets. arXiv:1803.09010.

Andrew Gelman and Eric Loken. 2014. The statistical
crisis in science. American Scientist, 102:460.

Kyle Gorman and Steven Bedrick. 2019. We need to
talk about standard splits. In Proc. of ACL.

Odd Erik Gundersen and Sigbjrn Kjensmo. 2018. State
of the art: Reproducibility in artificial intelligence.
In Proc. of AAAI.

Peter Henderson, Riashat Islam, Philip Bachman,
Joelle Pineau, Doina Precup, and David Meger.
2018. Deep reinforcement learning that matters. In
Proc. of AAAI.

John P. A. Ioannidis. 2005. Why most published re-
search findings are false. PLoS Med, 2(8).

Tushar Khot, Ashutosh Sabharwal, and Peter Clark.
2018. SciTaiL: A textual entailment dataset from
science question answering. In Proc. of AAAI.

Liam Li and Ameet Talwalkar. 2019. Random search
and reproducibility for neural architecture search. In
Proc. of UAI.

Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Ros-
tamizadeh, and Ameet Talwalkar. 2017. Hyperband:
Bandit-based configuration evaluation for hyperpa-
rameter optimization. In Proc. of ICLR.

Richard Liaw, Eric Liang, Robert Nishihara, Philipp
Moritz, Joseph E Gonzalez, and Ion Stoica. 2018.
Tune: A research platform for distributed model se-
lection and training. In Proc. of the ICML Workshop
on AutoML.

Zachary C. Lipton and Jacob Steinhardt. 2018.
Troubling trends in machine learning scholarship.
arXiv:1807.03341.

Mario Lucic, Karol Kurach, Marcin Michalski, Olivier
Bousquet, and Sylvain Gelly. 2018. Are GANs
created equal? A large-scale study. In Proc. of
NeurIPS.



2194

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Proc. of NeurIPS.

Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On
the state of the art of evaluation in neural language
models. In Proc. of EMNLP.

Margaret Mitchell, Simone Wu, Andrew Zaldivar,
Parker Barnes, Lucy Vasserman, Ben Hutchinson,
Elena Spitzer, Inioluwa Deborah Raji, and Timnit
Gebru. 2019. Model cards for model reporting. In
Proc. of FAT*.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proc. of
EMNLP.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proc. of EMNLP.

Matthew Peters, Sebastian Ruder, and Noah A. Smith.
2019. To tune or not to tune? Adapting pretrained
representations to diverse tasks. In Proc. of the
RepL4NLP Workshop at ACL.

Matthew E. Peters, Mark Neumann, Mohit Iyyer,
Matt Gardner, Christopher Clark, Kenton Lee, and
Luke S. Zettlemoyer. 2018. Deep contextualized
word representations. In Proc. of NAACL.

Jason Phang, Thibault Févry, and Samuel R. Bow-
man. 2018. Sentence encoders on STILTs: Supple-
mentary training on intermediate labeled-data tasks.
arXiv:1811.01088.

Joelle Pineau. 2019. Machine learning reproducibility
checklist. https://www.cs.mcgill.ca/
˜jpineau/ReproducibilityChecklist.
pdf. Accessed: 2019-5-14.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions
for machine comprehension of text. In Proc. of
EMNLP.

Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt,
and Vaishaal Shankar. 2019a. Do CIFAR-10 classi-
fiers generalize to CIFAR-10? arXiv:1806.00451.

Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt,
and Vaishaal Shankar. 2019b. Do ImageNet classi-
fiers generalize to ImageNet? In Proc. of ICML.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of LSTM-networks for sequence tagging. In
Proc. of EMNLP.

Anna Rogers. 2019. How the transformers broke NLP
leaderboards. https://hackingsemantics.
xyz/2019/leaderboards/. Accessed: 2019-
8-29.

Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proc. of ACL.

Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren
Etzioni. 2019. Green AI. arXiv:1907.10597.

D. Sculley, Jasper Snoek, Ali Rahimi, and Alex
Wiltschko. 2018. Winner’s curse? On pace,
progress, and empirical rigor. In Proc. of ICLR
(Workshop Track).

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proc. of ICLR.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. of EMNLP.

Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. 2019. Energy and policy considerations for
deep learning in NLP. In Proc. of ACL.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2018.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Proc.
of ICLR.

Dani Yogatama and Noah A. Smith. 2015. Bayesian
optimization of text representations. In Proc. of
EMNLP.

Ye Zhang and Byron Wallace. 2015. A sensitivity
analysis of (and practitioners’ guide to) convolu-
tional neural networks for sentence classification.
arXiv:1510.03820.


