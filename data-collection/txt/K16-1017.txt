



















































Modelling Context with User Embeddings for Sarcasm Detection in Social Media


Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 167–177,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Modelling Context with User Embeddings
for Sarcasm Detection in Social Media

Silvio Amir Byron C. Wallace† Hao Lyu† Paula Carvalho Mário J. Silva
INESC-ID Lisboa, Instituto Superior Técnico, Universidade de Lisboa

†iSchool, University of Texas at Austin
samir@inesc-id.pt byron.wallace@utexas.edu xalh8083@gmail.com

pcc@inesc-id.pt mjs@inesc-id.pt

Abstract

We introduce a deep neural network for
automated sarcasm detection. Recent
work has emphasized the need for mod-
els to capitalize on contextual features, be-
yond lexical and syntactic cues present in
utterances. For example, different speak-
ers will tend to employ sarcasm regard-
ing different subjects and, thus, sarcasm
detection models ought to encode such
speaker information. Current methods
have achieved this by way of laborious
feature engineering. By contrast, we pro-
pose to automatically learn and then ex-
ploit user embeddings, to be used in con-
cert with lexical signals to recognize sar-
casm. Our approach does not require elab-
orate feature engineering (and concomi-
tant data scraping); fitting user embed-
dings requires only the text from their
previous posts. The experimental results
show that the our model outperforms a
state-of-the-art approach leveraging an ex-
tensive set of carefully crafted features.

1 Introduction

Existing social media analysis systems are ham-
pered by their inability to accurately detect and in-
terpret figurative language. This is particularly rel-
evant in domains like the social sciences and poli-
tics, in which the use of figurative communication
devices such as verbal irony (roughly, sarcasm) is
common. Sarcasm is often used by individuals to
express opinions on complex matters and regard-
ing specific targets (Carvalho et al., 2009).

Early computational models for verbal irony
and sarcasm detection tended to rely on shallow
methods exploiting conditional token count reg-
ularities. But lexical clues alone are insufficient

Figure 1: An illustrative tweet.

to discern ironic intent. Appreciating the con-
text of utterances is critical for this; even for hu-
mans (Wallace et al., 2014). Indeed, the exact
same sentence can be interpreted as literal or sar-
castic, depending on the speaker. Consider the
sarcastic tweet in Figure 1 (ignoring for the mo-
ment the attached #sarcasm hashtag). Without
knowing the author’s political leanings, it would
be difficult to conclude with certainty whether the
remark was intended sarcastically or in earnest.

Recent work in sarcasm detection on social me-
dia has tried to incorporate contextual information
by exploiting the preceding messages of a user, to
e.g., detect contrasts in sentiments expressed to-
wards named entities (Khattri et al., 2015), infer
behavioural traits (Rajadesingan et al., 2015) and
capture the relationship between authors and the
audience (Bamman and Smith, 2015). However,
all of these approaches require the design and im-
plementation of complex features that explicitly
encode the content and (relevant) context of mes-
sages to be classified. This feature engineering
is labor intensive, and depends on external tools
and resources. Therefore, deploying such systems
in practice is expensive, time-consuming and un-
wieldy.

We propose a novel approach to sarcasm detec-
tion on social media that does not require exten-
sive manual feature engineering. Instead, we de-
velop a neural model that learns to represent and
exploit embeddings of both content and context.
For the former, we induce vector lexical repre-

167



sentations via a convolutional layer; for the latter,
our model learns user embeddings. Inference con-
cerning whether an utterance (tweet) was intended
ironically (or not) is then modelled as a joint func-
tion of lexical representations and corresponding
author embeddings.

The main contributions of this paper are as fol-
lows. (i) We propose a novel convolutional neu-
ral network based model that explicitly learns and
exploits user embeddings in conjunction with fea-
tures derived from utterances. (ii) We show that
this model outperforms the strong baseline re-
cently proposed by Bamman and Smith (2015) by
more than 2% in absolute accuracy, while obviat-
ing the need to manually engineer features. (iii)
We show that the learned user embeddings can
capture relevant user attributes.

2 Related Work

Verbal irony is a rhetorical device in which speak-
ers say something other than, and often opposite
to, what they actually mean.1 Sarcasm may be
viewed as a special case of irony, where the posi-
tive literal meaning is perceived as an indirect in-
sult (Dews et al., 1995).

Most of the previously proposed computational
models to detect irony and sarcasm have used fea-
tures similar to those used in sentiment analy-
sis. Carvalho et al. (2009) analyzed comments
posted by users on a Portuguese online newspa-
per and found that oral and gestural cues indicate
irony. These included: emoticons, onomatopoeic
expressions for laughter, heavy punctuation, quo-
tation marks and positive interjections. Others
have used text classifiers with features based on
word and character n-grams, sentiment lexicons,
surface patterns and textual markers (Davidov et
al., 2010; González-Ibánez et al., 2011; Reyes et
al., 2013; Lukin and Walker, 2013). Elsewhere,
Barbieri and Saggion (2014) derived new word-
frequency based features to detect irony, e.g., com-
binations of frequent and rare words, ambiguous
words, ‘spoken style’ words combined with ‘writ-
ten style’ words and intensity of adjectives. Riloff
et al. (2013) demonstrated that one may exploit
the apparent expression of contrasting sentiment
in the same utterance as a marker of verbal irony.

The aforementioned approaches rely predomi-
nantly on features intrinsic to texts, but these will

1Like other forms of subjective expression, irony and sar-
casm are difficult to define precisely.

often be insufficient to infer figurative meaning:
context is needed. There have been some recent
attempts to exploit contextual information, e.g.
Khattri et al. (2015) extended the notion of con-
trasting sentiments beyond the textual content at
hand. In particular, they analyzed previous posts
to estimate the author’s prior sentiment towards
specific targets (i.e., entities). A tweet is then pre-
dicted to be sarcastic if it expresses a sentiment
about an entity that contradicts the author’s (esti-
mated) prior sentiment regarding the same.

Rajadesingan et al. (2015) built a system based
on theories of sarcasm expression from psychol-
ogy and behavioral sciences. To operationalize
such theories, they used several linguistic tools
and resources (e.g. lexicons, sentiment classifiers
and a PoS tagger), in addition to user profile in-
formation and previous posts, to model a range
of behavioural aspects (e.g., mood, writing style).
Wallace et al. (2015) developed an approach for
classifying posts on reddit2 as sarcastic or literal,
based in part on the interaction between the spe-
cific sub-reddit to which a post was made, the
entities mentioned, and the (apparent) sentiment
expressed. For example, if a post in the (polit-
ically) conservative sub-reddit mentions Obama,
it is more likely to have been intended ironically
than posts mentioning Obama in the progressive
sub-reddit. But this approach is limited because it
relies on the unique sub-reddit structure. Bamman
and Smith (2015) proposed an approach that relied
on an extensive, rich set of features capturing vari-
ous contextual information about authors of tweets
and the audience (in addition to lexical cues). We
review these at length in Section 5.1.

A major downside of these and related ap-
proaches, however, is the amount of manual ef-
fort required to derive these feature sets. A pri-
mary goal of this work is to explore whether neural
models can effectively learn these rich contextual-
izing features, thus obviating the need to manually
craft them. In particular, the model we propose
similarly aims to combine lexical clues with extra-
linguistic information. Unlike prior work, how-
ever, our model attempts to automatically induce
representations for the content and the author of a
message that are predictive of sarcasm.

2http://reddit.com is a social news aggregation
site comprising specific topical sub-reddits.

168



3 Learning User Embeddings

Our goal is to learn representations (vectors) that
encode latent aspects of users and capture ho-
mophily, by projecting similar users into nearby
regions of the embedding space. We hypothe-
size that such representations will naturally cap-
ture some of the signals that have been described
in the literature as important indicators of sarcasm,
such as contrasts between what someone believes
and what they have ostensibly expressed (Camp-
bell and Katz, 2012) or Kreuz (1996) principle of
inferability, stating that sarcasm requires a com-
mon ground between parties to be understood.

To induce the user embeddings, we adopt an ap-
proach similar to that described in the preliminary
work of Li et al. (2015). In particular, we capture
relations between users and the content they pro-
duce by optimizing the conditional probability of
texts, given their authors (or, more precisely, given
the vector representations of their authors). This
method is akin to Le and Mikolov (2014)’s Para-
graph Vector model, which jointly estimates em-
beddings for words and paragraphs by learning to
predict the occurrence of a word w within a para-
graph p conditioned on the (learned) representa-
tion for p.

Given a sentence S = {w1, . . . , wN} where wi
denotes a word drawn from a vocabulary V , we
aim to maximize the following probability:

P (S|userj) =
∑
wi∈S

logP (wi|uj)

+
∑
wi∈S

∑
wk∈C(wi)

logP (wi|ek)
(1)

Where C(wi) denotes the set of words in a pre-
specified window around word wi, ek ∈ Rd and
uj ∈ Rd denote the embeddings of word k and
user j, respectively. This objective function en-
codes the notion that the occurrence of a word w,
depends both on the author of S and it’s neigh-
bouring words.

The conditional probabilities in Equation 1 can
be estimated with log-linear models of the form:

P (wi|x) = exp(Wi · x + bi)∑Y
k=1 exp(Wk · x + bk)

(2)

Where x denotes a feature vector, Wk and bk are
the weight vectors and bias for class k. In our
case, we treat words as classes to be predicted.

Calculating the denominator thus requires sum-
ming over all of the words in the (large) vocabu-
lary, an expensive operation. To avoid this com-
putational bottleneck, we approximate the term
P (wi|ek) with Morin and Bengio (2005) Hierar-
chical Softmax.3

To learn meaningful user embeddings, we seek
representations that are predictive of individual
word-usage patterns. In light of this motiva-
tion, we approximate P (wi|uj) via the following
hinge-loss objective which we aim to minimize:

L(wi, userj) =∑
wl∈V,wl 6∈S

max(0, 1− ei · uj + el · uj)

(3)

Here, each wl (and corresponding embedding, el)
is a negative example, i.e., a word not in the sen-
tence under consideration, which was authored by
user j. The intuition is that in the aggregate, such
words are less likely to be employed by user j than
words observed in sentences she has authored.
Thus minimizing this objective attempts to induce
a representation that is discriminative with respect
to word usage.

In practice, V will be very large and hence we
approximate the objective via negative sampling, a
variant of Noise Contrastive Estimation. The idea
is to approximate the objective function in a binary
classification task by learning to discriminate be-
tween observed positive examples (sampled from
the true distribution) and pseudo-negative exam-
ples (sampled from a large space of predominantly
negative instances). Intuitively, this shifts prob-
ability mass to plausible observations. See Dyer
(2014) for notes on Negative Sampling and Noise
Contrastive Estimation.

Previous work by Collobert et al. (2011)
showed that this approach works well in represen-
tation learning tasks when a sufficient amount of
training data is available . However, we have ac-
cess to only a limited amount of text for each user
(see Section 5). We hypothesize that this problem
can be alleviated by carefully selecting the nega-
tive samples that mostly contribute to ‘push’ the
vectors into the appropriate region of the embed-
ding space (i.e., closer to the words commonly em-
ployed by a given user and far from other words).

3As implemented in Gensim (Řehůřek and Sojka, 2010).

169



This requires designing a strategy for selectively
sampling negative examples. One straightforward
approach would be to sample from a user-specific
unigram model, informing which words are less
likely to be utilized by that user. But estimating the
parameters of such model with scarce data would
be prone to overfitting. Instead, we sample from a
unigram distribution estimated from the posts au-
thored by all the users. The goal is to select the
most commonly used words as the negative sam-
ples, thereby forcing the representations to capture
the differences between the words a given indi-
vidual employs and the words that everyone com-
monly uses.

4 Proposed Model

We now present the details of our proposed sar-
casm detection model. Given a message S au-
thored by user u, we wish to capture both the rel-
evant aspects of the content and the relevant con-
textual information about the author. To represent
the content, we use pre-trained word embeddings
as the input to a convolutional layer that extracts
high-level features. More formally, let E ∈ Rd×|V|
be a pre-trained word embedding matrix, where
each column represents a word from the vocabu-
lary V as a d dimensional vector. By selecting the
columns of E corresponding to the words in S, we
form the sentence matrix:

S =

 e1...
em

 (4)
A convolutional layer is composed of a set of

filters F ∈ Rd×h where h is the height of the
filter. Filters slide across the input, extracting h-
gram features that constitute a feature map m ∈
R|S|−h+1, where each entry is obtained as

mi = α(F · S[i:i−h+1] + b) (5)

with i = 1, . . . i − h + 1. Here, S[i:j] de-
notes a sub-matrix of S (from row i to row j),
b ∈ R is an additive bias and α(·) denotes a
non-linear activation function, applied element-
wise. We transform the resultant feature map
into a scalar using max-pooling, i.e., we extract
the largest value in the map. We use 3 filters
(with varying heights) each of which gener-
ates M feature maps that are reduced to a vector
fk = [max(m1) ⊕max(m2) . . .⊕ max(mM )],

where ⊕ denotes concatenation. We set α(·) to be
the Rectified Linear Unit activation function (Nair
and Hinton, 2010). The output of all the filters
is then combined to form the final representation
c = [f1 ⊕ f2 ⊕ f3]. We will denote this feature
vector of a specific sentence S by cS .

To represent the context, we assume there is a
user embedding matrix U ∈ Rd×N , where each
column represents one of N users with a d dimen-
sional vector. The parameters of this embedding
matrix can be initialized randomly or using the ap-
proach described in Section 3. Then, we simply
map the author of the message into the user em-
bedding space by selecting the corresponding col-
umn of U. Letting Uu denote the user embedding
of author u, we formulate our sarcasm detection
model as follows:

P (y = k|s, u; θ) ∝ Yk · g(cS ⊕Uu) + bk
g(x) = α(H · x + h) (6)

where g(·) denote the activations of a hid-
den layer, capturing the relations between
the content and context representations, and
θ = {Y,b,H,h,F1,F2,F3,E,U} are parame-
ters to be estimated during training. Here, Y ∈
R2×z and b ∈ R2 are the weights and bias of the
output layer; H ∈ Rz×3M+d and h ∈ Rz are the
weights and bias of the hidden layer; and Fi are
the weights of the convolutional filters. Hence-
forth, we will refer to this approach as Content
and User Embedding Convolutional Neural Net-
work (CUE-CNN). Figure 2 provides an illustra-
tive schematic depicting this model.

5 Experimental Setup

We replicated Bamman and Smith (2015) exper-
imental setup using (a subset of) the same Twit-
ter corpus. The labels were inferred from self-
declarations of sarcasm, i.e., a tweet is considered
sarcastic if it contains the hashtag #sarcasm or
#sarcastic and deemed non-sarcastic other-
wise.4 To comply with Twitter terms of service,
we were given only the tweet ids along with the
corresponding labels and had to retrieve the mes-
sages ourselves. By the time we tried to retrieve
the messages, some of them were not available.
We also did not have access to the historical user
tweets used by Bamman and Smith, hence, for

4Note that this is a form of noisy supervision, as of course
all sarcastic tweets will not be explicitly flagged as such.

170



Figure 2: Illustration of the CUE-CNN model for sarcasm detection. The model learns to represent and
exploit embeddings of both content and users in social media.

each author and mentioned user, we scraped ad-
ditional tweets from their Twitter feed. Due to re-
strictions in the Twitter API, we were only able
to crawl at most 1000 historical tweets per user.5

Furthermore, we were unable to collect historical
tweets for a significant proportion of the users,
thus, we discarded messages for which no con-
textual information was available, resulting in a
corpus of 11, 541 tweets involving 12, 500 unique
users (authors and mentioned users). It should also
be noted that our historical tweets were posted af-
ter the ones in the corpus used for the experiments.

5.1 Baselines

We reimplemented Bamman and Smith (2015)’s
sarcasm detection model. This a simple, logistic-
regression based classifier that exploits rich fea-
ture sets to achieve strong performance. These
are detailed at length in the original paper, but we
briefly summarize them here:

• tweet-features, encoding attributes of the
target tweet text, including: uni- and bi-
gram bag of words (BoW) features; Brown
et al. (1992) word clusters indicators; unla-
beled dependency bigrams (both BoW and
with Brown cluster representations); part-of-
speech, spelling and abbreviation features;
inferred sentiment, at both the tweet and
word level; and ‘intensifier’ indicators.

• author-features, aimed at encoding at-
tributes of the author, including: historically

5The original study (Bamman and Smith, 2015) was done
with at most 3, 200 historical tweets.

‘salient’ terms used by the author; the in-
ferred distribution over topics6 historically
tweeted about by the user; inferred sentiment
historically expressed by the user; and author
profile information (e.g., profile BoW fea-
tures).

• audience-features, capturing properties of
the addressee of tweets, in those cases that
a tweet is directed at someone (via the @
symbol). A subset of these, duplicate the
aforementioned author features for the ad-
dressee. Additionally, author/audience inter-
action features are introduced, which capture
similarity between the author and addressee,
w.r.t. inferred topic distributions. Finally,
this set includes a feature capturing the fre-
quency of past communication between the
author and addressee.

• response-features, for tweets written in re-
sponse to another tweet. This set of fea-
tures captures information relating the two,
with BoW features of the original tweet and
pairwise cluster indicator features, which the
encode Brown clusters observed in both the
original and response tweet.

We emphasize that implementing this rich set
of features took considerable time and effort. This
motivates our approach, which aims to effectively
induce and exploit contextually-aware representa-
tions without manual feature engineering.

6The topics were extracted from Latent Dirichlet Alloca-
tion (Blei et al., 2003).

171



To assess the importance of modelling contex-
tual information in sarcasm detection, we consid-
ered two groups of models as baselines: the first
only takes into account the content of a target
tweet. The second, combines lexical cues with
contextual information. The first group includes
the following models:

• UNIGRAMS: `2-regularized logistic regres-
sion classifier with binary unigrams as fea-
tures.

• TWEET ONLY: `2-regularized logistic re-
gression classifier with binary unigrams and
tweet-features.

• NBOW: Logistic regression with neural word
embeddings as features. Given a sentence
matrix S (Eq. 4) as input, a d-dimensional
feature vector is computed by summing the
individual word embeddings.

• NLSE: The Non-linear subspace embedding
model due to Astudillo et al. (2015). The
NLSE model adapts pre-trained word embed-
dings for specific tasks by learning a pro-
jection into a small subspace. The idea is
that this subspace captures the most discrim-
inative latent aspects encoded in the word
embeddings. Given a sentence matrix S,
each word vector is first projected into the
subspace and then transformed through an
element-wise sigmoid function. The final
sentence representation is obtained by sum-
ming the (adapted) word embeddings and
passed into a softmax layer that outputs the
predictions.

• CNN: The CNN model for text classification
proposed by Kim (2014), using only features
extracted from the convolutional layer act-
ing on the lexical content. The input layer
was initialized with pre-trained word embed-
dings.

The second group of baselines consists of the
following models:

• TWEET+*: `2-regularized logistic regres-
sion classifier with a combination of tweet-
features and each of the aforementioned
Bamman and Smith (2015) feature sets.

• SHALLOW CUE-CNN: A simplified version
of our neural model for sarcasm detection,
without the hidden layer. We evaluated two

variants: initializing the user embeddings
at random, and initializing the user embed-
dings with the approach described in Section
3 (SHALLOW CUE-CNN+USER2VEC). In
both cases, the (word and user) embeddings
weights were updated during training.

• CUE-CNN+*: Our proposed neural net-
work for sarcasm detection. We also eval-
uated the two aforementioned variants: ran-
domly initialized user embeddings and pre-
trained user embeddings. But here we
compared two different approaches for the
negative sampling procedure, namely, sam-
pling from a unigram distribution (CUE-
CNN+USER2VEC) and sampling uniformly
at random from the vocabulary (CUE-
CNN+USER2VEC-UNIFRAND).

5.2 Pre-Training Word and User
Embeddings

We first trained Mikolov et al. (2013)’s skip-gram
model variant to induce word embeddings using
the union of: Owoputi et al. (2013)’s dataset of
52 Million unlabeled tweets, Bamman and Smith
(2015) sarcasm dataset and 5 Million historical
tweets collected from users.

To induce user embeddings, we estimated a un-
igram distribution with maximum likelihood es-
timation. Then, for each word in a tweet, we
extracted 15 negative samples (for the first term
in Eq.1) and used the skip-gram model to pre-
compute the conditional probabilities of words oc-
curring in a window of size 5 (for the second term
in Eq.1). Finally, Equation 1 was minimized via
Stochastic Gradient Descent on 90% of the histor-
ical data, holding out the remainder for validation
and using the P (tweet text|user) as early stopping
criteria.

Note that the parameters for each user only de-
pend on their own tweets; this allowed us to per-
form these computations in parallel to speed-up
the training.

m

5.3 Model Training and Evaluation
Evaluation was performed via 10-fold cross-
validation. For each split, we fit models to 80%
of the data, tuned them on 10% and tested on the
remaining, held-out 10%. These data splits were
kept fixed in all the experiments. For the linear
classifiers, in each split, the regularization con-
stant was selected with a linear search over the

172



(a) Performance of the linear classifier baselines. We include
the results reported by Bamman and Smith (2015) as a refer-
ence. Discrepancies between their reported results and those
we achieved with our re-implementation reflect the fact that
their experiments were performed using a significantly larger
training set and more historical tweets than we had access to.

(b) Performance of the proposed neural models. We compare
simple neural models that only consider the lexical content of a
message (first 3 bars) with architectures that explicitly model
the context. Bars 4 and 5 show the gains obtained by pre-
training the user embeddings. The last 2 bars compare dif-
ferent negative sampling procedures for the user embedding
pre-training.

Figure 3: Comparison of different models. The left sub-plot shows linear model performance; the right
shows the performance of neural variants. The horizontal line corresponds to the best performance
achieved via linear models with rich feature sets. Performance was measured in terms of average accu-
racy over 10-fold cross-validation; error bars depict the variance.

range C = [1e−4, 1e−3, 1e−2, 1e−1, 1, 10] using
the training set to fit the model and evaluating on
the tuning set. After selecting the best regular-
ization constant, the model was re-trained on the
union of the train and tune sets, and evaluated on
the test set.

To train our neural model, we first had to
choose a suitable architecture and hyperpa-
rameter set. However, selecting the optimal
network parametrization would require an ex-
tensive search over a large configuration space.
Therefore, in these experiments, we followed
the recommendations in Zhang and Wallace
(2015), focusing our search over combinations of
dropout ratesD = [0.0, 0.1, 0.3, 0.5], filter heights
H = [(1, 3, 5), (2, 4, 6), (3, 5, 7), (4, 6, 8), (5, 7, 9)],
number of feature maps M = [100, 200, 400, 600]
and size of the hidden layer Z = [25, 50, 75, 100].

We performed random search by sampling with-
out replacement over half of the possible configu-
rations. For each data split, 20% of the training
set was reserved for early stopping. We compared
the sampled configurations by fitting the model on
the remaining training data and testing on the tune
set. After choosing the best configuration, we re-
trained the model on the union of the train and tune
set (again reserving 20% of the data for early stop-
ping) and evaluated on the test set.

The model was trained by minimizing the cross-
entropy error between the predictions and true la-
bels, the gradients w.r.t to the network parameters
were computed with backpropagation (Rumelhart
et al., 1988) and the model weights were updated
with the AdaDelta rule (Zeiler, 2012).

6 Results

6.1 Classification Results

Figure 3 presents the main experimental results.
In Figure 3a, we show the performance of lin-
ear classifiers with the manually engineered fea-
ture sets proposed by Bamman and Smith (2015).
Our results differ slightly from those originally re-
ported. Nonetheless, we observe the same general
trends: namely, that including contextual features
significantly improves the performance, and that
the biggest gains are attributable to features encod-
ing information about the authors of tweets.

The results of neural model variants are shown
in Figure 3b. Once again, we find that modelling
the context (i.e., the author) of a tweet yields sig-
nificant gains in accuracy. The difference is that
here the network jointly learns appropriate user
representations, lexical feature extractors and, fi-
nally, the classification model. Further improve-
ments are realized by pre-training the user embed-

173



(a) Users colored according to the politicians they follow on
Twitter: the blue circles represent users that follow at least one
of the (democrats) accounts: @BarackObama, @HillaryClin-
ton and @BernieSanders; the red circles represent users that
follow at least one of the (republicans) accounts: @marcoru-
bio, @tedcruz and @realDonaldTrump. Users that follow ac-
counts from both groups were excluded. We can see that users
with a similar political leaning tend to have similar vectors.

(b) Users colored with respect to the likelihood of following a
sports account. The 500 most popular accounts (according to
the authors in our training data) were manually inspected and
100 sports related accounts were selected, e.g., @SkySports,
@NBA and @cristiano. We should note that users for which
the probabilities lied in the range between 0.3− 0.7 were dis-
carded to emphasize the extremes.

Figure 4: T-SNE projection of the user embeddings into 2-dimensions. The users are color coded ac-
cording to their political preferences and interest in sports. The visualization suggests that the learned
embeddings capture some notion of homophily.

dings (we elaborate on this in the following sec-
tion). We see additional gains when we introduce
a hidden layer capturing the interactions between
the context (i.e., user vectors) and the content (lex-
ical vectors). This is intuitively agreeable: the
recognition of sarcasm is possible when we jointly
consider the speaker and the utterance at hand. In-
terestingly, we observed that our proposed model
not only outperforms all the other baselines, but
also shows less variance over the cross-validation
experiments.

Finally, we compared the effect of obtaining
negative samples uniformly at random with sam-
pling from a unigram distribution. The experimen-
tal results show that the latter improves the accu-
racy of the model by 0.8%. We believe the reason
is that considering the most likely words (under
the model) as negative samples, helps by pushing
the user vectors away from non-informative words
and simultaneously closer to the most discrimina-
tive words for that user.

6.2 User Embedding Analysis

We now investigate the user embeddings in more
detail. In particular, we are interested in two ques-
tions: first, what aspects are being captured in
these representations; and second, how they con-
tribute to the improved performance of our model.

Figure 5: Two sarcastic examples that were mis-
classified by a simple CNN (no user). Using
the CUE-CNN with contextual information drasti-
cally changes the model’s predictions on the same
examples.

To investigate the first question, we plotted a T-
SNE projection (Maaten and Hinton, 2008) of the
high-dimensional vector space where the users are
represented into two-dimensions. We then colored
each point (representing a user) according to their
apparent political leaning (Figure 4a), and accord-
ing to their interest in sports (Figure 4b). These
attributes were inferred using the Twitter accounts
that a user follows, as a proxy. The plots sug-
gest that the user vectors are indeed able to cap-

174



ture latent aspects, such as political preferences
and personal interests. Moreover, the embeddings
seem to uncover a notion of homophily, i.e. sim-
ilar users tend to occupy neighbouring regions of
the embedding space. Regarding the second ques-
tion, we examined the influence of the contextual
information on the model’s predictions. To this
end, we measured the response of our model to
the same textual content with different hypothet-
ical contexts (authors). We selected two exam-
ples that were misclassified by a simple CNN and
ran them trough the CUE-CNN model with three
different user embeddings. In Figure 5, we show
these examples along with the predicted probabil-
ities of being a sarcastic post, when no user in-
formation is considered and when the author is
taken into account. We can see that the predic-
tions drastically change when contextual informa-
tion is available and that two of the authors trigger
similar responses on both examples. This example
provides evidence that our model captures the in-
tuition that the same utterance can be interpreted
as sarcastic or not, depending on the speaker.

7 Conclusions

We have introduced CUE-CNN, a novel, deep
neural network for automatically recognizing sar-
castic utterances on social media. Our model
jointly learns and exploits embeddings for the con-
tent and users, thus integrating information about
the speaker and what he or she has said. This is
accomplished without manual feature engineering.
Nonetheless, our model outperforms (by over 2%
in absolute accuracy) a recently proposed state-
of-the-art model that exploits an extensive, hand-
crafted set of features encoding user attributes and
other contextual information. Unlike other ap-
proaches that explicitly exploit the structure of
particular social media services, such as the forum
where a message was posted or metadata about
the users, learning user embeddings only requires
their preceding messages. Yet, the obtained vec-
tors are able to capture relevant user attributes and
a soft notion of homophily. This, we believe,
makes our model easier to deploy over different
social media environments.

Our implementation of the proposed method
and the datasets used in this paper have been made
publicly available7. As future work, we intended
to further explore the user embeddings for context

7https://github.com/samiroid/CUE-CNN

representation, namely by also incorporating the
interaction between the author and the audience
into the model.

Acknowledgments

This work was supported in part by the
Army Research Office (grant W911NF-14-
1-0442) and by The Foundation for Science
and Technology, Portugal (FCT), through
contracts UID/CEC/50021/2013, EXCL/EEI-
ESS/0257/2012 (DataStorm), grant UTAP-
EXPL/EEIESS/0031/2014 and Ph.D. scholarship
SFRH/BD/89020/2012. This work was also made
possible by the support of the Texas Advanced
Computer Center (TACC) at UT Austin.

References
Ramón Astudillo, Silvio Amir, Wang Ling, Mario

Silva, and Isabel Trancoso. 2015. Learning word
representations from scarce and noisy data with em-
bedding subspaces. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1074–1084, Beijing, China,
July.

David Bamman and Noah A Smith. 2015. Contextual-
ized sarcasm detection on twitter. In Proceedings of
the 9th International Conference on Web and Social
Media, pages 574–77. AAAI Menlo Park, CA.

Francesco Barbieri and Horacio Saggion. 2014. Mod-
elling irony in twitter. In EACL, pages 56–64.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

John D Campbell and Albert N Katz. 2012. Are there
necessary conditions for inducing a sense of sarcas-
tic irony? Discourse Processes, 49(6):459–480.

Paula Carvalho, Luı́s Sarmento, Mário J Silva, and
Eugénio De Oliveira. 2009. Clues for detect-
ing irony in user-generated contents: oh...!! it’s
so easy;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion, pages 53–56. ACM.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

175



Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107–116. Association for
Computational Linguistics.

Shelly Dews, Joan Kaplan, and Ellen Winner. 1995.
Why not say it directly? the social functions of irony.
Discourse processes, 19(3):347–367.

Chris Dyer. 2014. Notes on noise contrastive es-
timation and negative sampling. arXiv preprint
arXiv:1410.8251.

Roberto González-Ibánez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers-Volume 2, pages 581–586. Association for
Computational Linguistics.

Anupam Khattri, Aditya Joshi, Pushpak Bhat-
tacharyya, and Mark James Carman. 2015. Your
sentiment precedes you: Using an author’s histor-
ical tweets to predict sarcasm. In 6th Workshop
on Computational Approaches To Subjectivity, Sen-
timent And Social Media Analysis WASSA 2015,
page 25.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1746–
1751, Doha, Qatar, October. Association for Com-
putational Linguistics.

Roger J Kreuz. 1996. The use of verbal irony: Cues
and constraints. Metaphor: Implications and appli-
cations, pages 23–38.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.

Jiwei Li, Alan Ritter, and Dan Jurafsky. 2015.
Learning multi-faceted representations of individu-
als from heterogeneous evidence using neural net-
works. arXiv preprint arXiv:1510.05198.

Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for online
dialogue. In Proceedings of the Workshop on Lan-
guage Analysis in Social Media, pages 30–40.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(Nov):2579–2605.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Aistats, volume 5, pages 246–252. Citeseer.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pages 807–814.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. Asso-
ciation for Computational Linguistics.

Ashwin Rajadesingan, Reza Zafarani, and Huan Liu.
2015. Sarcasm detection on twitter: A behavioral
modeling approach. In Proceedings of the Eighth
ACM International Conference on Web Search and
Data Mining, pages 97–106. ACM.

Radim Řehůřek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.

Antonio Reyes, Paolo Rosso, and Tony Veale. 2013.
A multidimensional approach for detecting irony
in twitter. Language resources and evaluation,
47(1):239–268.

Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive senti-
ment and negative situation. In EMNLP, pages 704–
714.

David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. Cognitive modeling, 5(3):1.

Byron C. Wallace, Do Kook Choe, Laura Kertz, and
Eugene Charniak. 2014. Humans require context to
infer ironic intent (so computers probably do, too).
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 512–516, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.

Byron C. Wallace, Do Kook Choe, and Eugene Char-
niak. 2015. Sparse, contextually informed mod-
els for irony detection: Exploiting user communi-
ties, entities and sentiment. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1035–1044, Beijing,
China, July. Association for Computational Linguis-
tics.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

176



Ye Zhang and Byron Wallace. 2015. A sensitiv-
ity analysis of (and practitioners’ guide to) convo-
lutional neural networks for sentence classification.
arXiv preprint arXiv:1510.03820.

177


