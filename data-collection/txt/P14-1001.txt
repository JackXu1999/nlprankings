



















































Learning Ensembles of Structured Prediction Rules


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1–12,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Learning Ensembles of Structured Prediction Rules

Corinna Cortes
Google Research
111 8th Avenue,

New York, NY 10011
corinna@google.com

Vitaly Kuznetsov
Courant Institute

251 Mercer Street,
New York, NY 10012
vitaly@cims.nyu.edu

Mehryar Mohri
Courant Institute and Google Research

251 Mercer Street,
New York, NY 10012
mohri@cims.nyu.edu

Abstract

We present a series of algorithms with the-
oretical guarantees for learning accurate
ensembles of several structured prediction
rules for which no prior knowledge is as-
sumed. This includes a number of ran-
domized and deterministic algorithms de-
vised by converting on-line learning al-
gorithms to batch ones, and a boosting-
style algorithm applicable in the context of
structured prediction with a large number
of labels. We also report the results of ex-
tensive experiments with these algorithms.

1 Introduction

We study the problem of learning accurate en-
sembles of structured prediction experts. Ensem-
ble methods are widely used in machine learn-
ing and have been shown to be often very effec-
tive (Breiman, 1996; Freund and Schapire, 1997;
Smyth and Wolpert, 1999; MacKay, 1991; Fre-
und et al., 2004). However, ensemble methods and
their theory have been developed primarily for bi-
nary classification or regression tasks. Their tech-
niques do not readily apply to structured predic-
tion problems. While it is straightforward to com-
bine scalar outputs for a classification or regres-
sion problem, it is less clear how to combine struc-
tured predictions such as phonemic pronuncia-
tion hypotheses, speech recognition lattices, parse
trees, or alternative machine translations.

Consider for example the problem of devising
an ensemble method for pronunciation, a crit-
ical component of modern speech recognition
(Ghoshal et al., 2009). Often, several pronunci-
ation models or experts are available for transcrib-
ing words into sequences of phonemes. These
models may have been derived using other ma-
chine learning algorithms or they may be based on

carefully hand-crafted rules. In general, none of
these pronunciation experts is fully accurate and
each expert may be making mistakes at different
positions along the output sequence. One can hope
that a model that patches together the pronuncia-
tion of different experts could achieve a superior
performance.

Similar ensemble structured prediction problems
arise in other tasks, including machine translation,
part-of-speech tagging, optical character recogni-
tion and computer vision, with structures or sub-
structures varying with each task. We seek to
tackle all of these problems simultaneously and
consider the general setting where the label or out-
put associated to an input x ∈ X is a structure
y ∈ Y that can be decomposed and represented
by l substructures y1, . . . , yl. For the pronuncia-
tion example just discussed, x is a specific word
or word sequence and y its phonemic transcrip-
tion. A natural choice for the substructures yk is
then the individual phonemes forming y. Other
possible choices include n-grams of consecutive
phonemes or more general subsequences.

We will assume that the loss function considered
admits an additive decomposition over the sub-
structures, as is common in structured prediction.
We also assume access to a set of structured pre-
diction experts h1, . . . , hp that we treat as black
boxes. Given an input x ∈ X , each expert pre-
dicts a structure hj(x) = (h1j (x), . . . , h

l
j(x)). The

hypotheses hj may be the output of a structured
prediction algorithm such as Conditional Random
Fields (Lafferty et al., 2001), Averaged Perceptron
(Collins, 2002), StructSVM (Tsochantaridis et al.,
2005), Max Margin Markov Networks (Taskar et
al., 2004) or the Regression Technique for Learn-
ing Transductions (Cortes et al., 2005), or some
other algorithmic or human expert. Given a la-
beled training sample (x1,y1), . . . , (xm,ym), our
objective is to use the predictions of these experts

1



to form an accurate ensemble.

Variants of the ensemble problem just formulated
have been studied in the past in the natural lan-
guage processing and machine learning literature.
One of the most recent, and possibly most rele-
vant studies for sequence data is that of (Nguyen
and Guo, 2007), which is based on the forward
stepwise selection introduced by (Caruana et al.,
2004). However, one disadvantage of this greedy
approach is that it can be proven to fail to select
an optimal ensemble of experts even in favorable
cases where a specialized expert is available for
each local prediction (Cortes et al., 2014a). En-
semble methods for structured prediction based on
bagging, random forests and random subspaces
have also been proposed in (Kocev et al., 2013).
One of the limitations of this work is that it is
applicable only to a very specific class of tree-
based experts introduced in that paper. Similarly, a
boosting approach was developed in (Wang et al.,
2007) but it applies only to local experts. In the
context of natural language processing, a variety
of different re-ranking techniques have been pro-
posed for somewhat related problems (Collins and
Koo, 2005; Zeman and Žabokrtský, 2005; Sagae
and Lavie, 2006; Zhang et al., 2009). But, re-
ranking methods do not combine predictions at
the level of substructures, thus the final predic-
tion of the ensemble coincides with the prediction
made by one of the experts, which can be shown to
be suboptimal in many cases. Furthermore, these
methods typically assume the use of probabilistic
models, which is not a requirement in our learning
scenario. Other ensembles of probabilistic mod-
els have also been considered in text and speech
processing by forming a product of probabilis-
tic models via the intersection of lattices (Mohri
et al., 2008), or a straightforward combination of
the posteriors from probabilistic grammars trained
using EM with different starting points (Petrov,
2010), or some other rather intricate techniques
in speech recognition (Fiscus, 1997). Finally, an
algorithm of (MacKay, 1997) is another example
of an ensemble method for structured prediction
though it is not addressing directly the problem we
are considering.

Most of the references just mentioned do not give a
rigorous theoretical justification for the techniques
proposed. We are not aware of any prior theoret-
ical analysis for the ensemble structured predic-

tion problem we consider. Here, we present two
families of algorithms for learning ensembles of
structured prediction rules that both perform well
in practice and enjoy strong theoretical guaran-
tees. In Section 3, we develop ensemble methods
based on on-line algorithms. To do so, we extend
existing on-line-to-batch conversions to our more
general setting. In Section 4, we present a new
boosting-style algorithm which is applicable even
with a large set of classes as in the problem we
consider, and for which we present margin-based
learning guarantees. Section 5 reports the results
of our extensive experiments.1

2 Learning scenario

As in standard supervised learning problems, we
assume that the learner receives a training sample
S = ((x1,y1), . . . , (xm,ym)) ∈ X × Y of m
labeled points drawn i.i.d. according to the some
distribution D used both for training and testing.
We also assume that the learner has access to a set
of p predictors h1, . . . , hp mapping X to Y to de-
vise an accurate ensemble prediction. Thus, for
any input x ∈ X , he can use the prediction of
the p experts h1(x), . . . , hp(x). No other infor-
mation is available to the learner about these p ex-
perts, in particular the way they have been trained
or derived is not known to the learner. But, we
will assume that the training sample S is distinct
from what may have been used for training the al-
gorithms that generated h1(x), . . . , hp(x).

To simplify our analysis, we assume that the num-
ber of substructures l ≥ 1 is fixed. This does not
cause any loss of generality so long as the maxi-
mum number of substructures is bounded, which
is the case in all the applications we consider.
The quality of the predictions is measured by a
loss function L : Y × Y → R+ that can be de-
composed as a sum of loss functions `k : Yk →
R+ over the substructure sets Yk, that is, for all
y = (y1, . . . , yl) ∈ Y with yk ∈ Yk and y′ =
(y′1, . . . , y′l) ∈ Y with y′k ∈ Yk,

L(y,y′) =
l∑

k=1

`k(yk, y′k). (1)

We will assume in all that follows that the loss
function L is bounded: L(y,y′) ≤ M for all

1This paper is a modified version of (Cortes et al., 2014a)
to which we refer the reader for the proofs of the theorems
stated and a more detailed discussion of our algorithms.

2



(y,y′) for some M > 0. A prototypical example
of such loss functions is the normalized Hamming
loss LHam, which is the fraction of substructures
for which two labels y and y′ disagree, thus in that
case `k(yk, y′k) = 1l Iyk 6=y′k and M = 1.

3 On-line learning approach

In this section, we present an on-line learning so-
lution to the ensemble structured prediction prob-
lem just discussed. We first give a new formula-
tion of the problem as that of on-line learning with
expert advice, where the experts correspond to the
paths of an acyclic automaton. The on-line algo-
rithm generates at each iteration a distribution over
the path-experts. A critical component of our ap-
proach consists of using these distributions to de-
fine a prediction algorithm with favorable gener-
alization guarantees. This requires an extension
of the existing on-line-to-batch conversion tech-
niques to the more general case of combining dis-
tributions over path-experts, as opposed to com-
bining single hypotheses.

3.1 Path experts

Each expert hj induces a set of substructure hy-
potheses h1j , . . . , h

l
j . As already discussed, one

particular expert may be better at predicting the
kth substructure while some other expert may be
more accurate at predicting another substructure.
Therefore, it is desirable to combine the substruc-
ture predictions of all experts to derive a more ac-
curate prediction. This leads us to considering an
acyclic finite automaton G such as that of Figure 1
which admits all possible sequences of substruc-
ture hypotheses, or, more generally, a finite au-
tomaton such as that of Figure 2 which only allows
a subset of these sequences.

An automaton such as G compactly represents a
set of path experts: each path from the initial
vertex 0 to the final vertex l is labeled with a
sequence of substructure hypotheses h1j1 , . . . , h

l
jl

and defines a hypothesis which associates to input
x the output h1j1(x) · · ·hljl(x). We will denote by
H the set of all path experts. We also denote by
h each path expert defined by h1j1 , . . . , h

l
jl

, with
jk ∈ {1, . . . , p}, and denote by hk its kth sub-
structure hypothesis hkjk . Our ensemble structure
prediction problem can then be formulated as that
of selecting the best path expert (or collection of

Figure 1: Finite automaton G of path experts.

path experts) in G. Note that, in general, the path
expert selected does not coincide with any of the
original experts h1, . . . , hp.

3.2 On-line algorithm

Using an automaton G, the size of the pool of ex-
perts H we consider can be very large. For ex-
ample, in the case of the automaton of Figure 1,
the size of the pool of experts is pl, and thus is
exponentially large with respect to p. But, since
learning guarantees in on-line learning admit only
a logarithmic dependence on that size, they re-
main informative in this context. Nevertheless,
the computational complexity of most on-line al-
gorithms also directly depends on that size, which
could make them impractical in this context. But,
there exist several on-line solutions precisely de-
signed to address this issue by exploiting the struc-
ture of the experts as in the case of our path ex-
perts. These include the algorithm of (Takimoto
and Warmuth, 2003) denoted by WMWP, which
is an extension of the (randomized) weighted-
majority (WM) algorithm of (Littlestone and War-
muth, 1994) to more general bounded loss func-
tions combined with the Weight Pushing (WP) al-
gorithm of (Mohri, 1997); and the Follow the Per-
turbed Leader (FPL) algorithm of (Kalai and Vem-
pala, 2005). The WMWP algorithm admits a more
favorable regret guarantee than the FPL algorithm
in our context and our discussion will focus on
the use of WMWP for the design of our batch al-
gorithm. However, we have also fully analyzed
and implemented a batch algorithm based on FPL
(Cortes et al., 2014a).

As in the standard WM algorithm (Littlestone and
Warmuth, 1994), WMWP maintains at each round
t ∈ [1, T ], a distribution pt over the set of all ex-
perts, which in this context are the path experts
h ∈ H. At each round t ∈ [1, T ], the algo-
rithm receives an input sequence xt, incurs the loss
Eh∼pt [L(h(xt),yt)] =

∑
h pt(h)L(h(xt),yt) and

multiplicatively updates the distribution weight
per expert:

∀h ∈ H, pt+1(h)= pt(h)β
L(h(xt),yt)∑

h′∈H pt(h′)βL(h
′(xt),yt)

, (2)

3



Figure 2: Alternative experts automaton.

where β ∈ (0, 1) is some fixed parameter. The
number of paths is exponentially large in p and the
cost of updating all paths is therefore prohibitive.
However, since the loss function is additive in the
substructures and the updates are multiplicative,
it suffices to maintain instead a weight wt(e) per
transition e, following the update

wt+1(e)=
wt(e)β`e(xt,yt)∑

orig(e′)=orig(e) wt(e′)β`e′ (xt,yt)
(3)

where `e(xt,yt) denotes the loss incurred by the
substructure predictor labeling e for the input xt
and output yt, and orig(e′) denotes the origin
state of a transition e′ (Takimoto and Warmuth,
2003). Thus, the cost of the update is then linear
in the size of the automaton. To use the result-
ing weighted automaton for sampling, the weight
pushing algorithm is used, whose complexity is
also linear in the size of the automaton (Mohri,
1997).

3.3 On-line-to-batch conversion

The WMWP algorithm does not produce a se-
quence of path experts, rather, a sequence of dis-
tributions p1, . . . , pT over path experts. Thus, the
on-line-to-batch conversion techniques described
in (Littlestone, 1989; Cesa-Bianchi et al., 2004;
Dekel and Singer, 2005) do not readily apply. In-
stead, we propose a generalization of the tech-
niques of (Dekel and Singer, 2005). The conver-
sion consists of two steps: extract a good collec-
tion of distributions P ⊆ {p1, . . . , pT }; next use
P to define an accurate hypothesis for prediction.
For a subset P ⊆ {p1, . . . , pT }, we define

Γ(P)=
1
|P|
∑
pt∈P

∑
h∈H

pt(h)L(h(xt),yt)+M

√
log 1δ
|P|

=
1
|P|
∑
pt∈P

∑
e

wt(e)`e(xt),yt)+M

√
log 1δ
|P| ,

where δ > 0 is a fixed parameter. With this defini-
tion, we choose Pδ as a minimizer of Γ(P) over

some collection P of subsets of {p1, . . . , pT }:
Pδ ∈ argminP∈P Γ(P). The choice of P is re-
stricted by computational considerations. One nat-
ural option is to let P be the union of the suf-
fix sets {pt, . . . , pT }, t = 1, . . . , T . We will
assume in what follows that P includes the set
{p1, . . . , pT }.
Next, we define a randomized algorithm based on
Pδ. Given an input x, the algorithm consists of
randomly selecting a path h according to

p(h) =
1
|Pδ|

∑
pt∈Pδ

pt(h), (4)

and returning the prediction h(x). Note that com-
puting and storing p directly is not efficient. To
sample from p, we first choose pt ∈ Pδ uniformly
at random and then sample a path h according to
that pt. Sampling a path according to pt can be
done efficiently using the weight pushing algo-
rithm. Note that once an input x is received, the
distribution p over the path experts h induces a
probability distribution px over the output space
Y . It is not hard to see that sampling a predic-
tion y according to px is statistically equivalent to
first sampling h according to p and then predicting
h(x). We will denote by HRand the randomized
hypothesis thereby generated.

An inherent drawback of randomized solutions
such as the one just described is that for the same
input x the user can receive different predictions
over time. Randomized solutions are also typi-
cally more costly to store. A collection of distri-
butions P can also be used to define a determin-
istic prediction rule based on the scoring function
approach. The majority vote scoring function is
defined by

h̃MVote(x,y) =
l∏

k=1

( 1
|Pδ|

∑
pt∈Pδ

p∑
j=1

wt,kj1hkj (x)=yk
)
. (5)

The majority vote algorithm denoted by HMVote
is then defined for all x ∈ X , by HMVote(x) =
argmaxy∈Y h̃MVote(x,y). For an expert automa-
ton accepting all path experts such as that of Fig-
ure 1, the maximizer of h̃MVote can be found very
efficiently by choosing y such that yk has the max-
imum weight in position k.

In the next section, we present learning guarantees
for HRand and HMVote. For a more extensive dis-

4



cussion of alternative prediction rules, see (Cortes
et al., 2014a).

3.4 Batch learning guarantees

We first present learning bounds for the random-
ized prediction rule HRand. Next, we upper bound
the generalization error of HMVote in terms of that
of HRand.
Theorem 1. For any δ > 0, with probabil-
ity at least 1 − δ over the choice of the sample
((x1,y1), . . . , (xT ,yT )) drawn i.i.d. according to
D, the following inequalities hold:

E[L(HRand(x),y)]≤ inf
h∈H

E[L(h(x),y)]

+ 2M

√
l log p

T
+ 2M

√
log 2δ

T
.

For the normalized Hamming loss LHam, the
bound of Theorem 1 holds with M = 1.

We now upper bound the generalization error of
the majority-vote algorithm HMVote in terms of
that of the randomized algorithm HRand, which,
combined with Theorem 1, immediately yields
generalization bounds for the majority-vote algo-
rithm HMVote.
Proposition 2. The following inequality relates
the generalization error of the majority-vote algo-
rithm to that of the randomized one:

E[LHam(HMVote(x),y)]≤2 E[LHam(HRand(x),y)],
where the expectations are taken over (x,y)∼D
and h∼p.
Proposition 2 suggests that the price to pay for
derandomization is a factor of 2. More refined
and more favorable guarantees can be proven
for the majority-vote algorithm (Cortes et al.,
2014a).

4 Boosting-style algorithm

In this section, we devise a boosting-style al-
gorithm for our ensemble structured prediction
problem. The variants of AdaBoost for multi-
class classification such as AdaBoost.MH or Ad-
aBoost.MR (Freund and Schapire, 1997; Schapire
and Singer, 1999; Schapire and Singer, 2000) can-
not be readily applied in this context. First, the
number of classes to consider here is quite large,

as in all structured prediction problems, since it is
exponential in the number of substructures l. For
example, in the case of the pronunciation prob-
lem where the number of phonemes for English
is in the order of 50, the number of classes is 50l.
But, the objective function for AdaBoost.MH or
AdaBoost.MR as well as the main steps of the al-
gorithms include a sum over all possible labels,
whose computational cost in this context would be
prohibitive. Second, the loss function we consider
is the normalized Hamming loss over the substruc-
tures predictions, which does not match the multi-
class losses for the variants of AdaBoost.2 Finally,
the natural base hypotheses for this problem admit
a structure that can be exploited to devise a more
efficient solution, which of course was not part of
the original considerations for the design of these
variants of AdaBoost.

4.1 Hypothesis sets

The predictor HBoost returned by our boosting al-
gorithm is based on a scoring function h̃ : X ×
Y → R, which, as for standard ensemble algo-
rithms such as AdaBoost, is a convex combination
of base scoring functions h̃t: h̃ =

∑T
t=1 αth̃t, with

αt ≥ 0. The base scoring functions used in our al-
gorithm have the form

∀(x,y) ∈ X × Y, h̃t(x,y) =
l∑

k=1

h̃kt (x,y).

In particular, these can be derived from the path
experts in H by letting hkt (x,y) = 1hkt (x)=yk .
Thus, the score assigned to y by the base scoring
function h̃t is the number of positions at which y
matches the prediction of path expert ht given in-
put x. HBoost is defined as follows in terms of h̃ or
hts:

∀x ∈ X ,HBoost(x) = argmax
y∈Y

h̃(x,y)

We remark that the analysis and algorithm pre-
sented in this section are also applicable with a
scoring function that is the product of the scores

2(Schapire and Singer, 1999) also present an algorithm
using the Hamming loss for multi-class classification, but that
is a Hamming loss over the set of classes and differs from
the loss function relevant to our problem. Additionally, the
main steps of that algorithm are also based on a sum over all
classes.

5



at each substructure k as opposed to a sum, that
is,

h̃(x,y) =
l∏

k=1

(
T∑

t=1

αth̃
k
t (x,y)

)
.

This can be used for example in the case where
the experts are derived from probabilistic mod-
els.

4.2 ESPBoost algorithm

To simplify our exposition, the algorithm that
we now present uses base learners of the form
hkt (x,y) = 1hkt (x)=yk . The general case can be
handled in the same fashion with the only dif-
ference being the definition of the direction and
step of the optimization procedure described be-
low. For any i ∈ [1,m] and k ∈ [1, l], we
define the margin of h̃k for point (xi,yi) by
ρ(h̃k,xi,yi) = h̃k(xi, yki )−maxyk 6=yki h̃

k(xi, yk).
We first derive an upper bound on the empirical
normalized Hamming loss of a hypothesis HBoost,
with h̃ =

∑T
t=1 αth̃t.

Lemma 3. The following upper bound holds for
the empirical normalized Hamming loss of the hy-
pothesis HBoost:

E
(x,y)∼S

[LHam(HBoost(x),y)]

≤ 1
ml

m∑
i=1

l∑
k=1

exp
(
−

T∑
t=1

αtρ(h̃kt ,xi,yi)
)
.

The proof of this lemma as well as that of sev-
eral other theorems related to this algorithm can
be found in (Cortes et al., 2014a).

In view of this upper bound, we consider the ob-
jective function F : RN → R defined for all α =
(α1, . . . , αN ) ∈ RN by

F (α) =
1

ml

m∑
i=1

l∑
k=1

exp
(
−

N∑
j=1

αjρ(h̃kj ,xi,yi)
)
,

where h1, . . . , hN denote the set of all path ex-
perts in H. F is a convex and differentiable func-
tion of α. Our algorithm, ESPBoost (Ensemble
Structured Prediction Boosting), is defined by the
application of coordinate descent to the objective
F . Algorithm 1 shows the pseudocode of the ESP-
Boost.

Algorithm 1 ESPBoost Algorithm
Inputs: S = ((x1,y1), . . . , (xm,ym)); set of
experts {h1, . . . , hp}
for i = 1 to m and k = 1 to l do
D1(i, k)← 1ml

end for
for t = 1 to T do

ht ← argminh∈H E(i,k)∼Dt [1hk(xi) 6=yki ]
�t ← E(i,k)∼Dt [1hkt (xi) 6=yki ]
αt ← 12 log 1−�t�t
Zt ← 2

√
�t(1− �t)

for i = 1 to m and k = 1 to l do
Dt+1(i, k)← exp(−αtρ(ehkt ,xi,yi))Dt(i,k)Zt

end for
end for
Return h̃ =

∑T
t=1 αth̃t

Let αt−1 ∈ RN denote the vector obtained after
t − 1 iterations and et the tth unit vector in RN .
We denote byDt the distribution over [1,m]×[1, l]
defined by

Dt(i, k) =
1

ml exp
(
−∑t−1u=1 αuρ(h̃ku,xi,yi))

At−1

where At−1 is a normalization factor, At−1 =
1

ml

∑m
i=1

∑l
k=1 exp

( − ∑t−1u=1 αuρ(h̃ku,xi,yi)).
The direction et selected at the tth round is the
one minimizing the directional derivative, that
is

dF (αt−1 + ηet)
dη

∣∣∣∣
η=0

=−
m∑

i=1

l∑
k=1

ρ(h̃kt ,xi,yi)Dt(i, k)At−1

=
[
2

∑
i,k:hkt (xi) 6=yki

Dt(i, k)− 1
]
At−1

=(2�t − 1)At−1,
where �t is the average error of ht given by

�t =
m∑

i=1

l∑
k=1

Dt(i, k)1hkt (xi) 6=yki
= E

(i,k)∼Dt
[1hkt (xi) 6=yki ].

The remaining steps of our algorithm can be de-
termined as in the case of AdaBoost. In particu-
lar, given the direction et, the best step αt is ob-
tained by solving the equation dF (αt−1+αtet)dαt =

6



0, which admits the closed-form solution αt =
1
2 log

1−�t
�t

. The distribution Dt+1 can be ex-
pressed in terms of Dt with the normalization fac-
tor Zt = 2

√
�t(1− �t).

Our weak learning assumption in this context is
that there exists γ > 0 such that at each round,
�t verifies �t < 12 − γ. Note that, at each round,
the path expert ht with the smallest error �t can be
determined easily and efficiently by first finding
for each substructure k, the hkt that is the best with
respect to the distribution weights Dt(i, k).
Observe that, while the steps of our algorithm are
syntactically close to those of AdaBoost and its
multi-class variants, our algorithm is distinct and
does not require sums over the exponential number
of all possible labelings of the substructures and is
quite efficient.

4.3 Learning guarantees

We have derived both a margin-based generaliza-
tion bound in support of the ESPBoost algorithm
and a bound on the empirical margin loss.

For any ρ > 0, define the empirical margin loss of
HBoost by the following:

R̂ρ

(
h̃

‖α‖1

)
=

1
ml

m∑
i=1

l∑
k=1

1
ρ(ehk,xi,yi)≤ρ‖α‖1 ,

where h̃ is the corresponding scoring function.
The following theorem can be proven using the
multi-class classification bounds of (Koltchinskii
and Panchenko, 2002; Mohri et al., 2012) as can
be shown in (Cortes et al., 2014a).
Theorem 4. Let F denote the set of func-
tions HBoost with h̃ =

∑T
t=1 αth̃t for some

α1, . . . , αt ≥ 0 and ht ∈ H for all t ∈ [1, T ]. Fix
ρ > 0. Then, for any δ > 0, with probability at
least 1−δ, the following holds for all HBoost ∈ F:

E
(x,y)∼D

[LHam(HBoost(x),y)] ≤ R̂ρ
(

h̃

‖α‖1

)

+ 2ρl

l∑
k=1

|Yk|2Rm(Hk) +
√

log lδ
2m

,

where Rm(Hk) denotes the Rademacher com-
plexity of the class of functions

Hk = {x 7→ h̃kt : j ∈ [1, p], y ∈ Yk}.

Table 1: Average Normalized Hamming Loss,
ADS1 and ADS2. βADS1 = 0.95, βADS2 = 0.95,
TSLE = 100, δ = 0.05.

ADS1, m = 200 ADS2, m = 200
HMVote 0.0197 ± 0.00002 0.2172 ± 0.00983
HFPL 0.0228 ± 0.00947 0.2517 ± 0.05322
HCV 0.0197 ± 0.00002 0.2385 ± 0.00002
HFPL-CV 0.0741 ± 0.04087 0.4001 ± 0.00028
HESPBoost 0.0197 ± 0.00002 0.2267 ± 0.00834
HSLE 0.5641 ± 0.00044 0.2500 ± 0.05003
HRand 0.1112 ± 0.00540 0.4000 ± 0.00018
Best hj 0.5635 ± 0.00004 0.4000

This theorem provides a margin-based guarantee
for convex ensembles such as those returned by
ESPBoost. The following theorem further pro-
vides an upper bound on the empirical margin loss
for ESPBoost.
Theorem 5. Let h̃ denote the scoring function re-
turned by ESPBoost after T ≥ 1 rounds. Then, for
any ρ > 0, the following inequality holds:

R̂ρ

(
h̃

‖α‖1

)
≤ 2T

T∏
t=1

√
�1−ρt (1− �t)1+ρ.

As in the case of AdaBoost (Schapire et al., 1997),
it can be shown that for ρ < γ, �1−ρt (1− �t)1+ρ ≤
(1 − 2γ)1−ρ(1 + 2γ)1+ρ < 1 and the right-hand
side of this bound decreases exponentially with
T .

5 Experiments

We used a number of artificial and real-world
data sets for our experiments. For each data set,
we performed 10-fold cross-validation with dis-
joint training sets.3 We report the average error
for each task. In addition to the HMVote, HRand
and HESPBoost hypotheses, we experimented with
two algorithms discussed in more detail in (Cortes
et al., 2014a): a cross-validation on-line-to-
batch conversion of the WMWP algorithm, HCV,
a majority-vote on-line-to-batch conversion with
FPL, HFPL, and a cross-validation on-line-to-
batch conversion with FPL, HFPL-CV. Finally, we
compare with the HSLE algorithm of (Nguyen and
Guo, 2007).

5.1 Artificial data sets

Our artificial data set, ADS1 and ADS2 simulate
the scenarios described in Section 1. In ADS1 the

3For the OCR data set, these subsets are predefined.

7



kth expert has a high accuracy on the kth position,
in ADS2 an expert has low accuracy in a fixed set
of positions.

For the first artificial data set, ADS1, we used lo-
cal experts h1, . . . , hp with p = 5. To generate
the data we chose an arbitrary Markov chain over
the English alphabet and sampled 40,000 random
sequences each consisting of 10 symbols. Each
of the five experts was designed to have a certain
probability of making a mistake at each position in
the sequence. Expert hj correctly predicted posi-
tions 2j−1 and 2j with probability 0.97 and other
positions with probability 0.5. We forced experts
to make similar mistakes by making them select an
adjacent alphabet symbol in case of an error. For
example, when a mistake was made on a symbol b,
the expert prediction was forced to be either a or c.
The second artificial data set, ADS2, modeled the
case of rather poor experts. ADS2 was generated
in the same way as ADS1, but the expert predic-
tions were different. This time each expert made
mistakes at four out of the ten distinct random po-
sitions in each sequence.

Table 1 reports the results of our experiments.
For all experiments with the algorithms HRand,
HMVote, and HCV, we ran the WMWP algorithm
for T = m rounds with the β values listed in
the caption of Table 1, generating distributions
P ⊆ {p1, . . . , pT }. For P we used the collection
of all suffix sets {pt, . . . , pT } and δ = 0.05. For
the algorithms based on FPL, we used � = 0.5/pl.
The same parameter choices were used for the
subsequent experiments.

As can be seen from Table 1, in both cases,
HMVote, our majority-vote algorithm based on our
on-line-to-batch conversion using the WMWP al-
gorithm (together with most of the other on-line
based algorithms), yields a significant improve-
ment over the best expert. It also outperforms
HSLE, which in the case of ADS1 even fails to
outperform the best hj . After 100 iterations on
ADS1, the ensemble learned by HSLE consists of
a single expert, which is why it leads to such a
poor performance.

It is also worth pointing out that HFPL-CV and
HRand fail to outperform the best model on ADS2
set. This is in total agreement with our theoreti-
cal analysis since, in this case, any path expert has
exactly the same performance and the error of the

Table 2: Average Normalized Hamming Loss for
ADS3. βADS1 = 0.95, βADS2 = 0.95, TSLE =
100, δ = 0.05.

HMVote 0.1788 ± 0.00004
HFPL 0.2189 ± 0.04097
HCV 0.1788 ± 0.00004
HFPL-CV 0.3148 ± 0.00387
HESPBoost 0.1831 ± 0.00240
HSLE 0.1954 ± 0.00185
HRand 0.3196 ± 0.00018
Best hj 0.2957 ± 0.00005

Table 3: Average Normalized Hamming Loss,
PDS1 and PDS2. βPDS1 = 0.85, βPDS2 = 0.97,
TSLE = 100, δ = 0.05.

PDS1, m = 130 PDS2, m = 400
HMVote 0.2225 ± 0.00301 0.2323 ± 0.00069
HFPL 0.2657 ± 0.07947 0.2337 ± 0.00229
HCV 0.2316 ± 0.00189 0.2364 ± 0.00080
HFPL-CV 0.4451 ± 0.02743 0.4090 ± 0.01388
HESPBoost 0.3625 ± 0.01054 0.3499 ± 0.00509
HSLE 0.3130 ± 0.05137 0.3308 ± 0.03182
HRand 0.4713 ± 0.00360 0.4607 ± 0.00131
Best hj 0.3449 ± 0.00368 0.3413 ± 0.00067

best path expert is an asymptotic upper bound on
the errors of these algorithms. The superior perfor-
mance of the majority-vote-based algorithms sug-
gests that these algorithms may have an advantage
over other prediction rules beyond what is sug-
gested by our learning bounds.

We also synthesized a third data set, ADS3. Here,
we simulated the case where each expert special-
ized in predicting some subset of the labels. In
particular, we generated 40,000 random sequences
over the English alphabet in the same way as for
ADS1 and ADS2. To generate expert predictions,
we partitioned the alphabet into 5 disjoint subsets
Aj . Expert j always correctly predicted the label
in Aj and the probability of correctly predicting
the label not in Aj was set to 0.7. To train the en-
semble algorithms, we used a training set of size
m = 200.

The results are presented in Table 2. HMVote, HCV
and HESPBoost achieve the best performance on
this data set with a considerable improvement in
accuracy over the best expert hj . We also ob-
serve as for the ADS2 experiment that HRand and
HFPL-CV fail to outperform the best model and ap-
proach the accuracy of the best path expert only
asymptotically.

8



Table 4: Average edit distance, PDS1 and PDS2.
βPDS1 = 0.85, βPDS2 = 0.97, TSLE = 100,
δ = 0.05.

PDS1, m = 130 PDS2, m = 400
HMVote 0.8395 ± 0.01076 0.9626 ± 0.00341
HFPL 1.0158 ± 0.34379 0.9744 ± 0.01277
HCV 0.8668 ± 0.00553 0.9840 ± 0.00364
HFPL-CV 1.8044 ± 0.09315 1.8625 ± 0.06016
HESPBoost 1.3977 ± 0.06017 1.4092 ± 0.04352
HSLE 1.1762 ± 0.12530 1.2477 ± 0.12267
HRand 1.8962 ± 0.01064 2.0838 ± 0.00518
Best hj 1.2163 ± 0.00619 1.2883 ± 0.00219

5.2 Pronunciation data sets

We had access to two proprietary pronunciation
data sets, PDS1 and PDS2. In both sets, each
example is an English word, typically a proper
name. For each word, 20 possible phonemic se-
quences are available, ranked by some pronuncia-
tion model. Since the true pronunciation was not
available, we set the top sequence to be the tar-
get label and used the remaining as the predictions
made by the experts. The only difference between
PDS1 and PDS2 is their size: 1,313 words for
PDS1 and 6,354 for PDS2.

In both cases, on-line based algorithms, specif-
ically HMVote, significantly outperform the best
model as well as HSLE, see Table 3. The poor
performance of HESPBoost is due to the fact that the
weak learning assumption is violated after 5-8 iter-
ations and hence the algorithm terminates.

It can be argued that for this task the edit-distance
is a more suitable measure of performance than
the average Hamming loss. Thus, we also re-
port the results of our experiments in terms of the
edit-distance in Table 4. Remarkably, our on-line
based algorithms achieve a comparable improve-
ment over the performance of the best model in
the case of edit-distance as well.

5.3 OCR data set

Rob Kassel’s OCR data set is available for down-
load from http://ai.stanford.edu/˜btaskar/
ocr/. It contains 6,877 word instances with a to-
tal of 52,152 characters. Each character is rep-
resented by 16 × 8 = 128 binary pixels. The
task is to predict a word given its sequence of
pixel vectors. To generate experts, we used several
software packages: CRFsuite (Okazaki, 2007) and
SVMstruct, SVMmulticlass (Joachims, 2008), and

Table 5: Average Normalized Hamming Loss,
TR1 and TR2. βTR1 = 0.95, βTR2 = 0.98,
TSLE = 100, δ = 0.05.

TR1, m = 800 TR2, m = 1000
HMVote 0.0850 ± 0.00096 0.0746 ± 0.00014
HFPL 0.0859 ± 0.00110 0.0769 ± 0.00218
HCV 0.0843 ± 0.00006 0.0741 ± 0.00011
HFPL-CV 0.1093 ± 0.00129 0.1550 ± 0.00182
HESPBoost 0.1041 ± 0.00056 0.1414 ± 0.00233
HSLE 0.0778 ± 0.00934 0.0814 ± 0.02558
HRand 0.1128 ± 0.00048 0.1652 ± 0.00077
Best hj 0.1032 ± 0.00007 0.1415 ± 0.00005

the Stanford Classifier (Rafferty et al., 2014). We
trained these algorithms on each of the predefined
folds of the data set and generated predictions on
the test fold using the resulting models.

Our results (see (Cortes et al., 2014a)) show that
ensemble methods lead only to a small improve-
ment in performance over the best hj . This is be-
cause here the best model hj dominates all other
experts and ensemble methods cannot benefit from
patching together different outputs.

5.4 Penn Treebank data set

The part-of-speech task, POS, consists of label-
ing each word of a sentence with its correct
part-of-speech tag. The Penn Treebank 2 data
set is available through LDC license at http:
//www.cis.upenn.edu/˜treebank/ and contains
251,854 sentences with a total of 6,080,493 tokens
and 45 different parts-of-speech.

For the first experiment, TR1, we used 4 disjoint
training sets to produce 4 SVMmulticlass mod-
els and 4 maximum entropy models using the
Stanford Classifier. We also used the union of
these training sets to devise one CRFsuite model.
For the second experiment, TR2, we trained 5
SVMstruct models. The same features were used
for both experiments. For the SVM algorithms, we
generated 267,214 bag-of-word binary features.
The Stanford Classifier and CRFsuite packages
use internal routines to generate features.

The results of the experiments are summarized in
Table 5. For TR1, our on-line ensemble meth-
ods improve over the best model. Note that HSLE
has the best average loss over 10 runs for this ex-
periment. This comes at a price of much higher
standard deviation which does not allow us to con-
clude that the difference in performance between
our methods and HSLE is statistically significant.

9



Table 6: Average Normalized Hamming Loss,
SDS. l ≥ 4, β = 0.97, δ = 0.05, TSLE = 100.

p = 5, m = 1500 p = 10, m = 1200
HMVote 0.2465 ± 0.00248 0.2606 ± 0.00320
HFPL 0.2500 ± 0.00248 0.2622 ± 0.00316
HCV 0.2504 ± 0.00576 0.2755 ± 0.00212
HFPL-CV 0.2726 ± 0.00839 0.3219 ± 0.01176
HESPBoost 0.2572 ± 0.00062 0.2864 ± 0.00103
HSLE 0.2572 ± 0.00061 0.2864 ± 0.00102
HRand 0.2877 ± 0.00480 0.3430 ± 0.00468
Best hj 0.2573 ± 0.00060 0.2865 ± 0.00101

In fact, on two runs, HSLE chooses an ensemble
consisting of a single expert and fails to outper-
form the best model.

5.5 Speech recognition data set

For our last set of experiments, we used another
proprietary speech recognition data set, SDS. Each
example in this data set is represented by a se-
quence of length l ∈ [2, 15]. Therefore, for train-
ing we padded the true labels and the expert pre-
dictions to normalize the sequence lengths. For
each of the 22,298 examples, there are between
2 and 251 expert predictions available. Since the
ensemble methods we presented assume that the
predictions of all p experts are available for each
example in the training and test sets, we needed to
restrict ourselves to the subsets of the data where
at least some fixed number of expert predictions
were available. In particular, we considered p =
5, 10, 20 and 50. For each value of p we used only
the top p experts in our ensembles.

Our initial experiments showed that, as in the case
of OCR data set, ensemble methods offer only a
modest increase in performance over the best hj .
This is again largely due to the dominant perfor-
mance of the best expert hj . However, it was ob-
served that the accuracy of the best model is a de-
creasing function of l, suggesting that ensemble
algorithm may be used to improve performance
for longer sequences. Subsequent experiments
show that this is indeed the case: when training
and testing with l ≥ 4, ensemble algorithms out-
perform the best model. Table 6 and Table 7 sum-
marize these results for p = 5, 10, 20, 50.

Our results suggest that the following simple
scheme can be used: for short sequences use the
best expert model and for longer sequences, use
the ensemble model. A more elaborate variant of
this algorithm can be derived based on the obser-

Table 7: Average Normalized Hamming Loss,
SDS. l ≥ 4, β = 0.97,δ = 0.05, TSLE = 100.

p = 20, m = 900 p = 50, m = 700
HMVote 0.2773 ± 0.00139 0.3217 ± 0.00375
HFPL 0.2797 ± 0.00154 0.3189 ± 0.00344
HCV 0.2986 ± 0.00075 0.3401 ± 0.00054
HFPL-CV 0.3816 ± 0.01457 0.4451 ± 0.01360
HESPBoost 0.3115 ± 0.00089 0.3426 ± 0.00071
HSLE 0.3114 ± 0.00087 0.3425 ± 0.00076
HRand 0.3977 ± 0.00302 0.4608 ± 0.00303
Best hj 0.3116 ± 0.00087 0.3427 ± 0.00077

vation that the improvement in accuracy of the en-
semble model over the best expert increases with
the number of experts available.

6 Conclusion

We presented a broad analysis of the problem of
ensemble structured prediction, including a series
of algorithms with learning guarantees and exten-
sive experiments. Our results show that our al-
gorithms, most notably HMVote, can result in sig-
nificant benefits in several tasks, which can be of
a critical practical importance. We also reported
very favorable results for HMVote when used with
the edit-distance, which is the standard loss used
in many applications. A natural extension of this
work consists of devising new algorithms and pro-
viding learning guarantees specific to other loss
functions such as the edit-distance. While we
aimed for an exhaustive study, including multi-
ple on-learning algorithms, different conversions
to batch and derandomizations, we are aware that
the problem we studied is very rich and admits
many more facets and scenarios that we plan to in-
vestigate in the future. Finally, the boosting-style
algorithm we presented can be enhanced using re-
cent theoretical and algorithmic results on deep
boosting (Cortes et al., 2014b).

Acknowledgments

We warmly thank our colleagues Francoise Bea-
ufays and Fuchun Peng for kindly extracting and
making available to us the pronunciation data sets,
Cyril Allauzen for providing us with the speech
recognition data, and Richard Sproat and Brian
Roark for help with other data sets. This work was
partly funded by the NSF award IIS-1117591 and
the NSERC PGS D3 award.

10



References

[Breiman1996] Leo Breiman. 1996. Bagging predic-
tors. Machine Learning, 24(2):123–140.

[Caruana et al.2004] R. Caruana, A. Niculescu-Mizil,
G. Crew, and A. Ksikes. 2004. Ensemble selection
from libraries of models. In Proceedings of ICML,
pages 18–.

[Cesa-Bianchi et al.2004] N. Cesa-Bianchi, A. Con-
coni, and C. Gentile. 2004. On the generalization abil-
ity of on-line learning algorithms. IEEE Transactions
on Information Theory, 50(9):2050–2057.

[Collins and Koo2005] Michael Collins and Terry Koo.
2005. Discriminative reranking for natural language
parsing. Computational Linguistics, 31(1):25–70.

[Collins2002] M. Collins. 2002. Discriminative train-
ing methods for hidden Markov models: theory and ex-
periments with perceptron algorithms. In Proceedings
of ACL, pages 1–8.

[Cortes et al.2005] C. Cortes, M. Mohri, and J. Weston.
2005. A general regression technique for learning
transductions. In Proceedings of ICML 2005, pages
153–160, New York, NY, USA. ACM.

[Cortes et al.2014a] Corinna Cortes, Vitaly Kuznetsov,
and Mehryar Mohri. 2014a. Ensemble methods for
structured prediction. In Proceedings of ICML.

[Cortes et al.2014b] Corinna Cortes, Mehryar Mohri,
and Umar Syed. 2014b. Deep boosting. In Proceed-
ings of the Fourteenth International Conference on Ma-
chine Learning (ICML 2014).

[Dekel and Singer2005] O. Dekel and Y. Singer. 2005.
Data-driven online to batch conversion. In Advances in
NIPS 18, pages 1207–1216.

[Fiscus1997] Jonathan G Fiscus. 1997. Post-
processing system to yield reduced word error rates:
Recognizer output voting error reduction (rover). In
Proceedings of the 1997 IEEE ASRU Workshop, pages
347–354, Santa Barbara, CA.

[Freund and Schapire1997] Y. Freund and R. Schapire.
1997. A decision-theoretic generalization of on-line
learning and application to boosting. Journal of Com-
puter and System Sciences, 55(1):119–139.

[Freund et al.2004] Yoav Freund, Yishay Mansour, and
Robert E. Schapire. 2004. Generalization bounds for
averaged classifiers. The Annals of Statistics, 32:1698–
1722.

[Ghoshal et al.2009] Arnab Ghoshal, Martin Jansche,
Sanjeev Khudanpur, Michael Riley, and Morgan Ulin-
ski. 2009. Web-derived pronunciations. In Proceed-
ings of ICASSP, pages 4289–4292.

[Joachims2008] T. Joachims. 2008. Support vector
machines for complex outputs.

[Kalai and Vempala2005] A. Kalai and S. Vempala.
2005. Efficient algorithms for online decision prob-

lems. Journal of Computer and System Sciences,
71(3):291–307.

[Kocev et al.2013] D. Kocev, C. Vens, J. Struyf, and
S. Deroski. 2013. Tree ensembles for predicting struc-
tured outputs. Pattern Recognition, 46(3):817–833,
March.

[Koltchinskii and Panchenko2002] Vladmir Koltchin-
skii and Dmitry Panchenko. 2002. Empirical margin
distributions and bounding the generalization error of
combined classifiers. Annals of Statistics, 30.

[Lafferty et al.2001] J. Lafferty, A. McCallum, and
F. Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of ICML, pages 282–289.

[Littlestone and Warmuth1994] N. Littlestone and
M. Warmuth. 1994. The weighted majority algorithm.
Information and Computation, 108(2):212–261.

[Littlestone1989] N. Littlestone. 1989. From on-line
to batch learning. In Proceedings of COLT 2, pages
269–284.

[MacKay1991] David J. C. MacKay. 1991. Bayesian
methods for adaptive models. Ph.D. thesis, California
Institute of Technology.

[MacKay1997] David J.C. MacKay. 1997. Ensemble
learning for hidden markov models. Technical report,
Cavendish Laboratory, Cambridge UK.

[Mohri et al.2008] Mehryar Mohri, Fernando C. N.
Pereira, and Michael Riley. 2008. Speech recognition
with weighted finite-state transducers. In Handbook on
Speech Processing and Speech Communication, Part
E: Speech recognition. Springer-Verlag.

[Mohri et al.2012] Mehryar Mohri, Afshin Ros-
tamizadeh, and Ameet Talwalkar. 2012. Foundations
of Machine Learning. The MIT Press.

[Mohri1997] Mehryar Mohri. 1997. Finite-state trans-
ducers in language and speech processing. Computa-
tional Linguistics, 23(2):269–311.

[Nguyen and Guo2007] N. Nguyen and Y. Guo. 2007.
Comparison of sequence labeling algorithms and ex-
tensions. In Proceedings of ICML, pages 681–688.

[Okazaki2007] N. Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (crfs).

[Petrov2010] Slav Petrov. 2010. Products of random
latent variable grammars. In HLT-NAACL, pages 19–
27.

[Rafferty et al.2014] A. Rafferty, A. Kleeman, J. Finkel,
and C. Manning. 2014. Stanford classifer.

[Sagae and Lavie2006] K. Sagae and A. Lavie. 2006.
Parser combination by reparsing. In Proceedings of
HLT/NAACL, pages 129–132.

[Schapire and Singer1999] Robert E. Schapire and
Yoram Singer. 1999. Improved boosting algorithms

11



using confidence-rated predictions. Machine Learning,
37(3):297–336.

[Schapire and Singer2000] Robert E. Schapire and
Yoram Singer. 2000. Boostexter: A boosting-based
system for text categorization. Machine Learning,
39(2-3):135–168.

[Schapire et al.1997] Robert E. Schapire, Yoav Freund,
Peter Bartlett, and Wee Sun Lee. 1997. Boosting the
margin: A new explanation for the effectiveness of vot-
ing methods. In ICML, pages 322–330.

[Smyth and Wolpert1999] Padhraic Smyth and David
Wolpert. 1999. Linearly combining density estimators
via stacking. Machine Learning, 36:59–83, July.

[Takimoto and Warmuth2003] E. Takimoto and M. K.
Warmuth. 2003. Path kernels and multiplicative up-
dates. JMLR, 4:773–818.

[Taskar et al.2004] B. Taskar, C. Guestrin, and
D. Koller. 2004. Max-margin Markov networks. In
Advances in NIPS 16. MIT Press, Cambridge, MA.

[Tsochantaridis et al.2005] I. Tsochantaridis,
T. Joachims, T. Hofmann, and Y. Altun. 2005.
Large margin methods for structured and interde-
pendent output variables. JMLR, 6:1453–1484,
December.

[Wang et al.2007] Q. Wang, D. Lin, and D. Schuur-
mans. 2007. Simple training of dependency parsers
via structured boosting. In Proceedings of IJCAI 20,
pages 1756–1762.

[Zeman and Žabokrtský2005] D. Zeman and
Z. Žabokrtský. 2005. Improving parsing accu-
racy by combining diverse dependency parsers. In
Proceedings of IWPT 9, pages 171–178.

[Zhang et al.2009] H. Zhang, M. Zhang, C. Tan, and
H. Li. 2009. K-best combination of syntactic parsers.
In Proceedings of EMNLP: Volume 3, pages 1552–
1560.

12


