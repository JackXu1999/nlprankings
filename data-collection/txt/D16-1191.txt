



















































A Graph Degeneracy-based Approach to Keyword Extraction


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1860–1870,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

A Graph Degeneracy-based Approach to Keyword Extraction ∗

Antoine J.-P. Tixier1, Fragkiskos D. Malliaros1,2, Michalis Vazirgiannis1
1Computer Science Laboratory, École Polytechnique, Palaiseau, France

2Department of Computer Science and Engineering, UC San Diego, La Jolla, CA, USA{
anti5662,fmalliaros,mvazirg

}
@lix.polytechnique.fr

Abstract

We operate a change of paradigm and hy-
pothesize that keywords are more likely to be
found among influential nodes of a graph-of-
words rather than among its nodes high on
eigenvector-related centrality measures. To
test this hypothesis, we introduce unsuper-
vised techniques that capitalize on graph de-
generacy. Our methods strongly and sig-
nificantly outperform all baselines on two
datasets (short and medium size documents),
and reach best performance on the third one
(long documents).

1 Introduction

Keyword extraction is a central task in NLP. It finds
applications from information retrieval (notably web
search) to text classification, summarization, and vi-
sualization. In this study, we focus on the task of
unsupervised single-document keyword extraction.
Following (Mihalcea and Tarau, 2004), we concen-
trate on keywords only, letting the task of keyphrase
reconstruction as a post-processing step.

More precisely, while we capitalize on a graph
representation of text like several previous ap-
proaches, we deviate from them by making the as-
sumption that keywords are not found among pres-
tigious nodes (or more generally, nodes high on
eigenvector-related centrality metrics), but rather
among influential nodes. Those nodes may not have
many important connections (like their prestigious
counterparts), but they are ideally placed at the core

∗This research is supported in part by the OpenPaaS::NG
project.

of the network. In other words, this switches the
objective from capturing the quality and quantity of
single node connections, to taking into account the
density and cohesiveness of groups of nodes. To op-
erate this change of paradigm, we propose several
algorithms that leverage the concept of graph degen-
eracy (Malliaros et al., 2016a).

Our contributions are threefold: (1) we propose
new unsupervised keyword extraction techniques
that reach state-of-the art performance, (2) we ap-
ply the K-truss algorithm to the task of keyword ex-
traction for the first time, and (3) we report new in-
sights on the interplay between window size, graph-
of-words structure, and performance.

2 Graph-of-Words Representation

Many ways of encoding text as a graph have been
explored in order to escape the very limiting term-
independence assumption made by the traditional
vector space model. In this study, we adopt the sem-
inal Graph-of-Words representation (GoW) of (Mi-
halcea and Tarau, 2004), for its simplicity, high his-
torical success, and above all because it was recently
used in several approaches that reached very good
performance on various tasks such as information
retrieval (Rousseau and Vazirgiannis, 2013), doc-
ument classification (Malliaros and Skianis, 2015;
Rousseau et al., 2015), event detection (Meladianos
et al., 2015), and keyword extraction (Rousseau and
Vazirgiannis, 2015).

While sophisticated variants of the GoW model
would be worth exploring (edge weights based on
mutual information or word embeddings, adaptive
window size, etc.), we aim here at making a bet-

1860



●

●

●

●

●

●

●

●

●

●

●

●

●

●

mathemat

aspect

computer−aid

share

trade

problem

statist

analysi

price

probabilist

characterist

seri

method

model

Edge weights

1
2
3
4
5

Core numbers

8
9
10
11

●

●

●

inf

shells

di
ffe

re
nc

e 
in

 s
he

ll 
si

ze
s

−
6 

   
 −

4 
   

 −
2 

   
  0

   
   

 2

(10 − 11) (9 − 10) (8 − 9)

●

●

●

●

0.
38

0.
42

0.
46

0.
50

dens

k−cores

de
ns

ity

11 10 9 8

●

● ● ●

● ●
●

● ●
●

●

●

● ●

2 4 6 8 10 12 14

70
90

11
0

13
0TR

nodes

TR
 s

co
re

s

elbow
top 33%

Mathematical aspects of computer-aided share 
trading. We consider problems of statistical 
analysis of share prices and propose probabilistic 
characteristics to describe the price series. We 
discuss three methods of mathematical modelling 
of price series with given probabilistic 
characteristics.

Core numbers TR scores
P R F1 P R F1

MAIN 0.86 0.55 0.67 ELB 1 0.18 0.31
INF 0.83 0.91 0.87

PER 1 0.45 0.63DENS 0.88 0.64 0.74

mathemat 11 price .1359

price 11 share .0948

probabilist 11 .0906

characterist 11 .0870

seri 11 .0860

method 11 mathemat .0812

model 11 analysi .0633

share 10 statist .0595

trade 9 method .0569

problem 9 problem .0560

statist 9 trade .0525

analysi 9 model .0493

aspect 8 computer-aid .0453 

computer-aid 8 aspect .0417

MAIN

INF

DENS

ELB

probabilist

characterist

seri

CR scores
P R F1

ELB 0.90 0.82 0.86

PER 1 0.45 0.63

mathemat 128

price 120

analysi 119

share 118

probabilist 112

characterist 112

statist 108

trade 97

problem 97

seri 94

method 85

computer-aid 76

model 66 

aspect 65

PER

ELB

PER

(a)

(c)

(b)

●

●
●

● ●
●

●
●

● ●
●

●
●

●

2 4 6 8 10 12 14

0.
04

0.
08

0.
12

CR
elbow
top 33%

nodes

C
R

 s
co

re
s

Figure 1: (a) Graph-of-words (W = 8) for document #1512 of Hulth2003 decomposed with k-core (non-(nouns and
adjectives) in italic). (b) Keywords extracted by each proposed and baseline method (human assigned keywords in

bold). (c) Selection criterion of each method except main (does not apply).

ter use of an existing representation of text, not at
proposing a new one. This is why, to demonstrate
the additional skill brought by our proposed meth-
ods, we stick to the basic GoW framework.

As shown in Figure 1 (a), the GoW representation
of (Mihalcea and Tarau, 2004) encodes a piece of
text as an undirected graph where nodes are unique
nouns and adjectives in the document, and where
there is an edge between two nodes if the terms they
represent co-occur within a window of predeter-
mined size W that is slided over the entire document
from start to finish, overspanning sentences. Fur-
thermore, edges are assigned integer weights match-
ing co-occurrence counts. This fully statistical ap-
proach is based on the Distributional Hypothesis
(Harris, 1954), that is, on the premise that the re-
lationship between words can be determined by the
frequency with which they share local contexts of
occurrence.

3 Graph Degeneracy

The concept of graph degeneracy was introduced
by (Seidman, 1983) with the k-core decomposition
technique and was first applied to the study of cohe-
sion in social networks. Here, we consider it as an
umbrella term also encompassing the K-truss algo-
rithm (Cohen, 2008). In what follows, G(V,E) is a
graph with |V | nodes and |E| edges. Note that for
graphs-of-words, the nodes V are labeled according
to the unique terms they represent.

3.1 k-core subgraph
A core of order k (or k-core) of G is a maximal con-
nected subgraph of G in which every vertex v has
at least degree k (Seidman, 1983). If the edges are
unweighted, the degree of v is simply equal to the
count of its neighbors, while in the weighted case,
the degree of v is the sum of the weights of its in-
cident edges. Note that with the definition of GoW
previously given, node degrees (and thus, k) are in-
tegers in both cases since edge weights are integers.

1861



As shown in Figure 2 (a), the k-core decomposi-
tion of G is the set of all its cores from 0 (G itself) to
kmax (its main core). It forms a hierarchy of nested
subgraphs whose cohesiveness and size respectively
increase and decrease with k (Seidman, 1983). The
main core of G is a coarse approximation of its dens-
est subgraph (Wang and Cheng, 2012), and should
be seen as a seedbed within which it is possible to
find more cohesive subgraphs (Seidman, 1983). Fi-
nally, the core number of a node is the highest order
of a k-core subgraph that contains this node.

3.2 K-truss subgraph
K-truss is a triangle-based extension of k-core in-
troduced by (Cohen, 2008). More precisely, the K-
truss subgraph of G is its largest subgraph where ev-
ery edge belongs to at least K−2 triangles. In other
words, every edge in the K-truss joins two vertices
that have at least K − 2 common neighbors. Com-
pared to k-core, K-truss thus does not iteratively
prune nodes out based on the number of their direct
links, but also based on the number of their shared
connections. This more accurately captures cohe-
siveness.

As a result, the K-trusses are smaller and denser
subgraphs than the k-cores, and the maximal K-
truss of G better approximates its densest subgraph.
In essence, and as illustrated in Figure 2 (b), the
K-trusses can be viewed as the essential parts of
the k-cores, i.e., what is left after the less cohesive
elements have been filtered out (Wang and Cheng,
2012). By analogy with k-core, the K-truss de-
composition of G is the set of all its K-trusses from
K = 2 to Kmax, and the truss number of an edge
is the highest order of a truss the edge belongs to.
By extension, we define the truss number of a node
as the maximum truss number of its incident edges.

3.3 k-shell
Depending on context, we will refer to the k-shell as
the part of the k-core (or truss) that is not included
in the (k + 1)-core (or truss).

3.4 Graph degeneracy and spreading influence
In social networks, it has been shown that the best
spreaders (i.e., the nodes able to propagate informa-
tion to a large portion of the network at minimal time
and cost) are not necessarily the highly connected

individuals (i.e., the hubs), but rather those located
at the core of the network (i.e., forming dense and
cohesive subgraphs with other central nodes), as in-
dicated by graph degeneracy algorithms (Kitsak et
al., 2010). Put differently, the spreading influence
of a node is related to its structural position within
the graph (density and cohesiveness) rather than to
its prestige (random walk-based degree). More re-
cently, (Malliaros et al., 2016b) found that the truss
number is an even better indicator of spreading in-
fluence than the core number. Motivated by these
findings, we posit that taking cohesiveness into ac-
count with the core and truss decomposition of a
graph-of-words could improve keyword extraction
performance. That way, by analogy with the notion
of influential spreaders in social networks, we hy-
pothesize that influential words in graphs-of-words
will act as representative keywords.

3.5 Complexity
(Batagelj and Zaveršnik, 2002) proposed O(|V | +
|E|) and O(|E| log |V |) time algorithms for k-core
decomposition in the unweighted (resp. weighted)
case. These algorithms are both O(|V |) in space.
Computing the K-truss decomposition is more ex-
pensive, and requires O(|E|1.5) time and O(|V | +
|E|) space (Wang and Cheng, 2012). Finally, build-
ing a graph-of-words is linear in time and space:
O(|V |W ) and O(|V |+ |E|), respectively.

4 Related Work and Point of Departure

TextRank. One of the most popular approaches to
the task of unsupervised single-document keyword
extraction is TextRank (Mihalcea and Tarau, 2004),
or TR in what follows. In TR, the nodes of graphs-
of-words are ranked based on a modified version of
the PageRank algorithm taking edge weights into ac-
count, and the top p% vertices are kept as keywords.
Limitations. TR has proven successful and has
been widely used and adapted. However, PageR-
ank, which is based on the concept of random walks
and is also related to eigenvector centrality, tends
to favor nodes with many important connections
regardless of any cohesiveness consideration. For
undirected graphs, it was even shown that PageR-
ank values are proportional to node degrees (Grol-
musz, 2015). While well suited to the task of

1862



3-core

2-core

1-core

Core number c = 1 Core number c = 2 Core number c = 3

Main k-core subgraph 
Main K-truss subgraph

a) b)

*
**

Figure 2: (a) k-core decomposition illustrative example. Note that while nodes * and ** have same degree (3), node
** makes a more influential spreader as it lies in a higher core than node *. (b) k-core versus K-truss. The main

K-truss subgraph can be considered as the core of the main core.

prestige-based ranking in the Web and social net-
works (among other things), PageRank may thus
not be ideal for keyword extraction. Indeed, a fun-
damental difference when dealing with text is the
paramount importance of cohesiveness: keywords
need not only to have many important connections
but also to form dense substructures with these con-
nections. Actually, most of the time, keywords are
n-grams (Rousseau and Vazirgiannis, 2015). There-
fore, we hypothesize that keywords are more likely
to be found among the influential spreaders of a
graph-of-words – as extracted by degeneracy-based
methods – rather than among the nodes high on
eigenvector-related centrality measures.

Topical vs. network coherence. Note that, un-
like a body of work that tackled the task of key-
word extraction and document summarization from
a topical coherence perspective (Celikyilmaz and
Hakkani-Tür, 2011; Chen et al., 2012; Christensen
et al., 2013), we deal here with network coherence,
a purely graph theoretic notion orthogonal to topical
coherence.

Graph degeneracy. The aforementioned limita-
tion of TR motivated the use of graph degeneracy to
not only extract central nodes, but also nodes form-
ing dense subgraphs. More precisely, (Rousseau and
Vazirgiannis, 2015) applied both unweighted and

weighted k-core decomposition on graphs-of-words
and retained the members of the main cores as key-
words. Best results were obtained in the weighted
case, with small main cores yielding good precision
but low recall, and significantly outperforming TR.
As expected, (Rousseau and Vazirgiannis, 2015) ob-
served that cores exhibited the desirable property of
containing “long-distance n-grams”.

In addition to superior quantitative performance,
another advantage of degeneracy-based techniques
(compared to TR, which extracts a constant percent-
age of nodes) is adaptability. Indeed, the size of
the main core (and more generally, of every level in
the hierarchy) depends on the structure of the graph-
of-words, which by nature is uniquely tailored to
the document at hand. Consequently, the distribu-
tion of the number of extracted keywords matches
more closely that of the human assigned keywords
(Rousseau and Vazirgiannis, 2015).
Limitations. Nevertheless, while (Rousseau and
Vazirgiannis, 2015) made great strides, it suffers the
following limitations: (1) k-core is good but not
best in capturing cohesiveness; (2) retaining only
the main core (or truss) is suboptimal, as one cannot
expect all the gold standard keywords to be found
within a unique subgraph – actually, many valu-
able keywords live in lower levels of the hierarchy
(see Figure 1); and (3) the coarseness of the k-core

1863



decomposition implies to work at a high granular-
ity level (selecting or discarding a large group of
words at a time), which diminishes the flexibility of
the extraction process and negatively impacts perfor-
mance.

Research objectives. To address the aforemen-
tioned limitations, we investigate in this study (1)
the use of K-truss to get a finer-grained hierarchy of
more cohesive subgraphs, in order to filter unwanted
words out of the upper levels and improve flexibil-
ity; (2) the automated selection of the best level in
the core (or truss) hierarchy to increase recall while
preserving most of the precision; and (3) the conver-
sion of node core (or truss) numbers into ranks, to
decrease granularity from the subgraph to the node
level, while still leveraging the valuable cohesive-
ness information captured by degeneracy.

5 Proposed Methods

In what follows, we introduce the strategies we de-
vised to implement our research objectives.

5.1 Density
With the underlying assumption that keywords are
found in cohesive subgraphs-of-words and are not
all contained in the main core (or truss), an intu-
itive, straightforward stopping criterion when going
down the core (or truss) hierarchy is a density-based
one. More precisely, it may be advantageous to go
down the hierarchy as long as the desirable cohesive-
ness properties of the main core or truss are main-
tained, and to stop when these properties are lost.
This strategy is more formally detailed in Algorithm
1, where G(V,E) is a graph-of-words, levels corre-
sponds to the vector of the nlevels unique k-core (or
truss) numbers of V sorted in decreasing order, and
the density of G is defined as:

density(G) =
|E|

|V | (|V | − 1) (1)

As can be seen in Figure 1 (c) and as detailed in
Algorithm 2, the elbow is determined as the most
distant point from the line joining the first and last
point of the curve. When all points are aligned, the
top level is retained (i.e., main core or truss). When
there are only two levels, the one giving the highest
density is returned.

Algorithm 1: dens method
Input : core (or truss) decomposition of G
Output: set of keywords

1 D← empty vector of length nlevels
2 for n← 1 to nlevels do
3 D[n]← density(levels[n]-core (or truss))
4 end
5 kbest ← levels[elbow(n,D[n])]
6 return kbest-core (or truss) of G as keywords

Algorithm 2: elbow
Input : set of |S| ≥ 2 points

S =
{
(x0, y0), ..., (x|S|, y|S|)

}

Output: xelbow
1 line←

{
(x0, y0); (x|S|, y|S|)

}

2 if |S| > 2 then
3 distance← empty vector of length |S|
4 s← 1
5 for (x, y) in S do
6 distance[s]← distance from (x, y) to

line
7 s← s+ 1
8 end
9 if ∃!s | distance[s] = max(distance) then

10 xelbow ← x | (x, y) is most distant from
line

11 else
12 xelbow ← x0
13 end
14 else
15 xelbow ← x | y is maximum
16 end
17 return xelbow

5.2 Inflexion
The Inflexion method (inf in what follows) is an
empirically-grounded heuristic that relies on detect-
ing changes in the variation of shell sizes (where size
denotes the number of nodes). Recall from subsec-
tion 3.3 than the k-shell is the part of the k-core
(or truss) that does not survive in the (k + 1)-core
(or truss), that is, the subgraph of G induced by the
nodes with core (or truss) number exactly equal to
k. In simple terms, the inf rule-of-thumb consists
in going down the hierarchy as long as the shells in-

1864



crease in size, and stopping otherwise. More pre-
cisely, inf is implemented as shown in Algorithm
3, by computing the consecutive differences in size
across the shells and selecting the first positive point
before the drop into the negative half (see Figure 1c).
If no point satisfies this requirement, the main core
(or truss) is extracted.

Algorithm 3: inf method
Input : core (or truss) decomposition of G
Output: set of keywords

1 CD← empty vector of length n− 1
2 for n← 1 to (nlevels − 1) do
3 kl ← levels[n+ 1]; ku ← levels[n]
4 CD[n]←size

(
kl−shell

)
−size

(
ku−shell

)

5 end
6 if ∃n | (CD[n+ 1] < 0 ∧ CD[n] > 0) then
7 nbest ← n
8 else
9 nbest ← 1

10 end
11 kbest ← levels[nbest]
12 return kbest-core (or truss) as keywords

Note that both dens and inf enjoy the same
adaptability as the main core retention method of
(Rousseau and Vazirgiannis, 2015) explained in Sec-
tion 4, since the sizes of all the subgraphs in the hi-
erarchy suit the structure of the graph-of-words.

5.3 CoreRank
Techniques based on retaining the kbest-core (or
truss), such as dens and inf previously described, are
better than retaining only the top level but lack flex-
ibility, in that they can only select an entire batch
of nodes at a time. This is suboptimal, because of
course not all the nodes in a given group are equally
good. To address this issue, our proposed CoreRank
method (CR in what follows) converts nodes core
(or truss) numbers into scores, ranks nodes in de-
creasing order of their scores, and selects the top p%
nodes (CRP) or the nodes before the elbow in the
scores curve (CRE). Note that for simplicity, we still
refer to the technique as CR even when dealing with
truss numbers.

Flexibility is obviously improved by decreasing
granularity from the subgraph level to the node

level. However, to avoid going back to the lim-
itations of TR (absence of cohesiveness consider-
ations), it is crucial to decrease granularity while
retaining as much of the desirable information en-
coded by degeneracy as possible. To accomplish this
task, we followed (Bae and Kim, 2014) and assigned
to each node the sum of the core (or truss) numbers
of its neighbors.

Our CRE method is outlined in Algorithm 4,
where N(v) denotes the set of neighbors of vertex
v, and number(v) is the core (or truss) number of
v. CRP implements the exact same strategy, the only
difference being nbest ← round(|V | ∗ percentage)
at step 8 (where percentage is a real number be-
tween 0 and 1).

Algorithm 4: CRE method
Input : core (or truss) decomposition of G
Output: set of keywords

1 CR← empty vector of length |V |
2 for n← 1 to |V | do
3 v ← V [n]
4 CR[n]←∑u∈N(v) number(u)
5 name(CR[n])← label(v)
6 end
7 sort CR in decreasing order
8 nbest ← elbow(n,CR[n])
9 return names(CR[1 : nbest]) as keywords

6 Experimental Setup

6.1 Baseline methods
TextRank (TR). We used as our first benchmark the
system of (Mihalcea and Tarau, 2004) discussed in
Section 4. For the sake of fair comparison with our
CRE and CRP methods, we considered two variants
of TR that respectively retain nodes based on both
the elbow (TRE) and percentage criteria (TRP).
Main. Our second baseline is the main core re-
tention technique of (Rousseau and Vazirgiannis,
2015), also described in Section 4. This method is
referred to as main in the remainder of this paper.

6.2 Datasets
To evaluate performance, we used three standard,
publicly available datasets featuring documents of

1865



various types and sizes. Figure 3 shows the distribu-
tions of document size and manually assigned key-
words for each dataset.

The Hulth20031 (Hulth, 2003) dataset contains
abstracts drawn from the Inspec database of physics
and engineering papers. Following our baselines, we
used the 500 documents in the validation set and the
“uncontrolled” keywords assigned by human anno-
tators. The mean document size is 120 words and
on average, 21 keywords (in terms of unigrams) are
available for each document.

We also used the training set of Marujo20121,
containing 450 web news stories of about 440 words
on average, covering 10 different topics from art and
culture to business, sport, and technology (Marujo et
al., 2012). For each story, the keyphrases assigned
by at least 9 out of 10 Amazon Mechanical Turk-
ers are provided as gold standard. After splitting the
keyphrases into unigrams, this makes for an aver-
age of 68 keywords per document, which is much
higher than for the two other datasets, even the one
comprising long documents (Semeval, see next).

The Semeval2 dataset (Kim et al., 2010) offers
parsed scientific papers collected from the ACM
Digital Library. More precisely, we used the 100
articles in the test set and the corresponding author-
and-reader-assigned keyphrases. Each document is
approximately 1,860 words in length and is associ-
ated with about 24 keywords.

Notes. In Marujo2012, the keywords were as-
signed in an extractive manner, but many of them
are verbs. In the two other datasets, keywords were
freely chosen by human coders in an abstractive way
and as such, some of them are not present in the orig-
inal text. On these datasets, reaching perfect recall is
therefore impossible for our methods (and the base-
lines), which by definition all are extractive.

6.3 Implementation
Before constructing the graphs-of-words and pass-
ing them to the keyword extraction methods, we per-
formed the following pre-processing steps:

Stopwords removal. Stopwords from the
1https://github.com/snkim/

AutomaticKeyphraseExtraction
2https://github.com/boudinfl/centrality_

measures_ijcnlp13/tree/master/data

0
50

0
15

00
25

00
0

50
0

15
00

25
00

document size (in words)

0
20

40
60

80
10

0
14

0
0

20
40

60
80

10
0

14
0

number of manually 
assigned keywords 

Hulth2003 SemevalMarujo2012Hulth2003 SemevalMarujo2012

Figure 3: Basic dataset statistics.

SMART information retrieval system3 were dis-
carded.

Part-of-Speech tagging and screening using the
openNLP (Hornik, 2015) R (R Core Team, 2015)
implementation of the Apache OpenNLP Maxent
POS tagger. Then, following (Mihalcea and Tarau,
2004), only nouns and adjectives were kept. For
Marujo2012, as many gold standard keywords are
verbs, this step was skipped (note that we did exper-
iment with and without POS-based screening but got
better results in the second case).

Stemming with the R SnowballC package
(Bouchet-Valat, 2014) (Porter’s stemmer). Gold
standard keywords were also stemmed.

After pre-processing, graphs-of-words (as de-
scribed in Section 2) were constructed for each doc-
ument and various window sizes (from 3, increasing
by 1, until a plateau in scores was reached). We used
the R igraph package (Csardi and Nepusz, 2006)
to write graph building and weighted k-core imple-
mentation code. For K-truss, we used the C++ im-
plementation offered by (Wang and Cheng, 2012).

Finally, for TRP and CRP, we retained the
top 33% keywords on Hulth2003 and Marujo2012
(short and medium size documents), whereas on Se-
meval (long documents), we retained the top 15 key-
words. This is consistent with our baselines. In-
deed, the number of manually assigned keywords in-
creases with document size up to a certain point, and
stabilizes afterwards.

The code of the implementation and the datasets
can be found here4.

3http://jmlr.org/papers/volume5/
lewis04a/a11-smart-stop-list/english.stop

4https://github.com/Tixierae/EMNLP_2016

1866



5
1

0
1

5
2

0
2

5

3 4 5 6 7 8 9 10 12 14

4
6

8
1

2
1

6

Hulth 2003

1311

11
1
3

C
or

e 
nu

m
be

r
Tr

us
s 

nu
m

be
r

●●●

●●

●●

●
●●●●●

●
●●●●●

●

●●●
●

●

●●●●

●

●

●

●●
●●

●

●

●

●

●●●●

●

●

●

●●
●●
●

●

●

●●●
●●

●
●

●

●●●

●●●

●●

0
5

0
0

0
1

0
0

0
0

2
0

0
0

0

3 4 5 6 7 8 9 10 11 12 13 14

Figure 4: Triangle count versus window size
(Hulth2003).

7 Experimental Results

7.1 Evaluation
We computed the standard precision, recall, and F-1
measures for each document and averaged them at
the dataset level (macro-averaging).

7.2 Precision/Recall trade-off
As shown in Figure 5, our methods dens and inf
outperform the baselines by a wide margin on the
datasets containing small and medium size docu-
ments (Hulth2003 and Marujo2012). As expected,
this superiority is gained from a drastic improve-
ment in recall, for a comparatively lower precision
loss. TR and main exhibit higher precision than
recall, which is in accordance with (Rousseau and
Vazirgiannis, 2015). The same observation can be
made for our CR method. For TR, the unbalance
is more severe on the Hulth2003 and Marujo2012
datasets (short/medium documents) when the elbow
method is used (TRE), probably because it tends to
retain only a few nodes. However, on Semeval (long
documents), using the elbow method (TRE) gives
the best trade-off between precision and recall. For
CR, still on Semeval, using the elbow method (CRE)
even gives better recall than precision.

Overall, compared to the baselines, the unbalance
between precision and recall for our methods is less
extreme or equivalent. On the Marujo2012 dataset
for instance, our proposed inf method is almost per-
fectly balanced and ranks second (significantly bet-
ter than all baselines).

7.3 Impact of window size
The performance of k-core does not dramatically
increase with window size, while K-truss exhibits

precision recall F1-score
dens 48.79 72.78 56.09*

inf 48.96 72.19 55.98*
CRP 61.53 38.73 45.75
CRE 65.33 37.90 44.11

main† 51.95 54.99 50.49
TRP† 65.43 41.37 48.79
TRE† 71.34 36.44 45.77

Table 1: Hulth2003, K-truss, W = 11.
*statistical significance at p < 0.001 with respect to all baselines.

†baseline systems.

precision recall F1-score
dens 47.62 71.46 52.94*

inf 53.88 57.54 49.10*
CRP 54.88 36.01 40.75
CRE 63.17 25.77 34.41

main† 64.05 34.02 36.44
TRP† 55.96 36.48 41.44
TRE† 65.50 21.32 30.68

Table 2: Marujo2012, k-core, W = 13.
*statistical significance at p < 0.001 with respect to all baselines.

†baseline systems.

a surprising “cold start” behavior and only begins
to kick-in for sizes greater than 4-5. A possible
explanation is that the ability of K-truss (which
is triangle-based) to extract meaningful information
from a graph depends, up to a certain point, on
the amount of triangles in the graph. In the case
of graph-of-words, the number of triangles is pos-
itively correlated with window size (see Figure 4).
It also appears that document size (i.e., graph-of-
words structure) is responsible for a lot of perfor-
mance variability. Specifically, on longer docu-
ments, performance plateaus at higher window sizes.

7.4 Best models comparison
For each dataset, we retained the degeneracy tech-
nique and window size giving the absolute best per-
formance in F1-score, and compared all methods un-
der these settings (see Tables 1–3). We tested for
statistical significance in macro-averaged F1 scores
using the non-parametric version of the t-test, the
Mann-Whitney U test5.

On Hulth2003 and Marujo2012 (short and
medium size documents), our methods dens and inf
strongly and significantly outperform all baselines,
with respective absolute improvements of more than
5.5% with respect to the best performing baseline

5https://stat.ethz.ch/R-manual/R-
devel/library/stats/html/wilcox.test.html

1867



4 6 8 10 12 14

0.
45

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

PR
EC

IS
IO
N

●
●

● ●
● ●

● ● ● ● ●
●

4 6 8 10 12 14

0.
3

0.
4

0.
5

0.
6

0.
7

0.
8

● ● ● ● ● ● ● ● ● ● ● ●

4 6 8 10 12 14

0.
40

0.
45

0.
50

0.
55

0.
60

window size

●
●

● ●
● ● ● ● ● ● ● ●

k−truss
Hulth 2003

● ●
● ●

● ● ● ● ● ● ●
●

● ● ● ● ● ● ● ● ● ● ● ●

● ●
● ●

● ● ● ● ● ● ● ●

0.
40

0.
45

0.
50

0.
55

0.
60

4 6 8 10 12 14

window size

4 6 8 10 12 14

4 6 8 10 12 14

0.
45

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

0.
3

0.
4

0.
5

0.
6

0.
7

0.
8

k−core

5 10 15 20

0.
45

0.
50

0.
55

0.
60

0.
65

0.
70

● ●
● ● ● ●

● ● ● ● ● ● ● ● ●
●

●
●

5 10 15 20

0.
2

0.
3

0.
4

0.
5

0.
6

0.
7

● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●

5 10 15 20

0.
25

0.
30

0.
35

0.
40

0.
45

0.
50

0.
55

● ●
● ● ● ● ● ● ● ● ● ● ● ● ●

● ● ●

5 10 15 20

0.
45

0.
50

0.
55

0.
60

0.
65

0.
70

● ●
● ● ● ●

● ● ● ● ● ● ● ● ●
●

●
●

5 10 15 20

0.
2

0.
3

0.
4

0.
5

0.
6

0.
7

● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●

5 10 15 20

0.
25

0.
30

0.
35

0.
40

0.
45

0.
50

0.
55

● ●
● ● ● ● ● ● ● ● ● ● ● ● ●

● ● ●

k−truss
Marujo 2012k−core

window sizewindow size

TRP ● TRE maininf CRP CREdens

Semeval
k−trussk−core

5 10 15 20

0.
1

0.
2

0.
3

0.
4

0.
5

0.
6

● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●

5 10 15 20

0.
2

0.
3

0.
4

0.
5

0.
6

0.
7

0.
8

● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●

5 10 15 20

0.
15

0.
20

0.
25

0.
30

0.
35

0.
40

● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ●

5 10 15 20

0.
1

0.
2

0.
3

0.
4

0.
5

0.
6

● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●

5 10 15 20

0.
2

0.
3

0.
4

0.
5

0.
6

0.
7

0.
8

● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●

5 10 15 20

0.
15

0.
20

0.
25

0.
30

0.
35

0.
40

● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ●

window sizewindow size

Figure 5: Impact of window size on performance.

precision recall F1-score
dens 8.44 79.45 15.06

inf 17.70 65.53 26.68
CRP 49.67 32.88 38.98*
CRE 25.82 58.80 34.86

main† 25.73 49.61 32.83
TRP† 47.93 31.74 37.64
TRE† 33.87 46.08 37.55

Table 3: Semeval, K-truss, W = 20.
*statistical significance at p < 0.001 w.r.t. main. †baseline systems.

(main). On Semeval, which features larger pieces
of text, our CRP technique improves on TRP, the
best performing baseline, by more than 1 %, altough
the difference is not statistically significant. How-
ever, CRP is head and shoulders above main, with an
absolute gain of 6%. This suggests that converting
the cohesiveness information captured by degener-
acy into ranks may be valuable for large documents.

Finally, the poor performance of the dens and inf
methods on Semeval (Table 3) might be explained
by the fact that these methods are only capable of
selecting an entire batch of nodes (i.e., subgraph-
of-words) at a time. This lack of flexibility seems
to become a handicap on long documents for which

the graphs-of-words, and thus the subgraphs corre-
sponding to the k-core (or truss) hierarchy levels, are
very large. This analysis is consistent with the ob-
servation that conversely, approaches that work at a
finer granularity level (node level) prove superior on
long documents, such as our proposed CRP method
which reaches best performance on Semeval.

8 Conclusion and Future Work

Our results provide empirical evidence that spread-
ing influence may be a better “keywordness” met-
ric than eigenvector (or random walk)-based crite-
ria. Our CRP method is currently very basic and
leveraging edge weights/direction or combining it
with other scores could yield better results. Also,
more meaningful edge weights could be obtained
by merging local co-occurrence statistics with exter-
nal semantic knowledge offered by pre-trained word
embeddings (Wang et al., 2014). The direct use of
density-based objective functions could also prove
valuable.

1868



References
Joonhyun Bae and Sangwook Kim. 2014. Identifying

and ranking influential spreaders in complex networks
by neighborhood coreness. Physica A: Statistical Me-
chanics and its Applications, 395:549–559.

Vladimir Batagelj and Matjaž Zaveršnik. 2002. General-
ized cores. arXiv preprint cs/0202039.

Milan Bouchet-Valat, 2014. SnowballC: Snowball stem-
mers based on the C libstemmer UTF-8 library. R
package version 0.5.1.

Asli Celikyilmaz and Dilek Hakkani-Tür. 2011. Discov-
ery of topically coherent sentences for extractive sum-
marization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 491–
499. Association for Computational Linguistics.

Yun-Nung Chen, Yu Huang, Hung-Yi Lee, and Lin-Shan
Lee. 2012. Unsupervised two-stage keyword extrac-
tion from spoken documents by topic coherence and
support vector machine. In 2012 IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), pages 5041–5044. IEEE.

Janara Christensen, Stephen Soderland Mausam, Stephen
Soderland, and Oren Etzioni. 2013. Towards coher-
ent multi-document summarization. In HLT-NAACL,
pages 1163–1173. Citeseer.

Jonathan Cohen. 2008. Trusses: Cohesive subgraphs
for social network analysis. National Security Agency
Technical Report, page 16.

Gabor Csardi and Tamas Nepusz. 2006. The igraph soft-
ware package for complex network research. Inter-
Journal, Complex Systems:1695.

Vince Grolmusz. 2015. A note on the pagerank of
undirected graphs. Information Processing Letters,
115(6):633–634.

Zellig S Harris. 1954. Distributional structure. Word,
10(2-3):146–162.

Kurt Hornik, 2015. openNLP: Apache OpenNLP Tools
Interface. R package version 0.2-5.

Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of the 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 216–
223. Association for Computational Linguistics.

Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010. Semeval-2010 task 5: Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 21–26. Association for Compu-
tational Linguistics.

Maksim Kitsak, Lazaros K Gallos, Shlomo Havlin,
Fredrik Liljeros, Lev Muchnik, H Eugene Stanley, and

Hernán A Makse. 2010. Identification of influen-
tial spreaders in complex networks. Nature Physics,
6(11):888–893.

Fragkiskos D Malliaros and Konstantinos Skianis. 2015.
Graph-based term weighting for text categorization.
In Proceedings of the 2015 IEEE/ACM International
Conference on Advances in Social Networks Analysis
and Mining, pages 1473–1479. ACM.

Fragkiskos D. Malliaros, Apostolos N. Papadopoulos,
and Michalis Vazirgiannis. 2016a. Core decompo-
sition in graphs: concepts, algorithms and applica-
tions. In Proceedings of the 19th International Confer-
ence on Extending Database Technology, EDBT, pages
720–721.

Fragkiskos D Malliaros, Maria-Evgenia G Rossi, and
Michalis Vazirgiannis. 2016b. Locating influen-
tial nodes in complex networks. Scientific Reports,
6:19307.

Luis Marujo, Anatole Gershman, Jaime Carbonell,
Robert Frederking, and Jo ao P. Neto. 2012. Su-
pervised topical key phrase extraction of news stories
using crowdsourcing, light filtering and co-reference
normalization. In Proceedings of LREC 2012. ELRA.

Polykarpos Meladianos, Giannis Nikolentzos, François
Rousseau, Yannis Stavrakas, and Michalis Vazirgian-
nis. 2015. Degeneracy-based real-time sub-event de-
tection in twitter stream. In Ninth International AAAI
Conference on Web and Social Media (ICWSM).

Rada Mihalcea and Paul Tarau. 2004. TextRank: bring-
ing order into texts. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computational
Linguistics.

R Core Team, 2015. R: A Language and Environment
for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria.

François Rousseau and Michalis Vazirgiannis. 2013.
Graph-of-word and tw-idf: new approach to ad hoc
ir. In Proceedings of the 22nd ACM international con-
ference on Conference on Information & Knowledge
Management (CIKM), pages 59–68. ACM.

François Rousseau and Michalis Vazirgiannis. 2015.
Main core retention on graph-of-words for single-
document keyword extraction. In Advances in Infor-
mation Retrieval, pages 382–393. Springer.

François Rousseau, Emmanouil Kiagias, and Michalis
Vazirgiannis. 2015. Text categorization as a graph
classification problem. In ACL, volume 15, page 107.

Stephen B Seidman. 1983. Network structure and mini-
mum degree. Social networks, 5(3):269–287.

Jia Wang and James Cheng. 2012. Truss decomposition
in massive networks. Proceedings of the VLDB En-
dowment, 5(9):812–823.

1869



Rui Wang, Wei Liu, and Chris McDonald. 2014. Corpus-
independent generic keyphrase extraction using word
embedding vectors. In Software Engineering Research
Conference, page 39.

1870


