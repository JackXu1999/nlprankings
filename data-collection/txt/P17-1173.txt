



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1891–1900
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1173

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1891–1900
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1173

An Algebra for Feature Extraction

Vivek Srikumar
School of Computing

University of Utah
svivek@cs.utah.edu

Abstract

Though feature extraction is a necessary
first step in statistical NLP, it is often seen
as a mere preprocessing step. Yet, it can
dominate computation time, both during
training, and especially at deployment. In
this paper, we formalize feature extraction
from an algebraic perspective. Our for-
malization allows us to define a message
passing algorithm that can restructure fea-
ture templates to be more computationally
efficient. We show via experiments on text
chunking and relation extraction that this
restructuring does indeed speed up feature
extraction in practice by reducing redun-
dant computation.

1 Introduction

Often, the first step in building statistical NLP
models involves feature extraction. It is well un-
derstood that the right choice of features can sub-
stantially improve classifier performance. How-
ever, from the computational point of view, the
process of feature extraction is typically treated,
at best as the preprocessing step of caching fea-
turized inputs over entire datasets, and at worst,
as ‘somebody else’s problem’. While such ap-
proaches work for training, when trained models
are deployed, the computational cost of feature ex-
traction cannot be ignored.

In this paper, we present the first (to our knowl-
edge) algebraic characterization of the process of
feature extraction. We formalize feature extrac-
tors as arbitrary functions that map objects (words,
sentences, etc) to a vector space and show that this
set forms a commutative semiring with respect to
feature addition and feature conjunction.

An immediate consequence of the semiring
characterization is a computational one. Every

semiring admits the Generalized Distributive Law
(GDL) Algorithm (Aji and McEliece, 2000) that
exploits the distributive property to provide com-
putational speedups. Perhaps the most common
manifestation of this algorithm in NLP is in the
form of inference algorithms for factor graphs and
Bayesian networks like the max-product, max-
sum and sum-product algorithms (e.g. Goodman,
1999; Kschischang et al., 2001). When applied to
feature extractors, the GDL algorithm can refactor
a feature extractor into a faster one by reducing re-
dundant computation. In this paper, we propose a
junction tree construction to allow such refactor-
ing. Since the refactoring is done at the feature
template level, the actual computational savings
grow as classifiers encounter more examples.

We demonstrate the practical utility of our ap-
proach by factorizing existing feature sets for text
chunking and relation extraction. We show that,
by reducing the number of operations performed,
we can obtain significant savings in the time taken
to extract features.

To summarize, the main contribution of this pa-
per is the recognition that feature extractors form a
commutative semiring over addition and conjunc-
tion. We demonstrate a practical consequence of
this characterization in the form of a mechanism
for automatically refactoring any feature extractor
into a faster one. Finally, we show the empirical
usefulness of our approach on relation extraction
and text chunking tasks.

2 Problem Definition

Before formal definitions, let us first see a running
example.

2.1 Motivating Example
Consider the frequently used unigram, bigram and
trigram features. Each of these is a template that
specifies a feature representation for a word. In

1891

https://doi.org/10.18653/v1/P17-1173
https://doi.org/10.18653/v1/P17-1173


fact, the bigram and trigram templates them-
selves are compositional by definition. A bigram
is simply the conjunction of a word w and pre-
vious word, which we will denote as w-1; i.e.,
bigram = w-1&w. Similarity, a trigram is the
conjunction of w-2 and bigram.

These templates are a function that operate on
inputs. Given a sentence, say John ate alone, and
a target word, say alone, they will produce indi-
cators for the strings w=alone, w-1=ate&w=alone
and w-2=John&w-1=ate&w=alone respectively.
Equivalently, each template maps an input to a
vector. Here, the three vectors will be basis vec-
tors associated with the feature strings.

Observe that the function that extracts the target
word (i.e., w) has to be executed in all three feature
templates. Similarly, w-1 has to be extracted to
compute both the bigrams and the trigrams. Can
we optimize feature computation by automatically
detecting such repetitions?

2.2 Definitions and Preliminaries

Let X be a set of inputs to a classification prob-
lem at hand; e.g., X could be words, sentences,
etc. Let V be a possibly infinite dimensional vec-
tor space that represents the feature space. Feature
extractors are functions that map the input space
X to the feature space V to produce feature vec-
tors for inputs. Let F represent the set of feature
functions, defined as the set {f : X → V}. We
will use the typewriter font to denote feature func-
tions like w and bigram.

To round up the definitions, we will name two
special feature extractors inF . The feature extrac-
tor 0 maps all inputs to the zero vector. The feature
extractor 1 maps all inputs to a bias feature vector.
Without loss of generality, we will designate the
basis vector i0 ∈ V as the bias feature vector.

In this paper, we are concerned about two gen-
erally well understood operators on feature func-
tions – addition and conjunction. However, let us
see formal definitions for completeness.
Feature Addition. Given two feature extractors
f1, f2 ∈ F , feature addition (denoted by +) pro-
duces a feature extractor f1 + f2 that adds up the
images of f1 and f2. That is, for any example
x ∈ X , we have

(f1 + f2) (x) = f1 (x) + f2 (x) (1)

For example, the feature extractor w + w-1 will
map the word alone to a vector that is one for the

basis elements w=alone and w-1=went. This vec-
tor is the sum of the indicator vectors produced by
the two operands w and w-1.
Feature Conjunction. Given two feature extrac-
tors f1, f2 ∈ F , their conjunction (denoted by
&) can be interpreted as an extension of Boolean
conjunction. Indicator features like bigram are
predicates for certain observations. Conjoining in-
dicator features for two predicates is equivalent
to an indicator feature for the Boolean conjunc-
tion of the predicates. More generally, with fea-
ture extractors that produce real valued vectors,
the conjunction will produce their tensor prod-
uct. The equivalence of feature conjunctions to
tensor products has been explored and exploited
in recent literature for various NLP tasks (Lei
et al., 2014; Srikumar and Manning, 2014; Gorm-
ley et al., 2015; Lei et al., 2015).

We can further generalize this with an addi-
tional observation that is crucial for the rest of
this paper. We argue that the conjunction opera-
tor produces symmetric tensor products rather than
general tensor products. To see why, consider the
bigram example. Though we defined the bigram
feature as the conjunction of w-1 and w, their or-
dering is irrelevant from classification perspective
– the eventual goal is to associate weights with this
combination of features. This observation allows
us to formally define the conjunction operator as:

(f1&f2) (x) = vec (f1 (x)� f2 (x)) (2)

Here, vec (·) stands for vectorize, which simply
converts the resulting tensor into a vector and �
denotes the symmetric tensor product, introduced
by Ryan (1980, Proposition 1.1). A symmetric
tensor product is defined to be the average of the
tensor products of all possible permutations of the
operands, and thus, unlike a simple tensor product,
is invariant to permutation of is operands. Infor-
mally, if we think of a tensor as a mapping from
an ordered sequence of keys to real numbers, then,
symmetric tensor product can be thought of as a
mapping from a set of keys to numbers.

3 An Algebra for Feature Extraction

In this section, we will see that the set of feature
extractors F form a commutative semiring with
respect to addition and conjunction. First, let us
revisit the definition of a commutative semiring.

Definition 1. A commutative semiring is an alge-
braic structure consisting of a set K and two bi-

1892



nary operations ⊕ and ⊗ (addition and multipli-
cation respectively) such that:
S1. (K,⊕) is a commutative monoid: ⊕ is asso-

ciative and commutative, and the set K con-
tains a unique additive identity 0 such that
∀x ∈ K, we have 0⊕ x = x⊕ 0 = x.

S2. (K,⊗) is a commutative monoid: ⊗ is asso-
ciative and commutative, and the set K con-
tains a unique multiplicative identity 1 such
that ∀x ∈ K, we have 1⊗ x = x⊗ 1 = x.

S3. Multiplication distributes over addition on
both sides. That is, for any x, y, z ∈ K, we
have x ⊗ (y ⊕ z) = (x ⊗ y) ⊕ (x ⊗ z) and
(x⊕ y)⊗ z = (x⊗ z)⊕ (y ⊗ z).

S4. The additive identity is an annihilating ele-
ment with respect to multiplication. That is,
for any x ∈ K, we have x⊗ 0 = 0 = 0⊗ x.

We refer the reader to Golan (2013) for a broad-
ranging survey of semiring theory. We can now
state and prove the main result of this paper.
Theorem 1. LetX be any set and letF denote the
set of feature extractors defined on the set. Then,
(F ,+,&) is a commutative semiring.
Proof. We will show that the properties of a
commutative semiring hold for (F ,+,&) using
the definitions of the operators from §2.2. Let
f1, f2 and f ∈ F be feature extractors.
S1. For any example x ∈ X , we have

(f1 + f2) (x) = f1 (x) + f2 (y). The right
hand side denotes vector addition, which is
associative and commutative. The 0 feature
extractor is the additive identify because it
produces the zero vector for any input. Thus,
(F ,+) is a commutative monoid.

S2. To show that the conjunction operator is as-
sociative over feature extractors, it suffices
to observe that the tensor product (and hence
the symmetric tensor product) is associative.
Furthermore, the symmetric tensor product is
commutative by definition, because it is in-
variant to permutation of its operands.
Finally, the bias feature extractor, 1, that
maps all inputs to the bias vector i0, is the
multiplicative identity. To see this, consider
the conjunction f&1, applied to an input x:

(f&1) (x) = vec (f (x)� 1 (x))
= vec (f (x)� i0)

The product term within the vec (·) in the
final expression is a symmetric tensor, de-
fined by basis vectors that are sets of the form

{i0, i0}, {i1, i0}, · · · . Each basis {ij , i0} is
associated with a feature value f (x)j . Thus,
the vectorized form of this tensor will contain
the same elements as f (x), perhaps mapped
to different bases. The mapping from f (x)
to the final vector is independent of the input
x because the bias feature extractor is inde-
pendent of x. Without loss of generality, we
can fix this mapping to be the identity map-
ping, thereby rendering the final vectorized
form equal to f (x). That is, f&1 = f.
Thus, (F ,&) is a commutative monoid.

S3. Since tensor products distribute over addi-
tion, we get the distributive property.

S4. By definition, conjoining with the 0 feature
extractor annihilates all feature functions be-
cause 0 maps all inputs to the zero vector.

�

4 From Algebra to an Algorithm

The fact that feature extractors form a commuta-
tive semiring has a computational consequence.
The generalized distributive law (GDL) algo-
rithm (Aji and McEliece, 2000) exploits the prop-
erties of a commutative semiring to potentially re-
duce the computational effort for marginalizing
sums of products. The GDL algorithm manifests
itself as the Viterbi, Baum-Welch, Floyd-Warshall
and belief propagation algorithms, and the Fast
Fourier and Hadamard transforms. Each corre-
sponds to a different commutative semiring and a
specific associated marginalization problem.

Here, we briefly describe the general marginal-
ization problem from Aji and McEliece (2000) to
introduce notation and also highlight the analogies
to inference in factor graphs. Let x1, x2, · · · , xn
denote a collection of variables that can take val-
ues from finite sets A1, A2, · · · , An respectively.
Let boldface x denote the entire set of variables.
These variables are akin to inference variables
in factor graphs that may be assigned values or
marginalized away.

Let (K,⊕,⊗) denote a commutative semiring.
Suppose αi is a function that maps a subset of the
variables {xi1 , xi2 , · · · } to the set K. The sub-
set of variables that constitute the domain of αi is
called the local domain of the corresponding local
function. Local domains and local functions are
analogous to factors and factor potentials in a fac-
tor graph. With a collection of local domains, each
associated with a function αi, the “marginalize the

1893



product” problem is that of computing:
∑

x

∏

i

αi (xi1 , xi2 , · · · ) (3)

Here, the sum and product use the semiring op-
erators. The summation is over all possible valid
assignments of the variables x over the cross prod-
uct of the setsA1, A2, · · · , An. This problem gen-
eralizes the familiar max-product or sum-product
settings. Indeed, the GDL algorithm is a gener-
alization of the message passing (Pearl, 2014) for
efficiently computing marginals.

To make feature extraction efficient using the
GDL algorithm, in the next section, we will define
a marginalization problem in terms of the semir-
ing operators by specifying the variables involved,
the local domains and local functions. Instead of
describing the algorithm in the general setting, we
will instantiate it on the semiring at hand.

5 Marginalizing Feature Extractors

First, let us see why we can expect any benefit
from the GDL algorithm by revisiting our running
example (unigrams, bigrams and trigrams), writ-
ten below using the semiring operations:

f = w+ (w-1&w) + (w-2&w-1&w) (4)

When applied to a token, f performs two additions
and three conjunctions. However, by applying the
distributive property, we can refactor it as follows
to reduce the number of operations:

f′ = (1 + (1 + w-2)&w-1)&w (5)

The refactored version f′ – equivalent to the orig-
inal one – only performs two additions and two
conjunctions, offering a computational saving of
one operation. This refactoring is done at the level
of feature templates (i.e., feature extractors); the
actual savings are realized when the feature vec-
tors are computed by applying this feature func-
tion to an input. Thus, the simplification, though
seemingly modest at the template level, can lead
to a substantial speed improvements when the fea-
tures vectors are actually manifested from data.

The GDL algorithm instantiated with the fea-
ture extractor semiring, automates such factoriza-
tion at a symbolic level. In the rest of this sec-
tion, first (§5.1), we will write our problem as a
marginalization problem, as in Equation (3). Then
(§5.2), we will construct a junction tree to apply
the message passing algorithm.

5.1 Canonicalizing Feature Extractors

To frame feature simplification as marginalization,
we need to first write any feature extractor as a
canonical sum of products that is amenable for
factorization (i.e., as in (3)). To do so, in this sec-
tion, we will define: (a) the variables involved,
(b) the local domains (i.e., subsets of variables
contributing to each product term), and, (c) a lo-
cal function for each local domain (i.e., the αi’s).
Variables. First, we write a feature extrac-
tor as a sum of products. Our running exam-
ple (4) is already one. If we had an expres-
sion like f1&(f2 + f3), we can expand it into
f1&f2 + f1&f3. From the sum of products, we
identify the base feature extractors (i.e., ones not
composed of other feature extractors) and define a
variable xi for each. In our example, we have w,
w-1 and w-2.

Next, recall from §4 that each variable xi can
take values from a finite set Ai. If a base feature
extractor fi corresponds to the variable xi, then,
we define xi’s domain to be the set Ai = {1, fi}.
That is, each variable can either be the bias feature
extractor or the feature extractor associated with
it. Our example gives three variables x1, x2, x3
with domains A1 = {1, w}, A2 = {1, w-1}, A3 =
{1, w-2} respectively.
Local domains. Local domains are subsets of the
variables defined above. They are the domains of
functions that constitute products in the canonical
form of a feature extractor. We define the follow-
ing local domains, each illustrated with the corre-
sponding instantiation in our running example:

1. A singleton set for each variable: {x1}, {x2},
and {x3}.

2. One local domain consisting of all the vari-
ables: The set {x1, x2, x3}.

3. One local domain consisting of no variables:
The empty set {}.

4. One local domain for each subset of base
feature extractors that participate in at least
two conjunctions in the sum-of-products (i.
e., the ones that can be factored away): Only
{x1, x2} in our example, because only w and
w-1 participate in two conjunctions in (4).

Local functions. Each local domain is associated
with a function that maps variable assignments to
feature extractors. These functions (called local
kernels by Aji and McEliece (2000)) are like po-
tential functions in a factor graph. We define two
kinds of local functions, driven by the goal of de-

1894



signing a marginalization problem that pushes to-
wards simpler feature functions.

1. We associate the identity function with all
singleton local domains, and the constant
function that returns the bias 1 with the
empty domain {}.

2. With all other local domains, we asso-
ciate an indicator function, denoted by z.
For a local domain, z is an indicator for
those assignments of the variables involved,
whose conjunctions are present in any prod-
uct term in sum-of-products. In our run-
ning example, the function z(x1, x2) is the
indicator for (x1, x2) belonging to the set
{(w, 1) , (w, w-1)}, represented by the table:

x1 x2 z(x1, x2)
1 1 0
1 w-1 0
w 1 1
w w-1 1

The indicator returns the semiring’s multi-
plicative and additive identities. The value
of z above for inputs (w,1) is 1 because the
first term in (4) that defines the feature ex-
tractor contains w, but not w-1. On the other
hand, the input (1,1) is mapped to 0 be-
cause every product term contains either w
or w-1. For the local domain {x1, x2, x3},
the local function is the indicator for the set
{(w, 1, 1), (w, w-1,1), (w, w-1, w-2)}, corre-
sponding to each product term.

In summary, for the running example we have:
Local domain Local function
{x1} x1
{x2} x2
{x3} x3
{x1, x2, x3} z(x1, x2, x3)
{} 1
{x1, x2} z(x1, x2)

The procedure described here aims to convert
any feature function into a canonical form that can
be factorized using the GDL algorithm. Indeed,
using local domains and functions specified above,
any feature extractor can we written as a canonical
sum of products as in (3). For example, using the
table above, our running example is identical to

∑

x1,x2,x3

z(x1, x2, x3)&z(x1, x2)&x1&x2&x3 (6)

Here, the summation is over the cross product of
theAi’s. The choice of the z functions ensures that
only those conjunctions that were in the original
feature extractor remain.

This section shows one approach for canonical-
ization; the local domains and functions are a de-

sign choice that may be optimized in future work.
We should also point out that, while this process is
notationally tedious, its actual computational cost
is negligible, especially given that it is to be per-
formed only once at the template level.

5.2 Simplifying feature extractors

As mentioned in §4, a commutative semiring can
allow us to employ the GDL algorithm to effi-
ciently compute a sum of products. Starting from a
canonical sum-of-products expression such as the
one in (6), this process is similar to variable elim-
ination for Bayesian networks. The junction tree
algorithm is a general scheme to avoid redundant
computation in such networks (Cowell, 2006). To
formalize this, we will first build a junction tree
and then define the messages sent from the leaves
to the root. The final message at the root will give
us the simplified feature function.
Constructing a Junction Tree. First, we will con-
struct a junction tree using the local domains from
§ 5.1. In any junction tree, the edges should satisfy
the running intersection property: i.e., if a vari-
able xi is in two nodes in the tree, then it should
be in every node in the path connecting them. To
build a junction tree, we will first create a graph
whose nodes are the local domains. The edges of
this graph connect pairs of nodes if the variables
in one are a subset of the other. For simplicity, we
will assume that our nodes are arranged in a lat-
tice as shown in Figure 1, with edges connecting
nodes in subsequent levels. For example, there is
no edge connecting nodes B and C.

Every spanning tree of this lattice is a junction
tree. Which one should we consider? Let us ex-
amine the properties that we need. First, the root
of the tree should correspond to the empty local
domain {} because messages arriving at this node
will accumulate all products. Second, as we will
see, feature extractors farther from the root will
appear in inner terms in the factorized form. That
is, frequent or more expensive feature extractors
should be incentivized to appear higher in the tree.

To capture these preferences, we frame the task
of constructing the junction tree as a maximum
spanning tree problem over the graph, with edge
weights incorporating the preferences. One nat-
ural weighting function is the computational ex-
pense of the base feature extractors associated
with that edge. For example, the weight associated
with the edge connecting nodes E and D in the fig-

1895



{}

{x1} {x2} {x3}

{x1, x2}

{x1, x2, x3}

A

B

C

D

E

F

Figure 1: The junction tree for our running example. The
process of constructing the junction tree is described in the
text. Here, we show both the tree and the graph from which
it is constructed; dashed lines show edges are not in the tree.
Filled circles denote the names of the nodes. The local do-
main {x1} is connected to the empty local domain because
the feature w corresponding to it is most frequent.

ure can be the average cost of the w and w-1 feature
extractors. If computational costs are unavailable,
we can use the number times a feature extractor
appears in the expression to be simplified. Under
this criterion, in our example, edges connecting E
to its neighbors will be weighted highest.

Once we have a spanning tree, we make the
edges directed so that the empty set is the root.
Figure 1 shows the junction tree obtained for our
running example.
Message Passing for Feature Simplification.
Given the junction tree, we can use a standard
message passing scheme for factorization. The
goal is to collect information at each node in the
tree from its children all the way to the root.

Suppose vi, vj denote two nodes in the tree.
Since nodes are associated with sets of variables,
their intersections vi ∩ vj and differences vi \ vj
are defined. For example, in the example, A ∩ B =
{x3} and B \ D = {x3}. We will denote children
of a node vi in the junction tree by C(vi).

The message from any node vi to its parent vj
is a function that maps the variables vi ∩ vj to a
feature extractor by marginalizing out all variables
that are in vi but not in vj . Formally, we define the
message µij from a node vi to a node vj as:

µij (vi ∩ vj) =
∑

vj\vi

αi (vi)
∏

vk∈C(vi)
µki (vk ∩ vi) . (7)

Here, αi is the local function at node vi. To com-
plete the formal definition of the algorithm, we
note that by performing post-order traversal of the
junction tree, we will accumulate all messages at
the root of the tree, that corresponds to the empty
set of variables. The incoming message at this
node represents the factorized feature extractor.

Algorithm 1 briefly summarizes the entire simpli-
fication process. The proof of correctness of the
algorithm follows from the fact that the range of
all the local functions is a commutative semiring,
namely the feature extractor semiring. We refer
the reader to (Aji and McEliece, 2000, Appendix
A) for details.

Algorithm 1 The Generalized Distributive Law Algorithm
for simplifying a feature extractor f. See the text for details.
1: Convert f into a canonical sum of products representa-

tion (§ 5.1).
2: Construct a junction tree whose nodes are local domains.
3: for edge (vj , vi) in the post-order traversal of the tree do
4: Receive a message µij at vj using (7).
5: end for
6: return the incoming message at the root

Example run of message propagation. As an il-
lustration, let us apply it to our running example.

1. The first message is from A to B. Since A has
no children and its local function is the iden-
tity function, we have µAB(x) = x. Simi-
larly, we have µCD(x) = x.

2. The message from B to D has to marginal-
ize out the variable x3. That is, we have
µBD(x1, x2) =

∑
x3

z(x1, x2, x3)µAB(x3).

The summation is over the domain of x3,
namely {1, w-2}. By substituting for z and
µAB , and simplifying, we get the message:

x1 x2 µBD(x1, x2)
1 1 0
1 w-1 0
w 1 1
w w-1 1 + w-2

3. The message from D to E marginalizes out
the variable x2 to give us µDE(x1) =∑
x2

z(x1, x2)µCD(x2)µBD(x1, x2). Here, the

summation is over the domain of x2, namely
{1, w-1}. We can simplify the message as:

x1 µDE(x1)
1 0
w 1 + (1 + w-2)&w-1

4. Finally, the message from E to the root F
marginalizes out the variable x1 by summing
over its domain {1, w} to give us the message
(1 + (1 + w-2)&w-1)&w.

The message received at the root is the factorized
feature extractor. Note that the final form is iden-
tical to (5) at the beginning of §5.
Discussion. An optimal refactoring algorithm
would produce a feature extractor that is both cor-
rect and fastest. The algorithm above has the for-
mer guarantee. While it does reduce the number of
operations performed, the closeness of the refac-

1896



tored feature function to the fastest one depends
on the heuristic used to weight edges for iden-
tifying the junction tree. Changing the heuristic
can change the junction tree, thus changing the fi-
nal factorized function. We found via experiments
that using the number of times a feature extractor
occurs in the sum-of-products to weight edges is
promising. A formal study of optimality of factor-
ization is an avenue of future research.

6 Experiments

We show the practical usefulness of feature func-
tion refactoring using text chunking and relation
extraction. In both cases, the question we seek to
evaluate empirically is: Does the feature function
refactoring algorithm improve feature extraction
time? We should point out that our goal is not to
measure accuracy of prediction, but the efficiency
of feature extraction. Indeed, we are guaranteed
that refactoring will not change accuracy; factor-
ized feature extractors produce the same feature
vectors as the original ones.

In all experiments, we compare a feature extrac-
tor and its refactored variant. For the factorization,
we incentivized the junction tree to factor out base
feature extractors that occurred most frequently in
the feature extractor. For both tasks, we use ex-
isting feature representations that we briefly de-
scribe. We refer the reader to the original work
that developed the feature representations for fur-
ther details. For both the original and the factor-
ized feature extractors, we report (a) the number
of additions and conjunctions at the template level,
and, (b) the time for feature extraction on the en-
tire dataset. For the time measurements, we report
average times for the original and factorized fea-
ture extractors over five paired runs to average out
variations in system load.1

6.1 Text Chunking

We use data from the CoNLL 2000 shared
task (Tjong Kim Sang and Buchholz, 2000) of text
chunking and the feature set described by Martins
et al. (2011), consisting of the following templates
extracted at each word: (1) Up to 3-grams of POS
tags within a window of size ten centered at the
word, (2) up to 3-grams of words, within a win-
dow of size six centered at the word, and (3) up to
2-grams of word shapes, within a window of size

1We performed all our experiments on a server with
128GB RAM and 24 CPU cores, each clocking at 2600 MHz.

Size Average feature
Setting + & extraction time (ms)
Original 47 75 17776.6

Factorized 47 54 4294.2

Table 1: Comparison of the original and factorized feature
extractors for the text chunking task. The time improvement
is statistically significant using the paired t-test at p < 0.01.

Size Average feature
Setting + & extraction time (ms)
Original 43 19 8173.0

Factorized 43 11 6276.4

Table 2: Comparison of the original and factorized feature
extractors for the relation extraction task. We measured time
using 3191 training mention pairs. The time improvement is
statistically significant using the paired t-test at p < 0.01.

four centered at the word. In all, there are 96 fea-
ture templates.

We factorized the feature representation using
Algorithm 1. Table 1 reports the number of opera-
tions (addition and conjunction) in the templates in
the original and factorized versions of the feature
extractor. The table also reports feature extraction
time taken from the entire training set of 8,936
sentences, corresponding to 211,727 tokens. First,
we see that the factorization reduces the number of
feature conjunction operations. Thus, to produce
exactly the same feature vector, the factorized fea-
ture extractor does less work. The time results
show that this computational gain is not merely a
theoretical one; it also manifests itself practically.

6.2 Relation Extraction

Our second experiment is based on the task of re-
lation extraction using the English section of the
ACE 2005 corpus (Walker et al., 2006). The goal
is to identify semantic relations between two en-
tity mentions in text. We use the feature represen-
tation developed by Zhou et al. (2005) as part of
an investigation of how various lexical, syntactic
and semantic sources of information affect the re-
lation extraction task. To this end, the feature set
consists of word level information about mentions,
their entity types, their relationships with chunks,
path features from parse trees, and semantic fea-
tures based on WordNet and various word lists.
Given the complexity of the features, we do not
describe them here and refer the reader to the orig-
inal work for details. Note that compared to the
chunking features, these features are more diverse
in their computational costs.

We report the results of our experiments in Ta-

1897



ble 2. As before, we see that the number of
conjunction operations decreases after factoriza-
tion. Curiously, however, despite the complexity
of the feature set, the actual number of operations
is smaller than text chunking. Due to this, we see
a more modest, yet significant decrease in the time
for feature extraction after factorization.

7 Related Work and Discussion

Simplifying Expressions. The problem of sim-
plifying expressions with an eye on computa-
tional efficiency is the focus of logic synthesis (cf.
Hachtel and Somenzi, 2006), albeit largely geared
towards analyzing and verifying digital circuits.
Logic synthesis is NP-hard in general. In our case,
the hardness is hidden in the fact that our approach
does not guarantee that we will find the smallest
(or most efficient) factorization. The junction tree
construction determines the factorization quality.
Semirings in NLP. Semirings abound in NLP,
though primarily as devices to design efficient
inference algorithms for various graphical mod-
els (e.g. Wainwright and Jordan, 2008; Sutton
et al., 2012). Goodman (1999) synthesized var-
ious parsing algorithms in terms of semiring op-
erations. Since then, we have seen several ex-
plorations of the interplay between weighted dy-
namic programs and semirings for inference in
tasks such as parsing and machine translation (e.
g. Eisner et al., 2005; Li and Eisner, 2009; Lopez,
2009; Gimpel and Smith, 2009). Allauzen et al.
(2003) developed efficient algorithms for con-
structing statistical language models by exploiting
the algebraic structure of the probability semiring.
Feature Extraction and Modeling Languages.
Much work around features in NLP is aimed at
improving classifier accuracy. There is some work
on developing languages to better construct fea-
ture spaces (Cumby and Roth, 2002; Broda et al.,
2013; Sammons et al., 2016), but they do not for-
malize feature extraction from an algebraic per-
spective. We expect that the algorithm proposed in
this paper can be integrated into such feature con-
struction languages, and also into libraries geared
towards designing feature rich models (e.g. Mc-
Callum et al., 2009; Chang et al., 2015).
Representation vs. Speed. As the recent suc-
cesses (Goodfellow et al., 2016) of distributed rep-
resentations show, the representational capacity of
a feature space is of primary importance. Indeed,
several recent lines of work that use distributed

representations have independently identified the
connection between conjunctions (of features or
factors in a factor graph) and tensor products (Lei
et al., 2014; Srikumar and Manning, 2014; Gorm-
ley et al., 2015; Yu et al., 2015; Lei et al., 2015;
Primadhanty et al., 2015). They typically impose
sparsity or low-rank requirements to induce better
representations for learning. In this paper, we use
the connection between tensor products and con-
junctions to prove algebraic properties of feature
extractors, leading to speed improvements via fac-
torization.

In this context, we note that in both our experi-
ments, the number of conjunctions are reduced by
factorization. We argue that this is an important
saving because conjunctions can be a more expen-
sive operation. This is especially true when deal-
ing with dense feature representations, as is in-
creasingly common with word vectors and neural
networks, because conjunctions of dense feature
vectors are tensor products, which can be slow.

Finally, while training classifiers can be time
consuming, when trained classifiers are deployed,
feature extraction will dominate computation time
over the classifier’s lifetime. However, the pre-
diction step includes both feature extraction and
computing inner products between features and
weights. Many features may be associated with
zero weights because of sparsity-inducing learn-
ing (e.g. Andrew and Gao, 2007; Martins et al.,
2011; Strubell et al., 2015). Since these two as-
pects are orthogonal to each other, the factoriza-
tion algorithm presented in this paper can be used
to speed up extraction of those features that have
non-zero weights.

8 Conclusion

In this paper, we studied the process of feature ex-
traction using an algebraic lens. We showed that
the set of feature extractors form a commutative
semiring over addition and conjunction. We ex-
ploited this characterization to develop a factor-
ization algorithm that simplifies feature extractors
to be more computationally efficient. We demon-
strated the practical value of the refactoring algo-
rithm by speeding up feature extraction for text
chunking and relation extraction tasks.

Acknowledgments

The author thanks the anonymous reviewers for
their insightful comments and feedback.

1898



References
Srinivas M Aji and Robert J McEliece. 2000. The gen-

eralized distributive law. IEEE Transactions on In-
formation Theory 46(2).

Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In ACL.

Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML.

Bartosz Broda, Paweł Kędzia, Michał Marcińczuk,
Adam Radziszewski, Radosław Ramocki, and
Adam Wardyński. 2013. Fextor: A feature extrac-
tion framework for natural language processing: A
case study in word sense disambiguation, relation
recognition and anaphora resolution. In Computa-
tional Linguistics, Springer, pages 41–62.

Kai-Wei Chang, Shyam Upadhyay, Ming-Wei Chang,
Vivek Srikumar, and Dan Roth. 2015. IllinoisSL:
A JAVA library for Structured Prediction. arXiv
preprint arXiv:1509.07179 .

Robert G Cowell. 2006. Probabilistic networks and
expert systems: Exact computational methods for
Bayesian networks. Springer Science & Business
Media.

Chad M Cumby and Dan Roth. 2002. Learning with
feature description logics. In Inductive logic pro-
gramming, Springer.

Jason Eisner, Eric Goldlust, and Noah A Smith. 2005.
Compiling Comp Ling: Practical weighted dynamic
programming and the Dyna language. In HLT-
EMNLP.

Kevin Gimpel and Noah A Smith. 2009. Cube sum-
ming, approximate inference with non-local fea-
tures, and dynamic programming without semirings.
In EACL.

Jonathan S Golan. 2013. Semirings and their Applica-
tions. Springer Science & Business Media.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep Learning. MIT Press.

Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics 25(4):573–605.

Matthew R. Gormley, Mo Yu, and Mark Dredze. 2015.
Improved relation extraction with feature-rich com-
positional embedding models. In EMNLP.

Gary D Hachtel and Fabio Somenzi. 2006. Logic syn-
thesis and verification algorithms. Springer Science
& Business Media.

Frank R Kschischang, Brendan J Frey, and H-A
Loeliger. 2001. Factor graphs and the sum-product
algorithm. IEEE Transactions on information the-
ory 47(2):498–519.

Tao Lei, Yuan Zhang, Regina Barzilay, and Tommi
Jaakkola. 2014. Low-rank tensors for scoring de-
pendency structures. In ACL.

Tao Lei, Yuan Zhang, Lluís Màrquez, Alessandro
Moschitti, and Regina Barzilay. 2015. High-order
lowrank tensors for semantic role labeling. In
NAACL.

Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP.

Adam Lopez. 2009. Translation as weighted deduc-
tion. In EACL.

André FT Martins, Noah A Smith, Pedro MQ Aguiar,
and Mário AT Figueiredo. 2011. Structured sparsity
in structured prediction. In CoNLL.

Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via im-
peratively defined factor graphs. In NIPS.

Judea Pearl. 2014. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Mor-
gan Kaufmann.

Audi Primadhanty, Xavier Carreras, and Ariadna Quat-
toni. 2015. Low-rank regularization for sparse con-
junctive feature spaces: An application to named en-
tity classification. In ACL.

Raymond A Ryan. 1980. Applications of topological
tensor products to infinite dimensional holomorphy.
Ph.D. thesis, Trinity College.

Mark Sammons, Christos Christodoulopoulos, Parisa
Kordjamshidi, Daniel Khashabi, Vivek Srikumar,
and Dan Roth. 2016. EDISON: Feature Extraction
for NLP. In LREC.

Vivek Srikumar and Christopher D. Manning. 2014.
Learning distributed representations for structured
output prediction. In NIPS.

Emma Strubell, Luke Vilnis, Kate Silverstein, and An-
drew McCallum. 2015. Learning Dynamic Feature
Selection for Fast Sequential Prediction. In ACL.

Charles Sutton, Andrew McCallum, et al. 2012. An
introduction to conditional random fields. Founda-
tions and Trends R© in Machine Learning 4(4):267–
373.

Erik F Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In CoNLL.

Martin J Wainwright and Michael I Jordan. 2008.
Graphical models, exponential families, and varia-
tional inference. Foundations and Trends R© in Ma-
chine Learning 1(1–2):1–305.

1899



Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia 57.

Mo Yu, Matthew R. Gormley, and Mark Dredze. 2015.
Combining Word Embeddings and Feature Embed-
dings for Fine-grained Relation Extraction. In
NAACL.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In ACL.

1900


	An Algebra for Feature Extraction

