



















































INF-HatEval at SemEval-2019 Task 5: Convolutional Neural Networks for Hate Speech Detection Against Women and Immigrants on Twitter


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 420–425
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

420

INF-HatEval at SemEval-2019 Task 5: Convolutional Neural Networks
for Hate Speech Detection Against Women and Immigrants on Twitter

Alison P. Ribeiro
Institute of Informatics

Federal University of Goiás
Goiânia – Goiás – Brazil

alisonrib17@gmail.com

Nádia F. F. da Silva
Institute of Informatics

Federal University of Goiás
Goiânia – Goiás – Brazil
nadia@inf.ufg.br

Abstract

In this paper, we describe our approach to
detect hate speech against women and immi-
grants on Twitter in a multilingual context, En-
glish and Spanish. This challenge was propo-
sed by the SemEval-2019 Task 5, where par-
ticipants should develop models for hate spe-
ech detection, a two-class classification where
systems have to predict whether a tweet in En-
glish or in Spanish with a given target (women
or immigrants) is hateful or not hateful (Task
A), and whether the hate speech is directed at a
specific person or a group of individuals (Task
B). For this, we implemented a Convolutio-
nal Neural Networks (CNN) using pre-trained
word embeddings (GloVe and FastText) with
300 dimensions. Our proposed model obtai-
ned in Task A 0.488 and 0.696 F1-score for
English and Spanish, respectively. For Task B,
the CNN obtained 0.297 and 0.430 EMR for
English and Spanish, respectively.

1 Introduction

With the growth of users in social networks,
there was also an increase in the odious activi-
ties that permeate these communicative structures.
According to Nockleby et al. (2000), hate speech
can be defined as any communication that depre-
cates a person or a group based on some charac-
teristics such as race, color, ethnicity, gender, na-
tionality, religion or other features. And the main
motive that encourages users to spread hate on so-
cial networks is anonymity, so users can spread
hate words to a particular target. For this reason,
the hatred propagated can generate irreversible
consequences, where young people who approach
with cyberbullying and homophobia, mainly, com-
mit suicide.

Nowadays, social networks like Twitter1, Fa-

1https://twitter.com/

cebook2 and YouTube3 are pressured to develop
tools to fight the proliferation of hate in their
networks. A good example of this is the German
government that threatened to fine social networks
by up to 50 million euros if they did not fight the
spread of hate (Gambäck and Sikdar, 2017).

However, while there is plenty of available con-
tent on social networks, the task of detecting hate
speech remains difficult, largely because of the use
of different sets of data for work, lack of bench-
marking, and efficient approaches. Waseem, for
example, bring a study focused on the detection
of racism and sexism, whereas Nahar et al. 2012
and Sanchez and Kumar 2011) conducted a sur-
vey on detecting bullying. For the detection of ho-
mophobia, misogyny and xenophobia, the number
of papers is still limited, one can cite a recent pa-
per (Sanguinetti et al., 2018), where the authors
sought to identify hate speech against immigrants.
However, it is important that new research is publi-
cized, because only in this way will it be possible
to fight against hate in social networks.

Introducing a brief definition of hate speech and
the importance of combating it, SemEval-2019
proposed a task in which it challenges partici-
pants to develop systems for detecting hate speech
against women and immigrants on Twitter from a
multilingual perspective , for English and Spanish.

The task was articulated around two related sub-
tasks for each one of the languages involved: a
basic task about hate speech, and another where
refined hate content resources will be investigated
to understand how existing approaches can handle
the identification of especially dangerous forms of
hatred, that is, those in which incitement is di-
rected against an individual rather than against a
group of people, and where aggressive behavior of

2https://www.facebook.com/
3https://www.youtube.com/

https://twitter.com/
https://www.facebook.com/
https://www.youtube.com/


421

the perpetrator can be identified as a prominent fe-
ature of the expression of hatred. In order to reach
this goal, this work proposed to develop a Con-
volutional Neural Network with the use of word
embeddings.

The paper is organised as follows: previous
work on hate-speech identification is discussed in
Section 2. Section 3 presents details about the
task, data sets and evaluation methods. Section
4 describes the methodology for categorizing hate
speech based on deep learning, while experiments
and results are reported in Section 5. Finally, Sec-
tion 6 summarises the discussion.

2 Related Work

Some computational methods to detect hate
speech are presented in this section. An example is
the work of Badjatiya et al. (2017) that applied se-
veral algorithms of machine learning and deep le-
arning, among them: Logistic Regression, Support
Vector Machine, Random Forest, Gradient Boos-
ted Decision (GBDT), CNN and Long Short-Term
Memory (LSTM). As a baseline they used char
n-grams and bag-of-words, and as word embed-
dings they used GloVe and FastText. The objec-
tive was to classify if a tweet is racist, sexist or
none, and the best result was 0.930 of F1-score,
which was obtained through a LSTM model with
Random Embedding and GBDT.

Another work that also follows a line of ternary
classification was proposed by Malmasi and Zam-
pieri (2017), where the purpose is to classify a
tweet as hateful, offensive (but not hateful) or of-
fensive language. For this, the researchers propo-
sed an approach based on n-grams and word skip-
grams using Support Vector Machine with cross-
validation, the best result achieved was 0.78 of ac-
curacy.

Gambäck and Sikdar (2017) developed a Con-
volutional Neural Network to classify hate speech
on Twitter. In this case, the authors used 4 catego-
ries: racism, sexism, both (racism and sexism) and
non-hate-speech. The structure of CNN was cons-
tructed with convolutional layers and pooling of
4 modes: character 4-grams, word vector based on
semantic information built using word2vec (Miko-
lov et al., 2013a), randomly generated word vec-
tors and word embeddings with character n-grams.
In the classification phase, the softmax function
and cross-validation with 10-folds were applied,
the model based on word2vec embeddings best

performed with 0.783 of F-score.
A recent study developed by Gaydhani et al.

(2018) sought to address the difference between
offensive language and hate speech, then the
authors proposed several machine learning models
based on n-grams and TF-IDF. The models were
analyzed considering several n-values in n-grams
and TF-IDF normalization methods. Consequen-
tly, the best result among several approaches was
0.956 of accuracy.

3 SemEval-2019 Task 5

In this section we will describe some details
about data sets, tasks, and evaluation methods.

3.1 Dataset
The data for the task consists of 9000 tweets in

English for training, 1000 for develop and 2805
for test. For Spanish, 4469 tweets for training, 500
for develop and 415 for test. The data were struc-
tured in 5 columns: id, text, Hate Speech (HS),
Target Range (TR) and Aggressiveness (AG). See
an example in the Table 1 (Basile et al., 2019). If
the @username is a woman, we have a case of
feminicide.

id text HS TR AG
93874 @username stupid wish 1 1 1you die.
18267 Leftwing filth Deport them 1 0 1all. #Sendthemback
18345 1,500 migrants have died 0 0 0in Mediterranean in 2018

Table 1: Example of hate speech. Some examples are
also taken from the data.

3.2 Task A
The task A is a two-class classification problem

in which participants have to predict whether a
tweet, in English or Spanish, with a particular tar-
get (women or immigrants) is hateful or not hate-
ful – Hate Speech (1/0).

3.3 Task B
The purpose of this task is to: (i) classify hate

tweets into English and Spanish, where tweets
with hate speech, against women or immigrants,
were identified as aggressive or non-aggressive,
and (ii) identify the harassed target as just one per-
son or group of individuals.



422

3.4 Evaluation
For the results evaluation of both tasks A and

B, different metrics were used in order to allow
more refined conclusions.

Task A. The systems will be evaluated according
to the following metrics: accuracy, precision, re-
call and F1-score. The equations below show how
the calculations are done. In the case of this task,
the scores will be classified by F1-score. For better
understanding, we will show the following defini-
tions:

• True positive (TP): means a correct classifica-
tion as odious. For example, the royal class
is hateful and the model ranks as hateful.

• True negative (TN): means a correct classifi-
cation as not hateful. For example, the royal
class is not hateful and the model ranked as
not hateful.

• False positive (FP): means a wrong classifica-
tion as odious. For example, the royal class is
not hateful and the model rated it as hateful.

• False negative (FN): means a wrong classi-
fication not hateful. For example, the royal
class is hateful and the model ranked as not
hateful.

Accuracy = TP + TN
TP + FN + FP + TN (1)

Precision = TP
TP + FP (2)

Recall = TP
TP + FN (3)

F1-score = 2 ∗ Precision ∗Recall
Precision +Recall (4)

Task B. In this task, the evaluation metrics are
two: partial match and exact match. The stra-
tegy for the partial match is to evaluate the Hate
Speech, Target Range and Agressiveness classes
independently of each other using the metrics de-
fined above. However, each system will include
all measures and a summary of the performance in
terms of macro-average F1-score, calculated ac-
cording to the Equation 5. The exact match consi-
ders the predicted classes together, thus computing

the Exact Match Ratio (Kazawa et al., 2005). Gi-
ven the set of data consisting of n multi-label sam-
ples (Xi, Yi), where Xi denotes the i-th instance
and Yi corresponds to the labels to be predicted
(HS, TR and AG), the Exact Match Ratio (EMR)
is calculated according to Equation 6.

F1-score = F1(HS) + F1(AG) + F1(TR)
3

(5)

EMR = 1
n

n

∑
i=1

I(Y i,Zi) (6)

where Zi denotes the set of labels predicted for
the i-th instance and I is the indicator function.

4 Methodology

In this section, we describe the details of our
proposed methods, including data preprocessing,
neural networks and word embeddings.

4.1 Preprocessing
This step consists in eliminating noises and

terms that have no semantic significance in classes
prediction. For this, we performed the removal of
links, numbers, special characters, and stop words
(words with low discriminative power, for exam-
ple, “is”, “that” etc.) and standardized in lower-
case.

4.2 Word embeddings
Word Embeddings (Bengio et al., 2003) is a su-

pervised statistical language model trained using
deep neural networks. The purpose of this lan-
guage model is to predict the next word, given the
previous words of the sentence. The vector em-
beddings was a great advance in relation to the
strategies based on the bag-of-words, which jus-
tifies its use in several works (Nakov et al., 2016;
Poria et al., 2015; Cliche, 2017; Zhou et al., 2018;
Rotim et al., 2017). For the proposed task, we use
the GloVe (Pennington et al., 2014) and FastText
(Joulin et al., 2016) model with 300 dimensions.

For the English language, we made use of the
Stanford pre-trained GloVe (Pennington et al.,
2014) where word embedding were trained with
Wikipedia 2014 and Gigaword 5, while Fast-
Text (Joulin et al., 2016) was trained in Wikipe-
dia 2017, UMBC webbase corpus and statmt.org
news.

For the Spanish language, the GloVe vocabu-
lary was computed from SBWC (Pennington et al.,



423

2014; Cardellino, 2016), while FastText was com-
puted from the Spanish Wikipedia (Bojanowski
et al., 2016).

4.3 Convolutional Neural Networks
Initially, the Convolutional Neural Network ar-

chitecture was designed for image processing,
however it has been commonly used in the sen-
timent analysis (Wang et al., 2016; Cambria et al.,
2016; Rosenthal et al., 2017; dos Santos and Gatti,
2014; Poria et al., 2015).

For the purpose of the task, a CNN was im-
plemented based on the architecture proposed by
(Zhang and Wallace, 2015), and this implementa-
tion can be divided in two steps: feature extrac-
tion and classification. In the feature extraction
step, only two layers of convolution and two layers
of pooling were used, with tanh activation func-
tion. Four filters were used, 2 of 3 dimensions
and 2 of 4 dimensions. Each filter refers to the
classic n-grams technique (extremely used in bag-
of-words based models), which consists of proces-
sing a group of n words, in order to consider not
only isolated words in a tweet, but also the context
in which they are inserted. The filters are applied
under the vector representation of the input tweets
(embedding layer), using the concept of Back pro-
pagation to adjust the weights dynamically. Ac-
cording to Poria et al. (2016), these filters can
extract lexical, syntactic or semantic features au-
tomatically. Finally, the two layers of convolution
and pooling are concatenated and directed to the
next step.

In the classification stage, we used two dense
layers, the first one has 512 neurons, relu func-
tion of activation and dropout of 0.5. The se-
cond has a neuron and sigmoid activation func-
tion, phase where classification occurs. For the
training, the loss and optimization functions used
were binary crossentropy and RMSprop (Hinton
et al., 2012) (with learning rate 0.001), respecti-
vely.

5 Results

In this section we will discuss the results ob-
tained by using a CNN for the detection of hate
speech and the target of hate.

Task A

English

Model F1 P R Acc

CNN-FastText 0.488 0.628 0.574 0.520

Spanish

Model F1 P R Acc

CNN-GloVe 0.696 0.708 0.712 0.696

Table 2: Results obtained related to Task A.

We obtained 0.488 of F1-score for English and
0.696 for Spanish with our CNN model using
word embeddings, as shown in Table 2. This re-
sult also suggests that the combination of CNN
and GloVe provides better results for this task.

English

Class F1 P R

hateful 0.617 0.465 0.916

not hateful 0.359 0.792 0.232

Spanish

Class F1 P R

hateful 0.685 0.598 0.802

not hateful 0.707 0.817 0.622

Table 3: Confusion matrix concerning task A.

The Table 3 displays the results of F1-score,
Precision and Recall reached by class for each lan-
guage. The F1-score can be used to measure the
performance of the classifier, in this case CNN
ranked the hateful class better, obtaining 0.617 of
F1-score, while the result for not hateful class was
0.359 of F1-score in the English language.

From the perspective of the Spanish language,
CNN obtained good results in the classification of
both classes, hateful and not hateful, with 0.685
F1-score and 0.707 F1-score, respectively.

Task B

English

Model F1 EMR

CNN-FastText 0.577 0.297

Spanish

Model F1 EMR

CNN-FastText 0.609 0.430

Table 4: Results obtained related to Task B.



424

Recapitulating the idea of task B, where the goal
is to identify the target of the hate speech, that is,
whether it is a single person or a group of indi-
viduals. Knowing that there is hate speech in the
tweet (HS is 1), then one must detect if the target
is only one person (TR is 1) or if it is a group of in-
dividuals (TR is 0), and if there is presence of ag-
gressiveness in speech (AG is 1) or not (AG is 0).
In this case, the EMR measure shows a percentage
in which it corresponds to an accuracy rate, that
is, it measures how much the model has managed
not only to classify the hate speech, but also the
target and the aggressiveness. The Table 4 shows
the results of task B, where it was possible to ob-
tain 0.297 EMR for English and 0.430 EMR for
Spanish.

6 Conclusion

In this paper, we introduced the system that we
proposed for SemEval-2019, task 5. Our goal
was to experience an architecture that was adap-
ted from a CNN using word embeddings. The task
was to detect hate speech against women and im-
migrants on Twitter from a multilingual perspec-
tive, English and Spanish. We participate in two
subtasks directed to the two languages, and we ob-
tain the 18th position in the ranking of task A and
the 19th position of task B in the English language.
In the Spanish language, we obtain the 24th posi-
tion in the ranking for both tasks.

The success of deep learning depends on finding
an architecture to fit the task. Furthermore, as deep
learning has scaled up to more challenging tasks,
the architectures have become difficult to design
by hand. In this paper, a CNN was implemen-
ted based on the architecture proposed by Zhang
and Wallace 2015 and a fine-tuning of hyperpara-
meters was not done for the proposed tasks (tasks
A and B). In addition, other features were not ex-
ploited as sarcasm and irony, inherent in this type
of domain. We intend to explore these and other
features in future work.

Another discussion can be raised regarding the
best performance to have happened in Spanish.
The main hypothesis is related to the nature of the
corpus used. It is observed that the test set of Spa-
nish is smaller than that of English, besides being
a corpus with “simpler texts to be classified” (Spa-
nish texts have few signs of sarcasm). Such analy-
zes need further studies and will be evaluated in
future work.

For future work as well, it would be interes-
ting to explore systems that use different parame-
ters for CNN and other word embeddings, such as
Word2Vec (Mikolov et al., 2013b). It would also
be interesting to construct an LSTM with atten-
tion mechanism proposed by Lin et al. (2017) and
compare its performance.

References
Pinkesh Badjatiya, Shashank Gupta, Manish Gupta,

and Vasudeva Varma. 2017. Deep learning for hate
speech detection in tweets. In Proceedings of the
26th International Conference on World Wide Web
Companion, pages 759–760. International World
Wide Web Conferences Steering Committee.

Valerio Basile, Cristina Bosco, Elisabetta Fersini, De-
bora Nozza, Viviana Patti, Francisco Rangel, Paolo
Rosso, and Manuela Sanguinetti. 2019. Semeval-
2019 task 5: Multilingual detection of hate speech
against immigrants and women in twitter. In Pro-
ceedings of the 13th International Workshop on Se-
mantic Evaluation (SemEval-2019). Association for
Computational Linguistics”, location = “Minneapo-
lis, Minnesota.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint ar-
Xiv:1607.04606.

Erik Cambria, Soujanya Poria, Rajiv Bajpai, and Björn
Schuller. 2016. Senticnet 4: A semantic resource
for sentiment analysis based on conceptual primiti-
ves. In Proceedings of COLING 2016, the 26th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 2666–2677.

Cristian Cardellino. 2016. Spanish Billion Words Cor-
pus and Embeddings.

Mathieu Cliche. 2017. Bb twtr at semeval-2017 task
4: Twitter sentiment analysis with cnns and lstms.
arXiv preprint arXiv:1704.06125.

Björn Gambäck and Utpal Kumar Sikdar. 2017. Using
convolutional neural networks to classify hate-
speech. In Proceedings of the First Workshop on
Abusive Language Online, pages 85–90.

Aditya Gaydhani, Vikrant Doma, Shrikant Kendre, and
Laxmi Bhagwat. 2018. Detecting hate speech and
offensive language on twitter using machine lear-
ning: An n-gram and tfidf based approach. arXiv
preprint arXiv:1809.08651.

https://crscardellino.github.io/SBWCE/
https://crscardellino.github.io/SBWCE/


425

Geoffrey Hinton, Nitish Srivastava, and Kevin
Swersky. 2012. Neural networks for machine lear-
ning lecture 6a overview of mini-batch gradient des-
cent.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efficient text
classification. CoRR, abs/1607.01759.

Hideto Kazawa, Tomonori Izumitani, Hirotoshi Taira,
and Eisaku Maeda. 2005. Maximal margin labeling
for multi-topic text categorization. In Advances in
neural information processing systems, pages 649–
656.

Zhouhan Lin, Minwei Feng, Cı́cero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. CoRR, abs/1703.03130.

Shervin Malmasi and Marcos Zampieri. 2017. De-
tecting hate speech in social media. CoRR,
abs/1712.06427.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word re-
presentations in vector space. arXiv preprint ar-
Xiv:1301.3781.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013b. Efficient estimation of word re-
presentations in vector space. arXiv preprint ar-
Xiv:1301.3781.

Vinita Nahar, Sayan Unankard, Xue Li, and Chaoyi
Pang. 2012. Sentiment analysis for effective detec-
tion of cyber bullying. In Asia-Pacific Web Confe-
rence, pages 767–774. Springer.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 task 4: Sentiment analysis in twitter. In Pro-
ceedings of the 10th international workshop on se-
mantic evaluation (semeval-2016), pages 1–18.

John T. Nockleby, Kenneth L. Karst Leonard W. Levy,
and Adam Winkler. 2000. Hate Speech. In Ency-
clopedia of the American Constitution. New York :
Macmillan Reference USA, ©2000.

Jeffrey Pennington, Richard Socher, and Chris-
topher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Na-
tural Language Processing (EMNLP), pages 1532–
1543.

Soujanya Poria, Erik Cambria, and Alexander Gel-
bukh. 2015. Deep convolutional neural network
textual features and multiple kernel learning for
utterance-level multimodal sentiment analysis. In
Proceedings of the 2015 conference on empiri-
cal methods in natural language processing, pages
2539–2544.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
and Prateek Vij. 2016. A deeper look into sarcas-
tic tweets using deep convolutional neural networks.
arXiv preprint arXiv:1610.08815.

Sara Rosenthal, Noura Farra, and Preslav Nakov.
2017. Semeval-2017 task 4: Sentiment analysis in
twitter. In Proceedings of the 11th International
Workshop on Semantic Evaluation (SemEval-2017),
pages 502–518.

Leon Rotim, Martin Tutek, and Jan Šnajder. 2017.
Takelab at semeval-2017 task 5: Linear aggrega-
tion of word embeddings for fine-grained sentiment
analysis of financial news. In Proceedings of the
11th International Workshop on Semantic Evalua-
tion (SemEval-2017), pages 866–871.

Huascar Sanchez and Shreyas Kumar. 2011. Twitter
bullying detection. ser. NSDI, 12:15–15.

Manuela Sanguinetti, Fabio Poletto, Cristina Bosco,
Viviana Patti, and Marco Stranisci. 2018. An italian
twitter corpus of hate speech against immigrants. In
Proceedings of LREC.

Cicero dos Santos and Maira Gatti. 2014. Deep con-
volutional neural networks for sentiment analysis
of short texts. In Proceedings of COLING 2014,
the 25th International Conference on Computatio-
nal Linguistics: Technical Papers, pages 69–78.

Jin Wang, Liang-Chih Yu, K Robert Lai, and Xuejie
Zhang. 2016. Dimensional sentiment analysis using
a regional cnn-lstm model. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vo-
lume 2, pages 225–230.

Zeerak Waseem. 2016. Are you a racist or am i seeing
things? annotator influence on hate speech detection
on twitter. In Proceedings of the first workshop on
NLP and computational social science, pages 138–
142.

Ye Zhang and Byron Wallace. 2015. A sensitivity
analysis of (and practitioners’ guide to) convoluti-
onal neural networks for sentence classification. ar-
Xiv preprint arXiv:1510.03820.

Liyuan Zhou, Qiongkai Xu, Hanna Suominen, and
Tom Gedeon. 2018. Epution at semeval-2018 task
2: Emoji prediction with user adaption. In Proce-
edings of The 12th International Workshop on Se-
mantic Evaluation, pages 449–453.

http://arxiv.org/abs/1607.01759
http://arxiv.org/abs/1607.01759
http://arxiv.org/abs/1703.03130
http://arxiv.org/abs/1703.03130
http://arxiv.org/abs/1712.06427
http://arxiv.org/abs/1712.06427
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162

