



















































Look Harder: A Neural Machine Translation Model with Hard Attention


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3037–3043
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3037

Look Harder: A Neural Machine Translation Model with Hard Attention

Sathish Indurthi Insoo Chung Sangha Kim
Samsung Research, Seoul, South Korea

{s.indurthi, insooo.chung, sangha01.kim}@samsung.com

Abstract
Soft-attention based Neural Machine Trans-
lation (NMT) models have achieved promis-
ing results on several translation tasks. These
models attend all the words in the source se-
quence for each target token, which makes
them ineffective for long sequence transla-
tion. In this work, we propose a hard-attention
based NMT model which selects a subset of
source tokens for each target token to effec-
tively handle long sequence translation. Due
to the discrete nature of the hard-attention
mechanism, we design a reinforcement learn-
ing algorithm coupled with reward shaping
strategy to efficiently train it. Experimen-
tal results show that the proposed model per-
forms better on long sequences and thereby
achieves significant BLEU score improvement
on English-German (EN-DE) and English-
French (EN-FR) translation tasks compared to
the soft-attention based NMT.

1 Introduction

In recent years, soft-attention based neural ma-
chine translation models (Bahdanau et al., 2015;
Gehring et al., 2017; Hassan et al., 2018) have
achieved state-of-the-art results on different ma-
chine translation tasks. The soft-attention mech-
anism computes the context (encoder-decoder at-
tention) vector for each target token by weight-
ing and combining all the tokens of the source se-
quence, which makes them ineffective for long se-
quence translation (Lawson et al., 2017). More-
over, weighting and combining all the tokens of
the source sequence may not be required – a few
relevant tokens are sufficient for each target token.

Different attention mechanisms have been pro-
posed to improve the quality of the context vec-
tor. For example, Luong et al. (2015); Yang et al.
(2018) proposed a local-attention mechanism to
selectively focus on a small window of source to-
kens to compute the context vector. Even though

local-attention has improved the translation qual-
ity, it is not flexible enough to focus on relevant to-
kens when they fall outside the specified window
size.

To overcome the shortcomings of the above ap-
proaches, we propose a hard-attention mechanism
for a deep NMT model (Vaswani et al., 2017). The
proposed model solely selects a few relevant to-
kens across the entire source sequence for each
target token to effectively handle long sequence
translation. Due to the discrete nature of the
hard-attention mechanism, we design a Reinforce-
ment Learning (RL) algorithm with reward shap-
ing strategy (Ng et al., 1999) to train it. The pro-
posed hard-attention based NMT model consis-
tently outperforms the soft-attention based NMT
model (Vaswani et al., 2017), and the gap grows
as the sequence length increases.

2 Background

A typical NMT model based on encoder-decoder
architecture generates a target sequence y =
{y1, · · · , yn} given a source sequence x =
{x1, · · · , xm} by modeling the conditional prob-
ability p(y|x, θ). The encoder (θe) computes a set
of representations Z = {z1, · · · , zm} ∈ Rm×d
corresponding to x and the decoder (θd) generates
one target word at a time using the context vec-
tor computed using Z. It is trained on a set of D
parallel sequences to maximize the log likelihood:

J1(θ) =
1

N

D∑
i=1

log p
(
yi|xi; θ

)
, (1)

where θ = {θe, θd}.
In recent years, among all the encoder-decoder

architectures for NMT, Transformer (Vaswani
et al., 2017) has achieved the best translation qual-
ity (Wu et al., 2018). The encoder and decoder
blocks of the Transformer are composed of a stack



3038

(a) (b)

Figure 1: (a) Overview of hard-attention based Transformer network. (b) Overview of RL agent based hard-
attention and objective function.

of N (=6) identical layers as shown in Figure
1a. Each layer in the encoder contains two sub-
layers, a multi-head self-attention mechanism and
a position-wise fully connected feed-forward net-
work. Each decoder layer consists of three sub-
layers; the first and third sub-layers are similar
to the encoder sub-layers, and the additional sec-
ond sub-layer is used to compute the encoder-
decoder attention (context) vector based on the
soft-attention based approaches (Bahdanau et al.,
2015; Gehring et al., 2017). Here we briefly de-
scribe the soft computation of encoder-decoder
attention vector in the Transformer architecture.
Please refer Vaswani et al. (2017) for the detailed
architecture.

For each target word ŷt, the second sub-layer in
the decoder computes encoder-decoder attention
at based on the encoder representations, Z. In
practice we compute the attention vectors simul-
taneously for all the time steps by packing ŷt’s
and zi’s in to matrices. The soft attention of the
encoder-decoder, Ai, for all the decoding steps is
computed as follows:

Ai(Ŷ
i−1
,Z) = softmax

(
Ŷ

i−1
Z√

d

)
Z,∀i ∈ N,

(2)
where d is the dimension and Ŷ

i−1 ∈ Rn×d is the
decoder output from the previous layer.

3 Proposed Model

Section 3.1 introduces our proposed hard-attention
mechanism to compute the context vector for each
target token. We train the proposed model by de-
signing a RL algorithm with reward shaping strat-
egy - described in Section 3.2.

3.1 Hard Attention
Instead of computing the weighted average over
all the encoder output as shown in Eq. 2, we
specifically select a subset of encoder outputs
(zi’s) for the last layer (N ) of the decoder using
the hard-attention mechanism as shown in Fig-
ure 1a. This allows us to efficiently compute
the encoder-decoder attention vector for long se-
quences. To compute the hard-attention between
the last layers of the Transformer encoder-decoder
blocks, we replace the second sub-layer of the de-
coder block’s last layer with the RL agent based
attention mechanism. Overview of the proposed
RL agent based attention mechanism is shown in
Figure 1b and computed as follows:

First, we learn the projections Ỹ
N−1

, Z̃ for
Ŷ

N−1
and Z as,

Ỹ
N−1

= tanh(W d2 (W
d
1 Ŷ

N−1
+ bd1) + b

d
2),

Z̃ = tanh(W e2 (W
e
1Z + b

e
1) + b

e
2).

We then compute the attention scores S as,

S(Ỹ
N−1

, Z̃) = Ỹ
N−1

Z̃. (3)



3039

We apply the hard-attention mechanism on at-
tention scores (S) to dynamically choose mul-
tiple relevant encoder tokens for each decoding
token. Given S, this mechanism generates an
equal length of binary random-variables, β =
{β1, · · · , βm} for each target token, where βi =
1 indicates that zi is relevant whereas βi = 0
indicates that zi is irrelevant. The relevant to-
kens are sampled using bernoulli distribution over
each βi for all the target tokens. This hard selec-
tion of encoder outputs introduces discrete latent
variables and estimating them requires RL algo-
rithms. Hence, we design the following reinforce-
ment learner policy for the hard-attention for each
decoding step t.

πt(r|st,θh) = βti (4)

where βti ∈ β represents the probability of a
encoder output (agent’s action) being selected at
time t, and st ∈ S is the state of the environ-
ment. Now, the hard encoder-decoder attention,
Ã, is calculated as, follows:

Ẑ = tanh(W ê2 (W
ê
1Z + b

ê
1) + b

ê
2) (5)

Ã = βẐ (6)

Unlike the soft encoder-decoder attentionA in Eq.
2, which contains the weighted average of entire
encoder outputs, the hard encoder-decoder atten-
tion Ã in Eq. 6 contains information from only
relevant encoder outputs for each decoding step.

3.2 Strategies for RL training
The model parameters come from the encoder,
decoder blocks and reinforcement learning agent,
which are denoted as θe, θd and θh respectively.
Estimation of θe and θd is done by using the ob-
jective J1 in Eq. 1 and gradient descent algorithm.
However, estimating θh is difficult given their dis-
crete nature. Therefore, we formulate the estima-
tion of θh as a reinforcement learning problem and
design a reward function over it. An overveiw of
the proposed RL training is given in Algorithm 1.

We use BLEU (Papineni et al., 2002) score
between the predicted sequence and the target
sequence as our reward function, denoted as
R(y

′
,y), where y

′
is the predicted output se-

quence. The objective is to maximize the reward
with respect to the agent’s action in Eq. 4 and de-
fined as,

J2(θh) =

n∑
t=1

logp(r|st,θh)R(y
′
,y) (7)

Algorithm 1: Hard Attention based NMT
1 Input: Training examples, {xi, yi}Li=1,

hyperparameters such as learning rate (α), λ
2 Initialize model parameters θ = {θe, θh, θd}
3 while not done do
4 Sample k training examples
5 Compute attention scores S using Eq. 3
6 for each decoding step do
7 Compute the policy using Eq. 4 to

select the relevant source sequence
tokens

8 Compute the reward rt using Eq. 8
9 end

10 Compute J(θ) = −(J1(θe, θd) + J2(θh))
using Eq. 1 and Eq. 9

11 Update the parameters θ with gradient
descent: θ

′
= θ − α∇J(θ)

12 end
13 Return: θ

Reward Shaping To generate the complete target
sentence, the agent needs to take actions at each
target word, but only one reward is available for
all these tens of thousands of actions. This makes
RL training inefficient since the same terminal re-
ward is applied to all the intermediate actions. To
overcome this issue we adopt reward shaping strat-
egy of Ng et al. (1999). This strategy assigns dis-
tinct rewards to each intermediate action taken by
the agent. The intermediate reward, denoted as
rt(y

′
t, y), for the agent action at decoding step t

is computed as:

rt(y
′

t,y) = R(y
′

1..t,y)−R(y
′

1..t−1,y) (8)

During training, we use cumulative reward(∑n
t=1 rt(y

′
t,y)

)
achieved from the decoding

step t to update the agent’s policy.
Entropy Bonus We add entropy bonus to avoid

policy to collapse too quickly. The entropy bonus
encourages an agent to take actions more unpre-
dictably, rather than less so. The RL objective
function in Eq. 7 becomes,

Ĵ2(θh) = J2(θh) + λH(πt(r|st,θh)) (9)

We approximate the gradient ∆θh Ĵ2(θh) by
using REINFORCE (Williams, 1992) algorithm
which allows us to jointly train J1(θe, θd) and
Ĵ2(θh).



3040

Architecture Model
BLEU

EN-DE EN-FR
Vaswani et al. (2017) Transformer big 28.40 41.00

Wu et al. (2018) Transformer big + sequence-loss 28.75 41.47
Yang et al. (2018) Transformer big + localness 28.89 n/a

this work Transformer big + hard-attention 29.29 42.26

Table 1: Performance of various models on EN-DE and EN-FR translation tasks.

# Sequences in each group
1-10 11-20 21-30 31-40 41-50 51-60 ≥ 61

EN-DE 469 1148 796 383 160 40 8
EN-FR 235 765 796 596 396 165 90

Table 2: Number of sequences present in each group (based on sequence length) of EN-DE and EN-FR testsets.

Figure 2: Performance of Transformer with Soft-Attention (TSA), and Transformer with Hard Attention (THA)
for various sequence lengths on EN-DE and EN-FR translation tasks.

4 Experimental Results

4.1 Datasets
We conduct experiments on WMT 2014 English-
German (EN-DE) and English-French (EN-FR)
translation tasks. The approximate number of
training pairs in EN-DE and EN-FR datasets are
4.5M and 36M respectively; newstest2013 and
newstest2014 are used as the dev and test sets. We
follow the similar preprocessing steps as described
in Vaswani et al. (2017) for both the datasets. We
encode the sentences using word-piece vocabulary
(Wu et al., 2016) and the shared source-target vo-
cabulary size is set to 32000 tokens.

4.2 Results
4.3 Implementation Details
We adopt the implementation of the Transformer
(Vaswani et al., 2018) with transformer big set-
tings. All the models are trained using 8 NVIDIA
Tesla P40 GPUs on a single machine. The BLEU
score used in the reward shaping is calculated sim-
ilarly to Bahdanau et al. (2017); all the n-gram
counts start from 1, and the resulting score is mul-

tiplied by the length of the target reference sen-
tence. The beam search width (=4) is set empir-
ically based on the dev set performance and λ in
Eq. 9 is set to 1e-3 in all the experiments.

Models We compare the proposed model
with the soft-attention based Transformer
model(Vaswani et al., 2017). To check whether
the performance improvements are coming from
the hard-attention mechanism (Eq. 4) or from
the sequence reward incorporated in the objective
function (Eq. 7), we compare our work with previ-
ously proposed sequence loss based NMT method
(Wu et al., 2018). This NMT method is built
on top of the Transformer model and trained by
combing cross-entropy loss and sequence reward
(BLEU score). We also compare our model with
the recently proposed Localness Self-Attention
network (Yang et al., 2018) which incorporates
a localness bias into the Transformer attention
distribution to capture useful local context.

Main Results Table 1 shows the performance of
various models on EN-DE and EN-FR translation
tasks. These test set case sensitive BLEU scores



3041

are obtained using SacreBLEU toolkit1 (Post,
2018). The BLEU score difference between our
hard-attention based Transformer model and the
original soft-attention based Transformer model
indicates the effectiveness of selecting a few rele-
vant source tokens for each target token. The per-
formance gap between our method and sequence
loss based Transformer (Wu et al., 2018) shows
that the improvements are indeed coming from the
hard-attention mechanism. Our approach of in-
corporating hard-attention into decoder’s top self-
attention layer to select relevant tokens yielded
better results compared to the Localness Self-
Attention (Yang et al., 2018) approach of incorpo-
rating localness bias only to lower self-attention
layers. It can be noted that our model achieved
29.29 and 42.26 BLEU points on EN-DE and EN-
FR tasks respectively – surpassing the previously
published models.

Analysis To see the effect of the hard-attention
mechanism for longer sequences, we group the se-
quences in the test set based on their length and
compute the BLEU score for each group. Table
2 shows the number of sequences present in each
group. Figure 2 shows that Transformer with hard
attention is more effective in handling the long se-
quences. Specifically, the performance gap be-
tween our model (THA) and the original Trans-
former model (TSA) grows bigger as sequences
become longer.

5 Related Work

Even though RL based models are difficult to train,
in recent years, multiple works (Mnih et al., 2014;
Choi et al., 2017; Yu et al., 2017; Narayan et al.,
2018; Sathish et al., 2018; Shen et al., 2018) have
shown to improve the performance of several nat-
ural language processing tasks. Also, it has been
used in NMT (Edunov et al., 2018; Wu et al., 2017;
Bahdanau et al., 2017) to overcome the incon-
sistency between the token level objective func-
tion and sequence-level evaluation metrics such as
BLEU. Our approach is also related to the method
proposed by Lei et al. (2016) to explain the deci-
sion of text classifier. However, here we focus on
selecting a few relevant tokens from a source se-
quence in a translation task.

Recently, several innovations are proposed on
top of the Transformer model to improve perfor-
mance and training speed. For example, Shaw

1https://github.com/mjpost/sacrebleu

et al. (2018) incorporated relative positions and
Ott et al. (2018) proposed efficient training strate-
gies. These improvements are complementary to
the proposed method. Incorporating these tech-
niques will further improve the performance of the
proposed method.

6 Conclusion

In this work, we proposed a hard-attention based
NMT model which focuses solely on a few rel-
evant source sequence tokens for each target to-
ken to effectively handle long sequence transla-
tion. We train our model by designing an RL
algorithm with the reward shaping strategy. Our
model sets new state-of-the-art results on EN-DE
and EN-FR translation tasks.

References
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,

Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In Proceedings
of the Fifth International Conference on Learning
Representations, ICLR-2017.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate.

Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia
Polosukhin, Alexandre Lacoste, and Jonathan Be-
rant. 2017. Coarse-to-fine question answering for
long documents. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 209–220.
Association for Computational Linguistics.

Sergey Edunov, Myle Ott, Michael Auli, David Grang-
ier, and Marc’Aurelio Ranzato. 2018. Classical
structured prediction losses for sequence to se-
quence learning. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
355–364. Association for Computational Linguis-
tics.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Con-
volutional sequence to sequence learning. CoRR,
abs/1705.03122.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu,
Renqian Luo, Arul Menezes, Tao Qin, Frank Seide,
Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce
Xia, Dongdong Zhang, Zhirui Zhang, and Ming

https://github.com/mjpost/sacrebleu
https://github.com/mjpost/sacrebleu
https://openreview.net/pdf?id=SJDaqqveg
https://openreview.net/pdf?id=SJDaqqveg
https://doi.org/10.18653/v1/P17-1020
https://doi.org/10.18653/v1/P17-1020
https://doi.org/10.18653/v1/N18-1033
https://doi.org/10.18653/v1/N18-1033
https://doi.org/10.18653/v1/N18-1033
http://arxiv.org/abs/1705.03122
http://arxiv.org/abs/1705.03122


3042

Zhou. 2018. Achieving human parity on auto-
matic chinese to english news translation. CoRR,
abs/1803.05567.

Dieterich Lawson, George Tucker, Chung-Cheng Chiu,
Colin Raffel, Kevin Swersky, and Navdeep Jaitly.
2017. Learning hard alignments with variational in-
ference. CoRR, abs/1705.05524.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
Rationalizing neural predictions. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 107–117, Austin,
Texas. Association for Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421. Associa-
tion for Computational Linguistics.

Volodymyr Mnih, Nicolas Heess, Alex Graves, and Ko-
ray Kavukcuoglu. 2014. Recurrent models of visual
attention. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems - Volume 2, NIPS’14, pages 2204–2212, Cam-
bridge, MA, USA. MIT Press.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 1747–1759. Association for Computa-
tional Linguistics.

Andrew Y Ng, Daishi Harada, and Stuart J Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping.
pages 278–287.

Myle Ott, Sergey Edunov, David Grangier, and
Michael Auli. 2018. Scaling neural machine trans-
lation. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 1–9,
Belgium, Brussels. Association for Computational
Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Matt Post. 2018. A call for clarity in reporting bleu
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191. Association for Computational Linguistics.

Indurthi Sathish, Seunghak Yu, Seohyun Back, and
Heriberto Cuayahuitl. 2018. Cut to the chase: A

context zoom-in network for reading comprehen-
sion. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 570–575. Association for Computational
Linguistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 464–468,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Sen Wang, and Chengqi Zhang. 2018. Reinforced
self-attention network: A hybrid of hard and soft
attention for sequence modeling. In Proceedings
of the 27th International Joint Conference on Ar-
tificial Intelligence, IJCAI’18, pages 4345–4352.
AAAI Press.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
cois Chollet, Aidan Gomez, Stephan Gouws, Llion
Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-
mar, Ryan Sepassi, Noam Shazeer, and Jakob
Uszkoreit. 2018. Tensor2Tensor for neural machine
translation. In Proceedings of the 13th Conference
of the Association for Machine Translation in the
Americas (Volume 1: Research Papers), pages 193–
199, Boston, MA. Association for Machine Transla-
tion in the Americas.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran Asso-
ciates, Inc.

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Mach. Learn., 8(3-4):229–256.

Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-
Yan Liu. 2018. A study of reinforcement learning
for neural machine translation. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 3612–3621. Asso-
ciation for Computational Linguistics.

Lijun Wu, Li Zhao, Tao Qin, Jianhuang Lai, and Tie-
Yan Liu. 2017. Sequence prediction with unlabeled
data by reward function learning. In Proceedings of
the Twenty-Sixth International Joint Conference on
Artificial Intelligence, IJCAI-17, pages 3098–3104.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant

http://arxiv.org/abs/1803.05567
http://arxiv.org/abs/1803.05567
http://arxiv.org/abs/1705.05524
http://arxiv.org/abs/1705.05524
https://doi.org/10.18653/v1/D16-1011
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
http://dl.acm.org/citation.cfm?id=2969033.2969073
http://dl.acm.org/citation.cfm?id=2969033.2969073
https://doi.org/10.18653/v1/N18-1158
https://doi.org/10.18653/v1/N18-1158
http://jmlr.org/proceedings/papers/v37/xuc15.pdf
http://jmlr.org/proceedings/papers/v37/xuc15.pdf
https://www.aclweb.org/anthology/W18-6301
https://www.aclweb.org/anthology/W18-6301
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
http://aclweb.org/anthology/W18-6319
http://aclweb.org/anthology/W18-6319
http://aclweb.org/anthology/D18-1054
http://aclweb.org/anthology/D18-1054
http://aclweb.org/anthology/D18-1054
https://doi.org/10.18653/v1/N18-2074
https://doi.org/10.18653/v1/N18-2074
http://dl.acm.org/citation.cfm?id=3304222.3304374
http://dl.acm.org/citation.cfm?id=3304222.3304374
http://dl.acm.org/citation.cfm?id=3304222.3304374
https://www.aclweb.org/anthology/W18-1819
https://www.aclweb.org/anthology/W18-1819
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://doi.org/10.1007/BF00992696
https://doi.org/10.1007/BF00992696
https://doi.org/10.1007/BF00992696
http://aclweb.org/anthology/D18-1397
http://aclweb.org/anthology/D18-1397
https://doi.org/10.24963/ijcai.2017/432
https://doi.org/10.24963/ijcai.2017/432


3043

Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR,
abs/1609.08144.

Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong
Meng, Lidia S. Chao, and Tong Zhang. 2018. Mod-
eling localness for self-attention networks. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 4449–
4458. Association for Computational Linguistics.

Adams Wei Yu, Hongrae Lee, and Quoc Le. 2017.
Learning to skim text. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1880–1890. Association for Computational Linguis-
tics.

http://arxiv.org/abs/1609.08144
http://arxiv.org/abs/1609.08144
http://arxiv.org/abs/1609.08144
http://aclweb.org/anthology/D18-1475
http://aclweb.org/anthology/D18-1475
https://doi.org/10.18653/v1/P17-1172

