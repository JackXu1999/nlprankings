



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1789–1798
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1164

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1789–1798
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1164

Exploiting Argument Information to Improve Event Detection
via Supervised Attention Mechanisms

Shulin Liu1,2, Yubo Chen1,2, Kang Liu1 and Jun Zhao1,2
1 National Laboratory of Pattern Recognition, Institute of Automation,

Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China

{shulin.liu, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn

Abstract

This paper tackles the task of event
detection (ED), which involves identi-
fying and categorizing events. We ar-
gue that arguments provide significant
clues to this task, but they are either
completely ignored or exploited in an
indirect manner in existing detection
approaches. In this work, we propose to
exploit argument information explicitly
for ED via supervised attention mech-
anisms. In specific, we systematically
investigate the proposed model under
the supervision of different attention
strategies. Experimental results show
that our approach advances state-of-
the-arts and achieves the best F1 score
on ACE 2005 dataset.

1 Introduction

In the ACE (Automatic Context Extraction)
event extraction program, an event is repre-
sented as a structure comprising an event
trigger and a set of arguments. This work
tackles event detection (ED) task, which is a
crucial part of event extraction (EE) and fo-
cuses on identifying event triggers and cate-
gorizing them. For instance, in the sentence
“He died in the hospital”, an ED system
is expected to detect a Die event along with
the trigger word “died”. Besides, the task of
EE also includes event argument extraction
(AE), which involves event argument identi-
fication and role classification. In the above
sentence, the arguments of the event include
“He”(Role = Person) and “hospital”(Role =
Place). However, this paper does not focus on
AE and only tackles the former task.

According to the above definitions, event ar-
guments seem to be not essentially necessary
to ED. However, we argue that they are capa-
ble of providing significant clues for identifying
and categorizing events. They are especially
useful for ambiguous trigger words. For exam-
ple, consider a sentence in ACE 2005 dataset:

Mohamad fired Anwar, his for-
mer protege, in 1998.

In this sentence, “fired” is the trigger word
and the other bold words are event arguments.
The correct type of the event triggered by
“fired” in this case is End-Position . How-
ever, it might be easily misidentified as At-
tack because “fired” is a multivocal word.
In this case, if we consider the phrase “for-
mer protege”, which serves as an argumen-
t (Role = Position) of the target event, we
would have more confidence in predicting it as
an End-Position event.

Unfortunately, most existing methods per-
formed event detection individually, where the
annotated arguments in training set are totally
ignored (Ji and Grishman, 2008; Gupta and Ji,
2009; Hong et al., 2011; Chen et al., 2015; N-
guyen and Grishman, 2015; Liu et al., 2016a,b;
Nguyen and Grishman, 2016). Although some
joint learning based methods have been pro-
posed, which tackled event detection and argu-
ment extraction simultaneously (Riedel et al.,
2009; Li et al., 2013; Venugopal et al., 2014;
Nguyen et al., 2016), these approaches usu-
ally only make remarkable improvements to
AE, but insignificant to ED. Table 1 illustrates
our observations. Li et al. (2013) and Nguyen
et al. (2016) are state-of-the-art joint mod-
els in symbolic and embedding methods for
event extraction, respectively. Compared with
state-of-the-art pipeline systems, both join-

1789

https://doi.org/10.18653/v1/P17-1164
https://doi.org/10.18653/v1/P17-1164


Methods ED AE

Symbolic Hong’s pipeline (2011) 68.3 48.3

Methods Li’s joint (2013) 67.5 52.7

Embedding Chen’s pipeline (2015) 69.1 53.5

Methods Nguyen’s joint (2016) 69.3 55.4

Table 1: Performances of pipeline and joint
approaches on ACE 2005 dataset. The pipeline
method in each group was the state-of-the-art
system when the corresponding joint method
was proposed.

t methods achieved remarkable improvements
on AE (over 1.9 points), whereas achieved in-
significant improvements on ED (less than 0.2
points). The symbolic joint method even per-
formed worse (67.5 vs. 68.3) than pipeline sys-
tem on ED.

We believe that this phenomenon may be
caused by the following two reasons. On the
one hand, since joint methods simultaneous-
ly solve ED and AE, methods following this
paradigm usually combine the loss functions
of these two tasks and are jointly trained un-
der the supervision of annotated triggers and
arguments. However, training corpus contains
much more annotated arguments than trigger-
s (about 9800 arguments and 5300 triggers in
ACE 2005 dataset) because each trigger may
be along with multiple event arguments. Thus,
the unbalanced data may cause joint models
to favor AE task. On the other hand, in im-
plementation, joint models usually pre-predict
several potential triggers and arguments first
and then make global inference to select cor-
rect items. When pre-predicting potential trig-
gers, almost all existing approaches do not
leverage any argument information. In this
way, ED does hardly benefit from the anno-
tated arguments. By contrast, the component
for pre-prediction of arguments always exploit-
s the extracted trigger information. Thus, we
argue that annotated arguments are actually
used for AE, not for ED in existing joint meth-
ods, which is also the reason we call it an in-
direct way to use arguments for ED.

Contrast to joint methods, this paper pro-
poses to exploit argument information explic-
itly for ED. We have analyzed that arguments
are capable of providing significant clues to
ED, which gives us an enlightenment that ar-

guments should be focused on when perform-
ing this task. Therefore, we propose a neural
network based approach to detect events in
texts. And in the proposed approach, we adop-
t a supervised attention mechanism to achieve
this goal, where argument words are expect-
ed to acquire more attention than other word-
s. The attention value of each word in a giv-
en sentence is calculated by an operation be-
tween the current word and the target trigger
candidate. Specifically, in training procedure,
we first construct gold attentions for each trig-
ger candidate based on annotated arguments.
Then, treating gold attentions as the super-
vision to train the attention mechanism, we
learn attention and event detector jointly both
in supervised manner. In testing procedure,
we use the ED model with learned attention
mechanisms to detect events.

In the experiment section, we systemati-
cally conduct comparisons on a widely used
benchmark dataset ACE20051. In order to fur-
ther demonstrate the effectiveness of our ap-
proach, we also use events from FrameNet
(FN) (F. Baker et al., 1998) as extra training
data, as the same as Liu et al. (2016a) to al-
leviate the data-sparseness problem for ED to
augment the performance of the proposed ap-
proach. The experimental results demonstrate
that the proposed approach is effective for ED
task, and it outperforms state-of-the-art ap-
proaches with remarkable gains.

To sum up, our main contributions are: (1)
we analyze the problem of joint models on the
task of ED, and propose to use the annotated
argument information explicitly for this task.
(2) to achieve this goal, we introduce a su-
pervised attention based ED model. Further-
more, we systematically investigate different
attention strategies for the proposed model.
(3) we improve the performance of ED and
achieve the best performance on the widely
used benchmark dataset ACE 2005.

2 Task Description

The ED task is a subtask of ACE event eval-
uations where an event is defined as a specif-
ic occurrence involving one or more partici-
pants. Event extraction task requires certain
specified types of events, which are mentioned

1https://catalog.ldc.upenn.edu/LDC2006T06

1790



in the source language data, be detected. We
firstly introduce some ACE terminologies to
facilitate the understanding of this task:

Entity: an object or a set of objects in one
of the semantic categories of interests.

Entity mention: a reference to an entity
(typically, a noun phrase).

Event trigger: the main word that most
clearly expresses an event occurrence.

Event arguments: the mentions that are
involved in an event (participants).

Event mention: a phrase or sentence with-
in which an event is described, including the
trigger and arguments.

The goal of ED is to identify event triggers
and categorize their event types. For instance,
in the sentence “He died in the hospital”, an
ED system is expected to detect a Die event
along with the trigger word “died”. The detec-
tion of event arguments “He”(Role = Person)
and “hospital”(Role = Place) is not involved
in the ED task. The 2005 ACE evaluation in-
cluded 8 super types of events, with 33 sub-
types. Following previous work, we treat these
simply as 33 separate event types and ignore
the hierarchical structure among them.

3 The Proposed Approach

Similar to existing work, we model ED as a
multi-class classification task. In detail, given
a sentence, we treat every token in that sen-
tence as a trigger candidate, and our goal is to
classify each of these candidates into one of 34
classes (33 event types plus an NA class).

In our approach, every word along with its
context, which includes the contextual words
and entities, constitute an event trigger candi-
date. Figure 1 describes the architecture of the
proposed approach, which involves two com-
ponents: (i) Context Representation Learn-
ing (CRL), which reveals the representation
of both contextual words and entities via at-
tention mechanisms; (ii) Event Detector (ED),
which assigns an event type (including the NA
type) to each candidate based on the learned
contextual representations.

3.1 Context Representation Learning

In order to prepare for Context Representa-
tion Learning (CRL), we limit the contex-
t to a fixed length by trimming longer sen-

Figure 1: The architecture of the proposed
approach for event detection. In this figure,
w is the candidate word, [w1, ..., wn] is the
contextual words of w, and [e1, ..., en] is the
corresponding entity types of [w1, ... , wn].

tences and padding shorter sentences with a
special token when necessary. Let n be the
fixed length and w0 be the current candidate
trigger word, then its contextual words Cw is
[w− n

2
, w− n

2
+1, ..., w−1, w1, ..., wn

2
−1, wn

2
]2, and

its contextual entities, which is the corre-
sponding entity types (including an NA type)
of Cw, is [e− n

2
, e− n

2
+1, ..., e−1, e1, ..., en

2
−1, en

2
].

For convenience, we use w to denote the cur-
rent word, [w1, w2, ..., wn] to denote the con-
textual words Cw and [e1, e2, ..., en] to denote
the contextual entities Ce in figure 1. Note
that, both w, Cw and Ce mentioned above
are originally in symbolic representation. Be-
fore entering CRL component, we transform
them into real-valued vector by looking up
word embedding table and entity type embed-
ding table. Then we calculate attention vec-
tors for both contextual words and entities
by performing operations between the curren-
t word w and its contexts. Finally, the con-
textual words representation cw and contex-
tual entities representation ce are formed by
the weighted sum of the corresponding embed-
dings of each word and entity in Cw and Ce,
respectively. We will give the details in the fol-

2The current candidate trigger word w0 is not in-
cluded in the context.

1791



lowing subsections.

3.1.1 Word Embedding Table

Word embeddings learned from a large amoun-
t of unlabeled data have been shown to be
able to capture the meaningful semantic reg-
ularities of words (Bengio et al., 2003; Er-
han et al., 2010). This paper uses the learned
word embeddings as the source of basic fea-
tures. Specifically, we use the Skip-gram mod-
el (Mikolov et al., 2013) to learn word embed-
dings on the NYT corpus3.

3.1.2 Entity Type Embedding Table

The ACE 2005 corpus annotated not only
events but also entities for each given sentence.
Following existing work (Li et al., 2013; Chen
et al., 2015; Nguyen and Grishman, 2015), we
exploit the annotated entity information in our
ED system. We randomly initialize embedding
vector for each entity type (including the NA
type) and update it in training procedure.

3.1.3 Representation Learning

In this subsection, we illustrate our proposed
approach to learn representations of both con-
textual words and entities, which serve as in-
puts to the following event detector compo-
nent. Recall that, we use the matrix Cw and
Ce to denote contextual words and contextual
entities, respectively.

As illustrated in figure 1, the CRL compo-
nent needs three inputs: the current candidate
trigger word w, the contextual words Cw and
the contextual entities Ce. Then, two atten-
tion vectors, which reflect different aspects of
the context, are calculated in the next step.
The contextual word attention vector αw
is computed based on the current word w and
its contextual words Cw. We firstly transform
each word wk (including w and every word in
Cw) into a hidden representation wk by the
following equation:

wk = f(wk � Ww) (1)

where f(·) is a non-linear function such as the
hyperbolic tangent, and Ww is the transforma-
tion matrix. Then, we use the hidden represen-
tations to compute the attention value for each

3https://catalog.ldc.upenn.edu/LDC2008T19

word in Cw:

αkw =
exp(w � wTk )∑
i exp(w � wTi )

(2)

The contextual entity attention vector
αe is calculated with a similar method to αw.

αke =
exp(we � eTk )∑
i exp(we � eTi )

(3)

Note that, we do not use the entity informa-
tion of the current candidate token to compute
the attention vector. The reason is that only
a small percentage of true event triggers are
entities4. Therefore, the entity type of a can-
didate trigger is meaningless for ED. Instead,
we use we, which is calculated by transform-
ing w from the word space into the entity type
space, as the attention source.

We combine αw and αe to obtain the final
attention vector, α = αw+αe. Finally, the con-
textual words representation cw and the con-
textual entities representation ce are formed
by weighted sum of Cw and Ce, respectively:

cw = Cwα
T (4)

ce = Ceα
T (5)

3.2 Event Detector

As illustrated in figure 1, we employ a three-
layer (an input layer, a hidden layer and a soft-
max output layer) Artificial Neural Networks
(ANNs) (Hagan et al., 1996) to model the ED
task, which has been demonstrated very effec-
tive for event detection by Liu et al. (2016a).

3.2.1 Basic ED Model

Given a sentence, as illustrated in figure 1, we
concatenate the embedding vectors of the con-
text (including contextual words and entities)
and the current candidate trigger to serve as
the input to ED model. Then, for a given in-
put sample x, ANN with parameter θ outputs
a vector O, where the i-th value oi of O is
the confident score for classifying x to the i-th
event type. To obtain the conditional proba-
bility p(i|x, θ), we apply a softmax operation
over all event types:

p(i|x, θ) = e
oi

∑m
k=1 e

ok
(6)

4Only 10% of triggers in ACE 2005 are entities.

1792



Given all of our (suppose T) training instances
(x(i); y(i)), we can then define the negative log-
likelihood loss function:

J(θ) = −
T∑

i=1

log p(y(i)|x(i), θ) (7)

We train the model by using a simple opti-
mization technique called stochastic gradient
descent (SGD) over shuffled mini-batches with
the Adadelta rule (Zeiler, 2012). Regulariza-
tion is implemented by a dropout (Kim, 2014;
Hinton et al., 2012) and L2 norm.

3.2.2 Supervised Attention

In this subsection, we introduce supervised at-
tention to explicitly use annotated argument
information to improve ED. Our basic idea is
simple: argument words should acquire more
attention than other words. To achieve this
goal, we first construct vectors using annotat-
ed arguments as the gold attentions. Then, we
employ them as supervision to train the atten-
tion mechanism.

Constructing Gold Attention Vectors

Our goal is to encourage argument words to
obtain more attention than other words. To
achieve this goal, we propose two strategies to
construct gold attention vectors:

S1: only pay attention to argument
words. That is, all argument words in the giv-
en context obtain the same attention, whereas
other words get no attention. For candidates
without any annotated arguments in context
(such as negative samples), we force all entities
to average the whole attention. Figure 2 illus-
trates the details, where α∗ is the final gold
attention vector.

Figure 2: An example of S1 to construct gold
attention vector. The word fired is the trigger
candidate, and underline words are arguments
of fired annotated in the corpus.

S2: pay attention to both arguments
and the words around them. The assump-
tion is that, not only arguments are important

to ED, the words around them are also help-
ful. And the nearer a word is to arguments,
the more attention it should obtain. Inspired
by Mi et al. (2016), we use a gaussian distri-
bution g(·) to model the attention distribution
of words around arguments. In detail, given an
instance, we first obtain the raw attention vec-
tor α in the same manner as S1 (see figure 2).
Then, we create a new vector α

′
with all points

initialized with zero, and for each αi = 1, we
update α

′
by the following algorithm:

Algorithm 1: Updating α
′

for k ∈ {−w, ..., 0, ..., w} do
α

′
i+k = α

′
i+k + g(|k|, µ, σ)

end

where w is the window size of the attention
mechanism and µ, σ are hyper-parameters of
the gaussian distribution. Finally, we normal-
ize α

′
to obtain the target attention vector α∗.

Similar with S1, we treat all entities in the
context as arguments if the current candidate
does not has any annotated arguments (such
as netative samples).

Jointly Training ED and Attention

Given the gold attention α∗ (see subsection
3.2.2) and the machine attention α produced
by our model (see subsection 3.1.3), we em-
ploy the square error as the loss function of
attentions:

D(θ) =
T∑

i=1

n∑

j=1

(α∗ij − αij)2 (8)

Combining equation 7 and equation 8, we de-
fine the joint loss function of our proposed
model as follows:

J
′
(θ) = J(θ) + λD(θ) (9)

where λ is a hyper-parameter for trade-off be-
tween J and D. Similar to basic ED model, we
minimize the loss function J

′
(θ) by using SGD

over shuffled mini-batches with the Adadelta
update rule.

4 Experiments

4.1 Dataset and Experimental Setup

Dataset

We conducted experiments on ACE 2005
dataset. For the purpose of comparison, we fol-

1793



lowed the evaluation of (Li et al., 2013; Chen
et al., 2015; Liu et al., 2016b): randomly se-
lected 30 articles from different genres as the
development set, and subsequently conducted
a blind test on a separate set of 40 ACE 2005
newswire documents. We used the remaining
529 articles as our training set.

Hyper-parameter Setting

Hyper-parameters are tuned on the develop-
ment dataset. We set the dimension of word
embeddings to 200, the dimension of entity
type embeddings to 50, the size of hidden lay-
er to 300, the output size of word transfor-
mation matrix Ww in equation 1 to 200, the
batch size to 100, the hyper-parameter for the
L2 norm to 10

−6 and the dropout rate to 0.6.
In addition, we use the standard normal dis-
tribution to model attention distributions of
words around arguments, which means that
µ = 0.0, σ = 1.0, and the window size is set to
3 (see Subsection 3.2.2). The hyper-parameter
λ in equation 9 is various for different atten-
tion strategies, we will give its setting in the
next section.

4.2 Correctness of Our Assumption

In this section, we conduct experiments on
ACE 2005 corpus to demonstrate the correct-
ness of our assumption that argument infor-
mation is crucial to ED. To achieve this goal,
we design a series of systems for comparison.

ANN is the basic event detection model, in
which the hyper-parameter λ is set to 0. This
system does not employ argument information
and computes attentions without supervision
(see Subsection 3.1.3).

ANN-ENT assigns λ with 0, too. The dif-
ference is that it constructs the attention vec-
tor α by forcing all entities in the context to
average the attention instead of computing it
in the manner introduced in Subsection 3.1.3.
Since all arguments are entities, this system is
designed to investigate the effects of entities.

ANN-Gold1 uses the gold attentions con-
structed by strategy S1 in both training and
testing procedure.

ANN-Gold2 is akin to ANN-Gold1, but
uses the second strategy to construct its gold
attentions.

Note that, in order to avoid the interference
of attention mechanisms, the last two systems

are designed to use argument information (via
gold attentions) in both training and testing
procedure. Thus both ANN-Gold1 and ANN-
Gold2 assign λ with 0.

Methods P R F1
ANN 69.9 60.8 65.0

ANN-ENT 79.4 60.7 68.8

ANN-Gold1† 81.9 65.1 72.5
ANN-Gold2† 81.4 66.9 73.4

Table 2: Experimental results on ACE 2005
corpus. † designates the systems that employ
argument information.

Table 2 compares these systems on ACE
2005 corpus. From the table, we observe that
systems with argument information (the last
two systems) significantly outperform system-
s without argument information (the first t-
wo systems), which demonstrates that argu-
ment information is very useful for this task.
Moreover, since all arguments are entities, for
preciseness we also investigate that whether
ANN-Gold1/2 on earth benefits from entities
or arguments. Compared with ANN-ENT (re-
vising that this system only uses entity infor-
mation), ANN-Gold1/2 performs much bet-
ter, which illustrates that entity information
is not enough and further demonstrates that
argument information is necessary for ED.

4.3 Results on ACE 2005 Corpus

In this section, we conduct experiments on
ACE 2005 corpus to demonstrate the effective-
ness of the proposed approach. Firstly, we in-
troduce systems implemented in this work.

ANN-S1 uses gold attentions constructed
by strategy S1 as supervision to learn atten-
tion. In our experiments, λ is set to 1.0.

ANN-S2 is akin to ANN-S1, but use strat-
egy S2 to construct gold attentions and the
hyper-parameter λ is set to 5.0.

These two systems both employ supervised
attention mechanisms. For comparison, we use
an unsupervised-attention system ANN as our
baseline, which is introduced in Subsection 4.2.
In addition, we select the following state-of-
the-art methods for comparison.

1). Li’s joint model (Li et al., 2013) extracts
events based on structure prediction. It is the
best structure-based system.

1794



Methods P R F1
Li’s joint model (2013) 73.7 62.3 67.5

Liu’s PSL (2016) 75.3 64.4 69.4

Liu’s FN-Based (2016) 77.6 65.2 70.7

Ngyuen’s joint (2016) 66.0 73.0 69.3

Skip-CNN (2016) N/A 71.3

ANN 69.9 60.8 65.0

ANN-S1† 81.4 62.4 70.8
ANN-S2† 78.0 66.3 71.7

Table 3: Experimental results on ACE 2005.
The first group illustrates the performances of
state-of-the-art approaches. The second group
illustrates the performances of the proposed
approach. † designates the systems that em-
ploy arguments information.

2). Liu’s PSL (Liu et al., 2016b) employs
both latent local and global information for
event detection. It is the best-reported feature-
based system.

3). Liu’s FN-Based approach (Liu et al.,
2016a) leverages the annotated corpus of
FrameNet to alleviate data sparseness problem
of ED based on the observation that frames in
FN are analogous to events in ACE.

4). Ngyen’s joint model (Nguyen et al., 2016)
employs a bi-directional RNN to jointly ex-
tract event triggers and arguments. It is the
best-reported representation-based joint ap-
proach proposed on this task.

5). Skip-CNN (Nguyen and Grishman,
2016) introduces the non-consecutive convo-
lution to capture non-consecutive k-grams
for event detection. It is the best reported
representation-based approach on this task.

Table 3 presents the experimental results on
ACE 2005 corpus. From the table, we make the
following observations:

1). ANN performs unexpectedly poorly,
which indicates that unsupervised-attention
mechanisms do not work well for ED. We be-
lieve the reason is that the training data of
ACE 2005 corpus is insufficient to train a pre-
cise attention in an unsupervised manner, con-
sidering that data sparseness is an important
issue of ED (Zhu et al., 2014; Liu et al., 2016a).

2). With argument information employed
via supervised attention mechanisms, both
ANN-S1 and ANN-S2 outperform ANN with
remarkable gains, which illustrates the effec-

tiveness of the proposed approach.

3). ANN-S2 outperforms ANN-S1, but the
latter achieves higher precision. It is not d-
ifficult to understand. On the one hand, s-
trategy S1 only focuses on argument words,
which provides accurate information to iden-
tify event type, thus ANN-S1 could achieve
higher precision. On the other hand, S2 focus-
es on both arguments and words around them,
which provides more general but noised clues.
Thus, ANN-S2 achieves higher recall with a
little loss of precision.

4). Compared with state-of-the-art ap-
proaches, our method ANN-S2 achieves the
best performance. We also perform a t-test
(p 6 0.05), which indicates that our method
significantly outperforms all of the compared
methods. Furthermore, another noticeable ad-
vantage of our approach is that it achieves
much higher precision than state-of-the-arts.

4.4 Augmentation with FrameNet

Recently, Liu et al. (2016a) used events auto-
matically detected from FN as extra training
data to alleviate the data-sparseness problem
for event detection. To further demonstrate
the effectiveness of the proposed approach, we
also use the events from FN to augment the
performance of our approach.

In this work, we use the events published
by Liu et al. (2016a)5 as extra training data.
However, their data can not be used in the
proposed approach without further processing,
because it lacks of both argument and entity
information. Figure 3 shows several examples
of this data.

Figure 3: Examples of events detected from
FrameNet (published by Liu et al. (2016a)).

Processing of Events from FN

Liu et al. (2016a) detected events from
FrameNet based on the observation that
frames in FN are analogous to events in ACE

5https://github.com/subacl/acl16

1795



(lexical unit of a frame ↔ trigger of an even-
t, frame elements of a frame ↔ arguments of
an event). All events they published are also
frames in FN. Thus, we treat frame elements
annotated in FN corpus as event arguments.
Since frames generally contain more frame el-
ements than events, we only use core6 elements
in this work. Moreover, to obtain entity infor-
mation, we use RPI Joint Information Extrac-
tion System7 (Li et al., 2013, 2014; Li and Ji,
2014) to label ACE entity mentions.

Experimental Results

We use the events from FN as extra train-
ing data and keep the development and test
datasets unchanged.Table 4 presents the ex-
perimental results.

Methods P R F1
ANN 69.9 60.8 65.0

ANN-S1 81.4 62.4 70.8

ANN-S2 78.0 66.3 71.7

ANN +FrameNet 72.5 61.7 66.7

ANN-S1 +FrameNet 80.1 63.6 70.9

ANN-S2 +FrameNet 76.8 67.5 71.9

Table 4: Experimental results on ACE 2005
corpus. “+FrameNet” designates the systems
that are augmented by events from FrameNet.

From the results, we observe that:

1). With extra training data, ANN achieves
significant improvements on F1 measure (66.7
vs. 65.0). This result, to some extent, demon-
strates the correctness of our assumption that
the data sparseness problem is the reason that
causes unsupervised attention mechanisms to
be ineffective to ED.

2). Augmented with external data, both
ANN-S1 and ANN-S2 achieve higher recall
with a little loss of precision. This is to be ex-
pected. On the one hand, more positive train-
ing samples consequently make higher recal-
l. On the other hand, the extra event sam-
ples are automatically extracted from FN, thus
false-positive samples are inevitable to be in-
volved, which may result in hurting the preci-
sion. Anyhow, with events from FN, our ap-
proach achieves higher F1 score.

6FrameNet classifies frame elements into three
groups: core, peripheral and extra-thematic.

7http://nlp.cs.rpi.edu/software/

5 Related Work

Event detection is an increasingly hot and
challenging research topic in NLP. Generally,
existing approaches could roughly be divided
into two groups.

The first kind of approach tackled this
task under the supervision of annotated trig-
gers and entities, but totally ignored anno-
tated arguments. The majority of existing
work followed this paradigm, which includes
feature-based methods and representation-
based methods. Feature-based methods ex-
ploited a diverse set of strategies to convert
classification clues (i.e., POS tags, dependen-
cy relations) into feature vectors (Ahn, 2006;
Ji and Grishman, 2008; Patwardhan and Rilof-
f, 2009; Gupta and Ji, 2009; Liao and Gr-
ishman, 2010; Hong et al., 2011; Liu et al.,
2016b). Representation-based methods typi-
cally represent candidate event mentions by
embeddings and feed them into neural net-
works (Chen et al., 2015; Nguyen and Grish-
man, 2015; Liu et al., 2016a; Nguyen and Gr-
ishman, 2016).

The second kind of approach, on the con-
trast, tackled event detection and argument
extraction simultaneously, which is called joint
approach (Riedel et al., 2009; Poon and Van-
derwende, 2010; Li et al., 2013, 2014; Venu-
gopal et al., 2014; Nguyen et al., 2016). Join-
t approach is proposed to capture internal
and external dependencies of events, includ-
ing trigger-trigger, argument-argument and
trigger-argument dependencies. Theoretically,
both ED and AE are expected to benefit from
joint methods because triggers and arguments
are jointly considered. However, in practice,
existing joint methods usually only make re-
markable improvements to AE, but insignif-
icant to ED. Different from them, this work
investigates the exploitation of argument in-
formation to improve the performance of ED.

6 Conclusions

In this work, we propose a novel approach to
model argument information explicitly for ED
via supervised attention mechanisms. Besides,
we also investigate two strategies to construc-
t gold attentions using the annotated argu-
ments. To demonstrate the effectiveness of the
proposed method, we systematically conduc-

1796



t a series of experiments on the widely used
benchmark dataset ACE 2005. Moreover, we
also use events from FN to augment the per-
formance of the proposed approach. Experi-
mental results show that our approach outper-
forms state-of-the-art methods, which demon-
strates that the proposed approach is effective
for event detection.

Acknowledgments

This work was supported by the Natural Sci-
ence Foundation of China (No. 61533018) and
the National Basic Research Program of China
(No. 2014CB340503). And this research work
was also supported by Google through focused
research awards program.

References

David Ahn. 2006. Proceedings of the
workshop on annotating and reasoning
about time and events. Association for
Computational Linguistics, pages 1–8.
http://aclweb.org/anthology/W06-0901.

Yoshua Bengio, Réjean Ducharme, Pascal Vincen-
t, and Christian Janvin. 2003. A neural proba-
bilistic language model. The Journal of Machine
Learning Research 3:1137–1155.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction vi-
a dynamic multi-pooling convolutional neural
networks. In Proceedings of the 53rd An-
nual Meeting of the Association for Compu-
tational Linguistics and the 7th Internation-
al Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers). Association
for Computational Linguistics, pages 167–176.
https://doi.org/10.3115/v1/P15-1017.

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and
Samy Bengio. 2010. Why does unsupervised
pre-training help deep learning? The Journal
of Machine Learning Research 11:625–660.

Collin F. Baker, Charles J. Fillmore, and John
B. Lowe. 1998. The berkeley framenet project.
In COLING 1998 Volume 1: The 17th Interna-
tional Conference on Computational Linguistic-
s. http://aclweb.org/anthology/C98-1013.

Prashant Gupta and Heng Ji. 2009. Predict-
ing unknown time arguments based on cross-
event propagation. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers. Associ-
ation for Computational Linguistics, pages 369–
372. http://aclweb.org/anthology/P09-2093.

Martin T Hagan, Howard B Demuth, Mark H
Beale, et al. 1996. Neural network design. P-
ws Pub. Boston.

Geoffrey E Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R
Salakhutdinov. 2012. Improving neural net-
works by preventing co-adaptation of feature de-
tectors. arXiv preprint arXiv:1207.0580 http-
s://arxiv.org/abs/1207.0580.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extrac-
tion. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistic-
s: Human Language Technologies. Association
for Computational Linguistics, pages 1127–1136.
http://aclweb.org/anthology/P11-1113.

Heng Ji and Ralph Grishman. 2008. Refining even-
t extraction through cross-document inference.
In Proceedings of ACL-08: HLT . Association
for Computational Linguistics, pages 254–262.
http://aclweb.org/anthology/P08-1030.

Yoon Kim. 2014. Convolutional neural network-
s for sentence classification. In Proceedings of
the 2014 Conference on Empirical Methods in
Natural Language Processing . pages 1746–1751.
http://www.anthology.aclweb.org/D14-1181.

Qi Li and Heng Ji. 2014. Incremental join-
t extraction of entity mentions and relation-
s. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 402–412.
https://doi.org/10.3115/v1/P14-1038.

Qi Li, Heng Ji, Yu HONG, and Sujian Li. 2014.
Constructing information networks using one s-
ingle model. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for
Computational Linguistics, pages 1846–1851.
https://doi.org/10.3115/v1/D14-1198.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with glob-
al features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Associa-
tion for Computational Linguistics, pages 73–82.
http://aclweb.org/anthology/P13-1008.

Shasha Liao and Ralph Grishman. 2010. Us-
ing document level cross-event inference to
improve event extraction. In Proceedings
of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics. Association
for Computational Linguistics, pages 789–797.
http://aclweb.org/anthology/P10-1081.

1797



Shulin Liu, Yubo Chen, Shizhu He, Kang Liu,
and Jun Zhao. 2016a. Leveraging framenet
to improve automatic event detection. In
Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistic-
s (Volume 1: Long Papers). Association for
Computational Linguistics, pages 2134–2143.
https://doi.org/10.18653/v1/P16-1201.

Shulin Liu, Kang Liu, Shizhu He, and Jun
Zhao. 2016b. A probabilistic soft logic based
approach to exploiting latent and global
information in event classification. In Pro-
ceedings of the thirtieth AAAI Conference
on Artificail Intelligence. pages 2993–2999.
http://www.aaai.org/ocs/index.php/AAAI
/AAAI16/paper/view/11990/12052.

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah.
2016. Supervised attentions for neural machine
translation. arXiv preprint arXiv:1608.00112
https://arxiv.org/abs/1608.00112.

Tomas Mikolov, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013. Efficient estima-
tion of word representations in vector s-
pace. arXiv preprint arXiv:1301.3781 http-
s://arxiv.org/abs/1301.3781.

Huu Thien Nguyen, Kyunghyun Cho, and Ralph
Grishman. 2016. Joint event extraction via re-
current neural networks. In Proceedings of the
2016 Conference of the North American Chapter
of the Association for Computational Linguistic-
s: Human Language Technologies. Association
for Computational Linguistics, pages 300–309.
https://doi.org/10.18653/v1/N16-1034.

Huu Thien Nguyen and Ralph Grishman. 2015.
Event detection and domain adaptation with
convolutional neural networks. In Proceedings
of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language
Processing (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 365–
371. https://doi.org/10.3115/v1/P15-2060.

Huu Thien Nguyen and Ralph Grishman. 2016.
Modeling skip-grams for event detection with
convolutional neural networks. In Proceedings
of the 2016 Conference on Empirical Method-
s in Natural Language Processing . Association
for Computational Linguistics, pages 886–891.
http://aclweb.org/anthology/D16-1085.

Siddharth Patwardhan and Ellen Riloff. 2009. A
unified model of phrasal and sentential evi-
dence for information extraction. In Proceedings
of the 2009 Conference on Empirical Method-
s in Natural Language Processing . Association
for Computational Linguistics, pages 151–160.
http://aclweb.org/anthology/D09-1016.

Hoifung Poon and Lucy Vanderwende. 2010.
Joint inference for knowledge extraction from

biomedical literature. In Human Language
Technologies: The 2010 Annual Conference of
the North American Chapter of the Associa-
tion for Computational Linguistics. Association
for Computational Linguistics, pages 813–821.
http://aclweb.org/anthology/N10-1123.

Sebastian Riedel, Hong-Woo Chun, Toshihisa Tak-
agi, and Jun’ichi Tsujii. 2009. Proceed-
ings of the bionlp 2009 workshop compan-
ion volume for shared task. Association
for Computational Linguistics, pages 41–49.
http://aclweb.org/anthology/W09-1406.

Deepak Venugopal, Chen Chen, Vibhav Gogate,
and Vincent Ng. 2014. Relieving the
computational bottleneck: Joint inference for
event extraction with high-dimensional fea-
tures. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural
Language Processing (EMNLP). Association
for Computational Linguistics, pages 831–843.
https://doi.org/10.3115/v1/D14-1090.

Matthew D Zeiler. 2012. Adadelta: An adap-
tive learning rate method. arXiv preprint arX-
iv:1212.5701 https://arxiv.org/abs/1212.5701.

Zhu Zhu, Shoushan Li, Guodong Zhou, and
Rui Xia. 2014. Bilingual event extraction:
a case study on trigger type determination.
In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguis-
tics (Volume 2: Short Papers). Association
for Computational Linguistics, pages 842–847.
https://doi.org/10.3115/v1/P14-2136.

1798


	Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms

