




































Dependency Structure for Incremental Parsing of Japanese and Its Application

Tomohiro Ohno
Information Technology Center,

Nagoya University
Nagoya, Japan

ohno@nagoya-u.jp

Shigeki Matsubara
Graduate Schoool of Information Science,

Nagoya University
Nagoya, Japan

matubara@nagoya-u.jp

Abstract

Previous researches on incremental dependency
parsing have not discussed how a parser should
output the information about dependency rela-
tion where the modified word has not been in-
putted yet. This paper proposes a dependency
structure which a Japanese incremental depen-
dency parser should produce in the middle of an
input sentence. The dependency structure also
expresses the fact that a word depends on a word
which the parser would receive later. In addi-
tion, we present a method for incremental de-
pendency parsing to produce our proposed de-
pendency structure. As the result of an experi-
ment on incremental parsing, we confirmed the
feasibility of our incremental dependency pars-
ing. Furthermore, this paper shows its applica-
tion in the simultaneous linefeed insertion for
real-time generation of more readable captions,
and then describes the effectiveness of our pro-
posed dependency structure.

1 Introduction

In applications such as simultaneous machine inter-
pretation, spoken dialogue and real-time captioning,
it is necessary to execute processing simultaneously
with speech input, and thus, the questions of what
syntactic information is acquirable and when the in-
formation can be acquired become issues. However,
previous researches on incremental dependency pars-
ing (Kato et al., 2005; Johansson and Nugues, 2007;
Nivre, 2008) have not discussed how a parser should
output the information about dependency relation
where the modified word has not been inputted.

This paper proposes a dependency structure which
a Japanese incremental dependency parser should pro-
duce in the middle of an input sentence. Our proposed

dependency structure makes a parser clarify the fact
that the modified bunsetsu1 of a bunsetsu will be in-
putted later. This enables a higher layer application
to use the information that the bunsetsu does not de-
pend on any bunsetsus which have been already in-
putted. Especially, in the case of Japanese, since the
modified bunsetsu is always inputted after the mod-
ifier bunsetsu, our proposal is effective for the appli-
cations which execute processing simultaneously with
speech input.

In addition, this paper describes a method for incre-
mental dependency parsing to produce our proposed
dependency structure whenever receiving a bunsetsu.
We conducted an experiment on incremental parsing
and confirmed the feasibility of our method. Further-
more, this paper shows its application to the simul-
taneous linefeed insertion for real-time generation of
more readable captions. As the result of an experi-
ment on linefeed insertion, we confirmed the effec-
tiveness of our proposed dependency structure.

2 Dependency structure for incremental
parsing

Since applications such as simultaneous machine in-
terpretation and real-time captioning need to execute
the process simultaneously with speech input, it is de-
sirable for them to be able to acquire the dependency
information at any time. In our research, incremen-
tal dependency parsing shall output the parsing result
whenever receiving a bunsetsu.

1Bunsetsu is a linguistic unit in Japanese that roughly corre-
sponds to a basic phrase in English. A bunsetsu consists of one
independent word and zero or more ancillary words. A depen-
dency relation in Japanese is a modification relation in which a
modifier bunsetsu depends on a modified bunsetsu. That is, the
modifier bunsetsu and the modified bunsetsu work as modifier and
modifyee, respectively.

91



nihon-ni kaet-te-kite suguwatashi-mo ie-no terebi-de warudokappu-wo mita-wake-desu

“nihon-ni (to Japan)”

“kaet-te-kite (had come back)”

“sugu (soon after)”

“watashi-mo (I also)”

“ie-no (at home)”

dependency structure：

“non-inputted bunsetsu”

“non-inputted bunsetsu”

“non-inputted bunsetsu”

“terebi-de (on TV)”

(to Japan) (had come 
back)

(soon
after)

(I also) (at  
home)

(on TV) (the world cup) (watched)

<
“kaet-te-kite (had come back)”<

<
<
<

>,
>,

>,
>,

>

(I also watched the world cup on TV at home soon after I had come back to Japan.)

Figure 1: Example of a dependency structure outputted in the middle of an input sentence

Next, we discuss the output contents of incre-
mental dependency parsing. When applications use
incremental dependency parsing, the information
about whether or not the dependency structure
of a sequence of busnetsus is closed, that is, the
information about a syntactically sufficient unit is
also useful. In fact, when identifying the timing of
the start of interpreting in the simultaneous machine
interpretation (Ryu et al., 2006; Fügen et al., 2007;
Paulik and Waibel, 2010), the timing of insert-
ing back-channel feedbacks (Fujie et al., 2004;
Kamiya et al., 2010), and the appropriate position
of a linefeed in captioning (Ohno et al., 2009), the
information about a syntactically sufficient unit is
used as an important clue.

In our research, an incremental dependency parsing
shall clearly output the fact that a bunsetsu depends
on a bunsetsu which will be later inputted. If it be-
comes clear that the modified bunsetsu of a bunsetsu
has not been inputted yet, the higher layer applications
can identify the syntactically sufficient units in the in-
putted sequence of bunsetsus and use the information
effectively.

In what follows, we describe the dependency struc-
ture which incremental dependency parsing in our re-
search outputs in the middle of an input sentence.
When a sentence S composed of a sequence of bun-
setsus b1 · · · bn is parsed2, we define the dependency
structure Dx which is outputted at the time that a bun-
setsu bx(1 ≤ x ≤ n) is inputted as follows:

Dx =
{
⟨b1→dep(b1)⟩, · · · , ⟨bx−1→dep(bx−1)⟩

}


Here, dep(by) ∈

{
{by+1, · · · , bx, bover}(if bx ̸= bn)
{by+1, · · · , bx} (otherwise)

}

(1≤y≤x−1)




2In this research, we assume that sentence boundaries are de-
tected as its preprocessing.

where ⟨by → dep(by)⟩ means the dependency rela-
tion where the modifier bunsetsu is by and where the
modified bunsetsu is dep(by). dep(by) is a member
of a set which contains not only by+1, · · · , bx, which
are inputted after the bunsetsu by, but also bover.
dep(by) = bover means that the modified bunsetsu of
by is not any of by+1,· · ·, bx but a bunsetsu which
will be inputted later. In addition, we assume the
following three Japanese syntactic constraints:

• No dependency is directed from right to left.
• Dependencies don’t cross each other.
• Each bunsetsu, except the final one in a sentence,

depends on only one bunsetsu.
Figure 1 shows an example of a dependency struc-

ture outputted in the middle of an input sentence.
This example shows the dependency structure which
is outputted right after the bunsetsu “terebi-de (on
TV)” was inputted when a parser parses the sentence
“watashi-mo nihon-ni kaet-te-kite sugu ie-no terebi-
de warudokappu-wo mita-wake-desu (I also watched
the world cup on TV at home soon after I had come
back to Japan.)”

3 Incremental dependency parsing

This section describes a method to produce the de-
pendency structure proposed in Section 2 whenever
receiving a bunsetsu. This method is implemented by
improving the dependency parsing method proposed
by Uchimoto et al. (2000).

Generally, stochastic dependency parsing methods
identify the dependency structure which maximizes
the conditional probability P (D|S), which means the
probability that the dependency structure for a sen-
tence S is D. In usual methods, a relationship be-
tween two bunsetsus is tagged with a “1” or “0” to
indicate whether or not there is a dependency rela-
tion between the bunsetsus, respectively. On the other

92



hand, in the Uchimoto et al.’s method, a relationship
between two bunsetsus is tagged with a “between,”
“depend,” or “beyond” to indicate the following three
cases, respectively. The anterior bunsetsu can depend
on (1) a bunsetsu between the two, (2) the posterior
bunsetsu, or (3) a bunsetsu beyond the posterior one.
Then, the dependency probability of two bunsetsus is
estimated by using the product of the probabilities of
the relationship between the left bunsetsu and those to
its right in a sentence.

In our research, we thought that by adapting the ba-
sic idea in the Uchimoto et al.’s method, we might
be able to calculate the probability that a bunsetsu
does not depend on any bunsetsus which have been
already inputted, that is, the probability that a bun-
setsu depends on a non-inputted bunsetsu (bover). As
a difference from the Uchimoto et al.’s method, our
method identifies the dependency structure Dx which
should be outputted when the sequence of bunsetsus
Bx = b1, · · · , bx(1 ≤ x ≤ n), which is a subse-
quence of a sentence S = b1 · · · bn, is inputted. Our
method outputs the dependency structure which max-
imizes the probability P (Dx|Bx) as the most appro-
priate dependency structure for the sequence of bun-
setsus Bx.

Here, Dx is described as an ordered
set {d1, · · · , dx−1} of dependency relations
di (1 ≤ i ≤ x − 1), where the modifier bun-
setsu is a bunsetsu bi. In addition, di is described as
di = {di,i+1, · · · , di,x, di,x+1} (1 ≤ i ≤ x−1). di,i+j
is a flag which indicates the relationship between
bunsetsus bi and bi+j , and takes the following value:

di,i+j =





0 (1 ≤ j < dist(i))
1 (j = dist(i))
2 (dist(i) < j ≤ n − i)





where dist(i) is defined as dist(i) = l−i when a bun-
setsu bi depends on a bunsetsu bl(i < l ≤ x+1 ≤ n).
Note that our method adds a flag di,x+1, which indi-
cates the relationship between bi and a non-inputted
bunsetsu bx+1, to the elements of di as a dummy.

Using the above-mentioned notation, our method
calculates P (Dx|Bx) as follows3:

P (Dx|Bx)2 =
x−1∏

i=1

P (di|Bx)
3 Uchimoto et al. (2000a) discussed the rationale and assump-

tions behind the form of this model.

１ ２ ３

dpnd

0.4

btwn

0.1

dpnd

0.3

bynd

0.6

bynd

0.6

bynd

0.6

btwn

1/2

dpnd

1/2

４

btwn

1/2

１ ２ ３ ４

１ ２ ３ ４

× × = 0.141

× × = 0.300

× × = 0.424

inputted

=→ )|(
321

BbbP

non-inputted

=→ )|(
331

BbbP

=→=→ )|()|(
31341

BbbPBbbP
over

Figure 2: Calculation of dependency probability

=
x−1∏

i=1

P (di,i+1, · · · , di,x+1|Bx)

=
x−1∏

i=1

(dist(i)−1∏

j=1

P (di,i+j = 0|Bx)

×P (di,i+dist(i) = 1|Bx)

×
x−i+1∏

j=dist(i)+1

P (di,i+j = 2|Bx)
)

where the value of dist(i) is decided based on the Dx.
Here, we use the following value as P (di,i+j |Bx),
which means the probability of the relationship (“be-
yond,” “depend,” or “between”) between two bunset-
sus.

• When 1 ≤ j ≤ x − i: P (di,i+j |Bx) = the value
which is estimated using the maximum entropy
method in the same way4 as the Uchimoto et al.’s
method.

• When j = x − i + 1: P (di,i+j = 0|Bx) = 0,
P (di,i+j = 1|Bx) = P (di,i+j = 2|Bx) = 1/2

The reasons that we decided the second itemization
are as follows: The relationship di,x+1 between a
bunsetsu bi and a non-inputted bunsetsu bx+1 does
not become the relationship “beyond,” but becomes
either “depend” or “between.” Moreover, we as-
sume that the probability that the relationship be-
comes “depend” is equal to that of “between” because
the probabilities are unknown. This assumption en-
ables argmaxDxP (Dx|Bx) to be calculated with no
relation to the probabilities between bi and bx+1.

Finally, if there exists a ⟨bi → bx+1⟩ in the de-
pendency structure Dx which maximizes P (Dx|Bx),

4 However, the features which we used in the maximum en-
tropy method are the same as those used by Ohno et al. (2007).

93



our method outputs the dependency structure which is
made by replacing every ⟨bi → bx+1⟩ in the Dx with
⟨bi → bover⟩.

Figure 2 shows how to calculate the dependency
probability P (d1|B3) when a bunsetsu b3 in a sen-
tence composed of b1, · · · , b5 is inputted. Note that we
consider P (⟨b1 → b4⟩|B3) = P (⟨b1 → bover⟩|B3).

4 Experiment

To evaluate the feasibility of incrementally producing
the dependency structure proposed in Section 2, we
conducted an experiment on dependency parsing.

4.1 Outline of experiment
As the experimental data, we used the transcribed
data of Japanese discourse speech in the Simultane-
ous Interpretation Database (Matsubara et al., 2002).
All the data had been manually annotated with infor-
mation on morphological analysis, bunsetsu boundary
detection, clause boundary detection and dependency
analysis (Ohno et al., 2009).

We performed a cross-validation experiment by us-
ing 16 discourses. That is, we repeated the experi-
ment, in which we used one discourse from among 16
discourses as the test data and the others as the learn-
ing data, 16 times. However, since we used 2 dis-
courses among 16 discourses as the preliminary anal-
ysis data, we evaluated the experimental results for
the remaining 14 discourses (1,714 sentences, 20,707
bunsetsus). Here, as the morphological information,
bunsetsu boundary information, and clause boundary
information in the input of dependency parsing, we
used one which had been manually annotated. In ad-
dition, we used the maximum entropy method tool
(Zhang, 2013) with the default options except “-i (it-
eration) 2000.”

4.2 Evaluation index
Our method is designed to output the dependency
structure defined in Section 2 whenever a bunsetsu is
inputted. Therefore, we introduced the following new
evaluation index to evaluate the accuracy of incremen-
tal dependency parsing:

acc. =

∑N
i

∑ni
j=1 DepNum(Match(D

i
j , G

i
j))∑N

i

∑ni
j=1 DepNum(D

i
j)

where Dij and G
i
j indicate the dependency struc-

ture which a parser outputs and the correct one which

１ ２ ３ ４ ５

(0/0)１

１ ２ (0/1)

１ ２ ３ (1/2)

１ ２ ３ ４ (3/3)

１ ２ ３ ４ ５ (4/4)

DepNum(Match(D, G))/ DepNum(D)    

Acc.

=(8/10)

=iG5
•correct data

•results of parsing
When     is inputted： =iD1

=iD2

=iD3

=iD4

=iD5

1b

When      is inputted：2b

When      is inputted：3b

When      is inputted：4b

When      is inputted：5b

Figure 3: Calculation of accuracy

recall precision f-measure
non- 70.4% 75.0% 72.6%

inputted (22,811/ (22,811/
32,389) 30,413)

inputted 74.8% 73.8% 74.3%
(102,778/ (102,778/
137,376) 139,352)

Table 1: Recalls and precisions of our method

a parser should output respectively when a bunsetsu
bj(1 ≤ j ≤ ni) in a sentence Si(1 ≤ i ≤ N) is in-
putted. In addition, DepNum() is a function which
returns the number of elements of the input set of de-
pendency relations, and Match() is a function which
returns an intersection between the two input sets of
dependency relations.

Figure 3 shows an example of the calculation of
our defined accuracy in case of parsing a sentence
Si composed of 5 bunsetsus. Here, we explain how
to calculate the DepNum(Match(Di3, G

i
3)) when

the 3rd bunsetsu is inputted. From the correct data
of the dependency structure of a whole sentence
Si, Gi3 = {⟨b1 → b2⟩, ⟨b2 → b3⟩} is derived.
On the other hand, the result of parsing is Di3 =
{⟨b1 → b2⟩, ⟨b2 → bover⟩}. From the Gi3 and Di3,
DepNum(Match(Di3, G

i
3)) = 1 is derived.

Furthermore, we obtained the recalls and preci-
sions, separately for the case that the modified bun-
setsu has not been inputted and the case that the mod-
ified bunsetsu has been inputted. The recall and pre-
cision for the former case are respectively defined as
follows:

rec. =

∑N
i

∑ni
j=1 DepNum(Over(Match(D

i
j , G

i
j)))∑N

i

∑ni
j=1 DepNum(Over(G

i
j))

94



prec. =

∑N
i

∑ni
j=1 DepNum(Over(Match(D

i
j , G

i
j)))∑N

i

∑ni
j=1 DepNum(Over(D

i
j))

where Over() is a function which returns a set of de-
pendency relations where the modified bunsetsu has
not been inputted yet from among the input set of
dependency relations. The recall and precision for
the latter case are defined by replacing the Over() in
the above-mentioned definition with the NonOver().
NonOver() is a function which returns a set of depen-
dency relations where the modified bunsetsu has been
inputted from among the input set of dependency re-
lations.

4.3 Experimental results
The accuracy obtained using our method was 74.0%
(125,589/169,765). Only as a guide, we conducted
a parsing experiment by using the Uchimoto et al.’s
method5 and CaboCha(Kudo and Matsumoto, 2002),
which identify the dependency structure after the
whole sentence is inputted. As the result of mea-
suring the conventional dependency accuracy (=∑N

i DepNum(Match(D
i
ni ,G

i
ni))/

∑N
i DepNum(D

i
ni)),

the dependency accuracy of the Uchimoto et al.’s
method and CaboCha were 75.8% (14.391/18,993)
and 78.0% (14,814/18,993) respectively.

Table 1 shows the recalls and precisions of our
method, separately for the case that the modified bun-
setsu has not been inputted and the case that the mod-
ified bunsetsu has been inputted. The f-measure for
the case that the modified bunsetsu has not been in-
putted was not as high as that for the non-inputted
case. However, the precision for the inputted case was
higher than that for the non-inputted case. We con-
firmed the feasibility of outputting our proposed de-
pendency structure whenever a bunsetsu is inputted.

5 Application to sequential linefeed insertion

In this section, we describe application of our incre-
mental dependency parsing described in Section 3 to
sequential linefeed insertion.

5.1 Sequential linefeed insertion
Whenever a bunsetu is inputted, our method deci-
sively judges whether or not a linefeed should be in-
serted between the inputted bunsetsu and the adjacent

5We used the same features in Section 3 when estimating the
probability of the relationship between two bunsetsus.

Clause boundary analysis (CBAP)

Incremental dependency parsing (Section 3)

Judgment of linefeed insertion

Bunsetsus in a sentence on which morphological analysis and bunsetsu

boundary analysis have been performed are inputted one by one.

Whether or not a linefeed should be  inserted between the 

inputted bunsetsu and the adjacent bunsetsu is outputted.

Figure 4: Flow of sequential linefeed insertion

bunsetsu. Here, morphological analysis and bunsetsu
boundary analysis have been already performed for
the input sequence of bunsetsus in a sentence.

Figure 4 shows the flow of our sequential linefeed
insertion. First, our method decides whether there
exists a clause boundary between the inputted bun-
setsu bx and the adjacent bunsetsu bx−1 or not by
using Clause Boundary Annotation Program (CBAP)
(Kashioka and Maruyama, 2004). Next, our method
parses the dependency structure Dx proposed in Sec-
tion 2 by using the incremental dependency parsing
method proposed in Section 3.

Finally, in judgment of linefeed insertion, our
method decisively judges whether a linefeed should
be inserted between the inputted bunsetsu bx and the
adjacent bunsetsu bx−1 by using the maximum en-
tropy method. The maximum entropy method esti-
mates the probability that a linefeed should be in-
serted at a point by considering such information as
morphological analysis results, bunsetsu boundaries,
clause boundaries, and dependency structures. When
the probability is larger than 0.5, our method decides
to insert a linefeed at the point.

The features used in the ME method, which esti-
mates the probability that a linefeed is inserted be-
tween a bunsetsu bx−1 and bx, are as roughly the
same as those for the conventional sentence-based
method (Ohno et al., 2009)6. The difference is that
our method does not use the following three features
among those for the sentence-based method:

• whether or not bx−1 depends on the final bun-
setsu of a clause

• whether or not bx−1 depends on a bunsetsu to
6The sentence-based method decides the most appropriate

linefeed insertion points in the whole of a sentence after the
sentence-end bunsetsu is inputted.

95



recall precision f-measure
our method 79.6% 70.2% 74.6%

(4,375/5,497) (4,375/6,228)
conv. method 73.7% 74.4% 74.0%

(4,052/5,497) (4,052/5,447)

Table 2: Experimental results of linefeed insertion

which the number of characters from the start
of the line is less than or equal to the maximum
number of characters (20 characters)

• whether or not there exists a bunsetsu which de-
pends on the modified bunsetsu of bx−1, among
bunsetsus which are located after bx−1 and to
which the number of characters from the start
of the line is less than or equal to the maximum
number of characters

The values of these features can not be decided until
the modified bunsetsu of bx−1 is inputted if bx−1 does
not depend on bx. Since our sequential linefeed in-
sertion method needs to identify the linefeed insertion
points simultaneously with speech input, our method
does not use these features.

5.2 Experiment on linefeed insertion
To evaluate the application potency of the dependency
structure proposed in Section 2, we conducted an ex-
periment on linefeed insertion using the sequential
linefeed insertion method described in the previous
section.

5.2.1 Experimental outline
We used the same experimental data and performed a
cross-validation experiment in the same way as Sec-
tion 4.1. However, when inputting the test data, we
eliminated the information on clause boundaries, de-
pendency, and linefeeds. In the evaluation, we ob-
tained the recall, precision and f-measure on the line-
feed insertion performance.

For comparison, we also performed linefeed inser-
tion using the conventional sentence-based method
(Ohno et al., 2009). Here, when performing the con-
ventional method, clause boundary information and
dependency information were beforehand provided
using Clause Boundary Annotation Program (CBAP)
and Uchimoto et al.’s dependency parsing method
(Uchimoto et al., 2000b), respectively.

5.2.2 Experimental results on linefeed insertion

Table 2 shows the recalls, precisions and f-measures
of our method and conventional method. The f-
measure for our method was 74.6%, which was
slightly larger than that for the conventional method.
We confirmed that our method can output linefeed
insertion points simultaneously with speech input,
maintaining the approximately-same f-measure of
linefeed insertion as that for the conventional method.

However, we measured the sentence accuracy,
which is the ratio of sentences of which all linefeed
insertion points were correctly detected. The sentence
accuracies of our method and the conventional method
were 35.8% (614/1,714) and 46.2% (792/1,714) re-
spectively. The conventional method decides the most
appropriate linefeed insertion points by considering
the whole sentence while our method judges whether
a linefeed should be inserted or not whenever a bun-
setsu is inputted. This difference is thought to have
caused the result. We will confirm that the readabil-
ity of captions generated by our method does not de-
crease, by conducting not only objective evaluation
based on comparing the linefeed insertion result with
the correct data but also subjective evaluation.

6 Conclusion

This paper proposed the dependency structure which
an incremental dependency parser outputs for an in-
putted bunsetsu sequence. Our proposed dependency
structure makes a parser clarify the fact that the mod-
ified bunsetsu is a bunsetsu which will be inputted
later. In addition, we proposed a method for incre-
mental dependency parsing to output our proposed de-
pendency structure. As the result of an experiment on
incremental parsing, we confirmed the feasibility of
our method. Furthermore, we applied our proposed
incremental dependency parsing to the simultaneous
linefeed insertion, and then we confirmed the applica-
tion potency of our proposed dependency structure.

Acknowledgments

This research was partially supported by the Grant-
in-Aid for Young Scientists (B) (No.25730134) and
Scientific Research (B) (No.22300051) of JSPS.

96



References

Christian Fügen, Alex Waibel, and Muntsin Kolss. 2007.
Simultaneous translation of lectures and speeches. Ma-
chine Translation, 21:209–252. 10.1007/s10590-008-
9047-0.

Shinya Fujie, Kenta Fukushima, and Tetsunori Kobayashi.
2004. A conversation robot with back-channel feedback
function based on linguistic and nonlinguistic informa-
tion. In Procedings of the 2nd International Conference
on Autonomous Robots and Agents (ICARA2004), pages
379–384.

Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL2007), pages 1134–1138.

Yuki Kamiya, Tomohiro Ohno, and Shigeki Matsubara.
2010. Coherent back-channel feedback tagging of in-car
spoken dialogue corpus. In Proceedings of the 11th An-
nual SIGdial Meeting on Discourse and Dialogue (SIG-
DIAL2010), pages 205–208.

Hideki Kashioka and Takehiko Maruyama. 2004. Seg-
mentation of semantic units in Japanese monologues.
In Proceedings of International Conference on Speech
and Language Technology and Oriental COCOSDA
(ICSLT/O-COCOSDA2004), pages 87–92.

Yoshihide Kato, Shigeki Matsubara, Katsuhiko Toyama,
and Yasuyoshi Inagaki. 2005. Incremental dependency
parsing based on headed context-free grammar. Systems
and Computers in Japan, 36:63–77.

Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of the 6th Conference on Natural Language Learn-
ing 2002 (CoNLL 2002), pages 63–69.

Shigeki Matsubara, Akira Takagi, Nobuo Kawaguchi, and
Yasuyoshi Inagaki. 2002. Bilingual spoken mono-
logue corpus for simultaneous machine interpretation
research. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC2002), pages 153–159.

Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguistics,
34:513–553.

Tomohiro Ohno, Masaki Murata, and Shigeki Matsubara.
2009. Linefeed insertion into Japanese spoken mono-
logue for captioning. In Proceedings of Joint Con-
ference of the 47th Annual Meeting of the Associa-
tion for Computational Linguistics and the 4th Interna-

tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP2009), pages 531–539.

Matthias Paulik and Alex Waibel. 2010. Spoken lan-
guage translation from parallel speech audio: simultane-
ous interpretation as SLT training data. In Proceedings
of the 2010 IEEE International Conference on Acous-
tics Speech and Signal Processing (ICASSP2010), pages
5210–5213.

Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi Inagaki.
2006. Simultaneous English-Japanese spoken language
translation based on incremental dependency parsing
and transfer. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pages 683–690.

Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and
Hitoshi Isahara. 2000a. Dependency model using pos-
terior context. Journal of Natural Language Processing,
7(5):3–18. (In Japanese).

Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and
Hitoshi Isahara. 2000b. Dependency model using pos-
terior context. In Proceedings of the 6th International
Workshop on Parsing Technologies (IWPT2000).

Le Zhang. 2013. Maximum entropy mod-
eling toolkit for Python and C++ (online).
http://homepages.inf.ed.ac.uk/lzhang10/

maxent toolkit.html (accessed 2013-08-21).

97


