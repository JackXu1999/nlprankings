A Multi-Hop Attention for Neural Machine Translation

Attention over Heads:

Shohei Iiday, Ryuichiro Kimuray, Hongyi Cuiy, Po-Hsuan Hungy,

Takehito Utsuroy and Masaaki Nagataz

yGraduate School of Systems and Information Engineering, University of Tsukuba, Japan

zNTT Communication Science Laboratories, NTT Corporation, Japan

Abstract

In this paper, we propose a multi-hop attention
for the Transformer. It reﬁnes the attention for
an output symbol by integrating that of each
head, and consists of two hops. The ﬁrst hop
attention is the scaled dot-product attention
which is the same attention mechanism used in
the original Transformer. The second hop at-
tention is a combination of multi-layer percep-
tron (MLP) attention and head gate, which ef-
ﬁciently increases the complexity of the model
by adding dependencies between heads. We
demonstrate that the translation accuracy of
the proposed multi-hop attention outperforms
the baseline Transformer signiﬁcantly, +0.85
BLEU point for the IWSLT-2017 German-to-
English task and +2.58 BLEU point for the
WMT-2017 German-to-English task. We also
ﬁnd that the number of parameters required for
a multi-hop attention is smaller than that for
stacking another self-attention layer and the
proposed model converges signiﬁcantly faster
than the original Transformer.

1 Introduction
Multi-hop attention was ﬁrst proposed in end-to-
end memory networks (Sukhbaatar et al., 2015)
for machine comprehension. In this paper, we de-
ﬁne a hop as a computational step which could
be performed for an output symbol many times.
By “multi-hop attention”, we mean that some
kind of attention is calculated many times for
generating an output symbol. Previous multi-
hop attention can be classiﬁed into “recurrent at-
tention” (Sukhbaatar et al., 2015) and “hierarchi-
cal attention” (Libovick´y and Helcl, 2017). The
former repeats the calculation of attention many
times to reﬁne the attention itself while the latter
integrates attentions for multiple input information
sources. The proposed multi-hop attention for the
Transformer is different from previous recurrent
attentions because the mechanism for the ﬁrst hop
attention and that for the second hop attention is

different. It is also different from previous hierar-
chical attention because it is designed to integrate
attentions from different heads for the same infor-
mation source.

In

attention

neural machine

translation,
(Bawden et al.,
can be

hier-
2018;
archical
Libovick´y and Helcl, 2017)
thought
of a multi-hop attention because it repeats atten-
tion calculation to integrate the information from
multiple source encoders. On the other hand, in
the Transformer (Vaswani et al., 2017), the state-
of-the-art model for neural machine translation,
feed-forward neural network (FFNN) integrates
information from multiple heads.
In this paper,
we propose a multi-hop attention mechanism as a
possible alternative to integrate information from
multi-head attention in the Transformer.

We ﬁnd that the proposed Transformer with
multi-hop attention converges faster than the orig-
inal Transformer. This is likely because all heads
learn to inﬂuence each other, through a head gate
mechanism,
in the second hop attention (Fig-
ure 1). Recently, many Transformer-based pre-
trained language models such as BERT have been
proposed and take about a month for training. The
speed at which the proposed model converges may
be even more important than the fact that its accu-
racy is slightly better.

2 Multi-Hop Multi-Head Attention for

the Transformer

2.1 Multi-Head Attention
One of the Transformer’s major successes is multi-
head attention, which allows each head to capture
different features and achieve better results com-
pared to a single-head case.

a(h) = sof tmax(

Q(h)K(h)Tp

d

)V (h)

m = Concat(a(1); :::; a(h))WO

(1)

(2)

Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 217–222

Florence, Italy, July 28 - August 2, 2019. c(cid:13)2019 Association for Computational Linguistics

217

Figure 1: Multi-hop attention

Given the query Q, the key K, and the value
V , they are divided into each head. Here, h (=
1; : : : ; H) denotes the index of the head, where a
is the output of scaled dot-product attention, WO
is a parameter for a linear transformation, and d is
a scaling factor. Finally, the output of multi-head
attention, m, is input to the next layer. The calcu-
lation of attention using scaled dot-product atten-
tion is deﬁned as the ﬁrst hop (Figure 1).

2.2 Multi-Hop Attention

In the original Transformer (Vaswani et al., 2017),
information from each head is integrated by sim-
ple concatenation followed by a linear transforma-
tion. Attention is reﬁned by stacking the combina-
tion of self-attention sub-layer and position-wise
feed-forward neural network sub-layer. However,
as layers are stacked, convergence becomes unsta-
ble. Consequently, there is a limit to the iterative
approach by layering. Therefore, we propose a
mechanism to repeat the calculation of attention
based on a mechanism other than stacking layers.
The original Transformer is considered to con-
sist of six single-hop attention layers. On the con-
trary, in the proposed method, some layers have

Model
Baseline
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop
Multi-hop

2nd hop
-
1
2
3
4
5
6
1,2
1,2,3
1,2,3,4
1,2,3,4,5
1,2,3,4,5,6
2,3,4,5,6
3,4,5,6
4,5,6
5,6

IWSLT2017

de!en
33.46
33.52
33.86y
33.74z
34.31y
33.81y
33.83y
33.77z
33.71z
33.58
33.30
32.53
32.80
33.22
33.40
33.60

en!de
27.21
27.75y
27.98y
27.98y
28.08y
27.81y
27.96y
27.73y
27.90y
27.88y
27.60y
27.30
27.54z
27.75y
27.74y
27.92y

y(p (cid:20) 0:01) and z(p (cid:20) 0:05) indicate that the proposed
methods signiﬁcantly outperform the Transformer baseline.

The encoder and the decoder each had six layers,

respectively.

Table 1: Best position for multi-hop

a multi-hop (two-hop) attention. By experiments,
we have established the appropriate position of
the proposed multi-hop attention in the neural ma-
chine translation system. If the number of layers
for encoders and decoders are six, then there are

218

WMT17

IWSLT2017

de!en
33.46
34.31y

en!de
Model
Baseline
18.15
19.88y
Multi-hop
y(p (cid:20) 0:01) indicates that the proposed methods
signiﬁcantly outperform the Transformer baseline.

de!en
21.33
23.91y

en!de
27.21
28.08y

Table 2: Evaluation Result

Model
Vanilla

Multi-hop

Vanilla

Multi-hop

Vanilla

Multi-hop

Vanilla

Multi-hop

Layers

4
4
5
5
6
6
7
7

IWSLT2017

de!en
30.02
30.09
33.80
33.78
33.46
34.31y
31.80
32.55y

en!de
27.60
27.63
28.00
28.15
27.21
28.08y
26.58
27.36y

Table 3: Difference between 6-layer Transformer with
multi-hop and 7-layer stacked vanilla Transformer

six self-attention layers in both the encoder and
the decoder, respectively, and six source-to-target
attention layers in the decoder.

The ﬁrst hop attention of the multi-hop at-
tention is equivalent to the calculation of scaled
dot-product attention (Equation 1) in the original
Transformer. The second hop attention consists of
multi-layer perceptron (MLP) attention and head
gate, as shown in Figure 1 and the following equa-
tions.

e(h)
i

(cid:12)(h)
i
′(h)
a
i

= vT

b tanh(WbQ(h) + U (h)

b a(h)

i

∑

=

= (cid:12)(h)

i

)

exp(e(h)
n=1 exp(e(h)
N
i U (h)

c a(h)

i

i

)

) (3)

(4)

(5)

i

First, MLP attention between the output of the
ﬁrst hop, a(h)
, and the query, Q, is calculated. At-
tention is considered as the calculation of a re-
lationship between the query and the key/value.
Therefore, in the second hop, attention is calcu-
lated again by using the output of the ﬁrst hop,
rather than the key/value.

i

Equations 4 and 5 are head gate in Figure 1.
The head gate normalizes the attention score of
each head to (cid:12)(h)
, using the softmax function,
where h ranges over all heads. In hierarchical at-
tention (Bawden et al., 2018), the softmax func-
tion is used to select a single source from multi-
ple sources. Here, the proposed head gate uses
the softmax function to select a head from multi-

Model
Vanilla

Multi-hop

Vanilla

Multi-hop

Vanilla

Multi-hop

Vanilla

Multi-hop

Layers

4
4
5
5
6
6
7
7

IWSLT2017

en!de
de!en
40,747K 41,882K
40,763K 41,898K
48,103K 49,238K
48,120K 49,254K
55,459K 56,594K
55,492K 56.627K
62,816K 63,951K
62,833K 63,967K

Table 4: Model Parameters

i

c

, and a(h)

ple heads. Finally, the head gate calculates new at-
′(h)
, using the learnable parameters U (h)
tention, a
,
i
(cid:12)(h)
. The second hop MLP attention
i
learns the optimal parameters for integration under
the inﬂuence of the head gate. Although Vaswani
et al. (2017) reported that dot-product attention is
superior to MLP attention, we used MLP atten-
tion in the second hop of the proposed multi-hop
attention because it can learn the dependence be-
tween heads by appropriately tuning the MLP pa-
rameters. We conclude that we can increase the
expressive power of the network more efﬁciently
by adding the second hop attention layer, rather
than by stacking another single-hop multi-head at-
tention layer.

3 Experiment

3.1 Data
We used German-English parallel data obtained
from the IWSLT2017 1 and the WMT17 2 shared
tasks.

The IWSLT2017 training, validation, and test
sets contain approximately 160K, 7.3K, and 6.7K
sentence pairs, respectively. There are approxi-
mately 5.9M sentence pairs in the WMT17 train-
ing dataset. For the WMT17 corpus, we used new-
stest2013 as the validation set and newstest2014
and newstest2017 as the test sets.

For tokenization, we used the subword-nmt
tool (Sennrich et al., 2016) to set a vocabulary size
of 32,000 for both German and English.

3.2 Experimental Setup
In our experiments, the baseline was the Trans-
former (Vaswani et al., 2017) model. We used

1https://sites.google.com/site/
iwsltevaluation2017/
2http://www.statmt.org/wmt17/
translation-task.html

219

(a) All learning curve view

(b) Enlarged view (loss 3.9 to 4.4)

Figure 2: Validation loss by each epoch for IWSLT2017 de-en - second hop in layer n to 6

fairseq (Gehring et al., 2017) 3 toolkit and the
source code will be available at our github reposi-
tory 4. For training, we used the Adam optimizer
with a learning rate of 0.0003. The embedding
size was 512, the hidden size was 2048, and the
number of heads was 8. The encoder and the
decoder each had six layers. The number of to-
kens per batch was 2,000. The number of train-
ing epochs for IWSLT2017 and WMT17 were 50
and 10, respectively. In all experiments using the
IWSLT2017, models were trained on an Nvidia
GeForce RTX 2080 Ti GPU, while in all experi-
ments using the WMT17, models were trained on
an Nvidia Tesla P100 GPU.

3.3 Results
Results of the evaluation are presented in Tables 1
and 2. In Table 2, the proposed multi-hop atten-
tion is used only at the fourth layer in the encoder.
In the evaluation of German-to-English translation
for IWSLT2017, the proposed method achieved
a BLEU score of 34.31, which indicates that it
signiﬁcantly outperforms the Transformer base-
line, which returned a BLEU score of 33.46. For
WMT17, the proposed method achieved a BLEU
score of 23.91, indicating that it also signiﬁcantly
outperformed the Transformer baseline, which re-
turned a BLEU score of 21.33.

In

IWSLT2017 German-to-English

and
English-to-German translation tasks,
various
conditions were investigated, as shown in Table 1.

3https://github.com/pytorch/fairseq
4https://github.com/siida36/
fairseq_mhda

The best models are shown in Figure 2.

The baseline training time was 1143.2s per
epoch in IWSLT2017 German-to-English transla-
tion, and the training time for the proposed method
is 1145.6s per epoch. We found that increasing the
number of parameters did not affect training time.

4 Analysis

4.1 Difference between Multi-Hop and

7-layer Stacked Transformer

We compared the proposed method with the origi-
nal Transformer. Table 3 shows the translation ac-
curacies when the number of layers was changed
from 4 to 7, encoder and decoder, respectively.
Here,“Vanilla”refers to the original Transformer
and“ Multi-hop ”refers to the proposed method
where the multi-hop attention layer is used at the
fourth layer in the encoder. As shown in Table 3,
the 7-layer model BLEU score is lower than that
of the 6-layer model. In the experiments, the num-
ber of parameters required by the 6- and 7-layer
models was 55,459K, and 62,816K, respectively,
and the number of parameters for the multi-hop
method was 55,492K. The proposed method only
increases the number of parameters by one percent
compared to simply stacking one multi-head layer.
Thus, it is evident that simply increasing the num-
ber of parameters and repeating the attention cal-
culation doesn’t necessarily improve performance.
On the other hand, the proposed method does not
improve the BLEU score when the number of lay-
ers is four and ﬁve. This is probably because the
parameters of each head in the baseline Trans-

220

Epoch

Baseline

Layer 1,2,3,4,5,6

Layer 2,3,4,5,6

Second hop
Layer 3,4,5,6

Layer 4,5,6

Layer 5,6

Layer 6

1
10
20
30
40
50

7.87
4.80
4.15
4.01
3.97
3.98

7.49
4.21
4.04
4.04
4.05
4.09

7.49
4.18
4.00
4.00
4.02
4.05

7.53
4.17
3.99
3.97
4.00
4.03

7.56
4.17
3.98
3.96
3.98
4.02

7.70
4.17
3.97
3.95
3.97
4.00

7.82
4.21
3.97
3.93
3.94
3.98

Table 5: Validation loss by epoch for IWSLT2017 de-en

30%. All models have 6 layers and the positions
of the second hop layers have narrowed from all 6
layers to only 6th layers. It should be noted that,
in the ﬁrst epoch (row 1, Table 5), the model with
the second hop in all layers has the lowest valida-
tion loss, while the baseline model has the highest
validation loss.

Figure 2(a) shows the learning curve based on
the same data shown in Table 5, It is apparent that
the models with the second hop converge faster
than the baseline model. Figure 2(b) is an enlarged
view of Figure 2(a), focused on the lowest valida-
tion loss for different models. We ﬁnd that the val-
idation loss is lower when there are fewer second
hop attentions.

Figure 3 shows the learning curves for the mod-
els with multi-hop attention used only once any-
where in layer 1 to 6. We ﬁnd the model with
second hop attention in layer 6 converges fastest.
In terms of convergence, as opposed to accuracy,
it seems appropriate to use second hop attention
only in the last (6th) layer in the encoder.

5 Related Work

The mechanism of the proposed multi-hop at-
tention for the Transformer was inspired by the
hierarchical attention in multi-source sequence-
(Libovick´y and Helcl, 2017).
to-sequence model
The term “multi-hop ” is borrowed from the
end-to-end memory network (Sukhbaatar et al.,
2015) and the title “attention over heads” is in-
spired by Attention-over-Attention neural network
(Cui et al., 2017), respectively.

Ahmed et al. (2018) proposed Weighted Trans-
former which replaces multi-head attention by
multiple self-attention branches that learn to com-
bine during the training process. They reported
that it slightly outperformed the baseline Trans-
former (0.5 BLEU points on the WMT 2014
English-to-German translation task) and con-
verges 15-40% faster. They linearly combined
the multiple sources of attention, while we com-

Figure 3:
IWSLT2017 de-en - second hop in only n layer

Validation loss by each epoch for

former are likely to converge properly when there
are relatively few parameters. Another interpreta-
tion is that the normalization among heads forced
by the proposed method works as noise.

As a conclusion, the proposed method demon-
strates that appropriate connection can be obtained
by recalculating attention in the layer where the
head has a dependency.

Table 1 shows the effect of introducing second
hop attention to various positions in the encoder.
The second column shows the positions where the
second hop attention is used. The best result was
obtained when the second hop attention was used
only for the fourth layer in the encoder. Perfor-
mance decreased as the second hop attention was
introduced to more layers, i.e., the worst result was
obtained when using the second hop in all layers
(second hop in layer 1,2,3,4,5,6). Further studies
are needed to elucidate the relationship between
performance and position of the second hop atten-
tion.

4.2 Effect on Learning Speed
Table 5 shows the validation loss of models for the
IWSLT2017 German-to-English translation task
with the second hop layers whose dropout rate is

221

tion. arXiv preprint arXiv:1711.02132.

R. Bawden, R. Sennrich, A. Birch, and B. Haddow.
2018. Evaluating discourse phenomena in neural
In Proc. NAACL-HLT, pages
machine translation.
1304–1313.

Y. Cui, Z. Chen, S. Wei, S. Wang, T. Liu, and G. Hu.
2017. Attention-over-attention neural networks for
reading comprehension. In Proc. 55th ACL, pages
593–602.

M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and
In Proc.

Ł. Kaiser. 2019. Universal transformers.
7th ICLR.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.
2019. BERT: Pre-training of deep bidirectional
In Proc.
transformers for language understanding.
NAACL-HLT, volume abs/1810.04805.

J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N.
Dauphin. 2017. Convolutional Sequence to Se-
quence Learning. In Proc. 34th ICML.

J. Libovick´y and J. Helcl. 2017. Attention strategies
for multi-source sequence-to-sequence learning. In
Proc. 55th ACL, pages 196–202.

P. Michel, O. Levy, and G. Neubig. 2019. Are six-
arXiv preprint

teen heads really better than one?
arXiv:1905.10650.

M. Popel and O. Bojar. 2018. Training tips for the
transformer model. The Prague Bulletin of Math-
ematical Linguistics, 110(1):43–70.

R. Sennrich, B. Haddow, and A. Birch. 2016. Neu-
ral machine translation of rare words with subword
units. In Proc. 54th ACL, pages 1715–1725.

S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus.
2015. End-to-end memory networks. In Proc. 28th
NIPS, pages 2440–2448.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin.
2017. Attention is all you need. In Proc. 30th NIPS,
pages 5998–6008.

E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and
I. Titov. 2019. Analyzing multi-head self-attention:
Specialized heads do the heavy lifting, the rest can
be pruned. arXiv preprint arXiv:1905.09418.

bined multiple attention non-linearly using soft-
max function in the second hop.

It is well known that the Transformer is difﬁ-
cult to train (Popel and Bojar, 2018). As it has a
large number of parameters, it takes time to con-
verge and sometimes it does not do so at all with-
out appropriate hyper parameter tuning. Consid-
ering the experimental results of our multi-hop at-
tention experiments, and that of the Weight Trans-
former, an appropriate design of the network to
combine multi-head attention could result in faster
and more stable convergence of the Transformer.
As the Transformer is used as a building block for
the recently proposed pre-trained language models
such as BERT (Devlin et al., 2019) which takes
about a month for training, we think it is worth-
while to pursue this line of research including the
proposed multi-hop attention.

Universal Transformer (Dehghani et al., 2019)
can be thought of variable-depth recurrent at-
tention.
It obtained Turing-complete expressive
power in exchange for a vast increase in the num-
ber of parameters and training time. As shown in
Table 4, we have proposed an efﬁcient method to
increase the depth of recurrence in terms of the
number of parameters and training time. Recently,
Voita et al. (2019) and Michel et al. (2019) inde-
pendently reported that only a certain subset of the
heads plays an important role in the Transformer.
They performed analyses by pruning heads from
an already trained model, while we have proposed
a method to assign weights to heads automati-
cally. We assume our method (multi-hop attention
or attention-over-heads) selects important heads in
the early stage of training, which results in faster
convergence than the original Transformer.
6 Conclusion
In this paper, we have proposed a multi-hop atten-
tion mechanism for a Transformer model in which
all heads depend on each other repeatedly. We
found that the proposed method signiﬁcantly out-
performs the original Transformer in accuracy and
converges faster with little increase in the number
of parameters. In future work, we would like to
implement a multi-hop attention mechanism to the
decoder side and investigate other language pairs.

References
K. Ahmed, N. S. Keskar, and R.Socher. 2018.
Weighted transformer network for machine transla-

222

