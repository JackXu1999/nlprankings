



















































Semi-Supervised Sequence Modeling with Cross-View Training


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1914

Semi-Supervised Sequence Modeling with Cross-View Training

Kevin Clark1 Minh-Thang Luong2 Christopher D. Manning1 Quoc V. Le2
1Computer Science Department, Stanford University 2Google Brain

kevclark@cs.stanford.edu, thangluong@google.com, manning@cs.stanford.edu, qvl@google.com

Abstract

Unsupervised representation learning algo-
rithms such as word2vec and ELMo improve
the accuracy of many supervised NLP mod-
els, mainly because they can take advantage
of large amounts of unlabeled text. However,
the supervised models only learn from task-
specific labeled data during the main train-
ing phase. We therefore propose Cross-View
Training (CVT), a semi-supervised learning
algorithm that improves the representations of
a Bi-LSTM sentence encoder using a mix of
labeled and unlabeled data. On labeled exam-
ples, standard supervised learning is used. On
unlabeled examples, CVT teaches auxiliary
prediction modules that see restricted views
of the input (e.g., only part of a sentence) to
match the predictions of the full model see-
ing the whole input. Since the auxiliary mod-
ules and the full model share intermediate
representations, this in turn improves the full
model. Moreover, we show that CVT is par-
ticularly effective when combined with multi-
task learning. We evaluate CVT on five se-
quence tagging tasks, machine translation, and
dependency parsing, achieving state-of-the-art
results.1

1 Introduction

Deep learning models work best when trained on
large amounts of labeled data. However, acquir-
ing labels is costly, motivating the need for ef-
fective semi-supervised learning techniques that
leverage unlabeled examples. A widely successful
semi-supervised learning strategy for neural NLP
is pre-training word vectors (Mikolov et al., 2013).
More recent work trains a Bi-LSTM sentence en-
coder to do language modeling and then incorpo-
rates its context-sensitive representations into su-
pervised models (Dai and Le, 2015; Peters et al.,

1Code will be made available at https:
//github.com/tensorflow/models/tree/
master/research/cvt_text

2018). Such pre-training methods perform unsu-
pervised representation learning on a large corpus
of unlabeled data followed by supervised training.

A key disadvantage of pre-training is that the
first representation learning phase does not take
advantage of labeled data – the model attempts
to learn generally effective representations rather
than ones that are targeted towards a particular
task. Older semi-supervised learning algorithms
like self-training do not suffer from this prob-
lem because they continually learn about a task
on a mix of labeled and unlabeled data. Self-
training has historically been effective for NLP
(Yarowsky, 1995; McClosky et al., 2006), but is
less commonly used with neural models. This pa-
per presents Cross-View Training (CVT), a new
self-training algorithm that works well for neural
sequence models.

In self-training, the model learns as normal on
labeled examples. On unlabeled examples, the
model acts as both a teacher that makes predictions
about the examples and a student that is trained
on those predictions. Although this process has
shown value for some tasks, it is somewhat tau-
tological: the model already produces the predic-
tions it is being trained on. Recent research on
computer vision addresses this by adding noise to
the student’s input, training the model so it is ro-
bust to input perturbations (Sajjadi et al., 2016;
Wei et al., 2018). However, applying noise is dif-
ficult for discrete inputs like text.

As a solution, we take inspiration from multi-
view learning (Blum and Mitchell, 1998; Xu et al.,
2013) and train the model to produce consistent
predictions across different views of the input. In-
stead of only training the full model as a student,
CVT adds auxiliary prediction modules – neu-
ral networks that transform vector representations
into predictions – to the model and also trains them
as students. The input to each student prediction
module is a subset of the model’s intermediate rep-

https://github.com/tensorflow/models/tree/master/research/cvt_text
https://github.com/tensorflow/models/tree/master/research/cvt_text
https://github.com/tensorflow/models/tree/master/research/cvt_text


1915

resentations corresponding to a restricted view of
the input example. For example, one auxiliary pre-
diction module for sequence tagging is attached to
only the “forward” LSTM in the model’s first Bi-
LSTM layer, so it makes predictions without see-
ing any tokens to the right of the current one.

CVT works by improving the model’s represen-
tation learning. The auxiliary prediction modules
can learn from the full model’s predictions be-
cause the full model has a better, unrestricted view
of the input. As the auxiliary modules learn to
make accurate predictions despite their restricted
views of the input, they improve the quality of the
representations they are built on top of. This in
turn improves the full model, which uses the same
shared representations. In short, our method com-
bines the idea of representation learning on unla-
beled data with classic self-training.

CVT can be applied to a variety of tasks and
neural architectures, but we focus on sequence
modeling tasks where the prediction modules are
attached to a shared Bi-LSTM encoder. We
propose auxiliary prediction modules that work
well for sequence taggers, graph-based depen-
dency parsers, and sequence-to-sequence mod-
els. We evaluate our approach on English de-
pendency parsing, combinatory categorial gram-
mar supertagging, named entity recognition, part-
of-speech tagging, and text chunking, as well as
English to Vietnamese machine translation. CVT
improves over previously published results on all
these tasks. Furthermore, CVT can easily and ef-
fectively be combined with multi-task learning:
we just add additional prediction modules for the
different tasks on top of the shared Bi-LSTM en-
coder. Training a unified model to jointly perform
all of the tasks except machine translation im-
proves results (outperforming a multi-task ELMo
model) while decreasing the total training time.

2 Cross-View Training

We first present Cross-View Training and describe
how it can be combined effectively with multi-task
learning. See Figure 1 for an overview of the train-
ing method.

2.1 Method

Let Dl = {(x1, y1), (x2, y2), ..., (xN , yN )} repre-
sent a labeled dataset and Dul = {x1, x2, ..., xM}
represent an unlabeled dataset We use pθ(y|xi)
to denote the output distribution over classes pro-

Auxiliary 4:    ________________________ by plane

Learning on a Labeled Example

"She lives in
Washington."

LOCATION 

BiLSTM
Encoder

Primary
Prediction
Module

pθ

 

loss

Learning on an Unlabeled Example

"They traveled to
Washington by plane"

Primary

Auxiliary 1

Auxiliary 2

Auxiliary 3

Auxiliary 4

Prediction Modules

losses

Inputs Seen by Auxiliary Prediction Modules 

Auxiliary 3:    _____________ Washington by plane 

Auxiliary 2:    They traveled to Washington  _______

BiLSTM
Encoder

Auxiliary 1:    They traveled to __________________

pθ

 p
1
θ

 
p2θ

 
p3θ

 p
4
θ

 

Figure 1: An overview of Cross-View Training. The
model is trained with standard supervised learning on
labeled examples. On unlabeled examples, auxiliary
prediction modules with different views of the input
are trained to agree with the primary prediction mod-
ule. This particular example shows CVT applied to
named entity recognition. From the labeled example,
the model can learn that “Washington” usually refers
to a location. Then, on unlabeled data, auxiliary pre-
diction modules are trained to reach the same predic-
tion without seeing some of the input. In doing so, they
improve the contextual representations produced by the
model, for example, learning that “traveled to” is usu-
ally followed by a location.

duced by the model with parameters θ on input xi.
During CVT, the model alternates learning on a
minibatch of labeled examples and learning on a
minibatch of unlabeled examples. For labeled ex-
amples, CVT uses standard cross-entropy loss:

Lsup(θ) =
1

|Dl|
∑

xi,yi∈Dl

CE(yi, pθ(y|xi))

CVT adds k auxiliary prediction modules to the
model, which are used when learning on unlabeled
examples. A prediction module is usually a small
neural network (e.g., a hidden layer followed by
a softmax layer). Each one takes as input an in-
termediate representation hj(xi) produced by the
model (e.g., the outputs of one of the LSTMs in a
Bi-LSTM model). It outputs a distribution over la-
bels pjθ(y|xi). Each h

j is chosen such that it only
uses a part of the input xi; the particular choice



1916

can depend on the task and model architecture. We
propose variants for several tasks in Section 3. The
auxiliary prediction modules are only used during
training; the test-time prediction come from the
primary prediction module that produces pθ.

On an unlabeled example, the model first pro-
duces soft targets pθ(y|xi) by performing infer-
ence. CVT trains the auxiliary prediction modules
to match the primary prediction module on the un-
labeled data by minimizing

LCVT(θ) = 1|Dul|
∑

xi∈Dul
∑k

j=1D(pθ(y|xi), p
j
θ(y|xi))

where D is a distance function between probabil-
ity distributions (we use KL divergence). We hold
the primary module’s prediction pθ(y|xi) fixed
during training (i.e., we do not back-propagate
through it) so the auxiliary modules learn to im-
itate the primary one, but not vice versa. CVT
works by enhancing the model’s representation
learning. As the auxiliary modules train, the rep-
resentations they take as input improve so they are
useful for making predictions even when some of
the model’s inputs are not available. This in turn
improves the primary prediction module, which is
built on top of the same shared representations.

We combine the supervised and CVT losses into
the total loss, L = Lsup + LCVT, and minimize it
with stochastic gradient descent. In particular, we
alternate minimizing Lsup over a minibatch of la-
beled examples and minimizing LCVT over a mini-
batch of unlabeled examples.

For most neural networks, adding a few ad-
ditional prediction modules is computationally
cheap compared to the portion of the model build-
ing up representations (such as an RNN or CNN).
Therefore our method contributes little overhead
to training time over other self-training approaches
for most tasks. CVT does not change inference
time or the number of parameters in the fully-
trained model because the auxiliary prediction
modules are only used during training.

2.2 Combining CVT with Multi-Task
Learning

CVT can easily be combined with multi-task
learning by adding additional prediction modules
for the other tasks on top of the shared Bi-LSTM
encoder. During supervised learning, we ran-
domly select a task and then update Lsup using
a minibatch of labeled data for that task. When
learning on the unlabeled data, we optimize LCVT

jointly across all tasks at once, first running infer-
ence with all the primary prediction modules and
then learning from the predictions with all the aux-
iliary prediction modules. As before, the model
alternates training on minibatches of labeled and
unlabeled examples.

Examples labeled across many tasks are use-
ful for multi-task systems to learn from, but most
datasets are only labeled with one task. A benefit
of multi-task CVT is that the model creates (ar-
tificial) all-tasks-labeled examples from unlabeled
data. This significantly improves the model’s data
efficiency and training time. Since running pre-
diction modules is computationally cheap, com-
puting LCVT is not much slower for many tasks
than it is for a single one. However, we find
the all-tasks-labeled examples substantially speed
up model convergence. For example, our model
trained on six tasks takes about three times as long
to converge as the average model trained on one
task, a 2x decrease in total training time.

3 Cross-View Training Models

CVT relies on auxiliary prediction modules that
have restricted views of the input. In this section,
we describe specific constructions of the auxiliary
prediction modules that are effective for sequence
tagging, dependency parsing, and sequence-to-
sequence learning.

3.1 Bi-LSTM Sentence Encoder
All of our models use a two-layer CNN-BiLSTM
(Chiu and Nichols, 2016; Ma and Hovy, 2016)
sentence encoder. It takes as input a sequence of
words xi = [x1i , x

2
i , ..., x

T
i ]. First, each word is

represented as the sum of an embedding vector
and the output of a character-level Convolutional
Neural Network, resulting in a sequence of vectors
v = [v1, v2, ..., vT ]. The encoder applies a two-
layer bidirectional LSTM (Graves and Schmidhu-
ber, 2005) to these representations. The first layer
runs a Long Short-Term Memory unit (Hochre-
iter and Schmidhuber, 1997) in the forward di-
rection (taking vt as input at each step t) and the
backward direction (taking vT−t+1 at each step)
to produce vector sequences [

−→
h 11,
−→
h 21, ...

−→
h T1 ] and

[
←−
h 11,
←−
h 21, ...

←−
h T1 ]. The output of the Bi-LSTM is

the concatenation of these vectors: h1 = [
−→
h 11 ⊕←−

h 11, ...,
−→
h T1 ⊕

←−
h T1 ]. The second Bi-LSTM layer

works the same, producing outputs h2, except it
takes h1 as input instead of v.



1917

3.2 CVT for Sequence Tagging
In sequence tagging, each token xti has a corre-
sponding label yti . The primary prediction module
for sequence tagging produces a probability distri-
bution over classes for the tth label using a one-
hidden-layer neural network applied to the corre-
sponding encoder outputs:

p(yt|xi) = NN(ht1 ⊕ ht2)
= softmax(U · ReLU(W (ht1 ⊕ ht2)) + b)

The auxiliary prediction modules take
−→
h 1(xi)

and
←−
h 1(xi), the outputs of the forward and back-

ward LSTMs in the first2 Bi-LSTM layer, as in-
puts. We add the following four auxiliary predic-
tion modules to the model (see Figure 2):

pfwdθ (y
t|xi) = NNfwd(

−→
h t1(xi))

pbwdθ (y
t|xi) = NNbwd(

←−
h t1(xi))

pfutureθ (y
t|xi) = NNfuture(

−→
h t−11 (xi))

p
past
θ (y

t|xi) = NNpast(
←−
h t+11 (xi))

The “forward” module makes each prediction
without seeing the right context of the current to-
ken. The “future” module makes each predic-
tion without the right context or the current to-
ken itself. Therefore it works like a neural lan-
guage model that, instead of predicting which to-
ken comes next in the sequence, predicts which
class of token comes next. The “backward” and
“past” modules are analogous.

3.3 CVT for Dependency Parsing
In a dependency parse, words in a sentence are
treated as nodes in a graph. Typed directed
edges connect the words, forming a tree struc-
ture describing the syntactic structure of the sen-
tence. In particular, each word xti in a sentence
xi = x

1
i , ..., x

T
i receives exactly one in-going edge

(u, t, r) going from word xui (called the “head”)
to it (the “dependent”) of type r (the “relation”).
We use a graph-based dependency parser similar
to the one from Dozat and Manning (2017). This
treats dependency parsing as a classification task
where the goal is to predict which in-going edge
yti = (u, t, r) connects to each word x

t
i.

First, the representations produced by the en-
coder for the candidate head and dependent are

2Modules taking inputs from the second Bi-LSTM layer
would not have restricted views because information about
the whole sentence gets propagated through the first layer.

LSTM LSTM

ŷfuture  ŷfwd  ŷ   ŷbwd  ŷpast 

Backward LSTM

Forward LSTM

Predict

LSTM LSTM

LSTM

LSTM LSTM

Auxiliary
Prediction
Modules

Primary
Prediction
Module

x1 x2 x3

Embed

Backward LSTM

Forward LSTM

pθPredict p
fwd
θ

pfutureθ
pbwdθ

ppastθ

Auxiliary
Prediction
Modules

Primary
Prediction
Module

Loss

Figure 2: Auxiliary prediction modules for sequence
tagging models. Each one sees a restricted view of the
input. For example, the “forward” prediction module
does not see any context to the right of the current token
when predicting that token’s label. For simplicity, we
only show a one layer Bi-LSTM encoder and only show
the model’s predictions for a single time step.

passed through separate hidden layers. A bilin-
ear classifier applied to these representations pro-
duces a score for each candidate edge. Lastly,
these scores are passed through a softmax layer to
produce probabilities. Mathematically, the proba-
bility of an edge is given as:

pθ((u, t, r)|xi) ∝ es(h
u
1 (xi)⊕hu2 (xi),ht1(xi)⊕ht2(xi),r)

where s is the scoring function:

s(z1, z2, r) = ReLU(Wheadz1 + bhead)(Wr +W )

ReLU(Wdepz2 + bdep)

The bilinear classifier uses a weight matrix Wr
specific to the candidate relation as well as a
weight matrix W shared across all relations. Note
that unlike in most prior work, our dependency
parser only takes words as inputs, not words and
part-of-speech tags.

We add four auxiliary prediction modules to our
model for cross-view training:

pfwd-fwdθ ((u, t, r)|xi) ∝ es
fwd-fwd(

−→
h u1 (xi),

−→
h t1(xi),r)

pfwd-bwdθ ((u, t, r)|xi) ∝ es
fwd-bwd(

−→
h u1 (xi),

←−
h t1(xi),r)

pbwd-fwdθ ((u, t, r)|xi) ∝ es
bwd-fwd(

←−
h u1 (xi),

−→
h t1(xi),r)

pbwd-bwdθ ((u, t, r)|xi) ∝ es
bwd-bwd(

←−
h u1 (xi),

←−
h t1(xi),r)

Each one has some missing context (not seeing ei-
ther the preceding or following words) for the can-
didate head and candidate dependent.



1918

3.4 CVT for Sequence-to-Sequence Learning

We use an encoder-decoder sequence-to-sequence
model with attention (Sutskever et al., 2014; Bah-
danau et al., 2015). Each example consists of an
input (source) sequence xi = x1i , ..., x

T
i and out-

put (target) sequence yi = y1i , ..., y
K
i . The en-

coder’s representations are passed into an LSTM
decoder using a bilinear attention mechanism (Lu-
ong et al., 2015). In particular, at each time
step t the decoder computes an attention distribu-
tion over source sequence hidden states as αj ∝
eh

jWαh̄t where h̄t is the decoder’s current hid-
den state. The source hidden states weighted by
the attention distribution form a context vector:
ct =

∑
j αjh

j . Next, the context vector and
current hidden state are combined into an atten-
tion vector at = tanh(Wa[ct, ht]). Lastly, a soft-
max layer predicts the next token in the output se-
quence: p(yti |y<ti , xi) = softmax(Wsat).

We add two auxiliary decoders when apply-
ing CVT. The auxiliary decoders share embed-
ding and LSTM parameters with the primary de-
coder, but have different parameters for the atten-
tion mechanisms and softmax layers. For the first
one, we restrict its view of the input by applying
attention dropout, randomly zeroing out a fraction
of its attention weights. The second one is trained
to predict the next word in the target sequence
rather than the current one: pfutureθ (y

t
i |y<ti , xi) =

softmax(W futures a
future
t−1 ). Since there is no target se-

quence for unlabeled examples, we cannot apply
teacher forcing to get an output distribution over
the vocabulary from the primary decoder at each
time step. Instead, we produce hard targets for the
auxiliary modules by running the primary decoder
with beam search on the input sequence. This
idea has previously been applied to sequence-level
knowledge distillation by Kim and Rush (2016).

4 Experiments

We compare Cross-View Training against several
strong baselines on seven tasks:

Combinatory Categorial Grammar (CCG) Su-
pertagging: We use data from CCGBank (Hock-
enmaier and Steedman, 2007).

Text Chunking: We use the CoNLL-2000 data
(Tjong Kim Sang and Buchholz, 2000).

Named Entity Recognition (NER): We use the
CoNLL-2003 data (Tjong Kim Sang and De Meul-
der, 2003).

Fine-Grained NER (FGN): We use the
OntoNotes (Hovy et al., 2006) dataset.

Part-of-Speech (POS) Tagging: We use the Wall
Street Journal portion of the Penn Treebank (Mar-
cus et al., 1993).

Dependency Parsing: We use the Penn Treebank
converted to Stanford Dependencies version 3.3.0.

Machine Translation: We use the English-
Vietnamese translation dataset from IWSLT 2015
(Cettolo et al., 2015). We report (tokenized)
BLEU scores on the tst2013 test set.

We use the 1 Billion Word Language Model
Benchmark (Chelba et al., 2014) as a pool of un-
labeled sentences for semi-supervised learning.

4.1 Model Details and Baselines

We apply dropout during training, but not when
running the primary prediction module to produce
soft targets on unlabeled examples. In addition
to the auxiliary prediction modules listed in Sec-
tion 3, we find it slightly improves results to add
another one that sees the whole input rather than
a subset (but unlike the primary prediction mod-
ule, does have dropout applied to its representa-
tions). Unless indicated otherwise, our models
have LSTMs with 1024-sized hidden states and
512-sized projection layers. See the supplemen-
tary material for full training details and hyperpa-
rameters. We compare CVT with the following
other semi-supervised learning algorithms:

Word Dropout. In this method, we only train
the primary prediction module. When acting as
a teacher it is run as normal, but when acting as
a student, we randomly replace some of the input
words with a REMOVED token. This is similar to
CVT in that it exposes the model to a restricted
view of the input. However, it is less data effi-
cient. By carefully designing the auxiliary pre-
diction modules, it is possible to train the auxil-
iary prediction modules to match the primary one
across many different views of the input a once,
rather than just one view at a time.

Virtual Adversarial Training (VAT). VAT (Miy-
ato et al., 2016) works like word dropout, but
adds noise to the word embeddings of the stu-
dent instead of dropping out words. Notably, the
noise is chosen adversarially so it most changes
the model’s prediction. This method was applied
successfully to semi-supervised text classification



1919

Method
CCG Chunk NER FGN POS Dep. Parse Translate
Acc. F1 F1 F1 Acc. UAS LAS BLEU

Shortcut LSTM (Wu et al., 2017) 95.1 97.53
ID-CNN-CRF (Strubell et al., 2017) 90.7 86.8
JMT† (Hashimoto et al., 2017) 95.8 97.55 94.7 92.9
TagLM* (Peters et al., 2017) 96.4 91.9
ELMo* (Peters et al., 2018) 92.2

Biaffine (Dozat and Manning, 2017) 95.7 94.1
Stack Pointer (Ma et al., 2018) 95.9 94.2

Stanford (Luong and Manning, 2015) 23.3
Google (Luong et al., 2017) 26.1

Supervised 94.9 95.1 91.2 87.5 97.60 95.1 93.3 28.9
Virtual Adversarial Training* 95.1 95.1 91.8 87.9 97.64 95.4 93.7 –
Word Dropout* 95.2 95.8 92.1 88.1 97.66 95.6 93.8 29.3
ELMo (our implementation)* 95.8 96.5 92.2 88.5 97.72 96.2 94.4 29.3
ELMo + Multi-task*† 95.9 96.8 92.3 88.4 97.79 96.4 94.8 –
CVT* 95.7 96.6 92.3 88.7 97.70 95.9 94.1 29.6
CVT + Multi-task*† 96.0 96.9 92.4 88.4 97.76 96.4 94.8 –
CVT + Multi-task + Large*† 96.1 97.0 92.6 88.8 97.74 96.6 95.0 –

Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around 0.1
for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the supplementary materials for
results with them included. The +Large model has four times as many hidden units as the others, making it similar
in size to the models when ELMo is included. * denotes semi-supervised and † denotes multi-task.

by Miyato et al. (2017).

ELMo. ELMo incorporates the representations
from a large separately-trained language model
into a task-specific model. Our implementaiton
follows Peters et al. (2018). When combining
ELMo with multi-task learning, we allow each
task to learn its own weights for the ELMo em-
beddings going into each prediction module. We
found applying dropout to the ELMo embeddings
was crucial for achieving good performance.

4.2 Results

Results are shown in Table 1. CVT on its own out-
performs or is comparable to the best previously
published results on all tasks. Figure 3 shows an
example win for CVT over supervised learning.

Of the prior results listed in Table 1, only
TagLM and ELMo are semi-supervised. These
methods first train an enormous language model
on unlabeled data and incorporate the representa-
tions produced by the language model into a su-
pervised classifier. Our base models use 1024 hid-
den units in their LSTMs (compared to 4096 in
ELMo), require fewer training steps (around one
pass over the billion-word benchmark rather than

0 40000 80000 120000
Training Steps

0.4

0.6

0.8

p(
"W

ar
ne

r B
ro

s"
 =

 O
RG

) Dev Set Example: "...statement by Warner Bros."
Supervised
CVT

Figure 3: An NER example that CVT classifies cor-
rectly but supervised learning does not. “Warner” only
occurs as a last name in the train set, so the supervised
model classifies “Warner Bros” as a person. The CVT
model also mistakenly classifies “Warner Bros” as a
person to start with, but as it sees more of the unlabeled
data (in which “Warner” occurs thousands of times) it
learns that “Warner Bros” is an organization.

many passes), and do not require a pipelined train-
ing procedure. Therefore, although they perform
on par with ELMo, they are faster and simpler
to train. Increasing the size of our CVT+Multi-
task model so it has 4096 units in its LSTMs like
ELMo improves results further so they are signifi-
cantly better than the ELMo+Multi-task ones. We
suspect there could be further gains from combin-
ing our method with language model pre-training,
which we leave for future work.



1920

CVT + Multi-Task. We train a single shared-
encoder CVT model to perform all of the tasks
except machine translation (as it is quite differ-
ent and requires more training time than the other
ones). Multi-task learning improves results on
all of the tasks except fine-grained NER, some-
times by large margins. Prior work on many-task
NLP such as Hashimoto et al. (2017) uses compli-
cated architectures and training algorithms. Our
result shows that simple parameter sharing can be
enough for effective many-task learning when the
model is big and trained on a large amount of data.

Interestingly, multi-task learning works better
in conjunction with CVT than with ELMo. We
hypothesize that the ELMo models quickly fit to
the data primarily using the ELMo vectors, which
perhaps hinders the model from learning effective
representations that transfer across tasks. We also
believe CVT alleviates the danger of the model
“forgetting” one task while training on the other
ones, a well-known problem in many-task learn-
ing (Kirkpatrick et al., 2017). During multi-task
CVT, the model makes predictions about unla-
beled examples across all tasks, creating (artifi-
cial) all-tasks-labeled examples, so the model does
not only see one task at a time. In fact, multi-task
learning plus self training is similar to the Learn-
ing without Forgetting algorithm (Li and Hoiem,
2016), which trains the model to keep its predic-
tions on an old task unchanged when learning a
new task. To test the value of all-tasks-labeled ex-
amples, we trained a multi-task CVT model that
only computes LCVT on one task at a time (chosen
randomly for each unlabeled minibatch) instead of
for all tasks in parallel. The one-at-a-time model
performs substantially worse (see Table 2).

Model CCG Chnk NER FGN POS Dep.

CVT-MT 95.7 97.4 96.0 86.7 97.74 94.4
w/out parallel 95.4 97.1 95.6 86.3 97.71 94.1

Table 2: Dev set performance of multi-task CVT with
and without producing all-tasks-labeled examples.

Model Generalization. In order to evaluate how
our models generalize to the dev set from the train
set, we plot the dev vs. train accuracy for our dif-
ferent methods as they learn (see Figure 4). Both
CVT and multi-task learning improve model gen-
eralization: for the same train accuracy, the mod-
els get better dev accuracy than purely supervised
learning. Interestingly, CVT continues to improve

92 94 96 98 100
Train Accuracy

93

94

95

96

De
v 

Ac
cu

ra
cy

CCG

96 98 100
Train F1

94

96

De
v 

F1

Chunking

90.0 92.5 95.0 97.5 100.0
Train LAS

90

92

94

De
v 

LA
S

Dependency Parsing

92 94 96 98 100
Train F1

92

94

96

De
v 

F1

NER

CVT
Supervised

CVT + Multitask
CVT + Multitask, Small

Figure 4: Dev set vs. Train set accuracy for various
methods. The “small” model has 1/4 the LSTM hidden
state size of the other ones (256 instead of 1024).

in dev set accuracy while close to 100% train ac-
curacy for CCG, Chunking, and NER, perhaps be-
cause the model is still learning from unlabeled
data even when it has completely fit to the train
set. We also show results for a smaller multi-task
+ CVT model. Although it generalizes at least as
well as the larger one, it halts making progress on
the train set earlier. This suggests it is important
to use sufficiently large neural networks for multi-
task learning: otherwise the model does not have
the capacity to fit to all the training data.

Auxiliary Prediction Module Ablation. We
briefly explore which auxiliary prediction modules
are more important for the sequence tagging tasks
in Table 3. We find that both kinds of auxiliary
prediction modules improve performance, but that
the future and past modules improve results more
than the forward and backward ones, perhaps be-
cause they see a more restricted and challenging
view of the input.

Model CCG Chnk NER FGN POS

Supervised 94.8 95.5 95.0 86.0 97.59
CVT 95.6 97.0 95.9 87.3 97.66

no fwd/bwd –0.1 –0.2 –0.2 –0.1 –0.01
no future/past –0.3 –0.4 –0.4 –0.3 –0.04

Table 3: Ablation study on auxiliary prediction mod-
ules for sequence tagging.

Training Models on Small Datasets. We ex-
plore how CVT scales with dataset size by vary-
ing the amount of training data the model has ac-



1921

10 25 50 75 100
Percent of Training Set Provided

92

93

94

95
Ac

cu
ra

cy
CCG

10 25 50 75 100
Percent of Training Set Provided

90

92

94

96

F1

Chunking

10 25 50 75 100
Percent of Training Set Provided

90

92

94

LA
S

Dependency Parsing

10 25 50 75 100
Percent of Training Set Provided

90

92

94

96

F1

NER

CVT Supervised

128 256 512 768 1024
Model Size

94.5

95.0

95.5

Ac
cu

ra
cy

CCG

128 256 512 768 1024
Model Size

95.0

95.5

96.0

96.5

F1

Chunking

128 256 512 768 1024
Model Size

91

92

93

94

LA
S

Dependency Parsing

128 256 512 768 1024
Model Size

95.0

95.5

F1

NER

CVT Supervised

Figure 5: Left: Dev set performance vs. percent of the training set provided to the model. Right: Dev set perfor-
mance vs. model size. The x axis shows the number of hidden units in the LSTM layers; the projection layers and
other hidden layers in the network are half that size. Points correspond to the mean of three runs.

cess to. Unsurprisingly, the improvement of CVT
over purely supervised learning grows larger as the
amount of labeled data decreases (see Figure 5,
left). Using only 25% of the labeled data, our ap-
proach already performs as well or better than a
fully supervised model using 100% of the training
data, demonstrating that CVT is particularly use-
ful on small datasets.

Training Larger Models. Most sequence taggers
and dependency parsers in prior work use small
LSTMs (hidden state sizes of around 300) because
larger models yield little to no gains in perfor-
mance (Reimers and Gurevych, 2017). We found
our own supervised approaches also do not ben-
efit greatly from increasing the model size. In
contrast, when using CVT accuracy scales better
with model size (see Figure 5, right). This finding
suggests the appropriate semi-supervised learning
methods may enable the development of larger,
more sophisticated models for NLP tasks with lim-
ited amounts of labeled data.

Generalizable Representations. Lastly, we ex-
plore training the CVT+multi-task model on five
tasks, freezing the encoder, and then only training
a prediction module on the sixth task. This tests
whether the encoder’s representations generalize
to a new task not seen during its training. Only
training the prediction module is very fast because
(1) the encoder (which is by far the slowest part of
the model) has to be run over each example only
once and (2) we do not back-propagate into the
encoder. Results are shown in Table 4.

Training only a prediction module on top of
multi-task representations works remarkably well,

Model CCG Chnk NER FGN POS Dep.

Supervised 94.8 95.6 95.0 86.0 97.59 92.9
CVT-MT frozen 95.1 96.6 94.6 83.2 97.66 92.5
ELMo frozen 94.3 92.2 91.3 80.6 97.50 89.4

Table 4: Comparison of single-task models on the dev
sets. “CVT-MT frozen” means we pretrain a CVT +
multi-task model on five tasks, and then train only the
prediction module for the sixth. “ELMo frozen” means
we train prediction modules (but no LSTMs) on top of
ELMo embeddings.

outperforming ELMo embeddings and sometimes
even a vanilla supervised model, showing the
multi-task model is building up effective repre-
sentations for language. In particular, the repre-
sentations could be used like skip-thought vectors
(Kiros et al., 2015) to quickly train models on new
tasks without slow representation learning.

5 Related Work

Unsupervised Representation Learning. Early
approaches to deep semi-supervised learning pre-
train neural models on unlabeled data, which has
been successful for applications in computer vi-
sion (Jarrett et al., 2009; LeCun et al., 2010) and
NLP. Particularly noteworthy for NLP are al-
gorithms for learning effective word embeddings
(Collobert et al., 2011; Mikolov et al., 2013; Pen-
nington et al., 2014) and language model pretrain-
ing (Dai and Le, 2015; Ramachandran et al., 2017;
Peters et al., 2018; Howard and Ruder, 2018; Rad-
ford et al., 2018). Pre-training on other tasks
such as machine translation has also been stud-
ied (McCann et al., 2017). Other approaches train



1922

“thought vectors” representing sentences through
unsupervised (Kiros et al., 2015; Hill et al., 2016)
or supervised (Conneau et al., 2017) learning.

Self-Training. One of the earliest approaches
to semi-supervised learning is self-training (Scud-
der, 1965), which has been successfully applied
to NLP tasks such as word-sense disambiguation
(Yarowsky, 1995) and parsing (McClosky et al.,
2006). In each round of training, the classifier,
acting as a “teacher,” labels some of the unlabeled
data and adds it to the training set. Then, acting as
a “student,” it is retrained on the new training set.
Many recent approaches (including the consisten-
tency regularization methods discussed below and
our own method) train the student with soft tar-
gets from the teacher’s output distribution rather
than a hard label, making the procedure more akin
to knowledge distillation (Hinton et al., 2015). It
is also possible to use multiple models or predic-
tion modules for the teacher, such as in tri-training
(Zhou and Li, 2005; Ruder and Plank, 2018).

Consistency Regularization. Recent works add
noise (e.g., drawn from a Gaussian distribution)
or apply stochastic transformations (e.g., horizon-
tally flipping an image) to the student’s inputs.
This trains the model to give consistent predictions
to nearby data points, encouraging distributional
smoothness in the model. Consistency regular-
ization has been very successful for computer vi-
sion applications (Bachman et al., 2014; Laine and
Aila, 2017; Tarvainen and Valpola, 2017). How-
ever, stochastic input alterations are more difficult
to apply to discrete data like text, making consis-
tency regularization less used for natural language
processing. One solution is to add noise to the
model’s word embeddings (Miyato et al., 2017);
we compare against this approach in our experi-
ments. CVT is easily applicable to text because it
does not require changing the student’s inputs.

Multi-View Learning. Multi-view learning on
data where features can be separated into distinct
subsets has been well studied (Xu et al., 2013).
Particularly relevant are co-training (Blum and
Mitchell, 1998) and co-regularization (Sindhwani
and Belkin, 2005), which trains two models with
disjoint views of the input. On unlabeled data,
each one acts as a “teacher” for the other model.
In contrast to these methods, our approach trains
a single unified model where auxiliary prediction
modules see different, but not necessarily indepen-

dent views of the input.

Self Supervision. Self-supervised learning meth-
ods train auxiliary prediction modules on tasks
where performance can be measured without
human-provided labels. Recent work has jointly
trained image classifiers with tasks like relative
position and colorization (Doersch and Zisserman,
2017), sequence taggers with language modeling
(Rei, 2017), and reinforcement learning agents
with predicting changes in the environment (Jader-
berg et al., 2017). Unlike these approaches, our
auxiliary losses are based on self-labeling, not la-
bels deterministically constructed from the input.

Multi-Task Learning. There has been extensive
prior work on multi-task learning (Caruana, 1997;
Ruder, 2017). For NLP, most work has focused
on a small number of closely related tasks (Lu-
ong et al., 2016; Zhang and Weiss, 2016; Søgaard
and Goldberg, 2016; Peng et al., 2017). Many-
task systems are less commonly developed. Col-
lobert and Weston (2008) propose a many-task
system sharing word embeddings between the
tasks, Hashimoto et al. (2017) train a many-task
model where the tasks are arranged hierarchically
according to their linguistic level, and Subrama-
nian et al. (2018) train a shared-encoder many-task
model for the purpose of learning better sentence
representations for use in downstream tasks, not
for improving results on the original tasks.

6 Conclusion

We propose Cross-View Training, a new method
for semi-supervised learning. Our approach al-
lows models to effectively leverage their own pre-
dictions on unlabeled data, training them to pro-
duce effective representations that yield accurate
predictions even when some of the input is not
available. We achieve excellent results across
seven NLP tasks, especially when CVT is com-
bined with multi-task learning.

Acknowledgements

We thank Abi See, Christopher Clark, He He,
Peng Qi, Reid Pryzant, Yuaho Zhang, and the
anonymous reviewers for their thoughtful com-
ments and suggestions. We thank Takeru Miyato
for help with his virtual adversarial training code
and Emma Strubell for answering our questions
about OntoNotes NER. Kevin is supported by a
Google PhD Fellowship.



1923

References
Philip Bachman, Ouais Alsharif, and Doina Precup.

2014. Learning with pseudo-ensembles. In NIPS.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
ACM.

Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41–75.

Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, Roldano Cattoni, and Marcello Federico.
2015. The IWSLT 2015 evaluation campaign. In In-
ternational Workshop on Spoken Language Transla-
tion.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2014. One billion word benchmark for mea-
suring progress in statistical language modeling. In
INTERSPEECH.

Jason PC Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional LSTM-CNNs. Trans-
actions of the Association for Computational Lin-
guistics.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In NIPS.

Carl Doersch and Andrew Zisserman. 2017. Multi-
task self-supervised visual learning. arXiv preprint
arXiv:1708.07860.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In ICLR.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5):602–610.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple nlp
tasks. In EMNLP.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In HLT-NAACL.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn treebank. Com-
putational Linguistics, 33(3):355–396.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In HLT-NAACL.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
ACL.

Max Jaderberg, Volodymyr Mnih, Wojciech Marian
Czarnecki, Tom Schaul, Joel Z Leibo, David Sil-
ver, and Koray Kavukcuoglu. 2017. Reinforcement
learning with unsupervised auxiliary tasks. In ICLR.

Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al.
2009. What is the best multi-stage architecture for
object recognition? In IEEE Conference on Com-
puter Vision.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In EMNLP.

James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ra-
malho, Agnieszka Grabska-Barwinska, Demis Has-
sabis, Claudia Clopath, Dharshan Kumaran, and
Raia Hadsell. 2017. Overcoming catastrophic for-
getting in neural networks. Proceedings of the Na-
tional Academy of Sciences of the United States of
America, 114 13:3521–3526.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In ICLR.

Yann LeCun, Koray Kavukcuoglu, and Clément Fara-
bet. 2010. Convolutional networks and applications
in vision. In ISCAS. IEEE.

Zhizhong Li and Derek Hoiem. 2016. Learning with-
out forgetting. In ECCV.

Minh-Thang Luong, Eugene Brevdo, and Rui Zhao.
2017. Neural machine translation (seq2seq) tutorial.
https://github.com/tensorflow/nmt.



1924

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In ICLR.

Minh-Thang Luong and Christopher D. Manning.
2015. Stanford neural machine translation systems
for spoken language domains. In IWSLT.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional LSTM-CNN-
CRF. In ACL.

Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,
Graham Neubig, and Eduard Hovy. 2018. Stack-
pointer networks for dependency parsing. In ACL.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The Penn treebank. Computa-
tional linguistics, 19(2):313–330.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NIPS.

David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
ACL.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In NIPS.

Takeru Miyato, Andrew M Dai, and Ian Goodfel-
low. 2017. Adversarial training methods for semi-
supervised text classification. In ICLR.

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama,
Ken Nakae, and Shin Ishii. 2016. Distributional
smoothing with virtual adversarial training. In
ICLR.

Hao Peng, Sam Thomson, and Noah A. Smith. 2017.
Deep multitask learning for semantic dependency
parsing. In ACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Matthew E Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In ACL.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever. 2018. Improving lan-
guage understanding by generative pre-training.
https://blog.openai.com/language-unsupervised.

Prajit Ramachandran, Peter J Liu, and Quoc V Le.
2017. Unsupervised pretraining for sequence to se-
quence learning. In EMNLP.

Marek Rei. 2017. Semi-supervised multitask learning
for sequence labeling. In ACL.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of LSTM-networks for sequence tagging. In
EMNLP.

Sebastian Ruder. 2017. An overview of multi-task
learning in deep neural networks. arXiv preprint
arXiv:1706.05098.

Sebastian Ruder and Barbara Plank. 2018. Strong
baselines for neural semi-supervised learning under
domain shift. In ACL.

Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tas-
dizen. 2016. Regularization with stochastic
transformations and perturbations for deep semi-
supervised learning. In NIPS.

H Scudder. 1965. Probability of error of some adap-
tive pattern-recognition machines. IEEE Transac-
tions on Information Theory, 11(3):363–371.

Vikas Sindhwani and Mikhail Belkin. 2005. A co-
regularization approach to semi-supervised learning
with multiple views. In ICML Workshop on Learn-
ing with Multiple Views.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In ACL.

Emma Strubell, Patrick Verga, David Belanger, and
Andrew McCallum. 2017. Fast and accurate se-
quence labeling with iterated dilated convolutions.
In EMNLP.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
large scale multi-task learning. In ICLR.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

Antti Tarvainen and Harri Valpola. 2017. Weight-
averaged consistency targets improve semi-
supervised deep learning results. In Workshop on
Learning with Limited Labeled Data, NIPS.

Erik F Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In CoNLL.



1925

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
HLT-NAACL.

Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing
Gong. 2018. Improving the improved training of
Wasserstein GANs. In ICLR.

Huijia Wu, Jiajun Zhang, and Chengqing Zong.
2017. Shortcut sequence tagging. arXiv preprint
arXiv:1701.00576.

Chang Xu, Dacheng Tao, and Chao Xu. 2013. A
survey on multi-view learning. arXiv preprint
arXiv:1304.5634.

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL.

Yuan Zhang and David Weiss. 2016. Stack-
propagation: Improved representation learning for
syntax. In ACL.

Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex-
ploiting unlabeled data using three classifiers. IEEE
Transactions on knowledge and Data Engineering.


