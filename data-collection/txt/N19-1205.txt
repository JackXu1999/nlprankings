



















































Improving Neural Machine Translation with Neural Syntactic Distance


Proceedings of NAACL-HLT 2019, pages 2032–2037
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2032

Improving Neural Machine Translation with Neural Syntactic Distance

Chunpeng Ma1,3, Akihiro Tamura2, Masao Utiyama3, Tiejun Zhao1, Eiichiro Sumita3
1Harbin Institute of Technology, Harbin, China

2Ehime University, Matsuyama, Japan
3National Institute of Information and Communications Technology, Kyoto, Japan
{cpma, tjzhao}@hit.edu.cn tamura@cs.ehime-u.ac.jp

{mutiyama, eiichiro.sumita}@nict.go.jp

Abstract

The explicit use of syntactic information has
been proved useful for neural machine trans-
lation (NMT). However, previous methods re-
sort to either tree-structured neural networks
or long linearized sequences, both of which are
inefficient. Neural syntactic distance (NSD)
enables us to represent a constituent tree us-
ing a sequence whose length is identical to the
number of words in the sentence. NSD has
been used for constituent parsing, but not in
machine translation. We propose five strate-
gies to improve NMT with NSD. Experiments
show that it is not trivial to improve NMT
with NSD; however, the proposed strategies
are shown to improve translation performance
of the baseline model (+2.1 (En–Ja), +1.3 (Ja–
En), +1.2 (En–Ch), and +1.0 (Ch–En) BLEU).

1 Introduction

In recent years, neural machine translation (NMT)
has been developing rapidly and has become the
de facto approach for machine translation. To im-
prove the performance of the conventional NMT
models (Sutskever et al., 2014; Bahdanau et al.,
2014), one effective approach is to incorporate
syntactic information into the encoder and/or de-
coder of the baseline model.

Based on how the syntactic information is
represented, there are two categories of syn-
tactic NMT methods: (1) those that use tree-
structured neural networks (NNs) to represent syn-
tax structures (Eriguchi et al., 2016; Hashimoto
and Tsuruoka, 2017), and (2) those that use
linear-structured NNs to represent linearized syn-
tax structures (Li et al., 2017; Ma et al., 2017,
2018). For the first category, there is a direct
corresponding relationship between the syntactic
structure and the NN structure, but the complex-
ity of NN structures usually makes training in-
efficient. In contrast, for the second category,

syntactic structures are linearized and represented
using linear-structured recurrent neural networks
(RNNs), but the linearized sequence can generally
be quite long and therefore training efficiency is
still a problem. Although using a shorter sequence
may improve the efficiency, some syntactic infor-
mation is lost.

We propose a method of using syntactic infor-
mation in NMT that overcomes the disadvantages
of both methods. The basis of our method is the
neural syntactic distance (NSD), a recently pro-
posed concept used for constituent parsing (Shen
et al., 2018; Gómez-Rodrı́guez and Vilares, 2018).
NSD makes it possible to represent a constituent
tree as a sequence whose length is identical to the
number of words in the sentence (almost) without
losing syntactic information. However, there are
no previous studies that use NSD in NMT. More-
over, as demonstrated by our experiments, using
NSD in NMT is far from straightforward, so we
propose five strategies and verify the effects em-
pirically. The strategies are summarized below.

• Extend NSD to dependency trees, which is
inspired by the dependency language model
(Shen et al., 2010).
• Use NSDs as input sequences1, where an

NSD is regarded as a linguistic input feature
(Sennrich and Haddow, 2016).
• Use NSDs as output sequences, where the

NMT and prediction of the NSD are simul-
taneously trained through multi-task learning
(Firat et al., 2016).
• Use NSD as positional encoding (PE), which

is a syntactic extension of the PE of the
Transformer (Vaswani et al., 2017).
• Add a loss function for NSD to achieve
1Throughout this paper, ”input” means the input of an en-

coder or a decoder rather than the input of the NMT model
(i.e., only source sentences), and ”output” is similar.



2033

distance-aware training (Shen et al., 2018).

2 Neural Syntactic Distance (NSD)

The NSD was firstly proposed by Shen et al.
(2018). This is the first method of linearizing a
constituent tree with a sequence of length n, with-
out loss of information, where n is the number of
words in the sentence.

Formally, given the sentence w =
(w1, . . . , wn), for any pairs of contiguous
words (wi, wi+1), we can define an NSD d(wi),2

where i ∈ [1, n − 1]. In Shen et al. (2018),
the NSD dS(wi) is defined as the height of the
lowest common ancestor (LCA) of the words.3

Subsequently, in Gómez-Rodrı́guez and Vilares
(2018), the NSD dG(wi) was defined as the
number of the common ancestors of the words.
To make the definition complete, we define d(wn)
as follows:4

dS(wn) = H, dG(wn) = 0, (1)

where H is the height of the constituent tree. It is
easy to prove that

dS(wi) + dG(wi) = H, i ∈ [1, n]. (2)

We call dS and dG the absolute NSD.
Furthermore, Gómez-Rodrı́guez and Vilares

(2018) define the relative NSD as follows:

dR(wi) =

{
dG(w1), i = 1,

dG(wi)− dG(wi−1), i ∈ [2, n].
(3)

Figure 1 illustrates these NSDs. It is easy
to see the one-to-one correspondence relationship
between the constituent tree and the (absolute or
relative) NSDs.

The effectiveness of all different NSDs has been
proven on constituent parsing. However, there has
been no attempt to use NSD in machine transla-
tion.

2Note that NSD is defined between two contiguous words.
For convenience of notation, we use d(wi) rather than
d(wi, wi+1) to denote an NSD.

3In Shen et al. (2018), NSD is defined as a real number
that is a function of LCA. However, in practice, NSD is sim-
ply identical to the depth of the LCA.

4dS(wn) and dG(wn) are undefined in both of the origi-
nal papers. We give the definitions here to enable the use of
NSD in NMT later.

She

PRP

enjoys

VBZ

playing

VBG

tennis

NN

.

.

NP

VP

VP

S

NP

S’

4 2 1 3 5dS
1 3 4 2 0dG
1 2 1 -2 -2dR

Figure 1: Example of different NSDs. This example is
from Shen et al. (2018).

She enjoys playing tennis .#
-1 2 1 1 3dD

Figure 2: Example of dependency NSDs. “#” is the
root. Dependency labels are omitted.

3 Strategies to improve NMT with NSD

3.1 Dependency NSD

There are many previous studies on using depen-
dency trees to improve NMT (Nguyen Le et al.,
2017; Wu et al., 2017). Therefore, we extend NSD
to dependency trees. Formally, the dependency
NSD between two nodes is defined as follows:

dD(wi) = i− h(i), (4)

where h(i) is the index of the head of wi, and we
let the index of root be 0. Note that dD(wi) can be
either positive or negative, representing the direc-
tional information. Figure 2 gives an example.

3.2 NSDs as Input Sequences

It is easy to see that for w = (w1, . . . , wn), the
lengths of dS , dG, dR and dD are all n. Denoting
the NSD sequence as d = (d1, . . . , dn), we can
see that di ∈ Z, i ∈ [1, n], so we can obtain a
sequence of embedding vectors ed = (ed1, . . . , e

d
n)

as follows:

edi = Ed[d
d
i + (max(d)−min(d) + 1)]. (5)

We callEd the distance embedding matrix and call
ed the syntactic embedding sequence. Note that d
can be the NSD on either the source side or the tar-
get side, so there are two possible Ed, which are



2034

denoted as Esd and E
t
d, respectively. The embed-

dings are calculated as follows:

xsi = femb(E
s
w[w

s
i ], e

ds
i ), (6)

xti = femb(E
t
w[w

t
i ], e

dt
i ), (7)

where edsi and e
dt
i are defined in Eq. 5 on the

source side and target side, respectively, and Esw
and Etw are the word embedding matrices on both
sides, respectively. Inspired by Sennrich and Had-
dow (2016), function femb is used to combine two
vectors. This function has many different options,
such as:

f
‖
emb(x, e) = x‖e, (8)
f+emb(x, e) = x+ e, (9)

fWbemb(x, e) =Wf · (x‖e) + bf , (10)

where x, e, bf ∈ Rd and Wf ∈ Rd×2d. The opera-
tor “‖” is the concatenation of two vectors.

When NSD is used as the input sequence on the
target side, there is one problem: edt is unknown
during testing. For this case, we use NSDs for
both the input and output sequences, let the de-
coder predict NSD on-the-fly using the strategy
introduced in Section 3.3, and use the predicted
NSD to calculate edt.

3.3 NSDs as Output Sequences
An NSD can be used to form the output sequence
to improve NMT using the idea of multi-task
learning. Specifically, we train the model to pre-
dict the NSD sequence. When NSD is used as the
output sequence of the encoder, we minimize the
distance (e.g., cross entropy Lentdist, see Section 3.5
for details) between the predicted and the golden
NSD sequences. When NSD is used as the output
sequence of the decoder, besides minimizing the
distance, we use the predicted NSD as the input of
the next time step.

Denote the hidden vector as h = (h1, . . . , hn).
For the encoder, hi = hsi and n = ns, while for
the decoder, hi = hti and n is the current time
step of decoding. Then, we can obtain a sequence
of predicted syntactic distance d̂ = (d̂1, . . . , d̂n),
which is calculated as follows:

p(d̂i | hi) = softmax(Wd · hi + bd), (11)

where Wd and bd are parameters to be learned. By
minimizing the distance between d̂i and di, NSD
can be used to enhance NMT.

3.4 NSD as Positional Encoding (PE)
PE is used by the Transformer (Vaswani et al.,
2017) to encode the positions of words. Formally,
it is defined as follows:

x′i = xi + PE(i), (12)

PE(i)2k = sin(i/10000
2k/d), (13)

PE(i)2k+1 = cos(i/10000
2k/d), (14)

where xi can be either xsi or x
t
i, and d is the di-

mension of the embedding vector. Similarly, we
define syntactic PE as follows:

PE(i)2k = sin
( i+max(d)−min(d)

λSPE
2k/d

· 2π
)
,

(15)

PE(i)2k+1 = cos
( i+max(d)−min(d)

λSPE
2k/d

· 2π
)
,

(16)

where λSPE is a hyperparameter to be tuned. In
this way, the periods of these two functions vary
from 1 to λSPE . We define syntactic PE in this
way because (1) according to a quantitative anal-
ysis of the experimental datasets, we found that
the ranges of possible values are quite different be-
tween NSD and word positions, so we tuned λSPE
instead of fixed it to 10000 as in Eqs. 13 and 14,
and (2) di may be negative, so we adjust it to be
positive.

3.5 Distance-aware Training
Instead of using conventional cross-entropy loss
function during training, we use the following loss
function to make the NMT model learn NSD bet-
ter:

L = LNMT + Ldist + Lentdist. (17)
The first item is the cross-entropy loss of the NMT
model, which is

LNMT = −
∑

〈ws,wt〉∈D

log p(wt | ws), (18)

whereD is the training dataset. The second item is
the distance-aware loss, which is inspired by Shen
et al. (2018) and is as follows:

Ldist =
∑

〈ws,wt〉∈D

(Lsdist(ws) + Ltdist(wt)),

Lsdist(ws) =
ns∑
i=1

(di − d̂i)2+∑
i,j>i

[1− sign(di − dj)(d̂i − d̂j)]+,

(19)



2035

and Ltdist can be defined similarly. The third item
is the cross-entropy loss for NSD, which is as fol-
lows:

Lentdist =
∑

〈ws,wt〉∈D

(Lent(s)dist (w
s) + Lent(t)dist (w

t)),

Lent(s)dist (w
s) = −

∑
di∈ds

p(di | hi) log p(d̂i | hi),

(20)
and Lent(t)dist can be defined similarly.

4 Experiments

4.1 Configuration
We experimented on two corpora: (1) ASPEC
(Nakazawa et al., 2016), using the top 100K sen-
tence pairs for training En–Ja models and top 1M
sentence pairs for training Ja–En models, and (2)
LDC,5 which contains about 1.2M sentence pairs,
for training En–Ch and Ch–En models. To tack-
ling the problem of memory consumption, sen-
tences longer than 150 were filtered out, so that
models can be trained successfully. Chinese sen-
tences were segmented by the Stanford segmen-
tation tool.6 For Japanese sentences, we fol-
lowed the preprocessing steps recommended in
WAT 2017.7

The test set is a concatenation of NIST MT
2003, 2004, and 2005. Constituent trees are gen-
erated by the parser of Kitaev and Klein (2018)8,
and dependency trees are generated by the parser
of Dyer et al. (2015)9. Note that although we only
used syntactic information of English in our ex-
periments, our method is also applicable to other
languages.

We implemented our method on OpenNMT10

(Klein et al., 2017), and used the Transformer as
our baseline. As far as we know, there are no pre-
vious studies on using syntactic informations in
the Transformer.

The vocabulary sizes for all languages are
50, 000. Both the encoder and decoder have 6 lay-
ers. The dimensions of hidden vectors and word
embeddings are 512. The multi-head attention has

5LDC2002E18, LDC2003E07, LDC2003E14, Hansards
portion of LDC2004T07, LDC2004T08, and LDC2005T06.

6https://nlp.stanford.edu/software/
stanford-segmenter-2017-06-09.zip

7http://lotus.kuee.kyoto-u.ac.jp/WAT/
WAT2017/baseline/dataPreparationJE.html

8https://github.com/nikitakit/
self-attentive-parser

9https://github.com/clab/lstm-parser
10http://opennmt.net

8 heads, and the dropout probability is 0.1 (Srivas-
tava et al., 2014). The number of training epochs
was fixed to 50, and we used the model which per-
forms the best on the development set for testing.

As for optimization, we used the Adam op-
timizer (Kingma and Ba, 2014), with β1 =
0.9, β2 = 0.998, and � = 10−9. Warmup
and decay strategy for learning rate of Vaswani
et al. (2017) are also used, with 8, 000 warmup
steps. We also used the label smoothing strategy
(Szegedy et al., 2016) with �ls = 0.1.

4.2 Experimental Results

Table 1 compares the effects of the strategies. We
evaluate the proposed strategies using character-
level BLEU (Papineni et al., 2002) for Chinese and
Japanese, and case-insensitive BLEU for English.

Comparison of different NSDs. The first five
rows of Table 1 compare the results of using dif-
ferent NSDs. When NSD was used at the source
side (En–Ja/En–Ch), all kinds of NSDs improved
translation performance. This indicates that NSD
can be regarded as a useful linguistic feature to im-
prove NMT. In contrast, when NSD was used at
the target side (Ja–En/Ch–En), dS and dG hurt the
performance. This is because the values of dS and
dG are volatile. A tiny change of syntactic struc-
ture often causes a big change of dS and dG. Since
the model has to predict the NSD during decoding,
once there is one error, the subsequent predictions
will be heavily influenced. The use of dR and dD
remedies this problem. Furthermore, the effects of
dS and dG are similar, because they are equivalent
in nature (refer to Eq. 2).

NSD as PE. Rows 5 to 8 of Table 1 evaluate
the use of dependency NSD (dD) as syntactic PE.
Note that for all the experiments, we used not only
the syntactic PE but the conventional PE. Experi-
ment results show that this strategy is indeed use-
ful. When the dominators of Eqs. 15 and 16,
λSPE , were set to 104, there was no improvement.
When they were set to 40, the improvement was
remarkable. This indicates that our design of syn-
tactic PE is reasonable.

NSD as input/output and source/target se-
quences. Rows 8 to 12 of Table 1 are the results
of using dependency NSD (i.e., dD) as the input
and/or output sequences on both sides. First, for
the choice of femb, we can see that f

‖
emb and f

+
emb

are similar, while fWbemb yields better performance.
This is because the model has to learn Wf and

https://nlp.stanford.edu/software/stanford-segmenter-2017-06-09.zip
https://nlp.stanford.edu/software/stanford-segmenter-2017-06-09.zip
http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/baseline/dataPreparationJE.html
http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/baseline/dataPreparationJE.html
https://github.com/nikitakit/self-attentive-parser
https://github.com/nikitakit/self-attentive-parser
https://github.com/clab/lstm-parser
http://opennmt.net


2036

Type I/O SPE Loss En–Ja Ja–En En–Ch Ch–En
1 N/A N/A No Eq. 18 34.59 26.43 29.41 31.60
2 dS I(fWbemb) No Eq. 18,19,20 35.54 24.57 29.66 28.77
3 dG I(fWbemb) No Eq. 18,19,20 35.38 24.71 29.60 28.82
4 dR I(fWbemb) No Eq. 18,19,20 35.83 26.88 29.87 31.82
5 dD I(fWbemb) No Eq. 18,19,20 36.17 27.21 30.11 32.08
6 dD I(fWbemb) 10

4 Eq. 18,19,20 36.06 27.18 30.02 32.03
7 dD I(fWbemb) 10

2 Eq. 18,19,20 36.22 27.47 30.23 32.19
8 dD I(fWbemb) 40 Eq. 18,19,20 36.44 27.59 30.59 32.36
9 dD I(f

‖
emb) 40 Eq. 18,19,20 36.17 27.21 30.15 32.11

10 dD I(f+emb) 40 Eq. 18,19,20 36.08 27.32 30.21 32.29
11 dD O 40 Eq. 18,19,20 36.31 27.42 30.42 32.32
12 dD I(fWbemb)&O 40 Eq. 18,19,20 36.69 27.71 30.56 32.55
13 dD O 40 Eq. 18 21.08 10.22 18.63 15.61
14 dD O 40 Eq. 18,20 33.70 23.31 27.43 30.02
15 dD O 40 Eq. 18,19 34.18 25.19 29.14 31.74

Table 1: Comparison of strategies. I/O: use NSDs as the input or output sequences. Functions f‖emb, f
+
emb, and

fWbemb are defined in Eqs. 8 to 10, respectively. SPE: use NSD as syntactic PE. Numbers are the values of λSPE in
Eqs. 15 and 16. Loss: items used in the final loss function. Note that when NSD is used as the input sequence of
the source language, Ldist + Lentdist ≡ 0, because the parsing tree is fixed.

bf , which increases the model capacity. Second,
performance improved for using NSDs both as
input and output sequences, and combining both
obtained further improvement. Third, NSDs im-
proved the performance both on the source and the
target sides. All these results indicate the robust-
ness of NSDs.

Effects of distance-aware training. The last
three rows compare the different effects of the
items in the loss function. When only LNMT are
used, the performance is extremely poor. This
is within expectations, because with only LNMT ,
weights related to NSDs are kept to the initial val-
ues and were not updated, and hence detrimental
to learning. Adding Lentdist improves the results sig-
nificantly, but the improvement is lower than that
of Ldist. This is because training with Lentdist treats
different values of NSDs equally, while Ldist pe-
nalizes larger differences between the predicted
NSD and the golden NSD more severely.

5 Conclusion

We proposed five strategies to improve NMT with
NSD. We found relative NSDs and dependency
NSDs are able to improve the performance consis-
tently, while absolute NSDs hurt the performance
for some cases. The improvement obtained by us-
ing NSDs is general in that NSDs can be used at
both the source side and target side, both as in-
put sequences and output sequences. Using NSDs
as syntactic PE is also useful, and training with a
distance-aware loss function is quite important.

Acknowledgements

We are grateful to the anonymous reviewers
for their insightful comments and suggestions.
Akihiro Tamura is supported by JSPS KAK-
ENHI Grant Number JP18K18110. Tiejun Zhao
is supported by the National Key Research
and Development Program of China via grant
2017YFB1002102.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
823–833.

Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,
Fatos T. Yarman Vural, and Kyunghyun Cho. 2016.
Zero-resource translation with multi-lingual neural
machine translation. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 268–277.



2037

Carlos Gómez-Rodrı́guez and David Vilares. 2018.
Constituent parsing as sequence labeling. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1314–
1324.

Kazuma Hashimoto and Yoshimasa Tsuruoka. 2017.
Neural machine translation with source-side latent
graph parsing. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 125–135.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Nikita Kitaev and Dan Klein. 2018. Constituency
parsing with a self-attentive encoder. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2676–2686.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-Source Toolkit for Neural Machine
Translation. arXiv preprint arXiv:1701.02810.

Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min
Zhang, and Guodong Zhou. 2017. Modeling source
syntax for neural machine translation. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 688–697.

Chunpeng Ma, Lemao Liu, Akihiro Tamura, Tiejun
Zhao, and Eiichiro Sumita. 2017. Deterministic at-
tention for sequence-to-sequence constituent pars-
ing. In Proc. of AAAI-2017, pages 3237–3243.

Chunpeng Ma, Akihiro Tamura, Masao Utiyama,
Tiejun Zhao, and Eiichiro Sumita. 2018. Forest-
based neural machine translation. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1253–1263.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. ASPEC:
Asian scientific paper excerpt corpus. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC 2016),
pages 2204–2208.

An Nguyen Le, Ander Martinez, Akifumi Yoshimoto,
and Yuji Matsumoto. 2017. Improving sequence
to sequence neural machine translation by utilizing
syntactic dependency information. In Proceedings
of the Eighth International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 21–29.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311–318.

Rico Sennrich and Barry Haddow. 2016. Linguistic
input features improve neural machine translation.
In Proceedings of the First Conference on Machine
Translation: Volume 1, Research Papers, pages 83–
91.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4).

Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessan-
dro Sordoni, Aaron Courville, and Yoshua Bengio.
2018. Straight to the tree: Constituency parsing
with neural syntactic distance. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1171–1180.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of machine learning re-
search, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethink-
ing the inception architecture for computer vision.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2818–2826.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li,
and Ming Zhou. 2017. Sequence-to-dependency
neural machine translation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
698–707.


