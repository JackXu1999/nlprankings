



















































Probing What Different NLP Tasks Teach Machines about Function Word Comprehension


Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 235–249
Minneapolis, June 6–7, 2019. c©2019 Association for Computational Linguistics

235

Probing What Different NLP Tasks Teach Machines
about Function Word Comprehension

Najoung Kim†,∗, Roma Patelφ, Adam Poliak†, Alex Wang∂ ,
Patrick Xia†, R. Thomas McCoy†,Ian Tenney∆, Alexis Ross�,

Tal Linzen†, Benjamin Van Durme†, Samuel R. Bowman∂ , Ellie Pavlickφ,∗
†Johns Hopkins University φBrown University

∂New York University ∆Google AI Language �Harvard University

Abstract

We introduce a set of nine challenge tasks that
test for the understanding of function words.
These tasks are created by structurally mutat-
ing sentences from existing datasets to target
the comprehension of specific types of func-
tion words (e.g., prepositions, wh-words). Us-
ing these probing tasks, we explore the ef-
fects of various pretraining objectives for sen-
tence encoders (e.g., language modeling, CCG
supertagging and natural language inference
(NLI)) on the learned representations. Our re-
sults show that pretraining on CCG—our most
syntactic objective—performs the best on av-
erage across our probing tasks, suggesting that
syntactic knowledge helps function word com-
prehension. Language modeling also shows
strong performance, supporting its widespread
use for pretraining state-of-the-art NLP mod-
els. Overall, no pretraining objective domi-
nates across the board, and our function word
probing tasks highlight several intuitive differ-
ences between pretraining objectives, e.g., that
NLI helps the comprehension of negation.

1 Introduction

Many recent advances in NLP have been driven by
new approaches to representation learning—i.e.,
the design of models whose primary aim is to yield
representations of words or sentences that useful
for a range of downstream applications (Bowman
et al., 2017). Approaches to representation learn-
ing typically differ in either the architecture of the
model used to learn the representations, the objec-
tive used to train that network, or both. Varying
these factors can significantly impact performance
on a broad range of NLP tasks (McCann et al.,
2017; Peters et al., 2018; Devlin et al., 2019).

∗Corresponding authors: Najoung Kim (n.kim@jhu.edu),
Ellie Pavlick (ellie pavlick@brown.edu)

This paper investigates the role of pretraining
objectives of sentence encoders, with respect to
their capacity to understand function words (e.g.,
prepositions, conjunctions). Although the impor-
tance of finding an effective pretraining objective
for learning better (or more generalizable) repre-
sentations is well acknowledged, relatively few
studies offer a controlled comparison of diverse
pretraining objectives, holding model architecture
constant.

We ask whether the linguistic properties im-
plicitly captured by pretraining objectives measur-
ably affect the types of linguistic information en-
coded in the learned representations. To this end,
we explore whether qualitatively different objec-
tives lead to demonstrably different sentence rep-
resentations. We focus our analysis on function
words because they play a key role in composi-
tional meaning—e.g., introducing and identifying
discourse referents or representing relationships
between entities or ideas—and are not yet con-
sidered to be well-modeled by distributional se-
mantics (Bernardi et al., 2015). Our results sug-
gest that different pretraining objectives give rise
to differences in function word comprehension;
for instance, we see that natural language infer-
ence helps understanding negation, and grounded
language helps understanding spatial descriptors.
However, overall, we find that the observed dif-
ferences are not always straightforwardly inter-
pretable, and further investigation is needed to de-
termine what specific aspects of pretraining tasks,
yield good representations of function words.

The analyses we present contribute new results
in an ongoing line of research aimed at providing
a finer-grained understanding of what neural net-
works capture about linguistic structure (Conneau
et al., 2018; Poliak et al., 2018b; Linzen et al.,
2018; Tenney et al., 2019, i.a.). Our contributions
are:



236

Acceptability

wh-word why are you so chippy about posh people? D
. . .a Mr. Nice Guy like Melcher, what is now 46 X

Def. . . . the case is remarkable for the cooperation . . . D
. . . a case is remarkable for a cooperation . . . X

Coord. I have also tried monthly data and the results are the same. D
Rooms very clean but smelled very fresh. X

EOS the forehead is gathered in a frown // the mouth is slightly parted to reveal the teeth D
the forehead is gathered in a frown the mouth // is slightly parted to reveal the teeth X

NLI

Prep. With a single jerk the man’s head tore free. → The man’s head tore free from a single jerk. D
With a single jerk the man’s head tore free. → The man’s head tore free without a single jerk. X

Negation This is a common problem. → This is not an uncommon issue we are facing. D
This is not a common problem. → This is not an uncommon issue we are facing. X

Spatial To reach . . . turn left up a small alleyway → do not turn right up the alleyway . . . D
To reach . . . turn left up a small alleyway → Turn right up the alleyway . . . X

Quant. all taken up yeah → There are not still some left D
all taken up yeah → There are still some left X

Comp. Today there are more than 300,000. → Today there are not less than 300,000. D
Today there are more than 300,000. → Today there are less than 300,000. X

Table 1: Examples of sentences and sentence pairs corresponding to each of our probing datasets. The highlighted
words are those that are relevant to the phenomena targeted by each set.

• We provide an in-depth exploration into how
different pretraining objectives for sentence
encoders affect the information encoded by
the output representations. We isolate the
effects of different pretraining objectives by
holding the model architecture constant.

• We study function words, which have been
under-studied in previous works on represen-
tation learning, but are critical to language
understanding.

• We release nine new datasets,1 quality-
controlled by both linguists and non-linguist
annotators, to facilitate ongoing work and
follow-up analysis.

2 Function Word Probing Tasks

2.1 Approach
We introduce nine new probing tasks aimed
at evaluating models’ understanding of function
words. We focus on function words because al-
though they are key building blocks of composi-

1The datasets are released as part of the Diverse Natural
Language Inference Collection (DNC, Poliak et al., 2018b),
available at http://decomp.io.

tional meaning and are highly frequent, they have
received relatively little attention in the probing
literature and in the distributional semantics litera-
ture. Each task targets the understanding of a spe-
cific type of function word; illustrative examples
are given in Table 1. Our expectation is that dif-
ferent pretraining objectives (see Section 3.2) will
yield sentence representations which measurably
differ in their performance on these probing tasks.

We use two different formats for our prob-
ing tasks: acceptability judgment and natural lan-
guage inference (NLI). The former uses a binary
classification approach (acceptable/unacceptable)
for probing a single sentence vector, in line with
works such as Conneau et al. (2018) and Adi et al.
(2017). The latter uses an entailment-based ap-
proach similar to White et al. (2017) and Poliak
et al. (2018b), which is a ternary classification task
(entailment, contradiction, neutral) over sentence
pairs. The format is selected based on the suitabil-
ity to the particular function word type in question.

To generate our probing datasets, we make
structural modifications to sentences drawn from
existing corpora, targeting a particular type of
function word. We heuristically apply modifica-

http://decomp.io


237

tions which we believe are likely to produce a spe-
cific label, and then recruit human annotators in
order to produce the final labels used in our eval-
uations. The result is a publicly available suite
of nine task datasets (four acceptability tasks and
five NLI tasks) consisting of 3,710 annotated ex-
amples. Appendix C lists the sizes of each dataset.

2.2 Acceptability Judgment-Based Tasks
We cast acceptability as a binary classification task
following the format of such judgments commonly
used in linguistics, in a similar manner to Warstadt
et al. (2018). All tasks follow a common proto-
col of first identifying sentences that contain the
construction that we are interested in, and then
mutating half of the identified sentences to gener-
ate infelicitous versions of the original sentences.
Unless stated otherwise, the original sentences are
drawn from the test set of the Billion Word Bench-
mark (BWB, Chelba et al., 2013).

Wh-Words Understanding wh-words (i.e., who,
what, where, when, why, how) depends on under-
standing the context and correctly identifying the
antecedent, which may not be overtly present in
the sentence. For instance, recognizing the infe-
licity of I talked about who I live requires know-
ing that the (unstated) antecedent must be a place
and not a person. Our dataset consists of sentences
that contain one of the six wh-words listed above.
Half of these sentences are mutated versions of the
original which are generated by replacing the orig-
inal wh-word with a different wh-word randomly
selected from the remaining five options.

Definite-Indefinite Articles The definiteness
task probes the understanding of definiteness that
arises by the use of the definite article (the) versus
indefinite articles (a and an). We find sentences
containing multiple occurrences of the or multi-
ple occurrences of a, and, for half of them, swap
all such occurrences (i.e., replacing the with a2 or
vice-versa). This gives us four types of sentences:
unchanged sentences with multiple definite arti-
cles, unchanged sentences with multiple indefi-
nite articles, sentences with all definite articles re-
placed by the indefinite article, and sentences with
all indefinite articles replaced by the definite arti-
cle. Our intent is that the former two types will be
judged felicitous while the latter two will be infe-
licitous despite the fact that the sentence would be

2When we replace the with a, we choose a/an as neces-
sary based on the word it precedes.

syntactically well-formed. We only focus on the
cases with multiple occurrences of the same arti-
cle, because replacing a single article most of the
time did not significantly affect the acceptability
(although it often did affect the actual meaning).

Coordinating Conjunctions Correct under-
standing of coordinating conjunctions (and, but,
or) requires contextual comprehension of the two
conjoined linguistic units, since different coor-
dinating conjunctions express different logical
relations, meaning their use is often restricted by
the meanings of the conjoined items. We take
sentences that contain coordinating conjunctions,
and replace half of them with a version that
contains a different conjunction. For example,
the sentence Room’s very clean but smelled very
fresh is infelicitous despite being syntactically
well-formed; but is unnatural here because the
conjoined clauses do not form a clear contrast.
Judging this sentence to be infelicitous requires a
proper understanding of the ideas expressed in the
clauses and how they relate to each other.

End-of-Sentence The end-of-sentence (EOS)
task tests a model’s ability to identify semantically
coherent chunks (i.e., sentences) in running text.
In written text this is often indicated by punctua-
tion marks such as periods, but humans are able to
easily identify sentences even without overt mark-
ers. Thus, we take pairs of sentences from the
same paragraph of the WikiText-103 (Merity et al.,
2017) test set and remove all punctuation marks
and capitalization, and concatenate each sentence
pair to create a line of running text.3 Half of the
dataset consists of a pair of valid sentences, and
the other half consists of a pair of potentially in-
valid sentences generated from an incorrect seg-
mentation of the running text, where the incorrect
segmentation index is obtained by sampling from
a Gaussian distribution centered around the correct
index (σ = 2) and rounding to the nearest integer.

2.3 NLI-Based Tasks
Our NLI-based probing tasks ask whether the
choice of function word affects the inferences li-
censed by a sentence. These tasks consist of a pair
of sentences—a premise p and a hypothesis h—
and ask whether or not p entails h. We exploit the
label changes induced by a targeted mutation of

3We use WikiText instead of BWB because adjacent sen-
tences in BWB are not logically contiguous and therefore
may not be from the same discourse context.



238

the sentence pairs taken from the Multi-genre Nat-
ural Language Inference dataset (MNLI, Williams
et al., 2018). The rationale is that, if a change to
a single function word in the premise changes the
entailment label, that function word must play a
significant role in the semantics of the sentence.

Prepositions We manually curate a list of prepo-
sitions (see Appendix D) that are likely to be
swapped with each other without affecting the
grammaticality of the sentence. We generate mu-
tated NLI pairs by finding occurrences of the
prepositions in our list and randomly replacing
them with other prepositions in the list. Our list
consists of a set of locatives4 and several other
manually-selected prepositions that are not strictly
locatives but are likely to be substitutable (about,
for, to, with, without).

Comparatives Comparatives express qualita-
tive or quantitative differences between entities.
For instance, a sentence that states A is more than
B and another that states B is more than A lead to
different inferences. We select a list of common
comparatives (e.g., more/less, bigger/smaller) and
select pairs from MNLI that contain a compara-
tive phrase in both the premise and the hypothe-
sis. We apply several mutations to the sentences,
including negating the premise and/or hypothesis,
and swapping comparatives (e.g., replacing bigger
with smaller).5

Quantification The quantification task tests the
understanding of natural language expressions
of quantities, including common quantifiers (all,
some), number words (two, twenty), and propor-
tion (half, one-third, quarter). We select NLI
pairs that contain at least one quantifier in both the
premise and the hypothesis, and apply mutations
of negating sentences and/or replacing quantifiers
with syntactically appropriate substitutes.

Spatial Expressions The spatial expressions
task probes the understanding of words that de-
note spatial relations between entities. Changing
the spatial configuration often leads to different
inferences; for instance, A is to the left of B im-
plies that B is to the right of A, but not that A is
to the right of B. We select a set of words that

4Locative prepositions are those that denote place or po-
sition: e.g., in, on, near, etc.

5We note that Dasgupta et al. (2018) also focus on com-
paratives, but they exclusively look at artificial sentences con-
taining more/less.

describe spatial configurations which are not nec-
essarily prepositions (e.g., left, right, close, far).
Again, we find MNLI pairs containing these words
and negate/substitute to generate mutated pairs.

Negation This task probes whether models are
able to understand negations, in particular explicit
negation using the word not, lexical negation us-
ing antonyms, and the interaction between them.
We first identify premise-hypothesis pairs from the
MNLI dataset that contain antonym pairs (e.g.,
dirty appears in p and clean in h) and generate
all possible patterns of negation with the two mu-
tation strategies: swapping antonyms and adding
explicit negation. That is, we use each of lexical
negation, explicit negation, and their combination
to mutate the premise and/or the hypothesis. We
generate all 16 possible patterns of negation for a
given premise-hypothesis (p, h) pair. For each of
p and h we can either apply or not apply each of
four possible mutations: lexical negation, explicit
negation, both, and none.

2.4 Annotation

We recruit human annotators on Amazon Me-
chanical Turk to produce the final labels for the
heuristically-generated datasets described above.
We collect three labels per sentence (or per pair of
sentences for EOS and NLI probing sets). We use
the majority label in our final dataset, and discard
examples on which there is no majority consensus.
For more details about our annotation protocol, in-
cluding compensation, refer to Appendix C.

Acceptability Tasks Human annotators are pre-
sented with a single (mutated or unmutated) sen-
tence and are given the options {natural, unnatu-
ral, neither}. We discard sentences in which the
majority label does not agree with our expected
label. That is, we only include mutated sentences
with a majority label of unnatural and unmutated
sentences with a majority label of natural. We col-
lect around 500 annotated examples with balanced
label ratio for each probing set. We release our
sentences in small batches until we have approxi-
mately 250 unnatural examples per task. To cre-
ate the final dataset, we pool all answers from all
batches and take a subset of the natural sentences
so that the label ratio is balanced, prioritizing ex-
amples with perfect inter-annotator agreement.

Natural Language Inference Tasks For the
NLI tasks, we collect common-sense entailment



239

judgments from annotators on a 5-point Likert
scale on which 1 denotes ‘definitely contradiction’
and 5 denotes ‘definitely entailment’, following
Zhang et al. (2017). This finer-grained scale is in-
tended to avoid confounds arising from borderline
cases. Except for the use of scaled judgments, our
instructions follow the MNLI guidelines. Specifi-
cally, our instructions said to assume that the sen-
tences co-refer and that the first sentence (p) states
a true fact, describes a scenario, or expresses an
opinion, and to then indicate how likely it is that
the second sentence (h) is also true, describes the
same scenario, or expresses the same opinion.

Annotators could also select an option indicat-
ing that one or both of the sentences did not make
sense; we discarded (p, h) pairs for which at least
one annotator chose this option. We map judg-
ments of 5 and 4 to entailment, 3 to neutral, and 2
and 1 to contradiction, and treat the majority label
as the correct label after this mapping.

Agreement and Quality Control In construct-
ing our final evaluation sets, we removed examples
on which there was no majority consensus. For the
binary acceptability tasks, we manually prefiltered
sentences that were felicitous even after the heuris-
tic modification. For the NLI tasks, we removed
pairs that contained ungrammatical sentences that
were not flagged by annotators via manual postfil-
tering. See Appendix C for more details.

3 Experimental Design

3.1 Pretraining Architecture

Since our focus is on comparing differences in
pretraining objectives, we fix the architecture
for all sentence encoders. We use the pre-
trained character-level convolutional neural net-
work (CNN) from ELMo (Peters et al., 2018) that
replaces word embeddings (see Bowman et al.
(2018) or Tenney et al. (2019) for similar usages of
the CNN layer). This acts as a base input layer that
uses no information beyond the word, and allows
us to avoid potentially difficult issues surrounding
unknown word handling in transfer learning.

We feed the word representations to a 2-
layer 1024d bidirectional LSTM (Hochreiter and
Schmidhuber, 1997). A downstream task-specific
model sees both the top-layer hidden states of this
model and, through a skip connection, the original
representation of each word. We train a version
of this model on each task in Section 3.2. Ad-

ditional experimental details are in given in Ap-
pendix A. Our codebase is open-source6 and built
using AllenNLP (Gardner et al., 2017) and Py-
Torch (Paszke et al., 2017).

Classification Tasks For classification pretrain-
ing tasks (NLI, DisSent), we use an attention
mechanism inspired by BiDAF (Seo et al., 2017).
Given the sequence of output states of the core
BiLSTM for both sentences in an example, we
compute dot-product based attention between all
pairs of words between the sentences to form a
sequence of attention-contextualized word repre-
sentations. We use an additional BiLSTM fol-
lowed by max-pooling to obtain an attention-
contextualized vector representation of each sen-
tence h1 and h2. We use the heuristic matching
feature vector [h1;h2;h1·h2; |h1−h2|] (Mou et al.,
2016) as input to an MLP.

Sequence-to-Sequence Tasks For sequence-to-
sequence pretraining tasks (machine translation
and skip-thought), we use a single-layer 1024d
LSTM as the decoder, initialized with the max-
pooled output of the encoder. We use a linear pro-
jection bottleneck layer to reduce the dimension of
the output of the decoder by half before the output
softmax layer.

3.2 Pretraining Tasks

Our main experiments compare seven pretraining
tasks which we believe capture different aspects
of linguistic meaning and which yield reasonable
performance when used on a benchmark task such
as MNLI.7 For our purposes, a task is a dataset-
training objective pair. We attempt to select a
set of tasks diverse enough to highlight perfor-
mance differences due to pretraining objectives.
We additionally report results using BERT (De-
vlin et al., 2019) (base, uncased) to demonstrate
that our probing sets prove challenging even for
state-of-the-art models.

Language Modeling We train a left-to-right
word-level language model on BWB, which was
successfully used by Peters et al. (2018) for pre-
training sentence encoders. Because language
modeling is trivial for a bidirectional LSTM, we
follow Peters et al. (2018) by training separate
forward and backward two-layer 1024d language

6https://github.com/
jsalt18-sentence-repl/jiant

7Around 70% development set accuracy; see Appendix B.

https://github.com/jsalt18-sentence-repl/jiant
https://github.com/jsalt18-sentence-repl/jiant


240

models and concatenate their hidden states as to-
ken representations.

Skip-Thought Drawing from Kiros et al. (2015)
and Tang et al. (2017), we train a sequence-to-
sequence model on skip-thought, which is a task
of generating the next sentence in the discourse
given the previous sentence. We use the learned
encoder as our sentence encoder. Since this objec-
tive requires running text, we use sentences from
WikiText-103 as training data.

CCG Supertagging We train a model to predict
the Combinatory Categorial Grammar (CCG) su-
pertag for each word, with sentences from CCG-
Bank (Hockenmaier and Steedman, 2007). Su-
pertags are similar to part-of-speech tags but
capture more syntactic context (“almost-parsing”;
Bangalore and Joshi, 1999).

Discourse (DisSent) We train a model on Dis-
Sent (Jernite et al., 2017; Nie et al., 2017), which
is an unsupervised task of predicting the discourse
marker (e.g., and, because, or so) that connects
two clauses. We train our model on a dataset
created from WikiText-103 following Nie et al.
(2017)’s protocol, which involves extracting pairs
of clauses with a specific dependency relation.

Natural Language Inference Inspired by Con-
neau et al. (2017), we use the MNLI dataset for
NLI pretraining. The task is to predict the entail-
ment label for premise-hypothesis pairs; the pos-
sible labels are entailment, contradiction, neutral.

Machine Translation We train a sequence-to-
sequence machine translation model with attention
on WMT14 English-German (Bojar et al., 2014)
and take the encoder as our sentence encoder. Mc-
Cann et al. (2017) previously showed that pretrain-
ing an encoder on translation led to good perfor-
mance on downstream NLP tasks.

Image-Caption Matching We train a model on
the task of grounding sentences to the images they
describe. We use image-caption pairs from the
MSCOCO dataset (Lin et al., 2014) with an ob-
jective that minimizes the cosine distance between
sentence representations and corresponding image
features, as described in Kiela et al. (2018).

3.3 Classifiers for Probing Tasks

To probe the sentence encoders pretrained on the
different objectives, we freeze the weights of the

encoder after pretraining and train an additional
model using the outputs of the fixed encoder as
inputs. We describe the implementation details for
the NLI and acceptability probing sets below.

NLI Tasks For NLI-type probing, we train an
NLI model on top of the representations produced
by the pretrained sentence encoder that uses an at-
tention mechanism inspired by Seo et al. (2017)
that computes attention between all pairs of words
in the two sentences (described in more detail in
Section 3.1). We train this component on MNLI
and evaluate directly on our NLI probing datasets
with no further dataset-specific training.

Acceptability Classification Tasks For all ac-
ceptability tasks except the EOS task, we take the
sequence of hidden state outputs from the pre-
trained encoder as the sentence representation. We
aggregate this sequence into a single vector via
max-pooling and train a 512d MLP on top of the
resulting vector. For the EOS task, we also use
max-pooling on each sentence in the pair. We then
concatenate the resulting vectors and train an MLP
on top of the joint representation.8 Each task has
around 400 training examples (see Appendix C).
Due to their small size, we use 10-fold cross val-
idation where each fold is used as the test set ex-
actly once, and report the average test set accuracy.

BERT For NLI-type probing tasks, we use the
fine-tuned MNLI classifier from (Devlin et al.,
2019)9. For the acceptability classification tasks,
we fine-tune the model by adding a sequence-level
classifier on top of the pretrained BERT model.
The sequence-level classifier is a linear layer that
takes in as input the final hidden vector corre-
sponding to the first input token as aggregate rep-
resentation in the input sequence, and then classi-
fies to the required number of classes for the task,
where the label probabilities are computed with a
standard softmax. The BERT fine-tuning setup al-
lows a classification output to be indicated with a
CLS token. Pairs of sequences are indicated with
a SEP token between the pairs. All parameters are
fine-tuned jointly to maximize the log-probability

8We tried training a general acceptability model using
CoLA and evaluating directly on our acceptability tasks, as
an analogous evaluation setup to the NLI tasks, but all mod-
els performed around chance under this setup. This is likely
due to the intrinsic difficulty of CoLA for our base model, as
suggested by low performance from similar models (“GLUE
Baselines”) on https://gluebenchmark.com.

9https://github.com/google-research/
bert

https://gluebenchmark.com
https://github.com/google-research/bert
https://github.com/google-research/bert


241

of the correct label while the hyperparameters are
the same as in pretraining.

3.4 Variation from Random Restarts

In order to calibrate the degree of variation that can
be expected due to random restarts, we run each of
our probing tasks on five different random initial-
izations of the sentence encoder weights. These
sentence encoders were not pretrained, and we
trained MLPs for each probing task on top of the
randomly initialized sentence encoders. The ex-
pectation is that if pretraining has measurable ef-
fects on the probing results, the variance across
different pretrained models would be greater than
the variance across random restart models. Across
five random restarts, the average standard devia-
tion across our probing set was around 1 percent-
age point. The mean and 95% confidence interval
for each probing task are reported in Appendix E.

4 Results

4.1 Overall Performance

Figure 1 shows the performances of models
trained on each pretraining task on our probing
datasets. We also provide comparison with a
randomly initialized encoder with no pretraining,
which is known to be a strong baseline (Bowman
et al., 2018). We observe that different pretrain-
ing tasks have different strengths and weaknesses;
there is no single pretraining task that achieves
the best (or worst) performance across the board.
This implies that even the best encoders, such as
BERT, are unable to capture function word se-
mantics fully, and suggests further research into
combining advantages of different tasks. Further-
more, most models are far from human perfor-
mance, with only a few exceptions (e.g., BERT on
conjunctions). This demonstrates that our probing
datasets serve as useful challenge sets, in addition
to permitting fine-grained analysis.

Looking into each probing set in more detail,
we see several intuitive patterns on how pretrain-
ing might affect probing performance. Among the
pretrained models (not including BERT), the NLI
model did best on the negation10 and conjunction
tasks, both of which involve words that play cen-
tral roles in inferential reasoning. The CCG model

10We additionally find that this improvement is specifically
due to the NLI model’s capacity to understand explicit nega-
tion using not, rather than lexical negation with antonymy.
See Appendix F for differences between negation subtypes.

yields the best result for EOS, which could be at-
tributed to the task’s emphasis on structure; it is
the only task that where the target labels directly
represent compositional structure.

Surprisingly, we find that pretraining can some-
times hurt performance. For instance, pretrain-
ing uniformly hurts performance on comparatives
with the exception of skip-thought, which is still
within random variation range. In fact, for many
probing sets, the choice of pretraining task af-
fects whether it helps or hurts performance; for
instance, pretraining on NLI helps with negation,
whereas pretraining on image-caption matching
and CCG lowers performance. This suggests that
pretraining can be helpful, but only helpful if we
pretrain on a task that provides useful information
in solving the probing set. For instance, in Sec-
tion 4.3 we discuss how the image-caption match-
ing objective may bias models to discard informa-
tion about certain preposition senses. Overall, we
observe that language modeling is a useful pre-
training task, which aligns with its effectiveness
for pretraining models that achieve state-of-the-art
NLP results. However, the most beneficial task on
average (in terms of both raw accuracy and gains
over random baseline) is CCG, our most syntactic
task, which suggests that syntactic knowledge is
important for function word comprehension. We
also note that CCG achieves this result with the
smallest number of training examples out of all
pretraining tasks compared.

We furthermore see that our probing sets are
challenging even for BERT—although BERT sub-
stantially improves performance on many probing
sets, and obtains superhuman performance on con-
junctions and EOS,11 it also shows clear weak-
nesses in several probing sets (e.g., wh-words and
prepositions) where it is outperformed even by a
randomly initialized baseline with no pretraining.

4.2 Correlations between Pretaining Tasks

To further investigate whether our probing sets dif-
ferentiate between pretraining objectives, we look
into correlations between the model predictions;
given two pretraining tasks i and j, how often does
a model trained on i make the exact same predic-
tion as a model trained on j? Figure 2 shows the
correlations across all probing sets in aggregate,
and for the wh-words and prepositions sets specif-

11We speculate that this might be an effect of the next-
sentence classification task that BERT is pretrained on.



242

majority rand ccg
38K

dis
151K

nli
393K

img
592K

skip
4M

mt
3.4M

lm
30.3M

bert human

whwords
spatial
quant
prep
neg
eos
def

conj
comp

all

0.50 0.51 0.63 0.53 0.55 0.52 0.54 0.55 0.67 0.50 0.86
0.46 0.29 0.35 0.25 0.23 0.37 0.38 0.29 0.29 0.47 0.85
0.48 0.19 0.22 0.19 0.18 0.21 0.25 0.19 0.22 0.48 0.87
0.38 0.46 0.45 0.47 0.50 0.41 0.42 0.47 0.45 0.34 0.77
0.40 0.55 0.49 0.56 0.59 0.49 0.50 0.54 0.52 0.64 0.80
0.50 0.51 0.82 0.59 0.62 0.70 0.61 0.68 0.71 0.90 0.82
0.50 0.59 0.66 0.60 0.61 0.62 0.59 0.61 0.72 0.52 0.86
0.50 0.53 0.61 0.63 0.68 0.55 0.57 0.59 0.63 0.97 0.88
0.49 0.34 0.32 0.28 0.30 0.28 0.35 0.29 0.28 0.49 0.84
0.47 0.44 0.51 0.46 0.47 0.46 0.47 0.47 0.50 0.59 0.84

Figure 1: Accuracy for each pretraining task on each probing set. The leftmost column shows the majority-class
baseline, and the rightmost column shows individual annotator accuracy on the final probing set. Blue denotes
performance improvement over randomly initialized encoder baseline and orange denotes performance decrease.

Overall Prepositions Wh-Words

Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often
models make identical predictions on a particular probing set).

ically (see Appendix G for all sets).
We observe that models pretrained on differ-

ent tasks do make different predictions overall,
with image-caption matching and skip-thought be-
ing the tasks that make predictions that deviate
the most from others (left). NLI and image-
caption matching are the least correlated pair of
tasks among all. The difference between image-
captioning and other tasks is the most prominent
in the preposition probing set; it makes predictions
that are only weakly correlated with others (mid-
dle). We hypothesize that this is due to the duality
of preposition semantics; most prepositions have
both concrete and abstract senses, and the image
model is biased to focus on the former.

To illustrate, consider the preposition below,
which can denote a spatial configuration (e.g., the
boots end below the knee) or an abstract rela-
tion (numeric or qualitative comparison; e.g., her
score is below sixty). In the preposition dataset,
below occurs 17 times, 11 of which are spatial
and 6 abstract. For the spatial usage, both MNLI
and image-caption models have 64% accuracy
(7/11). The NLI model shows 50% accuracy for

pairs containing abstract uses (3/6), but the image-
captioning model answers none of them correctly
(0/6). Here is an example of a numeric usage of
below that the NLI model answered correctly but
the image model answered incorrectly:

P: Only those whose incomes do not exceed 125
percent of the federal poverty level qualify . . .

H: Those whose incomes are below 125 percent
qualify . . . (P→H)
The image model’s bias towards the spatial usage
is intuitive, since the numeric usage of below (i.e.,
as a counterpart to exceed) is difficult to learn from
visual clues only. This concrete-abstract duality,
which is not specific to below but common to most
other prepositions (Schneider et al., 2018), may
partially explain why the image-caption model be-
haves so differently from all other models, which
are not trained on a multimodal objective.

4.3 Data Size and Genre Effects

As can be seen from the varying sizes of the pre-
training dataset reported in Figure 1, seeing more
data at pretrain time does not imply better perfor-
mance on probing tasks. Also, as noted before,



243

the fact that pretraining can hurt probing perfor-
mance suggests that if the task is not the “right”
task, adding more datapoints at pretrain time is not
necessarily beneficial for probing performance.

Another potential confound is vocabulary over-
lap between pretraining and probing task datasets.
Since all pretraining task datasets have different
sets of vocabulary, the variance in the results could
be attributed to the amount of words in the prob-
ing set already seen at pretrain time. To investigate
this possibility, we compute the ratio of overlap-
ping words between the pretraining and probing
datasets. A regression analysis shows that vocab-
ulary overlap overall does not predict better per-
formance on the probing set (p > .05). No single
probing set performance was significantly affected
by vocabulary overlap either (all p > .05 after
Bonferroni correction for multiple comparisons).

5 Related Work

An active line of work focuses on “probing” neural
representations of language. Ettinger et al. (2016,
2017); Zhu et al. (2018), i.a., use a task-based ap-
proach similar to ours, where tasks that require a
specific subset of linguistic knowledge are used to
perform qualitative evaluation. Gulordava et al.
(2018), Giulianelli et al. (2018), Rønning et al.
(2018), and Jumelet and Hupkes (2018) make a fo-
cused contribution towards a particular linguistic
phenomenon (agreement, ellipsis, negative polar-
ity). Using recast NLI, Poliak et al. (2018a) probe
for semantic phenomena in neural machine trans-
lation encoders. Staliūnaite and Bonfil (2017);
Mahler et al. (2017); Ribeiro et al. (2018) use sim-
ilar strategies to our structural mutation method,
although their primary goal was to break existing
systems by adversarial modifications rather than
to compare different models. Ribeiro et al. (2018)
and our work both test for proper comprehension
of the modified expressions, but our modifications
are designed to induce semantic changes whereas
their modifications are intended to preserve the
original meaning. Our strategy is close to that of
Naik et al. (2018), but our modifications are more
constrained and lexically targeted.

The design of our NLI-style probing tasks fol-
lows the recent line of work which advocates for
NLI as a general-purpose format for diagnostic
tasks (White et al., 2017; Poliak et al., 2018b).
This idea is similar in spirit to McCann et al.
(2018), which advocates for question answering as

a general-purpose format, to edge probing (Tenney
et al., 2019) which probes for syntactic and seman-
tic structures via a common labeling format, and
to GLUE (Wang et al., 2018) which aggregates
a variety of tasks that share a common sentence-
classification format. The primary difference in
our work is that we focus specifically on the un-
derstanding of function words in context. We also
present a suite of several tasks, but each one fo-
cuses on a particular structure, whereas tasks pro-
posed in the works above generally aggregate mul-
tiple phenomena. Each of our tasks isolates each
function word type and employ a targeted mod-
ification strategy that gives us a more narrowly-
focused, informative scope of analysis.

6 Conclusion

We propose a new challenge set of nine tasks
that focus on probing function word comprehen-
sion. Although we use our challenge set to com-
pare the effects of pretraining, the probing sets
themselves are architecture- and evaluation setup-
agnostic. The results show that models pretrained
with different objectives do generate different pre-
dictions (e.g., image models have a bias towards
concrete preposition senses), and that no single
objective leads to models that perform best or
worst across all probing tasks. This suggests
that there are ‘gaps’ in the linguistic knowledge
learned from a single pretraining objective that
could be complemented by other objectives, and
this calls for further research into how different
pretraining objectives could be productively com-
bined. On average, CCG supertagging—our most
syntactic task—is the most beneficial pretraining
task for function word comprehension, even more
so than language modeling which has achieved
state-of-the-art results in recent advances in NLP.
In addition to contributing to the discussion of
finding effective pretraining tasks, we hope that
our exploratory study initiates further discussions
about modeling function words and their contribu-
tion to compositional meaning.

Acknowledgments

The work was conducted at the 2018 Frederick Je-
linek Memorial Summer Workshop on Speech and
Language Technologies, and supported by Johns
Hopkins University with unrestricted gifts from
Amazon, Facebook, Google, Microsoft and Mit-
subishi Electric Research Laboratories.



244

References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer

Lavi, and Yoav Goldberg. 2017. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. In International Conference on Learning
Representations.

Srinivas Bangalore and Aravind K Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237–265.

Raffaella Bernardi, Gemma Boleda, Raquel Fernan-
dez, and Denis Paperno. 2015. Distributional se-
mantics in use. In Proceedings of the First Work-
shop on Linking Computational Models of Lexical,
Sentential and Discourse-level Semantics, pages 95–
101, Lisbon, Portugal. Association for Computa-
tional Linguistics.

Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 workshop on
statistical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 12–58, Baltimore, Maryland, USA. Associa-
tion for Computational Linguistics.

Samuel Bowman, Yoav Goldberg, Felix Hill, Ange-
liki Lazaridou, Omer Levy, Roi Reichart, and An-
ders Sgaard, editors. 2017. Proceedings of the 2nd
Workshop on Evaluating Vector Space Representa-
tions for NLP. Association for Computational Lin-
guistics, Copenhagen, Denmark.

Samuel R. Bowman, Ellie Pavlick, Edouard Grave,
Benjamin Van Durme, Alex Wang, Jan Hula, Patrick
Xia, Raghavendra Pappagari, R. Thomas McCoy,
Roma Patel, Najoung Kim, Ian Tenney, Yinghui
Huang, Katherin Yu, Shuning Jin, and Berlin Chen.
2018. Looking for ELMo’s friends: Sentence-
level pretraining beyond language modeling. CoRR,
abs/1812.10860.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for mea-
suring progress in statistical language modeling.
arXiv:1312.3005.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680.

Alexis Conneau, Germán Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single $&!#* vector:
Probing sentence embeddings for linguistic proper-
ties. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2126–2136. Associa-
tion for Computational Linguistics.

Ishita Dasgupta, Demi Guo, Andreas Stuhlmüller,
Samuel J Gershman, and Noah D Goodman. 2018.
Evaluating compositionality in sentence embed-
dings. arXiv:1802.04302.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers).

Allyson Ettinger, Ahmed Elgohary, and Philip Resnik.
2016. Probing for semantic evidence of composition
by means of simple classification tasks. In Proceed-
ings of the 1st Workshop on Evaluating Vector-Space
Representations for NLP, pages 134–139. Associa-
tion for Computational Linguistics.

Allyson Ettinger, Sudha Rao, Hal Daumé III, and
Emily M. Bender. 2017. Towards linguistically gen-
eralizable NLP systems: A workshop and shared
task. In Proceedings of the First Workshop on
Building Linguistically Generalizable NLP Systems,
pages 1–10, Copenhagen, Denmark. Association for
Computational Linguistics.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2017. AllenNLP: A deep semantic natural language
processing platform.

Mario Giulianelli, Jack Harding, Florian Mohnert,
Dieuwke Hupkes, and Willem Zuidema. 2018. Un-
der the hood: Using diagnostic classifiers to in-
vestigate and improve how language models track
agreement information. In Proceedings of the 2018
EMNLP Workshop BlackboxNLP: Analyzing and In-
terpreting Neural Networks for NLP, pages 240–
248, Brussels, Belgium. Association for Computa-
tional Linguistics.

Kristina Gulordava, Piotr Bojanowski, Edouard Grave,
Tal Linzen, and Marco Baroni. 2018. Colorless
green recurrent networks dream hierarchically. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1195–1205, New
Orleans, Louisiana. Association for Computational
Linguistics.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pages
770–778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

http://aclweb.org/anthology/W15-2712
http://aclweb.org/anthology/W15-2712
http://www.aclweb.org/anthology/W14-3302
http://www.aclweb.org/anthology/W14-3302
http://www.aclweb.org/anthology/W17-53
http://www.aclweb.org/anthology/W17-53
http://www.aclweb.org/anthology/W17-53
http://arxiv.org/abs/1812.10860
http://arxiv.org/abs/1812.10860
http://aclweb.org/anthology/P18-1198
http://aclweb.org/anthology/P18-1198
http://aclweb.org/anthology/P18-1198
https://doi.org/10.18653/v1/W16-2524
https://doi.org/10.18653/v1/W16-2524
http://www.aclweb.org/anthology/W17-5401
http://www.aclweb.org/anthology/W17-5401
http://www.aclweb.org/anthology/W17-5401
http://www.aclweb.org/anthology/W18-5426
http://www.aclweb.org/anthology/W18-5426
http://www.aclweb.org/anthology/W18-5426
http://www.aclweb.org/anthology/W18-5426
http://www.aclweb.org/anthology/N18-1108
http://www.aclweb.org/anthology/N18-1108


245

Julia Hockenmaier and Mark Steedman. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33(3).

Yacine Jernite, Samuel R. Bowman, and David
Sontag. 2017. Discourse-based objectives for
fast unsupervised sentence representation learning.
arXiv:1705.00557.

Jaap Jumelet and Dieuwke Hupkes. 2018. Do lan-
guage models understand anything? O n the ability
of lstms to understand negative polarity items. In
Proceedings of the 2018 EMNLP Workshop Black-
boxNLP: Analyzing and Interpreting Neural Net-
works for NLP, pages 222–231, Brussels, Belgium.
Association for Computational Linguistics.

Douwe Kiela, Alexis Conneau, Allan Jabri, and Max-
imilian Nickel. 2018. Learning visually grounded
sentence representations. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 408–418, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In Ad-
vances in Neural Information Processing Systems,
pages 3294–3302.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In European Confer-
ence on Computer Vision, pages 740–755. Springer.

Tal Linzen, Grzegorz Chrupala, and Afra Alishahi,
editors. 2018. Proceedings of the First Workshop
on Analyzing and Interpreting Neural Networks for
NLP (BlackboxNLP). Association for Computa-
tional Linguistics, Brussels, Belgium.

Taylor Mahler, Willy Cheung, Micha Elsner, David
King, Marie-Catherine de Marneffe, Cory Shain,
Symon Stevens-Guille, and Michael White. 2017.
Breaking NLP: Using morphosyntax, semantics,
pragmatics and world knowledge to fool sentiment
analysis systems. In Proceedings of the First Work-
shop on Building Linguistically Generalizable NLP
Systems, pages 33–39. Association for Computa-
tional Linguistics.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6297–6308.

Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language de-
cathlon: Multitask learning as question answering.
arXiv:1806.08730.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture
models. In International Conference on Learning
Representations.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan,
and Zhi Jin. 2016. Natural language inference by
tree-based convolution and heuristic matching. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 130–136, Berlin, Germany. As-
sociation for Computational Linguistics.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2340–2353.

Allen Nie, Erin D. Bennett, and Noah D. Good-
man. 2017. DisSent: Sentence representa-
tion learning from explicit discourse relations.
arXiv:1710.04334.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
NIPS 2017 Autodiff Workshop.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 2227–
2237. Association for Computational Linguistics.

Adam Poliak, Yonatan Belinkov, James Glass, and
Benjamin Van Durme. 2018a. On the evaluation
of semantic phenomena in neural machine transla-
tion using natural language inference. In Proceed-
ings of the 2018 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 2 (Short Papers), pages 513–523, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Adam Poliak, Aparajita Haldar, Rachel Rudinger,
J. Edward Hu, Ellie Pavlick, Aaron Steven White,
and Benjamin Van Durme. 2018b. Collecting di-
verse natural language inference problems for sen-
tence representation evaluation. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing, pages 67–81. Association
for Computational Linguistics.

Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.
2018. On the convergence of Adam and beyond.
In International Conference on Learning Represen-
tations.

http://aclweb.org/anthology/J07-3004
http://aclweb.org/anthology/J07-3004
http://aclweb.org/anthology/J07-3004
http://www.aclweb.org/anthology/W18-5424
http://www.aclweb.org/anthology/W18-5424
http://www.aclweb.org/anthology/W18-5424
http://www.aclweb.org/anthology/N18-1038
http://www.aclweb.org/anthology/N18-1038
http://aclweb.org/anthology/W18-54
http://aclweb.org/anthology/W18-54
http://aclweb.org/anthology/W18-54
https://doi.org/10.18653/v1/W17-5405
https://doi.org/10.18653/v1/W17-5405
https://doi.org/10.18653/v1/W17-5405
http://anthology.aclweb.org/P16-2022
http://anthology.aclweb.org/P16-2022
http://arxiv.org/abs/1710.04334
http://arxiv.org/abs/1710.04334
http://aclweb.org/anthology/N18-1202
http://aclweb.org/anthology/N18-1202
https://doi.org/10.18653/v1/N18-2082
https://doi.org/10.18653/v1/N18-2082
https://doi.org/10.18653/v1/N18-2082
http://aclweb.org/anthology/D18-1007
http://aclweb.org/anthology/D18-1007
http://aclweb.org/anthology/D18-1007


246

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging nlp models. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 856–865. Association for Computa-
tional Linguistics.

Ola Rønning, Daniel Hardt, and Anders Søgaard. 2018.
Linguistic representations in multi-task neural net-
works for ellipsis resolution. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP, pages
66–73. Association for Computational Linguistics.

Nathan Schneider, Jena D. Hwang, Vivek Srikumar,
Jakob Prange, Austin Blodgett, Sarah R. Moeller,
Aviram Stern, Adi Bitan, and Omri Abend. 2018.
Comprehensive supersense disambiguation of en-
glish prepositions and possessives. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 185–196. Association for Computa-
tional Linguistics.

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional atten-
tion flow for machine comprehension. International
Conference on Learning Representations.

Ieva Staliūnaite and Ben Bonfil. 2017. Breaking senti-
ment analysis of movie reviews. In Proceedings of
the First Workshop on Building Linguistically Gen-
eralizable NLP Systems, pages 61–64. Association
for Computational Linguistics.

Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang,
and Virginia de Sa. 2017. Rethinking skip-thought:
A neighborhood based approach. In Proceedings
of the 2nd Workshop on Representation Learning
for NLP, pages 211–218. Association for Compu-
tational Linguistics.

Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Sam Bowman, Dipanjan Das,
and Ellie Pavlick. 2019. What do you learn from
context? probing for sentence structure in contextu-
alized word representations. In International Con-
ference on Learning Representations.

Alex Wang, Amapreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel R Bowman.
2018. GLUE: A multi-task benchmark and anal-
ysis platform for natural language understanding.
arXiv:1804.07461.

Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man. 2018. Neural network acceptability judg-
ments. arXiv:1805.12471.

Aaron Steven White, Pushpendre Rastogi, Kevin Duh,
and Benjamin Van Durme. 2017. Inference is ev-
erything: Recasting semantic resources into a uni-
fied evaluation framework. In Proceedings of the

Eighth International Joint Conference on Natural
Language Processing (Volume 1: Long Papers),
pages 996–1005. Asian Federation of Natural Lan-
guage Processing.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1112–1122. Association for
Computational Linguistics.

Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben-
jamin Van Durme. 2017. Ordinal common-sense in-
ference. Transactions of the Association of Compu-
tational Linguistics, 5(1):379–395.

Xunjie Zhu, Tingfeng Li, and Gerard de Melo. 2018.
Exploring semantic properties of sentence embed-
dings. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 632–637, Melbourne,
Australia. Association for Computational Linguis-
tics.

Appendix

A Experimental Details

Image-Caption Matching We train on image-
caption pairs from the MSCOCO dataset (Lin
et al., 2014) to minimize the cosine distance be-
tween representations of the sentence and corre-
sponding image. Specifically, we encode the sen-
tence with the BiLSTM encoder. We use a Resnet-
101 (He et al., 2016), a CNN pretrained on Ima-
geNet to obtain a 1024-dimensional feature repre-
sentation for the image. We linearly transform the
encoder output of the sentence to the size of the
image representation and use a cosine embedding
loss against the two vectors, i.e., minimize the co-
sine distance between two representations to allow
mapping sentences to their corresponding images.

Regularization We regularize with dropout
with p = 0.2. Dropout is placed after the input
layer, each LSTM layer, and each MLP layer in
the task-specific classifier.

Preprocessing We use Moses tokenizer with a
maximum sequence length of 40 tokens. Because
we used a character-based word encoder, we have
no word-level vocabulary, except for sequence-to-
sequence tasks, where we use an output vocabu-
lary of 20,000 tokens. For translation, we use BPE
tokenization; for skip-thought we use the Moses
tokenizer.

http://aclweb.org/anthology/P18-1079
http://aclweb.org/anthology/P18-1079
http://aclweb.org/anthology/W18-5409
http://aclweb.org/anthology/W18-5409
http://aclweb.org/anthology/P18-1018
http://aclweb.org/anthology/P18-1018
https://doi.org/10.18653/v1/W17-5410
https://doi.org/10.18653/v1/W17-5410
http://aclweb.org/anthology/W17-2625
http://aclweb.org/anthology/W17-2625
https://openreview.net/forum?id=SJzSgnRcKX
https://openreview.net/forum?id=SJzSgnRcKX
https://openreview.net/forum?id=SJzSgnRcKX
http://aclweb.org/anthology/I17-1100
http://aclweb.org/anthology/I17-1100
http://aclweb.org/anthology/I17-1100
http://aclweb.org/anthology/N18-1101
http://aclweb.org/anthology/N18-1101
http://www.aclweb.org/anthology/P18-2100
http://www.aclweb.org/anthology/P18-2100


247

Training Details We optimize using AMSGrad
(Reddi et al., 2018) with a learning rate of 1e-3 for
text generation tasks and 1e-4 otherwise. We eval-
uate on the validation set every 1,000 iterations
and stop training if we fail to get a best result after
20 evaluations. We multiply the learning rate by
0.5 whenever validation performance fails to im-
prove for more than 4 validation checks. We also
stop training if the learning rate falls below 1e-6.
At the end of training, we load the best checkpoint.

Acceptability task evaluation For the accept-
ability tasks, we use a 10-fold cross-validation
evaluation setup because we are training task-
specific classifiers for each probing task and the
datasets are small. We split each dataset into 10
folds with balanced label ratio within each fold,
and test on each fold using the other 9 as train and
development sets (8 folds train, 1 fold dev). The
accuracy reported in the paper for the acceptability
tasks is test accuracy averaged across all folds.

B MNLI Development Set Accuracy for
Pretrained Models

MNLI (dev)

Random 73.8
CCG 69.6
DisSent 73.6
MNLI 75.6
IMG 62.6
Skip 67.4
MT 72.0
LM 72.6

Table 2: MNLI development set accuracy for each pre-
trained model.

C Annotation Protocol

We recruited three annotators per sentence or sen-
tence pair on Amazon Mechanical Turk to control
the quality of the labels for our heuristically gener-
ated datasets. For the acceptability judgment task
sentences, individual sentence or sentence pair ex-
ample was presented to the annotators and they
were asked to choose between the options natural,
unnatural, neither, after reading the given exam-
ple. The examples were presented in sets of five
sentences (individual sentence tasks) or three sen-
tence pairs (sentence pair tasks) in random order,

with the option to stop at any point during the pro-
cess. The annotators were compensated with $.1
per five sentences (or three sentence pairs). For
NLI task sentences, the annotators were presented
with six sentence pairs, for which they were asked
to provide judgment on a five-point scale about the
inferrability of the second sentence from the first.
The annotators were compensated with $.1 per six
sentence pairs. See Table 3 for inter-annotator
agreement and the final size of the dataset.

Agree Unan. Accuracy Size

Negation 60.3 40.5 80.2 598
Spatial 70.0 43.4 85.0 180
Quant. 73.8 48.8 86.9 448
Comp. 68.7 40.7 84.3 100
Prep 54.7 33.1 77.4 358

wh-words 72.5 58.7 86.2 584
Def. 72.0 58.1 86.0 508
Coord. 75.3 62.9 87.6 456
EOS 64.9 47.3 82.4 478

Table 3: Pairwise inter-annotator agreement (n = 3),
% of examples with unanimous agreement, and indi-
vidual annotator accuracy according to the expected la-
bel for each task in the final probing dataset.

D List of Prepositions Used for the NLI
Probing Set

about, above, across, after, against, ahead of, all
over, along, among, around, at, before, behind, be-
low, beneath, beside, by, for, from, in, in front of,
inside, inside of, into, near, nearby, next to, on,
on top of, out of, outside, outside of, over, past,
through, to, under, up, within, with, without

E Random Initialization Variance

µ (±σ)
Prep 46.14 (±0.78)
Negation 53.66 (±0.74)
Spatial 25.34 (±1.87)
Quant. 19.38 (±0.86)
Comp. 31.8 (±1.51)

wh-words 51.37 (±0.36)
def/indef articles 57.77 (±0.97)
coord. 54.39 (±0.96)
EOS 52.74 (±1.30)

Table 4: Mean and 95% CI of probing task perfor-
mance across five different random initializations.



248

majority rand ccg
38K

dis
151K

nli
393K

img
592K

skip
4M

mt
3.4M

lm
30.3M

bert human

expneg
neg

lexneg

0.41 0.43 0.37 0.42 0.53 0.39 0.39 0.42 0.40 0.58 0.82
0.40 0.55 0.49 0.56 0.59 0.49 0.50 0.54 0.52 0.64 0.80
0.42 0.65 0.62 0.67 0.65 0.55 0.57 0.65 0.65 0.68 0.81

Figure 3: Accuracy of each pretrained model on subsets of the negation probing set. neg is the accuracy for
the whole negation probing set. lexneg shows accuracy for the subset of sentence pairs negated using antonyms
and expneg sentences explicitly negated using not. The leftmost column shows the majority-class baseline, and
the rightmost column shows individual annotator accuracy on the final evaluation set. Blue denotes performance
improvement over randomly initialized encoder baseline and orange denotes performance decrease.

F Subset Accuracy for the Negation
Probing Set

In Figure 3, we see that the NLI model’s improve-
ment on the negation probing set mostly derives
from its improvement on explicit negation rather
than lexical negation.

G Prediction Overlap between Models

We show prediction overlap heatmaps for all prob-
ing tasks in Figure 4.



249

Figure 4: Heatmaps of prediction overlap for all probing tasks, between models pretrained with different objectives.


