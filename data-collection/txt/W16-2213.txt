



















































Examining the Relationship between Preordering and Word Order Freedom in Machine Translation


Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 118–130,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

Examining the Relationship between Preordering and
Word Order Freedom in Machine Translation

Joachim Daiber Miloš Stanojević Wilker Aziz Khalil Sima’an
Institute for Logic, Language and Computation (ILLC)

University of Amsterdam
{initial.last}@uva.nl

Abstract

We study the relationship between word
order freedom and preordering in statisti-
cal machine translation. To assess word
order freedom, we first introduce a novel
entropy measure which quantifies how dif-
ficult it is to predict word order given a
source sentence and its syntactic analysis.
We then address preordering for two target
languages at the far ends of the word order
freedom spectrum, German and Japanese,
and argue that for languages with more
word order freedom, attempting to predict
a unique word order given source clues
only is less justified. Subsequently, we ex-
amine lattices of n-best word order predic-
tions as a unified representation for lan-
guages from across this broad spectrum
and present an effective solution to a re-
sulting technical issue, namely how to se-
lect a suitable source word order from
the lattice during training. Our experi-
ments show that lattices are crucial for
good empirical performance for languages
with freer word order (English–German)
and can provide additional improvements
for fixed word order languages (English–
Japanese).

1 Introduction

Word order differences between a source and a tar-
get language are a major challenge for machine
translation systems. For phrase-based models,
the number of possible phrase permutations is so
large that reordering must be constrained locally
to make the search space for the best hypothe-
sis feasible. However, constraining the space lo-
cally runs the risk that the optimal hypothesis is
rendered out of reach. Preordering of the source

sentence has been embraced as a way to ensure
the reachability of certain target word order con-
stellations for improved prediction of the target
word order. Preordering aims at predicting a per-
mutation of the source sentence which has min-
imal word order differences with the target sen-
tence; the permuted source sentence is passed on
to a backend translation system trained to translate
target-order source sentences into target sentences.
In essence, the preordering approach makes the as-
sumption that it is feasible to predict target word
order given only clues from the source sentence.
In the vast majority of work on preordering, a sin-
gle preordered source sentence is passed on to the
backend system, thereby making the stronger as-
sumption that it is feasible to predict a unique pre-
ferred target word order. But how reasonable are
these assumptions and for which target languages?

Intuitively, the assumption of a unique pre-
ordering seems reasonable for translating into
fixed word order languages such as Japanese, but
for translation into languages with less strict word
order such as German, this is unlikely to work.
In such languages there are multiple comparably
plausible target word orders per source sentence
because the underlying predicate-argument struc-
ture can be expressed with mechanisms other than
word order alone (e.g. morphological inflections
or intonation). For these languages, it seems rather
unlikely to be able to choose a unique word order
given only source sentence clues. In this paper, we
want to shed light on the relationship between the
target language’s word order freedom and the fea-
sibility of preordering. We start out by contribut-
ing an information-theoretic measure to quantify
the difficulty in predicting a preferred word order
given the source sentence and its syntax. Our mea-
sure provides empirical support for the intuition
that it is often not possible to predict a unique word
order for free word order languages, whereas it is

118



more feasible for fixed word order languages such
as Japanese. Subsequently, we study the option of
passing the n-best word order predictions, instead
of 1-best, to the backend system as a lattice of pos-
sible word orders of the source sentence.

For the training of the backend system, the
use of such permutation lattices raises a question:
What should constitute the training corpus for a
lattice-preordered translation system? In previ-
ous work using single word order predictions, the
training data consists of pairs of source and target
sentences where the source sentence is either in
target order (i.e. order based on word alignments)
or preordered (i.e. predicted order). In this work
we contribute a novel approach for selecting train-
ing instances from the lattice of word order permu-
tations: We select the permutation providing the
best match with the target-order source sentence
(we call this process “lattice silver training”).

Our experiments show that for English–
Japanese and English–German lattice preordering
has a positive impact on the translation quality.
Whereas lattices enable further improvement for
preordering English into the strict word order lan-
guage Japanese, lattices in conjunction with our
proposed lattice silver training scheme turn out to
be crucial to reach satisfactory empirical perfor-
mance for English–German. This result highlights
that when predicting word order of free word order
languages given source clues only, it is important
to ensure that the word order predictions and the
backend system are suitably fitted together.

2 Related Work

Preordering has been explored from the per-
spective of the upper-bound achievable transla-
tion quality in several studies, including Khalilov
and Sima’an (2012) and Herrmann et al. (2013),
which compare various systems and provide or-
acle scores for syntax-based preordering models.
Target-order source sentences, in which the word
order is determined via automatic alignments, en-
able translation systems great jumps in translation
quality and provide improvements in compactness
and efficiency of downstream phrase-based trans-
lation models. Approaches have largely followed
two directions: (1) predicting word order based
on some form of source-syntactic representation
and (2) approaches which do not depend on source
syntax.

2.1 Source Syntax-Based Preordering

Many approaches to preordering have made use
of syntactic representations of the source sentence,
including Collins et al. (2005) who restructure the
source phrase structure parse tree by applying a
sequence of transformation rules. More recently,
Jehl et al. (2014) learn to order sibling nodes in
the source-side dependency parse tree. The space
of possible permutations is explored via depth-first
branch-and-bound search (Balas and Toth, 1983).
In later work, the authors further improve this
model by replacing the logistic regression classi-
fier with a feed-forward neural network (de Gis-
pert et al., 2015), which results in improved em-
pirical results and eliminates the need for feature
engineering. Lerner and Petrov (2013) train clas-
sifiers to predict the permutations of up to 6 tree
nodes in the source dependency tree. The authors
found that by only predicting the best 20 permuta-
tions of n nodes, they could cover a large majority
of the reorderings in their data.

2.2 Preordering without Source Syntax

Tromble and Eisner (2009) learn to predict the ori-
entation of any two words (straight or inverted or-
der) using a perceptron. The search for the best re-
ordering is performed with a O(n3) chart parsing
algorithm. More basic approaches to syntax-less
preordering include the application of multiple
MT systems (Costa-jussà and Fonollosa, 2006),
where a first system learns preordering and a sec-
ond learns to translate the preordered sentence
into the target sentence. Finally, there have been
successful attempts at the automatic induction of
parse trees from aligned data (DeNero and Uszko-
reit, 2011) and the estimation of latent reordering
grammars (Stanojević and Sima’an, 2015) based
on permutation trees (Zhang and Gildea, 2007).

2.3 Lattice Translation

A lattice is an acyclic finite-state automaton defin-
ing a finite language. A more restricted class of
lattices, namely, confusion networks (Bertoldi et
al., 2007), has been extensively used to pack alter-
native input sequences for decoding.1 However,
applications mostly focused on speech translation
(Ney, 1999; Bertoldi et al., 2007), or to account for
lexical and/or segmentation ambiguity due to pre-
processing (Xu et al., 2005; Dyer, 2007). In very

1A confusion network is a special case of a lattice where
every path from start to final state goes through every node.

119



few occasions, lattice input has been used to deter-
mine the space of permutations of the input con-
sidered by the decoder (Knight and Al-Onaizan,
1998; Kumar and Byrne, 2003). The effectiveness
of lattices of permutations was demonstrated by
Zhang et al. (2007). However, except in the cases
of n-gram based decoders (Khalilov et al., 2009)
this approach is not a common practice.

Dyer et al. (2008) formalized lattice transla-
tion both for phrase-based and hierarchical phrase-
based MT. The former requires a modification of
the standard phrase-based decoding algorithm as
to maintain a coverage vector over states, rather
than input word positions. The latter requires in-
tersecting a lattice and a context-free grammar,
which can be seen as a generalized form of pars-
ing (Klein and Manning, 2001). In this work, we
focus on phrase-based models.

The space of translation options in standard
phrase-based decoding with a distortion limit d
grows with O(stack size × n × 2d) where n repre-
sents the input length, and the number of transla-
tion options is capped due to beam search (Koehn
et al., 2003). With lattice input, the dependency
on n is replaced by |Q| where Q is the set of states
of the lattice. The stack size makes the number of
translation options explored by the decoder inde-
pendent of the number of transitions in the lattice.

As in standard decoding, the states of a lattice
can also be visited non-monotonically. However,
two states in a lattice are not always connected by
a path, and, in general, paths connecting two nodes
might differ in length. Dyer et al. (2008) proposed
to pick the shortest path between two nodes to be
representative of the distance between them.2 Just
like in standard decoding, a distortion limit is im-
posed to keep the space of translations tractable.

In this work, we use lattice input to constrain the
space of permutations of the source allowed within
the decoder. Moreover, in most cases we com-
pletely disable the decoder’s further reordering ca-
pabilities. Because our models can perform global
permutation operations without ad hoc distortion
limits, we can reach far more complex word or-
ders. Crucially, our models are better predictors
of word order than standard distortion-based re-
ordering, thus we manage to decode with rela-
tively small permutation lattices.

2This is achieved by running an all-pairs shortest path al-
gorithm prior to decoding – see for example Chapter 25 of
(Cormen et al., 2001). MOSES uses the Floyd-Warshall algo-
rithm, which runs in time O(|Q|3).

3 Quantifying Word Order Freedom

While varying degrees of word order freedom are
a well-studied topic in linguistics, word order free-
dom has only recently been studied from a quanti-
tative perspective. This has been enabled partly by
the increasing availability of syntactic treebanks.
Kuboň and Lopatková (2015) propose a measure
of word order freedom based on a set of six com-
mon word order types (SVO, SOV, etc.). Futrell et
al. (2015) define various entropy measures based
on the prediction of word order given unordered
dependency trees. Both approaches require a de-
pendency treebank for each language.

In practical applications such as machine trans-
lation, it is difficult to quantify the influence of
word order freedom. For an arbitrary language
pair, our goal is to quantify a notion of the target
language’s word order freedom based only on par-
allel sentences and source syntax. In their head
direction entropy measure, Futrell et al. (2015)
approach the problem of quantifying word order
freedom by measuring the difficulty of recover-
ing the correct linear order from a sentence’s un-
ordered dependency tree. We approach the prob-
lem of quantifying a target language’s word order
freedom by measuring the difficulty of predicting
target word order based on the source sentence’s
dependency tree. Hence, we ask questions such
as: How difficult is it to predict French word or-
der based on the syntax of the English source sen-
tence?

3.1 Source Syntax and Target Word Order

We represent the target sentence’s word order as a
sequence of order decisions. Each order decision
encodes for two source words, a and b, whether
their translation equivalents are in the order (a, b)
or (b, a). The source sentences are parsed with a
dependency parser.3 The target-language order of
the words in the source dependency tree is then
determined by comparing the target sentence po-
sitions of the words aligned to each source word.
Figure 1 shows the percentage of dependent-head
pairs in the source dependency tree whose target
order can be correctly guessed by always choos-
ing the more common decision.4

3http://cs.cmu.edu/˜ark/TurboParser/
4For English–Japanese, we use manual word alignments

of 1,235 sentences from the Kyoto Free Translation Task
(Neubig, 2011) and for English–German, we use a manu-
ally word-aligned subset of Europarl (Padó and Lapata, 2006)
consisting of 987 sentences.

120



verb

noun 79.7%

- Sbj 63.3%

- Obj 82.6%

- Adv 84.3%

adv 54.5%

Head label

D
ep

en
de

nt
la

be
l

noun

adj 79.5%

det 78.6%

prep 49.3%

Head label

(a) English–Japanese

verb

noun 44.7%

- Sbj 66.3%

- Obj 22.7%

- Adv 35.4%

adv 41.3%

Head label

D
ep

en
de

nt
la

be
l

noun

adj 76.7%

det 92.9%

prep 62.2%

Head label

(b) English–German

Figure 1: Source word pairs whose target order can be predicted using only the words’ labels.

German and Japanese Both language pairs dif-
fer significantly in how strictly the target lan-
guage’s word order is determined by the source
language’s syntax. English–German shows strict
order constraints within phrases, such as that ad-
jectives and determiners precede the noun they
modify in the vast majority of cases (Figure 1b).
However, English–German also shows more free-
dom on the clause level, where basic syntax-
based predictions for the positions of nouns rela-
tive to the main verb are insufficient. For English–
Japanese on the other hand, the position of the
nouns relative to the main verb is more rigid,
which is demonstrated by the high scores in Fig-
ure 1a. These results are in line with the linguistic
descriptions of both target languages. From a tech-
nical point of view, they highlight that any treat-
ment of English–German word order must take
into account information beyond the basic syntac-
tic level and must allow for a given amount of
word order freedom.

3.2 Bilingual Head Direction Entropy

While such a qualitative comparison provides in-
sight into the order differences of selected lan-
guage pairs, it is not straight-forward to compare
across many language pairs. From a linguistic per-
spective, Futrell et al. (2015) use entropy to com-
pare word order freedom in dependency corpora
across various languages. While the authors ob-
served that artifacts of the data such as treebank
annotation style can hamper comparability, they
found that a simple entropy measure for the pre-
diction of word order based on the dependency
structure provided a good quantitative measure of
word order freedom.

We follow Futrell et al. (2015) in basing our
measure on conditional entropy, which provides a
straight-forward way to quantify to which extent

target word order is determined by source syntax.

H(Y |X) = −
∑

x∈X
p(x)

∑

y∈Y
p(y|x) log p(y|x)

Conditional entropy measures the amount of infor-
mation required to describe the outcome of a ran-
dom variable Y given the value of a second ran-
dom variable X . Given a dependent-head pair in
the source dependency tree, X consists of the de-
pendent’s and the head’s part of speech, as well as
the dependency relation between them. Note that
as in all of our experiments the source language is
English, the space of outcomes of X is the same
across all language pairs.Y in this case is the word
pair’s target-side word order in the form of a (a, b)
or (b, a) decision. We estimate H(Y |X) using the
bootstrap estimator of DeDeo et al. (2013), which
is less prone to sample bias than maximum likeli-
hood estimation.5

Influence of word alignments Futrell et al.
(2015) use human-annotated dependency trees for
each language they consider. Our estimation only
involves word-aligned bilingual sentence pairs
with a source dependency tree. Manual align-
ments are available for a limited number of lan-
guage pairs and often only for a diminishingly
small number of sentences. Consequently the
question arises, whether automatic word align-
ments are sufficient for this task. To answer this
question, we apply our measure to a set of manu-
ally aligned as well as a larger set of automatically
aligned sentence pairs. In addition to the German
and Japanese alignments mentioned above, we use
manual alignments for English–Italian (Farajian et
al., 2014), English–French (Och and Ney, 2003),
English–Spanish (Graça et al., 2008) and English–
Portuguese (Graça et al., 2008).

5We observe an average of 1,033 values for X per lan-
guage pair and perform 10,000 Monte-Carlo samples.

121



0 0.2 0.4 0.6

Mandarin

Japanese

Esperanto

Portuguese

Italian

Spanish

French

Russian

German

Hebrew

Turkish

Berber

Figure 2: Bilingual head direction entropy with
English source side.

Since a limited number of manually aligned
sentences are available, it is important to avoid
bias due to sample size. Hence, we randomly sam-
ple the same number of dependency relations from
each language pair. Considering only those lan-
guages for which we have both manual and au-
tomatic alignments, we can determine how well
their word order freedom rankings correlate. Even
though the number of samples for the manually
aligned sentences is limited to 500 due to the size
of the smallest set of manual alignments, we find a
high correlation of Spearman’s ρ = 0.77 between
the rankings of the 6 languages that occur in both
sets (Zwillinger and Kokoska, 1999).

Influence of source syntax Another factor that
may influence our estimated degree of word order
freedom is the form and granularity of the source
side’s syntactic representation: More detailed rep-
resentations may disambiguate cases that are diffi-
cult to predict with a more bare representation. As
we are interested in the bilingual case and, specif-
ically, in preordering, we content ourselves with
using the same syntactic representation, i.e. de-
pendency trees, that many preordering models use
(e.g., Jehl et al. (2014), Lerner and Petrov (2013)).

Comparison to monolingual measures Our
measure is similar to Futrell et al. (2015)’s head
direction entropy; however, it also offers sev-
eral advantages. While monolingual head direc-
tion entropy requires a dependency treebank for
each language, our bilingual head direction en-
tropy only requires dependency annotation for the
source language (English in our case). One of

their caveats, the influence of the widely varying
dependency annotation styles across treebanks, is
also not present in our method, since a single de-
pendency style is used for the source language.
We have demonstrated that automatic alignments
perform on a comparable level to manual align-
ments. Accordingly, the amount of data that can be
used to estimate the measure is only limited by the
availability of parallel sentences. Finally, while
dependency treebanks rarely cover the same cor-
pora or even domains, our method can utilize sen-
tences from the same or similar corpora for each
language, thus minimizing potential corpus biases.

Translation from English Figure 2 plots bilin-
gual head direction entropy for an English source
side and a set of typologically diverse languages
on the target side. For each language pair, we use
18,000 sentence pairs and automatic alignments
from the Tatoeba corpus (Tiedemann, 2012).6

Languages at the top of the plot in Figure 2
show a greater degree of word order freedom with
respect to the English source syntax. Thus, pre-
dicting their word order from English source clues
alone is likely to be difficult. We argue that in such
cases it is crucial to pass on the ambiguity over
the space of predictions to the translation model.
By doing so, word order decisions can be influ-
enced by translation decisions, while still shaping
the space of reachable translations.

4 Preordering Free and Fixed Word
Order Languages

The measure of word order freedom introduced in
the previous section enables us to estimate how
difficult it is to predict the target language’s word
order based on the source language. In this sec-
tion, we introduce the two preordering models
we use to predict the word order of German and
Japanese. Experiments with these models will al-
low us to examine the relationship between pre-
ordering and word order freedom.

4.1 Neural Lattice Preordering

Based on their earlier work, which used logistic
regression and graph search for preordering (Jehl
et al., 2014), de Gispert et al. (2015) introduce a
neural preordering model. In this model, a feed-
forward neural network is trained to estimate the

6The alignments were produced using GIZA++ (Och and
Ney, 2003) with grow-diag-final-and symmetrization.

122



swap probabilities of nodes in the source-side de-
pendency tree. Search is performed via the depth-
first branch-and-bound algorithm. The authors
have found this model to be fast and to produce
high quality word order predictions for a variety
of languages.

Model estimation Training examples are ex-
tracted from all possible pairs of children of a de-
pendency tree node, including the head itself. For
each pair, the two nodes are swapped if swapping
them reduces the number of crossing alignment
links. The crossing score of two nodes a and b (a
precedes b in linear order) and their aligned target
indexes Aa and Ab is defined as follows:

cs(a, b) = | {(i, j) ∈ Aa × Ab : i > j} |

Training instances generated in this manner are
then used to estimate the swap probability p(i, j)
for two indexes i and j. For each node in the
source dependency tree, the best possible permu-
tation of its children (including the head) is deter-
mined via graph search. The score of a permuta-
tion of length k is defined as follows:

score(π) =
∏

1≤i<j≤k|π[i]>π[j]
p(i, j)

·
∏

1≤i<j≤k|π[i]<π[j]
1 − p(i, j)

We closely follow de Gispert et al. (2015) for the
implementation of the estimator of p(i, j). A feed-
forward neural network (Bengio et al., 2003) is
trained to predict the orientation of a and b based
on a sequence of 20 features, such as the words,
the words’ POS tags, the dependency labels, etc.7

The network consists of 50 nodes on the input
layer, 2 on the output layer, and 50 and 100 on
the two hidden layers. We use a learning rate of
0.01, batch size of 1000 and perform 20 training
epochs.

Search Search in this model consists of finding
the sequence of swaps leading to the best overall
score according to the model. Let a partial permu-
tation of k nodes be a sequence of length k′ < k
containing each integer in {1, ..., k} at most once.
The score of a new permutation obtained by ex-
tending a partial permutation π′ of length k′ by

7Our implementation is based on http://nlg.isi.
edu/software/nplm/.

one element can be computed efficiently as:

score(π′ · ⟨i⟩) = score(π′)
·

∏

j∈V |i>j
p(i, j)

·
∏

j∈V |i<j
1 − p(i, j)

k-best search Target languages such as German
allow for a significant amount of word order free-
dom; hence, the depth-first branch-and-bound al-
gorithm, which extracts the single best permu-
tation, may not be the best choice in this case.
In the context of the Traveling Salesman Prob-
lem, van der Poort et al. (1999) show that general
branch-and-bound search can be extended to re-
trieve k-best results while keeping the same guar-
antees and computational complexity. Only mi-
nor changes are necessary to adapt the search for
the best permutation to finding the k-best permu-
tations: We keep a set bestk of the best permuta-
tions and a single bound. If for a permutation π′,
score(π′) > bound, instead of updating the bound
to the single best permutation and remembering it,
the following steps are performed:

1. If |bestk| = k:
− Remove worst permutation from the set.

2. Add π′ to bestk.

3. The new bound will be the score of the worst
permutation in bestk.

4.2 Reordering Grammar Induction
Reordering Grammar (RG) (Stanojević and
Sima’an, 2015) is a recent approach for preorder-
ing that is hierarchical and fully unsupervised. It
is based on inducing a probabilistic context-free
grammar from aligned parallel data. This gram-
mar can predict permutation trees (PETs) (Zhang
and Gildea, 2007) — projective constituency trees
that can fully describe any permutation. PETs are
reminiscent of ITG (Wu, 1997) with the important
distinction that PETs can handle any permutation,
unlike ITG which can only handle binarizable
ones. As in ITG, constituents in PETs are labeled
with the permutation of their children.

Induction of RGs is performed by specifying a
generative probabilistic model and then estimating
its parameters using the EM algorithm. The rea-
soning behind using EM is that many latent vari-
ables are present in the model. Only the source

123



sentence and its permutation are observed during
training. The exact PET that generated this per-
mutation is not observed and there could be (ex-
ponentially) many PETs that could have generated
the observed permutation. Hence, the bracketings
of potential PETs are treated as latent variables.

The second source of latent variables is state
splitting of non-terminals (labels that indicate how
to reorder the children) in a similar way as done
in monolingual parsing (Matsuzaki et al., 2005;
Petrov et al., 2006; Prescher, 2005). Each la-
tent permutation tree has many latent derivations
and the generative probabilistic model needs to ac-
count for them. The probability of the observed
permutation π is defined in the following way:

P (π) =
∑

∆∈PEF(π)

∑

d∈∆

∏

r∈d
P (r)

where PEF(π) returns the Permutation Forest of
π (i.e., the set of PETs that can generate the per-
mutation π), ∆ represents a permutation tree, d
represents a derivation of a permutation tree and
r represents a production rule. Efficient estima-
tion for this model is done by using the standard
Inside-Outside algorithm (Lari and Young, 1990).

At test time, the source sentence is parsed with
the estimated grammar in order to find the deriva-
tion of a permutation tree with the lowest expected
cost. More formally, the decoding task can be de-
scribed as:

d̂ = arg min
d∈Chart(s)

∑

d′∈Chart(s)
P (d′) cost(d, d′)

where P (d) =
∏

r∈d P (r) is the probability of a
derivation, and Chart(s) is the space of all pos-
sible derivations of all possible permutation trees
for source sentence s. Two main modifications to
this formula are made in order to make inference
fast: First, Kendall τ is used as a cost function be-
cause it decomposes well,8 which allows usage of
efficient dynamic programming minimum Bayes-
risk (MBR) computation (DeNero et al., 2009).
Second, instead of computing the MBR deriva-
tion over the full chart, computation is done over
10,000 unbiased samples from the chart. To build
the permutation lattice with this model we use
the top n permutations which have the lowest ex-
pected Kendall τ cost.

8More precisely, we use the Kendall τ distance between
the permutations that are yields of the derivations.

0

1

0:0
20

0:0
39

0:0
58

0:0
77

0:0
96

0:0
115

0:0
134

0:0
153

0:0
172

0:0
191

0:0
210

0:0
229

0:0
248

0:0
267

0:0
286

0:0
305

0:0

3240:0

343

0:0

362

0:0

381

0:0

400

0:0

419

0:0

438

0:0

457

0:0

476

0:0

495

0:0

514

0:0

533

0:0

552

0:0

571

0:0

590

0:0

609

0:0

628

0:0

647

0:0

21:1

211:1

401:1

591:1

781:1

971:1

1161:1

1351:1

1541:1

1731:1

1921:1

2111:1

2301:1

2491:1

2681:1

2871:1

3061:1

3251:1

3441:1

3631:1

3821:1

4011:1

4201:1

4391:1

4581:1

4771:1

4961:1

5151:1

5341:1

5531:1

5721:1

591

1:1

610

1:1

629

1:1

648

1:1

32:2
43:3 57:7

66:6 74:4 85:5 98:8 10
9:9 1110:10 1212:12 1313:13 1414:14

1515:15

1616:16 17
17:17

1811:11 19
18:18

222:2
233:3 247:7

256:6 264:4 275:5 288:8 299:9 3010:10 3111:11 3212:12 3313:13
3414:14

3515:15
3616:16

3717:17 38
18:18

412:2 42
3:3 434:4

447:7
456:6 465:5 478:8 489:9 4910:10 5012:12 5113:13 5214:14

5315:15
5416:16

5517:17
5611:11 5718:18

602:2 61
3:3 624:4 63

7:7
646:6 655:5 668:8 679:9 6810:10 6911:11 7012:12 7113:13

7214:14
7315:15

7416:16
7517:17 7618:18

792:2 80
3:3 817:7 82

6:6
835:5 844:4 858:8 869:9 8710:10 8812:12 8913:13 9014:14

9115:15
9216:16

9317:17
9411:11 9518:18

982:2 997:7
1006:6 1013:3

1024:4 1035:5 1048:8 1059:9 10610:10 10712:12 10813:13 10914:14 110
15:15

11116:16 112
17:17

11311:11 114
18:18

1172:2 1183:3 119
7:7 1206:6

1215:5 1224:4 1238:8 1249:9 12510:10 12611:11 12712:12 12813:13 129
14:14

13015:15 13116:16
13217:17

13318:18

1362:2 1373:3 1384:4 139
5:5

1407:7 1416:6 1428:8 1439:9 14410:10 14512:12 14613:13 14714:14 148
15:15 14916:16 150

17:17 151
11:11 152

18:18

1552:2 1567:7 1576:6 158
3:3 1594:4 1605:5 1618:8 162

9:9 16310:10 16411:11 165
12:12 16613:13 16714:14 168

15:15 16916:16
17017:17 171

18:18

1742:2 1753:3 1764:4 1775:5 178
7:7 1796:6 1808:8 1819:9 18210:10 18311:11 18412:12 18513:13 186

14:14 18715:15
18816:16

18917:17 190
18:18

1932:2 1943:3 1955:5 1967:7 1976:6 198
4:4 1998:8 2009:9 20110:10 20212:12 20313:13 20414:14 205

15:15 20616:16
20717:17

20811:11 20918:18

2122:2 2133:3 2145:5 2157:7 2166:6 2174:4 2188:8 219
9:9 22010:10 22111:11 22212:12 22313:13 224

14:14 22515:15
22616:16 227

17:17 22818:18

2312:2 2324:4 2333:3 234
7:7 2356:6 2365:5 2378:8 2389:9 23910:10 24012:12 241

13:13 24214:14 24315:15 24416:16 245
17:17 246

11:11 24718:18

2502:2 2517:7 2526:6 253
3:3 2545:5 2554:4 2568:8 2579:9 25810:10 25912:12 260

13:13 26114:14 26215:15 26316:16 264
17:17 26511:11 266

18:18

2692:2 2704:4 2713:3 2727:7 273
6:6 2745:5 2758:8 2769:9 27710:10 27811:11 279

12:12 28013:13 28114:14 28215:15 28316:16 284
17:17 28518:18

2882:2 2893:3 2905:5 2914:4 292
7:7 2936:6 2948:8 2959:9 29610:10 29712:12 29813:13 29914:14 30015:15 30116:16 302

17:17 30311:11 30418:18

3072:2 3087:7 3096:6 3103:3 3115:5 3124:4 3138:8 3149:9 31510:10 31611:11 31712:12 31813:13 31914:14 32015:15 32116:16 322
17:17 32318:18

3262:2 3273:3 3285:5 3294:4 3307:7 3316:6 3328:8 3339:9 33410:10 33511:11 33612:12 33713:13 33814:14 33915:15 34016:16 34117:17 34218:18

3452:2 3467:7 3476:6 3484:4 3493:3 3505:5 3518:8 3529:9 35310:10 35412:12 35513:13 35614:14 35715:15 35816:16 35917:17 36011:11 36118:18

3642:2 3654:4 3663:3 3675:5 3687:7 3696:6 3708:8 3719:9 37210:10 37312:12 37413:13 37514:14 37615:15 37716:16
378

17:17
37911:11 38018:18

3832:2 3847:7 3856:6 3864:4 3873:3 3885:5 3898:8 3909:9 39110:10 392
11:11

393
12:12

39413:13 39514:14 396
15:15

39716:16 39817:17 39918:18

4022:2 4034:4 4043:3 4055:5 4067:7 4076:6 4088:8 4099:9 41010:10 411
11:11

41212:12 41313:13 41414:14 41515:15 41616:16 417
17:17

41818:18

4212:2 4225:5 4233:3 4247:7 4256:6 4264:4 4278:8 4289:9 42910:10 430
12:12

43113:13 43214:14 43315:15 43416:16 43517:17 436
11:11

437
18:18

4402:2 4414:4 4427:7 4436:6 4443:3 4455:5 4468:8 4479:9

448
10:10

44912:12 450
13:13

45114:14 45215:15 45316:16 45417:17 45511:11 456
18:18

4592:2 4605:5 4613:3 4627:7 4636:6 464
4:4

465
8:8

4669:9 46710:10 46811:11 46912:12 47013:13 471
14:14

47215:15 47316:16 474
17:17

475
18:18

4782:2 4794:4 4807:7 4816:6

482

3:3

4835:5 4848:8 4859:9 48610:10 48711:11 48812:12 48913:13 49014:14 49115:15 492
16:16

493
17:17

49418:18
4972:2 4987:7 4996:6 5005:5

501

3:3

5024:4 5038:8 5049:9 50510:10 50612:12 50713:13 50814:14 509
15:15

51016:16 511
17:17

512
11:11

51318:185162:2 5175:5 5183:3

519
4:4

5207:7 5216:6
522

8:8

523
9:9

52410:10 52512:12 52613:13 52714:14 52815:15 52916:16 53017:17 53111:11 53218:185352:2 5367:7 5376:6

538

5:5

539

3:3

5404:4 5418:8 5429:9 54310:10 54411:11 54512:12 54613:13 54714:14 54815:15 54916:16 550
17:17

551
18:18

554
2:2

5555:5

556
3:3

557
4:4

558
7:7

5596:6 5608:8 5619:9 56210:10 56311:11 56412:12 56513:13 56614:14 56715:15 56816:16
569

17:17
57018:18573

2:2

574
7:7

575

6:6

5764:4 5775:5 5783:3 5798:8 5809:9 58110:10 58211:11 58312:12 58413:13 58514:14 58615:15 587
16:16

588
17:17

58918:18
5922:2 5934:4

594

5:5

5953:3 5967:7 5976:6 5988:8 5999:9 60010:10 60111:11 60212:12 60313:13 60414:14 60515:15 606
16:16

607
17:17

608
18:18

6112:2 6125:5

613

7:7

6146:6 6153:3 6164:4 6178:8 6189:9 61910:10 62011:11 62112:12 62213:13 62314:14
624

15:15
625

16:16

626
17:17

62718:18630
2:2

6314:4

632
7:7

6336:6 6345:5 6353:3 6368:8 6379:9 63810:10 63911:11 64012:12 64113:13 64214:14 643
15:15

644
16:16

645
17:17

646
18:18

649
2:2

650
3:3

651
4:4

6525:5 6536:6 6547:7 6558:8 6569:9 65710:10 65811:11 65912:12 66013:13 66114:14 662
15:15

663
16:16

664
17:17

665
18:18

(a) Linear form.

0 10:0 21:1 32:2
193:3

11

4:4 7

5:5

4

7:7

225:5

26

4:4

20
7:7

183:3

15

5:5
127:7

3:3

8

7:7

5
6:6 21

3:3

13

4:4

65:5

24

5:5

28

4:4

3:3

145:5

3:3

31

4:4

25
4:4

237:7

96:6 103:3

42
4:4

43
8:8

5:5

27
7:7

163:3

6:6

5:5

3:3

177:7

6:6

307:7

6:6

7:7

29
5:5

6:6

6:6

32
8:8

6:6
7:7

41
6:6 7:7

339:9 3410:10

46

11:11
3512:12

4712:12

3613:13 3714:14 3815:15 3916:16 4017:17

52
11:11

5318:18

449:9 4510:10 11:11
4813:13 4914:14 5015:15 5116:16

17:17

(b) Minimized lattice.

Figure 3: Example permutation lattice.

5 Machine Translation with
Permutation Lattices

5.1 Permutation Lattices

We call a permutation lattice for sentence s =
⟨s1, . . . , sn⟩ an acyclic finite-state automaton
where every path from the initial state reaches
an accepting state in exactly n uniquely labeled
transitions. Transitions are labeled with pairs in
{(i, si)ni=1} and each path represents an arbitrary
permutation of the source’s n tokens.

In a permutation lattice with states Q and transi-
tions E, every path between any two states u, v ∈
Q has exactly the same length. Let out∗(x) de-
note the transitive closure of x ∈ Q, that is, the set
of states reachable from x. If two nodes are at all
connected, v ∈ out∗(u), then the distance between
them equals dv −du, where dx is x’s distance from
the initial state. This observation allows a speed
up of non-monotone translation of a permutation
lattice. Namely, to precompute shortest distances,
necessary to impose a distortion limit, instead of
running a fully fledged all-pairs shortest path al-
gorithm O(|Q|3) (Cormen et al., 2001), we can
compute transitive closure in time O(|Q| × |E|)
(Simon, 1988) followed by single-source distance
in time O(|Q| + |E|) (Mohri, 2002).

We produce permutation lattices by compress-
ing the n-best outputs from the reordering mod-
els into a minimal deterministic acceptor. Un-
weighted determinization and minimization are
performed using OpenFST (Allauzen et al., 2007).
The results of this process are very compact rep-
resentations that can be decoded efficiently. As an
illustration, Figure 3 shows an English sentence
from WMT newstest 2014 preordered for transla-
tion into German before (3a) and after minimiza-
tion (3b).9 Table 1 shows the influence of the num-
ber of predicted permutations on the lattice sizes

9Example sentence: The Kluser lights protect cyclists, as
well as those travelling by bus and the residents of Bergle.

124



for English–German. Permutation quality is mea-
sured by Kendall τ distance to the gold permuta-
tion (best-out-of-n).

Lattice

Permutations Kendall τ States Transitions

Monotone 83.78 23 22

5 84.69 24 52
10 85.23 33 69

100 86.20 72 138
1000 86.75 123 233

Table 1: Permutations and lattice size (En–De).

5.2 Lattice Silver Training

While for first-best word order predictions, there
are two straight-forward options for how to se-
lect training instances for the MT system, it is less
clear how to do this in the case of permutation lat-
tices. In standard preordering, the word order of
the source sentence in the training set is commonly
determined by reordering the source sentence to
minimize the number of crossing alignment links
(we denote this as s′). Alternatively, the trained
preordering model can be applied to the source
side of the training set, which we call ŝ′1. There
is a trade-off between both methods: While s′ will
generally produce more compact and less noisy
phrase tables, it may include phrases that are not
reachable by the preordering model. The predicted
order ŝ′1, on the other hand, may be too constrained
to reach helpful hypotheses. For lattices, one op-
tion would be to extract all possible phrases from
the lattice directly. Here, we consider a simpler al-
ternative: Instead of selecting either the gold order
s′ or the predicted order ŝ′1, we select the order ŝ

′

which is closest to both the lattice predictions and
the gold order s′. Since this order is a mix of the
lattice predictions and the gold order, we call this
training scheme lattice silver training.

Let (s, t) be a training instance consisting of a
source sentence s and a target sentence t and let
s′ be the target-order source sentence obtained via
the word alignments. For each training instance,
we select the preordered source ŝ′ as follows:

ŝ′ = arg max
ŝ′L∈ πk(s)

overlap(ŝ′
L
, s′)

where πk(s) is the set of k-best permutations pre-
dicted by the preordering model. Each ŝ′

L
∈ πk(s)

represents a single path through the lattice. As

the cost function, we use n-gram overlap, as com-
monly used in string kernels (Lodhi et al., 2002):

overlap(ŝ′
L
, s′) =

7∑

n=2


 ∑

c ∈ Cn
s′

countŝ′L(c)




where Cns′ denotes all candidate n-grams of length
n in s′ and countŝ′L(c) denotes the number of oc-
currences of n-gram c in ŝ′

L
. Ties between permu-

tations with the same overlap are broken using the
permutations’ scores from the preordering model.

6 Experiments

6.1 Experimental Setup
In our translation experiments, we use the follow-
ing experimental setup, datasets and parameters.

Translation system Translation experiments are
performed with a phrase-based machine transla-
tion system, a version of Moses (Koehn et al.,
2007) with extended lattice support.10 We use the
basic Moses features and perform 15 iterations of
batch MIRA (Cherry and Foster, 2012).

English–Japanese Our experiments are per-
formed on the NTCIR-8 Patent Translation
(PATMT) Task. Tuning is performed on the
NTCIR-7 dev sets, and translation is evaluated on
the test set from NTCIR-9. All data is tokenized
(using the Moses tokenizer for English and KyTea
5 for Japanese (Neubig et al., 2011)) and filtered
for sentences between 4 and 50 words. As a base-
line we use a translation system with distortion
limit 6 and a lexicalized reordering model (Galley
and Manning, 2008). We use a 5-gram language
model estimated using lmplz (Heafield et al., 2013)
on the target side of the parallel corpus.

English–German For translation into German,
we built a machine translation system based on the
WMT 2016 news translation data.11 The system is
trained on all available parallel data, consisting of
4.5m sentence pairs from Europarl (Koehn, 2005),
Common Crawl (Smith et al., 2013) and the News
Commentary corpus. We removed all sentences
longer than 80 words and tokenization and true-
casing is performed using the standard Moses tok-
enizer and truecaser. We use a 5-gram Kneser-Ney
language model, estimated using lmplz (Heafield

10Made available at https://github.com/
wilkeraziz/mosesdecoder.

11http://statmt.org/wmt16/

125



et al., 2013). The language model is trained on
189m sentences from the target sides of Europarl
and News Commentary, as well as the News Crawl
2007-2015 corpora. Word alignment is performed
using MGIZA (gdfa with 6, 6, 3 and 3 iterations
of IBM M1, HMM, IBM M3 and IBM M4). As
a baseline we use a translation system with dis-
tortion limit 6 and a distortion-based reordering
model. Tuning is performed on newstest 2014 and
we evaluate on newstest 2015.

Preordering models For German, we use the
neural lattice preordering model introduced in
Section 4.1. The model is trained on the full par-
allel training data (4.5m sentences) based on the
automatic word alignments used by the translation
system. Source dependency trees are produced by
TurboParser,12 which was trained on the English
version of HamleDT (Zeman et al., 2012) with
content-head dependencies. For translation into
Japanese, we train a Reordering Grammar model
for 10 iterations of EM on a training set consisting
of 786k sentence pairs with automatic alignments.

6.2 Translation Experiments

We report lowercased BLEU (Papineni et al.,
2002) and Kendall τ calculated from the force-
aligned hypothesis and reference. Statistical sig-
nificance tests are performed for the translation
scores using the bootstrap resampling method with
p-value < 0.05 (Koehn, 2004). The standard pre-
ordering systems (“first-best” in Table 2 and 4) use
an additional lexicalized reordering model (MSD),
while the lattice systems use only lattice distor-
tion. For training preordered translation models,
we recreate word alignments from the original
MGIZA alignments and the permutation for En–
De and re-align preordered and target sentences
for En–Ja using MGIZA.13

English–German Translation results for trans-
lation into German are shown in Table 2.

For this language pair, we found standard pre-
ordering to work poorly. This is despite the fact
that the oracle order (i.e. the source words in
the test set are preordered according to the word
alignments) shows significant potential. A lattice
packed with 1000 permutations on the other hand,

12http://cs.cmu.edu/˜ark/TurboParser/
13Re-aligning the sentences with MGIZA generally im-

proves results, which implies that we are likely underestimat-
ing the results for En–De.

Translation Word order

DL BLEU Kendall τ

Baseline 6 21.76 54.75

Oracle order 6 26.68 58.050 26.41 57.92

First-best 6 21.21A 53.44
Lattice (silver) 0 21.88B 54.51

AStat. significant against baseline. BStat. significant against first-best.

Table 2: Translation results English–German.

performs better even when translating monotoni-
cally with a distortion limit of 0.

Lattice silver training To examine the utility of
the lattice silver training scheme, we train sys-
tems which differ only in the way the training
data is extracted. Table 3 shows that for English–
German, lattice silver training is successful in
bridging the gap between the preordering model
and the alignment-based target word order, both
for monotonic translation and when allowing the
decoder to additionally reorder translations.

Distortion limit

0 3

Gold training 21.44 21.60
Lattice silver training 21.88 21.88

Table 3: Lattice silver training (BLEU, En–De).

English–Japanese Results for translation into
Japanese are shown in Table 4.

Discussion Although preordering with a single
permutation already works well for the strict word
order language Japanese, packing the word order
ambiguity into a lattice allows the machine trans-
lation system to achieve even better translation
monotonically than allowing a distortion of 6 and
an additional lexicalized reordering model on top

Translation Word order

DL BLEU Kendall τ

Baseline 6 29.65 44.87

Oracle order 6 34.22 56.230 30.55 53.98

First-best 6 32.14A 49.68
Lattice 0 32.50AB 50.79

AStat. significant against baseline. BStat. significant against first-best.

Table 4: Translation results English–Japanese.

126



of a single permutation. We noticed that lexical-
ized reordering helped the first-best systems and
hence report this stronger baseline. In principle,
lexicalized reordering can also be used with 0-
distortion lattice translation, and we plan to inves-
tigate this option in the future. Linguistic intuition
and the empirical results presented in Section 3
suggest that compared to Japanese, German shows
more word order freedom. Consequently, we as-
sumed that a first-best preordering model would
not perform well on the language pair English–
German, and indeed the results in Table 2 confirm
this assumption. For both language pairs, translat-
ing a lattice of predicted permutations outperforms
the baselines, thus reducing the gap between trans-
lation with predicted word order and oracle word
order. However, permutation lattices turn out to be
the key to enabling any improvement at all for the
language pair English–German in the context of
preordering. This language pair can benefit from
the improved interaction between word order and
translation decisions. These findings go in tandem
with our analysis in Section 3 (see Figures 1 and
2), particularly, the prediction of our information-
theoretic word order freedom metric that it should
be more difficult to determine German word or-
der from English clues. Our main focus in this
paper was on the language pairs English–German
and English–Japanese. Hence, while our results
provide an empirical data point for the utility of
permutation lattices for free word order languages,
we plan to provide further empirical support by
performing experiments with a broader range of
language pairs in future work.

7 Conclusion

The world’s languages differ widely in how they
express meaning, relying on indicators such as
word order, intonation or morphological mark-
ings. Consequently, some languages exhibit
stricter word order than others. Our goal in this
paper was to examine the effect of word order
freedom on machine translation and preordering.
We provided an empirical comparison of language
pairs in terms of the difficulty of predicting the tar-
get language’s word order based on the source lan-
guage. Our metric’s predictions agree both with
the intuition provided by linguistic theory and the
empirical support we present in the form of trans-
lation experiments. We show that addressing un-
certainty in word order predictions, and in par-

ticular doing so with permutation lattices, can be
an indispensable tool for dealing with word order
in machine translation. The experiments we per-
formed in this paper confirm this previous finding
and we further build on it by introducing a new
method for training machine translation systems
for lattice-preordered input, which we call lattice
silver training. Finally, we found that while lat-
tices are indeed helpful for English–Japanese, for
which standard preordering already works well,
they are crucial for translation into the freer word
order language German.

Acknowledgements

We thank the three anonymous reviewers for their
constructive comments and suggestions. This
work received funding from EXPERT (EU FP7
Marie Curie ITN nr. 317471), NWO VICI grant
nr. 277-89-002 (Khalil Sima’an), DatAptor project
STW grant nr. 12271 and QT21 project (H2020 nr.
645452).

References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-

jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Ninth International
Conference on Implementation and Application of
Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11–23. Springer.
http://www.openfst.org.

Egon Balas and Paolo Toth. 1983. Branch and bound
methods for the traveling salesman problem. Tech-
nical report, Carnegie-Mellon Univ. Pittsburgh PA
Management Sciences Research Group.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.

Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network
decoding. In IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 4
of ICASSP ’07, pages 1297–1300, Honolulu, HI,
April. IEEE.

Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427–436, Montréal, Canada, June.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine

127



translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 531–540, Ann Arbor,
Michigan, June.

Thomas H. Cormen, Clifford Stein, Ronald L. Rivest,
and Charles E. Leiserson. 2001. Introduction to Al-
gorithms. McGraw-Hill Higher Education, 2nd edi-
tion.

Marta R. Costa-jussà and José A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 70–76, Sydney,
Australia, July.

Adrià de Gispert, Gonzalo Iglesias, and Bill Byrne.
2015. Fast and accurate preordering for SMT us-
ing neural networks. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1012–1017, Denver,
Colorado, May–June.

Simon DeDeo, Robert X. D. Hawkins, Sara Klin-
genstein, and Tim Hitchcock. 2013. Bootstrap
methods for the empirical study of decision-making
and information flows in social systems. Entropy,
15(6):2246–2276.

John DeNero and Jakob Uszkoreit. 2011. Inducing
sentence structure from parallel corpora for reorder-
ing. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 193–203, Edinburgh, Scotland, UK., July.

John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages
567–575, Stroudsburg, PA, USA.

Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012–1020,
Columbus, Ohio, June.

Christopher J. Dyer. 2007. The “noisier chan-
nel”: Translation from morphologically complex
languages. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 207–211,
Prague, Czech Republic, June.

M. Amin Farajian, Nicola Bertoldi, and Marcello Fed-
erico. 2014. Online word alignment for online
adaptive machine translation. In Proceedings of the
EACL 2014 Workshop on Humans and Computer-
assisted Translation, pages 84–92, Gothenburg,
Sweden, April.

Richard Futrell, Kyle Mahowald, and Edward Gibson.
2015. Quantifying word order freedom in depen-
dency corpora. In Proceedings of the Third In-
ternational Conference on Dependency Linguistics

(Depling 2015), pages 91–100, Uppsala, Sweden,
August. Uppsala University, Uppsala, Sweden.

Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848–856, Honolulu, Hawaii, October.

João Graça, Joana Paulo Pardal, and Luı́sa Coheur.
2008. Building a golden collection of parallel multi-
language word alignments.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria, August.

Teresa Herrmann, Jochen Weiner, Jan Niehues, and
Alex Waibel. 2013. Analyzing the potential of
source sentence reordering in statistical machine
translation. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT
2013).

Laura Jehl, Adrià de Gispert, Mark Hopkins, and Bill
Byrne. 2014. Source-side preordering for transla-
tion using logistic regression and depth-first branch-
and-bound search. In Proceedings of the 14th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 239–248,
Gothenburg, Sweden, April.

Maxim Khalilov and Khalil Sima’an. 2012. Statistical
translation after source reordering: Oracles, context-
aware models, and empirical analysis. Natural Lan-
guage Engineering, 18:491–519, 10.

Maxim Khalilov, José A. R. Fonollosa, and Mark Dras.
2009. Coupling hierarchical word reordering and
decoding in phrase-based statistical machine transla-
tion. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, SSST
’09, pages 78–86, Stroudsburg, PA, USA.

Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Seventh International Work-
shop on Parsing Technologies (IWPT- 2001), Octo-
ber.

Kevin Knight and Yaser Al-Onaizan. 1998. Transla-
tion with finite-state devices. In Proceedings of the
Association for Machine Translation in the Ameri-
cas, AMTA, pages 421–437, Langhorne, PA, USA.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA.

128



Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388–395.

Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X, volume 5, pages
79–86.

Vladislav Kuboň and Markéta Lopatková. 2015. Free
or fixed word order: What can treebanks reveal?
In Jakub Yaghob, editor, ITAT 2015: Information
Technologies Applications and Theory, Proceedings
of the 15th conference ITAT 2015, volume 1422 of
CEUR Workshop Proceedings, pages 23–29, Praha,
Czechia. Charles University in Prague, CreateSpace
Independent Publishing Platform.

Shankar Kumar and William Byrne. 2003. A weighted
finite state transducer implementation of the align-
ment template model for statistical machine trans-
lation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ’03, pages 63–70,
Stroudsburg, PA, USA.

K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35–56.

Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 513–523,
Seattle, Washington, USA, October.

Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. J. Mach. Learn.
Res., 2:419–444, March.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05),
pages 75–82, Ann Arbor, Michigan, June.

Mehryar Mohri. 2002. Semiring frameworks and
algorithms for shortest-distance problems. Jour-
nal of Automata, Languages and Combinatorics,
7(3):321–350, January.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 529–533, Portland, Oregon, USA, June.

Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.

Hermann Ney. 1999. Speech translation: coupling of
recognition and translation. In IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, volume 1, pages 517–520, Phoenix, AZ,
March. IEEE.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Sebastian Padó and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161–1168, Sydney,
Australia, July.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, pages 311–318.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433–440, Stroudsburg, PA, USA.

Detlef Prescher. 2005. Inducing head-driven pcfgs
with latent heads: Refining a tree-bank grammar for
parsing. In In ECML05.

K. Simon. 1988. An improved algorithm for transitive
closure on acyclic digraphs. Theor. Comput. Sci.,
58(1-3):325–346, June.

Jason R. Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale parallel
text from the common crawl. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1374–1383, Sofia, Bulgaria, August.

Miloš Stanojević and Khalil Sima’an. 2015. Reorder-
ing grammar induction. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 44–54, Lisbon, Portugal,
September.

129



Jörg Tiedemann. 2012. Parallel data, tools and
interfaces in opus. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Ugur Dogan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12), Is-
tanbul, Turkey, may.

Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007–1016,
Singapore, August.

Edo S van der Poort, Marek Libura, Gerard Sierksma,
and Jack A.A van der Veen. 1999. Solving the k-
best traveling salesman problem. Computers & Op-
erations Research, 26(4):409 – 425.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.

Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated chinese word segmentation
in statistical machine translation. In International
Workshop on Spoken Language Translation, Pitts-
burgh.

Daniel Zeman, David Mareček, Martin Popel,
Loganathan Ramasamy, Jan Štěpánek, Zdeněk
Žabokrtský, and Jan Hajič. 2012. Hamledt:
To parse or not to parse? In Proceedings of
the Eight International Conference on Language
Resources and Evaluation (LREC’12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).

Hao Zhang and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time.
In NAACL Workshop on Syntax and Structure in Sta-
tistical Translation (SSST), pages 25–32.

Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statistical
machine translation. In Proceedings of the NAACL-
HLT 2007/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation, SSST ’07, pages 1–8,
Stroudsburg, PA, USA.

Daniel Zwillinger and Stephen Kokoska. 1999. CRC
Standard Probability and Statistics Tables and For-
mulae. CRC Press.

130


