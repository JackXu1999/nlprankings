











































Token-level and sequence-level loss smoothing for RNN language models


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2094–2103
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2094

Token-level and sequence-level loss smoothing for RNN language models

Maha Elbayad1,2 Laurent Besacier1 Jakob Verbeek2
Univ. Grenoble Alpes, CNRS, Grenoble INP, Inria, LIG, LJK, F-38000 Grenoble France

1 firstname.lastname@univ-grenoble-alpes.fr
2 firstname.lastname@inria.fr

Abstract

Despite the effectiveness of recurrent neu-
ral network language models, their max-
imum likelihood estimation suffers from
two limitations. It treats all sentences that
do not match the ground truth as equally
poor, ignoring the structure of the out-
put space. Second, it suffers from “ex-
posure bias”: during training tokens are
predicted given ground-truth sequences,
while at test time prediction is conditioned
on generated output sequences. To over-
come these limitations we build upon the
recent reward augmented maximum likeli-
hood approach i.e. sequence-level smooth-
ing that encourages the model to predict
sentences close to the ground truth accord-
ing to a given performance metric. We
extend this approach to token-level loss
smoothing, and propose improvements to
the sequence-level smoothing approach.
Our experiments on two different tasks,
image captioning and machine translation,
show that token-level and sequence-level
loss smoothing are complementary, and
significantly improve results.

1 Introduction

Recurrent neural networks (RNNs) have recently
proven to be very effective sequence modeling
tools, and are now state of the art for tasks such
as machine translation (Cho et al., 2014; Sutskever
et al., 2014; Bahdanau et al., 2015), image caption-
ing (Kiros et al., 2014; Vinyals et al., 2015; Ander-
son et al., 2017) and automatic speech recognition
(Chorowski et al., 2015; Chiu et al., 2017).

The basic principle of RNNs is to iteratively
compute a vectorial sequence representation, by
applying at each time-step the same trainable func-

tion to compute the new network state from the
previous state and the last symbol in the sequence.
These models are typically trained by maximizing
the likelihood of the target sentence given an en-
coded source (text, image, speech).

Maximum likelihood estimation (MLE), how-
ever, has two main limitations. First, the training
signal only differentiates the ground-truth target
output from all other outputs. It treats all other
output sequences as equally incorrect, regardless
of their semantic proximity from the ground-truth
target. While such a “zero-one” loss is probably
acceptable for coarse grained classification of im-
ages, e.g. across a limited number of basic ob-
ject categories (Everingham et al., 2010) it be-
comes problematic as the output space becomes
larger and some of its elements become semanti-
cally similar to each other. This is in particular the
case for tasks that involve natural language gener-
ation (captioning, translation, speech recognition)
where the number of possible outputs is practically
unbounded. For natural language generation tasks,
evaluation measures typically do take into account
structural similarity, e.g. based on n-grams, but
such structural information is not reflected in the
MLE criterion. The second limitation of MLE is
that training is based on predicting the next token
given the input and preceding ground-truth output
tokens, while at test time the model predicts condi-
tioned on the input and the so-far generated output
sequence. Given the exponentially large output
space of natural language sentences, it is not obvi-
ous that the learned RNNs generalize well beyond
the relatively sparse distribution of ground-truth
sequences used during MLE optimization. This
phenomenon is known as “exposure bias” (Ran-
zato et al., 2016; Bengio et al., 2015).

MLE minimizes the KL divergence between a
target Dirac distribution on the ground-truth sen-
tence(s) and the model’s distribution. In this pa-



2095

per, we build upon the “loss smoothing” approach
by Norouzi et al. (2016), which smooths the Dirac
target distribution over similar sentences, increas-
ing the support of the training data in the output
space. We make the following main contributions:
• We propose a token-level loss smooth-

ing approach, using word-embeddings, to
achieve smoothing among semantically sim-
ilar terms, and we introduce a special proce-
dure to promote rare tokens.
• For sequence-level smoothing, we propose to

use restricted token replacement vocabular-
ies, and a “lazy evaluation” method that sig-
nificantly speeds up training.
• We experimentally validate our approach on

the MSCOCO image captioning task and the
WMT’14 English to French machine trans-
lation task, showing that on both tasks com-
bining token-level and sequence-level loss
smoothing improves results significantly over
maximum likelihood baselines.

In the remainder of the paper, we review the ex-
isting methods to improve RNN training in Sec-
tion 2. Then, we present our token-level and
sequence-level approaches in Section 3. Experi-
mental evaluation results based on image caption-
ing and machine translation tasks are laid out in
Section 4.

2 Related work

Previous work aiming to improve the generaliza-
tion performance of RNNs can be roughly divided
into three categories: those based on regulariza-
tion, data augmentation, and alternatives to maxi-
mum likelihood estimation.

Regularization techniques are used to increase
the smoothness of the function learned by the
network, e.g. by imposing an `2 penalty on the
network weights, also known as “weight decay”.
More recent approaches mask network activations
during training, as in dropout (Srivastava et al.,
2014) and its variants adapted to recurrent mod-
els (Pham et al., 2014; Krueger et al., 2017). In-
stead of masking, batch-normalization (Ioffe and
Szegedy, 2015) rescales the network activations
to avoid saturating the network’s non-linearities.
Instead of regularizing the network parameters or
activations, it is also possible to directly regular-
ize based on the entropy of the output distribution
(Pereyra et al., 2017).

Data augmentation techniques improve the ro-

bustness of the learned models by applying trans-
formations that might be encountered at test time
to the training data. In computer vision, this is
common practice, and implemented by, e.g., scal-
ing, cropping, and rotating training images (Le-
Cun et al., 1998; Krizhevsky et al., 2012; Paulin
et al., 2014). In natural language processing, ex-
amples of data augmentation include input noising
by randomly dropping some input tokens (Iyyer
et al., 2015; Bowman et al., 2015; Kumar et al.,
2016), and randomly replacing words with sub-
stitutes sampled from the model (Bengio et al.,
2015). Xie et al. (2017) introduced data augmenta-
tion schemes for RNN language models that lever-
age n-gram statistics in order to mimic Kneser-
Ney smoothing of n-grams models. In the con-
text of machine translation, Fadaee et al. (2017)
modify sentences by replacing words with rare
ones when this is plausible according to a pre-
trained language model, and substitutes its equiv-
alent in the target sentence using automatic word
alignments. This approach, however, relies on the
availability of additional monolingual data for lan-
guage model training.

The de facto standard way to train RNN lan-
guage models is maximum likelihood estimation
(MLE) (Cho et al., 2014; Sutskever et al., 2014;
Bahdanau et al., 2015). The sequential factoriza-
tion of the sequence likelihood generates an ad-
ditive structure in the loss, with one term corre-
sponding to the prediction of each output token
given the input and the preceding ground-truth
output tokens. In order to directly optimize for
sequence-level structured loss functions, such as
measures based on n-grams like BLEU or CIDER,
Ranzato et al. (2016) use reinforcement learn-
ing techniques that optimize the expectation of a
sequence-level reward. In order to avoid early con-
vergence to poor local optima, they pre-train the
model using MLE.

Leblond et al. (2018) build on the learn-
ing to search approach to structured prediction
(Daumé III et al., 2009; Chang et al., 2015) and
adapts it to RNN training. The model generates
candidate sequences at each time-step using all
possible tokens, and scores these at sequence-level
to derive a training signal for each time step. This
leads to an approach that is structurally close to
MLE, but computationally expensive. Norouzi
et al. (2016) introduce a reward augmented maxi-
mum likelihood (RAML) approach, that incorpo-



2096

rates a notion of sequence-level reward without
facing the difficulties of reinforcement learning.
They define a target distribution over output sen-
tences using a soft-max over the reward over all
possible outputs. Then, they minimize the KL di-
vergence between the target distribution and the
model’s output distribution. Training with a gen-
eral reward distribution is similar to MLE train-
ing, except that we use multiple sentences sam-
pled from the target distribution instead of only the
ground-truth sentences.

In our work, we build upon the work of
Norouzi et al. (2016) by proposing improvements
to sequence-level smoothing, and extending it to
token-level smoothing. Our token-level smooth-
ing approach is related to the label smoothing ap-
proach of Szegedy et al. (2016) for image clas-
sification. Instead of maximizing the probability
of the correct class, they train the model to pre-
dict the correct class with a large probability and
all other classes with a small uniform probabil-
ity. This regularizes the model by preventing over-
confident predictions. In natural language gen-
eration with large vocabularies, preventing such
“narrow” over-confident distributions is impera-
tive, since for many tokens there are nearly inter-
changeable alternatives.

3 Loss smoothing for RNN training

We briefly recall standard recurrent neural net-
work training, before presenting sequence-level
and token-level loss smoothing below.

3.1 Maximum likelihood RNN training
We are interested in modeling the conditional
probability of a sequence y = (y1, . . . , yT ) given
a conditioning observation x,

pθ(y|x) =
T∏
t=1

pθ(yt|x, y<t), (1)

where y<t = (y1, . . . , yt−1), the model parame-
ters are given by θ, and x is a source sentence or an
image in the contexts of machine translation and
image captioning, respectively.

In a recurrent neural network, the sequence y is
predicted based on a sequence of states ht,

pθ(yt|x, y<t) = pθ(yt|ht), (2)

where the RNN state is computed recursively as

ht =

{
fθ(ht−1, yt−1, x) for t ∈ {1, ..T},
gθ(x) for t = 0.

(3)

The input is encoded by gθ and used to initialize
the state sequence, and fθ is a non-linear function
that updates the state given the previous state ht−1,
the last output token yt−1, and possibly the input
x. The state update function can take different
forms, the ones including gating mechanisms such
as LSTMs (Hochreiter and Schmidhuber, 1997)
and GRUs (Chung et al., 2014) are particularly ef-
fective to model long sequences.

In standard teacher-forced training, the hidden
states will be computed by forwarding the ground
truth sequence y∗ i.e. in Eq. (3), the RNN has ac-
cess to the true previous token y∗t−1. In this case
we will note the hidden states h∗t .

Given a ground-truth target sequence y∗, maxi-
mum likelihood estimation (MLE) of the network
parameters θ amounts to minimizing the loss

`MLE(y
∗, x) = − ln pθ(y∗|x) (4)

= −
T∑
t=1

ln pθ(y
∗
t |h∗t ). (5)

The loss can equivalently be expressed as the KL-
divergence between a Dirac centered on the target
output (with δa(x) = 1 at x = a and 0 otherwise)
and the model distribution, either at the sequence-
level or at the token-level:

`MLE(y
∗, x) = DKL

(
δy∗ ||pθ(y|x)

)
(6)

=
T∑
t=1

DKL
(
δy∗t ||pθ(yt|h

∗
t )
)
. (7)

Loss smoothing approaches considered in this pa-
per consist in replacing the Dirac on the ground-
truth sequence with distributions with larger sup-
port. These distributions can be designed in such
a manner that they reflect which deviations from
ground-truth predictions are preferred over others.

3.2 Sequence-level loss smoothing
The reward augmented maximum likelihood ap-
proach of Norouzi et al. (2016) consists in replac-
ing the sequence-level Dirac δy∗ in Eq. (6) with a
distribution

r(y|y∗) ∝ exp r(y, y∗)/τ, (8)

where r(y, y∗) is a “reward” function that mea-
sures the quality of sequence y w.r.t. y∗, e.g. met-
rics used for evaluation of natural language pro-
cessing tasks can be used, such as BLEU (Pap-
ineni et al., 2002) or CIDER (Vedantam et al.,



2097

2015). The temperature parameter τ controls
the concentration of the distribution around y∗.
When m > 1 ground-truth sequences are paired
with the same input x, the reward function can
be adapted to fit this setting and be defined
as r(y, {y∗(1), . . . , y∗(m)}). The sequence-level
smoothed loss function is then given by

`Seq(y
∗, x) = DKL

(
r(y|y∗)||pθ(y|x)

)
= H(r(y|y∗))− Er[ln pθ(y|x)] , (9)

where the entropy term H(r(y|y∗)) does not de-
pend on the model parameters θ.

In general, expectation in Eq. (9) is intractable
due to the exponentially large output space, and
replaced with a Monte-Carlo approximation:

Er[− ln pθ(y|x)] ≈ −
L∑
l=1

ln pθ(y
l|x). (10)

Stratified sampling. Norouzi et al. (2016) show
that when using the Hamming or edit distance as a
reward, we can sample directly from r(y|y∗) us-
ing a stratified sampling approach. In this case
sampling proceeds in three stages. (i) Sample a
distance d from {0, . . . , T} from a prior distribu-
tion on d. (ii) Uniformly select d positions in the
sequence to be modified. (iii) Sample the d substi-
tutions uniformly from the token vocabulary.

Details on the construction of the prior distri-
bution on d for a reward based on the Hamming
distance can be found in Appendix A.

Importance sampling. For a reward based on
BLEU or CIDER , we cannot directly sample from
r(y|y∗) since the normalizing constant, or “parti-
tion function”, of the distribution is intractable to
compute. In this case we can resort to importance
sampling. We first sample L sequences yl from
a tractable proposal distribution q(y|y∗). We then
compute the importance weights

ωl ≈
r(yl|y∗)/q(yl|y∗)∑L
k=1 r(y

k|y∗)/q(yk|y∗)
, (11)

where r(yk|y∗) is the un-normalized reward distri-
bution in Eq. (8). We finally approximate the ex-
pectation by reweighing the samples in the Monte
Carlo approximation as

Er[− ln pθ(y|x)] ≈ −
L∑
l=1

ωl ln pθ(y
l|x). (12)

In our experiments we use a proposal distribu-
tion based on the Hamming distance, which al-
lows for tractable stratified sampling, and gener-
ates sentences that do not stray away from the
ground truth.

We propose two modifications to the sequence-
level loss smoothing of Norouzi et al. (2016):
sampling to a restricted vocabulary (described in
the following paragraph) and lazy sequence-level
smoothing (described in section 3.4).

Restricted vocabulary sampling. In the strati-
fied sampling method for Hamming and edit dis-
tance rewards, instead of drawing from the large
vocabulary V , containing typically in the order of
104 words or more, we can restrict ourselves to a
smaller subset Vsub more adapted to our task. We
considered three different possibilities for Vsub.
V : the full vocabulary from which we sample

uniformly (default), or draw from our token-level
smoothing distribution defined below in Eq. (13).
Vrefs: uniformly sample from the set of tokens

that appear in the ground-truth sentence(s) associ-
ated with the current input.
Vbatch: uniformly sample from the tokens that

appear in the ground-truth sentences across all in-
puts that appear in a given training mini-batch.

Uniformly sampling from Vbatch has the effect
of boosting the frequencies of words that appear
in many reference sentences, and thus approxi-
mates to some extent sampling substitutions from
the uni-gram statistics of the training set.

3.3 Token-level loss smoothing

While the sequence-level smoothing can be di-
rectly based on performance measures of inter-
est such as BLEU or CIDEr, the support of the
smoothed distribution is limited to the number
of samples drawn during training. We propose
smoothing the token-level Diracs δy∗t in Eq. (7) to
increase its support to similar tokens. Since we
apply smoothing to each of the tokens indepen-
dently, this approach implicitly increases the sup-
port to an exponential number of sequences, un-
like the sequence-level smoothing approach. This
comes at the price, however, of a naive token-level
independence assumption in the smoothing.

We define the smoothed token-level distribu-
tion, similar as the sequence-level one, as a soft-
max over a token-level “reward” function,

r(yt|y∗t ) ∝ exp r(yt, y∗t )/τ, (13)



2098

where τ is again a temperature parameter. As
a token-level reward r(yt, y∗t ) we use the cosine
similarity between yt and y∗t in a semantic word-
embedding space. In our experiments we use
GloVe (Pennington et al., 2014); preliminary ex-
periments with word2vec (Mikolov et al., 2013)
yielded somewhat worse results.

Promoting rare tokens. We can further im-
prove the token-level smoothing by promoting rare
tokens. To do so, we penalize frequent tokens
when smoothing over the vocabulary, by subtract-
ing β freq(yt) from the reward, where freq(·) de-
notes the term frequency and β is a non-negative
weight. This modification encourages frequent to-
kens into considering the rare ones. We experi-
mentally found that it is also beneficial for rare
tokens to boost frequent ones, as they tend to
have mostly rare tokens as neighbors in the word-
embedding space. With this in mind, we define a
new token-level reward as:

rfreq(yt, y
∗
t ) = r(yt, y

∗
t ) (14)

− βmin
(

freq(yt)

freq(y∗t )
,
freq(y∗t )

freq(yt)

)
,

where the penalty term is strongest if both tokens
have similar frequencies.

3.4 Combining losses

In both loss smoothing methods presented above,
the temperature parameter τ controls the concen-
tration of the distribution. As τ gets smaller the
distribution peaks around the ground-truth, while
for large τ the uniform distribution is approached.
We can, however, not separately control the spread
of the distribution and the mass reserved for the
ground-truth output. We therefore introduce a sec-
ond parameter α ∈ [0, 1] to interpolate between
the Dirac on the ground-truth and the smooth dis-
tribution. Using ᾱ = 1 − α, the sequence-level
and token-level loss functions are then defined as

`αSeq(y
∗, x) = α`Seq(y

∗, x) + ᾱ`MLE(y
∗, x) (15)

= αEr[`MLE(y, x)] + ᾱ`MLE(y∗, x)
`αTok(y

∗, x) = α`Tok(y
∗, x) + ᾱ`MLE(y

∗, x) (16)

To benefit from both sequence-level and token-
level loss smoothing, we also combine them by ap-
plying token-level smoothing to the different se-
quences sampled for the sequence-level smooth-
ing. We introduce two mixing parameters α1 and

α2. The first controls to what extent sequence-
level smoothing is used, while the second controls
to what extent token-level smoothing is used. The
combined loss is defined as

`α1,α2Seq, Tok(y
∗, x, r) = α1Er[`Tok(y, x)] + ᾱ1`Tok(y∗, x)

= α1Er[α2`Tok(y, x) + ᾱ2`MLE(y, x)]
+ ᾱ1(α2`Tok(y

∗, x) + ᾱ2`MLE(y
∗, x)).

(17)

In our experiments, we use held out validation
data to set mixing and temperature parameters.

Algorithm 1 Sequence-level smoothing algorithm
Input: x, y∗
Output: `αseq(x, y∗)

Encode x to initialize the RNN
Forward y∗ in the RNN to compute the hidden states h∗t
Compute the MLE loss `MLE(y∗, x)
for l ∈ {1, . . . , L} do

Sample yl ∼ r(|̇y∗)
if Lazy then

Compute `(yl, x) = −
∑
t log pθ(y

l
t|h∗t )

else
Forward yl in the RNN to get its hidden states hlt
Compute `(yl, x) = `MLE(yl, x)

end if
end for
`αSeq(x, y

∗) = ᾱ`MLE(y
∗, x) + α

L

∑
l `(y

l, x)

Lazy sequence smoothing. Although sequence-
level smoothing is computationally efficient com-
pared to reinforcement learning approaches (Ran-
zato et al., 2016; Rennie et al., 2017), it is slower
compared to MLE. In particular, we need to for-
ward each of the samples yl through the RNN in
teacher-forcing mode so as to compute its hidden
states hlt, which are used to compute the sequence
MLE loss as

`MLE(y
l, x) = −

T∑
t=1

ln pθ(y
l
t|hlt). (18)

To speed up training, and since we already forward
the ground truth sequence in the RNN to evaluate
the MLE part of `αSeq(y

∗, x), we propose to use the
same hidden states h∗t to compute both the MLE
and the sequence-level smoothed loss. In this case:

`lazy(y
l, x) = −

T∑
t=1

ln pθ(y
l
t|h∗t ) (19)

In this manner, we only have a single instead of
L + 1 forwards-passes in the RNN. We provide
the pseudo-code for training in Algorithm 1.



2099

Without attention

Loss Reward Vsub BLEU-1 BLEU-4 CIDER

MLE 70.63 30.14 93.59
MLE + γH 70.79 30.29 93.61

Tok Glove sim 71.94 31.27 95.79
Tok Glove sim rfreq 72.39 31.76 97.47

Seq Hamming V 71.76 31.16 96.37
Seq Hamming Vbatch 71.46 31.15 96.53
Seq Hamming Vrefs 71.80 31.63 96.22

Seq, lazy Hamming V 70.81 30.43 94.26
Seq, lazy Hamming Vbatch 71.85 31.13 96.65
Seq, lazy Hamming Vrefs 71.96 31.23 95.34

Seq CIDER V 71.05 30.46 94.40
Seq CIDER Vbatch 71.51 31.17 95.78
Seq CIDER Vrefs 71.93 31.41 96.81

Seq, lazy CIDER V 71.43 31.18 96.32
Seq, lazy CIDER Vbatch 71.47 31.00 95.56
Seq, lazy CIDER Vrefs 71.82 31.06 95.66

Tok-Seq Hamming V 70.79 30.43 96.34
Tok-Seq Hamming Vbatch 72.28 31.65 96.73
Tok-Seq Hamming Vrefs 72.69 32.30 98.01
Tok-Seq CIDER V 70.80 30.55 96.89
Tok-Seq CIDER Vbatch 72.13 31.71 96.92
Tok-Seq CIDER Vrefs 73.08 32.82 99.92

With attention

BLEU-1 BLEU-4 CIDER

73.40 33.11 101.63
72.68 32.15 99.77

73.49 32.93 102.33
74.01 33.25 102.81

73.12 32.71 101.25
73.26 32.73 101.90
73.53 32.59 102.33

73.29 32.81 101.58
73.43 32.95 102.03
73.53 33.09 101.89

73.08 32.51 101.84
73.50 33.04 102.98
73.42 32.91 102.23

73.55 33.19 102.94
73.18 32.60 101.30
73.92 33.10 102.64

73.68 32.87 101.11
73.86 33.32 102.90
73.56 33.00 101.72
73.31 32.40 100.33
73.61 32.67 101.41
74.28 33.34 103.81

Table 1: MS-COCO ’s test set evaluation measures.

4 Experimental evaluation

In this section, we compare sequence prediction
models trained with maximum likelihood (MLE)
with our token and sequence-level loss smoothing
on two different tasks: image captioning and ma-
chine translation.

4.1 Image captioning

4.1.1 Experimental setup.
We use the MS-COCO datatset (Lin et al., 2014),
which consists of 82k training images each anno-
tated with five captions. We use the standard splits
of Karpathy and Li (2015), with 5k images for val-
idation, and 5k for test. The test set results are
generated via beam search (beam size 3) and are
evaluated with the MS-COCO captioning evalu-
ation tool. We report CIDER and BLEU scores
on this internal test set. We also report results ob-
tained on the official MS-COCO server that ad-
ditionally measures METEOR (Denkowski and
Lavie, 2014) and ROUGE-L (Lin, 2004). We ex-
periment with both non-attentive LSTMs (Vinyals
et al., 2015) and the ResNet baseline of the state-
of-the-art top-down attention (Anderson et al.,
2017).

The MS-COCO vocabulary consists of 9,800
words that occur at least 5 times in the training
set. Additional details and hyperparameters can

be found in Appendix B.1.

4.1.2 Results and discussion
Restricted vocabulary sampling In this sec-
tion, we evaluate the impact of the vocabulary
subset from which we sample the modified sen-
tences for sequence-level smoothing. We exper-
iment with two rewards: CIDER , which scores
w.r.t. all five available reference sentences, and
Hamming distance reward taking only a single ref-
erence into account. For each reward we train our
(Seq) models with each of the three subsets de-
tailed previously in Section 3.2, Restricted vocab-
ulary sampling.

From the results in Table 1 we note that for the
inattentive models, sampling from Vrefs or Vbatch
has a better performance than sampling from the
full vocabulary on all metrics. In fact, using
these subsets introduces a useful bias to the model
and improves performance. This improvement is
most notable using the CIDER reward that scores
candidate sequences w.r.t. to multiple references,
which stabilizes the scoring of the candidates.

With an attentive decoder, no matter the re-
ward, re-sampling sentences with words from Vref
rather than the full vocabulary V is better for both
reward functions, and all metrics. Additional ex-
perimental results, presented in Appendix B.2, ob-
tained with a BLEU-4 reward, in its single and



2100

BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDER SPICE

c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40

Google NIC+ (Vinyals et al., 2015) 71.3 89.5 54.2 80.2 40.7 69.4 30.9 58.7 25.4 34.6 53.0 68.2 94.3 94.6 18.2 63.6
Hard-Attention (Xu et al., 2015) 70.5 88.1 52.8 77.9 38.3 65.8 27.7 53.7 24.1 32.2 51.6 65.4 86.5 89.3 17.2 59.8
ATT-FCN+ (You et al., 2016) 73.1 90.0 56.5 81.5 42.4 70.9 31.6 59.9 25.0 33.5 53.5 68.2 94.3 95.8 18.2 63.1
Review Net+ (Yang et al., 2016) 72.0 90.0 55.0 81.2 41.4 70.5 31.3 59.7 25.6 34.7 53.3 68.6 96.5 96.9 18.5 64.9
Adaptive+ (Lu et al., 2017) 74.8 92.0 58.4 84.5 44.4 74.4 33.6 63.7 26.4 35.9 55.0 70.5 104.2 105.9 19.7 67.3

SCST:Att2all+† (Rennie et al., 2017) 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7 - -
LSTM-A3+†◦ (Yao et al., 2017) 78.7 93.7 62.7 86.7 47.6 76.5 35.6 65.2 27.0 35.4 56.4 70.5 116 118 - -
Up-Down+†◦ (Anderson et al., 2017) 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5 - -

Ours: Tok-Seq CIDER 72.6 89.7 55.7 80.9 41.2 69.8 30.2 58.3 25.5 34.0 53.5 68.0 96.4 99.4 - -
Ours: Tok-Seq CIDER + 74.9 92.4 58.5 84.9 44.8 75.1 34.3 64.7 26.5 36.1 55.2 71.1 103.9 104.2 - -

Table 2: MS-COCO ’s server evaluation . (+) for ensemble submissions, (†) for submissions with CIDEr
optimization and (◦) for models using additional data.

multiple references variants, further corroborate
this conclusion.

Lazy training. From the results of Table 1, we
see that lazy sequence-level smoothing is compet-
itive with exact non-lazy sequence-level smooth-
ing, while requiring roughly equivalent training
time as MLE. We provide detailed timing results
in Appendix B.3.

Overall For reference, we include in Table 1
baseline results obtained using MLE, and our im-
plementation of MLE with entropy regularization
(MLE+γH) (Pereyra et al., 2017), as well as the
RAML approach of Norouzi et al. (2016) which
corresponds to sequence-level smoothing based on
the Hamming reward and sampling replacements
from the full vocabulary (Seq, Hamming, V)

We observe that entropy smoothing is not able
to improve performance much over MLE for the
model without attention, and even deteriorates for
the attention model. We improve upon RAML
by choosing an adequate subset of vocabulary for
substitutions.

We also report the performances of token-level
smoothing, where the promotion of rare tokens
boosted the scores in both attentive and non-
attentive models.

For sequence-level smoothing, choosing a task-
relevant reward with importance sampling yielded
better results than plain Hamming distance.

Moreover, we used the two smoothing schemes
(Tok-Seq) and achieved the best results with
CIDER as a reward for sequence-level smoothing
combined with a token-level smoothing that pro-
motes rare tokens improving CIDER from 93.59
(MLE) to 99.92 for the model without attention,
and improving from 101.63 to 103.81 with atten-
tion.

Qualitative results. In Figure 1 we showcase
captions obtained with MLE and our three vari-
ants of smoothing i.e. token-level (Tok), sequence-
level (Seq) and the combination (Tok-Seq). We
note that the sequence-level smoothing tend to
generate lengthy captions overall, which is main-
tained in the combination. On the other hand, the
token-level smoothing allows for a better recogni-
tion of objects in the image that stems from the
robust training of the classifier e.g. the ’cement
block’ in the top right image or the carrots in the
bottom right. More examples are available in Ap-
pendix B.4

Comparison to the state of the art. We com-
pare our model to state-of-the-art systems on the
MS-COCO evaluation server in Table 2. We sub-
mitted a single model (Tok-Seq, CIDER , Vrefs)
as well as an ensemble of five models with differ-
ent initializations trained on the training set plus
35k images from the dev set (a total of 117k im-
ages) to the MS-COCO server. The three best
results on the server (Rennie et al., 2017; Yao
et al., 2017; Anderson et al., 2017) are trained in
two stages where they first train using MLE, be-
fore switching to policy gradient methods based
on CIDEr. Anderson et al. (2017) reported an in-
crease of 5.8% of CIDER on the test split after
the CIDER optimization. Moreover, Yao et al.
(2017) uses additional information about image
regions to train the attributes classifiers, while An-
derson et al. (2017) pre-trains its bottom-up atten-
tion model on the Visual Genome dataset (Krishna
et al., 2017). Lu et al. (2017); Yao et al. (2017)
use the same CNN encoder as ours (ResNet-
152), (Vinyals et al., 2015; Yang et al., 2016) use
Inception-v3 (Szegedy et al., 2016) for image en-
coding and Rennie et al. (2017); Anderson et al.



2101

Figure 1: Examples of generated captions with the baseline MLE and our models with attention.

(2017) use Resnet-101, both of which have similar
performances to ResNet-152 on ImageNet classi-
fication (Canziani et al., 2016).

4.2 Machine translation

4.2.1 Experimental setup.
For this task we validate the effectiveness of our
approaches on two different datasets. The first is
WMT’14 English to French, in its filtered version,
with 12M sentence pairs obtained after dynami-
cally selecting a “clean” subset of 348M words
out of the original “noisy” 850M words (Bahdanau
et al., 2015; Cho et al., 2014; Sutskever et al.,
2014). The second benchmark is IWSLT’14 Ger-
man to English consisting of around 150k pairs
for training. In all our experiments we use the at-
tentive model of (Bahdanau et al., 2015) The hy-
perparameters of each of these models as well as
any additional pre-processing can be found in Ap-
pendix C.1

To assess the translation quality we report the
BLEU-4 metric.

4.2.2 Results and analysis

Loss Reward Vsub WMT’14 IWSLT’14

MLE 30.03 27.55

tok Glove sim 30.16 27.69
tok Glove sim rfreq 30.19 27.83

Seq Hamming V 30.85 27.98
Seq Hamming Vbatch 31.18 28.54
Seq BLEU-4 Vbatch 31.29 28.56

Tok-Seq Hamming Vbatch 31.36 28.70
Tok-Seq BLEU-4 Vbatch 31.39 28.74

Table 3: Tokenized BLEU score on WMT’14
En-Fr evaluated on the news-test-2014 set. And
Tokenzied, case-insensitive BLEU on IWSLT’14
De-En.

We present our results in Table 3. On both
benchmarks, we improve on both MLE and
RAML approach of Norouzi et al. (2016) (Seq,
Hamming, V): using the smaller batch-vocabulary
for replacement improves results, and using im-
portance sampling based on BLEU-4 further
boosts results. In this case, unlike in the cap-
tioning experiment, token-level smoothing brings
smaller improvements. The combination of both
smoothing approaches gives best results, similar
to what was observed for image captioning, im-
proving the MLE BLEU-4 from 30.03 to 31.39 on
WMT’14 and from 27.55 to 28.74 on IWSLT’14.
The outputs of our best model are compared to
the MLE in some examples showcased in Ap-
pendix C.

5 Conclusion

We investigated the use of loss smoothing ap-
proaches to improve over maximum likelihood es-
timation of RNN language models. We gener-
alized the sequence-level smoothing RAML ap-
proach of Norouzi et al. (2016) to the token-
level by smoothing the ground-truth target across
semantically similar tokens. For the sequence-
level, which is computationally expensive, we in-
troduced an efficient “lazy” evaluation scheme,
and introduced an improved re-sampling strat-
egy. Experimental evaluation on image captioning
and machine translation demonstrates the comple-
mentarity of sequence-level and token-level loss
smoothing, improving over both the maximum
likelihood and RAML.

Acknowledgment. This work has been par-
tially supported by the grant ANR-16-CE23-0006
“Deep in France” and LabEx PERSYVAL-Lab
(ANR-11-LABX-0025-01).



2102

References
P. Anderson, X. He, C. Buehler, D. Teney, M. John-

son, S. Gould, and L. Zhang. 2017. Bottom-
up and top-down attention for image captioning
and visual question answering. arXiv preprint
arXiv:1707.07998.

D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural
machine translation by jointly learning to align and
translate. In ICLR.

S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. 2015.
Scheduled sampling for sequence prediction with re-
current neural networks. In NIPS.

S. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Jozefow-
icz, and S. Bengio. 2015. Generating sentences from
a continuous space. In CoNLL.

A. Canziani, A. Paszke, and E. Culurciello. 2016. An
analysis of deep neural network models for practical
applications. arXiv preprint arXiv:1605.07678.

K.-W. Chang, A. Krishnamurthy, A. Agarwal,
H. Daumé III, and J. Langford. 2015. Learning to
search better than your teacher. In ICML.

C.-C. Chiu, T. Sainath, Y. Wu, R. Prabhavalkar,
P. Nguyen, Z. Chen, A. Kannan, R.-J. Weiss, K. Rao,
E. Gonina, N. Jaitly, B. Li, J. Chorowski, and
M. Bacchiani. 2017. State-of-the-art speech recog-
nition with sequence-to-sequence models. arXiv
preprint arXiv:1712.01769.

K. Cho, B. van Merrienboer, Ç. Gülçehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Bengio.
2014. Learning phrase representations using RNN
encoder-decoder for statistical machine translation.
In Empirical Methods in Natural Language Process-
ing.

J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and
Y. Bengio. 2015. Attention-based models for speech
recognition. In NIPS.

J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. 2014.
Empirical evaluation of gated recurrent neural net-
works on sequence modeling. In NIPS Deep Learn-
ing Workshop.

H. Daumé III, J. Langford, and D. Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297–325.

M. Denkowski and A. Lavie. 2014. Meteor universal:
Language specific translation evaluation for any tar-
get language. In Workshop on statistical machine
translation.

M. Everingham, L. van Gool, C. Williams, J. Winn,
and A. Zisserman. 2010. The pascal visual object
classes (VOC) challenge. IJCV, 88(2):303–338.

M. Fadaee, A. Bisazza, and C. Monz. 2017. Data aug-
mentation for low-resource neural machine transla-
tion. In ACL.

K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep resid-
ual learning for image recognition. In CVPR.

S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735–
1780.

S. Ioffe and C. Szegedy. 2015. Batch normalization:
Accelerating deep network training by reducing in-
ternal covariate shift. In ICML.

M. Iyyer, V. Manjunatha, J. Boyd-Graber, and
H. Daumé III. 2015. Deep unordered composition
rivals syntactic methods for text classification. In
ACL.

A. Karpathy and Fei-Fei Li. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In CVPR.

D. Kingma and J. Ba. 2015. Adam: A method for
stochastic optimization. In ICLR.

R. Kiros, R. Salakhutdinov, and R. Zemel. 2014. Mul-
timodal neural language models. In ICML.

R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata,
J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li,
D. Shamma, M. Bernstein, and L. Fei-Fei. 2017. Vi-
sual genome: Connecting language and vision us-
ing crowdsourced dense image annotations. IJCV,
123(1):32–73.

A. Krizhevsky, I. Sutskever, and G. Hinton. 2012. Im-
agenet classification with deep convolutional neural
networks. In NIPS.

D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki,
N. Ballas, N. Ke, A. Goyal, Y. Bengio,
H. Larochelle, A. Courville, and C. Pal. 2017.
Zoneout: Regularizing RNNs by randomly preserv-
ing hidden activations. In ICLR.

A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury,
I. Gulrajani, V. Zhong, R. Paulus, and R. Socher.
2016. Ask me anything: Dynamic memory net-
works for natural language processing. In ICML.

R. Leblond, J.-B. Alayrac, A. Osokin, and S. Lacoste-
Julien. 2018. SeaRnn: Training RNNs with global-
local losses. In ICLR.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, pages 2278–2324.

C.-Y. Lin. 2004. Rouge: a package for automatic eval-
uation of summaries. In ACL Workshop Text Sum-
marization Branches Out.

T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Gir-
shick, J. Hays, P. Perona, D. Ramanan, P. Dollár,
and C. Zitnick. 2014. Microsoft COCO: common
objects in context. In ECCV.



2103

J. Lu, C. Xiong, D. Parikh, and R. Socher. 2017.
Knowing when to look: Adaptive attention via a vi-
sual sentinel for image captioning. In CVPR.

T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013.
Efficient estimation of word representations in vec-
tor space. In ICLR.

M. Norouzi, S. Bengio, Z. Chen, N. Jaitly, M. Schus-
ter, Y. Wu, and D. Schuurmans. 2016. Reward aug-
mented maximum likelihood for neural structured
prediction. In NIPS.

K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.

M. Paulin, J. Revaud, Z. Harchaoui, F. Perronnin, and
C. Schmid. 2014. Transformation pursuit for image
classification. In CVPR.

M. Pedersoli, T. Lucas, C. Schmid, and J. Verbeek.
2017. Areas of attention for image captioning. In
ICCV.

J. Pennington, R. Socher, and C. Manning. 2014.
GloVe: Global vectors for word representation. In
Empirical Methods in Natural Language Process-
ing.

G. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and
G. Hinton. 2017. Regularizing neural networks by
penalizing confident output distributions. In ICLR.

V. Pham, T. Bluche, C. Kermorvant, and J. Louradour.
2014. Dropout improves recurrent neural networks
for handwriting recognition. In Frontiers in Hand-
writing Recognition.

M. Ranzato, S. Chopra, M. Auli, and W. Zaremba.
2016. Sequence level training with recurrent neural
networks. In ICLR.

S. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and
V. Goel. 2017. Self-critical sequence training for
image captioning. In CVPR.

N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
and R. Salakhutdinov. 2014. Dropout: A simple
way to prevent neural networks from overfitting.
JMLR.

I. Sutskever, O. Vinyals, and Q. Le. 2014. Sequence to
sequence learning with neural networks. In NIPS.

C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and
Z. Wojna. 2016. Rethinking the inception architec-
ture for computer vision. In CVPR.

R. Vedantam, C. Zitnick, and D. Parikh. 2015. CIDEr:
Consensus-based image description evaluation. In
CVPR.

O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015.
Show and tell: A neural image caption generator. In
CVPR.

Z. Xie, S. Wang, J. Li, D. Lévy, A. Nie, D. Jurafsky, and
A. Ng. 2017. Data noising as smoothing in neural
network language models. In ICLR.

K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville,
R. Salakhutdinov, R. Zemel, and Y. Bengio. 2015.
Show, attend and tell: Neural image caption genera-
tion with visual attention. In ICML.

Z. Yang, Y. Yuan, Y. Wu, R. Salakhutdinov, and W. Co-
hen. 2016. Encode, review, and decode: Reviewer
module for caption generation. In NIPS.

T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei. 2017. Boost-
ing image captioning with attributes. In ICLR.

Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. 2016.
Image captioning with semantic attention. In CVPR.


