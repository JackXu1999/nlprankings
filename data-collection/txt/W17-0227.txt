



















































Docforia: A Multilayer Document Model


Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 226–230,
Gothenburg, Sweden, 23-24 May 2017. c©2017 Linköping University Electronic Press

Docforia: A Multilayer Document Model

Marcus Klang
Department of computer science

Lund University, Lund
marcus.klang@cs.lth.se

Pierre Nugues
Department of computer science

Lund University, Lund
pierre.nugues@cs.lth.se

Abstract

In this paper, we describe Docforia, a
multilayer document model and applica-
tion programming interface (API) to store
formatting, lexical, syntactic, and seman-
tic annotations on Wikipedia and other
kinds of text and visualize them. While
Wikipedia has become a major NLP re-
source, its scale and heterogeneity makes
it relatively difficult to do experimenta-
tions on the whole corpus. These exper-
imentations are rendered even more com-
plex as, to the best of our knowledge, there
is no available tool to visualize easily the
results of a processing pipeline.

We designed Docforia so that it can store
millions of documents and billions of to-
kens, annotated using different process-
ing tools, that themselves use multiple for-
mats, and compatible with cluster comput-
ing frameworks such as Hadoop or Spark.
The annotation output, either partial or
complete, can then be shared more eas-
ily. To validate Docforia, we processed
six language versions of Wikipedia: En-
glish, French, German, Spanish, Russian,
and Swedish, up to semantic role labeling,
depending on the NLP tools available for
a given language. We stored the results in
our document model and we created a vi-
sualization tool to inspect the annotation
results.

1 Introduction

Wikipedia is one of the largest freely available en-
cyclopedic sources: It is comprehensive, multilin-
gual, and continuously expanding. These unique
properties make it a popular resource now used
in scores of NLP projects such as translation
(Smith et al., 2010), semantic networks (Navigli

and Ponzetto, 2010), named entity linking (Mihal-
cea and Csomai, 2007), information extraction, or
question answering (Ferrucci, 2012).

Nonetheless, the Wikipedia size, where many
language versions have now more that one million
of articles makes it more difficult to handle than
“classic” and older corpora such as the Penn tree-
bank (Marcus et al., 1993). Processing the com-
plete collection of Wikipedia articles, or a part of
it, is a nontrivial task that requires dealing with
multiple markup variants across the language ver-
sions, multiple tools and storage models. In addi-
tion, the application of a NLP pipeline to carry out
the annotation (tokenization, POS tagging, depen-
dency parsing, and so on) is a relatively costly op-
eration that can take weeks on a single computer.

Docforia is a multilayer document model to
store formatting, lexical, syntactic, and semantic
annotations on Wikipedia and other kinds of text
and visualize them. To deliver results in a reason-
able time, Docforia is compatible with cluster pro-
gramming frameworks such as Spark or Hadoop.
Using the Langforia language processing pipelines
(Klang and Nugues, 2016a), we processed six lan-
guage versions of Wikipedia: English, French,
German, Spanish, Russian, and Swedish, up to se-
mantic role labeling, depending on the NLP tools
available for a given language. We stored the re-
sults in the document model. We designed an in-
teractive visualization tool, part of Langforia, so
that a user can select languages, documents, and
linguistic layers and examine the annotation out-
put.

2 The Document Model

We created the Docforia multilayer document
model library to store, query, and extract hyper-
textual information common to many NLP tasks
such as part-of-speech tagging, coreference reso-
lution, named entity recognition and linking, de-
pendency parsing, semantic role labeling, etc., in

226



a standalone package.
This model is intended for large and heteroge-

nous collection of text, including Wikipedia. We
designed it so that we could store the original
markup, as well as the the results of the subse-
quent linguistic processing. The model consists of
multiple layers, where each layer is dedicated to
a specific type of annotation. It is nondestructive
and preserves the original white spaces.

The annotations are encoded in the form of
graph nodes, where a node represents a piece of
data: A token, a sentence, a named entity, etc., de-
limited by ranges. These nodes are possibly con-
nected by edges as in dependency graphs. The
data structure used is similar to a property graph
and Fig. 1 shows the conversion pipeline from the
Wikimedia dumps to the abstract syntactic trees
(AST) and Docforia layers.

3 Previous Work

A few graph-based linguistic data models and se-
rializations have structures that are similar to Doc-
foria. They include HyGraphDB (Gleim et al.,
2007), the Linguistic Framework Annotation (Ide
and Suderman, 2014), Off-Road LAF (Lapponi et
al., 2014), the D-SPIN Text Corpus Format (Heid
et al., 2010), and the oft cited UIMA project (Fer-
rucci and Lally, 2004). Some tools also extend
UIMA such as DKPro Core (Eckart de Castilho
and Gurevych, 2014).

In contrast to the UIMA project (Ferrucci and
Lally, 2004), which also provides an infrastructure
to represent unstructured documents, the Docfo-
ria library by itself does not define an equivalent
analysis infrastructure or rich type system. Doc-
foria’s main focus is data extraction and storage of
informal heterogenous data, where the schema can
change many times during a project.

The primary motivation of Docforia was a faster
adaptability in research projects, where rigidity
can adversely affect productivity. Docforia is
semi-structured, contains a simplified static-type
system for common types of layers and has sup-
port for a dynamic-type system. The static types
are defined by convention, can be overridden, and
are by no means enforced.

4 Use-case: Wikipedia

We convert Wikipedia from HTML dumps into
Docforia records using an annotation pipeline.
The first step converts the HTML documents into

DOM trees using jsoup1. The second step extracts
the original page structure, text styles, links, lists,
and tables. We then resolve the links to unique
Wikidata identifiers. These steps are common to
all the language editions we process.

Wikidata is central to the multilingual nature of
Docforia. Wikidata is an entity database, which
assigns unique identifiers across all the language
editions of Wikipedia. The University of Gothen-
burg, for instance, has the unique id: Q371522
that enables to retrieve the article pages in English,
French, Swedish, or Russian.

In addition to the common processing steps and
depending on the available tools, we can apply lin-
guistic annotations that are language specific us-
ing Langforia. These annotations can range from
a simple tokenization to semantic-role labels or
coreference chains. We save all the results of the
intermediate and final steps as files in the Parquet
format; each record being a Docforia document as
binary blob in addition to metadata such as Wiki-
data Q-number, title, and page-type. We selected
this format because of its portability, efficiency,
and ease of use with the Apache Spark data pro-
cessing engine.

5 Application Programming Interface

The Docforia API builds on the concepts of doc-
ument storage and document engine. The docu-
ment storage consists of properties, layers (node
or edge layers) to store typed annotations, token,
sentence, relationship, where the nodes can have
a range, and finally sublayer variants: gold, pre-
dicted, coreference chains. The document engine
defines query primitives such as covers, for in-
stance the tokens in a anchor, transactions, and
partial lightweight documents called views.

The Docforia data structure is similar to a typed
property graph. It consists of nodes (tokens, sen-
tences, paragraphs, anchors, ...), edges (connec-
tions between e.g tokens to form a dependency
tree), and properties per node and edge (Token:
pos, lemma, ...).

The piece of code below shows how to create to-
kens from a string and assign a property to a range
of tokens, here a named entity with the Location
label:

Document doc = new MemoryDocument(

"Greetings from Lund, Sweden!");

1http://jsoup.org/

227



Wiki markup

Wikimedia
dump

HTML

XOWA

HTML/ZIM

HTML/XOWA

DOM

jsoup

Sections

Text

Multilayer document model

Paragraphs

…

List items
Anchors
Tokens
Italic characters

WDM parser

html

head

title

body

p

div

tableHTML/Online
Wikipedia
Online REST API

Wiki markup

Wikimedia
dump

HTML

XOWA

HTML/ZIM

HTML/XOWA

DOM

jsoup

Sections
Text

Multilayer document model

Paragraphs

…

List items
Anchors
Tokens
Italic characters

WDM parser

html

head

title

body

p

div

tableHTML/Online
Wikipedia
Online REST API

Figure 1: Conversion of Wikipedia dumps into abstract syntactic trees and the Docforia multilayer doc-
ument model (Klang and Nugues, 2016b).

// 01234567890123456789012345678

Token Greetings =

new Token(doc).setRange(0, 9);

Token from =

new Token(doc).setRange(10, 14);

Token Lund =

new Token(doc).setRange(15, 19);

Token comma =

new Token(doc).setRange(19, 20);

Token Sweden =

new Token(doc).setRange(21, 27);

Token exclamation =

new Token(doc).setRange(27, 28);

Sentence greetingsSentence =

new Sentence(doc).setRange(0, 28);

NamedEntity lundSwedenEntity =

new NamedEntity(doc)

.setRange(Lund.getStart(),

Sweden.getEnd())

.setLabel("Location");

The API provides SQL-like query capabilities
and the code below shows how to find the named
entities in a document:

NodeTVar<Token> T = Token.var();

NodeTVar<NamedEntity> NE =

NamedEntity.var();

List<Token> lundLocation =

doc.select(T, NE)

.where(T).coveredBy(NE)

.stream()

.sorted(StreamUtils.orderBy(T))

.map(StreamUtils.toNode(T))

.collect(Collectors.toList());

6 Visualization

We built a front-end application, part of Langfo-
ria, to enable the users to visualize the content of
Docforia-based corpora. This application has the
form of a web server that embeds the Docforia li-
brary and Lucene to index the documents. We cre-
ated a Javascript component for the text visualiza-
tion on the client. This client provides a user inter-
face for searching and visualizing Docforia data in
the index. The layers are selectable from a drop-
down menu and the supported visualizations are
the ranges and relationships between them.

Figure 2 shows the annotations of the parts of
speech, named entities, and dependency relations
of the sentence:

Göteborgs universitet är ett svenskt
statligt universitet med åtta fakul-
teter, 37 000 studenter, varav 25 000
helårsstudenter och 6000 anställda.

‘The University of Gothenburg is a
Swedish public university with eight
faculties, 37,000 students, (25,000
full-time), and 6,000 staff members.’

The visualization tool is similar to the brat2 com-
ponents (Stenetorp et al., 2012), but includes a
tooltip support and has a faster rendering. If we
hover over the words, it shows the properties at-
tached to a word in CoNLL-like format. In Fig. 3,
the properties correspond to the word Vasaparken.

7 Conclusion and Future Work

We described Docforia, a multilayer document
model, structured in the form of a graph. It en-
ables a user to represent the results of large-scale
multilingual annotations. Using it and the Lang-
foria language processing pipelines, we annotated

2http://brat.nlplab.org/

228



Figure 2: Visualization of six layers including: Tokens, named entities, and dependency relations

Figure 3: Visualization of properties

Wikipedia dump (Klang and Nugues, 2016a).
When applied to Wikipedia, MLDM links the dif-
ferent versions through an extensive use of URI
indices and Wikidata Q-numbers.

Together with Docforia, we used the Lucene li-
brary to index the records. The resulting system
can run on a single laptop, even with multiple ver-
sions of Wikipedia.

Docforia is written in Java. In the future, we
plan to develop a Python API, which will make it
possible to combine Python and Java tools. This
will enable the programmer to build prototypes
more quickly as well as experiment more easily
with machine learning algorithms.

Docforia is available from github at https://
github.com/marcusklang/docforia.

Acknowledgements

This research was supported by Vetenskapsrådet,
the Swedish research council, under the Det digi-
taliserade samhället program.

References
Richard Eckart de Castilho and Iryna Gurevych. 2014.

A broad-coverage collection of portable nlp compo-
nents for building shareable analysis pipelines. In
Proceedings of the Workshop on Open Infrastruc-
tures and Analysis Frameworks for HLT, pages 1–
11, Dublin, Ireland, August. Association for Com-
putational Linguistics and Dublin City University.

David Ferrucci and Adam Lally. 2004. UIMA: an
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10(3-4):327–348,
September.

David Angelo Ferrucci. 2012. Introduction to “This
is Watson”. IBM Journal of Research and Develop-
ment, 56(3.4):1:1 –1:15, May-June.

Rüdiger Gleim, Alexander Mehler, and Hans-Jürgen
Eikmeyer. 2007. Representing and maintaining
large corpora. In Proceedings of the Corpus Lin-
guistics 2007 Conference, Birmingham (UK).

Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Er-
hard Hinrichs. 2010. A corpus representation for-
mat for linguistic web services: The D-SPIN text

229



corpus format and its relationship with ISO stan-
dards. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).

Nancy Ide and Keith Suderman. 2014. The Linguis-
tic Annotation Framework: a standard for annota-
tion interchange and merging. Language Resources
and Evaluation, 48(3):395–418.

Marcus Klang and Pierre Nugues. 2016a. Langforia:
Language pipelines for annotating large collections
of documents. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: System Demonstrations, pages 74–78,
Osaka, Japan, December. The COLING 2016 Orga-
nizing Committee.

Marcus Klang and Pierre Nugues. 2016b.
WIKIPARQ: A tabulated Wikipedia resource
using the Parquet format. In Proceedings of the
Ninth International Conference on Language
Resources and Evaluation (LREC 2016), pages
4141–4148, Portorož, Slovenia, may.

Emanuele Lapponi, Erik Velldal, Stephan Oepen, and
Rune Lain Knudsen. 2014. Off-Road LAF: En-
coding and processing annotations in NLP work-
flows. In Proceedings of the Ninth International
Conference on Language Resources and Evalua-
tion (LREC’14), Reykjavik, Iceland, may. European
Language Resources Association (ELRA).

Mitchell Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
Proceedings of the Sixteenth ACM Conference on
CIKM, CIKM ’07, pages 233–242, Lisbon, Portu-
gal.

Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the ACL, pages 216–225, Uppsala.

Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
ACL, pages 403–411.

Pontus Stenetorp, Sampo Pyysalo, Goran Topić,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. brat: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102–107, Avignon, France, April. Association
for Computational Linguistics.

230


