



















































TJP: Identifying the Polarity of Tweets from Contexts


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 657–662,
Dublin, Ireland, August 23-24, 2014.

TJP: Identifying the Polarity of Tweets from Context 

 
Tawunrat Chalothorn Jeremy Ellman 

Department of Computer Science and 

Digital Technologies  

University of Northumbria at Newcas-

tle, Pandon Building, Camden Street 

Newcastle Upon Tyne, NE2 1XE, UK 

Department of Computer Science and 

Digital Technologies  

University of Northumbria at Newcas-

tle, Pandon Building, Camden Street 

Newcastle Upon Tyne, NE2 1XE, UK 
tawunrat.chalothorn 

@northumbria.ac.uk 

jeremy.ellman 

@northumbria.ac.uk 

 

 

Abstract 

The TJP system is presented, which partici-

pated in SemEval 2014 Task 9, Part A: 

Contextual Polarity Disambiguation. Our 

system is ‘constrained’, using only data 

provided by the organizers. The goal of this 

task is to identify whether marking contexts 

are positive, negative or neutral. Our system 

uses a support vector machine, with exten-

sive pre-processing and achieved an overall 

F-score of 81.96%. 

1 Introduction 

The aim of sentiment analysis is to identify 

whether the subject of a text is intended to be 

viewed positively of negatively by a reader. Such 

emotions are sometimes hidden in long sentences 

and are difficult to identify. Consequently senti-

ment analysis is an active research area in natural 

language processing.
 *
 

Sentiment is currently conceived terms of po-

larity. This has numerous interesting applica-

tions. For example, Grabner et al. (2012) used  
sentiment analysis to classify customers’ reviews 

of hotels by using a star rating to categorize the  

                                                           
*     This work is licensed under a Creative Commons At-

tribution 4.0 International Licence. Page numbers and pro-

ceedings footer are added by the organisers. Licence details: 

http://creativecommons.org/licenses/by/4.0/ 

 

reviews as bad, neutral and good. Similarly, 

Tumasjan et al. (2010) tried to predict the out-
come of the German federal election through the 

analysis more than 100,000 tweets posted in the 

lead up. Sentiment analysis has also used to 

classify whether dreams are positive or nega-

tive (Nadeau et al. 2006). 
This paper presents the TJP system which 

was  submitted to SemEval 2014 Task 9, Part A: 

Contextual Polarity Disambiguation (Rosenthal 

et al., 2014). TJP focused on the ‘Constrained’ 

task.  

The ‘Constrained’ task only uses data provid-

ed by the organizers. That is, external resources 

such as sentiment inventories (e.g. Sentiwordnet 

(Esuli, and Sebastiani 2006) are excluded. The 

objective of the TJP system was to use the results 

for comparison with our previous experiment 

(Chalothorn and Ellman, 2013). More details of 

these can be found in section 5.  

The TJP system was implemented using a 

support vector machine (SVM, e.g. Joachims, 

1999) with the addition of extensive pre-

processing such as stopword removal, negation, 

slang, contraction,  and emoticon expansions. 

The remainder of this paper is constructed as 

follows: firstly, related work is discussed in sec-

tion 2; the methodology, the experiment and re-

sults are presented in sections 3 and 4, 

657



respectively. Finally a discussion and future 

work are given in section 5. 

2 Related Work  

Twitter is a popular social networking and mi-

croblogging site that allows users to post mes-

sages of up to 140 characters; known as 

‘Tweets’. Tweets are extremely attractive to the 

marketing sector, since tweets may be searched 

in real-time. This means marketing can find cus-

tomer sentiment (both positive and negative) far 

more quickly than through the use of web pages 

or traditional media. Consequently analyzing the 

sentiment of tweets is currently active research 

task. 

The word 'emoticon' is a neologistic contrac-

tion of 'emotional icon'.  It refers specifically to 

the use of combinations of punctuation charac-

ters to indicate sentiment in a text. Well known 

emoticons include :) to represent a happy face, 

and :( a sad one. Emoticons allow writers to 

augment the impact of limited texts (such as in 

SMS messages or tweets) using few characters.  

Read (2005) used emoticons from a training 

set downloaded from Usenet newsgroups as an-

notations (positive and negative). Using the ma-

chine learning techniques of Naïve Bayes and 

SVM, Read (2005) achieved up to 61.50 % and 

70.10%, accuracy respectively in determining 

text polarity from the emoticons used.  

Go et al. (2009) used distant supervision to   

classify sentiment of Twitter, similar to Read 

(2005). Emoticons were used as noisy labels in 

training data. This allowed the performance of  

supervised learning (positive and negative) at a 

distance. Three classifiers were used: Naïve 

Bayes, Maximum   Entropy and SVM. These 

classifiers were able to   obtain more than 

81.30%, 80.50% and 82.20%, respectively accu-

racy on their unigram testing data . 

Aramaki et al. (2011) classified contexts on 

Twitter related to influenza using a SVM. The 

training data was annotated with the polarity la-

bel by humans, whether they are positive or neg-

ative. The contexts will be labelled as positive if 

the contexts mention the user or someone close 

to them has the flu, or if they mention a time 

when they caught the flu. The results demon-

strated that they obtained a 0.89 correction ratio 

for their testing data against a gold standard. 

Finally, a well known paper by Bollen and 

Mao (2011) identified a correlation between the 

movements of the Dow Jones stock market    

index, and prevailing sentiment as determined 

from twitter's live feed. This application has 

prompted considerable work such as Makrehchi 

et al (2013) that has attempted to create success-

ful trading strategies from sentiment analysis of 

tweets.  

These work both the wide ranging applica-

tions of analysing twitter data, and the             

importance of Sentiment Analysis. We now 

move on to look at our approach to SemEval 

2014 task 9. 

3 Methodology  

3.1 Corpus 

The training and development dataset of 

SemEval was built using Tweets from more than 

one thousand pieces of context. The contexts 

have various features often used in Tweets, such 

as emoticons, tags, usernames etc. These features 

were extracted from the datasets before training 

for the  supervised machine learning model. 

During initial pre-processing of the datasets, 

emoticons were labelled by matching with the 

emoticons that have been collect manually from 

the dataset. Those labelled were matched against 

a well-known collection of emoticons
†
. 

Subsequently, negative contractions
‡
 were 

expanded in place and converted to full form 

(e.g. don’t -> do not). Moreover, the features of 

                                                           
†http://en.wikipedia.org/wiki/List_of_emoticons 
‡http://en.wikipedia.org/wiki/English_auxiliaries_and_contr

actions#Negative_contractions 

658



twitters were also removed or replaced by words 

such as twitter usernames, URLs and hashtags. 

A Twitter username is a unique name that 

shows in the user's profile and may be used for 

both authentication and identification.  This is 

shown by prefacing the username with an @ 

symbol. When a tweet is directed at an individual 

or particular entity this can be shown in the tweet 

by including @username. For example a tweet 

directed at ‘tawunrat’ would include the text  

@tawunrat.  Before URLs are posted in twitter 

they are shortened automatically to use the t.co 

domain whose modified URLs are at most 22 

characters. However, both features have been 

removed from the datasets. For the hashtags, 

they are used for represent keyword and topics in 

twitter by using # follow by words or phrase 

such as #newcastleuk.  This feature has been re-

placed with the following word after # symbol. 

For example, #newcastleuk was replaced by 

newcastleuk. 

Frequently repeated letters are used in tweets 

for emphasis. These were reduced and replaced 

using a simple regular expression by two of the 

same character. For example, happpppppy will 

be replaced with happy, and coollllll will be re-

placed by cooll. Next, special character such as 

[,],{,},?,and ! were also removed. Slang and con-

tracted words were converted to their full form. 

E.g. ‘fyi’ was converted to ‘for your infor-

mation’. Finally, NLTK (Bird et al. 2009) stop-

words such as ‘a’, ‘the’, etc., were removed from 

the datasets. 

3.2 Classifier 

Our system uses the SVM classifier model 

(Hearst et al., 1998, Cristianini and Shawe-

Taylor, 2000), which is based on SVM-light (Jo-

achims, 1999). SVM is a binary linear classifica-

tion model with the learning algorithm for 

classification and regression analyzing the data 

and recognizing the pattern. 

Training SVMLight requires data to be for-

mulated into vectors of attribute value pairs pre-

ceded by a numeric value. For example, 
 

<target>  <feature>:<value> <feature>:<value> ... <feature>:<value> # 

<info> 

 

Here, ‘target’ represents the polarity of a sen-

tence or tweet; ‘feature’ refers to a term in the 

document, and ‘value’ refers to a feature weight. 

This could be used as the relative frequency of a 

term in the set of documents, or Tf-Idf. Tf-idf is 

the combination of term frequency (tf) and in-

verse document frequency (idf), is a weight value 

often used in text mining and information re-

trieval. This weight is a statistical measure used 

to evaluate the relative important of word in a 

document in the collection (Manning et al., 

2008).  

 
                     (1) 

where           is the weighting the scheme assigns to 
term   in document   

 
Term frequency (tf) is used to measure how fre-

quent the term appears in the document. 

 

       
    

∑      
 

(2) 

where      is the number of term   appears in a document    

∑       is the total number of terms   in the document  . 

 

Inverse document frequency (idf) is used to 

measure how important the term is – i.e. whether 

the term is common or rare in the collection.  

 

        
 

  
 

(3) 

where   is the total number of documents in the collection 
in corpus.    is the number of documents   which term   
appears. 

 

Therefore, we chose to work with both of these 

to observe which yielded the best results in the 

polarity classification.  

659



The default settings of SVMLight were used 

throughout. This meant that we used a linear 

kernel that did not require any parameters.
§
 

4 Experiment and Results  

In our experiment, we used the datasets and 

evaluated the system using the F-score measure-

ment. During pre-processing features were ex-

tracted from both datasets. First, we used a 

frequency of word as a featured weight by calcu-

lating the frequency of word in the dataset and, 

during pre-processing, we labelled the emotions 

in both datasets. The results revealed a lower 

than average F-score at 34.80%.  As this was 

quite low we disregarded further use of term fre-

quency as a feature weight. We moved on to use 

Tf-Idf as the feature weight and, again, emoti-

cons in both datasets were labelled. The score of 

78.10% was achieved. Then, we kept the pre-

possessing of the training set stable by combin-

ing the features to extract from the testing data. 

These results are presented in Table 1
**

.  

The highest score of 81.96% was recorded 

when all the features were combined and extract-

ed from both datasets.  

The lowest score of 36.48% was recorded 

when emoticons were extracted from testing data 

and all features were extracted from training da-

tasets. The results of the highest scoring experi-

ment were submitted to the task organizers. 

Following solution submissions, the task or-

ganizers announced the scores by separating the 

data into the following five groups: LiveJour-

nal2014; SMS2013; Twitter2013; Twitter2014; 

and Twitter2014 Sarcasm. This would allow the 

identification of any domain dependent effects. 

However, the results showed that we achieved 

above average in all the datasets, as illustrated in 

Figure 1. 

                                                           
§Based on SVMLight 
**The results in the table are from the test set 2014 in task 

2A. 

5 Conclusion and Future Work 

The TJP system participated in SemEval 2014 

Task 9, Part A: Contextual Polarity Disambigua-

tion. The system exploited considerable pre-

processing, before using the well known, 

SVMLight machine learning algorithm (Joa-

chims. 1999). The pre-processing used several 

twitter specific features, such as hashtags and 

ids, in addition to more traditional Information 

Retrieval concepts such as the Tf-Idf heuristic 

(Manning et al., 2008). The results showed that 

the combination of all features in both datasets 

achieved the best results, at 81.96%. 

An aspect of this contribution is the compara-

tive analysis of feature effectiveness. That is, we 

attempted to identify which factor(s) made the 

most significant improvement to system perfor-

mance. It is clear the pre-processing had a con-

siderable effect on system performance. The use 

of a different learning algorithm also contributed 

to performance since, on this task, SVMLight 

performed better than the Naive Bayes algorithm 

that was used by our team in 2013. 

Sentiment resources was not been used in our 

system in SemEval 2014 as same as in SemEval 

2013 whilst other user groups have employed a 

variety of resources of different sizes, and accu-

racy (Wilson et al., 2013). These points lead to 

the following plan for future activities. 

Our future work is to rigorously investigate 

the success factors for sentiment analysis, espe-

cially in the twitter domain. More specifically, 

we have formulated the following research ques-

tions as a result of our participation in SemEval 

 Are Sentiment resources essential for the 

Sentiment Analysis task? 

 Can the accuracy and effectiveness of 

sentiment lexicons be measured? If so, 

which feature of the resource (accuracy 

vs. coverage) is the most effective met-

ric. 

 Might it be more effective to use a range 

of sentiments (e.g. [-1.0 .. 1.0]), rather 

660



than binary approach(e.g. positive and 

negative) taken in SemEval 2013, and 

2014? 

 Is one machine learning algorithm suffi-

cient, and if so which is it? Or, alternate-

ly would an ensemble approach (Rokach, 

2005) significantly improve perfor-

mance?

 

 
Table 1: The results of each feature analyzed in the approach of TF-IDF 

 

Figure 1: The comparison of TJP and average scores 

 

References 

Alec Go, Richa Bhayani and Lei Huang. 2009. Twit-

ter sentiment classification using distant supervi-

sion. CS224N Project Report, Stanford, 1-12. 

Andrea Esuli, Fabrizio Sebastiani 2006 

"SENTIWORDNET: A Publicly Available Lexi-

cal Resource for Opinion Mining" in Proceedings 

of the 5th Conference on Language Resources and 

Evaluation, LREC (2006), pp. 417-422 

Andranik Tumasjan, Timm O. Sprenger, Philipp G. 

Sandner and Isabell M. Welpe. 2010. "Predicting 

elections with twitter: What 140 characters reveal 

about political sentiment," in Proceedings of the 

Fourth International AAAI Conference on Web-

logs and Social Media, pp. 178-185. 

Christopher D. Manning, Prabhakar Raghavan and 

Hinrich Schütze, Introduction to Information Re-

trieval, Cambridge University Press. 2008.  ISBN: 

0521865719.  

David Nadeau, Catherine Sabourin, Joseph De Kon-

inck, Stan Matwin and Peter D. Turney. 2006. 

661



"Automatic dream sentiment analysis," presented 

at the In: Proceedings of the Workshop on Compu-

tational Aesthetics at the Twenty-First National 

Conference on Artificial Intelligence, Boston, 

Massachussetts, USA. 

Dietmar Grabner, Markus Zanker, Gunther Fliedl and 

Matthias Fuchs. 2012."Classification of customer 

reviews based on sentiment analysis," presented at 

the 19th Conference on Information and Commu-

nication Technologies in Tourism (ENTER), Hel-

singborg, Sweden. 

Eiji Aramaki, Sachiko Maskawa and Mizuki Morita. 

2011. Twitter catches the flu: detecting influenza 

epidemics using Twitter. Proceedings of the Con-

ference on Empirical Methods in Natural Lan-

guage Processing. Edinburgh, United Kingdom: 

Association for Computational Linguistics. 

Johan. Bollen and Huina. Mao. Twitter mood as a 

stock market predictor. IEEE Computer, 

44(10):91–94. 

Jonathon Read. 2005. Using emoticons to reduce de-

pendency in machine learning techniques for sen-

timent classification. Proceedings of the ACL 

Student Research Workshop. Ann Arbor, Michi-

gan: Association for Computational Linguistics. 

Lior Rokach. 2005. Chapter 45 Ensemble Methods for 

Classifiers. In: Oded Maimon and Lior Rokach 

(eds.) Data Mining and Knowledge Discovery 

Handbook. Springer US. 

Marti A. Hearst, Susan T. Dumais, Edgar Osman, 

John Platt and Bernhard Scholkopf . 1998. Sup-

port vector machines. IEEE, Intelligent Systems 

and their Applications, 13, 18-28.  

Masoud Makrehchi, Sameena Shah and Wenhui Liao. 

2013. Stock Prediction Using Event-Based Senti-

ment Analysis.  Web Intelligence (WI) and Intelli-

gent Agent Technologies (IAT), 2013 

IEEE/WIC/ACM International Joint Conferences 

on,. 337-342. 

Nello Cristianini and John Shawe-Taylor. 2000. An 

introduction to support vector machines and other 

kernel-based learning methods, Cambridge uni-

versity press. 

Sara Rosenthal, Preslav Nakov, Alan Ritter and 

Veselin Stoyanov. 2014. SemEval-2014 Task 9: 

Sentiment Analysis in Twitter. International 

Workshop on Semantic Evaluation (SemEval-

2014). Dublin, Ireland. 

Steven Bird, Ewan Klein and Edward Loper. 2009. 

NLTK: Natural language processing with Python, 

O'Reilly. 

Takeshi Sakaki, Makoto Okazaki and Yutaka Matsuo. 

2010. Earthquake shakes Twitter users: real-time 

event detection by social sensors. Proceedings of 

the 19th international conference on World wide 

web. Raleigh, North Carolina, USA: ACM. 

Tawunrat Chalothorn and Jeremy Ellman. 2013. TJP: 

Using Twitter to Analyze the Polarity of Contexts. 

Second Joint Conference on Lexical and Compu-

tational Semantics (*SEM), Volume 2: Proceed-

ings of the Seventh International Workshop on 

Semantic Evaluation (SemEval 2013). Atlanta, 

Georgia, USA: Association for Computational 

Linguistics. 

Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, 

Alan Ritter, Sara Rosenthal and Veselin Stoyanov. 

2013. SemEval-2013 Task 2: Sentiment Analysis 

in Twitter. Proceedings of the 7th International 

Workshop on Semantic Evaluation. Association 

for Computational Linguistics. 

Thorsten Joachims. 1999. Making large-scale support 

vector machine learning practical. Advances in 

kernel methods. MIT Press. 

662


