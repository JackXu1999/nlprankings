



















































The Risk of Racial Bias in Hate Speech Detection


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668–1678
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1668

The Risk of Racial Bias in Hate Speech Detection

Maarten Sap♦ Dallas Card♣ Saadia Gabriel♦ Yejin Choi♦♥ Noah A. Smith♦♥
♦Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, USA

♣Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA
♥Allen Institute for Artificial Intelligence, Seattle, USA

msap@cs.washington.edu

Abstract
We investigate how annotators’ insensitivity
to differences in dialect can lead to racial
bias in automatic hate speech detection mod-
els, potentially amplifying harm against mi-
nority populations. We first uncover unex-
pected correlations between surface markers
of African American English (AAE) and rat-
ings of toxicity in several widely-used hate
speech datasets. Then, we show that models
trained on these corpora acquire and propagate
these biases, such that AAE tweets and tweets
by self-identified African Americans are up to
two times more likely to be labelled as of-
fensive compared to others. Finally, we pro-
pose dialect and race priming as ways to re-
duce the racial bias in annotation, showing that
when annotators are made explicitly aware of
an AAE tweet’s dialect they are significantly
less likely to label the tweet as offensive.

1 Introduction

Toxic language (e.g., hate speech, abusive speech,
or other offensive speech) primarily targets mem-
bers of minority groups and can catalyze real-
life violence towards them (O’Keeffe et al., 2011;
Cleland, 2014; Mozur, 2018). Social media
platforms are under increasing pressure to re-
spond (Trindade, 2018), but automated removal
of such content risks further suppressing already-
marginalized voices (Yasin, 2018; Dixon et al.,
2018). Thus, great care is needed when develop-
ing automatic toxic language identification tools.

The task is especially challenging because what
is considered toxic inherently depends on social
context (e.g., speaker’s identity or dialect). In-
deed, terms previously used to disparage com-
munities (e.g., “n*gga”, “queer”) have been re-
claimed by those communities while remaining
offensive when used by outsiders (Rahman, 2012).
Figure 1 illustrates how phrases in the African
American English dialect (AAE) are labelled by a
publicly available toxicity detection tool as much

crowdsourcing

PerspectiveAPI 
Toxicity score

I saw him 
yesterday. 

What's 
up, bro! 

I saw his ass 
yesterday. 95%

6%

Wussup, 
n*gga! 90%

7%

Wussup, 
n*gga! 

classifier

Non-toxic tweets
(per Spears, 1998)

Figure 1: Phrases in African American English (AAE),
their non-AAE equivalents (from Spears, 1998), and
toxicity scores from PerspectiveAPI.com. Per-
spective is a tool from Jigsaw/Alphabet that uses a
convolutional neural network to detect toxic language,
trained on crowdsourced data where annotators were
asked to label the toxicity of text without metadata.

more toxic than general American English equiv-
alents, despite their being understood as non-toxic
by AAE speakers (Spears, 1998, see §2).

In this work, we first empirically characterize
the racial bias present in several widely used Twit-
ter corpora annotated for toxic content, and quan-
tify the propagation of this bias through models
trained on them (§3). We establish strong asso-
ciations between AAE markers (e.g., “n*ggas”,
“ass”) and toxicity annotations, and show that
models acquire and replicate this bias: in other
corpora, tweets inferred to be in AAE and tweets
from self-identifying African American users are
more likely to be classified as offensive.

Second, through an annotation study, we intro-
duce a way of mitigating annotator bias through
dialect and race priming. Specifically, by design-
ing tasks that explicitly highlight the inferred di-
alect of a tweet or likely racial background of its
author, we show that annotators are significantly
less likely to label an AAE tweet as offensive than
when not shown this information (§4).

PerspectiveAPI.com


1669

Our findings show that existing approaches to
toxic language detection have racial biases, and
that text alone does not determine offensiveness.
Therefore, we encourage paying greater atten-
tion to the confounding effects of dialect and a
speaker’s social identity (e.g., race) so as to avoid
unintended negative impacts.

2 Race and Dialect on Social Media

Since previous research has exposed the potential
for other identity-based biases in offensive lan-
guage detection (e.g., gender bias; Park et al.,
2018), here we investigate racial bias against
speech by African Americans, focusing on Twit-
ter as it is a particularly important space for Black
activism (Williams and Domoszlai, 2013; Freelon
et al., 2016; Anderson et al., 2018). Race is a com-
plex, multi-faceted social construct (Sen and Wa-
sow, 2016) that has correlations with geography,
status, dialect, and more. As Twitter accounts typ-
ically do not have self-reported race information,
researchers rely on various correlates of race as
proxies. We use the African American English di-
alect (AAE) as a proxy for race. AAE is a widely
used dialect of English that is common among, but
not unique to, those who identify as African Amer-
ican,1 and is often used in written form on social
media to signal a cultural identity (Green, 2002;
Edwards, 2004; Florini, 2014).

Dialect estimation In this work, we infer di-
alect using a lexical detector of words associated
with AAE or white-aligned English. We use the
topic model from Blodgett et al. (2016), which
was trained on 60M geolocated tweets and relies
on US census race/ethnicity data as topics. The
model yields probabilities of a tweet being AAE
(pAAE) or White-aligned English (pwhite).2

3 Biases in Toxic Language Datasets

To understand the racial and dialectic bias in toxic
language detection, we focus our analyses on two
corpora of tweets (Davidson et al., 2017; Founta
et al., 2018) that are widely used in hate speech
detection (Park et al., 2018; van Aken et al., 2018;
Kapoor et al., 2018; Alorainy et al., 2018; Lee

1Of course, many African Americans might not use AAE
in every context, or at all. For further discussion of AAE,
please refer to Blodgett et al. (2016).

2The model yields AAE, Hispanic, Asian/Other and
White-aligned dialect probabilities, but for the purpose of our
study we only focus on AAE and White-aligned dialects.

category count AAE corr.

D
W

M
W

17

hate speech 1,430 −0.057
offensive 19,190 0.420
none 4,163 −0.414
total 24,783

F
D

C
L

18

hateful 4,965 0.141
abusive 27,150 0.355
spam 14,030 −0.102
none 53,851 −0.307
total 99,996

Table 1: Number of tweets in each category, and cor-
relation with AAE (Pearson r, p �0.001). We assign
tweets to categories based on the label for FDCL18, and
majority class for DWMW17. Correlations are colored
for interpretability.

et al., 2018; Waseem et al., 2018).3 Different pro-
tocols were used to collect the tweets in these cor-
pora, but both were annotated by Figure-Eight4

crowdworkers for various types of toxic language,
shown in Table 1.

DWMW17 (Davidson et al., 2017) includes an-
notations of 25K tweets as hate speech, offensive
(but not hate speech), or none. The authors col-
lected data from Twitter, starting with 1,000 terms
from HateBase (an online database of hate speech
terms) as seeds, and crowdsourced at least three
annotations per tweet.

FDCL18 (Founta et al., 2018) collects 100K
tweets annotated with four labels: hateful, abu-
sive, spam or none. Authors used a bootstrapping
approach to sampling tweets, which were then la-
belled by five crowdsource workers.

3.1 Data Bias

To quantify the racial bias that can arise during the
annotation process, we investigate the correlation
between toxicity annotations and dialect probabil-
ities given by Blodgett et al. (2016).

Table 1 shows the Pearson r correlation be-
tween pAAE and each toxicity category. For both
datasets, we uncover strong associations between

3Our findings also hold for the widely used data from
Waseem and Hovy (2016). However, because of severe limi-
tations of that dataset (see Schmidt and Wiegand, 2017; Klu-
bika and Fernandez, 2018), we relegate those analyses to sup-
plementary (§A.3).

4www.figure-eight.com

www.figure-eight.com


1670

Within dataset proportions Proportions on DEMOGRAPHIC16 Proportions on USERLEVELRACE18
D

W
M

W
17

% false identification

Group Acc. None Offensive Hate

AAE 94.3 1.1 46.3 0.8
White 87.5 7.9 9.0 3.8
Overall 91.4 2.9 17.9 2.3

% false identification

Group Acc. None Abusive Hateful

AAE 81.4 4.2 26.0 1.7
White 82.7 30.5 4.5 0.8
Overall 81.4 20.9 6.6 0.8

0 25 50 75 100

AAE

White

Overall

D
ia

le
ct

58.1 38.7

79.3 18.5

74.0 23.3

None Offensive Hate

0 25 50 75 100

AA

White

Overall

Se
lf-

re
po

rt
ed

 ra
ce

77.1 20.0

84.2 13.5

83.0 14.5

None Offensive Hate
F

D
C

L
18

0 25 50 75 100

AAE

White

Overall

D
ia

le
ct

56.8 24.6

77.9 11.4

72.1 14.4

Spam None Abusive Hateful

0 25 50 75 100

AA

White

Overall

Se
lf-

re
po

rt
ed

 ra
ce

70.6 10.8

75.5 7.4

74.6 7.9

Spam None Abusive Hateful

Figure 2: Left: classification accuracy and per-class rates of false positives (FP) on test data for models trained on
DWMW17 and FDCL18, where the group with highest rate of FP is bolded. Middle and right: average probabil-
ity mass of toxicity classes in DEMOGRAPHIC16 and USERLEVELRACE18, respectively, as given by classifiers
trained on DWMW17 (top) and FDCL18 (bottom). Proportions are shown for AAE, White-aligned English, and
overall (all tweets) for DEMOGRAPHIC16, and for self-identified White authors, African American authors (AA),
and overall for USERLEVELRACE18.

inferred AAE dialect and various hate speech cat-
egories, specifically the “offensive” label from
DWMW17 (r = 0.42) and the “abusive” label
from FDCL18 (r = 0.35), providing evidence that
dialect-based bias is present in these corpora. As
additional analyses, we examine the interaction
between unigrams indicative of dialect and hate
speech categories, shown in §A.1.

3.2 Bias Propagation through Models

To further quantify the impact of racial biases in
hate speech detection, we investigate how these bi-
ases are acquired by predictive models. First, we
report differences in rates of false positives (FP)
between AAE and White-aligned dialect groups
for models trained on DWMW17 or FDCL18.
Then, we apply these models to two reference
Twitter corpora, described below, and compute av-
erage rates of reported toxicity, showing how these
biases generalize to other data.5

DEMOGRAPHIC16 (Blodgett et al., 2016) con-
tains 56M tweets (2.8M users) with dialect es-
timated using a demographic-aware topic model
that leverages census race/ethnicity data and geo-
coordinates of the user profile. As recommended,
we assign dialect labels to tweets with dialect
probabilities greater than 80%.

5We assume a priori that the average tweet is not inher-
ently more toxic in a particular dialect. Assessing the veracity
of this assumption requires a deep understanding of socio-
cultural norms of profane and toxic speech.

USERLEVELRACE18 (Preoţiuc-Pietro and Un-
gar, 2018) is a corpus of 5.4M tweets, collected
from 4,132 survey participants (3,184 White, 374
AA) who reported their race/ethnicity and Twitter
user handle. For this dataset, we compare differ-
ences in toxicity predictions by self-reported race,
instead of inferring message-level dialect.6

For each of the two toxic language corpora, we
train a classifier to predict the toxicity label of a
tweet. Using a basic neural attention architecture
(Wang et al., 2016; Yang et al., 2016), we train a
classifier initialized with GloVe vectors (Penning-
ton et al., 2014) to minimize the cross-entropy of
the annotated class conditional on text, x:

p(class | x) ∝ exp(Woh+ bo), (1)
with h = f(x), where f is a BiLSTM with atten-
tion, followed by a projection layer to encode the
tweets into an H-dimensional vector.7 We refer
the reader to the appendix for experimental details
and hyperparameters (§A.2).

Results Figure 2 (left) shows that while both
models achieve high accuracy, the false positive
rates (FPR) differ across groups for several toxic-
ity labels. The DWMW17 classifier predicts almost
50% of non-offensive AAE tweets as being offen-
sive, and FDCL18 classifier shows higher FPR for

6Note that lexical dialect inferences of AAE (pAAE) sig-
nificantly correlate with both the AAE group from DEMO-
GRAPHIC16 (Pearson r = 0.61, p� 0.001) and self-reported
AA race from USERLEVELRACE18 (Pearson r = 0.21, p�
0.001).

7In preliminary experiments, our findings held regardless
of our choice of classifier.



1671

the “Abusive” and “Hateful” categories for AAE
tweets. Additionally, both classifiers show strong
tendencies to label White tweets as “none”. These
discrepancies in FPR across groups violate the
equality of opportunity criterion, indicating dis-
criminatory impact (Hardt et al., 2016).

We further quantify this potential discrimina-
tion in our two reference Twitter corpora. Figure 2
(middle and right) shows that the proportions of
tweets classified as toxic also differ by group in
these corpora. Specifically, in DEMOGRAPHIC16,
AAE tweets are more than twice as likely to be
labelled as “offensive” or “abusive” (by classifiers
trained on DWMW17 and FDCL18, respectively).
We show similar effects on USERLEVELRACE18,
where tweets by African American authors are 1.5
times more likely to be labelled “offensive”. Our
findings corroborate the existence of racial bias in
the toxic language datasets and confirm that mod-
els propagate this bias when trained on them.8

4 Effect of Dialect

To study the effect of dialect information on rat-
ings of offensiveness, we run a small controlled
experiment on Amazon Mechanical Turk where
we prime annotators to consider the dialect and
race of Twitter users. We ask workers to deter-
mine whether a tweet (a) is offensive to them, and
(b) could be seen as offensive to anyone. In the
dialect priming condition, we explicitly include
the tweet’s dialect as measured by Blodgett et al.
(2016), as well as extra instructions priming work-
ers to think of tweet dialect as a proxy for the au-
thor’s race. In the race priming condition, we en-
courage workers to consider the likely racial back-
ground of a tweet’s author, based on its inferred
dialect (e.g., an AAE tweet is likely authored by
an African American Twitter user; see §A.5 for the
task instructions). For all tasks, we ask annotators
to optionally report gender, age, race, and political
leaning.9

With a distinct set of workers for each condi-
tion, we gather five annotations apiece for a sam-
ple of 1,351 tweets stratified by dialect, toxicity
category, and dataset (DWMW17 and FDCL18).10

8As noted by Chung (2019), the PerspectiveAPI displays
similar racial biases shown in the appendix (§A.4).

9This study was approved by the Institutional Review
Board (IRB) at the University of Washington.

10Annotations in the control setting agreed moderately
with toxicity labels in DWMW17 and FDCL18 (Pearson r =
0.592 and r = 0.331, respectively; p� 0.001).

67.0

64.0

60.6

41.4

44.1

32.3

15.0

12.8

12.5

28.4

22.7

25.1

18.0

23.2

26.9

30.1

33.1

42.5

0 20 40 60 80 100

race

dialect

control

race

dialect

control

of
fe

ns
iv

e
to

 y
ou

of
fe

ns
iv

e
to

 a
ny

on
e

no maybe yes

Figure 3: Proportion (in %) of offensiveness annota-
tions of AAE tweets in control, dialect, and race prim-
ing conditions. Results show that dialect and race prim-
ing significantly reduces an AAE tweet’s likelihood of
being labelled offensive (p�0.001).

Despite the inherent subjectivity of these ques-
tions, workers frequently agreed about a tweet be-
ing offensive to anyone (76% pairwise agreement,
κ = 0.48) or to themselves (74% p.a., κ = 0.30).

Results Figure 3 shows that priming workers to
think about dialect and race makes them signifi-
cantly less likely to label an AAE tweet as (po-
tentially) offensive to anyone. Additionally, race
priming makes workers less likely to find AAE
tweets offensive to them.

To confirm these effects, we compare the means
of the control condition and treatment condi-
tions,11 and test significance with a t test. When
rating offensiveness to anyone, the mean for con-
trol condition (Mc = 0.55) differs from dialect
(Md = 0.44) and race (Mr = 0.44) conditions
significantly (p � 0.001). For ratings of offen-
siveness to workers, only the difference in means
for control (Mc = 0.33) and race (Md =0.25) con-
ditions is significant (p� 0.001).

Additionally, we find that overall, annotators
are substantially more likely to rate a tweet as be-
ing offensive to someone, than to rate it as offen-
sive to themselves, suggesting that people recog-
nize the subjectivity of offensive language.

Our experiment provide insight into racial bias
in annotations and shows the potential for re-
ducing it, but several limitations apply, includ-
ing the skewed demographics of our worker pool
(75% self-reported White). Additionally, research
suggests that motivations to not seem prejudiced

11We convert the offensiveness labels to real numbers (0:
“no”, 0.5: “maybe”, 1: “yes”).



1672

could buffer stereotype use, which could in turn
influence annotator responses (Plant and Devine,
1998; Moskowitz and Li, 2011).

5 Related Work

A robust body of work has emerged trying to ad-
dress the problem of hate speech and abusive lan-
guage on social media (Schmidt and Wiegand,
2017). Many datasets have been created, but
most are either small-scale pilots (∼100 instances;
Kwok and Wang, 2013; Burnap and Williams,
2015; Zhang et al., 2018), or focus on other
domains (e.g., Wikipedia edits; Wulczyn et al.,
2017). In addition to DWMW17 and FDCL18,
published Twitter corpora include Golbeck et al.
(2017), which uses a somewhat restrictive defini-
tion of abuse, and Ribeiro et al. (2018), which is
focused on network features, rather than text.

Past work on bias in hate speech datasets has
exclusively focused on finding and removing bias
against explicit identity mentions (e.g., woman,
atheist, queer; Park and Fung, 2017; Dixon et al.,
2018). In contrast, our work shows how insensitiv-
ity to dialect can lead to discrimination against mi-
norities, even without explicit identity mentions.

6 Conclusion

We analyze racial bias in widely-used corpora
of annotated toxic language, establishing corre-
lations between annotations of offensiveness and
the African American English (AAE) dialect. We
show that models trained on these corpora prop-
agate these biases, as AAE tweets are twice as
likely to be labelled offensive compared to others.
Finally, we introduce dialect and race priming,
two ways to reduce annotator bias by highlighting
the dialect of a tweet in the data annotation, and
show that it significantly decreases the likelihood
of AAE tweets being labelled as offensive. We
find strong evidence that extra attention should be
paid to the confounding effects of dialect so as to
avoid unintended racial biases in hate speech de-
tection.

Acknowledgments

The authors thank Dan Jurafsky, Emily Bender,
Emily Gade, Tal August, Wesley McClean, Victor
Zhong, and Laura Vianna, as well as anonymous
reviewers, for helpful feedback. This work was in
part supported by NSF grant IIS-1714566.

References
Betty van Aken, Julian Risch, Ralf Krestel, and

Alexander Löser. 2018. Challenges for toxic com-
ment classification: An in-depth error analysis.
CoRR, abs/1809.07572.

Wafa Alorainy, Pete Burnap, Han Liu, and Matthew
Williams. 2018. Cyber hate classification: ’other-
ing’ language and paragraph embedding. CoRR,
abs/1801.07495.

Monica Anderson, Skye Toor, Lee Rainie, and
Aaron Smith. 2018. Activism in the social media
ages. http://www.pewinternet.org/
2018/07/11/activism-in-the-social-
media-age/. Accessed: 2019-03-01.

Su Lin Blodgett, Lisa Green, and Brendan O’Connor.
2016. Demographic dialectal variation in social me-
dia: A case study of African-American english. In
EMNLP.

Pete Burnap and Matthew L. Williams. 2015. Cyber
hate speech on Twitter: An application of machine
classification and statistical modeling for policy and
decision making. Policy & Internet, 7:223–242.

Anna Chung. 2019. How automated tools dis-
criminate against black language. https://
onezero.medium.com/how-automated-
tools-discriminate-against-black-
language-2ac8eab8d6db. Accessed: 2019-
03-02.

Jamie Cleland. 2014. Racism, football fans, and online
message boards: How social media has added a new
dimension to racist discourse in English football. J.
Sport Soc. Issues, 38(5):415–431.

Thomas Davidson, Dana Warmsley, Michael W. Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
ICWSM.

Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,
and Lucy Vasserman. 2018. Measuring and mitigat-
ing unintended bias in text classification. In Pro-
ceedings of Conference on AI, Ethics, and Society.

Walter F. Edwards. 2004. African American Vernacu-
lar English: phonology. In A Handbook of Varieties
of English: Morphology and Syntax.

Sarah Florini. 2014. Tweets, tweeps, and signifyin’:
Communication and cultural performance on “Black
Twitter”. Television & New Media, 15(3):223–237.

Antigoni-Maria Founta, Constantinos Djouvas, De-
spoina Chatzakou, Ilias Leontiadis, Jeremy Black-
burn, Gianluca Stringhini, Athena Vakali, Michael
Sirivianos, and Nicolas Kourtellis. 2018. Large
scale crowdsourcing and characterization of twitter
abusive behavior. In ICWSM.

http://www.pewinternet.org/2018/07/11/activism-in-the-social-media-age/
http://www.pewinternet.org/2018/07/11/activism-in-the-social-media-age/
http://www.pewinternet.org/2018/07/11/activism-in-the-social-media-age/
https://onezero.medium.com/how-automated-tools-discriminate-against-black-language-2ac8eab8d6db
https://onezero.medium.com/how-automated-tools-discriminate-against-black-language-2ac8eab8d6db
https://onezero.medium.com/how-automated-tools-discriminate-against-black-language-2ac8eab8d6db
https://onezero.medium.com/how-automated-tools-discriminate-against-black-language-2ac8eab8d6db


1673

Deen Freelon, Charlton D. McIlwain, and
Meredith D. Clark. 2016. Beyond the hash-
tags. http://cmsimpact.org/wp-
content/uploads/2016/03/beyond_
the_hashtags_2016.pdf. Accessed: 2019-
03-01.

Jennifer Golbeck, Zahra Ashktorab, Rashad O. Banjo,
Alexandra Berlinger, Siddharth Bhagwan, Cody
Buntain, Paul Cheakalos, Alicia A. Geller, Quint
Gergory, Rajesh Kumar Gnanasekaran, Raja Ra-
jan Gunasekaran, Kelly M. Hoffman, Jenny Hot-
tle, Vichita Jienjitlert, Shivika Khare, Ryan Lau,
Marianna J. Martindale, Shalmali Naik, Heather L.
Nixon, Piyush Ramachandran, Kristine M. Rogers,
Lisa Rogers, Meghna Sardana Sarin, Gaurav Sha-
hane, Jayanee Thanki, Priyanka Vengataraman, Zi-
jian Wan, and Derek Michael Wu. 2017. A large
labeled corpus for online harassment research. In
WebSci, pages 229–233. ACM.

Lisa Green. 2002. African American English: A Lin-
guistic Introduction, 8.3.2002 edition edition. Cam-
bridge University Press.

Moritz Hardt, Eric Price, and Nati Srebro. 2016.
Equality of opportunity in supervised learning. In
NeurIPS.

Raghav Kapoor, Yaman Kumar, Kshitij Rajput, Ra-
jiv Ratn Shah, Ponnurangam Kumaraguru, and
Roger Zimmermann. 2018. Mind your language:
Abuse and offense detection for code-switched lan-
guages. CoRR, abs/1809.08652.

Filip Klubika and Raquel Fernandez. 2018. Examining
a hate speech corpus for hate speech detection and
popularity prediction. In LREC.

Irene Kwok and Yuzhou Wang. 2013. Locate the hate:
Detecting tweets against blacks. In AAAI.

Younghun Lee, Seunghyun Yoon, and Kyomin Jung.
2018. Comparative studies of detecting abusive lan-
guage on twitter. CoRR, abs/1808.10245.

Gordon B. Moskowitz and Peizhong Li. 2011. Egali-
tarian goals trigger stereotype inhibition: A proac-
tive form of stereotype control. J. Exp. Soc. Psy-
chol., 47(1):103–116.

Paul Mozur. 2018. A genocide incited on Face-
book, with posts from Myanmar’s military.
https://www.nytimes.com/2018/10/
15/technology/myanmar-facebook-
genocide.html. Accessed: 2018-12-6.

Gwenn Schurgin O’Keeffe, Kathleen Clarke-Pearson,
and Council on Communications and Media. 2011.
The impact of social media on children, adolescents,
and families. Pediatrics, 127(4):800–804.

Ji Ho Park and Pascale Fung. 2017. One-step and two-
step classification for abusive language detection on
Twitter. In Proceedings of the Workshop on Abusive
Language Online.

Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-
ducing gender bias in abusive language detection. In
EMNLP.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In EMNLP.

E. Ashby Plant and Patricia G. Devine. 1998. Inter-
nal and external motivation to respond without prej-
udice. J. Pers. Soc. Psychol., 75(3):811–832.

Daniel Preoţiuc-Pietro and Lyle Ungar. 2018. User-
level race and ethnicity predictors from Twitter text.
In COLING.

Jacquelyn Rahman. 2012. The N word: Its history and
use in the African American community. Journal of
English Linguistics, 40(2):137–171.

Manoel Horta Ribeiro, Pedro H. Calais, Yuri A. San-
tos, Virgı́lio A. F. Almeida, and Wagner Meira Jr.
2018. Characterizing and detecting hateful users on
Twitter. In ICWSM.

Anna Schmidt and Michael Wiegand. 2017. A survey
on hate speech detection using natural language pro-
cessing. In Proceedings of the Workshop on NLP for
Social Media.

Maya Sen and Omar Wasow. 2016. Race as a bundle
of sticks: Designs that estimate effects of seemingly
immutable characteristics. Annual Review of Politi-
cal Science, 19.

Arthur K Spears. 1998. African-American language
use: Ideology and so-called obscenity. In Salikoko S
Mufwene, John R Rickford, Guy Bailey, and John
Baugh, editors, African-American English: Struc-
ture, History and Use, pages 226–250. Routledge
New York.

Luiz Valério P Trindade. 2018. On the frontline:
The rise of hate speech and racism on social
media. https://discoversociety.org/
2018/09/04/on-the-frontline-the-
rise-of-hate-speech-and-racism-on-
social-media/. Accessed: 2018-12-6.

Yequan Wang, Minlie Huang, xiaoyan zhu, and
Li Zhao. 2016. Attention-based LSTM for aspect-
level sentiment classification. In EMNLP.

Zeerak Waseem and Dirk Hovy. 2016. Hateful sym-
bols or hateful people? Predictive features for hate
speech detection on Twitter. In NAACL Student Re-
search Workshop.

Zeerak Waseem, James Thorne, and Joachim Bingel.
2018. Bridging the gaps: Multi task learning for
domain transfer of hate speech detection. In Jennifer
Golbeck, editor, Online Harassment, pages 29–55.
Springer International Publishing, Cham.

Apryl Williams and Doris Domoszlai. 2013. Black-
Twitter: a networked cultural identity. Harmony In-
stitute.

http://cmsimpact.org/wp-content/uploads/2016/03/beyond_the_hashtags_2016.pdf
http://cmsimpact.org/wp-content/uploads/2016/03/beyond_the_hashtags_2016.pdf
http://cmsimpact.org/wp-content/uploads/2016/03/beyond_the_hashtags_2016.pdf
https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html
https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html
https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html
https://doi.org/10.18653/v1/W17-3006
https://doi.org/10.18653/v1/W17-3006
https://doi.org/10.18653/v1/W17-3006
http://aclweb.org/anthology/D18-1302
http://aclweb.org/anthology/D18-1302
https://discoversociety.org/2018/09/04/on-the-frontline-the-rise-of-hate-speech-and-racism-on-social-media/
https://discoversociety.org/2018/09/04/on-the-frontline-the-rise-of-hate-speech-and-racism-on-social-media/
https://discoversociety.org/2018/09/04/on-the-frontline-the-rise-of-hate-speech-and-racism-on-social-media/
https://discoversociety.org/2018/09/04/on-the-frontline-the-rise-of-hate-speech-and-racism-on-social-media/


1674

Ellery Wulczyn, Nithum Thain, and Lucas Dixon.
2017. Ex machina: Personal attacks seen at scale.
In WWW.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J. Smola, and Eduard H. Hovy. 2016. Hi-
erarchical attention networks for document classifi-
cation. In NAACL.

Danyaal Yasin. 2018. Black and banned:
Who is free speech for? https:
//www.indexoncensorship.org/2018/
09/black-and-banned-who-is-free-
speech-for/. Accessed: 2018-12-6.

Ziqi Zhang, David Robinson, and Jonathan A. Tep-
per. 2018. Detecting hate speech on Twitter using
a convolution-GRU based deep neural network. In
Proceedings of ESWC.

https://www.indexoncensorship.org/2018/09/black-and-banned-who-is-free-speech-for/
https://www.indexoncensorship.org/2018/09/black-and-banned-who-is-free-speech-for/
https://www.indexoncensorship.org/2018/09/black-and-banned-who-is-free-speech-for/
https://www.indexoncensorship.org/2018/09/black-and-banned-who-is-free-speech-for/


1675

(FDCL18 – abusive) (FDCL18 – hateful)

(DWMW17 – offensive) (DWMW17 – hate speech)

Figure 4: Feature weights learned by l2-regularized multiclass logistic regression models with unigram features,
plotted against pAAE for each term, based on Blodgett et al. (2016). Top: weights for predicting abusive (left) and
hateful (right) from a model trained on FDCL18. Bottom: weights for predicting offensive (left) and hate speech
(right) from a model trained on DWMW17. Labels are shown for the most heavily-weighted terms, with label size
proportional to the log count of the term in validation data. Note: “c*nt”, “n*gger,” “f*ggot,” and their variations
are considered sexist, racist, and homophobic slurs, respectively, and are predictive of hate speech DWMW17.

A Appendix

We present further evidence of racial bias in hate
speech detection in this appendix.
Disclaimer: due to the nature of this research, fig-
ures and tables contain potentially offensive or up-
setting terms (e.g. racist, sexist, or homophobic
slurs). We do not censor these terms, as they are
illustrative of important features in the datasets.

A.1 Lexical Exploration of Data Bias
To better understand the correlations between in-
ferred dialect and the annotated hate speech cat-
egories (abusive, offensive, etc.) we use simple
linear models to look for influential terms. Specifi-
cally, we train l2-regularized multiclass logistic re-
gression classifiers operating on unigram features
for each of DWMW17 and FDCL18 (tuning the reg-
ularization strength on validation data). We then
use the Blodgett et al. (2016) model to infer pAAE

for each individual vocabulary term in isolation.
While this does not completely explain the corre-
lations observed in section §3.1, it does allow us
to identify individual words that are both strongly
associated with AAE, and highly predictive of par-
ticular categories.

Figure 4 shows the feature weights and pAAE
for each word in the models for FDCL18 (top)
and DWMW17 (bottom), with the most highly
weighted terms identified on the plots. The size
of words indicates how common they are (propor-
tional to the log of the number of times they appear
in the corpus).

These results reveal important limitations of
these datasets, and illustrate the potential for dis-
criminatory impact of any simple models trained
on this data. First, and most obviously, the most
highly weighted unigrams for predicting “hateful”
in FDCL18 are “n*gga” and “n*ggas”, which are



1676

on DEMOGRAPHIC16 on USERLEVELRACE18

WH16 % false identification

Group Acc. Racism Sexism None

AAE 83.8 0.9 2.8 32.5
White 83.5 3.2 2.7 34.6
Overall 84.1 2.7 3.0 35.9 0 25 50 75 100

AAE

White

Overall

D
ia

le
ct

81.1 17.5

90.5 8.2

88.8 9.9

None Sexism Racism

0 25 50 75 100

AA

White

Overall

Se
lf-

re
po

rt
ed

 ra
ce

88.9 10.0

90.5 8.4

90.3 8.6

None Sexism Racism

Figure 5: Left: classification accuracy and per-class rates of false positives (FP) on test data for the model trained
on WH16. Middle and right: average probability mass of toxicity classes in DEMOGRAPHIC16 and USERLEVEL-
RACE18, respectively, as given by the WH16 classifier. As in Figure 2, proportions are shown for AAE, White-
aligned English, and overall (all tweets) for DEMOGRAPHIC16, and for self-identified White authors, African
American authors (AA), and overall for USERLEVELRACE18.

strongly associated with AAE (and their offen-
siveness depends on speaker and context; Spears,
1998). Because these terms are both frequent and
highly weighted, any simple model trained on this
data would indiscriminately label large numbers
of tweets containing either of these terms as “hate-
ful”.

By contrast, the terms that are highly predictive
of “hate speech” in DWMW17 (i.e., slurs) partly
reflect the HateBase lexicon used in constructing
this dataset, and the resulting emphasis is differ-
ent. (We also see artefacts of the dataset construc-
tion in the negative weights placed on “charlie”,
“bird”, and “yankees” — terms which occur in
HateBase, but have harmless primary meanings.)

To verify that no single term is responsible for
the correlations reported in section §3.1, we con-
sider each word in the vocabulary in turn, and
compute correlations excluding tweets containing
that term. The results of this analysis (not shown)
find that almost all of the correlations we observe
are robust. For example, the correlation between
pAAE and “abusive” in FDCL18 increases the most
if we drop tweets containing “fucking” (highly
positively weighted, but non-AAE aligned), and
decreases slightly if we drop terms like “ass” or
“bitch”. The one exception is the correlation be-
tween “hateful” and pAAE in FDCL18: if we ex-
clude tweets which contain “n*gga” or “n*ggas”,
the correlation drops to r=0.047. However, this
also causes the correlation between pAAE and
“abusive” to increase to r=0.376.

A.2 Experimental Details for Classification

For each dataset, we randomly split the data into
train/dev./test sets (73/12/15%), and perform early
stopping when classification accuracy on dev. data
stops increasing. For DWMW17, which has multi-

category count AAE corr.

racism 1,976 −0.117
sexism 3,430 0.168
none 11,501 −0.064
total 16,907

Table 2: Data statistics in WH16, as well as the Pearson
r correlations with the labels and inferred AAE dialect.
All correlations are p�0.001.

ple annotations per instance, we use the majority
class as the label, dropping instances that are tied.
For both datasets, we preprocess the text using
an adapted version of the script for Twitter GloVe
vectors.12 In our experiments, we setH = 64, and
use a vocabulary size of |V | = 19k and |V | = 74k
for DWMW17 and FDCL18, respectively, and ini-
tialize the embedding layer with 300-dimensional
GloVe vectors trained on 840 billion tokens. We
experimented with using ELMo embeddings, but
found that they did not boost performance for this
task. We optimize these models using Adam with
a learning rate of 0.001, and a batch size of 64.

A.3 Bias in Waseem and Hovy (2016)
We replicate our analyses in §3 on the widely used
dataset by Waseem and Hovy (2016, henceforth,
WH16), which categorizes tweets in three hate
speech categories: racist, sexist, or none, shown
in Table 2, along with their correlations with AAE.
This dataset suffers from severe sampling bias that
limit the conclusions to be drawn from this data:
70% of sexist tweets were written by two users,
and 99% of racist tweets were written by a single
user (Schmidt and Wiegand, 2017; Klubika and
Fernandez, 2018).

12https://nlp.stanford.edu/projects/
glove/preprocess-twitter.rb

https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb
https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb


1677

Full Instructions     (Expand/Collapse)

You will read a tweet, and describe whether it could be considered toxic/disrespectful, to you or to anyone. 
Note: we will assume that MTurk workers only have good intentions when annotating these posts.

A note on race/ethnicity of the tweet author
 We also provide an estimate of the tweet dialect, as determined by an AI system. Previous research has showed that dialects of

English are strongly associated to a speaker's racial or ethnic identity. Additionally, certain words are usually less toxic when used by
a minority (e.g., the word "n*gga" or the suffix "-ass" are considered harmless in African American English), therefore it's useful to
know the dialect a tweet is in before labelling it for toxic content. Our AI system detects the following dialects:

General American English (gen Eng): associated with generic newscaster English.
African-American English (Afr-Am Eng): dialect spoken usually by African-American or Black folks.
Latino American English (Lat Eng): dialect spoken usually by Latino/a folks both in New York and California, Texas, Chicago, etc.

(dialect priming)

Instructions

Read a potentially toxic post from the internet and tell us why it's toxic (this should take approx. 5 minutes). Note: You can complete
as many HIT's in this batch as you want! But if your responses tend to be very different from what we're looking for, we might put a
quota on the number of HIT's you can do in future batches. Also note: this is a pilot task, more HITs will be available in the future. 
Participation restriction: providers/turkers for this task cannot currently be employed by or a student at the University of
Washington.

Full Instructions     (Expand/Collapse)

You will read a tweet, and describe whether it could be considered toxic/disrespectful, to you or to anyone. 
Note: we will assume that MTurk workers only have good intentions when annotating these posts.

A note on race/ethnicity of the tweet author
 We also provide an estimate of the Twitter user's race or ethnicity, as inferred by our AI system. Note that certain words are usually

less toxic when used by a minority (e.g., the word "n*gga" or the suffix "-ass" are considered harmless when spoken by Black folks),
therefore it's useful to know the identity of a Tweeter before labelling it for toxic content.

Annotation instructions
1.a) Tell us whether this tweet seems
toxic/hateful/disrespectful to you.

 Our purpose is to understand how disrespect/offense can show
up in language, we are not making statements about the actual
content of the posts.

1.b) Considering a wide set of perspectives, tell us whether this
could be considered toxic/hateful/disrespectful to others.

 Try to answer this questions while considering a broad set of
people from different backgrounds, not just your own.

1.c) Tell us whether the tweet was intentionally offensive or
not.

 It can be hard to infer the intent behind a statement, but
sometimes posts are clearly offensive jokes, insults, snobism,
condescension, profanity, back-handed compliments, name
calling, bullying, intimidation, or aggression.

2) If the post contains sexual content (explicitly or innuendo),
explain which part.

 Sexual content can be used in disrespectful language, either
overtly or hidden. Use the first text box to describe which parts
of the post contain euphemism, double entendre or explicit
sexual content. Then, use the second text box to explain why
you answered this; try to explain what the phrase means, what
it refers to, what the double-entendre is about, etc.

3) Indicate your gender, age, race, political leaning, and
whether you identify as a minority (this will remain
confidential).

 Your own personal background and experiences influence what
you think of as disrespectful or offensive. We collect this
information to account for all types of backgrounds that
MTurkers come from in our research. If you answered this
question once, you can skip it in subsequent HITs.

Background on our research project
At the University of Washington, we're passionate about
understanding how potentially toxic or disrespectful language or
stereotypes can be used against certain demographics/groups of
people (e.g. racism, sexism, etc.). Although there is no direct
benefit to you for participating, we very much appreciate your
help in identifying and explaining such language/stereotypes,
since this is something computational models have no clue
about. We do not agree with any of the content/stereotypes
presented to you, but it's important that we gather these
annotations for research purposes.

Data collection & sharing
 We will not ask you for your name, and the data collected in this

study will be made unidentifiable to the best of our extent. We
will securely store the data on our servers and only share with
qualified researchers (e.g. who want to further the study of hate
speech detection). If you later decide that you do not want your
responses included in this study, please email so we can exclude
your work. 
If you have questions about your rights as a research participant, or wish to
obtain information, ask questions or discuss any concerns about this study with
someone other than the researcher(s), please contact the University of
Washington Human Subjects Division at 206-543-0098 (for international calls
include the US Calling Code: +1-206-543-0098).

Content Warning: posts were found on the (uncensored)
internet; while it's crucial for us to annotate them, we do not
endorse any of the stereotypes or offensive/immoral/rude
material. You may find some of the content upsetting. If you
have concerns, questions, or strong negative reactions to some
of the content, please either email us (Maarten Sap at
msap@cs.washington.edu, or Professor Yejin Choi at
yejin@cs.washington.edu) or reach out if in crisis.

Examples [-] less examples
Sentence Race/Ethnicity Toxic Intentional Sex
You only got the job because you're a woman. White Yes Yes No

The movie with the all-muslim cast was a box
office bomb.

White Yes Probably No

I got my black ass handed to me during this
basketball game.

Black No No No

A white woman called police on black people
barbecuing.

White No No (statement) No

This nigga tried to call me three times during
my shift smh

Black No No No

You are a MORON. White Yes Yes No

Cause of dem stupid traffic lights I almost miss
the beginning of my shift lololol

Latino/Latina Yes Yes No

Wh t d t ll t i d f i i t Whit Y Y N

(race priming)

Figure 6: Additional instructions shown to workers in the dialect and race priming. In the dialect condition, we
provide links to the dialect Wikipedia pages.

In Figure 5 (left), we show how models trained
on this dataset have slightly higher false positive
rates of sexism on AAE tweets, and of the “none”
label for White tweets compared to AAE tweets.
When predicting on our reference corpora (Fig-
ure 5, middle and right), we see AAE tweets (or
tweets by African Americans) are labelled as sex-
ist more than White-aligned tweets or tweets by
White users. Again, due to the sampling issues,
these results should be interpreted cautiously.

A.4 Toxicity and AAE in the PerspectiveAPI

We compare the toxicity detection rates from our
trained models to those of PerspectiveAPI,13 a Jig-
saw/Alphabet initiative to detect hate speech on-
line. We obtain TOXICITY scores for all tweets in
DWMW17 and FDCL18, as well as for 100K ran-
dom tweets from DEMOGRAPHIC16 and USER-
LEVELRACE18. We show correlations (Pearson
r) between dialects/groups in our datasets and the
Perspective TOXICITY scores. All correlations are
significant (p �0.001, Holm-corrected for multi-
ple comparisons) which indicates potential racial
bias for all datasets.

dataset dialect/group TOXICITY corr.

DWMW17 White −0.320
AAE 0.310

FDCL18 White −0.340
AAE 0.453

DEMOGRAPHIC16 White −0.096
AAE 0.056

USERLEVELRACE18 White −0.046
AA 0.042

13The API (http://perspectiveapi.com) was ac-
cessed in December 2018

A.5 Dialect Priming Experimental Details
We collected annotations from 110 (76% White),
143 (77% White), and 81 (72% White) workers in
the control, dialect, and race priming conditions,
respectively. Figure 6 shows the instruction snip-
pet related to dialect and race shown to workers
in the two treatment conditions. Additionally, Fig-
ure 7 shows the annotation interface, with (a) and
without priming (b,c).

http://perspectiveapi.com


1678

A Twitter user tweeted:

I swear I saw him yesterday.

1.a) Does this post seem
offensive/disrespectful to you?

   Yes
   Maybe

   No
 

  Post doesn't make sense/is just a
link

 

1.b) Could this post be considered
offensive/disrespectful to anyone?

   Yes
   Maybe

   No
 

(a)

A Twitter user tweeted:

I swear I saw his ass yesterday.

which our AI system thinks is in African American English.

The AI prediction seems wrong.

1.a) Does this post seem offensive or
disrespectful to you?

  Yes
   Maybe

  No

  Post doesn't make sense/is just a
link

1.b) Could this post be considered
offensive or disrespectful to anyone?

  Yes
   Maybe

  No

(b)

A Twitter user that is likely Black/African American tweeted:

I swear I saw his ass yesterday.

The AI prediction for the user's race/ethnicity seems wrong.

1.a) Does this post seem
offensive/disrespectful to you?

   Yes
   Maybe

   No
 

  Post doesn't make sense/is just a
link

 

1.b) Could this post be considered
offensive/disrespectful to anyone?

   Yes
   Maybe

   No
 

(c)

Figure 7: Interface for the controlled experiment. (a) shows the control condition along with the offensiveness
questions. (b) and (c) show the changes to the treatment interface in the dialect and race priming conditions.


