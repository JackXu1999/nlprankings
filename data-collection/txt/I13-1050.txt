










































Scalable Variational Inference for Extracting Hierarchical Phrase-based Translation Rules


International Joint Conference on Natural Language Processing, pages 438–446,
Nagoya, Japan, 14-18 October 2013.

Scalable Variational Inference for Extracting
Hierarchical Phrase-based Translation Rules∗

Baskaran Sankaran
Simon Fraser University

Burnaby BC. Canada
baskaran@cs.sfu.ca

Gholamreza Haffari
Monash University

Clayton VIC. Australia
reza@monash.edu

Anoop Sarkar
Simon Fraser University

Burnaby BC. Canada
anoop@cs.sfu.ca

Abstract

We present a Variational-Bayes model for
learning rules for the Hierarchical phrase-
based model directly from the phrasal
alignments. Our model is an alternative
to heuristic rule extraction in hierarchical
phrase-based translation (Chiang, 2007),
which uniformly distributes the probabil-
ity mass to the extracted rules locally. In
contrast, in our approach the probability
assigned to a rule is globally determined
by its contribution towards all phrase pairs
and results in a sparser rule set. We
also propose a distributed framework for
efficiently running inference for realistic
MT corpora. Our experiments translating
Korean, Arabic and Chinese into English
demonstrate that they are able to exceed
or retain the performance of baseline hier-
archical phrase-based models.

1 Introduction

Hierarchical phrase-based translation (Hiero) as
described in (Chiang, 2005; Chiang, 2007) uses
a synchronous context-free grammar (SCFG) de-
rived from heuristically extracted phrase pairs
obtained by symmetrizing bidirectional many-to-
many word alignments (Och and Ney, 2004). The
phrase-pairs are constrained by the source-target
alignments such that all the alignment links from
the source (target) words are connected to the
target (source) words within the phrase. Given
a word-aligned sentence pair 〈fJ1 , eI1, A〉, where
A indicate the alignments, the source-target se-
quence pair 〈f ji , e

j′

i′ 〉 can be a phrase-pair iff the
following alignment constraint is satisfied.

(k, k′) ∈ A : k ∈ [i, j]⇔ k′ ∈ [i′, j′]

Given the phrase-pairs, SCFG rules are extracted
by replacing aligned sequences of words in source

∗This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Faculty Award
to the third author.

and target sides by co-indexed non-terminals
and rewriting the replaced source-target word se-
quences as separate rules. Consider a rule X →
〈β, γ〉, where β and γ are sequences of termi-
nals and non-terminals. Now, given another rule
X → 〈f ji , e

j′

i′ 〉, such that f
j
i and e

j′

i′ are contained
fully within β and γ as sub-phrases, the larger rule
could be rewritten to create a new rule.

X → 〈βpXkβs, γpXkγs〉 (1)

Here βp (βs) refers to any prefix (suffix) of β that
precedes (follows) f ji . Note that the non-terminals
are co-indexed with a unique index so that they are
rewritten simultaneously.

数 月 , 联合国 难民 专员 公署

months , the unhcr

Figure 1: Chinese-English phrase-pair with align-
ments

As a concrete example, consider the word
aligned Chinese-English phrase pair shown in Fig-
ure 1. Notice that the phrase 联合国 (united na-
tions) is incorrectly aligned to English determiner
the, even though in ideal case the entire Chinese
phrase联合国难民专员公署 should be aligned
on the English side to the unhcr. The heuristic ap-
proach extracts 32 rules, some of which are shown
in Figure 2.

The distribution of the rules is unknown, as the
different derivations of the sentences are not ex-
plicitly observed. Thus, Chiang (2007) follows an
approach similar to that of Bod (1998) and hypoth-
esizes a distribution over the rules. Under this each
phrase-pair is assumed to have a unit count, which
is uniformly distributed to all the rules extracted
from this phrase-pair. The locally assigned rule
counts are then aggregated across the entire set of
phrase-pairs. The probability for each phrase pair

438



Translation rule PHeu(e|f) PV B(e|f)
∗X → 〈在中南海 | at chong nan hai〉 0.333 0.003
∗X → 〈在中南海 | at zhong nan hai〉 0.333 0.008
X → 〈在中南海 | at zhongnanhai〉 0.333 0.988

Figure 3: Rules extracted for translating the Chinese phrase 在 中 南海. The probabilities are shown
for grammar extracted from heuristic as well our proposed method. The least preferred translations are
shown with ∗. Our Variational-Bayes method extracts a grammar having a peaked distribution as shown.

∗X → 〈联合国 | the〉
∗X → 〈数月 ,联合国 | months , the〉
X → 〈数月 , X1 | months , X1〉
X → 〈数月 | months〉
X → 〈联合国难民专员公署 | the unhcr〉

Figure 2: Rules extracted for the example phrase-
pair in Figure 1. The rules encoding incorrect
translations are marked with ∗.

is then estimated using relative frequency estima-
tion.

1.1 Motivation
A major problem with this heuristic rule extrac-
tion method is the lack of global re-weighting of
the pseudo-counts beyond their local assignments.
By assigning uniform weight to the rules, Chi-
ang (2007) assumes all the rules extracted from a
given phrase-pair to be equally good. However,
some rules might be better than others in terms
of generalization, for capturing a syntactic phrase-
pair, or being a semantically coherent unit of trans-
lation.

Due to this uniform treatment of good and poor
translations, probability mass is wasted on poor
translation candidates. For example the phrase-
pair in Fig 1 would generate several poor trans-
lation rules (shown with ∗ in Fig. 2). This is due to
the incorrect alignment link between 联合国 and
the (note that the word the is typically aligned with
a large number of words due to its frequency). The
heuristic extraction method simply assigns uni-
form count to all translations and as a result the
first translation in Fig. 2 becomes the fourth best
translation for this source phrase.

In Chiang (2007) the rule extraction algorithm
produces a fairly flat distribution over rules. For
example the different translation options of the
Chinese phrase 在 中 南海 (at zhongnanhai) all

have the same p(e|f) probability as shown in
Figure 3. In contrast, our method produces a
peaked distribution and shifts the probability mass
towards at zhongnanhai, which is the preferred
translation.

In this paper, we propose a method which dis-
tributes the probability mass among the rules (gen-
erated from a phrase-pair) based on their contri-
bution in explaining the collection of all phrase-
pairs in a global manner. This difference in esti-
mation methods can lead to a peaked distribution
of rule probabilities. Secondly we also present
a distributed framework that enables rule extrac-
tion on large datasets that are typical in SMT. Our
Variational-Bayes approach for rule extraction im-
proves/ retains the translation quality for the three
different language pairs. Finally, we also present
a detailed analysis comparing the extracted SCFG
with the heuristically extracted SCFG.

2 Model

Our model uses the notion of a derivation: the set
of rules that fully derive an aligned phrase pair,
and learns the estimates for the rules contained in
the derivations through Variational inference. Set-
ting the notations, we denote the set of derivations
for a given phrase pair x as φx and the set of all
rules as G. Given the set of initial phrase pairs
X and a prior over the grammars G, we formulate
Hiero grammar extraction as the task of inferring
a posterior distribution over Hiero grammars. Us-
ing Bayes’ rule, we can express the posterior over
the grammar G given the set of bilingual phrases
X as: P (G|X ) ∝ P (G)P (X|G).

As mentioned earlier, our model replaces the
heuristic rule extraction step in Hiero pipeline.
Consequently our model assumes the existence
of initial phrase-pairs obtained from bidirectional
symmetrization of word alignments. We use the
following two-step generative story to create an
aligned phrase pair from the Hiero rules.

439



φz ∼ Dirichlet(αz) [draw derivation type parameters]
θ ∼ Dirichlet(αhp0) [draw rule parameters]

zd ∼ Multinomial(φz) [decide the derivation type]
r|r ∈ dx ∼ Multinomial(θ) [generate rules deriving phrase-pair x]

Figure 4: Definition of the proposed model

1. First decide the derivation type zd for gen-
erating the aligned phrase pair x. It can ei-
ther be a terminal derivation or hierarchical
derivation with one/two gaps,1 i.e. zd =
{TERM,HIER-A1,HIER-A2}.

2. Then identify the constituent rules r in the
derivation to generate the phrase pair.

Under this model the probability of a particular
derivation d ∈ φx for a given phrase pair x can be
expressed as:

p(d) ∝ p(zd)
∏
r∈d

p(r|G, θ) (2)

where r is a rule in grammar G and θ is the gram-
mar parameter.

Figure 4 depicts the generative story of our gen-
erative model. The derivation-type zd is sampled
from a multinomial distribution parameterized by
φz , where φz is distributed itself by a Dirichlet
distribution with hyper-parameter αz. The gram-
mar rules are generated from a multinomial distri-
bution parameterized by θ, where θ itself is dis-
tributed according to a Dirichlet distribution pa-
rameterized by a concentration parameter αh and
a base distribution p0. For the base distribution,
we use a simple but yet informative prior based
on geometric mean of the bidirectional alignment
scores. This allows us to only explore the rules
that would be consistent with the underlying word
alignments.2 Thus our setting closely resembles
that of the Hiero heuristic rule extraction.

Our goal is thus to infer the joint posterior
p(θ,Φ|αh, p0,αz,X ), where θ are the model pa-
rameters and Φ the latent derivations over all the
phrase pairs.

1This refers to the maximum arity of a rule involved in the
derivation.

2While a non-parametric prior would be better from a
Bayesian perspective, we leave it for future consideration.

3 Training

For inference we resort to a variational approxima-
tion and factorize the posterior distributions over
grammar parameters θ and latent derivations Φ as:

p(θ,Φ|αh, p0,αz,X ) ≈ q(θ|u)q(Φ|π)

where u and π are the parameters of the variational
distributions.

The inference is then performed in an EM-style
algorithm- iteratively updating the parameters u
and π. We initialize u0 := αhp0, which is then
updated with expected rule counts in subsequent
iterations. The expected count for a rule r at time-
step t can be written as:

E[rt] =
∑
d∈φx

p(d|πt−1, x)fd(r) (3)

where p(d|πt−1, x) is the probability of the deriva-
tion d for the phrase pair x and fd(r) is the fre-
quency of the rule r in derivation d. The p(d|.)
term in Equation 3 can then be written in terms of
π as:

p(d|πt−1, x) ∝ p(zd)
∏
r∈d

πt−1r (4)

The p(d|.) are normalized across all the deriva-
tions of a given phrase pair to yield probabilities.
For each derivation type zd, its expected count (at
time t) is the sum of the probabilities of all the
derivations of its type.

E[ztd] =
∑
x

∑
{zd=zd′ |d′∈φx}

p(d′|πt−1, x) (5)

We initialize the Dirichlet hyperparameters αzd
using a Gamma prior ranging between 10−1 and
103: αzd ∼ Gamma(10−1, 103). 3

3In initial experiments we used an initial prior of αz =
[100, 10, 104] to compensate for the smaller probabilities for
arity-2 derivation resulting from two multiplications. How-
ever, our later experiments showed it to be unnecessary and
so we used an initial prior that does not prefer any particular
outcome.

440



Algorithm 1 Variational-Bayes for Hiero Rules
Input: Set of aligned phrase-pairs X
Get prior distribution u = {ur = αhp0(r)|r ∈ G}
Set u0 = u
for time-step t = 1, 2, . . . do

for zd ∈ Z do
p(zd)← exp

(
ψ(αt−1zd )− ψ(

∑
zd
αt−1zd )

)
end for
for r ∈ G do
πt−1r ← exp

(
ψ(ut−1r )− ψ(

∑
r u

t−1
r )

)
end for
for x ∈ X do

for d ∈ φx do
Compute p(d|πt−1, x) as in (4)
E[ztd]← E[ztd] + p(d|πt−1, x)
for r ∈ d do

E[rt]← E[rt] + p(d|πt−1, x) fd(r)
end for

end for
end for
for zd ∈ Z do

Estimate αtzd ← α
0
zd + E[z

t
d]

end for
for r ∈ G do

Estimate posterior ut : utr ← u0r + E[rt]
end for

end for
Output: Posterior distribution ut

We run inference for a fixed number of itera-
tions4 and use the grammar along with their pos-
terior counts from the last iteration for the transla-
tion table. Following (Sankaran et al., 2011), we
use the shift-reduce style algorithm to efficiently
encode the word aligned phrase-pair as a normal-
ized decomposition tree (Zhang et al., 2008). The
possible derivations (that are consistent with the
word alignments) could then be enumerated by
simply traversing every node in the decomposition
tree and replacing its span by a non-terminal X .

3.1 Distributing Inference
While the above training procedure works well for
smaller datasets, it does not scale well for the real-
istic MT datasets (which have millions of sentence
pairs) due to greater memory and time require-
ments. To address this shortcoming, we distribute
the training using a Map-Reduce style framework,
where each node works on the local dataset in
computing the required statistics and then commu-
nicates the statistics to a central aggregator reduce
node.

Distributed inference for Expectation Maxi-
mization algorithm was studied in (Wolfe et al.,
2008). They used three different topologies in

4In our experiments, we set the number of iterations to 10.

terms of computation time, bandwidth require-
ment and so on. While Map-Reduce is substan-
tially slower than the All-pairs and Junction-tree
topologies, it takes much lesser bandwidth than
the other two apart from being much easier to im-
plement. Furthermore our choice of the Varia-
tional inference naturally lends itself to distributed
training.

We simply shard the set of aligned phrase pairs
and parallelize the training steps for the shards
across different nodes. At the end of local com-
putation of the statistics (expected rule counts for
example), we need to aggregate the statistics to get
a global view, which will then be used in the next
iteration/training step. We parallelize this aggre-
gation across several nodes in one or two reduce
steps as required. At the end of aggregation we
communicate the updated statistics to each node
on a need basis.5

4 Experiments

We experiment with three datasets of varying
sizes. We use the University of Rochester Korean-
English dataset consisting of almost 60K sentence
pairs for the small data setting. For moderate and
large datasets we use Arabic-English (ISI parallel
corpus) and Chinese-English (Hong Kong paral-
lel text and GALE phase-1) corpora. We use the
MTC dataset having 4 references for tuning and
testing for our Chinese-English experiments. The
statistics of the corpora used in our experiments
are summarized in Table 1.

Lang. Training Corpus Train/ Tune/ Test
Ko-En URochester data 59218/ 1118/ 1118
Ar-En ISI Ar-En corpus 1.1 M/ 1982/ 987
Cn-En HK + GALE ph-1 2.3 M/ 1928/ 919

Table 1: Corpus Statistics in # of sentences

We follow the standard MT practice and use
GIZA++ (Och and Ney, 2003) for word align-
ing the parallel corpus. We then use the heuris-
tic step that symmetrizes the bidirectional align-
ments (Och et al., 1999) to extract the initial
phrase-pairs up to a certain length, consistent with
the word alignments. Finally we employ our pro-
posed Variational-Bayes training to learn rules for

5We simulate the Map-Reduce style of computation using
a regular high-performance cluster using a mounted filesys-
tem rather than a Hadoop cluster with a distributed filesystem.

441



Model Ko-En Ar-En Cn-En
Baseline 7.18 37.82 28.58
Variational-Bayes 7.68 37.76 28.40

Table 2: BLEU scores for baseline heuristic extrac-
tion and the proposed Variational-Bayes model.
Best scores are in boldface and statistically sig-
nificant differences are italicized.

Hiero. As a baseline Hiero model, we use the
heuristic rule extraction (Chiang, 2007) approach
to extract the rules. In both cases the parameters
are estimated by the relative frequency estimation.

For decoding we use our in-house hierarchi-
cal phrase-based system- Kriya6 (Sankaran et al.,
2012b). We use the following 8 standard features
for the log-linear model: translation probabilities
(p(e|f) and p(f |e)), lexical probabilities (pl(e|f)
and pl(f |e)), phrase and word penalties, language
model and glue rule penalty.

4.1 Results

The main BLEU score results are summarized in
Table 2 and the key aspects are summarized below.

• Higher BLEU scores: Our Bayesian model
performs better than the baseline heuristic
rule extractor for Korean-English. Further-
more, the improvement of 0.5 BLEU is statis-
tically significant at p-value of 0.01.

• Large corpora: Our distributed inference
model easily scales to the large corpora and
the inference completes in less than a day for
Chinese-English. It also retains BLEU scores
in the same level as the baseline models for
both Arabic-English and Chinese-English.

4.2 Compact Models

Some earlier research on Hiero have explored
model size reduction as a means of reducing the
time and space complexity of the Hiero decoder as
well as for mitigating issues such as overgenera-
tion (Setiawan et al., 2009). These approaches use
a variety of compression strategies, viz. threshold
pruning (Zollmann et al., 2008), pattern-based fil-
tering (He et al., 2009; Iglesias et al., 2009) and
significance pruning (Yang and Zheng, 2009).

While compact models is not the central idea
of our work, we nevertheless explore the effect of

6https://github.com/sfu-natlang/Kriya

a simple threshold pruning strategy on the gram-
mar learned from our proposed model. Table 3
shows the results for the pruned grammars, where
we prune the rules having expected count below
a mincount threshold. We present the results for
specific mincount settings based on our experi-
ments on the held-out tuning set for each language
pair.

Model Ko-En Ar-En Cn-En
Model size: VB 2.67 331.6 471.7
BLEU: VB 7.68 37.76 28.40
Pruning mincount 0.25 1.0 1.0
Model size: pruned 1.65 58.9 87.3
Reduction: 38.2% 82.2% 81.5%
BLEU: pruned 7.64 37.58 28.45

Table 3: Model sizes (in millions) and BLEU
scores of the full VB and pruned VB grammars.
Mincount implies the expected rule count thresh-
old used for pruning the full VB grammar. Best/
indistinguishable BLEU scores are shown in bold-
face.

• Retains score with smaller grammar: The
pruned grammars retain the performance of
the full grammar, even while using just 18%
of the complete model.

• Higher reduction for large dataset: Varia-
tional inference reduces the model size over
80% for the large corpora. While this is sim-
ilar to the findings of Johnson et al. (2007)
and that of the pruning strategies mentioned
above; the question of whether an intelligent
model selection strategy can yield higher
BLEU scores is still open.

• Faster decoding: The compact grammars
naturally result in faster decoding and we ob-
served up to 20-30% speedup in the transla-
tion including the time spent for loading the
model.

Sankaran et al. (2012a) proposed a model for
extracting compact Hiero grammar with restricted
arity (at most 1 non-terminal). In contrast our
model is close the classical Hiero model Chiang
(2007) having an arity of two. Though our results
are not directly comparable to theirs, we neverthe-
less find our model to yield a better model size
reduction than theirs. While they claim up to 57%

442



reduction, we achieve over 80% for the two large
data conditions and about 38% reduction for the
Korean-English small data setting.

4.3 Analysis

We now compare the probability distributions of
the two grammars at the level of individual rules
to understand the differences between them. We
considered a set of source phrases that are com-
mon in both grammars and analyzed their proba-
bility distributions over the translation options.

Specifically we use the Q-Q plot to study the
behaviour of two probability distributions as ex-
plained below considering the Chinese phrase 联
合国 (united nations) as a representative example.
The Q-Q plot in Figure 5 plots the p(e|f) proba-
bilities (sorted for the baseline grammar) for dif-
ferent translations of the source phrase. The trans-
lations from the baseline grammar are then paired
off with the points in the sorted VB curve and the
corresponding probabilities are plotted in the same
order as the baseline translations. The following
conclusions can be drawn from this plot:

• Penalize poor translations: Among the low-
probability translations, majority of the trans-
lations in the VB-grammar have probability
less than the corresponding baseline transla-
tions. This has the desired property of poten-
tially shifting the probability mass away from
poor translations.

• Reward good translations: VB-grammar
rewards some translations that were deemed
to be poor by the heuristic method, by as-
signing a slightly higher probability than the
heuristic grammar. A manual inspection
showed that the rules with higher probabili-
ties were objectively better translation rules.
For example Table 4 contrasts the probabili-
ties assigned by the two methods for the first
four translation options in Fig. 5.

• Uniform probability is not informative:
The heuristic extraction method tends to as-
sign an uniform probability for groups of
translations and this is evident in the flat seg-
ments of the baseline curve and is especially
dominant in the low probability region. In
contrast, the VB-grammar is more peaked (in
Fig. 5 the probabilities are sorted for the VB
grammar).

Translations Heu p(e|f) VB p(e|f)
alert the united nations 4.28e-04 3.46e-04
during 4.28e-04 3.20e-04
for un 4.28e-04 5.02e-04
human 4.28e-04 3.20e-04

Table 4: Probabilities assigned by the two methods
for the first four translations in Fig. 5. The better
translation among four and the higher probability
assigned by our model are italicized.

We also observe similar trend for several source
phrases in both Arabic-English and Chinese-
English corpora.

At the macro level, we compare the sizes of
the different types of rules in the heuristic and the
Variational-Bayes grammar. The baseline gram-
mar extract slightly more rules with arity-1 than
the grammar extracted by our model (see Fig-
ure 6). Our model extracts rules used in a deriva-
tion of a phrase-pair, only if all its constituent rules
are consistent with the Hiero rule constraints (such
as restriction on the total number of terminals and
non-terminals in the rule). However the heuristic
method extracts all the consistent rules and does
not consider the derivations. While this is a more
stricter constraint, the VB model extracts slightly
more (about 170K) arity-2 rules as we allow the
unaligned words to be attached to different lev-
els of hierarchical rules during the construction of
the decomposition tree. This extracts translation
rules that are beyond the purview of the heuristic
method, since the Viterbi alignments cannot cap-
ture them.

We earlier examined the effect of pruning the
VB grammar in Section 4.2 and noted that the
grammar could be substantially reduced for differ-
ent language pairs without sacrificing translation
quality. In this context, we compare the effects
of pruning the heuristic and VB grammars in Fig-
ure 6 for Chinese-English. For the same mincount
threshold of 1 as the best performing VB setting,
the BLEU score of the heuristic grammar drops by
over 1 point. However this setting prunes over
99% of the arity-1 and arity-2 rules even while
it retains all the terminal rules. This is primar-
ily because of the way the heuristic method es-
timates rule counts by uniformly distributing the
weight among all the rules. The terminal rules are
sufficient for coverage but does not capture long
distance movements; and the lack of arity-1 and
arity-2 rules further restrict the reordering ability
of the model. We have to substantially lower the

443



 0

 0.0005

 0.001

 0.0015

 0.002

 10  20  30  40  50  60

p
(e

|f
)

Translations

Baseline
VB

Figure 5: Q-Q plot comparing the p(e|f) distributions of the baseline and VB-grammars for the Chinese
phrase联合国 (united nations). The points on the two curves represent distinct target translations (the
numbers on the x-axis indicate the indices) and the points are sorted according to VB translation proba-
bilities against the paired-off translation probabilities from the baseline grammar. The y-axis is clipped
to highlight the variations in the low-probability range.

mincount threshold to 0.05, in order to get per-
formance comparable to the pruned VB grammar
setting. Interestingly, this uses about 7M more
rules (13M more arity-1 rules, but 6M fewer arity-
2 rules) than VB-Pr (1.0), but its BLEU score is
marginally lower than the latter. This could be as-
cribed to the missing arity-2 rules, which could be
crucial for certain long-distance reordering.

39.7	   39.7	   39.7	   39.7	   38.4	  

188.8	  

40	   1.9	  

188.5	  

27.9	  

243.4	  

17.1	  
0.3	  

243.5	  

23.3	  

26.8	  

27	  

27.2	  

27.4	  

27.6	  

27.8	  

28	  

28.2	  

28.4	  

28.6	  

28.8	  

0	  

50	  

100	  

150	  

200	  

250	  

300	  

350	  

400	  

450	  

500	  

Heu	   Heu-­‐Pr	  
(0.05)	  

Heu-­‐Pr	  
(1.0)	  

VB	   VB-­‐Pr	  	  	  
(1.0)	  

BL
EU
	  

M
od

el
	  si
ze
	  (i
n	  
M
ill
io
ns
)	  

Terminal	   Arity-­‐1	   Arity-­‐2	   BLEU	  

Figure 6: Cn-En: Model sizes and BLEU for dif-
ferent grammars. The pruned models are identi-
fied by the suffix ’Pr’, whose mincount is shown
in the brackets. The y-axis on the left marks the
model sizes and that on the right denotes BLEU.
The numbers in the stacked bars denote the # of
rules (in millions) for the corresponding rule type.

As the final part of the analysis, we also
present the 100 high probability lexical phrases
extracted by both rule extraction methods for the
Arabic-English corpus in Figure 7. As seen, the
heuristic grammar assigns high probability to the
rules translating proper nouns and short phrases,
whereas the VB method assigns high probability
to more generic translations.

5 Related Research

Most of the research on learning Hiero SCFG rules
has been focussed on inducing phrasal alignments
between source and target using Bayesian mod-
els (Blunsom et al., 2008; Blunsom et al., 2009;
Levenberg et al., 2012; Cohn and Haffari, 2013).
Broadly speaking, these generative approaches
learn a posterior over parallel tree structures on
the sentence pairs. While these methods extract
hierarchical rules, they do not conform to Hiero-
style rules. Consequently the hierarchical rules
are used only for learning an alignment model and
cannot be used directly in the Hiero decoder. In-
stead, these approaches employ the standard Hiero
heuristics to extract rules to be used by the decoder
from the alignments predicted by their model. In
this sense, these are similar to Bayesian models
for learning alignments using stochastic Inversion
transduction grammars (ITG) (Wu, 1997) or linear

444



Phrases from Heuristic Grammar
ولاوس/, واندونيسيا major news items in/فى الرئيسية العناوين يلى فيما b share index :/: " ب " الاسهم مؤشر hugo/هوجو million shares/سهم مليون
بنك/, ذكره لما وفقا ، jack straw/استرو جاك tony/تونى b share index/" ب " الاسهم مؤشر bayer/باير igor/ايجور yuan (/) يوان indonesia , laos ,
national/القومى الأمن khan jamali/جمالى خان manmohan singh/سينغ مانموهان masood khan/خان مسعود according to the bank of
) الصين jose luis/لويس خوسيه china ( hong kong )/( كونج هونج ) الصين kong )/( كونج daniel/دانييل nanjing/نانجينغ celta/سلتا security
vladimir/فلاديمير pradesh/براديش ( hong kong )/( كونج هونج ) mwai kibaki/كيباكى مواى athletic/اتلتيك china ( hong kong/كونج هونج
وكمبوديا john howard/هوارد جون social stability/الاجتماعى الاستقرار shares .// الخبر نهاية / . سهم carlo/كارلو daniel/دانيل a tael/للتايل
تشانغ toledo/توليدو ( nato/الناتو / president hu/جين هو الرئيس ( hong kong/كونج هونج ) cambodia , indonesia , laos ولاوس/, واندونيسيا
everton/ايفرتون ramirez/راميريز herve de/دو هيرفيه eduardo duhalde/دوهالدى ادواردو president chandrika/تشاندريكا الرئيسة changchun/تشون
dos santos/سانتوس دوس yoriko kawaguchi/كاواجوتشى يوريكو junichiro/جونتشيرو jose/خوسيه yashwant/ياشوانت glafcos/جلافكوس visiting/الزائرة
state warren/وارن الاميركي الخارجية yen/ينا lake victoria/فيكتوريا بحيرة nabil abu/ابو نبيل manh/مانه tanzanian president/التنزانى الرئيس yen/ينات
syndrome/سارس / اللانمطى tokyo stock price/طوكيو بورصة اسعار turnover :/: التداول قيمة kufuor/كوفور sese/سيسي march مارس/31 31 kong/كونج
عبدالله lithuania/وليتوانيا iraqi foreign/العراقى الخارجية jose/جوزيه : 50/50 : his palestinian counterpart/الفلسطينى نظيره yellow/الاصفر ( sars
المتعاملين كبار احد yen/ين international humanitarian/الدولى الانسانى gold dealers in hong/هونج فى الذهب فى المتعاملين كبار abdullah gul/جول
west/الغربية بالضفة atletico/اتلتيكو racing/راسينغ sao tome/تومى ساو fidel/فيديل one of the major gold dealers in hong/هونج فى الذهب فى
2 1/- 1 1 2 visiting chinese/الزائر الصينى dollars )/( امريكى fernando/فرناندو werder/فيردر silva/سيلفا zhaoxing/شينغ تشاو yen (// ين bank
president/فلاديمير الرئيس president museveni/موسيفينى الرئيس baghdad –/-- بغداد in 1996 ./. 1996 عام فى nicholas/نيكولاس الاميركية الخارجية 1 0

yen to/الى ين yang liwei/وى لى يانغ zoran/زوران hong/هونج the major/المتعاملين أكبر vladimir
Phrases from Variational-Bayes Grammar
dominique/دو دومنيك from 77/77 من scott mcclellan/مكليلان سكوت الابيض البيت 100 tons/طن 100 sheikh ali/علي الشيخ june يونيو/21 21
الابحاث french tourists/فرنسيا سائحا africa 1/1 افريقيا donald/دونالد الأمريكى electronic information/الالكترونية المعلومات jiangsu// جيانغسو de
legitimate concerns/المشروعة المخاوف border with egypt/مصر مع الحدودي south africa after/بعد افريقيا جنوب economic research/الاقتصادية
moshe/كاتساف موشيه الاسرائيلي in jerusalem/القدس مدينة في 150 million people/شخص مليون 150 1961/1961 العام hong kong ) , one/أحد ( كونج هونج
police patrol/الشرطة دورية jiang zemin as chairman/كرئيس مين تسه جيانغ generation to/الى جيل international pressure/دولي ضغط katsav
7 nicholas/نيكولاس الاميركية الخارجية between berlin/برلين بين amid reports/تقارير وسط in gabon/الغابون في 86 billion/مليار 86 alaves 33/33 الافيس
iran supports/تساند ايران dialogue with israel/اسرائيل مع الحوار russia 5/5 روسيا north of basra/البصرة شمال bulgaria 2/2 بلغاريا 7 15 8/8 15
من عناصر said/صرح ان peter struck/ستروك بيتر so far this/هذا من الان حتى million pounds/جنيه ملايين argentina and iran/وايران الارجنتين
مع coalition spokesman said/التحالف قوات باسم متحدث اعلن immediate release of/الفورى الافراج zeev/زئيف الاسرائيلي members of the/هذه
والسودان ha ’aretz/هاآرتس / / many european/كثيرة أوربية palestinian doctors/فلسطينيون اطباء accept without/بدون توافق with rwanda/رواندا
or/بدونها او billion yuan (/) يوان مليارات 1.4 billion/مليار 4 ، 1 witnesses/شهود اكد saturday مما/, السبت sudan , swaziland وسوازيلاند/,
examination and approval/والموافقة الفحص kilometres/حوالي بعد على 200 billion/مليار مئتي from muzaffarabad/اباد مظفر من without
reconciliation/والديمقراطية المصالحة on all fronts/الجبهات كل على tajikistan , kazakhstan/وقازاقستان طاجيكستان abbas to visit/لزيارة عباس
رادار liberians to/على ليبيرى fares/فارس اللبناني sunshine policy/المشرقة الشمس سياسة agreed in principle/المبدئية موافقته and democracy
josep pique , whose/الذي بيكيه جوزيب but peres/بيريز لكن president didier/ديدييه الرئيس on the level/مستوى حول warning radar/للانذار
give details/تفاصيل يعط to 96/96 الى geoff hoon/هون جوف crowded/بالركاب مزدحمة a total of 63/63 اجمالى partial elections/جزئية انتخابات
saudi/سعودي تعاون ’s state security/في الدولة امن apache/اباتشي نوع and experience in/في والخبرات remaining obstacles/المتبقية العقبات
maarib , demanding/مطالبين مأرب moussa/موسى كان level/لها مستوى only lead/الا يقود previously/سابقا كان ahmet/احمد التركي cooperation
mutual/المتبادل الدعم non-food/الغذائية غير mosques and hospitals/ومستشفيات مساجد from haifa/حيفا من eu and china/والصين الاوروبى
international/دولية تدخل andrew smith/سميث اندرو be speaking for/باسم يتحدث انه egypt for/لاجراء مصر new york :/: نيويورك - support

intervention

Figure 7: 100 high probability Arabic-English lexical rules extracted by the two methods.

ITG (Saers et al., 2010). In addition, most of these
works except for Levenberg et al. (2012) use small
datasets with fewer than 100K sentence pairs.

More recently Sankaran et al. (2012a) proposed
a Bayesian model employing Variational infer-
ence for directly extracting Hiero grammar from
aligned phrase pairs. While our work is similar to
theirs, there are notable differences as well. Firstly
their model extracts a simpler arity-1 grammar,
where the source and target sides can have at most
1 non-terminal. In contrast the grammar extracted
by our model fully conforms to Hiero-style rules
and hence can potentially capture larger reorder-
ing (than the unary grammar). Secondly, their in-
ference does not scale for the larger datasets and
consequently they train only on a subset of initial
phrase-pairs thresholded by some high frequency.
We further distribute the inference under a simple
MapReduce style framework, which allows us to
scale to large datasets.

A different line of work focuses on reducing the
size of Hiero models, and we discussed these pa-

pers in Section 4.2. While this is not the central
aspect of our work, we showed that simple prun-
ing of the grammar (extracted by our model) could
reduce the size by more than 80% without sacrific-
ing translation quality.

6 Conclusion

This paper introduced a novel Bayesian model
for learning Hiero SCFG translation rules which
is an alternative to the commonly used heuristic
rule extraction approach. For inference, we use
Variational-EM along with a Map-Reduce style
framework for distributing the training process.
This allowed us to efficiently train the model for
very large corpora. We provided quantitative re-
sults and also a detailed qualitative analysis to
demonstrate the superiority of the model trained
by our approach. In future work, we would like
to extend our model for inference directly on full
sentence pairs as has been applied for the syntax-
based model (Galley et al., 2006).

445



References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.

Bayesian synchronous grammar induction. In Pro-
ceedings of the Neural Information Processing Sys-
tems.

Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Annual meeting of Association of Computational
Linguistics.

Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI Publications,
Stanford.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 263–270.
Association for Computational Linguistics.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.

Trevor Cohn and Gholamreza Haffari. 2013. An infi-
nite hierarchical bayesian model of phrasal transla-
tion. In Proceedings of the Annual Meeting of As-
sociation for Computational Linguistics.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.

Zhongjun He, Yao Meng, and Hao Yu. 2009. Dis-
carding monotone composed rule for hierarchical
phrase-based statistical machine translation. In Pro-
ceedings of the 3rd International Universal Commu-
nication Symposium.

Gonzalo Iglesias, Adrià de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the European Association for Computational Lin-
guistics.

Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.

Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A bayesian model for learning scfgs with discon-
tiguous rules. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223–232, Jeju Island, Korea. Asso-
ciation for Computational Linguistics.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30:417–449.

F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 20–28, University of Mary-
land, College Park, MD, USA.

Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with stochastic bracketing linear
inversion transduction grammar. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.

Baskaran Sankaran, Gholamreza Haffari, and Anoop
Sarkar. 2011. Bayesian extraction of minimal scfg
rules for hierarchical phrase-based translation. In
Proceedings of the Sixth Workshop on SMT.

Baskaran Sankaran, Gholamreza Haffari, and Anoop
Sarkar. 2012a. Compact rule extraction for hierar-
chical phrase-based translation. In The 10th biennial
conference of the Association for Machine Transla-
tion in the Americas (AMTA), San Diego, CA. As-
sociation for Computational Linguistics.

Baskaran Sankaran, Majid Razmara, and Anoop
Sarkar. 2012b. Kriya – an end-to-end hierarchi-
cal phrase-based mt system. The Prague Bulletin of
Mathematical Linguistics (PBML), 97(97):83–98.

Hendra Setiawan, Min-Yen Kan, Haizhou Li, and
Philip Resnik. 2009. Topological ordering of func-
tion words in hierarchical phrase-based translation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 324–332. Association
for Computational Linguistics.

Jason Wolfe, Aria Haghighi, and Dan Klein. 2008.
Fully distributed EM for very large datasets. In Pro-
ceedings of the 25th international conference on Ma-
chine learning. ACM.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.

Mei Yang and Jing Zheng. 2009. Toward smaller,
faster, and better hierarchical phrase-based smt. In
Proceedings of the ACL-IJCNLP.

Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
the COLING.

Andreas Zollmann, Ashish Venugopal, Franz Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical mt. In Proceedings of the COLING.

446


