



















































Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Learning Anaphoricity and Antecedent Ranking Features
for Coreference Resolution

Sam Wiseman1 Alexander M. Rush1,2
1School of Engineering and Applied Sciences

Harvard University
Cambridge, MA, USA

{swiseman,srush,shieber}@seas.harvard.edu

Stuart M. Shieber1 Jason Weston2
2Facebook AI Research

New York, NY, USA
jase@fb.com

Abstract

We introduce a simple, non-linear
mention-ranking model for coreference
resolution that attempts to learn distinct
feature representations for anaphoricity
detection and antecedent ranking, which
we encourage by pre-training on a pair
of corresponding subtasks. Although we
use only simple, unconjoined features, the
model is able to learn useful representa-
tions, and we report the best overall score
on the CoNLL 2012 English test set to
date.

1 Introduction

One of the major challenges associated with re-
solving coreference is that in typical documents
the number of mentions (syntactic units capable
of referring or being referred to) that are non-
anaphoric – that is, that are not coreferent with
any previous mention – far exceeds the number
of mentions that are anaphoric (Kummerfeld and
Klein, 2013; Durrett and Klein, 2013).

This preponderance of non-anaphoric mentions
makes coreference resolution challenging, partly
because many basic coreference features, such as
those looking at head, number, or gender match
fail to distinguish between truly coreferent pairs
and the large number of matching but nonethe-
less non-coreferent pairs. Indeed, several au-
thors have noted that it is difficult to obtain good
performance on the coreference task using sim-
ple features (Lee et al., 2011; Fernandes et al.,
2012; Durrett and Klein, 2013; Kummerfeld and
Klein, 2013; Björkelund and Kuhn, 2014) and, as
a result, state-of-the-art systems tend to use lin-
ear models with complicated feature conjunction
schemes in order to capture more fine-grained in-
teractions. While this approach has shown suc-
cess, it is not obvious which additional feature

conjunctions will lead to improved performance,
which is problematic as systems attempt to scale
with new data and features.

In this work, we propose a data-driven
model for coreference that does not require pre-
specifying any feature relationships. Inspired by
recent work in learning representations for nat-
ural language tasks (Collobert et al., 2011), we
explore neural network models which take only
raw, unconjoined features as input, and attempt to
learn intermediate representations automatically.
In particular, the model we describe attempts to
create independent feature representations useful
for both detecting the anaphoricity of a mention
(that is, whether or not a mention is anaphoric) and
ranking the potential antecedents of an anaphoric
mention. Adequately capturing anaphoricity in-
formation has long been thought to be an impor-
tant aspect of the coreference task (see Ng (2004)
and Section 7), since a strong non-anaphoric sig-
nal might, for instance, discourage the erroneous
prediction of an antecedent for a non-anaphoric
mention even in the presence of a misleading head
match.

We furthermore attempt to encourage the learn-
ing of the desired feature representations by pre-
training the model’s weights on two correspond-
ing subtasks, namely, anaphoricity detection and
antecedent ranking of known anaphoric mentions.

Overall our best model has an absolute gain of
almost 2 points in CoNLL score over a similar
but linear mention-ranking model on the CoNLL
2012 English test set (Pradhan et al., 2012), and
of over 1.5 points over the state-of-the-art coref-
erence system. Moreover, unlike current state-of-
the-art systems, our model does only local infer-
ence, and is therefore significantly simpler.

1.1 Problem Setting

We consider here the mention-ranking (or
“mention-synchronous”) approach to coreference

1416



resolution (Denis and Baldridge, 2008; Bengtson
and Roth, 2008; Rahman and Ng, 2009), which
has been adopted by several recent coreference
systems (Durrett and Klein, 2013; Chang et al.,
2013). Such systems aim to identify whether a
mention is coreferent with an antecedent mention,
or whether it is instead non-anaphoric (the first
mention in the document referring to a particular
entity). This is accomplished by assigning a score
to the mention’s potential antecedents as well as
to the possibility that it is non-anaphoric, and
then predicting the greatest scoring option. We
furthermore assume the more realistic “system
mention” setting, where it is not known a priori
which mentions in a document participate in
coreference clusters, and so (all) mentions must
be automatically extracted, typically with the aid
of automatically detected parse trees.

Formally, we denote the set of automatically de-
tected mentions in a document by X . For a men-
tion x∈X , let A(x) denote the set of mentions
appearing before x; we refer to this set as x’s po-
tential antecedents. Additionally let the symbol
� denote the empty antecedent, to which we will
view x as referring when x is non-anaphoric.1 De-
noting the set A(x) ∪ {�} by Y(x), a mention-
ranking model defines a scoring function s(x, y) :
X × Y → R, and predicts the antecedent of x to
be y∗ = arg maxy∈Y(x) s(x, y).

It is common to be quite liberal when extracting
mentions, taking, essentially, every noun phrase or
pronoun to be a candidate mention, so as not to
prematurely discard those that might be coreferent
(Lee et al., 2011; Fernandes et al., 2012; Chang
et al., 2012; Durrett and Klein, 2013). For in-
stance, the Berkeley Coreference System (herein
BCS) (Durrett and Klein, 2013), which we use
for mention extraction in our experiments, recov-
ers approximately 96.4% of the truly anaphoric
mentions in the CoNLL 2012 training set, with
an almost 3.5:1 ratio of non-anaphoric mentions
to anaphoric mentions among the extracted men-
tions.

2 Mention Ranking Models

The structural simplicity of the mention-ranking
framework puts much of the burden on the scor-
ing function s(x, y). We begin by consider-
ing mention-ranking systems using linear scoring

1We make this stipulation for modeling convenience; it is
not intended to reflect any linguistic fact.

functions. In the next section, we will extend these
models to operate over learned non-linear repre-
sentations.

Linear mention-ranking models generally uti-
lize the following scoring function

slin(x, y) , wTφ(x, y) ,

where φ :X ×Y→Rd is a pairwise feature func-
tion defined on a mention and a potential an-
tecedent, and w is a learned parameter vector.

To add additional flexibility to the model, lin-
ear mention ranking models may duplicate indi-
vidual features in φ, with one version being used
when predicting an antecedent for x, and another
when predicting that x is non-anaphoric (Durrett
and Klein, 2013). Such a scheme effectively gives
rise to the following piecewise scoring function

slin+(x, y) ,
{
uT

[
φa(x)
φp(x,y)

]
if y 6= �

vTφa(x) if y = � ,

where φa : X → Rda is a feature function defined
on a mention and its context, φp : X × Y → Rdp
is a pairwise feature function defined on a mention
and a potential antecedent, and parameters u and
v replacew. Above, we have made an explicit dis-
tinction between pairwise features (φp) and those
strictly on x and its context (φa), and moreover as-
sumed that our features need not examine potential
antecedents when predicting y= �.

We refer to the basic, unconjoined features used
for φa and φp as raw features. Figure 2 shows
two versions of these features, a base set BASIC
and an extended set BASIC+. The BASIC set are
the raw features used in BCS, and BASIC+ in-
cludes additional raw features used in other recent
coreference sytems. For instance, BASIC+ addi-
tionally includes features suggested by Recasens
et al. (2013) to be useful for anaphoricity, such
as the number of a mention, its named entity sta-
tus, and its animacy, as well as number and gen-
der information. We additionally include bilexi-
cal head features, which are used in many well-
performing systems (for instance, that of Fernan-
des et al. (2012)).

2.1 Problems with Raw Features
Many authors have observed that, taken individu-
ally, raw features tend to not be particularly pre-
dictive for the coreference task. We examine
this phenomenon empirically in Figure 1. These

1417



Figure 1: Two histograms illustrating the predictive ability
of raw (unconjoined) features per feature occurrence: (top)
mention-context features from φa as independent predictors
of anaphoricity (y 6= �), and (bottom) antecedent-mention
features from φp as independent predictors of coreferent
mentions. Very few raw features are strong indicators of ei-
ther anaphoricity or an antecedent match. Data taken from
the CoNLL development set.

graphs show that the vast majority of individual
features do not give a strong positive signal either
of anaphoricity or for an antecedent match.

To address this issue, state-of-the-art mention-
ranking systems often rely on manual or otherwise
induced conjunction schemes to capture specific
feature interactions. Durrett and Klein (2013),
for instance, conjoin all raw features in φa with
the type of the mention x, and all raw features in
φp with the types of the current mention and an-
tecedent. For these purposes, the type of a mention
is either “nominal”, “proper”, or a canonicaliza-
tion of the pronoun if it is a pronominal mention.
Fernandes et al. (2012) and Björkelund and Kuhn
(2014) use an automatic but complicated scheme
to induce conjunctions by first extracting feature
templates from a separately trained decision tree,
and then doing greedy forward selection among
the templates. These conjunctions add some non-
linearity to the scoring function while still main-
taining a tractable, though large, feature set.

3 Learning Features for Ranking

As an alternative to the aforementioned feature
conjunction schemes, we consider learning feature
representations in order to better capture relevant
aspects of the task. Representation learning af-
fords the model more flexibility in exploiting fea-
ture interactions, although it can make the under-
lying training problem more difficult.

Mention Features (φa)
Feature Value Set

Mention Head V
Mention First Word V
Mention Last Word V
Word Preceding Mention V
Word Following Mention V
# Words in Mention {1, 2, . . .}
Mention Synt. Ancestry see BCS (2013)
Mention Type T

+ Mention Governor V
+ Mention Sentence Index {1, 2, . . .}
+ Mention Entity Type NER tags
+ Mention Number {sing.,plur.,unk}
+ Mention Animacy {an.,inan.,unk}
+ Mention Gender {m,f,neut.,unk}
+ Mention Person {1,2,3,unk}

Pairwise Features (φp)
Feature Value Set

BASIC features on Mention see above
BASIC features on Antecedent see above
Mentions between Ment., Ante. {0. . . 10}
Sentences between Ment., Ante. {0. . . 10}
i-within-i {T,F}
Same Speaker {T,F}
Document Type {Conv.,Art.}
Ante., Ment. String Match {T,F}
Ante. contains Ment. {T,F}
Ment. contains Ante. {T,F}
Ante. contains Ment. Head {T,F}
Mention contains Ante. Head {T,F}
Ante., Ment. Head Match {T,F}
Ante., Ment. Synt. Ancestries see above

+ BASIC+ features on Ment. see above
+ BASIC+ features on Ante. see above
+ Ante., Ment. Numbers see above
+ Ante., Ment. Genders see above
+ Ante., Ment. Persons see above
+ Ante., Ment., Entity Types see above
+ Ante., Ment. Heads see above
+ Ante., Ment. Types see above

Figure 2: Features used for φa(x) and φp(x, y). The ’+’
indicates a feature is in BASIC+ feature set. V denotes the
training vocabulary, and T denotes the set of mention types,
viz., {nominal,proper} ∪ {canonical pronouns}, as defined
in BCS. Conv. and Art. abbreviate conversation and article
(resp.). Lexicalized features occurring fewer than 20 times
in the training set back off to part-of-speech; bilexical heads
occurring fewer than 10 times back off to an indicator feature.
Animacy information is taken from a list and rules used in the
Stanford Coreference system (Lee et al., 2013).

3.1 Model

We use a neural network to define our model as
an extension to the mention-ranking model intro-
duced in Section 2. We consider in particular the
scoring function:

s(x, y) ,
{
uTg(

[
ha(x)
hp(x,y)

]
) + u0 if y 6= �

vTha(x) + v0 if y = � ,

1418



where ha and hp are feature representations, non-
linear functions of the features φa and φp (respec-
tively), and g is a function of these representa-
tions. In particular, we define

ha(x) , tanh(W aφa(x) + ba)
hp(x, y) , tanh(W pφp(x, y) + bp) ,

and we take g to either be the identity func-
tion, in which case the above model is analo-
gous to slin+ but defined over non-linear fea-
ture representations, or to be an additional hidden
layer: g(

[
ha(x)
hp(x,y)

]
) = tanh(W

[
ha(x)
hp(x,y)

]
+ b).

For ease of exposition, we will refer to these two
settings of g as g1 and g2 (respectively) in what
follows. As we will see below, both settings lead
to comparable performance, but to a different error
distribution.

In either case, by defining the functions ha and
hp, we allow the model to learn representations
of the input features φa and φp. The benefit of
the added non-linearities is that, in theory, it is no
longer necessary to explicitly specify feature con-
junctions, since the model may learn them auto-
matically if necessary. Accordingly, for this model
we use only φa and φp consisting of the raw fea-
tures in Figure 2 without conjunctions. Any inter-
action between these features must be learned by
the feature representations hp and ha.

3.2 Training

We can directly train our model using back-
propagation. To specify the training problem, we
first define notation for the training objective.

Define the set C(x) to contain just the mentions
in A(x) that are coreferent with x. We then define

C′(x) =
{
C(x) if x is anaphoric
{�} otherwise .

Finally, let y`n = arg maxy∈C′(xn) s(xn, y) be the
highest scoring correct antecedent of xn, which
may be �. (Thus, following recent work (Yu and
Joachims, 2009; Fernandes et al., 2012; Chang et
al., 2013; Durrett and Klein, 2013), we view each
mention as having a “latent antecedent”.2) We
train to minimize the regularized, slack-rescaled,

2Note that this renders the objectives of even models with
a linear scoring function non-convex.

latent-variable loss3 given by:

L(θ) =

N∑
n=1

max
ŷ∈Y(xn)

∆(xn, ŷ)(1 + s(xn, ŷ)−s(xn, y`n))

+ λ||θ||1,
where ∆ is a mistake-specific cost function,
which is 0 when ŷ ∈C′(xn). Above, we
use θ to refer to the full set of parameters
{W ,u,v,W a,W p, ba, bp}.

For experiments, we define ∆ to take on differ-
ent costs for the three kinds of mistakes possible
in a coreference task, as follows:

∆(x, ŷ) =

{
α1 if ŷ 6= � ∧ � ∈ C′(x)
α2 if ŷ = � ∧ � 6∈ C′(x)
α3 if ŷ 6= � ∧ ŷ 6∈ C′(x) .

The αi determine the trade-off between these mis-
takes (and thus precision and recall). Adopting the
terminology of BCS, we refer to these mistakes
as “false link” (FL), “false new” (FN), and “wrong
link” (WL), respectively.

4 Representations from Subtasks

While we could train our full model directly, it is
known to be difficult to train high performing non-
convex neural-network models from a random ini-
tialization (Erhan et al., 2010). In order to over-
come the problems associated with training from
this setting, and to learn feature representations
useful for the full coreference task, we pretrain
subparts of the model on the subtasks targeting
the desired feature representations. We then train
the entire model on the full coreference task (from
the pre-trained initializations). As we will see,
the pre-training scheme outlined below helps the
model achieve improved performance.

The proposed pre-training scheme involves
learning the parameters associated with ha and hp
using two natural subtasks: anaphoricity detection
and antecedent ranking. In particular, we (1) train
ha on the task of predicting whether a particular
mention is anaphoric or not, and (2) train hp on
the task of predicting the antecedent of mentions
known to be anaphoric.

4.1 Anaphoricity Detection
For the first subtask we attempt to predict whether
a mention is anaphoric or not based only on its

3Previous work divides between log-loss and margin loss.
We use the latter because gradient updates (within backprop)
for the non-probabilistic objectives only involve terms relat-
ing to ŷ and y`n, and are therefore faster.

1419



Feat. (Conj.) Model Anaphoric AnteP R F1 Acc.

BASIC (N) Lin. 74.15 74.20 74.18 69.10
BASIC (Y) Lin. 73.98 75.04 74.51 79.76
BASIC (N) NN 75.30 75.36 75.33 81.65

BASIC+ (N) Lin. 74.14 74.71 74.43 74.02
BASIC+ (Y) Lin. 74.24 75.39 74.81 80.44
BASIC+ (N) NN 75.84 76.02 75.93 82.86

Table 1: Performance of the two subtasks on the CoNLL 2012
development set by feature set and model type. “Conj.” indi-
cates whether conjunctions are used. The linear anaphoric
system is an SVM (LibLinear implementation (Fan et al.,
2008)), and the linear antecedent system is a linear model
with the margin-based objective.

local context.4 Anaphoricity detection in vari-
ous forms has been used as an initial step in sev-
eral coreference systems (Ng and Cardie, 2002;
Bengtson and Roth, 2008; Rahman and Ng, 2009;
Björkelund and Farkas, 2012), and the related
question of whether a mention can be determined
to be a singleton or not has been explored recently
by Recasens et al. (2013), Ma et al. (2014), and
others.5

Formally, let tn ∈ {−1, 1} indicate whether � ∈
C′(xn) or not (respectively). That is, tn = 1 if and
only if xn is anaphoric. Define the subtask scoring
function sa : X → R as

sa(x) , vaTha(x) + ν0 ,

where the vector va and the bias ν0 are specific to
this subtask and are discarded after pre-training.

We train this model to minimize the following
slack-rescaled objective

La(θa) =
N∑

n=1

∆a(tn)[1− tn sa(xn)]+ + λ||θa||1,

where ∆a is a class-specific cost used to help en-
courage anaphoric decisions given the imbalanced
data set, and θa = {va,W a, ba} are the parame-
ters of the subtask.

4.2 Antecedent Ranking
For the second subtask, antecedent ranking, we
predict the antecedent for mentions known a pri-
ori to be anaphoric. This subtask is inspired by

4While performance on this subtask can in fact be im-
proved further by looking at previous mentions, features
learned in this way led to inferior performance on the full
task.

5Note that singleton detection is slightly different from
anaphoricity detection, since a mention can be non-anaphoric
but not a singleton if it is the first mention in a cluster.

Figure 3: Visualization of the representation matrix W p.
A subset of the raw features were manually grouped into
five classes indicating: full lexical match [F], head match
[H], mention/sentence distance [D] (near versus far), gen-
der/number match [G], and type [P] (pronoun versus other).
The heat map illustrates 10-columns of W p as a weighted
combination of these classes, roughly illustrating the com-
bination of raw features required for this dimension of the
representation.

the “gold mention” version of the coreference task.
Systems designed for this task are forced to handle
many fewer non-anaphoric mentions and can often
successfully utilize richer feature representations.

The setup for this task is similar to the full
coreference problem, except that we discard any
mention xn such that � ∈ C′(xn). Thus, we define
the pairwise scoring function sp : X × Y → R as

sp(x, y) , upThp(x, y) + υ0 .

As before, up and υ0 are discarded after train-
ing for this subtask, but we keep the rest of the
parameters. For training, we use an analogous
latent-variable loss function to that used for the
full coreference task, except we replace C′ with
C, and the cost ∆(x, ŷ) is always 1 (when it is
nonzero).

4.3 Subtask Performance
As a preliminary experiment, we train models for
these two subtasks using both the BASIC and BA-
SIC+ raw features. Table 1 shows the results. For
the first subtask, experiments look at the preci-
sion, recall, and F1 score of predicting anaphoric
mentions on the CoNLL 2012 development set.
As a baseline we use an L1-regularized SVM
implemented using LibLinear (Fan et al., 2008),
both using raw features and using features con-
joined according to the BCS scheme. For the sec-
ond subtask, experiments look at the accuracy of
the model in predicting the correct antecedent on
known anaphoric mentions. As a baseline we use
a linear mention ranking model, with and without

1420



conjunctions, trained using the same margin-based
loss.

In both subtasks, the neural network model
performs quite well, significantly better than the
unconjoined baselines and better than the model
trained with manually conjoined features. We pro-
vide a visual representation of the antecedent rank-
ing features learned in Figure 3. While the im-
proved subtask performance does not imply better
performance on the full coreference task, it shows
that model can learn useful feature representations
with only raw input features.

5 Coreference Experiments

Our experiments examine performance as com-
pared with other coreference systems, as well as
the effect of features, pre-training, and model ar-
chitecture. We also perform a qualitative compar-
ison of our model with the analogous linear model
on some challenging non-anaphoric cases.

5.1 Methods

All experiments use the CoNLL 2012 English
dataset (Pradhan et al., 2012), which is based on
the OntoNotes corpus (Hovy et al., 2006). The
data set contains 3,493 documents consisting of
1.6 million words. We use the standard experi-
mental split with the training set containing 2,802
documents and 156K annotated mentions, the de-
velopment set containing 343 documents and 19K
annotated mentions, and the test set containing
348 documents and 20K annotated mentions. For
all experiments, we use BCS (Durrett and Klein,
2013) to extract system mentions and to compute
some of the features.

For training, we minimize the loss described
above using the composite mirror descent Ada-
Grad update (Duchi et al., 2011) with docu-
ment sized mini-batches.6 We tuned the Ada-
Grad learning rate and regularization parameters
using a grid search over possible learning rates
η ∈{0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and over
regularization parameters λ∈{10−6, . . . , 10−1}.
For the full coreference task, we use a differ-
ent learning rate for the pre-trained weights and
for the second-layer weights, using η1 = 0.1 and
η2 = 0.001, respectively, and λ= 10−6. When ini-
tializing weight-matrices that were not pre-trained

6In preliminary experiments we also used Nesterov’s ac-
celerated gradient (Nesterov, 1983), but found AdaGrad to
perform better.

we used the sparse initialization technique pro-
posed by Sutskever et al. (2013). For all experi-
ments we use the cost-weights α = 〈0.5, 1.2, 1〉
in defining ∆.

For the anaphoricity representations the ma-
trix dimensions used are W a ∈R128×da , and for
the pairwise representations the matrix dimensions
used are W p ∈R700×dp . In the g2 model, the
outer matrix dimensions are W ∈R128×(dp+da).
With the BASIC+ features, dp and da come out
to be slightly less than 106 and 104, respectively,
with bilexical head features accounting for the vast
majority of dp.7 We tuned all hyper-parameters (as
well as those of baseline systems) on the develop-
ment set.

We use the CoNLL 2012 scoring script v8.018

(Pradhan et al., 2014; Luo et al., 2014), which
scores based on 3 metrics, including MUC (Vilain
et al., 1995), CEAFe (Luo, 2005), and B3 (Bagga
and Baldwin, 1998), as well as the CoNLL score,
which is the arithmetic mean of the 3 metrics.

Code implementing our models is available
at https://github.com/swiseman/nn_
coref. The system trains in time comparable to
that of linear systems, mainly because we use only
raw features and sparse margin-based gradient up-
dates.

5.2 Results

Our main results are shown in Table 2. This table
compares the performance of our system with the
performance reported by several other state-of-the
art systems on the CoNLL 2012 English corefer-
ence test set. Our full models achieve the best F1
score across two of the three metrics and have the
best aggregate (CoNLL) score, with an improve-
ment of over 1.5 points over the best reported re-
sult, and of almost 2 points over the best mention-
ranking system. Our F1 improvements on all three
metrics are significant (p < 0.05 under the boot-
strap resample test (Koehn, 2004)) as compared
with both Björkelund and Kuhn (2014), and Dur-
rett and Klein (2014), the two most recent, state-
of-the-art systems.

Since our full models use some additional raw
features (although an order of magnitude fewer
total features than the comparable conjunction-

7Note that the BCS conjunction scheme, for instance, ap-
plied to our raw features gives a dp and da that are over an
order of magnitude larger.

8http://conll.github.io/
reference-coreference-scorers/

1421



System MUC B
3 CEAFe

P R F1 P R F1 P R F1 CoNLL

BCS (2013) 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41
Prune&Score (2014) 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56
B&K (2014) 74.30 67.46 70.72 62.71 54.96 58.58 59.40 52.27 55.61 61.63
D&K (2014) 72.73 69.98 71.33 61.18 56.60 58.80 56.20 54.31 55.24 61.79
This work (g2) 76.96 68.10 72.26 66.90 54.12 59.84 59.02 53.34 56.03 62.71
This work (g1) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39

Table 2: Results on CoNLL 2012 English test set. We compare against recent state-of-the-art systems, including (in order)
Durrett and Klein (2013), Ma et al. (2014), Björkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01
scorer). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&K and
D&K for all metrics.

Model Features MUC B3 CEAFe CoNLL

Lin.
BASIC

70.44 59.10 55.57 61.71
NN (g2) 71.59 60.56 57.45 63.20
NN (g1) 71.86 60.90 57.90 63.55

Lin.
BASIC+

70.92 60.05 56.39 62.45
NN (g2) 72.68 61.70 58.32 64.23
NN (g1) 72.74 61.77 58.63 64.38

Table 3: F1 performance comparison between state-of-the-art
linear mention-ranking model (Durrett and Klein, 2013) and
our full models on CoNLL 2012 development set for different
feature sets.

based linear model), we are interested in what part
of the improvement in performance comes from
features rather than modeling power. Table 3 com-
pares the full model to BCS, a system effectively
using the slin+ scoring function together with a
manual conjunction scheme, on both BASIC and
BASIC+ features. While our models outperform
BCS in both cases, we see that as we add more
features (as in the BASIC+ set), the performance
gap between our model and the linear system be-
comes even more pronounced.

We may also wonder whether the architecture
represented by our scoring function, where the in-
termediate representations ha and hp are sepa-
rated in the first layer, is necessary for these re-
sults. We accordingly compare with the fully
connected versions of these two models (which
are equivalent to 1 and 2 layer multi-layer per-
ceptrons) using the BASIC+ features in Table 4.9

There, we also evaluate the effect of pre-training
on these models by comparing with the results of
training from a random initialization. We see that
while even randomly initialized models are capa-
ble of excellent performance, pre-training is bene-
ficial, especially for g1.

9We also experimented with bilinear models both with
and without non-linearities; these were also inferior.

Model MUC B3 CEAFe CoNLL

Fully Conn. 1 Layer 71.80 60.93 57.51 63.41
Fully Conn. 2 Layer 71.77 60.84 57.05 63.22
g1 + RI 71.92 61.06 57.59 63.52
g1 + PT 72.74 61.77 58.63 64.38
g2 + RI 72.31 61.79 58.06 64.05
g2 + PT 72.68 61.70 58.32 64.23

Table 4: Comparison of performance (in F1 score) of vari-
ous models on CoNLL 2012 development set using BASIC+
features. “PT” and “RI” refer to pretraining and random ini-
tialization respectively. “Fully Conn.” refers to baseline fully
connected networks. See text for further model descriptions.

6 Discussion

We attempt to gain insight into our model’s er-
rors using using two different error breakdowns.
In Table 5 we show the errors as reported by the
analysis tool of Kummerfeld and Klein (2013). In
Table 6 we show a more fine-grained breakdown
inspired by a similar analysis in Durrett and Klein
(2013). In the latter table, we categorize the er-
rors made by our system on the CoNLL 2012 de-
velopment data in terms of (1) whether or not the
mention has a head match with a previously oc-
curring mention in the document, unless it is a
pronominal mention, which we treat separately,
(2) in terms of the status of the mention in the
gold clustering, namely, singleton, first-in-cluster,
or anaphoric, and (3) in terms of the type of error
made (which, as discussed in Section 3, are one of
FL, FN, and WL).

We note that the two models have slightly dif-
ferent error profiles, with g1 being slightly better
at recall and g2 being slightly better at precision.
Indeed, we see from Table 6 that the two mod-
els make a comparable number of total errors (g1
makes only 17 fewer errors overall). The increased
precision of the g2 model is presumably due to the
second layer around ha and hp in g2 allowing for
antecedent evidence to interact with anaphoricity

1422



Error Type BCS NN (g1) NN (g2)

Conflated Entities 1603 1434 1371
Extra Mention 0651 0568 0529
Extra Entity 0655 0623 0561

Divided Entity 1989 1837 1835
Missing Mention 1004 0997 1005
Missing Entity 1070 1026 1114

Table 5: Absolute error counts from the coreference analysis
tool of Kummerfeld and Klein (2013). The upper set roughly
corresponds to the precision and the lower to the recall of the
coreference clusters produced by the model.

NN (g1)
Singleton 1st in clust. Anaphoric
FL # FL # FN + WL #

HM 817 08.2K 147 0.8K 700 + 318 4.7K
No HM 086 19.8K 041 2.4K 677 + 59 1.0K
Pron. 948 02.6K 257 0.5K 434 + 875 7.3K

NN (g2)
Singleton 1st in clust. Anaphoric
FL # FL # FN + WL #

HM 770 08.2K 130 0.8K 803 + 306 4.7K
No HM 073 19.8K 039 2.4K 699 + 52 1.0K
Pron. 896 02.6K 249 0.5K 456 + 869 7.3K

Table 6: Errors made by NN (g1) (top) and NN (g2) (bottom)
on CoNLL 2012 English development data. Rows correspond
to (1) mentions with a (previous) head match (HM), that is,
mentions x such thatA(x) contains another mention with the
same head word, (2) with no previous head match (no HM),
and (3) to pronominal mentions, respectively. The 3 column
groups correspond to singleton, first-in-cluster, and anaphoric
mentions (resp.), as determined by the gold clustering, with
the number and type of errors on the left and the total number
of mentions in the category (#) on the right.

evidence in a more complicated way. Ultimately,
however, coreference systems operating over sys-
tem mentions are already biased toward precision,
and so the increased precision of g2 is not as help-
ful as the increased recall of g1 in the final CoNLL
score.

In further analysis we found that many of the
correct predictions made by the g2 model not
made by g1 and the linear model involve predict-
ing non-anaphoric even in the presence of highly
misleading antecedent features like head-match.
Figure 4 shows some examples of mentions with
previous head matches that the linear system pre-
dicted as anaphoric and that our system correctly
identifies as non-anaphoric.

We illustrate how the features in Figure 2 might
be useful in such cases by considering the first
example in Figure 4. There, a comma follows
”the Nika TV company” in the text (and is picked
up by the “word following” feature), perhaps in-
dicating an appositive, which makes anaphoric-
ity unlikely. The model can also learn that the

Non-Anaphoric (x) Spurious Antecedent (y)

the Nika TV company an independent company
Lexus sales GM ’s domestic car sales
The storage area the harbor area
the Budapest location Radio Free Europe ’s new location
the synagogue the synagogue too or something
the equity market The junk market
their silver coin one silver coin
the international school The Hong Kong elementary school
the 1970s the early 1970s
the 2003 season the 2001 season

Figure 4: Example mentions x that were correctly marked
non-anaphoric by g2, but incorrectly marked anaphoric with
y as an antecedent by the BASIC+ linear model. These ex-
amples highlight the difficult case where there is a spurious
head-match between non-coreferent pairs. See text for fur-
ther details.

”company-company” head match is often mislead-
ing, and, in general, distance features may also
rule out head matches. Note that while these fea-
tures on their own may be more or less correlated
with a mention being non-anaphoric, the model
learns to combine them in a predictive way.

6.1 Further Improving Coreference Systems

Table 6 also gives a sense of where coreference
systems such as ours need to improve. It is
first important to note that the case of resolving
an anaphoric mention that has no previous head
matches (e.g., identifying that “the team” and “the
New York Giants” are coreferent), which is of-
ten taken to be one of the major challenges fac-
ing coreference systems because it presumably
requires semantic information, is not the largest
source of errors. In fact, we see from Table 6
(second row, third column in both sub-tables) that
while these cases do indeed account for a substan-
tial percentage of errors, we make hundreds more
errors predicting singleton pronominal mentions
to be anaphoric (in the case of g1) and on incor-
rectly linking anaphoric pronominal mentions (in
the case of g2). Further analysis indicates that
these errors are almost entirely related to incor-
rectly linking pleonastic pronouns, such as “it” or
“you,” and that moreover the incorrectly predicted
antecedent for these pleonastic pronouns is almost
always (another instance of) the same pronoun.

That these pleonastic cases are so problematic
is interesting when considered against the back-
drop of the inference strategies typically employed
by coreference systems, which we briefly men-
tion here but discuss more fully in the next sec-
tion. Currently, coreference systems divide be-

1423



tween those using “local” models, which choose
antecedents for potentially anaphoric mentions in-
dependently of each other, and “non-local” mod-
els, which make predictions that take into ac-
count predictions made for previous mentions, and
perhaps even attempt to jointly predict all men-
tions in a document. While our model is en-
tirely local, other recent high performing sys-
tems, such as that of Björkelund and Kuhn (2014),
are not. One might suspect, then, that “non-
local” inference might allow us to capture the fact
that, for instance, a cluster of coreferent mentions
should generally not consist solely of pronouns,
and thereby avoid predicting (identical) pronomi-
nal antecedents for pleonastic pronouns.

As it turns out, however, almost 30% of the
anaphoric pronominal mentions in the CoNLL de-
velopment data participate in pronoun-only clus-
ters (primarily in the context of broadcast or tele-
phone conversations), which suggests that such a
“non-local” rule may not be particularly useful,
though further experiments are required. It is also
worth noting that a suitably modified loss function
may also be able to prevent excessive pronoun-
pronoun linking, even in a local model.

7 Related Work

There is a voluminous literature on machine learn-
ing approaches to coreference resolution, effec-
tively beginning with Soon et al. (2001). The re-
cent introduction of the CoNLL datasets (Pradhan
et al., 2012) has spurred research that takes ad-
vantage of more fine-grained features and richer
models (Björkelund and Farkas, 2012; Chang et
al., 2012; Durrett and Klein, 2013; Chang et al.,
2013; Björkelund and Kuhn, 2014; Ma et al.,
2014). Of these approaches, our model is related
to the mention-ranking approaches (Bengtson and
Roth, 2008; Denis and Baldridge, 2008; Rahman
and Ng, 2009; Durrett and Klein, 2013; Chang
et al., 2013), as opposed to those that focus on
non-local, structured prediction (McCallum and
Wellner, 2003; Culotta et al., 2006; Haghighi and
Klein, 2010; Fernandes et al., 2012; Stoyanov and
Eisner, 2012; Björkelund and Farkas, 2012; Wick
et al., 2012; Björkelund and Kuhn, 2014; Durrett
and Klein, 2014).

In motivation, our work is most similar to that of
Ng (2004), who notes that anaphoricity informa-
tion is useful within the broader coreference task,
and who accordingly attempts to “globally” opti-

mize performance based on this information, as
well as that of Denis et al. (2007), who do joint
decoding of anaphoricity and coreference predic-
tions using ILP. Both of these works are taken to
contrast with the more popular approach of do-
ing an initial non-anaphoric pruning step (Ng and
Cardie, 2002; Rahman and Ng, 2009; Recasens et
al., 2013; Lee et al., 2013). In contrast, we jointly
learn non-linear functions of anaphoricity and an-
tecedent features, rather than tune a threshold,
or jointly decode based on independently trained
classifiers (as in Denis et al. (2007)). In a simi-
lar vein, several authors have also proposed using
the output of an anaphoricity classifier as a feature
in a downstream coreference system (Ng, 2004;
Bengtson and Roth, 2008). In our framework we
(re)learn features jointly with the full task, after
a pre-training scheme that targets anaphoricity as
well antecedent representations.

There has also been some work on automat-
ically inducing feature conjunctions for use in
coreference systems (Fernandes et al., 2012; Las-
salle and Denis, 2013), though the approach we
present here is somewhat simpler, and unlike that
of Lassalle and Denis (2013) is designed for use
on system rather than gold mentions.

There has been much interest recently in us-
ing neural networks for classic natural language
tasks such as tagging and semantic role labeling
Collobert et al. (2011), sentiment analysis (Socher
et al., 2011; Socher et al., 2012), prepositional
phrase attachment (Belinkov et al., 2014) among
others. These systems often use some form of pre-
training for initialization, often word-embeddings
learned from external tasks. However, there has
been little work of this form for coreference reso-
lution.

8 Conclusion

We have presented a simple, local model ca-
pable of learning feature representations useful
for coreference-related subtasks, and of thereby
achieving state-of-the-art performance. Because
our approach automatically learns intermediate
representations given raw features, directions for
further research might alternately explore includ-
ing additional (perhaps semantic) raw features,
as well as developing loss functions that further
discourage learning representations that allow for
common errors (such as those involving pleonastic
pronouns).

1424



References
Amit Bagga and Breck Baldwin. 1998. Algorithms

for Scoring Coreference Chains. In The first in-
ternational conference on language resources and
evaluation workshop on linguistics coreference, vol-
ume 1, pages 563–566. Citeseer.

Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir
Globerson. 2014. Exploring Compositional Archi-
tectures and Word Vector Representations for Prepo-
sitional Phrase Attachment. Transactions of the As-
sociation for Computational Linguistics, 2:561–572.

Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294–303. ACL.

Anders Björkelund and Richárd Farkas. 2012. Data-
driven Multilingual Coreference Resolution using
Resolver Stacking. In Joint Conference on EMNLP
and CoNLL-Shared Task, pages 49–55. ACL.

Anders Björkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference Resolu-
tion with Latent Antecedents and Non-local Fea-
tures. ACL, Baltimore, MD, USA, June.

Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
coref: The UI System in the CoNLL-2012 Shared
Task. In Joint Conference on EMNLP and CoNLL-
Shared Task, pages 113–117. Association for Com-
putational Linguistics.

Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A Constrained Latent Variable Model for
Coreference Resolution. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 601–612.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (almost) from
Scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2006. First-order Probabilistic Models
for Coreference Resolution. NAACL-HLT.

Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
660–669. ACL.

Pascal Denis, Jason Baldridge, et al. 2007. Joint De-
termination of Anaphoricity and Coreference Reso-
lution using Integer Programming. In HLT-NAACL,
pages 236–243. Citeseer.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning

and Stochastic Optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Greg Durrett and Dan Klein. 2013. Easy Victories and
Uphill Battles in Coreference Resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1971–
1982.

Greg Durrett and Dan Klein. 2014. A Joint Model for
Entity Analysis: Coreference, Typing, and Linking.
Transactions of the Association for Computational
Linguistics, 2:477–490.

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
Library for Large Linear Classification. The Journal
of Machine Learning Research, 9:1871–1874.

Eraldo Rezende Fernandes, Cı́cero Nogueira Dos San-
tos, and Ruy Luiz Milidiú. 2012. Latent Structure
Perceptron with Feature Induction for Unrestricted
Coreference Resolution. In Joint Conference on
EMNLP and CoNLL-Shared Task, pages 41–48. As-
sociation for Computational Linguistics.

Aria Haghighi and Dan Klein. 2010. Coreference
Resolution in a Modular, Entity-centered Model. In
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 385–393. Association for Computa-
tional Linguistics.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% Solution. In Proceedings of the human lan-
guage technology conference of the NAACL, Com-
panion Volume: Short Papers, pages 57–60. Associ-
ation for Computational Linguistics.

Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388–395. Citeseer.

Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
driven Analysis of Challenges in Coreference Reso-
lution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, WA, USA, October.

Emmanuel Lassalle and Pascal Denis. 2013. Improv-
ing Pairwise Coreference Models through Feature
Space Hierarchy Learning. In ACL 2013-Annual
meeting of the Association for Computational Lin-
guistics.

Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s Multi-pass Sieve Corefer-
ence Resolution System at the CoNLL-2011 Shared

1425



Task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34. Association for Computational
Linguistics.

Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic Coreference Res-
olution based on Entity-centric, Precision-ranked
Rules. Computational Linguistics, 39(4):885–916.

Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An Extension of BLANC to
System Mentions. Proceedings of ACL, Baltimore,
Maryland, June.

Xiaoqiang Luo. 2005. On Coreference Resolution
Performance Metrics. In Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
25–32. Association for Computational Linguistics.

Chao Ma, Janardhan Rao Doppa, J Walker Orr,
Prashanth Mannem, Xiaoli Fern, Tom Dietterich,
and Prasad Tadepalli. 2014. Prune-and-score:
Learning for Greedy Coreference Resolution. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing.

Andrew McCallum and Ben Wellner. 2003. Toward
Conditional Models of Identity Uncertainty with
Application to Proper Noun Coreference. Advances
in Neural Information Processing Systems 17.

Yurii Nesterov. 1983. A Method of Solving a Convex
Programming Problem with Convergence Rate O
(1/k2). In Soviet Mathematics Doklady, volume 27,
pages 372–376.

Vincent Ng and Claire Cardie. 2002. Identifying
Anaphoric and Non-anaphoric Noun Phrases to Im-
prove Coreference Resolution. In Proceedings of
the 19th international conference on Computational
linguistics-Volume 1, pages 1–7. Association for
Computational Linguistics.

Vincent Ng. 2004. Learning Noun Phrase Anaphoric-
ity to Improve Coreference Resolution: Issues in
Representation and Optimization. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 151. Association for Com-
putational Linguistics.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In Joint Con-
ference on EMNLP and CoNLL-Shared Task, pages
1–40. ACL.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring Coreference Partitions of Predicted Men-
tions: A Reference Implementation. In Proceedings
of the Association for Computational Linguistics.

Altaf Rahman and Vincent Ng. 2009. Supervised
Models for Coreference Resolution. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 2-Volume
2, pages 968–977. ACL.

Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The Life and Death of Dis-
course Entities: Identifying Singleton Mentions. In
HLT-NAACL, pages 627–633.

Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 151–161. ACL.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic Compositional-
ity through Recursive Matrix-vector Spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. ACL.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A Machine Learning Ap-
proach to Coreference Resolution of Noun Phrases.
Computational Linguistics, 27(4):521–544.

Veselin Stoyanov and Jason Eisner. 2012. Easy-first
Coreference Resolution. In COLING, pages 2519–
2534. Citeseer.

Ilya Sutskever, James Martens, George Dahl, and Geof-
frey Hinton. 2013. On the Importance of Initializa-
tion and Momentum in Deep Learning. In Proceed-
ings of the 30th International Conference on Ma-
chine Learning, pages 1139–1147.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
theoretic Coreference Scoring Scheme. In Proceed-
ings of the 6th conference on Message Understand-
ing, pages 45–52. ACL.

Michael Wick, Sameer Singh, and Andrew McCallum.
2012. A Discriminative Hierarchical Model for Fast
Coreference at Large Scale. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
379–388. Association for Computational Linguis-
tics.

Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning Structural SVMs with Latent Variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169–1176.
ACM.

1426


