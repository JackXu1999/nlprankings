















































Relational Lasso An Improved Method Using the Relations Among Features


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1080–1088,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Relational Lasso
—An Improved Method Using the Relations among Features—

Kotaro Kitagawa Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology,

The University of Tokyo
kitagawa@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp

Abstract

Relational lasso is a method that incor-
porates feature relations within machine
learning. By using automatically obtained
noisy relations among features, relational
lasso learns an additional penalty parame-
ter per feature, which is then incorporated
in terms of a regularizer within the target
optimization function.

Relational lasso has been tested on three
different tasks: text categorization, po-
larity estimation, and parsing, where it
was compared with conventional lasso and
adaptive lasso (Zou, 2006) when using
a multi-class logistic regression optimiza-
tion method. Relational lasso outper-
formed these other lasso methods in the
tests.

1 Introduction

As machine learning methods scale up and now
deal with millions of features, we ideally want to
add all possible features without having to man-
ually verify or consider the effectiveness of each
feature with respect to the performance. In other
words, we need an automatic way to exploit any
usable information that can be obtained from fea-
tures. However, with current machine learning
methods, adding noisy features could lower per-
formance, so the user still has to decide which fea-
tures are worth adding.

Regularization methods have recently received
greater interest because of this need. Regular-
ization is expressed as a constraint term within
an optimization function, where the term is given
as a function regarding the importance weight of
each feature. Regularization provides a means
of importance control embedded within the target
optimization problem. Among the various regu-
larizers, lasso, proposed by (Tibshirani, 1996) is

the most widely used because of its mathematical
comprehensiveness.

This paper describes a method, which we call
relational lasso, that improves upon the conven-
tional lasso method. We show that relational
lasso improves the overall performance of classi-
fication compared with that of other lasso meth-
ods. This study was motivated through a limitation
we observed in conventional lasso: that features
could be inter-related, but such dependences are
not incorporated within the current regularization.
Therefore, the conventional method tends to favor
correlated features, which can lead to the impor-
tance of non-correlated features being neglected.

Relational lasso overcomes this limitation of
conventional lasso by introducing an additional
penalty parameter for each feature; this parame-
ter is estimated automatically given the noisy re-
lations among features, where the relations are
also automatically generated. While the proposed
method does not add to the computational com-
plexity of the conventional regularization method,
it improves the quality of classification. We ex-
plain the method and show empirical results from
tests based on three different text classification and
parsing tasks.

Regularization was originally proposed as a
way to avoid over-fitting by favoring some small
number of features. Our method presented in this
article exploits this approach further and attempts
to estimate the importance of each feature within
the relations it has with other features. As ex-
plained in more detail in the following section,
attempts along the same line have been made to
incorporate underlying relations among features,
such as by fused lasso through the ordering among
features (Tibshirani et al., 2005), or by group
lasso through groups among features (Yuan and
Lin, 2006). However, fused lasso assumes prob-
lems with features that can be ordered in some
meaningful way, and group lasso requires under-

1080



lying group information to be configured before
the method is applied. The closely related work
to ours is adaptive lasso (Zou, 2006), which intro-
duces an additional parameter per feature. How-
ever, since adaptive lasso does not explicitly pro-
cess relations among features, the estimation of
this additional parameter is completely different
from our method. Moreover, as will be empirically
shown, our method outperforms adaptive lasso,
which in fact performed worse than even the con-
ventional method.

Related work on regularization techniques has
shown the potential of regularization not only to
prevent over-fitting, but also to serve as a kind
of feature selection. Relational lasso provides an-
other step along this line. Here, we show that our
method outperforms other lasso methods in differ-
ent classification tasks. Moreover, it works well
even when noisy features are added, something
that degrades the performance of other lasso meth-
ods.

2 Related Work

As explained, feature selection is the key to our
method’s effectiveness, and here we summarize
the related work done along this line. A substantial
number of studies have been done on feature selec-
tion techniques, and these techniques can be clas-
sified into three categories according to (Guyon
and Elisseeff , 2003): wrapper methods, filter
methods and embedded methods.

The wrapper is the most naı̈ve way of select-
ing a subset of features through predictive accu-
racy (Kohavi and John, 1997). The user searches
the possible feature space greedily using an induc-
tion algorithm and selects the best subset with the
best predictive accuracy. Wrappers with greedy
algorithms can be computationally expensive and
the stepwise selection is often trapped into a lo-
cal optimal solution. Since the possible number
of subsets for a set is exponential to the number
of features in the original set, in practice the user
typically defines the subset arbitrarily depending
on the category of features, as was tested by (Scott
and Matwin , 1999). This makes impossible any
fine adjustment as to which individual features to
use.

Filter methods, on the other hand, select good
features according to some criteria, thus provid-
ing the means for selecting individual features.
These methods have been extensively studied, and

a good overview is available in (Manning, 1999).
The representatives of the evaluation function for
choosing good/bad features are chi-squares and
mutual information, and features having higher
scores for these functions are considered good fea-
tures. While filtering methods are effective and
therefore often used, these methods are indepen-
dent of the learning method that they are used
with. Moreover, the performance is not guaran-
teed to improve even though feature selection is
used.

The last category is embedded methods, where
the feature selection is embedded within the over-
all classification problem. The decision tree is an
example of an embedded method, and machine
learning techniques using pruning steps have been
studied (Perkins et al., 2003). Lasso, an acronym
for “least absolute shrinkage and selection oper-
ator”, using the L1 norm (Tibshirani, 1996) is a
computationally efficient method for simultane-
ously achieving the estimation and feature selec-
tion.

Although lasso helps achieve an effective
model, the L1 norm could cause biased estima-
tion among features (Knight and Fu, 2000) by not
being able to distinguish between truly significant
and noisy features.

To cope with this problem, (Zou and Hastie,
2005) proposed the elastic net method, which is
expressed as the conjunct of L1 and L2 norms of
the feature weights. They show that this method
works when the number of features substantially
exceeds the number of learning data, and also
when there is strong correlation between some fea-
tures. Another proposal is fused lasso, which in-
corporates the order of features (as found in their
numbers, such as found in the case when each
image pixel value forms a feature) (Tibshirani et
al., 2005). Here, the target application is protein
mass spectroscopy and gene expression data, and
the method is only applicable to a target where
the order among features is explicit, as in the case
of gene or image pixels. (Yuan and Lin, 2006)
proposed grouped lasso, which incorporates un-
derlying groups among features. The fused and
grouped lasso methods require configuration of
the structure among features.

Recently, a new approach called weighted lasso
is proposed which calculates the L1 norm on fea-
tures, each of which is weighted. As one method,
(Zou, 2006) proposed a two-step approach called

1081



adaptive lasso. This paper proposes an alternative
weighted lasso method, in which the estimation of
the weights is processed differently from that of
adaptive lasso. Although the procedure of adap-
tive lasso is closest to relational lasso, the learning
of adaptive lasso does not explicitly handle the re-
lations among features.

All these methods are attempts to incorporate
the relations, or structure, among features —such
as dependence, ordering and groups— into ma-
chine learning through the framework of regu-
larization. Although such dependence is not al-
ways given or tractable, we believe this informa-
tion can be learned from some automatically gen-
erated noisy relation among features.

3 L1-Regularization of Multi-Class
Logistic Regression

Before going on to the main points of relational
lasso, let us summarize the regularization frame-
work that we adopt. Regularization is the general
method used in classification. The target func-
tion has two terms, one for fitting and another
for regularization. This second term penalizes the
weights acquired by each feature, typically by in-
corporating the addition of their norms into a tar-
get function for the classifier. This prevents the
target function becoming too over-fitted by favor-
ing some specific sets of features. In this sense,
the regularization term can be considered as serv-
ing for feature selection.

Of the various ways to define the target func-
tion, in this paper we focus on the multi-class
logistic regression model and L1-regularization;
namely, the lasso method.

The fitting function adopted in this paper is a
multi-class logistic regression model, denoted as
LR in the following. LR is used to model the re-
lationship between the input vectors x = Rn and
labels y ∈ Y . The conditional probability for a
label y given x is defined as

p(y|x; w) = 1
Z(x)

exp
(
wT ϕ(x, y)

)

Z(x) =
∑

y′
exp

(
wT ϕ(x, y′)

)
,

where ϕ(x, y) ∈ Rm is the feature vector and
w ∈ Rm is the weight vector. When the train-
ing examples {(xi, yi)}(i = 1, · · · , l) are given,

minimization of the loss function

L(w) = −
∑

i

log p(yi|xi; w),

is equivalent to the maximum likelihood estima-
tion.

Regularization makes it possible to obtain a
good model for LR, without restricting the number
of features, by imposing appropriate restrictions
on weight w. Of the different ways of regulariza-
tion, in this paper we adopt (Tibshirani, 1996)’s
method of imposing an L1 norm on parameters
because of its mathematical simplicity, especially
when applied with LR. This method is called lasso
and facilitates both estimation and automatic vari-
able selection. When applying this lasso to logis-
tic regression, the MAP estimation of weights for
each feature is given by the following formula, the
target function to be optimized:

w∗ = arg min
w

L(w) + λ
∑

i

|wi|, (1)

where λ is the parameter defining the strength
of the regularization term’s influence on the op-
timization.

Although our proposal applies in general to var-
ious types of target function, in this paper we ex-
amine its effectiveness within this particular target
function. This target function was chosen because
the target function of LR-lasso is mathematically
comprehensive, so multi-class logistic regression
and lasso are widely applied. Further investiga-
tion to determine whether our method works well
for other target functions will be part of our future
work.

4 Relational Lasso —The Proposed
Method

The limitation of conventional lasso is that rela-
tions among features cannot be incorporated. For
example, highly correlated features which lead to-
wards a higher performance could all acquire rel-
atively large weights. This would lead to favor-
ing a single aspect that counts for the classifica-
tion and neglecting other minor but still impor-
tant aspects which would enable better classifi-
cation. This happens when a high correlation is
found among features. Therefore, when one fea-
ture is favored, the other correlated features must
be heavily penalized, so that features which count

1082



for classification from a different aspect are more
favored.

To express this, we adopt the weighted lasso ap-
proach, so an additional penalizing parameter αi
for each feature i is introduced in the second term
of formula (1):

w∗ = arg min
w

L(w) + λ
∑

i

αi|wi|. (2)

The solution found here, subject to the L1 regular-
izer, is equivalent to the solution obtained from the
constrained optimization problem:

minimize
w

L(w),

s.t.
∑

i

αi|wi| ≤ γ.

The parameter γ corresponds to λ of formula (2).
Each parameter αi determines the penalty for wi
and directly affects the importance of the ith fea-
ture.

Previous work on adaptive lasso (Zou, 2006)
also introduces an additional parameter, such as
αi, in addition to the weight parameter. Adap-
tive lasso focuses on the presence of oracle prop-
erties1, and works in two stages of optimization.
First weight wi is learned with conventional lasso.
Second, L1 norm is re-weighted with the param-
eters αi as αi = 1/|ŵi|δ being set from initial
lasso estimator ŵ using a parameter δ, and the op-
timization problem is processed using α. There-
fore, their way of learning this additional parame-
ter does not explicitly concern the exploitation of
additional information different from the original
weight wi. On the other hand, our α is estimated
given a noisy relation among features, thus it plays
a different role from w. In this sense, the way
to handle this additional parameter for relational
lasso is completely different from adaptive lasso.
In other words, the originality of our method lies
in using α to express the relations between under-
lying features.

The relations between features are denoted as
R, which is provided to the proposed algorithm
and used to estimate α. R denotes a pairwise de-
pendence relation between features. That is, if
there are m features in total, R ⊂ (1, · · · ,m) ×

1Oracle property (Fan and Li , 2001) is satisfied if the op-
timization problem can correctly select the nonzero weights
with probability converging to one and the estimators of the
nonzero weights are asymptotically normal with the same
means and covariance that they would have if the zero co-
efficients were known in advance.

(1, · · · , m). Unlike previous work such as fused
(Tibshirani et al., 2005) and grouped lasso (Yuan
and Lin, 2006), R in our work is a noisy relation
which can be automatically obtained by scanning
through features.

Although there are various possibilities for ob-
taining R, one way is through the inclusion rela-
tion among features. Given a pair of features p and
q, the feature q includes p, if in every data of the
learning data, when the value of feature q is non-
zero, the value of feature p is always non-zero.
For example, for the case of the adjective “eco-
nomic” and its stem “econom”, the latter includes
the former while also being a stem for other terms
such as “economy” and “economist”. In the final
classification, it is unknown which of “econom”
and “economic” counts. For part-of-speech tag-
ging, “econom” would not provide much infor-
mation since it does not have a complete form,
but for topic estimation, “econom” might provide
sufficient information by representing the terms
“economic”, “economy” and “economist”. In both
cases, when the two words appear as features, they
share a tight relation and when one is given high
importance, the other will as well in conventional
lasso. In relational lasso, if one representative is
selected, then other similar features in the same
group will acquire less importance by having a
larger penalty.

The overall procedure is shown in Procedure
1. The procedure obtains three kinds of learning
data input, a relation among features, and param-
eter values. Before optimizing the target func-
tion, denoted in the second line from the bot-
tom, the procedure calculates α depending on the
given relation R among features. This procedure
is expressed in terms of a while-structure, which
enables the adjustment of penalty parameters for
highly correlated features. The processed feature
is held in set F to avoid any duplicate processing
of features.

In the while-structure, features are selected one
at a time in the order of larger values of ∂L(w)∂w ,
with w being the zero vector2. The number of the
selected feature is denoted as k∗. Then, for all ks
which are related to k∗ in R, the α is enlarged by a

2There are other possibilities for this order of process-
ing features, such as randomizing the order. In the Graft-
ing method (Perkins et al., 2003), the processed feature is se-
lected by calculating ∂L(w)

∂w
every time in the while-structure,

which is also possible with relational lasso. This however re-
mains as future work.

1083



Procedure 1 Relational Lasso
Input:

• (xi, yi)(i = 1, · · · , n)
• Parameters λ, a1, a2
• Relation among m features

R ⊂ (1, · · · ,m) × (1, · · · ,m)

α = 1,w = 0, F = {}
v = ∂L(w)∂w
while |F | < m do

k∗ = arg max
k ̸∈F

|vk|

for all k ̸∈ F and (k∗, k) ∈ R do
αk = αk + a1

end for
for all k ̸∈ F and (k, k∗) ∈ R do

αk = αk + a2
end for
F = F ∪ k∗

end while
w = arg min

w
L(w) + λ

∑
k αk|wk|

return w

certain constant a1 and a2, depending on whether
the feature k is included or k∗ is included. When
the while procedure ends, F includes all the fea-
tures. Finally, w is estimated in terms of LR-lasso,
where the second term is weighted further with
the thus estimated α. When some specific wk be-
comes zero, this means that the weight is consid-
ered as not selected for the classification task.

One further improvement that might be possi-
ble for the above procedure is to repeat the while-
structure and the estimation of w, so that α and w
perform co-training; this also remains for our fu-
ture work. Moreover, the procedure presented here
remains an ad hoc modification of conventional
lasso based on our motivation. A more proper
mathematical reformulation of this method will be
part of our future work.

5 Evaluation

5.1 Experimental Settings

We will consider the following two feature sets.

• Standard features

• Standard and additional features

Here, an additional feature set is introduced so that
it can be noisy with respect to the classification.

Therefore, the interest lies in whether the perfor-
mance is better when we have the additional fea-
tures than it is when we have only the standard
features.

We consider three methods:

• Conventional lasso (Tibshirani, 1996)

• Adaptive lasso (Zou, 2006)

• Relational lasso (proposed method)

We are interested in whether relational lasso per-
forms better than the other methods. In practice
we can select the best λ parameter used for lasso
methods, using cross-validation, although we ex-
amined multiple λs, which determine the strength
of the regularizers’ influence as shown in formulas
(1) and (2) of Sections 3 and 4.

The other parameters introduced in Section 4
are set as follows. Adaptive lasso has the param-
eter δ = 1, which is set as the common choice in
(Krämer et al., 2009) and the parameter αi is de-
fined from initial estimator ŵ as follows:

αi = max

{
1

ˆ|wi|
, 1

}
.

For relational lasso, parameters a1 and a2 were
each set to 1. For L1 regularized LR, a coor-
dinate descent method is implemented by mod-
ifying LIBLINEAR3. Coordinate descent meth-
ods have been widely applied elsewhere because
of their suitability for application to higher-order
problems (Yuam et al., 2010).

For each pair of a feature and a method, we con-
sidered the following three problems of text clas-
sification, polarity estimation, and statistical pars-
ing. The next three sections explain the standard
and additional feature sets, the relation among fea-
tures R, and the evaluation scores.

Task 1: Text Classification

Twenty Newsgroups (20NG)4 were used as the
dataset for text classification. This collection con-
tains 18,846 English documents partitioned across
20 different news groups.

The data was sorted by date, with the first 60%
used as a training set and the remaining 40% used
as a test set. A simple bag of words was used

3http://www.csie.ntu.edu.tw/∼cjlin/
liblinear/

4provided by Jason Rennie, http://people.
csail.mit.edu/jrennie/20Newsgroups/

1084



Table 1: Features used for dependency parsing
unigram for w in wi−1, wi, wj , wj+1, wj+2, wj+2 pos(w), lex(w)

for w in wi−1, wi, wj , pos(wleft), lex(wleft)
for w in wi−1, wi, pos(w

right
i ), lex(w

right
i ), pos(w

head
i ),lex(w

head
i )

bigram for (v, w) in (wi, wj), (wi−1, wj) pos(v)pos(w),pos(v)lex(w),lex(v)pos(w),lex(v)lex(w)

add bigram for (v, w) in (wi, wj+1), (wj , wj+1) pos(v)pos(w),pos(v)lex(w),lex(v)pos(w),lex(v)lex(w)
add preposition for w in wj+1, wj+2, wj+3 lex(wi)lex(wj)pos(w), pos(wi)lex(wj)lex(w)

(if wj is a preposition)

as the standard feature sets, whereas all stems of
all words were used as additional features. R was
defined as the relation between each word and its
stems.

A multi-class classification task is typically
evaluated by macro and micro F1 values, so we
also provided these values.

Task 2: Polarity Estimation

Polarity dataset v2.05 was used as the second data
set. The content of each data was a movie re-
view in text, tagged with the sentiment of positive
or negative. The data consisted of 1,000 positive
and 1,000 negative reviews. Since the data set was
small, the average accuracy was obtained through
10-fold cross validation.

Feature sets were basically the same as for Task
1, where the standard was a bag of words, the ad-
ditional set consisted of word stems, and relation
R was the relation among words and their stems.

The evaluation was based on the accuracy of the
binary classification of positive/negative.

Task 3: Parsing

We also tested the methods on a parsing task,
which was a task drastically different from tasks
1 and 2. We used CoNLL-X formatted sentences
from the Wall Street Journal section of the Penn
Tree-bank. Sections 2-21 were used as training
data (39,832 sentences), and section 23 was used
as test data (2,416 sentences).

The parsing algorithm we tested is the standard
shift-reduce parsing proposed by (Nivre, 2003),
where the parsing proceeds by successive determi-
nation of the relation between two words (denoted
as wi and wj). Such a determination is consid-
ered a 4-class classification problem that is mod-
eled and learned by LR, augmented by the three
lasso methods being evaluated.

5provided by Bo Pang, http://www.cs.cornell.
edu/people/pabo/movie-review-data/

The standard features used are listed in Table
1. Here, pos(w) indicates the part of speech of
the word w, where wi indicates the ith word of
a given sentence, and wlefti indicates the already
parsed dependent word of wi placed to its farthest
left side. The additional feature set included all
dependent words involving wi and wj , and all bi-
grams concerning words used as features in the
standard set. In our dependency parsing task, we
measured the word accuracy which was defined as
the ratio of words assigned correct heads divided
by the total of all words.

number of non−zero features

m
ic

ro
 F

−
va

lu
e

0 20000 40000 60000 80000 100000 120000 140000

70
%

75
%

80
%

85
%

90
%

95
%

relational
conventional
adaptive

Figure 1: Task 1: Micro F1 values for the number
of non-zero features

number of non−zero features

m
ac

ro
 F

−
va

lu
e

0 20000 40000 60000 80000 100000 120000 140000

70
%

75
%

80
%

85
%

90
%

95
%

relation
conventional
adaptive

Figure 2: Task 1: Macro F1 values for the number
of non-zero features

1085



5.2 Results

Figures 1 and 2 show the results for Task 1, Fig-
ure 3 those for Task 2, and Figure 4 those for Task
3. Horizontal axes show the number of features
and vertical axes show accuracy. Each graph has
three lines, indicating the conventional, adaptive
and relational lasso methods applied to the stan-
dard and additional features all together. Each line
has five points, each corresponding to a different
value of λ. The horizontal coordinate was deter-
mined by counting how many features remained
non-zero for each value of λ.

Overall, all figures, except for Figure 3 show
that relational lasso outperformed the adaptive and
conventional lasso methods. This was to be ex-
pected, since relational lasso has the relation R as
input, unlike the conventional lasso method. This
confirms that information from the underlying R
does improve lasso performance. Curiously, the
performance of adaptive lasso for some figures
was lower than that of the conventional method.
The reason for this will be given later in this sec-
tion.

As Figure 3 and Figure 4 show, the performance
was competitive among the three lasso methods,
when the number of features were small. How-
ever, with a large number of features, relational
lasso generally outperforms the other lassos.

number of non−zero features

ac
cu

ra
cy

0 1000 2000 3000 4000 5000 6000 7000 8000 9000

80
%

82
%

84
%

86
%

88
%

90
%

92
%

relational
conventional
adaptive

Figure 3: Task 2: Accuracy for the number of non-
zero features

It is difficult to compare the performance since
different λ values lead to different levels of per-
formance, so the maximum performance obtained
by changing the λ value is shown in Table 2.
Columns are for different lasso methods with stan-
dard and additional feature sets, whereas rows rep-
resent different tasks. Note that the best values
of λ differ depending on the pairs of methods and
features.

number of non−zero features [10^6]

ac
cu

ra
cy

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

82
%

84
%

86
%

88
%

90
%

92
%

relational
conventional
adaptive

Figure 4: Task 3: Word accuracy for the number
of non-zero features

Overall, the last column presents the highest
performance in each row, thus suggesting the ef-
fectiveness of relational lasso.

For Task 2, when features of standard and ad-
ditional sets were used, the performance of the
conventional method decreased compared to that
when only the standard set was used. This could
happen if the additional feature set is noisy and the
regularizer cannot exploit the useful information
from the additional set of features. On the other
hand, the performance of relational and adaptive
lasso for the same task was improved by extract-
ing the useful information; that is, the performance
was higher than when using only the standard fea-
tures. This shows that the use of underlying infor-
mation among features enhances the overall per-
formance.

For Tasks 1 and 3, adding features led to bet-
ter performance than when using only the standard
set. Note, though, that the performance increase
was greatest for relational lasso. Thus, relational
lasso is the best among the three lasso methods at
exploiting information, and thus performs better in
terms of accuracy.

In this table, too, we see that for Task 2, the
performance of adaptive lasso is below that of the
conventional lasso. We consider the reason for this
to be as follows. Since the optimization for adap-
tive lasso is done in two stages, some of the fea-
tures are dropped within the first stage. In the sec-
ond stage, these features will never re-acquire any
importance. In other words, the feature selection
must be done at the very end in order to preserve
the possibility of some features to re-acqure the
importance through learning.

Before ending, we must note the impact of re-
lational lasso on the speed of overall processing.

1086



Table 2: Maximum Performance among Various Values of λ for Three Lasso Methods

Lasso Methods Conventional Adaptive Relational
Feature Sets Std Std+Add Std Std+Add Std Std+Add

Task 1 (micro) 79.67% 81.03% 76.78% 77.16% 78.87% 81.81%
Task 1 (macro) 79.43% 80.69% 76.53% 77.67% 78.52% 81.72%

Task 2 85.81% 84.95% 85.56% 86.1% 84.82% 86.45%
Task 3 75.87% 88.81% 74.64% 87.71% 75.97% 89.23%

The pre-processing to obtain α is very fast, since
it only scans the number of features once. The
bottleneck of the procedure lies in the estimation
of w since this requires convergence through a
repetitive procedure. Therefore, the computational
complexity of relational lasso will not change even
with α within the regularizer, and the overall speed
of relational lasso is almost the same as that of the
conventional method. In contrast, adaptive lasso
requires twice as much time since the bottleneck
part is done twice. In this sense, our method out-
performs adaptive lasso in speed and is not signif-
icantly slower than conventional, at least for the
settings we have examined in this section.

6 Conclusion

Relational lasso utilizes relations among features
to better exploit information through regulariza-
tion, especially through lasso methods. The con-
ventional lasso method is not designed to incorpo-
rate relations among features, and this leads to bi-
ased weighting of a group of features having sim-
ilar behavior. Relational lasso controls such rela-
tions underlying features by introducing a penalty
parameter for each feature. The penalty increases
when a feature is related to some other feature hav-
ing less of a penalty. This parameter score is in-
corporated as the regularization term of the target
machine learning function for the optimization ob-
jective.

We compared relational lasso to the conven-
tional method and the adaptive lasso proposed by
(Zou, 2006), which also uses an additional pa-
rameter per feature. We evaluated the methods
based on how well they performed three tasks of
text categorization, polarity estimation, and pars-
ing. Relational lasso outperformed the other lasso
methods in these tasks. Moreover, the perfor-
mance of the conventional lasso methods deterio-
rated when noisy features were added, while rela-
tional lasso successfully extracted useful informa-
tion from these features and its performance im-

proved.
As part of our future work, we plan to inves-

tigate whether our method works for other tasks
such as tagging, and with other target functions.
Moreover, there are many directions we can take
to further improve the method, such as through co-
training. Last, it will be interesting to see how our
method can be mathematically reformulated.

References
Jianqing Fan, Runze Li. 2001. Variable selection via non-

concave penalized likelihood and its oracle properties.
Journal of the American Statistical Association, vol.96,
pp.1348–1360.

Isabelle Guyon, André Elisseeff. 2003. An introduction to
variable and feature selection. The Journal of Machine
Learning Research, vol.3, pp.1157–1182.

G. V. Kass. 1980. An exploratory technique for investigating
large quantities of categorical data. Journal of the Royal
Statistical Society. Series C, vol.29, pp.119–127.

Keith Knight, Wenjiang Fu. 2000. Asymptotics for lasso-
type estimators. The Annals of Statistics, vol.28, pp.1356–
1378.

Ron Kohavi, George H. John. 1997. Wrappers for feature
subset selection, Artificial intelligence, vol.97, pp.273–
324.

Nicole Krämer, Juliane Schäfer, Anne-Laure Boulesteix.
2009. Regularized estimation of large-scale gene asso-
ciation networks using graphical Gaussian models. BMC
Bioinformatics, vo.10.

Christopher Manning, Hinrich Schuetze. 1999. Foundations
of Statistical Natural Language Processing. MIT Press.

Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th International
Workshop on Parsing Technologies, pp. 149-160.

Simon Perkins, Kervin Lacker, and James Theiler. 2003.
Grafting: Fast, Incremental Feature Selection by Gradi-
ent Descent in Function Space. The Journal of Machine
Learning Research, vol.3, pp.1333–1356.

Yvan Saeys, Iñaki Inza, and Pedro Larrañaga. 2007. A
review of feature selection techniques in bioinformatics.
Bioinformatics, vol.23, num.19, pp.2507–2517.

Sam Scott, Stan Matwin. 1999. Feature engineering for
text classification. Proceedings of ICML-99, 16th Inter-
national Conference on Machine Learning, pp.379–388.

1087



Robert Tibshirani. 1996. Regression shrinkage and selec-
tion via the lasso. Journal of the Royal Statistical Society,
Series B, vol.58, pp.267–288.

Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu,
and Keith Knight 2005. Sparsity and smoothness via the
fused lasso. Journal of the Royal Statistical Society, Se-
ries B, vol.67, pp.91–108.

Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen
Lin. 2010. A comparison of optimization methods and
software for large-scale l1-regularized linear classifica-
tion. The Journal of Machine Learning Research, vol.11,
pp.3183–3234.

Ming Yuan, Yi Lin. 2006. Model selection and estimation
in regression with grouped variables. Journal of the Royal
Statistical Society, Series B, vol.68, pp.49–67.

Ying Yang, Jan O. Pedersen. 1997. A comparative study on
feature selection in text categorization. Proceedings of the
Fourteenth International Conference on Machine Learn-
ing, pp.412–420.

Hui Zou. 2006. The adaptive lasso and its oracle properties.
Journal of the American Statistical Association, vol.101,
pp.1418–1429.

Hui Zou, Trevor Hastie. 2005. Regularization and variable
selection via the elastic net. Journal of the Royal Statisti-
cal Society, Series B, vol.67, pp.301–320.

1088


