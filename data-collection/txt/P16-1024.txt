



















































On the Role of Seed Lexicons in Learning Bilingual Word Embeddings


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

On the Role of Seed Lexicons in Learning Bilingual Word Embeddings

Ivan Vulić and Anna Korhonen
Language Technology Lab

DTAL, University of Cambridge
{iv250, alk23}@cam.ac.uk

Abstract

A shared bilingual word embedding space
(SBWES) is an indispensable resource in
a variety of cross-language NLP and IR
tasks. A common approach to the SB-
WES induction is to learn a mapping func-
tion between monolingual semantic spaces,
where the mapping critically relies on a
seed word lexicon used in the learning pro-
cess. In this work, we analyze the impor-
tance and properties of seed lexicons for
the SBWES induction across different di-
mensions (i.e., lexicon source, lexicon size,
translation method, translation pair relia-
bility). On the basis of our analysis, we
propose a simple but effective hybrid bilin-
gual word embedding (BWE) model. This
model (HYBWE) learns the mapping be-
tween two monolingual embedding spaces
using only highly reliable symmetric trans-
lation pairs from a seed document-level
embedding space. We perform bilingual
lexicon learning (BLL) with 3 language
pairs and show that by carefully selecting
reliable translation pairs our new HYBWE
model outperforms benchmarking BWE
learning models, all of which use more
expensive bilingual signals. Effectively,
we demonstrate that a SBWES may be in-
duced by leveraging only a very weak bilin-
gual signal (document alignments) along
with monolingual data.

1 Introduction

Dense real-valued vector representations of words
or word embeddings (WEs) have recently gained
increasing popularity in natural language process-
ing (NLP), serving as invaluable features in a broad

              Monolingual                 vs                   Bilingual

Figure 1: A toy example of a 3-dimensional mono-
lingual vs shared bilingual word embedding space
(further SBWES) from Gouws et al. (2015).

range of NLP tasks, e.g., (Turian et al., 2010; Col-
lobert et al., 2011; Chen and Manning, 2014). Sev-
eral studies have showcased a direct link and com-
parable performance to “more traditional” distribu-
tional models (Turney and Pantel, 2010). Yet the
widely used skip-gram model with negative sam-
pling (SGNS) (Mikolov et al., 2013b) is considered
as the state-of-the-art word representation model,
due to its simplicity, fast training, as well as its
solid and robust performance across a wide variety
of semantic tasks (Baroni et al., 2014; Levy and
Goldberg, 2014b; Levy et al., 2015).

Research interest has recently extended to bilin-
gual word embeddings (BWEs). BWE learning
models focus on the induction of a shared bilingual
word embedding space (SBWES) where words
from both languages are represented in a uniform
language-independent manner such that similar
words (regardless of the actual language) have sim-
ilar representations (see Fig. 1). A variety of BWE
learning models have been proposed, differing in
the essential requirement of a bilingual signal nec-
essary to construct such a SBWES (discussed later
in Sect. 2). SBWES may be used to support many
tasks, e.g., computing cross-lingual/multilingual
semantic word similarity (Faruqui and Dyer, 2014),
learning bilingual word lexicons (Mikolov et al.,
2013a; Gouws et al., 2015; Vulić et al., 2016),
cross-lingual entity linking (Tsai and Roth, 2016),

247



parsing (Guo et al., 2015; Johannsen et al., 2015),
machine translation (Zou et al., 2013), or cross-
lingual information retrieval (Vulić and Moens,
2015; Mitra et al., 2016).

BWE models should have two desirable prop-
erties: (P1) leverage (large) monolingual training
sets tied together through a bilingual signal, (P2)
use as inexpensive bilingual signal as possible in
order to learn a SBWES in a scalable and widely
applicable manner across languages and domains.

While we provide a classification of related work,
that is, different BWE models according to these
properties in Sect. 2.1, the focus of this work is
on a popular class of models labeled Post-Hoc
Mapping with Seed Lexicons. These models op-
erate as follows (Mikolov et al., 2013a; Dinu et al.,
2015; Lazaridou et al., 2015; Ammar et al., 2016):
(1) two separate non-aligned monolingual embed-
ding spaces are induced using any monolingual WE
learning model (SGNS is the typical choice), (2)
given a seed lexicon of word translation pairs as the
bilingual signal for training, a mapping function
is learned which ties the two monolingual spaces
together into a SBWES.

All existing work on this class of models as-
sumes that high-quality training seed lexicons are
readily available. In reality, little is understood
regarding what constitutes a high quality seed lexi-
con, even with “traditional” distributional models
(Gaussier et al., 2004; Holmlund et al., 2005; Vulić
and Moens, 2013). Therefore, in this work we ask
whether BWE learning could be improved by mak-
ing more intelligent choices when deciding over
seed lexicon entries. In order to do this we delve
deeper into the cross-lingual mapping problem by
analyzing a spectrum of seed lexicons with respect
to controllable parameters such as lexicon source,
its size, translation method, and translation pair
reliability.

The contributions of this paper are as follows:
(C1) We present a systematic study on the impor-
tance of seed lexicons for learning mapping func-
tions between monolingual WE spaces.
(C2) Given the insights gained, we propose a sim-
ple yet effective hybrid BWE model HYBWE that
removes the need for readily available seed lexi-
cons, and satisfies properties P1 and P2. HYBWE
relies on an inexpensive seed lexicon of highly reli-
able word translation pairs obtained by a document-
level BWE model (Vulić and Moens, 2016) from
document-aligned comparable data.

(C3) Using a careful pair selection process when
constructing a seed lexicon, we show that in the
BLL task HYBWE outperforms a BWE model
of Mikolov et al. (2013a) which relies on readily
available seed lexicons. HYBWE also outperforms
state-of-the-art models of (Hermann and Blunsom,
2014b; Gouws et al., 2015) which require sentence-
aligned parallel data.

2 Learning SBWES using Seed Lexicons

Given source and target language vocabularies V S

and V T , all BWE models learn a representation of
each word w ∈ V S t V T in a SBWES as a real-
valued vector: w = [f1, . . . , fd], where fk ∈ R
denotes the value for the k-th cross-lingual fea-
ture for w within a d-dimensional SBWES. Se-
mantic similarity sim(w, v) between two words
w, v ∈ V S t V T is then computed by applying
a similarity function (SF), e.g. cosine (cos) on
their representations in the SBWES: sim(w, v) =
SF (w,v) = cos(w,v).

2.1 Related Work: BWE Models and
Bilingual Signals

BWE models may be clustered into four different
types according to bilingual signals used in train-
ing, and properties P1 and P2 (see Sect. 1). Upad-
hyay et al. (2016) provide a similar overview of
recent bilingual embedding learning architectures
regarding different bilingual signals required for
the embedding induction.

(Type 1) Parallel-Only: This group of BWE mod-
els relies on sentence-aligned and/or word-aligned
parallel data as the only data source (Zou et al.,
2013; Hermann and Blunsom, 2014a; Kočiský et
al., 2014; Hermann and Blunsom, 2014b; Chandar
et al., 2014). In addition to an expensive bilingual
signal (colliding with P2), these models do not
leverage larger monolingual datasets for training
(not satisfying P1).

(Type 2) Joint Bilingual Training: These models
jointly optimize two monolingual objectives, with
the cross-lingual objective acting as a cross-lingual
regularizer during training (Klementiev et al., 2012;
Gouws et al., 2015; Soyer et al., 2015; Shi et al.,
2015; Coulmance et al., 2015). The idea may be
summarized by the simplified formulation (Luong
et al., 2015): γ(MonoS+MonoT )+δBi. The mono-
lingual objectives MonoS and MonoT ensure that
similar words in each language are assigned similar

248



embeddings and aim to capture the semantic struc-
ture of each language, whereas the cross-lingual
objective Bi ensures that similar words across lan-
guages are assigned similar embeddings. It ties the
two monolingual spaces together into a SBWES
(thus satisfying P1). Parameters γ and δ govern the
influence of the monolingual and bilingual compo-
nents.1 The main disadvantage of Type 2 models
is the costly parallel data needed for the bilingual
signal (thus colliding with P2).

(Type 3) Pseudo-Bilingual Training: This set
of models requires document alignments as bilin-
gual signal to induce a SBWES. Vulić and Moens
(2016) create a collection of pseudo-bilingual docu-
ments by merging every pair of aligned documents
in training data, in a way that preserves impor-
tant local information: words that appeared next to
other words within the same language and those
that appeared in the same region of the document
across different languages. This collection is then
used to train word embeddings with monolingual
SGNS from word2vec.

With pseudo-bilingual documents, the “context”
of a word is redefined as a mixture of neighbouring
words (in the original language) and words that
appeared in the same region of the document (in
the ”foreign” language). The bilingual contexts
for each word in each document steer the final
model towards constructing a SBWES. The advan-
tage over other BWE model types lies in exploiting
weaker document-level bilingual signals (satisfying
P2), but these models are unable to exploit mono-
lingual corpora during training (unlike Type 2 or
Type 4; thus colliding with P1).

(Type 4) Post-Hoc Mapping with Seed Lexicons:
These models learn post-hoc mapping functions be-
tween monolingual WE spaces induced separately
for two different languages (e.g., by SGNS). All
Type 4 models (Mikolov et al., 2013a; Faruqui
and Dyer, 2014; Dinu et al., 2015; Lazaridou et
al., 2015) rely on readily available seed lexicons
of highly frequent words obtained by e.g. Google
Translate (GT) to learn the mapping (again collid-
ing with P2), but they are able to satisfy P1.

1Type 1 models may be considered a special case of Type
2 models: Setting γ = 0 reduces Type 2 models to Type 1
models trained solely on parallel data, e.g., (Hermann and
Blunsom, 2014b; Chandar et al., 2014). γ = 1 results in the
models from (Klementiev et al., 2012; Gouws et al., 2015;
Soyer et al., 2015; Coulmance et al., 2015).

2.2 Post-Hoc Mapping with Seed Lexicons:
Methodology and Lexicons

Key Intuition One may infer that a type-hybrid
procedure which would retain only highly reliable
translation pairs obtained by a Type 3 model as a
seed lexicon for Type 4 models effectively satisfies
both requirements: (P1) unlike Type 1 and Type
3, it can learn from monolingual data and tie two
monolingual spaces using the highly reliable trans-
lation pairs, (P2) unlike Type 1 and Type 2, it does
not require parallel data; unlike Type 4, it does not
require external lexicons and translation systems.
The only bilingual signal required are document
alignments. Therefore, our focus is on novel less
expensive Type 4 models.

Overview The standard learning setup we use
is as follows: First, two monolingual embedding
spaces, RdS and RdT , are induced separately in
each of the two languages using a standard mono-
lingual WE model such as CBOW or SGNS. dS
and dT denote the dimensionality of monolingual
WE spaces. The bilingual signal is a seed lexicon,
i.e., a list of word translation pairs (xi, yi), where
xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT .
Learning Objectives Training is cast as a mul-
tivariate regression problem: it implies learning
a function that maps the source language vectors
from the training data to their corresponding target
language vectors. A standard approach (Mikolov
et al., 2013a; Dinu et al., 2015) is to assume a lin-
ear map W ∈ RdS×dT , where a L2-regularized
least-squares error objective (i.e., ridge regression)
is used to learn the map W. The map is learned by
solving the following optimization problem (typi-
cally by stochastic gradient descent (SGD)):

min
W∈RdS×dT

||XW −Y||2F + λ||W||2F (1)

X and Y are matrices obtained through the re-
spective concatenation of source language and tar-
get language vectors from training pairs. Once the
linear map W is estimated, any previously unseen
source language word vector xu may be straightfor-
wardly mapped into the target language embedding
space RdT as Wxu. After mapping all vectors x,
x ∈ V S , the target embedding space RdT in fact
serves as SBWES.2

2Another possible objective (found in the zero-shot learn-
ing literature) is a margin-based ranking loss (Weston et al.,
2011; Lazaridou et al., 2015). We omit the results with this
objective for brevity, and due to the fact that similar trends are
observed as with (more standard) linear maps.

249



Seed Lexicon Source and Translation Method
Prior work on post-hoc mapping with seed lexi-
cons used a translation system (i.e., GT) to translate
highly frequent English words to other languages
such as Czech, Spanish (Mikolov et al., 2013a;
Gouws et al., 2015) or Italian (Dinu et al., 2015;
Lazaridou et al., 2015). This method presupposes
the availability and high quality of such an exter-
nal translation system. To simulate this setup, we
take as a starting point the BNC word frequency
list from Kilgarriff (1997) containing 6, 318 most
frequent English lemmas. The list is then translated
to other languages via GT. We call the BNC-based
lexicons obtained by employing Google Translate
BNC+GT.

In this paper, we propose another option: first,
we learn the ”first” SBWES (i.e., SBWES-1) us-
ing another BWE model (see Sect. 2.1), and then
translate the BNC list through SBWES-1 by re-
taining the nearest cross-lingual neighbor yi ∈ V T
for each xi in the BNC list which is represented in
SBWES-1. The pairs (xi, yi) constitute the seed
lexicon needed for learning the mapping between
monolingual spaces, that is, to induce the final
SBWES-2.

Although in theory any BWE induction model
may be used to induce SBWES-1, we rely on
a document-level Type 3 BWE induction model
from (Vulić and Moens, 2016), since it requires
only document alignments as (weak) bilingual sig-
nal. The resulting hybrid BWE induction model
(HYBWE) combines the output of a Type 3 model
(SBWES-1) and a Type 4 model (SBWES-2).
This seed lexicon and BWE learning variant is
called BNC+HYB.

Our new hybrid model allows us to also use
source language words occurring in SBWES-1
sorted by frequency as seed lexicon source, again
leaning on the intuition that higher frequency phe-
nomena are more reliably translated using statisti-
cal models. Their translations can also be found
through SBWES-1 to obtain seed lexicon pairs
(xi, yi). This variant is called HFQ+HYB.

Another possibility, recently introduced by Kiros
et al. (2015) for vocabulary expansion in monolin-
gual settings, relies on all words shared between
two vocabularies to learn the mapping. In this work,
we test the ability and limits of such orthographic
evidence in cross-lingual settings: seed lexicon
pairs are (xi, xi), where xi ∈ V S and xi ∈ V T .
This seed lexicon variant is called ORTHO.

Seed Lexicon Size While all prior reported only
results with restricted seed lexicon sizes only (i.e.,
1K, 2K and 5K lexicon pairs are used as standard),
in this work we provide a full-fledged analysis of
the influence of seed lexicon size on the SBWES
performance in cross-lingual tasks. More extreme
settings are also investigated, in the attempt to an-
swer two important questions: (1) Can a Type 4
SBWES be induced in a limited setting with only
a few hundred lexicon pairs available (e.g., 100-
500)? (2) Can the Type 4 models profit from the
inclusion of more seed lexicon pairs (e.g., more
than 5K, even up to 40K-50K lexicon pairs)?

Translation Pair Reliability When building
seed lexicons through SBWES-1 (i.e., BNC+HYB
and HFQ+HYB methods), it is possible to con-
trol for the reliability of translation pairs to be in-
cluded in the final lexicon, with the idea that the
use of only highly reliable pairs can potentially
lead to an improved SBWES-2. A simple yet
effective reliability reliability feature for transla-
tion pairs is the symmetry constraint (Peirsman and
Padó, 2010; Vulić and Moens, 2013) : two words
xi ∈ V S and yi ∈ V S are used as seed lexicon
pairs only if they are mutual nearest neighbours
given their representations in SBWES-1. The two
variants of seed lexicons with only symmetric pairs
are BNC+HYB+SYM and HFREQ+HYB+SYM.
We also test the variants without the sym-
metry constraint (i.e., BNC+HYB+ASYM and
HFQ+HYB+ASYM).

Even more conservative reliability measures may
be applied by exploiting the scores in the lists of
translation candidates ranked by their similarity
to the cue word xi. We investigate a symmetry
constraint with a threshold: two words xi ∈ V S
and yi ∈ V S are included as seed lexicon pair
(xi, yi) iff they are mutual nearest neighbours in
SBWES-1 and it holds:

sim(xi, yi)− sim(xi, zi) > THR (2)
sim(yi, xi)− sim(yi, wi) > THR (3)

where zi ∈ V T is the second best translation can-
didate for xi, and wi ∈ V S for yi. THR is a param-
eter which specifies the margin between the two
best translation candidates. The intuition is that
highly unambiguous and monosemous translation
pairs (which is reflected in higher score margins)
are also highly reliable.3

3Other (more elaborate) reliability measures exist in the

250



3 Experimental Setup

Task: Bilingual Lexicon Learning (BLL) Af-
ter the final SBWES is induced, given a list of n
source language words xu1, . . . , xun, the task is to
find a target language word t for each xu in the list
using the SBWES. t is the target language word
closest to the source language word xu in the in-
duced SBWES, also known as the cross-lingual
nearest neighbor. The set of learned n (xu, t) pairs
is then run against a gold standard BLL test set.
Following the standard practice (Mikolov et al.,
2013a; Dinu et al., 2015), for all Type 4 models, all
pairs containing any of the test words xu1, . . . , xun
are removed from training seed lexicons.

Test Sets For each language pair, we evaluate on
standard 1,000 ground truth one-to-one translation
pairs built for three language pairs: Spanish (ES)-,
Dutch (NL)-, Italian (IT)-English (EN) by Vulić
and Moens (2013). The dataset is generally con-
sidered a benchmarking test set for BLL models
that learn from non-parallel data, and is available
online.4 We have also experimented with two other
benchmarking BLL test sets (Bergsma and Durme,
2011; Leviant and Reichart, 2015) observing a very
similar relative performance of all the models in
our comparison.

Evaluation Metrics We measure the BLL per-
formance using the standard Top 1 accuracy (Acc1)
metric (Gaussier et al., 2004; Mikolov et al., 2013a;
Gouws et al., 2015).5

Baseline Models To induce SBWES-1, we re-
sort to document-level embeddings of Vulić and
Moens (2016) (Type 3). We also compare to re-
sults obtained directly by their model (BWESG) to
measure the performance gains with HYBWE.

To compare with a representative Type 2 model,
we opt for the BilBOWA model of Gouws et al.
(2015) due to its solid performance and robustness
in the BLL task when trained on general-domain
corpora such as Wikipedia (Luong et al., 2015), its
reduced complexity reflected in fast computations
on massive datasets, as well as its public availabil-

literature (Smith and Eisner, 2007; Tu and Honavar, 2012;
Vulić and Moens, 2013), but we do not observe any significant
gains when resorting to the more complex reliability estimates.

4http://people.cs.kuleuven.be/~ivan.vulic/
5Similar trends are observed within a more lenient setting

with Acc5 and Acc10 scores, but we omit these results for
clarity and the fact that the actual BLL performance is best
reflected in Acc1 scores (i.e., best translation only).

ity.6 In short, BilBOWA combines the adapted
SGNS for monolingual objectives together with a
cross-lingual objective that minimizes the L2-loss
between the bag-of-word vectors of parallel sen-
tences. BilBOWA uses the same training setup as
HYBWE (monolingual datasets plus a bilingual
signal), but relies on a stronger bilingual signal
(sentence alignments as opposed to HYBWE’s doc-
ument alignments).

We also compare with a benchmarking Type 1
model from sentence-aligned parallel data called
BiCVM (Hermann and Blunsom, 2014b). Finally,
a SGNS-based BWE model with the BNC+GT
seed lexicon is taken as a baseline Type 4 model
(Mikolov et al., 2013a).7

Training Data and Setup We use standard train-
ing data and suggested settings to obtain BWEs
for all models involved in comparison. We retain
the 100K most frequent words in each language
for all models. To induce monolingual WE spaces,
two monolingual SGNS models were trained on the
cleaned and tokenized Wikipedias from the Poly-
glot website (Al-Rfou et al., 2013) using SGD with
a global learning rate of 0.025. For BilBOWA,
as in the original work (Gouws et al., 2015), the
bilingual signal for the cross-lingual regularization
is provided by the first 500K sentences from Eu-
roparl.v7 (Tiedemann, 2012). We use SGD with
a global rate of 0.15.8 The window size is varied
from 2 to 16 in steps of 2, and the best scoring
model is always reported in all comparisons.

BWESG was trained on the cleaned and tok-
enized document-aligned Wikipedias available on-
line9, SGD on pseudo-bilingual documents with
a global rate 0.025. For BiCVM, we use the tool
released by its authors10 and train on the whole
Europarl.v7 for each language pair: we train an
additive model, with hinge loss margin set to d
(i.e., dimensionality) as in the original paper, batch
size of 50, and noise parameter of 10. All BiCVM
models are trained with 200 iterations.

For all models, we obtain BWEs with d =
40, 64, 300, 500, but we report only results with
300-dimensional BWEs as similar trends were ob-
served with other d-s. Other parameters are: 15
epochs, 15 negatives, subsampling rate 1e− 4.

6https://github.com/gouwsmeister/bilbowa
7For details concerning all baseline models, the reader is

encouraged to check the relevant literature.
8Suggested by the authors (personal correspondence).
9http://linguatools.org/tools/corpora/

10https://github.com/karlmoritz/bicvm

251



BNC+GT BNC+HYB+ASYM BNC+HYB+SYM HFQ+HYB+ASYM HFQ+HYB+SYM ORTHO

casamiento casamiento casamiento casamiento casamiento casamiento

marriage marry marriage marriage marriage maría
marry marriage marry marry marry señor

marrying marrying marrying betrothal betrothal doña
betrothal wed wedding marrying marrying juana
wedding wedding betrothal wedding wedding noche

wed betrothal wed daughter wed amor
elopement remarry marriages betrothed elopement guerra

Table 1: Nearest EN neighbours of the Spanish word casamiento (marriage) with different seed lexicons.

Model ES-EN NL-EN IT-EN

BICVM (TYPE 1) 0.532 0.583 0.569
BILBOWA (TYPE 2) 0.632 0.636 0.647
BWESG (TYPE 3) 0.676 0.626 0.643

BNC+GT (Type 4) 0.677 0.641 0.646

ORTHO 0.233 0.506 0.224
BNC+HYB+ASYM 0.673 0.626 0.644
BNC+HYB+SYM 0.681 0.658* 0.663*
(3388; 2738; 3145)
HFQ+HYB+ASYM 0.673 0.596 0.635
HFQ+HYB+SYM 0.695* 0.657* 0.667*

Table 2: Acc1 scores in a standard BLL setup
(for Type 4 models): all seed lexicons contain 5K
translation pairs, except for BNC+HYB+SYM (its
sizes provided in parentheses). * denotes a statisti-
cally significant improvement over baselines and
BNC+GT using McNemar’s statistical significance
test with the Bonferroni correction, p < 0.05.

4 Results and Discussion

Exp. I: Standard BLL Setting First, we replicate
the previous BLL setups with Type 4 models from
(Mikolov et al., 2013a; Dinu et al., 2015) by relying
on seed lexicons of exactly 5K word pairs (except
for BNC+HYB+SYM which exhausts all possible
pairs before the 5K limit) sorted by frequency of
the source language word. Results with different
lexicons for the three language pairs are summa-
rized in Table 2, while Table 1 shows examples of
nearest neighbour words for a Spanish word not
present in any of the training lexicons.

Table 1 provides evidence for our first insight:
Type 4 models do not necessarily require external
lexicons (such as the BNC+GT model) to learn a
semantically plausible SBWES (i.e., the lists of
nearest neighbours are similar for all lexicons ex-
cluding ORTHO). Table 1 also suggests that the
choice of seed lexicon pairs may strongly influence
the properties of the resulting SBWES. Due to its
design, ORTHO finds a mapping which naturally
brings foreign words appearing in the English vo-

cabulary closer in the induced SBWES.
This first batch of quantitative results already

shows that Type 4 models with inexpensive auto-
matically induced lexicons (i.e., HYBWE) are on a
par with or even better than Type 4 models relying
on external resources or translation systems. In ad-
dition, the best reported scores using the more con-
strained symmetric BNC/HFQ+HYB+SYM lexi-
con variants are higher than those for three baseline
models (of Type 1, Type 2, and Type 3) that pre-
viously held highest scores on the BLL test sets
(Vulić and Moens, 2016). These improvements
over the baseline models and BNC+GT are sta-
tistically significant (using McNemar’s statistical
significance test, p < 0.05). Table 2 also suggests
that a careful selection of reliable pairs can lead to
peak performances even with a lower number of
pairs, i.e., see the results of BNC+HYB+SYM.

Exp. II: Lexicon Size BLL results for ES-EN
and NL-EN obtained by varying the seed lexicon
sizes are displayed in Fig. 2(a) and 2(b). Results for
IT-EN closely follow the patterns observed with ES-
EN. BNC+HYB+SYM and HFQ+HYB+ASYM
– the two models that do not blindly use all po-
tential training pairs, but rely on sets of symmet-
ric pairs (i.e., they include the simple measure of
translation pair reliability) – display the best per-
formance across all lexicon sizes. The finding con-
firms the intuition that a more intelligent pair selec-
tion strategy is essential for Type 4 BWE models.
HFQ+HYB+SYM – a simple hybrid BWE model
(HYBWE) combining a document-level Type 3
model with a Type 4 model and translation reliabil-
ity detection – is the strongest BWE model overall
(see also Table 2 again).

HYBWE-based models which do not perform
any pair selection (i.e., BNC/HFQ+HYB+ASYM)
closely follow the behaviour of the GT-based
model. This demonstrates that an external lexi-
con or translation system may be safely replaced

252



0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.1k 0.2k 0.5k 1k 2k 5k 10k 20k 50k

A
cc

1
sc
or
es

Lexicon size

BNC+GT
BNC+HYB+ASYM
BNC+HYB+SYM
HFQ+HYB+ASYM
HFQ+HYB+SYM
ORTHO

(a) Spanish-English

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.1k 0.2k 0.5k 1k 2k 5k 10k 20k 50k

Lexicon size

BNC+GT
BNC+HYB+ASYM
BNC+HYB+SYM
HFQ+HYB+ASYM
HFQ+HYB+SYM
ORTHO

(b) Dutch-English

Figure 2: BLL results (Acc1) across different seed lexicon sizes for all lexicons. x axes are in log scale.

by a document-level embedding model without any
significant performance loss in the BLL task. The
ORTHO-based model falls short of its competitors.
However, we observe that even this model with the
learning setting relying on the cheapest bilingual
signal may lead to reasonable BLL scores, espe-
cially for the more related NL-EN pair.

The two models with the symmetry constraint
display a particularly strong performance with set-
tings relying on scarce resources (i.e., only a small
portion of training pairs is available). For instance,
HFQ+HYB+SYM scores 0.129 for ES-EN with
only 200 training pairs (vs 0.002 with BNC+GT),
and 0.529 with 500 pairs (vs 0.145 with BNC+GT).
On the other hand, adding more pairs does not
lead to an improved BLL performance. In fact,
we observe a slow and steady decrease in perfor-
mance with lexicons containing 10, 000 and more
training pairs for all HYBWE variants. The phe-
nomenon may be attributed to the fact that highly
frequent words receive more accurate representa-
tions in SBWES-1, and adding less frequent and,
consequently, less accurate training pairs to the
SBWES-2 learning process brings in additional
noise. In plain language, when it comes to seed lex-
icons Type 4 models prefer quality over quantity.

Exp. III: Translation Pair Reliability In the
next experiment, we vary the threshold value
THR (see sect. 2.2) in the HFQ+HYB+SYM
variant with the following values in comparison:
0.0 (None), 0.01, 0.025, 0.05, 0.075, 0.1. We in-
vestigate whether retaining only highly unambigu-
ous pairs would lead to even better BLL perfor-
mance. The results for all three language pairs
are summarized in Fig. 3(a)-3(c). The results for
all variant models again decrease when employ-

ing larger lexicons (due to the usage of less fre-
quent word pairs in training). We observe that a
slightly stricter selection criterion (i.e., THR =
0.01, 0.025) also leads to slightly improved peak
BLL scores for ES-EN and IT-EN around the 5K
region. The improvements, however, are not statis-
tically significant. On the other hand, a too conser-
vative pair selection criterion with higher threshold
values significantly deteriorates the overall perfor-
mance of HYBWE with HFQ+HYB+SYM. The
conservative criteria discard plenty of potentially
useful training pairs. Therefore, as one line of
future research, we plan to investigate more sophis-
ticated models for the selection of reliable seed
lexicon pairs that will lead to a better trade-off be-
tween the lexicon size and reliability of the pairs.

Exp. IV: Another Task - Suggesting Word
Translations in Context (SWTC) In the final
experiment, we test whether the findings originat-
ing from the BLL task generalize to another cross-
lingual semantic task: suggesting word translations
in context (SWTC) recently proposed by Vulić and
Moens (2014). Given an occurrence of a polyse-
mous word w ∈ V S , the SWTC task is to choose
the correct translation in the target language of
that particular occurrence of w from the given set
T C(w) = {t1, . . . , ttq}, T C(w) ⊆ V T , of its tq
possible translations/meanings. Whereas in the
BLL task the candidate search is performed over
the entire vocabulary V T , the set TC(w) typically
comprises only a few pre-selected words/senses.
One may refer to T C(w) as an inventory of transla-
tion candidates for w. The best scoring translation
candidate in the ranked list is then the correct trans-
lation for that particular occurrence of w observing
its local context Con(w). SWTC is an extended

253



0.6

0.62

0.64

0.66

0.68

0.7

1k 2k 4k 5k 10k 20k 40k

A
cc

1
sc
or
es

Lexicon size

THR=None
THR=0.01
THR=0.025
THR=0.05
THR=0.075
THR=0.1

(a) Spanish-English

0.54

0.56

0.58

0.6

0.62

0.64

0.66

1k 2k 4k 5k 10k 20k 40k

Lexicon size

THR=None
THR=0.01
THR=0.025
THR=0.05
THR=0.075
THR=0.1

(b) Dutch-English

0.56

0.58

0.6

0.62

0.64

0.66

0.68

1k 2k 4k 5k 10k 20k 40k

Lexicon size

THR=None
THR=0.01
THR=0.025
THR=0.05
THR=0.075
THR=0.1

(c) Italian-English

Figure 3: BLL results across different threshold (THR) values with the HFQ+HYB+SYM seed lexicons.
Higher thresholds imply less ambiguous word translation pairs. Thicker horizontal lines denote the best
score from any of the baseline models. x axes are in log scale.

Model ES-EN NL-EN IT-EN

NO CONTEXT 0.406 0.433 0.408
BEST SYSTEM 0.703 0.712 0.789
(Vulić and Moens, 2014)

BICVM (TYPE 1) 0.506 0.586 0.522
BILBOWA (TYPE 2) 0.586 0.656 0.589
BWESG (TYPE 3) 0.783 0.858 0.792

BNC+GT (TYPE 4) 0.794 0.858 0.783

ORTHO 0.647 0.794 0.678
BNC+HYB+ASYM 0.806* 0.872 0.778
BNC+HYB+SYM 0.808* 0.875* 0.814*
(3839; 3117; 3693)
HFQ+HYB+ASYM 0.789 0.864 0.781
HFQ+HYB+SYM (THR = None) 0.792 0.869 0.786
HFQ+HYB+SYM (THR=0.01) 0.792 0.858 0.789
HFQ+HYB+SYM (THR=0.025) 0.800 0.853 0.792

Table 3: Acc1 scores in the SWTC task. All seed
lexicons contain 6K translation pairs, except for
BNC+HYB+SYM (its sizes provided in parenthe-
ses). * denotes a statistically significant improve-
ment over baselines and BNC+GT using McNe-
mar’s statistical significance test with the Bonfer-
roni correction, p < 0.05.

cross-lingual variant of the task proposed by Huang
et al. (2012) which evaluates monolingual context-
sensitive semantic similarity of words in sentential
context, and it is also very related to cross-lingual
lexical substitution (Mihalcea et al., 2010).

To isolate the performance of each BWE induc-
tion model from the details of the SWTC setup,
we use the same approach with all models: we
opt for the SWTC framework proven to yield
excellent results with BWEs in the SWTC task
(Vulić and Moens, 2016). In short, the context bag
Con(w) = {cw1, . . . , cwr} is obtained by harvest-
ing all r words that occur with w in the sentence.

The vector representation of Con(w) is the d-
dimensional embedding computed by aggregating
over all word embeddings for each cwj ∈ Con(w)
using standard addition as the compositional opera-
tor (Mitchell and Lapata, 2008) which was proven
a robust choice (Milajevs et al., 2014):

Con(w) = cw1 + cw2 + . . .+ cwr (4)

where cwj is the embedding of the j-th con-
text word, and Con(w) is the resulting embed-
ding of the context bag Con(w). Finally, for
each tj ∈ T C(w), the context-sensitive similar-
ity with w is computed as: sim(w, tj , Con(w)) =
cos(Con(w), tj), where Con(w) and tj are rep-
resentations of the (sentential) context bag and the
candidate translation tj in the same SBWES.11

The evaluation set consists of 360 sentences for
15 polysemous nouns (24 sentences for each noun)
in each of the three languages: Spanish, Dutch, Ital-
ian, along with the single gold standard single word
English translation given the sentential context.12

Table 3 summarizes the results (Acc1 scores) in the
SWTC task. NO-CONTEXT refers to the context-
insensitive majority baseline obtained by BNC+GT
(i.e., it always chooses the most semantically sim-
ilar translation candidate at the word type level).
We also report the results of the best SWTC model
from Vulić and Moens (2014).

The results largely support the claims estab-
lished with the BLL evaluation. An exter-

11The same ranking of different models (with lower abso-
lute scores) is observed when adapting the monolingual lexical
substitution framework of Melamud et al. (2015) to the SWTC
task as done by Vulić and Moens (2016).

12The SWTC evaluation set is available online at:
http://aclweb.org/anthology/attachments/D/D14/D14-
1040.Attachment.zip

254



nal seed lexicon of BNC+GT may be safely
replaced by an automatically induced inex-
pensive seed lexicon (as in HYBWE with
BNC+HYB+SYM/ASYM). The best perform-
ing models are again BNC+HYB+SYM and
HFQ+HYB+SYM. The comparison of ASYM and
SYM lexicon variants further suggests that filter-
ing translation pairs using the symmetry constraint
again leads to consistent improvements, but stricter
selection criteria with higher thresholds do not lead
to significant performance boosts, and may even
hurt the performance (see the results for NL-EN).
Various HYBWE variants significantly improve
over baseline BWE models (Types 1-4), also out-
performing previous best SWTC results.

5 Conclusions and Future Work

We presented a detailed analysis of the importance
and properties of seed bilingual lexicons in learn-
ing bilingual word embeddings (BWEs) which are
valuable for many cross-lingual/multilingual NLP
tasks. On the basis of the analysis, we proposed a
simple yet effective hybrid bilingual word embed-
ding model called HYBWE. It learns the mapping
between two monolingual embedding spaces us-
ing only highly reliable symmetric translation pairs
from an inexpensive seed document-level embed-
ding space. The results in the tasks of (1) bilingual
lexicon learning and (2) suggesting word transla-
tions in context demonstrate that – due to its careful
selection of reliable translation pairs for seed lexi-
cons – HYBWE outperforms benchmarking BWE
induction models, all of which use more expensive
bilingual signals for training.

In future work, we plan to investigate other meth-
ods for seed pairs selection, settings with scarce
resources (Agić et al., 2015; Zhang et al., 2016),
other context types inspired by recent work in the
monolingual settings (Levy and Goldberg, 2014a;
Melamud et al., 2016), as well as model adapta-
tions that can work with multi-word expressions.
Encouraged by the excellent results, we also plan
to test the portability of the approach to more lan-
guage pairs, and other tasks and applications.

Acknowledgments

This work is supported by ERC Consolidator Grant
LEXICAL: Lexical Acquisition Across Languages
(no 648909). The authors are grateful to Roi Re-
ichart and the anonymous reviewers for their help-
ful comments and suggestions.

References
Željko Agić, Dirk Hovy, and Anders Søgaard. 2015.

If all you have is a bit of the Bible: Learning POS
taggers for truly low-resource languages. In ACL,
pages 268–272.

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In CoNLL, pages 183–192.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A. Smith.
2016. Massively multilingual word embeddings.
CoRR, abs/1602.01925.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.

Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual similar-
ity of labeled web images. In IJCAI, pages 1764–
1769.

Sarath A.P. Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh M. Khapra, Balaraman Ravindran, Vikas C.
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In NIPS, pages 1853–1861.

Danqi Chen and Christopher D. Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wen-
zek, and Amine Benhalloum. 2015. Trans-gram,
fast cross-lingual word embeddings. In EMNLP,
pages 1109–1113.

Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2015. Improving zero-shot learning by miti-
gating the hubness problem. In ICLR Workshop Pa-
pers.

Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In EACL, pages 462–471.

Éric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Hervé Déjean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526–533.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast bilingual distributed repre-
sentations without word alignments. In ICML, pages
748–756.

255



Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In ACL, pages 1234–1244.

Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual distributed representations without word
alignment. In ICLR.

Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual models for compositional distributed se-
mantics. In ACL, pages 58–68.

Jon Holmlund, Magnus Sahlgren, and Jussi Karlgren.
2005. Creating bilingual lexica using reference
wordlists for alignment of monolingual semantic
vector spaces. In NODALIDA, pages 71–77.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In ACL, pages 873–882.

Anders Johannsen, Héctor Martínez Alonso, and An-
ders Søgaard. 2015. Any-language frame-semantic
parsing. In EMNLP, pages 2062–2066.

Adam Kilgarriff. 1997. Putting frequencies in the
dictionary. International Journal of Lexicography,
10(2):135–155.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. 2015. Skip-thought vectors.
In NIPS.

Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012. Inducing crosslingual distributed representa-
tions of words. In COLING, pages 1459–1474.

Tomáš Kočiský, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representations
by marginalizing alignments. In ACL, pages 224–
229.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In ACL,
pages 270–280.

Ira Leviant and Roi Reichart. 2015. Judgment lan-
guage matters: Multilingual vector space models for
judgment language aware lexical semantics. CoRR,
abs/1508.00106.

Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In ACL, pages 302–308.

Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In NIPS,
pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the ACL,
3:211–225.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Bilingual word representations with
monolingual quality in mind. In Proceedings of the
1st Workshop on Vector Space Modeling for Natural
Language Processing, pages 151–159.

Oren Melamud, Omer Levy, and Ido Dagan. 2015. A
simple word embedding model for lexical substitu-
tion. In Proceedings of the 1st Workshop on Vector
Space Modeling for Natural Language Processing,
pages 1–7.

Oren Melamud, David McClosky, Siddharth Patward-
han, and Mohit Bansal. 2016. The role of context
types and dimensionality in learning word embed-
dings. In NAACL-HLT.

Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. SemEval-2010 task 2: Cross-lingual lexical
substitution. In SEMEVAL, pages 9–14.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013a.
Exploiting similarities among languages for ma-
chine translation. CoRR, abs/1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111–3119.

Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh
Sadrzadeh, and Matthew Purver. 2014. Evaluating
neural word representations in tensor-based compo-
sitional settings. In EMNLP, pages 708–719.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL, pages
236–244.

Bhaskar Mitra, Eric T. Nalisnick, Nick Craswell,
and Rich Caruana. 2016. A dual embed-
ding space model for document ranking. CoRR,
abs/1602.01137.

Yves Peirsman and Sebastian Padó. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In NAACL, pages 921–929.

Tianze Shi, Zhiyuan Liu, Yang Liu, and Maosong Sun.
2015. Learning cross-lingual word embeddings via
matrix co-factorization. In ACL, pages 567–572.

David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic priors.
In EMNLP-CoNLL, pages 667–677.

Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa.
2015. Leveraging monolingual data for crosslingual
compositional word representations. In ICLR.

Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In LREC, pages 2214–2218.

Chen-Tse Tsai and Dan Roth. 2016. Cross-lingual
wikification using multilingual embeddings. In
NAACL-HLT.

256



Kewei Tu and Vasant Honavar. 2012. Unambiguity
regularization for unsupervised learning of proba-
bilistic grammars. In EMNLP-CoNLL, pages 1324–
1334.

Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word representations: A simple and gen-
eral method for semi-supervised learning. In ACL,
pages 384–394.

Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: vector space models of se-
mantics. Journal of Artifical Intelligence Research,
37(1):141–188.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word em-
beddings: An empirical comparison. In ACL.

Ivan Vulić and Marie-Francine Moens. 2013. A study
on bootstrapping bilingual vector spaces from non-
parallel data (and nothing else). In EMNLP, pages
1613–1624.

Ivan Vulić and Marie-Francine Moens. 2014. Proba-
bilistic models of cross-lingual semantic similarity
in context based on latent cross-lingual concepts in-
duced from comparable data. In EMNLP, pages
349–362.

Ivan Vulić and Marie-Francine Moens. 2015. Mono-
lingual and cross-lingual information retrieval mod-
els based on (bilingual) word embeddings. In SIGIR,
pages 363–372.

Ivan Vulić and Marie-Francine Moens. 2016.
Bilingual distributed word representations from
document-aligned comparable data. Journal of Ar-
tificial Intelligence Research, 55:953–994.

Ivan Vulić, Douwe Kiela, Stephen Clark, and Marie-
Francine Moens. 2016. Multi-modal representa-
tions for improved bilingual lexicon learning. In
ACL.

Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. WSABIE: scaling up to large vocabulary im-
age annotation. In IJCAI, pages 2764–2770.

Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi Jaakkola. 2016. Ten pairs to tag - Multilin-
gual POS tagging via coarse mapping between em-
beddings. In NAACL-HLT.

Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP, pages 1393–1398.

257


