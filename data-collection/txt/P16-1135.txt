



















































Identifying Causal Relations Using Parallel Wikipedia Articles


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1424–1433,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Identifying Causal Relations Using Parallel Wikipedia Articles

Christopher Hidey
Department of Computer Science

Columbia University
New York, NY 10027

chidey@cs.columbia.edu

Kathleen McKeown
Department of Computer Science

Columbia University
New York, NY 10027

kathy@cs.columbia.edu

Abstract

The automatic detection of causal relation-
ships in text is important for natural lan-
guage understanding. This task has proven
to be difficult, however, due to the need for
world knowledge and inference. We fo-
cus on a sub-task of this problem where
an open class set of linguistic markers
can provide clues towards understanding
causality. Unlike the explicit markers, a
closed class, these markers vary signifi-
cantly in their linguistic forms. We lever-
age parallel Wikipedia corpora to identify
new markers that are variations on known
causal phrases, creating a training set via
distant supervision. We also train a causal
classifier using features from the open
class markers and semantic features pro-
viding contextual information. The results
show that our features provide an 11.05
point absolute increase over the baseline
on the task of identifying causality in text.

1 Introduction

The automatic detection of causal relationships in
text is an important but difficult problem. The
identification of causality is useful for the under-
standing and description of events. Causal in-
ference may also aid upstream applications such
as question answering and text summarization.
Knowledge of causal relationships can improve
performance in question answering for “why”
questions. Summarization of event descriptions
can be improved by selecting causally motivated
sentences. However, causality is frequently ex-
pressed implicitly, which requires world knowl-
edge and inference. Even when causality is ex-
plicit, there is a wide variety in how it is expressed.

Causality is one type of relation in the Penn Dis-
course Tree Bank (PDTB) (Prasad et al, 2008).
In general, discourse relations indicate how two
text spans are logically connected. In PDTB the-
ory, these discourse relations can be marked ex-
plicitly or conveyed implicitly. In the PDTB, there
are 102 known explicit discourse markers such as
“and”, “but”, “after”, “in contrast”, or “in addi-
tion”. Of these, 28 explicitly mark causal relations
(e.g., “because”, “as a result”, “consequently”).

In addition to explicit markers, PDTB re-
searchers recognize the existence of an open class
of markers, which they call AltLex. There is a
tremendous amount of variation in how AltLexes
are expressed and so the set of AltLexes is ar-
guably infinite in size. In the PDTB, non-causal
AltLexes include “That compares with” and “In
any event.” Causal AltLexes include “This may
help explain why” and “This activity produced.”

Discourse relations with explicit discourse
markers can be identified with high precision
(Pitler and Nenkova, 2009) but they are also rela-
tively rare. Implicit relations are much more com-
mon but very difficult to identify. AltLexes fall
in the middle; their linguistic variety makes them
difficult to identify but their presence improves the
identification of causality.

One issue with causality identification is the
lack of data. Unsupervised identification on
open domain data yields low precision (Do et al,
2011) and while supervised methods on the PDTB
have improved (Ji and Eisenstein, 2015), creating
enough labeled data is difficult. Here, we present
a distant supervision method for causality identifi-
cation that uses parallel data to identify new causal
connectives given a seed set. We train a classi-
fier on this data and self-train to obtain new data.
Our novel approach uses AltLexes that were auto-
matically identified using semi-supervised learn-
ing over a parallel corpus. Since we do not know

1424



a priori what these phrases are, we used a mono-
lingual parallel corpus to identify new phrases that
are aligned with known causal connectives. As
large corpora of this type are rare, we used Sim-
ple and English Wikipedia to create one.

Section 2 discusses prior research in causality
and discourse. Section 4 describes how we created
a new corpus from Wikipedia for causality and ex-
tracted a subset of relations with AltLexes. In sec-
tion 5, we recount the semantic and marker fea-
tures and how they were incorporated into a classi-
fier for causality. We show that these features im-
prove causal inference by an 11.05 point increase
in F-measure over a naive baseline in 6. Finally,
we discuss the results and future work in 7.

2 Related Work

Recent work on causality involved a combination
of supervised discourse classification with unsu-
pervised metrics such as PMI (Do et al, 2011).
They used a minimally supervised approach us-
ing integer linear programming to infer causality.
Other work focused on specific causal construc-
tions events paired by verb/verb and verb/noun
(Riaz and Girju, 2013) (Riaz and Girju, 2014).
Their work considered semantic properties of
nouns and verbs as well as text-only features.

There has also been significant research into
discourse semantics over the past few years. One
theory of discourse structure is represented in the
PDTB (Prasad et al, 2008). The PDTB repre-
sents discourse relationships as connectives be-
tween two arguments. Early work with the PDTB
(Pitler and Nenkova, 2009) showed that discourse
classes with explicit discourse connectives can
be identified with high accuracy using a combi-
nation of the connective and syntactic features.
Further work (Pitler et al, 2009) resulted in the
identification of implicit discourse relations using
word pair features; this approach extended ear-
lier work using word pairs to identify rhetorical
relations (Marcu, 2001) (Blair-Goldensohn et al,
2007). These word pairs were created from text
by taking the cross product of words from the Gi-
gaword corpus for explicit causal and contrast re-
lations. Others built on this work by aggregat-
ing word pairs for every explicit discourse con-
nective (Biran and McKeown, 2013). They then
used the cosine similarity between a prospective
relation and these word pairs as a feature. Re-
cently, the first end-to-end discourse parser was

completed (Lin et al, 2012). This parser jointly
infers both argument spans and relations. The cur-
rent state-of-the-art discourse relation classifier is
a constituent parse recursive neural network with
coreference (Ji and Eisenstein, 2015).

Our work is similar to previous work to identify
discourse connectives using unsupervised meth-
ods (Laali, 2014). In their research, they used the
EuroParl parallel corpus to find discourse connec-
tives in French using known English connectives
and filtering connectives using patterns. Unlike
this effort, we created our own parallel corpus and
we determined new English connectives.

Compared to previous work on causality, we fo-
cus specifically on causality and the AltLex. The
work by Do and Riaz used minimally supervised
(Do et al, 2011) or unsupervised (Riaz and Girju,
2013) approaches and a slightly different defini-
tion of causality, similar to co-ocurrence. The
work of Riaz and Girju (2013) is most similar to
our own. We also examine causality as expressed
by the author of the text. However, they focus on
intra-sentence constructions between noun or verb
phrases directly whereas we attempt to examine
how the AltLex connectives express causality in
context. Lastly, Riaz and Girju used FrameNet and
WordNet to identify training instances for causal
verb-verb and verb-noun pairs (Riaz and Girju,
2014) whereas we use them as features for an an-
notated training set. Overall our contributions are
a new dataset created using a distant supervision
approach and new features for causality identifi-
cation. One major advantage is that our method
requires very little prior knowledge about the data
and requires only a small seed set of known con-
nectives.

3 Linguistic Background

One disadvantage of the PDTB is that the marked
AltLexes are limited only to discourse relations
across sentences. We know that there are addi-
tional phrases that indicate causality within sen-
tences but these phrases are neither found in the
set of Explicit connectives nor AltLexes. Thus we
expand our definition of AltLex to include these
markers when they occur within a sentence. Al-
though some phrases or words could be identified
by consulting a thesaurus or the Penn Paraphrase
Database (Ganitkevitch et al, 2013), we still need
the context of the phrase to identify causality.

We hypothesize that there is significant linguis-

1425



tic variety in causal AltLexes. In the set of known
explicit connectives there are adjectives (“subse-
quent”), adverbs (“consequently”), and preposi-
tions and prepositional phrases (“as a result”). We
consider that these parts of speech and syntactic
classes can be found in AltLexes as well. In addi-
tion, verbs and nouns often indicate causality but
are not considered explicit connectives.

Some obvious cases of AltLexes are the verbal
forms of connectives such as “cause” and “result”.
In addition to these verbs, there exist other verbs
that can occur in causal contexts but are ambigu-
ous. Consider that “make” and “force” can replace
“cause” in this context:

The explosion made people evacuate
the building.
The explosion forced people to evacuate
the building.
The explosion caused people to evacu-
ate the building.

However, the words can not be substituted in the
following sentence:

The baker made a cake.
*The baker caused a cake.
*The baker forced a cake.

Furthermore, verbs such as “given” may replace
additional causal markers:

It’s not surprising he is tired since he did
not get any sleep.
It’s not surprising he is tired given that
he did not get any sleep.

There are also some phrases with the same
structure as partial prepositional phrases like “as
a result” or “as a result of”, where the pattern
is preposition and noun phrase followed by an
optional preposition. Some examples of these
phrases include “on the basis of,” “with the goal
of,” and “with the idea of.”

We may also see phrases that are only causal
when ending in a preposition such as “thanks to”
or “owing to.” “Lead” may only be causal as a
part of “lead to” and the same for “develop” ver-
sus “develop from.” In addition, prepositions can
affect the direction of the causality. Comparing
“resulting in” versus “resulting from”, the prepo-
sition determines that the latter is of the “reason”
class and the former is of the “result” class.

Ultimately, we want to be able to detect these
phrases automatically and determine whether they
are a large/small and open/closed class of markers.

4 Data

In order to discover new causal connectives, we
can leverage existing information about known
causal connectives. It should be the case that
if a phrase is a causal AltLex, it will occur in
some context as a replacement for at least one
known explicit connective. Thus, given a large
dataset, we would expect to find some pairs of
sentences where the words are very similar ex-
cept for the connective. This approach requires
a parallel corpus to identify new AltLexes. As
large English paraphrase corpora are rare, we draw
from previous work identifying paraphrase pairs in
Wikipedia (Hwang et al, 2015).

The dataset we used was created from the En-
glish and Simple Wikipedias from September 11,
2015. We used the software WikiExtractor to con-
vert the XML into plain text. All articles with
the same title were paired and any extra arti-
cles were ignored. Each article was lemmatized,
parsed (both constituent and dependency), and
named-entity tagged using the Stanford CoreNLP
suite (Manning et al, 2014). We wish to identify
paraphrase pairs where one element is in English
Wikipedia and one is in Simple Wikipedia. Fur-
thermore, we do not limit these elements to be sin-
gle sentences because an AltLex can occur within
a sentence or across sentences.

Previous work (Hwang et al, 2015) created a
score for similarity (WikNet) between English
Wikipedia and Simple Wikipedia. Many similarity
scores are of the following form comparing sen-
tences W and W ′:

s(W,W ′) =
1
Z

∑
w∈W

max
w′∈W ′

σ(w,w′)idf(w) (1)

where σ(w,w′) is a score1 between 2 words and
Z is a normalizer ensuring the score is between 0
and 1. For their work, they created a score where
σ(w,w′) = σwk(w,w′) + σwk(h, h′)σr(r, r′).
σwk is a distance function derived from Wik-
tionary by creating a graph based on words appear-
ing in a definition. h and h′ are the governors of
w and w′ in a dependency parse and r and r′ are
the relation. Similar sentences should have similar
structure and the governors of two words in differ-
ent sentences should also be similar. σr is 0.5 if h
and h′ have the same relation and 0 otherwise.

For this work, we also include partial matches,
as we only need the connective and the immediate

1The score is not a metric, as it is not symmetric.

1426



Method Max F1
WikNet 0.4850
WikNet, λ = 0.75 0.5981
Doc2Vec 0.6226
Combined 0.6263

Table 1: Paraphrase Results

surrounding context on both sides. If one sentence
contains an additional clause, it does not affect
whether it contains a connective. Thus, one dis-
advantage to this score is that when determining
whether a sentence is a partial match to a longer
sentence or a shorter sentence, the longer sentence
will often be higher as there is no penalty for un-
matched words between the two elements. We
experimented with penalizing content words that
do not match any element in the other sentence.
The modified score, where W and W ′ are nouns,
verbs, adjectives, or adverbs, is then:

s(W,W ′) =
1
Z

∑
w∈W

max
w′∈W ′

σ(w,w′)idf(w)

−λ(|W ′ −W |+ |W −W ′|)
(2)

We also compared results with a model trained
using doc2vec (Le and Mikolov, 2014) on each
sentence and sentence pair and identifying para-
phrases with their cosine similarity.

As these methods are unsupervised, only a
small amount of annotated data is needed to tune
the similarity thresholds. Two graduate com-
puter science students annotated a total of 45 Sim-
ple/English article pairs. There are 3,891 total sen-
tences in the English articles and 794 total sen-
tences in the Simple Wikipedia articles. Inter-
annotator agreement (IAA) was 0.9626, computed
on five of the article pairs using Cohen’s Kappa.
We tune the threshold for each possible score: for
doc2vec the cosine similarity and for WikNet the
scoring function. We also tune the lambda penalty
for WikNet. F1 scores were calculated via grid
search over these parameters and the best settings
are a combined score using doc2vec and penalized
WikNet with λ = 0.75 where a pair is considered
to be a paraphrase if either threshold is greater than
0.69 or 0.65 respectively.

Using the combined score we obtain 187,590
paraphrase pairs. After combining and deduping
this dataset with the publicly available dataset re-
leased by (Hwang et al, 2015), we obtain 265,627
pairs, about 6 times as large as the PDTB.

In order to use this dataset for training a model
to distinguish between causal and non-causal in-

Class Type Subtype
Temporal
Contingency Cause reason

result
Pragmatic cause
Condition
Pragmatic condition

Comparison
Expansion

Table 2: PDTB Discourse Classes

stances, we use the paired data to identify pairs
where an explicit connective appears in at least
one element of the pair. The explicit connective
can appear in a Simple Wikipedia sentence or an
English Wikipedia sentence. We then use patterns
to find new phrases that align with these connec-
tives in the matching sentence.

To identify a set of seed words that unambigu-
ously identify causal and non-causal phrases we
examine the PDTB. As seen in Table 2, causal re-
lations fall under the Contingency class and Cause
type. We consider connectives from the PDTB that
either only or never appear as that type. The con-
nective “because” is the only connective to be al-
most always a “reason” connective, whereas there
are 11 unambiguous connectives for “result”, in-
cluding “accordingly”, “as a consequence”, “as a
result”, and “thus”. There were many markers
that were unambiguously not causal (e.g. “but”,
“though”, “still”, “in addition”).

In order to label paraphrase data, we use con-
straints to identify possible AltLexes.2 We used
Moses (Koehn et al, 2007) to train an alignment
model on the created paraphrase dataset. Then for
every paraphrase pair we identify any connectives
that match with any potential AltLexes. Based on
our linguistic analysis, we require these phrases to
contain at least one content word, which we iden-
tify based on part of speech. We also draw on pre-
vious work (Pitler and Nenkova, 2009) that used
the left and right sibling of a phrase. Therefore,
we use the following rules to label new AltLexes:

1. Must be less than 7 words.
2. Must contain at least one content word:

(a) A non-proper noun
(b) A non-modal and non-auxiliary verb
(c) An adjective or adverb

3. Left sibling of the connective must be a noun
phrase, verb phrase, or sentence.

4. Right sibling of the connective must be a
noun phrase, verb phrase, or sentence.

2We do not attempt to label arguments at this point.

1427



5. May not contain a modal or auxilary verb.
Because connectives identify causality between

events or agents, we require that each potential
connective link 2 events/agents. We define an
event or agent as a noun, verb, or an entire sen-
tence. This means that we require the left sib-
ling of the first word in a phrase and the right sib-
ling of the last word in a phrase to be an event,
where a sibling is the node at the same level in
the constituent parse. We also require the left and
right sibling rule for the explicit connectives, but
we allow additional non-content words (for exam-
ple, we would mark “because of” as a connective
rather than “because.” We then mark the AltLex
as causal or not causal.

Given that the paraphrases and word alignments
are noisy, we use the syntactic rules to decrease the
amount of noise in the data by more precisely de-
termining phrase boundaries. These rules are the
same features used by Pitler and Nenkova (2009)
for the early work on the PDTB on explicit con-
nectives. These features were successful on the
Wall Street Journal and they are applicable for
other corpora as well. Also, they are highly in-
dicative of discourse/non-discourse usage so we
believe that we are improving on noisy align-
ments without losing valuable data. In the future,
however, we would certainly like to move away
from encoding these constraints using a rule-based
method and use a machine learning approach to
automatically induce rules.

This method yields 72,135 non-causal and
9,190 causal training examples. Although these
examples are noisy, the dataset is larger than the
PDTB and was derived automatically. There are
35,136 argument pairs in the PDTB marked with
one of the 3 relations that implies a discourse con-
nective (Implicit, Explicit, and AltLex), and of
these 6,289 are causal. Of the 6,289 causal pairs,
2,099 are explicit and 273 contain an AltLex.

5 Methods

Given training data labeled by this distant supervi-
sion technique, we can now treat this problem as a
supervised learning problem and create a classifier
to identify causality.

We consider two classes of features: features
derived from the parallel corpus data and lexical
semantic features. The parallel corpus features
are created based on where AltLexes are used as
paraphrases for causal indicators and in what con-

text. The lexical semantic features use FrameNet,
WordNet, and VerbNet to derive features from all
the text in the sentence pair. These lexical re-
sources exploit different perspectives on the data
in complementary ways.

The parallel corpus features encourage the clas-
sifier to select examples with AltLexes that are
likely to be causal whereas the lexical semantic
features allow the classifier to consider context for
disambiguation. In addition to the dataset, the par-
allel corpus and lexical semantic features are the
main contributions of this effort.

5.1 Parallel Corpus Features
We create a subclass of features from the parallel
corpus: a KL-divergence score to encourage the
identification of phrases that replace causal con-
nectives. Consider the following datapoints and
assume that they are aligned in the parallel corpus:

I was late because of traffic.
I was late due to traffic.

We want both of these examples to have a high
score for causality because they are interchange-
able causal phrases. Similarly, we want non-causal
phrases that are often aligned to have a high score
for non-causality.

We define several distributions in order to de-
termine whether an AltLex is likely to replace a
known causal or non-causal connective. We con-
sider all aligned phrases, not just ones contain-
ing a causal or non-causal connective to attempt
to reduce noisy matches. The idea is that non-
connective paraphrases will occur often and in
other contexts.

The following conditional Bernoulli distribu-
tions are calculated for every aligned phrase in the
dataset, where w is the phrase, s is the sentence it
occurs in, c is “causal” and nc is “not causal”:

p1 = p(w1 ∈ s1|rel(s1) ∈ {c}, w1 /∈ s2) (3)
p2 = p(w1 ∈ s1|rel(s1) ∈ {nc}, w1 /∈ s2) (4)

We compare these two distributions to other dis-
tributions with the same word and in a different
context (where o represents “other”):

q1 = p(w1 ∈ s1|rel(s1) ∈ {nc, o}, w1 /∈ s2) (5)
q2 = p(w1 ∈ s1|rel(s1) ∈ {c, o}, w1 /∈ s2) (6)

We then calculate DKL(p1||q1) and
DKL(p2||q2). In order to use KL-divergence
as a feature, we multiply the score by (−1)p<q
and add a feature for causal and one for non-
causal.

1428



5.2 Lexical Semantic Features

As events are composed of predicates and ar-
guments and these are usually formed by nouns
and verbs, we consider using lexical semantic re-
sources that have defined hierarchies for nouns
and verbs. We thus use the lexical resources
FrameNet, WordNet, and VerbNet as complemen-
tary resources from which to derive features. We
hypothesize that these semantic features provide
context not present in the text; from these we are
able to infer causal and anti-causal properties.

FrameNet is a resource for frame semantics,
defining how objects and relations interact, and
provides an annotated corpus of English sen-
tences. WordNet provides a hierarchy of word
senses and we show that the top-level class of
verbs is useful for indicating causality. VerbNet
provides a more fine-grained approach to verb cat-
egorization that complements the views provided
by FrameNet and WordNet.

In FrameNet, a semantic frame is a concep-
tual construction describing events or relations
and their participants (Ruppenhofer et al, 2010).
Frame semantics abstracts away from specific ut-
terances and ordering of words in order to repre-
sent events at a higher level. There are over 1,200
semantic frames in FrameNet and some of these
can be used as evidence or counter-evidence for
causality (Riaz and Girju, 2013). In Riaz’s work,
they identified 18 frames as causal (e.g. “Pur-
pose”, “Internal cause”, “Reason”, “Trigger”).

We use these same frames to create a lexical
score based on the FrameNet 1.5 corpus. This
corpus contains 170,000 sentences manually an-
notated with frames. We used a part-of-speech
tagged version of the FrameNet corpus and for
each word and tag, we count how often it occurs
in the span of one of the given frames. We only
considered nouns, verbs, adjectives, and adverbs.
We then calculate pw(c|t) and cwct, the probability
that a wordw is causal given its tag t and its count,
respectively. The lexical score of a word i is calcu-
lated by using the assigned part-of-speech tag and
is given by CSi = pwi(c|ti) log cwicti . The total
score of a sequence of words is then

∑n
i=0CSi.

We also took this further and determined what
frames are likely to be anti-causal. We started
with a small set of seed words derived directly
from 11 discourse classes (types and subtypes
from Table 2), such as “Compare”, “Contrast”,
“Explain”, “Concede”, and “List”. We expanded

this list using WordNet synonyms for the seed
words. We then extracted every frame associated
with their stems in the stemmed FrameNet corpus.
These derived frames were manually examined to
develop a list of 48 anti-causal frames, including
“Statement”, “Occasion”, “Relative time”, “Evi-
dence”, and “Explaining the facts”.

We create an anti-causal score using the
FrameNet corpus just as we did for the causal
score. The total anti-causal score of a se-
quence of words is

∑n
i=0ACSi where ACSi =

pwi(a|ti) log cwiati for anti-causal probabilities
and counts. We split each example into three parts:
the text before the AltLex, the AltLex, and the text
after. Each section is given a causal score and an
anti-causal score. Overall, there are six features
derived using FrameNet: causal score and anti-
causal score for each part of the example.

In WordNet, words are grouped into “synsets,”
which represent all synonyms of a particular word
sense. Each word sense in the WordNet hierarchy
has a top-level category based on part of speech
(Miller, 1995). Every word sense tagged as noun,
verb, adjective, or adverb is categorized. Some
examples of categories are “change”, “stative”,
or “communication”. We only include the top
level because of the polysemous nature of Word-
Net synsets. We theorize that words having to do
with change or state should be causal indicators
and words for communication or emotion may be
anti-causal indicators.

Similar to the FrameNet features, we split the
example into three sections. However, we also
consider the dependency parse of the data. We be-
lieve that causal relations are between events and
agents which are represented by nouns and verbs.
Events can also be represented by predicates and
their arguments, which is captured by the depen-
dency parse. As the root of a dependency parse is
often a verb and sometimes a noun or adjective, we
consider the category of the root of a dependency
parse and its arguments.

We include a categorical feature indicating the
top-level category of the root of each of the three
sections, including the AltLex. For both sides of
the AltLex, we include the top-level category of
all arguments as well. If a noun has no category,
we mark it using its named-entity tag. If there is
still no tag, we mark the category as “none.”

VerbNet VerbNet is a resource devoted to stor-
ing information for verbs (Kipper et al, 2000).

1429



In contrast to WordNet, VerbNet provides a more
fine-grained description of events while focusing
less on polysemy. Some examples of VerbNet
classes are “force”, “indicate”, and “wish”. In
VerbNet, there are 273 verb classes, and we in-
clude their presence as a categorical feature. Sim-
ilar to WordNet, we use VerbNet categories for
three sections of the sentence: the text pre-AltLex,
the AltLex, and the text post-AltLex. Unlike
WordNet, we only mark the verbs in the AltLex,
root, or arguments.

Interaction Finally, we consider interactions
between the WordNet and VerbNet features. As
previous work (Marcu, 2001) (Biran and McKe-
own, 2013) used word pairs successfully, we hy-
pothesize that pairs of higher-level categories will
improve classification without being penalized as
heavily by the sparsity of dealing with individual
words. Thus we include interaction features be-
tween every categorical feature for the pre-AltLex
text and every feature for the post-AltLex text.

In all, we include the following features (L
refers to the AltLex, B refers to the text before the
AltLex and A refers to the text after the AltLex):

1. FrameNet causal score for L, B, and A.
2. FrameNet anti-causal score for L, B, and A.
3. WordNet top-level of L.
4. WordNet top-level of the root of B and A.
5. WordNet top-level for arguments of B and A.
6. VerbNet category for verb at the root of L.
7. VerbNet top-level category for any verb in the

root of B and A.
8. VerbNet top-level category for any verbs in

the arguments of B and A.
9. Categorical interaction features between the

features from B and the features from A.

6 Results

We evaluated our methods on two manually anno-
tated test sets. We used one of these test sets for
development only. For this set, one graduate com-
puter science student and two students from the
English department annotated a set of Wikipedia
articles by marking any phrases they considered
to indicate a causal relationship and marking the
phrase as “reason” or “result.” Wikipedia articles
from the following categories were chosen as we
believe they are more likely to contain causal re-
lationships: science, medicine, disasters, history,
television, and film. For each article in this cate-
gory, both the English and Simple Wikipedia ar-

ticles were annotated. A total of 12 article pairs
were annotated. IAA was computed to be 0.31 on
two article pairs using Kripendorff’s alpha.

IAA was very low and we also noticed that
annotators seemed to miss sentences containing
causal connectives. It is easy for an annotator to
overlook a causal relation when reading through
a large quantity of text. Thus, we created a new
task that required labeling a connective as causal
or not when provided with the sentence contain-
ing the connective. For testing, we used Crowd-
Flower to annotate the output of the system using
this method. We created a balanced test set by an-
notating 600 examples, where the system labeled
300 as causal and 300 as non-causal. Contribu-
tors were limited to the highest level of quality and
from English-speaking countries. We required 7
annotators for each data point. The IAA was com-
puted on the qualification task that all annotators
were required to complete. There were 15 ques-
tions on this task and 410 annotators. On this sim-
plified task, the IAA improved to 0.69.

We also considered evaluating the results on
the PDTB but encountered several issues. As
the PDTB only has a limited set of explicit intra-
sentence connectives marked, this would not show
the full strength of our method. Many causal con-
nectives that we discovered are not annotated in
the PDTB. Alternatively, we considered evaluat-
ing on the AltLexes in the PDTB but these ex-
amples are only limited to inter-sentence cases,
whereas the vast majority of our automatically an-
notated training data was for the intra-sentence
case. Thus we concluded that any evaluation on
the PDTB would require additional annotation.
Our goal in this work was to identify new ways
in which causality is expressed, unlike the PDTB
where annotators were given a list of connectives
and asked to determine discourse relations.

We tested our hypothesis by training a binary3

classifier on our data using the full set of features
we just described. We used a linear Support Vector
Machine (SVM) classifier (Vapnik, 1998) trained
using stochastic gradient descent (SGD) through
the sci-kit learn package. (Pedregosa et al, 2011)4

We used elasticnet to encourage sparsity and tuned
the regularization constant α through grid search.

We use two baselines. The first baseline is the
3We combine “reason” and “result” into one “causal”

class and plan to work on distinguishing between non-causal,
reason, and result in the future.

4We also considered a logistic regression classifier.

1430



Accuracy True Precision True Recall True F-measure
Most Common Class 63.50 60.32 82.96 69.85
CONN 62.21 78.47 35.64 49.02
LS 67.68 61.98 58.51 60.19
KLD 58.03 91.17 19.55 32.20
LS ∪KLD 73.95 80.63 64.35 71.57
LS ∪ LSinter 72.99 78.54 64.66 70.93
KLD ∪ LS ∪ LSinter 70.09 76.95 58.99 66.78
LS ∪KLD ∪ CONN 71.86 70.28 77.60 73.76
Bootstrapping1 79.26 77.97 82.64 80.24
Bootstrapping2 79.58 77.29 84.85 80.90

Table 3: Experimental Results

most common class of each AltLex according to
its class in the initial training set. For example,
“caused by” is almost always a causal AltLex. A
second baseline uses the AltLex itself as a cate-
gorical feature and is shown as CONN in Ta-
ble 3. For comparison, this is the same baseline
used in (Pitler and Nenkova, 2009) on the ex-
plicit discourse relations in the PDTB. We com-
pare these two baselines to ablated versions of our
system. We evaluate on the KLD (KLD) and
semantic (LS and LSinter) features described in
sections 5 and 5.1. LS consists of features 1-8,
all the FrameNet, VerbNet, and WordNet features.
LSinter includes only the interaction between cat-
egorical features from WordNet and VerbNet.

We calculate accuracy and true precision, recall,
and F-measure for the causal class. As seen in Ta-
ble 3, the best system (LS ∪ KLD ∪ CONN )
outperforms the baselines.5 The lexical semantic
features by themselves (LS) are similar to those
used by (Riaz and Girju, 2014) although on a dif-
ferent task and with the WordNet and VerbNet fea-
tures included. Note that the addition of the Altlex
words and KL divergence (LS∪KLD∪CONN )
yields an absolute increase in f-measure of 13.57
points over lexical semantic features alone.

6.1 Bootstrapping

Our method for labeling AltLexes lends itself nat-
urally to a bootstrapping approach. As we are us-
ing explicit connectives to identify new AltLexes,
we can also use these new AltLexes to identify ad-
ditional ones. We then consider any paraphrase
pairs where at least one of the phrases contains one
of our newly discovered AltLexes. We also use

5These results are statistically significant by a binomial
test with p < 7 ∗ 10−6.

our classifier to automatically label these new data
points and remove any phrases where the classifier
did not agree on both elements in the pair. The set
of features used were theKLD∪LS∪LSinter fea-
tures as these performed best on the development
set. We use early stopping on the development
data to identify the point when adding additional
data is not worthwhile. The bootstrapping method
converges quickly. After 2 iterations we see a de-
crease in the F-measure of the development data.

The increase in performance on the test data is
significant. In Table 3, Bootstrappingn refers
to results after n rounds of bootstrapping. Boot-
strapping yields improvement over the supervised
method with an absolute gain of 7.14 points.

6.2 Discussion

Of note is that the systems without connectives
(combinations of LS, LSinter, and KLD) per-
form well on the development set without using
any lexical features. Using this system enables the
discovery of new AltLexes during bootstrapping,
as we cannot rely on having a closed class of con-
nectives but need a way of classifying connectives
not seen in the initial training set.

Also important is that the Altlex by itself
(CONN ) performs poorly. In comparison, in the
task of identifying discourse relations in the PDTB
these features yield an 75.33 F-score and 85.85%
accuracy in distinguishing between discourse and
non-discourse usage (Pitler and Nenkova, 2009)
and an accuracy of 93.67% when distinguishing
between discourse classes. Although this is a dif-
ferent data set, this shows that identifying causal-
ity when there is an open class of connectives
is much more difficult. We believe the connec-
tive by itself performs poorly because of the wide

1431



True Precision True Recall True F-measure
FrameNet 67.88 53.14 59.61
WordNet 76.92 9.52 16.94
V erbNet 38.70 3.80 6.92

Table 4: Semantic Feature Ablation

linguistic variation in these alternative lexicaliza-
tions. Many connectives appear only once or not
at all in the training set, so the additional features
are required to improve performance.

In addition, the “most common class” baseline
is a strong baseline. The strength of this perfor-
mance provides some indication of the quality of
the training data, as the majority of the time the
connective is very indicative of its class in the
held-out test data. However, the the overall accu-
racy is still much lower than if we use informative
features.

The KLD and LS feature sets appear to be
complementary. The KLD feature sets have
higher precision on a smaller section of the data,
whereas the LS system has higher recall over-
all. These lexical semantic features likely have
higher recall because these resources are designed
to represent classes of words rather than individ-
ual words. Some connectives occur very rarely, so
it is necessary to generalize the key aspects of the
connectives and class-based resources provide this
capability.

In order to determine the contribution of each
lexical resource, we perform additional feature ab-
lation for each of FrameNet, WordNet, and Verb-
Net. As seen in Table 4, the lexical semantic re-
sources each contribute uniquely to the classifier.
The FrameNet features provide most of the perfor-
mance of the classifier. The WordNet and Verb-
Net features, though not strong individually, sup-
ply complementary information and improve the
overall performance of the LS system (see Table
3) compared to just using FrameNet alone.

Finally, the model (LS∪KLD∪CONN ) cor-
rectly identifies some causal relations that neither
baseline identifies, such as:

Language is reduced to simple phrases
or even single words, eventually leading
to complete loss of speech.
Kulap quickly accelerated north,
prompting the PAGASA to issue their
final advisory on the system.

These examples do not contain standard causal

connectives and occur infrequently in the data, so
the lexical semantic features help to identify them.

After two rounds of bootstrapping, the system is
able to recover additional examples that were not
found previously, such as:

When he finally changed back, Buffy
stabbed him in order to once again save
the world.

This connective occurs rarely or not at all in the
initial training data and is only recovered because
of the improvements in the model.

7 Conclusion

We have shown a method for identifying and clas-
sifying phrases that indicate causality. Our method
for automatically building a training set for causal-
ity is a new contribution. We have shown statisti-
cally significant improvement over the naive base-
line using semantic and parallel corpus features.
The text in the AltLex alone is not sufficient to ac-
curately identify causality. We show that our fea-
tures are informative by themselves and perform
well even on rarely occurring examples.

Ultimately, the focus of this work is to improve
detection of causal relations. Thus, we did not
evaluate some intermediate steps, such as the qual-
ity of the automatically annotated corpus. Our use
of distant supervision demonstrates that we can
use a large amount of possibly noisy data to de-
velop an accurate classifer. To evaluate on the in-
termediate step would have required an additional
annotation process. In the future, we may improve
this step using a machine learning approach.

Although we have focused exclusively on
Wikipedia, these methods could be adapted to
other domains and languages. Causality is not
easily expressed in English using a fixed set of
phrases, so we would expect these methods to ap-
ply to formal and informal text ranging from news
and journals to social media. Linguistic expres-
sions of causality in other languages is another av-
enue for future research, and it would be interest-
ing to note if other languages have the same vari-
ety of expression.

1432



References
Or Biran and Kathleen McKeown. 2013. Aggregated

Word Pair Features for Implicit Discourse Relation
Disambiguation. Proceedings of ACL.

Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and Refining
Rhetorical-Semantic Relation Models. Proceedings
of NAACL-HLT.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised Learning of Narrative Event Chains. Stan-
ford University. Stanford, CA 94305.

Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally Supervised Event Causality Identifica-
tion. Transactions of ACL, 3:329344.

William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,
and Wei Wu. 2015. Aligning Sentences from Stan-
dard Wikipedia to Simple Wikipedia. Proceedings of
NAACL-HLT.

Yangfeng Ji and Jacob Eisenstein. 2015. One Vec-
tor is Not Enough: Entity-Augmented Distributed
Semantics for Discourse Relations. Proceedings of
EMNLP.

Karin Kipper, Hoa Trang Dan, and Martha Palmer.
2000. Class-Based Construction of a Verb Lexicon.
American Association for Artifical Intelligence.

Majid Laali and Leila Kosseim. 2014. Inducing Dis-
course Connectives from Parallel Texts. Proceed-
ings of COLING: Technical Papers.

Quoc Le and Tomas Mikolov. 2014. Distributed Rep-
resentations of Sentences and Documents. Proceed-
ings of ICML.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012.
A PDTB-Styled End-to-End Discourse Parser. De-
partment of Computer Science, National University
of Singapore.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit. Proceedings of ACL:
System Demonstrations.

Daniel Marcu and Abdessamad Echihabi 2001. An
Unsupervised Approach to Recognizing Discourse
Relations. Proceedings of ACL.

George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM.

The PDTB Research Group 2008. The PDTB 2.0. An-
notation Manual. Technical Report IRCS-08-01. In-
stitute for Research in Cognitive Science, University
of Pennsylvania.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text Proceedings of ACL.

Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. Proceedings of ACL-IJCNLP Short Papers.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
Proceedings of LREC.

Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Realization of Discourse Relations by Other
Means: Alternative Lexicalizations. Proceedings of
COLING.

Kira Radinsky and Eric Horvitz. 2013. Mining the Web
to Predict Future Events. Proceedings of WSDM.

Mehwish Riaz and Roxana Girju. 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
SIGDIAL.

Mehwish Riaz and Roxana Girju. 2014. Recognizing
Causality in Verb-Noun Pairs via Noun and Verb Se-
mantics. Proceedings of the EACL 2014 Workshop
on Computational Approaches to Causality in Lan-
guage.

Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice. University of California, Berkeley.

Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley-Interscience.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. Proceedings of NAACL-HLT.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D.
Cournapeau, M. Brucher, M. Perrot, E. Duchesnay.
2011. Scikit-learn: Machine Learning in Python.
Journal of Machine Learning Research Volume 12.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. Proceedings of ACL: Interactive Poster and
Demonstration Sessions.

1433


