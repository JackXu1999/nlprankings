



















































Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–741,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Mining Informal Language from Chinese Microtext:
Joint Word Recognition and Segmentation

Aobo Wang1 Min-Yen Kan1,2∗
1 Web IR / NLP Group (WING)

2 Interactive and Digital Media Institute (IDMI)
National University of Singapore

13 Computing Link, Singapore 117590
{wangaobo,kanmy}@comp.nus.edu.sg

Abstract

We address the problem of informal word
recognition in Chinese microblogs. A key
problem is the lack of word delimiters in
Chinese. We exploit this reliance as an
opportunity: recognizing the relation be-
tween informal word recognition and Chi-
nese word segmentation, we propose to
model the two tasks jointly. Our joint in-
ference method significantly outperforms
baseline systems that conduct the tasks in-
dividually or sequentially.

1 Introduction

User generated content (UGC) – including mi-
croblogs, comments, SMS, chat and instant mes-
saging – collectively referred to as microtext by
Gouwset et al. (2011) or network informal lan-
guage by Xia et al. (2005), is the hallmark of the
participatory Web.

While a rich source that many applications are
interested in mining for knowledge, microtext pro-
cessing is difficult to process. One key reason
for this difficulty is the ubiquitous presence of
informal words – anomalous terms that manifest
as ad hoc abbreviations, neologisms, unconven-
tional spellings and phonetic substitutions. Such
informality is often present in oral conversation,
and user-generated microblogs reflect this infor-
mality. Natural language processing (NLP) tools
largely fail to work properly on microtext, as they
have largely been trained on formally written text
(i.e., newswire). Recent work has started to ad-
dress these shortcomings (Xia and Wong, 2006;
Kobus et al., 2008; Han and Baldwin, 2011). In-
formal words and their usage in microtext evolves
quickly, following social trends and news events.

∗This research is supported by the Singapore National
Research Foundation under its International Research Centre
@ Singapore Funding Initiative and administered by the IDM
Programme Office.

These characteristics make it difficult for lexicog-
raphers to compile lexica to keep with the pace of
language change.

We focus on this problem in the Chinese lan-
guage. Through our analysis of a gathered Chinese
microblog corpus, we observe that Chinese infor-
mal words originate from three primary sources,
as given in Table 1.

But unlike noisy words in English, Chinese in-
formal words are more difficult to mechanically
recognize for two critical reasons: first, Chinese
does not employ word delimiters; second, Chinese
informal words combine numbers, alphabetic let-
ters and Chinese characters. Techniques for En-
glish informal word detection that rely on word
boundaries and informal word orthography need
to be adapted for Chinese. Consider the micro-
text “g�” (meaning “Don’t tell me the
spoilers (to a movie or joke)”, also in Table 1).
If “” (“don’t”) and “” (past tense marker)
are correctly recognized as two words, we may
predict the previously unseen characters “g�”
(“tell spoilers”) as an informal word, based on
the learned Chinese language patterns. However,
state-of-the-art Chinese segmenters1 incorrectly
yield “ g �”, preferring to chunk “�
” (“thoroughly”) as a word, as they do not con-
sider the possibility that “g�” (“spoiler”) could
be an informal word. This example illustrates the
mutual dependency between Chinese word seg-
mentation (henceforth, CWS) and informal word
recognition (IWR) that should be solved jointly.

Hence, rather than pipeline the two processes
serially as previous work, we formulate it as a two-
layer sequential labeling problem. We employ fac-
torial conditional random field (FCRF) to solve
both CWS and IWR jointly. To our best knowl-
edge, this is the first work that shows how Chi-
nese microtext can be analyzed from raw text to

1http://www.ictclas.org/index.html

731



Table 1: Our classification of Chinese informal words as originating from three primary sources. For
Phonetic Substitutions, pronunciation is indicated by the phonetic Pinyin transcription system.

Informal Word Formal Word Example Sentence English Translation

1) Phonetic
(	(mu4 you3) ¡	(mei2 you3) �Ñ:(((			úßf No taxi in the development area

Substitutions
i¸ì(hai2 zhi3 men) iPì(hai2 zi men) wiii¸̧̧ììì Get up kids

bs �Æ(bi shi) �bs` I despise you

2) Abbreviation L8 Lb8� eLLL888' Let’s play board games
g� gÅ�2 ggg��� Don’t tell (me) the spoilers

3) Neologisms Ù Ò �ÙÙÙJ So awesome!
Ò@ Å�-p �¦ÒÒÒ@@@ Quickly purchase it

derive joint solutions for both problems of CWS
and IWR. We also propose novel features for in-
put to the joint inference. Our techniques signif-
icantly outperform both research and commercial
state-of-the-art for these problems, including two-
step linear CRF baselines which perform the two
tasks sequentially.

We detail our methods in Section 2. In Sec-
tion 3, we first describe the details of our dataset
and baseline systems, followed by demonstrating
two sets of experiments for CWS and IWR, re-
spectively. Section 4 offers the discussion on error
analysis and limitations. We discuss related work
in Section 5, before concluding our paper.

2 Methodology

Given an input Chinese microblog post, our
method simultaneously segments the sentences
into words (the Chinese Word Segmentation,
CWS, task), and marks the component words as
informal or formal ones (the Informal Word Re-
congition, IWR, task).

2.1 Problem Formalization

The two tasks are simple to formalize. The IWR
task labels each Chinese character with either an F
(part of a formal word) or IF (informal word). For
the CWS task, we follow the widely-used BIES
coding scheme (Low et al., 2005; Hai et al., 2006),
where B, I, E and S stand for beginning of a
word, inside a word, end of a word and single-
character word, respectively. As a result, we have
two (hidden) labels to associate with each (ob-
servable) character. Figure 1 illustrates an exam-
ple microblog post graphically, where the labels
are in circles and the observations are in squares.
The two informal words in the example post are
“(	” (normalized form: “¡	”; English gloss:
“no”) and “rp” (“ºÁ<”; “luck”).

2.2 Conditional Random Field Models

Given the general performance and discrimi-
native framework, Conditional Random Fields
(CRFs) (Lafferty et al., 2001) is a suitable frame-
work for tackling sequence labeling problems.
Other alternative frameworks such as Markov
Logic Networks (MLNs) and Integer Linear Pro-
gramming (ILP) could also be considered. How-
ever, we feel that for this task, formulating effi-
cient global formulas (constraints) for MLN (ILP)
is comparatively less straightforward than in other
tasks (e.g, compared to Semantic Role Labeling,
where the rules may come directly from grammat-
ical constraints). CRFs represent a basic, simple
and well-understood framework for sequence la-
beling, making it a suitable framework for adapt-
ing to perform joint inference.

2.2.1 Linear-Chain CRF
A linear-chain CRF (LCRF; Figure 2a) predicts
the output label based on feature functions pro-
vided by the scientist on the input. In fact, the
LCRF has been used for the exact problem of
CWS (Sun and Xu, 2011), garnering state-of-the-
art performance, and as such, validate it as a strong
baseline for comparison.

2.2.2 Factorial CRF
To properly model the interplay between the
two sub-problems, we employ the factorial CRF
(FCRF) model, which is based on the dynamic
CRF (DCRF) (Sutton et al., 2007). By introduc-
ing a pairwise factor between different variables
at each position, the FCRF model results as a spe-
cial case of the DCRF. A FCRF captures the joint
distribution among various layers and jointly pre-
dicts across layers. Figure 2 illustrates both the
LCRF and FCRF models, where cliques include
within-chain edges (e.g., yt, yt+1) in both LCRF
and FCRF models, and the between-chain edges
(e.g., yt, zt) only in the FCRF.

732



开 啊低值品人，车租出有没区发

F FIFIFFFFFIFIFF FF F

I EIBSEIBEBE SB S

开 啊低值pr，车租出有木区发

There charactermypoorhow,zonedevelopmenttheintaxinois is

Figure 1: A Chinese microtext (bottom layer) with annotations for IWR (top layer) and CWS (middle
layer). The bottom three lines give the normalized Chinese form, its pronuniciation in Pinyin and aligned
English translation.

yt

xt

yt+1

xt+1

yt-1

xt-1
(a) Linear-chain CRF

yt

xt

yt+1

xt+1

yt-1

xt-1

zt zt+1zt-1

(b) Two-layer Factorial CRF

Figure 2: Graphical representations of the two
types of CRFs used in this work. yt denotes the
1st layer label, zt denotes the 2nd layer label, and
xt denotes the observation sequence.

Although the FCRF can be collapsed into a
LCRF whose state space is the cross-product of
the outcomes of the state variables (i.e., 8 labels
in this case), Sutton et al. (2007) noted that such
a LCRF requires not only more parameters in the
number of variables, but also more training data
to achieve equivalent performance with an FCRF.
Given the limited scale of the state space and train-
ing data, we follow the FCRF model, using exact
Junction Tree (Jensen, 1996) inference and decod-
ing algorithm to perform prediction.

2.3 CRF Features

We use three broad feature classes – lexical,
dictionary-based and statistical features – aiming
to distinguish the output classes for the CWS and
IRW problems. Character-based sequence label-
ing is employed for word segmentation due to its
simplicity and robustness to the unknown word
problem (Xue, 2003).

A key contribution of our work is also to
propose novel features for joint inference. We

propose new features for the dictionary-based and
statistical feature classes, which we have marked
in the discussion below with “(*)”. We later
examine their efficacy in Section 3.

Lexical Features. As a foundation, we employ
lexical (n-gram) features informed by the previous
state-of-the-art for CWS (Sun and Xu, 2011; Low
et al., 2005). These features are listed below2:

• Character 1-gram: Ck(i− 4 < k < i+ 4)
• Character 2-gram: CkCk+1(i− 4 < k < i+
3)

• Character 3-gram: CkCk+1Ck+2(i − 3 <
k < i+ 2)

• Character lexicon: C−1C1
This feature is used to capture the common
indicators in Chinese interrogative sentences.
(e.g., “//” (“whether or not”), “}}”
(“OK or not”))

• Whether Ck and Ck+1 are identical, for i −
4 < k < i+ 3.
This feature is used to capture the words
of employing character doubling in Chinese.
(e.g., “ÜÜ” (“see you”), “))” (“every
day”))

Dictionary-based Features. We use features
that indicate whether the input character sequence

2For notational convenience, we denote a candidate char-
acter token Ci as having a context ...Ci−1CiCi+1.... We use
Cm:n to express a subsequence starting at the position m and
ending at n. len stands for the length of the subsequence, and
offset denotes the position offset of Cm:n from the current
character Ci. We use b (beginning), m (middle) and e (end-
ing) to indicate the position of Ck (m ≤ k ≤ n) within the
string Cm:n.

733



matches entries in certain lexica. We use the on-
line dictionary from Peking University as the for-
mal lexicon and the compiled informal word list
from our training instances as the informal lex-
icon. In addition, we employ additional online
word lists3 to distinguish named entities and func-
tion words from potential informal words.

As shown in Table 1, alphabetic sequences in
microblogs may refer to Chinese Pinyin or Pinyin
abbreviations, rather than English (e.g., “bs” for
bi shi; “to despise”). Hence, we added dictionary-
based features to indicate the presence of Pinyin
initials, finals and standard Pinyin expansions, us-
ing a UK English word list4. The final list of
dictionary-based features employed are:

• If Ck (i − 4 < k < i + 4) is a surname:
Surname@k

• (*) If Ck (i− 4 < k < i+ 4) is a stop word:
StopW@k

• (*) If Ck (i−4 < k < i+4) is a noun-suffix:
NSuffix@k

• (*) If Ck (i − 4 < k < i + 4) is a Pinyin
Initial: Initial@k

• (*) IfCk (i−4 < k < i+4) is a Pinyin Final:
Final@k

• If Ck (i− 4 < k < i+ 4) is a English letter:
En@k

• IfCm:n (i−4 < m < n < i+4, 0 < n−m <
5) matches one entry in the Peking University
dictionary:
FW@m:n; len@offset; FW-Ck@b-offset,
FW-Ck@n-offset or FW-Ck@e-offset

• (*) If Cm:n (i − 4 < m < n < i + 4, 0 <
n −m < 5) matches one entry in the infor-
mal word list:
IFW@m:n; len@offset; IFW-Ck@b-offset,
IFW-Ck@n-offset or IFW-Ck@e-offset

• (*) If Cm:n (i − 4 < m < n < i + 4, 0 <
n − m < 5) matches one entry in the valid
Pinyin list:
PY@m:n; len@offset; PY-Ck@b-offset, PY-
Ck@n-offset or PY-Ck@e-offset

Statistical Features. We use pointwise mutual
information (PMI) variant (Church and Hanks,

3Resources are available at http://www.sogou.
com/labs/resources.html

4http://www.bckelk.uklinux.net/menu.
html

1990) to account for global, corpus-wide informa-
tion. This measures the difference between the ob-
served probability of an event (i.e., several charac-
ters combined as an informal word) and its expec-
tation, based on the probabilities of the individual
events (i.e., the probability of the individual char-
acters occurring in the corpus). Compared with
other standard association measures such as MI,
PMI tends to assign rare events higher scores. This
makes it a useful signal for IWR, as it is sensi-
tive to informal words which often have low fre-
quency. However, the word frequency alone is
not reliable enough to distinguish informal words
from uncommon but formal words.

In response to these difficulties in differentiat-
ing linguistic registers, we compute two different
PMI scores for character-based bigrams from two
large corpora representing news and microblogs as
features. We also use the difference between the
two PMI scores as a differential feature. In ad-
dition, we also convert all the character-based bi-
grams into Pinyin-based bigrams (ignoring tones5)
and compute the Pinyin-level PMI in the same
way. These features capture inconsistent use of
the bigram across the two domains, which assists
to distinguish informal words. Note that we es-
chew smoothing in our computation of PMI, as it
is important to capture the inconsistent character
bigrams usage between the two domains. For ex-
ample, the word “rp” appears in the microblog do-
main, but not in news. If smoothing is conducted,
the character bigram “rp” will be given a non-zero
probability in both domains, not reflective of ac-
tual use. For each character Ci, we incorporate
the PMI of the character bigrams as follows:

• (*) If CkCk+1 (i − 4 < k < i + 4) is not a
Chinese word recorded in dictionaries:
CPMI-N@k+i; CPMI-M@k+i; CDiff@k+i;
PYPMI-N@k+i; PYPMI-M@k+i; PYD-
iff@k+i

3 Experiment

We discuss the dataset, baseline systems and ex-
periments results in detail in the following.

3.1 Data Preparation

We utilize the Chinese social media archive,
PrEV (Cui et al., 2012), to obtain Chinese mi-

5The informal word may have the same Pinyin transcrip-
tion as its formal counterpart without considering the differ-
ences in tones.

734



croblog posts from the public timeline of Sina
Weibo6. Sina Weibo is the largest microblogging
in China, where over 100 million Chinese mi-
croblog posts are posted daily (Cao, 2012), likely
the largest public source of informal and daily
Chinese language use. Our dataset has a total of
6,678,021 messages, covering two months from
June to July of 2011. To annotate the corpus,
we employ Zhubajie7, one of China mainland’s
largest crowdsourcing (Wang et al., 2010) plat-
forms to obtain informal word annotations. In
total, we spent US$110 on assembling a sub-
set of 5, 500 posts (12, 446 sentences) in which
1, 658 unique informal words are annotated within
five weeks via Zhubajie. Each post was anno-
tated by three annotators with moderate (0.57)
inter-annotator agreement measured by Fleiss’
κ (Joseph, 1971), and conflicts were resolved by
majority voting.

We divided the annotated corpus, taking 4, 000
posts for training, and the remainder (1, 500) for
testing. Through inspection, we note that 79.8%
of the informal words annotated in the testing set
are not covered by the training set. We also follow
Wang et al. (2012)’s conventions and apply rule-
sets to preprocess the corpus’ URLs, emoticons,
“@usernames” and Hashtags as pre-segmented
words, before input to CWS and IWR. For the
CWS task, the first author manually labelled the
same corpus following the segmentation guide-
lines published with the SIGHAN-58 MSR dataset.

3.2 Baseline Systems
We implemented several baseline systems to com-
pare with proposed FCRF joint inference method.

Existing Systems. We re-implemented Xia and
Wong (2008)’s extended Support Vector Machine
(SVM) based microtext IWR system to compare
with our method. Their system only does IWR,
using the CWS and POS tagging otuput of the
ICTCLAS segmenter (Zhang et al., 2003) as in-
put. To compare our joint inference versus other
learning models, we also employed a decision tree
(DT) learner, equipped with the same feature set
as our FCRF. Both the SVM and DT models are
provided by the Weka3 (Hall et al., 2009) toolkit,
using its default configuration.

To evaluate CWS performance, we compare
with two recent segmenters. Sun and Xu (2011)’s

6http://open.weibo.com
7http://www.zhubajie.com
8http://www.sighan.org

work achieves state-of-the-art performance and
is publicly available. They employ a LCRF
taking as input both lexical and statistical fea-
tures derived from unlabeled data. As a sec-
ond baseline, we also evaluate against a widely-
used, commercially-available alternative, the re-
cently released 2011 ICTCLAS segmenter9.

Two-stage Sequential Systems. To benchmark
the improvement that the factorial CRF model
has by doing the two tasks jointly, we com-
pare with a LCRF solution that chains these two
tasks together. For completeness, we test pipelin-
ing in both directions – CWS feeding features
for IWR (LCRFcws�LCRFiwr), and the reverse
(LCRFiwr�LCRFcws). We modify the open-
source Mallet GRMM package (Sutton, 2006) to
implement both this sequential LCRF model and
our proposed FCRF model. Both models take the
whole feature set described in Section 2.3.

Upper Bound Systems. To measure the upper-
bound achievable with perfect support from the
complementary task, we also provided gold stan-
dard labels of one task (e.g., IWR) as an input
feature to the other task (e.g., CWS). These sys-
tems (hereafter denoted as LCRF�LCRF-UB and
FCRF-UB) are meant for reference only, as they
have access to answers for the opposing tasks.

Adapted SVM for Joint Classification. For
completeness, we also compared our work against
the standard SVM classification model that per-
forms both tasks by predicting the cross-product
of the CWS and IWR individual classes (in to-
tal, 8 classes). We train the SVM classifier on the
same set of features as the FCRF, by providing the
cross-product of two layer labels as gold labels.
This system (hereafter denoted as SVM-JC) was
implemented using the LibSVM package (Chang
and Lin, 2011).

3.3 Evaluation Metrics

We use the standard metrics of precision, recall
and F1 for the IWR task. Only words that exactly
match the manually-annotated labels are consid-
ered correct. For example given the sentence “�
HËËËHHH}�b” (“�HÙÙÙHHH}�b”; “How deli-
cious it is”), if the IWR component identifies “Ë
H” as an informal word, it will be considered cor-
rect, whereas both “ËH}” and “Ë” are deemed
incorrect. For CWS evaluation, we employ the
conventional scoring script provided in SIGHAN-

9http://www.ictclas.org/index.html

735



5, which also provides out-of-vocabulary recall
(OOVR).

To determine statistical significance of the im-
provements, we also compute paired, one-tailed
t tests. As pointed out by Yeh and Alexan-
der (2000), the randomization method is more re-
liable in measuring the significance of F1 through
handling non-linear functions of random variables.
Thus we employ Padó (2006)’s implementation of
randomization algorithm to measure the signifi-
cance of F1.

3.4 Experimental Results

The goal of our experiments is to answer the fol-
lowing research questions:

RQ1 Do the two tasks of CWS and IWR benefit
from each other?

RQ2 Is jointly modeling both tasks more efficient
than conducting each task separately or se-
quentially?

RQ3 What is the upper bound improvement that
can be achieved with perfect support from the
opposing task?

RQ4 Are the features we designed for the joint
inference method effective?

RQ5 Is there a significant difference between the
performance of the joint inference of a cross-
product SVM and our proposed FCRF?

3.4.1 CWS Performance

Table 2: Performance comparison on the CWS
task. The two bottom-most rows show upper
bound performance. ‘‡’(‘∗’) in the top four lines
indicates statistical significance at p < 0.001
(0.05) when compared with the previous row.
Symbols in the bottom two lines indicate signifi-
cant difference between upper bound systems and
their corresponding counterparts.

Pre Rec F1 OOVR
ICTCLAS (2003) 0.640 0.767 0.698 0.551
Sun and Xu (2011) 0.661‡ 0.691‡ 0.675 0.572‡

LCRFiwr�LCRFcws 0.741‡ 0.775‡ 0.758∗ 0.607∗
FCRF 0.757‡ 0.801‡ 0.778∗ 0.633∗

LCRFiwr�LCRFcws-UB 0.807‡ 0.815‡ 0.811∗ 0.731‡
FCRF-UB 0.820‡ 0.833‡ 0.826∗ 0.758‡

In general, our FCRF yields the best perfor-
mance among all systems (top portion of Table 2),

answering RQ1. Given microblog posts as test
data, the F1 of ICTCLAS drops from 0.98510 to
0.698, clearly showing the difficulty of process-
ing microtext. The sequential LCRF model and
FCRF model both outperform the baselines, which
means with the novel features shared by the two
tasks, CWS benefits significantly from the results
of IWR. Hence our segmenter outperforms the ex-
isting segmenters by tackling one of the bottle-
necks of recognizing informal words in Chinese
microtext.

To illustrate, the sequence “...			(((			º...”
(“...			¡¡¡			º...”; “...is there anyone...”), is cor-
rectly labeled as BIES by our FCRF model but
mislabeled by baseline systems as SSBE. This is
likely due to the ignorance of the informal word
“	(	”, leading baseline systems to keep the
formal word “	º” (“someone”) as a segment.

More importantly, by jointly optimizing the
probabilities of labels on both layers, the FCRF
model slightly but significantly improves over the
sequential LCRF method, answering RQ2. Thus
we conclude that jointly modeling both tasks is
more effective than performing the tasks sequen-
tially.

For RQ3, the last two rows presents the upper-
bound systems that have access to gold standard
labels for IWR. Both upper-bound systems sta-
tistically outperform their counterparts, indicating
that there is still room to improve CWS perfor-
mance with better IWR as input. This also vali-
dates our assumption that CWS can benefit from
joint consideration of IWR. Taking the best pre-
vious work as our lower bound (0.69 F1), we see
that our FCRF methodology (0.77) makes signifi-
cant progress towards the upper bound (0.82).

3.4.2 IWR Performance
For RQ1 and RQ2, Table 3 compares the per-
formance of our method with the baseline sys-
tems on the IWR task. Overall, the FCRF
method again outperforms all the baseline sys-
tems. We note that the CRF based models achieve
much higher precision score than baseline sys-
tems, which means that the CRF based models
can make accurate predictions without enlarging
the scope of prospective informal words. Com-
pared with the CRF based models, the SVM and
DT both over-predict informal words, incurring
a larger precision penalty. Studying this phe-

10Self-declared segmentation accuracy on formal
text.http://www.ictclas.org/

736



Table 3: Performance comparison on the IWR
task. ‘‡’ or ‘∗’ in the top four rows indicates sta-
tistical significance at p < 0.001 or < 0.05 com-
pared with the previous row. Symbols in the bot-
tom two rows indicate differences between upper
bound systems and their counterparts.

Pre Rec F1
SVM 0.382 0.621 0.473
DT 0.402∗ 0.714∗ 0.514∗

LCRFcws�LCRFiwr 0.858‡ 0.591‡ 0.699∗
FCRF 0.877∗ 0.655∗ 0.750∗

LCRFcws�LCRFiwr-UB 0.840 0.726∗ 0.779∗
FCRF-UB 0.878 0.752∗ 0.810∗

nomenon more closely, we find it is difficult for
the baseline systems to classify segments mixed
with formal and informal characters. Taking the
microblog “�HËËËHHH}�b” (“�HÙÙÙHHH}�
b”; “how delicious it is”) as an example, with-
out considering the possible word boundaries sug-
gested by the contextual formal words – i.e., “�
H” (“how”) and “}�” (“delicious”) – the base-
lines chunk the informal words (i.e., “ËH”) to-
gether with adjacent characters mistakenly as “Ë
H}” or, “HËH”.

As indicated by the bold figures in Table 3, the
FCRF performs slightly better than the sequential
LCRF (p < 0.05) – a weaker trend when com-
pared with the CWS case. As an example, the se-
quential LCRF method fails to recognize “1¯”
(“iPhone”) as an informal word in the sentence
“�111¯̄̄}©” (“my iPhone is fun”), where the
FCRF succeeds. Inspecting the output, the LCRF
segmenter mislabels “1¯” as SS. By jointly con-
sidering the probabilities of the two layers, the
FCRF model infers better quality segmentation la-
bels, which in turn enhances the FCRF’s capabil-
ity to recognize the sequence of two characters as
an informal word. This is further validated by
the significant performance gulf between the up-
per bound and the basic system shown in the lower
half of the table.

For RQ3, interestingly, the difference in perfor-
mance between the LCRF and FCRF upper-bound
systems is not significant. However, these are up-
per bounds, and we expect on real-world data that
CWS performance will not be perfect. As such,
we still recommend using the FCRF model, as the
joint process is more robust to noisy input from
one channel.

Table 4: F1 comparison between FCRF and
FCRF−new. (‘∗’) indicates statistical significance
at p < 0.05 when compared with the previous row.

CWS IWR
FCRF−new 0.690 0.552
FCRF 0.778∗ 0.750∗

3.4.3 Feature set evaluation
For RQ4, to evaluate the effectiveness of our
newly-introduced feature sets (those marked with
“*” in Section 2.3), we also test a FCRF
(FCRF−new) without our new features. Accord-
ing to Table 4, performance drops by a signifi-
cant amount: 0.088 F1 on CWS and 0.198 F1 on
IWR. FCRF−new makes many mistakes identical
to the baselines: segmenting informal words into
several single-character words and chunking ad-
jacent characters from informal and formal words
together.

3.4.4 Adapted SVM-JC vs. FCRF

Table 5: F1 comparison between SVM, SVM-JC
and FCRF. ‘‡’(‘∗’) indicates statistical significance
at p < 0.001 (0.05) when compared with the pre-
vious row.

CWS IWR
SVM — 0.473
SVM-JC 0.741 0.624‡
FCRF 0.778∗ 0.750∗

For RQ5, according to Table 5, our SVM trained
to predict the cross-product CWS/IWR classifica-
tion (SVM-JC) performs quite well on its own.
Unsurprisingly, it does not outperform our pro-
posed FCRF, which has access to more struc-
tural correlation among the CWS and IWR labels.
SVM-JC significantly (p < 0.001) outperforms
the baseline SVM system by 0.151 in the IWR
task, which we think is partially explained by its
good performance (0.761) on the CWS task. The
over-prediction tendency of the individual SVM
is largely solved by simultaneously modeling the
CWS task, whereas FCRF turns out to be more
effective in solving joint inference problem, al-
though in a weaker trend in terms of the statistical
significance (p < 0.05).

We conclude that the use of the FCRF model
and the addition of our new features are both es-
sential for the high performance of our system.

737



4 Discussion

We wish to understand the causes of errors in our
models so that we may better understand its weak-
nesses. Manually inspecting the errors of our sys-
tem, we found three major categories of errors
which we dissect here.

For IWR, the major source of error, accounting
for more than 60% of all errors, is caused by what
we term the partially observed informal word phe-
nomenon. This refers to informal words contain-
ing multiple characters, where some of its compo-
nents have appeared in the training data as infor-
mal words individually. For instance, the single-
character informal word, “à” (“”; “very”) ap-
pears in training multiple times, thus the unseen
informal word “àE” (“E”; “long time”) is a
partially observed informal word. In this case, the
model incorrectly labels the known, single charac-
ter “à” with IF S as an informal word, instead of
labeling the unseen sequence “àE” with correct
labels IF B IF E. Errors then result in both tasks.

This observation motivates the use of the rela-
tion between the known informal word and its for-
mal counterpart in order to inform the model to
better predict in cases of partial observations. Fol-
lowing the same example, given that “à” is an in-
formal word, if the model also considers the prob-
ability of normalizing “à” to “”, while con-
sidering the higher probability that the character
sequence “E” could be a formal word, there
would be a higher likelihood of correctly predict-
ing the sequence “àE” as an informal word. So
informal word normalization is also an intrinsic
component of IWR and CWS, and we believe it
is an interesting direction for future work.

Another source of error is a side effect of mi-
crotext being extremely short. For example, in the
sentence “¥¥¥¶¶¶�*/���” (“ÞÞÞ¶¶¶�*
/���”; “Go home! Exhausted.”), the un-
seen informal word “¥¶” itself forms a short
sentence. Although it has a subsequent sentence
“*/���” (“Exhausted”) as context, and
the two are pragmatically related, (i.e., “I am ex-
hausted! [And as a result,] I want to go home.”),
the lexical relationship between the sentences is
weak; i.e., “*/���” appears frequently as
the context of various sentences, making the con-
text difficult to utilize. These phenomena makes it
difficult to recognize “¥¶” as an informal word.

A possible solution could factor in proximity,
similar to density-based matching, as in Tellex et

Table 6: Sample Chinese freestyle named entities
that are usernames.

Freestyle Named
Entity

Explanation

“´²ê�” “´²” (“durian”), “ê” (“snow”),
“�” (“charming lady”)

“É” It is short for the cartoon name “w
õ”.

“dje”, “pp” Usernames mixed of Chinese and al-
phabetic characters

al. (2003). We can assign a higher weight to fea-
tures related to characters closer to the current
target character. In particular, for this example,
given the current target character “¥”, we can as-
sign higher weight to features generated from fea-
tures from the proximal context “¥¶”, and lower
weight to features extracted from distal contexts.

Another major group of errors come from what
we term freestyle named entities as exemplified in
Table 6; i.e., person names in the form of user IDs
and nicknames, that have less constraint on form
in terms of length, canonical structure (not sur-
names with given names; as is standard in Chinese
names) and may mix alphabetic characters. Most
of these belong to the category of Person Name
(PER), as defined in CoNLL-200311 Named En-
tity Recognition shared task. Such freestyle en-
tities are often misrecognized as informal words,
as they share some of the same stylistic markings,
and are not marked by features used to recognize
previous Chinese named entity recognition meth-
ods (Gao et al., 2005; Zhao and Kit, 2008) that
work on news or general domain text. We recog-
nize this as a challenge in Chinese microtext, but
beyond the scope of our current work.

5 Related Work

In English, IWR has typically been investigated
alongside normalization. Several recent works
(Han and Baldwin, 2011; Gouws et al., 2011; Han
et al., 2012) aim to produce informal/formal word
lexicons and mappings. These works are based on
distributional similarity and string similarity that
address concerns of lexical variation and spelling.
These methods propose two-step unsupervised ap-
proaches to first detect and then normalize de-
tected informal words using dictionaries.

In processing Chinese informal language, work
conducted by Xia and Wong address the problem

11http://www.cnts.ua.ac.be/conll2003/
ner/

738



of in bulletin board system (BBS) chats. They em-
ploy pattern matching and SVM-based classifica-
tion to recognize Chinese informal sentences (not
individual words) chat (Xia et al., 2005). Both
methods had their advantages: the learning-based
method did better on recall, while the pattern
matching performed better on precision. To obtain
consistent performance on new unseen data, they
further employed an error-driven method which
performed more consistently over time-varying
data (Xia and Wong, 2006). In contrast, our
work identifies individual informal words, a finer-
grained (and more difficult) task.

While seminal, we feel that the difference in
scope (informal sentence detection rather than
word detection) shows the limitation of their work
for microblog IWR. Their chats cover only 651
unique informal words, as opposed to our study
covering almost triple the word types (1, 658).
Our corpus demonstrates a higher ratio of infor-
mal word use (a new informal word appears in
1,658
12,446 = 13% of sentences, as opposed to

651
22,400 =

2% in their BBS corpus). Further analysis of
their corpus reveals that phonetic substitution is
the primary origin of informal words in their cor-
pus – 99.2% as reported in (Wong and Xia, 2008).
In contrast, the origin for informal words in mi-
croblogs is more varied, where phonetic substitu-
tions abbreviations and neologisms, account for
53.1%, 21.4% and 18.7% of the informal word
types, respectively. Their method is best suited
for phonetic substitution, thus performing well on
their corpus but poorly on ours.

More closely related, Li and Yarowsky (2008)
tackle Chinese IWR. They bootstrap 500 infor-
mal/formal word pairs by using manually-tuned
queries to find definition sentences on the Web.
The resulting noisy list is further re-ranked based
on n-gram co-occurrence. However, their method
makes a basic assumption that informal/formal
word pairs co-occur within a definition sentence
(i.e., “<informal word> means <formal word>”)
may not hold in microblog data, as microbloggers
largely do not define the words they use.

Closely related to our work is the task of
Chinese new word detection, normally treated
as a separate process from word segmentation in
most previous works (Chen and Bai, 1998; Wu
and Jiang, 2000; Chen and Ma, 2002; Gao et al.,
2005). Aiming to improve both tasks, work by
Peng et al. (2004) and Sun et al. (2012) conduct

segmentation and detection sequentially, but in
an iterative manner rather than joint. This is
a weakness as their linear CRF model requires
re-training. Their method also requires thresholds
to be set through heuristic tuning, as to whether
the segmented words are indeed new words. We
note that the task of new word detection refers
to out-of-vocabulary (OOV) detection, and is
distinctly different from IWR (new words could
be both formal or informal words).

6 Conclusion

There is a close dependency between Chinese
word segmentation (CWS) and informal word
recognition (IWR). To leverage this, we employ a
factorial conditional random field to perform both
tasks of CWS and IWR jointly.

We propose novel features including statistical
and lexical features that improve the performance
of the inference process. We evaluate our method
on a manually-constructed data set and compare it
with multiple research and industrial baselines that
perform CWS and IWR individually or sequen-
tially. Our experimental results show our joint
inference model yields significantly better F1 for
both tasks. For analysis, we also construct upper
bound systems to assess the potential maximal im-
provement, by feeding one task with the gold stan-
dard labels from the complementary task. These
experiments further verify the necessity and ef-
fectiveness of modeling the two tasks jointly, and
point to the possibility of even better performance
with improved per-task performance.

Analyzing the classes of errors made by our sys-
tem, we identify a promising future work topic to
handle errors arising from partially observed in-
formal words – where parts of a multi-character
informal word have been observed before. We be-
lieve incorporating informal word normalization
into the inference process may help address this
important source of error.

Acknowledgments

We would like to thank the anonymous reviewers
for their valuable comments. We also appreciate
the proofreading effort made by Tao Chen, Xi-
angnan He, Ning Fang, Yushi Wang and Haochen
Zhan from WING. This work also benefits from
the discussion with Yang Liu, associate professor
from Tsinghua University.

739



References
Belinda Cao. 2012. Sina’s weibo outlook buoys inter-

net stock gains: China overnight.

Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
pages 27:1–27:27.

Keh-Jiann Chen and Ming-Hong Bai. 1998. Un-
known Word Detection for Chinese by a Corpus-
Based Learning Method. International Journal of
Computational Linguistics and Chinese Language
Processing, pages 27–44.

Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown
Word Extraction for Chinese Documents. In Pro-
ceedings of the 19th international conference on
Computational linguistics, pages 1–7.

Kenneth Ward Church and Patrick Hanks. 1990. Word
Association Norms, Mutual Information, and Lexi-
cography. Computional Linguistic, pages 22–29.

Anqi Cui, Liner Yang, Dejun Hou, Min-Yen Kan,
Yiqun Liu, Min Zhang, and Shaoping Ma. 2012.
PrEV: Preservation Explorer and Vault for Web 2.0
User-Generated Content. Theory and Practice of
Digital Libraries, pages 101–112.

Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese Word Segmentation and
Named Entity Recognition: A Pragmatic Approach.
Computitional Linguistic, pages 531–574.

Stephan Gouws, Donald Metzler, Congxing Cai, and
Eduard Hovy. 2011. Contextual Bearing on Lin-
guistic Variation in Social Media. In Proceedings of
the Workshop on Language in Social Media, pages
20–29.

Zhao Hai, Huang Chang-Ning, Li Mu, and Lu Bao-
Liang. 2006. Effective Tag Set Selection in Chi-
nese Word Segmentation via Conditional Random
Field Modeling. The 20th Pacific Asia Conference
on Language, Information and Computation, pages
pp.87–94.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer Pfahringer, Peter Reutemann, and
Ian H. Witten. 2009. The WEKA Data Mining
Software: An Update. ACM Special Interest Groups
on Knowledge Discovery and Data Mining Explo-
rations Newsletter, pages 10–18.

Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368–378.

Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically Constructing a Normalisation Dictio-
nary for Microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural

Language Processing and Computational Natural
Language Learning, pages 421–432.

Finn V Jensen. 1996. An Introduction to Bayesian
Networks, volume 74.

Fleiss L Joseph. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, pages 378–382.

Catherine Kobus, François Yvon, and Géraldine
Damnati. 2008. Normalizing SMS: Are Two
Metaphors Better Than One? In International Con-
ference on Computational Linguistics, pages 441–
448.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.

Zhifei Li and David Yarowsky. 2008. Mining and
Modeling Relations between Formal and Informal
Chinese Phrases from Web Corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 1031–1040.

Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing.

Sebastian Padó, 2006. User’s Guide to SIGF: Signifi-
cance Testing by Approximate Randomisation.

Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese Segmentation and New Word Detec-
tion Using Conditional Random Fields. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 562.

Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
Word Segmentation Using Unlabeled Data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970–979.

Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast
Online Training with Frequency-Adaptive Learn-
ing Rates for Chinese Word Segmentation and New
Word Detection. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, pages 253–262.

Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic Conditional Ran-
dom Fields: Factorized Probabilistic Models for La-
beling and Segmenting Sequence Data. Journal of
Machine Learning Research, pages 693–723.

Charles Sutton. 2006. GRMM: GRaphical Models in
Mallet. In URL http://mallet. cs. umass. edu/grmm.

740



Stephanie Tellex, Boris Katz, Jimmy Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and Development in Information Retrieval, pages
41–47.

Aobo. Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2010. Perspectives on Crowdsourcing Annotations
for Natural Language Processing, journal = Lan-
guage Resources and Evaluation. pages 1–23.

Aobo Wang, Tao Chen, and Min-Yen Kan. 2012. Re-
tweeting From A Linguistic Perspective. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 46–55.

Kam-Fai Wong and Yunqing Xia. 2008. Normal-
ization of Chinese Chat Language. Language Re-
sources and Evaluation, pages 219–242.

Andi Wu and Zixin Jiang. 2000. Statistically-
Enhanced New Word Identification in A Rule-based
Chinese Aystem. In Proceedings of the second
workshop on Chinese Language Processing, pages
46–51.

Yunqing Xia and Kam-Fai Wong. 2006. Anomaly De-
tecting within Dynamic Chinese Chat Text. NEW
TEXT Wikis and blogs and other dynamic text
sources, page 48.

Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005.
NIL Is Not Nothing: Recognition of Chinese Net-
work Informal Language Expressions. In 4th
SIGHAN Workshop on Chinese Language Process-
ing, volume 5.

Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. Computational Linguistics and
Chinese Language Processing, pages 29–48.

Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics - Volume 2, pages 947–953.

Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. HHMM-based Chinese Lexical An-
alyzer ICTCLAS. In Proceedings of the second
SIGHAN workshop on Chinese language process-
ing, pages 184–187.

Hai Zhao and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character
Tagging for Word Segmentation and Named Entity
Recognition. In Proceedings of the Sixth SIGHAN
Workshop on Chinese Language Processing, pages
106–111.

741


