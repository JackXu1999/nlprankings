



















































On the Relation between Position Information and Sentence Length in Neural Machine Translation


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 328–338
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

328

On the Relation between Position Information and Sentence Length
in Neural Machine Translation

Masato Neishi
The University of Tokyo

neishi@tkl.iis.u-tokyo.ac.jp

Naoki Yoshinaga
Institute of Industrial Science,

the University of Tokyo
ynaga@iis.u-tokyo.ac.jp

Abstract

Long sentences have been one of the ma-
jor challenges in neural machine translation
(NMT). Although some approaches such as
the attention mechanism have partially reme-
died the problem, we found that the current
standard NMT model, Transformer, has diffi-
culty in translating long sentences compared
to the former standard, Recurrent Neural Net-
work (RNN)-based model. One of the key
differences of these NMT models is how the
model handles position information which is
essential to process sequential data. In this
study, we focus on the position information
type of NMT models, and hypothesize that rel-
ative position is better than absolute position.
To examine the hypothesis, we propose RNN-
Transformer which replaces positional encod-
ing layer of Transformer by RNN, and then
compare RNN-based model and four vari-
ants of Transformer. Experiments on ASPEC
English-to-Japanese and WMT2014 English-
to-German translation tasks demonstrate that
relative position helps translating sentences
longer than those in the training data. Further
experiments on length-controlled training data
reveal that absolute position actually causes
overfitting to the sentence length.

1 Introduction

Sequence to sequence models for neural machine
translation (NMT) are now utilized for various text
generation tasks including automatic summariza-
tion (Chopra et al., 2016; Nallapati et al., 2016;
Rush et al., 2015) and dialogue systems (Vinyals
and Le, 2015; Shang et al., 2015); the models are
required to take inputs of various length. Early
studies on recurrent neural network (RNN)-based
model analyze the translation quality with respect
to the sentence length, and show that their mod-
els improve translations for long sentences, using
the long short-term memory (LSTM) (Sutskever

et al., 2014) or introducing the attention mecha-
nism (Bahdanau et al., 2015; Luong et al., 2015).
However, Koehn and Knowles (2017) report that
even RNN-based model with the attention mecha-
nism performs worse than phrase-based statistical
machine translation (Koehn et al., 2007) in trans-
lating very long sentences, which challenges us
to develop an NMT model that is robust to long
sentences or more generally, variations in input
length.

Have the recent advances in NMT achieved the
robustness to the variations in input length? NMT
has been advancing by upgrading the model ar-
chitecture: RNN-based model (Cho et al., 2014;
Sutskever et al., 2014; Bahdanau et al., 2015; Lu-
ong et al., 2015) followed by convolutional neural
network (CNN)-based model (Kalchbrenner et al.,
2016; Gehring et al., 2017) and attention-based
model (Vaswani et al., 2017) called Transformer
(§ 2). Transformer is the de facto standard NMT
model today for its better performance compared
to the former standard RNN-based model. We
thus came up with a question whether Transformer
have acquired the robustness to the variations in
input length.

On the length of input sentence(s), the key dif-
ference between existing NMT models is how they
incorporate information on word positions in the
input. RNN or CNN-based NMT captures rela-
tive positions which stem from sequential opera-
tion of RNN or convolution operation of CNN. On
the other hand, position embeddings or positional
encodings (vector representations of positions) are
used to handle absolute positions in Transformer.
Gehring et al. (2017) integrate position embed-
dings, which are induced together with the other
model parameters, into the CNN-based model, and
showed that absolute position is still beneficial for
their model in addition to the relative position cap-
tured by CNN. By contrast, Transformer only em-



329

ploys positional encodings, which give fixed vec-
tors to positions using sine and cosine functions.

In this study, we suspect that these differences
in position information types of the models have
an impact on the accuracy of translating long sen-
tences, and investigate the impact of position in-
formation on translating long sentences to realize
an NMT model that is robust to variations in input
length. We reveal that RNN-based model (rela-
tive position) is better than Transformer with po-
sitional encodings (absolute position) in translat-
ing longer sentences than those in the training data
(§ 5.2). Motivated from this result, we propose a
simple modification to Transformer, using RNN as
relative positional encoder (§ 4).

Whereas RNN and CNN-based models are in-
separable from relative position inside of RNN or
CNN, Transformer allows us to change the po-
sition information type. We therefore compare
the RNN-based model and four variants of Trans-
former: vanilla Transformer, the modified Trans-
former using self-attention with relative positional
encodings (Shaw et al., 2018), our modified Trans-
former with RNN instead of positional encoding
layer, and a mixture of the last two models (§ 5).
On ASPEC English-to-Japanese and WMT2014
English-to-German translation tasks, we show that
relative information improves Transformer to be
more robust to variations in input length.

Our contribution is as follows:

• We identified a defect in Transformer. Use of
absolute position makes it difficult to trans-
late very long sentences.
• We proposed a simple method to incorporate

relative position into Transformer; it gives an
additive improvement to the existing model
by Shaw et al. (2018) which also incorporates
relative position.
• We revealed the overfitting property of Trans-

former to both short and long sentences.

2 Related Work

Early studies on NMT, at that time RNN-based
model, analyze the translation quality in terms of
sentence length (Sutskever et al., 2014; Bahdanau
et al., 2015; Luong et al., 2015), and a few studies
shed light on the details. Shi et al. (2016) examine
why RNN-based model generates translations of
the right length without special mechanism for the
length, and report how LSTM regulates the out-
put length. Koehn and Knowles (2017) reveal that

RNN-based model has lower translation quality on
very long sentences. Although researchers have
proposed various new NMT architecture, they usu-
ally evaluate their models only in terms of the
overall translation quality and rarely mention how
the translation has changed (Gehring et al., 2017;
Kalchbrenner et al., 2016; Vaswani et al., 2017).
Only a few studies do the analysis on the transla-
tion quality in terms of sentence length (Elbayad
et al., 2018; Zhang et al., 2019). The robustness
of the recent NMT models on very long sentences
remains to be assessed.

What we focus on in this study is the word
position information which will closely relate to
the decodable sentence length. Relative informa-
tion has been implicitly used in the models us-
ing RNN or CNN. Gehring et al. (2017) in-
troduce position embeddings which represent ab-
solute position information to their CNN-based
model. Sukhbaatar et al. (2015) introduce an-
other absolute position information, positional en-
codings, which need no parameter training, and
Vaswani et al. (2017) adopt them in their model,
Transformer, which has neither RNN nor CNN.

Recently, Shaw et al. (2018) propose to incor-
porate relative position into Transformer by mod-
ifying the self-attention layer while removing po-
sitional encodings. Lei et al. (2018) propose a fast
RNN named Simple Recurrent Units (SRU) and
replace the feed-forward layers of Transformer by
SRU considering that recurrent process would bet-
ter capture sequential information. Although both
approaches succeeded in improving BLEU score,
the researchers did not report in what respect the
models improved the translation.

Chen et al. (2018) propose a RNN-based model,
RNMT+, which is based on stacked LSTMs and
incorporates some components from Transformer
such as layer normalization and multi-head atten-
tion. On the other hand, our model is based on
Transformer and incorporates RNN into Trans-
former.

3 Preliminaries

3.1 Transformer

Transformer (Vaswani et al., 2017) is a sequence
to sequence model that has an encoder to pro-
cess and represent input sequence and a decoder
to generate output sequence from the encoder out-
puts. Both the encoder and decoder have a word
embedding layer, a positional encoding layer, and



330

Feed-forward

Positional encoding

Self-attention 

xN

Word embedding

Inputs

Outputs

(a) Transformer
(Vaswani et al., 2017)

Feed-forward

Self-attention
w/ relative position 

xN

Word embedding

Inputs

Outputs

(b) Rel-Transformer
(Shaw et al., 2018)

Feed-forward

RNN

Self-attention 

xN

Word embedding

Inputs

Outputs

(c) RNN-Transformer
Our proposed

Feed-forward

RNN

Self-attention
w/ relative position 

xN

Word embedding

Inputs

Outputs

(d) RR-Transformer
(b) + Our proposed

Figure 1: The architectures of all the Transformer-based models we compare in this study; for simplicity, we show
the encoder architectures here since the same modification is applied to their decoders.

stacked encoder/decoder layers. The encoder ar-
chitecture is shown in Figure 1a.

Word embedding layers encode input words
into continuous low-dimension vectors, followed
by positional encoding layers that add position in-
formation to them. Encoder/decoder layers con-
sist of a few sub-layers, self-attention layer, atten-
tion layer (decoder only) and feed-forward layer,
with layer normalization (Ba et al., 2016) for each.
Both self-attention layer and attention layer em-
ploy the same architecture, and we explain the de-
tails in § 3.3. Feed-forward layer consists of two
linear transformations with a ReLU activation in
between. As for the decoder, a linear transforma-
tion and a softmax function follow the stacked lay-
ers to calculate probabilities of words to output.

Figure 1 illustrates the architectures of all the
Transformer-based models we compare in this
study including our porposed model which will be
introduced in § 4. The model in Shaw et al. (2018)
modifies the self-attention layer (§ 3.3).

3.2 Word Position Information
Transformer has positional encoding layers which
follow the word embedding layers and capture ab-
solute position. The process of positional encod-
ing layer is to add positional encodings (position
vectors) to input word embeddings. The positional
encodings are generated using sinusoids of vary-
ing frequencies, which is designed to allow the
model to attend to relative positions from the pe-
riodicity of positional encodings (sinusoids). This
is in contrast to the position embeddings (Gehring
et al., 2017), a learned position vectors, which are
not meant to attend to relative positions. Vaswani

et al. (2017) report that both approaches produced
nearly identical results in their experiments, and
also mentioned that the model with positional en-
codings may handle longer inputs in testing than
those in training, which implies that absolute posi-
tion approach might have problems at this point.1

3.3 Self-attention with Relative Position

Some studies modify Transformer to consider rel-
ative position instead of absolute position. Shaw
et al. (2018) propose an extension of self-attention
mechanism which handles relative position inside
in order to incorporate relative position into Trans-
former. We hereafter refer to their model as Rel-
Transformer. In what follows, we explain the self-
attention mechanism and their extension.

Self-attention is a special case of general atten-
tion mechanism, which uses three elements called
query, key and value. The basic idea is to com-
pute weighted sum of values where the weights are
computed using the query and keys. Each weight
represents how much attention is paid to the cor-
responding value. In the case of self-attention, the
input set of vectors behaves as all of the three ele-
ments (query, key and value) using three different
transformations. When taking a sentence as input,
it is processed as a set in the self-attention.

Self-attention operation is to compute output
sequence z = (z1, ..., zn) out of input sequence
x = (x1, ..., xn), where both sequences have the
same langth n and xi ∈ Rdx , zi ∈ Rdz . The output

1Our preliminary experiment confirmed that positional
encodings perform better for longer sentences than those in
the training data, while position embeddings perform slightly
better for the other length.



331

element zi is computed as follows.

zi =
n∑

j=1

αij(xjW
V ) (1)

αij =
exp eij∑n
k=1 exp eik

(2)

eij =
xiW

Q(xjW
K)T√

dz
, (3)

where WQ,WK ,W V ∈ Rdx×dz are the matrices
that transform input elements into querys, keys,
and values, respectively.

The extension proposed by Shaw et al. (2018)
adds only two terms to the original self-attention:
the relative position vectors wKj−i, w

V
j−i ∈ Rdz .

zi =

n∑
j=1

αij(xjW
V + wVj−i) (4)

αij =
exp eij∑n
k=1 exp eik

(5)

eij =
xiW

Q(xjW
K + wKj−i)

T

√
dz

, (6)

Note that when using the relative position vectors,
the input is processed as a directed graph instead
of a set. Maximum distance k is employed to
clip the relative distance within a certain distance
so that the value of relative distance is limited as
−k < j − i < k.

4 RNN as a Relative Positional Encoding

The approach by Shaw et al. (2018) is not the only
way to incorporate relative position into Trans-
former. Lei et al. (2018) replace feed-forward lay-
ers by their proposed SRU which also incorpo-
rates relative position. Both approaches modify
the encoder and decoder layers that are repeatedly
stacked, which means their models handle position
information multiple times. However, the origi-
nal Transformer does only once at the positional
encoding layer which locates shallow layer of the
deep layered network.

To conduct a clear comparison of the posi-
tion information types, we propose another simple
method that replaces the positional encoding layer
of Transformer by RNN. As the RNN has the na-
ture to handle a sequence using relative position
information, it can be used not only as a main pro-
cessing unit of RNN-based model, but also as a
relative positional encoder. While Lei et al. (2018)
also employ RNN, they use position embeddings.

Our approach is a pure replacement of position in-
formation type for Transformer.

In the original Transformer, the positional en-
coding layer adds the i-th position vector pe(i) ∈
Rdwv to the i-th input word vector wvi ∈ Rdwv
and outputs the position informed word vector
wv′i ∈ Rdwv :

wv′i = wvi + pe(i) (7)

In our approach, we adopt RNN, specifically
GRU (Cho et al., 2014) in this study, as a relative
positional encoder. GRU computes its output or
its i-th time hidden state hi ∈ Rdwv given the in-
put word vector wvi and the previous hidden state
hi−1 ∈ Rdwv , and we take hi as the position in-
formed word vector wv′i:

hi = GRU(wvi, hi−1) (8)

wv′i = hi (9)

Although LSTM (Hochreiter and Schmidhuber,
1997) is more often used as an RNN module in
RNN-based models, we employed GRU which has
less parameters. This is because, in our approach,
RNN is just a positional encoder which we do not
expect to work more, even though it can. We refer
to our proposed model as RNN-Transformer.

We also consider the mixture of Shaw et al.
(2018) and our method to investigate whether the
two methods of considering relative position have
additive improvements. Although both methods
are intended to incorporate relative position into
Transformer, they modify different parts of Trans-
former. By combining both, we can see either of
modification suffices to incorporate relative posi-
tion. We refer to this model as RR-Transformer.

5 Experiments

We conduct two experiments to evaluate our mod-
ification to Transformer and to investigate the im-
pact of using relative position in NMT models.
The first experiment is a basic translation experi-
ment which uses all the training data. We carry out
analysis on the translations generated by the NMT
models in terms of sentence length, especially fo-
cusing on long sentences. In the second experi-
ment, we control the training data by the sentence
length so that the NMT models are trained only
on sentences with lengths in a certain range. We
also analyze the result in terms of sentence length,
focusing on the short sentences.



332

Train (orig.) Dev Test

ASPEC (En-Ja) 1,166,725 (3,008,500) 1790 1812
WMT2014 (En-De) 3,661,035 (4,468,840) 3000 2737

Table 1: Number of sentence pairs in the preprocessed
corpus.

5.1 Setup

Dataset and Preprocess: We perform a se-
ries of experiments on English-to-Japanese and
English-to-German translation tasks. For English-
to-Japanese translation task, we exploit ASPEC
(Nakazawa et al., 2016), a parallel corpus com-
piled from abstract sections of scientific papers.
For English-to-German translation task, we ex-
ploit a dataset in WMT2014, which is one of the
most common dataset for translation task.

For ASPEC English-to-Japanese data, we used
scripts of Moses toolkit2(ver. 2.2.1) (Koehn et al.,
2007) for English tokenization and truecasing,
and KyTea3 (ver. 0.4.2) (Neubig et al., 2011) for
Japanese segmentations. Following those word-
level preprocess, we further applied Sentence-
Piece (Kudo and Richardson, 2018) to segment
texts down to subword level with shared vocab-
ulary size of 16,000. Finally we selected the first
1,500,000 sentence pairs for the poor quality of
the latter part, and filtered out sentence pairs with
more than 49 subwords in either of the languages.

For WMT2014 English-to-German translation
task, we used preprocessed data provided from
the Stanford NLP Group,4 and used newstest2013
and newstest2014 as development and test data,
respectively. We also applied SentencePiece to
this data to segment into subwords with shared vo-
cabulary size of 40,000. We filtered out the sen-
tence pairs in the same way as the ASPEC. Ta-
ble 1 shows the number of sentence pairs of pre-
processed data.

Figure 2 shows the distributions of the sentences
plotted against the length of input sentence. Al-
thought ASPEC data has slightly larger peak at
sentence length of 20-29 subwords, both datasets
have no big difference in length distributions. The
training and test data have almost identical curves.

Model: We compare the following five NMT
models:

2http://www.statmt.org/moses/
3http://www.phontron.com/kytea/
4https://nlp.stanford.edu/projects/

nmt/

0-9
10

-19
20

-29
30

-39
40

-49
50

-59
60

-69
70

-79
80

-89
90

-99
10

0-1
09

11
0-1

19
12

0-1
29

13
0-1

39

0

20

40
ASPEC (En-Ja) Train
ASPEC (En-Ja) Test

0-9
10

-19
20

-29
30

-39
40

-49
50

-59
60

-69
70

-79
80

-89
90

-99
10

0-1
09

11
0-1

19
12

0-1
29

0

20

40
WMT2014 (En-De) Train
WMT2014 (En-De) Test

Sentence length (source, subword count)

Pe
rc

en
t

Figure 2: Sentence length ratio of preprocessed corpus.

RNN-NMT is a RNN-based NMT model with
dot-attention and input-feeding (Luong et al.,
2015). This model consists of four layered
bi-directional LSTM for encoder and three
layered uni-directional LSTM for decoder.

Transformer is a vanilla Transformer model (the
base model in Vaswani et al. (2017)). This
model consists of six-layered Transformer
encoder and decoder.

Rel-Transformer is a modified version of Trans-
former by Shaw et al. (2018). Since the
modifications do not increase the number of
model parameter much, this model consists
of the same number of encoder/decoder lay-
ers as Transformer, with the modified self-
attention layer. We set the hyperparameter
k, relative distance limit, to 16 following the
base model in Shaw et al. (2018).

RNN-Transformer is another modified version
of Transformer proposed in § 4. Because the
replacement of the original positional encod-
ing layer with RNN increases the number of
model parameter, we employ uni-directional
GRU as relative positional encoder for both
encoder and decoder and reduced the num-
ber of decoder layer instead. Our RNN-
Transformer model consists of six layered en-
coder and five layered decoder with one GRU
layer for each.5

RR-Transformer is the mixture model of Rel-
Transformer and RNN-transformer. With the
same logic as Rel-Transformer, this model

5This configuration was chosen because it performed bet-
ter than a model with five-layered encoder and six-layered
decoder, and was comparable to five-layred encoder and de-
coder with bi-directional (instead of uni-directional) GRU for
the relative position encoder in preliminary experiments.

http://www.statmt.org/moses/
http://www.phontron.com/kytea/
https://nlp.stanford.edu/projects/nmt/
https://nlp.stanford.edu/projects/nmt/


333

ASPEC (En-Ja) WMT2014 (En-De)

RNN-NMT 70,521,476 107,409,476
Transformer 68,736,644 105,624,644
Rel-Transformer 68,787,332 105,675,332
RNN-Transformer 67,684,484 104,572,484
RR-Transformer 67,730,948 104,618,948

Table 2: Number of the model parameters.

consists of the same number of encoder and
decoder layers as RNN-Transformer model,
with the modified self-attention layer.

We implemented all the models using PyTorch6

(ver. 0.4.1). Taking the base model of Trans-
former (Vaswani et al., 2017) which consists of
six-layered encoder and decoder as a reference
model, we built the other models to have almost
the same number of model parameters for a fair
comparison. For all models, we set word embed-
ding dimension and model dimension (or hidden
size for RNNs) to 512. For the Transformer-based
models, we set feed-forward layer dimension to
2048, and the number of attention head to 8.

Table 2 shows the total number of model param-
eters for all the models in our implementation. The
difference of the numbers by the datasets comes
from the difference in vocabulary size.

Training: We used Adam optimizer (Kingma
and Ba, 2015) with initial learning rate of 0.0001,
and set dropout rate of 0.2 and gradient clip-
ping value of 3.0. We adopted warm-up strategy
(Vaswani et al., 2017) for fast convergence with
warm-up step of 4k, and trained all the model for
300k steps. The mini-batch size was set to 128.

Evaluation: We performed greedy search for
translation with the models, and evaluated the
translation quality in terms of BLEU score (Pa-
pineni et al., 2002) using multi-bleu.perl
in the Moses toolkit. We checked model’s BLEU
score on the development data at every 10k steps
during the training, and took the best performing
model for evaluation on the test data.

5.2 Long Sentence Translation

Table 3 shows the BLEU scores of the NMT mod-
els on the test data of ASPEC English-to-Japanese
and WMT2014 English-to-German when using all
the preprocessed training data for training. Ta-
ble 4 lists the results of statistical significance

6https://pytorch.org/

ASPEC (En-Ja) WMT2014 (En-De)
newstest2014

RNN-NMT 36.67 19.95
Transformer 38.44 21.00
Rel-Transformer 39.58 22.51
RNN-Transformer 39.17 22.35
RR-Transformer 40.34 23.01

Table 3: BLEU scores on test data.

RNN-NMT Trans Rel RNN RR

RNN-NMT << << << <<
Trans >> << << <<
Rel >> >> ∼ <

RNN >> >> ∼ <<
RR >> >> >> >>

Table 4: Results of statistical significance test on AS-
PEC English-to-Japanese (lower-left) and WMT2014
English-to-German (upper-right): “>>” or “<<”
means p < 0.01, “>” or “<” means p < 0.05 and
”∼” means p ≥ 0.05.

test using bootstrapping of 10,000 samples. The
evaluation is done on word-level, which means
that we converted the outputs of NMT mod-
els from subword-level into word-level before
scoring. On both datasets, Transformer outper-
forms RNN-NMT, and all of the three modified
versions of Transformer outperform the Trans-
former. RNN-Transformer was comparable to
Rel-Transformer, and RR-Transformer, the mix-
ture of RNN-Transformer and Rel-Transformer,
gives the best score.

In order to see the capability of translating long
sentences of the models, we split the test data
into different bins according to the length of in-
put sentences, and then calculated BLEU scores
on each bin. The following evaluation uses the
raw subword-level outputs of the models since the
sentence length is based on subwords.

Figure 3a and 3b show the BLEU scores on the
split test data of ASPEC English-to-Japanese and
WMT2014 English-to-German, respectively. The
BLEU score of Transformer, the only model that
uses absolute position, more sharply drops than
the BLEU scores of the other models at the in-
put length of 50-59, which is outside of the length
range of the training data. As for the input length
of 60-, Transformer performs the worst among all
the models. These results indicate that relative po-
sition works better than absolute position in trans-
lating sentences longer than those of the training
data. Meanwhile, for the lengths with enough

https://pytorch.org/


334

0-9 10-19 20-29 30-39 40-49 50-59 60-
Sentence length (tokens)

10

20

30

40
BL

EU RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(a) ASPEC English-to-Japanese

0-9 10-19 20-29 30-39 40-49 50-59 60-
Sentence length (tokens)

0

10

20

30

BL
EU RNN-NMT

Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(b) WMT2014 English-to-German

Figure 3: BLEU scores on test data split by the sentence length (no training data in the gray-colored area).

0-9 10-19 20-29 30-39 40-49 50-59 60-
Sentence length (tokens)

30

20

10

0

Di
ffe

re
nc

e 
(to

ke
ns

)

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(a) ASPEC English-to-Japanese

0-9 10-19 20-29 30-39 40-49 50-59 60-
Sentence length (tokens)

30

20

10

0

Di
ffe

re
nc

e 
(to

ke
ns

)

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(b) WMT2014 English-to-German

Figure 4: Averaged difference of sentence length between NMT model’s output and the reference translation (no
training data in the gray-colored area).

0 10 20 30 40 50 60 70 80
0

20
Transformer
RR-Transformer

0 10 20 30 40 50 60 70 80
0

20

Output sentence length (tokens)

Pe
rc

en
t

(src sent length: 40-49)

(src sent length: 50-59)

(a) ASPEC English-to-Japanese

0 10 20 30 40 50 60 70 80
0

20
Transformer
RR-Transformer

0 10 20 30 40 50 60 70 80
0

20

Output sentence length (tokens)

Pe
rc

en
t

(src sent length: 40-49)

(src sent length: 50-59)

(b) WMT2014 English-to-German

Figure 5: Distributions of output sentence length of Transformer and RR-Transformer.

amount of training data, both position information
types seem to work almost equally. On WMT2014
English-to-German, all the models except Trans-
former successfully keep as good performance in
50-59 and 60- bins as the other bins.

To figure out the effect of position information
on the ability of the models to generate output of
proper length, we look into the difference of sen-
tence length between the model’s output and the
reference translation. Figure 4a and 4b show the
averaged differences plotted against the input sen-
tence length on both language pairs. We can ob-

serve that all the models tend to output shorter sen-
tence than the reference. However, Transformer
shows the largest drop at the input length of 50-59
again among all the models, which is even more
than RNN-NMT. The difference between Trans-
former and RNN-Transformer indicates the advan-
tage of relative position against absolute position,
while the difference between the three modified
Transformer-based models and RNN-NMT indi-
cates the structural advantage of Transformer to
RNN-based model in generating translations with
appropriate lengths.



335

min len. max len. # of sentences # of tokens

Short 2 26 555,922 10,392,775
Middle 26 34 350,176 10,392,797
Long 34 49 260,626 10,392,729

(a) ASPEC English-to-Japanese

min len. max len. # of sentences # of tokens

Short 1 24 1,878,354 29,841,533
Middle 24 34 1,041,794 29,841,531
Long 34 49 740,887 29,841,519

(b) WMT2014 English-to-German

Table 5: Statistics of the split training data.

The above result that the models tend to out-
put shorter sentences suggests that the models may
have a limit in the range of output length. To con-
firm this possibility, we look into the distributions
of the model’s output length. Figure 5a and 5b
show distributions of output length of Transformer
and RR-Transformer for the input length of 40-49
(length within the training data) and 50-59 (length
outside of the training data). For the input length
of 40-49, the distributions of both models are flat
and have no big difference. For the input length
of 50-59, on the other hand, we can see a sharp
peak in the distribution of Transformer in which
most of the values distribute around 50 tokens or
less. These results indicate that Transformer tends
to overfit to a range of length of input sentences.

5.3 Length-Controlled Training Data

The above experiments focus on trainslation of
long sentences, or, strictly speaking, sentences
longer than those in the training data. With the
use of absolute position, it is no surprise that the
model fails to handle longer sentences since those
sentences demand the model to handle the position
vectors which are never seen during training.

In this section, we focus on short sentences
to investigate whether Transformer overfits to the
length of input sentences in the training data. Note
that position vectors of small numbers are in-
cluded in long sentences. If the problem is only
unseen position vectors, then the model shall be
able to handle short sentences because short sen-
tences do not include any unseen position num-
bers.

To figure out how the NMT models behave
on sentences shorter than those in the training
data, we conduct another experiment in which
the length of the training data is controlled. We
split the training data of both ASPEC English-to-
Japanese and WMT2014 English-to-German into
three portions according to the length of input sen-
tences so that each of them has almost the same
number of tokens. We then trained the five NMT

models on each of the three training data. We here-
after refer to these three length-controlled training
data as Short, Middle and Long. The statistics of
these data is summarized in Table 5a and 5b.

To see how the translation quality changes be-
tween inside and outside of the length within
the training data, we split the test data with re-
spect to the lengths of split training data. Fig-
ure 6a and 6b show the BLEU scores on all the
three training data of both language pairs. Trans-
former shows the worst performance among the
four Transformer-based models on the sentences
longer than those in the training data for any con-
trolled length. However, on the shorter sentences
than those in the training data, RNN-Transformer
scores almost the same as Transformer on the Mid-
dle and Long training data of ASPEC English-to-
Japanese and also shows a larger drop than RNN-
NMT at length of -24 on the Long training data
of WMT2014 English-to-German. This implies
that our proposed method to replace absolute posi-
tional encoding layer by RNN does not work well
in translating shorter sentences.

We can also see that Rel-Transformer and RR-
Transformer are quite competitive across all the
situations. This suggests that one Transformer
decoder layer and two GRUs contribute almost
equally to the translation quality.

Figure 7a and 7b show the averaged difference
of length between NMT model’s output and the
reference translation on Long training data of both
datasets.7 These figures indicate that Transformer
and RNN-Transformer tend to generate inappro-
priately long sentences in translating much shorter
sentences than those in the training data. As men-
tioned above, when translating short sentences,
there is no unseen positions in Transformer, while
there is no concrete position representation in
RNN-Transformer; the above results suggest that
these two models overfit to the (longer) length of
input sentences. In contrast, the result of Rel-

7Note that Figure 7a and 7b use different x-axis scale from
Figure 6a and 6b in order to show the difference clearly.



336

0-26 27-34 35-49 50-
Sentence length (tokens)

10

20

30

40
BL

EU
(i) Short

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

0-26 27-34 35-49 50-
Sentence length (tokens)

10

20

30

40

BL
EU

(ii) Middle

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

0-26 27-34 35-49 50-
Sentence length (tokens)

10

20

30

40

BL
EU

(iii) Long

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(a) ASPEC English-to-Japanese

0-24 25-34 35-49 50-
Sentence length (tokens)

0

10

20

30

BL
EU

(i) Short

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

0-24 25-34 35-49 50-
Sentence length (tokens)

0

10

20

30

BL
EU

(ii) Middle

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

0-24 25-34 35-49 50-
Sentence length (tokens)

0

10

20

30

BL
EU

(iii) Long

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(b) WMT2014 English-to-German

Figure 6: BLEU scores of models trained on three length-controlled training data on test data split in the same way
as the training data (almost no training data in the gray-colored area).

0-9 10-19 20-29 30-39 40-49 50-59 60-
Sentence length (tokens)

30

20

10

0

10

Di
ffe

re
nc

e 
(to

ke
ns

)

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(a) ASPEC English-to-Japanese

0-9 10-19 20-29 30-39 40-49 50-59 60-
Sentence length (tokens)

30

20

10

0

10

Di
ffe

re
nc

e 
(to

ke
ns

)

RNN-NMT
Transformer
Rel-Transformer
RNN-Transformer
RR-Transformer

(b) WMT2014 English-to-German

Figure 7: Averaged difference of sentence length between NMT model’s output translation and reference transla-
tion (almost no training data in the gray-colored area).

Transformer and RR-Transformer indicates that
self-attention with relative position prevents this
overfitting.

6 Conclusions

In this paper, we examined the relation between
position information and the length of input sen-
tences by comparing absolute position and relative
position using RNN-based model and variations of
Transformer models. Experiments on all the pre-
processed training data revealed the crucial weak-
ness of the original Transformer, which uses abso-
lute position, in translating sentences longer than
those of the training data. We also confirmed that
incorporating relative position into Transformer
helps to handle those long sentences and improves
the translation quality. Another experiment on the

length-controlled training data revealed that abso-
lute position of Transformer causes overfitting to
the input sentence length. To conclude, all the ex-
periments suggest to use relative position and not
to use absolute position.

Considering that the available data is not bal-
anced in terms of the sentence length in practice,
preventing the overfitting is useful for building a
practical NMT system.

Acknowledgments

We deeply thank Satoshi Tohda for proofread-
ing the draft of our paper. This work was par-
tially supported by JST CREST Grant Number JP-
MJCR19A4, Japan. This research was also par-
tially supported by NII CRIS Contract Research
2019.



337

References
Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.

2016. Layer normalization. CoRR, abs/1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the third International Conference on Learning Rep-
resentations (ICLR).

Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin
Johnson, Wolfgang Macherey, George Foster, Llion
Jones, Mike Schuster, Noam Shazeer, Niki Parmar,
Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,
Zhifeng Chen, Yonghui Wu, and Macduff Hughes.
2018. The best of both worlds: Combining recent
advances in neural machine translation. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 76–86.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with
attentive recurrent neural networks. In Proceed-
ings of the 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 93–98.

Maha Elbayad, Laurent Besacier, and Jakob Verbeek.
2018. Pervasive attention: 2D convolutional neural
networks for sequence-to-sequence prediction. In
Proceedings of the 22nd Conference on Computa-
tional Natural Language Learning (CoNLL), pages
97–107.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convolu-
tional sequence to sequence learning. In Proceed-
ings of the 34th International Conference on Ma-
chine Learning (ICML), pages 1243–1252.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
Aäron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. CoRR, abs/1610.10099.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the third International Conference on Learning
Representations (ICLR).

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL): Demo
and Poster Sessions, pages 177–180.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39.

Taku. Kudo and John Richardson. 2018. Sentence-
piece: A simple and language independent subword
tokenizer and detokenizer for neural text process-
ing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing
(EMNLP): System Demonstrations, pages 66–71.

Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav
Artzi. 2018. Simple recurrent units for highly par-
allelizable recurrence. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 4470–4481.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1412–1421.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. ASPEC:
Asian scientific paper excerpt corpus. In Pro-
ceedings of the tenth International Conference on
Language Resources and Evaluation (LREC 2016),
pages 2204–2208.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Çağlar Guçehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
RNNs and beyond. In Proceedings of the 20th
SIGNLL Conference on Computational Natural
Language Learning (CoNLL), pages 280–290.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-HLT), Short Papers, pages 529–533.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311–318.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015

http://arxiv.org/abs/1607.06450
https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1409.0473
https://doi.org/10.18653/v1/P18-1008
https://doi.org/10.18653/v1/P18-1008
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.18653/v1/N16-1012
https://doi.org/10.18653/v1/N16-1012
https://doi.org/10.18653/v1/K18-1010
https://doi.org/10.18653/v1/K18-1010
http://proceedings.mlr.press/v70/gehring17a.html
http://proceedings.mlr.press/v70/gehring17a.html
http://arxiv.org/abs/1610.10099
http://arxiv.org/abs/1610.10099
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
https://www.aclweb.org/anthology/P07-2045
https://www.aclweb.org/anthology/P07-2045
https://doi.org/10.18653/v1/W17-3204
https://doi.org/10.18653/v1/W17-3204
https://doi.org/10.18653/v1/D18-2012
https://doi.org/10.18653/v1/D18-2012
https://doi.org/10.18653/v1/D18-2012
https://doi.org/10.18653/v1/D18-2012
https://doi.org/10.18653/v1/D18-1477
https://doi.org/10.18653/v1/D18-1477
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
https://www.aclweb.org/anthology/L16-1350
https://www.aclweb.org/anthology/L16-1350
https://doi.org/10.18653/v1/K16-1028
https://doi.org/10.18653/v1/K16-1028
https://doi.org/10.18653/v1/K16-1028
https://www.aclweb.org/anthology/P11-2093
https://www.aclweb.org/anthology/P11-2093
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.18653/v1/D15-1044
https://doi.org/10.18653/v1/D15-1044


338

Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 379–389.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (ACL-IJCNLP), pages 1577–
1586.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT), Short Papers, pages 464–
468.

Xing Shi, Kevin Knight, and Deniz Yuret. 2016. Why
neural translations are the right length. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
2278–2282.

Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and
Rob Fergus. 2015. End-to-end memory networks.
In Advances in Neural Information Processing Sys-
tems (NIPS) 28, pages 2440–2448.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems (NIPS) 27, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems (NIPS) 30, pages 5998–6008.

Oriol Vinyals and Quoc V. Le. 2015. A neural conver-
sational model. In Proceedings of Deep Learning
Workshop held at the 31st International Conference
on Machine Learning (ICML).

Wen Zhang, Yang Feng, Fandong Meng, Di You, and
Qun Liu. 2019. Bridging the gap between train-
ing and inference for neural machine translation. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
4334–4343.

https://doi.org/10.3115/v1/P15-1152
https://doi.org/10.3115/v1/P15-1152
https://doi.org/10.18653/v1/N18-2074
https://doi.org/10.18653/v1/N18-2074
https://doi.org/10.18653/v1/D16-1248
https://doi.org/10.18653/v1/D16-1248
http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf
https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://sites.google.com/site/deeplearning2015/36.pdf?attredirects=0
https://sites.google.com/site/deeplearning2015/36.pdf?attredirects=0
https://doi.org/10.18653/v1/P19-1426
https://doi.org/10.18653/v1/P19-1426

