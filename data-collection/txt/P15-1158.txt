



















































Why discourse affects speakers' choice of referring expressions


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1639–1649,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Why discourse affects speakers’ choice of referring expressions

Naho Orita
Graduate School of Information Sciences

Tohoku University
naho@ecei.tohoku.ac.jp

Eliana Vornov
Computer Science and Linguistics

University of Maryland
evornov@umd.edu

Naomi H. Feldman
Linguistics and UMIACS
University of Maryland

nhf@umd.edu

Hal Daumé III
Computer Science and UMIACS

University of Maryland
hal@umiacs.umd.edu

Abstract

We propose a language production model
that uses dynamic discourse information
to account for speakers’ choices of refer-
ring expressions. Our model extends pre-
vious rational speech act models (Frank
and Goodman, 2012) to more naturally dis-
tributed linguistic data, instead of assuming
a controlled experimental setting. Simula-
tions show a close match between speakers’
utterances and model predictions, indicat-
ing that speakers’ behavior can be modeled
in a principled way by considering the prob-
abilities of referents in the discourse and
the information conveyed by each word.

1 Introduction

Discourse information plays an important role in
various aspects of linguistic processing, such as
predictions about upcoming words (Nieuwland and
Van Berkum, 2006) and scalar implicature process-
ing (Breheny et al., 2006). The relationship be-
tween discourse information and speakers’ choices
of referring expression is one of the most studied
problems. Speakers’ choices of referring expres-
sions have long been thought to depend on the
salience of entities in the discourse (Givón, 1983).
For example, speakers normally do not choose a
pronoun to refer to a new entity in the discourse,
but are more likely to use pronouns for referents
that have been referred to earlier in the discourse.
A number of grammatical, semantic, and distribu-
tional factors related to salience have been found to

influence choices of referring expressions (Arnold,
2008). While the relationship between discourse
salience and speakers’ choices of referring expres-
sions is well known, there is not yet a formal ac-
count of why this relationship exists.

In recent years, a number of formal models have
been proposed to capture inferences between speak-
ers and listeners in the context of Gricean prag-
matics (Grice, 1975; Frank and Goodman, 2012).
These models take a game theoretic approach in
which speakers optimize productions to convey in-
formation for listeners, and listeners infer meaning
based on speakers’ likely productions. These mod-
els have been argued to account for human commu-
nication (Jager, 2007; Frank and Goodman, 2012;
Bergen et al., 2012a; Smith et al., 2013), and stud-
ies report that they robustly predict various linguis-
tic phenomena in experimental settings (Goodman
and Stuhlmüller, 2013; Degen et al., 2013; Kao et
al., 2014; Nordmeyer and Frank, 2014). However,
these models have not yet been applied to language
produced outside of the laboratory, nor have they
incorporated measures of discourse salience that
can be computed over corpora.

In this paper, we propose a probabilistic model
to explain speakers’ choices of referring expres-
sions based on discourse salience. Our model ex-
tends the rational speech act model from Frank
and Goodman (2012) to incorporate updates to lis-
teners’ beliefs as discourse proceeds. The model
predicts that a speaker’s choice of referring expres-
sions should depend directly on the amount of in-
formation that each word carries in the discourse.
Simulations probe the contribution of each model
component and show that the model can predict

1639



speakers’ pronominalization in a corpus. These
results suggest that this model formalizes underly-
ing principles that account for speakers’ choices of
referring expressions.

The paper is organized as follows. Section 2
reviews relevant studies on choices of referring ex-
pressions. Section 3 describes the details of our
model. Section 4 describes the data, preprocessing
and annotation procedure. Section 5 presents simu-
lation results. Section 6 summarizes this study and
discusses implications and future directions.

2 Relevant Work

2.1 Discourse salience

Speakers’ choices of referring expressions have
long been an object of study. Pronominalization
has been examined particularly often in both theo-
retical and experimental studies. Discourse theories
predict that speakers use pronouns when they think
that a referent is salient in the discourse (Givón,
1983; Ariel, 1990; Gundel et al., 1993; Grosz et
al., 1995), where salience of the referent is influ-
enced by various factors such as grammatical posi-
tion (Brennan, 1995), recency (Chafe, 1994), top-
icality (Arnold, 1998), competitors (Fukumura et
al., 2011), visual salience (Vogels et al., 2013b),
and so on.

Discourse theories have characterized the link
between referring expressions and discourse
salience by stipulating constructs such as a scale
of topicality (Givón, 1983), accessibility hierarchy
(Ariel, 1990), or implicational hierarchy (Gundel et
al., 1993). All of these assume fixed form-salience
correspondences in that a certain referring expres-
sion encodes a certain degree of salience. However,
it is not clear how this form-salience mapping holds
nor why it should be.

There is also a rich body of research that points
to the importance of production cost (Rohde et al.,
2012; Bergen et al., 2012b; Degen et al., 2013)
and listener models (Bard et al., 2004; Van der
Wege, 2009; Galati and Brennan, 2010; Fukumura
and van Gompel, 2012) in language production.
These studies suggest that only considering dis-
course salience of the referent may not precisely
capture speakers’ choices of referring expressions,
and it is necessary to examine discourse salience in
relation to these other factors.

2.2 Formal models

Computational models relevant to speakers’
choices of referring expressions have been pro-
posed, but there is a gap between questions that
previous models have addressed and the questions
that we have raised above.

Grüning and Kibrik (2005) and Khudyakova et
al. (2011) examine the significance of various fac-
tors that might influence choices of referring ex-
pressions by using machine learning models such
as neural networks, logistic regression and decision
trees. Although these models qualitatively show
some significant factors, they are data-driven rather
than being explanatory, and have not focused on
why and how these factors result in the observed
referring choices.

Formal models that go beyond identifying super-
ficial factors focus on only pronouns rather than
accounting for speakers’ word choices per se. For
example, Kehler et al. (2008) formalize a relation-
ship between pronoun comprehension and produc-
tion using Bayes’ rule to account for comprehen-
der’s semantic bias in experimental data. Rij et al.
(2013) use ACT-R (Anderson, 2007) to examine
the effects of working memory load in pronoun
interpretation. These models show how certain fac-
tors influence pronoun production/interpretation,
but it is not clear how these models would predict
speakers’ choices of referring expressions.

Relevant formal models in computational lin-
guistics include Centering theory (Grosz et al.,
1995; Poesio et al., 2004) and Referring Expres-
sion Generation (Krahmer and Van Deemter, 2012).
These models propose deterministic constraints
governing when pronouns are preferred in local dis-
course, but it is not clear how these would account
for speakers’ choices of referring expressions, nor
it is clear why there should be such deterministic
constraints.

2.3 Uniform Information Density

One potential formal explanation for the relation
between discourse salience and speakers’ choices
of referring expressions is the Uniform Informa-
tion Density hypothesis (UID) (Levy and Jaeger,
2007; Tily and Piantadosi, 2009; Jaeger, 2010).
UID states that speakers prefer to smooth the in-
formation density distribution of their utterances
over time to achieve optimal communication. This
theory predicts that speakers should use pronouns
instead of longer forms (e.g., the president) when a

1640



referent is predictable in the context, whereas they
should use longer forms for unpredictable referents
that carry more information (Jaeger, 2010).

Tily and Piantadosi (2009) empirically exam-
ined the relationship between predictability of a
referent and choice of referring expressions. They
found that predictability is a significant predictor
of writers’ choices of referring expressions, in that
pronouns are used when a referent is predictable.

While these results appear to support UID, there
are several inconsistencies with previous UID ac-
counts. Information content of words has been
estimated using an n-gram language model (Levy
and Jaeger, 2007), a verb’s subcategorization fre-
quency (Jaeger, 2010), and so on, whereas here
the information content is that of referents with
respect to discourse salience. In addition, selecting
between a pronoun and a more specified referring
expression involves deciding how much informa-
tion to convey, whereas previous applications of
UID (Levy and Jaeger, 2007) have been concerned
with deciding between different ways of expressing
the same information content. We show in the next
section that we can derive predictions about refer-
ring expressions directly from a model of language
production.

2.4 Summary

Previous linguistic studies have focused on identi-
fying factors that might influence choices of refer-
ring expressions. However, it is not clear from this
previous work how and why these factors result
in the observed patterns of referring expressions.
Where formal models relevant to this topic do exist,
they have not been built to explain why there is a
relation between discourse salience and speakers’
choices of referring expressions. Even UID, which
relates predictability to word length, is not set up
to account for the choice between words that vary
in their information content.

In the next section, we propose a speaker model
that formalizes the relation between discourse
salience and speakers’ choices of referring expres-
sions, considering production cost and speakers’
inference about listeners in a principled and ex-
planatory way.

3 Speaker model

3.1 Rational speaker-listener model

We adopt the rational speaker-listener model from
Frank and Goodman (2012) and extend this model

to predict speakers’ choices of referring expres-
sions using discourse information.

The main idea of Frank and Goodman’s model
is that a rational pragmatic listener uses Bayesian
inference to infer the speaker’s intended referent
rs given the word w, their vocabulary (e.g., ‘blue’,
‘circle’), and shared context that consists of a set
of objects O (e.g., visual access to object referents)
as in (1), assuming that a speaker has chosen the
word informatively.

P (rs|w,O) = PS(w|rs, O)P (rs)Σr′∈OP (w|r′, O)P (r′) (1)

While our work does not make use of this pragmatic
listener, it does build on the speaker model assumed
by the pragmatic listener. This speaker model (the
likelihood term in the listener model) is defined
using an exponentiated utility function as in (2).

PS(w|rs, O) ∝ eαU(w;rs,O) (2)
The utility U(w; rs, O) is defined as I(w; rs, O)−
D(w), where I(w; rs, O) represents informative-
ness of word w (quantified as surprisal) and D(w)
represents its speech cost. If a listener interprets
word w literally and cost D(w) is constant, the ex-
ponentiated utility function can be reduced to (3)
where |w| denotes the number of referents that the
word w can be used to refer to.

PS(w|rs, O) ∝ 1|w| (3)

Thus, the speaker model chooses a word based on
its specificity. We show in the next section that
this corresponds to a speaker who is optimizing
informativeness for a listener with uniform beliefs
about what will be referred to in the discourse. The
assumption of uniform discourse salience works
well in a simple language game where there are
a limited number of referents that have roughly
equal salience, but we show that a model that lacks
a sophisticated notion of discourse falls short in
more realistic settings.

3.2 Incorporating discourse salience
To extend Frank and Goodman’s model to a natu-
ral linguistic situation, we assume that the speaker
estimates the listener’s interpretation of a word (or
referring expression)w based on discourse informa-
tion. We extend the speaker model from (3) by as-
suming that a speaker S choosesw to optimize a lis-
tener’s belief in speaker’s intended referent r rela-
tive to the speaker’s own speech cost Cw. This cost

1641



is another factor in the speaker model, roughly cor-
responding to utterance complexity such as word
length.1

PS(w|r) ∝ PL(r|w) · 1
Cw

(4)

The term PL(r|w) in (4) represents informative-
ness of word w: the speaker chooses w that most
helps a listener L to infer referent r. The term Cw
in (4) is a cost function: the speaker chooses w that
is least costly to speak.

The speaker’s listener model, PL(r|w), infers
referent r that is referred to by word w according
to Bayes’ rule as in (5).

PL(r|w) = P (w|r)P (r)Σr′P (w|r′)P (r′) (5)

The first term in the numerator, P (w|r), is a word
probability: the listener in the speaker’s mind
guesses how likely the speaker would be to usew to
refer to r. The second term in the numerator, P (r),
is the discourse salience (or predictability) of refer-
ent r. The denominator Σr′P (w|r′)P (r′) is a sum
of potential referents r′ that could be referred to by
word w. The terms in this sum are non-zero only
for referents that are compatible with the meaning
of the word. If there are many potential referents
that could be referred to by word w, that word
would be more ambiguous thus less informative.
The whole of the right side in Equation (5) repre-
sents the speaker’s assumption about the listener:
given word w the listener would infer referent r
that is salient in a discourse and less ambiguously
referred to by word w.

If P (r) is uniform over referents and P (w|r) is
constant across words and referents, this listener
model reduces to 1|w| . Thus, Frank and Goodman
(2012)’s speaker model in (3) is a special case of
our speaker model in (4) that assumes uniform
discourse salience and constant cost.

Our model predicts that the speaker’s probability
of choosing a word for a given referent should
depend on its cost relative to its information content.
To see this, we combine (4) and (5), yielding

PS(w|r) ∝ P (w|r)P (r)∑
r′ P (w|r′)P (r′)

· 1
Cw

(6)

Because the speaker is deciding what word to use
for an intended referent, and the term P (r) denotes

1Our speaker model corresponds to Frank and Goodman’s
exponentiated utility function (2), with α equal to one and
with their cost D(w) being the log of our cost Cw.

the probability of this referent, P (r) is constant in
the speaker model and does not affect the relative
probability of a speaker producing different words.
We further assume for simplicity that P (w|r) is
constant across words and referents. This means
that all referents have about the same number of
words that can be used to refer to them, and that
all words for a given referent are equally probable
for a naive listener. In this scenario, the speaker’s
probability of choosing a word is

PS(w|r) ∝ 1∑
r′ P (r′)

· 1
Cw

(7)

where the sum denotes the total discourse probabil-
ity of the referents referred to by that word.

The information content of an event is defined
as the negative log probability of that event. In this
scenario, the information conveyed by a word is the
logarithm of the first term in (7), − log ∑r′ P (r′).
This means that in deciding which word to use,
the highest cost a speaker should be willing to pay
for a word should depend directly on that word’s
information content.

This relationship between cost and information
content allows us to derive the prediction tested by
Tily and Piantadosi (2009) that the use of referring
expressions should depend on the predictability
of a referent. For referents that are highly pre-
dictable from the discourse, different referring ex-
pressions (e.g., pronouns and proper names) will
have roughly equal information content, and speak-
ers should choose the referring expression that has
the lowest cost. In contrast, for less predictable ref-
erents, proper names will carry substantially more
information than pronouns, leading speakers to pay
a higher cost for the proper names. These are the
same predictions that have been discussed in the
context of UID, but here the predictions are derived
from a principled model of speakers who are try-
ing to provide information to listeners. The extent
to which our model can also capture other cases
that have been put forward as evidence for the UID
hypothesis remains a question for future research.

3.3 Predicting behavior from corpora
The model described in Section 3.2 is fully general,
applying to arbitrary word choices, discourse prob-
abilities, and cost fuctions. As an initial step, our
simulations focus on the choice between pronouns
and proper names. Our work tests the speaker
model from (4) directly, asking whether it can pre-
dict the referring expressions from corpora of writ-

1642



ten and spoken language. Implementing the model
requires computing word probabilities P (w|r), dis-
course salience P (r), and word costs Cw.

We simplify the word probability P (w|r) in the
speaker’s listener model as in (8):

P (w|r) = 1
V

(8)

where the count V is the number of words that can
refer to referent r. We assume that V is constant
across all referents. Our reasoning is as follows.
There could be many ways to refer to a single entity.
For example, to refer to entity Barack Obama, we
could say ‘he’, ‘The U.S. president’, ‘Barack’, and
so on. We assume that there are the same number
of referring expressions for each entity and that
each referring expression is equally probable under
the listener’s likelihood model.

In our simulations, we assume that a speaker is
choosing between a proper name and a pronoun.
For example, we assume that an entity Barack
Obama has one and only one proper name ‘Barack
Obama’, and this entity is unambiguously associ-
ated with male and singular. Although we use an
example with two possible referring expressions,
as long as P (w|r) is constant across all referents
and words, it does not make a difference to the
computation in (5) how many competing words we
assume for each referent.

To estimate the salience of a referent, P (r), our
framework employs factors such as referent fre-
quency or recency. Although there are other impor-
tant factors such as topicality of the referent (Orita
et al., 2014) that are not incorporated in our sim-
ulations, this model sets up a framework to test
the role and interaction of various potential factors
suggested in the discourse literature.

Salience of the referent is computed differently
depending on its information status: old or new.
The following illustrates the speaker’s assumptions
about the listener’s discourse model:

For each referent r ∈ [1, Rd]:

1. If r = old, choose r in proportion to Nr (the
number of times referent r has been referred
to in the preceding discourse).

2. Otherwise, r = new with probability propor-
tional to α (a hyperparameter that controls
how likely the speaker is to refer to a new
referent).

3. If r = new, sample that new referent r from
the base distribution over entities with proba-
bility 1U· (count U· denotes a total number of
unseen entities that is estimated from a named
entity list (Bergsma and Lin, 2006)).

The above discourse model is frequency-based.
We can replace the termNr for the old referent with
f(di,j) = e−di,j/a that captures recency, where the
recency function f(di,j) decays exponentially with
the distance between the current referent ri and the
same referent rj that has previously been referred
to. This framework for frequency and recency of
new and old referents exactly corresponds to pri-
ors in the Chinese Restaurant Process (Teh et al.,
2006) and the distance-dependent Chinese Restau-
rant Process (Blei and Frazier, 2011).

The denominator in (5) represents the sum of
potential referents that could be referred to by word
w. We assume that a pronoun can refer to a large
number of unseen referents if gender and number
match, but a proper name cannot. For example, ‘he’
could refer to all singular and male referents, but
‘Barack Obama’ can only refer to Barack Obama.
This assumption is reflected as a probability of
unseen referents for the pronoun as illustrated in
(10) below.

In our simulations, the speaker’s cost function
Cw is estimated based on word length as in (9). We
assume that longer words are costly to produce.

Cw = length(w) (9)

Suppose that the speaker is considering using
‘he’ to refer to Barack Obama, which has been
referred to NO times in the preceding discourse,
and there is another singular and male entity, Joe
Biden, in the preceding discourse that has been
referred to NB times. In this situation, the model
computes the probability that the speaker uses ‘he’
to refer to Barack Obama as follows:

PS(‘he’|Obama)
∝ PL(Obama|‘he’) · 1C‘he’
= P (‘he’|Obama)P (Obama)

Σr′P (‘he’|r′)P (r′) ·
1

C‘he’

=
1
V
·NO

( 1
V
·NO)+( 1V ·NB)+( 1V ·α·

Using&masc
U· )

· 1C‘he’

(10)

where count Using&masc in the denominator of the
last line denotes the number of unseen singular &
male entities that could be referred to by ‘he’. We
estimate this number for each type of pronoun we

1643



evaluate (singular-female, singular-male, singular-
neuter, and plural) based on the named entity list
in Bergsma and Lin (2006). The term ( 1V · α ·
Using&masc

U· ) is the sum of probabilities of unseen
referents that could be referred to by the pronoun
‘he’. The unseen referents can be interpreted as a
penalty for the inexplicitness of pronouns. In the
case of proper names, the denominator is always
the same as the numerator, under the assumption
that each entity has one unique proper name.

4 Data

4.1 Corpora

Our model was run on both adult-directed speech
and child-directed speech. We chose to use the
SemEval-2010 Task 1 subset of OntoNotes (Re-
casens et al., 2011), a corpus of news text, as our
corpus of adult-directed speech. The Gleason et al.
(1984) subset of CHILDES (MacWhinney, 2000)
was chosen as our corpus of child-directed speech.

The model requires coreference chains, agree-
ment information, grammatical position, and part
of speech. These were extracted from each corpus,
either manually or automatically. The coreference
chains let us easily count how many times/how
recently each referent is mentioned in the dis-
course, which is necessary for computing discourse
salience. The agreement information (gender and
number of each referent) is required so that the
model can identify all possible competing refer-
ents for pronouns. For instance, Barack Obama
will be ruled out as a possible competitor for the
pronoun she. The grammatical position that each
proper name occupies2 determines the form of the
alternative pronoun that could be used there. For
example, the difference between he and him is the
grammatical position that each can appear in. The
part of speech is used to identify the form of the
referring expression (pronouns and proper names),
which is what our model aims to predict.3

OntoNotes includes information about corefer-
ence chains, part of speech, and grammatical de-
pendencies. Gleason CHILDES has parsed part of
speech and grammatical dependencies (Sagae et
al., 2010), but it does not have coreference chains.

2Dependency tags used were ‘SUBJ’, ‘OBJ’, and ‘PMOD’
in OntoNotes and ‘SBJ’ and ‘OBJ’ in CHILDES.

3The part of speech used to extract the target NPs were
‘PRP’ (pronoun), ‘NNP’ (proper name), and ‘NNPS’ (plu-
ral proper name) from OntoNotes and ‘pro’ (pronoun) and
‘n:prop’ (proper name) from CHILDES.

Neither corpus has agreement information. The fol-
lowing section describes manual annotations that
we have done for this study. Due to time constraints,
we annotated only a part of the CHILDES Gleason
corpus, 9 out of 70 scripts.

4.2 Annotation

4.2.1 Mention annotation

We considered only maximally spanning noun
phrases as mentions, ignoring nested NPs and
nested coreference chains. For the sentence “Both
Al Gore and George W. Bush have different ideas
on how to spend that extra money” from OntoNotes,
the extracted NPs are Both Al Gore and George W.
Bush and different ideas about how to spend that
extra money.

These maximally spanning NPs were automati-
cally extracted from the OntoNotes data, but were
manually annotated for the CHILDES data using
brat (Stenetorp et al., 2012) by two annotators.4

4.2.2 Agreement annotation

Many mentions (46,246 out of 56,575 mentions in
OntoNotes and 10,141 out of 10,530 mentions in
CHILDES Gleason) were automatically annotated
using agreement information from the named entity
list in Bergsma and Lin (2006), leaving 10,329
to be manually annotated from OntoNotes (about
18%) and 389 from CHILDES (about 4%).5

The guidelines we followed for this manual
agreement annotation were largely based on pro-
noun replacement tests. NPs that referred to a sin-
gle man and could be replaced with he or him were
labeled ‘male singular’, NPs that could be replaced
by it, such as the comment, were labeled ‘neuter
singular’, and so on. NPs that could not be replaced
with a pronoun, such as about 30 years earnings
for the average peasant, who makes $145 a year,
were excluded from the analysis.

4.2.3 Coreference annotation

We used the provided coreference chains for the
OntoNotes data, but for the CHILDES data, it was
necessary to do this manually using brat. The guide-
lines we followed for determining whether men-
tions coreferred came from the OntoNotes corefer-

4Interannotator agreement for the CHILDES mention an-
notation was: precision 0.97, recall 0.98, F-score 0.97 (for
two scripts).

5Interannotator agreement for the manual annotation of
agreement information was 97% (for 500 mentions).

1644



ence guidelines (BBN Technologies, 2007).6

5 Experiments

Our experiments are designed to quantify the contri-
butions of the various components of the complete
model described in Section 3.2 that incorporates
discourse salience, cost, and unseen referents. We
contrast the complete model with three impover-
ished models that lack precisely one of these com-
ponents. The comparison model without discourse
uses a uniform discourse salience distribution. The
model without cost uses constant speech cost. The
model without good estimates of unseen referents
always assigns probability 1V · α · 1C· to unseen
referents in the denominator of (5), regardless of
whether the word is a proper name or pronoun. In
other words, this model does not have good esti-
mates of unseen referents like the complete model
does.

We use the adult- and child-directed corpora to
examine to what extent each model captures speak-
ers’ referring expressions. We selected pronouns
and proper names in each corpus according to sev-
eral criteria. First, the referring expression had
to be in a coreference chain that had at least one
proper name, in order to facilitate computing the
cost of the proper name alternative. Second, pro-
nouns were only included if they were third person
pronouns in subject or object position, and index-
icals and reflexives were excluded. Finally, for
the CHILDES corpus, children’s utterances were
excluded.

After filtering pronouns and proper names ac-
cording to these criteria, 553 pronouns and 1,332
proper names (total 1,885 items) in the OntoNotes
corpus, and 165 pronouns and 149 proper names
(total 314 items) in the CHILDES Gleason corpus
remained for use in the analysis.

Each model chooses referring expressions given
information extracted from each corpus as de-
scribed in Section 4.1. For evaluation, we com-
puted accuracies (total, pronoun, and proper name)
and model log likelihood (summing logPS(w|r)
for the words in the corpus) for each model.

5.1 Results

Table 1 summarizes the results of each model with
the OntoNotes and CHILDES datasets. The new

6Interannotator agreement for CHILDES coreference an-
notation was computed using B3 (Bagga and Baldwin, 1998):
precision: 0.99, recall: 1.00 (for one script).

referent hyperparameter α and the decay parameter
for discourse recency salience were fixed at 0.1 and
3.0, respectively.7

5.1.1 News
Overall, the recency salience measure provides a
better fit than the frequency salience measure with
respect to accuracies, suggesting that recency bet-
ter captures speakers’ representations of discourse
salience that influence choices of referring expres-
sions. On the other hand, the models with fre-
quency discourse salience have higher model log
likelihood than the models with recency do. This
is because of the peakiness of the recency models.
Model log likelihood computed over pronouns and
proper names (complete model) were -1022.33 and
-222.76, respectively, with recency, and -491.81 and
-467.06 with frequency. The recency model tends
to return a higher probability for a proper name
than the frequency model does. Some pronouns
receive a very low probability for this reason, and
this lowers the model log likelihood.

The model without discourse and the model with-
out cost consistently failed to predict pronouns
(these models predicted all proper names). This
happens because in the model without discourse,
the information content of pronouns is extremely
low due to the large number of consistent unseen
referents. In the model without cost, pronouns are
disfavored because they always convey less infor-
mation than proper names. The log likelihoods of
these models were also below that of the complete
model. These results show that pronominalization
depends on subtle interaction between discourse
salience and speech cost. Neither of them is suf-
ficient to explain the distribution of pronouns and
nouns on its own.

The total accuracy of the model without good
estimates of unseen referents was the worst among
the four models, but this model did predict pro-
nouns to some extent. Because the number of
proper names is larger than the number of pronouns
in this dataset, the difference in total accuracies be-
tween the model without good estimates of unseen
referents and the models without discourse or cost
reflects this asymmetry. Comparison between the
complete model and the model without good esti-
mates of unseen referents also suggests that having
knowledge of unseen referents helps correctly pre-

7We chose the best parameter values based on multiple
runs, but results were qualitatively consistent across a range
of parameter values.

1645



Corpus Model Discourse Total accuracy Pronoun accuracy Proper name accuracy Log-lhood

OntoNotes

complete recency 80.27% 59.49% 88.89% -1245.09
frequency 73.10% 62.74% 77.40% -958.87

-discourse NA 70.66% 0.00% 100.00% -6904.77
-cost recency 70.66% 0.00% 100.00% -1537.71

frequency 70.66% 0.00% 100.00% -1017.38
-unseen recency 64.14% 68.17% 62.46% -1567.51

frequency 56.98% 76.67% 48.80% -1351.58

CHILDES

complete recency 49.68% 11.52% 91.95% -968.64
frequency 46.18% 10.30% 85.91% -360.28

-discourse NA 47.45% 0.00% 100.00% -2159.22
-cost recency 47.45% 0.00% 100.00% -1055.54

frequency 47.45% 0.00% 100.00% -392.72
-unseen recency 50.31% 13.94% 90.60% -961.54

frequency 48.41% 21.21% 78.52% -332.73

Table 1: Accuracies and model log-likelihood

dict the use of proper names in the first mention of
a referent.

5.1.2 Child-directed speech
Unlike the adult-directed news text, neither recency
nor frequency discourse salience provides a good
fit to the data. The low accuracies of pronouns and
the high accuracies of proper names in all models
indicate that the models are more likely to predict
proper names than pronouns. There are several
possible reasons for this. First, the CHILDES tran-
scripts involve long conversations in a natural set-
tings. Compared to the news, interlocutors are not
focusing on a specific topic, but rather they often
switch topics (e.g., a child interrupts her parents’
conversation about her father’s coworker to talk
about her eggs). This topic switching makes it dif-
ficult for the model to estimate discourse salience
using simple frequency or recency measures. Sec-
ond, interlocutors are a family and they share a
good deal of common knowledge/background (e.g.,
a mother said she as the first mention of her child’s
friend’s mother). The current model is not able
to incorporate this kind of background knowledge.
Third, many referents are visually available. The
current model is not able to use visual salience. In
general, these problems arise due to our impover-
ished estimates of salience, and we would expect a
more sophisticated discourse model that accurately
measured salience to show better performance.

5.2 Summary

Experiments with the adult-directed news corpus
show a close match between speakers’ utterances
and model predictions. On the other hand, exper-
iments with child-directed speech show that the
models were more likely to predict proper names

where pronouns were used, suggesting that the esti-
mates of discourse salience using simple measures
were not sufficient to capture a conversation.

6 Discussion

This paper proposes a language production model
that extends the rational speech act model from
Frank and Goodman (2012) to incorporate updates
to listeners’ beliefs as discourse proceeds. We show
that the predictions suggested from UID in this do-
main can be derived from our speaker model, pro-
viding an explanation from first principles for the
relation between discourse salience and speakers’
choices of referring expressions. Experiments with
an adult-directed news corpus show a close match
between speakers’ utterances and model predic-
tions, and experiments with child-directed speech
show a qualitatively similar pattern. This suggests
that speakers’ behavior can be modeled in a princi-
pled way by considering the probabilities of refer-
ents in the discourse and the information conveyed
by each word.

A controversial issue in language production is
to what extent speakers consider a listener’s dis-
course model (Fukumura and van Gompel, 2012).
By incorporating an explicit model of listeners, our
model provides a way to explore this question. For
example, the speaker’s listener model PL(r|w) in
(4) might differ between contexts and could also be
extended to sum over possible listener identity q in
mixed contexts, as in (11).

PL(r|w) = ΣqP (r|w, q)P (q) (11)

This provides a way to probe speakers’ sensitiv-
ity to differences in listener characteristics across
situations.

1646



Although the simulations in this paper employed
simple measures for discourse salience (referent
frequency and recency), the discourse models used
by speakers are likely to be more complex. Stud-
ies show that semantic information that cannot be
captured with these simple measures, such as topi-
cality (Orita et al., 2014) and animacy (Vogels et
al., 2013a), affects speakers’ choices of referring
expressions. Future work will test to what extent
this latent discourse information could affect the
model predictions.

Our model predicts a tight coupling between the
probability of a referent being mentioned, p(r),
and the choice of referring expression. However,
these two quantities appear to be dissociated in
some cases. For example, Fukumura and Van Gom-
pel (2010) show that semantic bias (as a measure
of predictability) affects what to refer to (i.e., the
referent), but not how to refer (i.e., the referring
expression), while grammatical position does af-
fect how you refer. One way of dissociating the
probability of mention from the choice of referring
expression in our model would be through the likeli-
hood term, the word probability p(w|r). While we
have assumed this word probability to be constant
across words and referents, Kehler et al. (2008) use
grammatical position to define this probability and
show that their model captures experimental data.
Syntactic constraints (such as Binding principles)
also influence form choices, and this kind of knowl-
edge may also be reflected in the word probability.
Examining the role of the word probability p(w|r)
more closely would allow us to further explore
these issues.

Despite these limitations, our model provides
a principled explanation for speakers’ choices of
referring expressions. In future work we hope to
look at a broader range of referring expressions,
such as null pronouns and definite descriptions,
and to explore the extent to which our model can
be applied to other linguistic phenomena that rely
on discourse information.

Acknowledgments

We thank the UMD probabilistic modeling reading
group for helpful comments and discussion.

References
John R Anderson. 2007. How can the human mind

occur in the physical universe? Oxford University
Press.

Mira Ariel. 1990. Accessing noun-phrase antecedents.
Routledge.

Jennifer Arnold. 1998. Reference form and discourse
patterns. Ph.D. thesis, Stanford University Stanford,
CA.

Jennifer Arnold. 2008. Reference produc-
tion: Production-internal and addressee-oriented
processes. Language and cognitive processes,
23(4):495–527.

Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The first interna-
tional conference on language resources and evalua-
tion workshop on linguistics coreference, volume 1,
pages 563–566.

Ellen Gurman Bard, Matthew P Aylett, J Trueswell,
and M Tanenhaus. 2004. Referential form, word du-
ration, and modeling the listener in spoken dialogue.
Approaches to studying world-situated language use:
Bridging the language-as-product and language-as-
action traditions, pages 173–191.

BBN Technologies. 2007. OntoNotes English co-
reference guidelines version 7.0.

Leon Bergen, Noah Goodman, and Roger Levy. 2012a.
That’s what she (could have) said: How alternative
utterances affect language use. In Proceedings of
the 34th Annual Conference of the Cognitive Science
Society.

Leon Bergen, Noah D Goodman, and Roger Levy.
2012b. That’s what she (could have) said: How
alternative utterances affect language use. In Pro-
ceedings of the thirty-fourth annual conference of
the cognitive science society.

Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 33–40,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

David M Blei and Peter I Frazier. 2011. Distance de-
pendent Chinese restaurant processes. The Journal
of Machine Learning Research, 12:2461–2488.

Richard Breheny, Napoleon Katsos, and John Williams.
2006. Are generalised scalar implicatures generated
by default? an on-line investigation into the role of
context in generating pragmatic inferences. Cogni-
tion, 100(3):434–463.

Susan E Brennan. 1995. Centering attention in
discourse. Language and Cognitive Processes,
10(2):137–167.

Wallace Chafe. 1994. Discourse, consciousness, and
time. Discourse, 2(1).

1647



Judith Degen, Michael Franke, and Gerhard Jäger.
2013. Cost-based pragmatic inference about referen-
tial expressions. In Proceedings of the 35th Annual
Conference of the Cognitive Science Society, pages
376–381.

Michael Frank and Noah Goodman. 2012. Predicting
pragmatic reasoning in language games. Science,
336(6084):998–998.

Kumiko Fukumura and Roger PG Van Gompel. 2010.
Choosing anaphoric expressions: Do people take
into account likelihood of reference? Journal of
Memory and Language, 62(1):52–66.

Kumiko Fukumura and Roger PG van Gompel. 2012.
Producing pronouns and definite noun phrases: Do
speakers use the addressee’s discourse model? Cog-
nitive Science, 36(7):1289–1311.

Kumiko Fukumura, Roger PG Van Gompel, Trevor
Harley, and Martin J Pickering. 2011. How does
similarity-based interference affect the choice of re-
ferring expression? Journal of Memory and Lan-
guage, 65(3):331–344.

Alexia Galati and Susan E Brennan. 2010. Attenuat-
ing information in spoken communication: For the
speaker, or for the addressee? Journal of Memory
and Language, 62(1):35–51.

Talmy Givón. 1983. Topic continuity in discourse: A
quantitative cross-language study, volume 3. John
Benjamins Publishing.

Jean Berko Gleason, Rivka Y Perlmann, and Es-
ther Blank Greif. 1984. What’s the magic word:
Learning language through politeness routines. Dis-
course Processes, 7(4):493–502.

Noah D Goodman and Andreas Stuhlmüller. 2013.
Knowledge and implicature: Modeling language un-
derstanding as social cognition. Topics in Cognitive
Science.

H Paul Grice. 1975. Logic and conversation. Syntax
and semantics, 3:41–58.

Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.

André Grüning and Andrej A Kibrik. 2005. Modeling
referential choice in discourse: A cognitive calcu-
lative approach and a neural network approach. In
Ruslan Mitkov, editor, Anaphora Processing: Lin-
guistic, Cognitive and Computational Modelling,
pages 163–198. John Benjamins.

Jeanette K Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, pages 274–307.

Florian T Jaeger. 2010. Redundancy and reduc-
tion: Speakers manage syntactic information density.
Cognitive psychology, 61(1):23–62.

Gerhard Jager. 2007. Game dynamics connects seman-
tics and pragmatics. In Ahti-Veikko Pietarinen, edi-
tor, Game theory and linguistic meaning, pages 89–
102. Elsevier.

Justine T Kao, Jean Wu, Leon Bergen, and Noah D
Goodman. 2014. Nonliteral understanding of num-
ber words. Proceedings of the National Academy of
Sciences, 111(33):12002–12007.

Andrew Kehler, Laura Kertz, Hannah Rohde, and Jef-
frey L Elman. 2008. Coherence and coreference
revisited. Journal of Semantics, 25(1):1–44.

Mariya V Khudyakova, Grigory B Dobrov, Andrej A
Kibrik, and Natalia V Loukachevitch. 2011. Com-
putational modeling of referential choice: Major and
minor referential options. In Proceedings of the
CogSci 2011 Workshop on the Production of Refer-
ring Expressions. Boston (July 2011).

Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.

Roger Levy and T. Florian Jaeger. 2007. Speakers op-
timize information density through syntactic reduc-
tion. In Proceedings of the 20th Conference on Neu-
ral Information Processing Systems (NIPS).

Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk.

Mante S Nieuwland and Jos JA Van Berkum. 2006.
When peanuts fall in love: N400 evidence for the
power of discourse. Journal of Cognitive Neuro-
science, 18(7):1098–1111.

Ann E Nordmeyer and Michael Frank. 2014. A
pragmatic account of the processing of negative sen-
tences. In Proceedings of the 36th Annual Confer-
ence of the Cognitive Science Society.

Naho Orita, Eliana Vornov, Naomi H Feldman, and Jor-
dan Boyd-Graber. 2014. Quantifying the role of
discourse topicality in speakers’ choices of referring
expressions. In Association for Computational Lin-
guistics, Workshop on Cognitive Modeling and Com-
putational Linguistics.

Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional Linguistics, 30(3):309–363.

Marta Recasens, Lluis Marquez, Emili Sapena,
M. Antònia Martı́, and Mariona Taulé. 2011.
SemEval-2010 task 1 OntoNotes English: Corefer-
ence resolution in multiple languages.

Jacolien Rij, Hedderik Rijn, and Petra Hendriks. 2013.
How WM load influences linguistic processing in
adults: A computational model of pronoun inter-
pretation in discourse. Topics in Cognitive Science,
5(3):564–580.

1648



Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard
Jäger, and Stefan Kaufmann. 2012. Communicat-
ing with cost-based implicature: A game-theoretic
approach to ambiguity. In The 16th Workshop on
the Semantics and Pragmatics of Dialogue, Paris,
September.

Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-
ney, and Shuly Wintner. 2010. Morphosyntactic an-
notation of CHILDES transcripts. Journal of Child
Language, 37(03):705–729.

Nathaniel J Smith, Noah Goodman, and Michael Frank.
2013. Learning and using language via recursive
pragmatic reasoning about other agents. In Ad-
vances in neural information processing systems,
pages 3039–3047.

Pontus Stenetorp, Sampo Pyysalo, Goran Topic,
Tomoko Ohta, Sophia Ananiadou, and Junichi Tsujii.
2012. brat: a web-based tool for NLP-assisted text
annotation. In Proceedings of the Demonstrations
Session at EACL 2012.

Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associ-
ation, 101.

Harry Tily and Steven Piantadosi. 2009. Refer effi-
ciently: Use less informative expressions for more
predictable meanings. In Proceedings of the work-
shop on the production of referring expressions:
Bridging the gap between computational and empir-
ical approaches to reference.

Mija Van der Wege. 2009. Lexical entrainment and
lexical differentiation in reference phrase choice.
Journal of Memory and Language, 60(4):448–463.

Jorrig Vogels, Emiel Krahmer, and Alfons Maes.
2013a. When a stone tries to climb up a slope: the
interplay between lexical and perceptual animacy in
referential choices. Frontiers in psychology, 4.

Jorrig Vogels, Emiel Krahmer, and Alfons Maes.
2013b. Who is where referred to how, and why? the
influence of visual saliency on referent accessibility
in spoken language production. Language and Cog-
nitive Processes, 28(9):1323–1349.

1649


