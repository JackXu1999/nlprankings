



















































Improved Language Modeling by Decoding the Past


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1468–1476
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1468

Improved Language Modeling by Decoding the Past

Siddhartha Brahma
IBM Research, Almaden, USA
brahma@us.ibm.com

Abstract

Highly regularized LSTMs achieve impressive
results on several benchmark datasets in lan-
guage modeling. We propose a new regu-
larization method based on decoding the last
token in the context using the predicted dis-
tribution of the next token. This biases the
model towards retaining more contextual in-
formation, in turn improving its ability to pre-
dict the next token. With negligible overhead
in the number of parameters and training time,
our Past Decode Regularization (PDR) method
improves perplexity on the Penn Treebank
dataset by up to 1.8 points and by up to 2.3
points on the WikiText-2 dataset, over strong
regularized baselines using a single softmax.
With a mixture-of-softmax model, we show
gains of up to 1.0 perplexity points on these
datasets. In addition, our method achieves
1.169 bits-per-character on the Penn Treebank
Character dataset for character level language
modeling. Each of these results constitute im-
provements over models without PDR in their
respective settings.

1 Introduction

Language modeling is a fundamental task in natu-
ral language processing. Given a sequence of to-
kens, its joint probability distribution can be mod-
eled using the auto-regressive conditional factor-
ization. This leads to a convenient formulation
where a language model has to predict the next to-
ken given a sequence of tokens as context. Recur-
rent neural networks are an effective way to com-
pute distributed representations of the context by
sequentially operating on the embeddings of the
tokens. These representations can then be used to
predict the next token as a probability distribution
over a fixed vocabulary using a linear decoder fol-
lowed by Softmax.

Starting from the work of (Mikolov et al.,
2010), there has been a long list of works that

seek to improve language modeling performance
using more sophisticated recurrent neural net-
works (RNNs) (Zaremba et al., 2014; Zilly et al.,
2017; Zoph and Le, 2016; Mujika et al., 2017).
However, in more recent work vanilla LSTMs
(Hochreiter and Schmidhuber, 1997) with rel-
atively large number of parameters have been
shown to achieve state-of-the-art performance on
several standard benchmark datasets both in word-
level and character-level perplexity (Merity et al.,
2018b,a; Melis et al., 2018; Yang et al., 2017).
A key component in these models is the use
of several forms of regularization e.g. varia-
tional dropout on the token embeddings (Gal and
Ghahramani, 2016), dropout on the hidden-to-
hidden weights in the LSTM (Wan et al., 2013),
norm regularization on the outputs of the LSTM
and classical dropout (Srivastava et al., 2014). By
carefully tuning the hyperparameters associated
with these regularizers combined with optimiza-
tion algorithms like NT-ASGD (a variant of Aver-
aged SGD), it is possible to achieve very good per-
formance. Each of these regularizations address
different parts of the LSTM model and are gen-
eral techniques that could be applied to any other
sequence modeling problem.

In this paper, we propose a regularization tech-
nique that exploits a symmetry in language mod-
els. A unique aspect of language modeling using
LSTMs (or any RNN) is that at each time step t,
the model takes as input a particular token xt from
a vocabulary W and using the hidden state of the
LSTM (which encodes the context till xt−1) pre-
dicts a probability distribution wt+1 on the next
token xt+1 over the same vocabulary as output.
Since xt can be mapped to a trivial probability
distribution over W , this operation can be inter-
preted as transforming distributions over W (Inan
et al., 2016). Clearly, the output distribution is de-
pendent on and is a function of xt and the context



1469

further in the past and encodes information about
it. We ask the following question – How much in-
formation is it possible to decode about the input
distribution (and hence xt) from the output distri-
bution wt+1? In general, it is impossible to decode
xt unambiguously. Even if the language model
is perfect and correctly predicts xt+1 with prob-
ability 1, there could be many tokens preceding it.
However, in this case the number of possibilities
for xt will be limited, as dictated by the bigram
statistics of the corpus and the language in gen-
eral. We argue that biasing the language model
such that it is possible to decode more information
about the past tokens from the predicted next token
distribution is beneficial. We incorporate this intu-
ition into a regularization term in the loss function
of the language model.

The symmetry in the inputs and outputs of the
language model at each step lends itself to a simple
decoding operation. It can be cast as a (pseudo)
language modeling problem in “reverse”, where
the future prediction wt+1 acts as the input and
the last token xt acts as the target of prediction.
The token embedding matrix and weights of the
linear decoder of the main language model can be
reused in the past decoding operation. We only
need a few extra parameters to model the nonlinear
transformation performed by the LSTM, which we
do by using a simple stateless layer. We compute
the cross-entropy loss between the decoded distri-
bution for the past token and xt and add it to the
main loss function after suitable weighting. The
extra parameters used in the past decoding are dis-
carded during inference time. We call our method
Past Decode Regularization or PDR for short.

We conduct extensive experiments on four
benchmark datasets for word level and charac-
ter level language modeling by combining PDR
with existing LSTM based language models and
achieve improved performance on three of them.

2 Past Decode Regularization (PDR)

Let X = (x1, x2, · · · , xt, · · · , xT ) be a sequence
of tokens. In this paper, we will experiment with
both word level and character level language mod-
eling. Therefore, tokens can be either words or
characters. The joint probability P (X) factorizes
into

P (X) =

T∏
t=1

P (xt|x1, x2, · · · , xt−1) (1)

Let ct = (x1, x2, · · · , xt) denote the context avail-
able to the language model for xt+1. LetW denote
the vocabulary of tokens, each of which is embed-
ded into a vector of dimension d. Let E denote the
token embedding matrix of dimension |W |×d and
ew denote the embedding of w ∈ W . An LSTM
computes a distributed representation of ct in the
form of its hidden state ht, which we assume has
dimension d as well. The probability that the next
token is w can then be calculated using a linear
decoder followed by a Softmax layer as

Pθ(w|ct) = Smax(htET + b)|w
= exp(hte

T
w+bw)∑

w′∈W exp(hte
T
w′+bw′ )

(2)

where bw′ is the entry corresponding tow′ in a bias
vector b of dimension |W | and |w represents pro-
jection onto w. Here we assume that the weights
of the decoder are tied with the token embedding
matrix E (Inan et al., 2016; Press and Wolf, 2017).
To optimize the parameters of the language model
θ, the loss function to be minimized during train-
ing is set as the cross-entropy between the pre-
dicted distribution Pθ(w|ct) and the actual token
xt+1.

LCE =
∑
t

− log(Pθ(xt+1|ct)) (3)

Note that Eq.(2), when applied to all w ∈ W
produces a 1 × |W | vector wt+1, encapsulating
the prediction the language model has about the
next token xt+1. Since this is dependent on and
conditioned on ct, wt+1 clearly encodes informa-
tion about it; in particular about the last token xt in
ct. In turn, it should be possible to infer or decode
some limited information about xt from wt+1. We
argue that by biasing the model to be more accu-
rate in recalling information about past tokens, we
can help it in predicting the next token better.

To this end, we define the following decod-
ing operation to compute a probability distribution
over wc ∈W as the last token in the context.

Pθr(wc|wt+1) = Smax(fθr(wt+1E)ET + b′θr)|wc
(4)

Here fθr is a non-linear function that maps vectors
in Rd to vectors in Rd and b′θr is a bias vector of
dimension |W |, together comprising the parame-
ters θr. In effect, we are decoding the past – the
last token in the context xt. This produces a vector
wrt of dimension 1× |W |. The cross-entropy loss



1470

PTB WT2 PTBC enwik8
Train Valid Test Train Valid Test Train Valid Test Train Valid Test

Tokens 888K 70.4K 78.7K 2.05M 213K 241K 5.01M 393k 442k 90M 5M 5M
Vocab 10K 33.3K 51 205

Table 1: Statistics of the language modeling benchmark datasets.

with respect to the actual last token xt can then be
computed as

LPDR =
∑
t

− log(Pθr(xt|wt+1)) (5)

Here PDR stands for Past Decode Regulariza-
tion. LPDR captures the extent to which the de-
coded distribution of tokens differs from the ac-
tual tokens xt in the context. Note the symme-
try between Eqs.(2) and (5). The “input” in the
latter case is wt+1 and the “context” is provided
by a nonlinear transformation of wt+1E. Differ-
ent from the former, the context in Eq.(5) does not
preserve any state information across time steps
as we want to decode only using wt+1. The term
wt+1E can be interpreted as a “soft” token em-
bedding lookup, where the token vector wt+1 is a
probability distribution instead of a unit vector.

We add λPDRLPDR to the loss function in
Eq.(3) as a regularization term, where λPDR is a
positive weighting coefficient, to construct the fol-
lowing new loss function for the language model.

L = LCE + λPDRLPDR (6)

Thus equivalently PDR can also be viewed as a
method of defining an augmented loss function for
language modeling. The choice of λPDR dictates
the degree to which we want the language model
to incorporate our inductive bias i.e. decodability
of the last token in the context. If it is too large,
the model will fail to predict the next token, which
is its primary task. If it is zero or too small, the
model will retain less information about the last
token which hampers its predictive performance.
In practice, we choose λPDR by a search based on
validation set performance.

Note that the trainable parameters θr associated
with PDR are used only during training to bias the
language model and are not used at inference time.
This also means that it is important to control the
complexity of the nonlinear function fθr so as not
to overly bias the training. As a simple choice,

we use a single fully connected layer of size d fol-
lowed by a Tanh nonlinearity as fθr . This intro-
duces few extra parameters and a small increase
in training time as compared to a model not using
PDR.

3 Experiments

We present extensive experimental results to show
the efficacy of using PDR for language modeling
on four standard benchmark datasets – two each
for word level and character level language mod-
eling. For the former, we evaluate our method on
the Penn Treebank (PTB) (Mikolov et al., 2010)
and the WikiText-2 (WT2) (Merity et al., 2016)
datasets. For the latter, we use the Penn Tree-
bank Character (PTBC) (Mikolov et al., 2010) and
the Hutter Prize Wikipedia Prize (Hutter, 2018)
(also known as Enwik8) datasets. Key statistics
for these datasets is presented in Table 1.

As mentioned in the introduction, some of the
best existing results on these datasets are obtained
by using extensive regularization techniques on
relatively large LSTMs (Merity et al., 2018b,a;
Yang et al., 2017). We apply our regularization
technique to these models, the so called AWD-
LSTM. We consider two versions of the model –
one with a single softmax (AWD-LSTM) and one
with a mixture-of-softmaxes (AWD-LSTM-MoS).
The PDR regularization term is computed accord-
ing to Eq.(4) and Eq.(5). We call our model AWD-
LSTM+PDR when using a single softmax and
AWD-LSTM-MoS+PDR when using a mixture-
of-softmaxes. We largely follow the experimental
procedure of the original models and incorporate
their dropouts and regularizations in our experi-
ments.

For completeness, we briefly mention the set of
dropouts and regularizations reused from AWD-
LSTM in our experiments. They are the following.

1. Embedding dropout – Variational or locked
dropout applied to the token embedding ma-
trix.



1471

2. Word dropout – Dropout applied to entire to-
kens.

3. LSTM layer dropout – Dropout between lay-
ers of the LSTM.

4. LSTM weight dropout – Dropout applied
to the hidden-to-hidden connections in the
LSTM.

5. LSTM output dropout – Dropout applied to
the final output of the LSTM.

6. Alpha/beta regularization – Activation and
temporal activation regularization applied to
the LSTM states.

7. Weight decay – L2 regularization on the pa-
rameters of the model.

Note that these regularizations are applied to the
input, hidden state and output of the LSTM and do
not exploit the special structure of language mod-
eling, which PDR does. The relative contribution
of these existing regularizations and PDR will be
analyzed in Section 6.

There are 7 hyperparameters associated with the
regularizations used in AWD-LSTM (and one ex-
tra with MoS). PDR also has an associated weight-
ing coefficient λPDR. For our experiments, we
set λPDR = 0.001 which was determined by a
coarse search on the PTB and WT2 validation sets.
For the remaining ones, we perform light hyperpa-
rameter search in the vicinity of those reported for
AWD-LSTM in (Merity et al., 2018b,a) and for
AWD-LSTM-MoS in (Yang et al., 2017).

3.1 Model and training for PTB and
WikiText-2

For the single softmax model (AWD-
LSTM+PDR), for both PTB and WT2, we
use a 3-layered LSTM with 1150, 1150 and 400
hidden dimensions. The word embedding dimen-
sion is set to d = 400. For the mixture-of-softmax
model, we use a 3-layer LSTM with dimensions
960, 960 and 620, an embedding dimension of
280 and 15 experts for PTB. For WT2, we use a
3-layer LSTM with dimensions 1150, 1150 and
650, an embedding dimension of d = 300 and 15
experts. Weight tying is used in all the models.
For training the models, we follow the same
procedure as AWD-LSTM i.e. a combination of
SGD and NT-ASGD, followed by finetuning. We
adopt the learning rate schedules and batch sizes

of (Merity et al., 2018b) and (Yang et al., 2017) in
our experiments.

3.2 Model and training for PTBC and
Enwik8

For PTBC, we use a 3-layer LSTM with 1000,
1000 and 200 hidden dimensions and a charac-
ter embedding dimension of d = 200. For En-
wik8, we use a LSTM with 1850, 1850 and 400
hidden dimensions and the characters are embed-
ded in d = 400 dimensions. For training, we
largely follow the procedure laid out in (Merity
et al., 2018a). For each of the datasets, AWD-
LSTM+PDR has less than 1% more parameters
than the corresponding AWD-LSTM model (dur-
ing training only). The maximum observed time
overhead due to the additional computation is less
than 3%.

4 Results on Word Level Language
Modeling

The results for PTB are shown in Table 2. With a
single softmax, our method (AWD-LSTM+PDR)
achieves a perplexity of 55.6 on the PTB test set,
which improves on the model without PDR by an
absolute 1.7 points. The advantages of better in-
formation retention due to PDR are maintained
when combined with a continuous cache pointer
(Grave et al., 2016), where our method yields an
absolute improvement of 1.2 over AWD-LSTM.
Notably, when coupled with dynamic evaluation
(Krause et al., 2018), the perplexity is decreased
further to 49.3. Note that, for both cache pointer
and dynamic evaluation, we coarsely tune the as-
sociated hyperparameters on the validation set.

Using a mixture-of-softmaxes, our method
(AWD-LSTM-MoS+PDR) achieves a test per-
plexity of 53.8, an improvement of 0.6 points
over the model without PDR. The use of dynamic
evaluation pushes the perplexity further down to
47.3. Note that our models do not use the re-
cently proposed frequency agnostic word embed-
dings FRAGE (Gong et al., 2018) and it is possible
that adding PDR can lead to similar gains when
applied to models using such embeddings. PTB
is a restrictive dataset with a vocabulary of 10K
words. Achieving good perplexity requires con-
siderable regularization. The fact that PDR can
improve upon existing heavily regularized models
is empirical evidence of its distinctive nature and
its effectiveness in improving language models.



1472

Model #Params Valid Test

State-of-the-art Methods (Single Softmax)

(Merity et al., 2018b) – AWD-LSTM 24.2M 60.0 57.3
(Merity et al., 2018b) – AWD-LSTM + continuous cache pointer 24.2M 53.9 52.8
(Krause et al., 2018) – AWD-LSTM + dynamic evaluation 24.2M 51.6 51.1
(Gong et al., 2018) – AWD-LSTM + cont. cache pointer w/ FRAGE 24.2M 52.3 51.8

Our Method (Single Softmax)

AWD-LSTM+PDR 24.2M 57.9 55.6 (-1.7)
AWD-LSTM+PDR + continuous cache pointer 24.2M 52.4 51.6 (-1.2)
AWD-LSTM+PDR + dynamic evaluation 24.2M 50.1 49.3 (-1.8)

Sate-of-the-art Methods (Mixture-of-Softmax)

(Yang et al., 2017) – AWD-LSTM-MoS 22M 56.5 54.4
(Yang et al., 2017) – AWD-LSTM-MoS + dynamic evaluation 22M 48.3 47.7
(Gong et al., 2018) – AWD-LSTM-MoS + dyn. evaluation w/ FRAGE 22M 47.4 46.5

Our Method (Mixture-of-Softmax)

AWD-LSTM-MoS+PDR 22M 56.2 53.8 (-0.6)
AWD-LSTM-MoS+PDR + dynamic evaluation 22M 48.0 47.3 (-0.4)

Table 2: Perplexities on Penn Treebank (PTB) for single and mixture-of-softmaxes models. Values in parentheses
show gain over respective models without using PDR. The number of parameters during training is 24.4M. Our
models do not use frequency agnostic word embeddings.

Table 3 shows the perplexities achieved by our
model on WT2. This dataset is considerably more
complex than PTB with a vocabulary of more
than 33K words. AWD-LSTM+PDR improves
over the single softmax model without PDR by
a significant 2.3 points, achieving a perplexity of
63.5. Similar gains are observed with the use of
cache pointer (2.4 points) and with the use of dy-
namic evaluation (1.7 points). Using a mixture-of-
softmaxes, AWD-LSTM-MoS+PDR achieves per-
plexities of 60.5 and 40.3 (with dynamic evalu-
ation) on the WT2 test set, improving upon the
models without PDR by 1.0 and 0.4 points respec-
tively. Here again, the use of PDR in models with
FRAGE could lead to further drops in perplexity.

4.1 Performance on Larger Datasets

We consider the Gigaword dataset (Chelba et al.,
2014) with a truncated vocabulary of about 100K
tokens with the highest frequency and apply PDR
to a baseline 2-layer LSTM language model with
embedding and hidden dimensions set to 1024.
We use all the shards from the training set for
training and a few shards from the heldout set
for validation (heldout-0,10) and test (heldout-
20,30,40). We tuned the PDR coefficient coarsely
in the vicinity of 0.001. While the baseline model
achieved a validation (test) perplexity of 44.3
(43.1), on applying PDR, the model achieved a

perplexity of 44.0 (42.5). Thus, PDR is relatively
less effective on larger datasets, a fact also ob-
served for other regularization techniques on such
datasets (Yang et al., 2017).

5 Results on Character Level Language
Modeling

The results on PTBC are shown in Table 4. Our
method achieves a bits-per-character (BPC) per-
formance of 1.169 on the PTBC test set, improv-
ing on the model without PDR by 0.006 or 0.5%.
It is notable that even with this highly processed
dataset and a small vocabulary of only 51 tokens,
our method improves on already highly regular-
ized models. Finally, we present results on En-
wik8 in Table 5. AWD-LSTM+PDR achieves
1.245 BPC. This is 0.012 or about 1% less than
the 1.257 BPC achieved by AWD-LSTM in our
experiments (with hyperparameters from (Merity
et al., 2018a).

6 Analysis of PDR

In this section, we analyze PDR by probing its per-
formance in several ways and comparing it with
models that do not use PDR.

6.1 A Valid Regularization

To verify that indeed PDR can act as a form of
regularization, we perform the following exper-



1473

Model #Params Valid Test

Sate-of-the-art Methods (Single Softmax)

(Merity et al., 2018b) – AWD-LSTM 33.6M 68.6 65.8
(Merity et al., 2018b) – AWD-LSTM + continuous cache pointer 33.6M 53.8 52.0
(Krause et al., 2018) – AWD-LSTM + dynamic evaluation 33.6M 46.4 44.3
(Gong et al., 2018) – AWD-LSTM + cont. cache ptr w/ FRAGE 33.6M 51.0 49.3

Our Method (Single Softmax)

AWD-LSTM+PDR 33.6M 66.5 63.5 (-2.3)
AWD-LSTM+PDR + continuous cache pointer 33.6M 51.5 49.6 (-2.4)
AWD-LSTM+PDR + dynamic evaluation 33.6M 44.6 42.6 (-1.7)

Sate-of-the-art Methods (Mixture-of-Softmax)

(Yang et al., 2017) – AWD-LSTM-MoS 35M 63.9 61.5
(Yang et al., 2017) – AWD-LSTM-MoS + dynamic evaluation 35M 42.4 40.7
(Gong et al., 2018) – AWD-LSTM-MoS + dyn. eval w/ FRAGE 35M 40.9 39.1

Our Method (Mixture-of-Softmax)

AWD-LSTM-MoS+PDR 35M 63.0 60.5 (-1.0)
AWD-LSTM-MoS+PDR + dynamic evaluation 35M 42.0 40.3 (-0.4)

Table 3: Perplexities on WikiText-2 (WT2) for single and mixture-of-softmaxes models. Values in parentheses
show gain over respective models without using PDR. The number of parameters during training is 33.8M. We do
not use frequency agnostic word embeddings.

Model #Prms Test

(Krueger et al., 2016) – Zoneout LSTM - 1.27
(Chung et al., 2016) – HM-LSTM - 1.24
(Ha et al., 2016) – HyperLSTM 14.4M 1.219
(Zoph and Le, 2016) – NAS Cell 16.3M 1.214
(Mujika et al., 2017) – FS-LSTM-4 6.5M 1.193
(Merity et al., 2018a) – AWD-LSTM 13.8M 1.175

Our Method

AWD-LSTM+PDR 13.8M 1.169
(-0.006)

Table 4: Bits-per-character on the PTBC test set.

iment. We take the models for PTB and WT2
and turn off all dropouts and regularization and
compare its performance with only PDR turned
on. The results, as shown in Table 6, validate the
premise of PDR. The model with only PDR turned
on achieves 2.4 and 5.1 better validation perplexity
on PTB and WT2 as compared to the model with-
out any regularization. Thus, biasing the LSTM by
decoding the distribution of past tokens from the
predicted next-token distribution can indeed act as
a regularizer leading to better generalization per-
formance.

Next, in Fig. 1(a) we plot histograms of the neg-
ative log-likelihoods of the correct context tokens
xt in the past decoded vector wrt computed using

Model #Prms Test

(Ha et al., 2016) – HyperLSTM 27M 1.340
(Chung et al., 2016) – HM-LSTM 35M 1.32
(Rocki et al., 2016) – SD Zoneout 64M 1.31
(Zilly et al., 2017) – RHN (depth 10) 21M 1.30
(Zilly et al., 2017) – Large RHN 46M 1.270
(Mujika et al., 2017) – FS-LSTM-4 27M 1.277
(Mujika et al., 2017) – Large FS-LSTM-4 47M 1.245
(Merity et al., 2018a) – AWD-LSTM 47M 1.232

Our Method

AWD-LSTM (Ours) 47M 1.257
AWD-LSTM+PDR 47M 1.245

Table 5: Bits-per-character on Enwik8 test set.

PTB Valid WT2 Valid

AWD-LSTM (NoReg) 108.6 142.7
AWD-LSTM (NoReg) + PDR 106.2 137.6

Table 6: Validation perplexities for AWD-LSTM with-
out any regularization and with only PDR.

our best models on the PTB and WT2 validation
sets. The NLL values are significantly peaked near
0, which means that the past decoding operation is
able to decode significant amount of information
about the last token in the context.

To investigate the effect of hyperparameters on
PDR, we pick 60 sets of random hyperparameters



1474

0 2 4 6 8 10
0

0.1

0.2

0.3

Negative log-likelihood

N
or

m
al

iz
ed

fr
eq

ue
nc

y

PTB-Valid
WT2-Valid

(a) Histogram of the NLL of xt in the past decoded
vector wrt .

60 60.5 61 61.5 62
0.00

0.10

0.20

0.30

Perplexity

N
or

m
al

iz
ed

fr
eq

ue
nc

y

AWD-LSTM+PDR
AWD-LSTM

(b) Histogram of validation perplexities on PTB for a
set of different hyperparameters.

Figure 1: Context token NLL for AWD-LSTM+PDR and comparison with AWD-LSTM.

0 2 4 6 8 10
0.00

0.05

0.10

0.15

Predicted token entropy

N
or

m
al

iz
ed

fr
eq

ue
nc

y

AWD-LSTM+PDR
AWD-LSTM

(a) Histogram of entropies of wt+1 for PTB valid.

200 400 600 800 1,000 1,200

30

40

50

60

70

80

No. of epochs

Pe
rp

le
xi

ty
AWD-LSTM+PDR (Train)
AWD-LSTM (Train)
AWD-LSTM+PDR (Valid)
AWD-LSTM (Valid)

(b) Training curves on PTB showing perplexity. The
kink in the middle represents the start of finetuning.

Figure 2: Comparison between AWD-LSTM+PDR and AWD-LSTM.

in the vicinity of those reported by (Merity et al.,
2018b) and compute the validation set perplex-
ity after training (without finetuning) on PTB, for
both AWD-LSTM+PDR and AWD-LSTM. Their
histograms are plotted in Fig.1(b). The perplexi-
ties for models with PDR are distributed slightly to
the left of those without PDR. There appears to be
more instances of perplexities in the higher range
for models without PDR. Note that there are cer-
tainly hyperparameter settings where adding PDR
leads to lower validation complexity, as is gener-
ally the case for any regularization method.

6.2 Comparison with AWD-LSTM

To show the qualitative difference between AWD-
LSTM+PDR and AWD-LSTM, in Fig.2(a), we
plot a histogram of the entropy of the predicted

next token distribution wt+1 for all the tokens in
the validation set of PTB achieved by their respec-
tive best models. The distributions for the two
models is slightly different, with some identifiable
patterns. The use of PDR has the effect of reduc-
ing the entropy of the predicted distribution when
it is in the higher range of 8 and above, pushing
it into the range of 5-8. This shows that one way
PDR biases the language model is by reducing the
entropy of the predicted next token distribution.
Indeed, one way to reduce the cross-entropy be-
tween xt and wrt is by making wt+1 less spread
out in Eq.(5). This tends to benefit the language
model when the predictions are correct.

We also compare the training curves for the
two models in Fig.2(b) on PTB. Although the
two models use slightly different hyperparame-



1475

PTB WT2
Model Valid Test Valid Test

AWD-LSTM+PDR 57.9 55.6 66.5 63.5
– finetune 60.4 58.0 68.5 65.6

– LSTM output dropout 67.6 65.4 75.4 72.1
– LSTM layer dropout 68.1 65.8 73.7 70.4
– embedding dropout 63.9 61.4 77.1 73.6
– word dropout 62.9 60.5 70.4 67.4
– LSTM weight dropout 68.4 65.8 79.0 75.5
– alpha/beta regularization 63.0 60.4 74.0 70.7
– weight decay 64.7 61.4 72.5 68.9
– PDR 60.5 57.7 69.5 66.4

Table 7: Ablation experiments on the PTB and WT2 validation and test sets.

ters, the regularization effect of PDR is apparent
with a lower validation perplexity but higher train-
ing perplexity. The corresponding trends shown in
Fig.2(a,b) for WT2 have similar characteristics.

6.3 Ablation Studies

We perform a set of ablation experiments on the
best AWD-LSTM+PDR models for PTB and WT2
to understand the relative contribution of PDR and
the other regularizations used in the model. The
results are shown in Table 7. In both cases, PDR
has a significant effect in decreasing the validation
set performance, albeit lesser than the other forms
of regularization. This is not surprising as PDR
does not act on the LSTM weights directly.

7 Related Work

Our proposed Past Decode Regularization method
builds on the work of using sophisticated regular-
ization techniques to train LSTMs for language
modeling. In particular, the AWD-LSTM model
achieves state-of-the-art performance with a sin-
gle softmax on the four datasets considered in this
paper (Merity et al., 2018b,a). (Melis et al., 2018)
also achieve similar results with highly regular-
ized LSTMs. By addressing the so-called soft-
max bottleneck in single softmax models, (Yang
et al., 2017) use a mixture-of-softmaxes to achieve
significantly lower perplexities. PDR utilizes the
symmetry between the inputs and outputs of a lan-
guage model, a fact that is also exploited in weight
tying (Inan et al., 2016; Press and Wolf, 2017).
Our method can be used with untied weights as
well. Although motivated by language model-
ing, PDR can also be applied to seq2seq mod-
els with shared input-output vocabularies, such
as those used for text summarization and neural
machine translation (with byte pair encoding of

words) (Press and Wolf, 2017). Regularizing the
training of an LSTM by combining the main ob-
jective function with auxiliary tasks has been suc-
cessfully applied to several tasks in NLP (Radford
et al., 2018; Rei, 2017). In fact, a popular choice
for the auxiliary task is language modeling itself.
This in turn is related to multi-task learning (Col-
lobert and Weston, 2008).

Specialized architectures like Recurrent High-
way Networks (Zilly et al., 2017) and NAS
(Zoph and Le, 2016) have been successfully used
to achieve competitive performance in language
modeling. The former makes the hidden-to-
hidden transition function more complex allow-
ing for more refined information flow. Such ar-
chitectures are especially important for character
level language modeling where strong results have
been shown using Fast-Slow RNNs (Mujika et al.,
2017), a two level architecture where the slowly
changing recurrent network tries to capture more
long range dependencies. The use of historical in-
formation can greatly help language models deal
with long range dependencies as shown by (Merity
et al., 2016; Krause et al., 2018; Rae et al., 2018).
In a recent paper, (Gong et al., 2018) achieve im-
proved performance for language modeling by us-
ing frequency agnostic word embeddings (called
FRAGE), a technique that is orthogonal to PDR
and can be combined with it.

8 Conclusion

We propose a new Past Decode Regularization
(PDR) method for language modeling that ex-
ploits the input-output symmetry in each step to
decode the last token in the context from the pre-
dicted next token distribution. We empirically
show reductions in perplexity on several bench-
mark datasets as compared to strong highly reg-



1476

ularized baseline models. Future work includes
exploring the application of PDR to other seq2seq
models that have a similar input-output symmetry.
Also, it will be worthwhile to ascertain the efficacy
of PDR for language models using Transformers
and in combination with FRAGE embeddings.

References
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,

Thorsten Brants, and Phillipp Koehn. 2014. One bil-
lion word benchmark for measuring progress in sta-
tistical language modeling. In INTERSPEECH.

Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2016. Hierarchical multiscale recurrent neural net-
works. CoRR, abs/1609.01704.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In NIPS.

ChengYue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,
and Tie-Yan Liu. 2018. Frage: Frequency-agnostic
word representation. CoRR, abs/1809.06858.

Edouard Grave, Armand Joulin, and Nicolas Usunier.
2016. Improving neural language models with a
continuous cache. CoRR, abs/1612.04426.

David Ha, Andrew M. Dai, and Quoc V. Le. 2016. Hy-
pernetworks. CoRR, abs/1609.09106.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

M. Hutter. 2018. The human knowledge compression
contest.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2016. Tying word vectors and word classifiers:
A loss framework for language modeling. CoRR,
abs/1611.01462.

Ben Krause, Emmanuel Kahembwe, Iain Murray, and
Steve Renals. 2018. Dynamic evaluation of neural
sequence models. In ICML.

David Krueger, Tegan Maharaj, János Kramár, Mo-
hammad Pezeshki, Nicolas Ballas, Nan Rose-
mary Ke, Anirudh Goyal, Yoshua Bengio, Hugo
Larochelle, Aaron C. Courville, and Christo-
pher Joseph Pal. 2016. Zoneout: Regularizing rnns
by randomly preserving hidden activations. CoRR,
abs/1606.01305.

Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On
the state of the art of evaluation in neural language
models. In ICLR.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018a. An analysis of neural language mod-
eling at multiple scales. CoRR, abs/1803.08240.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018b. Regularizing and optimizing LSTM
language models. In ICLR.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. CoRR, abs/1609.07843.

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.

Asier Mujika, Florian Meier, and Angelika Steger.
2017. Fast-slow recurrent neural networks. In NIPS.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In EACL.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Jack W. Rae, Chris Dyer, Peter Dayan, and Timothy P.
Lillicrap. 2018. Fast parametric learning with acti-
vation memorization. In ICML.

Marek Rei. 2017. Semi-supervised multitask learning
for sequence labeling. In ACL.

Kamil Rocki, Tomasz Kornuta, and Tegan Ma-
haraj. 2016. Surprisal-driven zoneout. CoRR,
abs/1610.07675.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research, 15:1929–1958.

Li Wan, Matthew D. Zeiler, Sixin Zhang, Yann LeCun,
and Rob Fergus. 2013. Regularization of neural net-
works using dropconnect. In ICML.

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W. Cohen. 2017. Breaking the softmax bot-
tleneck: A high-rank rnn language model. CoRR,
abs/1711.03953.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
CoRR, abs/1409.2329.

Julian G. Zilly, Rupesh Kumar Srivastava, Jan Koutnı́k,
and Jürgen Schmidhuber. 2017. Recurrent highway
networks. In ICML.

Barret Zoph and Quoc V. Le. 2016. Neural archi-
tecture search with reinforcement learning. CoRR,
abs/1611.01578.

https://doi.org/10.1162/neco.1997.9.8.1735
http://prize.hutter1.net
http://prize.hutter1.net
http://proceedings.mlr.press/v80/krause18a.html
http://proceedings.mlr.press/v80/krause18a.html
http://arxiv.org/abs/1707.05589
http://arxiv.org/abs/1707.05589
http://arxiv.org/abs/1707.05589
http://arxiv.org/abs/1708.02182
http://arxiv.org/abs/1708.02182

