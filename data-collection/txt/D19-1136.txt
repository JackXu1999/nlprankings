



















































Vecalign: Improved Sentence Alignment in Linear Time and Space


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1342–1348,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1342

Vecalign: Improved Sentence Alignment in Linear Time and Space

Brian Thompson
Johns Hopkins University

brian.thompson@jhu.edu

Philipp Koehn
Johns Hopkins University

phi@jhu.edu

Abstract

We introduce Vecalign, a novel bilingual sen-
tence alignment method which is linear in time
and space with respect to the number of sen-
tences being aligned and which requires only
bilingual sentence embeddings. On a standard
German–French test set, Vecalign outperforms
the previous state-of-the-art method (which
has quadratic time complexity and requires a
machine translation system) by 5 F1 points.
It substantially outperforms the popular Hun-
align toolkit at recovering Bible verse align-
ments in medium- to low-resource language
pairs, and it improves downstream MT qual-
ity by 1.7 and 1.6 BLEU in Sinhala→English
and Nepali→English, respectively, compared
to the Hunalign-based Paracrawl pipeline.

1 Introduction

Sentence alignment is the task of taking parallel
documents, which have been split into sentences,
and finding a bipartite graph which matches min-
imal groups of sentences that are translations of
each other (see Figure 1). Following prior work,
we assume non-crossing alignments but allow lo-
cal sentence reordering within an alignment.

Sentence-aligned bitext is used to train nearly
all machine translation (MT) systems. Alignment
errors have been noted to have a small effect on
statistical MT performance (Goutte et al., 2012).
However, misaligned sentences have been shown
to be much more detrimental to neural MT (NMT)
(Khayrallah and Koehn, 2018).

Sentence alignment was a popular research
topic in the early days of statistical MT, but
received less attention once standard sentence-
aligned parallel corpora became available. Inter-
est in low-resource MT has led to a resurgence in
data gathering methods (Buck and Koehn, 2016;
Zweigenbaum et al., 2018; Koehn et al., 2019), but

Figure 1: Sentence alignment takes sentences e1,...,eN
and f1,...,fM and locates minimal groups of sentences
which are translations of each other, in this case (e1)-
(f1, f2), (e2)-(f3), (e3,e4)-(f4), and (e5)-(f6).

we find limited recent work on bilingual sentence
alignment.

Automatic sentence alignment can be roughly
decomposed into two parts:

1. A score function which takes one or more ad-
jacent source sentences and one or more adja-
cent target sentences and returns a score indi-
cating the likelihood that they are translations
of each other;

2. An alignment algorithm which, using the
score function above, takes in two documents
and returns a hypothesis alignment.

We improve both parts, presenting (1) a novel
scoring function based on normalized cosine dis-
tance between multilingual sentence embeddings,
in conjunction with (2) a novel application of a dy-
namic programming approximation (Salvador and
Chan, 2007) which makes our algorithm linear
in time and space complexity with respect to the
number of sentences being aligned. We release a
toolkit containing our implementation.1

Our method outperforms previous state-of-the-
art, which has quadratic complexity, indicating
that our proposed score function outperforms prior
work and the approximations we make in align-
ment are sufficiently accurate.

1https://github.com/thompsonb/vecalign

https://github.com/thompsonb/vecalign


1343

2 Related Work

Early sentence aligners (Brown et al., 1991; Gale
and Church, 1993) use scoring functions based
only on the number of words or characters in each
sentence and alignment algorithms based on dy-
namic programming (DP; Bellman, 1953). DP is
O(NM ) time complexity, where N and M are
the number of sentences in the source and tar-
get documents. Later work added lexical features
and heuristics to speed up search, such as limiting
the search space to be near the diagonal (Moore,
2002; Varga et al., 2007). More recent work intro-
duced scoring methods that use MT to get both
documents into the same language (Bleualign;
Sennrich and Volk, 2010) or use pruned phrase
tables from a statistical MT system (Coverage-
Based; Gomes and Lopes, 2016). Both methods
“anchor” high-probability 1–1 alignments in the
search space and then fill in and refine alignments.
Locating anchors is O(NM) time complexity.

3 Method

We propose a novel sentence alignment scoring
function based on the similarity of bilingual sen-
tence embeddings. A distinct but non-obvious
advantage of sentence embeddings is that blocks
of sentences can be represented as the average
of their sentence embeddings. The size of the
resulting vector is not dependent on the num-
ber of sentence embeddings being averaged, thus
the time/space cost of comparing the similarity of
blocks of sentences does not depend on the num-
ber of sentences being compared. We show em-
pirically (see § 4.2) that average embeddings for
blocks of sentences are sufficient to produce ap-
proximate alignments, even in low-resource lan-
guages. This enables us to approximate DP in
O(N+M ) in time and space.

3.1 Bilingual Sentence Embeddings
We propose to use the similarity between sentence
embeddings as the scoring function for sentence
alignment. Sentence embedding similarity has
been shown effective at filtering out non-parallel
sentences (Hassan et al., 2018; Chaudhary et al.,
2019) and locating parallel sentences in compara-
ble corpora (Guo et al., 2018). We use the pub-
licly available LASER multilingual sentence em-
bedding method (Artetxe and Schwenk, 2018) and
model, which is pretrained on 93 languages. How-
ever, our method is not specific to LASER.

3.2 Scoring Function
Cosine similarity is an obvious choice for com-
paring embeddings but has been noted to be glob-
ally inconsistent due to “hubness” (Radovanović
et al., 2010; Lazaridou et al., 2015). Guo et al.
(2018) proposed a supervised training approach
for calibration, and Artetxe and Schwenk (2019)
proposed normalization using nearest neighbors.
We propose normalizing instead with randomly
selected embeddings as it has linear complexity.
Sentence alignment seeks minimal parallel units,
but we find that DP with cosine similarity favors
many-to-many alignments (e.g. reporting a 3–3
alignment when it should report three 1–1 align-
ments). To remedy this issue, we scale the cost
by the number of source and target sentences be-
ing considered in a given alignment. Our resulting
scoring cost function is:

c(x,y)=
(1−cos(x,y)) nSents(x) nSents(y)
S∑

s=1
1−cos(x,ys)+

S∑
s=1

1−cos(xs,y)

where x, y denote one or more sequential sen-
tences from the source/target document; cos(x,y)
is the cosine similarity between embeddings2 of
x, y; nSents(x), nSents(y) denote the number of
sentences in x, y; and x1,...,xS , y1,...,yS are sam-
pled uniformly from the given document.

Following standard practice, we model inser-
tions and deletions in DP using a skip cost cskip.
The raw value of cskip is only meaningful when
compared to other costs, thus we do not expect it
to generalize across different languages, normal-
izations, or resolutions. We propose specifying in-
stead a parameter βskip which defines the skip cost
in terms of the distribution of 1–1 alignment costs
at alignment time: cskip=CDF−1(βskip). CDF is
an estimate of the cumulative distribution function
of 1–1 alignments obtained by computing costs of
randomly selected source/target sentences pairs.

3.3 Recursive DP Approximation
Instead of searching all possible sentence align-
ments via DP, consider first averaging adjacent
pairs of sentence embeddings in both the source
and target documents, halving the number of em-
beddings for each document. Aligning these vec-
tors via DP (each of which are averages of 2 sen-
tence embeddings) produces an approximate sen-

2If multiple sentences are considered on one side, they are
concatenated together before embedding.



1344

Figure 2: 1–1 alignment costs (darker = lower) for the first 88 De lines (x-axis) and 128 Fr lines (y-axis) at 4
different resolutions. The red highlight denotes alignment found by DP. The algorithm only searches near the path
found at previous resolutions; light blue regions are excluded. The vertical part of the path in the top left of each
plot is due to 36 extra lines being present in the Fr document. Window size is increased for visualization purposes.

tence alignment, at a cost of
(
N
2

)(
M
2

)
compar-

isons. We can then refine this approximate align-
ment using the original sentence vectors, con-
straining ourselves to a small window around the
approximate alignment. At a minimum, we must
search a window size w large enough to consider
all paths covered by the lower-resolution align-
ment path, but w can also be increased to allow re-
covery from small errors in the approximate align-
ment.3 The length of the refinement path to search
is at most N +M (all deletions/insertions), so re-
fining the path requires at most (N +M)w com-
parisons. Thus the full NM comparisons can be
approximated by (N +M)w+

(
N
2

)(
M
2

)
compar-

isons. Applied recursively,4 we can approximate
our quadratic NM cost with a sum of linear costs:

(N+M)w+

(
N

2
+
M

2

)
w+

(
N

4
+
M

4

)
w+ ...

=
∑

k=0,1,2,...

(N+M)w

2k
=2(N+M)w

See Figure 2 for an illustration of this method. We
consider only insertions, deletions, and 1–1 align-
ments in all but the final search. Recursive down
sampling and refining of DP was proposed for dy-
namic time warping in Salvador and Chan (2007),
but has not previously been applied to sentence
alignment. We direct the reader to that work for a
more formal analysis showing the time/space com-
plexity is linear.

3We use w=10 for all experiments in this work.
4In practice, we compute the full DP alignment once the

down sampled sizes are below an acceptably small constant.
We also find vectors for large blocks of sentences become
correlated with each other, so we center them around ~0.

4 Experiments & Results

4.1 Text+Berg Alignment Accuracy

We evaluate sentence alignment accuracy using
the development/test split released with Bleualign,
consisting of manually aligned yearbook articles
published in both German and French by the Swiss
Alpine Club from the Text+Berg corpus (Volk
et al., 2010). Hyperparameters were chosen to op-
timize F1 on the development set. We consider
alignments of up to 6 total sentences; that is we
allow alignments of size Q–R where Q+R≤ 6.

We compare to Gale and Church (1993), Moore
(2002), Hunalign (Varga et al., 2007), Bleualign
(Sennrich and Volk, 2010), Gargantua (Braune and
Fraser, 2010), and Coverage-Based (Gomes and
Lopes, 2016). We run Hunalign in both bootstrap-
ping mode as well as using a publically available
De–Fr lexicon from OPUS (Tiedemann, 2012)5

created from Europarl (Koehn, 2005). Since
Bleualign depends on the quality of MT output,
we re-run it with a modern NMT system.6

Our proposed method outperforms the next best
method (Coverage-Based) by 5 F1 points: see Ta-
ble 1. Gargantua and bootstrapped Hunalign have
both been reported to perform well (Abdul-Rauf
et al., 2012); this dataset may be too small to boot-
strap good lexical features.7 Bleualign improves
by 3 F1 points by using an NMT system.

5https://object.pouta.csc.fi/
OPUS-Europarl/v7/dic/de-fr.dic.gz

6https://docs.microsoft.com/en-us/
azure/cognitive-services/translator/

7We run only on the test/development articles, not the full
Text+Berg corpus.

https://object.pouta.csc.fi/OPUS-Europarl/v7/dic/de-fr.dic.gz
https://object.pouta.csc.fi/OPUS-Europarl/v7/dic/de-fr.dic.gz
https://docs.microsoft.com/en-us/azure/cognitive-services/translator/
https://docs.microsoft.com/en-us/azure/cognitive-services/translator/


1345

Algorithm O( ) P R F1

Gargantua N2 0.48 0.54 0.51
Hunalign w/o lexicon N 0.59 0.70 0.64
Hunalign w/ lexicon N 0.61 0.73 0.66
Gale and Church (1993)† N2 0.71 0.72 0.72
Moore (2002)† ‡ 0.86 0.71 0.78
Bleualign† N2 0.83 0.78 0.81
Bleualign-NMT N2 0.85 0.83 0.84
Coverage-Based* N2 0.85 0.84 0.85
Vecalign N 0.89 0.90 0.90

Table 1: De–Fr test precision (P), recall (R), and F1.
*best reported in Gomes and Lopes (2016). †Best re-
ported in Sennrich and Volk (2010). ‡O( ) is data de-
pendent. We assume N =M for simplicity.

Language ISO Bible LASER
639-1 # Sents # Train Lines

Arabic Ar 45980 8.2M
Turkish Tr 48492 5.7M
Somali So 37413 85k
Afrikaans Af 37081 67k
Tagalog Tl 34207 36k
Norwegian No 37064 0*

Table 2: Bible statistics. *LASER was not trained on
Norwegian but appears to generalize to it.

4.2 Bible Alignment Accuracy

We are unaware of a multilingual, low resource,
parallel dataset with human sentence-level anno-
tations. As a substitute for gold standard sentence
alignment, we use Bible verse alignment and sen-
tence split each verse.8

The Bible has a number of properties which
make it appealing for sentence alignment evalua-
tion: It is much larger than existing sentence align-
ment test sets, and it is multi-way parallel in a
large number of languages. Bibles are not aligned
at the sentence level, but contain verse marking
denoting segments typically on the scale of a par-
tial sentence to a few sentences. This creates two
potential issues for sentence alignment evaluation:
First, a single sentence may span more than one
verse. Inspecting the English Bible suggests that
this is rare, and sentence aligners should be able to
handle occasional over-segmentation of sentences
as in practice they are run on errorful automatic
sentence segmentation. Second, a verse may con-
tain more than one sentence. This is problematic

8There is no clear choice for sentence seg-
mentation in low-resource languages. We use
https://github.com/berkmancenter/
mediacloud-sentence-splitter, falling back
on English for unsupported languages.

Verse-level F1
Languages Vecalign Hunalign

Af–Ar 0.863 0.339
Af–Tl 0.922 0.775
Ar–No 0.787 0.406
Ar–So 0.634 0.067
Tr–So 0.533 0.331
No–So 0.697 0.687
So–Af 0.782 0.738
Tl–No 0.874 0.764
Tr–Af 0.703 0.401
Tr–Tl 0.647 0.247

Table 3: Bible verse alignment results.

when it happens on both languages being aligned,
since the true sentence alignment cannot be deter-
mined (e.g., a verse which is two sentences in each
language could be two 1–1 alignments or one 2–2
alignment). To evaluate with verse-level annota-
tions, we propose converting the sentence align-
ment output into verse alignments by combining
any consecutive sentence alignments for which all
sentences in the alignments, on both the source
and target side, came from the same verse. We re-
port F1 compared to the gold-standard verse align-
ments, denoting it as verse-level F1 to distinguish
it from F1 computed at the sentence level.

We select six languages for which
Christodouloupoulos and Steedman (2015)
contains a full Bible: see Table 2. Languages
were chosen to provide a range of amounts of
training data used in LASER.9 From those six
languages, we randomly select 10 language pairs
for testing. All parameters are kept the same as
§ 4.1 except we only consider alignments of up to
4 total sentences. We compare to Hunalign, run in
bootstrap mode, as it is the only toolkit we tried
which was robust enough to run on documents of
this size. Results are shown in Table 3.

On average, we see an improvement of 28
verse-level F1 points over Hunalign. In manual
analysis of the alignments we find large stretches
where the Hunalign alignments are nowhere near
the gold alignment in the language pairs with
verse-level F1 < 0.35. By contrast, errors in the
proposed method are predominately local, indicat-
ing success of Vecalign’s recursive DP approxima-
tion even for very long documents in low-resource
languages.

9Data amounts are all between the given language and En-
glish. LASER used no bitext in the language pairs under test.

https://github.com/berkmancenter/mediacloud-sentence-splitter
https://github.com/berkmancenter/mediacloud-sentence-splitter


1346

4.3 Improvements to Downstream MT

One of the primary applications of sentence align-
ment is creating bitext for training MT systems.
To test Vecalign’s impact on downstream MT
quality, we re-align noisy, web-crawled data in
two low-resource language pairs: Sinhala–English
and Nepali–English. The data is collected via
Paracrawl10 and is very similar to that released
in the WMT 2019 sentence filtering task (Koehn
et al., 2019), but some new data has been col-
lected and a small amount of data was lost due to
a hard disk failure. Our baseline is the standard
Paracrawl pipeline using Hunalign in conjunction
with a dictionary extracted from the clean data re-
leased in the shared task.

We filter the output of Vecalign and Hunalign
following (Chaudhary et al., 2019), including fil-
tering out sentences with the wrong languages and
sentences with high token overlap, as this was the
best performing method from the shared task.11

We train and evaluate NMT models following the
procedure/hyperparameters from the shared task.

Results are shown in Figure 3. Using Ve-
calign, we see improvements of 1.7 and 1.6 BLEU
for the best data sizes in Sinhala→English and
Nepali→English, respectively, compared to the
systems trained on Hunalign output.

4

6

8

10

B
L

E
U

Si→En

1 2 3 4 5

·106

0

2

4

6

En words

B
L

E
U

Ne→En

Vecalign
Hunalign with lexicon

Figure 3: sacreBLEU scores (mean +/- standard devia-
tion for 5 training runs) on FLoRes test sets for systems
trained on data aligned with Vecalign vs Hunalign.

10https://paracrawl.eu/
11We use the publicly available multilingual LASER

model, which is not trained on Nepali.

5 Empirical Runtime Analysis

101 102 103 104 105
10−1

100

101

102

103

104

Document Size (# Sentences)

R
un

tim
e

(s
)

Gargantua
Bleualign
Vecalign
Hunalign

Figure 4: Time required to align various portions of
En→Hu Bibles, for various systems. Plot is logarith-
mic in both runtime and number of sentences, thus a
slope of one (i.e. runtime doubles each time the number
of sentences doubles) indicates O(N), while a slope of
two (i.e. runtime quadruples each time the number of
sentences doubles) indicates O(N2).

Time required to align documents of various
sizes are shown for Vecalign, Bleualign, Gartan-
tua, and Hunalign in see Figure 4. As expected,
Vecalign has approximately linear runtime char-
acteristics. We use truncated portions of Hu–
En Bibles in order to use the dictionary provided
with Hunalign. Bleualign is run on NMT out-
put. Vecalign settings match § 4.2. Experiments
are run on a Thinkpad T480 with 32GB RAM.
Times do not include translation (Bleualign), lex-
icon building (Hunalign), or sentence embedding
(Vecalign). For reference, producing embeddings
for 32k sentences, including overlaps, in each lan-
guage took ~120 s on a GeForce RTX 2080 Ti
GPU. Bleualign and Gargantua run out of mem-
ory on 32k sentences. Hunalign and Vecalign use
~1GB and are both very fast, aligning 32k sen-
tences in ~30 s.

6 Conclusions

We present Vecalign, a novel sentence alignment
method based on similarity of sentence embed-
dings and a DP approximation which is fast even
for long documents. Our method has state-of-the-
art accuracy in high and low resource settings and
improves downstream MT quality.

https://paracrawl.eu/


1347

Acknowledgments

Brian Thompson is supported through the Na-
tional Defense Science and Engineering Graduate
(NDSEG) Fellowship Program.

References
Sadaf Abdul-Rauf, Mark Fishel, Patrik Lambert, San-

dra Noubours, and Rico Sennrich. 2012. Extrinsic
evaluation of sentence alignment systems. Work-
shop on Creating Cross-language Resources for
Disconnected Languages and Styles, pages 6–10.

Mikel Artetxe and Holger Schwenk. 2018. Mas-
sively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. arXiv
preprint arXiv:1812.10464.

Mikel Artetxe and Holger Schwenk. 2019. Margin-
based parallel corpus mining with multilingual sen-
tence embeddings. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 3197–3203, Florence, Italy. Asso-
ciation for Computational Linguistics.

Richard Bellman. 1953. An introduction to the theory
of dynamic programming. Technical report, RAND
Corporation, Santa Monica, CA.

Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for sym-
metrical and asymmetrical parallel corpora. In Col-
ing 2010: Posters, pages 81–89, Beijing, China.
Coling 2010 Organizing Committee.

Peter F. Brown, Jennifer C. Lai, and Robert L. Mer-
cer. 1991. Aligning sentences in parallel corpora.
In Proceedings of the 29th Annual Meeting on As-
sociation for Computational Linguistics, ACL ’91,
pages 169–176, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Christian Buck and Philipp Koehn. 2016. Findings
of the WMT 2016 bilingual document alignment
shared task. In Proceedings of the First Conference
on Machine Translation: Volume 2, Shared Task Pa-
pers, pages 554–563, Berlin, Germany. Association
for Computational Linguistics.

Vishrav Chaudhary, Yuqing Tang, Francisco Guzmn,
Holger Schwenk, and Philipp Koehn. 2019. Low-
resource corpus filtering using multilingual sentence
embeddings. In Proceedings of the Fourth Con-
ference on Machine Translation (Volume 3: Shared
Task Papers, Day 2), pages 263–268, Florence, Italy.
Association for Computational Linguistics.

Christos Christodouloupoulos and Mark Steedman.
2015. A massively parallel corpus: the Bible in
100 languages. Language resources and evaluation,
49(2):375–395.

William A Gale and Kenneth W Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational linguistics, 19(1):75–102.

Luı́s Gomes and Gabriel Pereira Lopes. 2016. First
steps towards coverage-based sentence alignment.
In Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation (LREC
2016), pages 2228–2231, Portorož, Slovenia. Euro-
pean Language Resources Association (ELRA).

Cyril Goutte, Marine Carpuat, and George Foster.
2012. The impact of sentence alignment errors on
phrase-based machine translation performance. In
The Tenth Biennial Conference of the Association for
Machine Translation in the Americas (AMTA 2012).

Mandy Guo, Qinlan Shen, Yinfei Yang, Heming
Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith
Stevens, Noah Constant, Yun-Hsuan Sung, Brian
Strope, and Ray Kurzweil. 2018. Effective parallel
corpus mining using bilingual sentence embeddings.
In Proceedings of the Third Conference on Machine
Translation: Research Papers, pages 165–176, Bel-
gium, Brussels. Association for Computational Lin-
guistics.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic Chinese to English news
translation. arXiv preprint arXiv:1803.05567.

Huda Khayrallah and Philipp Koehn. 2018. On the
impact of various types of noise on neural machine
translation. In Proceedings of the 2nd Workshop on
Neural Machine Translation and Generation, pages
74–83, Melbourne, Australia. Association for Com-
putational Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion summit, volume 5, pages 79–86.

Philipp Koehn, Francisco Guzmn, Vishrav Chaud-
hary, and Juan Pino. 2019. Findings of the WMT
2019 shared task on parallel corpus filtering for
low-resource conditions. In Proceedings of the
Fourth Conference on Machine Translation (Volume
3: Shared Task Papers, Day 2), pages 56–74, Flo-
rence, Italy. Association for Computational Linguis-
tics.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 270–
280, Beijing, China. Association for Computational
Linguistics.

https://www.aclweb.org/anthology/P19-1309
https://www.aclweb.org/anthology/P19-1309
https://www.aclweb.org/anthology/P19-1309
https://www.aclweb.org/anthology/C10-2010
https://www.aclweb.org/anthology/C10-2010
https://www.aclweb.org/anthology/C10-2010
https://doi.org/10.3115/981344.981366
https://doi.org/10.18653/v1/W16-2347
https://doi.org/10.18653/v1/W16-2347
https://doi.org/10.18653/v1/W16-2347
http://www.aclweb.org/anthology/W19-5435
http://www.aclweb.org/anthology/W19-5435
http://www.aclweb.org/anthology/W19-5435
https://www.aclweb.org/anthology/J93-1004
https://www.aclweb.org/anthology/J93-1004
https://www.aclweb.org/anthology/L16-1354
https://www.aclweb.org/anthology/L16-1354
https://doi.org/10.18653/v1/W18-6317
https://doi.org/10.18653/v1/W18-6317
https://doi.org/10.18653/v1/W18-2709
https://doi.org/10.18653/v1/W18-2709
https://doi.org/10.18653/v1/W18-2709
http://www.aclweb.org/anthology/W19-5404
http://www.aclweb.org/anthology/W19-5404
http://www.aclweb.org/anthology/W19-5404
https://doi.org/10.3115/v1/P15-1027
https://doi.org/10.3115/v1/P15-1027


1348

Robert C Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Conference of the
Association for Machine Translation in the Ameri-
cas, pages 135–144. Springer.

Miloš Radovanović, Alexandros Nanopoulos, and Mir-
jana Ivanović. 2010. Hubs in space: Popular nearest
neighbors in high-dimensional data. Journal of Ma-
chine Learning Research, 11(Sep):2487–2531.

Stan Salvador and Philip Chan. 2007. Toward accu-
rate dynamic time warping in linear time and space.
Intelligent Data Analysis, 11(5):561–580.

Rico Sennrich and Martin Volk. 2010. MT-based sen-
tence alignment for OCR-generated parallel texts.
In The Ninth Conference of the Association for Ma-
chine Translation in the Americas (AMTA 2010).

Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC-2012), pages 2214–2218, Istan-
bul, Turkey. European Languages Resources Asso-
ciation (ELRA).

Dániel Varga, Péter Halácsy, András Kornai, Viktor
Nagy, László Németh, and Viktor Trón. 2007. Par-
allel corpora for medium density languages. Ams-
terdam Studies In The Theory And History Of Lin-
guistic Science Series 4, 292:247.

Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya
Bangerter, Lenz Furrer, and Beni Ruef. 2010. Chal-
lenges in building a multilingual alpine heritage cor-
pus. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC’10), Valletta, Malta. European Languages
Resources Association (ELRA).

Pierre Zweigenbaum, Serge Sharoff, and Reinhard
Rapp. 2018. Overview of the third BUCC shared
task: Spotting parallel sentences in comparable cor-
pora. In Proceedings of 11th Workshop on Building
and Using Comparable Corpora, pages 39–42.

http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/110_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/110_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/110_Paper.pdf

