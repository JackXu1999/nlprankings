



















































Executing Instructions in Situated Collaborative Interactions


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2119–2130,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2119

Executing Instructions in Situated Collaborative Interactions
Alane Suhr

Cornell University
suhr@cs.cornell.edu

Claudia Yan
IBM

claudiab.yan@gmail.com

Jacob Schluger∗
Cornell University
jes543@cornell.edu

Stanley Yu∗
Columbia University

stanley.yu@columbia.edu

Hadi Khader∗∗
Intel

hadi.kh.khader@gmail.com

Marwa Mouallem∗∗
IBM

marwamouallem@gmail.com

Iris Zhang
Facebook

irisz@fb.com

Yoav Artzi
Cornell University

yoav@cs.cornell.edu

Abstract

We study a collaborative scenario where a user
not only instructs a system to complete tasks,
but also acts alongside it. This allows the user
to adapt to the system abilities by changing
their language or deciding to simply accom-
plish some tasks themselves, and requires the
system to effectively recover from errors as
the user strategically assigns it new goals. We
build a game environment to study this sce-
nario, and learn to map user instructions to
system actions. We introduce a learning ap-
proach focused on recovery from cascading er-
rors between instructions, and modeling meth-
ods to explicitly reason about instructions with
multiple goals. We evaluate with a new evalu-
ation protocol using recorded interactions and
online games with human users, and observe
how users adapt to the system abilities.

1 Introduction
Sequential instruction scenarios commonly as-
sume only the system performs actions, and there-
fore only its behavior influences the world state.
This ignores the collaborative potential of such
interactive scenarios and the challenges it intro-
duces. When the user acts in the world as well,
they can adapt to the system abilities not only by
adopting simpler language, but also by deciding
to accomplish tasks themselves. The system must
then recover from errors as new instructions arrive
and be robust to changes in the environment that
are not a result of its own actions.

In this paper, we introduce CEREALBAR, a
collaborative game with natural language instruc-
tion, and design modeling, learning, and evalua-
tion methods for the problem of sequential instruc-
tion following in collaborative interactions. In CE-
REALBAR, two agents, a leader and a follower,

∗ ,∗∗: Equal contribution. All work done at Cornell.

Follower’s view

Leader’s view

Leader Follower

. . .
x̄3: turn left and head toward the yellow hearts, but don’t
pick them up yet. I’ll get the next card first.
x̄4: Okay, pick up yellow hearts and run past me toward
the bush sticking out, on the opposite side is 3 green stars
[Set made. New score: 4]
. . .

Figure 1: A snapshot from an interaction in CEREAL-
BAR. The current instruction is in bold. The large im-
age shows the entire environment. This overhead view
is available only to the leader. The follower sees a first-
person view only (bottom right). The zoom boxes (top)
show the leader and follower.

move in a 3D environment and collect valid sets
of cards to earn points. A valid set is a set of three
cards with distinct color, shape, and count. The
game is turn-based, and only one player can act
in each turn. In addition to collecting cards, the
leader sends natural language instructions to the
follower. The follower’s role is to execute these
instructions. Figure 1 shows a snapshot from the
game where the leader plans to pick up a nearby
card (red square) and delegates to the follower two
cards, one close and the other much further away.
Before that, the leader planned ahead and asked
the follower to move in preparation for the next
set. The agents have different skills to incentivize
collaboration. The follower has more moves per
turn, but can only see from first-person view, while
the leader observes the entire environment but has
fewer moves. This makes natural language inter-



2120

action key to success. We address the problem
of mapping the leader instructions to follower ac-
tions. In addition to the collaborative challenges,
this requires grounding natural language to resolve
spatial relations and references to objects, reason
about dependencies on the interaction history, re-
act to the changing environment as cards appear
and disappear, and generate actions.

CEREALBAR requires reasoning about the
changing environment (e.g., when selecting cards)
and instructions with multiple goals (e.g., select-
ing multiple cards). We build on the Visitation
Prediction Network model (VPN; Blukis et al.,
2018b), which casts planning as mapping instruc-
tions to the probability of visiting positions in the
environment. Our new model generalizes the plan-
ning space of VPN to reason about intermediate
goals and obstacles, and includes recurrent action
generation for trajectories with multiple goals.

We collect 1,202 human-to-human games for
training and evaluation. While our model could be
trained from these recorded games only, it would
often fail when an instruction would start at the
wrong position because of an error in following
the previous one. We design a learning algorithm
that dynamically augments the data with examples
that require recovering from such errors, and train
our model to distinguish such recovery reasoning
from regular instruction execution.

Evaluation with recorded games poses addi-
tional challenges. As agent errors lead to unex-
pected states, later instructions become invalid.
Because measuring task completion from such
states is meaningless, we propose cascaded eval-
uation, a new evaluation protocol that starts the
agent at different points in the interaction and mea-
sures how much of the remaining instructions it
can complete. In contrast to executing complete
sequences or single instructions, this method al-
lows to evaluate all instructions while still mea-
suring the effects of error propagation.

We evaluate using both static recorded games
and live interaction with human players. Our hu-
man evaluation shows users adapt to the system
and use the agent effectively, scoring on average
6.2 points, compared to 12.7 for human players.
Our data, code, and demo videos are available at
lil.nlp.cornell.edu/cerealbar/.

2 Setup and Technical Overview
We consider a setup where two agents, a leader
and a follower, collaborate. Both execute actions

in a shared environment. The leader, additionally,
instructs the follower using natural language. The
leader goal is to maximize the task reward, and the
follower goal is to execute leader instructions. We
consider a turn-based version, where at each turn
only one agent acts. We instantiate this scenario in
CEREALBAR, a navigation card game (Figure 1),
where a leader and follower move in an environ-
ment selecting cards to complete sets.1

CEREALBAR Overview The objective of CE-
REALBAR is to earn points by selecting valid sets
of cards. A valid set has three cards with distinct
color, shape, and count. When the only cards se-
lected in the world form a valid set, the players
receive a point, the selected cards disappear, three
new cards are added randomly, and the number of
remaining turns increases. The increase in turns
decays for each set completed. An agent stepping
on a card flips its selection status. The players
form sets together. The follower has more steps
per turn than the leader. This makes using the fol-
lower critical for success. The follower only sees
a first-person view of the environment, preventing
them from planning themselves, and requiring in-
structions to be sensible from the follower’s per-
spective. The leader chooses the next target set,
plans which of the two players should get which
card, and instructs the follower. The follower can
not respond to the leader, and should not plan
themselves, or risk sabotaging the leader’s plan,
wasting moves and lowering their potential score.
Followers mark an instruction as finished before
observing the next one. This provides alignment
between instructions and follower actions. In con-
trast to the original setup that we use for data col-
lection, in our model (Section 4), we assume the
follower has full observability, leaving the chal-
lenge of partial observability for future work. Ap-
pendix A provides further game design details.
Problem Setup We distinguish between the
world state and the interaction state. Let S be the
set of all world states, Γ be the set of all interac-
tion states, and X be the set of all natural language
instructions. A world state s ∈ S describes the
current environment. In CEREALBAR, the world
state describes the spatial environment, the loca-
tion of cards, whether they are selected or not, and
the location of the agents. An interaction state
γ ∈ Γ is a tuple 〈Q̄, α, ψ〉. The first-in-first-out

1The name CEREALBAR does not carry special meaning.
It was given to the project early on, and we came to like it.
Our game is inspired by the card game Set.

http://lil.nlp.cornell.edu/cerealbar/
https://en.wikipedia.org/wiki/Set_(card_game)


2121

queue Q̄ = [x̄q, . . . , x̄q′ ] contains the instructions
x̄i ∈ X available to execute. The current instruc-
tion is the left-most instruction x̄q. The current
turn-taker α ∈ {Leader,Follower} indicates the
agent currently executing actions, and ψ ∈ IN≥0 is
the number of steps remaining in the current turn.

At each time step, the current turn-taker agent
takes an action. An action may be the leader is-
suing an instruction, or either agent performing
an action in the environment. Let A = Aw ∪
{DONE} ∪ X be the set of all actions. The set Aw
includes the actions available to the agents in the
environment. In CEREALBAR, this includes mov-
ing forward or backward, and turning left or right.
Moving onto a card flips it selection status. DONE
indicates completing the current instruction for the
follower or ending the turn for the leader. An in-
struction action a = x̄ ∈ X can only be taken by
the leader and adds the instruction x̄ to the queue
Q̄. The effect of each action is determined by the
transition function T : S×Γ×A → S×Γ, which
is formally defined in Appendix B. Only world ac-
tions a ∈ Aw decrease the remaining steps ψ.

The goal of the leader is to maximize the to-
tal reward of the interaction. An interaction Ī =
〈(s1, γ1, a1), . . . , (s|Ī|, γ|Ī|, a|Ī|)〉 is a sequence
of state-action tuples, where T (si, γi, ai) =
(si+1, γi+1). The reward functionR : S×A → R
assigns a numerical reward to a world state and
an action. The total reward of an interaction Ī is∑|Ī|

i=0R(si, ai). In CEREALBAR, the agents re-
ceive a reward of 1 when a valid set is selected.
Task Our goal is to learn a follower policy to
execute the leader instructions. At time t, given
the current world and interaction states st and γt,
and the interaction so far Ī<t, the follower policy
π(st, γt, Ī<t) predicts the next action at.
Model We decompose the follower policy
π(st, γt, Ī<t) to predicting a set of distributions
over positions in the environment, including posi-
tions to visit, intermediate goals (e.g., cards to se-
lect), positions to avoid (e.g., cards not to touch),
and positions that are not passable. These distri-
bution are used in a second stage to generate a se-
quence of actions. Section 4 describes the model.
Learning We assume access to a set of N
recorded interactions {Ī(i)}Ni=1, and create exam-
ples where each instruction is paired with a se-
quence of state-action tuples. We maximize the
action-level cross entropy objective, and use two
auxiliary objectives (Section 5). We first train each
stage of the model separately, and then fine-tune

them jointly. During fine-turning, we continuously
generate additional examples using model failures.
These examples help the agent to learn how to re-
cover from errors in prior instructions.
Evaluation We measure correct execution of in-
structions and the overall game reward. We as-
sume access to a test set of M recorded interac-
tions {Ī(i)}Mi=1. We measure instruction-level and
interaction-level performance, and develop cas-
caded evaluation, an evaluation protocol that pro-
vides a more graded measure than treating each in-
teraction as a single example, while still account-
ing for error propagation (Section 6). Finally, we
conduct online evaluation with human leaders.

3 Related Work
Goal-driven natural language interactions have
been studied in various scenarios, including dia-
logue where only one side acts in the world (An-
derson et al., 1991; Williams et al., 2013; Vlachos
and Clark, 2014; de Vries et al., 2018; Kim et al.,
2019; Hu et al., 2019), coordination for agreed
selection of an object (He et al., 2017; Udagawa
and Aizawa, 2019), and negotiation (Lewis et al.,
2017; He et al., 2018). We focus on collaborative
interactions where both the user and the system
perform sequences of actions in the same environ-
ment. This allows the user to adapt to the language
understanding ability of the system and balance
between delegating goals to it and accomplishing
them themselves. For example, a user may decide
to complete a short but hard-to-describe task and
delegate to the system a long but easy-to-describe
one. In prior work, in contrast, recovery is limited
to users paraphrasing their requests. The Cards
corpus (Djalali et al., 2011, 2012; Potts, 2012)
was used for linguistic analysis of collaborative
bi-directional language interaction. The structure
of collaborative interactions was also studied us-
ing Wizard-of-Oz studies (Lochbaum, 1998; Sid-
ner et al., 2000; Koulouri and Lauria, 2009). In
contrast, we focus on building agents that follow
instructions. Ilinykh et al. (2019) present a cor-
pus for the related task of natural language co-
ordination in navigation. Collaboration has also
been studied for emergent communication (e.g.,
Andreas et al., 2017; Evtimova et al., 2017).

Understanding sequences of natural language
utterances has been addressed using semantic
parsing (e.g., Miller et al., 1996; MacMahon et al.,
2006; Chen and Mooney, 2011; Artzi and Zettle-
moyer, 2013; Artzi et al., 2014; Long et al., 2016;



2122

Iyyer et al., 2017; Suhr et al., 2018; Arkin et al.,
2017; Broad et al., 2017). Interactions were
also used for semantic parser induction (Artzi and
Zettlemoyer, 2011; Thomason et al., 2015; Wang
et al., 2016). These methods require hand-crafted
symbolic meaning representation, while we use
low-level actions (Suhr and Artzi, 2018). The in-
teractions in our environment interleave actions
of both agents with leader utterances, an aspect
not addressed by these methods. Executing single
instructions has been widely studied (e.g., Tellex
et al., 2011; Duvallet et al., 2013; Misra et al.,
2017, 2018; Anderson et al., 2018; Blukis et al.,
2018a,b; Chen et al., 2019). The distinction we
make between actions specified in the instruction
and implicit recovery actions is similar to how
Artzi and Zettlemoyer (2013) use implicit actions
for single instructions. Finally, our model is based
on the VPN model of Blukis et al. (2018b). While
we assume full observability, their original work
did not. This indicates that our model is likely to
generalize well to partially observable scenarios.

4 Model
We use a two-stage model for the follower policy
π(st, γt, Ī<t), where st is a world state, γt is an
interaction state, and Ī<t is the interaction history.
The instruction x̄ that is the first in the queue Q̄t,
which is part of γt, is the currently executed in-
struction. In our model, we assume the follower
observes the entire environment. First, we map
x̄ and st to distributions over locations in the en-
vironment, including what locations to visit and
what are the goals. These distributions are consid-
ered as an execution plan, and are used to generate
a sequence of actions in the second stage. The dis-
tribution can also be used to easily easily visualize
the agent plan. The first stage is used when start-
ing a new instruction, and the predicted distribu-
tions are re-used for all actions for that instruction.
Figure 2 illustrates the architecture and the distri-
butions visualization. The two-stage approach was
introduced by Blukis et al. (2018b). We general-
ize its planning space and add a recurrent action
generator for execution.
Input Representation The inputs to the first
stage are the instruction x̄ and the world state
st. We generate feature maps for both. We
use a learned embedding function φX and a
bi-directional recurrent neural network (RNN;
Elman, 1990) with a long short-term memory
cell (LSTM; Hochreiter and Schmidhuber, 1997)

RNNX to map x̄ to a vector x̄. The world state st
is a 3D tensor that encodes the properties of each
position. The dimensions of st are P ×W × H ,
where P is the number of properties, and W and
H are the environment width and height. Each of
theW×H positions is represented in st as a binary
vector of length P . For example, a position with
a red hut will have 1’s for the red and hut dimen-
sions and 0’s for all other dimensions. We map
the world state to a tensor feature map F0 by em-
bedding st and processing it using the text repre-
sentation x̄. We use a learned embedding function
φS to map each position vector to a dense embed-
ding of size Ns by summing embeddings of each
of the position’s properties. The embeddings are
combined to a tensor S of dimensionNs×W ×H
representing a featurized global view of the envi-
ronment. We create a text-conditioned state repre-
sentation by creating a kernel Ks and convolving
with it over S. We use a linear transformation to
create Ks = Wsx̄ + bs, where Ws and bs are
learned weights. We reshape Ks to a 1× 1 convo-
lution kernel with Ns′ output channels, and com-
pute S′ = S ∗Ks. We concatenate S and S′ along
the channel dimension and rotate and center so the
follower position is at center pixel to generate F0.2

Stage 1: Plan Prediction We treat plan gen-
eration as predicting distributions over positions
ρ in the environment. There are W × H
possible positions. We predict four distribu-
tions: (a) p(ρ | st, x̄), the probability of vis-
iting ρ while executing the instruction x̄; (b)
p(GOAL = 1 | ρ, st, x̄), the binary probability
that ρ is a goal (i.e., GOAL = 1 when contain-
ing a card to select); (c) p(AVOID = 1 | ρ, st, x̄),
the binary probability that the agent must not
pass in ρ (i.e., AVOID = 1 when it con-
tains a card that should not be touched); and (d)
p(NOPASS = 1 | ρ, st, x̄), the binary probability
the agent cannot pass in ρ (i.e., NOPASS = 1
when it contains another object).

We use LINGUNET (Misra et al., 2018) to pre-
dict the distributions. The inputs to LINGUNET
are the instruction embedding x̄ and featurized
world state F0, which is relative to the agent’s
frame of reference. The output are four matri-
ces, each of dimension W × H corresponding to
the environment. LINGUNET is formally defined
in Misra et al. (2018) and Appendix D. Roughly

2Appendix D describes the relationship between the en-
vironment representations and the agent’s initial and current
orientation.



2123

Stage 1: Plan 
prediction

s

x̄

⇤

LingUNet

Conv2D 
Transpose Conv2d 
LeakyReLU 
InstanceNorm2D 
Fully connected + bias 
L2 norm 

KT1

KT2

KT3

KT4

F1

F2

F3

F4

RNN

Goal  
auxiliary G1

G2

G3

G4 H4

H3

H2

H1

F0

Discriminator 
auxiliary

 Okay, pick up yellow hearts and run past me toward the 
bush sticking out, on the opposite side is 3 green stars
x̄ :

<latexit sha1_base64="SyiLA+p38Q5nHeVQIMUe5kZ73KA=">AAAB73icbVDLSgNBEOyNrxhfUY9eBoPgKexGQfEU9OIxgnlAsoTeySQZMju7zsyKYclPePGgiFd/x5t/4yTZgyYWNBRV3XR3BbHg2rjut5NbWV1b38hvFra2d3b3ivsHDR0lirI6jUSkWgFqJrhkdcONYK1YMQwDwZrB6GbqNx+Z0jyS92YcMz/EgeR9TtFYqdUJUKVPk6tuseSW3RnIMvEyUoIMtW7xq9OLaBIyaahArdueGxs/RWU4FWxS6CSaxUhHOGBtSyWGTPvp7N4JObFKj/QjZUsaMlN/T6QYaj0OA9sZohnqRW8q/ue1E9O/9FMu48QwSeeL+okgJiLT50mPK0aNGFuCVHF7K6FDVEiNjahgQ/AWX14mjUrZOytX7s5L1essjjwcwTGcggcXUIVbqEEdKAh4hld4cx6cF+fd+Zi35pxs5hD+wPn8ASUukAk=</latexit>

Plan distributionsTrajectory distribution 
Goal distribution 

Locations to avoid 
Impassable locations

S

Ks

S0

Stage 2: Action 
generation

LEFT TRNN
hs1, �1i

hs2, �2i

FORWARD TRNN

TRNN
hs3, �3i

FORWARD

Figure 2: Illustration of the model architecture. Given the instruction x̄ and the world state s, we compute F0 from
the embeddings of the instruction x̄ and environment S. We use LINGUNET to predict four distributions, which
are visualized over the map (grayscaled to emphasize the distributions). We show three action generation steps.
Each step receives the map cropped around the agent and the previous action, and outputs the next action.

speaking, LINGUNET reasons about the environ-
ment representation F0 at L levels. First, F0 is
used to generate feature maps of decreasing size
Fj , j = 1 . . . L using a series of convolutions.
We create convolution kernels from the instruc-
tion representation x̄, and apply them to the fea-
ture maps Fj to generate text-conditioned feature
maps Gj . Finally, feature maps of increasing size
Hj are generated using a series of L deconvolu-
tions. The last deconvolution generates a tensor
of size 4 × W × H with a channel for each of
the four distributions. We use a softmax over one
channel to compute p(ρ | st, x̄). Because the
other distributions are binary, we use a sigmoid
on each value independently for the other chan-
nels. When computing p(GOAL = 1 | ρ, st, x̄)
and p(AVOID = 1 | ρ, st, x̄) we mask positions
without objects that can be changed (i.e., positions
without cards) to assign them zero probability.

Stage 2: Action Generation We use the four
distributions to generate a sequence of actions.
We concatenate the distributions channel-wise to
a tensor P ∈ R4×W×H . We use a forward LSTM
RNN to predict a sequence of actions. At each
prediction step t, we rotate, transform, and crop P
to generate the egocentric tensor P′t ∈ RN

′×C×C ,
where the agent is always at the center and facing
in the same direction, such that P′t is relative to
the agent’s current frame of reference. The input
to the action generation RNN at time t is:

p′t = vec(NORM(RELU(CNN
P (P′t))))

pt = RELU(WP2 [p
′
t; RELU(W

P
1 p
′
t + b

P
1 )] + b

P
2 ) ,

where CNNP is a convolutional layer, RELU
is a non-linearity, NORM is instance normaliza-
tion (Ulyanov et al., 2017), and WP1 , W

P
2 , b

P
1 ,

bP2 are learned weights. The action probability is:
ht = RNNA(ht−1, [φA(at−1);pt])

p(a) ∝ exp(WA[ht;pt] + bA) , (1)

where RNNA is an LSTM RNN, φA is a learned
action embedding function, a0 is a special START
action, and WA and bA are learned. During in-
ference, we assign zero probabilities to actions a
when Tw(st, a) is invalid (Appendix B), for exam-
ple when an agent would move into an obstacle.

5 Learning
We assume access to a set of N recorded inter-
actions {Ī(i)}Ni=1. We generate instruction-level
examples D = ⋃Ni=1{Ī(i,j)}M(i)j=1 , where M (i) is
the number of examples from Ī(i). Each Ī(i,j) =
〈(s(i,j)1 , γ

(i,j)
1 , a

(i,j)
1 ), . . . , (s

(i,j)
k , γ

(i,j)
k , a

(i,j)
k )〉 is a

subsequence of tuples in Ī(i), where a(i,j)1 is the
first action the follower takes after observing the
j-th instruction in Ī(i), and a(i,j)k is the DONE action
completing that instruction. We first estimate the
parameters for plan prediction θ1 and action gen-
eration θ2 separately (Section 5.1), and then fine-
tune jointly with data augmentation (Section 5.2).

5.1 Pretraining
Stage 1: Plan Prediction The input of Stage 1 is
the world state s1 and the instruction x̄ at the head
of the queue Q̄.3 We generate labels for the four

3We omit example indices for succinctness.



2124

output distributions using Ī(i,j). The visitation dis-
tribution p(ρ | s1, x̄) label is proportional to num-
ber of states st ∈ Ī(i,j) where the follower is in
position ρ. The goal and avoidance distributions
model how the agent plans to manipulate parts of
its environment to achieve the specified goals, but
avoid manipulating other parts. In CEREALBAR,
this translates to changing the status of cards, or
avoiding doing so. For p(GOAL = 1 | ρ, s1, x̄),
we set the label to 1 for all ρ that contain a card that
the follower changed its selection status in Ī(i,j),
and 0 for all other positions. Similarly, for the
avoidance distribution p(AVOID = 1 | ρ, s1, x̄),
the label is 1 for all ρ that have cards that the fol-
lower does not change during the interaction Ī(i,j).
Finally, for p(NOPASS = 1 | ρ, s1, x̄), the label
is 1 for all positions the agent cannot move onto,
and zero otherwise. We define four cross-entropy
losses: visitation LV , goal LG, avoidance LA, and
no passing LP . We also use an auxiliary cross-
entropy goal-prediction loss LG′ using a probabil-
ity p′G(GOAL = 1 | ρ, s1, x̄) we predict from
the pre-LINGUNET representation S′ by classify-
ing each position. The complete loss is a weighted
sum with coefficients:4

L1(θ1) =λV LV (θ1) + λGLG(θ1) + λALA(θ1)
+ λPLP (θ1) + λG′LG′(θ1) .

Stage 2: Action Generation We use the gold
distribution to create the input P, and optimize to-
wards the annotated set of actions using teacher
forcing (Williams and Zipser, 1989). We compute
the loss only over actions taken by the follower:

L2(θ2) = −
∑n
t=1 1αt=Followerp(at) ,

where p(at) is computed by Equation 1.

5.2 Fine-tuning with Example Aggregation

Simply combining the separately-trained networks
together results in low performance. We perform
additional fine-tuning with the two stages com-
bined, and introduce a data augmentation method
to learn to recover from error propagation.
Error Propagation Executing a sequence of
instructions is susceptible to error propagation,
where an agent fails to correctly complete an in-
struction, and because of it also fails on the follow-
ing ones. While the collaborative, turn-switching
setup allows the leader to adjust their plan fol-
lowing a follower mistake, leaders often strategi-
cally issue multiple instructions to use the avail-

4Additional details are in Appendix E.1.

able follower steps optimally. Given an agent fail-
ure, subsequent instructions may not align with the
state of the world resulting from the follower’s er-
ror. In supervised learning, we do not have the
opportunity to learn to recover from such errors,
even when it is relatively simple. This usually re-
quires exploration. However, conventional frame-
works like reinforcement learning (RL) or imita-
tion learning (IL) are poorly suitable. In a live
interaction, when an agent makes a mistake (e.g.,
selecting the wrong card), the leader is likely to
adjust their actions. Because of this, in a recorded
interaction, which contains the leader actions fol-
lowing a correct execution, it is not possible to re-
liably compute an RL reward for states following
erroneous executions. For similar reasons, we can-
not compute an IL oracle.

We identify two classes of erroneous states in
CEREALBAR: (a) not selecting the correct set of
cards; and (b) finishing with the right card selec-
tion, but stopping at the wrong position.5 Case (a)
requires to modify the model, for example to know
when to skip instructions that refer to a state that
is no longer possible. We leave this case for fu-
ture work. We address case (b) by augmenting the
data with new examples that are aggregated during
learning. Our process is similar to DAGGER (Ross
et al., 2011). We alternate between: (a) collect-
ing new training examples using a heuristic ora-
cle, and (b) performing model updates. We gen-
erate training examples that demonstrate recovery
by starting in an incorrect initial position for an
instruction, having arrived there by executing the
previous instruction. We train our model to distin-
guish between the reasoning required for generat-
ing implicit actions to correct errors, and explicit
actions directly mentioned in the instruction.
Learning with Example Aggregation We al-
ternate between aggregating a new set of recov-
ery examples D′ and updating our parameters. At
each epoch, we first use the current policy to cre-
ate new training examples. We run inference for
each example Ī(i,j) in D, the original training set,
using the current policy.6 We compare the state s′

at the end of execution to the final state in Ī(i,j)

to generate an error-recovery example Ī ′(i,j+1) for
the subsequent example Ī(i,j+1). We only gener-
ate such examples if the position or rotation of the

5See Appendix E.2 for further discussion of the two cases.
6We do not perform inference for the last instruction in an

interaction, as there is no subsequent example for which to
generate a new example.



2125

agent are different, and there are no other differ-
ence between the states. Starting from s′, we gen-
erate the shortest-path sequence of actions that: (a)
changes the cards as specified in Ī(i,j+1), and (b)
executes DONE in the same position as in Ī(i,j+1).
We then create Ī ′(i,j+1) using Ī(i,j+1) and the new
sequence of state-action pairs, and add it to D′.7

Given the original set of examples D and the
aggregate examples D′ we update our model pa-
rameters. We randomly sample without replace-
ment at most

∑N
i=1M

(i) examples, the size of D,
from D′. We use all the examples in D and the
sampled examples to do a single parameter update
epoch. We limit the number of examples from D′
to maintain the effect of the original data.

Optimizing with Implicit Action Prediction
The examples we generate during aggregation of-
ten include sequences of state-action pairs that do
not align with the instruction, for example when
a mentioned spatial relation is incorrect from the
new starting position. Such examples require rea-
soning differently about the text and world state
than with the original data. We identify such ex-
amples in D′ by comparing their follower starting
position to the starting position in the original cor-
responding example in D. If the distance is over
two, we treat the examples as requiring implicit
actions (Artzi and Zettlemoyer, 2013). All other
examples, including all original examples inD are
considered as not requiring implicit reasoning. We
encourage the model to reason differently about
these examples with a discriminator that classifies
if the example requires implicit reasoning or not
using the internal activations of LINGUNET.

The discriminator classifies each of the L layers
in LINGUNET for implicit reasoning. The goal
is to encourage implicit reasoning at all levels of
reasoning in the first stage. The probability of im-
plicit reasoning for each LINGUNET layer l is:

p(IMPLICT = 1 | l, s1, x̄) ={
σ(AVGPOOL(G1 ∗KIMP1 )) l = 1
σ(AVGPOOL(Hl ∗KIMPl )) l > 1

,

where KIMPl are 1 × 1 learned kernels and
AVGPOOL does average pooling. We define a
cross-entropy loss LIMP that averages across the
L layers. The complete fine-tuning loss is:

L(θ1, θ2) = L1(θ1) + L2(θ2) + λIMPLIMP(θ1) .

7Appendix E.2 describes this process.

6 Cascaded Evaluation

Sequential instruction scenarios are commonly
evaluated using recorded interactions by executing
individual instructions or executing complete in-
teractions starting from their beginning (e.g., Chen
and Mooney, 2011; Long et al., 2016). Both have
limitations. Instruction-level metrics ignore error
propagation, and do not accurately reflect the sys-
tem’s performance. In contrast, interaction-level
metrics do consider error propagation and capture
overall system performance well. However, they
poorly utilize the test data, especially when perfor-
mance is relatively low. When early failures lead
to unexpected world states, later instructions be-
come impossible to follow, and measuring perfor-
mance on them is meaningless. For example, with
our best-performing model, 82% of development
instructions become impossible due cascading er-
rors when executing complete interactions.

The two measures may also fail to distinguish
models. For example, consider an interaction with
three instructions. Two models, A and B, success-
fully execute the third instruction in isolation, but
fail on the two others. They also both fail when
executing the entire interaction starting from the
beginning. According to common measures, the
models are equal. However, if model B can actu-
ally recover from failing on the second instruction
to successfully execute the third, it means it is bet-
ter than model A. Both metrics fail to reflect this.

We propose cascaded evaluation, an evaluation
protocol for sequential instruction using static cor-
pora. Our method utilizes all instructions during
testing, while still accounting for the effect of er-
ror propagation. Unlike instruction-level evalua-
tion, cascaded evaluation executes the instructions
in sequence. However, instead of starting of start-
ing only from the start state of the first instruc-
tion, we create separate examples for starting from
the starting state of each instruction in the inter-
action and continuing until the end of the inter-
action. For example, given a sequence of three
instructions 〈1, 2, 3〉 we will create three exam-
ples: 〈1, 2, 3〉, 〈2, 3〉, and 〈3〉. To evaluate perfor-
mance in CEREALBAR, we compute two statistics
using cascaded evaluation: the proportion of the
remaining instructions followed successfully, and
the proportion of potential points scored. We only
consider the remaining instructions and points left
to achieve in the example. For example, for the se-
quence 〈2, 3〉, we will subtract any points achieved



2126

before the second instruction to compute the pro-
portion of potential points scored. Appendix F de-
scribes cascaded evaluation formally.

7 Experimental Setup
Data We collect 1,202 human-human interac-
tions using Mechanical Turk, split into train (960
games), development (120), and test (122). Ap-
pendix C details data collection and statistics.
Recorded Interactions Metrics We evaluate
instruction-level, interaction-level, and cascaded
(Section 6) performance. We allow the follower
ten steps per turn, and interleave the actions taken
by the leader during each turn in the recorded
interaction. Instruction execution often crosses
turns. At the instruction-level, we evaluate the
mean card state accuracy comparing the state of
the cards after inference with the correct card
state, environment state accuracy comparing both
cards and the agent’s final position, and action se-
quence accuracy comparing the generated action
sequence with the correct action sequence. For
complete interactions, we measure mean full game
points. Finally, for cascaded evaluation, we mea-
sure the mean proportion of instructions correctly
executed and of possible points scored.
Human Evaluation We perform evaluation
with human leaders, comparing our model and hu-
man followers. Workers are told they will work
with a human or an automated follower, but are
not told which in each game. We evaluate both
human (105 games) and automated agents at the
same time (109 games). We evaluate the game
scores, and also elicit free-form feedback.
Systems We evaluate three systems: (a) the
full model; (b) SEQ2SEQ+ATTN:8 sequence-to-
sequence with attention; and (c) a static oracle
that executes the gold sequence of actions in the
recorded interaction. We report mean and standard
deviation across three trials for development re-
sults. We ablate model and learning components,
and additionally evaluate the action generator with
access to gold plans.9 On the test set and for hu-
man evaluation, we use the model with the highest
proportion of points scored. We provide imple-
mentation and learning details in Appendix G.

8 Results
Table 1 shows development and test results, in-
cluding ablations. We consider the proportion of

8This baseline is similar to Mei et al. (2016).
9We do not measure interaction-level metrics with gold

plans as they are only available for the gold start positions.

points scored computed with cascaded evaluation
as the main metric. Our complete approach signif-
icantly outperforms SEQ2SEQ+ATTN. Key to this
difference is the added structure within the model
and the direct supervision on it. The results also
show the large remaining gap to the static oracle.10

Our results show how considering error prop-
agation for all available instructions in cascaded
evaluation guides different design choices. For
example, example aggregation and the implicit
discriminator lower performance according to
instruction-level metrics, which do not consider
error propagation. We see a similar trend for the
implicit discriminator when looking at full game
points, an interaction-level metric that does not ac-
count for performance on over 80% of the data be-
cause of error propagation. In contrast, the pro-
portion of points scored computed using cascaded
evaluation shows the benefit of both mechanisms.

Our ablations demonstrate the benefit of each
model component. All four distributions help.
Without the trajectory distribution (– Trajectory
distribution), performance drops almost to the
level of SEQ2SEQ+ATTN. This indicates the ac-
tion predictor is not robust enough to construct
a path given only the three other disjoint distri-
butions. While the predicted trajectory distribu-
tion contains all information necessary to reach
the correct cards and goal location, the other three
distributions further improve performance. This is
likely because redundancy with the trajectory dis-
tribution makes the model more robust to noisy
predictions in the trajectory distribution. For ex-
ample, the GOAL distribution guides the agent to
move towards goal cards even if the predicted tra-
jectory is discontinuous. The action generation re-
currence is also critical (– Action recurrence), al-
lowing the agent to keep track of which locations
it already passed when navigating complex paths
that branch, loop, or overlap with themselves.

While we observe that each stage separately
performs well after pretraining, combining them
without fine-tuning (– Fine-tuning) leads to low
performance because of the shift in the second
stage input. Providing the gold distributions to
the action generator illustrates this (+ Gold plan).
Removing early goal auxiliary loss LG′ (Sec-
tion 5.1) leads to a slight drop in performance
on all metrics (– Early goal auxiliary). Learn-
ing with aggregated recovery examples helps the

10Appendix F explains the static oracle performance.



2127

System Card Env. Action Seq. Full Game Prop. Instr. Prop. PointsState Acc. State Acc. Accuracy Points Followed Scored
Development Results & Ablation Analysis
Full model 58.2±0.5 32.6±0.8 15.8±0.5 0.66±0.1 20.5±1.2 18.1±0.8
– Trajectory distribution 38.5±2.7 10.1±2.7 5.5±2.6 0.29±0.02 10.0±0.9 7.9±0.7
– GOAL distribution 56.2±1.5 30.8±0.4 14.9±0.3 0.66±0.09 17.9±1.0 15.9±1.3
– AVOID distribution 57.0±0.3 32.6±1.6 15.4±1.3 0.63±0.04 18.8±1.5 17.8±0.7
– NOPASS distribution 59.2±0.5 32.0±0.8 15.0±0.5 0.70±0.03 18.4±0.9 16.6±0.9
– Action recurrence 42.3±1.5 16.7±1.2 10.0±0.7 0.42±0.03 12.8±1.7 10.7±0.5
– Fine-tuning 43.6±1.9 8.5±1.1 4.5±0.5 0.65±0.09 14.1±1.3 9.2±0.9
– Early goal auxiliary 57.2±2.3 31.2±1.7 14.9±1.6 0.65±0.05 17.9±1.1 16.5±0.7
– Example aggregation 59.4±1.8 32.0±1.0 15.7±0.6 0.65±0.09 20.4±1.4 16.5±0.4
– Implicit discriminator 57.5±2.1 32.7±1.0 16.4±0.3 0.70±0.02 18.8±1.8 16.7±0.6
– Instructions 15.5±1.5 2.7±1.5 1.2±1.2 0.24±0.07 4.4±1.0 4.6±0.7
+ Gold plan 87.4±0.5 80.2±0.2 63.4±0.2 – – –
SEQ2SEQ+ATTN 35.3±0.8 11.1±0.5 9.4±0.5 0.20±0.04 8.8±0.1 6.3±0.1
Static oracle 99.7 99.7 100.0 6.58 98.5 97.9
Test Results
Full model 58.4 32.1 15.6 0.62 15.4 17.9
SEQ2SEQ+ATTN 37.3 10.8 8.5 0.22 8.7 6.5
Static oracle 99.7 99.7 100.0 6.66 96.8 95.6

Table 1: Development and test results on all systems, including ablation results.

model to learn to recover from errors in previ-
ous instructions and increases the proportion of
points scored (– Example aggregation). How-
ever, without the implicit reasoning discriminator
(– Implicit discriminator), the additional examples
make learning too difficult, and do not help. Fi-
nally, removing the language input (– Instructions)
significantly decreases performance, showing that
the data is relatively robust to observational biases
and language is necessary for the task.

In the human evaluation, we observe a mean of
6.2 points (max of 14) with our follower model,
compared to 12.7 (max of 20) with human fol-
lowers. While this shows there is much room
for improvement, it illustrates how human lead-
ers adapt and use the agent effectively. One key
strategy of adaptation is to use simplified language
that fits the model better. This includes shorter in-
structions, with 8.5 tokens on average with auto-
mated followers compared to 12.3 with humans,
and a smaller vocabulary, 578 word types with au-
tomated followers and 1037 with humans. In gen-
eral, human leaders commented that they are able
to easily distinguish between automated and hu-
man followers, and find working with the auto-
mated agent frustrating.

9 Discussion
Our human evaluation highlights several direc-
tions for future work. While human leaders adapt
to the agent, scoring up to 14 points, there remains
a significant gap to collaborations with human fol-
lowers. Reported errors include getting stuck be-
hind objects, selecting unmentioned cards, going

in the wrong direction, and ignoring instructions.
At least one worker developed a strategy that took
advantage of the agent’s full observability, writ-
ing instructions with only simple card references.
An important direction for future work is to re-
move our full observability assumption. Other
future directions include experimenting with us-
ing the interaction history, expanding the learning
example aggregation to error cases beyond incor-
rect start positions, and making agent reasoning
interpretable to reduce user frustration. CERE-
ALBAR also provides opportunities to study prag-
matic reasoning for language understanding (An-
dreas and Klein, 2016; Fried et al., 2018; Liang
et al., 2019). While we currently focus on lan-
guage understanding by limiting the communica-
tion to be unidirectional, bidirectional communi-
cation would allow for more natural and efficient
collaborations (Potts, 2012; Ilinykh et al., 2019).
CEREALBAR could be easily adapted to allow
bidirectional communication, and provide a plat-
form to study challenges in language generation.

Acknowledgments
This research was supported by the NSF under
Grant No. 1750499, an AI2 KSC Award, a
Workday Faculty Award, a Google Faculty Award,
Unity, and an Amazon Cloud Credits Grant. This
material is based on work supported by the Na-
tional Science Foundation Graduate Research Fel-
lowship under Grant No. DGE-1650441. We
thank Valts Blukis, Jin Sun, and Mark Yatskar for
comments and suggestions, the workers who par-
ticipated in our data collection, and the reviewers.



2128

References
Anne H. Anderson, Miles Bader, Ellen Gurman Bard,

Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34.

Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,
Mark Johnson, Niko Sünderhauf, Ian D. Reid,
Stephen Gould, and Anton van den Hengel.
2018. Vision-and-language navigation: Interpreting
visually-grounded navigation instructions in real en-
vironments. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 3674–3683.

Jacob Andreas, Anca Dragan, and Dan Klein. 2017.
Translating neuralese. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 232–242.

Jacob Andreas and Dan Klein. 2016. Reasoning about
pragmatics with neural listeners and speakers. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182.

Jacob Arkin, Matthew R. Walter, Adrian Boteanu,
Michael E. Napoli, Harel Biggie, Hadas Kress-
Gazit, and Thomas M. Howard. 2017. Contextual
awareness: Understanding monologic natural lan-
guage instructions for autonomous robots. In IEEE
International Symposium on Robot and Human In-
teractive Communication, pages 502–509.

Yoav Artzi, Dipanjan Das, and Slav Petrov. 2014.
Learning compact lexicons for CCG semantic pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1273–1283.

Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 421–432.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion of Computational Linguistics, 1:49–62.

Valts Blukis, Nataly Brukhim, Andrew Bennett,
Ross A. Knepper, and Yoav Artzi. 2018a. Follow-
ing high-level navigation instructions on a simulated
quadcopter with imitation learning. In Proceedings
of the Robotics: Science and Systems Conference.

Valts Blukis, Dipendra Misra, Ross A. Knepper, and
Yoav Artzi. 2018b. Mapping navigation instructions
to continuous control actions with position visita-
tion prediction. In Proceedings of the Conference
on Robot Learning.

Alexander Broad, Jacob Arkin, Nathan Ratliff, Thomas
Howard, and Brenna Argall. 2017. Real-time natu-
ral language corrections for assistive robotic manip-
ulators. The International Journal of Robotics Re-
search, 36(5-7):684–698.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artificial Intelligence.

Howard Chen, Alane Suhr, Dipendra Misra, Noah
Snavely, and Yoav Artzi. 2019. Touchdown: Natural
language navigation and spatial reasoning in visual
street environments. In IEEE Conference on Com-
puter Vision and Pattern Recognition.

Alex Djalali, David Clausen, Sven Lauer, Karl Schultz,
and Christopher Potts. 2011. Modeling expert ef-
fects and common ground using questions under
discussion. In AAAI Fall Symposium: Building
Representations of Common Ground with Intelligent
Agents.

Alex Djalali, Sven Lauer, and Christopher Potts. 2012.
Corpus evidence for preference-driven interpreta-
tion. In Logic, Language and Meaning, pages 150–
159.

Felix Duvallet, Thomas Kollar, and Anthony Stentz.
2013. Imitation learning for natural language direc-
tion following through unknown environments. In
IEEE International Conference on Robotics and Au-
tomation, pages 1047–1053.

Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science, 14:179–211.

Katrina Evtimova, Andrew Drozdov, Douwe Kiela,
and Kyunghyun Cho. 2017. Emergent communica-
tion in a multi-modal, multi-step referential game.
In Proceedings of the International Conference on
Learning Representations.

Daniel Fried, Jacob Andreas, and Dan Klein. 2018.
Unified pragmatic models for generating and follow-
ing instructions. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 1951–1963.

He He, Anusha Balakrishnan, Mihail Eric, and Percy
Liang. 2017. Learning symmetric collaborative di-
alogue agents with dynamic knowledge graph em-
beddings. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 1766–1776.

He He, Derek Chen, Anusha Balakrishnan, and Percy
Liang. 2018. Decoupling strategy and generation in
negotiation dialogues. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 2333–2343.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9.

https://doi.org/10.1109/CVPR.2018.00387
https://doi.org/10.1109/CVPR.2018.00387
https://doi.org/10.1109/CVPR.2018.00387
https://doi.org/10.18653/v1/P17-1022
https://doi.org/10.18653/v1/D16-1125
https://doi.org/10.18653/v1/D16-1125
https://doi.org/10.1109/ROMAN.2017.8172349
https://doi.org/10.1109/ROMAN.2017.8172349
https://doi.org/10.1109/ROMAN.2017.8172349
https://doi.org/10.3115/v1/D14-1134
https://doi.org/10.3115/v1/D14-1134
http://www.aclweb.org/anthology/D11-1039
http://www.aclweb.org/anthology/D11-1039
http://aclweb.org/anthology/Q13-1005
http://aclweb.org/anthology/Q13-1005
http://aclweb.org/anthology/Q13-1005
https://doi.org/10.18653/v1/N18-1177
https://doi.org/10.18653/v1/N18-1177
https://doi.org/10.18653/v1/P17-1162
https://doi.org/10.18653/v1/P17-1162
https://doi.org/10.18653/v1/P17-1162
https://www.aclweb.org/anthology/D18-1256
https://www.aclweb.org/anthology/D18-1256


2129

Hengyuan Hu, Denis Yarats, Qucheng Gong, Yuan-
dong Tian, and Mike Lewis. 2019. Hierarchical de-
cision making by generating and following natural
language instructions. CoRR, abs/906.00744.

Nikolai Ilinykh, Sina Zarrieß, and David Schlangen.
2019. MeetUp! A corpus of joint activity dialogues
in a visual environment. CoRR, abs/1907.05084.

Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, pages 1821–1831.

Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus
Rohrbach, Yuandong Tian, Dhruv Batra, and Devi
Parikh. 2019. CoDraw: Collaborative Drawing as a
Testbed for Grounded Goal-driven Communication.
CoRR, abs/1704.04517.

Theodora Koulouri and Stanislao Lauria. 2009. Ex-
ploring miscommunication and collaborative be-
haviour in human-robot interaction. In Proceedings
of the SIGDIAL Conference, pages 111–119.

Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh,
and Dhruv Batra. 2017. Deal or no deal? End-to-end
learning of negotiation dialogues. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 2443–2453.

Claire Liang, Julia Proft, Erik Andersen, and Ross A.
Knepper. 2019. Implicit communication of action-
able information in human-AI teams. In Proceed-
ings of the CHI Conference on Human Factors in
Computing Systems, pages 95:1–95:13.

Karen E. Lochbaum. 1998. A collaborative planning
model of intentional structure. Computational Lin-
guistics, 24(4):525–572.

Reginald Long, Panupong Pasupat, and Percy Liang.
2016. Simpler context-dependent logical forms via
model projections. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 1456–1465.

Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting
language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tificial Intelligence.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. Listen, attend, and walk: Neural mapping
of navigational instructions to action sequences. In
Proceedings of the AAAI Conference on Artificial In-
telligence.

Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical approach
to natural language interfaces. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics, pages 55–61.

Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind
Niklasson, Max Shatkhin, and Yoav Artzi. 2018.
Mapping instructions to actions in 3D environments
with visual goal prediction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2667–2678.

Dipendra Misra, John Langford, and Yoav Artzi. 2017.
Mapping instructions and visual observations to ac-
tions with reinforcement learning. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 1004–1015.

Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Proceedings of the West
Coast Conference on Formal Linguistics, pages 1–
20.

Stéphane Ross, Geoffrey Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Pro-
ceedings of the International Conference on Artifi-
cial Intelligence and Statistics.

Candace L. Sidner, Carolyn Boettner, and Charles
Rich. 2000. Lessons learned in building spoken
language collaborative interface agents. In ANLP-
NAACL Workshop: Conversational Systems.

Alane Suhr and Yoav Artzi. 2018. Situated mapping
of sequential instructions to actions with single-step
reward observation. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 2072–2082.

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to map context-dependent sentences to ex-
ecutable formal queries. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 2238–2249.

Stephanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proceedings of the Na-
tional Conference on Artificial Intelligence.

Jesse Thomason, Shiqi Zhang, Raymond Mooney, and
Peter Stone. 2015. Learning to interpret natural lan-
guage commands through human-robot dialog. In
Proceedings of the International Joint Conference
on Artificial Intelligence.

Takuma Udagawa and Akiko Aizawa. 2019. A nat-
ural language corpus of common grounding under
continuous and partially-observable context. In Pro-
ceedings of the Conference on Artificial Intelligence.

Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lem-
pitsky. 2017. Improved texture networks: Maximiz-
ing quality and diversity in feed-forward stylization
and texture synthesis. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 4105–
4113.

https://doi.org/10.18653/v1/P17-1167
https://doi.org/10.18653/v1/P17-1167
https://www.aclweb.org/anthology/W09-3915
https://www.aclweb.org/anthology/W09-3915
https://www.aclweb.org/anthology/W09-3915
https://doi.org/10.18653/v1/D17-1259
https://doi.org/10.18653/v1/D17-1259
https://doi.org/10.1145/3290605.3300325
https://doi.org/10.1145/3290605.3300325
https://www.aclweb.org/anthology/J98-4001
https://www.aclweb.org/anthology/J98-4001
https://doi.org/10.18653/v1/P16-1138
https://doi.org/10.18653/v1/P16-1138
https://doi.org/10.3115/981863.981871
https://doi.org/10.3115/981863.981871
https://www.aclweb.org/anthology/D18-1287
https://www.aclweb.org/anthology/D18-1287
https://doi.org/10.18653/v1/D17-1106
https://doi.org/10.18653/v1/D17-1106
https://www.aclweb.org/anthology/W00-0301
https://www.aclweb.org/anthology/W00-0301
http://aclweb.org/anthology/P18-1193
http://aclweb.org/anthology/P18-1193
http://aclweb.org/anthology/P18-1193
http://aclweb.org/anthology/N18-1203
http://aclweb.org/anthology/N18-1203
https://doi.org/10.1109/CVPR.2017.437
https://doi.org/10.1109/CVPR.2017.437
https://doi.org/10.1109/CVPR.2017.437


2130

Andreas Vlachos and Stephen Clark. 2014. A new cor-
pus and imitation learning framework for context-
dependent semantic parsing. Transactions of the As-
sociation for Computational Linguistics, 2:547–560.

Harm de Vries, Kurt Shuster, Dhruv Batra, Devi
Parikh, Jason Weston, and Douwe Kiela. 2018.
Talk the Walk: Navigating New York City
through grounded dialogue. arXiv preprint
arXiv:1807.03367.

Sida I. Wang, Percy Liang, and Christopher D. Man-
ning. 2016. Learning language games through in-
teraction. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 2368–2378.

Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL Con-
ference, pages 404–413.

Ronald J. Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurrent
neural networks. Neural Computation, 1:270–280.

https://doi.org/10.1162/tacl_a_00202
https://doi.org/10.1162/tacl_a_00202
https://doi.org/10.1162/tacl_a_00202
https://doi.org/10.18653/v1/P16-1224
https://doi.org/10.18653/v1/P16-1224
https://www.aclweb.org/anthology/W13-4065
https://www.aclweb.org/anthology/W13-4065

