



















































Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2271–2277,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Insertion Position Selection Model for Flexible Non-Terminals in
Dependency Tree-to-Tree Machine Translation

Toshiaki Nakazawa
Japan Science and Technology Agency

5-3, Yonbancho, Chiyoda-ku, Tokyo, 102-8666, Japan
nakazawa@pa.jst.jp

John Richardson and Sadao Kurohashi
Kyoto University

Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
john@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp

Abstract

Dependency tree-to-tree translation models
are powerful because they can naturally han-
dle long range reorderings which is important
for distant language pairs. The translation pro-
cess is easy if it can be accomplished only
by replacing non-terminals in translation rules
with other rules. However it is sometimes
necessary to adjoin translation rules. Flex-
ible non-terminals have been proposed as a
promising solution for this problem. A flex-
ible non-terminal provides several insertion
position candidates for the rules to be ad-
joined, but it increases the computational cost
of decoding. In this paper we propose a neu-
ral network based insertion position selection
model to reduce the computational cost by
selecting the appropriate insertion positions.
The experimental results show the proposed
model can select the appropriate insertion po-
sition with a high accuracy. It reduces the de-
coding time and improves the translation qual-
ity owing to reduced search space.

1 Introduction
Tree-to-tree machine translation models currently
receive limited attention. However, we believe that
using target-side syntax is important to achieve high-
quality translations between distant language pairs
which require long range reorderings. Especially,
using dependency trees on both source and tar-
get sides is promising for this purpose (Menezes
and Quirk, 2007; Nakazawa and Kurohashi, 2010;
Richardson et al., 2014). Tree-based translation
models naturally realize word reorderings using the
non-terminals or anchors for the attachment in the
translation rules: therefore they do not need a re-

ordering model which string-based models require.
For example, suppose we have a translation rule with
the word alignment shown in Figure 1, it is easy to
translate a new input sentence which has “図書館
(library)” instead of “公園 (park)” because we can
accomplish it by simply substituting “library” for the
target word “park” without considering the reorder-
ing. In this case, the source word “公園” and target
word “park” work as the non-terminals.

On the other hand, it is problematic when we need
to adjoin a subtree which is not presented in train-
ing sentences, which we call floating subtree in this
paper. The floating subtrees are not necessarily ad-
juncts, but any words or phrases. For example, sup-
pose the Japanese input sentence in Figure 1 has “
突然 (suddenly)”, but the training corpus provides
only a translation rule without the word. In this case
we cannot directly use the rule for the translation be-
cause it does not know where to insert the translation
of the floating word in the output. As another exam-
ple, there is no context information available for the
children of the OOV word in the input sentence, so
we need some special process to translate them.

Previous work deals with this problem by us-
ing glue rules (Chiang, 2005) or limiting the de-
pendency structures to be well-formed (Shen et al.,
2008). Richardson et al. (2016) introduces the con-
cept of flexible non-terminals. It provides multiple
possible insertion positions for the floating subtree
rather than fixed insertion positions. A possible in-
sertion position must satisfy the following condi-
tions:

• it must be a child of the aligned word of the
parent of the floating subtree

2271



• it must not violate the projectivity of the depen-
dency tree

For example, possible insertion positions for the
floating word “突然” are shown in gray arrows in
Figure 1. Since “突然” is a child of “電話する”, and
the translation of “電話する” is “called”, insertion
positions must be a child of “called”. Also, insertion
positions do not violate the projectivity of the tar-
get tree. Flexible non-terminals are analogous to the
auxiliary tree of the tree adjoining grammars (TAG)
(Joshi, 1985), which is successfully adopted in ma-
chine translation (DeNeefe and Knight, 2009). The
difference is that TAG is defined on the constituency
trees rather than the dependency trees.

Flexible non-terminals are powerful to handle
floating subtrees and it achieve better translation
quality. However the computational cost of decod-
ing becomes high even though they are compactly
represented in the lattice form (Cromieres and Kuro-
hashi, 2014). In our experiments, using flexible non-
terminals causes the decoding to be 3 to 6 times
slower than when they are not used. Flexible non-
terminals increase the number of translation rules
because the insertion positions are selected during
the decoding. However, we think it is possible to re-
strict possible insertion positions or even select only
one insertion position by looking at the tree struc-
tures on both sides.

In this paper, we propose a method to select the
appropriate insertion position before decoding. This
can not only reduce the decoding time but also
improve the translation quality because of reduced
search space.

2 Insertion Position Selection
We assume that correct insertion positions can be
determined before decoding, using the word to be
inserted (I) with the context on the source side and
the context of the insertion positions on the target
side. On the source side, we use the parent of I (Ps)
and the distance of I from Ps (Ds). On the target
side, we use the previous (Sp) and next (Sn) sibling
of the insertion position, the parent of the insertion
position (Pt) and the distance of the insertion posi-
tion from Pt (Dt). The distances are calculated on
the siblings rather than the words in the sentence,
and it is a positive or negative value if the insertion

彼	は	昨日	公園	で	彼女	に	電話した	

he	 called	 her	 in	 the	 park	 yesterday	

(to)	(nominative)	

 	  	  	  	  	  	

彼	は	昨日	公園	で	突然	彼女	に	電話した	
(suddenly)	

input	

translation 
rule	

Figure 1: Example of an input sentence and a translation rule.
We want to insert “突然 (suddenly)” which is not in the transla-

tion rule. The possible insertion positions in the target sentence

are shown in gray arrows.

position is to the left or to the right of the parent re-
spectively.

Taking the insertion position between “park” and
“yesterday” in Figure 1 as an example, I = “突然”,
Ps = “電話した”, Ds = +2, Sp = “park”, Sn = “yes-
terday”, Pt = “called” and Dt = -3. In cases where
Sp or Sn is empty, we use special words “[[LEFT-
START]]”, “[[LEFT-END]]”, “[[RIGHT-START]]”
and “[[RIGHT-END]]”. In the case of “yesterday”
in Figure 1, Sp = “in” and Sn = “[[RIGHT-END]]”.
These clues are fed into the neural network model to
solve the insertion position selection problem.

2.1 Neural Network Model
Figure 2 shows the neural network model for the in-
sertion position selection. Given an insertion posi-
tion candidate with an index k, the words (I , Ps,
Skp , S

k
n, Pt) are first converted into vector represen-

tations through the same three embedding layers:
surface form embedding (200 dimensions.), part-
of-speech embedding (10 dimensions) and depen-
dency type (or phrase category) embedding (10 di-
mensions), and they are concatenated to create the
220-dimension vectors. The word embedding is a
randomly initialized transformation from an one-hot
vector to a 200 or 10-dimensional vector, and it is
learned during the whole network training.

Using these words and the distances, we create
source and target context vectors cks and c

k
t which

represent the information of source and target sides,
respectively. The distances (integer values) are di-
rectly inputted to the network. Then the context vec-

2272



220I

Ps

P

Sp1

Sn1

Ds

Dtk

100
100

220
220

220
220

100

source	
context	
vector

target	
context	
vector

context	
vector	
of	the	
given	

inser4on	
posi4on	

	
	
	

1
1

1

	
	
	

inser4on	
posi4on	

1

inser4on	
posi4on	

2

inser4on	
posi4on	

N

score	
vector

0.1	
0.6	
		
		
		
0.1

0	
1	
		
		
	
0	

so8max gold

loss	=	
so8max	cross-entropy

word	to	be	
inserted

parent	of	I

previous	
sibling	

next	
sibling	

parent	of	
the	inser4on	
posi4on	

distance	from	
the	parent

distance	from	
the	parent

cs
1

ct
1

ci
1

s1

s2

sN

Figure 2: The neural network for the insertion position selec-
tion. The numbers inside the boxes show the dimensions of the

vectors.

tor of the given insertion position cki is created using
cks and c

k
t . Finally we get the score of the given in-

sertion position sk from cki . These vectors are calcu-
lated as follows:

cks = tanh(Wcs [I; Ps; Ds])

ckt = tanh(Wct [Sp; Sn; Pt; D
k
t ])

cki = tanh(Wci [c
k
s ; c

k
t ])

sk = Wsc
k
i

where “;” means concatenation of the vectors. The
size of cks , c

k
t and c

k
i is 100 in our experiments.

The same network is applied to all the other in-
sertion positions and get their scores. Finally the
scores are normalized by the softmax function, and
the loss is calculated by the softmax cross-entropy
as the loss function. All the links between layers
are fully-connected. We use dropout (50%) to avoid
overfitting.

2.2 Training Data Generation
The data for training the neural network model can
be automatically generated from the word-aligned
parallel corpus with dependency parses in both sides
by Algorithm 1. The ALIGNMENT function returns
the aligned word in the target tree for the given
source word1, and the ISPARENTCHILD function re-
turns TRUE if Pt is the parent of Ct.

1In case of the multiple word alignment, we only use the
root word of them in both source and target sides.

Algorithm 1 Training Data Generation for NN
for all Ps ∈ words in source tree do

Pt = ALIGNMENT(Ps)
for all Cs ∈ CHILDREN(Ps) do

Ct = ALIGNMENT(Cs)
if ISPARENTCHILD(Pt, Ct) then

GENERATEDATA(Ps, Cs, Pt, Ct)
end if

end for
end for

Ja ↔ En Ja ↔ Zh
Training 2,020,106 667,520
Development 1,789 2,115
Test 1,812 2,174

Table 1: The number of sentences in ASPEC used in our exper-
iments.

The GENERATEDATA function generates one in-
stance of training data to predict the position of Ct
from Ps, Cs and Pt with their contexts by removing
Ct in the target tree. The position where Ct exists is
regarded as the correct insertion position, and others
as incorrect insertion positions. Note that Cs corre-
sponds to I in Figure 2.

2.3 Insertion Position Selection in Translation
Once the neural network model is trained, it can be
applied to select the most appropriate insertion po-
sitions in the translation rules for the given float-
ing subtree by looking at the score of each inser-
tion position. Translation rules only contain part of
the original parallel sentence in most of the cases.
This means that the context used for selecting the in-
sertion position is different from that in the training
data for the neural network. For example, if the in-
put sentence does not have “公園で (in the park)” in
Figure 1, the number of possible insertion positions
is 6 and we do not use “in” as the context. How-
ever, this is not so problematic because similar or
same context may appear in the different part of the
corpus.

3 Experiments
We conducted two kinds of experiments: the in-
sertion position selection and translation. We used
ASPEC (Nakazawa et al., 2016) as the dataset and
the numbers of the sentences of the corpus are
shown in Table 1. The Japanese morphological ana-
lyzer (Kurohashi et al., 1994) and dependency parser
(Kurohashi and Nagao, 1994) are used for Japanese

2273



Ja → En En → Ja Ja → Zh Zh → Ja
Training 15.7M 5.7M
Development 160K 58K
Test 160K 58K
ave. # IP 3.39 3.15 3.72 3.41
best epoch 89 71 61 79
mean loss 0.089 0.058 0.105 0.056
Accuracy (%) 97.08 97.72 96.51 97.99
Logit Accuracy (%) 55.00 89.03 68.04 83.16
Table 2: The statistics of the data and results of the insertion
position selection experiments.

sentences. English sentences are first parsed by nl-
parser (Charniak and Johnson, 2005) and then con-
verted into word dependency trees using Collins’
head percolation table (Collins, 1999). We used Chi-
nese word segmenter KKN (Shen et al., 2014) and
dependency parser SKP (Shen et al., 2012) for Chi-
nese sentences. The supervised word alignment Nile
(Riesa et al., 2011) was used.

We used a state-of-the-art dependency tree-to-tree
decoder (Richardson et al., 2014) with the default
settings. The neural network is constructed and
trained using the Chainer (Tokui et al., 2015).

3.1 Insertion Position Selection
The training, development and test data for the neu-
ral network is automatically generated by the proce-
dure explained in Section 2.2. The size of the gen-
erated data from the ASPEC and the average num-
ber of insertion positions for each floating subtree
are shown in Table 2. We trained the model for 100
epochs and used the best model on the development
data for testing. The vocabulary size for the surface
form was 50,000.

For comparison, we also tried the logistic regres-
sion to predict the correct insertion positions. Be-
cause our training data is huge, we used Multi-core
LIBLINEAR2 with L2-regularized logistic regres-
sion (primal) solver. The format of training in-
stances are: one-hot (binary) vectors for surface
form, POS and dependency type, and distances
scaled to [0, 1]. We first find the best value for the C
parameter, then train the model. The best insertion
position is selected using the estimated probabilities
for each insertion position.

The experimental results are also shown in Table
2https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/multicore-

liblinear/

2. We evaluated the results by the mean loss of the
model and the accuracy on the test data. The re-
sult shows that our model can select the correct in-
sertion position with very high accuracy for every
language pair while the classical logistic regression
model cannot. This supports our claim stated in the
beginning of Section 2.

X → Ja is easier and achieved slightly better ac-
curacy than the reverse direction because Japanese
is a head-final language and all children are gener-
ally put on the left of their parents. There are some
cases judged as incorrect but acceptable insertion
positions, and hence the true accuracies are higher
than the ones reported above. We also investigated
the top-2 accuracy and found that it is above 99.0%
for Ja → X and 99.5% for X → Ja.

Table 3 shows the detailed result of Ja
rightarrow En experiment. The number of
insertion-position is at least 2 (left/right of the
parent) and it is easy to solve (more than 99%
accuracy). 3 is a situation where the parent has
one child, and it is still not so difficult (97-98%
accuracy). About 70% of the test data have only
2 or 3 insertion-positions. Difficult cases are the
sentences which have many adjuncts as in Figure 1,
but we used the scientific paper corpora, where not
so many adjuncts appear.

3.2 Translation
We conducted translation experiment using the AS-
PEC in 3 settings:

• No Flexible: not using the flexible non-
terminals and using simple glue rules as in the
baseline model of (Richardson et al., 2016) 3

• Baseline: using the flexible non-terminals
without the insertion position selection

• Proposed: using only the most appropriate in-
sertion position for the flexible non-terminals

We also report the translation quality of conven-
tional models for comparison: phrase-based SMT
(PBSMT) and hierarchical phrase-based SMT (Hi-
ero). We used the default settings of Moses except
-distortion-limit=20 for PBSMT.

The translation quality is evaluated by the auto-
matic evaluation measures BLEU (Papineni et al.,

352.5% of all the translation rules require glue rule, but it is
applied to 22.6% of the rules actually used in the translation.

2274



# of ins. pos. Top N accuracy (%)
and ratio 1 2 3 4 5 6 7 8 9
2 (49.39%) 99.22
3 (18.15%) 96.37 99.60
4 (11.77%) 95.76 99.06 99.82
5 (6.30%) 94.24 98.39 99.38 99.85
6 (4.99%) 93.50 97.82 99.11 99.63 99.98
7 (3.96%) 93.13 97.40 98.87 99.56 99.89 99.98
8 (2.42%) 92.77 97.07 98.43 99.23 99.61 99.77 99.97
9 (1.50%) 92.14 96.19 97.85 99.01 99.50 99.67 99.92 100.00
10 (0.77%) 92.04 96.62 97.75 98.47 99.03 99.52 99.92 99.92 100.00

Table 3: Detailed Ja → En insertion position selection experimental result.

Ja → En En → Ja Ja → Zh Zh → Ja
BLEU RIBES Time BLEU RIBES Time BLEU RIBES Time BLEU RIBES Time

PBSMT 18.45 64.51 - 27.48 68.37 - 27.96 78.90 - 34.65 77.25 -
Hiero 18.72 65.11 - 30.19 73.47 - 27.71 80.91 - 35.43 81.04 -
No Flexible 20.28 65.08 1.00 28.77 75.21 1.00 24.85 66.60 1.00 30.51 73.08 1.00
Baseline 21.61 69.82 6.28 30.57 76.13 3.30 28.79 78.11 5.16 34.32 77.82 5.28
Proposed 22.07† 70.49† 2.25 30.50 76.69† 1.27 29.83† 79.73† 2.21 34.71† 79.25† 1.89

Table 4: The results of the translation experiments. † means the Proposed method achieved significantly better score than the
Baseline (p < 0.01).

2002) and RIBES (Isozaki et al., 2010) with the sig-
nificance testing by bootstrap resampling (Koehn,
2004). RIBES is more sensitive to word order than
BLEU, so we expect an improvement in RIBES. We
also investigated relative decoding time compared to
the No Flexible setting. Note that we used the word
“decoding” for only exploring the search space, and
it does not include constructing the search space (as
the table lookup in Phrase-based SMT). Our whole
translation process is:

1. translation rule extraction
2. insertion-position selection
3. decoding

At the time of the second step, we have all the trans-
lation rules applicable to the input sentence. The
computation time for each step is 3 ≫ 1 ≫ 2 so we
only focus on the time for step 3 in the experiments
(the computation time for step 2 is negligibly small).

The results are shown in Table 4. The Proposed
method achieved significantly better automatic eval-
uation scores than the Baseline for all the language
pairs except the BLEU score of En → Ja direction.
Also, the decoding time is reduced by about 60%
relative to that of the Baseline.

Our tree-based model is better than the conven-
tional models except C → J, where the accuracy of

Chinese parsing for the input sentences has a bad ef-
fect.

4 Conclusion
In this paper we have proposed a neural network
based insertion position selection model to reduce
the computational cost of the decoding for de-
pendency tree-to-tree translation with flexible non-
terminals. The model successfully finds the appro-
priate insertion position from the candidates and it
leads to faster translation speed and better transla-
tion quality due to the reduced search space.

Currently, we use only words as the context but
it seems promising to use subtrees as well. For ex-
ample, using the information of the subtree “in the
park” is more informative than using only “in” in
Figure 1. This is especially important for Japanese
as the target language because children of verbs are
often case markers and they do not provide enough
information when selecting the appropriate insertion
position. It is possible to adopt existing models of
creating vector representation of dependency sub-
trees such as the model using recursive neural net-
works (Liu et al., 2015) and convolutional neural
networks (Mou et al., 2015).

2275



References

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
pages 173–180.

David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 263–270. As-
sociation for Computational Linguistics.

Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.

Fabien Cromieres and Sadao Kurohashi. 2014. Transla-
tion rules with right-hand side lattices. In Proceedings
of the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 577–588.
Association for Computational Linguistics.

Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 727–736. Association for
Computational Linguistics.

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 944–952, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

A. K. Joshi. 1985. Tree adjoining grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In D. R. Dowty, L. Kart-
tunen, and A. M. Zwicky, editors, Natural Language
Parsing: Psychological, Computational, and Theoret-
ical Perspectives, pages 206–250. Cambridge Univer-
sity Press, Cambridge.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507–534.

Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proceedings of
The International Workshop on Sharable Natural Lan-
guage, pages 22–28.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and
Houfeng WANG. 2015. A dependency-based neu-
ral network for relation classification. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Process-
ing (Volume 2: Short Papers), pages 285–290. Associ-
ation for Computational Linguistics.

Arul Menezes and Chris Quirk, 2007. Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, chapter Using Dependency Order Templates to
Improve Generality in Translation, pages 1–8. Asso-
ciation for Computational Linguistics.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Discriminative neural sentence mod-
eling by tree-based convolution. In Proceedings of
the 2015 Conference on Empirical Methods in Natural
Language Processing, pages 2315–2325, Lisbon, Por-
tugal, September. Association for Computational Lin-
guistics.

Toshiaki Nakazawa and Sadao Kurohashi. 2010. Fully
syntactic ebmt system of kyoto team in ntcir-8. In
In Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technologies
(NTCIR-8), pages 403–410.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao Kuro-
hashi, and Hitoshi Isahara. 2016. ASPEC: Asian
scientific paper excerpt corpus. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC 2016), pages 2204–
2208, Portorož, Slovenia, May.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, pages 311–318.

John Richardson, Fabien Cromières, Toshiaki Nakazawa,
and Sadao Kurohashi. 2014. Kyotoebmt: An
example-based dependency-to-dependency translation
framework. In Proceedings of 52nd Annual Meeting of
the Association for Computational Linguistics: System
Demonstrations, pages 79–84. Association for Com-
putational Linguistics.

John Richardson, Fabien Cromierès, Toshiaki Nakazawa,
and Sadao Kurohashi. 2016. Flexible non-terminals
for dependency tree-to-tree reordering. In Proceed-
ings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 11–19.
Association for Computational Linguistics.

Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proceedings of the 2011 Conference on Empirical

2276



Methods in Natural Language Processing, pages
497–507. Association for Computational Linguistics.

Libin Shen, Jinxi Xu, and Ralph M Weischedel. 2008.
A new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Association for Computational Linguistics.

Mo Shen, Daisuke Kawahara, and Sadao Kurohashi.
2012. A reranking approach for dependency parsing
with variable-sized subtree features. In Proceedings
of 26th Pacific Asia Conference on Language Infor-
mation and Computing, pages 308–317.

Mo Shen, Hongxiao Liu, Daisuke Kawahara, and Sadao
Kurohashi. 2014. Chinese morphological analysis
with character-level pos tagging (short paper). In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL2014), Balti-
more, USA.

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clay-
ton. 2015. Chainer: a next-generation open source
framework for deep learning. In Proceedings of Work-
shop on Machine Learning Systems (LearningSys) in
The Twenty-ninth Annual Conference on Neural Infor-
mation Processing Systems (NIPS).

2277


