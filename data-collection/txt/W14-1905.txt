



















































Individuality-preserving Voice Conversion for Articulation Disorders Using Dictionary Selective Non-negative Matrix Factorization


Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 29–37,
Baltimore, Maryland USA, August 26 2014. c©2014 Association for Computational Linguistics

Individuality-preserving Voice Conversion for Articulation Disorders
Using Dictionary Selective Non-negative Matrix Factorization

Ryo Aihara, Tetsuya Takiguchi, Yasuo Ariki
Graduate School of System Informatics, Kobe University

1-1, Rokkodai, Nada, Kobe, 6578501, Japan
aihara@me.cs.scitec.kobe-u.ac.jp,

takigu@kobe-u.ac.jp,
ariki@kobe-u.ac.jp

Abstract

We present in this paper a voice conver-
sion (VC) method for a person with an ar-
ticulation disorder resulting from athetoid
cerebral palsy. The movements of such
speakers are limited by their athetoid
symptoms, and their consonants are of-
ten unstable or unclear, which makes it
difficult for them to communicate. In
this paper, exemplar-based spectral con-
version using Non-negative Matrix Factor-
ization (NMF) is applied to a voice with
an articulation disorder. In order to pre-
serve the speaker’s individuality, we use a
combined dictionary that was constructed
from the source speaker’s vowels and tar-
get speaker’s consonants. However, this
exemplar-based approach needs to hold
all the training exemplars (frames), and
it may cause mismatching of phonemes
between input signals and selected ex-
emplars. In this paper, in order to re-
duce the mismatching of phoneme align-
ment, we propose a phoneme-categorized
sub-dictionary and a dictionary selec-
tion method using NMF. The effective-
ness of this method was confirmed by
comparing its effectiveness with that of
a conventional Gaussian Mixture Model
(GMM)-based and conventional NMF-
based method.

1 Introduction

In this study, we focused on a person with an
articulation disorder resulting from the athetoid
type of cerebral palsy. About two babies in 1,000
are born with cerebral palsy (Hollegaard et al.,
2013). Cerebral palsy results from damage to the
central nervous system, and the damage causes
movement disorders. Cerebral palsy is classified

into the following types: 1)spastic, 2)athetoid,
3)ataxic, 4)atonic, 5)rigid, and a mixture of these
types (Canale and Campbell, 2002).

Athetoid symptoms develop in about 10-15% of
cerebral palsy sufferers (Hollegaard et al., 2013).
In the case of a person with this type of articulation
disorder, his/her movements are sometimes more
unstable than usual. That means their utterances
(especially their consonants) are often unstable or
unclear due to the athetoid symptoms. Athetoid
symptoms also restrict the movement of their arms
and legs. Most people suffering from athetoid
cerebral palsy cannot communicate by sign lan-
guage or writing, so there is great need for voice
systems for them.

In this paper, we propose a voice conversion
(VC) method for articulation disorders. Regard-
ing speech recognition for articulation disorders,
the recognition rate using a speaker-independent
model which is trained by well-ordered speech, is
3.5% (Matsumasa et al., 2009). This result im-
plies that the utterance of a person with an artic-
ulation disorder is difficult to understand for peo-
ple who have not communicated with them before.
In recent years, people with an articulation dis-
order may use slideshows and a previously syn-
thesized voice when they give a lecture. How-
ever, because their movement is restricted by their
athetoid symptoms, to make slides or synthesize
their voice in advance is hard for them. Peo-
ple with articulation disorders desire a VC sys-
tem that converts their voice into a clear voice
that preserves their voice’s individuality. Rudz-
icz et al. (Rudzicz, 2011; Rudzicz, 2014) proposed
speech adjustment method for people with articu-
lation disorders based on the observations from the
database.

In (Aihara et al., 2014), we proposed
individuality-preserving VC for articulation
disorders. In our VC, source exemplars and
target exemplars are extracted from the parallel29



training data, having the same texts uttered by
the source and target speakers. The input source
signal is expressed with a sparse representation
of the source exemplars using Non-negative
Matrix Factorization (NMF). By replacing a
source speaker’s exemplar with a target speaker’s
exemplar, the original speech spectrum is replaced
with the target speaker’s spectrum. People with
articulation disorders wish to communicate by
their own voice if they can; therefore, we pro-
posed a combined-dictionary, which consists of
a source speaker’s vowels and target speaker’s
well-ordered consonants. In the voice of a person
with an articulation disorder, their consonants are
often unstable and that makes their voices unclear.
Their vowels are relatively stable compared
to their consonants. Hence, by replacing the
articulation-disordered basis of consonants only,
a voice with an articulation disorder is converted
into a non-disordered voice that preserves the
individuality of the speaker’s voice.

In this paper, we propose advanced
individuality-preserving VC using NMF. In
order to avoid a mixture of the source and target
spectra in a converted phoneme, we applied a
phoneme-categorized dictionary and a dictionary
selection method to our VC using NMF. In
conventional NMF-based VC, the number of
dictionary frames becomes large because the
dictionary holds all the training exemplar frames.
Therefore, it may cause phoneme mismatching
between input signals and selected exemplars and
some frames of converted spectra might be mixed
with the source and target spectra. In this paper,
a training exemplar is divided into a phoneme-
categorized sub-dictionary, and an input signal is
converted by using the selected sub-dictionary.
The effectiveness of this method was confirmed
by comparing it with a conventional NMF-based
method and a conventional Gaussian Mixture
Model (GMM)-based method.

The rest of this paper is organized as follows:
In Section 2, related works are introduced. In Sec-
tion 3, the basic idea of NMF-based VC is de-
scribed. In Section 4, our proposed method is de-
scribed. In Section 5, the experimental data are
evaluated, and the final section is devoted to our
conclusions.

2 Related Works

Voice conversion (VC) is a technique for convert-
ing specific information in speech while maintain-
ing the other information in the utterance. One of
the most popular VC applications is speaker con-
version (Stylianou et al., 1998). In speaker con-
version, a source speaker’s voice individuality is
changed to a specified target speaker’s so that the
input utterance sounds as though a specified target
speaker had spoken it.

There have also been studies on several tasks
that make use of VC. Emotion conversion is a
technique for changing emotional information in
input speech while maintaining linguistic informa-
tion and speaker individuality (Veaux and Robet,
2011). In recent years, VC has been used for auto-
matic speech recognition (ASR) or speaker adap-
tation in text-to-speech (TTS) systems (Kain and
Macon, 1998). These studies show the varied uses
of VC.

Many statistical approaches to VC have been
studied (Valbret et al., 1992). Among these ap-
proaches, the Gaussian mixture model (GMM)-
based mapping approach (Stylianou et al., 1998)
is widely used. In this approach, the conversion
function is interpreted as the expectation value
of the target spectral envelope. The conversion
parameters are evaluated using Minimum Mean-
Square Error (MMSE) on a parallel training set.
A number of improvements in this approach have
been proposed. Toda et al. (Toda et al., 2007)
introduced dynamic features and the global vari-
ance (GV) of the converted spectra over a time
sequence. Helander et al. (Helander et al., 2010)
proposed transforms based on partial least squares
(PLS) in order to prevent the over-fitting problem
associated with standard multivariate regression.
There have also been approaches that do not re-
quire parallel data that make use of GMM adapta-
tion techniques (Lee and Wu, 2006) or eigen-voice
GMM (EV-GMM) (Toda et al., 2006).

In the field of assistive technology, Nakamura
et al. (Nakamura et al., 2012; Nakamura et al.,
2006) proposed GMM-based VC systems that re-
construct a speaker’s individuality in electrolaryn-
geal speech and speech recorded by NAM micro-
phones. These systems are effective for electrola-
ryngeal speech and speech recorded by NAM mi-
crophones however, because these statistical ap-
proaches are mainly proposed for speaker con-
version, the target speaker’s individuality will be30



changed to the source speaker’s individuality. Peo-
ple with articulation disorders wish to communi-
cate by their own voice if they can and there is a
needs for individuality-preserving VC.

Text-to-speech synthesis (TTS) is a famous
voice application that is widely researched. Veaux
et al. (Veaux et al., 2012) used HMM-based speech
synthesis to reconstruct the voice of individu-
als with degenerative speech disorders resulting
from Amyotrophic Lateral Sclerosis (ALS). Ya-
magishi et al. (Yamagishi et al., 2013) proposed
a project named “Voice Banking and Reconstruc-
tion”. In that project, various types of voices are
collected and they proposed TTS for ALS using
that database. The difference between TTS and
VC is that TTS needs text input to synthesize
speech, whereas VC does not need text input. In
the case of people with articulation disorders re-
sulting from athetoid cerebral palsy, it is difficult
for them to input text because of their athetoid
symptoms.

Our proposed NMF-based VC (Takashima et
al., 2012) is an exemplar-based method using
sparse representation, which is different from the
conventional statistical method. In recent years,
approaches based on sparse representations have
gained interest in a broad range of signal process-
ing. In approaches based on sparse representa-
tions, the observed signal is represented by a lin-
ear combination of a small number of bases. In
some approaches for source separation, the atoms
are grouped for each source, and the mixed sig-
nals are expressed with a sparse representation of
these atoms. By using only the weights of the
atoms related to the target signal, the target sig-
nal can be reconstructed. Gemmeke et al. (Gem-
meke et al., 2011) also propose an exemplar-based
method for noise-robust speech recognition. In
that method, the observed speech is decomposed
into the speech atoms, noise atoms, and their
weights. Then the weights of the speech atoms are
used as phonetic scores (instead of the likelihoods
of hidden Markov models) for speech recognition.

In (Takashima et al., 2012), we proposed noise-
robust VC using NMF. The noise exemplars,
which are extracted from the before- and after-
utterance sections in an observed signal, are used
as the noise-dictionary, and the VC process is
combined with an NMF-based noise-reduction
method. On the other hand, NMF is one of the
clustering methods. In our exemplar-based VC, if

the phoneme label of the source exemplar is given,
we can discriminate the phoneme of the input sig-
nal by using NMF. In this paper, we proposed a
dictionary selection method using this property of
NMF.

3 Voice Conversion Based on
Non-negative Matrix Factorization

3.1 Basic Idea
In the exemplar-based approach, the observed sig-
nal is represented by a linear combination of a
small number of bases.

xl ≈
∑J

j=1 ajhj,l = Ahl (1)

xl represents the l-th frame of the observation.
aj and hj,l represent the j-th basis and the
weight, respectively. A = [a1 . . .aJ ] and hl =
[h1,l . . . hJ,l]

T are the collection of the bases and
the stack of weights. In this paper, each basis de-
notes the exemplar of the spectrum, and the col-
lection of exemplar A and the weight vector hl are
called the ‘dictionary’ and ‘activity’, respectively.
When the weight vector hl is sparse, the observed
signal can be represented by a linear combination
of a small number of bases that have non-zero
weights. Eq. (1) is expressed as the inner product
of two matrices using the collection of the frames
or bases.

X ≈ AH (2)
X = [x1, . . . ,xL], H = [h1, . . . ,hL]. (3)

L represents the number of the frames.
Fig. 1 shows the basic approach of our

exemplar-based VC, where D, L, and J represent
the numbers of dimensions, frames, and bases,
respectively. Our VC method needs two dictio-
naries that are phonemically parallel. As repre-
sents a source dictionary that consists of the source
speaker’s exemplars and At represents a target
dictionary that consists of the target speaker’s ex-
emplars. These two dictionaries consist of the
same words and are aligned with dynamic time
warping (DTW) just as conventional GMM-based
VC is. Hence, these dictionaries have the same
number of bases.

This method assumes that when the source sig-
nal and the target signal (which are the same words
but spoken by different speakers) are expressed
with sparse representations of the source dictio-
nary and the target dictionary, respectively, the ob-31



tained activity matrices are approximately equiv-
alent. Fig. 2 shows an example of the activity
matrices estimated from a Japanese word “ikioi”
(“vigor” in English), where one is uttered by a
male, the other is uttered by a female, and each
dictionary is structured from just one word “ikioi”
as the simple example.

As shown in Fig. 2, these activities have high
energies at similar elements. For this reason, we
assume that when there are parallel dictionaries,
the activity of the source features estimated with
the source dictionary may be able to be substi-
tuted with that of the target features. Therefore,
the target speech can be constructed using the tar-
get dictionary and the activity of the source signal
as shown in Fig. 1. In this paper, we use Non-
negative Matrix Factorization (NMF), which is a
sparse coding method in order to estimate the ac-
tivity matrix.

s
XD

L

s
A

t
A

Parallel data

J

Copy

s
H

s
H

t
X̂

Source

spectral features

(D x L)

Source and target

dictionaries

(D x J)

Converted 

spectral features

(D x L) Activity of

source signal

(J x L)

Activity

estimation

Construction

Figure 1: Basic approach of NMF-based voice
conversion

B
as

is
 I

D
 i

n
 s

o
u

rc
e 

d
ic

ti
o

n
ar

y

Frame of source speech

B
as

is
 I

D
 i

n
 t

ar
g
et

 d
ic

ti
o

n
ar

y

Frame of target speech

 

 

20 40 60 80 100 120 140

50

100

150

200

250

20 40 60 80 100 120

50

100

150

200

250

Figure 2: Activity matrices for parallel utterances

3.2 Individuality-preserving Voice
Conversion Using Combined Dictionary

In order to make a parallel dictionary, some pairs
of parallel utterances are needed, where each pair
consists of the same text. One is spoken by a per-
son with an articulation disorder (source speaker),

and the other is spoken by a physically unim-
paired person (target speaker). Spectrum en-
velopes, which are extracted from parallel utter-
ances, are phonemically aligned by using DTW.
In order to estimate activities of source features
precisely, segment features, which consist of some
consecutive frames, are constructed. Target fea-
tures are constructed from consonant frames of the
target’s aligned spectrum and vowel frames of the
source’s aligned spectrum. Source and target dic-
tionaries are constructed by lining up each of the
features extracted from parallel utterances.

The vowels voiced by a speaker strongly indi-
cate the speaker’s individuality. On the other hand,
consonants of people with articulation disorders
are often unstable. Fig. 3(a) shows an example
of the spectrogram for the word “ikioi” (“vigor”
in English) of a person with an articulation dis-
order. The spectrogram of a physically unim-
paired person speaking the same word is shown in
Fig. 3(b). In Fig. 3(a), the area labeled “k” is not
clear, compared to the same region in to Fig. 3(b).
These figures indicate that consonants of people
with articulation disorders are often unstable and
this deteriorates their voice intelligibility. In or-
der to preserve their voice individuality, we use
a “combined-dictionary” that consists of a source
speaker’s vowels and target speaker’s consonants.

We replace the target dictionary As in Fig. 1
with the “combined-dictionary”. Input source
features Xs, which consist of an articulation-
disordered spectrum and its segment features, are
decomposed into a linear combination of bases
from the source dictionary As by NMF. The
weights of the bases are estimated as an activity
Hs. Therefore, the activity includes the weight in-
formation of input features for each basis. Then,
the activity is multiplied by a combined-dictionary
in order to obtain converted spectral features X̂t,
which are represented by a linear combination of
bases from the source speaker’s vowels and tar-
get speaker’s consonants. Because the source and
target are parallel phonemically, the bases used in
the converted features are phonemically the same
as that of the source features.

3.3 Problems

In the NMF-based approach described in Sec. 3.2,
the parallel dictionary consists of the parallel train-
ing data themselves. Therefore, as the number
of the bases in the dictionary increases, the input32



signal comes to be represented by a linear com-
bination of a large number of bases rather than a
small number of bases. When the number of bases
that represent the input signal becomes large, the
assumption of similarity between source and tar-
get activities may be weak due to the influence of
the mismatch between the input signal and the se-
lected bases. Moreover, in the case of a combined-
dictionary, the input articulation-disordered spec-
trum may come to be represented by a combi-
nation of vowels and consonants. We assume
that this problem degrades the performance of our
exemplar-based VC. Hence, we use a phoneme-
categorized sub-dictionary in place of the large
dictionary in order to reduce the number of the
bases that represent the input signal and avoid the
mixture of vowels and consonants.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0

1000

2000

3000

4000

5000

6000

F
re

q
u

en
cy

 [
H

z]

Time [Sec]

i k            i oi

(a) Spoken by a person with an articulation disorder

0.2 0.4 0.6 0.8 1
0

1000

2000

3000

4000

5000

6000

F
re

q
u

en
cy

 [
H

z]

Time [Sec]

i k           i oi

(b) Spoken by a physically unimpaired person

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0

1000

2000

3000

4000

5000

6000

F
re

q
u

en
cy

 [
H

z]

Time [Sec]

i k            i oi

(c) Converted by NMF-based VC

Figure 3: Examples of spectrogram //i k i oi

4 Non-negative Matrix Factorization
Using a Phoneme-categorized
Dictionary

4.1 Phoneme-categorized Dictionary

Fig. 4 shows how to construct the sub-dictionary.
As and At imply the source and target dictionary
which hold all the bases from training data. These
dictionaries are divided into K dictionaries. In
this paper, the dictionaries are divided into 10 cat-
egories according to the Japanese phoneme cate-
gories shown in Table 1.

In order to select the sub-dictionary, a
“categorizing-dictionary”, which consists of the
representative vector from each sub-dictionary, is
constructed. The representative vectors for each
phoneme category consist of the mean vectors of
the Gaussian Mixture Model (GMM).

p(x(k)n ) =

Mk∑

m=1

α(k)m N(x
(k)
n , µ

(k)
m ,Σ

(k)
m ) (4)

Mk, α
(k)
m , µ

(k)
m and Σ

(k)
m represent the number

of the Gaussian mixture, the weights of mixture,
mean and variance of the m-th mixture of the
Gaussian, in the k-th sub-dictionary, respectively.
Each parameter is estimated by using an EM algo-
rithm.

The basis of the categorizing-dictionary, which
corresponds to the k-th sub-dictionary Φsk, is rep-
resented using the estimated phoneme GMM as
follows:

θk = [µ
(k)
1 , . . . , µ

(k)
Mk

] (5)

Φsk = [x
(k)
1 , . . . ,x

(k)
Nk

] (6)

Nk represents the number of frames of the k-th
sub-dictionary. The categorizing-dictionary Θ is
given as follows:

Θ = [θ1, . . . , θK ] (7)

4.2 Dictionary Selection and Voice
Conversion

Fig. 5 shows the flow of the dictionary selection
and VC. The input spectral features Xs are rep-
resented by a linear combination of bases from
the categorizing-dictionary Θ. The weights of the33



...

...

Source

training speech

Target

training speech

Spectral

envelope

s
A

t
A

Parallel Dictionaries

s

1Φ
s

kΦ

DP-matching

...

...Θ
Categorizing

Dictionary

STRAIGHT

STRAIGHT

Categorization

phoneme categorized 

sub-dictionaries

t

k 1+Φ

s

k 1+Φ

t

KΦ

s

KΦ

s

1Φ
s

kΦ

Vowel sub-dictionaries Consonant sub-dictionaries
Representative 

vectors

Articulation-

disordered spectrum

Well-ordered 

spectrum

Vowels are replaced with 

source speaker’s spectrum

Figure 4: Making a sub-dictionary

bases are represented as activities HsΘ.

Xs ≈ ΘHsΘ s.t. HsΘ ≥ 0 (8)
Xs = [xs1, . . . ,x

s
L] (9)

HsΘ = [h
s
Θ1, . . . ,h

s
ΘL] (10)

hsΘl = [h
s
θ1l, . . . ,h

s
θK l

]T (11)

hsθkl = [h
s
θ1l, . . . , h

s
θMkl

]T (12)

Then, the l-th frame of input feature xsl is rep-
resented by a linear combination of bases from the
sub-dictionary of the source speaker. The sub-
dictionary Φs

k̂
, which corresponds to xl, is se-

lected as follows:

k̂ = arg max
k

11×Mkhsθkl

= arg max
k

Mk∑

m=1

hsθml (13)

xl = Φ
s
k̂
hk̂,l (14)

The activity hl,k̂ in Eq. (14) is estimated from the
selected source speaker sub-dictionary.

If the selected sub-dictionary Φs
k̂

is related to
consonants, the l-th frame of the converted spec-
tral feature ŷl is constructed by using the activity
and the sub-dictionary of the target speaker Φt

k̂
.

ŷl = Φ
t
k̂
hk̂,l (15)

On the other hand, if the selected sub-dictionary
Φs

k̂
is related to vowels, the l-th frame of the con-

verted spectral feature ŷl is constructed by using
the activity and the sub-dictionary of the source
speaker Φs

k̂
.

ŷl = Φ
s
k̂
hk̂,l (16)

Table 1: Japanese phoneme categories

Category phoneme
a a
e e

vowels i i
o o
u u
plosives Q, b, d, dy, g, gy, k, ky, p, t
fricatives ch, f, h, hy, j, s, sh, ts, z

consonants nasals m, my ny, N
semivowels w,y
liquid r, ry

s

lΘh

s

1Φ

Categorizing

Dictionary

l
x

Select the sub-dictionary

Copy

l-th frame of 

input spectral features

l-th frame of converted 

spectral features

Θ

l
ŷ

s

l1θ
h

s

lKθ
h

s

l
k

θ
h

Activity

t

K
Φ

s

K
Φ

s

k 1+Φ

t

k 1+Φ

s

l

M

k
k

kk
θ

h1
×

=
1

maxargˆ

…

…

…

…

l
x

…

…

Activity
lk,ˆ

h

phoneme-categorized sub-dictionaries

s

1Φ

Figure 5: NMF-based voice conversion using cat-
egorized dictionary

5 Experimental Results

5.1 Experimental Conditions
The proposed VC technique was evaluated by
comparing it with the conventional NMF-based
method (Aihara et al., 2014) (referred to as the
“sample-based method” in this paper) and the
conventional GMM-based method (Stylianou et
al., 1998) using clean speech data. We recorded
432 utterances (216 words, each repeated two
times) included in the ATR Japanese speech34



database (Kurematsu et al., 1990). The speech sig-
nals were sampled at 12 kHz and windowed with
a 25-msec Hamming window every 10 msec. A
physically unimpaired Japanese male in the ATR
Japanese speech database, was chosen as a target
speaker.

In the proposed and sample-based methods,
the number of dimensions of the spectral fea-
ture is 2,565. It consists of a 513-dimensional
STRAIGHT spectrum (Kawahara et al., 1999)
and its consecutive frames (the 2 frames com-
ing before and the 2 frames coming after). The
Gaussian mixture, which is used to construct
a categorizing-dictionary, is 1/500 of the num-
ber of bases of each sub-dictionary. The num-
ber of iterations for estimating the activity in
the proposed and sample-based methods was
300. In the conventional GMM-based method,
MFCC+∆MFCC+∆∆MFCC is used as a spectral
feature. Its number of dimensions is 74. The num-
ber of Gaussian mixtures is set to 64, which is ex-
perimentally selected.

In this paper, F0 information is converted using
a conventional linear regression based on the mean
and standard deviation (Toda et al., 2007). The
other information such as aperiodic components,
is synthesized without any conversion.

We conducted a subjective evaluation of 3 top-
ics. A total of 10 Japanese speakers took part
in the test using headphones. For the “listening
intelligibility” evaluation, we performed a MOS
(Mean Opinion Score) test (”INTERNATIONAL
TELECOMMUNICATION UNION”, 2003). The
opinion score was set to a 5-point scale (5: excel-
lent, 4: good, 3: fair, 2: poor, 1: bad). Twenty-two
words that are difficult for a person with an artic-
ulation disorder to utter were evaluated. The sub-
jects were asked about the listening intelligibility
in the articulation-disordered voice, the voice con-
verted by our proposed method, and the GMM-
based converted voice.

On the “similarity” evaluation, the XAB test
was carried out. In the XAB test, each subject lis-
tened to the articulation-disordered voice. Then
the subject listened to the voice converted by the
two methods and selected which sample sounded
most similar to the articulation-disordered voice.
On the “naturalness” evaluation, a paired com-
parison test was carried out, where each subject
listened to pairs of speech converted by the two
methods and selected which sample sounded more

natural.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0

1000

2000

3000

4000

5000

6000

F
re

q
u

en
cy

 [
H

z]

Time [Sec]

i k            i oi

(a) Converted by proposed method

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0

1000

2000

3000

4000

5000

6000

F
re

q
u

en
cy

 [
H

z]

Time [Sec]

i k            i oi

(b) Converted by GMM-based VC

Figure 6: Examples of converted spectrogram //i k
i oi

5.2 Results and Discussion
Fig. 6(a) and 6(b) show examples of converted
spectrograms using our proposed method and the
conventional GMM-based method, respectively.
In Fig. 6(a), there are fewer misconversions in the
vowel part compared to Fig. 3(c). Moreover, by
using GMM-based conversion, the area labeled
“oi” becomes unclear compared to NMF-based
conversion.

Fig. 7 shows the results of the MOS test for lis-
tening intelligibility. The error bars show a 95%
confidence score; thus, our proposed VC method
is shown to be able to improve the listening intel-
ligibility and clarity of consonants. On the other
hand, GMM-based conversion can improve the
clarity of consonants, but it deteriorates the lis-
tening intelligibility. This is because GMM-based
conversion has the effect of noise resulting from
measurement error. Our proposed VC method also
has the effect of noise, but it is less than that cre-
ated by GMM-based conversion.

Fig. 8 shows the results of the XAB test on
the similarity to the source speaker and natural-
ness of the converted voice. The error bars show a
95% confidence score. Our proposed VC method
obtained a higher score than Sample-based and
GMM-based conversion on similarity. Fig. 935



shows the preference score on the naturalness. The
error bars show a 95% confidence score. Our pro-
posed VC also method obtained a higher score
than Sample-based and GMM-based conversion
methods in regard to naturalness.

2

2.1

2.2

2.3

2.4

2.5

2.6

2.7

Proposed GMM source

M
O
S

M
O
S

M
O
S

M
O
S

Figure 7: Results of MOS test on listening intelli-
gibility

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

NMF GMM

P
r
e
fe

re
n
c
e
 S

c
o
r
e

P
r
e
fe

re
n
c
e
 S

c
o
r
e

P
r
e
fe

re
n
c
e
 S

c
o
r
e

P
r
e
fe

re
n
c
e
 S

c
o
r
e

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Proposed NMF

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Proposed GMM

Figure 8: Preference scores for the individuality

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

NMF GMM

P
r
e
fe

re
n
c
e
 S

c
o
r
e

P
r
e
fe

re
n
c
e
 S

c
o
r
e

P
r
e
fe

re
n
c
e
 S

c
o
r
e

P
r
e
fe

re
n
c
e
 S

c
o
r
e

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Proposed NMF

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Proposed GMM

Figure 9: Preference scores for the naturalness

6 Conclusion

We proposed a spectral conversion method based
on NMF for a voice with an articulation disorder.
Our proposed method introduced a dictionary-
selection method for conventional NMF-based
VC. Experimental results demonstrated that our
VC method can improve the listening intelligibil-
ity of words uttered by a person with an articu-
lation disorder. Moreover, compared to conven-
tional GMM-based VC and conventional NMF-
based VC, our proposed VC method can preserve
the individuality of the source speaker’s voice and

the naturalness of the voice. In this study, there
was only one subject person, so in future experi-
ments, we will increase the number of subjects and
further examine the effectiveness of our method.

References
R. Aihara, R. Takashima, T. Takiguchi, and Y. Ariki.

2014. A preliminary demonstration of exemplar-
based voice conversion for articulation disor-
ders using an individuality-preserving dictionary.
EURASIP Journal on Audio, Speech, and Music
Processing, 2014:5, doi:10.1186/1687-4722-2014-
5.

S. T. Canale and W. C. Campbell. 2002. Campbell’s
operative orthopaedics. Technical report, Mosby-
Year Book.

J. F. Gemmeke, T. Viratnen, and A. Hurmalainen.
2011. Exemplar-based sparse representations for
noise robust automatic speech recognition. IEEE
Trans. Audio, Speech and Language Processing, vol.
19, no. 7, pages 2067–2080.

E. Helander, T. Virtanen, J. Nurminen, and M. Gab-
bouj. 2010. Voice conversion using partial least
squares regression. IEEE Trans. Audio, Speech,
Lang. Process., vol. 18, Issue:5, pages 912–921.

Mads Vilhelm Hollegaard, Kristin Skogstrand, Poul
Thorsen, Bent Norgaard-Pedersen, David Michael
Hougaard, and Jakob Grove. 2013. Joint analysis
of SNPs and proteins identifies regulatory IL18 gene
variations decreasing the chance of spastic cerebral
palsy. Human Mutation, Vol. 34, pages 143–148.

INTERNATIONAL TELECOMMUNICATION
UNION. 2003. Methods for objective and subjec-
tive assessment of quality. ITU-T Recommendation
P.800.

A. Kain and M. W. Macon. 1998. Spectral voice con-
version for text-to-speech synthesis. in ICASSP, vol.
1, pages 285–288.

H. Kawahara, I. Masuda-Katsuse, and A.de Cheveigne.
1999. Restructuring speech representations using
a pitch-adaptive time-frequency smoothing and an
instantaneous-frequencybased F0 extraction: possi-
ble role of a repetitive structure in sounds. Speech
Communication, 27(3-4):187–207.

A. Kurematsu, K. Takeda, Y. Sagisaka, S. Katagiri,
H. Kuwabara, and K. Shikano. 1990. ATR Japanese
speech database as a tool of speech recognition and
synthesis. Speech Communication, 9:357–363.

C. H. Lee and C. H. Wu. 2006. Map-based adaptation
for speech conversion using adaptation data selec-
tion and non-parallel training. in Interspeech, pages
2254–2257.36



H. Matsumasa, T. Takiguchi, Y. Ariki, I. Li, and
T. Nakabayachi. 2009. Integration of metamodel
and acoustic model for dysarthric speech recogni-
tion. Journal of Multimedia, Volume 4, Issue 4,
pages 254–261.

K. Nakamura, T. Toda, H. Saruwatari, and K. Shikano.
2006. Speaking aid system for total laryngectomees
using voice conversion of body transmitted artificial
speech. in Interspeech, pages 148–151.

K. Nakamura, T. Toda, H. Saruwatari, and K. Shikano.
2012. Speaking-aid systems using GMM-
based voice conversion for electrolaryngeal speech.
Speech Communication, 54(1):134–146.

F. Rudzicz. 2011. Acoustic transformations to im-
prove the intelligibility of dysarthric speech. in
proc. the Second Workshop on Speech and Language
Processing for Assistive Technologies.

F. Rudzicz. 2014. Adjusting dysarthric speech sig-
nals to be more intelligible. in Computer Speech and
Language, 27(6), September, pages 1163–1177.

Y. Stylianou, O. Cappe, and E. Moilines. 1998. Con-
tinuous probabilistic transform for voice conver-
sion. IEEE Trans. Speech and Audio Processing,
6(2):131–142.

R. Takashima, T. Takiguchi, and Y. Ariki. 2012.
Exemplar-based voice conversion in noisy environ-
ment. in SLT, pages 313–317.

T. Toda, Y. Ohtani, and K. Shikano. 2006. Eigenvoice
conversion based on Gaussian mixture model. in In-
terspeech, pages 2446–2449.

T. Toda, A. Black, and K. Tokuda. 2007. Voice con-
version based on maximum likelihood estimation of
spectral parameter trajectory. IEEE Trans. Audio,
Speech, Lang. Process., 15(8):2222–2235.

H. Valbret, E. Moulines, and J. P. Tubach. 1992. Voice
transformation using PSOLA technique. Speech
Communication, vol. 11, no. 2-3, pp. 175-187.

C. Veaux and X. Robet. 2011. Intonation conversion
from neutral to expressive speech. in Interspeech,
pages 2765–2768.

C. Veaux, J. Yamagishi, and S. King. 2012. Us-
ing HMM-based speech synthesis to reconstruct the
voice of individuals with degenerative speech disor-
ders. in Interspeech, pages 1–4.

J. Yamagishi, Christophe Veaux, Simon King, and
Steve Renals. 2013. Speech synthesis technolo-
gies for individuals with vocal disabilities: Voice
banking and reconstruction. Acoustical Science and
Technology, Vol. 33 (2012) No. 1, pages 1–5.

37


