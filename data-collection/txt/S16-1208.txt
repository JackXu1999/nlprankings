



















































TALN at SemEval-2016 Task 14: Semantic Taxonomy Enrichment Via Sense-Based Embeddings


Proceedings of SemEval-2016, pages 1332–1336,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

TALN at SemEval-2016 Task 14: Semantic Taxonomy Enrichment Via
Sense-Based Embeddings

Luis Espinosa-Anke, Horacio Saggion and Francesco Ronzano
TALN - Department of Information and Communication Technologies

Universitat Pompeu Fabra
Carrer Tànger 122-134, 08018 Barcelona (Spain)

{luis.espinosa, horacio.saggion, francesco.ronzano}@upf.edu

Abstract

This paper describes the participation of the
TALN team in SemEval-2016 Task 14: Se-
mantic Taxonomy Enrichment. The purpose
of the task is to find the best point of attach-
ment in WordNet for a set of Out of Vocabu-
lary (OOV) terms. These may come, to name a
few, from domain specific glossaries, slang or
typical jargon from Internet forums and cha-
trooms. Our contribution takes as input an
OOV term, its part of speech and its associ-
ated definition, and generates a set of WordNet
synset candidates derived from modelling the
term’s definition as a sense embedding repre-
sentation. We leverage a BabelNet-based vec-
tor space representation, which allows us to
map the algorithm’s prediction to WordNet.
Our approach is designed to be generic and
fitting to any domain, without exploiting, for
instance, HTML markup in source web pages.
Our system performs above the median of all
submitted systems, and rivals in performance
a powerful baseline based on extracting the
first word of the definition with the same part-
of-speech as the OOV term.

1 Introduction

Semantic knowledge, expressed in terms of concepts
and relations holding among them, is an essential
enabling component of NLP applications (Jurgens
and Pilehvar, 2015). One of the best known se-
mantic repository is WordNet (Miller et al., 1990), a
manually created semantic network with a coverage
of over 200k English senses and 155k word forms.
However, as knowledge domains advance and ex-
pand, novel terms beyond the coverage of Word-

Net are coined on a regular basis. Thus, the need
for automatic approaches which are able to gather
information from unstructured online sources and
structure them is in high demand. In this context,
WordNet has become the reference semantic net-
work in previous attempts to formalize novel knowl-
edge. The SemEval-2016 Task 14: Semantic Taxon-
omy Enrichment (Jurgens and Pilehvar, 2016) aims
at providing an experimental ground on the task of,
given an Out-of-Vocabulary (OOV) term, and an as-
sociated definition and part of speech, find the best
point of attachment in WordNet (its most similar
synset). This is a challenging problem because an
OOV term may be defined without any explicit men-
tion to its closest WordNet synset. For instance, for
the OOV term (from training data) “lectionary”, the
associated definition is “The book that contains all
the readings from the Scriptures for use in the cel-
ebration of the liturgy”, and the best point of at-
tachment is sacred text#n#01. However, if one was
to simply obtain the first head word of the defini-
tion and retrieve its first sense, the result would be
book#n#01, with a score of 0.125/1 according to the
official evaluation metric (see Section 4).

In this paper we describe our approach to such
task. We base our method on the intuition that un-
seen terminology may be understood in terms of
how terms are defined. Hence, we propose an al-
gorithm which, for each definition, performs part-
of-speech tagging and parsing, generates a set of
noun and verb phrases, and then takes advantage
of a vector space representation of word and phrase
senses for modelling the definition. Our algorithm
produces several WordNet attachment candidates as

1332



the modelling takes place, e.g. by obtaining the
definition’s centroid and mapping it to WordNet, or
by parsing the OOV term and searching for a map-
ping between its head and WordNet. For this task,
we submitted the three runs of our system with the
highest scores on the training data. Moreover, the
task organizers provided two baselines: One which
selected a random WordNet synset, and one which
extracted the first word in the definition with the
same part-of-speech, and assigned it its first sense in
WordNet (BaselineFirst). The experimental results
suggest that the baseline was a very strong competi-
tor, ranking way above the median of the participat-
ing systems. Our best run was 3 points below the
baseline and 5 points above the median according to
a metric designed to compute the distance between
two nodes in a hierarchical cluster (see Section 4).

The rest of this paper is structured as follows:
After presenting an overview of related work (Sec-
tion 2), we describe in detail the methodology fol-
lowed for our submission (Section 3), then we pro-
vide evaluation results as compared with baseline
systems and the participants’ median score, along
with a discussion of a few interesting cases. Fi-
nally, we outline directions of future work in a novel
and exciting task, which opens a very interesting re-
search problem to be tackled in the future.

2 Related Work

Expanding current semantic knowledge is a task that
may be approached either by expanding WordNet
with knowledge derived from structured or semi-
structured repositories, or by learning hyponym-
hypernym hierarchies separately in order to obtain
novel knowledge. Within the former group, we
find projects like BABELNET (Navigli and Ponzetto,
2012), a very large multilingual semantic network,
or pairwise lexical resources alignments (Miller and
Gurevych, 2014; Pilehvar and Navigli, 2014). These
approaches, however, are not designed to capture
novel terminology due to the fact that, for an align-
ment to occur, there must exist a direct correspon-
dence between lemmas. This is addressed in projects
belonging to the latter group, starting from (Snow
et al., 2006), and following more recently with ex-
traction of information from the web (Velardi et al.,
2013; Luu Anh et al., 2014) or from BABELNET’s

glosses (Espinosa-Anke et al., 2016). These ap-
proaches, however, do not perform a direct mapping
against WordNet, but rather separate taxonomies
with their own hierarchical structure.

3 Method

Our approach to finding the best match in WordNet
for an OOV term is based on transforming the as-
sociated textual definition into a representative vec-
tor. However, performing this action over plain text
would introduce an ambiguity problem due to the
occurrence of polysemic entities and specific lexi-
cal and syntactic formulations within the definition.
Therefore, we leveraged SENSEMBED (Iacobacci et
al., 2015), a state of the art vector space represen-
tation of word senses designed with BABELNET as
a reference sense inventory. In SENSEMBED, each
vector includes a concatenation of lemma and BA-
BELNET synset. We exploit the mapping BABEL-
NET→WordNet in order to find the best candidate
for the given OOV term.

Let t be an OOV term, and d its associated defini-
tion, our sense-finding algorithm aims at extracting a
suitable WordNet sense for t. To attain this goal, we
perform a set of steps which generate a candidate set
C, which we will afterwards rank according to cer-
tain criteria. The different candidate rankings that
we obtain constitute the three runs of our submis-
sion. Let us explain how each candidate is obtained.

Candidate Retrieval

First, we parse both t and d using the parser de-
scribed in (Bohnet, 2010). Note that t may be a
multiword term including prepositional phrases (e.g.
“margin of error”) and hence simply tagging and re-
trieving the last token would not suffice. We traverse
the parse trees obtained in both cases in a depth-first-
fashion until a lemma with a WordNet synset with
the same part-of-speech as t is found. This generates
two candidate WordNet synsets cwnt , and c

wn
d , which

correspond to the first sense of the matching lemma.
After completing this first step, C = {cwnt , cwnd }.

The process of retrieving these candidates is in-
spired by the baseline BaselineFirst provided by the
task organizers, which retrieved the first word in the
definition with the same part-of-speech as the novel
term. We evaluated a (non-submitted) baseline ver-

1333



sion of our system, which only considers cwnt and
cwnd . We obtained slightly worse performance than
the organizers’ baseline on both trial and training
data, mostly due to parsing errors.

The second step of the algorithm aims at find-
ing optimal candidate synsets for t by modeling d
in terms of its vector representation. We proceed as
follows. First, after stopword removal, we apply a
shallow parsing technique over part-of-speech tags
in order to isolate both single and multiword NPs
and VPs. Specifically, we group all textual chunks
that match the following regular expressions:

NP = <JJ|NN.*>+

VP = <VB.*><NP>?

This step results in dch, the set of noun and
verb phrases identified in a definition. We did not
consider multiword expressions including a prepo-
sitional phrase, as these introduced a consider-
able amount of noise. For example, for d =
“Genetic drift is a mechanism of evolution”, dch =
{genetic drift, drift, mechanism, evolution}. Then,
for each ch ∈ dch, we obtain, where possible, all its
available senses in SENSEMBED and store them in
two separate sets. The first one, denoted as DSenses,
contains all senses extracted from all ch ∈ dch with-
out distinguishing the original chunk from which
they come from. It is simply a set of senses. The sec-
ond set, denoted as DchSenses, isolates all available
senses corresponding to each chunk in an individual
set, and thus constitutes a set of sets.

We use these two sets for obtaining (1) A cen-
troid sense µ over the definition’s associated senses
DSenses, and (2) The set containing only the
best sense for each chunk. For the former case,
µ(DSenses) is obtained as follows:

µ =
1

|DSenses|
∑

s∈DSenses

s

‖s‖ (1)

As for the latter, best sense refers to the closest
sense to µ by cosine score obtained from the set of
senses retrieved for each chunk, namely dchSen ∈
DchSenses, which is computed as follows:

bestS(dchSen) = argmaxs∈dchSen
s · µ
‖s‖ ‖µ‖ (2)

The set of best senses per chunk is simply
DbestS = {bestS (dchSen) , dchSen ∈ DchSenses},
and will be used to generate the last candidate in our
process.

At this stage, we obtain three more candidates
to our candidate set, namely (1) The closest vec-
tor to the centroid from of all available senses in
SENSEMBED, µwn; (2) The sense bestS (dchSen)
appearing at the first position of the sentence, de-
noted as firstBest; and (3) The sense in DSenses
with highest cosine similarity with µ, denoted as
highestBest. In our sample sentence, µwn =
branching ratiobn

1, firstBest = driftsyn, and
highestBest = driftsyn.

Finally, we introduce one last additional candidate
to our candidate pool. We observed that µ may not
be an optimal centroid for the sentence as it is com-
puted by considering all possible senses for each
extracted chunk. For this reason, we take advan-
tage of the fact that we have an available set of best
senses for each chunk, namely DbestS , and create
a sense-aware centroid, denoted as µwnbest. Finally,
C = C ∪ {µwn, firstBest, highestBest, µwnbest}.

Ranking Candidates

Having obtained a set of candidate WordNet synsets
C for an OOV term t, we propose a ranking scheme
aimed at providing three possible outcomes for our
system. Note that we may not have all candidates
available from all the previously described steps, as
there may be BABELNET synsets without a direct
WordNet mapping, definitions may be very short
(even one word), or there may be no available can-
didate with the same part-of-speech as t’s head.

We submitted three runs of our system, which we
describe as follows:

• RunHeads The baseline BaselineFirst, pro-
vided by the organizers, performed well, i.e.
it found the exact correct attachment for al-
most half of the training instances. Based on
this observation, in this submission we prior-
itized those cases in which there was a direct
mapping between the first sense of the def-
inition root and a WordNet synset (cd). If

1We denote with subscript syn those WordNet senses ob-
tained via direct mapping from SENSEMBED, and with sub-
script bn those only found in BABELNET.

1334



this was not the case, we searched for the
mapping of t’s head (ct), as we found that
in many cases, when t was a multiword ex-
pression, an optimal point of attachment could
be found thanks to its head word. If none
of these attempts were successful, we opted
for selecting candidates in the following order:
µwnbest, µ

wn, firstBest, and highestBest.

• RunSenses This run first incorporated a voting
scheme based on candidates in C providing one
same answer. If this was the case across three
or more candidates, this was the synset selected
by the system. However, if agreement across
candidates was below three, priority was given
to candidates coming from the definition mod-
eling via SENSEMBED, and if no candidate was
found, the system searched for mappings in cwnd
and cwnt .

• RunHyps This run of our system was the most
conservative approach. From all possible can-
didates extracted either via SENSEMBED, or
from cd and ct, we computed the number of
hyponyms each candidate had in the WordNet
hierarchy. Then, our system selected as answer
the most general synset, i.e. the one with the
highest number of hyponyms. Our hypothesis
was that by including the most generic terms
possible (upper in the hierarchy) we would be
less likely to incur in wrong senses of highly
specific terms, where different senses may be
placed more sparsely than in more generic
ones.

4 Evaluation

Evaluation is performed over several criteria. First,
the distance between the selected point of attach-
ment and the gold standard is computed via the Wu
& Palmer similarity (W&P) (Wu and Palmer, 1994).
Second, a Recall measure (R), aimed at allowing
systems to not provide an answer in doubtful cases,
where the chances of an incorrect attachment would
be high. It is simply computed as the percentage of
items answered by the system. Finally, a score on
lemma match (LM) is provided, in order to cover
cases where the system identifies a correct lemma
but selects the wrong sense of the lemma.

4.1 System Performance

We observe in the results shown in Table 1 that the
three runs performed similarly, with only a few no-
ticeable differences which we comment as follows.
First, RunHeads and RunSenses seem to perform
similarly. This may be due to the fact that in some
cases no candidate was obtained from the sense-
based modeling and hence the system resorted to di-
rect matching from definition and term heads2. An-
other reason can be found in cases in which the def-
inition is semantically very compact, i.e. all its ex-
tracted chunks belong to a similar semantic field and
therefore the centroid vector we obtain is the same
as the definition head, and also the first synset of the
definition. This is the case, for instance, the term
“complex disease”, with definition “A complex dis-
ease is caused by the interaction of multiple genes
and environmental factors”. Here, the gold synset
was disease#n#01, and this answer was provided by
ct, firstBest, and µbest.

Another interesting discussion can be derived
from observing the difference between scores in LM
by the first two systems, on one hand, and RunHyps
on the other. While RunHyps ranks 2nd in our local
rank, its LM score is more than 10 points below its
closest competitor. This is due to the fact that, since
RunHyps is a conservative approach more aimed at
providing generic answers rather than matches, it
shows a reasonable performance in terms of W&P,
but falls short in LM as it almost never will find the
correct point of attachment at the top of WordNet’s
hierarchy.

In terms of comparative evaluation, all our sub-
mitted systems rank below the baseline Baseline-
First, which shows that a simpler approach based
on majority class (selecting the first WordNet synset
for a given lemma, which usually corresponds to
the most generic or widespread sense) is difficult to
beat. However, in comparison with the median of all
participating systems, all our submissions achieve
F1 scores between 4 and 5 points higher.

2In general we were reasonably satisfied with the coverage
of SENSEMBED, as with our approach, we found at least one
candidate BABELNET synset with WordNet mapping in the vast
majority of cases. For instance, our run RunHeads missed only
two nouns and four verbs. In these cases we set a default point
of attachment entity#n#01 and breathe#v#01 respectively.

1335



Subm W&P LM R F1

RunHeads 0.476 0.36 1 0.645

RunSenses 0.463 0.353 1 0.635

RunHyps 0.471 0.24 1 0.641

BaselineFirst 0.513 0.415 1 0.678

Median - - - 0.590

Table 1: Evaluation summary for our submission

5 Future Work

We computed an upper bound on the training data,
i.e. the score we would have achieved if, for each
case, we had selected the best of the candidates pro-
posed by our system. This artificial upper bound
reached a W&P score of almost 0.70, which sug-
gests that our approach has clear and well defined
room for improvement. We are currently investigat-
ing potential ways to address this issue in order to
surpass the baseline scores.

Acknowledgments

This work was partially funded by Dr. Inven-
tor (FP7-ICT-2013.8.1611383), and by the Span-
ish Ministry of Economy and Competitiveness un-
der the Maria de Maeztu Units of Excellence Pro-
gramme (MDM-2015-0502).

References

Bernd Bohnet. 2010. Very High Accuracy and Fast De-
pendency Parsing is not a Contradiction. In COLING,
pages 89–97.

Luis Espinosa-Anke, Horacio Saggion, Francesco Ron-
zano, and Roberto Navigli. 2016. ExTaSem! Extend-
ing, Taxonomizing and Semantifying Domain Termi-
nologies. In AAAI.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. SENSEMBED: Enhancing
Word Embeddings for Semantic Similarity and Relat-
edness. In Proceedings of ACL, Beijing, China, July.
Association for Computational Linguistics.

David Jurgens and Mohammad Taher Pilehvar. 2015.
Reserating the awesometastic: An automatic extension
of the WordNet taxonomy for novel terms. In Pro-
ceedings of the 2015 conference of the North Ameri-

can chapter of the association for computational lin-
guistics: Human language technologies, Denver, CO,
pages 1459–1465.

David Jurgens and Mohammad Taher Pilehvar. 2016.
Semeval-2016 Task 14: Semantic Taxonomy Enrich-
ment. In Semeval 2016.

Tuan Luu Anh, Jung-jae Kim, and See Kiong Ng. 2014.
Taxonomy Construction Using Syntactic Contextual
Evidence. In EMNLP, pages 810–819, October.

Tristan Miller and Iryna Gurevych. 2014. WordNet -
Wikipedia - Wiktionary: Construction of a Three-Way
Alignment. In LREC, pages 2094–2100.

George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet:
an Online Lexical Database. International Journal of
Lexicography, 3(4):235–244.

Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The Automatic Construction, Evaluation and
Application of a Wide-Coverage Multilingual Seman-
tic Network. Artificial Intelligence, 193:217–250.

Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A Robust Approach to Aligning Heterogeneous Lex-
ical Resources. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics, ACL 2014, June 22-27, 2014, Baltimore, MD,
USA, Volume 1: Long Papers, pages 468–478.

Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of COLING/ACL 2006, pages
801–808. Association for Computational Linguistics.

Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. Ontolearn Reloaded: A graph-Based Algorithm
for Taxonomy Induction. Computational Linguistics,
39(3):665–707.

Zhibiao Wu and Martha Palmer. 1994. Verbs Semantics
and Lexical Selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 133–138. Association for Computa-
tional Linguistics.

1336


