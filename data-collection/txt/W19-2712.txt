



















































RST-Tace A tool for automatic comparison and evaluation of RST trees


Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 88–96
Minneapolis, MN, June 6, 2019. c©2019 Association for Computational Linguistics

88

RST-Tace
A tool for automatic comparison and evaluation of RST trees

Shujun Wan
Humboldt University of Berlin

Berlin, Germany
shujun.wan@hu-berlin.de

Tino Kutschbach
Independent Researcher

Berlin, Germany
tino.kutschbach@mailbox.org

Anke Lüdeling
Humboldt University of Berlin

Berlin, Germany
anke.luedeling@hu-berlin.de

Manfred Stede
University of Potsdam

Potsdam, Germany
stede@uni-potsdam.de

Abstract

This paper presents RST-Tace, a tool for au-
tomatic comparison and evaluation of RST
trees. RST-Tace serves as an implementation
of Iruskieta’s comparison method, which al-
lows trees to be compared and evaluated with-
out the influence of decisions at lower levels
in a tree in terms of four factors: constituent,
attachment point, nuclearity as well as rela-
tion. RST-Tace can be used regardless of the
language or the size of rhetorical trees. This
tool aims to measure the agreement between
two annotators. The result is reflected by F-
measure and inter-annotator agreement. Both
the comparison table and the result of the eval-
uation can be obtained automatically.

1 Introduction

Rhetorical Structure Theory (Mann and Thomp-
son, 1988) is intended to describe discourse struc-
ture and text organization by labeling the dis-
course relations that hold between elementary dis-
course units (EDU) or between larger spans of
text. It is widely used throughout the discourse
community as a theory for discourse analysis.
RST is defined as the reconstruction of the au-
thor’s plan from the perspective of the reader
(Stede, 2017), that is to say it implies a certain
subjectivity. According to this view, different an-
notators might very well produce different analy-
ses, which can nonetheless be equally legitimate
(Das et al., 2017).

However, differences in the analysis based on
the legitimate scope of explication ought to be dis-
tinguished from unexpected errors or ambiguities
resulting from unclear annotation guidelines. In

order to assess and ensure the accuracy and reli-
ability of the annotation, it is crucial to measure
the agreement between the annotators. Compared
with other types of annotation, evaluating rhetor-
ical structures and calculating the inter-annotator
agreement are not trivial. There are several chal-
lenges: 1) RST tree parsing, 2) finding an appro-
priate method for comparison and evaluation, 3)
applying this method efficiently.

So far, Marcu’s (2000) method for the com-
parison of RST annotations by several annota-
tors is widely-used. Building on Marcu’s method,
Maziero and Pardo (2009) developed RSTeval in
order to obtain the results of comparison automat-
ically. While being widely used, the method has
also been criticized. For instance, da Cunha and
Iruskieta (2010) argue that it amalgamates agree-
ment coming from different sources, with the re-
sult that decisions at lower levels in the tree sig-
nificantly affect agreement at the upper rhetorical
relations in a tree (Iruskieta et al., 2015), and re-
lations cannot be able to be compared where con-
stituents do not coincide.

In this regard, Iruskieta et al. (2015) pro-
posed an evaluation method which accepts that
constituents do not need to coincide in their en-
tirety to be compared. Iruskieta’s method provides
a qualitative description of dispersion annotation
while allowing quantitative evaluation (details are
introduced in section 2). Nevertheless, using this
method to evaluate discourse structures manu-
ally is an extremely time- and resource-consuming
task. Thus, inspired by RSTeval, we have devel-
oped RST-Tace as a tool for automatic comparison
and evaluation of RST trees based on Iruskieta’s
method.



89

Example CS1 CS2
[1] 1 1
[2] 23 23|24
[3] 17-18|26 17-18|26
[4] 10|15 11|15

Table 1: Examples of matching central subconstiutents
(extracted from CMN_008, RST German Learner Tree-
bank8)

This research paper focuses on the theoretical
foundations of RST-Tace as well as the implemen-
tation process. In addition, an example of using
RST-Tace to compare and evaluate rhetorical trees
(extracted from a self-built RST treebank) by two
linguists will be presented in the final section.

2 Theoretical Framework

According to Iruskieta’s method, the correspon-
dence of constituents is not a necessary condition
for comparison. Only the central subconstituent
(CS)1 which indicates the most important unit of
the satellite span, has to be identical. With this re-
striction, discourse structures are compared using
four independent factors:

• Constituent (C): the unit(s) where the satellite
(or one of the nuclei in case of multinuclear
relations) is located.

• Attachment point (A): the unit(s) where the
constituent is linked.

• Nuclearity (N): the direction of the relation.

• Relation (R): the name of relations.

In order to use Iruskieta’s method, each RST
tree must first be converted into a table which con-
sists of the four above factors as well as the cen-
tral subconstituent. Subsequently, pairs should be
matched according to the central subconstituent.
The third stage is evaluation. According to Iruski-
eta’s method, both agreement and disagreement
are considered. Lastly, the result of the evaluation
(F-measure) is calculated.

1According to Iruskieta (2015), there is an agreement that
the most important unit of an RST tree is the "central unit(s)"
(Stede, 2008) and the most important unit of a span is the
"central subconstituent" (Egg and Redeker, 2010). Following
this framework, Iruskieta et al. use the term "Central Sub-
constituent(s)" of a relation for the most important unit of the
modifier span that is the most important unit of the satellite
span.

(a) Annotator I (b) Annotator II

Figure 1: Annotations from two annotators (extracted
from DEU_006, RST German Learner Treebank2)

Anno CS R N C A
I 29 cause → 29S3 30N4
II 30 elaboration ← 30S 29N

Table 2: Matching table of Figure 1

Modifications
In light of the basic principles of Iruskieta’s
method, we highlight, in this part, some points
which are crucial for the use of our tool or are
slightly modified by us:

1. The use of this method for comparison and
evaluation takes the harmonization of dis-
course segmentation as a given.

2. As a general rule, CS has to be the same so
that the relations are able to be compared (see
example [1] in Table 1 ). A case complicat-
ing the comparison occurs when multinuclear
relations become involved. When there is a
multinuclear relation, all of its constituents
must be described as CS. Consequently, a
multinuclear relation has more than one CS.
Under such a circumstance, when a relation
with more than one CS is able to be com-
pared with another that has only one CS, at
least one of the CSs has to be identical (see
example [2] in Table 1). When two multinu-
clear relations are to be compared, their CSs
do not have to remain the same entirely (see
example [3] in Table 1). Similarly, they need
to possess at least one identical CS (see ex-
ample [4] in Table 1).

3. The association of CS is the prerequisite for
2Since we aim to show the tree structures rather than the

linguistic decision, we decided not to translate the language
of RST trees into English in this paper.

3S represents Satellite
4N refers to Nucleus



90

(a) Annotator I (b) Annotator II

Figure 2: Annotations from two annotators (Extracted
from DEU_006, RST German Learner Treebank)

Anno CS R N C A
I 11-12 elaboration ← 11-12S 10N
II 12 elaboration ← 11-12S 10N

Table 3: Matching table of Figure 2

comparison of relations according to Iruski-
eta’s method. However, there are two cases
that still deserve to be compared, even though
the CSs are not identical.

The RST tree of the example in Figure 1(a)
is quite similar to the RST tree of 1(b). Apart
from the names of relations which are coin-
cidentally not the same in this example, the
main reason why the two trees have differ-
ent CSs, constituents and attachment points,
is that they differ merely in nuclearity. How-
ever, due to the discrepancy in CS, this pair
cannot be detected using Iruskieta’s method.
From Table 2 which is converted from the
RST trees, we can observe that the CS of Fig-
ure 1(a) is 29 whereas the CS of Figure 1(b) is
30; yet, C1 equals A2 as well as C2 equaling
A1.

The other case in which relations are still
associated despite distinct CSs is when C1
equals C2 and A1 equals A2. The relation
10 and 11-12 of Figure 2(b) is not able to
be compared with the one of Figure 2(a), be-
cause they have different CSs (see Table 3).
This discrepancy stems from the micro level,
i.e. the relation between EDU 10 to EDU 11.

In brief, we match relations following the de-
cision tree below (see Figure 3).

4. Iruskieta’s method is originally designed for
comparing and evaluating discourse struc-
tures in different languages and/or by dif-
ferent annotators. Hence, in the case of
disagreement, two sources of disagreement
are distinguished: type A and type L for

Figure 3: Decision tree for matching

annotator-based discrepancies and language-
based discrepancies respectively5. In com-
parison to Iruskieta’s method, we focus more
on agreement instead of disagreement, be-
cause we aim to compare and evaluate anno-
tations which are in the same language but
annotated by different annotators. To check
agreement in rhetorical relation, the con-
stituent of this relation must have the same
central subconstituent. If this condition is ful-
filled, relation (R), constituent (C) and attach-
ment point (A) will be further checked.

5. Concerning the results of evaluation, both F-
measure and inter-annotator agreement for
each factor are calculated. The agreement
score for the whole RST trees is the mean
value of the four factors.

3 Implementation

For evaluation, comparison, and analysis of RST
trees, RST-Tace supports three main use cases:

1. Analysis of a single RST tree (i.e. extraction
and listing of all annotated relations), see sec-
tion 3.3.

2. Comparison of the annotated relations of a
pair of RST trees, see section 3.4.

3. Statistical evaluation of a whole dataset.

These use cases depend on each other, e.g. in order
to compare the annotated relations of a pair of RST
trees (second use case), the annotated relations of
both trees are first extracted and listed (first use
case).

5For more details and examples regarding the evaluation
of disagreement, refer to (Iruskieta et al., 2015), p.15-20.



91

3.1 Commandline Interface

In its current state, RST-Tace can be used via a
commandline interface (command: rsttace), and
each of the main use cases has its own command
(analyse / compare / evaluate):

$ r s t t a c e
Usage : r s t t a c e [ OPTIONS ] COMMAND [ARGS ] . . .

Th i s t o o l a n a l y s e s , compares , and e v a l u a t e s RST t r e e s .

O p t i o n s :
−−h e l p Show t h i s message and e x i t .

Commands :
a n a l y s e P a r s e s i n g l e RST t r e e s , a n a l y s e

and l i s t t h e i r a n n o t a t e d r e l a t i o n s .
compare P a r s e RST t r e e p a i r s and compare

them wi th each o t h e r .
e v a l u a t e Per fo rm a s t a t i s t i c a l e v a l u a t i o n

o f a s e t o f RST t r e e p a i r s .

The results are either written back to the com-
mandline, or to csv-files if needed. For exam-
ple, the command for comparing two rs3-files and
writing the result to a CSV-file is:

$ r s t t a c e compare f i l e 1 . r s 3 f i l e 2 . r s 3 \
−o r e s u l t . c sv

As the tool is currently under active develop-
ment the user interface may still be subject to
change. For up-to-date information and complete
documentation, please refer to the GitHub reposi-
tory of RST-Tace.6

Before we discuss how each of the three tasks is
performed in detail, the parsing process of rs3-files
will be described.

3.2 RST Tree Parsing

After a set of texts has been annotated with tools
such as RSTTool (O’Donnell, 1997) and rstWeb
(Zeldes, 2016), the resulting RST trees are typ-
ically exported as *.rs3-files. In order to work
with the RST trees efficiently, these files have to
be parsed and converted into an internal tree based
data structure, which allows convenient data ac-
cess for the desired evaluation task.

File Format
RSTTool and rstWeb both use the same file format
for exported rs3-files with only minor differences.
The file parser of RST-Tace is designed to handle
the rs3-files of both tools.

The file format is based on XML and contains a
header and a body. Figure 4 shows an example of a
RST tree annotated with RSTTool. The header de-
fines which relations are used in the RST tree and

6https://github.com/tkutschbach/
RST-Tace

< r s t >
< h e a d e r >

< r e l a t i o n s >
< r e l name=" background " t y p e =" r s t " / >
< r e l name=" a n t i t h e s i s " t y p e =" r s t " / >
. . .
< r e l name=" r e a s o n " t y p e =" r s t " / >
< r e l name=" summary " t y p e =" r s t " / >
. . .
< r e l name=" s e q u e n c e " t y p e =" m u l t i n u c " / >
< r e l name=" j o i n t " t y p e =" m u l t i n u c " / >
< r e l name=" c o n t r a s t " t y p e =" m u l t i n u c " / >
< r e l name=" l i s t " t y p e =" m u l t i n u c " / >

< / r e l a t i o n s >
< / h e a d e r >
<body>

<segment i d =" 1 " p a r e n t =" 45 " re lname =" span ">Man muss vor . . .
<segment i d =" 2 " p a r e n t =" 1 " re lname =" r e a s o n ">da d i e e r s t . . .
<segment i d =" 4 " p a r e n t =" 5 " re lname =" background ">Nehmen wir .
. . .
<segment i d =" 8 " p a r e n t =" 7 " re lname =" c o n d i t i o n ">wenn s i e . . .
<segment i d =" 46 " p a r e n t =" 68 " re lname =" span ">Wenn man es . . .
<segment i d =" 9 " p a r e n t =" 69 " re lname =" c o n t r a s t ">Die K r i e g s . .
<segment i d =" 10 " p a r e n t =" 11 " re lname =" span "> B l i c k e n wi r . . .
<segment i d =" 13 " p a r e n t =" 51 " re lname =" c o n j u n c t i o n ">Es ex . . .
<segment i d =" 12 " p a r e n t =" 51 " re lname =" c o n j u n c t i o n ">und d i e .
. . .
<group i d =" 3 " t y p e =" span " p a r e n t =" 49 " re lname =" c o n t r a s t " / >
<group i d =" 47 " t y p e =" span " p a r e n t =" 6 " re lname =" e v i d e n c e " / >
<group i d =" 48 " t y p e =" span " p a r e n t =" 49 " re lname =" c o n t r a s t " / >
. . .
<group i d =" 75 " t y p e =" span " p a r e n t =" 76 " re lname =" span " / >
<group i d =" 76 " t y p e =" span " / >
. . .

< / body>
< / r s t >

Figure 4: Example of an rs3-file, encoding an RST tree

their corresponding type, i.e. either mono- or mult-
inuclear. The body represents the actual RST tree.
An rs3-file consists of a list of XML elements (ei-
ther segments and groups), each of which has the
following attributes: Node ID, parent ID, relation
name. Elements of the type segment correspond
to the EDUs and contain the corresponding text
of the EDU. Group elements have an additional
attribute type, encoding whether the element is a
span or corresponds to a multinuclear relation.

As shown in Figures 1(a), 1(b), and 2(a), 2(b):
each leaf node of an RST tree has an EDU-ID, and
higher level nodes have ranges of EDU-IDs which
depend on the EDU-IDs of their child nodes.

In the XML file format, the segment elements
correspond to the EDUs; their order inside the
body corresponds the order of occurence of the
EDUs in the original text. For example, the first
segment element in the body corresponds to the
first EDU (i.e. having the EDU-ID “1”), the third
segment element in the body corresponds to the
third EDU (i.e. having the EDU-ID “3”), and so
on.

For the case of rs3-files from RSTTool, it is im-
portant to note that the IDs encoded in the segment
elements do not necessariliy correspond to the de-
sired EDU-IDs, as shown in Figure 47.

7In the example in Figure 4, the third segment has the ID

https://github.com/tkutschbach/RST-Tace
https://github.com/tkutschbach/RST-Tace


92

Because the proposed evaluation method of
RST trees relies on correct EDU-IDs, the parser
of RST-Tace infers the EDU-ID of each segment
itself based on its position inside the list. The IDs
of the segment and group elements are then used
to reconstruct the tree structure. Afterwards, the
ranges of EDU-IDs that higher level nodes span
(i.e. represented by the group elements) can be in-
ferred.

Internal Data Representation
As mentioned above, the data arrangement of rs3-
files does not precisely represent an RST tree
structure, which would allow convenient data ac-
cess for the proposed evaluation method. As a
remedy, the given data is parsed and converted into
an internal tree representation, which consists of
nodes and three different types of edges.

The three different types of edges are:

1. Horizontal edges, connecting two nodes on
the same level: Encoding mononuclear rela-
tions.

2. Vertical edges, connecting one parent node
with multiple child nodes one level below:
Encoding multinuclear relations.

3. Vertical edges, connecting one parent node
with one child node one level below: Encod-
ing spans.

The tree nodes correspond to the segment and
group elements in the rs3-file format. Each node
is connected to a parent node via an edge and can
have one horizontal edge connecting it to another
node on the same level. Furthermore, it can have
one vertical edge, connecting it to one or several
child nodes on the next lower level. Additionally,
the nodes encode their corresponding EDU ID or
ID-range. If a node is a leaf node, then it also
contains the text information of its corresponding
EDU.

Parsing Process
The encoding step is implemented straightfor-
wardly by first reading all XML elements and
searching the element which corresponds to the
root node of the RST tree (which is character-
ized by a missing parent ID, e.g. the element with
id = 76 in Figure 4). Afterwards, for all elements

“4” instead of “3”, and the on the segment with ID “8” follows
the segment with ID “46“, which itself is followed by the
segment with ID “9”.

that refer to this root as parent, nodes and edges are
generated and connected depending on the type of
their relation. In the next step, all elements that re-
fer to those nodes are processed respectively. This
process is repeated until all elements have been ap-
pended as nodes to the internal data structure.

Finally, after the complete tree has been built,
the EDU ID-ranges of the higher level nodes have
to be inferred based on their corresponding lower
level children, because only the EDU IDs of the
leaf nodes can be directly extracted from the rs3-
file. This inference is done by iterating over the
whole tree bottom-up, i.e. from the leaf nodes to-
wards the root node, and gradually augmenting the
higher nodes level-by-level until the root node is
reached.

3.3 Extraction of Annotated Relations
In order to compare and evaluate an RST tree us-
ing Iruskieta’s method, its annotated relations have
to be extracted and listed together with additional
information (e.g. constituent, nuclearity).

Once the RST tree is available in the form of
the previously described data structure, the extrac-
tion of relations becomes a simple task of iterat-
ing over the set of edges in the tree and listing
their corresponding relations. Also, the additional
information is directly accessible: The nuclearity
corresponds to the direction of an edge, and the
other information such as constituent, attachment-
point and central sub-constituent can be directly
acquired from EDU IDs and ID-ranges encoded in
the nodes that each edge is connected to.

3.4 Comparison of RST Tree Pairs
An important task in RST research is comparing
different RST annotations (of different annotators)
for a single text or a set of texts. Under the condi-
tion that two annotations of a text are based on the
same segmentation of EDUs, RST-Tace can com-
pare the two different RST trees and calculate an
equivalence score.

In order to compare the RST annotations of two
trees using Iruskieta’s method, the annotation lists
of both RST trees are generated first, as described
in section 3.3. Afterwards, the annotated relations
of both RST trees have to be associated to each
other.

As mentioned above, two different annotators
might create RST trees with different structures
for the same text; thus, it is not always clear
which annotated relation in one tree corresponds



93

Equivalence Cost
Same CS 0

C1 = C2 and A1 = A2 1
C1 = A2 and C2 = A1 2
At least one identical CS 3

No matching 4

Table 4: Cost values used for matching annotated rela-
tions of two RST trees

to which one in the other. This ambiguity means
that the association is not a trivial task.

Optimal Association
RST-Tace deals with this ambiguity by searching
for an optimal association. For this, each anno-
tated relation of the first RST tree is compared to
each annotated relation of the second RST tree by
the scheme introduced previously and shown in
Figure 3. Because each of the possible matching
outcomes stands for a different degree of equiv-
alence, they can be prioritized by assigning cost
values to each of them (low cost values for high
priorities, high cost values for low priorities). The
cost values used in this work are shown in Table 4.

While comparing all annotated relations of both
RST trees with each other, these cost values are
used to populate a cost matrix C. With N being
the number of relations annotated in the first RST
tree and M being the number of relations anno-
tated in the second tree, the matrix C has the form
N ×M . An element Ci,j represents the cost of
matching relation i in the first tree with relation j
of the second tree.

The optimal association is then calculated by
applying the Hungarian algorithm (Kuhn, 1955),
also known as Kuhn-Munkres algorithm, to this
cost matrix. Matches are categorized as com-
pletely identical CS, C1=C2 and A1=A2, C1=A2
and C2=A1, partially identical CS as well as no
matching.

Evaluation and Results
After the annotations of both RST trees have been
associated, all annotation pairs are compared ac-
cording to Iruskieta’s method, i.e. their nucleari-
ties (N), relations (R), constituents (C), and attach-
ment points (A) are compared and marked as equal
or non-equal. These values are then used to calcu-
late F-measure and inter-annotator agreement.

RST-Tace also offers the possiblity to process
a whole batch of RST tree pairs and calculate the

equivalence scores and inter-annotator agreement
over a whole dataset.

4 An Example of Comparison and
Evaluation using RST-Tace

In this section, we provide an example of using
RST-Tace to compare and evaluate RST trees. Ex-
tracted from RST German Learner Treebank8, two
annotations 9 on the same German text by two lin-
guists are compared and evaluated. A part of the
two RST trees where the annotations are different
is shown in Figure 5; the comparison table and the
results of evaluation are presented in Figure 6.

5 Summary

To conclude, RST-Tace allows comparison and
evaluation of RST trees by different annotators au-
tomatically. It can be used for rhetorical structures
in any language as well as with any size. The
modifications that are made based on Iruskieta’s
method provide a further perspective of RST re-
lated theories. Currently, the statistical part of the
implementation, i.e. the automatic calculation of
F-measure and inter-annotator agreement, is un-
der active development. In the future, additional
features could be added to RST-Tace, for instance,
a user-friendly interface, or a more sophisticated
statistical analysis of larger datasets and RST tree-
banks.

Acknowledgments

This work was financially supported by the China
Scholarship Council. We wish to express our ap-
preciation to Felix Golcher from IDSL, Humboldt
University of Berlin, who provided his statistical
expertise that greatly assisted the research, and to
Shuyuan Cao from Polytechnic University of Cat-
alonia for his theoretical support. We would like
to thank Matthew Plews and Dawn Nichols for
proofreading this paper. We also owe our special
thanks to the anonymous reviewers for their valu-
able comments.

8RST German Learner Treebank consists of 40 RST trees.
The texts in the treebank are argumentative essays (around
25,000 tokens in total) extracted from Kobalt-DaF Corpus
(Zinsmeister et al., 2012). All the data was annotated by two
professional linguists according to the guidelines of the Pots-
dam Commentary Corpus (Stede, 2016), which was designed
for German pro-contra essays. The quantity of EDUs of each
text is between 40-80.

9Segmentation has been harmonized before annotating.



94

Fi
gu

re
5:

R
ST

tr
ee

s
fr

om
tw

o
an

no
ta

to
rs

.D
E

U
_0

06
,R

ST
G

er
m

an
Le

ar
ne

r
Tr

ee
ba

nk



95

ID
C

S-
A

R
el

at
io

n-
A

N
uc

-A
C

1-
A

C
2-

A
C

N
-A

A
1-

A
A

2-
A

A
N

-A
C

S-
B

R
el

at
io

n-
B

N
uc

-B
C

1-
B

C
2-

B
C

N
-B

A
1-

B
A

2-
B

A
N

-B
M

at
ch

in
g

N
R

C
A

1
1

pr
ep

ar
at

io
n

→
1

2
S

3
31

N
1

pr
ep

ar
at

io
n

→
1

2
S

3
31

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
2

3-
31

ev
id

en
ce

→
1

31
S

32
37

N
3-

31
ev

id
en

ce
→

1
31

S
32

39
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
×

3
32

-3
7|

38
-4

1
jo

in
t

↔
1

37
N

38
41

N
N

o
m

at
ch

in
g

×
×

×
×

4
40

-4
1

ev
al

ua
tio

n-
s

←
40

41
S

1
39

N
N

o
m

at
ch

in
g

×
×

×
×

5
2

re
as

on
←

2
2

S
1

1
N

2
re

as
on

←
2

2
S

1
1

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
6

3
ba

ck
gr

ou
nd

→
3

3
S

4
4

N
3

ba
ck

gr
ou

nd
→

3
3

S
4

4
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
X

7
3-

7
ev

id
en

ce
→

3
7

S
8

8
N

3-
7

ev
id

en
ce

→
3

7
S

8
8

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
8

8|
9

co
nt

ra
st

↔
3

8
N

9
9

N
8|

9
co

nt
ra

st
↔

3
8

N
9

9
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
X

9
4|

5
co

nt
ra

st
↔

3
4

N
5

7
N

4|
5

co
nt

ra
st

↔
3

4
N

5
7

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
10

3-
9|

10
-2

4|
25

lis
t

↔
3

9
N

10
31

N
3-

9|
10

|1
3|

25
lis

t
↔

3
9

N
10

31
N

C
1=

C
2

an
d

A
1=

A
2

X
X

X
X

11
6

ev
id

en
ce

←
6

7
S

5
5

N
6

ev
id

en
ce

←
6

7
S

5
5

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
12

7
ci

rc
um

st
an

ce
←

7
7

S
6

6
N

7
co

nd
iti

on
←

7
7

S
6

6
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
×

X
X

13
10

-2
4|

25
lis

t
↔

10
24

N
25

31
N

10
|1

3|
25

lis
t

↔
10

12
N

13
31

N
Pa

rt
ia

lly
id

en
tic

al
C

S
X

X
×

×
14

10
|1

3
co

nt
ra

st
↔

10
12

N
13

24
N

13
|2

5
lis

t
↔

13
24

N
25

31
N

Pa
rt

ia
lly

id
en

tic
al

C
S

X
×

×
×

15
11

ca
us

e
→

11
11

S
12

12
N

11
|1

2
co

nj
un

ct
io

n
↔

11
11

N
12

12
N

C
1=

C
2

an
d

A
1=

A
2

×
×

×
X

16
12

el
ab

or
at

io
n

←
11

12
S

10
10

N
11

-1
2

el
ab

or
at

io
n

←
11

12
S

10
10

N
C

1=
C

2
an

d
A

1=
A

2
X

X
X

X
17

14
-2

4
ev

id
en

ce
←

14
24

S
13

13
N

14
-2

4
ev

id
en

ce
←

14
24

S
13

13
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
X

18
14

|1
6-

17
|1

9-
20

lis
t

↔
14

15
N

16
24

N
14

|1
6-

17
|1

9-
20

|2
2-

24
lis

t
↔

14
15

N
16

24
N

C
1=

C
2

an
d

A
1=

A
2

X
X

X
X

19
15

el
ab

or
at

io
n

←
15

15
S

14
14

N
15

el
ab

or
at

io
n

←
15

15
S

14
14

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
20

16
-1

7|
19

-2
0

lis
t

↔
16

18
N

19
24

N
16

-1
7|

19
-2

0|
22

-2
4

lis
t

↔
16

18
N

19
24

N
C

1=
C

2
an

d
A

1=
A

2
X

X
X

X
21

17
co

nc
es

si
on

←
17

17
S

16
16

N
17

co
nc

es
si

on
←

17
17

S
16

16
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
X

22
18

ev
al

ua
tio

n-
s

←
18

18
S

16
17

N
18

ev
al

ua
tio

n-
s

←
18

18
S

16
17

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
23

20
pu

rp
os

e
←

20
20

S
19

19
N

19
|2

0
se

qu
en

ce
↔

19
19

N
20

20
N

C
1=

A
2

an
d

A
1=

C
2

×
×

×
×

24
21

co
nc

es
si

on
→

21
21

S
22

24
N

21
co

nc
es

si
on

→
21

21
S

22
24

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
25

22
-2

4
e-

el
ab

or
at

io
n

←
21

24
S

19
20

N
19

-2
0|

22
-2

4
lis

t
↔

19
20

N
21

24
N

C
1=

A
2

an
d

A
1=

C
2

×
×

×
×

26
23

-2
4

ev
id

en
ce

←
23

24
S

22
22

N
23

ev
id

en
ce

←
23

24
S

22
22

N
C

1=
C

2
an

d
A

1=
A

2
X

X
X

X
27

23
|2

4
co

nt
ra

st
↔

23
23

N
24

24
N

24
el

ab
or

at
io

n
←

24
24

S
23

23
N

C
1=

A
2

an
d

A
1=

C
2

×
×

×
×

28
26

-3
1

ev
id

en
ce

←
26

31
S

25
25

N
26

-3
1

ev
id

en
ce

←
26

31
S

25
25

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
29

26
|2

7|
28

|3
0|

31
lis

t
↔

26
26

N
27

31
N

26
|2

7|
28

-3
0|

31
lis

t
↔

26
26

N
27

31
N

C
1=

C
2

an
d

A
1=

A
2

X
X

X
X

30
27

|2
8|

30
|3

1
lis

t
↔

27
27

N
28

31
N

27
|2

8-
30

|3
1

lis
t

↔
27

27
N

28
31

N
C

1=
C

2
an

d
A

1=
A

2
X

X
X

X
31

28
|3

0|
31

lis
t

↔
28

28
N

29
31

N
28

|2
9

lis
t

↔
28

28
N

29
30

N
Pa

rt
ia

lly
id

en
tic

al
C

S
X

X
X

×
32

29
in

te
rp

re
ta

tio
n

→
29

29
S

30
30

N
30

el
ab

or
at

io
n

←
30

30
S

29
29

N
C

1=
A

2
an

d
A

1=
C

2
×

×
×

×
33

30
|3

1
lis

t
↔

29
30

N
31

31
N

28
-3

0|
31

lis
t

↔
28

30
N

31
31

N
Pa

rt
ia

lly
id

en
tic

al
C

S
X

X
×

X
34

32
co

nc
es

si
on

→
32

32
S

33
35

N
32

co
nc

es
si

on
→

32
32

S
33

35
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
X

35
33

-3
5

an
tit

he
si

s
→

32
35

S
36

36
N

33
-3

5
an

tit
he

si
s

→
32

35
S

36
39

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

×
36

36
re

as
on

→
32

36
S

37
37

N
36

re
as

on
→

36
36

S
37

37
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

×
X

37
34

ci
rc

um
st

an
ce

←
34

34
S

33
33

N
34

ci
rc

um
st

an
ce

←
34

34
S

33
33

N
C

om
pl

et
el

y
id

en
tic

al
C

S
X

X
X

X
38

35
ev

id
en

ce
←

35
35

S
33

34
N

35
ev

id
en

ce
←

35
35

S
33

34
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
X

39
38

ci
rc

um
st

an
ce

→
38

38
S

39
39

N
38

ca
us

e
→

38
38

S
39

39
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
×

X
X

40
39

|4
0-

41
jo

in
t

↔
38

39
N

40
41

N
39

el
ab

or
at

io
n

←
38

39
S

36
37

N
Pa

rt
ia

lly
id

en
tic

al
C

S
×

×
×

×
41

40
|4

1
lis

t
↔

40
40

N
41

41
N

40
|4

1
lis

t
↔

40
40

N
41

41
N

C
om

pl
et

el
y

id
en

tic
al

C
S

X
X

X
X

R
es

ul
ts

N
uc

le
ar

ity
R

el
at

io
n

C
on

st
itu

en
t

A
tta

ch
m

en
tp

oi
nt

R
ST

tr
ee

s

F-
M

ea
su

re
:

(3
3

of
40

)→
0.

80
4

(3
0

of
41

)→
0.

73
1

(2
9

of
41

)→
0.

70
7

(2
9

of
41

)→
0.

70
7

0.
73

7
In

te
r-

A
nn

ot
at

or
A

gr
ee

m
en

t:
0.

71
0.

69
8

0.
57

0.
59

0.
56

7

Fi
gu

re
6:

C
om

pa
ri

so
n

ta
bl

e
an

d
th

e
re

su
lts

fo
rt

ex
tD

E
U

_0
06

,R
ST

G
er

m
an

Le
ar

ne
r

Tr
ee

ba
nk

,u
si

ng
R

ST
-T

A
C

E



96

References
Iria da Cunha and Mikel Iruskieta. 2010. Comparing

rhetorical structures in different languages: The in-
fluence of translation strategies. Discourse Studies,
12(5):563–598.

Debopam Das, Maite Taboada, and Manfred Stede.
2017. The Good , the Bad , and the Disagreement
: Complex ground truth in rhetorical structure anal-
ysis. In In workshop on Recent Advances in RST
and Related Formalisms, Santiago de Compostela,
Spain.

Markus Egg and Gisela Redeker. 2010. How Complex
is Discourse Structure ? How Complex is Discourse
Structure ? (May 2014).

Mikel Iruskieta, Iria da Cunha, and Maite Taboada.
2015. A qualitative comparison method for rhetori-
cal structures: identifying different discourse struc-
tures in multilingual corpora. Language Resources
and Evaluation, 49(2):263–309.

H. W. Kuhn. 1955. The hungarian method for the as-
signment problem. Naval Research Logistics Quar-
terly, 2(1-2):83–97.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243–281.

Daniel Marcu. 2000. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-based Approach. Compu-
tational Linguistics, 26(3):395–448.

E Maziero and Thiago AS Pardo. 2009. Metodologia
de avaliação automática de estruturas retóricas. In
Proceedings of the III RST Meeting (7th Brazilian
Symposium in Information and Human Language
Technology), Brasil.

Michael O’Donnell. 1997. RST-Tool: An RST analysis
tool. Proceedings of the 6th European Workshop on
Natural Language Generation, (March).

Manfred Stede. 2008. RST Revisited : Disentangling
Nuclearity. In Benjamins, editor, ‘Subordination’
versus ‘coordination’ in sentence and text – from a
cross-linguistic perspective. Amsterdam.

Manfred Stede. 2016. Handbuch Textannotation. Pots-
damer Kommentarkorpus 2.0. Universitätsverlag
Potsdam, Potsdam.

Manfred Stede. 2017. Annotation Guidelines for
Rhetorical Structure.

Amir Zeldes. 2016. rstWeb – A Browser-based Anno-
tation Interface for Rhetorical Structure Theory and
Discourse Relations. 2016:1–5.

Heike Zinsmeister, Marc Reznicek, Julia Ricart Brede,
Christina Rosén, and Dirk Skiba. 2012. Das Wis-
senschaftliche Netzwerk „Kobalt-DaF“. Zeitschrift
für germanistische Linguistik, 40(3):457–458.

https://doi.org/10.1177/1461445610371054
https://doi.org/10.1177/1461445610371054
https://doi.org/10.1177/1461445610371054
https://aclweb.org/anthology/W17-3602
https://aclweb.org/anthology/W17-3602
https://aclweb.org/anthology/W17-3602
https://www.researchgate.net/publication/220747001{_}How{_}Complex{_}is{_}Discourse{_}Structure/download
https://www.researchgate.net/publication/220747001{_}How{_}Complex{_}is{_}Discourse{_}Structure/download
https://www.researchgate.net/publication/220747001{_}How{_}Complex{_}is{_}Discourse{_}Structure/download
https://doi.org/10.1007/s10579-014-9271-6
https://doi.org/10.1007/s10579-014-9271-6
https://doi.org/10.1007/s10579-014-9271-6
https://doi.org/10.1002/nav.3800020109
https://doi.org/10.1002/nav.3800020109
https://doi.org/10.1515/text.1.1988.8.3.243
https://doi.org/10.1515/text.1.1988.8.3.243
https://doi.org/10.1162/089120100561755
https://doi.org/10.1162/089120100561755
http://www.ling.uni-potsdam.de/{~}stede/Papers/nucl08.pdf
http://www.ling.uni-potsdam.de/{~}stede/Papers/nucl08.pdf
https://publishup.uni-potsdam.de/opus4-ubp/frontdoor/deliver/index/docId/8276/file/pcss8.pdf
https://publishup.uni-potsdam.de/opus4-ubp/frontdoor/deliver/index/docId/8276/file/pcss8.pdf
https://www.sfu.ca/{~}mtaboada/docs/research/RST{_}Annotation{_}Guidelines.pdf
https://www.sfu.ca/{~}mtaboada/docs/research/RST{_}Annotation{_}Guidelines.pdf
http://aclweb.org/anthology/N/N16/N16-3001.pdf
http://aclweb.org/anthology/N/N16/N16-3001.pdf
http://aclweb.org/anthology/N/N16/N16-3001.pdf
https://doi.org/10.1515/zgl-2012-0030
https://doi.org/10.1515/zgl-2012-0030

