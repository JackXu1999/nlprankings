









































Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications


Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 213–223,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

Evaluating the performance of Automated Text Scoring systems

Helen Yannakoudakis
The ALTA Institute

Computer Laboratory
University of Cambridge

helen.yannakoudakis@cl.cam.ac.uk

Ronan Cummins
The ALTA Institute

Computer Laboratory
University of Cambridge

ronan.cummins@cl.cam.ac.uk

Abstract

Various measures have been used to evalu-
ate the effectiveness of automated text scor-
ing (ATS) systems with respect to a human
gold standard. However, there is no system-
atic study comparing the efficacy of these met-
rics under different experimental conditions.
In this paper we first argue that measures of
agreement are more appropriate than mea-
sures of association (i.e., correlation) for mea-
suring the effectiveness of ATS systems. We
then present a thorough review and analysis of
frequently used measures of agreement. We
outline desirable properties for measuring the
effectiveness of an ATS system, and experi-
mentally demonstrate using both synthetic and
real ATS data, that some commonly used mea-
sures (e.g., Cohen’s kappa) lack these prop-
erties. Finally, we identify the most ap-
propriate measures of agreement and present
general recommendations for best evaluation
practices.

1 Introduction

Automated assessment of text was introduced in the
early 1960s in an attempt to address several issues
with manual assessment (e.g., expense, speed, and
consistency). Further advantages become more pro-
nounced when it comes to scoring extended texts
such as essays, a task prone to an element of subjec-
tivity. Automated systems enable rigid application
of scoring criteria, thus reducing the inconsistencies
which may arise, in particular, when many human
examiners are employed for large-scale assessment.

There is a substantial literature describing and
evaluating ATS systems (Page, 1968; Powers et al.,
2002; Rudner and Liang, 2002; Burstein et al., 2003;
Landauer et al., 2003; Higgins et al., 2004; Attali
and Burstein, 2006; Attali et al., 2008; Williamson,
2009; Briscoe et al., 2010; Chen and He, 2013).
Such systems are increasingly used but remain con-
troversial. Although a comprehensive comparison
of the capabilities of eight existing commercial es-
say scoring systems (Shermis and Hamner, 2012)
across five different performance metrics in the re-
cent ATS competition organised by Kaggle1 claimed
that ATS systems grade similarly to humans, critics
(Wang and Brown, 2007; Wang and Brown, 2008;
Perelman, 2013) have continued to dispute this.

For the evaluation of ATS systems (Williamson,
2009; Williamson et al., 2012), emphasis has
been given to the “agreement” of machine-predicted
scores (ordinal grades) with that of a human gold
standard; that is, scores assigned by human exam-
iners to the same texts that the machine is eval-
uated on. Various metrics have been used, the
most prominent being Pearson’s correlation, per-
centage of agreement, and variations of Cohen’s
kappa statistic. Inconsistencies in the reporting of,
and misconceptions in the interpretation of, these
metrics in published work makes cross-system com-
parisons on publicly-available datasets more diffi-
cult. The lack of careful motivation of any metric
fuels opposition to the deployment of ATS. To date,
several ATS systems are being used operationally
for high-stakes assessment in addition to them be-
ing part of self-assessment and self-tutoring sys-

1https://www.kaggle.com/c/asap-aes

213



tems, underscoring the need for common and well-
motivated metrics that establish true system perfor-
mance.

In this paper, we define the task of ATS as the
accurate prediction of gold-standard scores (pre-
defined ordinal grades), and we experimentally ex-
amine the robustness and efficacy of measures of
agreement for a number of different conditions un-
der two different experimental setups. First, we use
synthetic data to simulate various experimental con-
ditions, and second, we use real ATS data to assess
the effectiveness of the metrics under realistic sce-
narios. For the latter, we run a series of experiments
on the output of state-of-the-art ATS systems. We
outline some deficiencies in commonly used met-
rics that have been previously overlooked, and con-
sequently we propose more appropriate metrics for
evaluating ATS systems focusing primarily on op-
timising system effectiveness and facilitating cross-
system comparison.

The focus on measures of agreement is motivated
by their use as the primary metric for evaluating sys-
tem effectiveness in the recent Kaggle essay scoring
competition. To the best of our knowledge, there is
no systematic study comparing the efficacy of dif-
ferent measures of agreement under different exper-
imental conditions. Although we focus on the task
of ATS, the recommendations regarding the metrics
covered in this paper extend naturally to many sim-
ilar NLP tasks, i.e., those where the task is to accu-
rately predict a gold-standard score.

The remainder of the paper is structured as fol-
lows: Section 2 defines our task and objectives. Sec-
tion 3 reviews a number of performance metrics rel-
evant to the ATS task. Section 4 describes a set of
desired metric properties and presents an analysis of
some prominently used metrics for the ATS task that
uses the output of both simulated and real systems.
Section 5 concludes with a discussion, general rec-
ommendations for evaluation practices and an out-
line of future work.

2 Task Definition

In the standard ATS evaluation, there exists a set of n
texts where each text is indexed t1 to tn. Each text ti
is assigned a gold standard score gs(ti) by a human
assessor (or group of human assessors). This score

is one of g ordinal scores, which for convenience we
index 1 to g. It is worth noting that, in general, it is
not a requirement that the differences in scores are
uniform. Furthermore, there exists some number of
ATS systems atsj indexed j = 1 to j = m that
predict scores atsj(ti) for each of the n texts.

Given two ATS systems ats1 and ats2, we would
like to determine a metric M that returns a mea-
sure of performance for ats1 and ats2 for which
M(ats1, gs, t) > M(ats2, gs, t) when ats1 is a
better system than ats2. Note that we have not de-
fined what “better” means at this stage. We will re-
turn to describing some desirable properties ofM in
Section 4.

From an educational point of view, our task is
to ascertain whether the writing abilities required to
warrant a particular score/grade have been attained.
From this perspective, measures of agreement seem
the appropriate type of measurement to apply to the
output of ATS systems to address the accuracy of the
(numerical) solution compared to the gold standard.

3 Measuring Performance of ATS systems

In this section, we review and critique metrics that
have been frequently used in the literature to ascer-
tain the performance of ATS systems. These per-
formance metrics can be broadly categorised into
measures of association and measures of agreement
(e.g., see Williamson et al. (2012)).

3.1 Measures of Association

Measures of association (i.e., correlation coeffi-
cients) have been widely used in ATS (e.g., Yan-
nakoudakis et al. (2011)), with Pearson’s product-
moment correlation coefficient being the most com-
mon. Pearson’s correlation is a parametric measure
of association that quantifies the degree of linear de-
pendence between two variables and, more specif-
ically, describes the extent to which the variables
co-vary relative to the degree to which they vary in-
dependently. The greater the association, the more
accurately one can use the value of one variable
to predict the other. As the data depart from the
coefficient’s assumptions (e.g., unequal marginals),
its maximum values may not be attainable (Carroll,
1961). For ordinal data, unequal marginals will al-

214



ways involve ties.2 As the number of ties increases
relative to the number of observations, its appropri-
ateness largely diminishes.3

Spearman’s rank correlation coefficient is a non-
parametric measure of association that has the same
range as Pearson, and it is calculated by ranking the
variables and computing Pearson on the ranks rather
than the raw values. In contrast to Pearson, it as-
sesses the strength of a monotonic rather than lin-
ear relation between two variables, and has the ad-
vantage of independence from various assumptions.
Unlike Pearson, it exhibits robustness to outliers;
however, its reliability also decreases as the num-
ber of ties increases. It is worth noting at this point
Kendall’s τb, which is a more effective tie-adjusted
non-parametric bivariate coefficient that quantifies
the degree of agreement between rankings, and it is
defined in terms of concordant and discordant pairs,
although ties also affect its reliability.

3.1.1 Discussion
In essence, non-parametric measures are mea-

sures of rank correlation. In the context of the eval-
uation of ATS, they measure agreement with respect
to the ranks, that is, whether an ATS system ranks
texts similarly to the gold standard. However, this
is not an appropriate type of measurement given the
task definition in Section 2, where we would like
to ascertain actual agreement with respect to the
scores. Furthermore, correlation coefficients do not
account for any systematic biases in the data; for ex-
ample, a high correlation can be observed even if
the predicted scores are consistently n points higher
than the gold standard.

In the presence of outliers, the coefficient can be
misleading and pulled in either direction. For ex-
ample, for Pearson’s correlation an outlier can influ-
ence the value of the correlation to the extent that
a high correlation is observed even though the data
may not be linearly dependent. Furthermore, it is
well known that the value of the correlation will be
greater if there is more variability in the data than
if there is less. This is caused by the mathematical

2Of course, ties exist even when the marginals are identi-
cal if the number of observations is larger than the number of
scores.

3For more details see (Maimon et al., 1986; Goodwin and
Leech, 2006; Hauke and Kossowski, 2011) among others.

constraints in their formulation, and does not nec-
essarily reflect the true relationship of predicted to
gold standard scores. Finally, their reliability de-
creases as the number of ties increases. We come
back to the appropriateness and recommended use
of correlation metrics in Section 5.

In summary, (non-parametric) correlation mea-
sures are more apt at measuring the ability of the
ATS system to correctly rank texts (i.e., placing a
well written text above a poorly written text), rather
than the ability of the ATS system to correctly as-
sign a score/grade. In other words, correlation mea-
sures do not reward ATS systems for their abil-
ity to correctly identify the thresholds that separate
score/grade boundaries (1 : 2 to g − 1 : g).
3.2 Measures of Agreement
A simple way of gauging the agreement between
gold and predicted scores is to use percentage agree-
ment, calculated as the number of times the gold and
predicted scores are the same, divided by the total
number of texts assessed. A closely-related variant
is percentage of adjacent agreement, in which agree-
ment is based on the number of times the gold and
predicted scores are no more than n points apart.

Despite its simplicity, it has been argued (Cohen,
1960) that this measure can be misleading as it does
not exclude the percentage of agreement that is ex-
pected on the basis of pure chance. That is, a cer-
tain amount of agreement can occur even if there is
no systematic tendency for the gold and predicted
scores to agree. The kappa coefficient (Cκ) was in-
troduced by Cohen (1960) as a measure of agree-
ment adjusted for chance. Let Pa denote the percent-
age of observed agreement and Pe the percentage of
agreement expected by chance, Cohen’s kappa coef-
ficient is calculated as the ratio between the “true”
observed agreement and its maximum value:

Cκ =
Pa − Pe(κ)
1− Pe(κ) (1)

where Pe(κ) is the estimated agreement due to
chance, and is calculated as the inner-product of the
marginal distribution of each assessor (a worked ex-
ample of this follows in Section 3.2.1). The values
of the coefficient range between −1 and 1, where
1 represents perfect agreement and 0 represents no
agreement beyond that occurring by chance. Most

215



measures of agreement that are corrected for chance
agreement, of which there are many, follow the gen-
eral formula above where Pe varies depending on
the specific measure. The disadvantage of this basic
measure applied to ordinal data (scores in our case)
is that it does not allow for weighting of different
degrees of disagreement.

Weighted kappa (Cohen, 1968) was developed to
address this problem. Note that this was the main
metric used for evaluation and cross-system com-
parison of essay scoring systems in the recent Kag-
gle shared-task competition on ATS. It is commonly
employed with ordinal data and can be defined ei-
ther in terms of agreement or disagreement weights.
The most common weights used are the (absolute)
linear error weights and the quadratic error weights
(Fleiss, 1981). The linear error weights are propor-
tional to the actual difference between the predicted
scores and the gold standard, while the quadratic
error weights are proportional to the squared ac-
tual difference between these scores. The choice of
weights is important as they can have a large effect
on the results (Graham and Jackson, 1993).

In what follows, we discuss two of the main prob-
lems regarding the kappa coefficient: its dependency
on trait prevalence and on marginal homogeneity.
We note that the properties kappa exhibits (as shown
below) are independent of the type of data on which
it is used, that is, whether there is a categorical or an
ordinal (gold standard) scale.

3.2.1 Trait Prevalence
Trait prevalence occurs when the underlying char-

acteristic being measured is not distributed uni-
formly across items. It is usually the case that gold
standard scores are normally distributed in the ATS
task (i.e., the scores/grades are biased towards the
mean).

Table 1 shows an example of the effect of
trait prevalence on the Cκ statistic using a contin-
gency table. In this simple example there are two
scores/grades (i.e., pass P or fail F) for two different
sets of 100 essays with different gold-score distribu-
tions, gs1 and gs2. The rows of the matrix indicate
the frequency of the scores predicted by the ATS,
while the columns are the gold-standard scores. Al-
though percentage agreement (along the main diag-
onal) in both cases is quite high, Pa = 0.8, the Cκ

ats \gs1 P F
P 40 10 50
F 10 40 50

50 50 100

ats \gs2 P F
P 64 4 68
F 16 16 32

80 20 100

Table 1: Cohen’s κ for an ats system on two sets
of essays. Although percentage agreement is 0.8 for
both sets of essays, Cκ = 0.6 (left) and Cκ = 0.49
(right).

statistic varies quite considerably. As the observed
marginals (i.e., the totals either vertically or hori-
zontally, or otherwise, the distribution of the scores
/ grades) in ats\gs1 are uniformly distributed, the
correction for chance agreement is much lower (i.e.,
Pe(κ) = 0.5×0.5+0.5×0.5 = 0.5) than for ats\gs2
(i.e., Pe(κ) = 0.68× 0.8 + 0.32× 0.2 = 0.61) with
unequal marginals, which leads to a lower absolute
Cκ value for ats\gs2. In this example, it is not clear
why one would want a measure of agreement with
this behaviour, where Pe is essentially artificially in-
creased when the marginals are unequal.

Fundamentally, this implies that the comparison
of systems across datasets (or indeed the comparison
of datasets given the same system) is very difficult
because the value of Cκ not only depends on actual
agreement, but crucially also on the distribution of
the gold standard scores.

3.2.2 Marginal Homogeneity
A second problem with Cκ is that the difference

in the marginal probabilities affects the coefficient
considerably. Consider Table 2, which shows two
different ATS system ratings (ats1 and ats2) along
the same gold standard scores. The value of Cκ
for ats2 is much smaller (and actually it is 0) com-
pared to that for ats1 (0.12), even though ats2 has
higher percentage and marginal agreement; that is,
ats2 predicts scores with frequencies that are more
similar to those in the gold standard.4 The root cause
of this paradox is similar to the one described earlier,
and arises from the way Pe is calculated, and more
specifically the assumption that marginal probabili-
ties are classification propensities that are fixed, that
is, they are known to the assessor before classify-
ing the instances/texts into categories/scores. This

4Of course higher marginal agreement does not translate to
overall higher agreement if percent agreement is low.

216



ats1 \gs P F
P 20 0 20
F 60 20 80

80 20 100

ats2 \gs P F
P 60 15 75
F 20 5 25

80 20 100

Table 2: Cκ for two systems ats1 and ats2 for the
same set of gold scores. Although percentage agree-
ment for ats1 and ats2 is 0.4 and 0.65 respectively,
Cκ for ats1 and ats2 isCκ = 0.12 (left) andCκ = 0
(right).

is clearly not the case for ATS systems, and there-
fore the dependence of chance agreement on the
level of marginal agreement is questionable when
the marginals are free to vary (Brennan and Predi-
ger, 1981).5

Essentially, the end result when a system predicts
scores with a marginal distribution that is more sim-
ilar to the gold standard (i.e., ats2), is that any mis-
classification is penalised more severely even though
percent agreement may be high. This is not the be-
haviour we want in a performance metric for ATS
systems. Using kappa as an objective function in
any machine learning algorithm could easily lead to
learning functions that favour assigning distributions
that are different to that of the gold standard (e.g.,
Chen and He (2013)).

3.2.3 Discussion
Previous work has also demonstrated that there

exist cases where high values of (quadratic) kappa
can be achieved even when there is low agreement
(Graham and Jackson, 1993). Additionally, Bren-
ner and Kliebsch (1996) investigated the effect of
the score range on the magnitude of weighted kappa
and found that the quadratic weighted kappa coef-
ficient tends to have high variation and increases as
the score range increases, particularly in ranges be-
tween two and five distinct scores. In contrast, lin-
early weighted kappa appeared to be less affected,
although a slight increase in value was observed as
the range increased.

The correction for chance agreement in Cohen’s
kappa has been the subject of much controversy
(Brennan and Prediger, 1981; Feinstein and Cic-
chetti, 1990; Uebersax, 1987; Byrt et al., 1993;

5However, we would like to penalise trivial systems that e.g.,
always assign the most prevalent gold score, in which case the
marginals are indeed fixed.

Gwet, 2002; Di Eugenio and Glass, 2004; Sim and
Wright, 2005; Craggs and Wood, 2005; Artstein and
Poesio, 2008; Powers, 2012). Firstly, it assumes
that when assessors are unsure of a score, they guess
at random according to a fixed prior distribution of
scores. Secondly, it includes chance correction for
every single prediction instance (i.e., not only when
an assessor is in doubt). Many have argued (Bren-
nan and Prediger, 1981; Uebersax, 1987) that this
is a highly improbable model of assessor error and
vastly over-estimates agreement due to chance, es-
pecially in the case when prior distributions are free
to vary. Although it is likely that there is some agree-
ment due to chance when an assessor is unsure of a
score (Gwet, 2002), it is unlikely that human asses-
sors simply guess at random, and it is unlikely that
this happens for all predictions. For the task of ATS,
the distribution of scores to assign are not fixed a pri-
ori. Although trained assessors may have a certain
expectation of the final distribution, it is certainly
not fixed.6

Consequently, there are a number of different
agreement metrics – for example, Scott’s π (Scott,
1955), which is sensitive to trait prevalence but not
the marginals, and Krippendorff’s α (Krippendorff,
1970) which is nearly equivalent to π (Artstein and
Poesio, 2008) – all of which vary in the manner in
which chance agreement (i.e., Pe) is calculated, but
have similar problems (Zhao, 2011; Gwet, 2014).
It is also worth noting that weighted versions of
kappa do not solve the issues of trait prevalence and
marginal homogeneity.

The two most noteworthy variants are the agree-
ment coefficient AC (Gwet, 2002) and the Brennan-
Prediger (BP) coefficient (Brennan and Prediger,
1981), which both estimate Pe more conservatively
using more plausible assumptions. In particular,
the BP coefficient estimates Pe using 1/g, with
the assumption that the probability that an asses-
sor would guess the score of an item by chance is
inversely related to the number of scores g in the
rating scale.7 Substituting Pe in equation (1) gives
(Pa − 1/g)/(1− 1/g), which is better suited when
one or both of the marginals are free to vary. When

6See Brennan and Prediger (1981) for a more detailed dis-
cussion.

7We note that this is equivalent to the S coefficient (Bennett
et al., 1954) discussed in (Artstein and Poesio, 2008).

217



the grades are not uniformly distributed, Pe may
be higher than 1/g; nevertheless, it can be a useful
lower limit for Pe (Lawlis and Lu, 1972). Note that
in the example in Table 1, BP would be the same for
both ats\gs1 and ats\gs2, (0.8 − 0.5)/(1 − 0.5) =
0.6, and thus effectively remains unaltered under the
effects of trait prevalence.8

The AC coefficient calculates Pe as follows:

Pe =
1

(g − 1)
∑g

k=1
πk(1− πk) (2)

πk = (pa,k + pb,k)/2 (3)

where πk represents the probability of assigning
grade/score k to a randomly selected item by a ran-
domly selected assessor, calculated based on pa,k
and pb,k, which are the marginal probabilities of
each assessor a and b respectively for grade/score
k. More specifically, pa,k = na,k/n and pb,k =
nb,k/n, where na,k refers to the number of instances
assigned to grade k by assessor a, nb,k refers to
the number of instances assigned to grade k by as-
sessor b, and n refers to the total number of in-
stances. Gwet (2002;2014) defines chance agree-
ment as the product of the probability that two asses-
sors agree given a non-deterministic instance,9 de-
fined as 1/g, by the propensity for an assessor to
assign a non-deterministic grade/score, estimated as∑g

k=1πk(1− πk)/(1− 1/g).10
In the example in Table 1, Pe = (0.5×(1−0.5)+

0.5 × (1 − 0.5))/(2 − 1) = 0.5 for ats\gs1 (for
which πpass = πfail = 0.5), and Pe = (0.74× (1−
0.74)+0.26×(1−0.26))/(2−1) = 0.38 for ats\gs2,
which is in contrast to Cκ that overestimated Pe for
ats\gs2 with unequal marginals. More specifically,
the AC coefficient would be higher for ats\gs2 than
for ats\gs1: 0.67 versus 0.60 respectively.11

4 Metric Properties

On the basis of the discussion so far, we propose the
following list of desirable properties of an evaluation

8However, it can be artificially increased as the scoring scale
increases.

9That is, it is a hard-to-score instance, which is the case
where random ratings occur.

10See (Gwet, 2014) for more details regarding the differences
between AC and Aickin’s alpha (Aickin, 1990).

11The reader is referred to (Gwet, 2014; Brennan and Predi-
ger, 1981) for more details on AC and BP and their extensions
to at least ordinal data and to more than two assessors.

measure for an ATS system:

• Robustness to trait prevalence

• Robustness to marginal homogeneity

• Sensitivity to magnitude of misclassification

• Robustness to score range

In this section, we analyse the aforementioned
metrics of agreement (with different weights) with
respect to these properties using both synthetic and
real ATS-system scores (where applicable).

4.1 Robustness to Trait Prevalence

In order to test metrics for robustness to trait preva-
lence, we simulated 5,000 gold standard scores on
a 5-point scale using a Gaussian (normal) distribu-
tion with a mean score at the mid-point. By con-
trolling the variance of this Gaussian, we can create
gold standard scores that are more peaked at the cen-
ter (high trait prevalence) or more uniform across all
grades (low trait prevalence). We simulated systems
by randomly introducing errors in these scores. The
system output in Figure 1 (left) was created by ran-
domly sampling 25% of the gold standard scores and
perturbing them by 2 points in a random direction.12

This led to a simulated system with 75% percent-
age agreement, which also translates to a constant
mean absolute error (MAE) of 0.5 (i.e., on average,
each predicted score is 0.5 scores away from its gold
counterpart).

Figure 1 (left) shows that nearly all evaluation
measures are very sensitive to the distribution of the
gold standard scores, and the magnitude of the met-
rics does change as the distribution becomes more
peaked. The AC measure is less sensitive than Cκ
(and actually rewards systems), but the only measure
of agreement that is invariant under changes in trait
prevalence is the BP coefficient, which actually is
in line with percentage agreement and assigns 75%
agreement using quadratic weights.

To study the effect of trait prevalence on real sys-
tems, we replicated an existing state-of-the-art ATS
system (Yannakoudakis et al., 2011). The model

12If this could not be done (i.e., a score of 4 cannot be
changed by +2 on a 5-point scale), a different score was ran-
domly sampled.

218



 0

 0.2

 0.4

 0.6

 0.8

 1

 0.5  1  1.5  2  2.5  3  3.5  4  4.5  5

M
a

g
n

it
u

d
e

 o
f 

M
e

tr
ic

Standard Deviation of Gold Score Distribution

Effect of Trait Prevalence on Metrics

Ck linear
Ck quad

BP linear
BP quad
AC linear
AC quad

MAE  0

 0.2

 0.4

 0.6

 0.8

 1

 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

M
a
g
n
it
u
d
e
 o

f 
M

e
tr

ic

Prevalence Rate of Positive Class

Effect of Trait Prevalence on Metrics

Ck
BP
AC
PA

Figure 1: Effect of trait prevalence on metrics of agreement for synthetic (left) and real (right) ATS scores.

evaluates writing competence on the basis of lexi-
cal and grammatical features, as well as errors, and
achieves a correlation of around 0.75 on the publicly
available First Certificate in English (FCE) examina-
tion scripts, that have been manually annotated with
a score in the range 1 to 40 (with 40 being the high-
est). In this setting, robustness to trait prevalence
was evaluated by plotting the magnitude of the met-
rics as a function of the prevalence rate, calculated
as the proportion of passing scores in the data, as
judged by both the ATS system and the examiner, as
we varied the passing threshold from 1 to 40.

In Figure 1 (right) we can see that all metrics
are sensitive to trait prevalence.13 In order to get
a clearer picture on the effect of trait prevalence on
real systems, it is important we plot percent agree-
ment (PA) along with the metrics. The reason is
that chance-corrected agreement measures should
remain reasonably close to the quantity that they ad-
just for chance, as this quantity varies (Gwet, 2014).
AC and BP remain reasonably close to PA as the
prevalence rate increases. On the other hand, Cκ is
further away, and at times considerably lower. The
behaviour of kappa is difficult to explain, and in fact,
even when the prevalence rate approaches 1, Cκ still
produces very low results. Note that in the binary
pass/fail case, linear and quadratic weights do not
affect the value of kappa and produce the same re-
sults.

13Curve fitting is performed to be able to observe the ten-
dency of the metrics.

4.2 Robustness to Marginal Homogeneity

In order to test the metrics for robustness to marginal
homogeneity, we simulated 5,000 gold standard
scores on a 10-point scale using a Gaussian distribu-
tion with a mean score at the mid-point and a stan-
dard deviation of one score. We simulated differ-
ent systems by randomly introducing errors in these
scores. In particular, we simulated outputs that had
distributions different to that of the gold standard by
drawing a number of incorrect scores from a differ-
ent Gaussian centred around a different mean (0–9 in
Figure 2). We kept percentage agreement with lin-
ear weights constant, which again also translates to a
constant MAE (1.0). We are looking for metrics that
are less sensitive to varying marginals, and ideally
which promote systems that distribute scores simi-
larly to the gold standard when agreement is other-
wise identical.

For the measures of agreement, as expected, Co-
hen’s kappa (both linear and quadratic) penalises
systems that distribute scores similarly to those of
the gold standard. However, AC (with linear and
quadratic weights) and quadratic BP promote sys-
tems that distribute scores similarly to the gold
scores. On the other hand, BP linear remains un-
changed.

To study the sensitivity of the metrics to the vari-
ations in the marginal distributions in real ATS sys-
tems, we plot their variation as a function of the sim-
ilarity of the passing-score distributions, where the
similarity is calculated as simpass = 1−|pgold,pass−
pats,pass|, which is based on the absolute difference

219



 0

 0.2

 0.4

 0.6

 0.8

 1

 0  1  2  3  4  5  6  7  8  9

M
a

g
n

itu
d

e
 o

f 
M

e
tr

ic

Mean of Incorrect Predictions

Effect of System Score Distribution on Metrics

Ck linear
Ck quad

BP linear
BP quad
AC linear
AC quad

MAE
 0

 0.2

 0.4

 0.6

 0.8

 1

 0.7  0.75  0.8  0.85  0.9  0.95  1

M
a
g
n
it
u
d
e
 o

f 
M

e
tr

ic

Similarity of Passing-score Distributions

Effect of Marginal Distribution on Metrics

Ck
BP
AC
PA

Figure 2: Sensitivity of metrics on the marginal distribution of synthetic (left) and real (right) ATS scores.

between the marginal probabilities of the gold and
predicted passing scores. The higher the value of
sim, the more similar the distributions are. Again,
we employ Yannakoudakis et al. (2011)’s ATS sys-
tem.

Similarly to the simulated experiments, we ob-
serve increases in the magnitude of AC and BP
as the similarity increases, whereas Cκ is consid-
erably lower and has a decreasing tendency which
does not stay close to PA. In fact, marginal homo-
geneity does not guarantee the validity of the re-
sults for Cohen’s kappa. This can be seen more
clearly in Figure 3, where we plot the magnitude
of the metrics as a function of the overall probabil-
ity of assigning a passing score, as judged by both
the human assessor and the ATS system. That is,
(pgold,pass + pats,pass)/2. As the overall probability
of a passing score becomes very large or very small,
Cκ yields considerable lower results, regardless of
whether the marginal probabilities are equal or not.

4.3 Sensitivity to Magnitude of
Misclassification

It is common that human assessors disagree by
small margins given the subjectivity of the ATS task.
However, larger disagreements are usually treated
more seriously. Therefore, given two ATS systems,
we would prefer a system that makes more small
misclassifications over a system that makes a few
large misclassifications when all else is equal. A
metric with quadratic-weighting is likely to adhere
to this property.

To test the sensitivity of the metrics to the mag-

nitude of misclassification, we simulated 5,000 gold
standard scores on a 10-point scale using a Gaussian
(normal) distribution with a mean score at the mid-
point. Again, we simulated systems by randomly in-
troducing errors to the scores. For each system, we
varied the magnitude of the misclassification while
the total misclassification distance (i.e., MAE or PA)
was kept constant. Figure 4 confirms that measures
of agreement that use quadratic weights decrease as
the magnitude of each error increases. The met-
rics of agreement that use linear weights actually in-
crease slightly.14

4.4 Robustness to score scales

Robustness of the metrics to the score range or scale
was tested by binning the gold and predicted scores
at fixed cutpoints and re-evaluating the results. In
the FCE dataset, the scale was varied between 40
and 3 points by successively binning scores. Met-
rics that are less sensitive to scoring scales facilitate
cross-dataset comparisons.

All metrics were affected by the scale, al-
though those with quadratic weights appeared to
be more sensitive compared to those with linear
ones. Quadratic Cκ was the most sensitive met-
ric and showed larger decreases compared to the
others as the scoring scale was reduced, while AC
quadratic exhibited higher stability compared to BP
quadratic.15

14Note that such an experiment cannot be controlled and re-
liably executed for real systems.

15Detailed results omitted due to space restrictions.

220



 0

 0.2

 0.4

 0.6

 0.8

 1

 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

M
a

g
n

it
u

d
e

 o
f 

M
e

tr
ic

Overall Probability of Positive Class

Effect of Marginal Distribution on Metrics

Ck
BP
AC
PA

Figure 3: Sensitivity of metrics on the marginal
distribution of real ATS-model scores.

 0

 0.2

 0.4

 0.6

 0.8

 1

 1  1.5  2  2.5  3  3.5  4  4.5  5

M
a

g
n

it
u

d
e

 o
f 

M
e

tr
ic

Mean Magnitude of Error

Effect of Magnitude of Error on Metrics

Ck linear
Ck quad

BP linear
BP quad
AC linear
AC quad

MAE

Figure 4: Change in the magnitude of performance
metrics as only the size of each misclassification in-
creases.

5 Recommendations and Conclusion

Our results suggest that AC and BP (with quadratic
weights) overall are the most robust agreement co-
efficients. On the basis of this analysis, we make the
following recommendations:

• We recommend against using Cohen’s kappa.
Interpretation of the magnitude of kappa within
/ across system and dataset comparisons is
problematic, as it depends on trait prevalence,
the marginal distributions and the scoring scale.
It is worth noting at this point that the inefficacy
of kappa is independent on the type of data that
it is being used on, that is, whether there is a
categorical or ordinal (gold standard) scale.

• We recommend using the AC coefficient with
quadratic weights. Although BP is a good al-
ternative as it adjusts percent agreement simply
based on the inverse of the scoring scale, it is
more sensitive to, and directly affected by the
scoring scale.

• We recommend reporting a rank correlation co-
efficient (Spearman’s or Kendall’s τb rank cor-
relation coefficient), rather than using it for sys-
tem evaluation and comparison, as it can facil-
itate error analysis and system interpretation;
for example, low agreement and high rank cor-
relation would indicate a large misclassification
magnitude, but high agreement with respect to

the ranking (i.e., the system ranks texts simi-
larly to the gold standard); high agreement and
low rank correlation would indicate high accu-
racy in predicting the gold scores, but small
ranking errors.16 Kendall’s τb may be pre-
ferred, as it is a more effective tie-adjusted co-
efficient that is defined in terms of concordant
and discordant pairs; however, further experi-
ments beyond the scope of this paper would be
needed to confirm this.

It is worth noting that given the generality of the
ATS task setting as presented in this paper (i.e.,
aiming to predict gold standard scores on an or-
dinal scale) and the metric-evaluation setup (us-
ing synthetic data in addition to real output), the
properties discussed and resulting recommendations
may be more widely relevant within NLP and may
serve as a useful benchmark for the wider commu-
nity (Siddharthan and Katsos, 2010; Bloodgood and
Grothendieck, 2013; Chen and He, 2013; Liu et al.,
2013, among others) as well as for shared task or-
ganisers.

An interesting direction for future work would be
to explore the use of evaluation measures that lie
outside of those commonly used by the ATS com-
munity, such as macro-averaged root mean squared
error that has been argued as being suitable for ordi-
nal regression tasks (Baccianella et al., 2009).

16A low correlation could also point to effects of the under-
lying properties of the data as the metric is sensitive to trait
prevalence (see Section 3.1.1).

221



Acknowledgments

We would like to thank Ted Briscoe for his valu-
able comments and suggestions, Cambridge English
Language Assessment for supporting this research,
and the anonymous reviewers for their useful feed-
back.

References

Mikel Aickin. 1990. Maximum likelihood estimation
of agreement in the constant predictive probability
model, and its relation to Cohen’s kappa. Biometrics,
46(2):293–302.

Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.

Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-Rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3):1–30.

Yigal Attali, Don Powers, Marshall Freedman, Marissa
Harrison, and Susan Obetz. 2008. Automated Scoring
of short-answer open-ended GRE subject test items.
Technical Report 04, ETS.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Evaluation measures for ordinal regres-
sion. In 9th IEEE International Conference on Intel-
ligent Systems Design and Applications, pages 283–
287. IEEE Comput. Soc.

E M Bennett, R Alpert, and A C Goldstein. 1954.
Communications through limited-response question-
ing. Public Opinion Quarterly, 18(3):303–308.

Michael Bloodgood and John Grothendieck. 2013.
Analysis of Stopping Active Learning based on Sta-
bilizing Predictions. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 10–19.

Robert L Brennan and Dale J Prediger. 1981. Coefficient
kappa: Some uses, misuses, and alternatives. Edu-
cational and psychological measurement, 41(3):687–
699.

Hermann Brenner and Ulrike Kliebsch. 1996. Depen-
dence of weighted kappa coefficients on the number
of categories. Epidemiology, 7(2):199–202.

Ted Briscoe, Ben Medlock, and Øistein E. Andersen.
2010. Automated assessment of ESOL free text exam-
inations. Technical Report UCAM-CL-TR-790, Uni-
versity of Cambridge, Computer Laboratory, nov.

Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003. Criterion: Online essay evaluation: An appli-
cation for automated evaluation of student essays. In

Proceedings of the fifteenth annual conference on in-
novative applications of artificial intelligence, pages
3–10.

Ted Byrt, Janet Bishop, and John B Carlin. 1993. Bias,
prevalence and kappa. Journal of clinical epidemiol-
ogy, 46(5):423–429.

John B. Carroll. 1961. The nature of the data, or how
to choose a correlation coefficient. Psychometrika,
26(4):347–372, December.

Hongbo Chen and Ben He. 2013. Automated Essay
Scoring by Maximizing Human-machine Agreement.
In Empirical Methods in Natural Language Process-
ing, pages 1741–1752.

Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37–46, April.

Jacob Cohen. 1968. Weighted kappa: nominal scale
agreement with provision for scaled disagreement or
partial credit. Psychological bulletin, 4(70):213–220.

Richard Craggs and Mary McGee Wood. 2005. Evaluat-
ing discourse and dialogue coding schemes. Compu-
tational Linguistics, 31(3):289–295.

Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: A second look. Computational linguistics,
30(1):95–101.

Alvan R Feinstein and Domenic V Cicchetti. 1990. High
agreement but low kappa: I. the problems of two para-
doxes. Journal of clinical epidemiology, 43(6):543–
549.

Joseph L. Fleiss. 1981. Statistical methods for rates and
proportions. Wiley series in probability and math-
ematical statistics. Applied probability and statistics.
Wiley.

Laura D. Goodwin and Nancy L. Leech. 2006. Under-
standing Correlation: Factors That Affect the Size of
r. The Journal of Experimental Education, 74(3):249–
266, April.

Patrick Graham and Rodney Jackson. 1993. The analy-
sis of ordinal agreement data: beyond weighted kappa.
Journal of clinical epidemiology, 46(9):1055–1062,
September.

Kilem Gwet. 2002. Inter-rater reliability: dependency on
trait prevalence and marginal homogeneity. Statistical
Methods for Inter-Rater Reliability Assessment Series,
2:1–9.

Kilem L. Gwet. 2014. Handbook of Inter-Rater Relia-
bility, 4th Edition: The Definitive Guide to Measuring
The Extent of Agreement Among Raters. Advanced
Analytics, LLC.

Jan Hauke and Tomasz Kossowski. 2011. Comparison of
Values of Pearson’s and Spearman’s Correlation Coef-
ficients on the Same Sets of Data. Quaestiones Geo-
graphicae, 30(2):87–93, January.

222



Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.

Klaus Krippendorff. 1970. Estimating the reliabil-
ity, systematic error and random er- ror of interval
data. Educational and Psychological Measurement,
30(1):61–70.

Thomas K. Landauer, Darrell Laham, and Peter W. Foltz.
2003. Automated scoring and annotation of essays
with the Intelligent Essay Assessor. In M.D. Shermis
and J. C. Burstein, editors, Automated essay scoring:
A cross-disciplinary perspective, pages 87–112.

G Frank Lawlis and Elba Lu. 1972. Judgment of coun-
seling process: reliability, agreement, and error. Psy-
chological bulletin, 78(1):17–20, July.

Tsun-Jui Liu, Shu-Kai Hsieh, and Laurent PREVOT.
2013. Observing Features of PTT Neologisms: A
Corpus-driven Study with N-gram Model. In Twenty-
Fifth Conference on Computational Linguistics and
Speech Processing (ROCLING 2013), pages 250–259.

Zvi Maimon, Adi Raveh, and Gur Mosheiov. 1986. Ad-
ditional cautionary notes about the Pearson’s correla-
tion coefficient. Quality and Quantity, 20(4).

Ellis B. Page. 1968. The use of the computer in analyz-
ing student essays. International Review of Education,
14(2):210–225, June.

L Perelman. 2013. Critique (ver. 3.4) of mark d. sher-
mis and ben hammer,contrasting state-of-the-art auto-
mated scoring of essays: Analysis. New York Times.

Donald E. Powers, Jill C. Burstein, Martin Chodorow,
Mary E. Fowles, and Karen Kukich. 2002. Stump-
ing e-rater: challenging the validity of automated essay
scoring. Computers in Human Behavior, 18(2):103–
134.

David M W Powers. 2012. The problem with kappa. In
13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 345–355.

Lawrence M. Rudner and Tahung Liang. 2002. Auto-
mated essay scoring using Bayes’ theorem. The Jour-
nal of Technology, Learning and Assessment, 1(2):3–
21.

William A Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public opinion
quarterly, 19(3):321–325.

Mark D Shermis and Ben Hamner. 2012. Contrasting
state-of-the-art automated scoring of essays: Analysis.
In Annual National Council on Measurement in Edu-
cation Meeting, pages 14–16.

Advaith Siddharthan and Napoleon Katsos. 2010. Re-
formulating Discourse Connectives for Non-Expert

Readers. In North American Chapter of the ACL,
pages 1002–1010.

Julius Sim and Chris C Wright. 2005. The Kappa
Statistic in Reliability Studies: Use, Interpretation,
and Sample Size Requirements. Physical Therapy,
85(3):257–268, March.

John S Uebersax. 1987. Diversity of decision-making
models and the measurement of interrater agreement.
Psychological Bulletin, 101(1):140.

Jinhao Wang and Michelle Stallone Brown. 2007. Auto-
mated Essay Scoring Versus Human Scoring: A Com-
parative Study. The Journal of Technology, Learning
and Assessment, 6(2).

Jinhao Wang and Michelle Stallone Brown. 2008. Auto-
mated Essay Scoring Versus Human Scoring: A Cor-
relational Study. Contemporary Issues in Technology
and Teacher Education, 8(4):310–325.

David M. Williamson, Xiaoming Xi, and Jay F. Breyer.
2012. A framework for evaluation and use of auto-
mated scoring. Educational Measurement: Issues and
Practice, 31(1):2–13.

David M. Williamson. 2009. A Framework for Im-
plementing Automated Scoring. In Proceedings of
the Annual Meeting of the American Educational Re-
search Association and the National Council on Mea-
surement in Education, San Diego, CA.

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies.

Xinshu Zhao. 2011. When to use scott’s π or krippen-
dorff’s α, if ever? In The annual Conference of the As-
sociation for Education in Journalism and Mass Com-
munication.

223


