



















































WAT-SL: A Customizable Web Annotation Tool for Segment Labeling


Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 13–16
c©2017 Association for Computational Linguistics

WAT-SL: A Customizable Web Annotation Tool for Segment Labeling

Johannes Kiesel and Henning Wachsmuth and Khalid Al-Khatib and Benno Stein
Faculty of Media

Bauhaus-Universität Weimar
99423 Weimar, Germany

<first name>.<last name>@uni-weimar.de

Abstract

A frequent type of annotations in text cor-
pora are labeled text segments. General-
purpose annotation tools tend to be overly
comprehensive, often making the annota-
tion process slower and more error-prone.
We present WAT-SL, a new web-based tool
that is dedicated to segment labeling and
highly customizable to the labeling task at
hand. We outline its main features and ex-
emplify how we used it for a crowdsourced
corpus with labeled argument units.

1 Introduction

Human-annotated corpora are essential for the de-
velopment and evaluation of natural language pro-
cessing methods. However, creating such corpora
is expensive and time-consuming. While remote
annotation processes on the web such as crowd-
sourcing provide a way to obtain numerous an-
notations in short time, the bottleneck often lies
in the used annotation tool and the required train-
ing of annotators. In particular, most popular an-
notation tools aim to be general purpose, such as
the built-in editors of GATE (Cunningham, 2002)
or web-based tools like BRAT (Stenetorp et al.,
2012) and WebAnno (Yimam et al., 2014). Their
comprehensiveness comes at the cost of higher in-
terface complexity, which often also decreases the
readability of the texts to be annotated.

Most of the typical annotation work is very fo-
cused, though, requiring only few annotation func-
tionalities. An example is segment labeling, i.e.,
the assignment of one predefined label to each seg-
ment of a given text. Such labels may, e.g., capture
clause-level argument units, sentence-level sen-
timent, or paragraph-level information extraction
events. We argue that, for segment labeling, a ded-
icated tool is favorable in order to speed up the an-

notator training and the annotation process. While
crowdsourcing platforms such as mturk.com or
crowdflower.com follow a similar approach, their
interfaces are either hardly customizable at all or
need to be implemented from scratch.

This paper presents WAT-SL (Web Annotation
Tool for Segment Labeling), an open-source web-
based annotation tool dedicated to segment label-
ing.1 WAT-SL provides all functionalities to effi-
ciently run and manage segment labeling projects.
Its self-descriptive annotation interface requires
only a web browser, making it particularly conve-
nient for remote annotation processes. The inter-
face can be easily tailored to the requirements of
the project using standard web technologies in or-
der to focus on the specific segment labels at hand
and to match the layout expectations of the anno-
tators. At the same time, it ensures that the texts
to be labeled remain readable during the whole
annotation process. This process is server-based
and preemptable at any point. The annotator’s
progress can be constantly monitored, as all rel-
evant interactions of the annotators are logged in a
simple key-value based plain text format.

In Section 2, we detail the main functionalities
of WAT-SL, and we explain its general usage. Sec-
tion 3 then outlines how we customized and used
WAT-SL ourselves in previous work to label over
35,000 argumentative segments in a corpus with
300 news editorials (Al-Khatib et al., 2016).

2 Segment Labeling with WAT-SL

WAT-SL is a ready-to-use and easily customizable
web-based annotation tool that is dedicated to seg-
ment labeling and that puts the focus on easy us-
age for all involved parties: annotators, annotation
curators, and annotation project organizers.

1WAT-SL is available open source under a MIT license at:
https://github.com/webis-de/wat

13



Figure 1: Screenshot of an exemplary task selec-
tion page in WAT-SL, listing three assigned tasks.

In WAT-SL, the annotation process is split into
tasks, usually corresponding to single texts. To-
gether, these tasks form a project.

2.1 Annotating Segments
The annotation interface is designed with a focus
on easy and efficient usage. It can be accessed
with any modern web browser. In order to start,
annotators require only a login and a password.

When logging in, an annotator sees the task se-
lection page that lists all assigned tasks, includ-
ing the annotator’s current progress in terms of
the number of labeled segments (Figure 1). Com-
pleted tasks are marked green, and the web page
automatically scrolls down to the first uncom-
pleted task. This allows annotators to seamless
interrupt and continue the annotation process.

After selecting a task to work on, the annotator
sees the main annotation interface (Figure 2). The
design of the interface seeks for clarity and self-
descriptiveness, following the templates of today’s
most popular framework for responsive web sites,
Bootstrap. As a result, we expect that many anno-
tators will feel familiar with the style.

The central panel of the annotation interface
shows the text to be labeled and its title. One de-
sign objective was to obtain a non-intrusive anno-
tation interface that remains close to just display-
ing the text in order to maximize readability. As
shown in Figure 2, we decided to indicate seg-
ments only by a shaded background and a small
button at the end. To ensure a natural text flow,
line breaks are possible within segments. The but-
ton reveals a menu for selecting the label. When

moving the mouse cursor over a label, the label de-
scription is displayed to prevent a faulty selection.
Once a segment is labeled, its background color
changes, and the button displays an abbreviation
of the respective label. To assist the annotators in
forming a mental model of the annotation inter-
face, the background colors of labeled segments
match the label colors in the menu. All labels are
saved automatically, avoiding any data loss in case
of power outages, connection issues, or similar.

In some cases, texts might be over-segmented,
for example due to an automatic segmentation. If
this is the case, WAT-SL allows annotators to mark
a segment as being continued in the next segment.
The interface will then visually connect these seg-
ments (cf. the buttons showing “->” in Figure 2).

Finally, the annotation interface includes a text
box for leaving comments to the project organiz-
ers. To simplify the formulation of comments,
each segment is numbered, with the number being
shown when the mouse cursor is moved over it.

2.2 Curating Annotations
After an annotation process is completed, a cura-
tion phase usually follows where the annotations
of different annotators are consolidated into one.
The WAT-SL curation interface enables an effi-
cient curation by mimicking the annotation inter-
face with three adjustments (Figure 3): First, seg-
ments for which the majority of annotators agreed
on a label are pre-labeled accordingly. Second, the
menu shows for each label how many annotators
chose it. And third, the label description shows
(anonymized) which annotator chose the label, so
that curators can interpret each label in its context.

The curation may be accessed under the same
URL as the annotation in order to allow annotators
of some tasks being curators of other tasks.

2.3 Running an Annotation Project
WAT-SL is a platform-independent and easily de-
ployable standalone Java application, with few
configurations stored in a simple “key = value”
file. Among others, annotators are managed in this
file by assigning a login, a password, and a set of
tasks to each of them. For each task, the organizer
of an annotation project creates a directory (see
below). WAT-SL uses the directory name as the
task name in all occasions. Once the Java archive
file we provide is then executed, it reads all config-
urations and starts a server. The server is immedi-
ately ready to accept requests from the annotators.

14



Figure 2: Screenshot of the annotation interface of WAT-SL, capturing the annotation of one news edito-
rial within the argumentative segment labeling project described in Section 3. In particular, the screenshot
illustrates how the annotator selects a label (Testimony) for one segment of the news editorial.

Figure 3: Screenshot of the curation interface, il-
lustrating how a label (Anecdote) is selected based
on counts of all labels the annotators selected for
the respective segment (Anecdote (2), No unit (1)).

WAT-SL logs all relevant annotator interactions.
Whenever an annotator changes a segment label,
the new label is immediately sent to the server.
This prevents data loss and allows to monitor the
progress. In addition to the new label, WAT-SL
logs the current date, and the time offset and IP ad-
dress of the annotator. With these logs, researchers
can analyze the annotators’ behavior, e.g., to iden-
tify hard cases where annotators needed much
time or changed the label multiple times.

Annotating Segments Only few steps are needed
to set up the segment labeling tasks: (1) List the la-
bels and their descriptions in the configuration file,
and place the button images in the corresponding
directory. (2) Set the title displayed above each
text (see Figure 2) in a separate configuration file
in the respective task directory, or project-wide in
the same file as the labels. (3) Finally, put the texts
in the task directories. For easy usage, the required
text format is as simple as possible: one segment
per line and empty lines for paragraph breaks. Op-
tionally, organizers can add Cascading Style Sheet
and JavaScript files to customize the interface.

Curating Annotations To curate a task, an or-
ganizer duplicates the annotation task and then
copies the annotation logs into the new task direc-
tory. The organizer then specifies curators for the
curation task analog to assigning annotators.

Result In addition to the logs, the web interface
also allows the organizer to see the final labels
without history in a simple “key = value” format,
which is useful when distributing the annotations.

15



3 Case Study: Labeling Argument Units

WAT-SL was already used successfully in the past,
namely, for creating the Webis-Editorials-16 cor-
pus with 300 news editorials split into a total of
35,665 segments, each labeled by three annotators
and finally curated by the authors (Al-Khatib et
al., 2016). In particular, each segment was as-
signed one of eight labels, where the labels in-
clude six types of argument units (e.g., assump-
tion and anecdote), a label for non-argumentative
segments, and a label indicating that a unit is con-
tinued in the next segment (see the “->” label in
Section 2). On average, each editorial contains
957 tokens in 118 segments.

The annotation project of the Webis-Editorials-
16 corpus included one task per editorial. The edi-
torial’s text had been pre-segmented with a heuris-
tic unit segmentation algorithm before. This algo-
rithm was tuned towards oversegmenting a text in
case of doubt, i.e., to avoid false negatives (seg-
ments that should have been split further) at the
cost of more false positives (segments that need to
be merged). Note that WAT-SL allows fixing such
false positives using “->”.

For annotation, we hired four workers from
the crowdsourcing platform upwork.com. Given
the segmented editorials, each worker iteratively
chose one assigned editorial (see Figure 1), read
it completely, and then selected the appropriate la-
bel for each segment in the editorial (see Figure 2).
This annotation process was repeated for all edito-
rials, with some annotators interrupting their work
on an editorial and returning to it later on. All ed-
itorials were labeled by three workers, resulting in
106,995 annotations in total. The average time per
editorial taken by a worker was ~20 minutes.

To create the final version of the corpus, we
curated each editorial using WAT-SL. In particu-
lar, we automatically kept all labels with majority
agreement, and let one expert decide on the oth-
ers. Also, difficult cases where annotators tend to
disagree were identified in the curation phase.

From the perspective of WAT-SL, the annota-
tion of the Webis-Editorials-16 corpus served as
a case study, which provided evidence that the
tool is easy to learn and master. From the begin-
ning, the workers used WAT-SL without notewor-
thy problems, as far as we could see from monitor-
ing the interaction logs. Also, the comment area
turned out to be useful, i.e., the workers left sev-
eral valuable suggestions and questions there.

4 Conclusion and Future Work

This paper has presented WAT-SL, an open-source
web annotation tool dedicated to segment label-
ing. WAT-SL is designed for easily configuration
and deployment, while allowing project organiz-
ers to tailor its annotation interface to the project
needs using standard web technologies. WAT-SL
aims for simplicity by featuring a self-explanatory
interface that does not distract from the text to be
annotated, as well as by always showing the an-
notation progress at a glance and saving it auto-
matically. The curation of the annotations can be
done using exactly the same interface. In case of
oversegmented texts, annotators can merge seg-
ments. Moreover, all relevant annotator interac-
tions are logged, allowing projects organizers to
monitor and analyze the annotation process.

Recently, WAT-SL was used successfully on
a corpus with 300 news editorials, where four
remote annotators labeled 35,000 argumentative
segments. In the future, we plan to create more
dedicated annotation tools in the spirit of WAT-SL
for other annotation types (e.g., relation identifica-
tion). In particular, we plan to improve the anno-
tator management functionalities of WAT-SL and
reuse them for these other tools.

References
Khalid Al-Khatib, Henning Wachsmuth, Johannes

Kiesel, Matthias Hagen, and Benno Stein. 2016.
A News Editorial Corpus for Mining Argumenta-
tion Strategies. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics,
pages 3433–3443.

Hamish Cunningham. 2002. GATE, a General Ar-
chitecture for Text Engineering. Computers and the
Humanities, 36(2):223–254.

Pontus Stenetorp, Sampo Pyysalo, Goran Topić,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. BRAT: A Web-based Tool for NLP-
assisted Text Annotation. In Proceedings of the
Demonstrations at the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 102–107.

Seid Muhie Yimam, Chris Biemann, Richard Eckart de
Castilho, and Iryna Gurevych. 2014. Automatic
Annotation Suggestions and Custom Annotation
Layers in WebAnno. In Proceedings of 52nd An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 91–96.

16


