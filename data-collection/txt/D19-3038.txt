



















































Tanbih: Get To Know What You Are Reading


Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 223–228
Hong Kong, China, November 3 – 7, 2019. c©2019 Association for Computational Linguistics

223

Tanbih: Get To Know What You Are Reading
Yifan Zhang1, Giovanni Da San Martino1, Alberto Barrón-Cedeño2, Salvatore Romeo1,

Jisun An1, Haewoon Kwak1, Todor Staykovski3, Israa Jaradat4, Georgi Karadzhov5,
Ramy Baly6, Kareem Darwish1, James Glass6, Preslav Nakov1

1Qatar Computing Research Institute, HBKU, 2Università di Bologna, Forlı̀, Italy
3Sofia University 4University of Texas at Arlington, 5SiteGround Hosting EOOD

6MIT Computer Science and Artificial Intelligence Laboratory
{yzhang,gmartino,sromeo,jan,hkwak,kdarwish,pnakov}@hbku.edu.qa
a.barron@unibo.it, todorstaykovski@gmail.com, {baly,glass}@mit.edu

israa.jaradat@mavs.uta.edu, gogokaradjov@gmail.com

Abstract

We introduce Tanbih, a news aggregator with
intelligent analysis tools to help readers un-
derstanding what is behind a news story. Our
system displays news grouped into events and
generates media profiles that show the general
factuality of reporting, the degree of propa-
gandistic content, hyper-partisanship, leading
political ideology, general frame of reporting,
and stance with respect to various claims and
topics of a news outlet. In addition, we auto-
matically analyze each article in order to detect
whether it is propagandistic and to determine
its stance with respect to a number of contro-
versial topics.

1 Introduction

Nowadays, more and more readers consume news
online. The reduced costs and, generally speak-
ing, the less strict regulations with respect to the
standard press, have led to the proliferation of on-
line news media. However, this does not necessar-
ily entail that readers are exposed to a plurality of
viewpoints as news consumed via social networks
are known to reinforce the bias of the user (Flax-
man et al., 2016) because of filtering bubbles and
echo chambers. Moreover, visiting multiple web-
sites to gather a more comprehensive analysis of
an event might be too time-consuming for the av-
erage reader.

News aggregators —such as Flipboard1, News
Lens2 and Google News3—, gather news from dif-
ferent sources and, in the case of the latter two,
cluster them into events. In addition, News Lens
displays all articles about an event in a timeline
and provides additional information, such as sum-
mary of the event and a description for each entity
mentioned in an article.

1http://flipboard.com
2http://newslens.berkeley.edu
3http://news.google.com.

While these news aggregators help readers to get
a more comprehensive coverage of an event, some
of the sources might be unknown to the user, and
thus he/she could naturally question the validity
and the trustworthiness of the information pro-
vided. Deep analysis of the content published by
news outlets has been performed by expert jour-
nalists. For example, Media Bias/Fact Check4 pro-
vides reports on the bias and factuality of report-
ing of entire news outlets, whereas Snopes5, Poli-
tifact,6 and FactCheck7 are popular fact-checking
websites. All these manual efforts cannot cope
with the rate at which news contents are produced.

Here, we propose Tanbih, a news platform that
displays news grouped into events and provides
additional information about the articles and their
media source in order to promote media literacy. It
automatically generates profiles for the news me-
dia with reports on their factuality, leading po-
litical ideology, hyper-partisanship, use of propa-
ganda, and bias. Furthermore, Tanbih automati-
cally categorizes articles in English and Arabic,
flags potentially propagandistic ones, and exam-
ines their framing bias.

2 System Architecture

The architecture of Tanbih is sketched in Figure 1.
The system consists of three main components: an
online streaming processing pipeline for data col-
lection and article level analysis, offline process-
ing for event and media source level analysis, and
a website for delivering news to the users. The
online streaming processing pipeline continuously
retrieves articles in English and Arabic, which are
then translated, categorized, and analyzed for their
general frame of reporting and use of propaganda.

4http://mediabiasfactcheck.com
5http://www.snopes.com/
6http://www.politifact.com/
7http://www.factcheck.org/

http://flipboard.com
http://newslens.berkeley.edu
http://news.google.com
http://mediabiasfactcheck.com
http://www.snopes.com/
http://www.politifact.com/
http://www.factcheck.org/


224

Figure 1: The architecture of Tanbih. The arrows indicate the information flow.

We perform clustering on the articles that have
been collected every 30 minutes. The offline
processing includes factuality prediction, leading
political ideology detection, audience reach and
Twitter user based bias prediction at the media
level, and stance detection and aggregation of
statistics at the article level, e.g., propaganda index
(see Section 2.3) for each news medium. The of-
fline processing does not have strict time require-
ments, and thus the choice of the models we de-
velop favors accuracy over speed.

In order to run everything in a streaming and
scalable fashion, we use KAFKA8 as a messaging
queue and Kubernetes9, thus ensuring scalability
and fault-tolerant deployment. In the following,
we describe each component of the system. We
have open-sourced the code for some of those, and
we plan to do that for the remaining ones in the
near future.

2.1 Crawlers and Translation
Our crawlers collect articles from a growing list
of sources10, which currently includes 155 RSS
feeds, 82 Twitter accounts and two websites. Once
a link to an article has been obtained from any of
these sources, we rely on the Newspaper3k Python
library to extract its contents.11 After dedupli-
cation based on both URL and text content, our
crawlers currently download 7k-10k articles per
day. As of present, we have more than 700k ar-
ticles stored in our database. We use QCRI’s Ma-
chine Translation (Dalvi et al., 2017) to translate
English content into Arabic and vice versa. Since
translation is performed offline, we select the most
accurate system in Dalvi et al. (2017), i.e., the
neural-based one.

8http://kafka.apache.org
9http://kubernetes.io

10http://www.tanbih.org/about
11http://newspaper.readthedocs.io

2.2 Section Categorization

We built a model to classify an article into one
of six news sections: Entertainment, Sports, Busi-
ness, Technology, Politics, and Health. We built a
corpus using the New York Times articles from the
FakeNews dataset12 published between January 1,
2000 and December 31, 2017. We extracted the
news section information embedded in the arti-
cle URL and we used a total of 538k articles for
training our models using TF.IDF representation.
On a test set of 107k articles, our best-performing
logistic regression model achieved F1 scores of
0.82, 0.58, 0.80, and 0.90 for Sports, Business,
Technology, and Politics, respectively. The overall
F1 for the baseline was 0.497.

2.3 Propaganda Detection

We developed a propaganda detection component
to flag articles that could be potentially propa-
gandistic, i.e., purposefully biased to influence its
readers and ultimately to pursue a specific agenda.
Given a corpus of news that is labelled as pro-
pagandistic/non propagandistic (Barrón-Cedeño
et al., 2019), we train a maximum entropy classi-
fier on 51k articles, represented with various style-
related features, such as character n-grams and
a number of vocabulary richness and readability
measures, and we obtain state-of-the-art F1=82.89
on a separate test set of 10k articles. We refer to
the score p ∈ [0, 1] of the classifier as propaganda
index, and we define the following propaganda la-
bels, which we use to flag articles (see Figure 2;
right news): very unlikely (p < 0.2), unlikely
(0.2 ≤ p < 0.4), somehow (0.4 ≤ p < 0.6),
likely (0.6 ≤ p < 0.8), and very likely (p ≥ 0.8).

12http://github.com/several27/
FakeNewsCorpus

http://kafka.apache.org
http://kubernetes.io
http://www.tanbih.org/about
http://newspaper.readthedocs.io
http://github.com/several27/FakeNewsCorpus
http://github.com/several27/FakeNewsCorpus


225

2.4 Framing Bias Detection

Framing is a central concept in political communi-
cation, which intentionally emphasizes or ignores
certain dimensions of an issue (Entman, 1993).
In Tanbih, we infer the frames of news articles,
thus making them explicit. In particular, we use
the Media Frames Corpus (MFC) (Card et al.,
2015) to train a fine-tuned BERT model to detect
topic-agnostic media frames. For training, we use
a small learning rate of 0.0002, a maximum se-
quence length of 128, and a batch size of 32. Our
model, when trained on 11k articles from MFC,
achieved an accuracy of 66.7% on a test set of
1,138 articles. This is better than the previously
reported state-of-the-art (58.4%) on a subset of
MFC (Ji and Smith, 2017).

2.5 Factuality of Reporting and Leading
Political Ideology of a Source

The factuality of reporting and the bias of an in-
formation source are key indicators that investiga-
tive journalists use to judge the reliability of in-
formation. In Tanbih, we model the factuality
and the bias at the media level, learning from the
Media Bias/Fact Check (MBFC) website, which
covers over 2,800 news outlets. The model im-
proves over our recent research (Baly et al., 2018,
2019; Dinkov et al., 2019), and combines informa-
tion from articles published by the target medium,
from their Wikipedia page accounts, from their so-
cial media accounts (Twitter, Facebook, Youtube)
as well as from the social media accounts of the
users who interact with the medium. We model
factuality on a 3-point scale (low, mixed and high),
with 80.1% accuracy (baseline 46.0%), and bias
on a 7-point left-to-right scale, with 69% accuracy
(baseline 24.7%), and also on a 3-point scale, with
81.9% accuracy (baseline 37.1%).

2.6 Stance Detection

Stance detection aims to identify the relative per-
spective of a piece of text with respect to a claim,
typically modeled using labels such as agree, dis-
agree, discuss, and unrelated. An interesting ap-
plication of stance detection is medium profiling
with respect to controversial topics. In this setting,
given a particular medium, the stance for each arti-
cle is computed with respect to a set of predefined
claims. The stance of a medium is then obtained
by aggregating the article-level stances. In Tanbih,
the stance is used to profile media sources.

We implemented our stance detection model as
fine-tuning of BERT on the FNC-1 dataset from
the Fake News Challenge13. Our model outper-
formed the best submitted system (Hanselowski
et al., 2018), obtaining an F1macro of 75.30 and
an F1 of 69.61, 49.76, 83.01, and 98.81 for agree,
disagree, discuss, and unrelated, respectively.

2.7 Audience Reach
User interactions on Facebook enable the platform
to generate comprehensive user profiles for gen-
der, age, income bracket, and political preferences.
After marketers have determined a set of criteria
for their target audience, Facebook can provide
them with an estimate of the size of this audience
on its platform. As an illustration, there are about
160K Facebook users who are 20 years old, are
very liberal, are female, and have an interest in
The New York Times. In Tanbih, we use the polit-
ical leaning of Facebook users who follow a news
medium as a feature to potentially improve media
bias and factuality prediction; we also show it in
the media profiles. To get the audience of each
news medium, we use Facebook’s Marketing API
to extract the demographic data of the medium’s
audience with a focus on audience members who
reside in USA and their political leanings (ideol-
ogy): (Very Conservative, Conservative, Moder-
ate, Liberal, and Very Liberal).14

2.8 Twitter User-Based Bias Classification
Controversial social and political issues may spur
social media users to express their opinion through
sharing supporting newspaper articles. Our intu-
ition is that the bias of news sources can be in-
ferred based on the bias of social media users.
For example, if articles from a news source are
strictly shared by left- or right-leaning users, then
the source is likely left- or right-leaning, respec-
tively. Similarly, if it is being cited by both groups,
then it is likely closer to the center. We used an un-
supervised user-based stance detection method on
different controversial topics in order to find core
groups of right- and left-leaning users (Darwish
et al., 2019). Given that the stance detection pro-
duces clusters with nearly perfect purity (> 97%
purity), we used the identified core users to train a
deep learning-based classifier, fastText, using the
accounts that they retweeted as features to further
tag more users.

13http://www.fakenewschallenge.org/
14These are only available for US-based Facebook users.

http://www.fakenewschallenge.org/


226

Next, we computed a valence score for each news
outlet and for each topic. The valence scores range
between -1 and 1, with higher absolute values in-
dicating being cited with greater proportion by one
group as opposed to the other. The score is calcu-
lated as follows (Conover et al., 2011):

V (u) = 2

tf(u,C0)
total(C0)

tf(u,C0)
total(C0)

+ tf(u,C1)total(C1)

− 1 (1)

where tf(u,C0) is the number of times (term
frequency) item u is cited by group C0, and
total(C0) is the sum of the term frequencies of all
items cited by C0. tf(u,C1) and total(C1) are de-
fined in a similar fashion. We subdivided the range
between -1 and 1 into 5 equal size ranges and we
assigned the labels far-left, left, center, right, and
far-right to those ranges.

2.9 Event Identification / Clustering
The clustering module aggregates news articles
into stories. The pipeline is divided into two
stages: (i) local topic identification and (ii) long-
term topic matching for story generation.

For step (i), we represent each article as a
TF.IDF vector, built from the title and the body
concatenated. The pre-processing consists of
casefolding, lemmatization, punctuation and stop-
word removal. In order to obtain the preliminary
clusters, in stage (i), we compute the cosine simi-
larity between all article pairs in a predefined time
window. We set n = 6 as the number of days
withing a window with an overlap of three days.
The resulting matrix of similarities for each win-
dow is then used to build a graph G = (V,E),
where V is the set of vertices, i.e., the news ar-
ticles, and E is the set of edges. An edge be-
tween two articles {di, dj} ∈ V is drawn only if
sim(di, dj) ≥ T1, with T1 = 0.31. We selected
all parameters empirically on the training part of
the corpus from (Miranda et al., 2018). The se-
quence of overlapping local graphs is merged in
the order of their creation, thus generating sto-
ries from the topics. After merging, a commu-
nity detection algorithm is used in order to find
the correct assignment of the nodes into clusters.
We used one of the fastest modularity-based algo-
rithms: the Louvain method (Blondel et al., 2008).

For step (ii), the topics created from the pre-
ceding stage are merged if the cosine similarity
sim(ti, tj) ≥ T2, where ti (tj) is the mean of all
vectors belonging to topic i (j), with T2 = 0.8.

Figure 2: The Tanbih main page.

The model achieved state-of-the-art perfor-
mance on the testing partition of the corpus
from Miranda et al. (2018): an F1 of 98.11 and
an F1BCubed of 94.41.

15 As a comparison, the best
model described in (Miranda et al., 2018) achieved
an F1 of 94.1. See Staykovski et al. (2019) for
further details.

3 Interface

The home page of Tanbih16 displays news articles
grouped into stories (see the screenshot in Fig-
ure 2). Each story is displayed as a card. The
users can go back and forth between the articles
from the same event by clicking on the left/right
arrows below the title of the article. A propaganda
label is displayed if the article is predicted to be
likely propagandistic. Such an example is shown
on the right of Figure 2. The source of each article
is displayed with the logo or the avatar of the re-
spective news organization, and it links to a special
profile page for this organization (see Figure 3).
On the top-left of the home page, Tanbih provides
language selection buttons, currently English and
Arabic only, to switch the language the news are
display in. Finally, a search box in the top-right
corner allows the user to find the profile page of a
particular news medium of interest.

On the media profile page (Figure 3a), a short
extract from the Wikipedia page of the medium
is displayed on the top, with recently published
articles on the right-hand side. The profile page
includes a number of statistics automatically de-
rived from the models in Section 2. We use as an
example Figure 3, which shows screenshots of the
profile of CNN.17

15F1BCubed is an evaluation measure specifically designed
to evaluate clustering algorithms (Amigó et al., 2009).

16http://www.tanbih.org
17Here is a direct link to the profile: http://www.

tanbih.org/media/1

http://www.tanbih.org
http://www.tanbih.org/media/1
http://www.tanbih.org/media/1


227

(a) (b) (c)

Figure 3: A partial screenshot of the media profile page for CNN in Tanbih.

The first two charts in Figure 3a show the cen-
trality and the hyper-partisanship (we can see that
CNN is estimated to be fairly central and low
in hyper-partisanship) and the distribution of pro-
pagandistic articles (CNN publishes mostly non-
propagandistic articles).

Figure 3b shows the overall framing bias distri-
bution for the medium (CNN focuses mostly on
cultural identity and politics), and the factuality
of reporting (CNN is mostly factual). The pro-
file also shows the leading political ideology of the
medium on a 3-point and also on a 7-point scale.

Figure 3c shows the audience reach of the
medium and the bias classification according to
users’ retweets (see Section 2.8). We can see that
CNN is popular among readers with all political
views, although it tends to have a left-leaning ide-
ology on the topics listed. The profile also fea-
tures reports on the stance of CNN with respect to
a number of topics.

Finally, Tanbih features pages about specific
topics. These are accessible via the search box on
the top-right of Tanbih’s main page. An example is
given in Figure 4, which shows the topic page for
the Khashoggi’s murder. Recent stories about this
topic are listed on the top of the page, followed by
statistics such as the number of countries, the num-
ber of articles, and the number of media reporting
on it. A map shows how much reporting there is
on the event per country, which allows users to get
an idea of how important the topic is there.

Figure 4: A partial screenshot of the topic page for the
Khashoggi Murder in Tanbih.

The topic page further features charts showing
(i) the top countries in terms of coverage of the
event, both in absolute and in relative numbers
with respect to the total number of articles pub-
lished, and (ii) the media sources that published
most propagandistic content on the topic, again
both in absolute and in relative terms with respect
to the total number of articles published by the re-
spective medium on the topic. Finally, the topic
page displays plots showing the overall distribu-
tion of propagandistic articles and of the overall
framing bias when reporting on the topic.



228

4 Conclusions and Future Work

We have introduced Tanbih, a news aggregator that
performs media-level and article-level analysis of
the news aiming to help users better understand
what they are reading. Tanbih features factual-
ity prediction, propaganda detection, stance detec-
tion, leading political ideology identification, me-
dia framing bias detection, event clustering, and
machine translation.

In future work, we plan to include more me-
dia sources, especially from non-English speak-
ing regions and to add interactive components,
e.g., letting users ask a question about a spe-
cific topic. We also plan to add sentence-level
and sub-sentence-level annotations for check-
worthiness (Jaradat et al., 2018) and fine-grained
propaganda (Da San Martino et al., 2019).

Acknowledgements

This research is part of the Tanbih project,18 which
aims to limit the effect of “fake news”, propaganda
and media bias by making users aware of what
they are reading. It is developed in collaboration
between the Qatar Computing Research Institute
(QCRI), HBKU and the MIT Computer Science
and Artificial Intelligence Laboratory (CSAIL).

References
Enrique Amigó, Julio Gonzalo, Javier Artiles, and

Felisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Inf. Retr., 12(4):461–486.

Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov,
James Glass, and Preslav Nakov. 2018. Predict-
ing factuality of reporting and bias of news media
sources. In Proc. of EMNLP’18, pages 3528–3539.

Ramy Baly, Georgi Karadzhov, Abdelrhman Saleh,
James Glass, and Preslav Nakov. 2019. Multi-task
ordinal regression for jointly predicting the trustwor-
thiness and the leading political ideology of news
media. In Proc. of NAACL-HLT’19, pages 2109–
2116.

Alberto Barrón-Cedeño, Giovanni Da San Martino, Is-
raa Jaradat, and Preslav Nakov. 2019. Proppy: Or-
ganizing news coverage on the basis of their propa-
gandistic content. Information Processing and Man-
agement, 56(5):1849–1864.

Vincent D Blondel, Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal

18http://tanbih.qcri.org/

of Statistical Mechanics: Theory and Experiment,
2008(10):P10008.

Dallas Card, Amber E Boydstun, Justin H Gross, Philip
Resnik, and Noah A Smith. 2015. The media frames
corpus: Annotations of frames across issues. In
Proc. of ACL ’15, pages 438–444.

Michael Conover, Jacob Ratkiewicz, Matthew R Fran-
cisco, Bruno Gonçalves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on Twitter. In Proc. of ICWSM’11, pages 89–96.

Giovanni Da San Martino, Seunghak Yu, Alberto
Barron-Cedeno, Rostislav Petrov, and Preslav
Nakov. 2019. Fine-grained analysis of propaganda
in news articles. In Proc. of EMNLP’19.

Fahim Dalvi, Yifan Zhang, Sameer Khurana, Nadir
Durrani, Hassan Sajjad, Ahmed Abdelali, Hamdy
Mubarak, Ahmed Ali, and Stephan Vogel. 2017.
QCRI live speech translation system. In Proc. of
EACL’17, pages 61–64.

Kareem Darwish, Peter Stefanov, Michaël J Aupetit,
and Preslav Nakov. 2019. Unsupervised user stance
detection on Twitter. In Proc. of ICWSM’20.

Yoan Dinkov, Ahmed Ali, Ivan Koychev, and Preslav
Nakov. 2019. Predicting the leading political ide-
ology of Youtube channels using acoustic, textual
and metadata information. In Proc. of INTER-
SPEECH’19.

Robert M Entman. 1993. Framing: Toward clarifica-
tion of a fractured paradigm. Journal of communi-
cation, 43(4):51–58.

Seth Flaxman, Sharad Goel, and Justin M Rao. 2016.
Filter bubbles, echo chambers, and online news con-
sumption. Public opinion quarterly, 80(S1):298–
320.

Andreas Hanselowski, Avinesh P.V.S., Benjamin
Schiller, Felix Caspelherr, Debanjan Chaudhuri,
Christian M. Meyer, and Iryna Gurevych. 2018. A
retrospective analysis of the fake news challenge
stance-detection task. In Proc. of COLING’18,
pages 1859–1874.

Israa Jaradat, Pepa Gencheva, Alberto Barrón-Cedeño,
Lluı́s Màrquez, and Preslav Nakov. 2018. Claim-
Rank: Detecting check-worthy claims in Arabic and
English. In Proc. of NAACL-HLT ’18, pages 26–30.

Yangfeng Ji and Noah A. Smith. 2017. Neural dis-
course structure for text categorization. In Proc.
ACL’17, pages 996–1005.

Sebastião Miranda, Arturs Znotins, Shay B. Cohen,
and Guntis Barzdins. 2018. Multilingual clustering
of streaming news. In Proc. of EMNLP’18, pages
4535–4544.

Todor Staykovski, Alberto Barrón-Cedeño, Giovanni
Da San Martino, and Preslav Nakov. 2019. Dense
vs. sparse representations for news stream cluster-
ing. In Proc. of Text2story’19, pages 47–52.

http://tanbih.qcri.org/

