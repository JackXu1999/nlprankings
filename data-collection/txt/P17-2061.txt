



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 385–391
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2061

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 385–391
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2061

An Empirical Comparison of Domain Adaptation Methods for
Neural Machine Translation

Chenhui Chu1∗, Raj Dabre2, and Sadao Kurohashi2
1Institute for Datability Science, Osaka University
2Graduate School of Informatics, Kyoto University

chu@ids.osaka-u.ac.jp, {raj, kuro}@nlp.ist.i.kyoto-u.ac.jp

Abstract

In this paper, we propose a novel do-
main adaptation method named “mixed
fine tuning” for neural machine transla-
tion (NMT). We combine two existing ap-
proaches namely fine tuning and multi do-
main NMT. We first train an NMT model
on an out-of-domain parallel corpus, and
then fine tune it on a parallel corpus which
is a mix of the in-domain and out-of-
domain corpora. All corpora are aug-
mented with artificial tags to indicate spe-
cific domains. We empirically compare
our proposed method against fine tuning
and multi domain methods and discuss its
benefits and shortcomings.

1 Introduction

One of the most attractive features of neural ma-
chine translation (NMT) (Bahdanau et al., 2015;
Cho et al., 2014; Sutskever et al., 2014) is that it
is possible to train an end to end system without
the need to deal with word alignments, translation
rules and complicated decoding algorithms, which
are a characteristic of statistical machine transla-
tion (SMT) systems. However, it is reported that
NMT works better than SMT only when there is an
abundance of parallel corpora. In the case of low
resource domains, vanilla NMT is either worse
than or comparable to SMT (Zoph et al., 2016).

Domain adaptation has been shown to be effec-
tive for low resource NMT. The conventional do-
main adaptation method is fine tuning, in which
an out-of-domain model is further trained on in-
domain data (Luong and Manning, 2015; Sen-
nrich et al., 2016b; Servan et al., 2016; Freitag
and Al-Onaizan, 2016). However, fine tuning

∗This work was done when the first author was a re-
searcher of Japan Science and Technology Agency.

tends to overfit quickly due to the small size of
the in-domain data. On the other hand, multi do-
main NMT (Kobus et al., 2016) involves training
a single NMT model for multiple domains. This
method adds tags “<2domain>” to the source sen-
tences in the parallel corpora to indicate domains
without any modifications to the NMT system ar-
chitecture. However, this method has not been
studied for domain adaptation in particular.

Motivated by these two lines of studies, we
propose a new domain adaptation method called
“mixed fine tuning,” where we first train an NMT
model on an out-of-domain parallel corpus, and
then fine tune it on a parallel corpus that is a
mix of the in-domain and out-of-domain corpora.
Fine tuning on the mixed corpus instead of the in-
domain corpus can address the overfitting prob-
lem. All corpora are augmented with artificial tags
to indicate specific domains. We tried two dif-
ferent corpora settings on two different language
pairs:

• Manually created resource poor corpus
(Chinese-to-English translation): Using the
NTCIR data (patent domain; resource rich)
(Goto et al., 2013) to improve the translation
quality for the IWSLT data (TED talks; re-
source poor) (Cettolo et al., 2015).

• Automatically extracted resource poor cor-
pus (Chinese-to-Japanese translation): Using
the ASPEC data (scientific domain; resource
rich) (Nakazawa et al., 2016) to improve the
translation quality for the Wiki data (resource
poor). The parallel corpus of the latter do-
main was automatically extracted (Chu et al.,
2016a).

We observed that “mixed fine tuning” works sig-
nificantly better than methods that use fine tuning

385

https://doi.org/10.18653/v1/P17-2061
https://doi.org/10.18653/v1/P17-2061


Source-Target  
(out-of-domain) NMT 

Model 
(out-of-domain) 

Source-Target   
(in-domain) NMT 

Model  
(in-domain) 

Init
ializ

e 

Figure 1: Fine tuning for domain adaptation.

and domain tag based approaches separately. Our
contributions are twofold:

• We propose a novel method that combines the
best of existing approaches and show that it is
effective.

• To the best of our knowledge this is the first
work on an empirical comparison of various
domain adaptation methods.

2 Related Work

Fine tuning has also been explored for various
NLP tasks using neural networks such as sen-
timent analysis and paraphrase detection (Mou
et al., 2016). Tag based NMT has also been shown
to be effective for control the politeness of trans-
lations (Sennrich et al., 2016a) and multilingual
NMT (Johnson et al., 2016).

Besides fine tuning and multi domain NMT us-
ing tags, another direction of domain adaptation
for NMT is using in-domain monolingual data. Ei-
ther training an in-domain recurrent neural net-
work (RNN) language model for the NMT de-
coder (Gülçehre et al., 2015) or generating syn-
thetic data by back translating target in-domain
monolingual data (Sennrich et al., 2016b) have
been studied.

3 Methods for Comparison

All the methods that we compare are simple and
do not need any modifications to the NMT system.

3.1 Fine Tuning
Fine tuning is the conventional way for domain
adaptation, and thus serves as a baseline in this
study. In this method, we first train an NMT sys-
tem on a resource rich out-of-domain corpus till
convergence, and then fine tune its parameters on
a resource poor in-domain corpus (Figure 1).

3.2 Multi Domain
The multi domain method is originally motivated
by (Sennrich et al., 2016a), which uses tags to

control the politeness of NMT translations. The
overview of this method is shown in the dotted
section in Figure 2. In this method, we simply con-
catenate the corpora of multiple domains with two
small modifications:

• Appending the domain tag “<2domain>” to
the source sentences of the respective cor-
pora.1 This primes the NMT decoder to gen-
erate sentences for the specific domain.

• Oversampling the smaller corpus so that the
training procedure pays equal attention to
each domain.

We can further fine tune the multi domain model
on the in-domain data, which is named as “multi
domain + fine tuning.”

3.3 Mixed Fine Tuning
The proposed mixed fine tuning method is a com-
bination of the above methods (shown in Figure
2). The training procedure is as follows:

1. Train an NMT model on out-of-domain data
till convergence.

2. Resume training the NMT model from step
1 on a mix of in-domain and out-of-domain
data (by oversampling the in-domain data) till
convergence.

By default, we utilize domain tags, but we also
consider settings where we do not use them (i.e.,
“w/o tags”). We can further fine tune the model
from step 2 on the in-domain data, which is named
as “mixed fine tuning + fine tuning.”

Note that in the “fine tuning” method, the vo-
cabulary obtained from the out-of-domain data is
used for the in-domain data; while for the “multi
domain” and “mixed fine tuning” methods, we use
a vocabulary obtained from the mixed in-domain
and out-of-domain data for all the training stages.
Regarding development data, for fine tuning, an
out-of-domain development set is first used for
training the out-of-domain NMT model, then an
in-domain development set is used for fine tuning;
For multi-domain, a mix of in-domain and out-of-
domain development sets are used; For mixed fine
tuning, an out-of-domain development set is first
used for training the out-of-domain NMT model,
then a mix of in-domain and out-of-domain devel-
opment sets are used for mixed fine tuning.

1We verified the effectiveness of the domain tags by com-
paring against a setting that does not use them, see the “w/o
tags” settings in Tables 1 and 2.

386



NMT Model 

(mixed) 
merge 

Append in-domain 

tag (<2in-domain>) 

NMT Model  

(out-of-domain) 

Ini
tia

liz
e 

NMT Model 

(in-domain) 

In
iti

al
iz

e 

Append out-of-domain  

tag (<2out-of-domain>) 

Oversample the smaller  

in-domain corpus 

Source-Target  

(out-of-domain) 

Source-Target 

(in-domain) 

Figure 2: Mixed fine tuning with domain tags for domain adaptation (The section in the dotted rectangle
denotes the multi domain method).

4 Experimental Settings

We conducted NMT domain adaptation experi-
ments in two different settings as follows:

4.1 High Quality In-domain Corpus Setting

Chinese-to-English translation was the focus of
the high quality in-domain corpus setting. We uti-
lized the resource rich patent out-of-domain data
to augment the resource poor spoken language in-
domain data. The patent domain MT was con-
ducted on the Chinese-English subtask (NTCIR-
CE) of the patent MT task at the NTCIR-10 work-
shop2 (Goto et al., 2013). The NTCIR-CE task
uses 1M, 2k, and 2k sentences for training, devel-
opment, and testing, respectively. The spoken do-
main MT was conducted on the Chinese-English
subtask (IWSLT-CE) of the TED talk MT task at
the IWSLT 2015 workshop (Cettolo et al., 2015).
The IWSLT-CE task contains 209,491 sentences
for training. We used the dev 2010 set for devel-
opment, containing 887 sentences. We evaluated
all methods on the 2010, 2011, 2012, and 2013
test sets, containing 1570, 1245, 1397, and 1261
sentences, respectively.

4.2 Low Quality In-domain Corpus Setting

Chinese-to-Japanese translation was the focus of
the low quality in-domain corpus setting. We
utilized the resource rich scientific out-of-domain
data to augment the resource poor Wikipedia (es-
sentially open) in-domain data. The scientific do-
main MT was conducted on the Chinese-Japanese
paper excerpt corpus (ASPEC-CJ)3 (Nakazawa
et al., 2016), which is one subtask of the workshop
on Asian translation (WAT)4 (Nakazawa et al.,

2http://ntcir.nii.ac.jp/PatentMT-2/
3http://lotus.kuee.kyoto-u.ac.jp/ASPEC/
4http://orchid.kuee.kyoto-u.ac.jp/WAT/

2015). The ASPEC-CJ task uses 672315, 2090,
and 2107 sentences for training, development, and
testing, respectively. The Wikipedia domain task
was conducted on a Chinese-Japanese corpus au-
tomatically extracted from Wikipedia (WIKI-CJ)
(Chu et al., 2016a) using the ASPEC-CJ corpus as
a seed. The WIKI-CJ task contains 136013, 198,
and 198 sentences for training, development, and
testing, respectively.

4.3 MT Systems

For NMT, we used the KyotoNMT system5

(Cromieres et al., 2016). The NMT settings were
the same as (Cromieres et al., 2016) except that
we used a vocabulary size of 32k for all the ex-
periments, and did not ensemble independently
trained parameters. The sizes of the source and
target vocabularies, the source and target side em-
beddings, the hidden states, the attention mecha-
nism hidden states, and the deep softmax output
with a 2-maxout layer were set to 32,000, 620,
1000, 1000, and 500, respectively. We used 2-
layer LSTMs for both the source and target sides.
ADAM was used as the learning algorithm, with
a dropout rate of 20% for the inter-layer dropout,
and L2 regularization with a weight decay coef-
ficient of 1e-6. The mini batch size was 64, and
sentences longer than 80 tokens were discarded.
We early stopped the training process when we
observed that the BLEU score of the development
set converges. For testing, we ensembled the three
parameters of the best development loss, the best
development BLEU, and the final parameters in a
single training run. Beam size was set to 100. The
maximum length of the translation was set to 2,
and 1.5 times of the source sentences for Chinese-
to-English, and Chinese-to-Japanese, respectively.

5https://github.com/fabiencro/knmt

387



IWSLT-CE
System NTCIR-CE test 2010 test 2011 test 2012 test 2013 average
IWSLT-CE SMT - 12.73 16.27 14.01 14.67 14.31
IWSLT-CE NMT - 6.75 9.08 9.05 7.29 7.87
NTCIR-CE SMT 29.54 3.57 4.70 4.21 4.74 4.33
NTCIR-CE NMT 37.11 2.23 2.83 2.55 2.85 2.60
Fine tuning 17.37 13.93 18.99 16.12 17.12 16.41
Multi domain 36.40 13.42 19.07 16.56 17.54 16.34
Multi domain w/o tags 37.32 12.57 17.40 15.02 15.96 14.97
Multi domain + Fine tuning 14.47 13.18 18.03 16.41 16.80 15.82
Mixed fine tuning 37.01 15.04 20.96 18.77 18.63 18.01
Mixed fine tuning w/o tags 39.67 14.47 20.53 18.10 17.97 17.43
Mixed fine tuning + Fine tuning 32.03 14.40 19.53 17.65 17.94 17.11

Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE.

For performance comparison, we also con-
ducted experiments on phrase based SMT (PB-
SMT). We used the Moses PBSMT system (Koehn
et al., 2007) for all of our MT experiments. For
the respective tasks, we trained 5-gram language
models on the target side of the training data us-
ing the KenLM toolkit6 with interpolated Kneser-
Ney discounting, respectively. In all of our ex-
periments, we used the GIZA++ toolkit7 for word
alignment; tuning was performed by minimum er-
ror rate training (Och, 2003), and it was re-run for
every experiment.

For both MT systems, we preprocessed the data
as follows. For Chinese, we used KyotoMorph8

for segmentation, which was trained on the CTB
version 5 (CTB5) and SCTB (Chu et al., 2016b).
For English, we tokenized and lowercased the sen-
tences using the tokenizer.perl script in Moses.
Japanese was segmented using JUMAN9 (Kuro-
hashi et al., 1994).

For NMT, we further split the words into sub-
words using byte pair encoding (BPE) (Sennrich
et al., 2016c), which has been shown to be effec-
tive for the rare word problem in NMT. Another
motivation of using sub-words is making the dif-
ferent domains share more vocabulary, which is
important especially for the resource poor domain.
For the Chinese-to-English tasks, we trained two
BPE models on the Chinese and English vocabu-
laries, respectively. For the Chinese-to-Japanese
tasks, we trained a joint BPE model on both of the
Chinese and Japanese vocabularies, because Chi-
nese and Japanese could share some vocabularies
of Chinese characters. The number of merge op-
erations was set to 30k for all the tasks.

6https://github.com/kpu/kenlm/
7http://code.google.com/p/giza-pp
8https://bitbucket.org/msmoshen/kyotomorph-beta
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN

System ASPEC-CJ WIKI-CJ
WIKI-CJ SMT - 36.83
WIKI-CJ NMT - 18.29
ASPEC-CJ SMT 36.39 17.43
ASPEC-CJ NMT 42.92 20.01
Fine tuning 22.10 37.66
Multi domain 42.52 35.79
Multi domain w/o tags 40.78 33.74
Multi domain + Fine tuning 22.78 34.61
Mixed fine tuning 42.56 37.57
Mixed fine tuning w/o tags 41.86 37.23
Mixed fine tuning + Fine tuning 31.63 37.77

Table 2: Domain adaptation results (BLEU-4
scores) for WIKI-CJ using ASPEC-CJ.

5 Results

Tables 1 and 2 show the translation results on
the Chinese-to-English and Chinese-to-Japanese
tasks, respectively. The entries with SMT and
NMT are the PBSMT and NMT systems, respec-
tively; others are the different methods described
in Section 3. In both tables, the numbers in bold
indicate the best system and all systems that were
not significantly different from the best system.
The significance tests were performed using the
bootstrap resampling method (Koehn, 2004) at
p < 0.05.

We can see that without domain adaptation,
the SMT systems perform significantly better than
the NMT system on the resource poor domains,
i.e., IWSLT-CE and WIKI-CJ; while on the re-
source rich domains, i.e., NTCIR-CE and ASPEC-
CJ, NMT outperforms SMT. Directly using the
SMT/NMT models trained on the out-of-domain
data to translate the in-domain data shows bad
performance. With our proposed “Mixed fine
tuning” domain adaptation method, NMT signif-
icantly outperforms SMT on the in-domain tasks.

Comparing different domain adaptation meth-
ods, “Mixed fine tuning” shows the best perfor-

388



mance. We believe the reason for this is that
“Mixed fine tuning” can address the over-fitting
problem of “Fine tuning.” We observed that both
fine-tuning and mixed fine-tuning tends to con-
verge after 1 epoch of training, and thus we early
stopped training soon after 1 epoch. After 1 epoch
of training, fine-tuning overfitted very quickly,
while mixed fine-tuning did not overfit. In ad-
dition, “Mixed fine tuning” does not worsen the
quality of out-of-domain translations, while “Fine
tuning” and “Multi domain” do. One shortcoming
of “Mixed fine tuning” is that compared to “Fine
tuning,” it took a longer time for the fine tuning
process, as the time until convergence is essen-
tially proportional to the size of the data used for
fine tuning. Note that training as long as “Mixed
fine tuning” is not helpful for “Fine tuning” due to
overfitting.

“Multi domain” performs either as well as
(IWSLT-CE) or worse than (WIKI-CJ) “Fine tun-
ing,” but “Mixed fine tuning” performs either sig-
nificantly better than (IWSLT-CE) or is compara-
ble to (WIKI-CJ) “Fine tuning.” We believe the
performance difference between the two tasks is
due to their unique characteristics. As WIKI-CJ
data is of relatively poorer quality, mixing it with
out-of-domain data does not have the same level
of positive effects as those obtained by the IWSLT-
CE data.

The domain tags are helpful for both “Multi do-
main” and “Mixed fine tuning.” Essentially, fur-
ther fine tuning on in-domain data does not help
for both “Multi domain” and “Mixed fine tun-
ing.” We believe that there are two reasons for
this. Firstly, the “Multi domain” and “Mixed fine
tuning” methods already utilize the in-domain data
used for fine tuning. Secondly, fine tuning on the
small in-domain data overfits very quickly. Actu-
ally, we observed that adding fine-tuning on top
of both “Multi domain” and “Mixed fine tuning”
overfits at the beginning of training.

Mixed fine tuning performs significantly better
on the out-domain NTCIR-CE test set without tags
as compared to with tags (39.67 v.s. 37.01). We
believe the reason for this is that without tags the
IWSLT-CE in-domain data can contribute more to
the out-of-domain NTCIR-CE data. With tags, the
NMT training tends to learn a model that pays
equal attention to each domain. Without tags, the
NMT training pays more attention to the NTCIR-
CE data as it contains much longer sentences, al-

though we oversampled the IWSLT-CE data. As
the IWSLT-CE data is TED talks, there could be
some vocabulary and content overlaps between the
IWSLT-CE the NTCIR-CE data, and thus append-
ing the IWSLT-CE data to the NTCIR-CE data can
benefit for the NTCIR-CE translation. In the case
of WIKI-CJ and ASPEC-CJ, due to the low qual-
ity of WIKI-CJ, appending WIKI-CJ to ASPEC-
CJ does not improve the ASPEC-CJ translation.

6 Conclusion

In this paper, we proposed a novel domain adapta-
tion method named “mixed fine tuning” for NMT.
We empirically compared our proposed method
against fine tuning and multi domain methods, and
have shown that it is effective but is sensitive to the
quality of the in-domain data used. The presented
methods are language and domain independent,
and thus we believe that the general observations
also hold on other languages and domains. Fur-
thermore, we believe the contribution in this paper
can be helpful for domain adaptation of other NN
based natural language processing tasks.

In the future, we plan to incorporate an RNN
model into our architecture to leverage abundant
in-domain monolingual corpora. We also plan
on exploring the effects of synthetic data by back
translating large in-domain monolingual corpora.

Acknowledgments

This work was partly supported by JSPS and DST
under the Japan-India Science Cooperative Pro-
gram. We are very appreciated to Prof. Daisuke
Kawahara, Dr. Toshiaki Nakazawa, and Dr. Fa-
bien Cromieres for helping improving the writing
quality of this paper. We also thank the anony-
mous reviewers for their insightful comments.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In In Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR 2015). International Conference
on Learning Representations, San Diego, USA.

M Cettolo, J Niehues, S Stüker, L Bentivogli, R Cat-
toni, and M Federico. 2015. The iwslt 2015 evalua-
tion campaign. In Proceedings of the Twelfth Inter-
national Workshop on Spoken Language Translation
(IWSLT).

389



Kyunghyun Cho, Bart van Merriënboer, Çalar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
1724–1734. http://www.aclweb.org/anthology/D14-
1179.

Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2016a.
Parallel sentence extraction from comparable cor-
pora with neural network features. In Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC 2016). Eu-
ropean Language Resources Association (ELRA),
Paris, France.

Chenhui Chu, Toshiaki Nakazawa, Daisuke Kawa-
hara, and Sadao Kurohashi. 2016b. SCTB: A
Chinese treebank in scientific domain. In Pro-
ceedings of the 12th Workshop on Asian Lan-
guage Resources (ALR12). The COLING 2016 Or-
ganizing Committee, Osaka, Japan, pages 59–67.
http://aclweb.org/anthology/W16-5407.

Fabien Cromieres, Chenhui Chu, Toshiaki Nakazawa,
and Sadao Kurohashi. 2016. Kyoto univer-
sity participation to wat 2016. In Proceed-
ings of the 3rd Workshop on Asian Transla-
tion (WAT2016). The COLING 2016 Organiz-
ing Committee, Osaka, Japan, pages 166–174.
http://aclweb.org/anthology/W16-4616.

Markus Freitag and Yaser Al-Onaizan. 2016. Fast
domain adaptation for neural machine translation.
arXiv preprint arXiv:1612.06897 .

Isao Goto, Ka-Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Conference. Na-
tional Institute of Informatics (NII), Tokyo, Japan,
pages 260–286.

Çaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loı̈c Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015.
On using monolingual corpora in neural ma-
chine translation. CoRR abs/1503.03535.
http://arxiv.org/abs/1503.03535.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Viégas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2016.
Google’s multilingual neural machine translation
system: Enabling zero-shot translation. CoRR
abs/1611.04558. http://arxiv.org/abs/1611.04558.

Catherine Kobus, Josep Crego, and Jean Senellart.
2016. Domain control for neural machine transla-
tion. arXiv preprint arXiv:1612.06140 .

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004. Association for Computational Lin-
guistics, Barcelona, Spain, pages 388–395.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 177–180.
http://www.aclweb.org/anthology/P/P07/P07-2045.

Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements of
Japanese morphological analyzer JUMAN. In Pro-
ceedings of the International Workshop on Sharable
Natural Language. pages 22–28.

Minh-Thang Luong and Christopher D Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In Proceedings of the 12th
International Workshop on Spoken Language Trans-
lation. Da Nang, Vietnam, pages 76–79.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
479–489. https://aclweb.org/anthology/D16-1046.

Toshiaki Nakazawa, Hideya Mino, Isao Goto, Gra-
ham Neubig, Sadao Kurohashi, and Eiichiro Sumita.
2015. Overview of the 2nd Workshop on Asian
Translation. In Proceedings of the 2nd Workshop on
Asian Translation (WAT2015). Kyoto, Japan, pages
1–28.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. Aspec: Asian
scientific paper excerpt corpus. In Proceedings of
the Tenth International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association (ELRA), Paris,
France.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics. Association for Com-
putational Linguistics, Sapporo, Japan, pages 160–
167. http://www.aclweb.org/anthology/P03-1021.

Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016a. Controlling politeness in neu-
ral machine translation via side constraints. In
Proceedings of the 2016 Conference of the

390



North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies. Association for Computational Lin-
guistics, San Diego, California, pages 35–40.
http://www.aclweb.org/anthology/N16-1005.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Improving neural machine translation
models with monolingual data. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 86–96.
http://www.aclweb.org/anthology/P16-1009.

Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016c. Neural machine translation of
rare words with subword units. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1715–1725.
http://www.aclweb.org/anthology/P16-1162.

Christophe Servan, Josep Crego, and Jean Senel-
lart. 2016. Domain specialization: a post-training
domain adaptation for neural machine translation.
arXiv preprint arXiv:1612.06141 .

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014. Sequence to sequence learning with
neural networks. In Proceedings of the 27th
International Conference on Neural Informa-
tion Processing Systems. MIT Press, Cam-
bridge, MA, USA, NIPS’14, pages 3104–3112.
http://dl.acm.org/citation.cfm?id=2969033.2969173.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016. pages 1568–1575.
http://aclweb.org/anthology/D/D16/D16-1163.pdf.

391


	An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation

