



















































Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 578–589,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Multi-level Representations
for Fine-Grained Typing of Knowledge Base Entities

Yadollah Yaghoobzadeh and Hinrich Schütze
Center for Information and Language Processing

LMU Munich, Germany
yadollah@cis.lmu.de

Abstract

Entities are essential elements of natu-
ral language. In this paper, we present
methods for learning multi-level represen-
tations of entities on three complemen-
tary levels: character (character patterns
in entity names extracted, e.g., by neural
networks), word (embeddings of words
in entity names) and entity (entity em-
beddings). We investigate state-of-the-
art learning methods on each level and
find large differences, e.g., for deep learn-
ing models, traditional ngram features and
the subword model of fasttext (Bo-
janowski et al., 2016) on the character
level; for word2vec (Mikolov et al.,
2013) on the word level; and for the
order-aware model wang2vec (Ling et
al., 2015a) on the entity level.
We confirm experimentally that each level
of representation contributes complemen-
tary information and a joint representation
of all three levels improves the existing
embedding based baseline for fine-grained
entity typing by a large margin. Addi-
tionally, we show that adding information
from entity descriptions further improves
multi-level representations of entities.

1 Introduction

Knowledge about entities is essential for under-
standing human language. This knowledge can
be attributional (e.g., canFly, isEdible), type-based
(e.g., isFood, isPolitician, isDisease) or relational
(e.g, marriedTo, bornIn). Knowledge bases (KBs)
are designed to store this information in a struc-
tured way, so that it can be queried easily. Exam-
ples of such KBs are Freebase (Bollacker et al.,
2008), Wikipedia, Google knowledge graph and

YAGO (Suchanek et al., 2007). For automatic up-
dating and completing the entity knowledge, text
resources such as news, user forums, textbooks or
any other data in the form of text are important
sources. Therefore, information extraction meth-
ods have been introduced to extract knowledge
about entities from text. In this paper, we focus on
the extraction of entity types, i.e., assigning types
to – or typing – entities. Type information can help
extraction of relations by applying constraints on
relation arguments.

We address a problem setting in which the fol-
lowing are given: a KB with a set of entities
E, a set of types T and a membership function
m : E × T 7→ {0, 1} such that m(e, t) = 1 iff
entity e has type t; and a large corpus C in which
mentions of E are annotated. In this setting, we
address the task of fine-grained entity typing: we
want to learn a probability function S(e, t) for a
pair of entity e and type t and based on S(e, t) in-
fer whether m(e, t) = 1 holds, i.e., whether entity
e is a member of type t.

We address this problem by learning a multi-
level representation for an entity that contains the
information necessary for typing it. One important
source is the contexts in which the entity is used.
We can take the standard method of learning em-
beddings for words and extend it to learning em-
beddings for entities. This requires the use of an
entity linker and can be implemented by replac-
ing all occurrences of the entity by a unique to-
ken. We refer to entity embeddings as entity-level
representations. Previously, entity embeddings
have been learned mostly using bag-of-word mod-
els like word2vec (e.g., by Wang et al. (2014)
and Yaghoobzadeh and Schütze (2015)). We show
below that order information is critical for high-
quality entity embeddings.

Entity-level representations are often uninfor-
mative for rare entities, so that using only entity

578



embeddings is likely to produce poor results. In
this paper, we use entity names as a source of in-
formation that is complementary to entity embed-
dings. We define an entity name as a noun phrase
that is used to refer to an entity. We learn character
and word level representations of entity names.

For the character-level representation, we adopt
different character-level neural network architec-
tures. Our intuition is that there is sub/cross word
information, e.g., orthographic patterns, that is
helpful to get better entity representations, espe-
cially for rare entities. A simple example is that
a three-token sequence containing an initial like
“P.” surrounded by two capitalized words (“Rolph
P. Kugl”) is likely to refer to a person.

We compute the word-level representation as
the sum of the embeddings of the words that make
up the entity name. The sum of the embeddings
accumulates evidence for a type/property over all
constituents, e.g., a name containing “stadium”,
“lake” or “cemetery” is likely to refer to a location.
In this paper, we compute our word level repre-
sentation with two types of word embeddings: (i)
using only contextual information of words in the
corpus, e.g., by word2vec (Mikolov et al., 2013)
and (ii) using subword as well as contextual in-
formation of words, e.g., by Facebook’s recently
released fasttext (Bojanowski et al., 2016).

In this paper, we integrate character-level and
word-level with entity-level representations to im-
prove the results of previous work on fine-grained
typing of KB entities. We also show how descrip-
tions of entities in a KB can be a complementary
source of information to our multi-level represen-
tation to improve the results of entity typing, espe-
cially for rare entities.

Our main contributions in this paper are:

• We propose new methods for learning en-
tity representations on three levels: character-
level, word-level and entity-level.

• We show that these levels are complementary
and a joint model that uses all three levels im-
proves the state of the art on the task of fine-
grained entity typing by a large margin.

• We experimentally show that an order depen-
dent embedding is more informative than its
bag-of-word counterpart for entity represen-
tation.

We release our dataset and source codes:
cistern.cis.lmu.de/figment2/.

2 Related Work

Entity representation. Two main sources of in-
formation used for learning entity representation
are: (i) links and descriptions in KB, (ii) name and
contexts in corpora. We focus on name and con-
texts in corpora, but we also include (Wikipedia)
descriptions. We represent entities on three levels:
entity, word and character. Our entity-level repre-
sentation is similar to work on relation extraction
(Wang et al., 2014; Wang and Li, 2016), entity
linking (Yamada et al., 2016; Fang et al., 2016),
and entity typing (Yaghoobzadeh and Schütze,
2015). Our word-level representation with distri-
butional word embeddings is similarly used to rep-
resent entities for entity linking (Sun et al., 2015)
and relation extraction (Socher et al., 2013; Wang
et al., 2014). Novel entity representation methods
we introduce in this paper are representation based
on fasttext (Bojanowski et al., 2016) sub-
word embeddings, several character-level repre-
sentations, “order-aware” entity-level embeddings
and the combination of several different represen-
tations into one multi-level representation.

Character-subword level neural networks.
Character-level convolutional neural networks
(CNNs) are applied by dos Santos and Zadrozny
(2014) to part of speech (POS) tagging, by dos
Santos and Guimarães (2015), Ma and Hovy
(2016), and Chiu and Nichols (2016) to named
entity recognition (NER), by Zhang et al. (2015)
and Zhang and LeCun (2015) to sentiment anal-
ysis and text categorization, and by Kim et al.
(2016) to language modeling (LM). Character-
level LSTM is applied by Ling et al. (2015b) to
LM and POS tagging, by Lample et al. (2016) to
NER, by Ballesteros et al. (2015) to parsing mor-
phologically rich languages, and by Cao and Rei
(2016) to learning word embeddings. Bojanowski
et al. (2016) learn word embeddings by repre-
senting words with the average of their character
ngrams (subwords) embeddings. Similarly, Chen
et al. (2015) extends word2vec for Chinese with
joint modeling with characters.

Fine-grained entity typing. Our task is to in-
fer fine-grained types of KB entities. KB comple-
tion is an application of this task. Yaghoobzadeh
and Schütze (2015)’s FIGMENT system addresses
this task with only contextual information; they
do not use character-level and word-level features
of entity names. Neelakantan and Chang (2015)
and Xie et al. (2016) also address a similar task,

579



Entity Representation

Hidden Layer

Output Layer (type probabilities)

Figure 1: Schematic diagram of our architecture
for entity classification. “Entity Representation”
(~v(e)) is the (one-level or multi-level) vector rep-
resentation of entity. Size of output layer is |T |.

but they rely on entity descriptions in KBs, which
in many settings are not available. The problem
of Fine-grained mention typing (FGMT) (Yosef
et al., 2012; Ling and Weld, 2012; Yogatama et
al., 2015; Del Corro et al., 2015; Shimaoka et
al., 2016; Ren et al., 2016) is related to our task.
FGMT classifies single mentions of named enti-
ties to their context dependent types whereas we
attempt to identify all types of a KB entity from
the aggregation of all its mentions. FGMT can
still be evaluated in our task by aggregating the
mention level decisions but as we will show in our
experiments for one system, i.e., FIGER (Ling and
Weld, 2012), our entity embedding based models
are better in entity typing.

3 Fine-grained entity typing

Given (i) a KB with a set of entities E, (ii) a set of
types T , and (iii) a large corpus C in which men-
tions of E are linked, we address the task of fine-
grained entity typing (Yaghoobzadeh and Schütze,
2015): predict whether entity e is a member of
type t or not. To do so, we use a set of training
examples to learn P (t|e): the probability that en-
tity e has type t. These probabilities can be used
to assign new types to entities covered in the KB
as well as typing unknown entities.

We learn P (t|e) with a general architecture; see
Figure 1. The output layer has size |T |. Unit t of
this layer outputs the probability for type t. “En-
tity Representation” (~v(e)) is the vector represen-
tation of entity e – we will describe in detail in
the rest of this section what forms ~v(e) takes. We
model P (t|e) as a multi-label classification, and
train a multilayer perceptron (MLP) with one hid-
den layer:[
P (t1|e) . . . P (tT |e)

]
= σ

(
Woutf

(
Win~v(e)

))
(1)

where Win ∈ Rh×d is the weight matrix from

~v(e) ∈ Rd to the hidden layer with size h. f is
the rectifier function. Wout ∈ R|T |×h is the weight
matrix from hidden layer to output layer of size
|T |. σ is the sigmoid function. Our objective is
binary cross entropy summed over types:∑

t

−
(
mt log pt + (1−mt) log (1− pt)

)
where mt is the truth and pt the prediction.

The key difficulty when trying to compute
P (t|e) is in learning a good representation for en-
tity e. We make use of contexts and name of e to
represent its feature vector on the three levels of
entity, word and character.

3.1 Entity-level representation

Distributional representations or embeddings are
commonly used for words. The underlying hy-
pothesis is that words with similar meanings tend
to occur in similar contexts (Harris, 1954) and
therefore cooccur with similar context words. We
can extend the distributional hypothesis to enti-
ties (cf. Wang et al. (2014), Yaghoobzadeh and
Schütze (2015)): entities with similar meanings
tend to have similar contexts. Thus, we can learn
a d dimensional embedding ~v(e) of entity e from
a corpus in which all mentions of the entity have
been replaced by a special identifier. We refer to
these entity vectors as the entity level representa-
tion (ELR).

In previous work, order information of context
words (relative position of words in the contexts)
was generally ignored and objectives similar to the
SkipGram (henceforth: SKIP) model were used
to learn ~v(e). However, the bag-of-word context
is difficult to distinguish for pairs of types like
(restaurant,food) and (author,book). This suggests
that using order aware embedding models is im-
portant for entities. Therefore, we apply Ling et
al. (2015a)’s extended version of SKIP, Structured
SKIP (SSKIP). It incorporates the order of context
words into the objective. We compare it with SKIP
embeddings in our experiments.

3.2 Word-level representation

Words inside entity names are important sources
of information for typing entities. We define the
word-level representation (WLR) as the average of
the embeddings of the words that the entity name
contains ~v(e) = 1/n

∑n
i=1 ~v(wi) where ~v(wi) is

the embedding of the ith word of an entity name

580



of length n. We opt for simple averaging since
entity names often consist of a small number of
words with clear semantics. Thus, averaging is a
promising way of combining the information that
each word contributes.

The word embedding, ~w, itself can be learned
from models with different granularity levels. Em-
bedding models that consider words as atomic
units in the corpus, e.g., SKIP and SSKIP, are
word-level. On the other hand, embedding
models that represent words with their charac-
ter ngrams, e.g., fasttext (Bojanowski et al.,
2016), are subword-level. Based on this, we con-
sider and evaluate word-level WLR (WWLR)
and subword-level WLR (SWLR) in this paper.1

Lipofen

Convolution 
layer

Max Pooling

Lookup table 
layer

Character-level Representation

Figure 2: Example architecture for the character-
level CNN with max pooling. The input is
“Lipofen”. Character embedding size is three.
There are three filters of width 2 and four filters
of width 4.

3.3 Character-level representation
For computing the character level representation
(CLR), we design models that try to type an entity
based on the sequence of characters of its name.
Our hypothesis is that names of entities of a spe-
cific type often have similar character patterns.
Entities of type ETHNICITY often end in “ish”
and “ian”, e.g., “Spanish” and “Russian”. Entities
of type MEDICINE often end in “en”: “Lipofen”,
“acetaminophen”. Also, some types tend to have
specific cross-word shapes in their entities, e.g.,

1Subword models have properties of both character-level
models (subwords are character ngrams) and of word-level
models (they do not cross boundaries between words). They
probably could be put in either category, but in our context fit
the word-level category better because we see the granularity
level with respect to the entities and not words.

PERSON names usually consist of two words, or
MUSIC names are usually long, containing several
words.

The first layer of the character-level models is a
lookup table that maps each character to an em-
bedding of size dc. These embeddings capture
similarities between characters, e.g., similarity in
type of phoneme encoded (consonant/vowel) or
similarity in case (lower/upper). The output of
the lookup layer for an entity name is a matrix
C ∈ Rl×dc where l is the maximum length of a
name and all names are padded to length l. This
length l includes special start/end characters that
bracket the entity name.

We experiment with four architectures to pro-
duce character-level representations in this paper:
FORWARD (direct forwarding of character em-
beddings), CNNs, LSTMs and BiLSTMs. The
output of each architecture then takes the place of
the entity representation ~v(e) in Figure 1.

FORWARD simply concatenates all rows of
matrix C; thus, ~v(e) ∈ Rdc∗l.

The CNN uses k filters of different window
widths w to narrowly convolve C. For each fil-
ter H ∈ Rdc×w, the result of the convolution of H
over matrix C is feature map f ∈ Rl−w+1:
f [i] = rectifier(C[:,i:i+w−1] �H + b)

where rectifier is the activation function, b is the
bias, C[:,i:i+w−1] are the columns i to i+w− 1 of
C, 1 ≤ w ≤ 10 are the window widths we con-
sider and � is the sum of element-wise multipli-
cation. Max pooling then gives us one feature for
each filter. The concatenation of all these features
is our representation: ~v(e) ∈ Rk. An example
CNN architecture is show in Figure 2.

The input to the LSTM is the character se-
quence in matrix C, i.e., x1, . . . , xl ∈ Rdc . It
generates the state sequence h1, ..., hl+1 and the
output is the last state ~v(e) ∈ Rdh .2

The BiLSTM consists of two LSTMs, one go-
ing forward, one going backward. The first state of
the backward LSTM is initialized as hl+1, the last
state of the forward LSTM. The BiLSTM entity
representation is the concatenation of last states of
forward and backward LSTMs, i.e., ~v(e) ∈ R2∗dh .

3.4 Multi-level representations

Our different levels of representations can give
complementary information about entities.

2We use Blocks (van Merriënboer et al., 2015).

581



Character-level 
Representation

Word-level 
Representation

Entity-level 
Representation

Entity Representation

Figure 3: Multi-level representation

WLR and CLR. Both WLR models, SWLR
and WWLR, do not have access to the cross-word
character ngrams of entity names while CLR mod-
els do. Also, CLR is task specific by training on
the entity typing dataset while WLR is generic. On
the other hand, WWLR and SWLR models have
access to information that CLR ignores: the tok-
enization of entity names into words and embed-
dings of these words. It is clear that words are par-
ticularly important character sequences since they
often correspond to linguistic units with clearly
identifiable semantics – which is not true for most
character sequences. For many entities, the words
they contain are a better basis for typing than
the character sequence. For example, even if
“nectarine” and “compote” did not occur in any
names in the training corpus, we can still learn
good word embeddings from their non-entity oc-
currences. This then allows us to correctly type the
entity “Aunt Mary’s Nectarine Compote” as FOOD
based on the sum of the word embeddings.

WLR/CLR and ELR. Representations from
entity names, i.e., WLR and CLR, by themselves
are limited because many classes of names can be
used for different types of entities; e.g., person
names do not contain hints as to whether they are
referring to a politician or athlete. In contrast, the
ELR embedding is based on an entity’s contexts,
which are often informative for each entity and
can distinguish politicians from athletes. On the
other hand, not all entities have sufficiently many
informative contexts in the corpus. For these en-
tities, their name can be a complementary source
of information and character/word level represen-
tations can increase typing accuracy.

Thus, we introduce joint models that use com-
binations of the three levels. Each multi-level
model concatenates several levels. We train the
constituent embeddings as follows. WLR and
ELR are computed as described above and are not
changed during training. CLR – produced by one
of the character-level networks described above
– is initialized randomly and then tuned during

training. Thus, it can focus on complementary in-
formation related to the task that is not already
present in other levels. The schematic diagram
of our multi-level representation is shown in Fig-
ure 3.

4 Experimental setup and results

4.1 Setup
Entity datasets and corpus. We address
the task of fine-grained entity typing and use
Yaghoobzadeh and Schütze (2015)’s FIGMENT
dataset3 for evaluation. The FIGMENT corpus
is part of a version of ClueWeb in which Free-
base entities are annotated using FACC1 (URL,
2016b; Gabrilovich et al., 2013). The FIGMENT
entity datasets contain 200,000 Freebase entities
that were mapped to 102 FIGER types (Ling and
Weld, 2012). We use the same train (50%), dev
(20%) and test (30%) partitions as Yaghoobzadeh
and Schütze (2015) and extract the names from
mentions of dataset entities in the corpus. We take
the most frequent name for dev and test entities
and three most frequent names for train (each one
tagged with entity types).

Adding parent types to refine entity dataset.
FIGMENT ignores that FIGER is a proper hierar-
chy of types; e.g., while HOSPITAL is a subtype of
BUILDING according to FIGER, there are entities
in FIGMENT that are hospitals, but not buildings.4

Therefore, we modified the FIGMENT dataset by
adding for each assigned type (e.g., HOSPITAL) its
parents (e.g., BUILDING). This makes FIGMENT
more consistent and eliminates spurious false neg-
atives (BUILDING in the example).

We now describe our baselines: (i) BOW
& NSL: hand-crafted features, (ii) FIGMENT
(Yaghoobzadeh and Schütze, 2015) and (iii)
adapted version of FIGER (Ling and Weld, 2012).

We implement the following two feature sets
from the literature as a hand-crafted baseline for
our character and word level models. (i) BOW: in-
dividual words of entity name (both as-is and low-
ercased); (ii) NSL (ngram-shape-length): shape
and length of the entity name (cf. Ling and Weld
(2012)), character n-grams, 1 ≤ n ≤ nmax, nmax =
5 (we also tried nmax = 7, but results were worse
on dev) and normalized character n-grams: lower-
cased, digits replaced by “7”, punctuation replaced
by “.”. These features are represented as a sparse

3cistern.cis.lmu.de/figment/
4See github.com/xiaoling/figer for FIGER

582



binary vector ~v(e) that is input to the architecture
in Figure 1.

FIGMENT is the model for entity typing pre-
sented by Yaghoobzadeh and Schütze (2015).
The authors only use entity-level representations
for entities trained by SkipGram, so the FIG-
MENT baseline corresponds to the entity-level re-
sult shown as ELR(SKIP) in the tables.

The third baseline is using an existing mention-
level entity typing system, FIGER (Ling and Weld,
2012). FIGER uses a wide variety of features
on different levels (including parsing-based fea-
tures) from contexts of entity mentions as well as
the mentions themselves and returns a score for
each mention-type instance in the corpus. We pro-
vide the ClueWeb/FACC1 segmentation of enti-
ties, so FIGER does not need to recognize enti-
ties.5 We use the trained model provided by the
authors and normalize FIGER scores using soft-
max to make them comparable for aggregation.
We experimented with different aggregation func-
tions (including maximum and k-largest-scores for
a type), but we use the average of scores since it
gave us the best result on dev. We call this baseline
AGG-FIGER.

Distributional embeddings. For WWLR and
ELR, we use SkipGram model in word2vec and
SSkip model in wang2vec (Ling et al., 2015a) to
learn embeddings for words, entities and types. To
obtain embeddings for all three in the same space,
we process ClueWeb/FACC1 as follows. For each
sentence s, we add three copies: s itself, a copy
of s in which each entity is replaced with its Free-
base identifier (MID) and a copy in which each
entity (not test entities though) is replaced with an
ID indicating its notable type. The resulting cor-
pus contains around 4 billion tokens and 1.5 bil-
lion types.

We run SKIP and SSkip with the same setup
(200 dimensions, 10 negative samples, window
size 5, word frequency threshold of 100)6 on this
corpus to learn embeddings for words, entities and
FIGER types. Having entities and types in the
same vector space, we can add another feature
vector ~v(e) ∈ R|T | (referred to as TC below): for
each entity, we compute cosine similarity of its en-
tity vector with all type vectors.

For SWLR, we use fasttext7 to learn word
5Mention typing is separated from recognition in FIGER

model. So it can use our segmentation of entities.
6The threshold does not apply for MIDs.
7github.com/facebookresearch/fastText

embeddings from the ClueWeb/FACC1 corpus.
We use similar settings as our WWLR SKIP and
SSkip embeddings and keep the defaults of other
hyperparameters. Since the trained model of
fasttext is applicable for new words, we ap-
ply the model to get embeddings for the filtered
rare words as well.

model hyperparameters
CLR(FF) dc = 15, hmlp = 600
CLR(LSTM) dc = 70, dh = 70, hmlp = 300
CLR(BiLSTM) dc = 50, dh = 50, hmlp = 200
CLR(CNN) dc = 10, w = [1, .., 8]

n = 100, hmlp = 800
CLR(NSL) hmlp = 800
BOW hmlp = 200
BOW+CLR(NSL) hmlp = 300
WWLR hmlp = 400
SWLR hmlp = 400
WWLR+CLR(CNN) w = [1, ..., 7]

dc = 10, n = 50, hmlp = 700
SWLR+CLR(CNN) w = [1, ..., 7]

dc = 10, n = 50, hmlp = 700
ELR(SKIP) hmlp = 400
ELR(SSKIP) hmlp = 400
ELR+CLR dc = 10, w = [1, ..., 7]

n = 100, hmlp = 700
ELR+WWLR hmlp = 600
ELR+SWLR hmlp = 600
ELR+WWLR+CLR dc = 10, w = [1, ..., 7]

n = 50, hmlp = 700
ELR+SWLR+CLR dc = 10, w = [1, ..., 7]

n = 50, hmlp = 700
ELR+WWLR+CNN+TC dc = 10, w = [1, ..., 7]

n = 50, hmlp = 900
ELR+SWLR+CNN+TC(MuLR) dc = 10, w = [1, ..., 7]

n = 50, hmlp = 900
AVG-DES hmlp = 400
MuLR+AVG-DES dc = 10, w = [1, ..., 7]

n = 50, hmlp = 1000

Table 1: Hyperparameters of different models. w
is the filter size. n is the number of CNN feature
maps for each filter size. dc is the character em-
bedding size. dh is the LSTM hidden state size.
hmlp is the number of hidden units in the MLP.

Our hyperparameter values are given in Ta-
ble 1. The values are optimized on dev. We use
AdaGrad and minibatch training. For each experi-
ment, we select the best model on dev.

We use these evaluation measures: (i) accu-
racy: an entity is correct if all its types and no
incorrect types are assigned to it; (ii) micro aver-
age F1: F1 of all type-entity assignment decisions;
(iii) entity macro average F1: F1 of types assigned
to an entity, averaged over entities; (iv) type macro
average F1: F1 of entities assigned to a type, aver-
aged over types.

The assignment decision is based on thresh-
olding the probability function P (t|e). For each
model and type, we select the threshold that max-
imizes F1 of entities assigned to the type on dev.

583



all entities head entities tail entities
acc mic mac acc mic mac acc mic mac

1 MFT .000 .041 .041 .000 .044 .044 .000 .038 .038
2 CLR(FORWARD) .066 .379 .352 .067 .342 .369 .061 .374 .350
3 CLR(LSTM) .121 .425 .396 .122 .433 .390 .116 .408 .391
4 CLR(BiLSTM) .133 .440 .404 .129 .443 .394 .135 .428 .404
5 CLR(NSL) .164 .484 .464 .157 .470 .443 .173 .483 .472
6 CLR(CNN) .177 .494 .468 .171 .484 .450 .187 .489 .474
7 BOW .113 .346 .379 .109 .323 .353 .120 .356 .396
8 WWLR(SKIP) .214 .581 .531 .293 .660 .634 .173 .528 .478
9 WWLR(SSKIP) .223 .584 .543 .306 .667 .642 .183 .533 .494

10 SWLR .236 .590 .554 .301 .665 .632 .209 .551 .522
11 BOW+CLR(NSL) .156 .487 .464 .157 .480 .452 .159 .485 .469
12 WWLR+CLR(CNN) .257 .603 .568 .317 .668 .637 .235 .567 .538
13 SWLR+CLR(CNN) .241 .594 .561 .295 .659 .628 .227 .560 .536
14 ELR(SKIP) .488 .774 .741 .551 .834 .815 .337 .621 .560
15 ELR(SSKIP) .515 .796 .763 .560 .839 .819 .394 .677 .619
16 AGG-FIGER .320 .694 .660 .396 .762 .724 .220 .593 .568
17 ELR+CLR .554 .816 .788 .580 .844 .825 .467 .733 .690
18 ELR+WWLR .557 .819 .793 .582 .846 .827 .480 .749 .708
19 ELR+SWLR .558 .820 .796 .584 .846 .829 .480 .751 .714
20 ELR+WWLR+CLR .568 .823 .798 .590 .847 .829 .491 .755 .716
21 ELR+SWLR+CLR .569 .824 .801 .590 .849 .831 .497 .760 .724
22 ELR+WWLR+CLR+TC .572 .824 .801 .594 .849 .831 .499 .759 .722
23 ELR+SWLR+CLR+TC .575 .826 .802 .597 .851 .831 .508 .762 .727

Table 2: Accuracy (acc), micro (mic) and macro (mac) F1
on test for all, head and tail entities.

types: all head tail
AGG-FIGER .566 .702 .438
ELR .621 .784 .480
MuLR .669 .811 .541

Table 3: Type macro aver-
age F1 on test for all, head
and tail types. MuLR =
ELR+SWLR+CLR+TC

all known?
yes no

CLR(NSL) .484 .521 .341
CLR(CNN) .494 .524 .374
BOW .346 .435 .065
SWLR .590 .612 .499
BOW+NSL .497 .535 .358
SWLR+CLR(CNN) .594 .616 .508

Table 4: Micro F1 on test of
character, word level models
for all, known (“known? yes”)
and unknown (“known? no”)
entities.

4.2 Results
Table 2 gives results on the test entities for all
(about 60,000 entities), head (frequency > 100;
about 12,200) and tail (frequency < 5; about
10,000). MFT (line 1) is the most frequent type
baseline that ranks types according to their fre-
quency in the train entities. Each level of represen-
tation is separated with dashed lines, and – unless
noted otherwise – the best of each level is joined
in multi level representations.8

Character-level models are on lines 2-6. The
order of systems is: CNN > NSL > BiLSTM
> LSTM > FORWARD. The results show that
complex neural networks are more effective than
simple forwarding. BiLSTM works better than
LSTM, confirming other related work. CNNs
probably work better than LSTMs because there
are few complex non-local dependencies in the se-
quence, but many important local features. CNNs
with maxpooling can more straightforwardly cap-
ture local and position-independent features. CNN
also beats NSL baseline; a possible reason is that
CNN – an automatic method of feature learning

8For accuracy measure: in the following ordered lists of
sets, A<B means that all members (row numbers in Table 2)
of A are significantly worse than all members of B: {1} <
{2}< {3, . . . , 11}< {12,13}< {14,15,16}<{17, . . . , 23}.
Test of equal proportions, α < 0.05. See Table 6 in the ap-
pendix for more details.

– is more robust than hand engineered feature
based NSL. We show more detailed results in Sec-
tion 4.3.

Word-level models are on lines 7-10. BOW
performs worse than WWLR because it cannot
deal well with sparseness. SSKIP uses word order
information in WWLR and performs better than
SKIP. SWLR uses subword information and per-
forms better than WWLR, especially for tail en-
tities. Integrating subword information improves
the quality of embeddings for rare words and mit-
igates the problem of unknown words.

Joint word-character level models are
on lines 11-13. WWLR+CLR(CNN) and
SWLR+CLR(CNN) beat the component models.
This confirms our underlying assumption in
designing the complementary multi-level models.
BOW problem with rare words does not allow
its joint model with NSL to work better than
NSL. WWLR+CLR(CNN) works better than
BOW+CLR(NSL) by 10% micro F1, again due
to the limits of BOW compared to WWLR.
Interestingly WWLR+CLR works better than
SWLR+CLR and this suggests that WWLR is
indeed richer than SWLR when CLR mitigates its
problem with rare/unknown words

Entity-level models are on lines 14–15 and
they are better than all previous models on lines

584



Figure 4: t-SNE result of entity-level representa-
tions

1–13. This shows the power of entity-level embed-
dings. In Figure 4, a t-SNE (Van der Maaten and
Hinton, 2008) visualization of ELR(SKIP) embed-
dings using different colors for entity types shows
that entities of the same type are clustered to-
gether. SSKIP works marginally better than SKIP
for ELR, especially for tail entities, confirming our
hypothesis that order information is important for
a good distributional entity representation. This
is also confirming the results of Yaghoobzadeh
and Schütze (2016), where they also get better en-
tity typing results with SSKIP compared to SKIP.
They propose to use entity typing as an extrinsic
evaluation for embedding models.

Joint entity, word, and character level mod-
els are on lines 16-23. The AGG-FIGER baseline
works better than the systems on lines 1-13, but
worse than ELRs. This is probably due to the fact
that AGG-FIGER is optimized for mention typing
and it is trained using distant supervision assump-
tion. Parallel to our work, Yaghoobzadeh et al.
(2017) optimize a mention typing model for our
entity typing task by introducing multi instance
learning algorithms, resulting comparable perfor-
mance to ELR(SKIP). We will investigate their
method in future.

Joining CLR with ELR (line 17) results in
large improvements, especially for tail entities
(5% micro F1). This demonstrates that for rare
entities, contextual information is often not suf-
ficient for an informative representation, hence
name features are important. This is also true
for the joint models of WWLR/SWLR and ELR
(lines 18-19). Joining WWLR works better than

CLR, and SWLR is slightly better than WWLR.
Joint models of WWLR/SWLR with ELR+CLR
gives more improvements, and SWLR is again
slightly better than WWLR. ELR+WWLR+CLR
and ELR+SWLR+CLR, are better than their two-
level counterparts, again confirming that these lev-
els are complementary.

We get a further boost, especially for tail en-
tities, by also including TC (type cosine) in the
combinations (lines 22-23). This demonstrates the
potential advantage of having a common represen-
tation space for entities and types. Our best model,
ELR+SWLR+CLR+TC (line 22), which we refer
to as MuLR in the other tables, beats our initial
baselines (ELR and AGG-FIGER) by large mar-
gins, e.g., in tail entities improvements are more
than 8% in micro F1.

Table 3 shows type macro F1 for MuLR
(ELR+SWLR+CLR+TC) and two baselines.
There are 11 head types (those with ≥3000 train
entities) and 36 tail types (those with <200 train
entities). These results again confirm the superi-
ority of our multi-level models over the baselines:
AGG-FIGER and ELR, the best single-level
model baseline.

4.3 Analysis

Unknown vs. known entities. To analyze the
complementarity of character and word level rep-
resentations, as well as more fine-grained com-
parison of our models and the baselines, we di-
vide test entities into known entities – at least one
word of the entity’s name appears in a train entity
– and unknown entities (the complement). There
are 45,000 (resp. 15,000) known (resp. unknown)
test entities.

Table 4 shows that the CNN works only slightly
better (by 0.3%) than NSL on known entities, but
works much better on unknown entities (by 3.3%),
justifying our preference for deep learning CLR
models. As expected, BOW works relatively well
for known entities and really poorly for unknown
entities. SWLR beats CLR models as well as
BOW. The reason is that in our setup, word em-
beddings are induced on the entire corpus using
an unsupervised algorithm. Thus, even for many
words that did not occur in train, SWLR has ac-
cess to informative representations of words. The
joint model, SWLR+CLR(CNN), is significantly
better than BOW+CLR(NSL) again due to limits
of BOW. SWLR+CLR(CNN) is better than SWLR

585



in unknown entities.
Case study of LIVING-THING. To understand

the interplay of different levels better, we perform
a case study of the type LIVING-THING. Living
beings that are not humans belong to this type.

WLRs incorrectly assign “Walter Leaf”
(PERSON) and “Along Came A Spider” (MUSIC)
to LIVING-THING because these names contain a
word referring to a LIVING-THING (“leaf”, “spi-
der”), but the entity itself is not a LIVING-THING.
In these cases, the averaging of embeddings that
WLR performs is misleading. The CLR(CNN)
types these two entities correctly because their
names contain character ngram/shape patterns
that are indicative of PERSON and MUSIC.

ELR incorrectly assigns “Zumpango” (CITY)
and “Lake Kasumigaura” (LOCATION) to LIVING-
THING because these entities are rare and words
associated with living things (e.g., “wildlife”)
dominate in their contexts. However, CLR(CNN)
and WLR enable the joint model to type the two
entites correctly: “Zumpango” because of the in-
formative suffix “-go” and “Lake Kasumigaura”
because of the informative word “Lake”.

While some of the remaining errors of our best
system MuLR are due to the inherent difficulty of
entity typing (e.g., it is difficult to correctly type a
one-word entity that occurs once and whose name
is not informative), many other errors are due to
artifacts of our setup. First, ClueWeb/FACC1 is
the result of an automatic entity linking system
and any entity linking errors propagate to our mod-
els. Second, due to the incompleteness of Freebase
(Yaghoobzadeh and Schütze, 2015), many entities
in the FIGMENT dataset are incompletely anno-
tated, resulting in correctly typed entities being
evaluated as incorrect.

Adding another source: description-based
embeddings. While in this paper, we focus on the
contexts and names of entities, there is a textual
source of information about entities in KBs which
we can also make use of: descriptions of entities.
We extract Wikipedia descriptions of FIGMENT
entities filtering out the entities (∼ 40,000 out of
∼ 200,000) without description.

We then build a simple entity representation by
averaging the embeddings of the top k words (wrt
tf-idf) of the description (henceforth, AVG-DES).9

This representation is used as input in Figure 1
to train the MLP. We also train our best multi-

9k = 20 gives the best results on dev.

entities: all head tail
AVG-DES .773 .791 .745
MuLR .825 .846 .757
MuLR+AVG-DES .873 .877 .852

Table 5: Micro average F1 results of MuLR and
description based model and their joint.

level model as well as the joint of the two on this
smaller dataset. Since the descriptions are coming
from Wikipedia, we use 300-dimensional Glove
(URL, 2016a) embeddings pretrained on Wikip-
dia+Gigaword to get more coverage of words. For
MuLR, we still use the embeddings we trained be-
fore.

Results are shown in Table 5. While for head
entities, MuLR works marginally better, the differ-
ence is very small in tail entities. The joint model
of the two (by concatenation of vectors) improves
the micro F1, with clear boost for tail entities. This
suggests that for tail entities, the contextual and
name information is not enough by itself and some
keywords from descriptions can be really helpful.
Integrating more complex description-based em-
beddings, e.g., by using CNN (Xie et al., 2016),
may improve the results further. We leave it for
future work.

5 Conclusion

In this paper, we have introduced representations
of entities on different levels: character, word
and entity. The character level representation is
learned from the entity name. The word level rep-
resentation is computed from the embeddings of
the words wi in the entity name where the embed-
ding of wi is derived from the corpus contexts of
wi. The entity level representation of entity ei is
derived from the corpus contexts of ei. Our exper-
iments show that each of these levels contributes
complementary information for the task of fine-
grained typing of entities. The joint model of all
three levels beats the state-of-the-art baseline by
large margins. We further showed that extracting
some keywords from Wikipedia descriptions of
entities, when available, can considerably improve
entity representations, especially for rare entities.
We believe that our findings can be transferred to
other tasks where entity representation matters.

Acknowledgments. This work was supported
by DFG (SCHU 2246/8-2).

586



References
Miguel Ballesteros, Chris Dyer, and Noah A. Smith.

2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 349–
359, Lisbon, Portugal, September. Association for
Computational Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2016. Enriching word vectors with
subword information. CoRR, abs/1607.04606.

Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the ACM SIG-
MOD International Conference on Management of
Data, SIGMOD 2008, Vancouver, BC, Canada, June
10-12, 2008, pages 1247–1250.

Kris Cao and Marek Rei. 2016. A joint model for word
embedding and word morphology. In Proceedings
of the 1st Workshop on Representation Learning for
NLP, pages 18–26, Berlin, Germany, August. Asso-
ciation for Computational Linguistics.

Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,
and Huan-Bo Luan. 2015. Joint learning of char-
acter and word embeddings. In Proceedings of
the Twenty-Fourth International Joint Conference
on Artificial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, pages 1236–1242.

Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics, 4:357–370.

Luciano Del Corro, Abdalghani Abujabal, Rainer
Gemulla, and Gerhard Weikum. 2015. Finet:
Context-aware fine-grained named entity typing. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
868–878, Lisbon, Portugal, September. Association
for Computational Linguistics.

Cı́cero Nogueira dos Santos and Victor Guimarães.
2015. Boosting named entity recognition with neu-
ral character embeddings. CoRR, abs/1505.05008.

Cı́cero Nogueira dos Santos and Bianca Zadrozny.
2014. Learning character-level representations for
part-of-speech tagging. In Proceedings of the
31th International Conference on Machine Learn-
ing, ICML 2014, Beijing, China, 21-26 June 2014,
pages 1818–1826.

Wei Fang, Jianwen Zhang, Dilin Wang, Zheng Chen,
and Ming Li. 2016. Entity disambiguation by
knowledge and text jointly embedding. In Proceed-
ings of The 20th SIGNLL Conference on Computa-
tional Natural Language Learning, pages 260–269,
Berlin, Germany, August. Association for Computa-
tional Linguistics.

Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. Facc1: Freebase annotation
of clueweb corpora.

Zellig S. Harris. 1954. Distributional structure. Word,
10:146–162.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence, February 12-
17, 2016, Phoenix, Arizona, USA., pages 2741–
2749.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer.
2016. Neural architectures for named entity recog-
nition. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 260–270, San Diego, California,
June. Association for Computational Linguistics.

Xiao Ling and Daniel S. Weld. 2012. Fine-grained en-
tity recognition. In Proceedings of the Twenty-Sixth
AAAI Conference on Artificial Intelligence, July 22-
26, 2012, Toronto, Ontario, Canada.

Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2015a. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1299–1304,
Denver, Colorado, May–June. Association for Com-
putational Linguistics.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015b. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1520–1530, Lisbon, Portu-
gal, September. Association for Computational Lin-
guistics.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1064–1074, Berlin, Germany,
August. Association for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Arvind Neelakantan and Ming-Wei Chang. 2015. In-
ferring missing entity type instances for knowledge
base completion: New dataset and methods. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
515–525, Denver, Colorado, May–June. Association
for Computational Linguistics.

587



Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng
Ji, and Jiawei Han. 2016. Label noise reduction
in entity typing by heterogeneous partial-label em-
bedding. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, San Francisco, CA, USA, August
13-17, 2016, pages 1825–1834.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2016. An attentive neural ar-
chitecture for fine-grained entity type classification.
pages 69–74, June.

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with neural
tensor networks for knowledge base completion. In
Advances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meet-
ing held December 5-8, 2013, Lake Tahoe, Nevada,
United States., pages 926–934.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, WWW 2007, Banff, Al-
berta, Canada, May 8-12, 2007, pages 697–706.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-
zhou Ji, and Xiaolong Wang. 2015. Modeling men-
tion, context and entity with neural networks for en-
tity disambiguation. In Proceedings of the Twenty-
Fourth International Joint Conference on Artificial
Intelligence, IJCAI 2015, Buenos Aires, Argentina,
July 25-31, 2015, pages 1333–1339.

URL. 2016a. Glove project. http://nlp.
stanford.edu/projects/glove.

URL. 2016b. Lemur project. http://
lemurproject.org/clueweb12/FACC1.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(2579-2605):85.

Bart van Merriënboer, Dzmitry Bahdanau, Vincent Du-
moulin, Dmitriy Serdyuk, David Warde-Farley, Jan
Chorowski, and Yoshua Bengio. 2015. Blocks
and fuel: Frameworks for deep learning. CoRR,
abs/1506.00619.

Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced
representation learning for knowledge graph. In
Proceedings of the Twenty-Fifth International Joint
Conference on Artificial Intelligence, IJCAI 2016,
New York, NY, USA, 9-15 July 2016, pages 1293–
1299.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1591–1601, Doha, Qatar, October.
Association for Computational Linguistics.

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and
Maosong Sun. 2016. Representation learning of
knowledge graphs with entity descriptions. In Pro-
ceedings of the Thirtieth AAAI Conference on Arti-
ficial Intelligence, February 12-17, 2016, Phoenix,
Arizona, USA., pages 2659–2665.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2015.
Corpus-level fine-grained entity typing using con-
textual information. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715–725, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2016.
Intrinsic subspace evaluation of word embedding
representations. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 236–246,
Berlin, Germany, August. Association for Computa-
tional Linguistics.

Yadollah Yaghoobzadeh, Heike Adel, and Hinrich
Schütze. 2017. Noise mitigation for neural entity
typing and relation extraction. In EACL, Valencia,
Spain.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2016. Joint learning of the em-
bedding of words and entities for named entity dis-
ambiguation. pages 250–259, August.

Dani Yogatama, Daniel Gillick, and Nevena Lazic.
2015. Embedding methods for fine grained entity
type classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 291–296, Beijing, China, July.
Association for Computational Linguistics.

Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
HYENA: hierarchical type classification for entity
names. In COLING 2012, 24th International Con-
ference on Computational Linguistics, Proceedings
of the Conference: Posters, 8-15 December 2012,
Mumbai, India, pages 1361–1370.

Xiang Zhang and Yann LeCun. 2015. Text understand-
ing from scratch. CoRR, abs/1502.01710.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun.
2015. Character-level convolutional networks for
text classification. pages 649–657.

A Supplementary Material

588



All entities
Models 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23

01 MFT 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02 CLR(FORWARD) * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03 CLR(LSTM) * * 0 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
04 CLR(BiLSTM) * * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05 CLR(CNN) * * * * 0 * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
06 CLR(NSL) * * * * 0 0 * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
07 BOW * * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
08 WWLR(SkipG) * * * * * * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
09 WWLR(SSkipG) * * * * * * * * 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
10 SWLR * * * * * * * * * 0 * 0 0 0 0 0 0 0 0 0 0 0 0
11 BOW+CLR(NSL) * * * * 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
12 WWLR+CLR(CNN) * * * * * * * * * * * 0 * 0 0 0 0 0 0 0 0 0 0
13 SWLR+CLR(CNN) * * * * * * * * * * * 0 0 0 0 0 0 0 0 0 0 0 0
14 ELR(SkipG) * * * * * * * * * * * * * 0 0 * 0 0 0 0 0 0 0
15 ELR(SSkipG) * * * * * * * * * * * * * * 0 * 0 0 0 0 0 0 0
16 AGG-FIGER * * * * * * * * * * * * * 0 0 0 0 0 0 0 0 0 0
17 ELR+CLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
18 ELR+WWLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
19 ELR+SWLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
20 ELR+WWLR+CLR * * * * * * * * * * * * * * * * * * * 0 0 0 0
21 ELR+SWLR+CLR * * * * * * * * * * * * * * * * * * * 0 0 0 0
22 ELR+WWLR+CLR+TC * * * * * * * * * * * * * * * * * * * 0 0 0 0
23 ELR+SWLR+CLR+TC * * * * * * * * * * * * * * * * * * * * * 0 0

Head entities
Models 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23

01 MFT 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02 CLR(FORWARD) * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03 CLR(LSTM) * * 0 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
04 CLR(BiLSTM) * * 0 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05 CLR(CNN) * * * * 0 * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
06 CLR(NSL) * * * * 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
07 BOW * * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
08 WWLR(SkipG) * * * * * * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
09 WWLR(SSkipG) * * * * * * * * 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
10 SWLR * * * * * * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
11 BOW+CLR(NSL) * * * * 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
12 WWLR+CLR(CNN) * * * * * * * * 0 * * 0 * 0 0 0 0 0 0 0 0 0 0
13 SWLR+CLR(CNN) * * * * * * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
14 ELR(SkipG) * * * * * * * * * * * * * 0 0 * 0 0 0 0 0 0 0
15 ELR(SSkipG) * * * * * * * * * * * * * 0 0 * 0 0 0 0 0 0 0
16 AGG-FIGER * * * * * * * * * * * * * 0 0 0 0 0 0 0 0 0 0
17 ELR+CLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
18 ELR+WWLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
19 ELR+SWLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
20 ELR+WWLR+CLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
21 ELR+SWLR+CLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
22 ELR+WWLR+CLR+TC * * * * * * * * * * * * * * * * * 0 0 0 0 0 0
23 ELR+SWLR+CLR+TC * * * * * * * * * * * * * * * * * * * 0 0 0 0

Tail entities
Models 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23

01 MFT 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
02 CLR(FORWARD) * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03 CLR(LSTM) * * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
04 CLR(BiLSTM) * * * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05 CLR(CNN) * * * * 0 * * * 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
06 CLR(NSL) * * * * 0 0 * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
07 BOW * * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
08 WWLR(SkipG) * * * * 0 0 * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
09 WWLR(SSkipG) * * * * 0 0 * 0 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0
10 SWLR * * * * * * * * * 0 * 0 0 0 0 0 0 0 0 0 0 0 0
11 BOW+CLR(NSL) * * * * 0 0 * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
12 WWLR+CLR(CNN) * * * * * * * * * * * 0 0 0 0 * 0 0 0 0 0 0 0
13 SWLR+CLR(CNN) * * * * * * * * * * * 0 0 0 0 0 0 0 0 0 0 0 0
14 ELR(SkipG) * * * * * * * * * * * * * 0 0 * 0 0 0 0 0 0 0
15 ELR(SSkipG) * * * * * * * * * * * * * * 0 * 0 0 0 0 0 0 0
16 AGG-FIGER * * * * * * * * * 0 * 0 0 0 0 0 0 0 0 0 0 0 0
17 ELR+CLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
18 ELR+WWLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
19 ELR+SWLR * * * * * * * * * * * * * * * * 0 0 0 0 0 0 0
20 ELR+WWLR+CLR * * * * * * * * * * * * * * * * * 0 0 0 0 0 0
21 ELR+SWLR+CLR * * * * * * * * * * * * * * * * * * * 0 0 0 0
22 ELR+WWLR+CLR+TC * * * * * * * * * * * * * * * * * * * 0 0 0 0
23 ELR+SWLR+CLR+TC * * * * * * * * * * * * * * * * * * * * 0 0 0

Table 6: Significance-test results for accuracy measure for all, head and tail entities. If the result for the
model in a row is significantly larger than the result for the model in a column, then the value in the
corresponding (row,column) is * and otherwise is 0.

589


