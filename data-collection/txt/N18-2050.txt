



















































A Bi-Model Based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling


Proceedings of NAACL-HLT 2018, pages 309–314
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

A Bi-model based RNN Semantic Frame Parsing Model for Intent
Detection and Slot Filling

Yu Wang Yilin Shen
Samsung Research America

{yu.wang1, yilin.shen, hongxia.jin}@samsung.com

Hongxia Jin

Abstract

Intent detection and slot filling are two main
tasks for building a spoken language under-
standing(SLU) system. Multiple deep learning
based models have demonstrated good results
on these tasks . The most effective algorithms
are based on the structures of sequence to se-
quence models (or ”encoder-decoder” mod-
els), and generate the intents and semantic
tags either using separate models((Yao et al.,
2014; Mesnil et al., 2015; Peng and Yao, 2015;
Kurata et al., 2016; Hahn et al., 2011)) or a
joint model ((Liu and Lane, 2016a; Hakkani-
Tür et al., 2016; Guo et al., 2014)). Most of
the previous studies, however, either treat the
intent detection and slot filling as two sepa-
rate parallel tasks, or use a sequence to se-
quence model to generate both semantic tags
and intent. Most of these approaches use one
(joint) NN based model (including encoder-
decoder structure) to model two tasks, hence
may not fully take advantage of the cross-
impact between them. In this paper, new
Bi-model based RNN semantic frame pars-
ing network structures are designed to per-
form the intent detection and slot filling tasks
jointly, by considering their cross-impact to
each other using two correlated bidirectional
LSTMs (BLSTM). Our Bi-model structure
with a decoder achieves state-of-the-art result
on the benchmark ATIS data (Hemphill et al.,
1990; Tur et al., 2010), with about 0.5% intent
accuracy improvement and 0.9 % slot filling
improvement.

1 Introduction

The research on spoken language understanding
(SLU) system has progressed extremely fast dur-
ing the past decades. Two important tasks in an
SLU system are intent detection and slot filling.
These two tasks are normally considered as paral-
lel tasks but may have cross-impact on each other.
The intent detection is treated as an utterance clas-
sification problem, which can be modeled using

conventional classifiers including regression, sup-
port vector machines (SVMs) or even deep neu-
ral networks (Haffner et al., 2003; Sarikaya et al.,
2011). The slot filling task can be formulated as
a sequence labeling problem, and the most pop-
ular approaches with good performances are us-
ing conditional random fields (CRFs) and recur-
rent neural networks (RNN) as recent works (Xu
and Sarikaya, 2013).

Some works also suggested using one joint
RNN model for generating results of the two tasks
together, by taking advantage of the sequence
to sequence(Sutskever et al., 2014) (or encoder-
decoder) model, which also gives decent results as
in literature(Liu and Lane, 2016a).

In this paper, Bi-model based RNN structures
are proposed to take the cross-impact between two
tasks into account, hence can further improve the
performance of modeling an SLU system. These
models can generate the intent and semantic tags
concurrently for each utterance. In our Bi-model
structures, two task-networks are built for the pur-
pose of intent detection and slot filling. Each
task-network includes one BLSTM with or with-
out a LSTM decoder (Hochreiter and Schmidhu-
ber, 1997; Graves and Schmidhuber, 2005).

The paper is organized as following: In sec-
tion 2, a brief overview of existing deep learn-
ing approaches for intent detection and slot fill-
ings are given. The new proposed Bi-model based
RNN approach will be illustrated in detail in sec-
tion 3. In section 4, two experiments on different
datasets will be given. One is performed on the
ATIS benchmark dataset, in order to demonstrate
a state-of-the-art result for both semantic parsing
tasks. The other experiment is tested on our inter-
nal multi-domain dataset by comparing our new
algorithm with the current best performed RNN
based joint model in literature for intent detection
and slot filling.

309



2 Background

In this section, a brief background overview on
using deep learning and RNN based approaches
to perform intent detection and slot filling tasks is
given. The joint model algorithm is also discussed
for further comparison purpose.

2.1 Deep neural network for intent detection

Using deep neural networks for intent detection
is similar to a standard classification problem, the
only difference is that this classifier is trained un-
der a specific domain. For example, all data in
ATIS dataset is under the flight reservation do-
main with 18 different intent labels. There are
mainly two types of models that can be used: one
is a feed-forward model by taking the average of
all words’ vectors in an utterance as its input, the
other way is by using the recurrent neural network
which can take each word in an utterance as a vec-
tor one by one (Xu and Sarikaya, 2014).

2.2 Recurrent Neural network for slot filling

The slot filling task is a bit different from intent
detection as there are multiple outputs for the task,
hence only RNN model is a feasible approach for
this scenario. The most straight-forward way is
using single RNN model generating multiple se-
manctic tags sequentially by reading in each word
one by one (Liu and Lane, 2015; Mesnil et al.,
2015; Peng and Yao, 2015). This approach has
a constrain that the number of slot tags generated
should be the same as that of words in an utter-
ance. Another way to overcome this limitation
is by using an encoder-decoder model containing
two RNN models as an encoder for input and a
decoder for output (Liu and Lane, 2016a). The ad-
vantage of doing this is that it gives the system ca-
pability of matching an input utterance and output
slot tags with different lengths without the need of
alignment. Besides using RNN, It is also possible
to use the convolutional neural network (CNN) to-
gether with a conditional random field (CRF) to
achieve slot filling task (Xu and Sarikaya, 2013).

2.3 Joint model for two tasks

It is also possible to use one joint model for intent
detection and slot filling (Guo et al., 2014; Liu and
Lane, 2016a,b; Zhang and Wang, 2016; Hakkani-
Tür et al., 2016). One way is by using one en-
coder with two decoders, the first decoder will
generate sequential semantic tags and the second
decoder generates the intent. Another approach

is by consolidating the hidden states information
from an RNN slot filling model, then generates
its intent using an attention model (Liu and Lane,
2016a). Both of the two approaches demonstrates
very good results on ATIS dataset.

3 Bi-model RNN structures for joint
semantic frame parsing

Despite the success of RNN based sequence to se-
quence (or encoder-decoder) model on both tasks,
most of the approaches in literature still use one
single RNN model for each task or both tasks.
They treat the intent detection and slot filling
as two separate tasks. In this section, two new
Bi-model structures are proposed to take their
cross-impact into account, hence further improve
their performance. One structure takes the advan-
tage of a decoder structure and the other doesn’t.
An asynchronous training approach based on two
models’ cost functions is designed to adapt to
these new structures.

3.1 Bi-model RNN Structures
A graphical illustration of two Bi-model structures
with and without a decoder is shown in Figure 1.
The two structures are quite similar to each other
except that Figure 1a contains a LSTM based de-
coder, hence there is an extra decoder state st to
be cascaded besides the encoder state ht.
Remarks:
The concept of using information from multiple-
model/multi-modal to achieve better performance
has been widely used in deep learning (Dean et al.,
2012; Wang, 2017; Ngiam et al., 2011; Srivas-
tava and Salakhutdinov, 2012), system identifica-
tion (Murray-Smith and Johansen, 1997; Naren-
dra et al., 2014, 2015) and also reinforcement
learning field recently (Narendra et al., 2016;
Wang and Jin, 2018). Instead of using collective
information, in this paper, our work introduces a
totally new approach of training multiple neural
networks asynchronously by sharing their internal
state information.

3.1.1 Bi-model structure with a decoder
The Bi-model structure with a decoder is shown as
in Figure 1a. There are two inter-connected bidi-
rectional LSTMs (BLSTMs) in the structure, one
is for intent detection and the other is for slot fill-
ing. Each BLSTM reads in the input utterance se-
quences (x1, x2, · · · , xn) forward and backward,
and generates two sequences of hidden states hft
and hbt. A concatenation of hft and hbt forms a

310



𝒙𝟏: Flight 𝒙𝟐: from 𝒙𝟑: Denver 𝒙𝟒: to 𝒙𝟓: Philadelphia

𝒉𝟏
𝟏 𝒉𝟐

𝟏 𝒉𝟑
𝟏 𝒉𝟒

𝟏 𝒉𝟓
𝟏

𝒉𝟏
𝟐 𝒉𝟐

𝟐 𝒉𝟑
𝟐 𝒉𝟒

𝟐 𝒉𝟓
𝟐

𝒉𝟏
𝟐

𝒉𝟏
𝟏 𝒉𝟐

𝟐

𝒉𝟐
𝟏 𝒉𝟑

𝟏 𝒉𝟑
𝟐

𝒉𝟒
𝟏 𝒉𝟒

𝟐

𝒉𝟓
𝟏 𝒉𝟓

𝟐

Intent: Flight

O O O ToLocFromLoc

BLSTM

LSTM

BLSTM

LSTM

𝒉𝟏
𝒊

𝒉𝟐
𝒊

Flow of 

hidden states

𝒔𝟏
𝟏 𝒔𝟐

𝟏 𝒔𝟑
𝟏 𝒔𝟒

𝟏

𝒔𝟏
𝟐 𝒔𝟐

𝟐 𝒔𝟑
𝟐 𝒔𝟒

𝟐

𝒈𝟏 ∙ :

𝒈𝟐 ∙ :

𝒚𝟏
𝟐 𝒚𝟐

𝟐 𝒚𝟑
𝟐 𝒚𝟒

𝟐 𝒚𝟓
𝟐

𝒚𝒊𝒏𝒕𝒆𝒏𝒕
𝟏

Intent detection

task-network

Slot filling model 

task-network

(a) Bi-model structure with a decoder

Intent detection

task-network

Slot filling model 

task-network

𝒙𝟐: from 𝒙𝟑: Denver 𝒙𝟒: to 𝒙𝟓: Philadelphia

𝒉𝟏
𝟐

𝒉𝟓
𝟏 𝒉𝟓

𝟐

Intent: Flight

O O O ToLocFromLoc

𝒉𝟏
𝟏

BLSTM

BLSTM

𝒉𝟏
𝒊

𝒉𝟐
𝒊

𝒉𝟐
𝟐 𝒉𝟐

𝟏 𝒉𝟑
𝟐 𝒉𝟑

𝟏 𝒉𝟒
𝟐 𝒉𝟒

𝟏 𝒉𝟓
𝟐 𝒉𝟓

𝟏

𝒉𝟒
𝟏 𝒉𝟒

𝟐
𝒉𝟑
𝟏 𝒉𝟑

𝟐𝒉𝟐
𝟏 𝒉𝟐

𝟐𝒉𝟏
𝟏 𝒉𝟏

𝟐

Flow of 

hidden states

𝒚𝟏
𝟐 𝒚𝟐

𝟐 𝒚𝟑
𝟐 𝒚𝟒

𝟐 𝒚𝟓
𝟐

𝒚𝒊𝒏𝒕𝒆𝒏𝒕
𝟏

𝒙𝟏: Flight

(b) Bi-model structure without a decoder

Figure 1: Bi-model structure

final BLSTM state ht = [hft, hbt] at time step t.
Hence, Our bidirectional LSTM fi(·) generates a
sequence of hidden states (hi1, h

i
2, · · · , hin), where

i = 1 corresponds the network for intent detection
task and i = 2 is for the slot filling task.

In order to detect intent, hidden state h1t is com-
bined together with h2t from the other bidirectional
LSTM f2(·) in slot filling task-network to generate
the state of g1(·), s1t , at time step t:

s1t = φ(s
1
t−1, h

1
n−1, h

2
n−1)

y1intent = argmax
ŷ1n

P (ŷ1n|s1n−1, h1n−1, h2n−1) (1)

where ŷ1n contains the predicted probabilities for
all intent labels at the last time step n.

For the slot filling task, a similar network struc-
ture is constructed with a BLSTM f2(·) and a
LSTM g2(·). f2(·) is the same as f1(·), by reading
in the a word sequence as its input. The difference
is that there will be an output y2t at each time step
t for g2(·), as it is a sequence labeling problem. At
each step t:

s2t = ψ(h
2
t−1, h

1
t−1, s

2
t−1, y

2
t−1)

y2t = argmax
ŷ2t

P (ŷ2t |h1t−1, h2t−1, s2t−1, y2t−1) (2)

where y2t is the predicted semantic tags at time step
t.

3.1.2 Bi-Model structure without a decoder
The Bi-model structure without a decoder is
shown as in Figure 1b. In this model, there is no
LSTM decoder as in the previous model.

For the intent task, only one predicted output la-
bel y1intent is generated from BLSTM f1(·) at the
last time step n, where n is the length of the ut-
terance. Similarly, the state value h1t and output
intent label are generated as:

h1t = φ(h
1
t−1, h

2
t−1)

y1intent = argmax
ŷ1n

P (ŷ1n|h1n−1, h2n−1) (3)

For the slot filling task, the basic structure of
BLSTM f2(·) is similar to that for the intent de-
tection task f1(·), except that there is one slot tag
label y2t generated at each time step t. It also
takes the hidden state from two BLSTMs f1(·) and
f2(·), i.e. h1t−1 and h2t−1, plus the output tag y2t−1
together to generate its next state value h2t and also
the slot tag y2t . To represent this as a function
mathematically:

h2t = ψ(h
2
t−1, h

1
t−1, y

2
t−1)

y2t = argmax
ŷ2t

P (ŷ2t |h1t−1, h2t−1, y2t−1) (4)

3.1.3 Asynchronous training
One of the major differences in the Bi-model
structure is its asynchronous training, which trains
two task-networks based on their own cost func-
tions in an asynchronous manner. The loss func-
tion for intent detection task-network is L1, and
for slot filling is L2. L1 and L2 are defined using
cross entropy as:

L1 , −
k∑

i=1

ŷ1,iintent log(y
1,i
intent) (5)

and

L2 , −
n∑

j=1

m∑

i=1

ŷ2,ij log(y
2,i
j ) (6)

where k is the number of intent label types, m is
the number of semantic tag types and n is the num-
ber of words in a word sequence. In each train-
ing iteration, both intent detection and slot filling
networks will generate a groups of hidden states
h1 and h2 from the models in previous iteration.
The intent detection task-network reads in a batch

311



of input data xi and hidden states h2, and gener-
ates the estimated intent labels ŷ1intent. The intent
detection task-network computes its cost based on
function L1 and trained on that. Then the same
batch of data xi will be fed into the slot filling task-
network together with the hidden state h1 from in-
tent task-network, and further generates a batch of
outputs y2i for each time step. Its cost value is then
computed based on cost function L2, and further
trained on that.

The reason of using asynchronous training ap-
proach is because of the importance of keeping
two separate cost functions for different tasks. Do-
ing this has two main advantages:
1. It filters the negative impact between two tasks
in comparison to using only one joint model, by
capturing more useful information and overcom-
ing the structural limitation of one model.
2. The cross-impact between two tasks can only
be learned by sharing hidden states of two models,
which are trained using two cost functions sepa-
rately.

4 Experiments

In this section, our new proposed Bi-model struc-
tures are trained and tested on two datasets, one
is the public ATIS dataset (Hemphill et al., 1990)
containing audio recordings of flight reservations,
and the other is our self-collected datset in three
different domains: Food, Home and Movie. The
ATIS dataset used in this paper follows the same
format as in (Liu and Lane, 2015; Mesnil et al.,
2015; Xu and Sarikaya, 2013; Liu and Lane,
2016a). The training set contains 4978 utterance
and the test set contains 893 utterance, with a to-
tal of 18 intent classes and 127 slot labels. The
number of data for our self-collected dataset will
be given in the corresponding experiment sections
with a more detailed explanation. The perfor-
mance is evaluated based on the classification ac-
curacy for intent detection task and F1-score for
slot filling task.

4.1 Training Setup

The layer sizes for both the LSTM and BLSTM
networks in our model are chosen as 200. Based
on the size of our dataset, the number of hidden
layers is chosen as 2 and Adam optimization is
used as in (Kingma and Ba, 2014). The size of
word embedding is 300, which are initialized ran-
domly at the beginning of experiment.

4.2 Performance on the ATIS dataset

Our first experiment is conducted on the ATIS
benchmark dataset, and compared with the current
existing approaches, by evaluating their intent
detection accuracy and slot filling F1 scores. A

Model F1 Score Intent Accuracy

Recursive NN 93.96% 95.4%
(Guo et al., 2014)

Joint model with recurrent intent
and slot label context

94.47% 98.43%

(Liu and Lane, 2016b)
Joint model with recurrent slot

label context
94.64% 98.21%

(Liu and Lane, 2016b)
RNN with Label Sampling 94.89% NA

(Liu and Lane, 2015)
Hybrid RNN 95.06% NA

(Mesnil et al., 2015)
RNN-EM 95.25% NA

(Peng and Yao, 2015)
CNN CRF 95.35% NA

(Xu and Sarikaya, 2013)
Encoder-labeler Deep LSTM 95.66% NA

(Kurata et al., 2016)
Joint GRU Model (W) 95.49% 98.10%

(Zhang and Wang, 2016)
Attention Encoder-Decoder NN 95.87% 98.43%

(Liu and Lane, 2016a)
Attention BiRNN 95.98% 98.21%

(Liu and Lane, 2016a)

Bi-model without a decoder 96.65% 98.76%
Bi-model with a decoder 96.89% 98.99%

Table 1: Performance of Different Models on ATIS
Dataset

detailed comparison is given in Table 1. Some
of the models are designed for single slot filling
task, hence only F1 scores are given. It can
be observed that the new proposed Bi-model
structures outperform the current state-of-the-art
results on both intent detection and slot filling
tasks, and the Bi-model with a decoder also
outperform that without a decoder on our ATIS
dataset. The current Bi-model with a decoder
shows the state-of-the-art performance on ATIS
benchmark dataset with 0.9% improvement on F1
score and 0.5% improvement on intent accuracy.
Remarks:
1. It is worth noticing that the complexities of
encoder-decoder based models are normally
higher than the models without using encoder-
decoder structures, since two networks are
used and more parameters need to be updated.
This is another reason why we use two models
with/without using encoder-decoder structures to
demonstrate the new bi-model structure design.
It can also be observed that the model with a
decoder gives a better result due to its higher
complexity.
2. It is also shown in the table that the joint
model in (Liu and Lane, 2015, 2016a) achieves

312



better performance on intent detection task with
slight degradation on slot filling, so a joint model
is not necessary always better for both tasks.
The bi-model approach overcomes this issue by
generating two tasks’ results separately.
3. Despite the absolute improvement of intent
accuracy and F1 scores are only 0.5% and 0.9%
on ATIS dataset, the relative improvement is not
small. For intent accuracy, the number of wrongly
classified utterances in test dataset reduced
from 14 to 9, which gives us the 35.7% relative
improvement on intent accuracy. Similarly, the
relative improvement on F1 score is 22.63%.

4.3 Performance on multi-domain data

In this experiment, the Bi-model structures are fur-
ther tested on an internal collected dataset from
our users in three domains: food, home and movie.
There are 3 intents for each domain, 15 semantic
tags in food domain, 16 semantic tags in home do-
main, 14 semantic tags in movie domain. The data
size of each domain is listed as in Table 2, and the
split is 70% for training, 10% for validation and
20% for test.

Due to the space limitation, only the best per-
formed semantic frame parsing model on ATIS
dataset in literature,i.e. attention based BiRNN
(Liu and Lane, 2016a) is used for comparison with
our Bi-model structures. Table 2 shows a perfor-

Domain SLU model Size F1
Score

Accuracy

Movie
Attention BiRNN 979 92.1% 92.86%

Bi-model without a
decoder

979 93.3% 94.89%

Bi-model with a decoder 979 93.8% 95.91%

Food
Attention BiRNN 983 92.3% 98.48%

Bi-model without a
decoder

983 93.6% 98.98%

Bi-model with a decoder 983 95.8% 99.49%

Home
Attention BiRNN 689 96.5% 97.83%

Bi-model without a
decoder

689 97.8% 98.55%

Bi-model with a decoder 689 98.2% 99.27%

Table 2: Performance Comparison between Bi-model
Structures and Attention BiRNN

mance comparison in three domains of data. The
Bi-model structure with a decoder gives the best
performance in all cases based on its intent accu-
racy and slot filling F1 score. The intent accuracy
has at least 0.5% improvement, the F1 score im-
provement is around 1% to 3% for different do-
mains.

5 Conclusion

In this paper, a novel Bi-model based RNN seman-
tic frame parsing model for intent detection and
slot filling is proposed and tested. Two substruc-
tures are discussed with the help of a decoder or
not. The Bi-model structures achieve state-of-the-
art performance for both intent detection and slot
filling on ATIS benchmark data, and also surpass
the previous best SLU model on the multi-domain
data. The Bi-model based RNN structure with a
decoder also outperforms the Bi-model structure
without a decoder on both ATIS and multi-domain
data.

References
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,

Matthieu Devin, Mark Mao, Andrew Senior, Paul
Tucker, Ke Yang, Quoc V Le, et al. 2012. Large
scale distributed deep networks. In Advances in
neural information processing systems. pages 1223–
1231.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works 18(5):602–610.

Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey
Zweig. 2014. Joint semantic utterance classifica-
tion and slot filling with recursive neural networks.
In Spoken Language Technology Workshop (SLT),
2014 IEEE. IEEE, pages 554–559.

Patrick Haffner, Gokhan Tur, and Jerry H Wright.
2003. Optimizing svms for complex call classifica-
tion. In Acoustics, Speech, and Signal Processing,
2003. Proceedings.(ICASSP’03). 2003 IEEE Inter-
national Conference on. IEEE, volume 1, pages I–I.

Stefan Hahn, Marco Dinarelli, Christian Raymond,
Fabrice Lefevre, Patrick Lehnen, Renato De Mori,
Alessandro Moschitti, Hermann Ney, and Giuseppe
Riccardi. 2011. Comparing stochastic approaches
to spoken language understanding in multiple lan-
guages. IEEE Transactions on Audio, Speech, and
Language Processing 19(6):1569–1583.

Dilek Hakkani-Tür, Gökhan Tür, Asli Celikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame
parsing using bi-directional rnn-lstm. In INTER-
SPEECH. pages 715–719.

Charles T Hemphill, John J Godfrey, George R Dod-
dington, et al. 1990. The atis spoken language sys-
tems pilot corpus. In Proceedings of the DARPA
speech and natural language workshop. pages 96–
101.

313



Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu.
2016. Leveraging sentence-level information with
encoder lstm for semantic slot filling. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing. pages 2077–2083.

Bing Liu and Ian Lane. 2015. Recurrent neural net-
work structured output prediction for spoken lan-
guage understanding. In Proc. NIPS Workshop
on Machine Learning for Spoken Language Under-
standing and Interactions.

Bing Liu and Ian Lane. 2016a. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. Interspeech 2016 pages 685–689.

Bing Liu and Ian Lane. 2016b. Joint online spoken lan-
guage understanding and language modeling with
recurrent neural networks. In 17th Annual Meeting
of the Special Interest Group on Discourse and Dia-
logue. page 22.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.
2015. Using recurrent neural networks for slot fill-
ing in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech and Language Pro-
cessing (TASLP) 23(3):530–539.

Roderick Murray-Smith and T Johansen. 1997. Mul-
tiple model approaches to nonlinear modelling and
control. CRC press.

Kumpati S Narendra, Yu Wang, and Wei Chen. 2014.
Stability, robustness, and performance issues in sec-
ond level adaptation. In American Control Confer-
ence (ACC), 2014. IEEE, pages 2377–2382.

Kumpati S Narendra, Yu Wang, and Wei Chen. 2015.
Extension of second level adaptation using multiple
models to siso systems. In American Control Con-
ference (ACC), 2015. IEEE, pages 171–176.

Kumpati S Narendra, Yu Wang, and Snehasis
Mukhopadhay. 2016. Fast reinforcement learning
using multiple models. In Decision and Control
(CDC), 2016 IEEE 55th Conference on. IEEE, pages
7183–7188.

Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan
Nam, Honglak Lee, and Andrew Y Ng. 2011. Multi-
modal deep learning. In Proceedings of the 28th in-
ternational conference on machine learning (ICML-
11). pages 689–696.

Baolin Peng and Kaisheng Yao. 2015. Recurrent neural
networks with external memory for language under-
standing. arXiv preprint arXiv:1506.00195 .

Ruhi Sarikaya, Geoffrey E Hinton, and Bhuvana Ram-
abhadran. 2011. Deep belief nets for natural lan-
guage call-routing. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2011 IEEE International
Conference on. IEEE, pages 5680–5683.

Nitish Srivastava and Ruslan R Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In Advances in neural information process-
ing systems. pages 2222–2230.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Gokhan Tur, Dilek Hakkani-Tür, and Larry Heck.
2010. What is left to be understood in atis? In Spo-
ken Language Technology Workshop (SLT), 2010
IEEE. IEEE, pages 19–24.

Yu Wang. 2017. A new concept using lstm neural net-
works for dynamic system identification. In Amer-
ican Control Conference (ACC), 2017. IEEE, pages
5324–5329.

Yu Wang and Hongxia Jin. 2018. A boosting-based
deep neural networks algorithm for reinforcement
learning. In American Control Conference (ACC),
2018. IEEE.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular crf for joint in-
tent detection and slot filling. In Automatic Speech
Recognition and Understanding (ASRU), 2013 IEEE
Workshop on. IEEE, pages 78–83.

Puyang Xu and Ruhi Sarikaya. 2014. Contextual do-
main classification in spoken language understand-
ing systems using recurrent neural network. In
Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on. IEEE,
pages 136–140.

Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-
offrey Zweig, and Yangyang Shi. 2014. Spoken lan-
guage understanding using long short-term memory
neural networks. In Spoken Language Technology
Workshop (SLT), 2014 IEEE. IEEE, pages 189–194.

Xiaodong Zhang and Houfeng Wang. 2016. A joint
model of intent determination and slot filling for
spoken language understanding. In Proceedings
of the Twenty-Fifth International Joint Conference
on Artificial Intelligence. AAAI Press, pages 2993–
2999.

314


