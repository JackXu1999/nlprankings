



















































Evaluating Composition Models for Verb Phrase Elliptical Sentence Embeddings


Proceedings of NAACL-HLT 2019, pages 261–271
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

261

Evaluating Composition Models for Verb Phrase Elliptical Sentence
Embeddings

Gijs Wijnholds
Queen Mary University of London
g.j.wijnholds@qmul.ac.uk

Mehrnoosh Sadrzadeh
Queen Mary University of London

mehrnoosh.sadrzadeh@qmul.ac.uk

Abstract

Ellipsis is a natural language phenomenon
where part of a sentence is missing and its
information must be recovered from its sur-
rounding context, as in “Cats chase dogs and
so do foxes.”. Formal semantics has differ-
ent methods for resolving ellipsis and recov-
ering the missing information, but the prob-
lem has not been considered for distributional
semantics, where words have vector embed-
dings and combinations thereof provide em-
beddings for sentences. In elliptical sentences
these combinations go beyond linear as copy-
ing of elided information is necessary. In
this paper, we develop different models for
embedding VP-elliptical sentences. We ex-
tend existing verb disambiguation and sen-
tence similarity datasets to ones containing el-
liptical phrases and evaluate our models on
these datasets for a variety of non-linear com-
binations and their linear counterparts. We
compare results of these compositional mod-
els to state of the art holistic sentence en-
coders. Our results show that non-linear addi-
tion and a non-linear tensor-based composition
outperform the naive non-compositional base-
lines and the linear models, and that sentence
encoders perform well on sentence similarity,
but not on verb disambiguation.

1 Introduction

Compositional distributional semantics has so far
relied on a tight connection between syntactic and
semantic resources. Based on the assembly prin-
ciple of compositionality, these models assign a
sentence vector by applying a linear map to the in-
dividual word embeddings therein. The meaning
of “cats chase dogs” is as follows in (1) additive,
(2) multiplicative, and (3) tensor-based models:

(1)
−−→
cats+

−−−→
chase+

−−→
dogs

(2)
−−→
cats�

−−−→
chase�

−−→
dogs

(3)
−−→
cats> × (chase×

−−→
dogs)

Some linguistic phenomena, however, rely on
copying resources while computing meaning;
canonical examples thereof are anaphora and el-
lipsis, exemplified below:

(a) Cats clean themselves.
(b) Cats chase dogs, children do too.

More complex examples involve a structural am-
biguity such as the following:

(c) Cats chase their tail, dogs too.

These lend themselves to a strict (dogs chase the
cat’s tail) and a sloppy reading (dogs chase their
own tail). In these examples, the meaning of at
least one part of the sentence is used twice, e.g.
the subject in a, the verb phrase “chase dogs” in b.
Such cases can often be extended to a situation in
which a meaning is used more than twice, e.g. in
“Cats chase their tail, dogs too, and so do foxes”.

In order to develop distributional semantics for
such sentences while respecting the principle of
compositionality, one has a choice between a lin-
ear or a non-linear composition of resources. In
the linear case, no information is copied, resulting
in vector embeddings such as the following one
(when only considering content words):

−−→
cats+

−−−→
chase+

−−→
dogs+

−−−−−→
children

In the non-linear case, the necessary resources are
copied to resolve the ellipsis, resulting in vectors
embeddings such as:

−−→
cats+

−−−→
chase+

−−→
dogs+

−−−−−→
children+

−−−→
chase+

−−→
dogs

One has the same choice when dealing with mul-
tiplicative and tensor-based models. The ques-
tion is which of these composition frameworks,
i.e. linear versus non-linear, provides a better
choice for embedding elliptical sentences. To our
knowledge, this has remained an open question:
although some theoretical work has been done to



262

model verb phrase ellipsis in compositional dis-
tributional semantics (Wijnholds and Sadrzadeh,
2018), none of the existing datasets or evaluation
methods for distributional semantics focus on el-
liptical phenomena.

In this paper, we provide some answers. Our
starting point is the lambda logical forms of sen-
tences, e.g. those produced by the approach of
Dalrymple et al. (1991), which uses a higher or-
der unification algorithm to resolve ellipsis. We
apply to these the lambdas-to-vectors mapping of
Muskens and Sadrzadeh (2016, 2017) to homo-
morphically map the lambda terms into concrete
vector embeddings resulting from a multitude of
composition operators, such as addition, multipli-
cation, and tensor-based. We work with four vec-
tor spaces (count-based, Word2Vec, GloVe, Fast-
Text) and three different verb embeddings, and
contrast our compositional models with state of
the art holistic sentence encoders.

We evaluate the sentence embeddings by us-
ing them in a verb disambiguation and in a sen-
tence similarity task, created by extending previ-
ous SVO tasks from Grefenstette and Sadrzadeh
(2011a) and Kartsaklis and Sadrzadeh (2013) to an
elliptical setting, and obtaining new human judge-
ments using the Amazon Mechanical Turk crowd-
sourcing tool. Our experiments show that in both
tasks, the models that use a non-linear form of
composition perform better than the models whose
composition framework is linear, suggesting that
resolving ellipsis contributes to the quality of the
sentence embedding.

2 Background

Single-Word Embeddings: Distributional se-
mantics on the word level relies on the embed-
ding of word meaning in a vectorial form: by tak-
ing context words as the basis of a vector space
one computes the vector components of each word
by considering its distribution among corpus data.
Then a similarity measure is defined on the vector
space via the cosine similarity. In a count-based
model, the context is taken to be a linear win-
dow and the corpus is traversed to collect raw co-
occurrence counts. Then, a weighting scheme is
applied to smooth the raw frequencies in the mean-
ing representation. More discussion on count-
based vector space models can be found in (Tur-
ney and Pantel, 2010), and a systematic study of
the parameters of count-based word embeddings

is given by (Kiela and Clark, 2014).

With the rise of deep learning techniques, much
attention has been given to neural word embed-
dings (Mikolov et al., 2013; Pennington et al.,
2014; Bojanowski et al., 2017), which try to pre-
dict rather than observe, the context of a word
by optimising an objective function based on the
probability of observing a context.

Compositional Models: The key idea of com-
positional models is that the meaning of elemen-
tary constituents can be combined in a structured
way to obtain a representation for larger phrases.
In a distributional setting, having a compositional
operator is imperative: a data-driven model would
not be adequate given the sparsity of full sentences
in a corpus. Moreover, it is not clear that sentences
follow the distributional hypothesis.

Concrete composition operators can roughly be
classified as simple and tensor-based. Simple
models add or multiply the word vectors to obtain
a sentence vector. The work of Mitchell and Lap-
ata (2010) experiments with these models. Tensor-
based models differ in that they represent complex
words as vectors of a higher order: Baroni and
Zamparelli (2010) represents adjectives as matri-
ces which, applied to a word vector produce a vec-
tor representation of the compound adjective-noun
combination. The account of (Coecke et al., 2010,
2013; Clark, 2015) generalises this to higher-order
tensors, e.g. cubes for transitive verbs and hy-
percubes for ditransitive verbs. The benefit of a
type-driven approach over the simple models is
that they respect the grammatical structure of sen-
tences: the meaning of “man bites dog” is distinct
from that of “dog bites man” whereas in an ad-
ditive/multiplicative model they would be identi-
cal. The trade-off is that the tensors themselves
have to be learnt; where Baroni and Zampar-
elli (2010) apply regression learning to learn the
content of adjective matrices, for transitive verbs
there have been several approaches using multi-
step regression learning (Grefenstette et al., 2013),
relational learning (Grefenstette and Sadrzadeh,
2011a), or a combination of co-occurrence infor-
mation with machine learning techniques (Polaj-
nar et al., 2014a,b; Fried et al., 2015). A compara-
tive study between count-based and neural embed-
dings in a compositional setting was carried out by
(Milajevs et al., 2014).

Neural composition turns the problem of com-
positionality around by learning the composition



263

operator instead of predicting the result. Examples
are Skip-Thought Vectors (Kiros et al., 2015), the
Distributed Bag of Words model (Le and Mikolov,
2014), InferSent (Conneau et al., 2017), and Uni-
versal Sentence Encoder (Cer et al., 2018).

Ellipsis, Formally: There exists many formal
approaches to ellipsis and anaphora in the litera-
ture. These have generally taken either a syntactic
or a semantic form1. Examples of the syntactic ap-
proaches are in the work of Hendriks and Dekker
(1995); Morrill and Valentı́n (2015); Jäger (2006);
Kubota and Levine (2017); these use directional
extensions of categorial grammars that allow for
the syntactic types at the site of ellipsis be unified
with copies of the types at the antecedent of the el-
liptical phrase. Another approach deletes the syn-
tactic structure at the ellipsis site and reconstruct it
by copying across the antecedent structure (Fiengo
and May, 1994; Merchant, 2004).

Semantic approaches (Dalrymple et al., 1991;
Szabolcsi, 1987; Pulman, 1997) assume that ellip-
sis involves underspecification of content and re-
solve this by producing a predicate via a suitable
abstraction from the antecedent. For instance, the
elliptical phrase (b) “Cats chase dogs, children do
too”, will take an initial logical form (b1); a res-
olution step (b2) provides it with the lambda term
in (b3), which constitutes its final semantic form:

(b1) chase(cats, dogs) ∧ P (children)
(b2) P = λx.chase(x, dogs)

(b3) (b1) ;β chase(cats, dogs)
∧ chase(children, dogs)

The ambiguous example (d) “Cats chase their tails,
dogs too” is treated similarly, but can now obtain
its respective strict and sloppy readings by produc-
ing predicates (d1) and (d2) below:

(d2) P = λx.chase(x, tail(cats))

(d3) P = λx.chase(x, tail(x))

Mixed syntactic/semantic approaches have also
been proposed to cover wider ranges of phenom-
ena; see Kempson et al. (2015) for an overview.

The only existing work attempting to join el-
lipsis analysis with vector embeddings is the pro-
posal of (Kartsaklis et al., 2016), which is prelimi-
nary work and gives unwanted results2. Below, we
develop a new such approach.

1Although pragmatics approaches exist (Merchant, 2010),
we focus here on syntactic and semantic approaches.

2The meaning of “Bill brought apples and John pears” co-
incides with that of “Bill and John brought apples and pears”.

3 Embeddings for Elliptical Phrases

Vectors and their basic operations can be emu-
lated using a lambda calculus with constants for
the relevant operations, as shown in (Muskens and
Sadrzadeh, 2016). They assume a type I (a finite
index set) and R (modelling the real numbers) and
model any vector as a term of type V := IR; that
is, as a function from indices to real numbers. Ma-
trices can then be represented by typesM := IIR
and in general a tensor of rank n will have type
Tn := I1...InR. The standard operations like
scalar multiplication, addition, element wise mul-
tiplication and tensor contraction can be modelled
with lambda terms as follows:
· := λrvi.r · vi : RV V
+ := λvwi.vi + wi : V V V
� := λvwi.vi · wi : V V V
×1 := λmvij.

∑
j
mij · vj :MV V

×2 := λcvijk.
∑

k cijk · vk : T 3VM
The first three definitions above extend the arith-
metic operations of addition and multiplication on
real numbers in R to lists of numbers in IR and
define corresponding definitions on vectors, and
so � defines the pointwise multiplication of two
vectors. The operation×1 defines matrix multipli-
cation; ×2 defines the tensor contraction between
a cube c (in I3R) and a list of numbers v.

c H(c) T (c)
cn cn V
adj λv.(adj×1 v) V V
adv λv.(adv×1 v) V V
itv λv.(itv×1 v) V V
tv λuv.(tv×2 v)×1 u V V V
coord λP.λQ.P∇Q V V V
quant λvZ.Z(quant×1 v) V (V V )V

Table 1: Lambda Vector look up table for a tensor-
based composition model. cn: a common noun, adj:
adjective, adv: adverb, itv: intransitive verb, tv:
transitive verb, coord: coordinator, quant: gener-
alised quantifier; P,Q are variables of type V , and so
are v, u, Z is a variable of type VV,∇ is either � or +.

The vector semantics of a lambda term m is com-
puted by taking a homomorphic image over the set
of its constants c. This image is computed compo-
sitionally from the vector or tensor embeddings of
the constants c of m via their homomorphic im-
agesH(c), whose types are denoted by T (c).

Examples of these are given in Table 1 for a



264

tensor-based composition model, where the bold-
face c denotes the vector/tensor embedding of c.

Using this table, we obtain homomorphic im-
ages of any lambda term over the constants. For
instance, the lambda term of our exemplary re-
solved ellipsis phrase (b3) chase(cats, dogs) ∧
chase(children, dogs) is given the following se-
mantic, obtained by computingH(b3):

((chase×2 dogs)×1 cats)∇
((chase×2 dogs)×1 children)

The constituents of theH(c) entries of Table 1 are
only exemplary. Many other interpretations are
possible. For instance, taking vector embeddings
for all words and replacing all tensor contractions
and ∇ by + defines a purely additive model. The
concrete models for transitive sentences that were
evaluated by Milajevs et al. (2014) can all be de-
rived by varying the H(c) entries. Below are
the sentences obtained by using the Copy Object
(CO), Frobenius Additive (FA), Frobenius Multi-
plicative (FM) and Frobenius Outer (FO) instanti-
ations of the verb, respectively:

CO : λos.o� (verb×> s)
FA : λos.s� (verb× o) + o� (verb×> s)

FM : λos.s� (verb× o)� o� (verb×> s)
FO : λos.s� (verb× o)⊗ o� (verb×> s)

The vector semantics of the extensions of transi-
tive sentences with VP elliptical phrases are ob-
tained by taking each of the above as the seman-
tics of each conjunct of the lambda logical form
and interpreting the conjunction operation of ∧ as
either sum or multiplication.

4 Experimental Evaluation

For the evaluation of the model(s) in the previ-
ous section, we built two new datasets and experi-
mented with count based and neural vector spaces,
and sentence encoders.3.

4.1 Building new datasets
In order to experiment with ellipsis, we extended
the verb disambiguation dataset of Grefenstette
and Sadrzadeh (2011a) and the transitive sentence
similarity dataset of Kartsaklis and Sadrzadeh
(2013), henceforth GS2011 and KS2013.

3All models, the new datasets, and evaluation code
are available at github.com/gijswijnholds/
compdisteval-ellipsis

4.1.1 GS2011
The GS2011 verb disambiguation dataset con-
tains 10 verbs, each with two possible interpre-
tations. For each verb v and its two interpreta-
tions v1 and v2, the dataset contains human sim-
ilarity judgments for 10 subject-object combina-
tions. For instance, for the verb meet – ambigu-
ous between visit and satisfy – the dataset contains
the pairs 〈system meet requirements, system sat-
isfy requirements〉 and 〈system meet requirements,
system visit requirements〉. The more likely inter-
pretation is marked as HIGH whereas the unlikely
interpretation is marked LOW.

We extended this dataset as follows: for each
combination of a verb triple (v, v1, v2) and a
subject-object pair (s, o), where 〈s v o, s v1 o〉
is expected to have LOW similarity in the dataset
and 〈s v o, s v2 o〉 is thus expected to have HIGH
similarity, we selected a new subject s∗ from the
list of most frequent subjects for the verb v2 such
that it was significantly more frequent for v2 than
for v14. By doing so we strengthened the disam-
biguating effect of the context for each verb. The
subject was selected such that the resulting ellip-
tical phrase pairs made sense. For each combina-
tion and new subject considered, we added the two
sentence pairs in the elliptical form〈
s v o and s∗ does too, s v1 o and s∗ does too

〉〈
s v o and s∗ does too, s v2 o and s∗ does too

〉
For example, for the verb triple (draw, depict, at-
tract), and original sentence pairs

〈man draw sword, man depict sword 〉
〈man draw sword, man attract sword 〉

we selected the new subject artist and added two
pairs, comparing man draw sword and artist does
too with

man depict sword and artist does too
man attract sword and artist does too

We selected two new subjects for each combina-
tion, and in this way we obtained a dataset of
roughly 400 entries. New human judgments were
collected through Amazon Mechanical Turk, by
prepending the to each noun and putting the phrase
in the past tense. As with the original dataset,
participants were asked to judge the similarity be-
tween sentence pairs using a discrete number be-
tween 1 and 7; 1 for highly dissimilar, 7 for highly
similar. By inserting gold standard pairs of iden-
tical sentences we checked if participants were

4As found in the combined ukWaC+WackyPedia corpus.

https://github.com/gijswijnholds/compdisteval-ellipsis
https://github.com/gijswijnholds/compdisteval-ellipsis


265

trustworthy. We collected 25 judgments per sen-
tence pair but excluded participants that annotated
less than 20 entries of the total dataset. We ended
up with 55 different participants who ranked more
than 20 entries of the total dataset, to give a fi-
nal amount of ca. 9200 annotations. As an ex-
ample, the verb show was a very hard case to dis-
ambiguate in the GS2011 dataset: child show sign
had an average score of 2.5 with both child picture
sign and child express sign. In the new dataset,
with the extra subject patient, it got much clearer
that the verb had to be interpreted as express with
an average score of 5.869, versus 4.875 for picture.

4.1.2 KS2013
The KS2013 sentence similarity dataset contains
108 transitive sentence pairs annotated with hu-
man similarity judgments. As opposed to the
GS2011 dataset, subjects and objects of each sen-
tence pair are not the same, so several different
contexts get compared to one another. In this
sense, the KS2013 dataset aims to investigate the
role of content of individual words versus the
role of composition, as the similarity of sentences
might be predictable from the contribution of indi-
vidual words rather than the specific way of com-
posing them.

We extend this dataset to cover VP ellipsis by
following a similar procedure as for GS2011. For
each transitive sentence of the form s v o in the
dataset, we selected a new subject s∗ from a list of
most frequent subjects of the verb5 and built ellip-
tical entries s v o and s∗ does too in such a way
that the meaning of the original transitive sentence
got changed as little as possible and that the re-
sulting elliptical phrase made sense. We then con-
sidered every transitive sentence pair in the dataset
and added the new respective subjects to both sen-
tences. For example, for the pair〈
school encourage child, employee leave company

〉
we selected parent and student to get the new pair〈

school encourage child and parent does too,
employee leave company and student does too

〉
We chose two subjects for every original sen-
tence, generating four possibilities for each sen-
tence pair, and a new dataset of 432 entries. This
dataset was also annotated using Amazon Me-
chanical Turk, after putting each verb in the past
tense and prepending the to each noun in the

5Again taken from the ukWaC+Wackypedia corpus.

dataset. Gold standard pairs of identical sentences
were inserted to validate trustworthiness of partic-
ipants. The final dataset contains ca. 9800 annota-
tions by 42 different participants.

4.2 Vector Spaces
To provide a comprehensive study with robust
results, we used four vector spaces: a count
based vector space, and newly trained Word2Vec,
GloVe, and FastText spaces, as detailed below.

Count-Based: We used the combined ukWaC
and Wackypedia corpora6 to extract raw co-
occurrence counts, using as a basis the 2000 most
frequently occurring tokens (after excluding the 50
most frequent ones). When extracting counts, we
disregarded a list of stopwords that do not con-
tribute to the content of the vectors. We used a
context window of 5 around the focus word, and
PPMI as weighting scheme. These settings were
use in the original KS2013 dataset (Kartsaklis and
Sadrzadeh, 2013).

Word2Vec: The Word2Vec embeddings we
used were trained with the continuous bag of
words model of (Mikolov et al., 2013) (CBOW).
We trained this model on the combined and lem-
matised ukWaC and Wackypedia corpora, using
the implementation for Python available in the
gensim package7, with a minimum word fre-
quency of 50, a window of 5, dimensionality 300,
and 5 training iterations.

GloVe: The GloVe model (Pennington et al.,
2014) considers the ratio of co-occurrence prob-
abilities by minimising the least-squares objec-
tive between the dot product of two word embed-
dings and the log-probability of the words’ co-
occurrence. We trained a GloVe space on the
combined and lemmatised ukWaC and Wackype-
dia corpora, using the code provided by the orig-
inal authors8. Similar to the Word2Vec settings
above, we trained 300 dimensional vectors with a
minimum word frequency of 50 and a window of
5, but we trained with 15 iterations.

FastText: The FastText vectors are like
Word2Vec, except the word vector takes into ac-
count subword information: words are represented
as n-grams, for which vectors are trained. The fi-
nal word vector will then be the sum of its con-
stituent n-gram vectors (Bojanowski et al., 2017).
We trained a FastText space with the same settings

6wacky.sslmit.unibo.it
7radimrehurek.com/gensim
8nlp.stanford.edu/projects/glove

http://wacky.sslmit.unibo.it
https://radimrehurek.com/gensim
https://nlp.stanford.edu/projects/glove


266

as the Word2Vec space (CBOW, minimum word
frequency 50, dimensions 300, window 5, with 5
iterations), again using gensim.

4.2.1 Verb Matrices
In order to work with tensor-based models we had
to represent verbs as matrices rather than as vec-
tors. We generated verb tensors using two meth-
ods that have been used previously in the litera-
ture (Grefenstette and Sadrzadeh, 2011a; Kartsak-
lis and Sadrzadeh, 2014).

Relational: For each verb, its corresponding
matrix is obtained by summing over the tensor
product of the respective subject and object vec-
tors of the verb (subjects and objects collected
from the corpus):

verb =
∑
i

subji ⊗ obji

Kronecker: For each verb, its corresponding ma-
trix is obtained by taking the tensor product of the
verb vector with itself:

ṽerb =
−−→
verb⊗

−−→
verb

In the case of the count-based space, we trained
verb matrices of dimensions 2000 × 2000, for
the neural word embeddings the matrices had di-
mensions 300 × 300. We also experimented with
the skip-gram extension of Maillard and Clark
(2015) and the plausibility model of Polajnar et al.
(2014a) but excluded the results because the ob-
tained verb matrices were far below par.

4.3 Concrete Models
For the experiments, we had two main goals in
mind: primarily, we wanted to verify that resolv-
ing ellipsis contributes to the performance of a
compositional model. For this purpose we exper-
imented with non-linear models, i.e. models that
resolve the ellipsis (and thus use the verb and ob-
ject resources twice) versus linear models, which
do not resolve the ellipsis (and thus only use the
verb and object once). Our second goal was to in-
vestigate whether amongst the models that resolve
the ellipsis, the ones that did so in a tensor-based
way, i.e. using tensors instead of vectors to repre-
sent the verbs, performed better than additive and
multiplicative models, and how these compare to
holistic sentence encoders. Hence, we considered
three classes of models: linear vector models, non-
linear vector models and tensor-based models.

Linear Vector Models: These models use
every resource exactly once, following the pat-
tern −→w1 ? −→w2... ? −→wn for any sequence of words
w1w2...wn. For an elliptical phrase “subj verb obj
and subj∗ does too” it will compute the vector

−−→
subj ?

−−→
verb ?

−→
obj ?

−−→
and ?

−−−→
subj∗ ?

−−→
does ?

−→
too

where ? denotes either addition or multiplication.
Non-Linear Vector Models: Here, the as-

sumption is that ellipsis is resolved but models do
not respect word order. The meaning of “subj verb
obj and subj∗ does too” now is

−−→
subj ?

−−→
verb ?

−→
obj ?

−−−→
subj∗ ?

−−→
verb ?

−→
obj

Tensor-Based Models: These models all are
assumed to resolve ellipsis and are based on vari-
ous previous models (Grefenstette and Sadrzadeh,
2011b,a; Kartsaklis et al., 2012; Kartsaklis and
Sadrzadeh, 2014). Essentially, the tensor-based
meaning of “subj verb obj and subj∗ does too” is

T (
−−→
subj, verb,

−→
obj) ? T (

−−−→
subj∗, verb,

−→
obj)

where T is a transitive model from (Milajevs et al.,
2014) and ? interprets the conjunction of the two
subclauses. For the verb matrix we used either the
relational verb or the Kronecker verb, and for ?
we tried both addition and multiplication. We did
consider a model which simply adds or multiplies
the second subject without duplicating the verb
phrase, but it performed worse than non-linear ad-
dition and multiplication so we did not include it
in this paper.

Sentence Encoders: To compare the men-
tioned compositional models with state of the art
neural baselines, we carried out our experiments
with a four types of holistic sentence encoders,
that take arbitrary text as input and produce an em-
bedding. To properly compare with the composi-
tional models above, we gave three different inputs
to the encoders: a baseline encoding (Base), a re-
solved encoding (Res), and an encoding without
functional words (Abl), all as below:

Base: “subj verb obj and subj∗ does too”
Res: “subj verb obj and subj∗ verb obj”
Abl: “subj verb obj subj∗”

We used six concrete pretrained encoders, avail-
able online: 4800-dimensional embeddings from
the Skip-Thought model9, 300-dimensional em-
beddings from two Doc2Vec implementations

9github.com/ryankiros/skip-thoughts

https://github.com/ryankiros/skip-thoughts


267

(Lau and Baldwin, 2016)10, 4096-dimensional
embeddings from two InferSent encoders11, and
512-dimensional embeddings from Universal Sen-
tence Encoder12.

5 Results

To validate the quality of the trained word spaces,
we evaluate on several standard word similarity
tasks: we used Rubenstein & Goodenough (RG,
1965), WordSim353 (WS353, 2001), Miller &
Charles (MC, 1991), SimLex-999 (SL999, 2015),
and the MEN dataset (Bruni et al., 2012). The re-
sults are displayed in Table 2, for the spaces de-
scribed in the previous section.

RG WS353 MC SL999 MEN

Count .6081 .3583 .5455 .2593 .5527
Word2Vec .8227 .6983 .7682 .4026 .7810
GloVe .8312 .6180 .7377 .3902 .7727
FastText .7724 .5461 .6961 .4021 .7683

Table 2: Spearman ρ scores on word similarity tasks.

Verb Disambiguation: Table 3 shows the results
of the linear, non-linear and tensor-based models
for this task, compared against a baseline in which
only the verb vector or verb matrix is compared.

Our first observation is that generally, the high-
est performing models were tensor-based. The
highest found correlation score was 0.5385 in the
count based space for a tensor-based model (CO
model above, Kronecker matrix, ∇ = +), with
the Frobenius Additive model giving the second
best result of 0.5263 (FA model above, Kronecker
matrix, ∇ = +). For the neural spaces, the high-
est performing models were mostly tensor-based
as well; they were always the Frobenius Addi-
tive (FA) model and the Frobenius Outer (FO)
model, using the relational tensor and addition
for the coordinator, except in the case of GloVe,
where the Copy Object (CO) model was the sec-
ond best. The only exception to this observation
is the GloVe space, for which the baseline Vector
Only model in fact has a higher correlation than
any other model on that space.
Our second observation is that the non-linear vari-
ants of the additive and multiplicative models
(which resolve ellipsis but in a naive way) show

10github.com/jhlau/doc2vec
11github.com/facebookresearch/InferSent
12tfhub.dev/google/

universal-sentence-encoder

CB W2V GloVe FT

Verb Only Vector .4363 .2406 .4451 .2290
Verb Only Tensor .3295 .4376 .3942 .3876

Add. Linear .4416 .2728 .3046 .1409
Mult. Linear 3250 -.0123 .1821 .2928

Add. Non-Linear .4448 .3275 .3262 .1399
Mult. Non-Linear .5029 .2087 .2446 .0440

Best Tensor .5385 .4621 .3688 .4937
2nd Best Tensor .5263 .4544 .3581 .4652

Table 3: Spearman ρ scores for the ellipsis disambigua-
tion experiment. CB: count based, W2V: Word2Vec,
FT: FastText.

D2V1 D2V2 ST IS1 IS2 USE

Base .1448 .2432 -.1932 .3471 .3841 .2693
Res .2340 .2980 -.1720 .3436 .3373 .2770
Abl .1899 .2423 -.1297 .3525 .3571 .2402

Table 4: Spearman ρ scores for the ellipsis disambigua-
tion experiment. D2V1: Doc2Vec1, D2V2: Doc2Vec
2, ST: Skip-Thought, IS1: InferSent 1, IS2: InferSent
2, USE: Universal Sentence Encoder.

an increased performance over the linear models
(which do not resolve ellipsis). All of this holds
for all the four vector spaces, except for the Fast-
Text space where the linear multiplicative model
achieves significantly higher correlation (0.2928)
than its non-linear counterpart (0.0440).

Overall, these results suggests that a logical re-
solving of ellipsis and further grammatical sensi-
tivity benefits the performance of composition.

One interesting fact about our results is that
the best compositional methods across the board
were those that interpret the coordinator ‘and’ as
addition; in set-theoretic semantics one interprets
this coordinator as set intersection, which corre-
sponds to multiplication rather than addition in a
vectorial setting. We suggest that the feature in-
tersection approach using multiplication leads to
sparsity in the resulting vectorial representation,
which then has a negative effect on the overall
result. This would explain the case of FastText,
since those vectors take into account subword in-
formation one would expect them to be more fine-
grained and therefore conflate more of their fea-
tures under multiplication.

The choice of verb matrix was mixed: for the
count-based models the Kronecker matrix worked
best, for the neural embeddings it was best to use

https://github.com/jhlau/doc2vec
https://github.com/facebookresearch/InferSent
https://tfhub.dev/google/universal-sentence-encoder/2
https://tfhub.dev/google/universal-sentence-encoder/2


268

the relational matrix.
In comparison, the sentence encoder results of

Table 4 show the same trend that suggests that re-
solving ellipsis improves the quality of the embed-
dings: with the exception of the two InferSent en-
coders, the resolved models gave a higher correla-
tion than their linear baseline. However, none of
the encoder models come near the results achieved
using the compositional models. Since the verb
disambiguation dataset contains pairs of sentence
that only differ in the verb, the task becomes very
much grammar-oriented, and so we argue that the
tensor-based models work better since they explic-
itly emphasise syntactic structure.

Sentence Similarity: For the extension of the
KS2013 sentence similarity dataset, the results are
shown in Table 5. We again wanted to see if re-
solving ellipsis benefits the compositional process.
This was in general true, although we observed a
different pattern to the previous experiment.

In all cases, except for the FastText space, we
saw that non-linear models in fact perform better
than their linear counterparts. But this time the
best tensor-based models only outperformed ad-
dition for the count-based space: the best models
scored 0.7410 and 0.7370 (respectively for the FO
and FA models above, Kronecker matrix,∇ = �).
Both Word2Vec and GloVe worked best with a
non-linear additive model, with Word2Vec achiev-
ing the overall highest correlation score of 0.7617,
and GloVe achieving 0.7103. For FastText, the
highest score of 0.7408 was achieved by linear ad-
dition. What is more, the multiplicative model did
not benefit from a non-linear approach in the case
of GloVe (from 0.3666 to 0.2439), and the addi-
tive model had a similar decline in performance
for the count-based space (from 0.7000 to 0.6808)
and FastText (0.7408 to 0.7387). We can see that
for the neural word embeddings the additive mod-
els work best, with all of them seeing a drop in
performance for the tensor-based models.

Again, the best count-based models use the
Kronecker matrix whereas the neural models ben-
efit the most from using the relational matrix.
However, this time the best count-based models
used multiplication for coordination, the neural
models preferring addition.

The sentence encoders worked a lot better in the
similarity task, with all non-linear resolved mod-
els outperforming the baseline model, and the In-
ferSent model even outperforming non-linear ad-

CB W2V GloVe FT

Verb Only Vector .4562 .5833 .4348 .6513
Verb Only Tensor .3946 .5664 .4426 .5337

Add. Linear .7000 .7258 .6964 .7408
Mult. Linear .6330 .1302 .3666 .1995

Add. Non-Linear .6808 .7617 .7103 .7387
Mult. Non-Linear .7237 .3550 .2439 .4500

Best Tensor .7410 .7061 .4907 .6989
2nd Best Tensor .7370 .6713 .4819 .6871

Table 5: Spearman ρ scores for the ellipsis similarity
experiment.

D2V1 D2V2 ST IS1 IS2 USE

Base .5901 .6188 .5851 .7785 .7009 .6463
Res .6878 .6875 .6039 .8022 .7486 .6791
Abl .1840 .6599 .4715 .7815 .7301 .6397

Table 6: Spearman ρ scores for the ellipsis similarity
experiment. D2V1: Doc2Vec1, D2V2: Doc2Vec 2, ST:
Skip-Thought, IS1: InferSent 1, IS2: InferSent 2, USE:
Universal Sentence Encoder.

dition on a Word2Vec space. We argue this is the
case for two reasons: first, the similarity dataset is
more diffuse than the verb disambiguation dataset
since sentence pairs now differ for every word in
the sentence, giving more opportunity to exploit
semantic similarity rather than syntactic similar-
ity. Second, the embeddings from the sentence
encoder are larger (4096), allowing them to effec-
tively store more information to benefit the simi-
larity score.

Overall we conclude again that resolving ellip-
sis improves the performance of composition, but
this time the InferSent sentence encoder seems
to work best, followed by the non-linear additive
compositional model on Word2Vec, with tensor-
based models only performing well in a count-
based space.

6 Conclusion

In this paper we experimented with vector space
semantics for VP ellipsis, working with a large va-
riety of compositional models. We created two
new datasets and compared the performance of
several compositional methods, both linear and
non-linear, across four vector spaces, and against
state of the art holistic sentence encoders.

Our main conclusion is that resolving ellipsis
improves performance: non-linear models almost



269

always performed better than linear ones in both
a verb disambiguation and a sentence similarity
task. The highest performance on the verb dis-
ambiguation task was given by a grammar-driven,
tensor-based model in a count-based vector space,
whereas for the similarity task, the highest per-
formance was achieved by the InferSent sentence
encoder, followed by a non-linear additive model
on a Word2Vec space. Although the neural word
embeddings and sentence encoders were largely
outperformed on the disambiguation dataset that
places more emphasis on syntactic structure than
on semantic similarity, they generally performed
better in the sentence similarity case, where the
distinction between syntactic and semantic simi-
larity is more diffuse.

Acknowledgments

The authors gratefully acknowledge the three
anonymous reviewers for their valuable com-
ments. Mehrnoosh Sadrzadeh is grateful to
the Royal Society for an International Exchange
Award IE161631 - Dynamic Vector Semantics for
Lambda Calculus Models of Natural Language
and discussion with Reinhard Muskens in this con-
text. Gijs Wijnholds would like to express grati-
tude for support by a Queen Mary Principal Stu-
dentship, and the Theory group of the School of
Electronic Engineering and Computer Science at
Queen Mary University of London. Both authors
would like to thank Ruth Kempson and Matthew
Purver for many helpful discussions.

References

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193. Association for Computational Linguis-
tics.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion of Computational Linguistics, 5(1):135–146.

Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
et al. 2018. Universal sentence encoder. arXiv
preprint arXiv:1803.11175.

Stephen Clark. 2015. Vector space models of lexical
meaning, chapter 16. John Wiley & Sons, Ltd.

Bob Coecke, Edward Grefenstette, and Mehrnoosh
Sadrzadeh. 2013. Lambek vs. Lambek: Functorial
vector space semantics and string diagrams for Lam-
bek calculus. Annals of Pure and Applied Logic,
164(11):1079–1100.

Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. arXiv
preprint arXiv:1003.4394.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680.

Mary Dalrymple, Stuart M Shieber, and Fernando CN
Pereira. 1991. Ellipsis and higher-order unification.
Linguistics and Philosophy, 14(4):399–452.

Robert Fiengo and Robert May. 1994. Indices and
identity. MIT Press, Cambridge, Mass.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th In-
ternational Conference on World Wide Web, pages
406–414. ACM.

Daniel Fried, Tamara Polajnar, and Stephen Clark.
2015. Low-Rank tensors for verbs in compositional
distributional semantics. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), volume 2, pages 731–736.

Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-Step regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).

Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1394–1404.
Association for Computational Linguistics.

Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of the GEMS 2011 Work-
shop on GEometrical Models of Natural Language

https://www.aclweb.org/anthology/D10-1115
https://www.aclweb.org/anthology/D10-1115
https://www.aclweb.org/anthology/D10-1115
https://www.aclweb.org/anthology/Q17-1010
https://www.aclweb.org/anthology/Q17-1010
https://www.aclweb.org/anthology/P12-1015
https://www.aclweb.org/anthology/P12-1015
https://arxiv.org/pdf/1803.11175.pdf
https://doi.org/10.1002/9781118882139.ch16
https://doi.org/10.1002/9781118882139.ch16
https://doi.org/10.1016/j.apal.2013.05.009
https://doi.org/10.1016/j.apal.2013.05.009
https://doi.org/10.1016/j.apal.2013.05.009
https://arxiv.org/pdf/1003.4394.pdf
https://arxiv.org/pdf/1003.4394.pdf
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://arxiv.org/pdf/cmp-lg/9503008.pdf
http://www.cs.tau.ac.il/~ruppin/p116-finkelstein.pdf
http://www.cs.tau.ac.il/~ruppin/p116-finkelstein.pdf
https://doi.org/10.3115/v1/P15-2120
https://doi.org/10.3115/v1/P15-2120
https://www.aclweb.org/anthology/W13-0112
https://www.aclweb.org/anthology/W13-0112
https://www.aclweb.org/anthology/D11-1129
https://www.aclweb.org/anthology/D11-1129
https://www.aclweb.org/anthology/W11-2507
https://www.aclweb.org/anthology/W11-2507


270

Semantics, pages 62–66. Association for Computa-
tional Linguistics.

Herman Hendriks and Paul Dekker. 1995. Links with-
out locations. In Proceedings of the Tenth Amster-
dam Colloquium, pages 339–358. Citeseer.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4):665–695.

Gerhard Jäger. 2006. Anaphora and type logical gram-
mar, volume 24. Trends in Logic. Springer Science
& Business Media.

Dimitri Kartsaklis, Matthew Purver, and Mehrnoosh
Sadrzadeh. 2016. Verb phrase ellipsis using Frobe-
nius algebras in categorical compositional distribu-
tional semantics. DSALT Workshop, European Sum-
mer School on Logic, Language and Information.

Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1590–1601.

Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL). Kyoto
Japan.

Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for categor-
ical distributional-compositional semantics: Theory
and experiments. In Proceedings of 24th Inter-
national Conference on Computational Linguistics
(COLING): Posters. Mumbai, India.

Ruth Kempson, Ronnie Cann, Arash Eshghi, Eleni
Gregoromichelaki, and Matthew Purver. 2015. El-
lipsis. In S. Lappin and C. Fox, editors, Hand-
book of Contemporary Semantic Theory, 2nd edi-
tion, chapter 4. Wiley.

Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continu-
ous Vector Space Models and their Compositionality
(CVSC), pages 21–30.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-Thought vectors. In
Advances in Neural Information Processing Sys-
tems, pages 3294–3302.

Yusuke Kubota and Robert Levine. 2017. Pseudo-
gapping as pseudo-VP-ellipsis. Linguistic Inquiry,
48(2):213–257.

Jey Han Lau and Timothy Baldwin. 2016. An empiri-
cal evaluation of doc2vec with practical insights into
document embedding generation. In Proceedings

of the 1st Workshop on Representation Learning for
NLP, pages 78–86.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

Jean Maillard and Stephen Clark. 2015. Learning
adjective meanings with a tensor-based skip-gram
model. In Proceedings of the Nineteenth Confer-
ence on Computational Natural Language Learning,
pages 327–331.

Jason Merchant. 2004. Fragments and ellipsis. Lin-
guistics and Philosophy, 27:661–738.

Jason Merchant. 2010. Three types of ellipsis.
Context-Dependence, Perspective and Relativity,
6:141.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh
Sadrzadeh, and Matthew Purver. 2014. Evaluating
neural word representations in tensor-based compo-
sitional settings. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 708–719.

George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1–28.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.

Glyn Morrill and Oriol Valentı́n. 2015. Computational
coverage of TLG: Nonlinearity. In Proceedings
of NLCS’15. Third Workshop on Natural Language
and Computer Science, volume 32, pages 51–63.
EasyChair Publications.

Reinhard Muskens and Mehrnoosh Sadrzadeh. 2016.
Context update for lambdas and vectors. In Interna-
tional Conference on Logical Aspects of Computa-
tional Linguistics, pages 247–254. Springer.

Reinhard Muskens and Mehrnoosh Sadrzadeh. 2017.
Lambdas, vectors, and word meaning in context.
In Proceedings of the 21st Amsterdam Colloquium,
pages 65–74.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543.

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.2438&rep=rep1&type=pdf
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.2438&rep=rep1&type=pdf
https://doi.org/10.1162/COLI_a_00237
https://doi.org/10.1162/COLI_a_00237
https://doi.org/10.1007/1-4020-3905-0
https://doi.org/10.1007/1-4020-3905-0
http://www.eecs.qmul.ac.uk/~mpurver/papers/kartsaklis-et-al16dsalt.pdf
http://www.eecs.qmul.ac.uk/~mpurver/papers/kartsaklis-et-al16dsalt.pdf
http://www.eecs.qmul.ac.uk/~mpurver/papers/kartsaklis-et-al16dsalt.pdf
https://www.aclweb.org/anthology/D13-1166
https://www.aclweb.org/anthology/D13-1166
https://arxiv.org/pdf/1405.2874.pdf
https://arxiv.org/pdf/1405.2874.pdf
https://arxiv.org/pdf/1405.2874.pdf
https://www.aclweb.org/anthology/C12-2054
https://www.aclweb.org/anthology/C12-2054
https://www.aclweb.org/anthology/C12-2054
http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470670738.html
http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470670738.html
https://doi.org/10.3115/v1/W14-1503
https://doi.org/10.3115/v1/W14-1503
https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf
https://muse.jhu.edu/article/659047/pdf?casa_token=Idhic4CxGNUAAAAA:HMIOKEhZ-qeU1KXrGJ19W9dacoXExXlGZpsINNuEslcjPqLivcaZWaOVbb5NGIMuVdTky5poo64
https://muse.jhu.edu/article/659047/pdf?casa_token=Idhic4CxGNUAAAAA:HMIOKEhZ-qeU1KXrGJ19W9dacoXExXlGZpsINNuEslcjPqLivcaZWaOVbb5NGIMuVdTky5poo64
https://doi.org/10.18653/v1/W16-1609
https://doi.org/10.18653/v1/W16-1609
https://doi.org/10.18653/v1/W16-1609
http://proceedings.mlr.press/v32/le14.pdf
http://proceedings.mlr.press/v32/le14.pdf
https://doi.org/10.18653/v1/K15-1035
https://doi.org/10.18653/v1/K15-1035
https://doi.org/10.18653/v1/K15-1035
https://doi.org/10.1007/s10988-005-7378-3
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.8711&rep=rep1&type=pdf
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
https://www.aclweb.org/anthology/D14-1079
https://www.aclweb.org/anthology/D14-1079
https://www.aclweb.org/anthology/D14-1079
https://doi.org/10.1080/01690969108406936
https://doi.org/10.1080/01690969108406936
https://doi.org/10.1111/j.1551-6709.2010.01106.x
https://doi.org/10.1111/j.1551-6709.2010.01106.x
https://doi.org/10.29007/96j5
https://doi.org/10.29007/96j5
https://doi.org/10.1007/978-3-662-53826-5_15
https://semanticsarchive.net/Archive/jZiM2FhZ/AC2017-Proceedings.pdf
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162


271

Tamara Polajnar, Luana Fagarasan, and Stephen Clark.
2014a. Reducing dimensions of tensors in type-
driven distributional semantics. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1036–
1046.

Tamara Polajnar, Laura Rimell, and Stephen Clark.
2014b. Using sentence plausibility to learn the
semantics of transitive verbs. arXiv preprint
arXiv:1411.7942.

Stephen G. Pulman. 1997. Higher order unification and
the interpretation of focus. Linguistics and Philoso-
phy, 20(1):73–115.

Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.

Anna Szabolcsi. 1987. Bound variables in syntax (are
there any?). Sixth Amsterdam Colloquium Proceed-
ings.

Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.

Gijs Wijnholds and Mehrnoosh Sadrzadeh. 2018. Clas-
sical copying versus quantum entanglement in natu-
ral language: The case of VP-ellipsis. In EPTCS
Proceedings of the Workshop on Compositional Ap-
proaches for Physics, NLP, and Social Sciences, vol-
ume 283, pages 103–119.

https://doi.org/10.3115/v1/D14-1111
https://doi.org/10.3115/v1/D14-1111
https://arxiv.org/pdf/1411.7942.pdf
https://arxiv.org/pdf/1411.7942.pdf
https://doi.org/10.1023/A:1005394619746
https://doi.org/10.1023/A:1005394619746
https://doi.org/10.1145/365628.365657
https://philarchive.org/archive/SZABVIv1
https://philarchive.org/archive/SZABVIv1
https://doi.org/10.1613/jair.2934
https://doi.org/10.1613/jair.2934
https://doi.org/10.1613/jair.2934
https://doi.org/10.4204/EPTCS.283.8
https://doi.org/10.4204/EPTCS.283.8
https://doi.org/10.4204/EPTCS.283.8

