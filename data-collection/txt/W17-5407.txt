



















































Cross-genre Document Retrieval: Matching between Conversational and Formal Writings


Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 48–53
Copenhagen, Denmark, September 8, 2017. c©2017 Association for Computational Linguistics

Cross-genre Document Retrieval:
Matching between Conversational and Formal Writings

Tomasz Jurczyk
Mathematics and Computer Science

Emory University
Atlanta, GA 30322, USA

tomasz.jurczyk@emory.edu

Jinho D. Choi
Mathematics and Computer Science

Emory University
Atlanta, GA 30322, USA

jinho.choi@emory.edu

Abstract

This paper challenges a cross-genre docu-
ment retrieval task, where the queries are
in formal writing and the target documents
are in conversational writing. In this task,
a query, is a sentence extracted from either
a summary or a plot of an episode in a TV
show, and the target document consists of
transcripts from the corresponding episode.
To establish a strong baseline, we employ
the current state-of-the-art search engine to
perform document retrieval on the dataset
collected for this work. We then introduce
a structure reranking approach to improve
the initial ranking by utilizing syntactic and
semantic structures generated by NLP tools.
Our evaluation shows an improvement of
more than 4% when the structure reranking
is applied, which is very promising.

1 Introduction

Document retrieval has been a central task in natu-
ral language processing and information retrieval.
The goal is to match a query against a set of docu-
ments. Over the last decade, advanced techniques
have emerged and provided powerful systems that
can accurately retrieve relevant documents (Blair
and Maron, 1985; Callan, 1994; Cao et al., 2006).
While the retrieval part is crucial, proper ranking of
the retrieved documents can significantly improve
the overall user satisfation by putting more relevant
documents at the top (Baliński and Daniłowicz,
2005; Yang et al., 2006; Zhou and Wade, 2009).
Many previous works provide strong baselines for
unstructured text retrieval and ranking problems;
however, these systems usually assume a homoge-
neous domain for queries and target documents.

Due to the spike of applications that are required
to maintain the conversation, dialog data has re-

cently become a popular target among researchers.
The work in this field concerns problems such
as learning facts through conversation (Fernández
et al., 2011; Williams et al., 2015; Hixon et al.,
2015) or dialog summarization (Oya and Carenini,
2014; Misra et al., 2015). More recent work in
this field has focused on several inter-dialogue
tasks (Xu and Reitter, 2016; Kim et al., 2016; He
et al., 2016). To the best of our knowledge our
work is the first, where the cross-genre document
retrieval is analyzed based on conversational and
formal writings.

This paper analyzes the performance of state-
of-the-art retrieval techniques targeting TV show
transcripts and their descriptions. We first collect a
dataset comprising transcripts from a popular TV
show and their summaries and plots (Section 3).
We then establish a solid baseline by adapting an
advanced search engine and implement structure
reranking to improve the initial ranking from the
search engine (Section 4). Our evaluation shows a
4% improvement, which is significant (Section 5).

2 Related work

Information extraction for dialogue data has al-
ready been widely explored. Yoshino et al. (2011)
presented a spoken dialogue system that extracts
predicate-argument structures and uses them to ex-
tract facts from news documents. Flycht-Eriksson
and Jönsson (2003) developed a dialogue inter-
action process of accessing textual data from a
bird encyclopedia. An unsupervised technique for
meeting summarization using decision-related ut-
terances has been presented by Wang and Cardie
(2012). Gorinski and Lapata (2015) studied movie
script summarization. All the aforementioned work
uses the syntactic and semantic relation extraction
and thus is similar to ours; however, it is distin-
guished in a way that it lacks a cross-genre aspect.

48



Dialogue Summary + Plot

Joey
One woman? That’s like saying there’s only one flavor
of ice cream for you. Lemme tell you something, Ross.
There’s lots of flavors out there.

Joey compares women to ice cream. (S)

Ross You know you probably didn’t know this, but back inhigh school, I had a, um, major crush on you. Ross reveals his high school crush on Rachel. (S)
Rachel I knew.

Chandler Alright, one of you give me your underpants. Chandler asks Joey for his underwear, but
Joey Can’t help you, I’m not wearing any. Joey can’t help him out as he’s not wearing any. (P)

Table 1: Three manually curated examples of dialogues and their descriptions.

3 Data

The Character Mining project provides transcripts
of the TV show, Friends; transcripts from 8 seasons
of the show are publicly available in the JSON for-
mat,1 where the first 2 seasons are annotated for the
character identification task (Chen and Choi, 2016).
Each season consists of episodes, each episode con-
tains scenes, each scene includes utterances, where
each utterance comes with the speaker information.

For each episode, the episode summary and plot
are first collected from fan sites,2 then sentence
segmented by NLP4J,3 the same tool used for the
provided transcripts. Generally, summaries give
broad descriptions of the episodes, whereas plots
describe facts within individual scenes. Finally, we
create a dataset by treating each sentence as a query
and its relevant episode as the target document.
Table 2 shows the distributions of this dataset.

Dialogue Summary + Plot
# of episodes 194 # of queries 5,075
# of tokens 897,446 # of tokens 119,624

Table 2: Dialogue, summary, and plot data.

4 Structure Reranking

For each query (summary or plot) in the dataset,
the task is to retrieve the document (episode) most
relevant to the query. The challenge comes from the
cross-genre aspect: how to retrieve documents in
dialogues given the queries in formal writing. This
section describes our structure reranking approach
that significantly outperforms an advanced search
engine, Elasticsearch4.

4.1 Relation Extraction
Since our queries and documents appear very differ-
ent on the surface level (Table 1), relations are first
extracted from them and matching is performed
1nlp.mathcs.emory.edu/character-mining
2friends-tv.org, friends.wikia.com
3github.com/emorynlp/nlp4j
4https://www.elastic.co/

on the relation level, which abstracts certain prag-
matic differences between these two types of writ-
ings. All data are lemmatized, tagged with parts-of-
speech and named entities, parsed into dependency
trees, and labeled with semantic roles using NLP4J.

A sentence may consist of multiple predicates,
and each predicate comes with a set of arguments.
A predicate together with its arguments is consid-
ered a relation. For each argument, heuristics are
applied to extract meaningful contextual words by
traversing the subtree of the argument. Our heuris-
tics are designed for the type of dependency trees
generated by NLP4J, but similar rules can be gener-
alized to other types of dependency trees. Relations
from dialogues are attached with the speaker names
to compensate the lack of entity information.

you

give

me underpant

help

can not you I not

wear

any

chandler joey

Chandler: Alright, one of you give me your underpants. 
Joey: Can't help you, I'm not wearing any.

ask

chandler joey underwear

help

joey can not him out wear

wear

he not any

Chandler asks Joey for his underwear, 
but Joey can't help him out as he's not wearing any.

Figure 1: Two sets of relations, from dialogue and plot,
extracted from the examples in Table 1.

By extracting relations that comprise only mean-
ingful words, it prunes out much noise (e.g., disflu-
ency), which allows the system to retrieve relevant
documents with higher precision. While our rela-
tion extraction is based on the sentence level, it
can be extended to the document level by adding
coreference relations, which we will explore in the
future.

49



Bin
. . . wk

. . . lk

. . . mk

Word Matching

Lemma Matching

Embedding Matching

w1 w2

l1 l2

m1 m2

. . . eke1 e2
True

False

P

RR PRank

q

d1

dk

. 

. 

.

Elasticsearch

Scoring
Binary classification

Reranking

Figure 2: The overview of our structure reranking. Given documents d1, . . . , dk and a query q, 4 sets
of scores are generated: the Elasticsearch scores and the matching scores using 3 comparators: word,
lemma, and embedding. The binary classifier Bin predicts whether the highest ranked document from
Elasticsearch is the correct answer. If not, the system RR reranks the documents using all scores and
returns a new top-ranked prediction.

4.2 Structure Matching

All relations extracted from dialogues are stored
in an inverted index manner, where words in each
relation are associated with the relation and the
episode in which the relation occurs. Algorithm 1
shows how our structure matching works. Given
a list of documents retrieved from the index based
on a query q, it first initializes scores for all doc-
uments to 0. For each document di, it compares
each relation rq from q to relations extracted from
di. The relation r from di is kept within Rd if it
has at least one word that overlaps with rq. For
each relation rd 2 Rd, the comparator function
returns the matching score between rd and rq. The
maximum matching score is added to the overall
score of this document. This procedure is repeated;
finally, the algorithm returns the overall matching
scores for all documents.

Input: D: a list of documents, q: a query.
fr: a function returning all relations.
fc: a comparator function.

Output: S: a list of matching scores for D.
S  [0 for i 2 [1, |S|]
foreach di 2 D do

foreach rq 2 fr(q) do
Rd  [r for r 2 fr(di) if |r \ rq| � 1]
sm  0
foreach rd in Rd do

s fc(rd, rq)
sm  max(sm, s)

end
Si  Si + sm

end
end

Algorithm 1: The structure matching algorithm.

The comparator function fc takes two relation sets,
rd and rq, and returns the matching score between
those two sets. For word and lemma, the count of

overlapping words between them is used to produce
two scores, rds , and r

q
s , normalized by the length

of the utterance and the query, respectively. The
harmonic mean of the two scores is then returned
as the final score. For embedding, fc uses embed-
dings to generate sum vectors from both sets and
returns the cosine similarity of these two vectors.

4.3 Document Reranking

The Elasticsearch scores and the 3 sets of matching
scores for the top-k documents (ranked by Elastic-
search) are fed into a binary classifier to determine
whether or not to accept the highest ranked docu-
ment. A Feed Forward Neural Network with one
hidden layer of size 15 is used for this classification.
If the binary classifier disqualifies the top-ranked
document, the top-k documents are reranked by the
weighted sums of these scores. A grid search is
performed on the development set to find the opti-
mized set of the weights. At last, the system returns
the document with the highest reranked scores:
di = arg maxi(�e ·ei+�w ·wi+�l · li+�m ·mi).

5 Experiments

The data in Section 3 is split into training, develop-
ment and evaluation sets, where queries from each
episode are randomly assigned. Two standard met-
rics are used for evaluation: precision at k (P@k)
and mean reciprocal rank (MRR).

Dataset Summary Plot Total
Training 970 3,013 3,983 (78.48%)
Development 97 403 500 (9.85%)
Evaluation 150 442 592 (11.67%)

Table 3: Data split (# of queries).

50



Model
Development Evaluation

Summary Plot All Summary Plot All
P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR

Elastic10 44.33 53.64 46.40 54.97 46.00 54.71 50.67 60.87 46.61 55.06 47.64 56.53
Structw 38.14 48.42 34.00 45.11 34.80 45.75 35.33 48.34 35.52 47.08 35.47 47.40
Structl 39.18 49.24 34.74 46.29 35.60 46.86 44.00 55.55 38.01 49.24 39.53 50.84
Structm 35.05 46.71 33.50 44.72 33.80 45.10 36.00 50.14 35.97 46.95 35.98 47.76
Rerank1 47.42 55.66 48.39 56.10 48.20 56.02 56.67 63.77 50.23 57.99 51.86 59.46
Rerank� 50.52 57.66 51.36 57.76 51.20 57.74 55.33 63.88 50.90 58.47 52.03 59.84

Table 4: Evaluation on the development and evaluation sets for summary, plot, and all (summary + plot).
Elastic10: Elasticsearch with k = 10, Structw,l,m: structure matching using words, lemmas, embeddings,
Rerank1,�: unweighted and weighted reranking.

5.1 Elasticsearch
Elasticsearch is used to establish a strong baseline.5

Each episode is indexed as a document using the de-
fault setting, Okapi BM25 (Robertson et al., 2009),
and the TF-IDF based similarity with improved
normalization; the top-k most relevant documents
are retrieved for each query. While P@1 is less
than 50% (Table 5), P@10 shows greater than 70%
coverage implying that it is possible to achieve a
higher P@1 by reranking results from k � 10.

k Development EvaluationP@k MRR P@k MRR
1 46.00 46.00 47.64 47.64
5 65.80 53.80 69.26 69.26

10 72.60 54.71 74.66 56.53
20 78.80 55.13 79.73 56.91
40 83.80 55.31 84.80 57.08

Table 5: Elasticsearch results on (summary + plot).

5.2 Structure Matching
The Struct⇤ rows in Table 4 show the results based
on structure matching (Section 4.2). The highest
P@1 of 39.53% is achieved on the evaluation set
using lemmas. Although it is about 8% lower than
the one achieved by Elasticsearch, we hypothesize
that this approach can correctly retrieve documents
for certain queries that Elasticsearch cannot.

Model Development EvaluationP@1 MRR P@1 MRR
Elastic10 0 16.07 0 16.99
Structw 14.44 23.57 19.68 28.11
Structl 14.81 25.59 20.97 30.14
Structe 15.56 24.47 20.32 29.22

Table 6: Results on queries failed by Elasticsearch.

To validate our hypothesis, we test structure match-
ing on the subset of queries failed by Elasticsearch.
We first take the top-10 results from Elasticsearch
5www.elastic.co/products/elasticsearch

then rerank the results using the scores from struc-
ture matching for queries that Elasticsearch gives
P@1 of 0%. As shown in Table 6, structure match-
ing is capable of reranking a significant portion
(around 20%) of these queries correctly, establish-
ing that our hypothesis is true.

5.3 Document Reranking

The scores from Elastic10 and Struct⇤ for each doc-
ument are fed into the binary classifier that decides
whether or not to accept the top-1 result from Elas-
ticsearch. If not, the documents are reranked by the
weighted sum of these scores (Section 4.3). The
Rerank1 row in Table 4 shows the results when all
the weights = 1, which gives an over 4% improve-
ment of P@1 on the evaluation set. The Rerank�
row shows the results when the optimized weights
are used, which gives an additional 3% boost on
the development set but not on the evaluation set.

It is worth mentioning that we initially tackled
this as a document classification task using con-
volutional neural networks similar to Kim (2014);
however, it gave P@1 ⇡ 20% and MRR ⇡ 33%.
Such poor results were due to the huge size of our
documents, over 4.6K words on average, beyond
the capacity of a CNN. Thus, we decided to focus
on reranking, which gave the best performance.

6 Conclusion

We propose a cross-genre document retrieval task
that matches between TV show transcripts and their
descriptions in summaries and plots. Our structure
reranking approach gives an improvement of more
than 4% of P@1, showing promising results for
this task. In the future, we will add more struc-
tural information such as coreference relations to
our structure matching and apply a more sophisti-
cated parameter optimization technique such as the
Bayesian optimization for finding �⇤.

51



References
Jaroslaw Baliński and Czeslaw Daniłowicz. 2005. Re-

ranking method based on inter-document distances.
Information processing & management 41(4):759–
775.

David C Blair and Melvin E Maron. 1985. An
evaluation of retrieval effectiveness for a full-text
document-retrieval system. Communications of the
ACM 28(3):289–299.

James P Callan. 1994. Passage-level evidence in doc-
ument retrieval. In Proceedings of the 17th annual
international ACM SIGIR conference on Research
and development in information retrieval. Springer-
Verlag New York, Inc., pages 302–310.

Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou
Huang, and Hsiao-Wuen Hon. 2006. Adapting rank-
ing svm to document retrieval. In Proceedings of
the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval. ACM, pages 186–193.

Yu-Hsin Chen and Jinho D. Choi. 2016. Character
identification on multiparty conversation: Identify-
ing mentions of characters in tv shows. In Proceed-
ings of the 17th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue. Association
for Computational Linguistics, Los Angeles, pages
90–100. http://www.aclweb.org/anthology/W16-
3612.

Raquel Fernández, Staffan Larsson, Robin Cooper,
Jonathan Ginzburg, and David Schlangen. 2011. Re-
ciprocal learning via dialogue interaction: Chal-
lenges and prospects. In Proceedings of the IJCAI
2011 Workshop on Agents Learning Interactively
from Human Teachers (ALIHT 2011).

Annika Flycht-Eriksson and Arne Jönsson. 2003.
Some empirical findings on dialogue management
and domain ontologies in dialogue systems - impli-
cations from an evaluation of birdquest. In Akira
Kurematsu, Alexander Rudnicky, and Syun Tutiya,
editors, Proceedings of the Fourth SIGdial Work-
shop on Discourse and Dialogue. pages 158–167.
http://www.aclweb.org/anthology/W03-2113.

Philip John Gorinski and Mirella Lapata. 2015. Movie
script summarization as graph-based scene extrac-
tion. In Proceedings of the 2015 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, Denver, Colorado, pages 1066–1076.
http://www.aclweb.org/anthology/N15-1113.

Zhiyang He, Xien Liu, Ping Lv, and Ji Wu. 2016. Hid-
den softmax sequence model for dialogue structure
analysis. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Compu-
tational Linguistics, Berlin, Germany, pages 2063–
2072. http://www.aclweb.org/anthology/P16-1194.

Ben Hixon, Peter Clark, and Hannaneh Hajishirzi.
2015. Learning knowledge graphs for question an-
swering through conversational dialog. In HLT-
NAACL. pages 851–861.

Seokhwan Kim, Rafael Banchs, and Haizhou Li. 2016.
Exploring convolutional and recurrent neural net-
works in sequential labelling for dialogue topic
tracking. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 963–973.
http://www.aclweb.org/anthology/P16-1091.

Yoon Kim. 2014. Convolutional Neural Networks for
Sentence Classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Doha, Qatar, EMNLP’14, pages 1746–
1751. http://www.aclweb.org/anthology/D14-1181.

Amita Misra, Pranav Anand, Jean E. Fox Tree,
and Marilyn Walker. 2015. Using summariza-
tion to discover argument facets in online idealog-
ical dialog. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Denver, Colorado, pages 430–
440. http://www.aclweb.org/anthology/N15-1046.

Tatsuro Oya and Giuseppe Carenini. 2014. Extrac-
tive summarization and dialogue act modeling on
email threads: An integrated probabilistic approach.
In Proceedings of the 15th Annual Meeting of
the Special Interest Group on Discourse and Di-
alogue (SIGDIAL). Association for Computational
Linguistics, Philadelphia, PA, U.S.A., pages 133–
140. http://www.aclweb.org/anthology/W14-4318.

Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends R� in Information Re-
trieval 3(4):333–389.

Lu Wang and Claire Cardie. 2012. Focused meet-
ing summarization via unsupervised relation ex-
traction. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Dis-
course and Dialogue. Association for Computa-
tional Linguistics, Seoul, South Korea, pages 304–
313. http://www.aclweb.org/anthology/W12-1642.

Jason D Williams, Nobal B Niraula, Pradeep Dasigi,
Aparna Lakshmiratan, Carlos Garcia Jurado Suarez,
Mouni Reddy, and Geoff Zweig. 2015. Rapidly scal-
ing dialog systems with interactive learning. In Nat-
ural Language Dialog Systems and Intelligent Assis-
tants, Springer, pages 1–13.

Yang Xu and David Reitter. 2016. Entropy con-
verges between dialogue participants: Explana-
tions from an information-theoretic perspective. In
Proceedings of the 54th Annual Meeting of the

52



Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 537–546.
http://www.aclweb.org/anthology/P16-1051.

Lingpeng Yang, Donghong Ji, Guodong Zhou, Yu Nie,
and Guozheng Xiao. 2006. Document re-ranking us-
ing cluster validation and label propagation. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management. ACM,
pages 690–697.

Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken dialogue system based on in-
formation extraction using similarity of predicate
argument structures. In Proceedings of the SIG-
DIAL 2011 Conference. Association for Computa-
tional Linguistics, Portland, Oregon, pages 59–66.
http://www.aclweb.org/anthology/W11-2008.

Dong Zhou and Vincent Wade. 2009. Latent document
re-ranking. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3. Association for Computa-
tional Linguistics, pages 1571–1580.

53


