



















































Modeling Input Uncertainty in Neural Network Dependency Parsing


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4984–4991
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4984

Modeling Input Uncertainty in Neural Network Dependency Parsing

Rob van der Goot
University of Groningen

r.van.der.goot@rug.nl

Gertjan van Noord
University of Groningen

g.j.m.van.noord@rug.nl

Abstract
Recently introduced neural network parsers al-
low for new approaches to circumvent data
sparsity issues by modeling character level in-
formation and by exploiting raw data in a
semi-supervised setting. Data sparsity is es-
pecially prevailing when transferring to non-
standard domains. In this setting, lexical nor-
malization has often been used in the past to
circumvent data sparsity. In this paper, we in-
vestigate whether these new neural approaches
provide similar functionality as lexical nor-
malization, or whether they are complemen-
tary.

We provide experimental results which show
that a separate normalization component im-
proves performance of a neural network parser
even if it has access to character level informa-
tion as well as external word embeddings. Fur-
ther improvements are obtained by a straight-
forward but novel approach in which the top-N
best candidates provided by the normalization
component are available to the parser.

1 Introduction

Recently, neural network dependency
parsers (Chen and Manning, 2014; Dyer et al.,
2015; Kiperwasser and Goldberg, 2016) obtained
state-of-the-art performance for dependency
parsing. These parsers incorporate character level
information (de Lhoneux et al., 2017a; Ballesteros
et al., 2015; Nguyen et al., 2017) and can more
easily exploit raw text in a semi-supervised setup.
These new methods are especially beneficial
for words not occurring in the training data. In
practice, such unseen words often are spelling
mistakes, or alternative spellings of known words.
In more classical parsing models, these unseen
words were usually clustered using ad-hoc rules.
For non-standard domains, the number of unseen
words is much larger. To minimize the degra-
dation in performance, lexical normalization is

often used. Lexical normalization is the task of
converting non-standard input to a more standard
format. Previous work has shown that this is
beneficial, in particular for parsing social media
data (Foster, 2010; Zhang et al., 2013; van der
Goot and van Noord, 2017b).

This leads to the question whether normaliza-
tion is indeed no longer required for these mod-
ern character-based neural network parsers, or
whether normalization is capable of solving prob-
lems beyond the scope of this type of neural net-
work parsers.

Our main contributions are:

• We show that using normalization as pre-
processing improves parser performance for
non-standard language, even if pre-trained
embeddings and character level information
are used.

• We propose a novel technique to exploit the
top-N candidates provided by the normaliza-
tion component, and we show that this tech-
nique leads to a further increase in parser per-
formance.

• A treebank containing non-standard language
is created to evaluate the effect of normal-
ization on parser performance. The treebank
consists of 10,005 tokens annotated with lex-
ical normalization and Universal Dependen-
cies (Nivre et al., 2017). The treebank has
been made publicly available.

2 Related Work

Early work on parser adaptation focused on
relatively canonical domains, like biomedical
data (McClosky and Charniak, 2008). More re-
cently, there has been an increasing interest in
parsing of the notoriously noisy domain of social
media. A lot of previous work is orthogonal to our



4985

Original word new pix comming tomoroe
Cand. 1 (p1) new (0.95) pix (0.79) coming (0.57) tomorrow (0.54)
Cand. 2 (p2) news (0.03) selfies (0.08) comming (0.43) tomoroe (0.39)
Cand. 3 (p3) knew (0.01) pictures (0.06) combing (<0.01) tomorrow’s (0.02)

Table 1: Output of the normalization model for the example sentence “new pix comming tomoroe” including
candidate probabilities. Only the top-3 candidates are shown here.

approach, as it focuses on adaptation of the train-
ing data (Foster et al., 2011; Khan et al., 2013;
Kong et al., 2014; Blodgett et al., 2018). In the
remainder of this section we will shortly review
work which evaluated the effect of normalization
on dependency parsing.

Zhang et al. (2013) tune a normalization model
for the parsing task, and show performance im-
provement on a silver treebank obtained from
manually normalized data. Daiber and van der
Goot (2016) use an existing normalization model
as pre-processing for a graph-based dependency
parser, and show a small but significant perfor-
mance improvement. In the shared task of pars-
ing the web (Petrov and McDonald, 2012) held
at SANCL 2012, some teams used a simple rule-
based normalization, but the effect on final perfor-
mance remained untested.

Baldwin and Li (2015) examined the theoretical
impact of different normalization actions on pars-
ing performance. To this end they use manual nor-
malization. They show that edits beyond the word
level can also be crucial for parsing (e.g. inser-
tion of copulas and subjects). However, these are
difficult to obtain automatically.

Note that all this previous work, except
for Blodgett et al. (2018), is based on traditional
feature-based dependency parsers, whereas we fo-
cus on neural network parsing.

3 Method

In this section we will first shortly review the two
models we will combine: a lexical normalization
model and a neural network parser. Then we de-
scribe how they can be combined.

3.1 Normalization

In this work we use an existing normalization
model: MoNoise (van der Goot and van Noord,
2017a)1. This model is based on the observation
that normalization requires a variety of different

1https://bitbucket.org/robvanderg/
monoise

word1

~v1
~t1 ~c1 ~e1

LSTM f

LSTM b

word2

~v2
~t2 ~c2 ~e2

LSTM f

LSTM b

word3

~v3
~t3 ~c3 ~e3

LSTM f

LSTM b

Figure 1: Overview of the conversion of input words to
vectors which are used in the shift-reduce algorithm.

replacement actions. For these different actions,
different modules are used to generate candidates,
including: the Aspell spell checker2, word embed-
dings and a lookup list generated from the training
data. Features from these generation modules are
complemented with N-gram features from canon-
ical data and non-canonical data. A random forest
classifier is used to score and rank the candidates.
In this work, we use the top-N candidates and con-
vert the confidence scores of the classifier to prob-
abilities. An example of this output is shown in
Table 1.

We train MoNoise on 2,577 tweets annotated
with normalization by Li and Liu (2014), which
only contains word-word replacements. In our ini-
tal experiments, we noted that the normalization
model wrongfully normalized some words due to
the different tokenization in the treebank (e.g. “ca
n’t”), because these do not occur in the normal-
ization data. We manually created a list of excep-
tions, which are not considered for normalization
process.

3.2 Neural Network Parser

As a starting point, we use the shift-reduce UU-
Parser 2.0 (de Lhoneux et al., 2017b; Kiper-
wasser and Goldberg, 2016). This parser uses the
Arc-Hybrid Transition system (Kuhlmann et al.,
2011). Words are first converted to continu-

2www.aspell.net

https://bitbucket.org/robvanderg/monoise
https://bitbucket.org/robvanderg/monoise
www.aspell.net


4986

ous vectors, which are then processed through a
Bidirectional Long-Short Term Memory network
(BiLSTM) (Graves and Schmidhuber, 2005) be-
fore they are passed on to the parsing algorithm.
The decision whether to shift, reduce or swap is
made by a multi-layer perceptron with one hid-
den layer. The BiLSTM is trained jointly with the
parsing objective, so that the vectors are optimized
for the parsing task.

Figure 1 shows an overview of how the input
words are converted to vectors which are used in
the shift-reduce algorithm. We denote the vec-
tor used as input to the BiLSTM for word i by
~vi. This vector is a concatenation of three vec-
tors which are derived from the input word. ~ti
is optimized on the training data, ~ci is the result
of a separate BiLSTM ran over the characters of
word i and ~ei is the external vector; it is obtained
from external embeddings which are trained on
huge amounts of raw texts. In this work we use
the same word embeddings as used by the nor-
malization model (van der Goot and van Noord,
2017a), which are trained on 760,744,676 tweets
using word2vec (Mikolov et al., 2013).

3.3 Adaptation Strategy

Notation We use ~w0... ~wn to represent the vec-
tors of the original words of a sentence. The
vectors of the normalization candidates are repre-
sented by ~nij , where i is the index of the original
word in the sentence, and j is the rank of the can-
didate. The corresponding probability as given by
the normalization model is pij . We use ~gi for the
vector of the manual normalization of word i

Our baseline setup is to simply use the vector of
the original word:

ORIG: ~vi = ~wi

The most straightforward use of normalization
is to use the best normalization sequence as input
to the parser. In our setup, this means that we use
the vector of the best normalization candidate for
each position:

NORM: ~vi = ~ni0

To give more information to the parser, we will
exploit the top-n candidates of the normalization
model. The vectors of the top-N candidates are
merged using linear interpolation:

INTEGRATED: ~vi =
n∑

j=0

pij ∗ ~nij

An interesting property of this integration ap-
proach is that it does not influence the size of the
search space, so the effect on complexity of the
parsing algorithm is negligible. The only extra
runtime compared to ORIG originates from run-
ning the normalization model.

Finally, we include a theoretical upperbound of
the effect of normalization, which uses manually
annotated normalization:

GOLD: ~vi = ~gi

4 Data

To test the effect of normalization, we need a tree-
bank containing non-standard language, prefer-
ably with a corresponding training treebank from
a more standard domain. Since the existing tree-
banks are not noisy enough (Foster et al., 2011;
Kaljahi et al., 2015)3 or do not have a correspond-
ing training treebank in the same annotation for-
mat (Kong et al., 2014; Daiber and van der Goot,
2016) we annotate a small treebank for develop-
ment and testing purposes4. We choose to use
the Universal Dependencies 2.1 annotation for-
mat (Nivre et al., 2017), since the annotation ef-
forts on the the English Web Treebank (Silveira
et al., 2014) provide suitable training data. This
treebank already contains web specific phenomena
like URL’s, E-Mail addresses and emoticons, so
we do not have to create special annotation guide-
lines and the parser can learn these phenomena
from the training data.

Our treebank consists of tweets, taken from Li
and Liu (2015). The tweets in this dataset origi-
nate from two sources: the LexNorm corpus (Han
and Baldwin, 2011), which was originally anno-
tated with normalization, and a corpus originally
annotated with POS tags (Owoputi et al., 2013).
Li and Liu (2015) complemented this annotation
for both datasets, so that they both have a normal-
ization layer and a POS layer. To avoid overfit-
ting on a specific filtering or time-frame we use the
data collected by Owoputi et al. (2013) as devel-
opment data and LexNorm as test data. We only
keep the tweets which are still available on Twit-
ter, resulting in a dataset of 305 development and

3Kaljahi et al. (2015) only normalize 3.6%, and we man-
ually normalized the development data from Foster et al.
(2011), were even less words were in need of normalization.

4It should be noted that two other suitable Twitter tree-
banks in the UD format where created in parallel to our tree-
bank (Liu et al., 2018; Blodgett et al., 2018), which were re-
leased after submission of this paper.



4987

327 test tweets (10,005 tokens in total). It should
be noted that these corpora were filtered to contain
domain-specific phenomena and non-standard lan-
guage, and thus provide an ideal testbed for our ex-
periments but are not representative of the whole
Twitter domain.

Tokenization and normalization are first re-
annotated, because the Universal Dependencies
format requires treebank specific tokenization. To
avoid parser bias, dependency relations are anno-
tated from scratch. For more details on annotation
decisions for domain-specific structures, we refer
to the appendix.

MoNoise reaches 90% accuracy on the word
level for the normalization task for our develop-
ment data. In this dataset, 18% of all words are
in need of normalization, so a baseline which sim-
ply copies the original words would reach an ac-
curacy of 82%. The most common mistakes made
by MoNoise are due to treebank specific normal-
izations, like ‘na’ 7→ go. However, these also oc-
cur in the training treebank, so normalization is
not crucial.

5 Evaluation

In this section, we first use the development data to
compare the effect of the different normalization
settings with the use of character level information
and external embeddings. Secondly, we confirm
our main results on the test set. Thirdly, we test
if our model is sensitive to over-normalization on
standard data. Finally, we perform some analysis
to examine why normalization is beneficial. All
scores reported in this section are obtained using
the CoNLL 2017 evaluation script (Zeman et al.,
2017). In Section 5.1 the results are the average
over ten runs, using a different seed for the BiL-
STM and the shuffling of the training data. In the
remainder of this section, the best model is used to
simplify interpretation. The parser is trained using
default settings (de Lhoneux et al., 2017b).

In our initial experiments, it became apparent
that the parser often considered a username men-
tion or retweet in the beginning of the tweet as
root, resulting in a propagation of errors. Be-
cause we want to exclude any influences from this
simple construction, we added an heuristic to our
parser which exclude usernames and retweets in
the beginning of a tweet, and connects them to the
root after parsing. We use this heuristic in all ex-
periments.

w w + c w + e w + c + e
Vector Contents

54

56

58

60

62

LA
S

Normalization
Orig
Norm
Integrated
Gold

Figure 2: The effect of normalization on LAS for the
different parsing models on the development data.

5.1 Normalization Strategies

The results of the different parser and normaliza-
tion settings on the development data are plotted
in Figure 2. Using external embeddings (~e) re-
sults in a much bigger performance improvement
compared to using character level information (~c).
Adding character level embeddings on top of ex-
ternal embeddings only leads to a very minor im-
provement. This can partly be explained by the
coverage of 98.4% of the embeddings on the de-
velopment data.

In the settings without external embeddings,
the direct use of normalization (NORM) results in
a improvement of approximately 3 LAS points.
However, when external embeddings are included
the improvement becomes more than twice as
small, indicating that the approaches target some
common issues, but are also complementary to
each other. When external embeddings and nor-
malization are already used, the character level
embeddings slightly harm performance. Integra-
tion of the normalization (INTEGRATED) consis-
tently results in a slightly higher LAS compared
to direct normalization. Interestingly, gold nor-
malization still performs substantially better com-
pared to automatic normalization.

5.2 Test Data

Table 2 shows the results of the parser with exter-
nal embeddings and character embeddings (using
the best seed from the development data), for the
different normalization strategies on the test data.
These results confirm the observations on the de-
velopment data: normalization helps on top of ex-



4988

Model UAS LAS

ORIG 69.63 59.64
NORM 70.51 61.76*

INTEGRATED 70.62 62.30*

GOLD 70.71 62.33

Table 2: LAS scores for the Twitter test data.
*Statistically significant compared to the previous row
at P < 0.05 using a paired t-test.

ternal embeddings, and integrating normalization
results in an even higher score. In contrast to the
development data, the integrated approach almost
reaches the theoretical upper bound of gold nor-
malization on the test data. However, since this is
only the case on the test data, not too strong con-
clusions can be drawn from this result. The perfor-
mance difference between the datasets is probably
partly due to the differences in filtering5. Interest-
ingly, integrating normalization is especially ben-
eficial for the LAS, meaning that it is most useful
for choosing the type of relation.

5.3 Robustness
As stated in Section 4, our development and test
data is filtered to be very non-standard. However,
it is undesirable to have a parser that performs bad
on more standard texts. Hence, we also tested per-
formance on the English Web Treebank develop-
ment set. This dataset also consists of data from
the web, however, it contains much less words in
need of normalization; MoNoise normalizes less
than 0.5% of all words. We compared the per-
formance using no normalization (ORIG) versus
our INTEGRATED approach, which showed a very
minor performance improvement from 81.42 to
81.43 LAS. This is a direct effect of the normaliza-
tion model giving high probabilities to the original
words on this more canonical data.

5.4 Analysis
To gain insights into which constructions are
parsed better when using normalization, we com-
pared the predictions of the vanilla parser with our
NORM and INTEGRATED methods on the develop-
ment data. Starting with NORM, the first observa-
tion is that the incoming arcs of the words which
are normalized are responsible for 44.1% of all

5Even when using the best seed on the development data,
INTEGRATED results in two-thirds of the performance im-
provement compared to GOLD.

improvements, whereas the outgoing arcs are re-
sponsible for 17.6% of al improvements. So, the
direct context of the normalized words is respon-
sible for only 61.7% of all improvements. Consid-
ering the type of syntactic constructions for which
parsing improved, it is hard to identify trends, be-
cause the improvements are based on the output of
the normalization model, which normalizes a wide
variety of words. One clearly influential effect of
using normalization, was that the parser improved
upon finding the root. When multiple unknown
words occured in the beginning of a sentence, the
vanilla parser often failed at identifying the root,
which improved considerably after normalizing.

For the INTEGRATED method, almost all the im-
provements made by NORM remained. On top
of these, some additional improvements where
made. Manual inspection revealed that these im-
provements often originated from a non-standard
word, for which the correct normalization was
ranked high. This then leads to improvements for
the non-standard word as well as its context. In
some cases, even incorrect normalization candi-
dates lead to performance improvements. For ex-
ample for ‘Gma’, where the normalization model
ranked the original word first, but ‘mom’ sec-
ond. Even though ‘grandma’ is the correct nor-
malization, ‘mom’ occurs in similar contexts, and
is much easier for the parser to process.

6 Conclusion

We showed that normalization can improve perfor-
mance of a neural network parser, even when mak-
ing use of character level information and external
word embeddings. Integrating multiple normal-
ization candidates into the parser leads to an even
larger performance increase. Normalization has
shown to be complementary to external embed-
dings, in contrast to character embeddings, which
add no additional information. Our experiments
revealed that our approach is robust, and it does
not harm performance on more canonical data.
However, when comparing our approach to the
theoretical upperbound of using gold normaliza-
tion, we saw that on different datasets the perfor-
mance gain is of a different magnitude. Further-
more, we release a dataset containing 636 tweets
annotated with both normalization and Universal
Dependencies. The data and all code to reproduce
the results in this paper is available at: https://
bitbucket.org/robvanderg/normpar

https://bitbucket.org/robvanderg/normpar
https://bitbucket.org/robvanderg/normpar


4989

Acknowledgements
We would like to thank Gosse Bouma for help with
the data annotation, and Antonio Toral, Barbara
Plank and the anonymous reviewers for feedback
on the paper. This work is part of the ‘Parsing
Algorithms for Uncertain Input’ project sponsored
by the Nuance Foundation.

References
Tyler Baldwin and Yunyao Li. 2015. An in-depth

analysis of the effect of text normalization in social
media. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 420–429, Denver, Colorado. Asso-
ciation for Computational Linguistics.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by mod-
eling characters instead of words with LSTMs. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
349–359, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Su Lin Blodgett, Johnny Wei, and Brendan O’Connor.
2018. Twitter Universal Dependency parsing for
African-American and mainstream American En-
glish. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1415–1425. Associa-
tion for Computational Linguistics.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750, Doha, Qatar. Association
for Computational Linguistics.

Joachim Daiber and Rob van der Goot. 2016. The
Denoised Web Treebank: Evaluating dependency
parsing under noisy input conditions. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
Paris, France. European Language Resources Asso-
ciation (ELRA).

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343, Beijing, China. Associa-
tion for Computational Linguistics.

Jennifer Foster. 2010. “cba to check the spelling”: In-
vestigating parser performance on discussion forum
posts. In Human Language Technologies: The 2010

Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 381–384, Los Angeles, California. Associa-
tion for Computational Linguistics.

Jennifer Foster, Özlem Çetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From news to comment:
Resources and benchmarks for parsing the language
of web 2.0. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
893–901, Chiang Mai, Thailand. Asian Federation
of Natural Language Processing.

Rob van der Goot and Gertjan van Noord. 2017a.
MoNoise: Modeling noise using a modular normal-
ization system. Computational Linguistics in the
Netherlands Journal, 7:129–144.

Rob van der Goot and Gertjan van Noord. 2017b.
Parser adaptation for social media by integrating
normalization. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 491–497,
Vancouver, Canada. Association for Computational
Linguistics.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368–378, Port-
land, Oregon, USA. Association for Computational
Linguistics.

Rasoul Kaljahi, Jennifer Foster, Johann Roturier,
Corentin Ribeyre, Teresa Lynn, and Joseph Le Roux.
2015. Foreebank: Syntactic analysis of customer
support forums. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1341–1347, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Mohammad Khan, Markus Dickinson, and Sandra
Kuebler. 2013. Does size matter? text and gram-
mar revision for parsing social media data. In Pro-
ceedings of the Workshop on Language Analysis in
Social Media, pages 1–10, Atlanta, Georgia. Asso-
ciation for Computational Linguistics.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. TACL, 4:313–
327.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference



4990

on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1001–1012, Doha, Qatar.
Association for Computational Linguistics.

Marco Kuhlmann, Carlos Gómez-Rodrı́guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673–682, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.

Miryam de Lhoneux, Yan Shao, Ali Basirat, Eliyahu
Kiperwasser, Sara Stymne, Yoav Goldberg, and
Joakim Nivre. 2017a. From raw text to Universal
Dependencies – look, no tags! In Proceedings of
the CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies., Vancou-
ver, Canada.

Miryam de Lhoneux, Sara Stymne, and Joakim Nivre.
2017b. Arc-hybrid non-projective dependency pars-
ing with a static-dynamic oracle. In Proceedings of
the The 15th International Conference on Parsing
Technologies (IWPT)., Pisa, Italy.

Chen Li and Yang Liu. 2014. Improving text normal-
ization via unsupervised model and discriminative
reranking. In Proceedings of the ACL 2014 Student
Research Workshop, pages 86–93, Baltimore, Mary-
land, USA. Association for Computational Linguis-
tics.

Chen Li and Yang Liu. 2015. Joint POS tagging and
text normalization for informal text. In Proceedings
of the Twenty-Fourth International Joint Conference
on Artificial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, pages 1263–1269.

Yijia Liu, Yi Zhu, Wanxiang Che, Bing Qin, Nathan
Schneider, and Noah A. Smith. 2018. Parsing tweets
into Universal Dependencies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 965–975. Association for Computa-
tional Linguistics.

David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics on Human Language Tech-
nologies: Short Papers, pages 101–104. Association
for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of Workshop at
ICLR.

Dat Quoc Nguyen, Mark Dras, and Mark Johnson.
2017. A novel neural network model for joint POS
tagging and graph-based dependency parsing. In

Proceedings of the CoNLL 2017 Shared Task: Mul-
tilingual Parsing from Raw Text to Universal Depen-
dencies, pages 134–142.

Joakim Nivre, Zeljko Agić, Lars Ahrenberg, Lene
Antonsen, Maria Jesus Aranzabe, Masayuki Asa-
hara, Luma Ateyah, Mohammed Attia, Aitz-
iber Atutxa, Liesbeth Augustinus, Elena Bad-
maeva, Miguel Ballesteros, Esha Banerjee, Sebas-
tian Bank, Verginica Barbu Mititelu, John Bauer,
Kepa Bengoetxea, Riyaz Ahmad Bhat, Eckhard
Bick, Victoria Bobicev, Carl Börstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Aljoscha Bur-
chardt, Marie Candito, Gauthier Caron, Gülşen
Cebiroğlu Eryiğit, Giuseppe G. A. Celano, Savas
Cetin, Fabricio Chalub, Jinho Choi, Silvie Cinková,
Çağr Çöltekin, Miriam Connor, Elizabeth David-
son, Marie-Catherine de Marneffe, Valeria de Paiva,
Arantza Diaz de Ilarraza, Peter Dirix, Kaja Do-
brovoljc, Timothy Dozat, Kira Droganova, Puneet
Dwivedi, Marhaba Eli, Ali Elkahky, Tomaž Erjavec,
Richárd Farkas, Hector Fernandez Alcalde, Jennifer
Foster, Cláudia Freitas, Katarı́na Gajdošová, Daniel
Galbraith, Marcos Garcia, Moa Gärdenfors, Kim
Gerdes, Filip Ginter, Iakes Goenaga, Koldo Go-
jenola, Memduh Gökırmak, Yoav Goldberg, Xavier
Gómez Guinovart, Berta Gonzáles Saavedra, Ma-
tias Grioni, Normunds Grūzı̄tis, Bruno Guillaume,
Nizar Habash, Jan Hajič, Jan Hajič jr., Linh Hà Mỹ,
Kim Harris, Dag Haug, Barbora Hladká, Jaroslava
Hlaváčová, Florinel Hociung, Petter Hohle, Radu
Ion, Elena Irimia, Tomáš Jelı́nek, Anders Jo-
hannsen, Fredrik Jørgensen, Hüner Kaşıkara, Hi-
roshi Kanayama, Jenna Kanerva, Tolga Kayade-
len, Václava Kettnerová, Jesse Kirchner, Natalia
Kotsyba, Simon Krek, Veronika Laippala, Lorenzo
Lambertino, Tatiana Lando, John Lee, Phuong
Lê H`ông, Alessandro Lenci, Saran Lertpradit, Her-
man Leung, Cheuk Ying Li, Josie Li, Keying
Li, Nikola Ljubešić, Olga Loginova, Olga Lya-
shevskaya, Teresa Lynn, Vivien Macketanz, Aibek
Makazhanov, Michael Mandl, Christopher Man-
ning, Cătălina Mărănduc, David Mareček, Katrin
Marheinecke, Héctor Martı́nez Alonso, André Mar-
tins, Jan Mašek, Yuji Matsumoto, Ryan McDon-
ald, Gustavo Mendonça, Niko Miekka, Anna Mis-
silä, Cătălin Mititelu, Yusuke Miyao, Simonetta
Montemagni, Amir More, Laura Moreno Romero,
Shinsuke Mori, Bohdan Moskalevskyi, Kadri Muis-
chnek, Kaili Müürisep, Pinkey Nainwani, Anna
Nedoluzhko, Gunta Nešpore-Bērzkalne, Luong
Nguy˜ên Thi., Huy`ên Nguy˜ên Thi. Minh, Vitaly
Nikolaev, Hanna Nurmi, Stina Ojala, Petya Osen-
ova, Robert Ostling, Lilja Ovrelid, Elena Pascual,
Marco Passarotti, Cenel-Augusto Perez, Guy Per-
rier, Slav Petrov, Jussi Piitulainen, Emily Pitler,
Barbara Plank, Martin Popel, Lauma Pretkalniņa,
Prokopis Prokopidis, Tiina Puolakainen, Sampo
Pyysalo, Alexandre Rademaker, Loganathan Ra-
masamy, Taraka Rama, Vinit Ravishankar, Livy
Real, Siva Reddy, Georg Rehm, Larissa Rinaldi,
Laura Rituma, Mykhailo Romanenko, Rudolf Rosa,
Davide Rovati, Benoı̂t Sagot, Shadi Saleh, Tanja
Samardžić, Manuela Sanguinetti, Baiba Saulı̄te, Se-



4991

bastian Schuster, Djamé Seddah, Wolfgang Seeker,
Mojgan Seraji, Mo Shen, Atsuko Shimada, Dmitry
Sichinava, Natalia Silveira, Maria Simi, Radu
Simionescu, Katalin Simkó, Mária Simková, Kiril
Simov, Aaron Smith, Antonio Stella, Milan Straka,
Jana Strnadová, Alane Suhr, Umut Sulubacak,
Zsolt Szántó, Dima Taji, Takaaki Tanaka, Trond
Trosterud, Anna Trukhina, Reut Tsarfaty, Francis
Tyers, Sumire Uematsu, Zdeňka Urešová, Larraitz
Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van
Niekerk, Gertjan van Noord, Viktor Varga, Eric
Villemonte de la Clergerie, Veronika Vincze, Lars
Wallin, Jonathan North Washington, Mats Wirén,
Tak-sum Wong, Zhuoran Yu, Zdeněk Zabokrtský,
Amir Zeldes, Daniel Zeman, and Hanzhi Zhu. 2017.
Universal Dependencies 2.1. LINDAT/CLARIN
digital library at the Institute of Formal and Applied
Linguistics (’UFAL), Faculty of Mathematics and
Physics, Charles University.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380–390, Atlanta, Georgia. Association for
Computational Linguistics.

Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), volume 59.

Natalia Silveira, Timothy Dozat, Marie-Catherine
de Marneffe, Samuel Bowman, Miriam Connor,
John Bauer, and Christopher D. Manning. 2014. A
gold standard dependency corpus for English. In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014).

Daniel Zeman, Martin Popel, Milan Straka, Jan Ha-
jic, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, Fran-
cis Tyers, Elena Badmaeva, Memduh Gokirmak,
Anna Nedoluzhko, Silvie Cinkova, Jan Hajic jr.,
Jaroslava Hlavacova, Václava Kettnerová, Zdenka
Uresova, Jenna Kanerva, Stina Ojala, Anna Mis-
silä, Christopher D. Manning, Sebastian Schuster,
Siva Reddy, Dima Taji, Nizar Habash, Herman Le-
ung, Marie-Catherine de Marneffe, Manuela San-
guinetti, Maria Simi, Hiroshi Kanayama, Valeria de-
Paiva, Kira Droganova, Héctor Martı́nez Alonso,
Çağr Çöltekin, Umut Sulubacak, Hans Uszkor-
eit, Vivien Macketanz, Aljoscha Burchardt, Kim
Harris, Katrin Marheinecke, Georg Rehm, Tolga
Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran
Yu, Emily Pitler, Saran Lertpradit, Michael Mandl,
Jesse Kirchner, Hector Fernandez Alcalde, Jana Str-
nadová, Esha Banerjee, Ruli Manurung, Antonio
Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo
Mendonca, Tatiana Lando, Rattima Nitisaroj, and

Josie Li. 2017. Conll 2017 shared task: Multilingual
parsing from raw text to Universal Dependencies. In
Proceedings of the CoNLL 2017 Shared Task: Multi-
lingual Parsing from Raw Text to Universal Depen-
dencies, pages 1–19, Vancouver, Canada. Associa-
tion for Computational Linguistics.

Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1159–1168, Sofia, Bulgaria. Association for Com-
putational Linguistics.


