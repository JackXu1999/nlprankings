



















































Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1410

Exploiting Multilingualism through Multistage Fine-Tuning
for Low-Resource Neural Machine Translation

Raj Dabre Atsushi Fujita
National Institute of Information and Communications Technology

3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
firstname.lastname@nict.go.jp

Chenhui Chu
Osaka University, Japan

chu@ids.osaka-u.ac.jp

Abstract
This paper highlights the impressive utility
of multi-parallel corpora for transfer learn-
ing in a one-to-many low-resource neural ma-
chine translation (NMT) setting. We report
on a systematic comparison of multistage fine-
tuning configurations, consisting of (1) pre-
training on an external large (209k–440k) par-
allel corpus for English and a helping target
language, (2) mixed pre-training or fine-tuning
on a mixture of the external and low-resource
(18k) target parallel corpora, and (3) pure fine-
tuning on the target parallel corpora. Our ex-
periments confirm that multi-parallel corpora
are extremely useful despite their scarcity and
content-wise redundancy thus exhibiting the
true power of multilingualism. Even when the
helping target language is not one of the target
languages of our concern, our multistage fine-
tuning can give 3–9 BLEU score gains over a
simple one-to-one model.

1 Introduction

Encoder-decoder based neural machine translation
(NMT) (Sutskever et al., 2014; Bahdanau et al.,
2015) allows for end-to-end training of a transla-
tion model. While NMT is known to perform well
for resource-rich language pairs, such as French–
English and German–English (Bojar et al., 2018),
it performs poorly for resource-poor pairs (Zoph
et al., 2016; Riza et al., 2016).

Translating from a resource-poor language into
English is typically easier than the other direc-
tion, because large parallel corpora between En-
glish and other languages can be used for trans-
fer learning (Zoph et al., 2016) and many-to-one
modeling.1 Additionally, large English monolin-
gual data can be back-translated to obtain pseudo-
parallel corpora for augmenting the performance
of such NMT models.

1Note that “many-to-one” does not mean multi-source
machine translation (Zoph and Knight, 2016).

Translating from English into a resource-poor
language is substantially more difficult. Dong
et al. (2015) have shown that a one-to-many model
trained on middle-sized parallel data (200k sen-
tence pairs) can improve the translation quality
over a one-to-one model. However, it is unclear
whether this works for much resource-poorer,
more distant, and more diverse language pairs.
Using pseudo-parallel data is a potential solution,
but for most resource-poor languages, the amount
of available clean and in-domain monolingual data
are limited. It is also unclear what the real reason
behind improvements in translation is: increase in
the training data or multilingualism.

This paper focuses on (a) improving the per-
formance of NMT for translating English to low-
resource languages (b) via exploiting multilingual-
ism (c) through transfer learning based on multi-
stage fine-tuning. The power of multilingualism
is verified by using multi-parallel corpora, i.e., the
same text in different languages.2 Unlike previ-
ous studies that only apply single-stage fine-tuning
for one-to-one translation (Zoph et al., 2016), we
systematically compare multistage fine-tuning that
exploits one-to-many modeling. We show that
our approach can significantly improve the quality
of translations from English to seven Asian lan-
guages (Bengali, Filipino, Indonesian, Japanese,
Khmer, Malay, and Vietnamese) in the Asian Lan-
guage Treebank (ALT) corpus (Riza et al., 2016),3

as a result of soft division of labor: training of
a strong English encoder, domain adaptation, and
tuning of the decoder for each target language.

2Adding a new language does not add any new transla-
tion knowledge. Such corpora deserve more attention, since
adding a new language to it leads to parallel corpora between
the new language and all the other languages in the corpora.

3There exist other multi-parallel corpora, such as those
for United Nations (Ziemski et al., 2016), Europarl (Koehn,
2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and
the Bible (Christodouloupoulos and Steedman, 2015).



1411

2 Related Work

In this paper, we address (a) the importance
of multilingualism (b) via one-to-many NMT
(c) through robust fine-tuning for transfer learn-
ing. Dong et al. (2015) have worked on one-to-
many NMT, whereas Firat et al. (2016) and John-
son et al. (2017) have worked on multilingual and
multi-way NMT. These studies have focused on
training a multilingual model from scratch, but we
explore transfer learning for one-to-many NMT.

Fine-tuning based transfer learning has been
studied for transferring proper parameters (Zoph
et al., 2016; Gu et al., 2018b), lexical (Zoph
et al., 2016; Nguyen and Chiang, 2017; Gu et al.,
2018a; Lakew et al., 2018), and syntactic (Gu
et al., 2018a; Murthy et al., 2018) knowledge from
a resource-rich language pair to a resource-poor
language pair. On the other hand, Chu et al.
(2017) proposed a more robust training approach
for domain adaptation, called mixed fine-tuning,
which uses a mixture of data from different do-
mains. Imankulova et al. (2019) proposed a mul-
tistage fine-tuning method which combines fine-
tuning techniques for domain adaptation and back-
translation (Sennrich et al., 2016). Unlike us, these
approaches do not try to combine multilingualism
with multistage fine-tuning using only parallel cor-
pora involving a large number of languages.

3 Multistage Fine-Tuning for NMT

This paper focuses on translation from English
(En) to N different languages. In particular, we
consider exploiting two types of corpora. One
is a small-scale multi-parallel corpus, En-YY1-
· · ·-YYN , consisting of English and N target lan-
guages of interest. The other is a relatively larger
helping parallel corpus, En-XX, where XX indi-
cates the helping target language which needs not
be one of the target languages in the multi-parallel
corpus, YYk (1 ≤ k ≤ N).

All the fine-tuning techniques that we examine
can be generalized as follows.

Pre-training (Pre): An initial NMT model is
trained on a parallel corpus for a resource-
rich language pair, i.e., En-XX.

Mixed pre-training / fine-tuning (Mix):
Training of the NMT model is newly started
or continued (Chu et al., 2017) on a mixture
of parallel corpora for En-XX and one or
more low-resource En-YY pairs.

En-XX En-YY1 

Pre-train 
NMT Model 

Fine-tune 
NMT Model 

Pre-process 

En-ALL 
(prep) 

Mixed pre-training 
or fine-tuning (Mix) 

Fine-tune 
NMT Model 

Pure fine-tuning 
(Pure) 

Pre-training 
(Pre) 

En-YY2 En-YYN 
.  .  . 

 

En-YYk 
(prep) 

En-XX 
Model 

En-All 
Model 

En-YYk 
Model 

En-XX 
(prep) 

Figure 1: Our multistage fine-tuning for one-to-
many NMT. Three dotted sections respectively indicate
(a) the pre-training stage, (b) the mixed pre-training
/ fine-tuning stage, and (c) the pure fine-tuning stage
The training data for (a) and (c), i.e., “En-XX (prep)”
and “En-YYk (prep),” are selectively extracted from
the pre-processed data labeled “En-ALL (prep).”

Pure fine-tuning (Pure): Training further con-
tinues using only a parallel corpus for a par-
ticular En-YY pair.

Figure 1 illustrates the training procedure with
all of the above three stages. Before starting the
training, vocabularies for source and target lan-
guages are determined. First, to indicate the tar-
get language of each sentence pair, we prepend an
artificial token, for instance “2zz” for a target lan-
guage “zz,” onto source sentences (Johnson et al.,
2017). All corpora are then concatenated into a
single corpus, “En-ALL (prep),” where each of the
smaller corpora are enlarged by oversampling so
that the size of each corpus matches the largest
one. Finally, (sub-word) vocabularies are deter-
mined based on “En-ALL (prep).” For simplicity,
we keep source and target vocabularies separate,
but the target vocabulary is shared by multiple lan-
guages. Note that all the NMT models are essen-
tially one-to-many, because they use a shared tar-
get vocabulary of XX and one or all of YYk.

Our training procedure can be viewed as a soft
division of labor in NMT training: the first stage
learns a strong English encoder by training on
a large parallel corpus,4 whereas the remaining

4One may be interested in training an English encoder on
a gigantic monolingual corpus. We leave a comparison of
the utility of such monolingual corpora and the parallelism in
bilingual corpora for our future work.



1412

stages can focus more on training the decoder for
the target language(s) of interest and updating the
encoder. Whereas we pre-train an encoder for low-
resource NMT, so do McCann et al. (2017) to im-
prove several monolingual NLP tasks.

4 Experiments

We conducted a systematic comparison of mul-
tistage fine-tuning configurations, specifically fo-
cusing on low-resource English-to-Asian lan-
guage translation.

4.1 Datasets and Pre-processing

As the test-bed, we chose the ALT multilingual
multi-parallel corpus5 (Riza et al., 2016), because
it offers the same test sentences in different lan-
guages, allowing us to determine whether mul-
tilingualism is the true reason behind improved
translation quality. We used seven Asian lan-
guages6 as the target languages (i.e., YY): Bengali
(Bn), Filipino (Tl), Indonesian (Id), Japanese (Ja),
Khmer (Km), Malay (Ms), and Vietnamese (Vi).
We randomly7 split the ALT data into 18,000,
1,000, and 1,106 multi-parallel sentences for train-
ing, development, and testing, respectively.

As for the resource-rich En-XX data, we sep-
arately used two parallel corpora with two dif-
ferent target languages. One is the IWSLT 2015
English–Chinese corpus (IWSLT En-Zh) (Cettolo
et al., 2015), comprising 209,491 parallel sen-
tences. We chose it to examine whether the target
language (Chinese) other than the target languages
in our setting is helpful. Separately, we also ex-
perimented with the Kyoto free translation task
English–Japanese corpus (KFTT En-Ja),8 consist-
ing of Japanese-to-English translations: 440,288
parallel sentences. With this corpus, we could
additionally observe how ALT En-Ja can benefit
from KFTT En-Ja. Note that the ALT, IWSLT,
and KFTT corpora belong to the news, spoken,
and Wikipedia domains, respectively.

We tokenized English sentences using the tok-

5http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/
6Thai and Burmese (Myanmar) were excluded due to the

unavailability of reliable tokenized data. Although some of
the YY languages in our experiments, such as Japanese, were
relatively resource-rich, we exploited only a small parallel
corpus between them and English to highlight the importance
of multilingualism.

7Our results might not be comparable to those obtained
using the official splits mentioned on the website.

8http://www.phontron.com/kftt/

enizer.perl script in Moses.9 To tokenize Chinese
and Japanese sentences, we used KyotoMorph10

and JUMAN,11 respectively. For the other lan-
guages in the ALT corpus, we used the data in its
raw form, except Khmer for which segmented data
were provided by the ALT project.

4.2 Training NMT Models

We compared a total of seven training configura-
tions as shown in Table 1. The model #1 is the sim-
plest baseline model trained only on the En-YY
parallel corpus. The models #2 to #4 are three con-
ceivable combinations of fine-tuning stages that
exploit an external large parallel corpus for En-
XX, showing the limit reachable without a multi-
parallel corpus. The models #5 to #7 follow the
same training procedure with #2 to #4, respec-
tively, exploiting a small-scale but multi-parallel
corpus containing all the seven target languages.

Note that the source side vocabulary is the same
across models #2 to #7 and is determined using
the English side of the En-XX corpus and the En-
glish part of the multi-parallel corpus. On the
other hand, the target side vocabulary for mod-
els #2 to #4 is different from the vocabulary for
models #5 to #7. This is because vocabularies for
models #2 to #4 involve the target side of the En-
XX corpus and “one” non-English corpus from the
multi-parallel corpus. On the other hand, the vo-
cabularies for models #5 to #7 involve the target
sides of the En-XX corpus and “all” non-English
corpora from the multi-parallel corpus.

We used the open-source implementation of
the Transformer model (Vaswani et al., 2017) in
the version 1.6 branch of tensor2tensor,12 and
used hyper-parameter settings13 corresponding to
transformer base single gpu which uses dropout
by default. We also used the default sub-word seg-
mentation mechanism of tensor2tensor. The “En-
YY” models (#1) used a vocabulary of 8,192 (213)
sub-words due to small corpora sizes, whereas all
the other models used a vocabulary of 32,768 (215)
sub-words to account for the larger amount of par-
allel data in multiple languages. We trained the
models for a sufficient number of iterations till

9https://github.com/moses-smt/mosesdecoder
10https://bitbucket.org/msmoshen/kyotomorph-beta
11http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
12https://github.com/tensorflow/tensor2tensor
13We did not perform any hyper-parameter tuning be-

cause we wanted to focus more on simple out-of-the box ap-
proaches coupled with multilingualism.

http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/
http://www.phontron.com/kftt/
https://github.com/moses-smt/mosesdecoder
https://bitbucket.org/msmoshen/kyotomorph-beta
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
https://github.com/tensorflow/tensor2tensor


1413

# XX N Model Training configuration YY test setcapacity Pre Mix Pure Bn Tl Id Ja Km Ms Vi
1. - 1 1-to-1 - - X 3.99 24.04 24.10 11.03 22.53 29.85 27.39
2. Zh 1 1-to-2 X - X 8.86∗ 27.54∗ 27.10∗ 19.07∗ 28.41∗ 32.52∗ 34.63∗

3. Zh 1 1-to-2 - X X 4.90∗ 23.07 23.37 13.97∗ 26.13∗ 29.24 29.82∗

4. Zh 1 1-to-2 X X X 7.99∗ 26.61∗ 25.62∗ 18.39∗ 27.49∗ 31.63∗ 34.22∗

5. Zh 7 1-to-8 X - X 8.54∗ 26.88∗ 26.02∗ 18.99∗ 27.07∗ 32.39∗ 33.32∗

6. Zh 7 1-to-8 - X X 9.43∗ 25.86∗ 26.33∗ 19.34∗ 26.86∗ 32.39∗ 33.28∗

7. Zh 7 1-to-8 X X X 10.30∗+†28.22∗+† 27.24∗† 20.08∗+† 28.66∗† 33.19∗+†35.34∗+†
2. Ja 1 1-to-2 X - X 9.16∗ 28.06∗ 26.53∗ 21.55∗ 27.98∗ 33.68∗ 33.93∗

3. Ja 1 1-to-2 - X X 4.37 22.91 23.37 16.47∗ 23.36∗ 29.28 29.10∗
4. Ja 1 1-to-2 X X X 8.77∗ 26.64∗ 25.88∗ 21.61∗ 27.55∗ 32.45∗ 34.29∗

5. Ja 7 1-to-8 X - X 9.43∗ 27.45∗ 26.70∗ 21.79∗ 27.87∗ 32.92∗ 34.28∗

6. Ja 7 1-to-8 - X X 9.96∗+ 28.39∗ 27.22∗+ 21.03∗ 28.91∗+ 33.75∗ 36.00∗+

7. Ja 7 1-to-8 X X X 10.77∗+† 28.62∗+ 28.89∗+†22.60∗+†30.03∗+†34.75∗+†37.06∗+†

Table 1: BLEU scores for all the tested configurations. The “XX” column indicates the resource-rich external
(helping) parallel corpus if used, where “Zh” and “Ja” stand for using the IWSLT English–Chinese and the KFTT
English–Japanese corpora, respectively. The columns under “YY test set” indicate the the target languages in
our multi-parallel corpus, where bold marks the best scores for each target language with each external parallel
corpus. The “∗,” “+,” and “†” respectively mark the scores significantly (bootstrap re-sampling with p < 0.05)
better than the 1-to-1 “En-YY” model (#1), the best model without the multi-parallel corpus (#2–#4), and the
strongest baselines (#5–#6).

the BLEU score on the development set (simply
concatenated unlike training data) did not vary by
more than 0.1 BLEU over 10 checkpoints.

Instead of choosing the model with the best
development set BLEU, we averaged the last 10
checkpoints saved every after 1,000 updates, fol-
lowing Vaswani et al. (2017), and decoded the test
sets with a beam size of 4 and a length penalty, α,
of 0.6 consistently across all the models.

4.3 Results

Table 1 gives the BLEU scores (Papineni et al.,
2002) for all the configurations. Among the seven
configurations, irrespective of the external paral-
lel corpus for En-XX, the three-stage fine-tuned
model (#7) achieved the highest BLEU scores for
all the seven target languages.

Results for #1 demonstrate that NMT systems
trained on 18k parallel sentences can achieve only
poor results for Bn and Ja, whereas reasonably
high BLEU scores (> 20) are achieved for the
other target languages.

Introducing a large external En-XX parallel
corpus improved the translation quality consis-
tently and significantly for all the seven target lan-
guages,14 irrespective of the way of its use: sim-
ple pre-training (#2), mixed pre-training (#3), and

14We suspected that the gain is partially owing to the use
of the larger vocabularies of these models. However, we only
obtained translations of degraded quality with a larger vocab-
ulary size of #1, presumably due to the scarce parallel data
for En-YY language pair.

their combination (#4). Among these three train-
ing configurations, in most cases, the simple pre-
training (#2) attained the best performance. The
BLEU gains over #1 were 2.67 (Ms) to 8.04 (Ja)
points with the IWSLT En-Zh corpus and 2.43 (Id)
to 10.52 (Ja) points with the KFTT En-Ja corpus.

Simple fine-tuning (#2 and #5) gave great im-
provements over the baseline. However, #5 was
worse than #2, despite being trained in the same
way, because #5 involves up to eight target lan-
guages, inevitably reducing the allowance for each
target language. In contrast, exploiting the multi-
parallel corpus through mixed pre-training (#6)
or mixed fine-tuning (#7) brought consistent im-
provements over the corresponding 1-to-2 models
(#3 and #4). Three-stage fine-tuning (#4 and #7)
was always better than two-stage fine-tuning (#3
and #6), because the pre-final model in #4 and
#7, obtained using mixed-fine tuning, was supe-
rior to and more robust than the one in #3 and #6,
obtained using naive multilingual training. These
comparisons also justifies the utility of external
large parallel corpus for En-XX. #7 was substan-
tially better than #5 simply because the pre-final
model in #7 was also trained on multilingual data,
unlike the one in #5.

Irrespective of the external parallel corpus for
En-XX, the three-stage fine-tuned model (#7)
achieved the highest BLEU scores for all the seven
target languages, consistently outperforming the
other models, with gains over the best scores
of 0.14 (Id) to 0.87 (Bn) BLEU points with the



1414

IWSLT En-Zh corpus and 0.23 (Tl) to 1.67 (Id)
BLEU points with the KFTT En-Ja corpus. As
mentioned in Section 4.2, these models do not in-
troduce any new source sentences compared to the
models #2 to #4. Therefore, we can safely con-
clude that the gain is due to multilingualism.

5 Discussion

In this section, we discuss the effect of the two
types of corpora, i.e., external large parallel cor-
pora and small multi-parallel corpora.

5.1 Does the Nature of Corpora Matter?

When using only one En-YY language pair (#2 to
#4), only for the three-stage fine-tuned model (#4),
the KFTT En-Ja corpus showed slight but con-
sistent superiority (for Tl, Id, Km, and Vi) over
the IWSLT En-Zh corpus. In contrast, when all
the target languages were exploited (#5–#7), the
KFTT En-Ja corpus consistently helped achieve
significantly better results than the IWSLT En-Zh
corpus. The reason could be three-fold: corpus
size, overlap or similarity of helping target lan-
guage with the target language of interest, and
proximity of domain.

En-Ja translation using KFTT En-Ja corpus as a
helping data was better by 2.52 BLEU points than
when IWSLT En-Zh corpus was used. The differ-
ence in BLEU is significant, but it could be be-
cause the En-Ja corpus is more than twice the size
of the En-Zh corpus. Whereas it is possible that
En-Ja translation improves from the overlapping
vocabularies of Chinese and Japanese, the perfor-
mance of other languages also improves despite
no vocabulary overlap. Consequently, we believe
that translation into a low-resource target language
might benefit when the language is also target side
of the helping corpus but multistage fine-tuning
does not require such overlap and instead shows
optimal performance when it leverages multilin-
gualism during stage-wise tuning.

In the future, we will test the above hypothe-
ses thoroughly via some controlled experiments
by using, for instance, larger scale multi-parallel
corpora (Imamura and Sumita, 2018) and varieties
of helping corpora.

5.2 How Does Multilingualism Help?

The ALT corpus is multi-parallel and merely com-
prises of the same sentences in multiple languages.
Although we used seven target languages, we did

not introduce new English sentences to the source
side. Table 1 shows that some languages are better
translated (En-Vi) than others (En-Bn). We spec-
ulate that the representations for En-Vi might be
better learned than those for En-Bn. When learn-
ing all language pairs jointly, the representations
of sentences with the same meaning tend to be
similar. As such, the representations for En-Bn
will be similar to those of En-Vi and hence the for-
mer might benefit from the potentially better rep-
resentations of the latter, leading to improvements
in translation quality for the former. This might
be how multilingualism is responsible for the im-
provement in translation quality.

The results also shed light on the value of multi-
parallel corpora despite their resource-scarce na-
ture. In a practical situation, a collection of exist-
ing multiple bilingual corpora may potentially im-
prove translation quality. Nevertheless, when we
create new parallel data, multi-parallel fashion can
be a less expensive option, since adding new lan-
guage leads to parallel corpora between that lan-
guage and all the other languages in the corpora.

6 Conclusion

We explored the use of small multi-parallel cor-
pora and large helping parallel corpora for one-to-
many NMT, which translates English to multiple
languages. We proposed multistage fine-tuning
and confirmed that it works significantly better
than training models via fewer fine-tuning stages
despite the resource-scarce and content-redundant
nature of the multi-parallel corpora, thereby high-
lighting the usefulness of multilingualism behind
the improvement in translation quality.

In the future, we will explore the utility of mul-
tistage fine-tuning for many-to-one and many-to-
many NMT. We will also try to explicitly deter-
mine the impact of corpora sizes, language simi-
larity, and domain on our approach, and propose
further improvements according to our findings.

Acknowledgments

This work was partially conducted under the pro-
gram “Research and Development of Enhanced
Multilingual and Multipurpose Speech Translation
Systems” of the Ministry of Internal Affairs and
Communications (MIC), Japan.



1415

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR), San Diego, USA.

Ondřej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Philipp Koehn, and
Christof Monz. 2018. Findings of the 2018 Con-
ference on Machine Translation (WMT18). In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Shared Task Papers, pages 272–303, Bel-
gium, Brussels.

Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed and
translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261–268, Trento, Italy.

Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, Roldano Cattoni, and Marcello Federico.
2015. The IWSLT 2015 evaluation campaign. In
Proceedings of the Twelfth International Workshop
on Spoken Language Translation (IWSLT), pages 2–
14, Da Nang, Vietnam.

Christos Christodouloupoulos and Mark Steedman.
2015. A massively parallel corpus: the Bible in
100 languages. Language Resources and Evalua-
tion, 49(2):375–395.

Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017.
An empirical comparison of domain adaptation
methods for neural machine translation. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, pages 385–391, Van-
couver, Canada.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1723–1732, Beijing,
China.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 866–875, San Diego, USA.

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K.
Li. 2018a. Universal neural machine translation
for extremely low resource languages. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 344–354.

Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,
and Kyunghyun Cho. 2018b. Meta-learning for low-
resource neural machine translation. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 3622–3631.
Association for Computational Linguistics.

Kenji Imamura and Eiichiro Sumita. 2018. Multi-
lingual parallel corpus for Global Communication
Plan. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation
(LREC), pages 3453–3458, Miyazaki, Japan.

Aizhan Imankulova, Raj Dabre, Atsushi Fujita, and
Kenji Imamura. 2019. Exploiting out-of-domain
parallel data through multilingual transfer learning
for low-resource neural machine translation. In Pro-
ceedings of Machine Translation Summit XVII Vol-
ume 1: Research Track, pages 128–139, Dublin, Ire-
land.

Girish Nath Jha. 2010. The TDIL program and the In-
dian Language Corpora Initiative (ILCI). In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC), pages
982–985, Valletta, Malta.

Melvin Johnson, Mike Schuster, Quoc Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339–351.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand.

Surafel Melaku Lakew, Aliia Erofeeva, Matteo Negri,
Marcello Federico, and Marco Turchi. 2018. Trans-
fer learning in multilingual neural machine transla-
tion with dynamic vocabulary. In Proceedings of the
15th International Workshop on Spoken Language
Translation, pages 54–61, Bruges, Belgium.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Proceedings of the 30th
Neural Information Processing Systems Conference
(NIPS), pages 6294–6305, Long Beach, USA.

V. Rudra Murthy, Anoop Kunchukuttan, and Push-
pak Bhattacharyya. 2018. Addressing word-order
divergence in multilingual neural machine transla-
tion for extremely low resource languages. CoRR,
abs/1811.00383.

Toan Q. Nguyen and David Chiang. 2017. Trans-
fer learning across low-resource, related languages
for neural machine translation. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
pages 296–301, Taipei, Taiwan.

https://arxiv.org/pdf/1409.0473.pdf
https://arxiv.org/pdf/1409.0473.pdf
http://aclweb.org/anthology/W18-6401
http://aclweb.org/anthology/W18-6401
http://mt-archive.info/EAMT-2012-Cettolo.pdf
http://mt-archive.info/EAMT-2012-Cettolo.pdf
http://workshop2015.iwslt.org/downloads/IWSLT_2015_EP_0.pdf
https://link.springer.com/article/10.1007/s10579-014-9287-y
https://link.springer.com/article/10.1007/s10579-014-9287-y
http://aclweb.org/anthology/P17-2061
http://aclweb.org/anthology/P17-2061
http://aclweb.org/anthology/P15-1166
http://aclweb.org/anthology/P15-1166
http://aclweb.org/anthology/N16-1101
http://aclweb.org/anthology/N16-1101
http://aclweb.org/anthology/N18-1032
http://aclweb.org/anthology/N18-1032
http://aclweb.org/anthology/D18-1398
http://aclweb.org/anthology/D18-1398
http://www.lrec-conf.org/proceedings/lrec2018/pdf/104.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/104.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/104.pdf
http://aclweb.org/anthology/W19-6613
http://aclweb.org/anthology/W19-6613
http://aclweb.org/anthology/W19-6613
http://www.lrec-conf.org/proceedings/lrec2010/pdf/874_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/874_Paper.pdf
https://transacl.org/ojs/index.php/tacl/article/view/1081
https://transacl.org/ojs/index.php/tacl/article/view/1081
https://transacl.org/ojs/index.php/tacl/article/view/1081
http://mt-archive.info/MTS-2005-Koehn.pdf
http://mt-archive.info/MTS-2005-Koehn.pdf
http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf
http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf
http://arxiv.org/abs/1811.00383
http://arxiv.org/abs/1811.00383
http://arxiv.org/abs/1811.00383
http://aclweb.org/anthology/I17-2050
http://aclweb.org/anthology/I17-2050
http://aclweb.org/anthology/I17-2050


1416

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A method for auto-
matic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics (ACL), pages 311–318,
Philadelphia, USA.

Hammam Riza, Michael Purwoadi, Gunarso, Teduh
Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied,
Luong Chi Mai, Vu Tat Thang, Nguyen Phuong
Thai, Vichet Chea, Rapid Sun, Sethserey Sam,
Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet,
Masao Utiyama, and Chenchen Ding. 2016. Intro-
duction of the Asian Language Treebank. In Pro-
ceedings of the 2016 Conference of the Oriental
Chapter of International Committee for Coordina-
tion and Standardization of Speech Databases and
Assessment Technique (O-COCOSDA), pages 1–6,
Bali, Indonesia.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Volume 1: Long Papers,
pages 86–96, Berlin, Germany.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th Neural Informa-
tion Processing Systems Conference (NIPS), pages
3104–3112, Montréal, Canada.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 30th Neural In-
formation Processing Systems Conference (NIPS),
pages 5998–6008, Long Beach, USA.

Michał Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The United Nations parallel cor-
pus v1.0. In Proceedings of the Tenth International
Conference on Language Resources and Evaluation
(LREC), pages 3530–3534, Portorož, Slovenia.

Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 30–34, San Diego, USA.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1568–1575,
Austin, USA.

http://aclweb.org/anthology/P02-1040
http://aclweb.org/anthology/P02-1040
https://ieeexplore.ieee.org/document/7918974
https://ieeexplore.ieee.org/document/7918974
http://aclweb.org/anthology/P16-1009
http://aclweb.org/anthology/P16-1009
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/1195_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/1195_Paper.pdf
http://aclweb.org/anthology/N16-1004
http://aclweb.org/anthology/N16-1004
http://aclweb.org/anthology/D16-1163
http://aclweb.org/anthology/D16-1163

