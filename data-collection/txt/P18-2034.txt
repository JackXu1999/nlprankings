



















































Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 208–213
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

208

Transfer Learning for Context-Aware Question Matching in
Information-seeking Conversations in E-commerce

Minghui Qiu1, Liu Yang2, Feng Ji1, Wei Zhou1, Jun Huang1,
Haiqing Chen1, W. Bruce Croft2, Wei Lin1

1Alibaba Group, Hangzhou, China
2Center for Intelligent Information Retrieval, University of Massachusetts Amherst

{minghui.qmh,zhongxiu.jf}@alibaba-inc.com
{lyang,croft}@cs.umass.edu

Abstract

Building multi-turn information-seeking
conversation systems is an important and
challenging research topic. Although sev-
eral advanced neural text matching models
have been proposed for this task, they are
generally not efficient for industrial appli-
cations. Furthermore, they rely on a large
amount of labeled data, which may not be
available in real-world applications. To al-
leviate these problems, we study transfer
learning for multi-turn information seek-
ing conversations in this paper. We first
propose an efficient and effective multi-
turn conversation model based on convo-
lutional neural networks. After that, we
extend our model to adapt the knowledge
learned from a resource-rich domain to en-
hance the performance. Finally, we de-
ployed our model in an industrial chatbot
called AliMe Assist 1 and observed a sig-
nificant improvement over the existing on-
line model.

1 Introduction

With the popularity of online shopping, there is
an increasing number of customers seeking infor-
mation regarding their concerned items. To effi-
ciently handle customer questions, a common ap-
proach is to build a conversational customer ser-
vice system (Li et al., 2017; Yang et al., 2018).
In the E-commerce environment, the information-
seeking conversation system can serve millions
of customer questions per day. According to the
statistics from a real e-commerce website (Qiu
et al., 2017), the majority of customer questions

1 Interested readers can access AliMe Assist through
the Taobao App, or the web version via https://
consumerservice.taobao.com/online-help

(nearly 90%) are business-related or seeking infor-
mation about logistics, coupons etc. Among these
conversation sessions, 75% of them are more than
one turn2. Hence it is important to handle multi-
turn conversations or context information in these
conversation systems.

Recent researches in this area have focused on
deep learning and reinforcement learning (Shang
et al., 2015; Yan et al., 2016; Li et al., 2016a,b;
Sordoni et al., 2015; Wu et al., 2017). One
of these methods is Sequential Matching Net-
work(Wu et al., 2017), which matches a response
with each utterance in the context at multiple lev-
els of granularity and leads to state-of-the-art per-
formance on two multi-turn conversation corpora.
However, such methods suffer from at least two
problems: they may not be efficient enough for
industrial applications, and they rely on a large
amount of labeled data which may not be available
in reality.

To address the problem of efficiency, we made
three major modifications to SMN to boost the
efficiency of the model while preserving its ef-
fectiveness. First, we remove the RNN layers
of inputs from the model; Second, SMN uses
a Sentence Interaction based (SI-based) Pyramid
model (Pang et al., 2016) to model each utterance
and response pair. In practice, a Sentence Encod-
ing based (SE-based) model like BCNN (Yin and
Schütze, 2015) is complementary to the SI-based
model. Therefore, we extend the component to in-
corporate an SE-based BCNN model, resulting in
a hybrid CNN (hCNN) (Yu et al., 2017); Third,
instead of using a RNN to model the output rep-
resentations, we consider a CNN model followed
by a fully-connected layer to further boost the ef-
ficiency of our model. As shown in our experi-
ments, our final model yields comparable results

2According to a statistic in AliMe Assist in Alibaba Group

https://consumerservice.taobao.com/online-help
https://consumerservice.taobao.com/online-help


209

but with higher efficiency than SMN.
To address the second problem of insufficient

labeled data, we study transfer learning (TL) (Pan
and Yang, 2010) to utilize a source domain with
adequate labeling to help the target domain. A
typical TL approach is to use a shared NN (Mou
et al., 2016; Yang et al., 2017) and domain-specific
NNs to derive shared and domain-specific features
respectively. Recent studies (Ganin et al., 2016;
Taigman et al., 2017; Chen et al., 2017; Liu et al.,
2017) consider adversarial networks to learn more
robust shared features across domains. Inspired
by these studies, we extended our method with a
Transfer Learning module to leverage information
from a resource-rich domain. Similarly, our TL
module consists of a shared NN and two domain-
specific NNs for source and target domains. The
output of the shared NN is further linked to an
adversarial network as used in (Liu et al., 2017)
to help learn domain invariant features. Mean-
while, we also use domain discriminators on both
source and target features derived by domain-
specific NNs to help learn domain-specific fea-
tures. Experiments show that our TL method can
further improve the model performance on a target
domain with limited data.

To the best of our knowledge, our work is the
first to study transfer learning for context-aware
question matching in conversations. Experiments
on both benchmark and commercial data sets show
that our proposed model outperforms several base-
lines including the state-of-the-art SMN model.
We have also deployed our model in an industrial
bot called AliMe Assist 3 and observed a signifi-
cant improvement over the existing online model.

2 Model

Our model is designed to address the following
general problem. Given an input sequence of utter-
ances {u1, u2, . . . , un} and a candidate question r,
our task is to identify the matching degree between
the utterances and the question. When the num-
ber of utterances is one, our problem is identical
to paraphrase identification (PI) (Yin and Schütze,
2015) or natural language inference (NLI) (Bow-
man et al., 2015). Furthermore, we consider
a transfer learning setting to transfer knowledge
from a source domain to help a target domain.

3https://consumerservice.taobao.com/
online-help

2.1 Multi-Turn hCNN (MT-hCNN)
We present an overview of our model in Fig. 1. In
a nutshell, our model first obtains a representation
for each utterance and candidate question pair us-
ing hybrid CNN (hCNN), then concatenates all the
representations, and feeds them into a CNN and
fully-connected layer to obtain our final output.

Max$pooling

Convolu.on

Fully$Connected

hCNN

+

Candidate8Ques.on88888888888888U<erances

Output

P

O

r88888888888888888888888888888U18U288888888888888Un

CNN1 CNN1
CNN2

CNN3

H

Figure 1: Our proposed multi-turn hybrid CNN.

The hybrid CNN (hCNN) model (Yu et al.,
2017) is based on two models: a modified SE-
based BCNN model (Yin et al., 2016) and a SI-
based Pyramid model (Pang et al., 2016). The
former encode the two input sentences separately
with a CNN and then combines the resulting sen-
tence embeddings as follows:

h1 = CNN1(X1); h2 = CNN1(X2).

Hb = h1 ⊕ h2 ⊕ (h1 − h2)⊕ (h1 · h2).

where ‘−’ and ‘·’ refer to element-wise subtrac-
tion and multiplication, and ‘⊕’ refers to concate-
nation.

Furthermore, we add a SI-base Pyramid compo-
nent to the model, we first produce an interaction
matrix M ∈ Rm×m, where Mi,j denotes the dot-
product score between the ith word in X1 and the
jth word in X2. Next, we stack two 2-D convolu-
tional layers and two 2-D max-pooling layers on
it to obtain the hidden representation Hp. Finally,
we concatenate the hidden representations as out-
put for each input sentence pair: ZX1,X2 =
hCNN(X1, X2) = Hb ⊕Hp.

https://consumerservice.taobao.com/online-help
https://consumerservice.taobao.com/online-help


210

We now extend hCNN to handle multi-turn
conversations, resulting MT-hCNN model. Let
{u1, u2, u3, . . . , un} be the utterances, r is the
candidate question.

hui,r = hCNN(ui, r). for i ∈ [1, n]
H = [hu1,r;hu2,r; · · · ;hun,r].
P = CNN3(H).

O = Fully-Connected(P )

Note that H is obtained by stacking all the h,
CNN3 is another CNN with a 2-D convolutional
layer and a 2-D max-pooling layer, the output of
CNN3 is feed into a fully-connected layer to ob-
tain the final representation O.

2.2 Transfer with Domain Discriminators

We further study transfer learning (TL) to learn
knowledge from a source-rich domain to help our
target domain, in order to reduce the dependency
on a large scale labeled training data. As sim-
ilar to (Liu et al., 2017), we use a shared MT-
hCNN and two domain-specific MT-hCNNs to de-
rive shared features Oc and domain-specific fea-
tures Os and Ot. The domain specific output lay-
ers are:

ŷk =

{
σ(WscOc + WsOs + bs), if k = s
σ(WtcOc + WtOt + bt), if k = t

(1)

where Wsc, Wtc, Ws, and Wt are the weights
for shared-source, shared-target, source, and tar-
get domains respectively, while bs and bt are the
biases for source and target domains respectively.

Following (Liu et al., 2017), we use an adver-
sarial loss La to encourage the shared features
learned to be indiscriminate across two domains:

La =
1

n

n∑
i=1

∑
d∈s,t

p(di = d|U, r) log p(di = d|U, r).

where di is the domain label and p(di|·) is the do-
main probability from a domain discriminator.

Differently, to encourage the specific feature
space to be discriminable between different do-
mains, we consider applying domain discrimina-
tion losses on the two specific feature spaces. We
further add two negative cross-entropy losses: Ls

for source and Lt for target domain:

Ls =−
1

ns

ns∑
i=1

Idi=s log p(di = s|Us, rs).

Lt =−
1

nt

nt∑
i=1

Idi=t log p(di = t|Ut, rt).

where Idi=d is an indicator function set to 1 when
the statement (di = d) holds, or 0 otherwise.

Finally, we obtain a combined loss as follows:

L =
∑

k∈s,t
− 1
nk

nk∑
j=1

1

2
(ykj − ŷkj )2 +

λ1
2
La

+
λ2
2
Ls +

λ3
2
Lt +

λ4
2
||Θ||2F .

where Θ denotes model parameters.

3 Experiments

We evaluate the efficiency and effectiveness of our
base model, the transferability of the model, and
the online evaluation in an industrial chatbot.
Datasets: We evaluate our methods on two multi-
turn conversation corpus, namely Ubuntu Dialog
Corpus (UDC) (Lowe et al., 2015) and AliMe
data.

Ubuntu Dialog Corpus: The Ubuntu Dialog
Corpus (UDC) (Lowe et al., 2015) contains multi-
turn technical support conversation data collected
from the chat logs of the Freenode Internet Re-
lay Chat (IRC) network. We used the data copy
shared by Xu et al. (Xu et al., 2016), in which
numbers, urls and paths are replaced by special
placeholders. It is also used in several previous re-
lated works (Wu et al., 2017). It consists of 1 mil-
lion context-response pairs for training, 0.5 mil-
lion pairs for validation and 0.5 million pairs for
testing.

AliMe Data: We collect the chat logs between
customers and a chatbot called AliMe from “2017-
10-01” to “2017-10-20” in Alibaba 4. The chatbot
is built based on a question-to-question matching
system (Li et al., 2017), where for each query, it
finds the most similar candidate question in a QA
database and return its answer as the reply. It in-
dexes all the questions in our QA database using
Lucence5. For each given query, it uses TF-IDF
ranking algorithm to call back candidates. To form

4The textual contents related to user information are fil-
tered.

5https://lucene.apache.org/core/

https://lucene.apache.org/core/


211

Table 1: Comparison of base models on Ubuntu Dialog Corpus (UDC) and an E-commerce data (AliMe).

Data UDC AliMeData
Methods MAP R@5 R@2 R@1 Time MAP R@5 R@2 R@1 Time
ARC-I 0.2810 0.4887 0.1840 0.0873 16 0.7314 0.6383 0.3733 0.2171 23
ARC-II 0.5451 0.8197 0.5349 0.3498 17 0.7306 0.6595 0.3671 0.2236 24
Pyramid 0.6418 0.8324 0.6298 0.4986 17 0.8389 0.7604 0.4778 0.3114 27
Duet 0.5692 0.8272 0.5592 0.4756 20 0.7651 0.6870 0.4088 0.2433 30
MV-LSTM 0.6918 0.8982 0.7005 0.5457 1632 0.7734 0.7017 0.4105 0.2480 2495
SMN 0.7327 0.9273 0.7523 0.5948 64 0.8145 0.7271 0.4680 0.2881 91
MT-hCNN-d 0.7027 0.8992 0.7512 0.5838 20 0.8401 0.7712 0.4788 0.3238 31
MT-hCNN 0.7323 0.9172 0.7525 0.5978 24 0.8418 0.7810 0.4796 0.3241 36

our data set, we concatenated utterances within
three turns 6 to form a query, and used the chat-
bot system to call back top 15 most similar candi-
date questions as candidate “responses”. 7 We then
asked a business analyst to annotate the candidate
responses, where a “response” is labeled as pos-
itive if it matches the query, otherwise negative.
In all, we have annotated 63,000 context-response
pairs. This dataset is used as our Target data.

Furthermore, we build our Source data as fol-
lows. In the AliMe chatbot, if the confidence
score of answering a given user query is low, i.e.
the matching score is below a given threshold8,
we prompt top three related questions for users to
choose. We collected the user click logs as our
source data, where we treat the clicked question as
positive and the others as negative. We collected
510,000 query-question pairs from the click logs
in total as the source. For the source and target
datasets, we use 80% for training, 10% for valida-
tion, and 10% for testing.
Compared Methods: We compared our multi-
turn model (MT-hCNN) with two CNN based
models ARC-I and ARC-II (Hu et al., 2014), and
several advanced neural matching models: MV-
LSTM (Wan et al., 2016), Pyramid (Pang et al.,
2016) Duet (Mitra et al., 2017), SMN (Wu et al.,
2017)9, and a degenerated version of our model
that removes CNN3 from our MT-hCNN model
(MT-hCNN-d). All the methods in this paper
are implemented with TensorFlow and are trained
with NVIDIA Tesla K40M GPUs.
Settings: We use the same parameter settings of
hCNN in (Yu et al., 2017). For the CNN3 in our
model, we set window size of convolution layer as
2, ReLU as the activation function, and the stride

6Around 85% of conversations are within 3 turns.
7A “response” here is a question in our system.
8The threshold is determined by a business analyst
9The results are based on the TensorFlow code from au-

thors, and with no over sampling of negative training data.

of max-pooling layer as 2. The hidden node size of
the Fully-Connected layer is set as 128. AdaDelta
is used to train our model with an initial learning
rate of 0.08. We use MAP, Recall@5, Recall@2,
and Recall@1 as evaluation metrics. We set λ1 =
λ2 = λ3 = 0.05, and λ4 = 0.005.

3.1 Comparison on Base Models

The comparisons on base models are shown in Ta-
ble 1. First, the RNN based methods like MV-
LSTM and SMN have clear advantages over the
two CNN-based approaches like ARC-I and ARC-
II, and are better or comparable with the state-of-
the-art CNN-based models like Pyramid and Duet;
Second, our MT-hCNN outperforms MT-hCNN-
d, which shows the benefits of adding a convolu-
tional layer to the output representations of all the
utterances; Third, we find SMN does not perform
well in AliMeData compared to UDC. One po-
tential reason is that UDC has significantly larger
data size than AliMeData (1000k vs. 51k), which
can help to train a complex model like SMN; Last
but not least, our proposed MT-hCNN shows the
best results in terms of all the metrics in AliMe-
Data, and the best results in terms of R@2 and
R@1 in UDC, which shows the effectiveness of
MT-hCNN.

We further evaluate the inference time 10 of
these models. As shown in Table 1, MT-hCNN
has comparable or better results when compared
with SMN (the state-of-the-art multi-turn conver-
sation model), but is much more efficient than
SMN (∼60% time reduction). MT-hCNN also has
similar efficiency with CNN-based methods but
with better performance. As a result, our MT-
hCNN module is able to support a peak QPS 11

of 40 on a cluster of 2 service instances, where
each instance reserves 2 cores and 4G memory on

10The time of scoring a query and N candidate questions,
where N is 10 in UDC, and 15 in AliMeData.

11Queries Per Second



212

an Intel Xeon E5-2430 machine. This shows the
model is applicable to industrial bots. In all, our
proposed MT-hCNN is shown to be both efficient
and effective for question matching in multi-turn
conversations.

3.2 Transferablity of our model
To evaluate the effectiveness of our transfer learn-
ing setting, we compare our full model with three
baselines: Src-only that uses only source data,
Tgt-only that uses only target data, and TL-S that
uses both source and target data with the adversar-
ial training as in (Liu et al., 2017). All the methods
are evaluated on the test set of the target data.

As in Table 2, Src-only performs worse than
Tgt-only. This shows the source and target do-
mains are related but different. Despite the domain
shift, TL-S is able to leverage knowledge from the
source domain and boost performance; Last, our
model shows better performance than TL-S, this
shows the helpfulness of adding domain discrimi-
nators on both source and target domains.

Table 2: Transferablity of our model.

Data E-commerce data (AliMeData)
Methods MAP R@5 R@2 R@1
Src-only 0.7012 0.7123 0.4343 0.2846
Tgt-only 0.8418 0.7810 0.4796 0.3241
TL-S 0.8521 0.8022 0.4812 0.3255
Ours 0.8523 0.8125 0.4881 0.3291

3.3 Online Evaluations
We deployed our model online in AliMe Assist
Bot. For each query, the bot uses the TF-IDF
model in Lucene to return a set of candidates, then
uses our model to rerank all the candidates and re-
turns the top. We set the candidate size as 15 and
context length as 3. To accelerate the computation,
we bundle the 15 candidates into a mini-batch to
feed into our model. We compare our method with
the online model - a degenerated version of our
model that only uses the current query to retrieve
candidate, i.e. context length is 1. We have run 3-
day A/B testing on the Click-Through-Rate (CTR)
of the models. As shown in Table 3, our method
consistently outperforms the online model, yield-
ing 5% ∼ 10% improvement.
4 Related Work

Recent research in multi-turn conversation sys-
tems has focused on deep learning and reinforce-

Table 3: Comparison with the online model.

CTR Day1 Day2 Day3
Online Model 0.214 0.194 0.221
Our Model 0.266 0.291 0.288

ment learning (Shang et al., 2015; Yan et al., 2016;
Li et al., 2016a,b; Sordoni et al., 2015; Wu et al.,
2017). The recent proposed Sequential Matching
Network (SMN) (Wu et al., 2017) matches a re-
sponse with each utterance in the context at multi-
ple levels of granularity, leading to state-of-the-art
performance on two multi-turn conversation cor-
pora. Different from SMN, our model is built on
CNN based modules, which yields comparable re-
sults but with better efficiency.

We study transfer learning (TL) (Pan and Yang,
2010) to help domains with limited data. TL has
been extensively studied in the last decade. With
the popularity of deep learning, many Neural Net-
work (NN) based methods are proposed (Yosin-
ski et al., 2014). A typical framework uses a
shared NN to learn shared features for both source
and target domains (Mou et al., 2016; Yang et al.,
2017). Another approach is to use both a shared
NN and domain-specific NNs to derive shared and
domain-specific features (Liu et al., 2017). This
is improved by some studies (Ganin et al., 2016;
Taigman et al., 2017; Chen et al., 2017; Liu et al.,
2017) that consider adversarial networks to learn
more robust shared features across domains. Our
TL model is based on (Liu et al., 2017), with en-
hanced source and target specific domain discrim-
ination losses.

5 Conclusion

In this paper, we proposed a conversation model
based on Multi-Turn hybrid CNN (MT-hCNN).
We extended our model to adapt knowledge
learned from a resource-rich domain. Extensive
experiments and an online deployment in AliMe
E-commerce chatbot showed the efficiency, effec-
tiveness, and transferablity of our proposed model.

Acknowledgments

The authors would like to thank Weipeng Zhao,
Juwei Ren, Zhiyu Min and other members of Al-
iMe team for their help in deploying our model
online. This work was supported in part by NSF
grant IIS-1419693. We would also like to thank
reviewers for their valuable comments.



213

References
S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning.

2015. A large annotated corpus for learning natural
language inference. In EMNLP.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning for
chinese word segmentation. CoRR abs/1704.07556.

Y. Ganin, E. Ustinova, H. Ajakan, P. Germain,
H. Larochelle, F. Laviolette, M. Marchand, and
V. Lempitsky. 2016. Domain-adversarial training of
neural networks. Journal of Machine Learning Re-
search 17(59):1–35.

B. Hu, Z. Lu, H. Li, and Q. Chen. 2014. Convolu-
tional neural network architectures for matching nat-
ural language sentences. In NIPS ’14.

Feng-Lin Li, Minghui Qiu, Haiqing Chen, Xiong-
wei Wang, Xing Gao, Jun Huang, Juwei Ren,
Zhongzhou Zhao, Weipeng Zhao, Lei Wang, and
Guwei Jin. 2017. Alime assist: An intelligent as-
sistant for creating an innovative e-commerce expe-
rience. In CIKM 2017. Demo.

J. Li, M. Galley, C. Brockett, G. P. Spithourakis, J. Gao,
and W. B. Dolan. 2016a. A persona-based neural
conversation model. In ACL’16.

J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley,
and J. Gao. 2016b. Deep reinforcement learning for
dialogue generation. In EMNLP’16.

P. Liu, X. Qiu, and X. Huang. 2017. Adversarial multi-
task learning for text classification. In ACL.

R. Lowe, N. Pow, I. Serban, and J. Pineau. 2015. The
ubuntu dialogue corpus: A large dataset for research
in unstructured multi-turn dialogue systems. CoRR
abs/1506.08909.

B. Mitra, F. Diaz, and N. Craswell. 2017. Learning to
match using local and distributed representations of
text for web search. In WWW ’17.

L. Mou, Z. Meng, R. Yan, G. Li, Y. Xu, L. Zhang, and
Z. Jin. 2016. How transferable are neural networks
in nlp applications? In EMNLP.

S. Pan and Q. Yang. 2010. A survey on transfer learn-
ing. IEEE Transactions on knowledge and data en-
gineering 22(10):1345–1359.

L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, and X. Cheng.
2016. Text matching as image recognition. In AAAI.

Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan
Chen, Weipeng Zhao, Haiqing Chen, Jun Huang,
and Wei Chu. 2017. Alime chat: A sequence to se-
quence and rerank based chatbot engine. In ACL.

L. Shang, Z. Lu, and H. Li. 2015. Neural responding
machine for short-text conversation. In ACL ’15.

A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,
M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015. A
neural network approach to context-sensitive gener-
ation of conversational responses. In NAACL ’15.

Y. Taigman, A. Polyak, and L. Wolf. 2017. Unsuper-
vised cross-domain image generation. ICLR .

S. Wan, Y. Lan, J. Guo, J. Xu, L. Pang, and X. Cheng.
2016. A deep architecture for semantic matching
with multiple positional sentence representations. In
AAAI ’16.

Y. Wu, W. Wu, C. Xing, M. Zhou, and Z. Li. 2017. Se-
quential matching network: A new architecture for
multi-turn response selection in retrieval-based chat-
bots. In ACL ’17.

Z. Xu, B. Liu, B. Wang, C. Sun, and X. Wang.
2016. Incorporating loose-structured knowledge
into LSTM with recall gate for conversation mod-
eling. CoRR .

R. Yan, Y. Song, and H. Wu. 2016. Learning to re-
spond with deep neural networks for retrieval-based
human-computer conversation system. In SIGIR
’16.

Liu Yang, Minghui Qiu, Chen Qu, Jiafeng Guo,
Yongfeng Zhang, W. Bruce Croft, Jun Huang,
and Haiqing Chen. 2018. Response ranking with
deep matching networks and external knowledge in
information-seeking conversation systems. In SI-
GIR ’18.

Z. Yang, R. Salakhutdinov, and W. W Cohen. 2017.
Transfer learning for sequence tagging with hierar-
chical recurrent networks. ICLR .

W. Yin and H. Schütze. 2015. Convolutional neural
network for paraphrase identification. In NAACL-
HLT .

W. Yin, H. Schütze, B. Xiang, and B. Zhou. 2016.
Abcnn: Attention-based convolutional neural net-
work for modeling sentence pairs. Transactions of
ACL 4:259–272.

J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. 2014.
How transferable are features in deep neural net-
works? In NIPS.

Jianfei Yu, Minghui Qiu, Jing Jiang, Jun Huang,
Shuangyong Song, Wei Chu, and Haiqing Chen.
2017. Modelling domain relationships for transfer
learning on retrieval-based question answering sys-
tems in e-commerce. WSDM .


