



















































Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3316–3325
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3316

Hierarchical Dirichlet Gaussian Marked Hawkes Process
for Narrative Reconstruction in Continuous Time Domain

Yeon Seonwoo, Sungjoon Park and Alice Oh
Department of Computing, KAIST, Republic of Korea

{yeon.seonwoo, sungjoon.park}@kaist.ac.kr, alice.oh@kaist.edu

Abstract

In news and discussions, many articles and
posts are provided without their related pre-
vious articles or posts. Hence, it is difficult
to understand the context from which the ar-
ticles and posts have occurred. In this paper,
we propose the Hierarchical Dirichlet Gaus-
sian Marked Hawkes process (HD-GMHP) for
reconstructing the narratives and thread struc-
tures of news articles and discussion posts.
HD-GMHP unifies three modeling strategies
in previous research: temporal characteris-
tics, triggering event relations, and meta in-
formation of text in news articles and discus-
sion threads. To show the effectiveness of
the model, we perform experiments in narra-
tive reconstruction and thread reconstruction
with real world datasets: articles from the New
York Times and a corpus of Wikipedia con-
versations. The experimental results show that
HD-GMHP outperforms the baselines of LDA,
HDP, and HDHP for both tasks.

1 Introduction

Online news sites and discussion forums generate
large volumes of articles and discussions, which
we can call “events”. To fully understand the dis-
cussions and the news stories, one often needs a
larger context for that text, such as what related
posts and relevant articles have been posted be-
fore. For instance, to understand a news article
about the presidential elections, we would need
to know the history of the candidates’ political
actions through relevant previous articles. While
there are some news articles with a curated set of
related articles and discussion threads with a well-
organized structure, there are many more articles
and discussion threads for which the structure is
absent or incomplete. In this context, automat-
ically reconstructing the narrative of articles and
thread structure is an important problem.

Generally, textual information and various meta
information such as location and keywords are
used as features to solve this problem of narra-
tive reconstruction. With these features, previous
research mainly focus on three modeling strate-
gies. First, they model the triggering relationship
of events to identify which preceding events led
to the occurrence of the current event. Second,
they use meta information such as location and
keywords. Third, they consider the temporal char-
acteristics in the event stream, such that events in
close temporal proximity are more likely to be re-
lated. However, there is no method that effectively
considers all three of these. In narrative recon-
struction, there are several approaches that focus
on using meta information and temporal character-
istics with clustering methods (Zhou et al., 2016;
Tang et al., 2015; Ahmed et al., 2011), and there
are several approaches using the Hawkes process
to model the temporal characteristics (Du et al.,
2015; Mavroforakis et al., 2017; Jankowiak and
Gomez-Rodriguez, 2017). In thread reconstruc-
tion, there are approaches that focus on modeling
triggering relationships of events and using meta
information (Kim et al., 2010; Louis and Cohen,
2015; Wang et al., 2011b).

In this paper, we propose a novel Gaussian
Marked Hawkes Process (GMHP) that effectively
reconstructs the narrative structure of articles and
the thread structure of discussions considering
all three modeling strategies. GMHP uses the
Hawkes process to model events in continuous
time, a Gaussian distribution for modeling the
meta information of text, and the mixture of Gaus-
sian for modeling the triggering relationships of
events. The detailed modeling strategies are de-
scribed as follows. We use the Hawkes process
to model time in the continuous domain, as the
Hawkes process is a stochastic process used to un-
derstand a sequence of events in continuous time



3317

(Iwata et al., 2013; Rong et al., 2015). To use meta
information, we represent text and meta informa-
tion in a general vector form and use the Hawkes
process to handle the vector of event information
with a Gaussian distribution. To model the trig-
gering relationships, we assume a model structure
parameterized by each preceding event so that an
event can be directly generated from a probability
distribution parameterized by preceding events.

The GMHP models a single narrative or thread
in event streams. To find the narratives or threads
from a mixture of event streams, we combine our
GMHP model with the Hierarchical Dirichlet Pro-
cess to build HD-GMHP.

We evaluate the effectiveness of our model
with two real world datasets: articles from the
New York Times, and discussion threads from
Wikipedia. In the New York Times dataset, we
perform a narrative reconstruction experiment and
compare the results with the human annotated nar-
rative labels. In the Wikipedia discussion cor-
pus, we perform two kinds of thread reconstruc-
tion experiment. One is grouping posts in the same
thread. The other is reconstructing the post-reply
structure of the posts. From these experiments, we
see that our model outperforms the state-of-the-art
model, the hierarchical Dirichlet Hawkes process
(HDHP) (Mavroforakis et al., 2017).

The contributions of our research are threefold.
First, we propose the Gaussian Marked Hawkes
Process that effectively models a single narrative
(event stream) with all three modeling strategies
used in previous research. Second, we propose
HD-GMHP, a combination of the GMHP model
with the HDP to reconstruct the narratives of ar-
ticles and the thread structure of discussions from
a mixture of event streams. Finally, we propose a
novel inference algorithm of the HD-GMHP with
the Sequential Monte Carlo method (Doucet et al.,
2001).

2 Related Work

Narrative Reconstruction: One major approach
to reconstructing narratives from news articles is
clustering articles by using a variant of the Chi-
nese Restaurant Process (CRP). Related work such
as (Zhou et al., 2016; Tang et al., 2015; Ahmed
et al., 2011) models chronologically ordered news
articles with text and various meta information in-
cluding author, organization, keywords, and loca-
tion. They use the CRP, distant-dependent CRP

(Blei and Frazier, 2011). There is research that
uses recurrent CRP (Ahmed and Xing, 2008) and
exponential time decaying kernel to model prob-
ability of time difference between two relevant
events. But they use discrete time information in-
stead of continuous form and handcrafted param-
eters of the kernel (Ahmed et al., 2011).

There is another approach that reconstructs nar-
ratives by directly extracting important sentences
from articles. (Xu et al., 2013) proposes a model
that considers the sentence and image level narra-
tive reconstruction as an optimization problem and
solves it by maximizing the divergence of narra-
tives with some constraints. (Wang et al., 2016)
solves the narrative reconstruction problem as a
sentence recommendation problem and uses ma-
trix factorization. But these existing models focus
on how to handle text and meta information of ar-
ticles, while our model uses the Hawkes process
to effectively model continuous time information
of events.
Discussion Thread Reconstruction: There are
several approaches to reconstruct threads from a
corpus of unstructured discussions. (Wang et al.,
2011a) uses Conditional Random Field to recon-
struct reply structure in discussion corpus. (Balali
et al., 2014) uses content, time and author infor-
mation as features of a single post with rank SVM
to reconstruct thread structure. (Dehghani et al.,
2013; Aumayr et al., 2011) uses SVM and a deci-
sion tree with meta information of posts.

However, a major limitation in these previous
research is that they are assuming that for each
post, the main thread where it belongs is given.
That is, the problem they solve is finding the post
for which a post is immediately replying, rather
than treating the corpus as a single set of posts
with no known information about the threads, the
initial post of each thread, and the posts that be-
long to each thread. This limitation of the previous
research means those approaches are not applica-
ble in more general online conversation data, such
as IRC or a Facebook group chat which is a mas-
sive unstructured online discussion for which the
initial post of a thread is not labeled. Unlike this
strong assumption in previous research, we use a
more general assumption that the initial posts are
unknown, so our approach would be applicable to
a wider, more general discussion data. Also, as
in the narrative reconstruction research area, pre-
vious research focuses on how to handle text and



3318

meta information in posts. Again, unlike previous
research, our research uses the Hawkes process to
model continuous time information.
Continuous Time Modeling: The Hawkes pro-
cess, a stochastic process that models continuous
time information of events with event occurrence
history, is an effective solution to model events
in continuous time. One of the main research
themes in the Hawkes process literature is find-
ing which events trigger which other events. (He
et al., 2015) models the topic diffusion patterns in
a social network by inferring the triggering node
with the Hawkes process. The Hawkes process
is also used to model social event streams (Rong
et al., 2015) and to classify rumors (Lukasik et al.,
2016), and a combination of the Hawkes process
and the Dirichlet mixture model is used to cluster
event streams (Xu and Zha, 2017).

Recent research clusters text streams with the
Hawkes process and the Chinese Restaurant Pro-
cess or the Chinese Restaurant Franchise (Mavro-
forakis et al., 2017; Du et al., 2015). They use the
bag-of-words representation of text in their model,
while (Jankowiak and Gomez-Rodriguez, 2017)
proposes a Hawkes process model that can han-
dle a more general vector representation of events.
The main difference of our model compared to this
research is that we add the triggering relationship
of two events. With this addition, our model can
reconstruct narratives with an explicit relation of
two documents.

3 Hawkes Processes

Before we describe our proposed model, we
briefly explain the Hawkes process, one of two
main stochastic processes used in our model. We
leave out the explanation of the HDP due to space.

The Hawkes process (Hawkes, 1971) is a sub-
class of temporal point processes, whose func-
tional form for intensity with exponential decay-
ing kernel is represented as

λ∗(t) = λ0(t) +

∫ t
0
αβe−β(t−s)dN(s),

where the intensity, λ∗(t) represents the condi-
tional probability of an event occurrence within
time window [t, t + dt). The Hawkes process is
used to model the number of occurrences of events
where one event can trigger other events. In the
equation above, the base intensity λ0(t) models
the intensity of events that occur on their own ini-
tiative whereas αβe−β(t−s) models the intensity of

events that are triggered by the previous event that
occurred at time s. Here, multiplication of α and
β represents influence of the previous event and
β represents decaying rate of the influence. Thus,
the effect of the previous event exponentially de-
cays with respect to the time difference. From the
definition of intensity λ∗(t), the derived likelihood
form of the Hawkes process is as follows,

f(D|Θ) = e−Λ(T )
n∏
i=1

λ∗(ti), (1)

where Λ(T ) =
∫ T

0 λ
∗(t)dt.

4 Problem Setting

In this section, we define the event stream and the
narrative and the thread reconstruction problems.

Definition of Event Stream: If a text appears
at time ti, we define the event si as (ti, ~ei, zi, xi).
Here, ~ei is the feature vector of the text, xi is the
latent global cluster indicator of event si which
represents the cluster for events with similar text
information, and zi is the latent local cluster in-
dicator for events that are temporally related in
the same cluster. We define event stream S as
[s1, .., sn].

Assumptions: 1) We assume that two events in
same local cluster occur in near time and have sim-
ilar feature vectors ~e. These properties are called
temporal and spatial locality. 2) We assume hierar-
chy structure of a global cluster and a local cluster.
That is, one global cluster can consist of multiple
local clusters.

Problem Formulation: We formulate the spa-
tial locality of two events in the same local cluster
with a Gaussian distribution. If two events si and
sj are in the same local cluster and ti > tj , then
we assume the later event ~ei is generated from one
of two relations,

~ei ∼ N (~ej ,Σv), ~ei,∼ N (~e0,Σ0).

Here, ~e0 is the base event vector and Σ0 is the co-
variance matrix of the cluster. Σv is the covariance
matrix of the Gaussian distribution generated by a
past event in the cluster.

We use the Hawkes process to formulate the
temporal locality of two events in the same local
cluster. If event si and sj are in the same local
cluster and ti > tj , then ti is generated from in
either following relations,

ti ∼ Poisson Process(µ),
ti − tj ∼ Hawkes(α, β).



3319

Here, if ti is generated from Hawkes process of
parameter α and β with time tj and ~ei is generated
from ~ej , then we say that event sj is the parent
event of event si.

We formulate the hierarchy structure of the
global cluster and the local cluster with Hierarchi-
cal Dirichlet Process (Teh et al., 2006). If the pa-
rameters θzi of local clusters z1, z2, .., zn are equal
to the parameters of the global cluster Θx, then we
say that there is a hierarchy between all the local
clusters and the global cluster. And this hierarchy
structure can be written as follows,

Θx = θz1 = θz2 = ... = θzn .

Now, we define the narrative reconstruction and
the thread reconstruction problem as a problem of
inferring the latent variables in S.

5 Model

We now describe clustering a mixture of event
stream S with the Gaussian Marked Hawkes Pro-
cess and the hierarchical Dirichlet process. We
first propose Gaussian Marked Hawkes Process
(GMHP) that models temporal and spatial local-
ity assumptions that described in section 4. And
after defining the GMHP, we propose Hierarchical
Dirichlet Gaussian Marked Hawkes Process (HD-
GMHP), a combination of the GMHP with the Hi-
erarchical Dirichlet Process. The GMHP models
event streams with the same local cluster z and
HDP groups the local clusters to one global cluster
x.

5.1 Gaussian Marked Hawkes Processes

5.1.1 Model Description
In GMHP, we assume events are generated by a
past event or by their own initiative. If event si is
generated by event sj , then we say that event sj is
the parent event of event si. If event si occurs on
their own initiative, the index of the parent event is
0. We define the intensity function with the given
parent event ci as follows,

λ(ti|ci) =

{
µ if ci = 0
αβe−β(ti−tci ) otherwise

(2)

To model the spatial locality of two D-
dimensional event vectors ~ei, ~eci , we define prob-
ability distribution for ~ei as follows,

pci(~ei) =

{
N (~ei|~e0,Σ0) if ci = 0
N (~ei| ~eci ,Σv) otherwise

(3)

Here, ~e0 is the base event vector for when ci = 0.
Σ0 and Σv are covariance matrix for when an event
occurs by their own initiative or occurs by past
event. From the above definitions, we can calcu-
late the intensity of the event vector ~e at time t as
follows,

λ~e(t) = µN (~e|~e0,Σ0) +
∑
tj<t

λ(t|j)N (~e|~ej ,Σv).

(4)
The total intensity of GMHP can be obtained by
integrating the above intensity with the event vec-
tor ~e.

λ(t) =

∫
RD

λ~e(t) d~e = µ+
∑
tj<t

λ(t|j). (5)

5.1.2 Parameter estimation
From equation 1, the likelihood of the observed
event stream can be computed as follows,

f(D|θ) = e−Λ(T )
n∏
i=1

∑
0≤j<i

pj(~ei)λ(ti|j), (6)

where Λ(T ) = µT +
n∑
i=1

α(1− e−β(T−ti)).

Since the likelihood of GMHP is hard to maxi-
mize, instead of using the likelihood, we define a
likelihood with the given parent events as follows,

f(D|C, θ) =e−Λ(T ) ×
n∏
i=1

{(µN (~ei|~e0,Σ0))Ci0×

i−1∏
j=1

(αβeβ(ti−tj)N (~ei|~ej ,Σv))Cij},

(7)

where Cij becomes 1 when ci = j and 0 other-
wise. By maximizing equation 7, we can estimate
the parameter θ = {µ, α,~e0,Σ0,Σv}. The infer-
ence step of the parent events is described in sec-
tion 6.

5.2 Modeling a Mixture of GMHP with the
HDP

When clustering a mixture of streams using the
Hawkes process, the exponential triggering func-
tion prevents two events with a large time differ-
ence from being assigned to the same global clus-
ter. To solve this problem, (Mavroforakis et al.,
2017) uses the HDP instead of using the Dirichlet
process used in (Du et al., 2015). The hierarchy
structure of the HDP assigns a cluster label with
a probability proportional to the size of the clus-
ter. This allows assignment of two events with a



3320

large time difference to the same cluster. For the
same reason, we use the HDP to model mixture
of the GMHP. We consider each GMHP in mix-
ture as a table in the Chinese Restaurant Franchise
metaphor. Since the intensity of k’th GMHP, λk(t)
represents how likely an event occurs in table k at
time t, we use the intensity as the number of cus-
tomers in the CRF metaphor. The whole genera-
tive process of HD-GMHP is as follows.

1. Initialize the number of local clustersK = 0,
the number of global clusters M = 0.

2. For n ∈ 1, 2, ..., N
(a) Draw tn from Hawkes(λ0 +

K∑
k=0

λk)

(b) Draw zn as follows.

zn ∼ λ0δ(K + 1) +
K∑
k=1

λkδ(k) (8)

(c) If zn = K + 1, assign global cluster xn,
which is interpreted as parameter(θxn)
for local cluster zn, and Increment K.
Here, Nm is number of local cluster in
global cluster m.

xn ∼ γδ(M + 1) +
M∑
m=1

Nmδ(m) (9)

(d) If xn = M + 1, increment M and draw
new parameter as follows.
αxn ∼ Γ(αa, βa), µxn ∼ Γ(αµ, βµ)

1
Σxn0
∼ Γ(α0, β0), 1Σxnv ∼ Γ(αv, βv)

~e0,xn ∼ N (~e0,Σxn0 /~λe0)
Note that we assume that the covariance
matrix Σxnv , and Σ

xn
0 are diagonal.(e) Draw cn and ~en. Here, gxn(t) =

αxnβe
−β(tn−t).

cn ∼ µxnδ(Nzn + 1) +
Nzn∑
j=1

gxn(tj)δ(j)

(10)
if cn = Nzn + 1, then replace cn with 0
and sample event vector.

~en ∼

{
N (~e0,xn ,Σxn0 ) if cn = 0
N (~ecn ,Σxnv ) otherwise

(11)
λ0, γ, ~e0, ~λe0 , (αa, βa), (αµ, βµ), (α0, β0), and
(αv, βv) are the hyperparameters used in HD-
GMHP.

6 Inference

To infer the latent variables z and x for each event
from an observed event stream s1:no where s

i
o =

(ti, ~ei) with observation time T , we propose an

Algorithm 1 Inference
Input: Stream data So
Initialize wi1 =

1
P , i ∈ {1, 2, ..., P}.

for n = 1 to N do
for i = 1 to P do

Update Θ as described in section 6.2.
Sample (x, z, c)in with equation 16, 10
Update win with equation 17

end for
Normalize w1:Pn
if ||wn||−22 < thresh then

Resample particles
end if

end for

online inference algorithm with Sequential Monte
Carlo (SMC) (Doucet et al., 2001). To calculate
the posterior of the latent variables z and x for
each timestamp ti in the inference, we need the es-
timated parameter to calculate the intensity at each
time ti, λ(ti). As described in section 5.1.2, the
parameter estimation step needs the parent event
information. In our proposed inference, the par-
ent events are inferred from SMC. The inference
algorithm is summarized in algorithm 1.

6.1 Sequential Monte Carlo with parent
event inference

To approximate the posterior of the latent vari-
ables, SMC samples the latent variables from the
proposal distribution and calculates the weight of
each sampled variables which is called the parti-
cle weight. To infer the parent event in SMC, we
define the particle weight of our modified SMC as
follows:

win =
p(ψi1:n|s1:no )
q(ψi1:n|s1:no )

p(ci1:n|ψi1:n, s1:no )
q(ci1:n|ψi1:n, s1:no )

(12)

Here, ψin is (x
i
n, z

i
n). Let the left part on the right

hand term and right part on the right hand term
of the equation 12 are wψin and wc

i
n. Then the

terms can be calculated by wψin = ηψwψ
i
n−1 and

wc
i
n = ηcwc

i
n−1, where the ηψ is

p(~en, tn, ψn|s1:n−1o , ψi1:n−1)
q(ψin|ψi1:n−1, s1:no )

. (13)

and ηc is

p(~en|tn, s1:n−1o , δi1:n)
p(cin|tn, δi1:n−1, s1:n−1o , ψin)
q(cin|δi1:n−1, s1:no , ψin)

.

(14)
Here, δin is (x

i
n, z

i
n, c

i
n).



3321

We use p(ψn|ψ1:n−1, s1:no ) as the proposal dis-
tribution of ψin in the equation 13 to minimize
the variance of win (Doucet et al., 2000) and
p(cn|δ1:n−1, ψn, tn, s1:n−1o ) as the proposal distri-
bution of cin in the equation 14. From the above
proposal distribution, ηcin can be calculated as
ηc
i
n = p(~en|tn, s1:n−1o , ci1:n, ψ1:n) and ηψin can be

calculated by the following form .
ηψ

i
n =p(tn|ψ1:n−1, zn, s

1:n−1
o )

×
∑
zn

(p(~en|ψ1:n−1, zn, tn, s1:n−1o )

× p(zn|tn, ψ1:n−1, s1:n−1o ))

(15)

From the proposal distribution of ψin, we can
sample ψin as follows:
p(ψn|rest) ∝p(~en|ψ1:n, tn, s1:n−1o )

× p(ψn|ψ1:n−1, tn, s1:n−1o )
× p(tn|ψ1:n−1, s1:n−1o )

(16)

Here, the term p(~en|ψ1:n, tn, s1:n−1o ) ×
p(ψn|ψ1:n−1, tn, s1:no ) can be simply calcu-
lated by the student’s t-distribution derived from
the conjugate relation between the parameter
{~ek0, Σ0,k, Σv,k} and the normal-inverse-gamma
and inverse-gamma prior in the generative process
of HD-GMHP.

From ηcin = p(~en|tn, s1:n−1o , ci1:n, ψ1:n) and 15,
the particle weight can be updated by the follow-
ing,
win ∝win−1

× p(tn|s1:n−1o , ψi1:n)p(~en|cn, tn, s1:n−1o , ψ1:n)

×
∑
zn

(p(~en|zn, ψ1:n−1, tn, s1:n−1o )

× p(zn|tn, ψ1:n−1, s1:n−1o )).
(17)

When calculating the probability of tn in 17, we
assume the parameters µ1:K , α1:K are given (Car-
valho et al., 2010). From the likelihood of GMHP,
the probability term p(tn|ψ1:n, s1:n−1o ) in equa-
tion 17 can be calculated by λzn(tn)e

−Λ(tn,tn−1).
Where Λ(tn, tn − 1) is

λ0(tn − tn−1) + (tn − tn−1)
K∑
k=1

µk

+
1

β
(1− e−β(tn−tn−1))

K∑
k=1

λk(tn−1).

(18)

In the case of the probability term p(~en|cn, rest)
and p(~en|zn, rest) in 17, as explained in the sam-
pling process of ψn, we can calculate the terms by

student’s t-distribution. With the particle weight
update rule 17 and the parameter update rule de-
scribed in section 6.2, we infer latent variables
with algorithm 1.

6.2 Updating Parameter
From the equation 7 and the prior of the parame-
ters used in GMHP, we can estimate the parame-
ters by following form.

αm =

αa − 1 +
∑
xi=m

∑
0<j<i

Cij

βa +
∑
xi=m

(1− e−β(T−ti))
(19)

µm =

αµ − 1 +
∑
xi=m

Ci0

βµ +
∑

θk=Θm

(T − t0,k)
(20)

~e0,m =

~e0 ◦ ~λe0 +
∑
xi=m

Ci0~ei

~λe0 +
∑
xi=m

Ci0
(21)

diag(Σm0 ) ={~λe0 ◦ (~e0,m − ~e0)2 + 2~β0
+

∑
xi=m

Ci0(~ei − ~e0,m)2}

× {2~α0 + 3 +
∑
xi=m

Ci0}−1
(22)

diag(Σmv ) =

2~βv +
∑
xi=m

∑
0<j<i

Cij(~ei − ~ej)2

2 + 2~αv +
∑
xi=m

∑
0<j<i

Cij

(23)

6.3 Approximation
To reduce the computation time in the inference al-
gorithm, we use several approximation strategies.

6.3.1 Marginal distribution Approximation
To calculate p(~en|zn, ψ1:n−1, tn, so1:n−1) in
the equation 17, we need marginalization of
p(~en, cn|zn, ψ1:n−1, tn, s1:n−1o ) which takes
time complexity of O(n of events in zn) and
cause the time complexity of the equation 17
to be O(n). To reduce the time complex-
ity, we note that event vector ~en is sampled
from a Gaussian mixture that the influence
of each Gaussian distribution is exponentially
decreases. We assume the marginal distribution
p(~en|zn, ψ1:n−1, tn, s1:n−1o ) can be approximated
to p(~en|c1:n = 0, zn, ψ1:n−1, tn, s1:n−1o ). From



3322

the approximation, we can calculate the posterior
predictive with student’s t-distribution. The result
of approximation is as follows,

p(~en|zn, ψ1:n−1, tn, s1:n−1o )

= tνn(~en|~mn,
~κn + 1

~κnνn
~Sn),

(24)

where

νn = 2α0 +Nzn , κn =
~λe0 +Nzn ,

~mn =

~λe0 ◦ ~e0 +
∑

zi=zn

~ei

~κn
,

~Sn = 2β0 +
∑
zi=zn

~e 2i +
~λe0 ◦ ~e 20 − κn ~m 2n .

To calculate p(~en|cn, tn, s1:n−1o , ψ1:n) in the
equation 17, we need to calculate posterior pre-
dictive for each past event. To reduce the
computation time in the process of calcula-
tion, we approximate the probability distribution
p(~en|cn, tn, s1:n−1o , ψ1:n) as follows.

p(~en|cn, tn, s1:n−1o , ψ1:n)

≈

{
N (~e0,xn ,Σxn0 ) if cn = 0
N (~ecn ,Σxnv ) otherwise

(25)

6.3.2 Sampling cn from recent W events
Sampling cn has time complexity of O(Nzn). To
reduce the time complexity to O(1), we sample cn
from recent W events in the local cluster zn.

7 Experiment

In this section, we demonstrate the narrative
reconstruction and thread reconstruction perfor-
mance of our model on a corpus of the New
York Times articles and the Wikipedia conversa-
tion dataset.

7.1 Dataset
New York Times Dataset: We collected 112,538
New York Times news articles from January 2016
to July 2017. The dataset contains the text, times-
tamp, the news section, and the keywords. These
keywords are semantic tags specified by the news-
room to indicate the main topics of the articles. We
select news articles in sections “U.S.”, “World”,
“Opinion”, and “Sports” that contain at least one

Table 1: Statistics of keywords. “N” column lists the
number of articles with the corresponding keyword.

Keyword N

Trump, Donald J 7940
Presidential Election of 2016 5737
United States Politics and Government 4986
Republican Party 2371
Clinton, Hillary Rodham 2330
Baseball 2058
United States International Relations 1817
Terrorism 1618
Obama, Barack 1551
Russia 1400

of the top ten most frequently used keywords. The
statistics of these keywords are described in table
1. Further, we select articles with more than ten
words in its body. The final number of articles
used in our experiment is 16,858. The dataset is
publicly available 1.
Wikipedia Conversation Dataset is released by
(Danescu-Niculescu-Mizil et al., 2012). The
dataset contains the timestamp, the initial post of
the conversation, “reply to” link information, and
the text information of each post in conversation
threads in Wikipedia talk pages. We select threads
that have ten or more posts from September 2010
to December 2010. The final number of posts used
in our experiment is 2,004 and the final number of
threads is 154.

7.2 Preprocessing

To apply our model to the real world datasets, we
represent each event with time information and an
event vector. For the time information, we take the
first article or post and set the time as zero, the last
article or post as one, and scale the timestamps of
all other articles and posts accordingly. To extract
the event vectors, we use different vectorization
methods for the two datasets. For the NYT dataset,
we use the document topic vector from LDA (Blei
et al., 2003). For the Wikipedia dataset, because
there are only a few words in each post, we cannot
use the LDA topic vector, so we use the averaged
word embedding vector (Mikolov et al., 2013) of
the words used in each post.

1https://github.com/yeonsw/NYT-dataset

https://github.com/yeonsw/NYT-dataset


3323

Table 2: Narrative reconstruction results in NYT
dataset and post grouping results in Wikipedia conver-
sation dataset.

AMI ARI

LDA + DBSCAN 0.0627 0.0117
HDP + DBSCAN 0.0260 0.0203
HDHP 0.1768 0.0746

NYT

HD-GMHP (100D) 0.2479 0.1416

W2V + DBSCAN 0.0055 0.0001
HDHP 0.4240 0.3512Wiki
HD-GMHP (100D) 0.5848 0.3834

7.3 Task

Narrative reconstruction: To demonstrate the
narrative reconstruction performance of our
model, we apply the inference method to our cor-
pus of NYT articles. We use a set of multiple
keywords of each article as the ground truth la-
bel. Then we run our model and consider the set
of articles with the same global cluster information
as one narrative. We compare the results with the
ground truth labels using the common clustering
metrics AMI and ARI (Hubert and Arabie, 1985;
Vinh et al., 2010) to evaluate the narrative recon-
struction performance of our model. We compare
HD-GMHP with the following baselines: LDA
and HDP with DBSCAN, and the Hierarchical
Dirichlet Hawkes Process (HDHP) (Mavroforakis
et al., 2017) which is a state-of-the-art model for
text and continuous timestamps of an event. Also,
to measure the similarity of each recovered narra-
tive and the ground truth narrative, we use the F1
score of the top ten narratives.
Thread reconstruction: In this experiment, we
use two evaluation criteria. One is post grouping
and the other is reply structure recovery, which
is simply the recovery of the child nodes. Here,
we use a different child node recovery task com-
pared to the child node recovery used in previous
research. In our task, we do not give the initial
post of each thread, while previous research does.
This makes thread reconstruction problem more
general and more difficult.

In post grouping, we use the initial post of each
of the posts as the ground truth label and measure
the clustering metrics used in the NYT dataset. In
the child node recovery experiment, we use the
parent event information inferred from our method
as the recovered tree structure of the threads. We

Table 3: F1-score of each label

Label N
HD-

GMHP
HDHP

Baseball 2011 0.8114 0.8899
Trump, Donald
&Politics and
Government

1664 0.1833 0.2157

Terrorism 1260 0.6052 0.4059
Trump, Donald
&Election

1110 0.2537 0.1939

Trump, Donald 994 0.0975 0.1227
Politics and
Government

822 0.1677 0.1215

Clinton, Hillary
&Election
&Trump, Donald

755 0.1754 0.1402

Election 714 0.1378 0.1280
Clinton, Hillary
&Election

665 0.1669 0.1157

Russia 637 0.3177 0.2223

Micro F-score N/A 0.2874 0.2189
Macro F-score N/A 0.3637 0.3165

measure the performance with node precision and
node recall metrics (Wang et al., 2011a; Dehghani
et al., 2013). We compare our model with the fol-
lowing baselines: HDHP, and a naive baseline that
reconstructs threads in the form of a single linked
list of posts in chronological order.

7.4 Metrics
AMI, ARI are commonly used to measure cluster-
ing performance (Hubert and Arabie, 1985; Vinh
et al., 2010). Pnode, Rnode measure local simi-
larity between two thread structures (Wang et al.,
2011a).

Pnode =
1

N

∑
i=1:N

|childGT(i) ∩ childE(i)|
|childE(i)|

Rnode =
1

N

∑
i=1:N

|childGT(i) ∩ childE(i)|
|childGT(i)|

where, childGT(i) and childE(i) are the sets of
children of node i in the ground truth thread struc-
ture and the recovered thread structure, respec-
tively. The author (Wang et al., 2011a) also pro-
posed Ppath, Rpath to measure the similarity of the
global structure of two threads. The path metrics
are sensitive to the recovered initial post of each
thread, but since we do not give the initial post of
each thread in our experiment, the path metrics are



3324

Table 4: Reply structure recovery results in Wikipedia
conversation dataset.

Pnode Rnode F1node

Naive Baseline 0.3223 0.6501 0.4310
HDHP 0.5598 0.5834 0.5714
HD-GMHP 0.6433 0.5468 0.5911

no longer proper in our experiment. So we mea-
sure the node metrics only.

7.5 Results
Table 2 shows the clustering accuracy of our
method and the baseline methods in real world
datasets. We average the results with five runs for
each model. The highest value for each metric
is indicated with boldface. From the results, we
establish that our model outperforms the baseline
methods in both the NYT narrative reconstruction
task and the Wikipedia thread reconstruction task.

For the NYT, to see the accuracy of our model
in more detail, we compute and show the F-scores
for the top ten most frequent labels and the micro
and macro averages in table 3. To compute the
F-score between the true labels and the recovered
cluster labels, we select the cluster with the high-
est F-score as the corresponding cluster. From the
results, we establish that our model performs bet-
ter than the baseline model, HDHP.

Table 4 shows the thread reconstruction re-
sults of our model and the baseline models in the
Wikipedia conversation dataset. Since the HDHP
model does not infer the parent event, we recon-
struct threads in the form of chronologically or-
dered linked list of posts in each local cluster that
inferred from HDHP. From the F1node score of the
results, we establish our model performs better
than other baseline models.

To demonstrate the robustness of HD-GMHP on
dimensional change of the input vector, we mea-
sure the performance of each task in using 50, 100,
and 150 dimensional vectors. The results are de-
scribed in table 5 and 6. From the results, we ver-
ify there are no drastic changes in performance in
both the NYT dataset and the Wikipedia dataset.

8 Conclusion

In this paper, we defined the narrative and thread
reconstruction problems as clustering problems.
To cluster the event streams with continuous time
information and triggering event information, we

Table 5: Model Robustness on dimensional change of
input vectors in NYT dataset.

AMI ARI

HD-GMHP (50D) 0.2310 0.1518
HD-GMHP (100D) 0.2479 0.1416
HD-GMHP (150D) 0.2421 0.1191

Table 6: HD-GMHP model robustness on dimen-
sional change of input vector in Wikipedia conversation
dataset.

AMI ARI Pnode Rnode

50D 0.5836 0.3782 0.6466 0.5554
100D 0.5848 0.3834 0.6433 0.5468
150D 0.5948 0.3670 0.6450 0.5473

proposed the Gaussian Marked Hawkes process
that models event streams with additional event in-
formation represented in a vector form. Further-
more, we combined our model GMHP with the
HDP to cluster event streams (HD-GMHP). We
showed that our model performs better than sev-
eral baseline methods in both narrative reconstruc-
tion in a dataset of NYT articles and thread recon-
struction in a dataset of Wikipedia conversations.

Acknowledgments

Yeon Seonwoo is supported by NCSoft Corpo-
ration. Sungjoon Park and Alice Oh are sup-
ported by the Engineering Research Center Pro-
gram through the National Research Foundation
of Korea (NRF) funded by the Korean Govern-
ment MSIT (NRF-2018R1A5A1059921).

References
Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,

Alexander J Smola, and Choon Hui Teo. 2011. Uni-
fied analysis of streaming news. In WWW.

Amr Ahmed and Eric Xing. 2008. Dynamic non-
parametric mixture models and the recurrent chinese
restaurant process: with applications to evolution-
ary clustering. In SIAM International Conference
on Data Mining.

Erik Aumayr, Jeffrey Chan, and Conor Hayes. 2011.
Reconstruction of threaded conversations in online
discussion forums. In ICWSM.

A Balali, H Faili, and M Asadpour. 2014. A super-
vised approach to predict the hierarchical structure
of conversation threads for comments. The Scien-
tific World Journal, 2014.



3325

David M Blei and Peter I Frazier. 2011. Distance de-
pendent chinese restaurant processes. Journal of
Machine Learning Research, 12(Aug):2461–2488.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Carlos M Carvalho, Michael S Johannes, Hedibert F
Lopes, Nicholas G Polson, et al. 2010. Parti-
cle learning and smoothing. Statistical Science,
25(1):88–106.

Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: Language effects and power differences in
social interaction. In WWW.

Mostafa Dehghani, Azadeh Shakery, Masoud Asad-
pour, and Arash Koushkestani. 2013. A learning ap-
proach for email conversation thread reconstruction.
Journal of Information Science, 39(6):846–863.

Arnaud Doucet, Nando De Freitas, and Neil Gordon.
2001. An introduction to sequential monte carlo
methods. In Sequential Monte Carlo methods in
practice, pages 3–14. Springer.

Arnaud Doucet, Nando De Freitas, Kevin Murphy, and
Stuart Russell. 2000. Rao-blackwellised particle fil-
tering for dynamic bayesian networks. In UAI.

Nan Du, Mehrdad Farajtabar, Amr Ahmed, Alexan-
der J Smola, and Le Song. 2015. Dirichlet-
hawkes processes with applications to clustering
continuous-time document streams. In SIGKDD.

Alan Hawkes. 1971. Spectra of some self-exciting
and mutually exciting point processes. Biometrika,
pages 83–90.

Xinran He, Theodoros Rekatsinas, James Foulds, Lise
Getoor, and Yan Liu. 2015. Hawkestopic: A joint
model for network inference and topic modeling
from text-based cascades. In ICML.

Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing partitions. Journal of classification, 2(1):193–
218.

Tomoharu Iwata, Amar Shah, and Zoubin Ghahramani.
2013. Discovering latent influence in online social
activities via shared cascade poisson processes. In
SIGKDD.

Martin Jankowiak and Manuel Gomez-Rodriguez.
2017. Uncovering the spatiotemporal patterns of
collective social activity. In SIAM International
Conference on Data Mining.

Su Nam Kim, Li Wang, and Timothy Baldwin. 2010.
Tagging and linking web forum posts. In CoNLL.

Annie P Louis and Shay B Cohen. 2015. Conversa-
tion trees: A grammar model for topic structure in
forums. In EMNLP.

Michal Lukasik, PK Srijith, Duy Vu, Kalina
Bontcheva, Arkaitz Zubiaga, and Trevor Cohn.
2016. Hawkes processes for continuous time se-
quence classification: an application to rumour
stance classification in twitter. In ACL.

Charalampos Mavroforakis, Isabel Valera, and Manuel
Gomez-Rodriguez. 2017. Modeling the dynamics of
learning activity on the web. In WWW.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS.

Yu Rong, Hong Cheng, and Zhiyu Mo. 2015. Why it
happened: Identifying and modeling the reasons of
the happening of social events. In SIGKDD.

Siliang Tang, Fei Wu, Si Li, Weiming Lu, Zhongfei
Zhang, and Yueting Zhuang. 2015. Sketch the sto-
ryline with charcoal: A non-parametric approach. In
IJCAI.

Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.

Nguyen Xuan Vinh, Julien Epps, and James Bailey.
2010. Information theoretic measures for cluster-
ings comparison: Variants, properties, normaliza-
tion and correction for chance. Journal of Machine
Learning Research, 11(Oct):2837–2854.

Hongning Wang, Chi Wang, ChengXiang Zhai, and Ji-
awei Han. 2011a. Learning online discussion struc-
tures by conditional random fields. In SIGIR.

Li Wang, Marco Lui, Su Nam Kim, Joakim Nivre,
and Timothy Baldwin. 2011b. Predicting thread
discourse structure over technical web forums. In
EMNLP.

William Yang Wang, Yashar Mehdad, Dragomir R
Radev, and Amanda Stent. 2016. A low-rank ap-
proximation approach to learning joint embeddings
of news stories and images for timeline summariza-
tion. In NAACL.

Hongteng Xu and Hongyuan Zha. 2017. A dirichlet
mixture model of hawkes processes for event se-
quence clustering. In NIPS.

Shize Xu, Shanshan Wang, and Yan Zhang. 2013.
Summarizing complex events: a cross-modal solu-
tion of storylines extraction and reconstruction. In
EMNLP.

Deyu Zhou, Haiyang Xu, Xin-Yu Dai, and Yulan He.
2016. Unsupervised storyline extraction from news
articles. In IJCAI.


