




































Exploiting Common Characters in Chinese and Japanese to Learn Cross-Lingual Word Embeddings via Matrix Factorization


Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 113–121
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

113

Exploiting Common Characters in Chinese and Japanese to
Learn Cross-lingual Word Embeddings via Matrix Factorization

Jilei Wang ∗†
Tsinghua University, China
wangjileiruc@gmail.com

Shiying Luo †
Northeastern University, China

neulsy@hotmail.com

Weiyan Shi
University of California, Berkeley, USA

wyshi@berkeley.edu

Tao Dai
Tsinghua University, China

dait14@mails.tsinghua.edu.cn

Shu-Tao Xia ∗
Tsinghua University, China

xiast@sz.tsinghua.edu.cn

Abstract

Learning vector space representation
of words (i.e., word embeddings) has
recently attracted wide research in-
terests, and has been extended to
cross-lingual scenario. Currently most
cross-lingual word embedding learn-
ing models are based on sentence
alignment, which inevitably introduces
much noise. In this paper, we show
in Chinese and Japanese, the acquisi-
tion of semantic relation among words
can benefit from the large number of
common characters shared by both lan-
guages; inspired by this unique fea-
ture, we design a method named CJC
targeting to generate cross-lingual con-
text of words. We combine CJC with
GloVe based on matrix factorization,
and then propose an integrated model
named CJ-Glo. Taking two sentence-
aligned models and CJ-BOC (also ex-
ploits common characters but is based
on CBOW) as baseline algorithms, we
compare them with CJ-Glo on a series
of NLP tasks including cross-lingual
synonym, word analogy and sentence
alignment. The result indicates CJ-Glo
achieves the best performance among
these methods, and is more stable
in cross-lingual tasks; moreover, com-
pared with CJ-BOC, CJ-Glo is less sen-
sitive to the alteration of parameters.

∗ Corresponding authors.
† Contributed equally to the paper.

1 Introduction

Word representation is critical to various NLP
tasks, and the traditional one-hot representa-
tion, despite its simplicity, suffers from at least
two aspects: the vector dimensionality in-
creases with vocabulary size, leading to “curse
of dimensionality”; more importantly, it fails
to capture the semantic relation among words.

Due to the defects of one-hot representa-
tion, the majority of research interests now
have switched to distributed word representa-
tion (also known as “word embedding”), which
represents word as a real-valued vector. Rep-
resented as vectors, the semantics of words are
better reflected, as the relatedness of words
can be quantified using vector arithmetic.

To efficiently train word embeddings, a
range of models have been proposed, most of
them targeting to train monolingual word em-
bedding. Though word embedding is often
discussed under monolingual scenario, cross-
lingual embedding can serve as a useful tool
in several NLP tasks including machine trans-
lation (Wu et al., 2016), word sense disam-
biguation (Chen et al., 2014), and so on. This
is because cross-lingual word embeddings map
words from two languages into one vector
space, thereby making it possible to measure
the semantic relation among words from dif-
ferent languages. However, compared with the
bulk of works studying monolingual word em-
bedding, cross-lingual word embedding is still
at its initial stage, with no learning model be-
ing widely accepted.

In this paper, we present a method named
CJC (Chinese-Japanese Common Character)



114

aiming to extract cross-lingual context of
words from sentence aligned Chinese-Japanese
corpus. Given the large amount of common
characters shared by both languages and the
rich semantic connections thereof, we exploit
them to acquire potential word level align-
ment. The acquired cross-lingual contexts can
be flexibly integrated with various models; in
this paper, CJC is mainly integrated with a
matrix factorization model called Glove (Pen-
nington et al., 2014), and the integrated model
is thus called CJ-Glo.

To evaluate the performance of CJ-Glo, we
take 2 sentence aligned models respectively
based on CBOW(Mikolov et al., 2013a) and
GloVe, and CJ-BOC model (based on Com-
mon Character + CBOW) (Wang et al., 2016)
as contrast, and compare the trained word em-
beddings of these methods using three typical
NLP tasks, including cross-lingual synonym,
word analogy and sentence alignment. Ac-
cording to the experiment results, the acquired
word embeddings by using CJ-Glo have better
quality than those of the other models; more-
over, CJ-Glo performs more stably than its
competitors, and is less sensitive to parame-
ter alteration.

2 Related work

Word embedding was initiated by Hinton
(1986), which essentially encodes word using
a real-valued vector. With word embeddings,
the intrinsic relatedness among words can be
explicitly measured as the distances or angles
between word pairs. This favorable feature
of word embedding soon led to its popular-
ity in industry and academia in past decades.
Specifically, word embedding has found its ap-
plications in machine translation (Wu et al.,
2016; Lample et al., 2017), word sense disam-
biguation (Chen et al., 2014; Guo et al., 2014),
information retrieval (Vulić and Moens, 2015)
and so on.

To efficiently acquire high-quality word em-
beddings, vast research efforts have therefore
emerged. A representative framework to learn
word embeddings is Neural Network Language
Model (NNLM) proposed by Bengio et al.
(2003), which adopts back-propagation when
training word embeddings and parameters for
the model. Another typical approach is matrix

factorization, whose basic idea is to approxi-
mate original matrices with low-rank matrices
by leveraging statistic information. For exam-
ple, GloVe (Pennington et al., 2014) explic-
itly factorizes the co-occurrence matrix, train-
ing only non-zero elements instead of an entire
spare matrix.

Traditionally, word embedding was studied
under monolingual setting, and then naturally
extended to bilingual scenario. Compared
with monolingual word embeddings, bilingual
word embedding reveals the internal relation
among words of different languages; and such
capability makes bilingual word embeddings a
powerful tool to assist machine translation, or
even serves as a substitute for word mapping
matrix and dictionary in previous machine
translation methods. A range of works have
been proposed to learn bilingual word embed-
dings, such as (Mikolov et al., 2013b), which
attempts to map separately trained word em-
beddings into one vector space, and acquire
bilingual word embeddings. BilBOWA is a
model proposed in (Gouws et al., 2015), whose
most notable merit is the whole training pro-
cess does not require word alignment or dic-
tionary. word alignment or dictionary. (Shi
et al., 2015) is another work that utilizes ma-
trix factorization in word embeddings learn-
ing. Ruder et al. (2017) provides a detailed
survey, which enumerates the input format
and basic principles of various bilingual word
embedding learning methods.

When it comes to non-alphabet-based lan-
guage like Chinese and Japanese, an essen-
tial difference from alphabet-based languages
is that each character in a word contains abun-
dant information, and makes sense itself. In
addition to this, an underlying correlation be-
tween Chinese and Japanese is the large por-
tion of shared characters in both languages;
with the help of these characters, Chu et al.
(2014) extracted texts from Wikipedia web
pages of Chinese and Japanese version, based
on which they then constructed a Chinese-
Japanese parallel corpus. A natural conjecture
about the common characters is the seman-
tic similarity or even equivalence among them.
In light of this, we proposed CJ-BOC model
in our previous work (Wang et al., 2016) to
learn Chinese-Japanese bilingual word embed-



115

dings, which outperforms sentence-alignment
approaches in terms of embedding quality. To
our knowledge, our previous work is the first
attempt to learn Chinese-Japanese word em-
beddings using common Chinese characters.

3 Chinese-Japanese Common
Character

Historically, Chinese character has spread to
a group of countries in East Asia as a ma-
jor carrier of Chinese culture, thereby influ-
encing the writing systems in these countries.
Traditional Chinese, Simplified Chinese and
Japanese Kanji are now being used, all de-
veloping from Traditional Chinese; and given
the same root of them, these three writing sys-
tems actually share a large portion of common
characters: for a certain character in one of
them, we can find its counterparts in the other
two, with minor variation or even of the same
shape. Chu et al. (2012) proposed a Chinese
character table comparing traditional Chinese,
simplified Chinese and Japanese. As summa-
rized in Table 1, the glyphs of such common
characters can be 1) the same in all these three
writing systems; 2) consistent in two of them;
3) different in all these three.

And with regard to their semantics, simpli-
fied and traditional Chinese are only two writ-
ten forms of the same language, and therefore
common characters within them are seman-
tically equivalent. For Japanese Kanji, most
characters are semantically equivalent or rele-
vant to their counterparts in Chinese.

We in our previous work (Wang et al., 2016)
quantified such semantic relatedness from the
view of information theory using mutual in-
formation (MI) and conditional mutual infor-
mation (CMI). By repeating the experiments
in this paper, we acquired the results in Ta-
ble 2. All these 5 characters have multiple
meanings in both Chinese and Japanese, and
their respective meanings differ to some ex-
tent in both languages. Normally CMI should
be larger than MI, which indicates that in a
translation-sentence pair, if 2 words from each
sentence share a common character, they are
likely to form a translation word pair. The
results of shown in Table 2 are no exception,
providing theoretical root for our model which
will be proposed in section 4.

4 Model
4.1 Context of Word and CJC Method
Before delving into the learning models, we
should first clarify the concept of context.
In natural language processing, a widely
adopted semantic representation model is
Bag-of-Words (Zhang et al., 2010). The funda-
mental assumption of this model is: within a
given sentence or paragraph, the target word
is prone to have the most intimate semantic
relation with its closest context words. For-
mally define a sentence S with l words as an
ordered sequence: S = ⟨w0, w1, ..., wl⟩, and
context function Ctx(·) is often formulated as:

Ctx(wi, S) = {wk|i−K ≤ k ≤ i+K}. (1)

In cross-lingual scenario, besides two mono-
lingual corpora of both languages, a parallel
corpus is often required in most models, which
is aligned in either word-level (Guo et al.,
2016) or sentence-level. Some recent works
attempted to learn embeddings without using
parallel corpus, such as (Artetxe et al., 2017).

Now try to consider bilingual context of a
given target word in aligned parallel corpus.
Let ⟨Szh, Sja⟩ be a sentence pair, then define:

Ctx(wzh,i) = Ctx(wzh,i, Szh)
∪ Ctx(wzh,i, Sja).

(2)

As formulated above, the context of a tar-
get word is the union of its contexts in both
sentences. Therefore in word-aligned par-
allel corpus, let ⟨wzh,i, wja,j⟩ be a pair of
aligned words, and the cross-lingual context
Ctxw(wzh,i, Sja) is equal to Ctxw(wja,j , Sja),
since contexts in both languages are taken
into account in this definition. In sentence-
aligned parallel corpus, the cross-lingual con-
text Ctxs(wzh,i, Sja) is defined as the set of all
the words in the respective sentence.

In real applications, sentence alignment
data are usually easier to acquire. For exam-
ple, Chu et al. (2014) proposed an approach
to align Chinese-Japanese cross-lingual wiki
corpus, using the common characters between
both languages.

According to the analysis in Section 3, given
an aligned Chinese-Japanese sentence pair,
word alignment can be performed upon word
pairs that share common characters. Based on



116

Type Example of Characters with Unicode PercentageSC TC KJ Meaning
1 - AAA 人 (U+4EBA) 人 (U+4EBA) 人 (U+4EBA) People 56.55
2 - AAB 窗 (U+7A97) 窗 (U+7A97) 窓 (U+7A93) Window 4.63
3 - ABA 国 (U+56FD) 國 (U+570B) 国 (U+56FD) Country 3.45
4 - ABB 习 (U+4E60) 習 (U+7FD2) 習 (U+7FD2) Study 29.17
5 - ABC 图 (U+56FE) 圖 (U+5716) 図 (U+56F3) Picture 6.19

Table 1: Corresponding examples and percentages(%) of common characters in Simplified Chi-
nese (SC), Traditional Chinese (TC), and Japanese Kanji (KJ).

Table 2: Estimated MI and CMI of 5 Common
Characters.

MI CMI
天 0.3369 30.3057
地 0.5804 87.4515
人 0.8942 151.0069
中 0.7337 138.9676
学 0.4173 119.8921

this conclusion, using common characters, we
can now give a definition for context similar
to context in sentence-align corpus.

Define a character matching function CC(·)
that generates a set of word in which each word
has at least one common character with target
word wzh,i:

CC(wzh,i, Sja) = {wja|wja ∈ Sja,
c ∈ wzh,i, c ∈ wja}.

(3)

Thus parallel context Ctxc(wzh,i, Sja) can be
acquired via common character matching:

Ctxc(wzh,i, Sja) = {w|w ∈ Ctx(wja, Sja),
wja ∈ CC(wzh,i, Sja)}.

(4)
Hence, when multiple words in the correspond-
ing sentence have common characters with the
target word, all of them will be included in
Ctxc(wzh,i, Sja). However, such case rarely oc-
curs during our experiments.

For example, “天 气/不 错/一 起/去/散
步/吧” and “天気/が/良い/から/散歩/し
ま/しょう” are a parallel sentence-pair, mean-
ing “The weather is nice, let’s take a walk”.
There are two corresponding word pairs de-
tected by common characters: “天气-天気
(Weather)” and “散步-散歩 (Take a walk)”.
Two words in a pair share their respective con-
text during training.

We name this method as CJC (Chinese-
Japanese Common Character) which uses
CC(·) to determine context. Different from
our previous work (Wang et al., 2016) which
exploited common characters to facilitate only
CBOW, this CJC method is more of a gen-
eralized scheme that can be integrated with
various models including CBOW, Skip-Gram,
GloVe etc.

4.2 CBOW-like Models
CBOW was a model proposed by Mikolov et
al. in (Mikolov et al., 2013a), whose opti-
mization goal is maximizing a probabilistic
language model. In cross-lingual especially
Chinese-Japanese scenario, the objective func-
tion for training wzh,i is:

L(Szh) =
1
N

N∑
i=1

{
Pzh,i,zh

+ λ · Pzh,i,ja,c
+ µ · Pzh,i,ja,s

}
,

(5)

where Pzh,i,zh, Pzh,i,ja,c, andPzh,i,ja,s are soft-
max function of the target word wzh,i to its
corresponding monolingual context, sentence
aligned cross-lingual context, and CJC con-
text. Both λ and µ here are parameters of
the model. If λ = 0, this is a trial sen-
tence aligned CBOW model, otherwise it is
a CJC+CBOW model; the CJ-BOC model in
our previous work (Wang et al., 2016) used
similar approach, and would be used as a base-
line in our experiments.

4.3 GloVe-like Models
4.3.1 GloVe
GloVe model was originally proposed by Pen-
nington et al. (2014). As the name implies,
GloVe utilizes the global information of the
corpus for vector training. GloVe and CBOW,



117

as commonly adopted learning models, how-
ever differ a lot in terms of mathematical mod-
els, as they are respectively based on matrix
factorization and neural network. The process
of GloVe is as follows:

First, construct a word-word cooccurence
matrix M = (mij)n×n , where n is the size
of the corpus, and mij represents the number
of occurrence of wj in the context of wi in all
the sentences S.

The learning problem of GloVe can then be
transformed into the optimization of function
F (·), such that for any word embeddings xi, xj
and probe word embedding x̃k, the objective
function is defined below:

L =
n∑

i,j=1

f(mij)(x
T
i x̃j+bi+b̃j−logmij)2, (6)

f(m) =

{
( mmmax )

α if m < mmax
1 otherwise. (7)

In this function, both bi and bj are bias, and f
is a weighing function aiming to mitigate the
impact of dataset size on training results. In
GloVe, mmax is set to 100 and α to 34 .

4.3.2 Cross-lingual GloVe and CJ-Glo
To fit GloVe in cross-lingual scenario, one
should first expand the word-word co-
occurrence matrix. Suppose two languages re-
spectively contain n and t words, the new ma-
trix would have a size of If wi and wj belong
to the same language, mij can be computed
using exactly the same way as in GloVe; oth-
erwise, suppose (Szh, Sja) is a pair of parallel
sentences, wi ∈ Szh, wj ∈ Sja, and we have:

mij =
∑

(Szh,Sja)

(λ · Cij,c + µ · Cij,s), (8)

Cij,c = Cnt(wj , Ctxc(wi, Sja)),
Cij,s = Cnt(wj , Ctxs(wi, Sja)),

(9)

Cnt(·) counts the frequency of wj in certain
context of wi, either sentence aligned context
or CJC context.

Once the cross-lingual word-word co-
occurence matrix is obtained, the following op-
timization unfolds similarly with the monolin-
gual GloVe model, using the objective function
(6) and weighting function (7) to train.

Similar to Cross-lingual CBOW model, if
the CJC learning rate λ = 0 in equation

Figure 1: An example of CJ-Glo model, where
the window size is 7 and the common character
is “樱 (桜)”.

(8), this is a sentence aligned cross-lingual
GloVe model. Otherwise, it is a CJC-enhanced
model, and is thus called CJ-Glo.

Figure 1 demonstrates the operational prin-
ciple of CJ-Glo: the square in this figure is
a cross-lingual word co-occurrence matrix, in
which the green square is a Chinese mono-
lingual co-occurrence sub-matrix, and the or-
ange square is for Japanese. The blue sec-
tions are cross-lingual sub-matrices and ele-
ments in them are calculated using equation
(8). When two parallel sentences each con-
tain a word sharing common characters, each
word would be taken as a co-occurrence in the
context of the other. Every point crossed by
dotted lines and dotted rectangles represents
an element to increment when processing the
sentence pair.

5 Experiments and Analysis
5.1 Evaluation Methods
To evaluate the quality of cross-lingual word
embeddings obtained from various models, we
conducted three groups of experiments: 1) the
straightforward cross-lingual synonym com-
parison; 2) cross-lingual word analogy; 3) sen-
tence alignment.

Cross-lingual synonym comparison.
In monolingual scenario, the word embed-

dings of a pair of synonyms should have a high
cosine similarity. This property is also appli-
cable in cross-lingual word embeddings, i.e.,
the cosine similarity between a word embed-
ding and its translated counterpart should also



118

be high. In real applications, the correspon-
dence between words in source language and
words in target language can be one-to-one,
one-to-many, or vice versa. To effectively elim-
inate ambiguity, we picked 200 one-to-one cor-
responding word pairs ⟨wzh, wja⟩ at random,
then for each word pair, calculated the cosine
similarity between wzh and wja, denoted as d,
and computed the rank of d among the cosine
similarities from wzh to every Japanese word
in corpus Vja. Use the rank to calculate its
relative rate among all words:

rate = (1− rank − 1
total_word_num)×100%. (10)

Conducted the same operation for wja and all
words in corpus Vzh. Calculate the average
rate for all the 200 word pairs, and acquire
the average rate of wzh → wja and wja →
wzh respectively. Ambiguity is eliminated in
all these word pairs, so a large rate is therefore
favored.

Cross-lingual word analogy.
Word analogy is probably the most widely

adopted task to evaluate the performance
of word embeddings, because it depicts the
connection between trained vector space and
word semantics. Both CBOW(Mikolov et al.,
2013a) and GloVe(Pennington et al., 2014)
used a dataset with 19,544 queries for eval-
uation.

Given several related words from different
languages, cross-lingual analogical reasoning
works as follows: y=v(はは)-v(ちち)+v(男
孩), we hope that the relatedness between
Japanese words “はは (mother)” and “ちち
(father)” could help us find the Chinese word
“女孩 (girl)” and Japanese “女の子 (girl)”
through Chinese word “男孩 (boy)”.

More formally, the cross-lingual analogy
task was undertaken as follows:
1. Input a quadruple of word embeddings ⟨w1 :
w2 :: w3 : w4⟩, where each word could be either
Chinese or Japanese;
2. Compute the target vector u = w2−w1+w3,
acquire the corresponding rank and rate as in
cross-lingual synonym comparison for u → w4;
3. Based on the ratio of Chinese word count
to Japanese word count in the quadruple ⟨w1 :
w2 :: w3 : w4⟩, the word analogy task is divided
into 5 subtasks, whose ratio are (0 : 4), (1 : 3),
(2 : 2), (3 : 1) and (4 : 0), and their respective

query amount is 420, 1680, 2520, 1680, and
420 in our experiment;
4. Calculate the average rate on every subtask.

Also, the average rate here is expected to be
as large as possible.

Sentence alignment.
The above experiments respectively eval-

uated the direct similarity and cross-lingual
feature of word embeddings. And now we
consider a more complicated task: sentence
alignment. In the dataset from (Chu et al.,
2014), other than training data, a manual test
dataset was also attached, which are 198 sen-
tence pairs. Using this dataset, we conduct
this experiment as follows:
1. For a Chinese sentence Szh,i, calculate its
average vector Uzh,i and all Uja of all sentences
Sja, and compute the cosine similarity.
2. Sort all the cosine similarities in step 2, and
acquire the rank of the average vector Uja,i of
Sja,i (the parallel sentence of Szh,i).
3. Transform rank into rate using formula 10,
where total number is 198.
4. Compute the average rate Szh → Sja;
5. Follow the same steps above to generate
Sja → Szh.

Compared with the previous experiments,
which evaluate only the relation between indi-
vidual word embeddings, sentence alignment
is a comprehensive task using word embed-
ding, and is a critical indicator for the overall
quality of the trained word embeddings.

5.2 Dataset and Training Details
As mentioned previously, (Chu et al., 2014)
generated a parallel corpus including Chinese-
Japanese sentence pairs from Wikipedia;
train.ja and train.zh in this dataset were used
throughout our empirical study, both contain-
ing 126,811 lines of text. Concretely, ev-
ery single line in these two files is a com-
plete sentence, which is parallel to its coun-
terpart in the other file. As the preprocess-
ing for datasets, both files were segmented us-
ing MeCab1 and Jieba2 for Japanese and Chi-
nese, respectively. During the preprocessing,
we assured the segmentation on Chinese and

1http://taku910.github.io/mecab, accessed date:
December 20, 2017.

2https://github.com/fxsjy/jieba, commit number:
cb0de2973b2fafaa67a0245a14206d8be70db515.



119

Table 3: Parameters of CJC and sentence
learning rates in each models.

Model λ µ
SenBow 0 0.2
CJ-BOC 0.4 0.2
SenGlo 0 0.2
CJ-Glo 0.4 0.2

Table 4: Cross-lingual synonym comparison
results on 200 one-to-one word pairs, the av-
erage rates(%) of each models.

Model wzh → wja wja → wzh
SenBow 83.97 83.76
CJ-BOC 96.75 97.61
SenGlo 91.17 90.05
CJ-Glo 97.97 98.80

Japanese were approximately grained, by tun-
ing parameters.

Four models in total are put into comparison
in our experiment:

1. SenBow model is the bilingual CBOW
model applying sentence-aligned method;

2. CJ-BOC model from (Wang et al., 2016),
considered as a CJC+CBOW model;

3. SenGlo model applies sentence-aligned
method to GloVe model;

4. CJ-Glo model is our CJC method en-
hanced GloVe model.

The parameters of CJC learning rate λ and
sentence learning rate µ are showed in Table 3.
Both SenGlo and CJ-Glo have a mmax of 100,
and an α of 34 . The thread count is 16 in the
implementations of all these four models, the
output vector dimensionality is 100, and the
training process is iterated 15 times. We set
the parameters to the above values, since these
models achieved the optimal performances un-
der such settings in our evaluation. All mod-
els are implemented using C language, and the
code can be found on GitHub3.

3https://github.com/jileiwang/CJC, commit num-
ber: a10592d200bc15f7b53d81a8f895e7de9ef8676d.

Figure 2: Cross-lingual word analogy experi-
ment result. X-axis is the number ratio of Chi-
nese words and Japanese words in the analogy
query (w1 : w2 :: w3 : w4).

5.3 Results
The result of cross-lingual synonym compar-
ison is shown in Table 4, from which we
can see the integration of Common Character
leads to obvious performance improvement for
both CBOW-like and GloVe-like models, com-
pared with sentence-aligned models, and CJ-
Glo achieve the best result.

Figure 2 summarizes the results of the cross-
lingual word analogy task, whose X-axis rep-
resents the ratio of Chinese word count to
Japanese word count. In the figure, the
leftmost point represents the result of pure
Japanese word analogy, and the rightmost is
the pure Chinese word analogy. We can see
that all 4 models achieve fair performances in
pure Chinese/Japanese word analogy. How-
ever, when it comes to the cross-lingual word
analogy, CJ- models outperform Sen- models,
and GloVe-like models generally beat CBOW-
like ones. Another noticeable fact is that CJ-
Glo performs approximately good under all 5
ratios, showing basically no difference between
cross-lingual and monolingual word analogy.

We display the sentence alignment results
in Table 5. Similarly, we still find CJ- models
outperform Sen-, and GloVe-like models beat
CBOW-like ones. Again, CJ-Glo has the best
performance.

According to the above experiments, we can
see compared with typical sentence-aligned
methods, Common Character enhanced mod-



120

Table 5: Sentence alignment results on 198
parallel sentence pairs, the average rates(%)
of each models.

Model Szh → Sja Sja → Szh
SenBow 79.14 74.63
CJ-BOC 86.39 83.14
SenGlo 90.33 84.90
CJ-Glo 91.57 86.00

els are superior in learning Chinese-Japanese
cross-lingual word embeddings, as it achieves
obvious performance boost in various tasks.
Moreover, CJ-Glo performs better than CJ-
BOC, and is non-sensitive in cross-lingual
tasks.

5.4 Model Analysis: CJC Learning
Rate

CJC learning rate here refers to the multiply-
ing factor of CJC context Ctxc(·), which is
λ in CJ-BOC and CJ-Glo. It worths discus-
sion that how would CJC learning rate affects
the performance of our proposed models. To
explore this issue, we conduct a simple exper-
iment: fixing the other parameters as set in
section 5.2, we only change CJC learning rate,
and apply the acquired word embeddings to
synonym wzh → wja tasks. The results are
displayed in Figure 3 , in which we can find
as λ increases in CJ-BOC, the accuracy de-
clines after an increase, showing a obvious lo-
cal optimal. While in CJ-Glo, the accuracy
keeps improving with the increase of λ. Note
that both parameters should be less than 1,
because otherwise the impact of cross-lingual
context would dominate the learning process,
obviously resulting in overfit. CJ-Glo is more
stable during the change of CJC learning rate,
this interesting difference between both mod-
els is related to the their underlying learning
mechanisms.

6 Conclusion and Future Work
In this paper, we quantified the semantic con-
nection among common characters shared by
Chinese and Japanese, and utilized it as the
theoretical root to propose our cross-lingual
context extracting method CJC. CJC makes
use of common characters of both languages to
assist the acquisition of parallel contexts. The

Figure 3: Accuracy of CJ-BOC and CJ-Glo
Models on cross-lingual synonym wzh → wja
with different CC learning rate.

effectiveness of CJC enhanced matrix factor-
ization model CJ-Glo was verified via a series
of tasks including cross-lingual synonym, word
analogy and sentence alignment. As the ex-
periment result shows, models like CBOW and
GloVe achieved notable performance gain af-
ter integrated with CJC. Furthermore, CJ-Glo
performed the best among all evaluated state-
of-the-art methods, and showed its stability
on cross-lingual tasks and non-sensitiveness of
training parameter changing.

Below are several directions we may work
on in the future: 1) The idea of training
character and word embeddings jointly (Chen
et al., 2015) is applicable to Chinese-Japanese
word embedding training. Meanwhile, we can
also align common characters and train cross-
lingual character embeddings to further im-
prove the quality of trained word embeddings.
2) A recent work (Lai et al., 2016) indicates
that the performances of a model may vary
given different tasks. Therefore, we shall study
the performance fluctuation of CJ-Glo with
more tasks including machine translation.

Acknowledgments

This work is supported by the National Nat-
ural Science Foundation of China under grant
Nos. 61771273, 61371078.

The authors would like to thank Xinyu
Zhou, Minghao Li, Yanning Li, and Qi Tang
for the insightful discussions and suggestions
that helped us improve the manuscript.



121

References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.

2017. Learning bilingual word embeddings with
(almost) no bilingual data. In Proceedings of
the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 451–462.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent,
and Christian Jauvin. 2003. A neural probabilis-
tic language model. Journal of machine learning
research, 3(Feb):1137–1155.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.
2014. A unified model for word sense represen-
tation and disambiguation. In Proceedings of the
2014 conference on empirical methods in natu-
ral language processing (EMNLP), pages 1025–
1035.

Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong
Sun, and Huan-Bo Luan. 2015. Joint learning
of character and word embeddings. In Proceed-
ings of the 24th International Joint Conference
On Artificial Intelligence (IJCAI), pages 1236–
1242.

Chenhui Chu, Toshiaki Nakazawa, and Sadao
Kurohashi. 2012. Chinese characters mapping
table of japanese, traditional chinese and simpli-
fied chinese. In Proceedings of the 8th Confer-
ence on International Language Resources and
Evaluation Conference (LREC), pages 2149–
2152. Citeseer.

Chenhui Chu, Toshiaki Nakazawa, and Sadao
Kurohashi. 2014. Constructing a chi-
nese�japanese parallel corpus from wikipedia.
In Proceedings of the 9th Conference on Inter-
national Language Resources and Evaluation
Conference (LREC), pages 642–647.

Stephan Gouws, Yoshua Bengio, and Greg Cor-
rado. 2015. Bilbowa: Fast bilingual distributed
representations without word alignments. In
Proceedings of the 32nd International Confer-
ence on Machine Learning (ICML), pages 748–
756.

Jiang Guo, Wanxiang Che, Haifeng Wang, and
Ting Liu. 2014. Learning sense-specific word
embeddings by exploiting bilingual resources. In
Proceedings of the 25th International Conference
on Computational Linguistics: Technical Papers
(COLING), pages 497–507.

Jiang Guo, Wanxiang Che, David Yarowsky,
Haifeng Wang, and Ting Liu. 2016. A repre-
sentation learning framework for multi-source
transfer parsing. In AAAI, pages 2734–2740.

Geoffrey E Hinton. 1986. Learning distributed rep-
resentations of concepts. In Proceedings of the
eighth annual conference of the cognitive science
society, volume 1, page 12. Amherst, MA.

Siwei Lai, Kang Liu, Shizhu He, and Jun Zhao.
2016. How to generate a good word embedding.
IEEE Intelligent Systems, 31(6):5–14.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora
only. arXiv preprint arXiv:1711.00043.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.

Jeffrey Pennington, Richard Socher, and Christo-
pher Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
conference on empirical methods in natural lan-
guage processing (EMNLP), pages 1532–1543.

Sebastian Ruder, Ivan Vulić, and Anders Søgaard.
2017. A survey of cross-lingual word embedding
models. arXiv preprint arXiv:1706.04902.

Tianze Shi, Zhiyuan Liu, Yang Liu, and Maosong
Sun. 2015. Learning cross-lingual word embed-
dings via matrix co-factorization. In Proceedings
of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language
Processing (Volume 2: Short Papers), volume 2,
pages 567–572.

Ivan Vulić and Marie-Francine Moens. 2015.
Monolingual and cross-lingual information re-
trieval models based on (bilingual) word embed-
dings. In Proceedings of the 38th International
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 363–372.
ACM.

Jilei Wang, Shiying Luo, Yanning Li, and Shu-Tao
Xia. 2016. Learning chinese-japanese bilingual
word embedding by using common characters.
In International Conference on Knowledge Sci-
ence, Engineering and Management, pages 82–
93. Springer.

Yonghui Wu, Mike Schuster, Zhifeng Chen,
Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao,
Klaus Macherey, et al. 2016. Google’s neural
machine translation system: Bridging the gap
between human and machine translation. arXiv
preprint arXiv:1609.08144.

Yin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010.
Understanding bag-of-words model: a statistical
framework. International Journal of Machine
Learning and Cybernetics, 1(1-4):43–52.


