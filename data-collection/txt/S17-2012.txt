



















































CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 109–114,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism
Detection Methods for Semantic Textual Similarity

Jérémy Ferrero
Compilatio

276 rue du Mont Blanc
74540 Saint-Félix, France

LIG-GETALP
Univ. Grenoble Alpes, France

jeremy.ferrero@imag.fr

Laurent Besacier
LIG-GETALP

Univ. Grenoble Alpes, France
laurent.besacier@imag.fr

Didier Schwab
LIG-GETALP

Univ. Grenoble Alpes, France
didier.schwab@imag.fr

Frédéric Agnès
Compilatio

276 rue du Mont Blanc
74540 Saint-Félix, France

frederic@compilatio.net

Abstract

We present our submitted systems for Se-
mantic Textual Similarity (STS) Track 4 at
SemEval-2017. Given a pair of Spanish-
English sentences, each system must esti-
mate their semantic similarity by a score
between 0 and 5. In our submission,
we use syntax-based, dictionary-based,
context-based, and MT-based methods.
We also combine these methods in unsu-
pervised and supervised way. Our best run
ranked 1st on track 4a with a correlation
of 83.02% with human annotations.

1 Introduction

CompiLIG is a collaboration between Compilatio1

- a company particularly interested in cross-
language plagiarism detection - and LIG research
group on natural language processing (GETALP).
Cross-language semantic textual similarity detec-
tion is an important step for cross-language plagia-
rism detection, and evaluation campaigns in this
new domain are rare. For the first time, SemEval
STS task (Agirre et al., 2016) was extended with
a Spanish-English cross-lingual sub-task in 2016.
This year, sub-task was renewed under track 4 (di-
vided in two sub-corpora: track 4a and track 4b).

Given a sentence in Spanish and a sentence in
English, the objective is to compute their seman-
tic textual similarity according to a score from 0

1www.compilatio.net

to 5, where 0 means no similarity and 5 means
full semantic similarity. The evaluation metric is
a Pearson correlation coefficient between the sub-
mitted scores and the gold standard scores from
human annotators. Last year, among 26 submis-
sions from 10 teams, the method that achieved the
best performance (Brychcin and Svoboda, 2016)
was a supervised system (SVM regression with
RBF kernel) based on word alignment algorithm
presented in Sultan et al. (2015).

Our submission in 2017 is based on cross-
language plagiarism detection methods combined
with the best performing STS detection method
published in 2016. CompiLIG team participated to
SemEval STS for the first time in 2017. The meth-
ods proposed are syntax-based, dictionary-based,
context-based, and MT-based. They show addi-
tive value when combined. The submitted runs
consist in (1) our best single unsupervised ap-
proach (2) an unsupervised combination of best
approaches (3) a fine-tuned combination of best
approaches. The best of our three runs ranked 1st

with a correlation of 83.02% with human annota-
tions on track 4a among all submitted systems (51
submissions from 20 teams for this track). Cor-
relation results of all participants (including ours)
on track 4b were much lower and we try to explain
why (and question the validity of track 4b) in the
last part of this paper.

109



2 Cross-Language Textual Similarity
Detection Methods

2.1 Cross-Language Character N-Gram
(CL-CnG)

CL-CnG aims to measure the syntactical similar-
ity between two texts. It is based on Mcnamee
and Mayfield (2004) work used in information re-
trieval. It compares two texts under their n-grams
vectors representation. The main advantage of this
kind of method is that it does not require any trans-
lation between source and target text.

After some tests on previous year’s dataset to
find the best n, we decide to use the Potthast et al.
(2011)’s CL-C3G implementation. Let Sx and Sy
two sentences in two different languages. First,
the alphabet of these sentences is normalized to
the ensemble

∑
= {a − z, 0 − 9, }, so only

spaces and alphanumeric characters are kept. Any
other diacritic or symbol is deleted and the whole
text is lower-cased. The texts are then segmented
into 3-grams (sequences of 3 contiguous charac-
ters) and transformed into tf.idf vectors of charac-
ter 3-grams. We directly build our idf model on
the evaluation data. We use a double normaliza-
tion K (with K = 0.5) as tf (Manning et al., 2008)
and a inverse document frequency smooth as idf.
Finally, a cosine similarity is computed between
the vectors of source and target sentences.

2.2 Cross-Language Conceptual
Thesaurus-based Similarity (CL-CTS)

CL-CTS (Gupta et al., 2012; Pataki, 2012) aims to
measure the semantic similarity between two vec-
tors of concepts. The model consists in represent-
ing texts as bag-of-words (or concepts) to compare
them. The method also does not require explicit
translation since the matching is performed using
internal connections in the used “ontology”.

Let S a sentence of length n, the n words of the
sentence are represented by wi as:

S = {w1, w2, w3, ..., wn} (1)
Sx and Sy are two sentences in two different

languages. A bag-of-words S′ from each sen-
tence S is built, by filtering stop words and by
using a function that returns for a given word all
its possible translations. These translations are
jointly given by a linked lexical resource, DBNary
(Sérasset, 2015), and by cross-lingual word em-
beddings. More precisely, we use the top 10 clos-
est words in the embeddings model and all the

available translations from DBNary to build the
bag-of-words of a word. We use the MultiVec
(Berard et al., 2016) toolkit for computing and
managing word embeddings. The corpora used to
build the embeddings are Europarl and Wikipedia
sub-corpus, part of the dataset of Ferrero et al.
(2016)2. For training our embeddings, we use
CBOW model with a vector size of 100, a win-
dow size of 5, a negative sampling parameter of 5,
and an alpha of 0.02.

So, the sets of words S′x and S′y are the con-
ceptual representations in the same language of Sx
and Sy respectively. To calculate the similarity be-
tween Sx and Sy, we use a syntactically and fre-
quentially weighted augmentation of the Jaccard
distance, defined as:

J(Sx, Sy) =
Ω(S′x) + Ω(S′y)
Ω(Sx) + Ω(Sy)

(2)

where Sx and Sy are the input sentences (also
represented as sets of words), and Ω is the sum of
the weights of the words of a set, defined as:

Ω(S) =
n∑

i=1 , wi∈S
ϕ(wi) (3)

where wi is the ith word of the bag S, and ϕ is
the weight of word in the Jaccard distance:

ϕ(w) = pos weight(w)1−α . idf(w)α (4)

where pos weight is the function which gives
the weight for each universal part-of-speech tag
of a word, idf is the function which gives the
inverse document frequency of a word, and . is
the scalar product. Equation (4) is a way to
syntactically (pos weight) and frequentially (idf )
weight the contribution of a word to the Jaccard
distance (both contributions being controlled with
the α parameter). We assume that for one word,
we have its part-of-speech within its original sen-
tence, and its inverse document frequency. We use
TreeTagger (Schmid, 1994) for POS tagging, and
we normalize the tags with Universal Tagset of
Petrov et al. (2012). Then, we assign a weight
for each of the 12 universal POS tags. The 12
POS weights and the value α are optimized with
Condor (Berghen and Bersini, 2005) in the same
way as in Ferrero et al. (2017). Condor applies
a Newton's method with a trust region algorithm

2https://github.com/FerreroJeremy/
Cross-Language-Dataset

110



to determinate the weights that optimize a de-
sired output score. No re-tuning of these hyper-
parameters for SemEval task was performed.

2.3 Cross-Language Word Embedding-based
Similarity

CL-WES (Ferrero et al., 2017) consists in a cosine
similarity on distributed representations of sen-
tences, which are obtained by the weighted sum
of each word vector in a sentence. As in previ-
ous section, each word vector is syntactically and
frequentially weighted.

If Sx and Sy are two sentences in two differ-
ent languages, then CL-WES builds their (bilin-
gual) common representation vectors Vx and Vy
and applies a cosine similarity between them. A
distributed representation V of a sentence S is cal-
culated as follows:

V =
n∑

i=1 , wi∈S
(vector(wi) . ϕ(wi)) (5)

where wi is the ith word of the sentence S,
vector is the function which gives the word em-
bedding vector of a word, ϕ is the same that in
formula (4), and . is the scalar product. We make
this method publicly available through MultiVec3

(Berard et al., 2016) toolkit.

2.4 Translation + Monolingual Word
Alignment (T+WA)

The last method used is a two-step process. First,
we translate the Spanish sentence into English
with Google Translate (i.e. we are bringing the
two sentences in the same language). Then, we
align both utterances. We reuse the monolingual
aligner4 of Sultan et al. (2015) with the improve-
ment of Brychcin and Svoboda (2016), who won
the cross-lingual sub-task in 2016 (Agirre et al.,
2016). Because this improvement has not been re-
leased by the initial authors, we propose to share
our re-implementation on GitHub5.

If Sx and Sy are two sentences in the same lan-
guage, then we try to measure their similarity with
the following formula:

J(Sx, Sy) =
ω(Ax) + ω(Ay)
ω(Sx) + ω(Sy)

(6)

3https://github.com/eske/multivec
4https://github.com/ma-sultan/

monolingual-word-aligner
5https://github.com/FerreroJeremy/

monolingual-word-aligner

where Sx and Sy are the input sentences (repre-
sented as sets of words), Ax and Ay are the sets of
aligned words for Sx and Sy respectively, and ω is
a frequency weight of a set of words, defined as:

ω(A) =
n∑

i=1 , wi∈A
idf(wi) (7)

where idf is the function which gives the in-
verse document frequency of a word.

2.5 System Combination

These methods are syntax-, dictionary-, context-
and MT- based, and are thus potentially comple-
mentary. That is why we also combine them in
unsupervised and supervised fashion. Our unsu-
pervised fusion is an average of the outputs of each
method. For supervised fusion, we recast fusion as
a regression problem and we experiment all avail-
able methods in Weka 3.8.0 (Hall et al., 2009).

3 Results on SemEval-2016 Dataset

Table 1 reports the results of the proposed systems
on SemEval-2016 STS cross-lingual evaluation
dataset. The dataset, the annotation and the eval-
uation systems were presented in the SemEval-
2016 STS task description paper (Agirre et al.,
2016), so we do not re-detail them here. The
lines in bold represent the methods that obtain
the best mean score in each category of system
(best method alone, unsupervised and supervised
fusion). The scores for the supervised systems are
obtained with 10-folds cross-validation.

4 Runs Submitted to SemEval-2017

First, it is important to mention that our outputs
are linearly re-scaled to a real-valued space [0 ; 5].

Run 1: Best Method Alone. Our first run is
only based on the best method alone during our
tests (see Table 1), i.e. Cross-Language Concep-
tual Thesaurus-based Similarity (CL-CTS) model,
as described in section 2.2.

Run 2: Fusion by Average. Our second run is
a fusion by average on three methods: CL-C3G,
CL-CTS and T+WA, all described in section 2.

Run 3: M5′Model Tree. Unlike the two prece-
dent runs, the third run is a supervised system.
We have selected the system that obtained the best
score during our tests on SemEval-2016 evalua-
tion dataset (see Table 1), which is the M5′ model
tree (Wang and Witten, 1997) (called M5P in

111



Methods News Multi Mean
Unsupervised systems

CL-C3G (1) 0.7522 0.6550 0.7042
CL-CTS (2) 0.9072 0.8283 0.8682
CL-WES (3) 0.7028 0.6312 0.6674
T+WA (4) 0.9060 0.8144 0.8607
Average (1-2-3-4) 0.8589 0.7824 0.8211
Average (1-2-4) 0.9051 0.8347 0.8703
Average (2-3-4) 0.8923 0.8239 0.8585
Average (2-4) 0.9082 0.8299 0.8695

Supervised systems (fine-tuned fusion)
GaussianProcesses 0.8712 0.7884 0.8303
LinearRegression 0.9099 0.8414 0.8761
MultilayerPerceptron 0.8966 0.7999 0.8488
SimpleLinearRegression 0.9048 0.8144 0.8601
SMOreg 0.9071 0.8375 0.8727
Ibk 0.8396 0.7330 0.7869
Kstar 0.8545 0.8173 0.8361
LWL 0.8572 0.7589 0.8086
DecisionTable 0.9139 0.8047 0.8599
M5Rules 0.9146 0.8406 0.8780
DecisionStump 0.8329 0.7380 0.7860
M5P 0.9154 0.8442 0.8802
RandomForest 0.9109 0.8418 0.8768
RandomTree 0.8364 0.7262 0.7819
REPTree 0.8972 0.7992 0.8488

Table 1: Results of the methods on SemEval-2016
STS cross-lingual evaluation dataset.

Weka 3.8.0 (Hall et al., 2009)). Model trees have
a conventional decision tree structure but use lin-
ear regression functions at the leaves instead of
discrete class labels. The first implementation of
model trees, M5, was proposed by Quinlan (1992)
and the approach was refined and improved in a
system called M5′ by Wang and Witten (1997). To
learn the model, we use all the methods described
in section 2 as features.

5 Results of the 2017 evaluation and
Discussion

Dataset, annotation and evaluation systems are
presented in SemEval-2017 STS task description
paper (Cer et al., 2017). We can see in Table 2
that our systems work well on SNLI6 (Bowman
et al., 2015) (track 4a), on which we ranked 1st

with more than 83% of correlation with human an-
notations. Conversely, correlations on the WMT
corpus (track 4b) are strangely low. This differ-
ence is notable on the scores of all participating
teams (Cer et al., 2017)7. This might be explained
by the fact that WMT was annotated by only one

6http://nlp.stanford.edu/projects/
snli/

7The best score for this track is 34%, while for the other
tracks it is around 85%.

annotator, while the SNLI corpus was annotated
by many.

Methods SNLI (4a) WMT (4b) Mean
CL-CTS 0.7684 0.1464 0.4574
Average 0.7910 0.1494 0.4702
M5P 0.8302 0.1550 0.4926

Table 2: Official results of our submitted systems
on SemEval-2017 STS track 4 evaluation dataset.

Methods SNLI (4a) WMT (4b) Mean
Our Annotations

CL-CTS 0.7981 0.5248 0.6614
Average 0.8105 0.4031 0.6068
M5P 0.8622 0.5374 0.6998

SemEval Gold Standard
CL-CTS 0.8123 0.1739 0.4931
Average 0.8277 0.2209 0.5243
M5P 0.8536 0.1706 0.5121

Table 3: Results of our submitted systems scored
on our 120 annotated pairs and on the same 120
SemEval annotated pairs.

To investigate deeper on this issue, we manu-
ally annotated 60 random pairs of each sub-corpus
(120 annotated pairs among 500). These annota-
tions provide a second annotator reference. We
can see in Table 3 that, on SNLI corpus (4a), our
methods behave the same way for both annotations
(a difference of about 1.3%). However, the dif-
ference in correlation is huge between our anno-
tations and SemEval gold standard on the WMT
corpus (4b): 30% on average. The Pearson corre-
lation between our annotated pairs and the related
gold standard is 85.76% for the SNLI corpus and
29.16% for the WMT corpus. These results ques-
tion the validity of the WMT corpus (4b) for se-
mantic textual similarity detection.

6 Conclusion

We described our submission to SemEval-2017
Semantic Textual Similarity task on track 4 (Sp-
En cross-lingual sub-task). Our best results were
achieved by a M5′ model tree combination of var-
ious textual similarity detection techniques. This
approach worked well on the SNLI corpus (4a -
finishes 1st with more than 83% of correlation with
human annotations), which corresponds to a real
cross-language plagiarism detection scenario. We
also questioned WMT corpus (4b) validity provid-
ing our own manual annotations and showing low
correlations with those of SemEval.

112



References
Eneko Agirre, Carmen Banea, Daniel Cer, Mona

Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, and
Janyce Wiebe. 2016. SemEval-2016 Task 1: Se-
mantic Textual Similarity, Monolingual and Cross-
Lingual Evaluation. In Proceedings of the 10th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2016). Association for Computational Lin-
guistics, San Diego, CA, USA, pages 497–511.
http://www.aclweb.org/anthology/S16-1081.

Alexandre Berard, Christophe Servan, Olivier Pietquin,
and Laurent Besacier. 2016. MultiVec: a Multilin-
gual and Multilevel Representation Learning Toolkit
for NLP. In Proceedings of the Tenth International
Conference on Language Resources and Evaluation
(LREC’16). European Language Resources Asso-
ciation (ELRA), Portoroz, Slovenia, pages 4188–
4192.

Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
Powell’s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics 181:157–
175. https://doi.org/10.1016/j.cam.2004.11.029.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A Large
Annotated Corpus for Learning Natural Language
Inference. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Lisbon, Portugal, pages 632–642.
http://aclweb.org/anthology/D/D15/D15-1075.pdf.

Tomas Brychcin and Lukas Svoboda. 2016. UWB
at SemEval-2016 Task 1: Semantic textual sim-
ilarity using lexical, syntactic, and semantic in-
formation. In Proceedings of the 10th Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2016). San Diego, CA, USA, pages 588–
594. https://www.aclweb.org/anthology/S/S16/S16-
1089.pdf.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017). Association for Computa-
tional Linguistics, Vancouver, Canada, pages 1–14.
http://www.aclweb.org/anthology/S17-2001.

Jérémy Ferrero, Frédéric Agnès, Laurent Besacier, and
Didier Schwab. 2016. A Multilingual, Multi-style
and Multi-granularity Dataset for Cross-language
Textual Similarity Detection. In Proceedings of the
Tenth International Conference on Language Re-
sources and Evaluation (LREC’16). European Lan-
guage Resources Association (ELRA), Portoroz,
Slovenia, pages 4162–4169. ISLRN: 723-785-513-
738-2. http://islrn.org/resources/723-785-513-738-
2.

Jérémy Ferrero, Laurent Besacier, Didier Schwab, and
Frédéric Agnès. 2017. Using Word Embedding for
Cross-Language Plagiarism Detection. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
(EACL 2017). Association for Computational Lin-
guistics, Valencia, Spain, volume 2, pages 415–421.
http://aclweb.org/anthology/E/E17/E17-2066.pdf.

Parth Gupta, Alberto Barrón-Cedeño, and Paolo Rosso.
2012. Cross-language High Similarity Search us-
ing a Conceptual Thesaurus. In Information Access
Evaluation. Multilinguality, Multimodality, and Vi-
sual Analytics. Springer Berlin Heidelberg, Rome,
Italy, pages 67–75. https://doi.org/10.1007/978-3-
642-33247-0 8.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. In SIGKDD Explorations. volume 11, pages
10–18. https://doi.org/10.1145/1656274.1656278.

Christopher D. Manning, Prabhakar Raghavan,
and Hinrich Schütze. 2008. Introduction to
Information Retrieval, Cambridge Univer-
sity Press, New York, chapter 6 - ”Scoring,
term weighting, and the vector space model”,
pages 109–133. ISBN: 9780511809071.
https://doi.org/10.1017/CBO9780511809071.007.

Paul Mcnamee and James Mayfield. 2004. Charac-
ter N-Gram Tokenization for European Language
Text Retrieval. In Information Retrieval Proceed-
ings. Kluwer Academic Publishers, volume 7, pages
73–97.

Màté Pataki. 2012. A New Approach for Searching
Translated Plagiarism. In Proceedings of the 5th In-
ternational Plagiarism Conference. Newcastle, UK,
pages 49–64.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12). Euro-
pean Language Resources Association (ELRA), Is-
tanbul, Turkey, pages 2089–2096.

Martin Potthast, Alberto Barrón-Cedeño, Benno
Stein, and Paolo Rosso. 2011. Cross-Language
Plagiarism Detection. In Language Ressources
and Evaluation. volume 45, pages 45–62.
https://doi.org/10.1007/s10579-009-9114-z.

J. R. Quinlan. 1992. Learning with continuous classes.
In Eds. Adams & Sterling, editor, Proceedings of the
Fifth Australian Joint Conference on Artificial In-
telligence. World Scientific, Singapore, pages 343–
348.

Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing. Manchester, UK, pages 44–
49.

113



Gilles Sérasset. 2015. DBnary: Wiktionary as
a Lemon-Based Multilingual Lexical Resource in
RDF. In Semantic Web Journal (special issue on
Multilingual Linked Open Data). volume 6, pages
355–361. https://doi.org/10.3233/SW-140147.

Md Arafat Sultan, Steven Bethard, and Tamara
Sumner. 2015. DLS@CU: Sentence similar-
ity from word alignment and semantic vector
composition. In Proceedings of the 9th Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2015). Denver, CO, USA, pages 148–153.
http://www.aclweb.org/anthology/S15-2027.

Yong Wang and Ian H. Witten. 1997. Induction of
model trees for predicting continuous classes. In
Proceedings of the poster papers of the European
Conference on Machine Learning. Prague, Czech
Republic, pages 128–137.

114


