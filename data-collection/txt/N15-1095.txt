



















































Joint Generation of Transliterations from Multiple Representations


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 943–952,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Joint Generation of Transliterations from Multiple Representations

Lei Yao and Grzegorz Kondrak
Department of Computing Science

University of Alberta
Edmonton, AB, Canada

{lyao1,gkondrak}@ualberta.ca

Abstract

Machine transliteration is often referred to
as phonetic translation. We show that
transliterations incorporate information from
both spelling and pronunciation, and pro-
pose an effective model for joint transliter-
ation generation from both representations.
We further generalize this model to include
transliterations from other languages, and en-
hance it with reranking and lexicon features.
We demonstrate significant improvements in
transliteration accuracy on several datasets.

1 Introduction

Transliteration is the conversion of a text from one
script to another. When a new name like Ey-
jafjallajökull appears in the news, it needs to be
promptly transliterated into dozens of languages.
Computer-generated transliterations can be more ac-
curate than those created by humans (Sherif and
Kondrak, 2007). When the names in question origi-
nate from languages that use the same writing script
as the target language, they are likely to be copied
verbatim; however, their pronunciation may still be
ambiguous. Existing transliterations and transcrip-
tions can help in establishing the correct pronuncia-
tion (Bhargava and Kondrak, 2012).

Transliteration is often defined as phonetic trans-
lation (Zhang et al., 2012). In the idealized model
of Knight and Graehl (1997), a bilingual expert pro-
nounces a name in the source language, modifies the
pronunciation to fit the target language phonology,
and writes it down using the orthographic rules of
the target script. In practice, however, it may be dif-
ficult to guess the correct pronunciation of an unfa-
miliar name from the spelling.

Phonetic-based models of transliteration tend to
achieve suboptimal performance. Al-Onaizan and
Knight (2002) report that a spelling-based model
outperforms a phonetic-based model even when pro-
nunciations are extracted from a pronunciation dic-
tionary. This can be attributed to the importance
of the source orthography in the transliteration pro-
cess. For example, the initial letters of the Russian
transliterations of the names Chicano ([tSIkAno]) and
Chicago ([SIkAgo]) are identical, but different from
Shilo ([SIlo]). The contrast is likely due to the id-
iosyncratic spelling of Chicago.

Typical transliteration systems learn direct ortho-
graphic mapping between the source and the tar-
get languages from parallel training sets of word
pairs (Zhang et al., 2012). Their accuracy is lim-
ited by the fact that the training data is likely to con-
tain names originating from different languages that
have different romanization rules. For example, the
Russian transliterations of Jedi, Juan, Jenins, Jelto-
qsan, and Jecheon all differ in their initial letters.
In addition, because of inconsistent correspondences
between letters and phonemes in some languages,
the pronunciation of a word may be difficult to de-
rive from its orthographic form.

We believe that transliteration is not simply pho-
netic translation, but rather a process that combines
both phonetic and orthographic information. This
observation prompted the development of several
hybrid approaches that take advantage of both types
of information, and improvements were reported on
some test corpora (Al-Onaizan and Knight, 2002;
Bilac and Tanaka, 2004; Oh and Choi, 2005). These
models, which we discuss in more detail in Sec-
tion 2.1, are well behind the current state of the art
in machine transliteration.

943



In this paper, we conduct experiments that show
the relative importance of spelling and pronuncia-
tion. We propose a new hybrid approach of joint
transliteration generation from both orthography and
pronunciation, which is based on a discriminative
string transduction approach. We demonstrate that
our approach results in significant improvements in
transliteration accuracy. Because phonetic transcrip-
tions are rarely available, we propose to capture the
phonetic information from supplemental transliter-
ations. We show that the most effective way of
utilizing supplemental transliterations is to directly
include their original orthographic representations.
We show improvements of up to 30% in word accu-
racy when using supplemental transliterations from
several languages.

The paper is organized as follows. We discuss
related work in Section 2. Section 3 describes our
hybrid model and a generalization of this model
that leverages supplemental transliterations. Sec-
tion 4 and 5 present our experiments of joint genera-
tion with supplemental transcriptions and transliter-
ations, respectively. Section 6 presents our conclu-
sions and future work.

2 Related work

In this section, we focus on hybrid transliteration
models, and on methods of leveraging supplemen-
tal transliterations.

2.1 Hybrid models

Al-Onaizan and Knight (2002) present a hybrid
model for Arabic-to-English transliteration, which
is a linear combination of phoneme-based and
grapheme-based models. The hybrid model is
shown to be superior to the phoneme-based model,
but inferior to the grapheme-based model.

Bilac and Tanaka (2004) propose a hybrid model
for Japanese-to-English back-transliteration, which
is also based on linear interpolation, but the interpo-
lation is performed during the transliteration genera-
tion process, rather than after candidate target words
have been generated. They report improvement over
the two component models on some, but not all, of
their test corpora.

Oh and Choi (2005) replace the fixed linear inter-
polation approach with a more flexible model that

takes into account the correspondence between the
phonemes and graphemes during the transliteration
generation process. They report superior perfor-
mance of their hybrid model over both component
models. However, their model does not consider the
coherence of the target word during the generation
process, nor other important features that have been
shown to significantly improve machine translitera-
tion (Li et al., 2004; Jiampojamarn et al., 2010).

Oh et al. (2009) report that their hybrid models
improve the accuracy of English-to-Chinese translit-
eration. However, since their focus is on investigat-
ing the influence of Chinese phonemes, their hybrid
model is again a simple linear combination of basic
models.

2.2 Leveraging supplemental transliterations

Previous work that explore the idea of taking advan-
tage of data from additional languages tend to em-
ploy supplemental transliterations indirectly, rather
than to incorporate them directly into the generation
process.

Khapra et al. (2010) propose a bridge approach of
transliterating low-resource language pair (X,Y ) by
pivoting on an high-resource language Z, with the
assumption that the pairwise data between (X,Z)
and (Y,Z) is relatively large. Their experiments
show that pivoting on Z results in lower accuracy
than directly transliterating X into Y . Zhang et al.
(2010) and Kumaran et al. (2010) combine the pivot
model with a grapheme-based model, which works
better than either of the two approaches alone. How-
ever, their model is not able to incorporate more than
two languages.

Bhargava and Kondrak (2011) propose a rerank-
ing approach that uses supplemental translitera-
tions to improve grapheme-to-phoneme conversion
of names. Bhargava and Kondrak (2012) generalize
this idea to improve transliteration accuracy by uti-
lizing either transliterations from other languages,
or phonetic transcriptions in the source language.
Specifically, they apply an SVM reranker to the top-
n outputs of a base spelling-based model. However,
the post-hoc property of reranking is a limiting fac-
tor; it can identify the correct transliteration only if
the base model includes it in its output candidate list.

944



3 Joint Generation

In this section, we describe our approach of the joint
transduction of a transliteration T from a source or-
thographic string S and a source phonemic string P
(Figure 1). We implement our approach by modi-
fying the DIRECTL+ system of Jiampojamarn et al.
(2010), which we describe in Section 3.1. In the fol-
lowing sections, we discuss other components of our
approach, namely alignment (3.2), scoring (3.3), and
search (3.4). In Section 3.5 we generalize the joint
model to accept multiple input strings.

3.1 DirecTL+
DIRECTL+ (Jiampojamarn et al., 2010) is a dis-
criminative string transducer which learns to con-
vert source strings into target strings from a set of
parallel training data. It requires pairs of strings to
be aligned at the character level prior to training.
M2M-ALIGNER (Jiampojamarn et al., 2007), an un-
supervised EM-based aligner, is often used to gener-
ate such alignments. The output is a ranked list of
candidate target strings with their confidence scores.
Below, we briefly describe the scoring model, the
training process, and the search algorithm.

The scoring model assigns a score to an aligned
pair of source and target strings (S, T ). Assum-
ing there are m aligned substrings, such that the ith
source substring generates the ith target substring,
the score is computed with the following formula:

m∑
i

α · Φ(i, S, T ) (1)

where α is the weight vector, and Φ is the feature
vector.

There are four sets of features. Context features
are character n-grams within the source word. Tran-
sition features are character n-grams within the tar-
get word. Linear-chain features combine context
features and transition features. Joint n-gram fea-
tures further capture the joint information on both
sides.

The feature weights α are learned with the Maxi-
mum Infused Relaxed Algorithm (MIRA) of Cram-
mer and Singer (2003). MIRA aims to find the
smallest change in current weights so that the new
weights separate the correct target strings from in-
correct ones by a margin defined by a loss func-

Figure 1: Triple alignment between the source phonemes,
source graphemes, and the target graphemesアロン (A-
RO-N).

tion. Given the training instance (S, T ) and the cur-
rent feature weights αk−1, the update of the feature
weights can be described as the following optimiza-
tion problem:

min
αk
‖αk − αk−1‖ s.t. ∀T̂ ∈ Tn :

αk · (Φ(S, T )− Φ(S, T̂ )) ≥ loss(T, T̂ )
where T̂ is a candidate target in the n-best list Tn
found under the current model parameterized by
αk−1. The loss function is the Levenshtein distance
between T and T̂ .

Given an unsegmented source string, the search
algorithm finds a target string that achieves the high-
est score according to the scoring model. It searches
through all the possible segmentations of the source
string and all possible target substrings using the fol-
lowing dynamic programming formulation:

Q(0, $) = 0

Q(j, t) = max
t′,t,j−N≤j′<j

α · φ(Sjj′+1, t′, t) +Q(j′, t′)
Q(J + 1, $) = max

t′
α · φ($, t′, $) +Q(J, t′)

Q(j, t) is defined as the maximum score of the tar-
get sequence ending with target substring t, gener-
ated by the letter sequence S1...Sj . φ describes the
features extracted from the current generator sub-
string Sjj′+1 of target substring t, with t

′ to be the
last generated target substring. N specifies the max-
imum length of the source substring. The $ sym-
bols are used to represent both the start and the end
of a string. Assuming that the source string con-
tains J characters, Q(J+1, $) gives the score of the
highest scoring target string, which can be recovered
through backtracking.

945



Figure 2: Three pairwise alignments between the En-
glish word abbey, its transcription [abi], and the Japanese
transliterationアベイ (A-BE-I).

3.2 Multi-alignment

M2M-ALIGNER applies the EM algorithm to align
sets of string pairs. For the purpose of joint gener-
ation, we need to align triples S, P and T prior to
training. The alignment of multiple strings is a chal-
lenging problem (Bhargava and Kondrak, 2009). In
general, there is no obvious way of merging three
pairwise alignments. Figure 2 shows an example
of three pairwise alignments that are mutually in-
consistent: the English letter e is aligned to the
phoneme [i] and to the graphemeベ(BE), which are
not aligned to each other

Our solution is to select one of the input strings
as the pivot for aligning the remaining two strings.
Specifically, we align the pivot string to each of
the other two strings through one-to-many align-
ments, where the maximum length of aligned sub-
strings in the pivot string is set to one. Then we
merge these two pairwise alignments according to
the pivot string. Since the source phoneme string
may or may not be available for a particular train-
ing instance, we use the source orthographic string
as the pivot. The one-to-many pairwise alignments
between the graphemes and phonemes, and between
the graphemes and the transliterations are generated
with M2M-ALIGNER. Figure 3 provides an exam-
ple of this process.

An alternative approach is to pivot on the tar-
get string. However, because the target string is
not available at test time, we need to search for the
highest-scoring target string, given an unsegmented
source string S and the corresponding unsegmented
phoneme string P . We can generalize the original
search algorithm by introducing another dimension
into the dynamic-programming table for segmenting
P , but it substantially increases the time complexity
of the decoding process. Our development experi-
ments indicated that pivoting on the target string not

Figure 3: Obtaining a triple alignment by pivoting on the
source word.

only requires more time, but also results in less ac-
curate transliterations.

3.3 Scoring Model

The scoring formula (1) is extended to compute
a linear combination of features of three aligned
strings (S, P, T ):

m∑
i

α · [Φ(i, S, T ),Φ(i, P, T )] (2)

The transition features on T are only computed
once, because they are independent of the input
strings. We observed no improvement by including
features between S and P in our development exper-
iments.

3.4 Search

Our search algorithm finds the highest-scoring target
string, given a source string and a phoneme string.
Since we pivot on the source string to achieve mul-
tiple alignment, the input to the search algorithm
is actually one-to-many aligned pair of the source
string and the phoneme string. The search space is
therefore the same as that of DirecTL+, i.e. the prod-
uct of all possible segmentations of the source string
and all possible target substrings. However, since
we apply one-to-many alignment, there is only one
possible segmentation of the source string, which is
obtained by treating every letter as a substring. We
apply the same dynamic programming search as Di-
recTL+, except that we extend the feature extraction
function φ(Sjj′+1, t

′, t) in the original formulation
to [φ(Sjj′+1, t

′, t), φ(P kk′+1, t
′, t)] so that features be-

tween the current phoneme substring P kk′+1 and the
target substrings are taken into consideration. The
time complexity of this search is only double of the
complexity of DIRECTL+, and is independent of the
length of the phoneme string.

946



3.5 Generalization

Since we may need to leverage information from
other sources, e.g., phonemes of supplemental
transliterations, each training instance can be com-
posed of a source word, a target word, and a list of
supplemental strings. The size of the list is not fixed
because we may not have access to some of the sup-
plemental strings for certain source words.

We first align all strings in each training instance
by merging one-to-many pairwise alignments be-
tween the source word and every other string in the
instance, as described in Section 3.2. The general-
ization of training is straightforward. For the scoring
model, we extract the same set of features as before
by pairing each supplemental string with the target
word. Since the alignment is performed beforehand,
the time complexity of the generalized search only
increases linearly in the number of input strings with
respect to the original complexity.

4 Leveraging transcriptions

In this section, we describe experiments that involve
generating transliterations jointly from the source
orthography and pronunciation. We test our method
on the English-to-Hindi and English-to-Japanese
transliteration data from the NEWS 2010 Machine
Transliteration Shared Task (Li et al., 2010). We ex-
tract the corresponding English pronunciations from
the Combilex Lexicon (Richmond et al., 2009). We
split each transliteration dataset into 80% for train-
ing, 10% for development, 10% for testing. We limit
the datasets to contain only transliterations that have
phonetic transcriptions in Combilex, so that each en-
try is composed of a source English word, a source
transcription, and a target Japanese or Hindi word.
The final results are obtained by joining the train-
ing and development sets as the final training set.
The final training/test sets contain 8,264/916 entries
for English-to-Japanese, and 3,503/353 entries for
English-to-Hindi.

4.1 Gold transcriptions

We compare three approaches that use differ-
ent sources of information: (a) graphemes only;
(b) phonemes only; and (c) both graphemes and
phonemes. The first two approaches use DI-
RECTL+, while the last approach uses our joint

Model En→Ja En→Hi
Graphemes only 58.0 42.6
Phonemes only 52.4 39.4
Joint 63.6 46.1

Table 1: Transliteration word accuracy depending on the
source information.

model described in Section 3. We evaluate each ap-
proach by computing the word accuracy.

Table 1 presents the transliteration results. Even
with gold-standard transcriptions, the phoneme-
based model is worse than the grapheme-based
model. This demonstrates that it is incorrect to refer
to the process of transliteration as phonetic transla-
tion. On the other hand, our joint generation ap-
proach outperforms both single-source models on
both test sets, which confirms that transliteration re-
quires a joint consideration of orthography and pro-
nunciation.

It is instructive to look at a couple of exam-
ples where outputs of the models differ. Consider
the name Marlon, pronounced [mArl@n], which is
transliterated into Japanese as マロン(MA-RO-N)
(correct), andマレン(MA-RE-N) (incorrect), by the
orthographic and phonetic approaches, respectively.
The letter bigram lo is always transliterated intoロ
in the orthographic training data, while the phoneme
bigram /l@/ has multiple correspondences in the pho-
netic training data. In this case, the unstressed vowel
reduction process in English causes a loss of the or-
thographic information, which needs to be preserved
in the transliteration.

In the joint model, the phonetic information
sometimes helps disambiguate the pronunciation of
the source word, thus benefiting the transliteration
process. For example, the outputs of the three
models for haddock, pronounced [had@k], are ハダ
ク(HA-DA-KU) (phonetic), ハドドック(HA-DO-
DO-K-KU) (orthographic), and ハドック(HA-DO-
K-KU) (joint, correct). The phonetic model is again
confused by the reduced vowel [@], while the ortho-
graphic model mistakenly replicates the rendering
of the consonant d, which is pronounced as a single
phoneme.

947



Model En→Ja En→Hi
Graphemes only 63.1 43.5
Joint (gold phon.) 67.4 48.0
Joint (generated phon.) 65.8 46.1

Table 2: Transliteration accuracy improvement with gold
and generated phonetic transcriptions.

4.2 Generated Transcriptions

The training entries that have no corresponding tran-
scriptions in our pronunciation lexicon were ex-
cluded from the experiment described above. When
we add those entries back to the datasets, we can
no longer apply the phonetic approach, but we can
still compare the orthographic approach to our joint
approach, which can handle the lack of a phonetic
transcription in some of the training instances. The
training sets are thus larger in the experiments de-
scribed in this section: 30,190 entries for English-
to-Japanese, and 12,070 for English-to-Hindi. The
test sets are the same as in Section 4.1. The results
in the first two rows in Table 2 show that the joint ap-
proach outperforms the orthographic approach even
when most training entries lack the pronunciation in-
formation.1

Gold transcriptions are not always available, espe-
cially for names that originate from other languages.
Next, we investigate whether we can replace the
gold transcriptions with transcriptions that are au-
tomatically generated from the source orthography.
We adopt DIRECTL+ as a grapheme-to-phoneme
(G2P) converter, train it on the entire Combilex lexi-
con, and include the generated transcriptions instead
of the gold transcriptions in the transliteration train-
ing and test sets for the joint model. The test sets are
unchanged.

The third row in Table 2 shows the result of lever-
aging generated transcriptions. We still see improve-
ment over the orthographic approach, albeit smaller
than with the gold transcriptions. However, we need
to be careful when interpreting these results. Since
our G2P converter is trained on Combilex, the gen-

1The improvement is statistically significant according to
the McNemar test with p < 0.05. The differences in the base-
line results between Table 1 and Table 2 are due to the differ-
ences in the training sets. The matching value of 46.1 across
both tables is a coincidence. The comparison of results within
any given table column is fair.

Model En→Ja En→Hi
Graphemes only 53.3 46.4
Phonemes only 19.2 10.4
Joint (suppl. phonemes) 54.8 50.0

Table 3: Transliteration accuracy with transcriptions gen-
erated from third-language transliterations.

erated transcriptions of words in the test set are quite
accurate. When we test the joint approach only
on words that are not found in Combilex, the im-
provement over the orthographic approach largely
disappears. We interpret this result as an indication
that the generated transcriptions help mostly by cap-
turing consistent grapheme-to-phoneme correspon-
dences in the pronunciation lexicon.

5 Leveraging transliterations

In the previous section, we have shown that pho-
netic transcriptions can improve the accuracy of the
transliteration process by disambiguating the pro-
nunciation of the source word. Unfortunately, pho-
netic transcriptions are rarely available, especially
for words which originate from other languages, and
generating them on the fly is less likely to help.
However, transliterations from other languages con-
stitute another potential source of information that
could be used to approximate the pronunciation in
the source language. In this section, we present ex-
periments of leveraging such supplemental translit-
erations through our joint model.

5.1 Third-language transcriptions
An intuitive way of employing transliterations from
another language is to convert them into phonetic
transcriptions using a G2P model, which are then
provided to our joint model together with the source
orthography. We test this idea on the data from
the NEWS 2010 shared task. We select Thai as
the third language, because it has the largest num-
ber of the corresponding transliterations. We re-
strict the training and test sets to include only words
for which Thai transliterations are available. The
resulting English-to-Japanese and English-to-Hindi
training/test sets contain 12,889/1,009, and 763/250
entries, respectively. We adopt DIRECTL+ as a
G2P converter, and train it on 911 Thai spelling-
pronunciation pairs extracted from Wiktionary. Be-

948



Language Acc. Data size
Thai 15.2 911
Hindi 25.9 819
Hebrew 21.3 475
Korean 40.9 3181

Table 4: Grapheme-to-phoneme word accuracy on the
Wiktionary data.

cause of the small size of the training data, it can
only achieve about 15% word accuracy in our G2P
development experiment.

Table 3 shows the transliteration results. The ac-
curacy of the model that uses only supplemental
transcriptions (row 2) is very low, but the joint model
obtains an improvement even with such inaccurate
third-language transcriptions. Note that the Thai
pronunciation is often quite different from English.
For instance, the phoneme sequence [waj] obtained
from the Thai transliteration of Whyte, helps the
joint model correctly transliterate the English name
into Japanese ホワイト(HO-WA-I-TO), which is
better than ホイト(HO-I-TO) produced by the or-
thographic model.

5.2 Multi-lingual transcriptions
Transcriptions obtained from a third language are
not only noisy because of the imperfect G2P con-
version, but often also lossy, in the sense of miss-
ing some phonetic information present in the source
pronunciation. In addition, supplemental transliter-
ations are not always available in a given third lan-
guage. In this section, we investigate the idea of
extracting phonetic information from multiple lan-
guages, with the goal of reducing the noise of gen-
erated transcriptions.

We first train G2P converters for several lan-
guages on the pronunciation data collected from
Wiktionary. Table 4 shows the sizes of the G2P
datasets, and the corresponding G2P word accuracy
numbers, which are obtained by using 90% of the
data for training, and the rest for testing.2 For the
highly-regular Japanese Katakana, we instead cre-
ate a rule-based converter. Then we convert sup-
plemental transliterations from those languages into

2We use the entire datasets to train G2P converters for the
transliteration experiments, but their accuracy is unlikely to im-
prove much due to a small increase in the training data.

Model En→Ja En→Hi
Graphemes only 54.5 46.1
Joint (suppl. phonemes) 58.6 46.4

Table 5: Transliteration accuracy with transcriptions gen-
erated from multiple transliterations.

noisy phonetic transcriptions. In order to obtain rep-
resentative results, we also include transliteration
pairs without supplemental transliterations, which
results in different datasets than in the previous ex-
periments. The sets for English-to-Japanese and
English-to-Hindi now contain 30,190/17,557/1,886
and 12,070/3,777/380 entries, where the sizes refer
to (1) the entire training set, (2) the subset of training
entries that have at least one supplemental transcrip-
tion, and (3) the test set (in which all entries have
supplemental transcriptions).

An interlingual approach holds the promise
of ultimately replacing n2 pairwise grapheme-
grapheme transliteration models involving n lan-
guages with 2n grapheme-phoneme and phoneme-
grapheme models based on a unified phonetic rep-
resentation. In our implementation, we merge dif-
ferent phonetic transcriptions of a given word into
a single abstract vector representation. Specifically,
we replace each phoneme with a phonetic feature
vector according to a phonological feature chart,
which includes features such as labial, voiced, and
tense. After merging the vectors by averaging their
weights, we incorporate them into the joint model
described in Section 3.3 by modifying Φ(i, P, T ).
Unfortunately, the results are disappointing. It ap-
pears that the vector merging process compounds
the information loss, which offsets the advantage of
incorporating multiple transcriptions.

Another way of utilizing supplemental transcrip-
tions is to provide them directly to our generalized
joint model described in Section 3.5, which can
handle multiple input strings. Table 5 presents the
results on leveraging transcriptions generated from
supplemental transliterations. We see that the joint
generation from multiple transcriptions significantly
boosts the accuracy on English-to-Japanese, but the
improvement on English-to-Hindi is minimal.

949



Model En→Ja Ja→En En→Hi Hi→En
DIRECTL+ 51.5 19.7 43.4 42.6
Reranking 56.8 30.3 50.8 48.9
Joint 56.4 38.8 51.6 51.1
Joint + Reranking 57.0 44.6 53.0 57.2
+ Lexicon - 53.1 - 61.7

Table 6: Transliteration accuracy with supplemental information.

5.3 Multi-lingual transliterations

The generated transcriptions of supplemental
transliterations discussed in the previous section are
quite inaccurate because of small and noisy G2P
training data. In addition, we are prevented from
taking advantage of supplemental transliterations
from other languages by the lack of the G2P train-
ing data. In order to circumvent these limitations,
we propose to directly incorporate supplemental
transliterations into the generation process. Specif-
ically, we train our generalized joint model on the
graphemes of the source word, as well as on the
graphemes of supplemental transliterations.

The experiments that we have conducted so far
suggest two additional methods of improving the
transliteration accuracy. We have observed that n-
best lists produced by our joint model contain the
correct transliteration more often than the baseline
models. Therefore, we follow the joint genera-
tion with a reranking step, in order to boost the
top-1 accuracy. We apply the reranking algorithm
of Bhargava and Kondrak (2011), except that our
joint model is the base system for reranking. In or-
der to ensure fair comparison, the held-out sets for
training the rerankers are subtracted from the origi-
nal training sets.

Another observation that we aim to exploit is that
a substantial number of the outputs generated by our
joint model are very close to gold-standard translit-
erations. In fact, news writers often use slightly
different transliterations of the same name, which
makes the model’s task more difficult. Therefore,
we rerank the model outputs using a target-language
lexicon, which is a list of words together with their
frequencies collected from a raw corpus. We fol-
low Cherry and Suzuki (2009) in extracting lexicon
features for a given word according to coarse bins,
i.e., [< 2000], [< 200], [< 20], [< 2], [< 1]. For

example, a word with the frequency 194 will cause
the features [< 2000] and [< 200] to fire.

We conduct our final experiment on forward and
backward transliteration. We utilize supplemen-
tal transliterations from all eight languages in the
NEWS 2010 dataset. The English-Japanese and
English-Hindi datasets contain 33,540 and 13,483
entries, of which 23,613 and 12,131 have at least one
supplemental transliteration, respectively. These
sets are split into training/development/test sets.
The entries that have no supplemental translitera-
tions are removed from the test sets, which results
in 2,321 and 1,226 test entries. In addition, we
extract an English lexicon comprising 7.5M word
types from the English gigaword monolingual cor-
pus (LDC2012T21) for the back-transliteration ex-
periments.

We evaluate the following models: (1) the
baseline DIRECTL+ model trained on source
graphemes; (2) the reranking model of Bhargava and
Kondrak (2011)3, with DIRECTL+ as the base sys-
tem; (3) our joint model described in Section 3.5; (4)
“combination”, which is a reranking model with our
joint model as the base system; and (5) a reranking
model that uses the English target lexicon and model
(4) as the base system.

Table 6 present the results. We see that our joint
model performs much better by directly incorporat-
ing the supplemental transliterations than by using
the corresponding phonetic transcriptions. This is
consistent with our experiments in Section 4 that
show the importance of the orthographic informa-
tion. We also observe that our joint model achieves
substantial improvements over the baseline on the
back-transliteration tasks from Japanese and Hindi
into English. This result suggests the orthographic
information from the supplemental transliterations is

3Code from http://www.cs.toronto.edu/˜aditya/g2p-tl-rr/

950



particularly effective in recovering the information
about the pronunciation of the original word which
is often obfuscated by the transliteration into a dif-
ferent language.

Our joint model is more effective in utilizing
supplemental transliterations than the reranking ap-
proach of Bhargava and Kondrak (2011), except on
English-to-Japanese. The combination of these two
approaches works better than either of them, partic-
ularly on the back-transliteration tasks. Finally, the
incorporation of a target-lexicon brings additional
gains.

Back-transliteration from Japanese to English is
more challenging than in the forward direction,
which was already noted by Knight and Graehl
(1997). Most of the names in the dataset origi-
nate from English, and Japanese phonotactics re-
quire introduction of extra vowels to separate con-
sonant clusters. During back-transliteration, it is of-
ten unclear which vowels should be removed and
which preserved. Our approach is able to dramati-
cally improve the quality of the results by recovering
the original information from multiple supplemental
transliterations.

6 Conclusion

We have investigated the relative importance of
the orthographic and phonetic information in the
transliteration process. We have proposed a novel
joint generation model that directly utilizes both
sources of information. We have shown that a gener-
alized joint model is able to achieve substantial im-
provements over the baseline represented by a state-
of-the-art transliteration tool by directly incorporat-
ing multiple supplemental transliterations. In the fu-
ture, we would like to further explore the idea of
using interlingual representations for transliteration
without parallel training data.

Acknowledgements

We thank Adam St Arnaud for help in improving the
final version of this paper.

This research was supported by the Natural Sci-
ences and Engineering Research Council of Canada
(NSERC).

References

Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in Arabic texts. In Proceed-
ings of the ACL-02 Workshop on Computational Ap-
proaches to Semitic Languages, Philadelphia, Penn-
sylvania, USA, July. Association for Computational
Linguistics.

Aditya Bhargava and Grzegorz Kondrak. 2009. Multi-
ple word alignment with Profile Hidden Markov Mod-
els. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 43–48, Boulder,
Colorado, June. Association for Computational Lin-
guistics.

Aditya Bhargava and Grzegorz Kondrak. 2011. How
do you pronounce your name? Improving G2P with
transliterations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 399–408,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.

Aditya Bhargava and Grzegorz Kondrak. 2012. Leverag-
ing supplemental representations for sequential trans-
duction. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 396–406, Montréal, Canada, June. Asso-
ciation for Computational Linguistics.

Slaven Bilac and Hozumi Tanaka. 2004. A hybrid back-
transliteration system for Japanese. In Proceedings
of Coling 2004, pages 597–603, Geneva, Switzerland,
Aug 23–Aug 27. COLING.

Colin Cherry and Hisami Suzuki. 2009. Discriminative
substring decoding for transliteration. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1066–1075, Singa-
pore, August. Association for Computational Linguis-
tics.

Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951–991, March.

Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Proceedings
of the Main Conference, pages 372–379, Rochester,
New York, April. Association for Computational Lin-
guistics.

951



Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010. Integrating joint n-gram features into
a discriminative training framework. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 697–700, Los An-
geles, California, June. Association for Computational
Linguistics.

Mitesh M. Khapra, A Kumaran, and Pushpak Bhat-
tacharyya. 2010. Everybody loves a rich cousin: An
empirical study of transliteration through bridge lan-
guages. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
420–428, Los Angeles, California, June. Association
for Computational Linguistics.

Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics, pages 128–135, Madrid, Spain, July. Association
for Computational Linguistics.

A. Kumaran, Mitesh M. Khapra, and Pushpak Bhat-
tacharyya. 2010. Compositional machine translitera-
tion. ACM Transactions on Asian Language Informa-
tion Processing (TALIP), 9(4):13:1–13:29, December.

Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of the 42Nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Report of NEWS 2010 transliteration
generation shared task. In Proceedings of the 2010
Named Entities Workshop, pages 1–11, Uppsala, Swe-
den, July. Association for Computational Linguistics.

Jong-Hoon Oh and Key-Sun Choi. 2005. Machine
learning based English-to-Korean transliteration using
grapheme and phoneme information. IEICE - Trans.
Inf. Syst., E88-D(7):1737–1748, July.

Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Can Chinese phonemes improve machine
transliteration?: A comparative study of English-to-
Chinese transliteration models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, EMNLP
’09, pages 658–667, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Korin Richmond, Robert Clark, and Sue Fitt. 2009. Ro-
bust LTS rules with the Combilex speech technology
lexicon. In Proceedings of Interspeech, pages 1259–
1298, Brighton, UK, September.

Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of the 45th An-

nual Meeting of the Association of Computational
Linguistics, pages 944–951, Prague, Czech Republic,
June. Association for Computational Linguistics.

Min Zhang, Xiangyu Duan, Vladimir Pervouchine, and
Haizhou Li. 2010. Machine transliteration: Leverag-
ing on third languages. In Coling 2010: Posters, pages
1444–1452, Beijing, China, August. Coling 2010 Or-
ganizing Committee.

Min Zhang, Haizhou Li, A Kumaran, and Ming Liu.
2012. Whitepaper of NEWS 2012 shared task on ma-
chine transliteration. In Proceedings of the 4th Named
Entity Workshop, pages 1–9, Jeju, Korea, July. Associ-
ation for Computational Linguistics.

952


