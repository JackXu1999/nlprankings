



















































DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 747‚Äì754,
Vancouver, Canada, August 3 - 4, 2017. c¬©2017 Association for Computational Linguistics

DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for
Message-level and Topic-based Sentiment Analysis

Christos Baziotis, Nikos Pelekis, Christos Doulkeridis
University of Piraeus - Data Science Lab

Piraeus, Greece
mpsp14057@unipi.gr, npelekis@unipi.gr, cdoulk@unipi.gr

Abstract

In this paper we present two deep-learning
systems that competed at SemEval-2017
Task 4 ‚ÄúSentiment Analysis in Twitter‚Äù.
We participated in all subtasks for En-
glish tweets, involving message-level and
topic-based sentiment polarity classifica-
tion and quantification. We use Long
Short-Term Memory (LSTM) networks
augmented with two kinds of attention
mechanisms, on top of word embeddings
pre-trained on a big collection of Twitter
messages. Also, we present a text process-
ing tool suitable for social network mes-
sages, which performs tokenization, word
normalization, segmentation and spell cor-
rection. Moreover, our approach uses no
hand-crafted features or sentiment lexi-
cons. We ranked 1st (tie) in Subtask A,
and achieved very competitive results in
the rest of the Subtasks. Both the word
embeddings and our text processing tool1

are available to the research community.

1 Introduction

Sentiment analysis is an area in Natural Language
Processing (NLP), studying the identification and
quantification of the sentiment expressed in text.
Sentiment analysis in Twitter is a particularly chal-
lenging task, because of the informal and ‚Äúcre-
ative‚Äù writing style, with improper use of gram-
mar, figurative language, misspellings and slang.

In previous runs of the Task, sentiment anal-
ysis was usually tackled using hand-crafted fea-
tures and/or sentiment lexicons (Mohammad et al.,
2013; Kiritchenko et al., 2014; Palogiannidi
et al., 2016), feeding them to classifiers such
as Naive Bayes or Support Vector Machines
(SVM). These approaches require a laborious

1github.com/cbaziotis/ekphrasis

feature-engineering process, which may also need
domain-specific knowledge, usually resulting both
in redundant and missing features. Whereas, arti-
ficial neural networks (Deriu et al., 2016; Rouvier
and Favre, 2016) which perform feature learning,
last year (Nakov et al., 2016) achieved very good
results, outperforming the competition.

In this paper, we present two deep-learning
systems that competed at SemEval-2017 Task
4 (Rosenthal et al., 2017). Our first model is de-
signed for addressing the problem of message-
level sentiment analysis. We employ a 2-layer
Bidirectional LSTM, equipped with an attention
mechanism (Rockt√§schel et al., 2015). For the
topic-based sentiment analysis tasks, we propose
a Siamese Bidirectional LSTM with a context-
aware attention mechanism (Yang et al., 2016).
In contrast to top-performing systems of previous
years, we do not rely on hand-crafted features,
sentiment lexicons and we do not use model en-
sembles. We make the following contributions:

‚Ä¢ A text processing tool for text tokenization,
word normalization, word segmentation and
spell correction, geared towards Twitter.

‚Ä¢ A deep learning system for short-text senti-
ment analysis using an attention mechanism,
in order to enforce the contribution of words
that determine the sentiment of a message.

‚Ä¢ A deep learning system for topic-based senti-
ment analysis, with a context-aware attention
mechanism utilizing the topic information.

2 Overview

Figure 1 provides a high-level overview of our
approach, which consists of two main steps and
an optional task-dependent third step: (1) the text
processing, where we use our own text processing
tool for preparing the data for our neural network,
(2) the learning step, where we train the neural

747



Unlabeled
Dataset

%

%

%

Training
Data

Processed 
Input Data

C
la

ss
if

ic
at

io
n

Q
u

an
ti

fi
ca

ti
o

nEmbeddings
training

Text 
processor

Neural Network

Word
Embeddings

Em
b

ed
d

in
g 

La
ye

r

Figure 1: High-level overview of our approach

networks and (3) the quantification step for esti-
mating the sentiment distribution for each topic.
Task definitions. In Subtask A, given a message
we must classify whether the message expresses
positive, negative, or neutral sentiment (3-point
scale). In Subtasks B, C (topic-based sentiment
polarity classification) we are given a message and
a topic and must classify the message on 2-point
scale (Subtask B) and a 5-point scale (Subtask C).
In Subtasks D, E (quantification) we are given a
set of messages about a set of topics and must es-
timate the distribution of the tweets across 2-point
scale (Subtask D) and a 5-point scale (Subtask E).
Unlabeled Dataset. We collected a big dataset
of 330M English Twitter messages, gathered from
12/2012 to 07/2016, which is used (1) for calculat-
ing words statistics needed by our text processor
and (2) for training our word embeddings.
Pre-trained Word Embeddings. Word em-
beddings are dense vector representations of
words (Collobert and Weston, 2008; Mikolov
et al., 2013), capturing their semantic and syntac-
tic information. We leverage our big collection
of Twitter messages to generate word embeddings,
with vocabulary size of 660K words, using GloVe
(Pennington et al., 2014). The pre-trained word
embeddings are used for initializing the first layer
(embedding layer) of our neural networks.

2.1 Text Processor
We developed our own text processing tool, in or-
der to utilize most of the information in text, per-
forming sentiment-aware tokenization, spell cor-
rection, word normalization, word segmentation
(for splitting hashtags) and word annotation.
Tokenizer. The difficulty in tokenization is to
avoid splitting expressions or words that should
be kept intact (as one token). Although there

are some tokenizers geared towards Twitter (Potts,
2011; Gimpel et al., 2011) that recognize the Twit-
ter markup and some basic sentiment expressions
or simple emoticons, our tokenizer is able to iden-
tify most emoticons, emojis, expressions such as
dates (e.g. 07/11/2011, April 23rd), times (e.g.
4:30pm, 11:00 am), currencies (e.g. $10, 25mil,
50e), acronyms, censored words (e.g. s**t),
words with emphasis (e.g. *very*) and more.
Text Postprocessing. After the tokenization we
add an extra postprocessing step, performing mod-
ifications on the extracted tokens. This is where
we perform spell correction, word normalization
and segmentation and decide which tokens to
omit, normalize or annotate (surround or replace
with special tags). For the tasks of spell correction
(Jurafsky and Martin, 2000) and word segmenta-
tion (Segaran and Hammerbacher, 2009) we used
the Viterbi algorithm, utilizing word statistics (un-
igrams and bigrams) from our unlabeled dataset,
to obtain word probabilities. Moreover, we low-
ercase all words, and normalize URLs, emails and
user handles (@user).

After performing the aforementioned steps we
decrease the vocabulary size, while keeping infor-
mation that is usually lost during the tokenization
phase. Table 1 shows an example of our text pro-
cessing pipeline, on a Twitter message.

2.2 Neural Networks

Last year, most of the top scoring systems used
Convolutional Neural Networks (CNN) (LeCun
et al., 1998). Even though CNNs are designed for
computer vision, the fact that they are fast and easy
to train, makes them a popular choice for NLP
problems. However CNNs have no notion of or-
der, thus when applying them to NLP tasks the
crucial information of the word order is lost.

2.2.1 Recurrent Neural Networks
A more natural choice is to use Recurrent Neu-
ral Networks (RNN). An RNN processes an in-
put sequentially, in a way that resembles how
humans do it. It performs the same operation,
ht = fW (xt, ht‚àí1), on every element of a se-
quence, where ht is the hidden state a timestep t,

original The *new* season of #TwinPeaks is coming on May 21, 2017. CANT WAIT \o/ !!! #tvseries #davidlynch :D
processed the new <emphasis> season of <hashtag> twin peaks </hashtag> is coming on <date> . cant <allcaps> wait

<allcaps> <happy> ! <repeated> <hashtag> tv series </hashtag> <hashtag> david lynch </hashtag> <laugh>

Table 1: Example of our text processor. The word annotations help the RNN to learn better features.

748



and W the weights of the network. The hidden
state at each timestep depends on the previous hid-
den states. This is why the order of the elements
(words) is important. This process also enables
RNNs to handle inputs of variable length.

RNNs are difficult to train (Pascanu et al.,
2013), because gradients may grow or decay expo-
nentially over long sequences (Bengio et al., 1994;
Hochreiter et al., 2001). A way to overcome these
problems is by using one of the more sophisti-
cated variants of the regular RNN, the Long Short-
Term Memory (LSTM) network (Hochreiter and
Schmidhuber, 1997) or the recently proposed
Gated Recurrent Units (GRU) (Cho et al., 2014).
Both variants introduce a gating mechanism, en-
suring proper gradient propagation through the
network. We use the LSTM, as it performed
slightly better than GRU in our experiments.
Attention Mechanism. An RNN updates its hid-
den state hi as it processes a sequence and at the
end, the hidden state holds a summary of all the
processed information. In order to amplify the
contribution of important elements in the final rep-
resentation we use an attention mechanism (Rock-
t√§schel et al., 2015; Raffel and Ellis, 2015), that
aggregates all the hidden states using their relative
importance (see Figure 2).

ùë•1

‚Ñé1

ùë•2

‚Ñé2

ùë•3

‚Ñé3

ùë•ùëá

ùíâùëª

‚Ä¶

(a) Regular RNN
ùë•1

ùëé1 ùëéùëá
ùëé2 ùëé3

‚Ñé1

ùë•2

‚Ñé2

ùë•3

‚Ñé3

ùë•ùëá

‚Ñéùëá

‚Ä¶

(b) Attention RNN

Figure 2: Comparison between the regular RNN
and the RNN with attention.

2.3 Quantification
For the quantification tasks an obvious approach is
the Classify & Count (Forman, 2008), where we
simply compute the fraction of a topic‚Äôs messages
that a classifier predicts to belong to a class c.
Another approach is the Probabilistic Classify &
Count (PCC) (Gao and Sebastiani, 2016), in which
first we train a classifier that produces a probabil-
ity distribution over the classes and then we av-
erage the estimated probabilities for each class to
obtain the final distribution. Let T be the set of
topics in the training set and p(c|tweet) the (pos-
terior) probability that a tweet belongs to class c as

estimated by the classifier. Then we estimate the
expected fraction of a topic‚Äôs tweets that belong to
class c as follows:

pÃÇT (c) =
1
|T |

‚àë
tweet‚ààT

p(c|tweet) (1)

3 Models Description

We propose two different models, a Message-level
Sentiment Analysis (MSA) model for Subtask A
(3.1) and a Topic-based Sentiment Analysis (TSA)
(3.2) model for Subtasks B,C,D,E.

3.1 MSA Model (message-level)
Our message-level sentiment analysis model
(MSA) consists of a 2-layer bidirectional LSTM
(BiLSTM) with an attention mechanism, for iden-
tifying the most informative words.
Embedding Layer. The input to the network is a
Twitter message, treated as a sequence of words.
We use an Embedding layer to project the words
X = (x1, x2, ..., xT ) to a low-dimensional vec-
tor space RE , where E the size of the Embedding
layer and T the number of words in a tweet. We
initialize the weights of the embedding layer with
our pre-trained word embeddings.
BiLSTM Layers. An LSTM takes as input the
words of a tweet and produces the word annota-
tions H = (h1, h2, ..., hT ), where hi is the hid-
den state of the LSTM at time-step i, summariz-
ing all the information of the sentence up to xi.
We use bidirectional LSTM (BiLSTM) in order to
get word annotations that summarize the informa-
tion from both directions. A bidirectional LSTM
consists of a forward LSTM

‚àí‚Üí
f that reads the sen-

tence from x1 to xT and a backward LSTM
‚Üê‚àí
f that

reads the sentence from xT to x1. We obtain the
final annotation for a given word xi, by concate-
nating the annotations from both directions:

hi =
‚àí‚Üí
hi ‚Äñ ‚Üê‚àíhi , hi ‚àà R2L (2)

where ‚Äñ denotes the concatenation operation and
L the size of each LSTM. We stack two layers of
BiLSTMs in order to learn more abstract features.
Attention Layer. Not all words contribute equally
to the expression of the sentiment in a message.
We use an attention mechanism to find the rela-
tive contribution (importance) of each word. The
attention mechanism assigns a weight ai to each
word annotation. We compute the fixed represen-
tation r of the whole message as the weighted sum

749



ùë•1 ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ

BiLSTM

ùëé1

ùëéùëá ‡∑çùëéùëñ = 1

‚Ñé1 ‚Ñé1

ùë•2 ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ ‚Ñé2 ‚Ñé2

ùë•3 ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ ‚Ñé3 ‚Ñé3

ùë•ùëá ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ ‚Ñéùëá ‚Ñéùëá

‚Ä¶ ‚Ä¶

ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ

BiLSTM

‚Ñé1 ‚Ñé1

ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ ‚Ñé2 ‚Ñé2

ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ ‚Ñé3 ‚Ñé3

ùêøùëÜùëáùëÄ ùêøùëÜùëáùëÄ ‚Ñéùëá ‚Ñéùëá

‚Ä¶ ‚Ä¶

ùëé2

ùëé3

‚Ä¶ ‚Ä¶ ‚Ä¶

class probabilities

ùëü

Figure 3: The MSA model: A 2-layer bidirectional LSTM with attention over that last layer.

of all the word annotations. Formally:

ei = tanh(Whhi + bh), ei ‚àà [‚àí1, 1] (3)

ai =
exp(ei)‚àëT
t=1 exp(et)

,
T‚àë
i=1

ai = 1 (4)

r =
T‚àë
i=1

aihi, r ‚àà R2L (5)

whereWh and bh are the attention layer‚Äôs weights,
optimized during training to assign bigger weights
to the most important words of a sentence.
Output Layer. We use the representation r as fea-
ture vector for classification and we feed it to a fi-
nal fully-connected softmax layer which outputs a
probability distribution over all classes.

3.2 TSA Model (topic-based)
For the topic-based sentiment analysis tasks, we
propose a Siamese2 bidirectional LSTM network
with a different attention mechanism than in MSA.
Our model is comparable to the work of (Wang
et al., 2016; Tang et al., 2015). However our model
differs in the way it incorporates topic information
and in the attention mechanism.
Embedding Layer. The network has two in-
puts, the sequence of words in the tweet Xtw =
(xtw1 , x

tw
2 , ..., x

tw
Ttw

), where Ttw the number of
words in the tweet, and the sequence of words in
the topic Xto = (xto1 , x

to
2 , ..., x

to
Tto

), where Tto the
number of words in the topic. We project all words
to RE , where E the size of the Embedding layer.
Siamese BiLSTM. We use a bidirectional LSTM
with shared weights to map the words of the
tweet and the topic to the same vector space,
in order to be able to make meaningful com-
parison between the two. The BiLSTM pro-
duces the annotations for the words of the tweet

2Siamese are called the networks that have identical con-
figuration and their weights are linked during training.

Htw = (htw1 , h
tw
2 , ..., h

tw
Ttw

) and the topic Hto =
(hto1 , h

to
2 , ..., h

to
Tto

), where each word annotation
consists of the concatenation of its forward and
backward annotations:

hji =
‚àí‚Üí
hji ‚Äñ

‚Üê‚àí
hji , h

j
i ‚àà R2L, j ‚àà {tw, to} (6)

where ‚Äñ denotes the concatenation operation and
L the size of each LSTM.
Mean-Pooling Layer. We use a Mean-Pooling
layer over the word annotations of the topic Hto

that aggregates them to produce a single annota-
tion. The layer computes the mean over time to

produce the topic annotation, hto =
1
Tto

‚àëTto
1 h

to
i .

Context-Aware Annotations. We append the
topic annotation hto to each word annotation to get
the final context-aware annotation for each word:

hi = htwi ‚Äñ hto, hji ‚àà R4L (7)
Context-Attention Layer. We use a context-
aware attention mechanism as in (Yang et al.,
2016), in order to strengthen the contribution
of words that express sentiment towards a given
topic. This is done by adding a context vector uh
that can be interpreted as a fixed query, like ‚Äúwhich
words express sentiment towards the given topic‚Äù,
over the words of the message. Concretely:

ei = tanh(Whhi + bh), ei ‚àà [‚àí1, 1] (8)

ai =
exp(e>i uh)‚àëTtw
t=1 exp(e

>
t uh)

,

Ttw‚àë
i=1

ai = 1 (9)

r =
Ttw‚àë
i=1

aihi, r ‚àà R4L (10)

where Wh, bh and uh are jointly learned weights.
Maxout Layer. We pass the representation r to a
Maxout (Goodfellow et al., 2013) layer to make
the final comparison between the tweet aspects

750



‚Ä¶T
w
e
e
t

T
o
p
ic

ùêµùëñùêøùëÜùëáùëÄ ùêµùëñùêøùëÜùëáùëÄ

Shared
weights

‚Ä¶ ‚Ä¶

‚Ä¶

ùëÄùëíùëéùëõ ùëÉùëúùëúùëôùëñùëõùëî

‚Ñé1
ùë°ùë§ ‚Ñé2

ùë°ùë§ ‚Ñé3
ùë°ùë§ ‚Ñé1

ùë°ùëú ‚Ñé2
ùë°ùëú ‚Ñé3

ùë°ùëú

‚Ñéùë°ùëú

‚Ä¶

‚Ñé1
ùë°ùë§ ‚Ñé2

ùë°ùë§ ‚Ñé3
ùë°ùë§ ‚Ñéùëá

ùë°ùë§

ùëé1 ùëéùëá

‡∑çùëéùëñ = 1

ùëé2 ùëé3
M
axo

u
t

class probabilities

ùë¢‚Ñé

ùë•1
ùë°ùë§ ùë•2

ùë°ùë§ ùë•3
ùë°ùë§ ùë•ùëáùë°ùë§

ùë°ùë§ ùë•1
ùë°ùëú ùë•2

ùë°ùëú ùë•3
ùë°ùëú

ùëü

ùë•ùëáùë°ùëú
ùë°ùëú

‚Ñéùëáùë°ùë§
ùë°ùë§ ‚Ñéùëáùë°ùëú

ùë°ùëú

Figure 4: The TSA model: A Siamese Bidirectional LSTM with context-aware attention.

and the topic aspects. We selected Maxout as it
amplifies the effects of dropout (Section 3.3).
Output Layer. We pass the output of the Max-
out layer to a final fully-connected softmax layer
which outputs a probability distribution over all
classes.

3.3 Regularization
In both of our models we add Gaussian noise at
the embedding layer, which can be interpreted
as a random data augmentation technique, mak-
ing our models more robust to overfitting. In ad-
dition to that, we use dropout (Srivastava et al.,
2014) to randomly turn-off neurons in our net-
work. Dropout prevents co-adaptation of neurons
and can also be thought as a form of ensemble
learning, because for each training example a sub-
part of the whole network is trained. Moreover, we
apply dropout to the recurrent connections of the
LSTM as in (Gal and Ghahramani, 2016). Further-
more we add L2 regularization penalty (weight
decay) to the loss function to discourage large
weights. Also, we stop training after the valida-
tion loss has stopped decreasing (early-stopping).

Finally, we do not fine-tune the embedding lay-
ers. Words occurring in the training set, will be
moved in the embedding space and the classifier
will correlate certain regions (in embedding space)
to certain sentiments. However, words in the test
set and not in the training set, will remain at their
initial position which may no longer reflect their
‚Äútrue‚Äù sentiment, leading to miss-classifications.

3.4 Class Weights
In the training data some classes have more train-
ing examples than others, introducing bias in our
models. In order to deal with this problem we ap-
ply class weights to the loss function of our mod-
els, penalizing more the misclassification of un-
derrepresented classes. Moreover, we introduce

a smoothing factor in order to smooth out the
weights in cases where the imbalances are very
strong, which would otherwise lead to extremely
large class weights. Let x be the vector with the
class counts and Œ± the smoothing factor, we ob-
tain class weights with wi =

max(x)
xi+Œ±√ómax(x) . In

Subtasks A, B, D we use no smoothing (Œ± = 0)
and in Subtasks C and E we set Œ± = 0.1.

3.5 Training

We train all of our networks to minimize the
cross-entropy loss, using back-propagation with
stochastic gradient descent and mini-batches of
size 128. We use Adam (Kingma and Ba, 2014)
for tuning the learning rate and we clip the norm
of the gradients (Pascanu et al., 2013) at 5, as an
extra safety measure against exploding gradients.
Dataset. For training we use the available data
from prior years (only tweets). Table 2 shows the
statistics of the data we used. Also, we do not use
any user information from the tweets (only text).

3.5.1 Hyper-parameters
In order to find good hyper-parameter values in
a relative short time, compared to grid or ran-
dom search, we adopt the Bayesian optimization
(Bergstra et al., 2013) approach, performing a
‚Äúsmart‚Äù search in the high dimensional space of
all the possible values.
MSA Model. The size of the embedding layer is
300, and the LSTM layers 150 (300 for BiLSTM).
We add Gaussian noise with œÉ = 0.2 and dropout
of 0.3 at the embedding layer, dropout of 0.5 at
the LSTM layers and dropout of 0.25 at the recur-
rent connections of the LSTM. Finally, we add L2
regularization of 0.0001 at the loss function.
TSA Model. The size of the embedding layer is
300, and the LSTM layers 64 (128 for BiLSTM).
We insert Gaussian noise with œÉ = 0.2 at the em-
bedding layer of both inputs and dropout of 0.3 at

751



Positive Neutral Negative
Dataset Task 2 1 0 -1 -2 Total
Train A 19652 (39.64%) 22195 (44.78%) 7723 (15.58%) 49570

B,D 14897 (78.85%) - 3997 (21.15%) 18894
C,E 1016 (3.34%) 12852 (42.23%) 12888 (42.35%) 3380 (11.11%) 296 (0.97%) 30432

Test A 2375 (19.33%) 5937 (48.33%) 3972 (32.33%) 12284
B,D 2463 (39.82%) - 3722 (60.18%) 6185
C,E 131 (1.06%) 2332 (18.84%) 6194 (50.04%) 3545 (28.64%) 177 (1.43%) 12379

Table 2: Dataset statistics. Notice the difference in the ratio of positive-negative classes this year.

the embedding layer of the message, dropout of
0.2 at the LSTM layer and the recurrent connec-
tion of the LSTM layer and dropout of 0.3 at the
attention layer and the Maxout layer. Finally, we
add L2 regularization of 0.001 at the loss function.

4 Experiments and Results

Semeval Results. Our official ranking (Rosenthal
et al., 2017) is 1/38 (tie) in Subtask A, 2/24 in Sub-
task B, 2/16 in Subtask C, 2/16 in Subtask D and
11/12 in Subtask E. All of our models performed
very good, with the exception of Subtask E. Since
the quantification was performed on top of the
classifier of Subtask C, which came in 2nd place,
we conclude that our quantification approach was
the reason for the bad results for Subtask E.
Attention Mechanism. In order to assess the im-
pact of the attention mechanisms, we evaluated the
performance of each model, with and without at-
tention. We report (Table 3) the average scores of
10 runs for each system, on the official test set.
The attention-based models performed better, but
only by a small margin.

RNN Subtask A (MSA) Subtask B (TSA)
œÅ F1pn œÅ F1pn

Regular 0.678 0.673 0.856 0.817
Attention 0.682 0.675 0.863 0.82

Table 3: Results of the impact of attention3.

Quantification. To get a better insight into the
quantification approaches, we compare the per-
formance of CC and PCC. It is inconclusive as
to which quantification approach is better. PCC
outperformed CC in (Bella et al., 2010) but un-
derperformed CC in (Esuli and Sebastiani, 2015).
Following the results from (Gao and Sebastiani,
2016), which are reported on sentiment analysis
in twitter, we decided to use PCC for both of our

3œÅ is the average recall and F1pn the macro-average F1
score of the positive and negative classes

quantification submissions. Table 4 shows the per-
formance of our models. PCC performed better
than CC for Subtask D but far worse than CC for
Subtask E. We hypothesize that two possible rea-
sons for the difference in performance between D
and E, might be (1) the difference in the number of
classes and (2) the big change in the ratio of pos-
to-neg classes between the training and test sets.

Method Subtask D Subtask E
KLD AE RAE EMD

CC 0.060 0.093 0.608 0.359
PCC 0.048 0.095 0.848 0.595

Table 4: Results of the quantification approaches4.

Experimental setup. For developing our mod-
els we used Keras (Chollet, 2015) with Theano
(Theano Dev Team, 2016) as backend and Scikit-
learn (Pedregosa et al., 2011). We trained our neu-
ral networks on a GTX750Ti (4GB). Lastly, we
share the source code of our models 5.

5 Conclusion

In this paper, we present two deep-learning sys-
tems for short text sentiment analysis developed
for SemEval-2017 Task 4 ‚ÄúSentiment Analysis in
Twitter‚Äù. We use RNNs, utilizing well established
methods in the literature. Additionally, we em-
power our networks with two different kinds of at-
tention mechanisms in order to amplify the contri-
bution of the most important words.

Our models achieved excellent results in the
classification tasks, but mixed results in the quan-
tification tasks. We would like to work more in this
area and explore more quantification techniques in
the future. Another interesting approach would be
to design models operating on the character-level.

4KLD is Kullback-Leibler Divergence, EMD is Earth
Mover‚Äôs Distance, AE is Absolute Error and RAE is Rela-
tive Absolute Error. For all metrics lower is better.

5https://github.com/cbaziotis/
datastories-semeval2017-task4

752



References
Antonio Bella, Cesar Ferri, Jos√© Hern√°ndez-Orallo,

and Maria Jose Ramirez-Quintana. 2010. Quantifi-
cation via probability estimators. In Proceedings of
ICDM. pages 737‚Äì742.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neu-
ral Networks 5(2):157‚Äì166.

James Bergstra, Daniel Yamins, and David D. Cox.
2013. Making a science of model search: Hyperpa-
rameter optimization in hundreds of dimensions for
vision architectures. In Proceedings of ICML. pages
115‚Äì123.

Kyunghyun Cho, Bart Van Merri√´nboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

FranÃßcois Chollet. 2015. Keras. https://github.
com/fchollet/keras.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML. pages 160‚Äì167.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
r√©lien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. SwissCheese at SemEval-2016 Task 4: Sen-
timent classification using an ensemble of convolu-
tional neural networks with distant supervision. In
Proceedings of SemEval. pages 1124‚Äì1128.

Andrea Esuli and Fabrizio Sebastiani. 2015. Optimiz-
ing Text Quantifiers for Multivariate Loss Functions.
ACM Trans. Knowl. Discov. Data 9(4):27:1‚Äì27:27.

George Forman. 2008. Quantifying counts and costs
via classification. Data Mining and Knowledge Dis-
covery 17(2):164‚Äì206.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS. pages
1019‚Äì1027.

Wei Gao and Fabrizio Sebastiani. 2016. From classifi-
cation to quantification in tweet sentiment analysis.
Social Network Analysis and Mining 6(1).

Kevin Gimpel, Nathan Schneider, Brendan O‚ÄôConnor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of ACL. pages 42‚Äì47.

Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron C. Courville, and Yoshua Bengio. 2013.
Maxout networks. In Proceedings of ICML. pages
1319‚Äì1327.

Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and
J√ºrgen Schmidhuber. 2001. Gradient Flow in Re-
current Nets: The Difficulty of Learning Long-Term
Dependencies. A field guide to dynamical recurrent
neural networks. IEEE Press.

Sepp Hochreiter and J√ºrgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735‚Äì1780.

Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall PTR,
1st edition.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. Journal of Artificial Intelligence Research
50:723‚Äì762.

Yann LeCun, L√©on Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE
86(11):2278‚Äì2324.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS. pages 3111‚Äì3119.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. arXiv
preprint arXiv:1308.6242 .

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 Task 4: Sentiment analysis in Twitter. In Pro-
ceedings of SemEval. pages 1‚Äì18.

Elisavet Palogiannidi, Athanasia Kolovou, Fenia
Christopoulou, Filippos Kokkinos, Elias Iosif, Niko-
laos Malandrakis, Haris Papageorgiou, Shrikanth
Narayanan, and Alexandros Potamianos. 2016.
Tweester at SemEval-2016 Task 4: Sentiment anal-
ysis in Twitter using semantic-affective model adap-
tation. In Proceedings of SemEval. pages 155‚Äì163.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neu-
ral networks. In Proceedings of ICML. pages 1310‚Äì
1318.

Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, and others. 2011. Scikit-
learn: Machine learning in Python. Journal of Ma-
chine Learning Research 12:2825‚Äì2830.

753



Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global Vectors for
Word Representation. In Proceedings of EMNLP.
pages 1532‚Äì1543.

Christopher Potts. 2011. Sentiment Sym-
posium Tutorial: Tokenizing. http:
//sentiment.christopherpotts.net/
tokenizing.html.

Colin Raffel and Daniel PW Ellis. 2015. Feed-
forward networks with attention can solve some
long-term memory problems. arXiv preprint
arXiv:1512.08756 .

Tim Rockt√§schel, Edward Grefenstette, Karl Moritz
Hermann, Tom√°≈° KocÃåisk√°zÃásÃß, and Phil Blunsom.
2015. Reasoning about entailment with neural at-
tention. arXiv preprint arXiv:1509.06664 .

Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017.
SemEval-2017 Task 4: Sentiment Analysis in Twit-
ter. In Proceedings of SemEval. Vancouver, Canada.

Mickael Rouvier and Beno√Æt Favre. 2016. SENSEI-
LIF at SemEval-2016 Task 4: Polarity embedding
fusion for robust sentiment analysis. In Proceedings
of SemEval. pages 202‚Äì208.

Toby Segaran and Jeff Hammerbacher. 2009. Beautiful
Data: The Stories Behind Elegant Data Solutions.
"O‚ÄôReilly Media, Inc.".

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research 15(1):1929‚Äì1958.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting
Liu. 2015. Target-dependent sentiment classifi-
cation with long short term memory. CoRR,
abs/1512.01100 .

Theano Dev Team. 2016. Theano: A Python frame-
work for fast computation of mathematical expres-
sions. arXiv e-prints abs/1605.02688.

Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016. Attention-based LSTM for aspect-
level sentiment classification. In Proceedings of
EMNLP. pages 606‚Äì615.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
Proceedings of NAACL-HLT . pages 1480‚Äì1489.

754


