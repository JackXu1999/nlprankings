



















































The En-Ru Two-way Integrated Machine Translation System Based on Transformer


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 434–439
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

434

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

The En-Ru Two-way Integrated Machine Translation System Based on
Transformer

Anonymous ACL submission

Abstract

Machine translation is one of the most popular
areas in natural language processing. WMT
is a conference to assess the level of ma-
chine translation capabilities of organization-
s around the world, which is the evaluation
activity we participated in. In this review
we participated in a two-way translation track
from Russian to English and English to Rus-
sian. We used official training data, 38 mil-
lion parallel corpora, and 10 million monolin-
gual corpora. The overall framework we use
is the Transformer(Vaswani et al., 2017) neu-
ral machine translation model, supplemented
by data filtering, post-processing, reordering
and other related processing methods. The
BLEU(Papineni et al., 2002) value of our final
translation result from Russian to English is
38.7, ranking 5th, while from English to Rus-
sian is 27.8, ranking 10th.

1 Introduction

Neural machine translation has been widely used
in the field of machine translation, because it is
more accurate than statistical machine translation
in most cases. The proposed attention mechanis-
m brought a new revolution in the neural machine
translation, making the overall effect of translation
much better than before. Then, the Transformer
that makes full use of the attention mechanism ,
both in terms of performance and effectiveness.
Up to now, most of the work has been carried out
on Transformer, and its superiority has been wide-
ly recognized.

From the beginning of machine translation re-
search, there has been the development of two-
way translation between Russian and English. As
early as 1954, Georgetown University in the U-
nited States under the IBM company completed
the English-Russian machine translation experi-
ment with IBM-701 computer, which opened the

prelude of machine translation research. Dur-
ing the period, there are three core technologies,
rule-based machine translation, statistical machine
translation(Koehn et al., 2007) and neural machine
translation(Bahdanau et al., 2014), which contin-
ue to develop. However, as the application field-
s of machine translation become more and more
complex, the limitations of different technologies
begin to appear. Because of the more application
scenarios and the higher requirements for accura-
cy, the problem of model optimization appeared.

The translation between Russian and English
is extremely difficult because their linguistic fea-
tures are distinguished and the lexical composition
and grammatical structure of Russian are more
complicated than English. Early statistical ma-
chine translations were hoped to be implemented
through phrase-based methods(Marcu and Wong,
2002), including rule-based lexical, phrase analy-
sis systems, and related techniques for language
models and translation models. These methods
have solved the translation problem between Rus-
sian and English to a certain extent. However, at
the same time, there is still a problem that the time
cost is long and the translation result is not good
enough.

Therefore, the emergence of neural machine
translation has brought a new dawn for the trans-
lation between Russian and English. The basic
modeling framework for neural machine transla-
tion is an end-to-end sequence generation model,
a framework and method for transforming input
sequences into output sequences. There are two
points in the core part. One is to represent the in-
put sequence through the encoder, and the other
is to obtain the output sequence through the de-
coder. In addition, for machine translation, neu-
ral machine translation not only includes encod-
ing and decoding, but also uses RNN(Sutskever
et al., 2014) or other methods to encode sentence



435

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

pairs. It also introduces an additional mechanis-
m, the attention mechanism(Luong et al., 2015),
to help us to convert sequences. The translation
results thus obtained more expectations than be-
fore. Later, Transformer appeared, which great-
ly enhances neural machine translation in terms of
performance and effect.

This paper is based on Transformer, a neural
machine translation network structure, to develop
a two-way evaluation task between Russian and
English. Taking into account the language char-
acteristics of Russian and English, we have done
appropriate operations in data preprocessing, in-
cluding removing duplicates, deleting unreason-
able sentence pairs, lowercase and Latinization
operations, and judging sentence alignment prob-
lems, removing the parallel corpus with problems.
The filtered parallel corpus is then sent to the mod-
el for training and the training results are tested.
After getting the trained model, we start to consid-
er using the back-translation operation to augment
the data, continuing to filter the generated artificial
corpus, and put it into the model training together
with the original parallel corpus.

Finally, ensemble(Dietterich, 2000), average
and rerank(Shen et al., 2004) operations are imple-
mented on different models to improve the overall
performance of the translation system.

2 Background

Neural network machine translation is based on a
sequence-to-sequence overall structure consisting
of an encoder and a decoder. The encoder converts
the source language sentence into an intermediate
sequence result, and the decoder converts the in-
termediate sequence result into a target language
sentence. There is also the Attention mechanis-
m to help make the results perform better. In the
construction of the overall translation system, we
used a lot of excellent methods proposed by the
predecessors.

The basic model used here is Transformer. This
is a paper published by Google in 2017 titled At-
tention Is All You Need, an attention-based struc-
ture proposed to deal with sequence model relat-
ed issues, such as machine translation. Tradition-
al neural machine translation mostly uses RNN or
CNN as the model base of encoder-decoder, and
Google’s latest Attention-based Transformer mod-
el abandons the inherent formula and does not use
any CNN or RNN structure. The model works in

Figure 1: Transformer Structure

high-level parallel process, so training speed is al-
so extremely fast while improving translation per-
formance.

The structure of Transformer is shown in Fig-
ure 1. The model is divided into two parts: the
encoder and the decoder. The encoder is stacked
by six identical layers, each with two more sub-
layers. The first sub-layer is a long self-attention
mechanism, and the second sub-layer is a simple
fully connected feed forward network. A residu-
al connection is added outside the two layers, and
then layer normalization is performed. The output
dimensions of all sub-layers and embedding layers
of the model are dmodels; the decoder also stack-
s six identical layers. However, in addition to the
two layers in the encoder, the decoder also adds a
third sub-layer, as shown in the figure which also
uses the residual and layer normalization.

3 Experiment

For this evaluation task, we start from the data
preprocessing, through the data augmentation op-
eration, get the parallel corpus that needs to be
trained, input the Transformer model for training,
and test the training results, and finally ensemble
results according to the model generated by differ-
ent strategies, average and rerank operations, for
the best results. Next, the experimental content
will be elaborated separately. The overall experi-
mental process is shown in Figure 2.

3.1 Data Preprocessing

The first is data preprocessing, which is crucial for
the translation of the model. The sentences used
in this evaluation with data preprocessing method
to filter out include parallel sentence pairs with



436

3

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Raw Parallel Data Data Filter Base Transformer Train

Back TranslationRaw Mono Data

Average

Rerank

Ensemble

Fine Tuning

Bert Score

Figure 2: Project Process

high repetition rate, length mismatch and align-
ment problems. The amount of data given by
the official at the beginning was about 38 million
lines. After data filtering, 33 million lines were
left, and 5 million lines were deleted, accounting
for 0.13 of the original quantity. This result is in
line with expectations and acceptable.

The sentence with higher repetition rate has lit-
tle meaning in the training corpus, which increases
the burden of the model and affects the translation
effect, so it needs to perform deduplication opera-
tion. The method used here is to calculate the co-
sine distance of the SimHash value between each
row of data. When the difference is less than 0.2,
we believe it is repeated, and can be deleted. Be-
cause the amount of data is large and the global
deduplication time is too long, so here is a simple
calculation of three sentences before and after cur-
rent line, that is, using a window of size 7 to check
the sentence repetition, which also conforms to the
principle of local consistency.

From the practical experience and linguistic
knowledge, the length of sentences generated by
the two languages expressing the same meaning is
not too different, especially for Russian and En-
glish. So we also screened the length of the sen-
tence. In the experimental processing, we control
the ratio of the source language and the target lan-
guage length to 1/2-2/1, which means that the sen-
tence lengths of the two languages are not more
than twice as large. The length of the sentence is
calculated by the number of tokens. The parallel
sentence pairs thus obtained are also reasonable in
length ratio.

Sentence alignment is a very important factor
to measure the quality of parallel sentence pairs
from the perspective of sentence meaning. Dif-
ferent from the previous method, it needs to enter
sentences themselves and judge whether the data

pairs are reasonable according to the correspon-
dence between words in the two languages. The
gize++ tool(Gao and Vogel, 2008) is used here to
help check for data alignment issues. By reading
Russian-English vocabulary and Russian-English
parallel corpus information, creating a new dictio-
nary, building an IBM model 1, making EM algo-
rithm iteration, generating word alignment infor-
mation, and obtaining a calculated sentence pair
for each data. We generate alignment scores and
eliminate sentence pairs with scores less than the
threshold 10e-10 for better alignment data.

3.2 Back Translation

In the process of data augmentation, the back-
translation strategy(Edunov et al., 2018) plays a
crucial role. The auxiliary translation system from
the target language to the source language first
trains on the available parallel data and then us-
es to generate translations from the monolingual
corpus of the large target. The pairs of these trans-
lations and their corresponding reference targets
are then used as additional training data for the o-
riginal translation system. Using this strategy can
greatly increase the data required for training and
improve the translation effect of the model. In the
back-translation, we trained a translation model
from the target language to the source language
based on the existing corpus. By inputting the
target language corpus into the model, the corre-
sponding source language corpus can be obtained,
and the two are combined to obtain a new parallel
corpus.

The data set size of this trial is not too large and
it is stipulated that external parallel corpus expan-
sion cannot be used, so we use the back translation
method to increase the amount of training data.

Using back translation extended corpus in NMT
is a common data enhancement technique. We
trained a translation model from the target lan-
guage to the source language based on the existing
corpus. By inputting the target language corpus in-
to the model, the corresponding source language
corpus can be obtained and combined to get a new
parallel corpus.

External data is not allowed in this competition,
so we use the mono part of the original corpus to
generate para data. However, there is a problem
with this approach that there may be duplication
between the new parallel corpus and the original
corpus. To solve this problem, we added some



437

4

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

random noise on the decoding side to avoid this
situation.

We selected 10 million Russian and English
sentences respectively from the official monolin-
gual corpus as raw data for back translation op-
erations. The model obtained through the train-
ing of the existing parallel corpus translated this
part of the monolingual corpus and obtained 10
million pseudo-parallel corpora. Then, we filter
this part of the data according to the data filtering
and noise strategy mentioned in the previous sec-
tion. Finally, 8 million individual parallel corpora
are obtained and the filtered parallel corpus input
model is used for training operations.

3.3 Model training

Considering the hardware cost and time cost of the
experiment, the model we selected for this experi-
ment is the basic version of Transformer. The en-
coder and decoder have 6 sub-layers and the multi-
head attention mechanism has 8 headers. The
word vector size is 512. Guaranteed to get the best
results in a limited time in a laboratory environ-
ment. The development environment for evalua-
tion is MXNET, which is the deep learning library
that Amazon chose.

The input model needs to be further processed
before training, including generating the corre-
sponding token for the sentence. The tool used
here is the commonly used tokenizer.perl, which
can separate the words and punctuation in English
and convert the special symbols to keep the same
symbol. Russian is the same. In addition, the
BPE method is needed to generate the subword
vocabulary to reduce the vocabulary size during
the model training and improve the performance
of the model.

After the above processing, the data can be di-
vided to obtain a training set, a test set and a verifi-
cation set, the training set is used for model train-
ing, the verification set is used for performance
detection in the training process, and the test set
is used for evaluating the result of model trained.

For the evaluation task, the following experi-
ment was designed:

1. Baseline Model
Use the official 38 million parallel corpus with-

out screening and direct it into the model for train-
ing and testing. The results of the base model are
used to compare with different strategy results and
generate reverse translation data to extend the cor-

pus and continue training. The purpose is to main-
tain the generalization ability and robustness of the
model to the greatest extent, and to provide refer-
ence for other model training results.

2. Filter Model
The data preprocessing operation is used to

screen the official data and the ideal training cor-
pus is obtained. The 33 million filtered paral-
lel sentences are trained to obtain a data filtering
model. Because the quality of the data used for
training is higher, the effect of model translation is
better than the basic model.

3. Back Model
10 million is extracted from the official mono-

lingual corpus as the source language input to the
baseline model for translation, and the artificial
parallel corpus based on the baseline model trans-
lation is obtained. Since the effect of the baseline
model is not good enough, the generated corpus
needs to be further filtered, and the method is also
the data preprocessing operation mentioned above.
After screening, we got about 8 million good qual-
ity artificial data and then combined the artificial
parallel corpus with the previously filtered official
parallel corpus and input them into the model for
training. Then we got the Back translation mod-
el. Because artificial corpus has been added, the
translation effect and the robustness is improved.

4. Fine-tuning Model
Fine-tuning a trained model using small-scale

corpus is a commonly used strategy in the field
of machine learning. It can make the model more
sensitive to specific domain scenarios, thus reflect-
ing better results. Here, we select a corpus with
much similarity to the test set from the training set
to fine-tune the trained model. The similarity s-
cores between the test corpus and the training cor-
pus are sorted and ranked. Then the parallel sen-
tence pairs with higher scores are found and the
corpus is extracted as a fine-tuning corpus. In this
way, about 5,000 pieces of data are obtained and
this part of the corpus is input into the previous-
ly trained model to obtain the result of fine-tuning
the model, so that it can perform better on the test
set.

5. Ensemble Model
Ensemble is a method that combines the results

of multiple models. The purpose of this is to com-
plement the advantages of different models, make
up for the problems that fall into the local opti-
mum and get the results of the machine translation



438

5

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

421

422

423

424

425

426

427

428

429

430

431

432

433

434

435

436

437

438

439

440

441

442

443

444

445

446

447

448

449

450

451

452

453

454

455

456

457

458

459

460

461

462

463

464

465

466

467

468

469

470

471

472

473

474

475

476

477

478

479

480

481

482

483

484

485

486

487

488

489

490

491

492

493

494

495

496

497

498

499

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

model with better comprehensive effects. For the
sake of simplicity, only different initialization ran-
dom seed parameters are set for the same model.
So training of multiple models is performed, gen-
erally two or three models, and finally the results
of all models are subjected to ensemble operation.
By composing and complementing multiple mod-
els, we obtain the comprehensive optimal results
of data translation.

6. Average Model
The Average operation is similar in thought to

ensemble, but it operates on different training pa-
rameters of the same model. The parameters in
these training results are subjected to the average
operation, and a set of training results are com-
prehensively obtained from the best training pa-
rameters of the single model. Top 5 of the model
training results is selected for averaging to prevent
a certain result from falling into the local optimum
and a plurality of parameters are integrated to ob-
tain an averaged optimal solution. This results in
the best combination of different training parame-
ters in the same model, thereby improving the per-
formance of a single model.

7. Nbest and Rerank Model
Extracting only one of the highest-scoring state-

ments from the translation results of the model as
an output is not necessarily the best result. So this
strategy can be used to extract the best three from
each translation model result as a candidate set.
Then use some rules to rerank and get the best
one as the output result. The translated content
thus obtained is the comprehensive output of mul-
tiple results of each model, which is theoretical-
ly optimal. The rules used here include weighted
summation of beam search score and the language
model scores. The first one is based on the beam
score returned during decoding, but different mod-
els have different performances, so it is difficult to
sort under a uniform metric. So we introduced d-
ifferent weights for different models. Using beam
score weight as the final score for each transla-
tion result, the final result was obtained by screen-
ing. The second one gives scores of the generated
translations using the pre-trained language model.
They are judged from the linguistics itself and the
sentences with the highest scores are selected. The
final result is an output that combines the highest
scores of the two methods described above.

The above models also had different batch sizes,
comparison of the number of graphics cards and

Name Pair Bleu Improve
base-re RU-EN 34.8 0
filter-re RU-EN 36.1 +1.3
average-filter-re RU-EN 36.2 +1.4
rerank-re RU-EN 37.5 +2.7
vote-re RU-EN 36.1 +1.3
base-er EN-RU 25.6 0
filter-er EN-RU 26.6 +1.0
average-filter-er EN-RU 26.8 +1.2
rerank-er EN-RU 27.8 +2.2
vote-er EN-RU 26.5 +0.9

Table 1: Experiment Result.

vocabulary sizes in the training process. We ex-
tracted them for the optimal results. Finally, the
output is simply post-processed. In order to com-
ply with normal text habits. However, due to the
limitations of time and hardware resources, not ev-
ery experiment has been refined and detailed total-
ly, so there is still improvement of results in the
future.

3.4 Results Analysis
The above experimental results are presented in
the Table 1. It should be noted that only the bet-
ter and more complete results in the experiment
are given here. We can see that the BLEU values
of the Baseline Model form English to Russian is
25.9, while from Russian to English is 35.2, re-
spectively as a benchmark, to provide reference
for the following models. The results after filtra-
tion are 27.0 and 36.5, which has 1.0 or so im-
provement over baseline. The results obtained by
the Average strategy are 27.2 and 36.5, which is
basically no improvement. The strategy of obtain-
ing nbest for translation results and reranking ac-
cording to the reference rules worked very well,
which got 28.2 and 38.0, from 2 to 3 points higher
than baseline. Back Model, Fine-tuning and En-
semble strategies are not very completed in detail,
so they are not shown here.

4 Conclusion

In this evaluation task, we established a Russian-
English two-way machine translation system
based on Transformer. Through data preprocess-
ing, model training, data post-processing and other
optimization strategies, the evaluation results were
finally from English to Russian BLEU value 28.2,
while from Russian to English 38.0, which was



439

6

500

501

502

503

504

505

506

507

508

509

510

511

512

513

514

515

516

517

518

519

520

521

522

523

524

525

526

527

528

529

530

531

532

533

534

535

536

537

538

539

540

541

542

543

544

545

546

547

548

549

550

551

552

553

554

555

556

557

558

559

560

561

562

563

564

565

566

567

568

569

570

571

572

573

574

575

576

577

578

579

580

581

582

583

584

585

586

587

588

589

590

591

592

593

594

595

596

597

598

599

ACL 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

about 3 points higher than the baseline result. In
the final list, we got 5th in Ru-En, and 10th in En-
Ru. Good results have been obtained in limited
time and hardware resources, which is also in line
with the industry’s demands for service construc-
tion. In the whole experiment process, we also
learned a lot of experience in data processing and
experimental design, which will be of great help in
later research and study. We will continue to im-
prove the previous experiments, strive to get better
results, and see what rankings can eventually be
achieved, in preparation for the next year.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint arX-
iv:1409.0473.

Thomas G Dietterich. 2000. Ensemble methods in ma-
chine learning. In International workshop on multi-
ple classifier systems, pages 1–15. Springer.

Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at
scale. arXiv preprint arXiv:1808.09381.

Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. Software engineer-
ing, testing, and quality assurance for natural lan-
guage processing, pages 49–57.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the associ-
ation for computational linguistics companion vol-
ume proceedings of the demo and poster sessions,
pages 177–180.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Daniel Marcu and Daniel Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2002).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics: HLT-
NAACL 2004.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in neural information processing sys-
tems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.


