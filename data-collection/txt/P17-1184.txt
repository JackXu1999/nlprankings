



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2016–2027
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1184

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2016–2027
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1184

From Characters to Words to in Between: Do We Capture Morphology?

Clara Vania and Adam Lopez
Institute for Language, Cognition and Computation

School of Informatics
University of Edinburgh

c.vania@ed.ac.uk, alopez@inf.ed.ac.uk

Abstract

Words can be represented by composing
the representations of subword units such
as word segments, characters, and/or char-
acter n-grams. While such representations
are effective and may capture the mor-
phological regularities of words, they have
not been systematically compared, and it
is not understood how they interact with
different morphological typologies. On a
language modeling task, we present ex-
periments that systematically vary (1) the
basic unit of representation, (2) the com-
position of these representations, and (3)
the morphological typology of the lan-
guage modeled. Our results extend previ-
ous findings that character representations
are effective across typologies, and we find
that a previously unstudied combination
of character trigram representations com-
posed with bi-LSTMs outperforms most
others. But we also find room for improve-
ment: none of the character-level models
match the predictive accuracy of a model
with access to true morphological analy-
ses, even when learned from an order of
magnitude more data.

1 Introduction

Continuous representations of words learned by
neural networks are central to many NLP tasks
(Cho et al., 2014; Chen and Manning, 2014; Dyer
et al., 2015). However, directly mapping a fi-
nite set of word types to a continuous representa-
tion has well-known limitations. First, it makes
a closed vocabulary assumption, enabling only
generic out-of-vocabulary handling. Second, it
cannot exploit systematic functional relationships
in learning. For example, cat and cats stand in the

same relationship as dog and dogs. While this re-
lationship might be discovered for these specific
frequent words, it does not help us learn that the
same relationship also holds for the much rarer
words sloth and sloths.

These functional relationships reflect the fact
that words are composed from smaller units of
meaning, or morphemes. For instance, cats con-
sists of two morphemes, cat and -s, with the latter
shared by the words dogs and tarsiers. Modeling
this effect is crucial for languages with rich mor-
phology, where vocabulary sizes are larger, many
more words are rare, and many more such func-
tional relationships exist. Hence, some models
produce word representations as a function of sub-
word units obtained from morphological segmen-
tation or analysis (Luong et al., 2013; Botha and
Blunsom, 2014; Cotterell and Schütze, 2015). A
downside of these models is that they depend on
morphological segmenters or analyzers.

Morphemes typically have similar orthographic
representations across words. For example, the
morpheme -s is realized as -es in finches. Since
this variation is limited, the general relationship
between morphology and orthography can be ex-
ploited by composing the representations of char-
acters (Ling et al., 2015; Kim et al., 2016), char-
acter n-grams (Sperr et al., 2013; Wieting et al.,
2016; Bojanowski et al., 2016; Botha and Blun-
som, 2014), bytes (Plank et al., 2016; Gillick
et al., 2016), or combinations thereof (Santos and
Zadrozny, 2014; Qiu et al., 2014). These mod-
els are compact, can represent rare and unknown
words, and do not require morphological analyz-
ers. They raise a provocative question: Does NLP
benefit from models of morphology, or can they be
replaced entirely by models of characters?

The relative merits of word, subword. and
character-level models are not fully understood
because each new model has been compared on

2016

https://doi.org/10.18653/v1/P17-1184
https://doi.org/10.18653/v1/P17-1184


different tasks and datasets, and often compared
against word-level models. A number of questions
remain open:

1. How do representations based on morphemes
compare with those based on characters?

2. What is the best way to compose subword
representations?

3. Do character-level models capture morphol-
ogy in terms of predictive utility?

4. How do different representations interact
with languages of different morphological ty-
pologies?

The last question is raised by Bender (2013):
languages are typologically diverse, and the
behavior of a model on one language may
not generalize to others. Character-level mod-
els implicitly assume concatenative morphology,
but many widely-spoken languages feature non-
concatenative morphology, and it is unclear how
such models will behave on these languages.

To answer these questions, we performed a sys-
tematic comparison across different models for the
simple and ubiquitous task of language model-
ing. We present experiments that vary (1) the type
of subword unit; (2) the composition function;
and (3) morphological typology. To understand
the extent to which character-level models capture
true morphological regularities, we present ora-
cle experiments using human morphological an-
notations instead of automatic morphological seg-
ments. Our results show that:

1. For most languages, character-level represen-
tations outperform the standard word repre-
sentations. Most interestingly, a previously
unstudied combination of character trigrams
composed with bi-LSTMs performs best on
the majority of languages.

2. Bi-LSTMs and CNNs are more effective
composition functions than addition.

3. Character-level models learn functional re-
lationships between orthographically similar
words, but don’t (yet) match the predictive
accuracy of models with access to true mor-
phological analyses.

4. Character-level models are effective across a
range of morphological typologies, but or-
thography influences their effectiveness.

word tries
morphemes try+s
morphs tri+es
morph. analysis try+VB+3rd+SG+Pres

Table 1: The morphemes, morphs, and morpho-
logical analysis of tries.

2 Morphological Typology

A morpheme is the smallest unit of meaning in
a word. Some morphemes express core meaning
(roots), while others express one or more depen-
dent features of the core meaning, such as per-
son, gender, or aspect. A morphological analysis
identifies the lemma and features of a word. A
morph is the surface realization of a morpheme
(Morley, 2000), which may vary from word to
word. These distinctions are shown in Table 1.

Morphological typology classifies languages
based on the processes by which morphemes are
composed to form words. While most languages
will exhibit a variety of such processes, for any
given language, some processes are much more
frequent than others, and we will broadly identify
our experimental languages with these processes.

When morphemes are combined sequentially,
the morphology is concatenative. However,
morphemes can also be composed by non-
concatenative processes. We consider four
broad categories of both concatenative and non-
concatenative processes in our experiments.

Fusional languages realize multiple features
in a single concatenated morpheme. For exam-
ple, English verbs can express number, person,
and tense in a single morpheme:

wanted (English)
want + ed

want + VB+1st+SG+Past
Agglutinative languages assign one feature

per morpheme. Morphemes are concatenated to
form a word and the morpheme boundaries are
clear. For example (Haspelmath, 2010):

okursam (Turkish)
oku+r+sa+m

“read”+AOR+COND+1SG
Root and Pattern Morphology forms words

by inserting consonants and vowels of dependent
morphemes into a consonantal root based on a
given pattern. For example, the Arabic root ktb
(“write”) produces (Roark and Sproat, 2007):

katab “wrote” (Arabic)

2017



takaatab “wrote to each other” (Arabic)
Reduplication is a process where a word form

is produced by repeating part or all of the root to
express new features. For example:

anak “child” (Indonesian)
anak-anak “children” (Indonesian)

buah “fruit” (Indonesian)
buah-buahan “various fruits” (Indonesian)

3 Representation Models

We compare ten different models, varying sub-
word units and composition functions that have
commonly been used in recent work, but evalu-
ated on various different tasks (Table 2). Given
word w, we compute its representation w as:

w = f(Ws, σ(w)) (1)

where σ is a deterministic function that returns a
sequence of subword units; Ws is a parameter ma-
trix of representations for the vocabulary of sub-
word units; and f is a composition function which
takes σ(w) and Ws as input and returns w. All of
the representations that we consider take this form,
varying only in f and σ.

3.1 Subword Units
We consider four variants of σ in Equation 1,
each returning a different type of subword unit:
character, character trigram, or one of two types
of morph. Morphs are obtained from Morfes-
sor (Smit et al., 2014) or a word segmentation
based on Byte Pair Encoding (BPE; Gage (1994)),
which has been shown to be effective for handling
rare words in neural machine translation (Sennrich
et al., 2016). BPE works by iteratively replac-
ing frequent pairs of characters with a single un-
used character. For Morfessor, we use default
parameters while for BPE we set the number of
merge operations to 10,000.1 When we segment
into character trigrams, we consider all trigrams in
the word, including those covering notional begin-
ning and end of word characters, as in Sperr et al.
(2013). Example output of σ is shown in Table 3.

3.2 Composition Functions
We use three variants of f in Eq. 1. The first con-
structs the representation w of word w by adding

1BPE takes a single parameter: the number of merge op-
erations. We tried different parameter values (1k, 10k, 100k)
and manually examined the resulting segmentation on the En-
glish dataset. Qualitatively, 10k gave the most plausible seg-
mentation and we used this setting across all languages.

the representations of its subwords s1, . . . , sn =
σ(w), where the representation of si is vector si.

w =
n∑

i=1

si (2)

The only subword unit that we don’t compose by
addition is characters, since this will produce the
same representation for many different words.

Our second composition function is a bidi-
rectional long-short-term memory (bi-LSTM),
which we adapt based on its use in the character-
level model of Ling et al. (2015) and its
widespread use in NLP generally. Given si and
the previous LSTM hidden state hi−1, an LSTM
(Hochreiter and Schmidhuber, 1997) computes the
following outputs for the subword at position i:

hi = LSTM(si,hi−1) (3)

ŝi+1 = g(VT · hi) (4)

where ŝi+1 is the predicted target subword, g is the
softmax function and V is a weight matrix.

A bi-LSTM (Graves et al., 2005) combines the
final state of an LSTM over the input sequence
with one over the reversed input sequence. Given
the hidden state produced from the final input of
the forward LSTM, hfwn and the hidden state pro-
duced from the final input of the backward LSTM
hbw0 , we compute the word representation as:

wt = Wf · hfwn + Wb · hbw0 + b (5)

where Wf , Wb, and b are parameter matrices and
hfwn and hbw0 are forward and backward LSTM
states, respectively.

The third composition function is a convolu-
tional neural network (CNN) with highway lay-
ers, as in Kim et al. (2016). Let c1, . . . , ck be the
sequence of characters of word w. The character
embedding matrix is C ∈ Rd×k, where the i-th
column corresponds to the embeddings of ci. We
first apply a narrow convolution between C and a
filter F ∈ Rd×n of width n to obtain a feature map
f ∈ Rk−n+1. In particular, the computation of the
j-th element of f is defined as

f [j] = tanh(〈C[∗, j : j + n− 1],F〉+ b) (6)

where 〈A,B〉 = Tr(ABT ) is the Frobenius in-
ner product and b is a bias. The CNN model ap-
plies filters of varying width, representing features

2018



Models Subword Unit(s) Composition Function
Sperr et al. (2013) words, character n-grams addition
Luong et al. (2013) morphs (Morfessor) recursive NN
Botha and Blunsom (2014) words, morphs (Morfessor) addition
Qiu et al. (2014) words, morphs (Morfessor) addition
Santos and Zadrozny (2014) words, characters CNN
Cotterell and Schütze (2015) words, morphological analyses addition
Sennrich et al. (2016) morphs (BPE) none
Kim et al. (2016) characters CNN
Ling et al. (2015) characters bi-LSTM
Wieting et al. (2016) character n-grams addition
Bojanowski et al. (2016) character n-grams addition
Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN
Miyamoto and Cho (2016) words, characters bi-LSTM
Rei et al. (2016) words, characters bi-LSTM
Lee et al. (2016) characters CNN
Kann and Schütze (2016) characters, morphological analyses none
Heigold et al. (2017) words, characters bi-LSTM, CNN

Table 2: Summary of previous work on representing words through compositions of subword units.

Unit Output of σ(wants)
Morfessor ˆwant, s$
BPE ˆw, ants$
char-trigram ˆwa, wan, ant, nts ts$
character ˆ, w, a, n, t, s, $
analysis want+VB, +3rd, +SG, +Pres

Table 3: Input representations for wants.

of character n-grams. We then calculate the max-
over-time of each feature map.

yj = max
j

f [j] (7)

and concatenate them to derive the word represen-
tation wt = [y1, . . . , ym], where m is the number
of filters applied. Highway layers allow some di-
mensions of wt to be carried or transformed. Since
it can learn character n-grams directly, we only use
the CNN with character input.

3.3 Language Model

We use language models (LM) because they are
simple and fundamental to many NLP applica-
tions. Given a sequence of text s = w1, . . . , wT ,
our LM computes the probability of s as:

P (w1, . . . , wT ) =
T∏

t=1

P (yt|w1, . . . , wt−1) (8)

Figure 1: Our LSTM-LM architecture.

where yt = wt if wt is in the output vocabulary
and yt = UNK otherwise.

Our language model is an LSTM variant of
recurrent neural network language (RNN) LM
(Mikolov et al., 2010). At time step t, it receives
input wt and predicts yt+1. Using Eq. 1, it first
computes representation wt of wt. Given this rep-
resentation and previous state ht−1, it produces a
new state ht and predicts yt+1:

ht = LSTM(wt,ht−1) (9)

ŷt+1 = g(VT · ht) (10)

where g is a softmax function over the vocabulary
yielding the probability in Equation 8. Note that
this design means that we can predict only words

2019



Typology Languages #tokens #types

Fusional
Czech 1.2M 125.4K
English 1.2M 81.1K
Russian 0.8M 103.5K

Agglutinative
Finnish 1.2M 188.4K
Japanese 1.2M 59.2K
Turkish 0.6M 126.2K

Root&Pattern
Arabic 1.4M 137.5K
Hebrew 1.1M 104.9K

Reduplication
Indonesian 1.2M 76.5K
Malaysian 1.2M 77.7K

Table 4: Statistics of our datasets.

from a finite output vocabulary, so our models dif-
fer only in their representation of context words.
This design makes it possible to compare language
models using perplexity, since they have the same
event space, though open vocabulary word predic-
tion is an interesting direction for future work.

The complete architecture of our system is
shown in Figure 1, showing segmentation function
σ and composition function f from Equation 1.

4 Experiments

We perform experiments on ten languages (Ta-
ble 4). We use datasets from Ling et al. (2015)
for English and Turkish. For Czech and Russian
we use Universal Dependencies (UD) v1.3 (Nivre
et al., 2015). For other languages, we use prepro-
cessed Wikipedia data (Al-Rfou et al., 2013).2 For
each dataset, we use approximately 1.2M tokens
to train, and approximately 150K tokens each for
development and testing. Preprocessing involves
lowercasing (except for character models) and re-
moving hyperlinks.

To ensure that we compared models and not im-
plementations, we reimplemented all models in a
single framework using Tensorflow (Abadi et al.,
2015).3 We use a common setup for all experi-
ments based on that of Ling et al. (2015), Kim
et al. (2016), and Miyamoto and Cho (2016). In
preliminary experiments, we confirmed that our
models produced similar patterns of perplexities
for the reimplemented word and character LSTM

2The Arabic and Hebrew dataset are unvocalized.
Japanese mixes Kanji, Katakana, Hiragana, and Latin charac-
ters (for foreign words). Hence, a Japanese character can cor-
respond to a character, syllable, or word. The preprocessed
dataset is already word-segmented.

3Our implementation of these models can be found at
https://github.com/claravania/subword-lstm-lm

models of Ling et al. (2015). Even following de-
tailed discussion with Ling (p.c.), we were unable
to reproduce their perplexities exactly—our En-
glish reimplementation gives lower perplexities;
our Turkish higher—but we do reproduce their
general result that character bi-LSTMs outperform
word models. We suspect that different prepro-
cessing and the stochastic learning explains dif-
ferences in perplexities. Our final model with bi-
LSTMs composition follows Miyamoto and Cho
(2016) as it gives us the same perplexity results
for our preliminary experiments on the Penn Tree-
bank dataset (Marcus et al., 1993), preprocessed
by Mikolov et al. (2010).

4.1 Training and Evaluation

Our LSTM-LM uses two hidden layers with 200
hidden units and representation vectors for words,
characters, and morphs all have dimension 200.
All parameters are initialized uniformly at random
from -0.1 to 0.1, trained by stochastic gradient de-
scent with mini-batch size of 32, time steps of
20, for 50 epochs. To avoid overfitting, we ap-
ply dropout with probability 0.5 on the input-to-
hidden layer and all of the LSTM cells (includ-
ing those in the bi-LSTM, if used). For all models
which do not use bi-LSTM composition, we start
with a learning rate of 1.0 and decrease it by half if
the validation perplexity does not decrease by 0.1
after 3 epochs. For models with bi-LSTMs com-
position, we use a constant learning rate of 0.2 and
stop training when validation perplexity does not
improve after 3 epochs. For the character CNN
model, we use the same settings as the small model
of Kim et al. (2016).

To make our results comparable to Ling et al.
(2015), for each language we limit the output vo-
cabulary to the most frequent 5,000 training words
plus an unknown word token. To learn to predict
unknown words, we follow Ling et al. (2015): in
training, words that occur only once are stochasti-
cally replaced with the unknown token with prob-
ability 0.5. To evaluate the models, we compute
perplexity on the test data.

5 Results and Analysis

Table 5 presents our main results. In six of ten
languages, character-trigram representations com-
posed with bi-LSTMs achieve the lowest perplex-
ities. As far as we know, this particular model
has not been tested before, though it is similar

2020



Language word
character char trigrams BPE Morfessor

%imp
bi-lstm CNN add bi-lstm add bi-lstm add bi-lstm

Czech 41.46 34.25 36.60 42.73 33.59 49.96 33.74 47.74 36.87 18.98
English 46.40 43.53 44.67 45.41 42.97 47.51 43.30 49.72 49.72 7.39
Russian 34.93 28.44 29.47 35.15 27.72 40.10 28.52 39.60 31.31 20.64
Finnish 24.21 20.05 20.29 24.89 18.62 26.77 19.08 27.79 22.45 23.09
Japanese 98.14 98.14 91.63 101.99 101.09 126.53 96.80 111.97 99.23 6.63
Turkish 66.97 54.46 55.07 50.07 54.23 59.49 57.32 62.20 62.70 25.24
Arabic 48.20 42.02 43.17 50.85 39.87 50.85 42.79 52.88 45.46 17.28
Hebrew 38.23 31.63 33.19 39.67 30.40 44.15 32.91 44.94 34.28 20.48
Indonesian 46.07 45.47 46.60 58.51 45.96 59.17 43.37 59.33 44.86 5.86
Malay 54.67 53.01 50.56 68.51 50.74 68.99 51.21 68.20 52.50 7.52

Table 5: Language model perplexities on test. The best model for each language is highlighted in bold
and the improvement of this model over the word-level model is shown in the final column.

to (but more general than) the model of Sperr
et al. (2013). We can see that the performance
of character, character trigrams, and BPE are very
competitive. Composition by bi-LSTMs or CNN
is more effective than addition, except for Turk-
ish. We also observe that BPE always outperforms
Morfessor, even for the agglutinative languages.
We now turn to a more detailed analysis by mor-
phological typology.

Fusional languages. For these languages,
character trigrams composed with bi-LSTMs
outperformed all other models, particularly for
Czech and Russian (up to 20%), which is unsur-
prising since both are morphologically richer than
English.

Agglutinative languages. We observe differ-
ent results for each language. For Finnish, char-
acter trigrams composed with bi-LSTMs achieves
the best perplexity. Surprisingly, for Turkish char-
acter trigrams composed via addition is best and
addition also performs quite well for other rep-
resentations, potentially useful since the addition
function is simpler and faster than bi-LSTMs. We
suspect that this is due to the fact that Turk-
ish morphemes are reasonably short, hence well-
approximated by character trigrams. For Japanese,
we improvements from character models are more
modest than in other languages.

Root and Pattern. For these languages, char-
acter trigrams composed with bi-LSTMs also
achieve the best perplexity. We had won-
dered whether CNNs would be more effective
for root-and-pattern morphology, but since these
data are unvocalized, it is more likely that non-
concatenative effects are minimized, though we do

still find morphological variants with consonan-
tal inflections that behave more like concatenation.
For example, maktab (root:ktb) is written as mktb.
We suspect this makes character trigrams quite ef-
fective since they match the tri-consonantal root
patterns among words which share the same root.

Reduplication. For Indonesian, BPE morphs
composed with bi-LSTMs model obtain the best
perplexity. For Malay, the character CNN out-
performs other models. However, these improve-
ments are small compared to other languages.
This likely reflects that Indonesian and Malay are
only moderately inflected, where inflection in-
volves both concatenative and non-concatenative
processes.

5.1 Effects of Morphological Analysis

In the experiments above, we used unsupervised
morphological segmentation as a proxy for mor-
phological analysis (Table 3). However, as dis-
cussed in Section 2, this is quite approximate, so
it is natural to wonder what would happen if we
had the true morphological analysis. If character-
level models are powerful enough to capture the
effects of morphology, then they should have the
predictive accuracy of a model with access to this
analysis. To find out, we conducted an oracle
experiment using the human-annotated morpho-
logical analyses provided in the UD datasets for
Czech and Russian, the only languages in our set
for which these analyses were available. In these
experiments we treat the lemma and each morpho-
logical feature as a subword unit.

The results (Table 6) show that bi-LSTM com-
position of these representations outperforms all

2021



Languages Addition bi-LSTM
Czech 51.8 30.07

Russian 41.82 26.44

Table 6: Perplexity results using hand-annotated
morphological analyses (cf. Table 5).

other models for both languages. These results
demonstrate that neither character representations
nor unsupervised segmentation is a perfect re-
placement for manual morphological analysis, at
least in terms of predictive accuracy. In light of
character-level results, they imply that current un-
supervised morphological analyzers are poor sub-
stitutes for real morphological analysis.

However, we can obtain much more unanno-
tated than annotated data, and we might guess
that the character-level models would outperform
those based on morphological analyses if trained
on larger data. To test this, we ran experiments
that varied the training data size on three represen-
tation models: word, character-trigram bi-LSTM,
and character CNN. Since we want to see how
much training data is needed to reach perplexity
obtained using annotated data, we use the same
output vocabulary derived from the original train-
ing. While this makes it possible to compare per-
plexities across models, it is unfavorable to the
models trained on larger data, which may focus on
other words. This is a limitation of our experimen-
tal setup, but does allow us to draw some tentative
conclusions. As shown in Table 7, a character-
level model trained on an order of magnitude more
data still does not match the predictive accuracy of
a model with access to morphological analysis.

5.2 Automatic Morphological Analysis

The oracle experiments show promising results if
we have annotated data. But these annotations are
expensive, so we also investigated the use of auto-
matic morphological analysis. We obtained analy-
ses for Arabic with the MADAMIRA (Pasha et al.,
2014).4 As in the experiment using annotations,
we treated each morphological feature as a sub-
word unit. The resulting perplexities of 71.94 and
42.85 for addition and bi-LSTMs, respectively, are
worse than those obtained with character trigrams
(39.87), though it approaches the best perplexities.

4We only experimented with Arabic since MADAMIRA
disambiguates words in contexts; most other analyzers we
found did not do this, and would require additional work to
add disambiguation.

#tokens word
char trigram char

bi-LSTM CNN
1M 39.69 32.34 35.15
2M 37.59 36.44 35.58
3M 36.71 35.60 35.75
4M 35.89 32.68 35.93
5M 35.20 34.80 37.02
10M 35.60 35.82 39.09

Table 7: Perplexity results on the Czech develop-
ment data, varying training data size. Perplexity
using ~1M tokens annotated data is 28.83.

5.3 Targeted Perplexity Results

A difficulty in interpreting the results of Table 5
with respect to specific morphological processes
is that perplexity is measured for all words. But
these processes do not apply to all words, so it
may be that the effects of specific morphological
processes are washed out. To get a clearer picture,
we measured perplexity for only specific subsets
of words in our test data: specifically, given tar-
get word wi, we measure perplexity of word wi+1.
In other words, we analyze the perplexities when
the inflected words of interest are in the most re-
cent history, exploiting the recency bias of our
LSTM-LM. This is the perplexity most likely to
be strongly affected by different representations,
since we do not vary representations of the pre-
dicted word itself.

We look at several cases: nouns and verbs in
Czech and Russian, where word classes can be
identified from annotations, and reduplication in
Indonesian, which we can identify mostly auto-
matically. For each analysis, we also distinguish
between frequent cases, where the inflected word
occurs more than ten times in the training data, and
rare cases, where it occurs fewer than ten times.
We compare only bi-LSTM models.

For Czech and Russian, we again use the UD
annotation to identify words of interest. The re-
sults (Table 8), show that manual morphologi-
cal analysis uniformly outperforms other subword
models, with an especially strong effect for Czech
nouns, suggesting that other models do not cap-
ture useful predictive properties of a morpholog-
ical analysis. We do however note that character
trigrams achieve low perplexities in most cases,
similar to overall results (Table 5). We also ob-
serve that the subword models are more effective
for rare words.

2022



Inflection Model all frequent rare
Czech word 61.21 56.84 72.96
nouns characters 51.01 47.94 59.01

char-trigrams 50.34 48.05 56.13
BPE 53.38 49.96 62.81
morph. analysis 40.86 40.08 42.64

Czech word 81.37 74.29 99.40
verbs characters 70.75 68.07 77.11

char-trigrams 65.77 63.71 70.58
BPE 74.18 72.45 78.25
morph. analysis 59.48 58.56 61.78

Russian word 45.11 41.88 48.26
nouns characters 37.90 37.52 38.25

char-trigrams 36.32 34.19 38.40
BPE 43.57 43.67 43.47
morph. analysis 31.38 31.30 31.50

Russian word 56.45 47.65 69.46
verbs characters 45.00 40.86 50.60

char-trigrams 42.55 39.05 47.17
BPE 54.58 47.81 64.12
morph. analysis 41.31 39.8 43.18

Table 8: Average perplexities of words that occur
after nouns and verbs. Frequent words occur more
than ten times in the training data; rare words oc-
cur fewer times than this. The best perplexity is in
bold while the second best is underlined.

For Indonesian, we exploit the fact that the hy-
phen symbol ‘-’ typically separates the first and
second occurrence of a reduplicated morpheme, as
in the examples of Section 2. We use the presence
of word tokens containing hyphens to estimate the
percentage of those exhibiting reduplication. As
shown in Table 9, the numbers are quite low.

Table 10 shows results for reduplication. In
contrast with the overall results, the BPE bi-LSTM
model has the worst perplexities, while character
bi-LSTM has the best, suggesting that these mod-
els are more effective for reduplication.

Looking more closely at BPE segmentation of
reduplicated words, we found that only 6 of 252
reduplicated words have a correct word segmenta-
tion, with the reduplicated morpheme often com-
bining differently with the notional start-of-word
or hyphen character. One the other hand BPE cor-
rectly learns 8 out of 9 Indonesian prefixes and 4
out of 7 Indonesian suffixes.5 This analysis sup-
ports our intuition that the improvement from BPE
might come from its modeling of concatenative
morphology.

5.4 Qualitative Analysis

Table 11 presents nearest neighbors under co-
sine similarity for in-vocabulary, rare, and out-of-

5We use Indonesian affixes listed in Larasati et al. (2011)

Language type-level (%) token-level (%)
Indonesian 1.10 2.60

Malay 1.29 2.89

Table 9: Percentage of full reduplication on the
type and token level.

Model all frequent rare
word 101.71 91.71 156.98
characters 99.21 91.35 137.42
BPE 117.2 108.86 156.81

Table 10: Average perplexities of words that occur
after reduplicated words in the test set.

vocabulary (OOV) words.6 For frequent words,
standard word embeddings are clearly superior for
lexical meaning. Character and morph representa-
tions tend to find words that are orthographically
similar, suggesting that they are better at model-
ing dependent than root morphemes. The same
pattern holds for rare and OOV words. We sus-
pect that the subword models outperform words
on language modeling because they exploit affixes
to signal word class. We also noticed similar pat-
terns in Japanese.

We analyze reduplication by querying redupli-
cated words to find their nearest neighbors using
the BPE bi-LSTM model. If the model were sensi-
tive to reduplication, we would expect to see mor-
phological variants of the query word among its
nearest neighbors. However, from Table 12, this
is not so. With the partially reduplicated query
berlembah-lembah, we do not find the lemma lem-
bah.

6 Conclusion

We presented a systematic comparison of word
representation models with different levels of mor-
phological awareness, across languages with dif-
ferent morphological typologies. Our results con-
firm previous findings that character-level models
are effective for many languages, but these mod-
els do not match the predictive accuracy of model
with explicit knowledge of morphology, even af-
ter we increase the training data size by ten times.
Moreover, our qualitative analysis suggests that
they learn orthographic similarity of affixes, and
lose the meaning of root morphemes.

Although morphological analyses are available

6https://radimrehurek.com/gensim/

2023



Model Frequent Words Rare Words OOV wordsman including relatively unconditional hydroplane uploading foodism

word

person like extremely nazi molybdenum - -
anyone featuring making fairly your - -
children include very joints imperial - -

men includes quite supreme intervene - -

BPE ii called newly unintentional emphasize upbeat vigilantism

LSTM hill involve never ungenerous heartbeat uprising pyrethrumtext like essentially unanimous hybridized handling pausanias
netherlands creating least unpalatable unplatable hand-colored footway

char- mak include resolutely unconstitutional selenocysteine drifted tuaregs
trigrams vill includes regeneratively constitutional guerrillas affected quft
LSTM cow undermining reproductively unimolecular scrofula conflicted subjectivism

maga under commonly medicinal seleucia convicted tune-up

char- mayr inclusion relates undamaged hydrolyzed musagte formulas

LSTM many insularity replicate unmyelinated hydraulics mutualism formallymary includes relativity unconditionally hysterotomy mutualists fecal
may include gravestones uncoordinated hydraulic meursault foreland

char- mtn include legislatively unconventional hydroxyproline unloading fordism

CNN mann includes lovely unintentional hydrate loading dadaismjan excluding creatively unconstitutional hydrangea upgrading popism
nun included negatively untraditional hyena upholding endemism

Table 11: Nearest neighbours of semantically and syntactically similar words.

Query Top nearest neighbours
kota-kota wilayah-wilayah (areas), pulau-pulau (islands), negara-negara (countries),
(cities) bahasa-bahasa (languages), koloni-koloni (colonies)

berlembah-lembah berargumentasi (argue), bercakap-cakap (converse), berkemauan (will),
(have many valleys) berimplikasi (imply), berketebalan (have a thickness)

Table 12: Nearest neighbours of Indonesian reduplicated words in the BPE bi-LSTM model.

in limited quantities, our results suggest that there
might be utility in semi-supervised learning from
partially annotated data. Across languages with
different typologies, our experiments show that the
subword unit models are most effective on agglu-
tinative languages. However, these results do not
generalize to all languages, since factors such as
morphology and orthography affect the utility of
these representations. We plan to explore these ef-
fects in future work.

Acknowledgments

Clara Vania is supported by the Indonesian En-
dowment Fund for Education (LPDP), the Cen-
tre for Doctoral Training in Data Science, funded
by the UK EPSRC (grant EP/L016427/1), and
the University of Edinburgh. We thank Sameer
Bansal, Toms Bergmanis, Marco Damonte, Fed-
erico Fancellu, Sorcha Gilroy, Sharon Gold-
water, Frank Keller, Mirella Lapata, Felicia
Liu, Jonathan Mallinson, Joana Ribeiro, Naomi
Saphra, Ida Szubert, and the anonymous reviewers
for helpful discussion of this work and comments
on previous drafts of the paper.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representa-
tions for multilingual nlp. In Proceedings of the
Seventeenth Conference on Computational Natu-
ral Language Learning. Association for Computa-
tional Linguistics, Sofia, Bulgaria, pages 183–192.
http://www.aclweb.org/anthology/W13-3520.

Emily M. Bender. 2013. Linguistic Fundamentals for
Natural Language Processing: 100 Essentials from
Morphology and Syntax. Morgan & Claypool Pub-
lishers.

2024



Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2016. Enriching word vectors
with subword information. CoRR abs/1607.04606.
http://arxiv.org/abs/1607.04606.

Jan A. Botha and Phil Blunsom. 2014. Com-
positional Morphology for Word Representa-
tions and Language Modeling. In Proceed-
ings of the 31st International Conference on
Machine Learning (ICML). Beijing, China.
http://jmlr.org/proceedings/papers/v32/botha14.pdf.

Danqi Chen and Christopher Manning. 2014. A
fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 740–750.
http://www.aclweb.org/anthology/D14-1082.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
1724–1734. http://www.aclweb.org/anthology/D14-
1179.

Ryan Cotterell and Hinrich Schütze. 2015. Morpho-
logical word-embeddings. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Com-
putational Linguistics, Denver, Colorado, pages
1287–1292. http://www.aclweb.org/anthology/N15-
1140.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 334–343.
http://www.aclweb.org/anthology/P15-1033.

Philip Gage. 1994. A new algorithm for
data compression. C Users J. 12(2):23–38.
http://dl.acm.org/citation.cfm?id=177910.177914.

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag
Subramanya. 2016. Multilingual language process-
ing from bytes. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 1296–
1306. http://www.aclweb.org/anthology/N16-1155.

Alex Graves, Santiago Fernández, and Jürgen
Schmidhuber. 2005. Bidirectional lstm net-
works for improved phoneme classification
and recognition. In Proceedings of the 15th
International Conference on Artificial Neu-
ral Networks: Formal Models and Their Ap-
plications - Volume Part II. Springer-Verlag,
Berlin, Heidelberg, ICANN’05, pages 799–804.
http://dl.acm.org/citation.cfm?id=1986079.1986220.

Martin Haspelmath. 2010. Understanding Morphol-
ogy. Understanding Language Series. Arnold, Lon-
don, second edition.

Georg Heigold, Guenter Neumann, and Josef van Gen-
abith. 2017. An extensive empirical evaluation of
character-based morphological tagging for 14 lan-
guages. In Proceedings of the 15th Conference of
the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers. As-
sociation for Computational Linguistics, pages 505–
513. http://aclweb.org/anthology/E17-1048.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780. https://doi.org/10.1162/neco.1997.9.8.1735.

Katharina Kann and Hinrich Schütze. 2016. Pro-
ceedings of the 14th SIGMORPHON Workshop
on Computational Research in Phonetics, Phonol-
ogy, and Morphology, Association for Compu-
tational Linguistics, chapter MED: The LMU
System for the SIGMORPHON 2016 Shared
Task on Morphological Reinflection, pages 62–70.
https://doi.org/10.18653/v1/W16-2010.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der Rush. 2016. Character-aware neural language
models. In Proceedings of the 2016 Conference on
Artificial Intelligence (AAAI).

Septina Dian Larasati, Vladislav Kuboň, and Daniel
Zeman. 2011. Indonesian Morphology Tool (Mor-
phInd): Towards an Indonesian Corpus, Springer
Berlin Heidelberg, Berlin, Heidelberg, pages 119–
129. https://doi.org/10.1007/978-3-642-23138-4 8.

Jason Lee, Kyunghyun Cho, and Thomas Hof-
mann. 2016. Fully character-level neural machine
translation without explicit segmentation. CoRR
abs/1610.03017. http://arxiv.org/abs/1610.03017.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1520–
1530. http://aclweb.org/anthology/D15-1176.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning. Association for

2025



Computational Linguistics, Sofia, Bulgaria, pages
104–113. http://www.aclweb.org/anthology/W13-
3512.

Mitchell P. Marcus, Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a
large annotated corpus of english: The penn
treebank. Comput. Linguist. 19(2):313–330.
http://dl.acm.org/citation.cfm?id=972470.972475.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget,
Jan Černocký, and Sanjeev Khudanpur. 2010.
Recurrent neural network based language model.
In Proceedings of the 11th Annual Conference
of the International Speech Communication
Association (INTERSPEECH 2010). Inter-
national Speech Communication Association,
volume 2010, pages 1045–1048. http://www.isca-
speech.org/archive/interspeech 2010/i10 1045.html.

Yasumasa Miyamoto and Kyunghyun Cho. 2016.
Gated word-character recurrent language model.
In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Pro-
cessing. Association for Computational Lin-
guistics, Austin, Texas, pages 1992–1997.
https://aclweb.org/anthology/D16-1209.

G. David Morley. 2000. Syntax in Functional Gram-
mar: An Introduction to Lexicogrammar in Systemic
Linguistics. Continuum.

Joakim Nivre, Željko Agić, Maria Jesus Aranzabe,
Masayuki Asahara, Aitziber Atutxa, Miguel Balles-
teros, John Bauer, Kepa Bengoetxea, Riyaz Ah-
mad Bhat, Cristina Bosco, Sam Bowman, Giuseppe
G. A. Celano, Miriam Connor, Marie-Catherine
de Marneffe, Arantza Diaz de Ilarraza, Kaja Do-
brovoljc, Timothy Dozat, Tomaž Erjavec, Richárd
Farkas, Jennifer Foster, Daniel Galbraith, Filip Gin-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Gold-
berg, Berta Gonzales, Bruno Guillaume, Jan Hajič,
Dag Haug, Radu Ion, Elena Irimia, Anders Jo-
hannsen, Hiroshi Kanayama, Jenna Kanerva, Simon
Krek, Veronika Laippala, Alessandro Lenci, Nikola
Ljubešić, Teresa Lynn, Christopher Manning, Ctlina
Mrnduc, David Mareček, Héctor Martı́nez Alonso,
Jan Mašek, Yuji Matsumoto, Ryan McDonald,
Anna Missilä, Verginica Mititelu, Yusuke Miyao,
Simonetta Montemagni, Shunsuke Mori, Hanna
Nurmi, Petya Osenova, Lilja Øvrelid, Elena Pascual,
Marco Passarotti, Cenel-Augusto Perez, Slav Petrov,
Jussi Piitulainen, Barbara Plank, Martin Popel,
Prokopis Prokopidis, Sampo Pyysalo, Loganathan
Ramasamy, Rudolf Rosa, Shadi Saleh, Sebastian
Schuster, Wolfgang Seeker, Mojgan Seraji, Natalia
Silveira, Maria Simi, Radu Simionescu, Katalin
Simkó, Kiril Simov, Aaron Smith, Jan Štěpánek,
Alane Suhr, Zsolt Szántó, Takaaki Tanaka, Reut
Tsarfaty, Sumire Uematsu, Larraitz Uria, Viktor
Varga, Veronika Vincze, Zdeněk Žabokrtský, Daniel
Zeman, and Hanzhi Zhu. 2015. Universal depen-
dencies 1.2 LINDAT/CLARIN digital library at In-
stitute of Formal and Applied Linguistics, Charles

University in Prague. http://hdl.handle.net/11234/1-
1548.

Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. Madamira: A fast, comprehensive tool for
morphological analysis and disambiguation of ara-
bic. In Nicoletta Calzolari, Khalid Choukri, Thierry
Declerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Ninth
International Conference on Language Resources
and Evaluation (LREC’14). European Language Re-
sources Association (ELRA), Reykjavik, Iceland,
pages 1094–1101. ACL Anthology Identifier: L14-
1479.

Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Association for
Computational Linguistics, Berlin, Germany, pages
412–418. http://anthology.aclweb.org/P16-2067.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-
Yan Liu. 2014. Co-learning of word representations
and morpheme representations. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers.
Dublin City University and Association for Com-
putational Linguistics, Dublin, Ireland, pages 141–
150. http://www.aclweb.org/anthology/C14-1015.

Marek Rei, Gamal Crichton, and Sampo Pyysalo. 2016.
Attending to characters in neural sequence label-
ing models. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers. The COLING 2016
Organizing Committee, Osaka, Japan, pages 309–
318. http://aclweb.org/anthology/C16-1030.

Brian Roark and Richard Sproat. 2007. Computational
Approach to Morphology and Syntax. Oxford Uni-
versity Press.

Cicero Dos Santos and Bianca Zadrozny. 2014.
Learning character-level representations for part-
of-speech tagging. In Eric P. Xing and Tony
Jebara, editors, Proceedings of the 31st Interna-
tional Conference on Machine Learning. PMLR,
Bejing, China, volume 32 of Proceedings of
Machine Learning Research, pages 1818–1826.
http://proceedings.mlr.press/v32/santos14.html.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1715–1725. http://www.aclweb.org/anthology/P16-
1162.

2026



Peter Smit, Sami Virpioja, Stig-Arne Grönroos, and
Mikko Kurimo. 2014. Morfessor 2.0: Toolkit for
statistical morphological segmentation. In Proceed-
ings of the Demonstrations at the 14th Conference of
the European Chapter of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Gothenburg, Sweden, pages 21–
24. http://www.aclweb.org/anthology/E14-2006.

Henning Sperr, Jan Niehues, and Alex Waibel. 2013.
Letter n-gram-based input encoding for continu-
ous space language models. In Proceedings of
the Workshop on Continuous Vector Space Models
and their Compositionality. Association for Compu-
tational Linguistics, Sofia, Bulgaria, pages 30–39.
http://www.aclweb.org/anthology/W13-3204.

Ekaterina Vylomova, Trevor Cohn, Xuanli He, and
Gholamreza Haffari. 2016. Word representation
models for morphologically rich languages in neu-
ral machine translation. CoRR abs/1606.04217.
http://arxiv.org/abs/1606.04217.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding words and
sentences via character n-grams. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1504–1515.
https://aclweb.org/anthology/D16-1157.

2027


	From Characters to Words to in Between: Do We Capture Morphology?

