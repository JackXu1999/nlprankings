




















































Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3428

Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in
Natural Language Inference

R. Thomas McCoy,1 Ellie Pavlick,2 & Tal Linzen1

1Department of Cognitive Science, Johns Hopkins University
2Department of Computer Science, Brown University

tom.mccoy@jhu.edu, ellie pavlick@brown.edu, tal.linzen@jhu.edu

Abstract

A machine learning system can score well on

a given test set by relying on heuristics that are

effective for frequent example types but break

down in more challenging cases. We study this

issue within natural language inference (NLI),

the task of determining whether one sentence

entails another. We hypothesize that statisti-

cal NLI models may adopt three fallible syn-

tactic heuristics: the lexical overlap heuristic,

the subsequence heuristic, and the constituent

heuristic. To determine whether models have

adopted these heuristics, we introduce a con-

trolled evaluation set called HANS (Heuris-

tic Analysis for NLI Systems), which contains

many examples where the heuristics fail. We

find that models trained on MNLI, including

BERT, a state-of-the-art model, perform very

poorly on HANS, suggesting that they have

indeed adopted these heuristics. We conclude

that there is substantial room for improvement

in NLI systems, and that the HANS dataset can

motivate and measure progress in this area.

1 Introduction

Neural networks excel at learning the statistical

patterns in a training set and applying them to

test cases drawn from the same distribution as

the training examples. This strength can also be

a weakness: statistical learners such as standard

neural network architectures are prone to adopting

shallow heuristics that succeed for the majority of

training examples, instead of learning the underly-

ing generalizations that they are intended to cap-

ture. If such heuristics often yield correct outputs,

the loss function provides little incentive for the

model to learn to generalize to more challenging

cases as a human performing the task would.

This issue has been documented across domains

in artificial intelligence. In computer vision, for

example, neural networks trained to recognize ob-

jects are misled by contextual heuristics: a net-

work that is able to recognize monkeys in a typ-

ical context with high accuracy may nevertheless

label a monkey holding a guitar as a human, since

in the training set guitars tend to co-occur with hu-

mans but not monkeys (Wang et al., 2018). Sim-

ilar heuristics arise in visual question answering

systems (Agrawal et al., 2016).

The current paper addresses this issue in the do-

main of natural language inference (NLI), the task

of determining whether a premise sentence entails

(i.e., implies the truth of) a hypothesis sentence

(Condoravdi et al., 2003; Dagan et al., 2006; Bow-

man et al., 2015). As in other domains, neural NLI

models have been shown to learn shallow heuris-

tics, in this case based on the presence of specific

words (Naik et al., 2018; Sanchez et al., 2018). For

example, a model might assign a label of contra-

diction to any input containing the word not, since

not often appears in the examples of contradiction

in standard NLI training sets.

The focus of our work is on heuristics that are

based on superficial syntactic properties. Con-

sider the following sentence pair, which has the

target label entailment:

(1) Premise: The judge was paid by the actor.

Hypothesis: The actor paid the judge.

An NLI system that labels this example correctly

might do so not by reasoning about the meanings

of these sentences, but rather by assuming that the

premise entails any hypothesis whose words all

appear in the premise (Dasgupta et al., 2018; Naik

et al., 2018). Crucially, if the model is using this

heuristic, it will predict entailment for (2) as well,

even though that label is incorrect in this case:

(2) Premise: The actor was paid by the judge.

Hypothesis: The actor paid the judge.



3429

Heuristic Definition Example

Lexical overlap Assume that a premise entails all hypothe-

ses constructed from words in the premise

The doctor was paid by the actor.

−−−−−→

WRONG
The doctor paid the actor.

Subsequence Assume that a premise entails all of its

contiguous subsequences.

The doctor near the actor danced.

−−−−−→

WRONG
The actor danced.

Constituent Assume that a premise entails all complete

subtrees in its parse tree.

If the artist slept, the actor ran.

−−−−−→

WRONG
The artist slept.

Table 1: The heuristics targeted by the HANS dataset, along with examples of incorrect entailment predictions that

these heuristics would lead to.

We introduce a new evaluation set called HANS

(Heuristic Analysis for NLI Systems), designed to

diagnose the use of such fallible structural heuris-

tics.1 We target three heuristics, defined in Ta-

ble 1. While these heuristics often yield correct

labels, they are not valid inference strategies be-

cause they fail on many examples. We design our

dataset around such examples, so that models that

employ these heuristics are guaranteed to fail on

particular subsets of the dataset, rather than sim-

ply show lower overall accuracy.

We evaluate four popular NLI models, includ-

ing BERT, a state-of-the-art model (Devlin et al.,

2019), on the HANS dataset. All models per-

formed substantially below chance on this dataset,

barely exceeding 0% accuracy in most cases. We

conclude that their behavior is consistent with the

hypothesis that they have adopted these heuristics.

Contributions: This paper has three main con-

tributions. First, we introduce the HANS dataset,

an NLI evaluation set that tests specific hypotheses

about invalid heuristics that NLI models are likely

to learn. Second, we use this dataset to illumi-

nate interpretable shortcomings in state-of-the-art

models trained on MNLI (Williams et al., 2018b);

these shortcoming may arise from inappropriate

model inductive biases, from insufficient signal

provided by training datasets, or both. Third, we

show that these shortcomings can be made less se-

vere by augmenting a model’s training set with the

types of examples present in HANS. These results

indicate that there is substantial room for improve-

ment for current NLI models and datasets, and that

HANS can serve as a tool for motivating and mea-

suring progress in this area.

1GitHub repository with data and code: https://
github.com/tommccoy1/hans

2 Syntactic Heuristics

We focus on three heuristics: the lexical overlap

heuristic, the subsequence heuristic, and the con-

stituent heuristic, all defined in Table 1. These

heuristics form a hierarchy: the constituent heuris-

tic is a special case of the subsequence heuristic,

which in turn is a special case of the lexical over-

lap heuristic. Table 2 in the next page gives exam-

ples where each heuristic succeeds and fails.

There are two reasons why we expect these

heuristics to be adopted by a statistical learner

trained on standard NLI training datasets such as

SNLI (Bowman et al., 2015) or MNLI (Williams

et al., 2018b). First, the MNLI training set con-

tains far more examples that support the heuristics

than examples that contradict them:2

Heuristic Supporting

Cases

Contradicting

Cases

Lexical overlap 2,158 261

Subsequence 1,274 72

Constituent 1,004 58

Even the 261 contradicting cases in MNLI may not

provide strong evidence against the heuristics. For

example, 133 of these cases contain negation in

the premise but not the hypothesis, as in (3). In-

stead of using these cases to overrule the lexical

overlap heuristic, a model might account for them

by learning to assume that the label is contradic-

tion whenever there is negation in the premise but

not the hypothesis (McCoy and Linzen, 2019):

(3) a. I don’t care. 9 I care.

b. This is not a contradiction. 9 This is a

contradiction.

2In this table, the lexical overlap counts include the sub-
sequence counts, which include the constituent counts.

https://github.com/tommccoy1/hans
https://github.com/tommccoy1/hans


3430

Heuristic Premise Hypothesis Label

Lexical The banker near the judge saw the actor. The banker saw the actor. E

overlap The lawyer was advised by the actor. The actor advised the lawyer. E

heuristic The doctors visited the lawyer. The lawyer visited the doctors. N

The judge by the actor stopped the banker. The banker stopped the actor. N

Subsequence The artist and the student called the judge. The student called the judge. E

heuristic Angry tourists helped the lawyer. Tourists helped the lawyer. E

The judges heard the actors resigned. The judges heard the actors. N

The senator near the lawyer danced. The lawyer danced. N

Constituent Before the actor slept, the senator ran. The actor slept. E

heuristic The lawyer knew that the judges shouted. The judges shouted. E

If the actor slept, the judge saw the artist. The actor slept. N

The lawyers resigned, or the artist slept. The artist slept. N

Table 2: Examples of sentences used to test the three heuristics. The label column shows the correct label for the

sentence pair; E stands for entailment and N stands for non-entailment. A model relying on the heuristics would

label all examples as entailment (incorrectly for those marked as N).

There are some examples in MNLI that contradict

the heuristics in ways that are not easily explained

away by other heuristics; see Appendix A for ex-

amples. However, such cases are likely too rare

to discourage a model from learning these heuris-

tics. MNLI contains data from multiple genres,

so we conjecture that the scarcity of contradicting

examples is not just a property of one genre, but

rather a general property of NLI data generated

in the crowdsourcing approach used for MNLI.

We thus hypothesize that any crowdsourced NLI

dataset would make our syntactic heuristics attrac-

tive to statistical learners without strong linguistic

priors.

The second reason we might expect current NLI

models to adopt these heuristics is that their in-

put representations may make them susceptible to

these heuristics. The lexical overlap heuristic dis-

regards the order of the words in the sentence and

considers only their identity, so it is likely to be

adopted by bag-of-words NLI models (e.g., Parikh

et al. 2016). The subsequence heuristic considers

linearly adjacent chunks of words, so one might

expect it to be adopted by standard RNNs, which

process sentences in linear order. Finally, the con-

stituent heuristic appeals to components of the

parse tree, so one might expect to see it adopted

by tree-based NLI models (Bowman et al., 2016).

3 Dataset Construction

For each heuristic, we generated five templates for

examples that support the heuristic and five tem-

plates for examples that contradict it. Below is

one template for the subsequence heuristic; see

Appendix B for a full list of templates.

(4) The N1 P the N2 V. 9 The N2 V.

The lawyer by the actor ran. 9 The actor ran.

We generated 1,000 examples from each template,

for a total of 10,000 examples per heuristic. Some

heuristics are special cases of others, but we made

sure that the examples for one heuristic did not

also fall under a more narrowly defined heuris-

tic. That is, for lexical overlap cases, the hy-

pothesis was not a subsequence or constituent of

the premise; for subsequence cases, the hypothe-

sis was not a constituent of the premise.

3.1 Dataset Controls

Plausibility: One advantage of generating ex-

amples from templates—instead of, e.g., modify-

ing naturally-occurring examples—is that we can

ensure the plausibility of all generated sentences.

For example, we do not generate cases such as

The student read the book 9 The book read the

student, which could ostensibly be solved using a

hypothesis-plausibility heuristic. To achieve this,

we drew our core vocabulary from Ettinger et al.

(2018), where every noun was a plausible subject

of every verb or a plausible object of every transi-

tive verb. Some templates required expanding this

core vocabulary; in those cases, we manually cu-

rated the additions to ensure plausibility.



3431

Selectional criteria: Some of our example types

depend on the availability of lexically-specific

verb frames. For example, (5) requires aware-

ness of the fact that believed can take a clause (the

lawyer saw the officer) as its complement:

(5) The doctor believed the lawyer saw the officer.

9 The doctor believed the lawyer.

It is arguably unfair to expect a model to under-

stand this example if it had only ever encountered

believe with a noun phrase object (e.g., I believed

the man). To control for this issue, we only chose

verbs that appeared at least 50 times in the MNLI

training set in all relevant frames.

4 Experimental Setup

Since HANS is designed to probe for structural

heuristics, we selected three models that exem-

plify popular strategies for representing the in-

put sentence: DA, a bag-of-words model; ESIM,

which uses a sequential structure; and SPINN,

which uses a syntactic parse tree. In addition to

these three models, we included BERT, a state-

of-the-art model for MNLI. The following para-

graphs provide more details on these models.

DA: The Decomposable Attention model (DA;

Parikh et al., 2016) uses a form of attention to align

words in the premise and hypothesis and to make

predictions based on the aggregation of this align-

ment. It uses no word order information and can

thus be viewed as a bag-of-words model.

ESIM: The Enhanced Sequential Inference

Model (ESIM; Chen et al., 2017) uses a modified

bidirectional LSTM to encode sentences. We use

the variant with a sequential encoder, rather than

the tree-based Hybrid Inference Model (HIM).

SPINN: The Stack-augmented Parser-

Interpreter Neural Network (SPINN; Bowman

et al., 2016) is tree-based: it encodes sentences by

combining phrases based on a syntactic parse. We

use the SPINN-PI-NT variant, which takes a parse

tree as an input (rather than learning to parse). For

MNLI, we used the parses provided in the MNLI

release; for HANS, we used parse templates that

we created based on parses from the Stanford

PCFG Parser 3.5.2 (Klein and Manning, 2003),

the same parser used to parse MNLI. Based on

manual inspection, this parser generally provided

correct parses for HANS examples.

BERT: The Bidirectional Encoder Representa-

tions from Transformers model (BERT; Devlin

et al., 2019) is a Transformer model that uses

attention, rather than recurrence, to process sen-

tences. We use the bert-base-uncased pre-

trained model and fine-tune it on MNLI.

Implementation and evaluation: For DA and

ESIM, we used the implementations from Al-

lenNLP (Gardner et al., 2017). For SPINN3 and

BERT,4 we used code from the GitHub reposito-

ries for the papers introducing those models.

We trained all models on MNLI. MNLI uses

three labels (entailment, contradiction, and neu-

tral). We chose to annotate HANS with two la-

bels only (entailment and non-entailment) because

the distinction between contradiction and neutral

was often unclear for our cases.5 For evaluating a

model on HANS, we took the highest-scoring la-

bel out of entailment, contradiction, and neutral;

we then translated contradiction or neutral labels

to non-entailment. An alternate approach would

have been to add the contradiction and neutral

scores to determine a score for non-entailment; we

found little difference between these approaches,

since the models almost always assigned more

than 50% of the label probability to a single label.6

5 Results

All models achieved high scores on the MNLI test

set (Figure 1a), replicating the accuracies found

in past work (DA: Gururangan et al. 2018; ESIM:

Williams et al. 2018b; SPINN: Williams et al.

2018a; BERT: Devlin et al. 2019). On the HANS

dataset, all models almost always assigned the cor-

rect label in the cases where the label is entail-

ment, i.e., where the correct answer is in line with

the hypothesized heuristics. However, they all per-

formed poorly—with accuracies less than 10% in

most cases, when chance is 50%—on the cases

where the heuristics make incorrect predictions

3https://github.com/stanfordnlp/spinn;
we used the NYU fork at https://github.com/
nyu-mll/spinn.

4https://github.com/google-research/

bert
5For example, with The actor was helped by the judge 9

The actor helped the judge, it is possible that the actor did
help the judge, pointing to a label of neutral; yet the premise
does pragmatically imply that the actor did not help the judge,
meaning that this pair could also fit the non-strict definition
of contradiction used in NLI annotation.

6We also tried training the models on MNLI with neutral
and contradiction collapsed into non-entailment; this gave
similar results as collapsing after training (Appendix D) .

https://github.com/stanfordnlp/spinn
https://github.com/nyu-mll/spinn
https://github.com/nyu-mll/spinn
https://github.com/google-research/bert
https://github.com/google-research/bert


3432

0%

25%

50%

75%

100%

D
A

E
S
IM

S
P
IN
N

B
E
R
T

A
c
c
u
ra
c
y

(a)

Lexical overlap Subsequence Constituent

E
n

ta
ile

d
N

o
n−

e
n

ta
ile

d

D
A

E
S
IM

S
P
IN

N

B
E
R
T

D
A

E
S
IM

S
P
IN

N

B
E
R
T

D
A

E
S
IM

S
P
IN

N

B
E
R
T

0%

25%

50%

75%

100%

0%

25%

50%

75%

100%

A
c
c
u
ra

c
y

(b)

Figure 1: (a) Accuracy on the MNLI test set. (b) Accuracies on six sub-components of the HANS evaluation

set; each sub-component is defined by its correct label and the heuristic it addresses. The dashed lines indicate

chance performance. All models behaved as we would expect them to if they had adopted the heuristics targeted

by HANS. That is, they nearly always predicted entailment for the examples in HANS, leading to near-perfect

accuracy when the true label is entailment, and near-zero accuracy when the true label is non-entailment.

(Figure 1b). Thus, despite their high scores on the

MNLI test set, all four models behaved in a way

consistent with the use of the heuristics targeted in

HANS, and not with the correct rules of inference.

Comparison of models: Both DA and ESIM

had near-zero performance across all three heuris-

tics. These models might therefore make no dis-

tinction between the three heuristics, but instead

treat them all as the same phenomenon, i.e. lexi-

cal overlap. Indeed, for DA, this must be the case,

as this model does not have access to word order;

ESIM does in theory have access to word order in-

formation but does not appear to use it here.

SPINN had the best performance on the sub-

sequence cases. This might be due to the tree-

based nature of its input: since the subsequences

targeted in these cases were explicitly chosen not

to be constituents, they do not form cohesive units

in SPINN’s input in the way they do for sequential

models. SPINN also outperformed DA and ESIM

on the constituent cases, suggesting that SPINN’s

tree-based representations moderately helped it

learn how specific constituents contribute to the

overall sentence. Finally, SPINN did worse than

the other models on constituent cases where the

correct answer is entailment. This moderately

greater balance between accuracy on entailment

and non-entailment cases further indicates that

SPINN is less likely than the other models to as-

sume that constituents of the premise are entailed;

this harms its performance in cases where that as-

sumption happens to lead to the correct answer.

BERT did slightly worse than SPINN on the

subsequence cases, but performed noticeably less

poorly than all other models at both the constituent

and lexical overlap cases (though it was still far

below chance). Its performance particularly stood

out for the lexical overlap cases, suggesting that

some of BERT’s success at MNLI may be due to a

greater tendency to incorporate word order infor-

mation compared to other models.

Analysis of particular example types: In the

cases where a model’s performance on a heuris-

tic was perceptibly above zero, accuracy was not

evenly spread across subcases (for case-by-case

results, see Appendix C). For example, within the

lexical overlap cases, BERT achieved 39% accu-

racy on conjunction (e.g., The actor and the doctor

saw the artist 9 The actor saw the doctor) but 0%

accuracy on subject/object swap (The judge called

the lawyer 9 The lawyer called the judge). Within

the constituent heuristic cases, BERT achieved

49% accuracy at determining that a clause embed-

ded under if and other conditional words is not en-

tailed (If the doctor resigned, the lawyer danced

9 The doctor resigned), but 0% accuracy at iden-

tifying that the clause outside of the conditional

clause is also not entailed (If the doctor resigned,

the lawyer danced 9 The lawyer danced).

6 Discussion

Independence of heuristics: Though each

heuristic is most closely related to one class of

model (e.g., the constituent heuristic is related

to tree-based models), all models failed on cases

illustrating all three heuristics. This finding is un-

surprising since these heuristics are closely related



3433

to each other, meaning that an NLI model may

adopt all of them, even the ones not specifically

targeting that class of model. For example, the

subsequence and constituent heuristics are special

cases of the lexical overlap heuristic, so all models

can fail on cases illustrating all heuristics, because

all models have access to individual words.

Though the heuristics form a hierarchy—the

constituent heuristic is a subcase of the subse-

quence heuristic, which is a subcase of the lexical

overlap heuristic—this hierarchy does not neces-

sarily predict the performance of our models. For

example, BERT performed worse on the subse-

quence heuristic than on the constituent heuristic,

even though the constituent heuristic is a special

case of the subsequence heuristic. Such behavior

has two possible causes. First, it could be due to

the specific cases we chose for each heuristic: the

cases chosen for the subsequence heuristic may be

inherently more challenging than the cases cho-

sen for the constituent heuristic, even though the

constituent heuristic as a whole is a subset of the

subsequence one. Alternately, it is possible for a

model to adopt a more general heuristic (e.g., the

subsequence heuristic) but to make an exception

for some special cases (e.g., the cases to which the

constituent heuristic could apply).

Do the heuristics arise from the architecture

or the training set? The behavior of a trained

model depends on both the training set and the

model’s architecture. The models’ poor results

on HANS could therefore arise from architectural

limitations, from insufficient signal in the MNLI

training set, or from both.

The fact that SPINN did markedly better at the

constituent and subsequence cases than ESIM and

DA, even though the three models were trained on

the same dataset, suggests that MNLI does con-

tain some signal that can counteract the appeal of

the syntactic heuristics tested by HANS. SPINN’s

structural inductive biases allow it to leverage this

signal, but the other models’ biases do not.

Other sources of evidence suggest that the mod-

els’ failure is due in large part to insufficient signal

from the MNLI training set, rather than the mod-

els’ representational capacities alone. The BERT

model we used (bert-base-uncased) was

found by Goldberg (2019) to achieve strong results

in syntactic tasks such as subject-verb agreement

prediction, a task that minimally requires a distinc-

tion between the subject and direct object of a sen-

tence (Linzen et al., 2016; Gulordava et al., 2018;

Marvin and Linzen, 2018). Despite this evidence

that BERT has access to relevant syntactic infor-

mation, its accuracy was 0% on the subject-object

swap cases (e.g., The doctor saw the lawyer 9

The lawyer saw the doctor). We believe it is un-

likely that our fine-tuning step on MNLI, a much

smaller corpus than the corpus BERT was trained

on, substantially changed the model’s representa-

tional capabilities. Even though the model most

likely had access to information about subjects and

objects, then, MNLI did not make it clear how that

information applies to inference. Supporting this

conclusion, McCoy et al. (2019) found little evi-

dence of compositional structure in the InferSent

model, which was trained on SNLI, even though

the same model type (an RNN) did learn clear

compositional structure when trained on tasks that

underscored the need for such structure. These re-

sults further suggest that the models’ poor compo-

sitional behavior arises more because of the train-

ing set than because of model architecture.

Finally, our BERT-based model differed from

the other models in that it was pretrained on a

massive amount of data on a masking task and a

next-sentence classification task, followed by fine-

tuning on MNLI, while the other models were only

trained on MNLI; we therefore cannot rule out

the possibility that BERT’s comparative success at

HANS was due to the greater amount of data it has

encountered rather than any architectural features.

Is the dataset too difficult? To assess the dif-

ficulty of our dataset, we obtained human judg-

ments on a subset of HANS from 95 participants

on Amazon Mechanical Turk as well as 3 expert

annotators (linguists who were unfamiliar with

HANS: 2 graduate students and 1 postdoctoral re-

searcher). The average accuracy was 76% for Me-

chanical Turk participants and 97% for expert an-

notators; further details are in Appendix F.

Our Mechanical Turk results contrast with those

of Nangia and Bowman (2019), who report an ac-

curacy of 92% in the same population on examples

from MNLI; this indicates that HANS is indeed

more challenging for humans than MNLI is. The

difficulty of some of our examples is in line with

past psycholinguistic work in which humans have

been shown to incorrectly answer comprehension

questions for some of our subsequence subcases.

For example, in an experiment in which partici-

pants read the sentence As Jerry played the violin



3434

gathered dust in the attic, some participants an-

swered yes to the question Did Jerry play the vio-

lin? (Christianson et al., 2001).

Crucially, although Mechanical Turk annotators

found HANS to be harder overall than MNLI, their

accuracy was similar whether the correct answer

was entailment (75% accuracy) or non-entailment

(77% accuracy). The contrast between the balance

in the human errors across labels and the stark im-

balance in the models’ errors (Figure 1b) indicates

that human errors are unlikely to be driven by the

heuristics targeted in the current work.

7 Augmenting the training data with

HANS-like examples

The failure of the models we tested raises the ques-

tion of what it would take to do well on HANS.

One possibility is that a different type of model

would perform better. For example, a model based

on hand-coded rules might handle HANS well.

However, since most models we tested are in the-

ory capable of handling HANS’s examples but

failed to do so when trained on MNLI, it is likely

that performance could also be improved by train-

ing the same architectures on a dataset in which

these heuristics are less successful.

To test that hypothesis, we retrained each model

on the MNLI training set augmented with a dataset

structured exactly like HANS (i.e. using the same

thirty subcases) but containing no specific exam-

ples that appeared in HANS. Our additions com-

prised 30,000 examples, roughly 8% of the size

of the original MNLI training set (392,702 ex-

amples). In general, the models trained on the

augmented MNLI performed very well on HANS

(Figure 2); the one exception was that the DA

model performed poorly on subcases for which

a bag-of-words representation was inadequate.7

This experiment is only an initial exploration and

leaves open many questions about the conditions

under which a model will successfully avoid a

heuristic; for example, how many contradicting

examples are required? At the same time, these

results do suggest that, to prevent a model from

learning a heuristic, one viable approach is to use

a training set that does not support this heuristic.

7The effect on MNLI test set performance was less clear;
the augmentation with HANS-like examples improved MNLI
test set performance for BERT (84.4% vs. 84.1%) and ESIM
(77.6% vs 77.3%) but hurt performance for DA (66.0% vs.
72.4%) and SPINN (63.9% vs. 67.0%).

Lexical overlap Subsequence Constituent
E

n
ta

ile
d

N
o

n−
e

n
ta

ile
d

D
A

E
S
IM

S
P
IN

N

B
E
R
T

D
A

E
S
IM

S
P
IN

N

B
E
R
T

D
A

E
S
IM

S
P
IN

N

B
E
R
T

0%

25%

50%

75%

100%

0%

25%

50%

75%

100%

A
c
c
u
ra

c
y

Figure 2: HANS accuracies for models trained on

MNLI plus examples of all 30 categories in HANS.

Transfer across HANS subcases: The positive

results of the HANS-like augmentation experi-

ment are compatible with the possibility that the

models simply memorized the templates that made

up HANS’s thirty subcases. To address this, we re-

trained our models on MNLI augmented with sub-

sets of the HANS cases (withholding some cases;

see Appendix E for details), then tested the models

on the withheld cases.

The results of one of the transfer experiments,

using BERT, are shown in Table 3. There were

some successful cases of transfer; e.g., BERT

performed well on the withheld categories with

sentence-initial adverbs, regardless of whether the

correct label was non-entailment or entailment.

Such successes suggest that BERT is able to learn

from some specific subcases that it should rule

out the broader heuristics; in this case, the non-

withheld cases plausibly informed BERT not to

indiscriminately follow the constituent heuristic,

encouraging it to instead base its judgments on

the specific adverbs in question (e.g., certainly vs.

probably). However, the models did not always

transfer successfully; e.g., BERT had 0% accu-

racy on entailed passive examples when such ex-

amples were withheld, likely because the training

set still included many non-entailed passive exam-

ples, meaning that BERT may have learned to as-

sume that all sentences with passive premises are

cases of non-entailment. Thus, though the models

do seem to be able to rule out the broadest ver-

sions of the heuristics and transfer that knowledge

to some new cases, they may still fall back to the

heuristics for other cases. For further results in-

volving withheld categories, see Appendix E.

Transfer to an external dataset: Finally, we

tested models on the comp same short and



3435

Withheld category Results

Lexical overlap: Conjunctions (9)

The doctor saw the author and the tourist.

9 The author saw the tourist.
0%

50%

100%

MNLI MNLI+

Lexical overlap: Passives (→)

The authors were helped by the actor.

→ The actor helped the authors.
0%

50%

100%

MNLI MNLI+

Subsequence: NP/Z (9)

Before the actor moved the doctor arrived.

9 The actor moved the doctor.
0%

50%

100%

MNLI MNLI+

Subsequence: PP on object (→)

The authors saw the judges by the doctor.

→ The authors saw the judges.
0%

50%

100%

MNLI MNLI+

Constituent: Adverbs (9)

Probably the artists helped the authors.

9 The artists helped the authors.
0%

50%

100%

MNLI MNLI+

Constituent: Adverbs (→)

Certainly the lawyers shouted.

→ The lawyers shouted.
0%

50%

100%

MNLI MNLI+

Table 3: Accuracies for BERT fine-tuned on basic

MNLI and on MNLI+, which is MNLI augmented with

most HANS categories except withholding the cate-

gories in this table. The two lexical overlap cases

shown here are adversarial in that MNLI+ contains

cases superficially similar to them but with opposite la-

bels (namely, the Conjunctions (→) and Passives (9)

cases from Table 4 in the Appendix). The remaining

cases in this table are not adversarial in this way.

comp same long datasets from Dasgupta et al.

(2018), which consist of lexical overlap cases:

(6) the famous and arrogant cat is not more nasty

than the dog with glasses in a white dress. 9

the dog with glasses in a white dress is not

more nasty than the famous and arrogant cat.

This dataset differs from HANS in at least three

important ways: it is based on a phenomenon not

present in HANS (namely, comparatives); it uses a

different vocabulary from HANS; and many of its

sentences are semantically implausible.

We used this dataset to test both BERT fine-

tuned on MNLI, and BERT fine-tuned on MNLI

augmented with HANS-like examples. The aug-

mentation improved performance modestly for the

long examples and dramatically for the short ex-

amples, suggesting that training with HANS-like

examples has benefits that extend beyond HANS.8

8We hypothesize that HANS helps more with short exam-
ples because most HANS sentences are short.

Short Long

E
n
ta
ile
d

N
o
n−
e
n
ta
ile
d

MNLI MNLI+ MNLI MNLI+

0%

25%

50%

75%

100%

0%

25%

50%

75%

100%

A
c
c
u
ra
c
y

Figure 3: Results on the lexical overlap cases from

Dasgupta et al. (2018) for BERT fine-tuned on MNLI

or on MNLI augmented with HANS-like examples.

8 Related Work

8.1 Analyzing trained models

This project relates to an extensive body of re-

search on exposing and understanding weaknesses

in models’ learned behavior and representations.

In the NLI literature, Poliak et al. (2018b) and

Gururangan et al. (2018) show that, due to bi-

ases in NLI datasets, it is possible to achieve

far better than chance accuracy on those datasets

by only looking at the hypothesis. Other recent

works address possible ways in which NLI models

might use fallible heuristics, focusing on semantic

phenomena, such as lexical inferences (Glockner

et al., 2018) or quantifiers (Geiger et al., 2018),

or biases based on specific words (Sanchez et al.,

2018). Our work focuses instead on structural

phenomena, following the proof-of-concept work

done by Dasgupta et al. (2018). Our focus on

using NLI to address how models capture struc-

ture follows some older work about using NLI for

the evaluation of parsers (Rimell and Clark, 2010;

Mehdad et al., 2010).

NLI has been used to investigate many other

types of linguistic information besides syntactic

structure (Poliak et al., 2018a; White et al., 2017).

Outside NLI, multiple projects have used classifi-

cation tasks to understand what linguistic and/or

structural information is present in vector encod-

ings of sentences (e.g., Adi et al., 2017; Ettinger

et al., 2018; Conneau et al., 2018). We instead

choose the behavioral approach of using task per-

formance on critical cases. Unlike the classifica-

tion approach, this approach is agnostic to model

structure; our dataset could be used to evaluate

a symbolic NLI system just as easily as a neu-

ral one, whereas typical classification approaches

only work for models with vector representations.



3436

8.2 Structural heuristics

Similar to our lexical overlap heuristic, Dasgupta

et al. (2018), Nie et al. (2018), and Kim et al.

(2018) also tested NLI models on specific phe-

nomena where word order matters; we use a larger

set of phenomena to study a more general notion

of lexical overlap that is less dependent on the

properties of a single phenomenon, such as pas-

sives. Naik et al. (2018) also find evidence that

NLI models use a lexical overlap heuristic, but our

approach is substantially different from theirs.9

This work builds on our pilot study in McCoy

and Linzen (2019), which studied one of the sub-

cases of the subsequence heuristic. Several of

our subsequence subcases are inspired by psy-

cholinguistics research (Bever, 1970; Frazier and

Rayner, 1982; Tabor et al., 2004); these works

have aims similar to ours but are concerned with

the representations used by humans rather than

neural networks.

Finally, all of our constituent heuristic subcases

depend on the implicational behavior of specific

words. Several past works (Pavlick and Callison-

Burch, 2016; Rudinger et al., 2018; White et al.,

2018; White and Rawlins, 2018) have studied such

behavior for verbs (e.g., He knows it is raining en-

tails It is raining, while He believes it is raining

does not). We extend that approach by including

other types of words with specific implicational

behavior, namely conjunctions (and, or), preposi-

tions that take clausal arguments (if, because), and

adverbs (definitely, supposedly). MacCartney and

Manning (2009) also discuss the implicational be-

havior of these various types of words within NLI.

8.3 Generalization

Our work suggests that test sets drawn from the

same distribution as the training set may be inade-

quate for assessing whether a model has learned to

perform the intended task. Instead, it is also neces-

sary to evaluate on a generalization set that departs

from the training distribution. McCoy et al. (2018)

found a similar result for the task of question for-

mation; different architectures that all succeeded

on the test set failed on the generalization set in

different ways, showing that the test set alone was

not sufficient to determine what the models had

9Naik et al. (2018) diagnose the lexical overlap heuristic
by appending and true is true to existing MNLI hypotheses,
which decreases lexical overlap but does not change the sen-
tence pair’s label. We instead generate new sentence pairs for
which the words in the hypothesis all appear in the premise.

learned. This effect can arise not just from differ-

ent architectures but also from different initializa-

tions of the same architecture (Weber et al., 2018).

9 Conclusions

Statistical learners such as neural networks closely

track the statistical regularities in their training

sets. This process makes them vulnerable to

adopting heuristics that are valid for frequent cases

but fail on less frequent ones. We have inves-

tigated three such heuristics that we hypothesize

NLI models are likely to learn. To evaluate

whether NLI models do behave consistently with

these heuristics, we have introduced the HANS

dataset, on which models using these heuristics

are guaranteed to fail. We find that four exist-

ing NLI models perform very poorly on HANS,

suggesting that their high accuracies on NLI test

sets may be due to the exploitation of invalid

heuristics rather than deeper understanding of lan-

guage. However, these models performed sig-

nificantly better on both HANS and on a sepa-

rate structure-dependent dataset when their train-

ing data was augmented with HANS-like exam-

ples. Overall, our results indicate that, despite

the impressive accuracies of state-of-the-art mod-

els on standard evaluations, there is still much

progress to be made and that targeted, challenging

datasets, such as HANS, are important for deter-

mining whether models are learning what they are

intended to learn.

Acknowledgments

We are grateful to Adam Poliak, Benjamin Van

Durme, Samuel Bowman, the members of the

JSALT General-Purpose Sentence Representation

Learning team, and the members of the Johns

Hopkins Computation and Psycholinguistics Lab

for helpful comments, and to Brian Leonard for

assistance with the Mechanical Turk experiment.

Any errors remain our own.

This material is based upon work supported

by the National Science Foundation Graduate

Research Fellowship Program under Grant No.

1746891 and the 2018 Jelinek Summer Workshop

on Speech and Language Technology (JSALT).

Any opinions, findings, and conclusions or recom-

mendations expressed in this material are those of

the authors and do not necessarily reflect the views

of the National Science Foundation or the JSALT

workshop.



3437

References

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2017. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. In International Conference on Learning
Representations.

Aishwarya Agrawal, Dhruv Batra, and Devi Parikh.
2016. Analyzing the behavior of visual question an-
swering models. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1955–1960. Association for Com-
putational Linguistics.

Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structures.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1466–1477. Association for Computa-
tional Linguistics.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1657–1668. Association for Computational Linguis-
tics.

Kiel Christianson, Andrew Hollingworth, John F Hal-
liwell, and Fernanda Ferreira. 2001. Thematic roles
assigned along the garden path linger. Cognitive
Psychology, 42(4):368–407.

Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entail-
ment, intensionality and text understanding. In Pro-
ceedings of the HLT-NAACL 2003 Workshop on Text
Meaning.

Alexis Conneau, Germán Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single vector: Probing
sentence embeddings for linguistic properties. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 2126–2136. Association for
Computational Linguistics.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Proceedings of the First In-

ternational Conference on Machine Learning Chal-
lenges: Evaluating Predictive Uncertainty Visual
Object Classification, and Recognizing Textual En-
tailment, MLCW’05, pages 177–190, Berlin, Hei-
delberg. Springer-Verlag.

Ishita Dasgupta, Demi Guo, Andreas Stuhlmüller,
Samuel J. Gershman, and Noah D. Goodman. 2018.
Evaluating compositionality in sentence embed-
dings. In Proceedings of the 40th Annual Confer-
ence of the Cognitive Science Society, pages 1596–
1601, Madison, WI.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Allyson Ettinger, Ahmed Elgohary, Colin Phillips, and
Philip Resnik. 2018. Assessing composition in sen-
tence vector representations. In Proceedings of
the 27th International Conference on Computational
Linguistics, pages 1790–1801. Association for Com-
putational Linguistics.

Lyn Frazier and Keith Rayner. 1982. Making and cor-
recting errors during sentence comprehension: Eye
movements in the analysis of structurally ambiguous
sentences. Cognitive Psychology, 14(2):178–210.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2017. AllenNLP: A Deep Semantic Natural Lan-
guage Processing Platform. In Proceedings of the
Workshop for NLP Open Source Software (NLP-
OSS).

Atticus Geiger, Ignacio Cases, Lauri Karttunen,
and Christopher Potts. 2018. Stress-testing neu-
ral models of natural language inference with
multiply-quantified sentences. arXiv preprint
arXiv:1810.13033.

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking NLI Systems with Sentences that
Require Simple Lexical Inferences. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 650–655. Association for Computa-
tional Linguistics.

Yoav Goldberg. 2019. Assessing BERT’s syntactic
abilities. arXiv preprint arXiv:1901.05287.

Kristina Gulordava, Piotr Bojanowski, Edouard Grave,
Tal Linzen, and Marco Baroni. 2018. Colorless
green recurrent networks dream hierarchically. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,

https://openreview.net/pdf?id=BJh6Ztuxl
https://openreview.net/pdf?id=BJh6Ztuxl
https://openreview.net/pdf?id=BJh6Ztuxl
https://aclweb.org/anthology/D16-1203
https://aclweb.org/anthology/D16-1203
http://aclweb.org/anthology/D15-1075
http://aclweb.org/anthology/D15-1075
https://doi.org/10.18653/v1/P16-1139
https://doi.org/10.18653/v1/P16-1139
https://doi.org/10.18653/v1/P17-1152
https://doi.org/10.18653/v1/P17-1152
http://aclweb.org/anthology/W03-0906
http://aclweb.org/anthology/W03-0906
http://www.aclweb.org/anthology/P18-1198
http://www.aclweb.org/anthology/P18-1198
https://doi.org/10.1007/11736790_9
https://doi.org/10.1007/11736790_9
https://cognitivesciencesociety.org/wp-content/uploads/2019/01/cogsci18_proceedings.pdf
https://cognitivesciencesociety.org/wp-content/uploads/2019/01/cogsci18_proceedings.pdf
http://aclweb.org/anthology/C18-1152
http://aclweb.org/anthology/C18-1152
http://arxiv.org/abs/arXiv:1803.07640
http://arxiv.org/abs/arXiv:1803.07640
https://arxiv.org/pdf/1810.13033.pdf
https://arxiv.org/pdf/1810.13033.pdf
https://arxiv.org/pdf/1810.13033.pdf
http://aclweb.org/anthology/P18-2103
http://aclweb.org/anthology/P18-2103
https://arxiv.org/pdf/1901.05287.pdf
https://arxiv.org/pdf/1901.05287.pdf
http://aclweb.org/anthology/N18-1108
http://aclweb.org/anthology/N18-1108


3438

Volume 1 (Long Papers), pages 1195–1205. Associ-
ation for Computational Linguistics.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel Bowman, and Noah A.
Smith. 2018. Annotation artifacts in natural lan-
guage inference data. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 107–112. Association for Computational Lin-
guistics.

Juho Kim, Christopher Malon, and Asim Kadav. 2018.
Teaching syntax by adversarial distraction. In Pro-
ceedings of the First Workshop on Fact Extraction
and VERification (FEVER), pages 79–84. Associa-
tion for Computational Linguistics.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521–
535.

Bill MacCartney and Christopher D Manning. 2009.
Natural language inference. Ph.D. thesis, Stanford
University.

Rebecca Marvin and Tal Linzen. 2018. Targeted syn-
tactic evaluation of language models. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 1192–1202.
Association for Computational Linguistics.

R. Thomas McCoy, Robert Frank, and Tal Linzen.
2018. Revisiting the poverty of the stimulus: Hi-
erarchical generalization without a hierarchical bias
in recurrent neural networks. In Proceedings of the
40th Annual Conference of the Cognitive Science
Society, pages 2093–2098, Madison, WI.

R. Thomas McCoy and Tal Linzen. 2019. Non-entailed
subsequences as a challenge for natural language in-
ference. In Proceedings of the Society for Computa-
tion in Linguistics, volume 2.

R. Thomas McCoy, Tal Linzen, Ewan Dunbar, and
Paul Smolensky. 2019. RNNs implicitly imple-
ment tensor-product representations. In Interna-
tional Conference on Learning Representations.

Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1020–1028.
Association for Computational Linguistics.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2340–2353.
Association for Computational Linguistics.

Nikita Nangia and Samuel R. Bowman. 2019. Human
vs. muppet: A conservative estimate of human per-
formance on the GLUE benchmark.

Yixin Nie, Yicheng Wang, and Mohit Bansal. 2018.
Analyzing compositionality-sensitivity of NLI mod-
els. arXiv preprint arXiv:1811.07033.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255.
Association for Computational Linguistics.

Ellie Pavlick and Chris Callison-Burch. 2016. Tense
manages to predict implicative behavior in verbs.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2225–2229. Association for Computational Linguis-
tics.

Adam Poliak, Aparajita Haldar, Rachel Rudinger,
J. Edward Hu, Ellie Pavlick, Aaron Steven White,
and Benjamin Van Durme. 2018a. Collecting di-
verse natural language inference problems for sen-
tence representation evaluation. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing, pages 67–81. Association
for Computational Linguistics.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018b.
Hypothesis only baselines in natural language in-
ference. In Proceedings of the Seventh Joint Con-
ference on Lexical and Computational Semantics,
pages 180–191. Association for Computational Lin-
guistics.

Laura Rimell and Stephen Clark. 2010. Cambridge:
Parser evaluation using textual entailment by gram-
matical relation comparison. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 268–271. Association for Computational Lin-
guistics.

Rachel Rudinger, Aaron Steven White, and Benjamin
Van Durme. 2018. Neural models of factuality. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 731–744. Associa-
tion for Computational Linguistics.

Ivan Sanchez, Jeff Mitchell, and Sebastian Riedel.
2018. Behavior analysis of NLI models: Uncov-
ering the influence of three factors on robustness.
In Proceedings of the 2018 Conference of the North

http://aclweb.org/anthology/N18-2017
http://aclweb.org/anthology/N18-2017
http://aclweb.org/anthology/W18-5512
http://aclweb.org/anthology/P03-1054
http://aclweb.org/anthology/P03-1054
http://aclweb.org/anthology/Q16-1037
http://aclweb.org/anthology/Q16-1037
http://aclweb.org/anthology/D18-1151
http://aclweb.org/anthology/D18-1151
https://mindmodeling.org/cogsci2018/papers/0399/0399.pdf
https://mindmodeling.org/cogsci2018/papers/0399/0399.pdf
https://mindmodeling.org/cogsci2018/papers/0399/0399.pdf
https://doi.org/10.7275/9hfp-2974
https://doi.org/10.7275/9hfp-2974
https://doi.org/10.7275/9hfp-2974
https://openreview.net/forum?id=BJx0sjC5FX
https://openreview.net/forum?id=BJx0sjC5FX
http://aclweb.org/anthology/N10-1146
http://aclweb.org/anthology/N10-1146
http://aclweb.org/anthology/C18-1198
https://woollysocks.github.io/assets/GLUE_Human_Baseline.pdf
https://woollysocks.github.io/assets/GLUE_Human_Baseline.pdf
https://woollysocks.github.io/assets/GLUE_Human_Baseline.pdf
https://arxiv.org/pdf/1811.07033.pdf
https://arxiv.org/pdf/1811.07033.pdf
https://doi.org/10.18653/v1/D16-1244
https://doi.org/10.18653/v1/D16-1244
https://doi.org/10.18653/v1/D16-1240
https://doi.org/10.18653/v1/D16-1240
http://aclweb.org/anthology/D18-1007
http://aclweb.org/anthology/D18-1007
http://aclweb.org/anthology/D18-1007
http://www.aclweb.org/anthology/S18-2023
http://www.aclweb.org/anthology/S18-2023
http://aclweb.org/anthology/S10-1060
http://aclweb.org/anthology/S10-1060
http://aclweb.org/anthology/S10-1060
https://doi.org/10.18653/v1/N18-1067
https://doi.org/10.18653/v1/N18-1179
https://doi.org/10.18653/v1/N18-1179


3439

American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1975–1985. Associ-
ation for Computational Linguistics.

Whitney Tabor, Bruno Galantucci, and Daniel Richard-
son. 2004. Effects of merely local syntactic coher-
ence on sentence processing. Journal of Memory
and Language, 50(4):355–370.

Jianyu Wang, Zhishuai Zhang, Cihang Xie, Yuyin
Zhou, Vittal Premachandran, Jun Zhu, Lingxi Xie,
and Alan Yuille. 2018. Visual concepts and com-
positional voting. Annals of Mathematical Sciences
and Applications, 3(1):151–188.

Noah Weber, Leena Shekhar, and Niranjan Balasubra-
manian. 2018. The fine line between linguistic gen-
eralization and failure in seq2seq-attention models.
In Proceedings of the Workshop on Generalization
in the Age of Deep Learning, pages 24–27. Associa-
tion for Computational Linguistics.

Aaron Steven White, Pushpendre Rastogi, Kevin Duh,
and Benjamin Van Durme. 2017. Inference is ev-
erything: Recasting semantic resources into a uni-
fied evaluation framework. In Proceedings of the
Eighth International Joint Conference on Natural
Language Processing (Volume 1: Long Papers),
pages 996–1005. Asian Federation of Natural Lan-
guage Processing.

Aaron Steven White and Kyle Rawlins. 2018. The role
of veridicality and factivity in clause selection. In
Proceedings of the 48th Annual Meeting of the North
East Linguistic Society.

Aaron Steven White, Rachel Rudinger, Kyle Rawlins,
and Benjamin Van Durme. 2018. Lexicosyntactic
inference in neural models. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 4717–4724. Associa-
tion for Computational Linguistics.

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018a. Do latent tree learning models
identify meaningful structure in sentences? Trans-
actions of the Association of Computational Linguis-
tics, 6:253–267.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018b. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1112–1122. Association for
Computational Linguistics.

A MNLI examples that contradict the

HANS heuristics

The sentences in (7) show examples from

the MNLI training set that contradict the lex-

ical overlap, subsequence, and constituent

heuristics. The full set of all 261 contra-

dicting examples in the MNLI training set

may be viewed at https://github.com/

tommccoy1/hans/blob/master/mnli_

contradicting_examples.

(7) a. A subcategory of accuracy is consistency.

9 Accuracy is a subcategory of consis-

tency.

b. At the same time, top Enron executives

were free to exercise their stock options,

and some did. 9 Top Enron executives

were free to exercise.

c. She was chagrined at The Nation’s recent

publication of a column by conservative

education activist Ron Unz arguing that

liberal education reform has been an un-

mitigated failure. 9 Liberal education re-

form has been an unmitigated failure.

B Templates

Tables 4, 5, and 6 contain the templates for the

lexical overlap heuristic, the subsequence heuris-

tic, and the constituent heuristic, respectively.

In some cases, a given template has multiple

versions, such as one version where a noun phrase

modifier attaches to the subject and another where

the modifier attaches to the object. For clarity, we

have only listed one version of each template here.

The full list of templates can be viewed in the code

on GitHub.10

C Fine-grained results

Table 7 shows the results by subcase for models

trained on MNLI for the subcases where the cor-

rect answer is entailment. Table 8 shows the re-

sults by subcase for these models for the subcases

where the correct answer is non-entailment.

D Results for models trained on MNLI

with neutral and contradiction merged

Table 9 shows the results on HANS for models

trained on MNLI with the labels neutral and con-

tradiction merged in the training set into the sin-

gle label non-entailment. The results are similar

to the results obtained by merging the labels after

training, with the models generally outputting en-

tailment for all HANS examples, whether that was

the correct answer or not.

10https://github.com/tommccoy1/hans

https://doi.org/10.18653/v1/W18-1004
https://doi.org/10.18653/v1/W18-1004
http://aclweb.org/anthology/I17-1100
http://aclweb.org/anthology/I17-1100
http://aclweb.org/anthology/I17-1100
http://aclweb.org/anthology/D18-1501
http://aclweb.org/anthology/D18-1501
http://aclweb.org/anthology/Q18-1019
http://aclweb.org/anthology/Q18-1019
http://aclweb.org/anthology/N18-1101
http://aclweb.org/anthology/N18-1101
https://github.com/tommccoy1/hans/blob/master/mnli_contradicting_examples
https://github.com/tommccoy1/hans/blob/master/mnli_contradicting_examples
https://github.com/tommccoy1/hans/blob/master/mnli_contradicting_examples
https://github.com/tommccoy1/hans


3440

Subcase Template Example

Entailment:

Untangling relative

clauses

The N1 who the N2 V1 V2 the N3
→ The N2 V1 the N1.

The athlete who the judges admired

called the manager.

→ The judges admired the athlete.

Entailment:

Sentences with PPs

The N1 P the N2 V the N3
→ The N1 V the N3

The tourists by the actor recommended

the authors.

→ The tourists recommended the au-

thors.

Entailment:

Sentences with

relative clauses

The N1 that V2 V1 the N2
→ The N1 V1 the N2

The actors that danced saw the author.

→ The actors saw the author.

Entailment:

Conjunctions

The N1 V the N2 and the N3
→ The N1 V the N3

The secretaries encouraged the scien-

tists and the actors.

→ The secretaries encouraged the ac-

tors.

Entailment:

Passives

The N1 were V by the N2
→ The N1 V the N2

The authors were supported by the

tourists.

→ The tourists supported the authors.

Non-entailment:

Subject-object swap

The N1 V the N2.

9 The N2 V the N1.

The senators mentioned the artist.

9 The artist mentioned the senators.

Non-entailment:

Sentences with PPs

The N1 P the N2 V the N3
9 The N3 V the N2

The judge behind the manager saw the

doctors.

9 The doctors saw the manager.

Non-entailment:

Sentences with

relative clauses

The N1 V1 the N2 who the N3 V2
9 The N2 V1 the N3

The actors advised the manager who

the tourists saw.

9 The manager advised the tourists.

Non-entailment:

Conjunctions

The N1 V the N2 and the N3
9 The N2 V the N3

The doctors advised the presidents and

the tourists.

9 The presidents advised the tourists.

Non-entailment:

Passives

The N1 were V by the N2
9 The N1 V the N2

The senators were recommended by

the managers.

9 The senators recommended the

managers.

Table 4: Templates for the lexical overlap heuristic



3441

Subcase Template Example

Entailment:

Conjunctions

The N1 and the N2 V the N3
→ The N2 V the N3

The actor and the professor mentioned

the lawyer.

→ The professor mentioned the lawyer.

Entailment:

Adjectives

Adj N1 V the N2
→ N1 V the N2

Happy professors mentioned the

lawyer.

→ Professors mentioned the lawyer.

Entailment:

Understood argument

The N1 V the N2
→ The N1 V

The author read the book.

→ The author read.

Entailment:

Relative clause on object

The N1 V1 the N2 that V2 the N3
→ The N1 V1 the N2

The artists avoided the senators that

thanked the tourists.

→ The artists avoided the senators.

Entailment:

PP on object

The N1 V the N2 P the N3
→ The N1 V the N2

The authors supported the judges in

front of the doctor.

→ The authors supported the judges.

Non-entailment:

NP/S

The N1 V1 the N2 V2 the N3
9 The N1 V1 the N2

The managers heard the secretary en-

couraged the author.

9 The managers heard the secretary.

Non-entailment:

PP on subject

The N1 P the N2 V

9 The N2 V

The managers near the scientist re-

signed.

9 The scientist resigned.

Non-entailment:

Relative clause on subject

The N1 that V1 the N2 V2 the N3
9 The N2 V2 the N3

The secretary that admired the senator

saw the actor.

9 The senator saw the actor.

Non-entailment:

MV/RR

The N1 V1 P the N2 V2
9 The N1 V1 P the N2

The senators paid in the office danced.

9 The senators paid in the office.

Non-entailment:

NP/Z

P the N1 V1 the N2 V2 the N3
9 The N1 V1 the N2

Before the actors presented the profes-

sors advised the manager.

9 The actors presented the professors.

Table 5: Templates for the subsequence heuristic



3442

Subcase Template Example

Entailment:

Embedded under preposi-

tion

P the N1 V1, the N2 V2 the N3
→ The N1 V1

Because the banker ran, the doctors

saw the professors.

→ The banker ran.

Entailment:

Outside embedded clause

P the N1 V1 the N2, the N3 V2
the N4
→ The N3 V2 the N4

Although the secretaries recommended

the managers, the judges supported the

scientist.

→ The judges supported the scientist.

Entailment:

Embedded under verb

The N1 V1 that the N2 V2
→ The N2 V2

The president remembered that the ac-

tors performed.

→ The actors performed.

Entailment:

Conjunction

The N1 V1, and the N2 V2 the

N3.

→ The N2 V2 the N3

The lawyer danced, and the judge sup-

ported the doctors.

→ The judge supported the doctors.

Entailment:

Adverbs

Adv the N V

→ The N V

Certainly the lawyers resigned.

→ The lawyers resigned.

Non-entailment:

Embedded under preposi-

tion

P the N1 V1, the N2 V2 the N2
9 The N1 V1

Unless the senators ran, the professors

recommended the doctor.

9 The senators ran.

Non-entailment:

Outside embedded clause

P the N1 V1 the N2, the N3 V2
the N4
9 The N3 V2 the N4

Unless the authors saw the students, the

doctors helped the bankers.

9 The doctors helped the bankers.

Non-entailment:

Embedded under verb

The N1 V1 that the N2 V2 the N3
9 The N2 V2 the N3

The tourists said that the lawyer saw

the banker.

9 The lawyer saw the banker.

Non-entailment:

Disjunction

The N1 V1, or the N2 V2 the N3
9 The N2 V2 the N3

The judges resigned, or the athletes

mentioned the author.

9 The athletes mentioned the author.

Non-entailment:

Adverbs

Adv the N1 V the N2
9 The N1 V the N2

Probably the artists saw the authors.

9 The artists saw the authors.

Table 6: Templates for the constituent heuristic



3443

Heuristic Subcase DA ESIM SPINN BERT

Lexical Untangling relative clauses 0.97 0.95 0.88 0.98

overlap The athlete who the judges saw called the manager. → The judges saw the athlete.

Sentences with PPs 1.00 1.00 1.00 1.00

The tourists by the actor called the authors. → The tourists called the authors.

Sentences with relative clauses 0.98 0.97 0.97 0.99

The actors that danced encouraged the author. → The actors encouraged the author.

Conjunctions 1.00 1.00 1.00 0.77

The secretaries saw the scientists and the actors. → The secretaries saw the actors.

Passives 1.00 1.00 0.95 1.00

The authors were supported by the tourists. → The tourists supported the authors.

Subsequence Conjunctions 1.00 1.00 1.00 0.98

The actor and the professor shouted. → The professor shouted.

Adjectives 1.00 1.00 1.00 1.00

Happy professors mentioned the lawyer. → Professors mentioned the lawyer.

Understood argument 1.00 1.00 0.84 1.00

The author read the book. → The author read.

Relative clause on object 0.98 0.99 0.95 0.99

The artists avoided the actors that performed. → The artists avoided the actors.

PP on object 1.00 1.00 1.00 1.00

The authors called the judges near the doctor. → The authors called the judges.

Constituent Embedded under preposition 0.99 0.99 0.85 1.00

Because the banker ran, the doctors saw the professors. → The banker ran.

Outside embedded clause 0.94 1.00 0.95 1.00

Although the secretaries slept, the judges danced. → The judges danced.

Embedded under verb 0.92 0.94 0.99 0.99

The president remembered that the actors performed. → The actors performed.

Conjunction 0.99 1.00 0.89 1.00

The lawyer danced, and the judge supported the doctors. → The lawyer danced.

Adverbs 1.00 1.00 0.98 1.00

Certainly the lawyers advised the manager. → The lawyers advised the manager.

Table 7: Results for the subcases where the correct label is entailment.



3444

Heuristic Subcase DA ESIM SPINN BERT

Lexical Subject-object swap 0.00 0.00 0.03 0.00

overlap The senators mentioned the artist. 9 The artist mentioned the senators.

Sentences with PPs 0.00 0.00 0.01 0.25

The judge behind the manager saw the doctors. 9 The doctors saw the manager.

Sentences with relative clauses 0.04 0.04 0.06 0.18

The actors called the banker who the tourists saw. 9 The banker called the tourists.

Conjunctions 0.00 0.00 0.01 0.39

The doctors saw the presidents and the tourists. 9 The presidents saw the tourists.

Passives 0.00 0.00 0.00 0.00

The senators were helped by the managers. 9 The senators helped the managers.

Subsequence NP/S 0.04 0.02 0.09 0.02

The managers heard the secretary resigned. 9 The managers heard the secretary.

PP on subject 0.00 0.00 0.00 0.06

The managers near the scientist shouted. 9 The scientist shouted.

Relative clause on subject 0.03 0.04 0.05 0.01

The secretary that admired the senator saw the actor. 9 The senator saw the actor.

MV/RR 0.04 0.03 0.03 0.00

The senators paid in the office danced. 9 The senators paid in the office.

NP/Z 0.02 0.01 0.11 0.10

Before the actors presented the doctors arrived. 9 The actors presented the doctors.

Constituent Embedded under preposition 0.14 0.02 0.29 0.50

Unless the senators ran, the professors recommended the doctor. 9 The senators ran.

Outside embedded clause 0.01 0.00 0.02 0.00

Unless the authors saw the students, the doctors resigned. 9 The doctors resigned.

Embedded under verb 0.00 0.00 0.01 0.22

The tourists said that the lawyer saw the banker. 9 The lawyer saw the banker.

Disjunction 0.01 0.03 0.20 0.01

The judges resigned, or the athletes saw the author. 9 The athletes saw the author.

Adverbs 0.00 0.00 0.00 0.08

Probably the artists saw the authors. 9 The artists saw the authors.

Table 8: Results for the subcases where the correct label is non-entailment.



3445

Correct: Entailment Correct: Non-entailment

Model Model class Lexical Subseq. Const. Lexical Subseq. Const.

DA Bag-of-words 1.00 1.00 0.98 0.00 0.00 0.03

ESIM RNN 0.99 1.00 1.00 0.00 0.01 0.00

SPINN TreeRNN 0.94 0.96 0.93 0.06 0.14 0.11

BERT Transformer 0.98 1.00 0.99 0.04 0.02 0.20

Table 9: Results for models trained on MNLI with neutral and contradiction merged into a single label, non-

entailment.

E Results with augmented training with

some subcases withheld

For each model, we ran five experiments, each one

having 6 of the 30 subcases withheld. Each trained

model was then evaluated on the categories that

had been withheld from it. The results of these

experiments are in Tables 10, 11, 12, 13 and 14.

F Human experiments

To obtain human results, we used Amazon Me-

chanical Turk. We subdivided HANS into 114

different categories of examples, covering all pos-

sible variations of the template used to generate

the example and the specific word around which

the template was built. For example, for the con-

stituent heuristic subcase of clauses embedded un-

der verbs (e.g. The doctor believed the lawyer

danced 9 The lawyer danced), each possible verb

under which the clause could be embedded (e.g.

believed, thought, or assumed) counted as a dif-

ferent category.

For each of these 114 categories, we chose 20

examples from HANS and obtained judgments

from 5 human participants for each of those 20

examples. Each participant provided judgments

for 57 examples plus 10 controls (67 stimuli to-

tal) and was paid $2.00. The controls consisted

of 5 examples where the premise and hypothesis

were the same (e.g. The doctor saw the lawyer

→ The doctor saw the lawyer) and 5 examples of

simple negation (e.g. The doctor saw the lawyer

9 The doctor did not see the lawyer). For analyz-

ing the data, we discarded any participants who

answered any of these controls incorrectly; this

led to 95 participants being retained and 105 be-

ing rejected (participants were still paid regardless

of whether they were retained or filtered out). On

average, each participant spent 6.5 seconds per ex-

ample; the participants we retained spent 8.9 sec-

onds per example, while the participants we dis-

carded spent 4.2 seconds per example. The total

amount of time from a participant accepting the

experiment to completing the experiment averaged

17.6 minutes. This included 9.1 minutes answer-

ing the prompts (6.4 minutes for discarded partic-

ipants and 12.1 minutes for retained participants)

and roughly one minute spent between prompts (1

second after each prompt). The remaining time

was spent reading the consent form, reading the

instructions, or waiting to start (Mechanical Turk

participants often wait several minutes between

accepting an experiment and beginning the exper-

iment).

The expert annotators were three native English

speakers who had a background in linguistics but

who had not heard about this project before pro-

viding judgments. Two of them were graduate stu-

dents and one was a postdoctoral researcher. Each

expert annotator labeled 124 examples (one exam-

ple from each of the 114 categories, plus 10 con-

trols).



3446

Heuristic Subcase DA ESIM SPINN BERT

Lexical Subject-object swap 0.01 1.00 1.00 1.00

overlap The senators mentioned the artist. 9 The artist mentioned the senators.

Lexical Untangling relative clauses 0.34 0.23 0.23 0.20

overlap The athlete who the judges saw called the manager. → The judges saw the athlete.

Subsequence NP/S 0.27 0.00 0.00 0.10

The managers heard the secretary resigned. 9 The managers heard the secretary.

Subsequence Conjunctions 0.49 0.38 0.38 0.38

The actor and the professor shouted. → The professor shouted.

Constituent Embedded under preposition 0.51 0.51 0.51 1.00

Unless the senators ran, the professors recommended the doctor. 9 The senators ran.

Constituent Embedded under preposition 1.00 0.06 1.00 0.03

Because the banker ran, the doctors saw the professors. → The banker ran.

Table 10: Accuracies for models trained on MNLI augmented with most HANS example categories except with-

holding the categories in this table (experiment 1/5 for the withheld category investigation).

Heuristic Subcase DA ESIM SPINN BERT

Lexical Sentences with PPs 0.00 0.96 0.71 0.97

overlap The judge behind the manager saw the doctors. 9 The doctors saw the manager.

Lexical Sentences with PPs 1.00 1.00 0.94 1.00

overlap The tourists by the actor called the authors. → The tourists called the authors.

Subsequence PP on subject 0.00 0.07 0.57 0.39

The managers near the scientist shouted. 9 The scientist shouted.

Subsequence Adjectives 0.71 0.99 0.64 1.00

Happy professors mentioned the lawyer. → Professors mentioned the lawyer.

Constituent Outside embedded clause 0.78 1.00 1.00 0.17

Unless the authors saw the students, the doctors resigned. 9 The doctors resigned.

Constituent Outside embedded clause 0.78 0.78 0.78 0.97

Although the secretaries slept, the judges danced. → The judges danced.

Table 11: Accuracies for models trained on MNLI augmented with most HANS example categories except with-

holding the categories in this table (experiment 2/5 for the withheld category investigation).



3447

Heuristic Subcase DA ESIM SPINN BERT

Lexical Sentences with relative clauses 0.00 0.04 0.02 0.84

overlap The actors called the banker who the tourists saw. 9 The banker called the tourists.

Lexical Sentences with relative clauses 1.00 0.97 1.00 1.00

overlap The actors that danced encouraged the author. → The actors encouraged the author.

Subsequence Relative clause on subject 0.00 0.04 0.00 0.93

The secretary that admired the senator saw the actor. 9 The senator saw the actor.

Subsequence Understood argument 0.28 1.00 0.81 0.94

The author read the book. → The author read.

Constituent Embedded under verb 0.00 0.00 0.05 0.98

The tourists said that the lawyer saw the banker. 9 The lawyer saw the banker.

Constituent Embedded under verb 1.00 0.94 0.98 0.43

The president remembered that the actors performed. → The actors performed.

Table 12: Accuracies for models trained on MNLI augmented with most HANS example categories except with-

holding the categories in this table (experiment 3/5 for the withheld category investigation).

Heuristic Subcase DA ESIM SPINN BERT

Lexical Passives 0.00 0.00 0.00 0.00

overlap The senators were helped by the managers. 9 The senators helped the managers.

Lexical Conjunctions 0.05 0.51 0.52 1.00

overlap The secretaries saw the scientists and the actors. → The secretaries saw the actors.

Subsequence MV/RR 0.76 0.44 0.32 0.07

The senators paid in the office danced. 9 The senators paid in the office.

Subsequence Relative clause on object 0.72 1.00 0.99 0.99

The artists avoided the actors that performed. → The artists avoided the actors.

Constituent Disjunction 0.11 0.29 0.51 0.44

The judges resigned, or the athletes saw the author. 9 The athletes saw the author.

Constituent Conjunction 0.99 1.00 0.74 1.00

The lawyer danced, and the judge supported the doctors. → The lawyer danced.

Table 13: Accuracies for models trained on MNLI augmented with most HANS example categories except with-

holding the categories in this table (experiment 4/5 for the withheld category investigation).



3448

Heuristic Subcase DA ESIM SPINN BERT

Lexical Conjunctions 0.00 0.44 0.00 0.08

overlap The doctors saw the presidents and the tourists. 9 The presidents saw the tourists.

Lexical Passives 0.00 0.00 0.00 0.00

overlap The authors were supported by the tourists. → The tourists supported the authors.

Subsequence NP/Z 0.00 0.10 0.18 0.57

Before the actors presented the doctors arrived. 9 The actors presented the doctors.

Subsequence PP on object 0.04 0.76 0.04 0.98

The authors called the judges near the doctor. → The authors called the judges.

Constituent Adverbs 0.76 0.33 0.20 0.84

Probably the artists saw the authors. 9 The artists saw the authors.

Constituent Adverbs 0.66 1.00 0.59 0.96

Certainly the lawyers advised the manager. → The lawyers advised the manager.

Table 14: Accuracies for models trained on MNLI augmented with most HANS example categories except with-

holding the categories in this table (experiment 5/5 for the withheld category investigation).


