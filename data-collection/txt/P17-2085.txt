



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 536–541
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2085

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 536–541
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2085

List-only Entity Linking

Ying Lin1 ∗, Chin-Yew Lin2, Heng Ji1
1 Computer Science Department,

Rensselaer Polytechnic Institute, Troy, NY, USA
{liny9,jih}@rpi.edu

2 Microsoft Research, Beijing, China
cyl@microsoft.com

Abstract

Traditional Entity Linking (EL) technolo-
gies rely on rich structures and proper-
ties in the target knowledge base (KB).
However, in many applications, the KB
may be as simple and sparse as lists of
names of the same type (e.g., lists of prod-
ucts). We call it as List-only Entity Link-
ing problem. Fortunately, some mentions
may have more cues for linking, which
can be used as seed mentions to bridge
other mentions and the uninformative en-
tities. In this work, we select the most
linkable mentions as seed mentions and
disambiguate other mentions by compar-
ing them with the seed mentions rather
than directly with the entities. Our exper-
iments on linking mentions to seven auto-
matically mined lists show promising re-
sults and demonstrate the effectiveness of
our approach.1

1 Introduction

Traditional Entity Linking (EL) methods usually
rely on rich structures and properties in the tar-
get knowledge base (KB). These methods may not
be effective in applications where detailed descrip-
tions and properties of target entities are absent in
the KB. Consider the following situations:

Disaster Response and Recovery. When a
disaster strikes, people rush to the web and post
tweets about the damage and casualties. Perform-
ing EL to extract key information, such as devas-
tated towns and donor agencies, can help us mon-
itor the situation and coordinate rescue and recov-
ery efforts. Although many involved entities are

∗Part of this work was done when the first author was on
an internship at Microsoft Research Asia.

1The data set is available at: http://nlp.cs.rpi.edu/data/
link-only-entity-linking.html

not well-known and usually absent in general KBs,
we may be able to acquire lists of these entities
from the local government as the target KB.

Voice of the Customer. EL also plays
an important role in mining customer opinions
from data generated on social platforms and e-
commerce websites, thereby helping companies
better understand the needs and expectations of
their customers. However, the target products are
often not covered by general KBs. For example,
(Cao et al., 2015) tested 32 names of General Mo-
tors car models and only found 4 in Wikipedia.
Although some companies may choose to main-
tain a comprehensive product KB, it will be much
more practical and less costly to provide only lists
of product names.

Figure 1: Link mentions to target entities in differ-
ent entity lists.

Under such circumstances, we need the ability
to perform EL to ad-hoc name lists instead of a
comprehensive KB, namely List-only Entity Link-
ing. Take Figure 1 as an example. For a human
reader, it is not difficult to figure out the referent
entities of mentions in each document based on

536

https://doi.org/10.18653/v1/P17-2085
https://doi.org/10.18653/v1/P17-2085


clues such as “basketball” and “LDA”, whereas we
will not be able to make such inference without the
knowledge of the target entities. However, even if
we lack the minimal knowledge (e.g., Jordan is a
country), we are more confident to link mentions
in d1, d4, and d5 because they co-occur with other
entities in the same list. We consider such men-
tions that we are confident to link as seed men-
tions, and use them to construct contextual and
non-contextual information of the target entities to
enhance entity disambiguation.

Therefore, in this work, we propose to tackle the
problem of List-only Entity Linking through seed
mentions. We automatically identify seed men-
tions for each list using a two-step method based
on the occurrence of entities and similarity be-
tween mentions. After that, in the entity disam-
biguation phase, we utilize the selected mentions
as a bridge between uninformative entities and
other mentions. Specifically, we comparing fea-
tures of a non-seed mention to those of seed men-
tions of its entity candidates to determine which
entity it should be linked to.

2 Problem Definition

Given a mention m and the entity e that it refers
to, we call e the referent entity of m and m the
referential mention of e. In Figure 1, for exam-
ple, Michael Jordan1 is the referent entity of
“Jordan” in document d6, while “Jordan” in docu-
ment d2 is an non-referential mention for Michael
Jordan1.

As Figure 1 shows, in the setting of List-
only Entity Linking, there are a set of manu-
ally or automatically generated entity lists E =
{E1, E2, ..., El} and a set of documents D =
{d1, d2, ..., dn}. Entities in the same list are ho-
mogeneous and share some common properties.
In our experiment, each document di contains a
mention mi to link. Our goal is to link mi to its
referent entity ei,j ∈ Ej or returns NIL if it is un-
linkable to any entities.

3 Approach

Our framework has two modules, entity candi-
date retrieval and entity disambiguation as Fig-
ure 2 shows. For a mention “Jordan,” we retrieve
two candidate entities, Michael Jordan1 and
Michael Jordan2, from the entity lists. Next,
we select a set of seed mentions for each entity
from all documents. To determine the referent en-

tity of “Jordan”, we compare it with seed mentions
of each candidate instead of the entity itself.

Figure 2: List-only Entity Linking Framework.

3.1 Entity Candidate Retrieval

For each mention mi, we first locate a set of en-
tity candidates Ci = {ei,j |ei,j ∈ Ej} that it pos-
sibly refers to. A mention and its referent entity
may have different surface forms (e.g., “BMW”
and Bayerische Motoren Werke). For this rea-
son, we design a set of matching rules to improve
the recall as shown in Table 1.

Category Rule Examples

Abbreviation

Acronym USDOD/USDD (United States
Department of Defense),

Initial
Letters

corp. (corporation), univ. (uni-
versity)

First and
Last Let-
ters

Dr. (Doctor), PA (Pennsylva-
nia)

Omission Address a person by his/her
given name or surname rather
than full name

Substitution

Numeral 7-11 (7-Eleven )
Symbol AT&T (American Telephone

and Telegraph Company)
Accent
Mark

hermes.com (Hermès)

Table 1: Alternative form matching rules.

3.2 Entity Disambiguation

Next, we proceed to score each candidate ei,j
and determine which one mi should be linked to.
However, we have no knowledge of the target enti-
ties except for names and thus can’t directly com-
pare mi with them. Rather, we propose to bridge
the gap between mentions and entities through
seed mentions.

537



Figure 3: Bridging the gap between uncertain
mentions and target entities using seed mentions.

We illustrate the idea in Figure 3. University
of Pennsylvania is retrieved as an entity can-
didate for mentions “University of Pennsylvania”,
“Penn”, and “Pennsylvania.” We are more confi-
dent to link “University of Pennsylvania” in dI and
“Penn” in dII to University of Pennsylvania
because other entities in the University list, such
as “Harvard” and “MIT,” also appear in the same
document. Thus, we select mentions in dI and dII
as seed mentions. From dI and dII , we can extract
both contextual features (e.g., “academic” and “re-
search”) and non-contextual features (e.g., the en-
tity type is ORG). After that, we compare mentions
in other documents with the seeds. We link “Penn”
in dIII to University of Pennsylvania be-
cause its entity type and context are consistent
with the seeds. “Pennsylvania” in dIV , however,
is not linked because it is recognized as a loca-
tion. To capture richer contextual information and
minimize the effect of noise, we select more than
one seed mention using a two-step approach as fol-
lows.

Figure 4: Seed selection.

1. Subset Selection. We assume that if multi-
ple names in the same list co-occur within a docu-

ment, they are all likely to be referential mentions
of this list, such as “Michael Jordan” and “LeBron
James” in d1. Hence, to identify seed mentions
of list Ej , we first narrow the scope down to a
subset D(n)j of documents containing more than
n mentions matching names in Ej . We gradually
increase the n until θ−size ≤ |D

(n)
j | ≤ θ+size. θ−size

and θ+size are set to 50 and 300 in our experiments.
2. Clustering. We expect most mentions in the

selected subset are referential of list Ej , while in
fact the subset is likely to contain a small number
of non-referential mentions. We need to eliminate
them from the subset, otherwise they will intro-
duce misleading features differing from the real
seed mentions, hence hurting the performance of
entity disambiguation. To separate referential and
non-referential mentions in the selected subset, we
make two assumptions: (1) Most mentions in the
subset are referential, and (2) Referential mentions
should be similar to each other while dissimilar
from non-referential ones. Due to the lack of an-
notated data, we approach this problem by per-
forming clustering, which works in an unsuper-
vised fashion. Specifically, we represent features
(described later in this section) of each mention
as a vector and measure the distance between two
mentions using cosine distance. After that, we run
the K-means++ algorithm on the subset to separate
referential and non-referential mentions, and pick
mentions in the largest cluster as seed mentions.

To determine the referent entity of mention mi,
we calculate the confidence score of linking mi
to ei,j ∈ Ej using the average cosine similarity
between mi and seed mentions of list Ej :

c(mi, ei,j) =
1

|Sj |

|Sj |∑

p=1

sim(mi,ms),ms ∈ Sj

where Sj is the seed set of Ej . Lastly, we link mi
to the candidate with the highest confidence score.

In this work, we use the following features.
Entity Type. The entity type of a mention

can be inferred from the text and used for dis-
ambiguation. For example, if most seed men-
tions for the University list are recognized as ORG,
while “Harvard” in the sentence “Harvard was
born and raised in Southwark, Surrey, England”
is tagged as PER, it is unlikely to refer to Harvard
University.

Textual Context. We also assume that refer-
ential mentions of the same entity should share

538



similar local contexts. We represent textual con-
text using the average embedding of words within
a window around the mention.

Punctuation. Punctuations preceding or fol-
lowing a mention may help resolve ambiguity. For
example, “MA” preceded by a comma is possible
to refer to a state, since states are usually the last
component of an address, such as “Boston, MA”.

4 Experiments

4.1 Data set

In our experiment, the construction of data set
consists of two steps: collecting name lists from
NeedleSeek2 (Shi et al., 2010) and extracting doc-
uments from Wikipedia. NeedleSeek is a project
aiming to mine semantic concepts from tera-scale
data (ClueWeb09) and classify them into a wide
range of semantic categories. For example, “KFC”
is mined as a concept in the restaurant category,
along with key sentences and attributes, such as
employee number and founder.

To obtain target name lists, we select 7 se-
mantic categories (see Table 2) generated by
NeedleSeek as target domains, and take the
top concepts in each category as target entities.
We manually map each name to its pertinent
Wikipedia page as a target entity (e.g., Starbucks
→ enwiki:Starbucks3). Thus, we collect lists
containing 139 target entities in total. Note that
category names are only for result presentation
purpose and not taken as input to our model.

Category Name Examples
President Barack Obama, Ronald Reagan
Company Microsoft, Apple, Adobe, IBM
University Harvard University, Yale University
State Washington, Florida, California, Texas
Character Gandalf, Aragorn, Legolas, Gimli, Frodo
Brand Prada, Chanel, Burberry, Gucci, Cartier
Restaurant Subway, McDonald’s, KFC, Starbucks

Table 2: Semantic categories from NeedleSeek.

Next, we derive a data set from Wikipedia
articles through wikilinks4, which are links to
pages within English Wikipedia. For example,
a wikilink [[Harvard University|Harvard]]
appears as “Harvard” in text and links to the
page enwiki:Harvard_University. Thus, we
can consider “Harvard” as a name mention and

2http://needleseek.msra.cn
3enwiki: is short for https://en.wikipedia.org/wiki/
4https://en.wikipedia.org/wiki/Help:Link

enwiki:Harvard_University as its referent en-
tity. Consider the following sentences:
∗ ... then left toattend graduate school on a scholarship at
[[Harvard University|Harvard University]]...

∗ On October 6, 2012, [[Allison Harvard|Harvard]]
made an appearance in an episode of...

Because enwiki:Harvard_University is in
the University list, the first mention will be con-
sidered as referential, whereas the second one is
non-referential. We also apply matching rules in
Table 1 to obtain more non-referential mentions.
After that, we extract sentences around wikilinks
as a document.

Category #Referential #Referential
(balanced)

#Non-
referential

President 51, 412 14, 722 14, 818
Company 13, 312 3, 604 3, 642
University 79, 285 30, 101 30, 187
State 86, 743 9, 602 9, 106
Character 729 483 476
Brand 5, 138 1, 739 1, 781
Restaurant 4, 261 4, 261 4, 850
Total 240, 588 64, 512 61, 632

Table 3: Data set stats.

From Table 3, we can see that referential en-
tities overwhelm non-referential ones in the ex-
tracted corpus. In order to evaluate our model
fairly, we perform downsampling to balance refer-
ential and non-referential mentions, otherwise we
can achieve high scores even if we link all mention
to the target entities. In the balanced data set, there
are 11, 065 unique entities.

4.2 Entity Linking Results

Category Complete Balanced SubsetR P F R P F
President 94.6 89.9 92.2 87.2 80.4 83.7
Company 86.6 95.8 91.0 90.8 85.1 87.9
University 96.7 96.4 96.5 96.9 92.0 94.4
State 96.2 92.1 94.1 95.0 58.6 72.5
Character 92.5 61.3 73.7 92.8 52.2 66.8
Brand 89.6 90.2 89.9 86.7 83.2 84.9
Restaurant 87.0 81.4 84.1 86.9 88.1 87.5
Overall 95.2 93.4 94.3 93.1 81.6 87.0

Table 4: Overall performance (%). R, P, and F rep-
resent recall, precision, and F1 score, respectively.

As Table 4 demonstrates, our method shows
promising results (87.0 F1 score) on the balanced
data set. Nevertheless, we notice the low linking
precisions for entities in the Character and State
lists, which are caused by different reasons. For
the Character list, mentions do not suffice to select

539



high-quality seeds, whereas for the State list, fea-
tures of referential and non-referential mentions
are usually similar. Consider the following sen-
tence:
∗ She witnessed his fatal shooting when they were together
in the President’s Box at Ford’s Theatre on Tenth Street in

Washington.

The mention “Washington” refers to
“Washington, D.C.”, which has the same
entity type, LOCATION, as our target entity
“Washington (state)”. In addition, we see
no obvious textual clue that indicates whether
it refers to the State of Washington or not.
Traditional EL approaches usually disambiguate
such mentions through collective inference.
They link “Ford’s Theatre” and “Washington”
to the KB simultaneously. Since there exists an
explicit relation between “Ford’s Theatre” and
“Washington, D.C.”, these two entities receive
high confidence scores and thus are determined as
the referents. Unfortunately, we cannot employ
the knowledge-rich approach in the List-only
Entity Linking scenario.

5 Related Work

In this paper, we define and study the List-only
Entity Linking problem based on previous stud-
ies on Target Entity Disambiguation (Wang et al.,
2012; Cao et al., 2015). The key difference is that
they target at the disambiguation of a single list of
entities, whereas we focus on entity linking to an
arbitrary number of lists. Another similar prob-
lem is Named Entity Disambiguation with Link-
less Knowledge Bases (LNED) (Li et al., 2016). It
assumes that entities are isolated in the “linkless”
KB, while each entity still has a description.

Our idea of selecting seed mentions based on
co-occurrence is similar to collective inference.
Most state-of-the-art EL methods utilize collec-
tive inference to link a set of coherent men-
tions simultaneously by selecting the most coher-
ent set of entity candidates on the KB side (Pan
et al., 2015; Huang et al., 2014; Cheng and Roth,
2013; Cassidy et al., 2012; Xu et al., 2012).
In this work, without explicit relations between
entities in different lists, we only take the co-
occurrence of mentions in the same list into con-
sideration. Therefore, our method is unable to
benefit from the co-occurrence of John Lennon
and Give Peace a Chance although they are ac-
tually strongly connected.

6 Conclusions and Future Work

In this paper, we proposed a novel framework to
tackle the problem of List-only Entity Linking.
The core of this framework is selecting seed men-
tions for each entity list to bridge the gap between
mentions and non-informative target entities. Our
results show this EL framework works well for this
task. At present, in the seed selection step, we
simply consider all co-occurring mentions of enti-
ties in the same list. In the future, we will employ
more precise approaches to choose co-occurring
mentions and mine relations between entities in
separate lists to improve seed selection and entity
disambiguation.

Acknowledgments

This work was supported by the DARPA DEFT
No.FA8750-13-2-0041, U.S. DARPA LORELEI
Program No. HR0011-15-C-0115, U.S. ARL NS-
CTA No. W911NF-09-2-0053, and NSF IIS-
1523198. The views and conclusions contained in
this document are those of the authors and should
not be interpreted as representing the official poli-
cies, either expressed or implied, of the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on.

References

Yixin Cao, Juanzi Li, Xiaofei Guo, Shuanhu Bai, Heng
Ji, and Jie Tang. 2015. Name list only? target
entity disambiguation in short texts. In EMNLP.
https://doi.org/10.18653/v1/D15-1077.

Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz
Zubiaga, and Hongzhao Huang. 2012. Anal-
ysis and enhancement of wikification for mi-
croblogs with context expansion. In COLING.
http://aclweb.org/anthology/C12-1028.

Xiao Cheng and Dan Roth. 2013. Relational
inference for wikification. In EMNLP.
http://aclweb.org/anthology/D13-1184.

Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng
Ji, and Chin-Yew Lin. 2014. Collective tweet wiki-
fication based on semi-supervised graph regulariza-
tion. In ACL. https://doi.org/10.3115/v1/P14-1036.

Yang Li, Shulong Tan, Huan Sun, Jiawei Han, Dan
Roth, and Xifeng Yan. 2016. Entity disambiguation
with linkless knowledge bases. In WWW.

540



Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, Heng
Ji, and Kevin Knight. 2015. Unsupervised entity
linking with abstract meaning representation. In
NAACL. https://doi.org/10.3115/v1/N15-1119.

Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class min-
ing: Distributional vs. pattern-based approaches. In
COLING. http://aclweb.org/anthology/C10-1112.

Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Sura-
jit Chaudhuri. 2012. Targeted disambiguation of ad-
hoc, homogeneous sets of named entities. In WWW.
https://doi.org/10.1145/2187836.2187934.

Jian Xu, Qin Lu, Jie Liu, and Ruifeng
Xu. 2012. NLP-comp in TAC 2012 en-
tity linking and slot-filling. In TAC.
https://tac.nist.gov//publications/2012/papers.html.

541


	List-only Entity Linking

