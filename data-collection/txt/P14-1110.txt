



















































Polylingual Tree-Based Topic Models for Translation Domain Adaptation


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166–1176,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Polylingual Tree-Based Topic Models for Translation Domain Adaptation

Yuening Hu†
Computer Science

University of Maryland
ynhu@cs.umd.edu

Ke Zhai†
Computer Science

University of Maryland
zhaike@cs.umd.edu

Vladimir Eidelman
FiscalNote Inc.
Washington DC

vlad@fiscalnote.com

Jordan Boyd-Graber
iSchool and UMIACS

University of Maryland
jbg@umiacs.umd.edu

Abstract

Topic models, an unsupervised technique
for inferring translation domains improve
machine translation quality. However, pre-
vious work uses only the source language
and completely ignores the target language,
which can disambiguate domains. We pro-
pose new polylingual tree-based topic mod-
els to extract domain knowledge that con-
siders both source and target languages and
derive three different inference schemes.
We evaluate our model on a Chinese to En-
glish translation task and obtain up to 1.2
BLEU improvement over strong baselines.

1 Introduction

Probabilistic topic models (Blei and Lafferty,
2009), exemplified by latent Dirichlet alloca-
tion (Blei et al., 2003, LDA), are one of the most
popular statistical frameworks for navigating large
unannotated document collections. Topic models
discover—without any supervision—the primary
themes presented in a dataset: the namesake topics.

Topic models have two primary applications: to
aid human exploration of corpora (Chang et al.,
2009) or serve as a low-dimensional representa-
tion for downstream applications. We focus on
the second application, which has been fruitful for
computer vision (Li Fei-Fei and Perona, 2005),
computational biology (Perina et al., 2010), and
information retrieval (Kataria et al., 2011).

In particular, we use topic models to aid statisti-
cal machine translation (Koehn, 2009, SMT). Mod-
ern machine translation systems use millions of
examples of translations to learn translation rules.
These systems work best when the training corpus
has consistent genre, register, and topic. Systems
that are robust to systematic variation in the train-
ing set are said to exhibit domain adaptation.

† indicates equal contributions.

As we review in Section 2, topic models are
a promising solution for automatically discover-
ing domains in machine translation corpora. How-
ever, past work either relies solely on monolingual
source-side models (Eidelman et al., 2012; Hasler
et al., 2012; Su et al., 2012), or limited modeling
of the target side (Xiao et al., 2012). In contrast,
machine translation uses inherently multilingual
data: an SMT system must translate a phrase or sen-
tence from a source language to a different target
language, so existing applications of topic mod-
els (Eidelman et al., 2012) are wilfully ignoring
available information on the target side that could
aid domain discovery.

This is not for a lack of multilingual topic mod-
els. Topic models bridge the chasm between lan-
guages using document connections (Mimno et
al., 2009), dictionaries (Boyd-Graber and Resnik,
2010), and word alignments (Zhao and Xing, 2006).
In Section 2, we review these models for discover-
ing topics in multilingual datasets and discuss how
they can improve SMT.

However, no models combine multiple bridges
between languages. In Section 3, we create a
model—the polylingual tree-based topic models
(ptLDA)—that uses information from both external
dictionaries and document alignments simultane-
ously. In Section 4, we derive both MCMC and
variational inference for this new topic model.

In Section 5, we evaluate our model on the task
of SMT using aligned datasets. We show that ptLDA
offers better domain adaptation than other topic
models for machine translation. Finally, in Sec-
tion 6, we show how these topic models improve
SMT with detailed examples.

2 Topic Models for Machine Translation

Before considering past approaches using topic
models to improve SMT, we briefly review lexical
weighting and domain adaptation for SMT.

1166



2.1 Statistical Machine Translation
Statistical machine translation casts machine trans-
lation as a probabilistic process (Koehn, 2009). For
a parallel corpus of aligned source and target sen-
tences (F , E), a phrase f̄ ∈ F is translated to a
phrase ē ∈ E according to a distribution pw(ē|f̄).
One popular method to estimate the probability
pw(ē|f̄) is via lexical weighting features.
Lexical Weighting In phrase-based SMT, lexi-
cal weighting features estimate the phrase pair
quality by combining lexical translation probabil-
ities of words in a phrase (Koehn et al., 2003).
Lexical conditional probabilities p(e|f) are maxi-
mum likelihood estimates from relative lexical fre-
quencies c(f, e)/

∑
e c(f, e) , where c(f, e) is the

count of observing lexical pair (f, e) in the train-
ing dataset. The phrase pair probabilities pw(ē|f̄)
are the normalized product of lexical probabili-
ties of the aligned word pairs within that phrase
pair (Koehn et al., 2003). In Section 2.2, we create
topic-specific lexical weighting features.

Cross-Domain SMT A SMT system is usu-
ally trained on documents with the same genre
(e.g., sports, business) from a similar style (e.g.,
newswire, blog-posts). These are called domains.
Translations within one domain are better than
translations across domains since they vary dra-
matically in their word choices and style. A correct
translation in one domain may be inappropriate in
another domain. For example, “潜水” in a newspa-
per usually means “underwater diving”. On social
media, it means a non-contributing “lurker”.

Domain Adaptation for SMT Training a SMT
system using diverse data requires domain adap-
tation. Early efforts focus on building separate
models (Foster and Kuhn, 2007) and adding fea-
tures (Matsoukas et al., 2009) to model domain
information. Chiang et al. (2011) combine these
approaches by directly optimizing genre and col-
lection features by computing separate translation
tables for each domain.

However, these approaches treat domains as
hand-labeled, constant, and known a priori. This
setup is at best expensive and at worst infeasible for
large data. Topic models provide a solution where
domains can be automatically induced from raw
data: treat each topic as a domain.1

1Henceforth we will use the term “topic” and “domain”
interchangeably: “topic” to refer to the concept in topic models
and “domain” to refer to SMT corpora.

2.2 Inducing Domains with Topic Models

Topic models take the number of topics K and a
collection of documents as input, where each docu-
ment is a bag of words. They output two distribu-
tions: a distribution over topics for each document
d; and a distribution over words for each topic. If
each topic defines a SMT domain, the document’s
topic distribution is a soft domain assignment for
that document.

Given the soft domain assignments, Eidelman et
al. (2012) extract lexical weighting features condi-
tioned on the topics, optimizing feature weights us-
ing the Margin Infused Relaxed Algorithm (Cram-
mer et al., 2006, MIRA). The topics come from
source documents only and create topic-specific
lexical weights from the per-document topic distri-
bution p(k | d). The lexical probability conditioned
on the topic is expected count ek(e, f) of a word
translation pair under topic k,

ĉk(e, f) =
∑

d p(k|d)cd(e, f), (1)

where cd(•) is the number of occurrences of the
word pair in document d. The lexical probability
conditioned on topic k is the unsmoothed probabil-
ity estimate of those expected counts

pw(e|f ; k) = ĉk(e,f)∑
e ĉk(e,f)

, (2)

from which we can compute the phrase pair proba-
bilities pw(ē|f̄ ; k) by multiplying the lexical prob-
abilities and normalizing as in Koehn et al. (2003).

For a test document d, the document topic dis-
tribution p(k | d) is inferred based on the topics
learned from training data. The feature value of a
phrase pair (ē, f̄) is

fk(ē|f̄) = − log
{
pw(ē|f̄ ; k) · p(k|d)

}
, (3)

a combination of the topic dependent lexical weight
and the topic distribution of the document, from
which we extract the phrase. Eidelman et al. (2012)
compute the resulting model score by combining
these features in a linear model with other standard
SMT features and optimizing the weights.

Conceptually, this approach is just reweighting
examples. The probability of a topic given a docu-
ment is never zero. Every translation observed in
the training set will contribute to pk(e|f); many of
the expected counts, however, will be less than one.
This obviates the explicit smoothing used in other
domain adaptation systems (Chiang et al., 2011).

1167



We adopt this framework in its entirety. Our
contribution are topics that capture multilingual
information and thus better capture the domains in
the parallel corpus.

2.3 Beyond Vanilla Topic Models
Eidelman et al. (2012) ignore a wealth of infor-
mation that could improve topic models and help
machine translation. Namely, they only use mono-
lingual data from the source language, ignoring all
target-language data and available lexical semantic
resources between source and target languages.

Different complement each other to reduce ambi-
guity. For example, “木马” in a Chinese document
can be either “hobbyhorse” in a children’s topic,
or “Trojan virus” in a technology topic. A short
Chinese context obscures the true topic. However,
these terms are unambiguous in English, revealing
the true topic.

While vanilla topic models (LDA) can only be
applied to monolingual data, there are a number
of topic models for parallel corpora: Zhao and
Xing (2006) assume aligned word pairs share same
topics; Mimno et al. (2009) connect different lan-
guages through comparable documents. These
models take advantage of word or document align-
ment information and infer more robust topics from
the aligned dataset.

On the other hand, lexical information can in-
duce topics from multilingual corpora. For in-
stance, orthographic similarity connects words with
the same meaning in related languages (Boyd-
Graber and Blei, 2009), and dictionaries are a
more general source of information on which words
share meaning (Boyd-Graber and Resnik, 2010).

These two approaches are not mutually exclu-
sive, however; they reveal different connections
across languages. In the next section, we combine
these two approaches into a polylingual tree-based
topic model.

3 Polylingual Tree-based Topic Models

In this section, we bring existing tree-based topic
models (Boyd-Graber et al., 2007, tLDA) and
polylingual topic models (Mimno et al., 2009,
pLDA) together and create the polylingual tree-
based topic model (ptLDA) that incorporates both
word-level correlations and document-level align-
ment information.

Word-level Correlations Tree-based topic mod-
els incorporate the correlations between words by

encouraging words that appear together in a con-
cept to have similar probabilities given a topic.
These concepts can come from WordNet (Boyd-
Graber and Resnik, 2010), domain experts (An-
drzejewski et al., 2009), or user constrains (Hu et
al., 2013). When we gather concepts from bilin-
gual resources, these concepts can connect different
languages. For example, if a bilingual dictionary
defines “电脑” as “computer”, we combine these
words in a concept.

We organize the vocabulary in a tree structure
based on these concepts (Figure 1): words in the
same concept share a common parent node, and
then that concept becomes one of many children of
the root node. Words that are not in any concept—
uncorrelated words—are directly connected to
the root node. We call this structure the tree prior.

When this tree serves as a prior for topic models,
words in the same concept are correlated in topics.
For example, if “电脑” has high probability in a
topic, so will “computer”, since they share the same
parent node. With the tree priors, each topic is no
longer a distribution over word types, instead, it is a
distribution over paths, and each path is associated
with a word type. The same word could appear in
multiple paths, and each path represents a unique
sense of this word.

Document-level Alignments Lexical resources
connect languages and help guide the topics. How-
ever, these resources are sometimes brittle and may
not cover the whole vocabulary. Aligned document
pairs provide a more corpus-specific, flexible asso-
ciation across languages.

Polylingual topic models (Mimno et al., 2009)
assume that the aligned documents in different lan-
guages share the same topic distribution and each
language has a unique topic distribution over its
word types. This level of connection between lan-
guages is flexible: instead of requiring the exact
matching on words and sentences, only a coarse
document alignment is necessary, as long as the
documents discuss the same topics.

Combine Words and Documents We propose
polylingual tree-based topic models (ptLDA),
which connect information across different lan-
guages by incorporating both word correlation (as
in tLDA) and document alignment information (as
in pLDA). We initially assume a given tree struc-
ture, deferring the tree’s provenance to the end of
this section.

1168



Generative Process As in LDA, each word to-
ken is associated with a topic. However, tree-based
topic models introduce an additional step of select-
ing a concept in a topic responsible for generating
each word token. This is represented by a path yd,n
through the topic’s tree.

The probability of a path in a topic depends on
the transition probabilities in a topic. Each concept
i in topic k has a distribution over its children nodes
is governed by a Dirichlet prior: πk,i ∼ Dir(βi).
Each path ends in a word (i.e., a leaf node) and
the probability of a path is the product of all of
the transitions between topics it traverses. Topics
have correlations over words because the Dirichlet
parameters can encode positive or negative correla-
tions (Andrzejewski et al., 2009).

With these correlated in topics in hand, the gen-
eration of documents are very similar to LDA. For
every document d, we first sample a distribution
over topics θd from a Dirichlet prior Dir(α). For
every token in the documents, we first sample a
topic zdn from the multinomial distribution θd, and
then sample a path ydn along the tree according to
the transition distributions specified by topic zdn.
Because every path ydn leads to a word wdn in lan-
guage ldn, we append the sampled word wdn to
document dldn . Aligned documents have words in
both languages; monolingual documents only have
words in a single language.

The full generative process is:
1: for topic k ∈ 1, · · · ,K do
2: for each internal node ni do
3: draw a distribution πki ∼ Dir(βi)
4: for document set d ∈ 1, · · · , D do
5: draw a distribution θd ∼ Dir(α)
6: for each word in documents d do
7: choose a topic zdn ∼ Mult(θd)
8: sample a path ydn with probability∏

(i,j)∈ydn πzdn,i,j
9: ydn leads to word wdn in language ldn

10: append token wdn to document dldn

If we use a flat symmetric Dirichlet prior instead
of the tree prior, we recover pLDA; and if all docu-
ments are monolingual (i.e., with distinct distribu-
tions over topics θ), we recover tLDA. ptLDA con-
nects different languages on both the word level (us-
ing the word correlations) and the document level
(using the document alignments). We compare
these models’ machine translation performance in
Section 5.

computer, 
market, 市
government, 政府
science, 科学

Dictionary: Vocabulary: English (0), Chinese (1)

computer market 市 government 政府 science 科学

天气scientific policy

0    scientific
0    policy
1    
1    市

0    computer  
0    market
0    government
0    science

1    政府
1    科学
1    天气

Prior Tree:  0  1

Figure 1: An example of constructing a prior tree
from a bilingual dictionary: word pairs with the
same meaning but in different languages are con-
cepts; we create a common parent node to group
words in a concept, and then connect to the root; un-
correlated words are connected to the root directly.
Each topic uses this tree structure as a prior.

Build Prior Tree Structures One remaining
question is the source of the word-level connections
across languages for the tree prior. We consider
two resources to build trees that correlate words
across languages. The first are a multilingual dic-
tionaries (dict), which match words with the same
meaning in different languages together. These re-
lations between words are used as the concepts in
the prior tree (Figure 1).

In addition, we extract the word alignments from
aligned sentences in a parallel corpus. The word
pairs define concepts for the prior tree (align). We
use both resources for our models (denoted as
ptLDA-dict and ptLDA-align) in our experiments
(Section 5) and show that they yield comparable
performance in SMT.

4 Inference

Inference of probabilistic models discovers the pos-
terior distribution over latent variables. For a col-
lection of D documents, each of which contains
Nd number of words, the latent variables of ptLDA
are: transition distributions πki for every topic k
and internal node i in the prior tree structure; multi-
nomial distributions over topics θd for every docu-
ment d; topic assignments zdn and path ydn for the
nth word wdn in document d. The joint distribution
of polylingual tree-based topic models is

p(w, z,y,θ,π;α, β) =
∏
k

∏
i p(πki|βi) (4)

·∏d p(θd|α) ·∏d∏n p(zdn|θd)
·∏d∏n (p(ydn|zdn,π)p(wdn|ydn)).

Exact inference is intractable, so we turn to ap-

1169



proximate posterior inference to discover the latent
variables that best explain our data. Two widely
used approximation approaches are Markov chain
Monte Carlo (Neal, 2000, MCMC) and variational
Bayesian inference (Blei et al., 2003, VB). Both
frameworks produce good approximations of the
posterior mode (Asuncion et al., 2009). In addition,
Mimno et al. (2012) propose hybrid inference that
takes advantage of parallelizable variational infer-
ence for global variables (Wolfe et al., 2008) while
enjoying the sparse, efficient updates for local vari-
ables (Neal, 1993). In the rest of this section, we
discuss all three methods in turn.

We explore multiple inference schemes because
while all of these methods optimize likelihood be-
cause they might give different results on the trans-
lation task.

4.1 Markov Chain Monte Carlo Inference
We use a collapsed Gibbs sampler for tree-based
topic models to sample the path ydn and topic as-
signment zdn for word wdn,

p(zdn = k, ydn = s|¬zdn,¬ydn,w;α,β)
∝ I [Ω(s) = wdn] · Nk|d+α∑

k′ (Nk′|d+α)

·∏i→j∈s Ni→j|k+βi→j∑
j′ (Ni→j′|k+βi→j′ )

,

where Ω(s) represents the word that path s leads
to, Nk|d is the number of tokens assigned to topic k
in document d and Ni→j|k is the number of times
edge i→ j in the tree assigned to topic k, exclud-
ing the topic assignment zdn and its path ydn of
current token wdn. In practice, we sample the la-
tent variables using efficient sparse updates (Yao et
al., 2009; Hu and Boyd-Graber, 2012).

4.2 Variational Bayesian Inference
Variational Bayesian inference approximates the
posterior distribution with a simplified variational
distribution q over the latent variables: document
topic proportions θ, transition probabilities π, topic
assignments z, and path assignments y.

Variational distributions typically assume a
mean-field distribution over these latent variables,
removing all dependencies between the latent vari-
ables. We follow this assumption for the transi-
tion probabilities q(π |λ) and the document topic
proportions q(θ |γ); both are variational Dirichlet
distributions. However, due to the tight coupling
between the path and topic variables, we must
model this joint distribution as one multinomial,

q(z,y |φ). If word token wdn has K topics and
S paths, it has a K ∗ S length variational multino-
mial φdnks, which represents the probability that
the word takes path s in topic k. The complete
variational distribution is

q(θ,π, z,y|γ,λ,φ) = ∏d q(θd|γd)· (5)∏
k

∏
i q(πki|λki) ·

∏
d

∏
n q(zdn, ydn|φdn).

Our goal is to find the variational distribution q
that is closest to the true posterior, as measured by
the Kullback-Leibler (KL) divergence between the
true posterior p and variational distribution q. This
induces an “evidence lower bound” (ELBO, L) as a
function of a variational distribution q: L =

Eq[log p(w, z,y,θ,π)]− Eq[log q(θ,π, z,y)]
=
∑

k

∑
i Eq[log p(πki|βi)]

+
∑

d Eq[log p(θd|α)]
+
∑

d

∑
n Eq[log p(zdn, ydn|θd,π)p(wdn|ydn)]

+ H[q(θ)] + H[q(π)] + H[q(z,y)], (6)

where H[•] represents the entropy of a distribution.
Optimizing L using coordinate descent provides
the following updates:

φdnkt ∝ exp{Ψ(γdk)−Ψ(
∑

k γdk) (7)
+
∑

i→j∈s
(
Ψ(λk,i→j)−Ψ(

∑
j′ λk,i→j′)

)};
γdk = αk +

∑
n

∑
s∈Ω−1(wdn) φdnkt; (8)

λk,i→j = βi→j (9)
+
∑

d

∑
n

∑
s∈Ω′(wdn) φdnktI [i→ j ∈ s] ;

where Ω′(wdn) is the set of all paths that lead to
wordwdn in the tree, and t represents one particular
path in this set. I [i→ j ∈ s] is the indicator of
whether path s contains an edge from node i to j.

4.3 Hybrid Stochastic Inference

Given the complementary strengths of MCMC and
VB, and following hybrid inference proposed by
Mimno et al. (2012), we also derive hybrid infer-
ence for ptLDA.

The transition distributions π are treated identi-
cally as in variational inference. We posit a varia-
tional Dirichlet distribution λ and choose the one
that minimizes the KL divergence between the true
posterior and the variational distribution.

For topic z and path y, instead of variational
updates, we use a Gibbs sampler within a document.
We sample zdn and ydn conditioned on the topic

1170



and path assignments of all other document tokens,
based on the variational expectation of π,

q(zdn = k, ydn = s|¬zdn,¬ydn;w) ∝ (10)
(α+

∑
m 6=n I [zdm = k])

· exp{Eq[log p(ydn|zdn,π)p(wdn|ydn)]}.

This equation embodies how this is a hybrid algo-
rithm: the first term resembles the Gibbs sampling
term encoding how much a document prefers a
topic, while the second term encodes the expecta-
tion under the variational distribution of how much
a path is preferred by this topic,

Eq[log p(ydn|zdn,π)p(wdn|ydn)] = I[Ω(ydn)=wdn]
·∑i→j∈ydn Eq[log λzdn,i→j ].

For every document, we sweep over all its to-
kens and resample their topic zdn and path ydn
conditioned on all the other tokens’ topic and path
assignments ¬zdn and ¬ydn. To avoid bias, we
discard the first B burn-in sweeps and take the
following M samples. We then use the empirical
average of these samples update the global varia-
tional parameter q(π|λ) based on how many times
we sampled these paths

λk,i→j = 1M
∑

d

∑
n

∑
s∈Ω−1(wdn)

(
I [i→ j ∈ s]

· I [zdn = k, ydn = s]
)

+ βi→j . (11)

For our experiments, we use the recommended set-
tingsB = 5 andM = 5 from Mimno et al. (2012).

5 Experiments

We evaluate our new topic model, ptLDA, and exist-
ing topic models—LDA, pLDA, and tLDA—on their
ability to induce domains for machine translation
and the resulting performance of the translations
on standard machine translation metrics.

Dataset and SMT Pipeline We use the NIST MT
Chinese-English parallel corpus (NIST), excluding
non-UN and non-HK Hansards portions as our train-
ing dataset. It contains 1.6M sentence pairs, with
40.4M Chinese tokens and 44.4M English tokens.
We replicate the SMT pipeline of Eidelman et al.
(2012): word segmentation (Tseng et al., 2005),
align (Och and Ney, 2003), and symmetrize (Koehn
et al., 2003) the data. We train a modified Kneser-
Ney trigram language model on English (Chen and
Goodman, 1996). We use CDEC (Dyer et al., 2010)
for decoding, and MIRA (Crammer et al., 2006)

for parameter training. To optimize SMT system,
we tune the parameters on NIST MT06, and report
results on three test sets: MT02, MT03 and MT05.2

Topic Models Configuration We compare our
polylingual tree-based topic model (ptLDA) against
tree-based topic models (tLDA), polylingual topic
models (pLDA) and vanilla topic models (LDA).3

We also examine different inference algorithms—
Gibbs sampling (gibbs), variational inference
(variational) and hybrid approach (variational-
hybrid)—on the effects of SMT performance. In
all experiments, we set the per-document Dirichlet
parameter α = 0.01 and the number of topics to
10, as used in Eidelman et al. (2012).

Resources for Prior Tree To build the tree for
tLDA and ptLDA, we extract the word correla-
tions from a Chinese-English bilingual dictio-
nary (Denisowski, 1997).4 We filter the dictionary
using the NIST vocabulary, and keep entries map-
ping single Chinese and single English words. The
prior tree has about 1000 word pairs (dict).

We also extract the bidirectional word align-
ments between Chinese and English using
GIZA++ (Och and Ney, 2003). We then remove
the word pairs appearing more than 50K times or
fewer than 500 times and construct a second prior
tree with about 2500 word pairs (align).

We apply both trees to tLDA and ptLDA, denoted
as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA-
align. However, tLDA-align and ptLDA-align do
worse than tLDA-dict and ptLDA-dict, so we omit
tLDA-align in the results.

Domain Adaptation using Topic Models We
examine the effectiveness of using topic models
for domain adaptation on standard SMT evalua-
tion metrics—BLEU (Papineni et al., 2002) and
TER (Snover et al., 2006). We report the results
on three different test sets (Figure 2), and all SMT
results are averaged over five runs.

We refer to the SMT model without domain adap-
tation as baseline.5 LDA marginally improves ma-
chine translation (less than half a BLEU point).

2The NIST datasets contain 878, 919, 1082 and 1664 sen-
tences for MT02, MT03, MT05 and MT06 respectively.

3For Gibbs sampling, we use implementations available in
Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum,
2002) for LDA and pLDA.

4This is a two-level tree structure. However, one could
build a more sophisticated tree prior with a hierarchical dictio-
nary such as multilingual WordNet.

5Our replication of Eidelman et al. (2012) yields slightly
higher baseline performance, but the trend is consistent.

1171



gibbs variational variational−hybrid

34.8 +0.3
+0.6 +0.4

+1.2
+0.5

35.1 +0.1 +0.3 +0.2
+0.7 +0.4

31.4 +0.4
+0.7 +0.4

+1
+0.4

34.8 +0.4
+0.5 +0.4 +0.8 +0.5

35.1 −0.1 +0.2 −0.1 +0.2 +0.2

31.4 +0.3
+0.5 +0.3

+0.8 +0.4

34.8 +0.2 +0.4 +0.2
+0.7 +0.4

35.1 −0.1 −0.1 −0.1 +0.2 +0.2

31.4 +0.3 +0.3 +0.1
+0.6 +0.3

31
32
33
34
35
36
37

31
32
33
34
35
36
37

31
32
33
34
35
36
37

m
t02

m
t03

m
t05

B
LE

U
 S

co
re

model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict

gibbs variational variational−hybrid

61.9 −0.1
−1 −1.2

−2.5
−1.1

60.1 −0.3 −0.9 −0.8
−1.9

−0.9

63.3
−0.9 −1.3 −1.2

−2.6
−1.1

61.9 −0.4 −1 −0.6 −1.6 −1.3

60.1 −0.2 −0.5 −0.1 −1 −0.7

63.3 −0.5 −1 −0.4 −1.5 −1.2

61.9 −0.3 −0.7 −0.1
−1.6 −0.9

60.1 0 −0.2 +0.2
−1.1 −0.5

63.3 −0.4 −0.7 −0.1
−1.6 −0.8

56
58
60
62
64
66

56
58
60
62
64
66

56
58
60
62
64
66

m
t02

m
t03

m
t05

T
E

R
 S

co
re

model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict

Figure 2: Machine translation performance for different models and inference algorithms against the
baseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores. Our proposed ptLDA
performs best. Results are averaged over 5 random runs. For model ptLDA-dict with different inference
schemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the results
on MT03 using variational and variational-hybrid inferences.

Polylingual topic models pLDA and tree-based
topic models tLDA-dict are consistently better than
LDA, suggesting that incorporating additional bilin-
gual knowledge improves topic models. These im-
provements are not redundant: our new ptLDA-dict
model, which has aspects of both models yields the
best performance among these approaches—up to a
1.2 BLEU point gain (higher is better), and -2.6 TER
improvement (lower is better). The BLEU improve-
ment is significant (Koehn, 2004) at p = 0.01,6

except on MT03 with variational and variational-
hybrid inference.

While ptLDA-align performs better than base-
line SMT and LDA, it is worse than ptLDA-dict,
possibly because of errors in the word alignments,
making the tree priors less effective.

Scalability While gibbs has better translation
scores than variational and variational-hybrid, it
is less scalable to larger datasets. With 1.6M NIST

6Because we have multiple runs of each topic model (and
thus different translation models), we select the run closest to
the average BLEU for the translation significance test.

training sentences, gibbs takes nearly a week to
run 1000 iterations. In contrast, the parallelized
variational and variational-hybrid approaches,
which we implement in MapReduce (Dean and
Ghemawat, 2004; Wolfe et al., 2008; Zhai et al.,
2012), take less than a day to converge.

6 Discussion

In this section, we qualitatively analyze the trans-
lation results and investigate how ptLDA and its
cousins improve SMT. We also discuss other ap-
proaches to improve unsupervised domain adapta-
tion for SMT.

6.1 How do Topic Models Help SMT?

We present two examples of how topic models can
improve SMT. The first example shows both LDA
and ptLDA improve the baseline. The second exam-
ple shows how LDA introduce biases that mislead
SMT and how ptLDA’s bilingual constraints correct
these mistakes.

Figure 3 shows a sentence about a company

1172



source 新力已在北美地区售出大  , 每套售价

reference sony has already sold about 570,000 units of narrowband connection 
kits in north america at the price of about 39 us dollars and some 20 
compatible games .

baseline
LDA
ptLDA

… internet links set ...
… internet links kit … 
… internet links kit …  

… with about 20 of the game .
… , there are about 20 compatible games .
… , there are about 20 compatible games .

source …  ... … 相容游

LDA-Topic 0 (business)

ptLDA-Topic 0 (business)
reference … connection kits ... … some 20 compatible games .

, 相容游 公司(company), 中国(China), 服 (service), 市
(market), 技 (technology), 企 (industry), 提供
(provide), (develop), 年(year), (product), 
上, 合作(coorporate), 中, 管理(manage), 投
(invest), (economy), 国 (international), 系
(system), (bank)

公司(company), 服 (service), 市 (market), 技
(technology), china, 企 (industry), 

(product), market, company, technology, services, 
系 (system), year, industry, products, business, 

(economy), information, 管理(manage), 投
(invest), percent, 网 (internet), companies, world, 
system, 信息(information), 增 (increase), 
(device), service, (service)

Figure 3: Better SMT result using topic models for domain adaptation. Top row: the source sentence and
its reference translation. Middle row: the highlighted translations from different approaches. Bottom row:
the change of relevant translation probabilities after incorporating the domain knowledge from LDA and
ptLDA. Right: most-probable words of the topic the source sentence is assigned to under LDA (top) and
ptLDA (bottom). The Chinese translations are in parenthesis.

introducing new technology gadgets where both
LDA and ptLDA improve translations. The base-
line translates “套件” to “set” (red), and “相容” to
“with” (blue), which do not capture the reference
meaning of a add-on device that works with com-
patible games. Both LDA and ptLDA assign this
sentence to a business domain, which makes the
translations probabilities shift toward correct trans-
lations: the probability of translating “相容” to
“compatible” and the probability of translating “套
件” to “kit” in the business domain are both signif-
icantly larger than without the domain knowledge;
and the probabilities of translating “相容” to “with”
and the probability of translating “set” to “套件”
in the business domain decrease.

The second example (Figure 4) illustrates how
ptLDA offers further improvements over LDA. The
source sentence discusses foreign affairs. The
baseline correctly translates the word “影响” to
“affect”. However, LDA—which only takes mono-
lingual information from the source language—
assigns this sentence to economic development.
This misleads SMT to lower the probability for
the correct translation “affect”; it chooses “impact”
instead. In contrast, ptLDA—which incorporates
bilingual constraints—successfully labels this sen-
tence as foreign affairs and produces a softer, more
nuanced translation that better matches the refer-
ence. The translation of “承诺” is very similar,
except in this case, both the baseline and LDA
produce the incorrect translation “the commitment
of”. This is possible because the probabilities of
translating “承诺” to “promised to” and translat-

ing “promised to” to “承诺” (the correct transla-
tion, in both directions) increase when conditioned
on ptLDA’s correct topic but decrease when condi-
tioned on LDA’s incorrect topic.

6.2 Other Approaches
Other approaches have used topic models for ma-
chine translation. Xiao et al. (2012) present a topic
similarity model based on LDA that produces a fea-
ture that weights grammar rules based on topic
compatibility. They also model the source and tar-
get side of rules and compare the target similarity
during decoding by projecting the target distribu-
tion into the source space. Hasler et al. (2012)
use the source-side topic assignments from hidden
topic Markov models (Gruber et al., 2007, HTMM)
which models documents as a Markov chain and
assign one topic to the whole sentence, instead of
a mixture of topics. Su et al. (2012) also apply
HTMM to monolingual data and apply the results to
machine translation. To our knowledge, however,
this is the first work to use multilingual topic mod-
els for domain adaptation in machine translation.

6.3 Improving Language Models
Topic models capture document-level properties
of language, but a critical component of machine
translation systems is the language model, which
provides local constraints and preferences. Do-
main adaptation for language models (Bellegarda,
2004; Wood and Teh, 2009) is an important avenue
for improving machine translation. Models that si-
multaneously discover global document themes as
well as local, contextual domain-specific informa-

1173



source 消息指出, 国使 人 向中方官 表示, 国方面并没有支持朝 人以 种方法前往 国, 国并不希望 类事件再次
生, 以免 中国和朝 半 双方 的关系 来影响, 国方面并向中国方面承 , 愿意 助中国管理好在京的 国居民

reference sources said rok embassy personnel told chinese officials that rok has not backed any dpr koreans to get to rok in such a manner 
and rok would not like such things happen again to affect relationship between china and the two sides of the korean peninsula . 
rok also promised to assist china in the administration of koreans in beijing .

baseline
LDA
ptLDA

… does not want ...
… does not hope that ... 
… does not hope that ...

source … 不希望 ...

LDA-Topic 5 (economic development) ptLDA-Topic 2 (foreign affairs)

… so as to avoid impact the relations… 
… so as not to affect the relations…  

… so as not to affect the relations… … south korea and the commitment of the chinese side ...
… the rok side , and the commitment of the chinese side ...
… south korea has promised to the chinese side ...

… 以免 ...关系 来影响... … 国方面并向中国方面承 …
reference … would not like ... … to affect the relationship … … rok also promised to the chinese side ...

(develop), 国(country), 两(two), 中国(China), 关系(relation), 
中, 合作(cooperate), (economy), 人民(people), 友好(friendly), 
国家(country), 新(new), (problem), 上, 加强(emphasize), 重要
(important), 和平(peace), 共同(together), 建 (build), 世界(world)

china, (issue), military, united, president, 国家(country), 地区(area), minister, 伊
拉克(Iraq), 和平(peace), nuclear, people, (president), peace, security, 
(UN), (military), 以色列(Israel), iraq, foreign, international, 部 (army), beijing, 
world, defense, south, 安 全(security), war, (agreement), 会 (conference)

Figure 4: Better SMT result using ptLDA compared to LDA and the baseline. Top row: the source sentence
and a reference translation. Second row: the highlighted translations from different models. Third row:
the change of relevant translation probabilities after incorporating domain knowledge from LDA and
ptLDA. Bottom row: most-probable words for the topics the source sentence is assigned to under LDA
(left) and ptLDA (right). The meanings of Chinese words are in parenthesis.

tion (Wallach, 2006; Boyd-Graber and Blei, 2008)
may offer further improvements.

6.4 External Data
The topic models presented here only require weak
alignment between documents at the document
level. Extending to larger datasets for learning
topics is straightforward in principle. For exam-
ple, ptLDA could learn domains from a much larger
corpus like Wikipedia and then apply the extracted
domains to machine translation data. However,
this presents further challenges, as Wikipedia’s do-
mains are not representative of newswire machine
translation datasets; a flexible hierarchical topic
model (Teh et al., 2006) would better distinguish
useful domains from extraneous ones.

7 Conclusion

Topic models generate great interest, but their use
in “real world” applications still lags; this is par-
ticularly true for multilingual topic models. As
topic models become more integrated in common-
place applications, their adoption, understanding,
and robustness will improve.

This paper contributes to the deeper integration
of topic models into critical applications by present-
ing a new multilingual topic model, ptLDA, com-
paring it with other multilingual topic models on
a machine translation task, and showing that these
topic models improve machine translation. ptLDA

models both source and target data to induce do-
mains from both dictionaries and alignments. Fur-
ther improvement is possible by incorporating topic
models deeper in the decoding process and adding
domain knowledge to the language model.

Acknowledgments

We would like to thank the anonymous reviewers,
Doug Oard, and John Morgan for their helpful com-
ments, and thank Junhui Li and Ke Wu for insight-
ful discussions. This work was supported by NSF
Grant IIS-1320538. Boyd-Graber is also supported
by NSF Grant CCF-1018625. Any opinions, find-
ings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.

References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.

2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings
of the International Conference of Machine Learn-
ing.

Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of Uncertainty in
Artificial Intelligence.

Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. vol-
ume 42, pages 93–108.

1174



David M. Blei and John D. Lafferty. 2009. Visualizing
topics with Multi-Word expressions. arXiv.

David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3.

Jordan Boyd-Graber and David M. Blei. 2008. Syn-
tactic topic models. In Proceedings of Advances in
Neural Information Processing Systems.

Jordan Boyd-Graber and David M. Blei. 2009. Multi-
lingual topic models for unaligned text. In Proceed-
ings of Uncertainty in Artificial Intelligence.

Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual
supervised latent Dirichlet allocation. In Proceed-
ings of Emperical Methods in Natural Language
Processing.

Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proceedings of Emperical Methods in Natu-
ral Language Processing.

Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Proceedings of Advances in Neural Information Pro-
cessing Systems.

Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the Association for
Computational Linguistics.

David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In
Proceedings of the Human Language Technology
Conference.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.

Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified data processing on large clusters.
In Symposium on Operating System Design and Im-
plementation.

Paul Denisowski. 1997. CEDICT.
http://www.mdbg.net/chindict/.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.

Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the Association
for Computational Linguistics.

George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation.

Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.
2007. Hidden topic Markov models. In Artificial
Intelligence and Statistics.

Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse lexicalised features and topic adaptation for
SMT. In Proceedings of IWSLT.

Yuening Hu and Jordan Boyd-Graber. 2012. Efficient
tree-based topic modeling. In Proceedings of the As-
sociation for Computational Linguistics.

Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2013. Interactive topic modeling.
Machine Learning Journal.

Saurabh S. Kataria, Krishnan S. Kumar, Rajeev R. Ras-
togi, Prithviraj Sen, and Srinivasan H. Sengamedu.
2011. Entity disambiguation with hierarchical topic
models. In Knowledge Discovery and Data Mining.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Emperical Methods in Natural Language Process-
ing.

Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.

Li Fei-Fei and Pietro Perona. 2005. A Bayesian hier-
archical model for learning natural scene categories.
In Computer Vision and Pattern Recognition.

Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of Em-
perical Methods in Natural Language Processing.

Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.

David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of Emper-
ical Methods in Natural Language Processing.

David Mimno, Matthew Hoffman, and David Blei.
2012. Sparse stochastic inference for latent Dirich-
let allocation. In Proceedings of the International
Conference of Machine Learning.

Radford M. Neal. 1993. Probabilistic inference using
Markov chain Monte Carlo methods. Technical Re-
port CRG-TR-93-1, University of Toronto.

1175



Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9(2):249–
265.

Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics,
pages 311–318.

Alessandro Perina, Pietro Lovato, Vittorio Murino, and
Manuele Bicego. 2010. Biologically-aware latent
Dirichlet allocation (balda) for the classification of
expression microarray. In Proceedings of the 5th
IAPR international conference on Pattern recogni-
tion in bioinformatics, PRIB’10.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas.

Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the Association for Computational
Linguistics.

Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In SIGHAN
Workshop on Chinese Language Processing.

Hanna M. Wallach. 2006. Topic modeling: Beyond
bag-of-words. In Proceedings of the International
Conference of Machine Learning.

Jason Wolfe, Aria Haghighi, and Dan Klein. 2008.
Fully distributed EM for very large datasets. In Pro-
ceedings of the International Conference of Machine
Learning, pages 1184–1191.

Frank Wood and Yee Whye Teh. 2009. A hierarchi-
cal nonparametric Bayesian approach to statistical
language model domain adaptation. In Proceedings
of the International Conference on Artificial Intelli-
gence and Statistics, volume 12.

Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proceedings
of the Association for Computational Linguistics.

Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Knowledge
Discovery and Data Mining.

Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-
hamad Alkhouja. 2012. Mr. LDA: A flexible large
scale topic modeling package using variational infer-
ence in mapreduce. In Proceedings of World Wide
Web Conference.

Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the Association for Computational Lin-
guistics.

1176


