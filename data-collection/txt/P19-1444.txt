




















































Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4524–4535
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4524

Towards Complex Text-to-SQL in Cross-Domain Database with
Intermediate Representation

Jiaqi Guo1∗, Zecheng Zhan2∗, Yan Gao3, Yan Xiao3, Jian-Guang Lou3

Ting Liu1, Dongmei Zhang3

1Xi’an Jiaotong University, Xi’an, China
2Beijing University of Posts and Telecommunications, Beijing, China

3Microsoft Research Asia, Beijing, China

jasperguo2013@stu.xjtu.edu.cn,zhanzecheng@bupt.edu.cn

{Yan.Gao,Yan.Xiao,jlou,dongmeiz}@microsoft.com
tingliu@mail.xjtu.edu.cn

Abstract

We present a neural approach called IRNet for

complex and cross-domain Text-to-SQL. IR-

Net aims to address two challenges: 1) the

mismatch between intents expressed in natu-

ral language (NL) and the implementation de-

tails in SQL; 2) the challenge in predicting

columns caused by the large number of out-

of-domain words. Instead of end-to-end syn-

thesizing a SQL query, IRNet decomposes the

synthesis process into three phases. In the

first phase, IRNet performs a schema link-

ing over a question and a database schema.

Then, IRNet adopts a grammar-based neural

model to synthesize a SemQL query which is

an intermediate representation that we design

to bridge NL and SQL. Finally, IRNet deter-

ministically infers a SQL query from the syn-

thesized SemQL query with domain knowl-

edge. On the challenging Text-to-SQL bench-

mark Spider, IRNet achieves 46.7% accuracy,
obtaining 19.5% absolute improvement over
previous state-of-the-art approaches. At the

time of writing, IRNet achieves the first po-

sition on the Spider leaderboard.

1 Introduction

Recent years have seen a great deal of renewed

interest in Text-to-SQL, i.e., synthesizing a SQL

query from a question. Advanced neural ap-

proaches synthesize SQL queries in an end-to-end

manner and achieve more than 80% exact match-
ing accuracy on public Text-to-SQL benchmarks

(e.g., ATIS, GeoQuery and WikiSQL) (Krishna-

murthy et al., 2017; Zhong et al., 2017; Xu et al.,

2017; Yaghmazadeh et al., 2017; Yu et al., 2018a;

Dong and Lapata, 2018; Wang et al., 2018; Hwang

et al., 2019). However, Yu et al. (2018c) yields

unsatisfactory performance of state-of-the-art ap-

∗ Equal Contributions. Work done during an internship
at MSRA.

NL: Show the names of students who have a grade higher 

than 5 and have at least 2 friends.

SQL: SELECT T1.name 

FROM friend AS T1 JOIN highschooler AS T2 

ON T1.student_id = T2.id WHERE T2.grade > 5 

GROUP BY T1.student_id HAVING count(*) >= 2

Figure 1: An example from the Spider benchmark to

illustrate the mismatch between the intent expressed in

NL and the implementation details in SQL. The column

‘student id’ to be grouped by in the SQL query is not

mentioned in the question.

proaches on a newly released, cross-domain Text-

to-SQL benchmark, Spider.

The Spider benchmark brings new challenges

that prove to be hard for existing approaches.

Firstly, the SQL queries in the Spider contain

nested queries and clauses like GROUPBY and

HAVING, which are far more complicated than

that in another well-studied cross-domain bench-

mark, WikiSQL (Zhong et al., 2017). Considering

the example in Figure 1, the column ‘student id’

to be grouped by in the SQL query is never men-

tioned in the question. In fact, the GROUPBY

clause is introduced in SQL to facilitate the im-

plementation of aggregate functions. Such imple-

mentation details, however, are rarely considered

by end users and therefore rarely mentioned in

questions. This poses a severe challenge for ex-

isting end-to-end neural approaches to synthesize

SQL queries in the absence of detailed specifica-

tion. The challenge in essence stems from the

fact that SQL is designed for effectively query-

ing relational databases instead of for represent-

ing the meaning of NL (Kate, 2008). Hence, there

inevitably exists a mismatch between intents ex-

pressed in natural language and the implementa-

tion details in SQL. We regard this challenge as a

mismatch problem.

Secondly, given the cross-domain settings of

Spider, there are a large number of out-of-domain



4525

(OOD) words. For example, 35% of words in
database schemas on the development set do not

occur in the schemas on the training set in Spi-

der. As a comparison, the number in WikiSQL is

only 22%. The large number of OOD words poses
another steep challenge in predicting columns in

SQL queries (Yu et al., 2018b), because the OOD

words usually lack of accurate representations in

neural models. We regard this challenge as a lexi-

cal problem.

In this work, we propose a neural approach,

called IRNet, towards tackling the mismatch prob-

lem and the lexical problem with intermediate

representation and schema linking. Specifically,

instead of end-to-end synthesizing a SQL query

from a question, IRNet decomposes the synthe-

sis process into three phases. In the first phase,

IRNet performs a schema linking over a question

and a schema. The goal of the schema linking

is to recognize the columns and the tables men-

tioned in a question, and to assign different types

to the columns based on how they are mentioned

in the question. Incorporating the schema linking

can enhance the representations of question and

schema, especially when the OOD words lack of

accurate representations in neural models during

testing. Then, IRNet adopts a grammar-based neu-

ral model to synthesize a SemQL query, which is

an intermediate representation (IR) that we design

to bridge NL and SQL. Finally, IRNet determin-

istically infers a SQL query from the synthesized

SemQL query with domain knowledge.

The insight behind IRNet is primarily inspired

by the success of using intermediate represen-

tations (e.g., lambda calculus (Carpenter, 1997),

FunQL (Kate et al., 2005) and DCS (Liang et al.,

2011)) in various semantic parsing tasks (Zelle

and Mooney, 1996; Berant et al., 2013; Pasupat

and Liang, 2015; Wang et al., 2017), and previ-

ous attempts in designing IR to decouple meaning

representations of NL from database schema and

database management system (Woods, 1986; Al-

shawi, 1992; Androutsopoulos et al., 1993).

On the challenging Spider benchmark (Yu et al.,

2018c), IRNet achieves 46.7% exact matching ac-
curacy, obtaining 19.5% absolute improvement
over previous state-of-the-art approaches. At the

time of writing, IRNet achieves the first position

on the Spider leaderboard. When augmented with

BERT (Devlin et al., 2018), IRNet reaches up to

54.7% accuracy. In addition, as we show in the ex-

∷= � � |

�� ∷= �� �� | �� ��| > � | > � | < � | < �| ≥ � | ≥ � | = � | = �| ≠ � | ≠ � | �� � � � | � � | � �
∷=� ∷=

� ∷= m � min� �� � � | �

� ∷= � | �� ∷= � | �
∷= | �� | �er � � ��| � ��∷= � � � � � � � � � � � �⋯�

Figure 2: The context-free grammar of SemQL.

column ranges over distinct column names in a

schema. table ranges over tables in a schema.

� ��� ���
name

friend

��� ��
�

>

highschooler

��� ��
�

≥

friend

grade ∗
Figure 3: An illustrative example of SemQL. Its cor-

responding question and SQL query are shown in Fig-

ure 1.

periments, learning to synthesize SemQL queries

rather than SQL queries can substantially benefit

other neural approaches for Text-to-SQL, such as

SQLNet (Xu et al., 2017), TypeSQL (Yu et al.,

2018a) and SyntaxSQLNet (Yu et al., 2018b).

Such results on the one hand demonstrate the ef-

fectiveness of SemQL in bridging NL and SQL.

On the other hand, it reveals that designing an ef-

fective intermediate representation to bridge NL

and SQL is a promising direction to being there

for complex and cross-domain Text-to-SQL.

2 Approach

In this section, we present IRNet in detail. We first

describe how to tackle the mismatch problem and

the lexical problem with intermediate representa-

tion and schema linking. Then we present the neu-

ral model to synthesize SemQL queries.



4526

2.1 Intermediate Representation

To eliminate the mismatch, we design a domain

specific language, called SemQL, which serves as

an intermediate representation between NL and

SQL. Figure 2 presents the context-free grammar

of SemQL. An illustrative SemQL query is shown

in Figure 3. We elaborate on the design of SemQL

in the following.

Inspired by lambda DCS (Liang, 2013), SemQL

is designed to be tree-structured. This structure, on

the one hand, can effectively constrain the search

space during synthesis. On the other hand, in

view of the tree-structure nature of SQL (Yu et al.,

2018b; Yin and Neubig, 2018), following the same

structure also makes it easier to translate to SQL

intuitively.

The mismatch problem is mainly caused by the

implementation details in SQL queries and miss-

ing specification in questions as discussed in Sec-

tion 1. Therefore, it is natural to hide the imple-

mentation details in the intermediate representa-

tion, which forms the basic idea of SemQL. Con-

sidering the example in Figure 3, the GROUPBY,

HAVING and FROM clauses in the SQL query are

eliminated in the SemQL query, and the conditions

in WHERE and HAVING are uniformly expressed

in the subtree of Filter in the SemQL query. The

implementation details can be deterministically in-

ferred from the SemQL query in the later infer-

ence phase with domain knowledge. For example,

a column in the GROUPBY clause of a SQL query

usually occurs in the SELECT clause or it is the

primary key of a table where an aggregate func-

tion is applied to one of its columns.

In addition, we strictly require to declare the ta-

ble that a column belongs to in SemQL. As illus-

trated in Figure 3, the column ‘name’ along with

its table ‘friend’ are declared in the SemQL query.

The declaration of tables helps to differentiate du-

plicated column names in the schema. We also de-

clare a table for the special column ‘*’ because we

observe that ‘*’ usually aligns with a table men-

tioned in a question. Considering the example in

Figure 3, the column ‘*’ in essence aligns with the

table ‘friend’, which is explicitly mentioned in the

question. Declaring a table for ‘*’ also helps infer

the FROM clause in the next inference phase.

When it comes to inferring a SQL query from

a SemQL query, we perform the inference based

on an assumption that the definition of a database

schema is precise and complete. Specifically, if

a column is a foreign key of another table, there

should be a foreign key constraint declared in the

schema. This assumption usually holds as it is the

best practice in database design. More than 95%
of examples in the training set of the Spider bench-

mark hold this assumption. The assumption forms

the basis of the inference. Take the inference of the

FROM clause in a SQL query as an example. We

first identify the shortest path that connects all the

declared tables in a SemQL query in the schema

(A database schema can be formulated as an undi-

rected graph, where vertex are tables and edges are

foreign key relations among tables). Joining all

the tables in the path eventually builds the FROM

clause. Supplementary materials provide detailed

procedures of the inference and more examples of

SemQL queries.

2.2 Schema Linking

The goal of schema linking in IRNet is to recog-

nize the columns and the tables mentioned in a

question, and assign different types to the columns

based on how they are mentioned in the question.

Schema linking is an instantiation of entity link-

ing in the context of Text-to-SQL, where entity

is referred to columns, tables and cell values in

a database. We use a simple yet effective string-

match based method to implement the linking. In

the followings, we illustrate how IRNet performs

schema linking in detail based on the assumption

that the cell values in a database are not available.

As a whole, we define three types of entities that

may be mentioned in a question, namely, table,

column and value, where value stands for a cell

value in the database. In order to recognize enti-

ties, we first enumerate all the n-grams of length

1-6 in a question. Then, we enumerate them in the

descending order of length. If an n-gram exactly

matches a column name or is a subset of a col-

umn name, we recognize this n-gram as a column.

The recognition of table follows the same way. If

an n-gram can be recognized as both column and

table, we prioritize column. If an n-gram begins

and ends with a single quote, we recognize it as

value. Once an n-gram is recognized, we will re-

move other n-grams that overlap with it. To this

end, we can recognize all the entities mentioned

in a question and obtain a non-overlap n-gram se-

quence of the question by joining those recognized

n-grams and the remaining 1-grams. We refer each

n-gram in the sequence as a span and assign each



4527

Show the book titles and years for all books in descending order by year

Type: none none Column none Column none none Table none none none none Column

Table: book club movie

Column: year book tittle ⋯ title
Exact 

Match

Exact

Match
⋯ Partial

Match
Type:�� ��

��
book club

Memory

Schema

NL Encoder

Schema Encoder

Decoder
Memory

Schema

Memory

1 book title

2 year

year book club

……

⋯
book title year

ApplyRule ApplyRule ApplyRule SelectColumn SelectTable ApplyRule SelectColumn SelectTable

� ∷= ∷= � � ∷= � � � ≔ �
ApplyRule

� ≔ � ≔ �
ApplyRule SelectColumn

book club

SelectTable

Encoder Units

Decoder Units

Embedding

Question:

Figure 4: An overview of the neural model to synthesize SemQL queries. Basically, IRNet is constituted by an

NL encoder, a schema encoder and a decoder. As shown in the figure, the column ‘book title’ is selected from the

schema, while the second column ‘year’ is selected from the memory.

span a type according to its entity. For example,

if a span is recognized as column, we will assign

it a type COLUMN. Figure 4 depicts the schema

linking results of a question.

For those spans recognized as column, if they

exactly match the column names in the schema,

we assign these columns a type EXACT MATCH,

otherwise a type PARTIAL MATCH. To link the

cell value with its corresponding column in the

schema, we first query the value span in Concept-

Net (Speer and Havasi, 2012) which is an open,

large-scale knowledge graph and search the re-

sults returned by ConceptNet over the schema. We

only consider the query results in two categories

of ConceptNet, namely, ‘is a type of’ and ‘related

terms’, as we observe that the column that a cell

value belongs to usually occurs in these two cate-

gories. If there exists a result exactly or partially

matches a column name in the schema, we as-

sign the column a type VALUE EXACT MATCH

or VALUE PARTIAL MATCH.

2.3 Model

We present the neural model to synthesize SemQL

queries, which takes a question, a database schema

and the schema linking results as input. Figure 4

depicts the overall architecture of the model via an

illustrative example.

To address the lexical problem, we consider

the schema linking results when constructing rep-

resentations for the question and columns in the

schema. In addition, we design a memory aug-

mented pointer network for selecting columns dur-

ing synthesis. When selecting a column, it makes

a decision first on whether to select from memory

or not, which sets it apart from the vanilla pointer

network (Vinyals et al., 2015). The motivation be-

hind the memory augmented pointer network is

that the vanilla pointer network is prone to select-

ing same columns according to our observations.

NL Encoder. Let x=[(x1, τ1), · · · , (xL, τL)] de-
note the non-overlap span sequence of a question,

where xi is the i
th span and τi is the type of span xi

assigned in schema linking. The NL encoder takes

x as input and encodes x into a sequence of hidden

states Hx. Each word in xi is converted into its

embedding vector and its type τi is also converted

into an embedding vector. Then, the NL encoder

takes the average of the type and word embeddings

as the span embedding eix. Finally, the NL en-

coder runs a bi-directional LSTM (Hochreiter and

Schmidhuber, 1997) over all the span embeddings.

The output hidden states of the forward and back-

ward LSTM are concatenated to construct Hx.

Schema Encoder. Let s=(c, t) denote a database
schema, where c={(c1, φi), · · · , (cn, φn)} is the
set of distinct columns and their types that we as-

sign in schema linking, and t={t1, · · · , tm} is the
set of tables. The schema encoder takes s as input

and outputs representations for columns Ec and

tables Et. We take the column representations as

an example below. The construction of table rep-

resentations follows the same way except that we

do not assign a type to a table in schema linking.

Concretely, each word in ci is first converted

into its embedding vector and its type φi is also

converted into an embedding vector ϕi. Then, the

schema encoder takes the average of word embed-



4528

dings as the initial representations êic for the col-

umn. The schema encoder further performs an

attention over the span embeddings and obtains

a context vector cic. Finally, the schema encoder

takes the sum of the initial embedding, context

vector and the type embedding as the column rep-

resentation eic. The calculation of the representa-

tions for column ci is as follows.

gik =
(êic)

Tekx
‖êic‖‖e

k
x‖

cic =

L∑

k=1

gike
k
x

eic = ê
i
c + c

i
c +ϕi,

Decoder. The goal of the decoder is to synthe-

size SemQL queries. Given the tree structure of

SemQL, we use a grammar-based decoder (Yin

and Neubig, 2017, 2018) which leverages a LSTM

to model the generation process of a SemQL query

via sequential applications of actions. Formally,

the generation process of a SemQL query y can be

formalized as follows.

p(y|x, s) =

T∏

i=1

p(ai|x, s, a<i),

where ai is an action taken at time step i, a<i is the

sequence of actions before i, and T is the number

of total time steps of the whole action sequence.

The decoder interacts with three types of ac-

tions to generate a SemQL query, including AP-

PLYRULE, SELECTCOLUMN and SELECTTABLE.

APPLYRULE(r) applies a production rule r to the

current derivation tree of a SemQL query. SE-

LECTCOLUMN(c) and SELECTTABLE(t) selects a

column c and a table t from the schema, respec-

tively. Here, we detail the action SELECTCOL-

UMN and SELECTTABLE. Interested readers can

refer to Yin and Neubig (2017) for details of the

action APPLYRULE.

We design a memory augmented pointer net-

work to implement the action SELECTCOLUMN.

The memory is used to record the selected

columns, which is similar to the memory mech-

anism used in Liang et al. (2017). When the de-

coder is going to select a column, it first makes a

decision on whether to select from the memory or

not, and then selects a column from the memory or

the schema based on the decision. Once a column

is selected, it will be removed from the schema

and be recorded in the memory. The probability

of selecting a column c is calculated as follows.

p(ai = SELECTCOLUMN[c]|x, s, a<i) =

p(MEM|x, s, a<i)p(c|x, s, a<i, MEM)

+ p(S|x, s, a<i)p(c|x, s, a<i, S)

p(MEM|x, s, a<i) = sigmod(w
T

mvi)

p(S|x, s, a<i) = 1− p(MEM|x, s, a<i)

p(c|x, s, a<i, MEM) ∝ exp(v
T

i E
m
c )

p(c|x, s, a<i, S) ∝ exp(v
T

i E
s
c ),

where S represents selecting from schema, MEM

represents selecting from memory, vi denotes the

context vector that is obtained by performing an

attention over Hx, E
m
c denotes the embedding of

columns in memory and Esc denotes the embed-

ding of columns that are never selected. wm is

trainable parameter.

When it comes to SELECTTABLE, the decoder

selects a table t from the schema via a pointer net-

work:

p(ai = SELECTTABLE[t]|x, s, a<i) ∝ exp(v
T

i Et).

As shown in Figure 4, the decoder first predicts a

column and then predicts the table that it belongs

to. To this end, we can leverage the relations be-

tween columns and tables to prune the irrelevant

tables.

Coarse-to-fine. We further adopt a coarse-to-fine

framework (Solar-Lezama, 2008; Bornholt et al.,

2016; Dong and Lapata, 2018), decomposing the

decoding process of a SemQL query into two

stages. In the first stage, a skeleton decoder out-

puts a skeleton of the SemQL query. Then, a de-

tail decoder fills in the missing details in the skele-

ton by selecting columns and tables. Supplemen-

tary materials provide a detailed description of the

skeleton of a SemQL query and the coarse-to-fine

framework.

3 Experiment

In this section, we evaluate the effectiveness of

IRNet by comparing it to the state-of-the-art ap-

proaches and ablating several design choices in

IRNet to understand their contributions.

3.1 Experiment Setup

Dataset. We conduct our experiments on the

Spider (Yu et al., 2018c), a large-scale, human-

annotated and cross-domain Text-to-SQL bench-

mark. Following Yu et al. (2018b), we use



4529

the database split for evaluations, where 206
databases are split into 146 training, 20 develop-
ment and 40 testing. There are 8625, 1034, 2147
question-SQL query pairs for training, develop-

ment and testing. Just like any competition bench-

mark, the test set of Spider is not publicly avail-

able, and our models are submitted to the data

owner for testing. We evaluate IRNet and other

approaches using SQL Exact Matching and Com-

ponent Matching proposed by Yu et al. (2018c).

Baselines. We also evaluate the sequence-to-

sequence model (Sutskever et al., 2014) aug-

mented with a neural attention mechanism (Bah-

danau et al., 2014) and a copying mechanism (Gu

et al., 2016), SQLNet (Xu et al., 2017), TypeSQL

(Yu et al., 2018a), and SyntaxSQLNet (Yu et al.,

2018b) which is the state-of-the-art approach on

the Spider benchmark.

Implementations. We implement IRNet and the

baseline approaches with PyTorch (Paszke et al.,

2017). Dimensions of word embeddings, type em-

beddings and hidden vectors are set to 300. Word

embeddings are initialized with Glove (Penning-

ton et al., 2014) and shared between the NL en-

coder and schema encoder. They are fixed during

training. The dimension of action embedding and

node type embedding are set to 128 and 64, re-

spectively. The dropout rate is 0.3. We use Adam

(Kingma and Ba, 2014) with default hyperparam-

eters for optimization. Batch size is set to 64.

BERT. Language model pre-training has shown to

be effective for learning universal language repre-

sentations. To further study the effectiveness of

our approach, inspired by SQLova (Hwang et al.,

2019), we leverage BERT (Devlin et al., 2018)

to encode questions, database schemas and the

schema linking results. The decoder remains the

same as in IRNet. Specifically, the sequence of

spans in the question are concatenated with all the

distinct column names in the schema. Each col-

umn name is separated with a special token [SEP].

BERT takes the concatenation as input. The rep-

resentation of a span in the question is taken as the

average hidden states of its words and type. To

construct the representation of a column, we first

run a bi-directional LSTM (BI-LSTM) over the

hidden states of its words. Then, we take the sum

of its type embedding and the final hidden state of

the BI-LSTM as the column representation. The

construction of table representations follows the

same way. Supplementary material provides a fig-

Approach Dev Test

Seq2Seq 1.9% 3.7%
Seq2Seq + Attention 1.8% 4.8%
Seq2Seq + Copying 4.1% 5.3%
TypeSQL 8.0% 8.2%
SQLNet 10.9% 12.4%
SyntaxSQLNet 18.9% 19.7%
SyntaxSQLNet(augment) 24.8% 27.2%
IRNet 53.2% 46.7%

BERT
SyntaxSQLNet(BERT) 25.0% 25.4%
IRNet(BERT) 61.9% 54.7%

Table 1: Exact matching accuracy on SQL queries.

SELECT KEYWORDS ORDER BY GROUP BY WHERE
Component

0

10

20

30

40

50

60

70

80

90

F1
 S

co
re

s o
f C

om
po

ne
nt

 M
at

ch
in

g 
(%

)

SyntaxSQLNet
SyntaxSQLNet(BERT)
IRNet
IRNet(BERT)

Figure 5: F1 scores of component matching of Syn-

taxSQLNet, SyntaxSQLNet(BERT), IRNet and IR-

Net(BERT) on the test set.

ure to illustrate the architecture of the encoder. To

establish baseline, we also augment SyntaxSQL-

Net with BERT. Note that we only use the base

version of BERT due to the resource limitations.

We do not perform any data augmentation for

fair comparison. All our code are publicly avail-

able. 1

3.2 Experimental Results

Table 1 presents the exact matching accuracy of

IRNet and various baselines on the development

set and the test set. IRNet clearly outperforms all

the baselines by a substantial margin. It obtains

27.0% absolute improvement over SyntaxSQLNet
on the test set. It also obtains 19.5% absolute
improvement over SyntaxSQLNet(augment) that

performs large-scale data augmentation. When in-

corporating BERT, the performance of both Syn-

taxSQLNet and IRNet is substantially improved

and the accuracy gap between them on both the

development set and the test set is widened.

To study the performance of IRNet in detail, fol-

lowing Yu et al. (2018b), we measure the average

F1 score on different SQL components on the test

1https://github.com/zhanzecheng/IRNet

https://github.com/zhanzecheng/IRNet


4530

Approach Easy Medium Hard
Extra
Hard

SyntaxSQLNet 38.6% 17.6% 16.3% 4.9%
SyntaxSQLNet

42.9% 24.9% 21.9% 8.6%
(BERT)

IRNet 70.1% 49.2% 39.5% 19.1%
IRNet(BERT) 77.2% 58.7% 48.1% 25.3%

Table 2: Exact matching accuracy of SyntaxSQL-

Net, SyntaxSQLNet(BERT), IRNet and IRNet(BERT)

on the test set by hardness level.

Approach SQL SemQL

Seq2Seq 1.9% 11.4%(+9.5)
Seq2Seq + Attention 1.8% 14.7%(+12.9)
Seq2Seq + Copying 4.1% 18.5%(+14.1)
TypeSQL 8.0% 14.4%(+6.4)
SQLNet 10.9% 17.5%(+6.6)
SyntaxSQLNet 18.9% 27.5%(+8.6)

BERT
SyntaxSQLNet(BERT) 25.0% 35.8%(+10.8)

Table 3: Exact matching accuracy on the develop-

ment set. The header ‘SQL’ means that the approaches

are learned to generate SQL queries, while the header

‘SemQL’ indicates that they are learned to generate

SemQL queries.

set. We compare between SyntaxSQLNet and IR-

Net. As shown in Figure 5, IRNet outperforms

SyntaxSQLNet on all components. There are at

least 18.2% absolute improvement on each com-
ponent except KEYWORDS. When incorporating

BERT, the performance of IRNet on each com-

ponent is further boosted, especially in WHERE

clause.

We further study the performance of IRNet

on different portions of the test set according to

the hardness levels of SQL defined in Yu et al.

(2018c). As shown in Table 2, IRNet significantly

outperforms SyntaxSQLNet in all four hardness

levels with or without BERT. For example, com-

pared with SyntaxSQLNet, IRNet obtains 23.3%
absolute improvement in Hard level.

To investigate the effectiveness of SemQL, we

alter the baseline approaches and let them learn to

generate SemQL queries rather than SQL queries.

As shown in Table 3, there are at least 6.6% and
up to 14.4% absolute improvements on accuracy
of exact matching on the development set. For ex-

ample, when SyntaxSQLNet is learned to generate

SemQL queries instead of SQL queries, it registers

8.6% absolute improvement and even outperforms
SyntaxSQLNet(augment) which performs large-

scale data augmentation. The relatively limited

improvement on TypeSQL and SQLNet is because

Technique IRNet IRNet(BERT)

Base model 40.5% 53.9%
+SL 48.5% 60.3%
+SL + MEM 51.3% 60.6%
+SL + MEM + CF 53.2% 61.9%

Table 4: Ablation study results. Base model means

that we does not use schema linking (SL), memory aug-

mented pointer network (MEM) and the coarse-to-fine

framework (CF) on IRNet.

their slot-filling based models only support a sub-

set of SemQL queries. The notable improvement,

on the one hand, demonstrates the effectiveness of

SemQL. On the other hand, it shows that design-

ing an intermediate representations to bridge NL

and SQL is promising in Text-to-SQL.

3.3 Ablation Study

We conduct ablation studies on IRNet and IR-

Net(BERT) to analyze the contribution of each de-

sign choice. Specifically, we first evaluate a base

model that does not apply schema linking (SL) and

the coarse-to-fine framework (CF), and replace the

memory augment pointer network (MEM) with

the vanilla pointer network (Vinyals et al., 2015).

Then, we gradually apply each component on the

base model. The ablation study is conducted on

the development set.

Table 4 presents the ablation study results. It

is clear that our base model significantly outper-

forms SyntaxSQLNet, SyntaxSQLNet(augment)

and SyntaxSQLNet(BERT). Performing schema

linking (‘+SL’) brings about 8.5% and 6.4% ab-
solute improvement on IRNet and IRNet(BERT).

Predicting columns in the WHERE clause is known

to be challenging (Yavuz et al., 2018). The F1

score on the WHERE clause increases by 12.5%
when IRNet performs schema linking. The signif-

icant improvement demonstrates the effectiveness

of schema linking in addressing the lexical prob-

lem. Using the memory augmented pointer net-

work (‘+MEM’) further improves the performance

of IRNet and IRNet(BERT). We observe that the

vanilla pointer network is prone to selecting same

columns during synthesis. The number of ex-

amples suffering from this problem decreases by

70%, when using the memory augmented pointer
network. At last, adopting the coarse-to-fine

framework (‘+CF’) can further boost performance.



4531

3.4 Error Analysis

To understand the source of errors, we analyze 483

failed examples of IRNet on the development set.

We identify three major causes for failures:

Column Prediction. We find that 32.3% of failed
examples are caused by incorrect column predic-

tions based on cell values. That is, the correct

column name is not mentioned in a question, but

the cell value that belongs to it is mentioned. As

the study points out (Yavuz et al., 2018), the cell

values of a database are crucial in order to solve

this problem. 15.2% of the failed examples fail
to predict correct columns that partially appear in

questions or appear in their synonym forms. Such

failures may can be further resolved by combining

our string-match based method with embedding-

match based methods (Krishnamurthy et al., 2017)

to improve the schema linking.

Nested Query. 23.9% of failed examples are
caused by the complicated nested queries. Most

of these examples are in the Extra Hard level. In

the current training set, the number of SQL queries

in Extra Hard level (∼20%) is the least, even less
than the SQL queries in Easy level (∼23%). In
view of the extremely large search space of the

complicated SQL queries, data augmentation tech-

niques may be indispensable.

Operator. 12.4% of failed examples make mis-
take in the operator as it requires common knowl-

edge to predict the correct one. Considering the

following question, ‘Find the name and member-

ship level of the visitors whose membership level

is higher than 4, and sort by their age from old to

young’, the phrase ‘from old to young’ indicates

that sorting should be conducted in descending or-

der. The operator defined here includes aggregate

functions, operators in the WHERE clause and the

sorting orders (ASC and DESC).

Other failed examples cannot be easily catego-

rized into one of the categories above. A few of

them are caused by the incorrect FROM clause, be-

cause the ground truth SQL queries join those ta-

bles without foreign key relations defined in the

schema. This violates our assumption that the def-

inition of a database schema should be precise and

complete.

When incorporated with BERT, 30.5% of failed
examples are fixed. Most of them are in the Col-

umn Prediction and Operator category, but the im-

provement on Nested Query is quite limited.

4 Discussion

Performance Gap. There exists a performance

gap on IRNet between the development set and the

test set, as shown in Table 1. Considering the ex-

plosive combination of nested queries in SQL and

the limited number of data (1034 in development,

2147 in test), the gap is probably caused by the dif-

ferent distributions of the SQL queries in Hard and

Extra level. To verify the hypothesis, we construct

a pseudo test set from the official training set. We

train IRNet on the remaining data in the training

set and evaluate them on the development set and

the pseudo test set, respectively. We find that even

though the pseudo set has the same number of

complicated SQL queries (Hard and Extra Hard)

with the development set, there still exists a per-

formance gap. Other approaches do not exhibit the

performance gap because of their relatively poor

performance on the complicated SQL queries. For

example, SyntaxSQLNet only achieves 4.6% on
the SQL queries in Extra Hard level on test set.

Supplementary material provides detailed experi-

mental settings and results on the pseudo test set.

Limitations of SemQL. There are a few limita-

tions of our intermediate representation. Firstly,

it cannot support the self join in the FROM clause

of SQL. In order to support the self join, the

variable mechanism in lambda calculus (Carpen-

ter, 1997) or the scope mechanism in Discourse

Representation Structure (Kamp and Reyle, 1993)

may be necessary. Secondly, SemQL has not com-

pletely eliminated the mismatch between NL and

SQL yet. For example, the INTERSECT clause

in SQL is often used to express disjoint condi-

tions. However, when specifying requirements,

end users rarely concern about whether two con-

ditions are disjointed or not. Despite the limita-

tions of SemQL, experimental results demonstrate

its effectiveness in Text-to-SQL. To this end, we

argue that designing an effective intermediate rep-

resentation to bridge NL and SQL is a promising

direction to being there for complex and cross-

domain Text-to-SQL. We leave a better interme-

diate representation as one of our future works.

5 Related Work

Natural Language Interface to Database. The

task of Natural Language Interface to Database

(NLIDB) has received significant attention since

the 1970s (Warren and Pereira, 1981; Androut-

sopoulos et al., 1995; Popescu et al., 2004; Hallett,



4532

2006; Giordani and Moschitti, 2012). Most of the

early proposed systems are hand-crafted to a spe-

cific database (Warren and Pereira, 1982; Woods,

1986; Hendrix et al., 1978), making it challeng-

ing to accommodate cross-domain settings. Later

work focus on building a system that can be reused

for multiple databases with minimal human ef-

forts (Grosz et al., 1987; Androutsopoulos et al.,

1993; Tang and Mooney, 2000). Recently, with

the development of advanced neural approaches

on Semantic Parsing and the release of large-scale,

cross-domain Text-to-SQL benchmarks such as

WikiSQL (Zhong et al., 2017) and Spider (Yu

et al., 2018c), there is a renewed interest in the

task (Xu et al., 2017; Iyer et al., 2017; Sun et al.,

2018; Gur et al., 2018; Yu et al., 2018a,b; Wang

et al., 2018; Finegan-Dollak et al., 2018; Hwang

et al., 2019). Unlike these neural approaches that

end-to-end synthesize a SQL query, IRNet first

synthesizes a SemQL query and then infers a SQL

query from it.

Intermediate Representations in NLIDB. Early

proposed systems like as LUNAR (Woods, 1986)

and MASQUE (Androutsopoulos et al., 1993) also

propose intermediate representations (IR) to rep-

resent the meaning of questions and then translate

it into SQL queries. The predicates in these IRs

are designed for a specific database, which sets

SemQL apart. SemQL targets a wide adoption and

no human effort is needed when it is used in a new

domain. Li and Jagadish (2014) propose a query

tree in their NLIDB system to represent the mean-

ing of a question and it mainly serves as an inter-

action medium between users and their system.

Entity Linking. The insight behind performing

schema linking is partly inspired by the success

of incorporating entity linking in knowledge base

question answering and semantic parsing (Yih

et al., 2016; Krishnamurthy et al., 2017; Yu et al.,

2018a; Herzig and Berant, 2018; Kolitsas et al.,

2018). In the context of semantic parsing, Krish-

namurthy et al. (2017) propose a neural entity link-

ing module for answering compositional questions

on semi-structured tables. TypeSQL (Yu et al.,

2018a) proposes to utilize type information to bet-

ter understand rare entities and numbers in ques-

tions. Similar to TypeSQL, IRNet also recognizes

the columns and tables mentioned in a question.

What sets IRNet apart is that IRNet assigns dif-

ferent types to the columns based on how they are

mentioned in the question.

6 Conclusion

We present a neural approach SemQL for com-

plex and cross-domain Text-to-SQL, aiming to ad-

dress the lexical problem and the mismatch prob-

lem with schema linking and intermediate repre-

sentation. Experimental results on the challeng-

ing Spider benchmark demonstrate the effective-

ness of IRNet.

Acknowledgments

We would like to thank Bo Pang and Tao Yu for

evaluating our submitted models on the test set of

the Spider benchmark. Ting Liu is the correspond-

ing author.

References

Hiyan Alshawi. 1992. The core language engine. MIT
press.

I. Androutsopoulos, G. Ritchie, and P. Thanisch. 1993.
Masque/sql: An efficient and portable natural lan-
guage query interface for relational databases. In
Proceedings of the 6th International Conference on
Industrial and Engineering Applications of Artifi-
cial Intelligence and Expert Systems, pages 327–
330. Gordon & Breach Science Publishers.

Ion Androutsopoulos, Graeme D Ritchie, and Peter
Thanisch. 1995. Natural language interfaces to
databases–an introduction. Natural language engi-
neering, 1:29–81.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473. Version 7.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544. Association
for Computational Linguistics.

James Bornholt, Emina Torlak, Dan Grossman, and
Luis Ceze. 2016. Optimizing synthesis with metas-
ketches. In Proceedings of the 43rd Annual
ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages, pages 775–788. ACM.

Bob Carpenter. 1997. Type-logical semantics. MIT
press.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805. Version 1.



4533

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics, pages 731–742. Associ-
ation for Computational Linguistics.

Catherine Finegan-Dollak, Jonathan K. Kummerfeld,
Li Zhang, Karthik Ramanathan, Sesh Sadasivam,
Rui Zhang, and Dragomir Radev. 2018. Improving
text-to-sql evaluation methodology. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics, pages 351–360. Associ-
ation for Computational Linguistics.

Alessandra Giordani and Alessandro Moschitti. 2012.
Generating sql queries using natural language syn-
tactic dependencies and metadata. In Proceedings
of the 17th International Conference on Applica-
tions of Natural Language Processing and Informa-
tion Systems, pages 164–170. Springer-Verlag.

Barbara J. Grosz, Douglas E. Appelt, Paul A. Martin,
and Fernando C. N. Pereira. 1987. Team: An experi-
ment in the design of transportable natural-language
interfaces. Artificial Intelligence, 32:173–243.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics, pages 1631–1640. Associa-
tion for Computational Linguistics.

Izzeddin Gur, Semih Yavuz, Yu Su, and Xifeng Yan.
2018. Dialsql: Dialogue based structured query
generation. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1339–1349. Association for Computa-
tional Linguistics.

Catalina Hallett. 2006. Generic querying of relational
databases using natural language generation tech-
niques. In Proceedings of the Fourth International
Natural Language Generation Conference, pages
95–102. Association for Computational Linguistics.

Gary G. Hendrix, Earl D. Sacerdoti, Daniel Sagalow-
icz, and Jonathan Slocum. 1978. Developing a natu-
ral language interface to complex data. ACM Trans-
actions on Database Systems, 3:105–147.

Jonathan Herzig and Jonathan Berant. 2018. Decou-
pling structure and lexicon for zero-shot semantic
parsing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1619–1629. Association for Computa-
tional Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9:1735–
1780.

Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and
Minjoon Seo. 2019. A comprehensive exploration
on wikisql with table-aware word contextualization.
arXiv preprint arXiv:1902.01069. Version 1.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
Jayant Krishnamurthy, and Luke Zettlemoyer. 2017.
Learning a neural semantic parser from user feed-
back. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics,
pages 963–973. Association for Computational Lin-
guistics.

Hans Kamp and U. Reyle. 1993. From Discourse
to Logic Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory.

Rohit Kate. 2008. Transforming meaning represen-
tation grammars to improve semantic parsing. In
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages 33–40.
Coling 2008 Organizing Committee.

Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artificial Intelligence, pages 1062–
1068. AAAI Press.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980. Version 9.

Nikolaos Kolitsas, Octavian-Eugen Ganea, and
Thomas Hofmann. 2018. End-to-end neural entity
linking. In Proceedings of the 22nd Conference
on Computational Natural Language Learning,
pages 519–529. Association for Computational
Linguistics.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1516–1526. Asso-
ciation for Computational Linguistics.

Fei Li and H. V. Jagadish. 2014. Constructing an
interactive natural language interface for relational
databases. Proceedings of the VLDB Endowment,
8:73–84.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2017. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, pages 23–33. Association for Computa-
tional Linguistics.

Percy Liang. 2013. Lambda dependency-based
compositional semantics. arXiv preprint
arXiv:1309.4408. Version 2.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590–599. Asso-
ciation for Computational Linguistics.



4534

Panupong Pasupat and Percy Liang. 2015. Compo-
sitional semantic parsing on semi-structured tables.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, pages 1470–1480. Association
for Computational Linguistics.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NIPS-W.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543. Association for Com-
putational Linguistics.

Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern nat-
ural language interfaces to databases: Composing
statistical parsing with semantic tractability. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics. Association for Compu-
tational Linguistics.

Armando Solar-Lezama. 2008. Program Synthesis
by Sketching. Ph.D. thesis, Berkeley, CA, USA.
AAI3353225.

Robert Speer and Catherine Havasi. 2012. Represent-
ing general relational knowledge in conceptnet 5.
In Proceedings of the Eighth International Confer-
ence on Language Resources and Evaluation, pages
3679–3686. European Language Resources Associ-
ation.

Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong
Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and Ming
Zhou. 2018. Semantic parsing with syntax- and
table-aware sql generation. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics, pages 361–372. Association for
Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems, pages 3104–3112. MIT Press.

Lappoon R. Tang and Raymond J. Mooney. 2000. Au-
tomated construction of database interfaces: Inte-
grating statistical and relational learning for seman-
tic parsing. In Proceedings of the 2000 Joint SIG-
DAT Conference on Empirical Methods in Natu-
ral Language Processing and Very Large Corpora:
Held in Conjunction with the 38th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 133–141. Association for Computational
Linguistics.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems 28, pages 2692–2700.
Curran Associates, Inc.

Chenglong Wang, Alvin Cheung, and Rastislav Bodik.
2017. Synthesizing highly expressive sql queries
from input-output examples. In Proceedings of the
38th ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 452–
466. ACM.

Chenglong Wang, Kedar Tatwawadi, Marc
Brockschmidt, Po-Sen Huang, Yi Mao, Olek-
sandr Polozov, and Rishabh Singh. 2018. Robust
text-to-sql generation with execution-guided de-
coding. arXiv preprint arXiv:1807.03100. Version
3.

David H. D. Warren and Fernando Pereira. 1981. Eas-
ily adaptable system for interpreting natural lan-
guage queries. American Journal of Computational
Linguistics, 8:110–122.

David H. D. Warren and Fernando C. N. Pereira. 1982.
An efficient easily adaptable system for interpreting
natural language queries. Computational Linguis-
tics, 8:110–122.

W A Woods. 1986. Readings in natural language pro-
cessing. chapter Semantics and Quantification in
Natural Language Question Answering, pages 205–
248. Morgan Kaufmann Publishers Inc.

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet:
Generating structured queries from natural language
without reinforcement learning. arXiv preprint
arXiv:1711.04436. Version 1.

Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and
Thomas Dillig. 2017. Sqlizer: Query synthesis from
natural language. Proceedings of the ACM on Pro-
gramming Languages, 1:63:1–63:26.

Semih Yavuz, Izzeddin Gur, Yu Su, and Xifeng Yan.
2018. What it takes to achieve 100 percent con-
dition accuracy on wikisql. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 1702–1711. Associa-
tion for Computational Linguistics.

Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-
Wei Chang, and Jina Suh. 2016. The value of se-
mantic parse labeling for knowledge base question
answering. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 201–206. Association for Computational
Linguistics.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, pages 440–
450. Association for Computational Linguistics.



4535

Pengcheng Yin and Graham Neubig. 2018. Tranx: A
transition-based neural abstract syntax parser for se-
mantic parsing and code generation. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstra-
tions, pages 7–12. Association for Computational
Linguistics.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018a. Typesql: Knowledge-
based type-aware neural text-to-sql generation. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 588–594. Association for Computational Lin-
guistics.

Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang,
Dongxu Wang, Zifan Li, and Dragomir Radev.
2018b. Syntaxsqlnet: Syntax tree networks for com-
plex and cross-domain text-to-sql task. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 1653–1663.
Association for Computational Linguistics.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018c. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-sql task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3911–3921. Association for Computational Linguis-
tics.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence, pages
1050–1055. AAAI Press.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language using reinforcement learning.
CoRR, abs/1709.00103.


