












































Untitled


Proceedings of NAACL-HLT 2019, pages 3432–3442
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3432

Learning Hierarchical Discourse-level Structure for Fake News Detection

Hamid Karimi

Computer Science and Engineering

Michigan State University

karimiha@msu.edu

Jiliang Tang

Computer Science and Engineering

Michigan State University

tangjili@msu.edu

Abstract

On the one hand, nowadays, fake news arti-

cles are easily propagated through various on-

line media platforms and have become a grand

threat to the trustworthiness of information.

On the other hand, our understanding of the

language of fake news is still minimal. In-

corporating hierarchical discourse-level struc-

ture of fake and real news articles is one cru-

cial step toward a better understanding of how

these articles are structured. Nevertheless, this

has rarely been investigated in the fake news

detection domain and faces tremendous chal-

lenges. First, existing methods for captur-

ing discourse-level structure rely on annotated

corpora which are not available for fake news

datasets. Second, how to extract out useful

information from such discovered structures

is another challenge. To address these chal-

lenges, we propose Hierarchical Discourse-

level Structure for Fake news detection. HDSF

learns and constructs a discourse-level struc-

ture for fake/real news articles in an auto-

mated and data-driven manner. Moreover,

we identify insightful structure-related proper-

ties, which can explain the discovered struc-

tures and boost our understating of fake news.

Conducted experiments show the effective-

ness of the proposed approach. Further struc-

tural analysis suggests that real and fake news

present substantial differences in the hierarchi-

cal discourse-level structures.

1 Introduction

In this work, we focus on detecting fake news ar-

ticles (hereafter referred to as documents) based

on their contents. Many existing linguistic ap-

proaches for fake news detection (Feng et al.,

2012; Pennebaker et al., 2015; Ott et al., 2011)

overlook a crucial linguistic aspect of fake/real

news documents i.e., the hierarchical discourse-

level structure. Usually, in a document, discourse

units (e.g., sentences) are organized in a hierarchi-

cal structure e.g., a tree. The importance of con-

sidering the hierarchical discourse-level structure

for fake news detection is three-fold. First, pre-

vious studies (Bachenko et al., 2008; Rubin and

Lukoianova, 2015) explored discourse-level struc-

ture in fake news detection and discovered that

the way two discourse units of a document are

connected could be quite revealing and insightful

about its truthfulness. For instance, (Rubin and

Lukoianova, 2015) applied Rhetorical Structure

Theory (RST) (Mann and Thompson, 1988) and

noted that fake stories lack “evidence” as a defined

inter-discourse relation. Second, fake news is typ-

ically produced by connecting disjoint pieces of

news and unlike well-established journalism (e.g.,

New York Times) fake news production lacks a

meticulous editorial board. Therefore, by incor-

porating the hierarchical discourse-level structure,

we can investigate the coherence of fake/real news

documents (we will show this later). Third, a sub-

stantial number of studies have shown that using

hierarchical structures yields a better document

representation in various downstream tasks whose

predictions depend on the entire text (Bhatia et al.,

2015; Morey et al., 2018; Li et al., 2014b). Since

typically fake news detection is considered as a

classification problem based on the entire text, ap-

plying discourse analysis has the potential to ad-

vance fake news detection (this will be verified

later).

On the other hand, incorporating the hierarchi-

cal structure at the discourse level for fake news

detection faces tremendous challenges. First,

many existing methods incorporating structural

discourse (Li et al., 2014a; Bhatia et al., 2015) (not

for fake news detection though) rely on annotated

corpora such as Penn Discourse Treebank (Prasad

et al., 2007). Constructing and annotating such

corpora is an arduous and costly process. Incor-



3433



3434



3435

where G is a non-linear activation function, W is
some weight matrix, b is a bias vector, and ⊙ de-
notes the dot product operator. Further, since we

need a root node in a dependency tree, we com-

pute the probability of a sentence sj being the root

node, denoted as rj , as follows.

uj = G(W × fj + b)

rj =
e
∑

∀y uj [y]

∑k
i=1 e

∑
∀y ui[y]

(4)

where uj [y] is the y-th element of vector uj . Sim-
ilarly, we calculate the root probabilities for all

sentences and obtain the array of root probabili-

ties denoted as r = {r1, r2, · · · , rk} where 0 ≤
rj ∈ r ≤ 1.

3.1.2 Discourse Dependency Tree

Construction

We use the learned matrix of inter-sentential

parent-child probabilities i.e., A (Eq. 3) as well

as the array of root probabilities i.e., r (Eq. 4)

and propose a greedy algorithm, illustrated in Al-

gorithm 1, to construct the discourse dependency

tree of a document. A sentence with the maxi-

mum value in r is considered the root node and is

inserted into the tree (line 5). Then, at each itera-

tion, the algorithm finds the maximum entry in a

block of the matrix A whose rows correspond to

the rows of current nodes added to the tree (i.e.,

nodes V in line 7) and its columns correspond to

columns of the rest of nodes (i.e., nodes N\V in
line 7). Note that the columns of the current nodes

are excluded because their parents have already

been identified and also each node should have

exactly one parent (except the root which has no

parent). Assume the search in line 7 results in the

entry (p, c) of A where 1 ≤ p, c ≤ k and p 6= c.
Then, the sentence sc is added as the child node

of the sentence sp (line 8). Algorithm 1 continues

until all sentences of a document are added to the

tree T .
To fix the idea of discourse dependency tree

construction algorithm, we present a step-by-step

execution of this algorithm demonstrated in Fig-

ure 3. In Step 0, the sentence s1 is added as the

root of the tree T since it has the maximum value
in the array of the root probabilities. Next in Step

1, the algorithm searches for the maximum prob-

ability value in the row s1 while the column s1 is

excluded. The maximum value is this block is 0.4
and corresponds to entry (s1,s2). Therefore, the

Algorithm 1: The proposed algorithm for dis-

course dependency tree construction

Input: A; r

Output: Discourse dependency tree T
1 T = empty
2 N = {s1, s2, · · · , sk} // All nodes
3 V = {} // Set of current nodes
4 Add N [argmax(r)] to V
5 T .root = N [argmax(r)] // Adding the

root

6 while |V | 6= k do
7 p, c = argmax(A[V,N\V ]) // Search

block

8 T .link(N [c], N [p]) // Child-parent
link

9 V.add(N [c]) // Adding the child
node

10 end

11 return T

sentence s2 is added as the child node of the sen-

tence s1. In a similar fashion, the algorithm con-

tinues until all 6 sentences are added to the tree.

3.2 Structural Document-level

Representation

We use a similar method presented in (Liu and

Lapata, 2018) to extract a structurally rich rep-

resentation for the entire document. First, for

each sentence (i.e., a discourse unit), we obtain a

structurally-aware representation. To achieve this,

we take into account the parent-child probabilities

as well as the root probabilities as follows.

pj = rj × eroot +
k∑

z=1

A[z, j]× fz

cj =
k∑

z=1

A[j, z]× fj

gj = G(W[pj ||cj ||fj ] + b)

(5)

where pj and cj are two context vectors taking

into account possible parents and children of a

sentence sj , respectively, eroot denotes a special

root embedding vector, || denotes vector concate-
nation operator, and gj is a structurally-aware rep-

resentation for sentence sj . Finally, to extract a

structurally rich representation for the entire doc-



3436



3437

ument. Recall that k is the total number of sen-

tences in a document.

The intuition behind defining Property 1 is as

follows. According to the description of the de-

pendency tree in Section 3.1, leaf nodes are iso-

lated discourse units and no other discourse units

depend on them. Thus, the more the number of

leaf nodes is, the less inter-linked the discourse

units will be, and vice versa. Therefore, Property 1

is likely to indicate the coherence of a document –

the higher Pl, the more isolated sentences and the

less coherent the document. Also, for a document

with k sentences Pl ∈ [
1

log(k) ,
k−1
log(k) ].

Property 2 (Preorder Difference) This property,

denoted as Pt, defines the normalized positional

difference between the preorder traversal of a doc-

ument’s discourse dependency tree and its original

sentential sequential order:

Pt =

∑k
j=1 |s

position
j − j|

log(k)
(9)

where s
position
j denotes the position of a sentence

sj in the preorder traversal
1 of dependency tree of

a document e.g., s
position
3 = 4 in Figure 1. The po-

sition of sj in the original sequential order is sim-

ply j2. The preorder traversal of the tree in Fig-

ure 1 is the sequence {s1, s2, s4, s3, s5} and the
sentential sequential order is {s1, s2, s3, s4, s5}.
Therefore, according to the definition of Prop-

erty 2:

Pt =
|1−1|+|2−2|+|4−3|+|3−4|+|5−5|

log(5) ≈ 2.86.
The preorder traversal of a document’s dis-

course dependency tree takes into consideration

the organization of a document respect to the de-

pendencies between sentences. Then, the purpose

of Property 2 is to measure how much the orga-

nization of a document, captured through the pre-

order traversal, deviates from its sentential sequen-

tial order. Sentence order is highly related to the

coherence of a document where the displaced or-

der of sentences in a document makes it less co-

herent (Li and Hovy, 2014). Thus, intuitively, the

less the value of Property 2 for a document is, the

more coherent that document should be. Also, for

a document with k sentences Pt ∈ [
k−1
log(k) ,

(k2−1)
2log(k) ]

if k is odd and Pt ∈ [
k−1
log(k) ,

k2

2log(k) ] if k is even.

1The subtrees are ordered based on when they are added
as the child nodes of a parent node in Algorithm 1. That is
why we can compute preorder traversal.

2We assume that the sentences in the sequential order are
numbered incrementally from 1 to k.

Property 3 (Parent-Child Distance) This prop-

erty, denoted as Pc, defines the normalized sum of

positional distances between child nodes and their

parents when they are considered in the original

sequential order:

Pc =

∑
∀c,p∈T |cposition − pposition|

log(k)
(10)

where cposition and pposition denote the positions

of a child node c and a parent node p, respectively,

in the original sentential sequential order. For in-

stance, in our running example, the parent node s3
has pposition = 3 (i.e., it is the third sentence) and
its child node s5 has cposition = 5. Therefore, their
parent-child distance is |5 − 3| = 2. Following a
similar calculation for other parent-child pairs, we

have Pc =
1+2+2+2
log(5) ≈ 10 .

Similar to Property 2, Property 3 pertains to the

organization of a document and takes into consid-

eration the deviation from sentential sequential or-

der. Intuitively speaking, usually, we expect that a

child node and its parent to be close to each other

in the original sequential order. Consequently, the

less value of this property is, the more coherent

a document is likely to be. The range of Prop-

erty 3 in a document containing k sentences is

Pc ∈ [
k−1
log(k) ,

k(k−1)
2log(k) ].

5 Experiments

To verify the performance of the proposed frame-

work HDSF, we conduct a set of experiments. We

seek to answer the following research questions:

1. How does the proposed framework perform

on fake news detection?

2. How do the defined structure-related prop-

erties describe the fake and real news docu-

ments?

In this section, we first describe the datasets fol-

lowed by presenting the experimental settings. Af-

terward, we evaluate the performance of HDSF

compared to several representative baselines. Fi-

nally, we present a structural analysis of the

fake/real news documents.

5.1 Datasets

We utilize five available fake news datasets in this

study. The first two datasets are collected by (Shu



3438

et al., 2017) and include online articles whose ve-

racities have been identified by experts in Buz-

zFeed1 and PolitiFact2. For the next two datasets,

we utilize two available online fake news datasets

provided by kaggle.com3 4. Finally, we include

the dataset constructed and shared by McIntire5.

Since the proposed framework HDSF is a general-

purpose framework investigating discourse-level

structures of fake/real news documents based on

their textual contents, we do not restrict HDSF to a

particular source of data and therefore combine all

datasets. Similar to the previous work (Shu et al.,

2017), we balance the dataset to avoid a trivial so-

lution as well as ensuring a fair performance com-

parison. In total, we have 3360 fake and 3360 real
documents.

5.2 Experimental Settings

First, we pre-process the documents by remov-

ing numbers, non-English characters, stop-words

(e.g., ‘with’), and converting all characters to

lower case. We randomly select 134 documents
as the development set, (67 from each class) and
134 documents (67 from each class) as the test
set. The remaining 6452 documents are used for
training. The development set is used for tun-

ing the hyper-parameters. We initialize the word

embeddings from the Google news pre-trained

word2vec embeddings (Mikolov et al., 2013)6.

LeakyReLU (Xu et al., 2015) is used as the non-

linear activation function and the number of hid-

den units in the BLSTM network is set to 100.
Each simulation is run for 200 steps with a ran-
dom mini-batch size of 40 documents. The learn-
ing rate starts at 0.01 with the decay rate of 0.9
after every 50 steps. We use the ADAM opti-
mizer (Kingma and Ba, 2014) to optimize the pa-

rameters. The PyTorch package7 is utilized for

the implementation and the code and data are

publicly available in https://github.com/

hamidkarimi/HDSF.

1https://www.buzzfeed.com
2http://www.politifact.com/
3https://www.kaggle.com/mrisdal/

fake-news/data
4https://www.kaggle.com/jruvika/

fake-news-detection
5https://github.com/GeorgeMcIntire/

fake_real_news_dataset
6Out-of-vocabulary words are initialized randomly.
7https://pytorch.org/

5.3 Comparison Results

To answer the research question (1), we compare

the performance of HDSF with the following rep-

resentative baselines.

N-grams. In this baseline method, we ex-

tract and combine unigrams, bigrams, and tri-

grams features and use SVM (Support Vector Ma-

chines) (Scholkopf and Smola, 2001) for classifi-

cation.

LIWC (Pennebaker et al., 2015). LIWC (Lin-

guistic Inquiry and Word Count) offers a set of rich

psycholinguistic features for a written document.

We extract 94 features for each document and use
SVM for classification.

RST (Rubin and Lukoianova, 2015). We ex-

tract a set of RST relations (Mann and Thompson,

1988) using the implementation of the method

proposed by (Ji and Eisenstein, 2014). Then, we

vectorize the relations and employ SVM for clas-

sification. This baseline takes into account the hi-

erarchical structure of documents via RST.

BiGRNN-CNN (Ren and Zhang, 2016). A

CNN (Convolutional Neural Network) is applied

at the sentence-level on word embeddings and a

BiGRNN (Bi-Directional Gated Recurrent Neural

Network) extracts features from a sequence of ex-

tracted sentential features. This baseline takes into

consideration a two-level sequential structure for a

document.

LSTM[w+s]. In this baseline, we apply an

LSTM network on a sequence of word embed-

dings belonging to a sentence and then apply an-

other LSTM on a sequence of extracted sentential

features. LSTM[w+s] also considers a two-level

sequential structure for a document.

LSTM[s]. This method is similar to

LSTM[w+s] except that the mean of word embed-

dings in a sentence is used instead of applying an

LSTM network. LSTM[s] considers a single se-

quential structure for a document.

We use accuracy as the metric of performance

evaluation given that the dataset is fully balanced.

Table 1 shows the comparison results on the test

set and we make the following observations:

• N-grams achieve a better performance than
LIWC. In line with the previous study (Ott

et al., 2011), this shows that for fake news

detection, taking into account the context of

a document as n-grams do is more effective

than employing the existing pre-defined dic-

tionaries as LIWC does.



3439

Method Accuracy (%)

N-grams 72.37

LIWC 70.26

RST 67.68

BiGRNN-CNN 77.06

LSTM[w+s] 80.54

LSTM[s] 73.63

HDSF 82.19

Table 1: Comparison results

• Most of the time, methods wherein a docu-
ment’s structure is somehow taken into ac-

count outperform n-grams and LIWC. This

observation shows that for fake news detec-

tion, the content’s structure plays an impor-

tant role.

• The poor performance of RST is because of
the following reasons: a) using RST without

an annotated corpus is not very effective, and

b) RST relations are extracted using auxiliary

tools optimized for other corpora which can-

not be applied effectively to the fake news

corpus in hand. Note that annotating RST for

our corpus is extremely unscalable and time-

consuming.

• The proposed framework HDSF significantly
outperforms all other methods. This obser-

vation shows that hierarchical discourse-level

representations are effectively rich for fake

news prediction.

5.4 The Inspection of HDSF

To further verify the working of the HDSF frame-

work, we inspect HDSF in more detail. Figure 4a

demonstrates the training error during model op-

timization. As we can observe from this figure,

the error is decreasing as the training process pro-

ceeds. Furthermore, Figure 4b demonstrates the

accuracy on the development set during the train-

ing and it is monotonically increasing as the train-

ing goes on. Hence, based on these figures, we

can ensure the framework is getting optimized and

learns to classify fake news documents correctly.

5.5 Structural Analysis for Fake/Real

Documents

In this section, we compute the average values

of structure-related properties, presented in Sec-

tion 4, for the fake/real news documents belonging

(a) The training error (b) The accuracy on dev set

Figure 4: The Inspection of HDSF

to the test set. Figure 5 shows the results. We make

the two key observations based on this figure:

• There is a significant difference in all three
properties for fake news documents vs. real

news documents. This observation shows the

fact that structures of fake news documents at

the discourse-level are substantially different

from those of real ones.

• Noticeably, in all three properties, the real
news documents show less value than the

fake news documents. As described in

Section 4, all three properties are closely

connected to the coherence of a document.

Therefore, real news documents indicate

more degree of coherence.

6 Related Work

Content-based fake news detection has been the

subject of many linguistic research endeavors.

(DePaulo et al., 2003) investigated fake stories in-

troduced insightful cues in fake stories and high-

lighted ‘unusual’ language in such stories. N-

grams and Part-of-Speech (POS) tags are funda-

mental features of a text which have been utilized

for fake news detection (Ahmed et al., 2017; Ott

et al., 2013). Also, LIWC (Pennebaker et al.,

2015) has been employed to investigate the role

of individual words in a document for deception

detection (Ott et al., 2011). Since POS tags, n-

grams, and LIWC features are considered as ‘shal-

low’ and hand-crafted features, deep neural net-

works have been utilized for fake news detection

where features are extracted automatically (Wang,

2017; Ren and Zhang, 2016; Volkova et al., 2017;

Karimi et al., 2018). In this study, we also utilized

an automated feature extraction instead of relying

on hand-craft features.



3440



3441

Joan Bachenko, Eileen Fitzpatrick, and Michael
Schonwetter. 2008. Verification and implementa-
tion of language-based deception indicators in civil
and criminal narratives. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 41–48. Association for
Computational Linguistics.

Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.

Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.
2015. Better document-level sentiment analysis
from rst discourse parsing. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 2212–2218.

Léon Bottou. 2010. Large-scale machine learning
with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pages 177–186. Springer.

Bella M DePaulo, James J Lindsay, Brian E Mal-
one, Laura Muhlenbruck, Kelly Charlton, and Har-
ris Cooper. 2003. Cues to deception. Psychological
bulletin, 129(1):74.

Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 171–175. Association for
Computational Linguistics.

Camille Guinaudeau and Michael Strube. 2013.
Graph-based local coherence modeling. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 93–103.

Robert Hecht-Nielsen. 1992. Theory of the backprop-
agation neural network. In Neural networks for per-
ception, pages 65–93. Elsevier.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 13–24.

Hamid Karimi, Proteek Roy, Sari Saba-Sadiya, and
Jiliang Tang. 2018. Multi-source multi-class fake
news detection. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 1546–1557.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M Rush. 2017. Structured attention networks.
arXiv preprint arXiv:1702.00887.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jiwei Li and Eduard Hovy. 2014. A model of co-
herence based on distributed sentence representa-
tion. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 2039–2048.

Jiwei Li, Rumeng Li, and Eduard Hovy. 2014a. Recur-
sive deep models for discourse parsing. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
2061–2069.

Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.
2014b. Text-level discourse dependency parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 25–35.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 997–1006. Association for Computational
Linguistics.

Yang Liu and Mirella Lapata. 2018. Learning struc-
tured text representations. Transactions of the Asso-
ciation of Computational Linguistics, 6:63–75.

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse, 8(3):243–281.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Mathieu Morey, Philippe Muller, and Nicholas Asher.
2018. A dependency perspective on rst discourse
parsing and evaluation. Computational Linguistics,
(Just Accepted):1–54.

Myle Ott, Claire Cardie, and Jeffrey T Hancock. 2013.
Negative deceptive opinion spam. In Proceedings
of the 2013 conference of the north american chap-
ter of the association for computational linguistics:
human language technologies, pages 497–501.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 309–319. Association
for Computational Linguistics.

James W Pennebaker, Ryan L Boyd, Kayla Jordan, and
Kate Blackburn. 2015. The development and psy-
chometric properties of liwc2015. Technical report.

Verónica Pérez-Rosas and Rada Mihalcea. 2015. Ex-
periments in open domain deception detection. In
Proceedings of the 2015 Conference on Empirical



3442

Methods in Natural Language Processing, pages
1120–1125.

Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie L
Webber. 2007. The penn discourse treebank 2.0 an-
notation manual.

Yafeng Ren and Yue Zhang. 2016. Deceptive opin-
ion spam detection using neural network. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 140–150.

Victoria L Rubin and Tatiana Lukoianova. 2015. Truth
and deception at the rhetorical structure level. Jour-
nal of the Association for Information Science and
Technology, 66(5):905–917.

Bernhard Scholkopf and Alexander J Smola. 2001.
Learning with kernels: support vector machines,
regularization, optimization, and beyond. MIT
press.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Kai Shu, Suhang Wang, and Huan Liu. 2017. Exploit-
ing tri-relationship for fake news detection. arXiv
preprint arXiv:1712.07709.

Angelika Storrer. 2002. Coherence in text and hyper-
text. Document Design, 3(2):156–168.

Alexander Strehl and Joydeep Ghosh. 2000. Impact of
similarity measures on web-page clustering.

Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and
Nathan Hodas. 2017. Separating facts from fiction:
Linguistic models to classify suspicious and trusted
news posts on twitter. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,
pages 647–653.

William Yang Wang. 2017. ” liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
arXiv preprint arXiv:1705.00648.

Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li.
2015. Empirical evaluation of rectified activa-
tions in convolutional network. arXiv preprint
arXiv:1505.00853.


