



















































KELabTeam: A Statistical Approach on Figurative Language Sentiment Analysis in Twitter


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 679–683,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

KELabTeam: A Statistical Approach on Figurative Language Sentiment
Analysis in Twitter

Hoang Long Nguyen, Trung Duc Nguyen and Dosam Hwang
Department of Computer Engineering

Yeungnam University, Korea
{longnh238, duc.nguyentrung, dosamhwang}@gmail.com

Jason J. Jung∗
Department of Computer Engineering

Chung-Ang University, Korea
j2jung@gmail.com

Abstract

In this paper, we propose a new statistical
method for sentiment analysis of figurative
language within short texts collected from
Twitter (called tweets) as a part of SemEval-
2015 Task 11. Particularly, the proposed
model focuses on classifying the tweets into
three categories (i.e., sarcastic, ironic, and
metaphorical tweet) by extracting two main
features (i.e., term features and emotion pat-
terns). Our experiments have been conducted
with two datasets, which are Trial set (1000
tweets) and Test set (4000 tweets). Perfor-
mance is evaluated by cosine similarity to gold
annotations. Using this evaluation methodol-
ogy, the proposed method achieves 0.74 on the
Trial set. On the Test set, we achieve 0.90 on
sarcastic tweets and 0.89 on ironic tweets.

1 Introduction

Sentiment analysis in computer science is a diffi-
cult task which aims to identify the emotion from a
given data source. The goal of sentiment analysis is
to dissect a given document and determine whether
its opinion represent positive, negative, or neutral.
There have been many studies (which use lexicon-
based methods and machine learning-based meth-
ods) to extract and identify the sentiment (Medhat et
al., 2014). In case of figurative language, the task
becomes more challenging because the document
can have secondary or extended meanings. Hence,
exactly finding the truth meaning of figurative lan-
guage is an interesting problem for researchers due
to its importance.

∗Corresponding author

The first work that we want to mention here
is contributed by Reyes and Rosso (2013a). The
authors captured ironic sentences from low-level
to high-level of irony according to three con-
ceptual layers and their eight textual features.
With customer reviews on Amazon, Reyes and
Rosso (2012a) contributed an approach for distin-
guishing irony and non-irony based on six mod-
els. Also focusing on detecting irony, Hao and
Veale (2010) classifies irony and non-irony by ana-
lyzing the large quantity of simile forms with 9-steps
sequence. By considering short texts with case-
study is Twitter, Reyes et al. (2013b) introduced
a model to detect verbal irony by combining four
types of conceptual features and their dimensions.
Focusing on comprehending metaphor, Shutova et
al. (2010) used unsupervised methods to find the
associate from a small set of metaphorical expres-
sions by verb and noun clustering processing to de-
tect similarity structure of metaphor. Finally, Reyes
et al. (2012b) analyzed humor and irony by adding
more features to express the favorable and unfavor-
able ironic contexts using the theory of textual.

These above studies tried to solve the problem by
focusing on lexical level. Therefore, the goal of
our research is to find a new way to identify fig-
urative meaning. In this work, we focus on ana-
lyzing three types of figurative languages (i.e., sar-
casm, irony, and metaphor) on tweets collected from
Twitter. With FLASA Model (Figurative Language
Analysis using Statistical Approach) to detect multi-
ple types of figurative language, we believe that this
is a general model to solve the problem and easy-
extending for characterizing other types.

679



2 System Description

The Training set includes 8000 tweets collected
from Twitter. All the tweets are presented in English
with three main types of tweets: sarcasm, irony, and
metaphor with the respective ratio: 5000 sarcastic
tweets, 1000 ironic tweets and 2000 metaphorical
tweets.

Z = {< t, s > | s ∈ [−5, 5]} (1)

where Z is a set of tweets in the Training set; t is a
tweet, and s is the score of that tweet.

Tweets are extracted into the set of terms. All
the tweets are pre-processed by: i) considering in
lower-case mode, ii) removing unnecessary infor-
mation such as: the tagged persons, pronouns, iii)
formalizing words (e.g., remove redundancy charac-
ters which repeat more than three times, and correct
the typos). The hash-tags and symbol in the tweets
are kept because of the sentiment expressing prop-
erty. The set of terms which is extracted from Z:

TZ =
n⋃
i=1

ti =
n⋃
i=1

{wj | wj ∈ ti}mj=1 (2)

where TZ is a set of terms that are extracted from Z;
n is the number of tweets in the Training set; wj is a
term; andm is the number of terms that are extracted
from Z.

2.1 FLASA Model
FLASA Model includes two main modules which
are: i) Content-based Approach Module, and ii)
Emotion Pattern-based Approach Module. The final
score of a tweet is calculated by using the following
formula:

S = α× SC + β × SE (3)
where S is the final score of a tweet; SC is the
score that is calculated by Content-based Approach
module; SE is the score that is calculated by Emo-
tion Pattern-based Approach Module; and α, and β
are coefficients identified based on the training error
score of the classification model of each approach,
with α+ β = 1.

2.1.1 Content-based Approach Module
Content-based approach module evaluate the sen-

timent of a tweet based on the co-occurrence of

terms which are extracted from a tweet using the
Training set. This method basically use statistics on
the Training set to predict the score of a tweet.

With a tweet tk that is needed to be annotated.
First, it is extracted into set of terms:

Tk =
⋃
{wi | wi ∈ tk}mki=1 (4)

where Tk is the set of terms extracted from tweet
tk; wi is a term belongs to tweet tk; and mk is the
number of terms which are extracted from tweet tk.

From Tk, we build all the possible combinations
from the set of terms to consider all the possible
co-occurrence of terms because terms can express
different meaning when they appear together. With
this step, we can achieve all these aspects: i) all the
meaning of the tweet tk when terms co-exist, and ii)
some main terms that affect the score of the tweet
tk. We can consider each of combination is a cluster
which can respective as a feature vector:

Ck =

{
(δk)

γk
i=1

∣∣∣∣∣γk =
mk∑
j=1

(
mk
j

)}
(5)

where Ck is the set of all possible clusters extract
from the given tweet; δk is a cluster, each cluster
can be represented as a feature vector; and γk is the
number of all combinations which are created from
terms in Tk.

Each cluster in Ck is represented as a feature vec-
tor, with the dimension equals with the number of
terms in Tk. From the set of tweets Z in the Train-
ing set, we cluster every tweet into the set of cluster
Ck. A tweet is assigned into a cluster in the case:
i) the distance between a vector to a cluster is mini-
mum comparing to its distance to other clusters, and
ii) the distance has to smaller than a defined thresh-
old. This has a significant meaning in expressing the
co-occurrence of terms in a tweet. The distance be-
tween a tweet and a cluster is calculated by using the
following formula:

dis(A,B) = 1− A
TB

|A||B| (6)

where dis(A,B) is the distance between a term and
a cluster.

Each cluster has a cluster coefficient which is cal-
culated from the number of feature terms of a clus-
ter. If a cluster has more terms, its coefficient will

680



−5
.0

−4
.5

−4
.0

−3
.5

−3
.0

−2
.5

−2
.0

−1
.5

−1
.0

−0
.5 0.
0

0.
5

1.
0

1.
5

2.
0

2.
5

3.
0

3.
5

4.
0

4.
5

5.
0

0

2

4

6

8

10

12

14

16

Tweet score

To
ta

lc
oe

ffi
ci

en
t

Figure 1: Histogram of score distribution.

be higher. The cluster coefficient can expresses how
important it affects the final score of a tweet. Then,
from tweets in clusters with their scores and coeffi-
cient, the histogram is built to represent the distribu-
tion of score in the Training set. Finally, the score
of a tweet is annotated by selecting the peak of the
histogram.

Example 1. We have 3 clusters: cluster {A,B,C}:
includes 3 tweets (< t1,−2.5 >; < t2,−3.5 >;
< t3,−3.5 >); cluster {B,C}: includes 3 tweets
(< t4, 0.0 >; < t5,−2.0 >; < t6,−2.0 >);
cluster {C}: includes 4 tweets (< t7,−4.5 >;
< t8,−3.0 >; < t9,−0.5 >; < t10,−1.5 >).
Figure 1 expresses the above data as histogram. In
this case, the score of tweet which is calculated by
Content-based Approach Module is −3.5.

2.1.2 Emotion Pattern-based Approach
Module

The Emotion Pattern-based Approach Module de-
termine the score of a given tweet based on the emo-
tion change pattern in the content. This approach
consists in calculating the sentiment score for each
term, then construct the emotion distribution pattern
using the termss score in the tweet corresponding to
its occurrence positions.

Each term has a score which is calculated based
on tweets in the Training set. By finding the score
of term and the pattern of tweet, we can understand
about how important a term contributes to the final
score of a tweet, and about the sentiment degree of a
term, whether it’s positive, negative, or neutral. The
score of a tweet is decided by the pattern of terms in
a sentence. Our goal is try to find the real score of
a term. In the Traning set, a term belongs to many
tweets, and in each tweet, it represents a different

score. Assuming that all the tweets have equatable
meaning, the score of a term is calculated by the fol-
lowing formula:

Sw =
∑l
i=1 Swi
l

(7)

where Sw is the score of a term; and l is the number
of tweets which contain this term.

From the set of tweets Z and the set of terms T ,
we can find the distribution of a term by using the
score of tweets which contain it. The peak of his-
togram is the point at that a term has highest distri-
bution with a score. At the beginning (i0 step), each
term has the score which is selected from the peak
of its respective histogram. Then, the score in the
step i+ 1 is calculated by using the formula:

Siw =
Si−1w ∗ P (St|w)∑n

j=1(S
i−1
wj ∗ P (St|wj))

∗ St (8)

where Siw is the score of a term at step ith; St is the
score of tweet that contains this term; and P (St|w)
is the probability that a term has the score with given
tweet score.

This step is conducted repeatably until the score
of term at step ith greater than the score of term at
step (i−1)th a value of defined epsilon, with epsilon
is extremely small.

With each tweet in the Training set, it is extracted
into the list of terms and then create a pattern based
on its term scores as we mentioned above. Due to
the different of the number of terms in a tweet, the
signal of pattern is needed to be scaled by using an
interpolation function. The pattern is scaled to the
maximum possible terms that a tweet in the Training
set contain in order to be able to map all the tweets
into vectors with same dimension.

Example 2. We have a tweet: @SamySam-
son wow you’re soooo funny #sarcasm it
actually hurts a bunch!. From this tweet,
we have list of terms and their scores:
(< wow,−0.2057831 >; < soo,−0.1552674 >; <
funny,−0.19274 >; < #sarcasm,−2.34994 >;
< actually,−0.03287 >; < hurts,−0.16091 >;
< bunch,−0.02096 >). Figure 2 expresses the
pattern of the above data after the term scores are
scaled down by the size of largest terms in a tweet

681



found in the Training set. Here, the maximum num-
ber of terms that a tweet contains in the Training set
is 24.

-5

-4

-3

-2

-1

0

1

2

3

4

5

 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24

T
e
rm

 s
c
o
re

Terms extracted from a tweet

Term Score

Figure 2: Sequential pattern of tweet term scores
after length normalization.

Using the set of patterns from the Training set,
we construct a vector space representation whereby
each dimension signifies a match to one of the ex-
tracted patterns. We then train a decision tree based
classifier to predict from these vectors the inte-
ger sentiment labels [−5..5] of the corresponding
tweets. And that is the score which is annotated by
using Emotion Pattern-based Approach Module.

3 Experimental results

The test data comprises 4000 tweets with both fig-
urative and non-figurative tweets with 70% of them
are sarcasm, irony, or metaphor; and 30% of the data
are other. We evaluate the test with: i) Content-
based Approach Module, ii) Emotion Pattern-based
Approach Module, and iii) Combined Module.

FLASA Model works well with figurative tweets.
Using cosine similarity to gold annotations to evalu-
ate the system, the highest performance that we got
is 0.90 with irony type, and the next is sarcastic type
with 0.89. With metaphor type, we achieve 0.34
with annotated tweets. About non-figurative tweets,
the performance is still low due to the tweets in the
Training set. The root cause is that there are no non-
figurative tweets in the Training set. If we add more
non-figurative tweets to the Training set in order to
learn, the result will be improved. Fig. 3 shows the
performance that we got from testing our approach
on the Test set.

 0

 0.2

 0.4

 0.6

 0.8

 1

Sarcasm Irony Metaphor Other Overall

Content-based

Emotion-Pattern-based

Combined

Figure 3: The performance of FLASA Model on
Test set using cosine similarity.

4 Conclusion

In this paper, we proposed a new approach for ana-
lyzing the sentiment of figurative language based-
on the statistics with two main approaches: con-
tent and emotional pattern. By combining all these
features, we enhanced the performance of our algo-
rithm. However, the result of FLASA Model is af-
fected by these following reasons:
i) Almost all the tweets in the Training set are

sarcastic tweets, and irony tweets. Due to this rea-
son, the performance on metaphor tweets, and non-
figurative tweets are still low.
ii) Is this work, we only consider unigram model

when calculating the score for terms in Emotion
Pattern-based Approach. This leads to the miss-
expressing meaning of terms if they are co-showing
an specific sense in a phrase.
iii) Our training data has a little noise be-

cause some tweets are written in an unstandardized
way(e.g. abbreviation word, and repeated word).

In the next work, we will improve the perfor-
mance by increasing the number of tweets in the
Training set, especially the metaphor tweets, and
non-figurative tweets. Bigram or trigram model will
be used to clearly comprehend the sentiment of a
tweet. Moreover, we will add more heuristic to com-
pletely formalize tweets. Finally, we will extend
FLASA Model to analyze the data from the other
social network, such as Facebook, Instagram, Flick,
and Google Plus also.

Acknowledgment

This work was supported by the National
Research Foundation of Korea (NRF) grant

682



funded by the Korea government (MSIP) (NRF-
2014R1A2A2A05007154). Also, this research
was supported by the MSIP (Ministry of Science,
ICT & Future Planning), Korea, under the ITRC
(Information Technology Research Cetner) support
program (NIPA-2014-H0301-14-1044) supervised
by the NIPA (National ICT Industry Promotion
Agency).

References
Hao, Y., & Veale, T. 2010. An Ironic Fist in a Velvet

Glove: Creative Mis-Representation in the Construc-
tion of Ironic Similes. Minds and Machines, 20(4),
635-650.

Kaur, A., & Gupta, V. 2013. A Survey on Sentiment
Analysis and Opinion Mining Techniques. Ain Journal
of Emerging Technologies in Web Intelligence, 5(4),
367-371.

Kumon-Nakamura, S., Glucksberg, S., & Brown, M.
1995. How about Another Piece of Pie: The Allu-
sional Pretense Theory of Discourse Irony. Journal
of Experimental Psychology General, 124(1), 3-21.

Medhat, W., Hassan, A., & Korashy, H. 2014. Sentiment
Analysis Algorithms and Applications: A Survey. Ain
Shams Engineering Journal, 5(4), 1093-1113.

Reyes, A., & Rosso, P. 2013a. On the Difficulty of Au-
tomatically Detecting Irony: Beyond a Simple Case
of Negation. Knowledge and Information Systems,
40(3), 595-614.

Reyes, A., Rosso, P., & Veale, T. 2013b. A Multidimen-
sional Approach for Detecting Irony in Twitter. Lan-
guages Resources and Evaluation, 47(1), 239-268.

Reyes, A., & Rosso, P. 2012a. Making Objective Deci-
sions from Subjective Data: Detecting Irony in Cus-
tomers Reviews. Journal on Decision Support Sys-
tems, 53(4), 754-760.

Reyes, A., Rosso, P., & Buscaldi, D. 2012b. From Hu-
mor Recognition to Irony Detection: The Figurative
Language of Social Media. Data & Knowledge Engi-
neering, 74(0), 1-12.

Shutova, E., Sun, L., & Korhonen, A. 2010. Metaphor
Identification Using Verb and Noun Clustering. Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING 2010), Beijing,
China, August 23-27, pp. 1002-1010.

683


