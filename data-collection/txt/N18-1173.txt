



















































Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem


Proceedings of NAACL-HLT 2018, pages 1907–1918
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Word Emotion Induction for Multiple Languages as
a Deep Multi-Task Learning Problem

Sven Buechel & Udo Hahn
{sven.buechel|udo.hahn}@uni-jena.de

Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universität Jena, Jena, Germany

http://www.julielab.de

Abstract

Predicting the emotional value of lexical items
is a well-known problem in sentiment analy-
sis. While research has focused on polarity for
quite a long time, meanwhile this early focus
has been shifted to more expressive emotion
representation models (such as Basic Emo-
tions or Valence-Arousal-Dominance). This
change resulted in a proliferation of hetero-
geneous formats and, in parallel, often small-
sized, non-interoperable resources (lexicons
and corpus annotations). In particular, the lim-
itations in size hampered the application of
deep learning methods in this area because
they typically require large amounts of input
data. We here present a solution to get around
this language data bottleneck by rephrasing
word emotion induction as a multi-task learn-
ing problem. In this approach, the predic-
tion of each independent emotion dimension
is considered as an individual task and hidden
layers are shared between these dimensions.
We investigate whether multi-task learning
is more advantageous than single-task learn-
ing for emotion prediction by comparing our
model against a wide range of alternative emo-
tion and polarity induction methods featuring
9 typologically diverse languages and a total
of 15 conditions. Our model turns out to out-
perform each one of them. Against all odds,
the proposed deep learning approach yields the
largest gain on the smallest data sets, merely
composed of one thousand samples.

1 Introduction

Deep Learning (DL) has radically changed the
rules of the game in NLP by dramatically boost-
ing performance figures in almost all applications
areas. Yet, one of the major premises of high-
performance DL engines is their dependence on
huge amounts of training data. As such, DL seems
ill-suited for areas where training data are scarce,
such as in the field of word emotion induction.

We will use the terms polarity and emotion here
to distinguish between research focusing on “se-
mantic orientation” (Hatzivassiloglou and McKe-
own, 1997) (the positiveness or negativeness) of
affective states, on the one hand, and approaches
which provide predictions based on some of the
many more elaborated representational systems
for affective states, on the other hand.

Originally, research activities focused on polar-
ity alone. In the meantime, a shift towards more
expressive representation models for emotion can
be observed that heavily draws inspirations from
psychological theory, e.g., Basic Emotions (Ek-
man, 1992) or the Valence-Arousal-Dominance
model (Bradley and Lang, 1994).

Though this change turned out to be really ben-
eficial for sentiment analysis in NLP, a large vari-
ety of mutually incompatible encodings schemes
for emotion and, consequently, annotation formats
for emotion metadata in corpora have emerged that
hinder the interoperability of these resources and
their subsequent reuse, e.g., on the basis of align-
ments or mergers (Buechel and Hahn, 2017).

As an alternative way of dealing with thus
unwarranted heterogeneity, we here examine the
potential of multi-task learning (MTL; Caruana
(1997)) for word-level emotion prediction. In
MTL for neural networks, a single model is fit-
ted to solve multiple, independent tasks (in our
case, to predict different emotional dimensions)
which typically results in learning more robust and
meaningful intermediate representations. MTL
has been shown to greatly decrease the risk of
overfitting (Baxter, 1997), work well for various
NLP tasks (Setiawan et al., 2015; Liu et al., 2015;
Søgaard and Goldberg, 2016; Cummins et al.,
2016; Liu et al., 2017; Peng et al., 2017), and
practically increases sample size, thus making it
a natural choice for small-sized data sets typically
found in the area of word emotion induction.

1907



After a discussion of related work in Section 2,
we will introduce several reference methods and
describe our proposed deep MTL model in Sec-
tion 3. In our experiments (Section 4), we will
first validate our claim that MTL is superior to
single-task learning for word emotion induction.
After that, we will provide a large-scale evalua-
tion of our model featuring 9 typologically diverse
languages and multiple publicly available embed-
ding models for a total of 15 conditions. Our
MTL model surpasses the current state-of-the-art
for each of them, and even performs competitive
relative to human reliability. Most notably how-
ever, our approach yields the largest benefit on the
smallest data sets, comprising merely one thou-
sand samples. This finding, counterintuitive as it
may be, strongly suggests that MTL is particularly
beneficial for solving the word emotion induction
problem. Our code base as well as the resulting
experimental data is freely available.1

2 Related Work

This section introduces the emotion representation
format underlying our study and describes exter-
nal resources we will use for evaluation before we
discuss previous methodological work.

Emotion Representation and Data Sets. Psy-
chological models of emotion can typically be
subdivided into discrete (or categorical) and di-
mensional ones (Stevenson et al., 2007; Calvo and
Mac Kim, 2013). Discrete models are centered
around particular sets of emotional categories con-
sidered to be fundamental. Ekman (1992), for in-
stance, identifies six Basic Emotions (Joy, Anger,
Sadness, Fear, Disgust and Surprise).

In contrast, dimensional models consider emo-
tions to be composed of several influencing fac-
tors (mainly two or three). These are often referred
to as Valence (a positive–negative scale), Arousal
(a calm–excited scale), and Dominance (perceived
degree of control over a (social) situation)—the
VAD model (Bradley and Lang (1994); see Figure
1 for an illustration). Many contributions though
omit Dominance (the VA model) (Russell, 1980).
For convenience, we will still use the term “VAD”
to jointly refer to both variants (with and without
Dominance).

VAD is the most common framework to acquire
empirical emotion values for words in psychology.

1 https://github.com/JULIELab/wordEmotions

−1.0 −0.5  0.0  0.5  1.0−
1.

0
−0

.5
 0

.0
 0

.5
 1

.0

−1.0

−0.5

 0.0

 0.5

 1.0

●

●

●
●

●

●

Anger

SurpriseDisgust

Fear

Sadness

Joy

Valence

D
om

in
an
ce

Figure 1: Affective space spanned by the Valence-
Arousal-Dominance (VAD) model, together with the
position of six Basic Emotions; as determined by Rus-
sell and Mehrabian (1977).

Over the years, a considerable number of such
resources (also called “emotion lexicons”) have
emerged from psychological research labs (as well
as some NLP labs) for diverse languages. The
emotion lexicons we use in our experiments are
listed in Table 1. An even more extensive list of
such data sets is presented by Buechel and Hahn
(2018). For illustration, we also provide three
sample entries from one of those lexicons in Ta-
ble 2. As can be seen, the three affective dimen-
sions behave complementary to each other, e.g.,
“terrorism” and “orgasm” display similar Arousal
but opposing Valence.

The task we address in this paper is to predict
the values for Valence, Arousal and Dominance,
given a lexical item. As is obvious from these ex-
amples, we consider emotion prediction as a re-
gression, not as a classification problem (see argu-
ments discussed in Buechel and Hahn (2016)).

In this paper, we focus on the VAD format for
the following reasons: First, note that the Valence
dimension exactly corresponds to polarity (Turney
and Littman, 2003). Hence, with the VAD model,
emotion prediction can be seen as a generalization
over classical polarity prediction. Second, to the
best of our knowledge, the amount and diversity of
available emotion lexicons with VAD encodings is
larger than for any other format (see Table 1).

Word Embeddings. Word embeddings are
dense, low-dimensional vector representations of
words trained on large volumes of raw text in an
unsupervised manner. The following are among
today’s most popular embedding algorithms:

1908



Source ID Language Format # Entries

Bradley and Lang (1999) EN English VAD 1,034
Warriner et al. (2013) EN+ English VAD 13,915
Redondo et al. (2007) ES Spanish VAD 1,034
Stadthagen-Gonzalez et al. (2017) ES+ Spanish VA 14,031
Schmidtke et al. (2014) DE German VAD 1,003
Yu et al. (2016a) ZH Chinese VA 2,802
Imbir (2016) PL Polish VAD 4,905
Montefinese et al. (2014) IT Italian VAD 1,121
Soares et al. (2012) PT Portuguese VAD 1,034
Moors et al. (2013) NL Dutch VAD 4,299
Sianipar et al. (2016) ID Indonesian VAD 1,490

Table 1: Emotion lexicons used in our experiments (with their bibliographic source, identifier, language they refer
to, emotion representation format, and number of lexical entries they contain).

Word Valence Arousal Dominance
sunshine 8.1 5.3 5.4
terrorism 1.6 7.4 2.7
orgasm 8.0 7.2 5.8

Table 2: Three sample entries from Warriner et al.
(2013). They use 9-point scales ranging from 1
(most negative/calm/submissive) to 9 (most posi-
tive/excited/dominant).

WORD2VEC (with its variants SGNS and CBOW)
features an extremely trimmed down neural
network (Mikolov et al., 2013). FASTTEXT is
a derivative of WORD2VEC, also incorporating
sub-word character n-grams (Bojanowski et al.,
2017). Unlike the former two algorithms which
fit word embeddings in a streaming fashion,
GLOVE trains word vectors directly on a word
co-occurrence matrix under the assumption to
make more efficient use of word statistics (Pen-
nington et al., 2014). Somewhat similar, SVDPPMI
performs singular value decomposition on top of
a point-wise mutual information co-occurrence
matrix (Levy et al., 2015).

In order to increase the reproducibility of our
experiments, we rely on the following widely
used, publicly available embedding models trained
on very large corpora (summarized in Table 3):
the SGNS model trained on the Google News cor-
pus2 (GOOGLE), the FASTTEXT model trained
on Common Crawl3 (COMMON), as well as the
FASTTEXT models for a wide range of languages
trained on the respective Wikipedias4 (WIKI).

2https://code.google.com/archive/p/
word2vec/

3https://fasttext.cc/docs/en/
english-vectors.html

4https://github.com/facebookresearch/
fastText/blob/master/pretrained-vectors.
md

Note that WIKI denotes multiple embedding mod-
els with different training and vocabulary sizes
(see Grave et al. (2018) for further details). Ad-
ditionally, we were given the opportunity to reuse
the English embedding model from Sedoc et al.
(2017) (GIGA), a strongly related contribution (see
below). Their embeddings were trained on the En-
glish Gigaword corpus (Parker et al., 2011).

Word-Level Prediction. One of the early ap-
proaches to word polarity induction which is
still popular today (Köper and Schulte im Walde,
2016) was introduced by Turney and Littman
(2003). They compute the polarity of an unseen
word based on its point-wise mutual information
(PMI) to a set of positive and negative seed words,
respectively.

SemEval-2015 Task 10E featured polarity in-
duction on Twitter (Rosenthal et al., 2015). The
best system relied on support vector regression
(SVR) using a radial base function kernel (Amir
et al., 2015). They employ the embedding vec-
tor of the target word as features. The results of
their SVR-based system were beaten by the DEN-
SIFIER algorithm (Rothe et al., 2016). DENSIFIER
learns an orthogonal transformation of an embed-
ding space into a subspace of strongly reduced di-
mensionality.

Hamilton et al. (2016) developed SENTPROP, a
graph-based, semi-supervised learning algorithm
which builds up a word graph, where vertices cor-
respond to words (of known as well as unknown
polarity) and edge weights correspond to the sim-
ilarity between them. The polarity information is
then propagated through the graph, thus comput-
ing scores for unlabeled nodes. According to their
evaluation, DENSIFIER seems to be superior over-
all, yet SENTPROP produces competitive results

1909



ID Language Method Corpus # Tokens # Types # Dimensions

GOOGLE English SGNS Google News 1× 1011 3× 106 300
COMMON English FASTTEXT Common Crawl 6× 1011 2× 106 300
GIGA English CBOW Gigawords 4× 10 9 2× 106 300
WIKI all FASTTEXT Wikipeda — — 300

Table 3: Embedding models used for our experiments with identifier, language, embedding algorithm, training
corpus, its size in the number of tokens, size of the vocabulary (types) of the resulting embedding model and its
dimensionality.

only when the seed lexicon or the corpus the word
embeddings are trained on is very small.5

For word emotion induction, a very similar ap-
proach to SENTPROP has been proposed by Wang
et al. (2016a). They also propagate affective infor-
mation (Valence and Arousal, in this case) through
a word graph with similarity weighted edges.

Sedoc et al. (2017) recently proposed an ap-
proach based on signed spectral clustering where a
word graph is constructed not only based on word
similarity but also on the considered affective in-
formation (again, Valence and Arousal). The emo-
tion value of a target word is then computed based
on the seed words in its cluster. They report to
outperform the results from Wang et al. (2016a).

Contrary to the trend to graph-based methods,
the best system of the IALP 2016 Shared Task
on Chinese word emotion induction (Yu et al.,
2016b) employed a simple feed-forward neural
network (FFNN) with one hidden layer in com-
bination with boosting (Du and Zhang, 2016).

Another very recent contribution which advo-
cates a supervised set-up was published by Li et al.
(2017). They propose ridge regression, again us-
ing word embeddings as features. Even with this
simple approach, they report to outperform many
of the above methods in the VAD prediction task.6

Sentence-Level and Text-Level Prediction.
Different from the word-level prediction task
(the one we focus on in this contribution), the
determination of emotion values for higher-level
linguistic units (especially sentences and texts)
is also heavily investigated. For this problem,
DL approaches are meanwhile fully established
as the method of choice (Wang et al., 2016b;
Abdul-Mageed and Ungar, 2017; Felbo et al.,
2017; Mohammad and Bravo-Marquez, 2017).

5Personal correspondence with William L. Hamil-
ton; See also README at https://github.com/
williamleif/socialsent

6However, they also report extremely weak performance
figures for some of their reference methods.

It is important to note, however, that the meth-
ods discussed for these higher-level units cannot
easily be transferred to solve the word emotion in-
duction problem. Sentence-level and text-level ar-
chitectures are either adapted to sequential input
data (typical for RNN, LSTM, GRNN and related
architectures) or spatially arranged input data (as
with CNN architectures). However, for word em-
beddings (the default input for word emotion in-
duction) there does not seem to be any meaningful
order of their components. Therefore, these more
sophisticated DL methods are, for the time being,
not applicable for the study at hand.

3 Methods

In this section, we will first introduce various ref-
erence methods (two originally polarity-based for
which we offer adaptations for VAD prediction)
before defining our own neural MTL model and
discussing its difference from previous work.

Let V := {w1, w2, ..., wm} be our word vocab-
ulary and let E := {e1, e2, ..., em} be a set of em-
bedding vectors such that ei ∈ Rn denotes the n-
dimensional vector representation of word wi. Let
D := {d1, d2, ..., dl} be a set of emotional dimen-
sions. Our task is to predict the empirically deter-
mined emotion vector emo(w) ∈ Rl given a word
w and the embedding space E.

3.1 Reference Methods

Linear Regression Baseline (LinReg). We pro-
pose (multi-variate) linear regression as an obvi-
ous baseline for the problem:

emoLR(wk) := Wek + b (1)

where W is a matrix, Wi∗ contains the regression
coefficients for the i-th affective dimension and b
is the vector of bias terms. The model parame-
ters are fitted using ordinary least squares. Tech-
nically, we use the scikit-learn.org imple-
mentation with default parameters.

1910



Ridge Regression (RidgReg). Li et al. (2017)
propose ridge regression for word emotion induc-
tion. Ridge regression works identically to lin-
ear regression during prediction, but introduces
L2 regularization during training. Following the
authors, for our implementation, we again use
the scikit-learn implementation with default
parameters.

Turney-Littman Algorithm (TL). As one of
the earliest contributions in the field, Turney and
Littman (2003) defined a simple PMI-based ap-
proach to determine the semantic polarity SPTL
of a word w:

SPTL(w) :=
∑

s∈seeds+
pmi(w, s) −

∑

s∈seeds−
pmi(w, s)

(2)
where seeds+ and seeds− are sets of positive and
negative seed words, respectively. Since this algo-
rithm is still popular today (Köper and Schulte im
Walde, 2016), we here provide a novel modifica-
tion for adapting this originally polarity-based ap-
proach to word emotion induction with vectorial
seed and output values.

First, we replace PMI-based association of seed
and target word w and s by their similarity sim
based on their word embeddings ew and es:

sim(w, s) := max(0,
ew · es

||ew|| × ||es||
) (3)

emo(w) :=
∑

s∈seeds+
sim(w, s) −

∑

s∈seeds−
sim(w, s)

(4)
Although this step is technically not required for
the adaptation, it renders the TL algorithm more
comparable to the other approaches evaluated in
Section 4 besides from most likely increasing per-
formance. Equation (4) can be rewritten as

emo(w) :=
∑

s∈seeds
sim(w, s)× emo(s) (5)

where seeds := seeds+ ∪ seeds− and emo(s)
maps to 1, if s ∈ seeds+, and −1, if s ∈ seeds−.

Equation (5) can be trivially adapted to an n-
dimensional emotion format by redefining emo(s)
such that it maps to a vector from Rn instead of
{−1, 1}. Our last step is to introduce a normal-
ization term such that emo(w)TL lies within the

range of the seed lexicon.

emoTL(w) :=

∑
s∈seeds sim(w, s)× emo(s)∑

s∈seeds sim(w, s)
(6)

As can be seen from Equation (6), for the more
general case of n-dimensional emotion prediction,
the Turney-Littman algorithm naturally translates
into a weighted average where the seed emotion
values are weighted according to the similarity to
the target item.

Densifier. Rothe et al. (2016) train an orthogo-
nal matrix Q ∈ Rn×n (n being the dimensionality
of the word embeddings) such that applying Q to
an embedding vector ei concentrates all the polar-
ity information in its first dimension such that the
polarity of a word wi can be computed as

SPDENSIFIER(wi) := pQei (7)

where p = (1, 0, 0, ..., 0)T ∈ R1×n .
For fitting Q, the seeds are arranged into pairs of

equal polarity (the set pairs=) and those of oppos-
ing polarity (pairs6=). A good fit for Q will mini-
mize the distance within the former and maximize
the distance within the latter which can be ex-
pressed by the following two training objectives:

argmin
Q

∑

(wi,wj)∈pairs=
|pQ(ei − ej)| (8)

argmax
Q

∑

(wi,wj)∈pairs6=
|pQ(ei − ej)| (9)

The objectives described in the expressions (8) and
(9) are combined into a single loss function (using
a weighting factor α ∈ [0, 1]) which is then mini-
mized using stochastic gradient descent (SGD).

To adapt this algorithm to dimensional emotion
formats, we construct a positive seed set, seeds+v ,
and a negative seed set, seeds−v , for each emotion
dimension v ∈ D. LetMv be the mean value of all
the entries of the training lexicon for the affective
dimension v. Let SDv be the respective standard
deviation and β ∈ R, β ≥ 0. Then all entries
greater than Mv + βSDv are assigned to seeds+v
and those less than Mv − βSDv are assigned to
seeds−v . Q is fitted individually for each emotion
dimension v.

Training was performed according to the orig-
inal paper with the exception that (following
Hamilton et al. (2016)) we did not apply the
proposed re-orthogonalization after each training

1911



step, since we did not find any evidence that
this procedure actually results in improved perfor-
mance. The hyperparameters α and β were set to
.7 and .5 (respectively) for all experiments based
on a pilot study. Since the original implementa-
tion is not accessible, we devised our own using
tensorflow.org.

Boosted Neural Networks (ensembleNN). Du
and Zhang (2016) propose simple FFNNs in com-
bination with a boosting algorithm. An FFNN
consists of an input or embedding layer with acti-
vation a(0) ∈ Rn which is equal to the embedding
vector ek when predicting the emotion of a word
wk. The input layer is followed by multiple hidden
layers with activation

a(l+1) := σ(W (l+1)a(l) + b(l+1)) (10)

where W (l+1) and b(l+1) are the weights and bi-
ases for layer l + 1 and σ is a nonlinear activation
function. Since we treat emotion prediction as a
regression problem, the activation on the output
layer aout (where out is the number of non-input
layers in the network) is computed as the affine
transformation

a(out) := W (out)a(out−1) + b(out) (11)

Boosting is a general machine learning tech-
nique where several weak estimators are combined
to form a strong estimator. The authors used
FFNNs with a single hidden layer of 100 units
and rectified linear unit (ReLU) activation. The
boosting algorithm AdaBoost.R2 (Drucker, 1997)
was used to train the ensemble (one per affective
dimension). Our re-implementation copies their
technical set-up7 exactly using scikit-learn.

3.2 Multi-Task Learning Neural Network
The approaches introduced in Section 3.1 and Sec-
tion 2 vary largely in their methodological founda-
tions, i.e., they comprise semi-supervised and su-
pervised machine learning techniques—both sta-
tistical and neural ones. Yet, they all have in com-
mon that they treat the prediction of the different
emotional dimensions as separate tasks. That is,
they fit one individual model per VAD dimension
without sharing parameters between them.

In contradistinction, the key feature of our ap-
proach is that we fit a single FFNN model to

7Original settings available at https://github.
com/StevenLOL/ialp2016_Shared_Task

a
(0)
1 a

(0)
2 a

(0)
3
. . . a(0)300

a
(1)
1 a

(1)
2 a

(1)
3
. . . a(1)256

a
(2)
1 a

(2)
2
. . . a(2)128

a
(3)
1 a

(3)
2 a

(3)
3

output layer
affine transformation

two hidden layers
shared across VAD

.5 dropout
LReLU activation

embedding layer
.2 dropout

Figure 2: MTL architecture for VAD prediction.

predict all VAD dimensions jointly, thus apply-
ing multi-task learning to word emotion induction.
Hence, we treat the prediction of Valence, Arousal
and Dominance as three independent tasks. Our
multi-task learning neural network (MTLNN) (de-
picted in Figure 2) has an output layer of three
units such that each output unit represents one of
the VAD dimensions. However, the activation in
our two hidden layers (of 256 and 128 units, re-
spectively) is shared across all VAD dimensions,
and so are the associated weights and biases.

Thus, while we train our MTLNN model it is
forced to learn intermediate representations of the
input which are generally informative for all VAD
dimensions. This serves as a form of regulariza-
tion, since it becomes less likely for our model to
fit the noise in the training set as noise patterns
may vary across emotional dimensions. Simulta-
neously, this has an effect similar to an increase
of the training size, since each sample now leads
to additional error signals during backpropagation.
Intuitively, both properties seem extremely use-
ful for relatively small-sized emotion lexicons (see
Section 4 for empirical evidence).

The remaining specifications of our model
are as follows. We use leaky ReLU activation
(LReLU) as nonlinearity (Maas et al., 2013).

LReLU(zi) := max(γzi, zi) (12)

with γ := .01 for our experiments. For regular-
ization, dropout (Srivastava et al., 2014) is applied
during training with a probability of .2 on the em-
bedding layer and .5 on the hidden layers. We train
for 15, 000 iterations (well beyond convergence on
each data set we use) with the ADAM optimizer
(Kingma and Ba, 2015) of .001 base learning rate,

1912



batch size of 128 and Mean-Squared-Error loss.
The weights are randomly initialized (drawn from
a normal distribution with a standard deviation
.001) and biases are uniformly initialized as .01.
Tensorflow is used for implementation.

4 Results

In this section, we first validate our assumption
that MTL is superior to single-task learning for
word emotion induction. Next, we compare our
proposed MTLNN model in a large-scale evalua-
tion experiment.

Performance figures will be measured as Pear-
son correlation (r) between our automatically pre-
dicted values and human gold ratings. The Pear-
son correlation between two data series X =
x1, x2, ..., xn and Y = y1, y2, ..., yn takes values
between +1 (perfect positive correlation) and −1
(perfect negative correlation) and is computed as

rxy :=

∑n
i=1(xi − x̄)(yi − ȳ)√∑n

i=1(xi − x̄)2
√∑n

i=1(yi − ȳ)2
(13)

where x̄ and ȳ denote the mean values for X and
Y , respectively.

4.1 Single-Task vs. Multi-Task Learning

The main hypothesis of this contribution is that an
MTL set-up is superior to single-task learning for
word emotion induction. Before proceeding to the
large-scale evaluation of our proposed model, we
will first examine this aspect of our work.

For this, we use the following experimental set-
up: We will compare the MTLNN model against
its single-task learning counterpart (SepNN).
SepNN simultaneously trains three separate neu-
ral networks where only the input layer, yet no
parameters of the intermediate layers are shared
across the models. Each of the separate networks
is identical to MTLNN (same layers, dropout, ini-
tialization, etc.), yet has only one output neuron,
thus modeling only one of the three affective VAD
dimensions. SepNN is equivalent to fitting our
proposed model (but with only one output unit)
to the different VAD dimensions individually, one
after the other. Yet, training these separate net-
works simultaneously (not jointly!) makes both
approaches, MTLNN and SepNN, easier to com-
pare.

We will run MTLNN against SepNN on the
EN and the EN+ data set (the former is very

2000 4000 6000 8000 10000 12000 14000
Training Steps

0.650

0.675

0.700

0.725

0.750

0.775

0.800

0.825

Pe
ar

so
n 

Co
rre

la
tio

n

MTLNN on EN
sepNN on EN
MTLNN on EN+
sepNN on EN+

Figure 3: Performance of our proposed MTLNN
model vs. its single-task learning counterpart SepNN
against training steps.

small, the latter relatively large; see Table 1) us-
ing the following set-up: for each gold lexicon
and model, we randomly split the data 9/1 and
train for 15, 000 iterations on the larger split (the
same number of steps is used for the main exper-
iment). After each one-thousand iterations step,
model performance is tested on the held-out data.
This process will be repeated 20 times and the per-
formance figures at each one-thousand iterations
step will be averaged. In a final step, we will av-
erage the results for each of the three emotional
dimensions and only plot this average value. The
results of this experiment are depicted in Figure 3.

First of all, each combination of model and data
set displays a satisfactory performance of at least
r ≈ .75 after 15,000 steps compared to previous
work (see below). Overall, performance is higher
for the smaller EN lexicon. Although counterintu-
itive (since smaller lexicons lead to fewer training
samples), this finding is consistent with prior work
(Sedoc et al., 2017; Li et al., 2017) and is prob-
ably related to the fact that smaller lexicons usu-
ally comprise a larger portion of strongly emotion-
bearing words. In contrast, larger lexicons add
more neutral words which tend to be harder to pre-
dict in terms of correlation.

As hypothesized, the MTLNN model does in-
deed outperform the single task model on both
data sets. Our data also suggest that the gain from
the MTL approach is larger on smaller data sets
(again in concordance with our expectations). Fig-
ure 3 reveals that this might be due to the regulariz-
ing effect of MTL, since the SepNN model shows
signs of overfitting on the EN data set. Yet, even

1913



Language Data Embeddings LinReg RidgReg TL Densifier ensembleNN MTLNN

English EN+ GOOGLE 0.696 0.696 0.631 0.622 0.728 0.739***
English EN+ COMMON 0.719 0.719 0.659 0.652 0.762 0.767***
English EN+ WIKI 0.666 0.666 0.591 0.584 0.706 0.712***
English EN GOOGLE 0.717 0.732 0.723 0.712 0.688 0.810***
English EN COMMON 0.731 0.741 0.741 0.726 0.717 0.824***
English EN WIKI 0.656 0.667 0.674 0.665 0.681 0.777***
Spanish ES WIKI 0.698 0.709 0.704 0.690 0.700 0.804***
Spanish ES+ WIKI 0.693 0.694 0.603 0.598 0.766 0.778***
German DE WIKI 0.709 0.719 0.714 0.710 0.700 0.801***
Chinese ZH WIKI 0.716 0.717 0.586 0.599 0.737 0.744**
Polish PL WIKI 0.650 0.650 0.577 0.553 0.687 0.712***
Italian IT WIKI 0.656 0.665 0.672 0.659 0.630 0.751***
Portuguese PT WIKI 0.673 0.684 0.685 0.678 0.672 0.768***
Dutch NL WIKI 0.651 0.652 0.559 0.532 0.704 0.730***
Indonesian ID WIKI 0.581 0.586 0.581 0.576 0.575 0.660***

Average 0.638 0.659 0.611 0.605 0.676 0.728***

Table 4: Results of our main experiment in averaged Pearson correlation; best result per condition (in rows) in
bold, second best result underlined; significant difference (paired two-tailed t-test) over the second best system
marked with “*”, “**”, or “***” for p < .05, .01, or .001, respectively.

when the separate model does not overfit (as on
the EN+ lexicon), MTLNN reveals better results.

Although SepNN needs fewer training steps be-
fore convergence, the MTLNN model trains much
faster, thus still converging faster in terms of run-
time (about a minute on a middle-class GPU). This
is because MTLNN has only about a third as many
parameters as the separate model SepNN.

4.2 Comparison against Reference Methods

We combined each of the selected lexicon data
sets (Table 1) with each of the applicable publicly
available embedding models (Section 2; the em-
bedding model provided by Sedoc et al. (2017)
will be used separately) for a total of 15 condi-
tions, i.e, the rows in Table 4.

For each of these conditions, we performed a
10-fold cross-validation (CV) for each of the 6
methods presented in Section 3 such that each
method is presented with the identical data splits.8

For each condition, algorithm, and VA(D) dimen-
sion, we compute the Pearson correlation r be-
tween gold ratings and predictions. For concise-
ness, we present only the average correlation over
the respective affective dimensions in Table 4 (Va-
lence and Arousal for ES+ and ZH, VAD for the
others). Note that the methods we compare our-
selves against comprise the current state-of-the art
in both polarity and emotion induction (as de-
scribed in Section 2).

8This procedure constitutes a more direct comparison
than using different splits for each method and allows using
paired t-tests.

As can be seen, our proposed MTLNN model
outperforms all other approaches in each of the 15
conditions. Regarding the average over all affec-
tive dimensions and conditions, it outperforms the
second best system, ensembleNN, by more than
5%-points. In line with our results from Sec-
tion 4.1, those improvements are especially pro-
nounced on smaller data sets containing one up
to two thousand entries (EN, ES, IT, PT, ID) with
close to 10%-points improvement over the respec-
tive second-best system.

Concerning the relative ordering of the affec-
tive dimensions, in line with former studies (Sedoc
et al., 2017; Li et al., 2017), the performance fig-
ures for the Valence dimension are usually much
higher than for Arousal and Dominance. Using
MTLNN, for many conditions, we see the pat-
tern that Valence is about 10%-points above the
VAD average, Arousal being 10%-points below
and Dominance being roughly equal to the aver-
age over VAD (this applies, e.g., to EN, EN+ and
IT). On other data sets (e.g., PL, NL and ID), the
ordering between Arousal and Dominance is less
clear though Valence still stands out with the best
results. We observe the same general pattern for
the reference methods, as well.

Concerning the comparison to Sedoc et al.
(2017), arguably one of most related contributions,
they report a performance of r = .768 for Valence
and .582 for Arousal on the EN+ data set in a 10-
fold CV using their own embeddings. In contrast,
MTLNN using the COMMON model achieves r =
.870 and .674 in the same set-up—about 10%-

1914



Valence Arousal Dominance
MTLNN EN .918 .730 .825
MTLNN EN+ .870 .674 .758
ISR EN ∼ EN+ .953 .759 .795
SHR EN+ .914 .689 .770

Table 5: Comparison of the MTLNN model against
inter-study reliability (ISR) between the EN and the
EN+ data set and split-half reliability (SHR) of the EN+
data set (in Pearson correlation).

points better on both dimensions. However, the
COMMON model was trained on much more data
than the embeddings Sedoc et al. (2017) use. For
the most direct comparison, we also repeated this
experiment using their embedding model (GIGA).
We find that MTLNN still clearly outperforms
their results with r = .814 for Valence and .607
for Arousal.9

MTLNN achieves also very strong results in di-
rect comparison to human performance (see Table
5). Warriner et al. (2013) (who created EN+) re-
port an inter-study reliability (ISR; i.e., the corre-
lation of the aggregated ratings from two different
studies) between the EN and the EN+ lexicon of
r = .953, .759 and .795 for VAD, respectively.
Since EN is a subset of EN+, we can compare
these performance figures against our own results
on the EN data set where we achieved r = .918,
.730 and .825, respectively. Thus, our proposed
method did actually outperform human reliability
for Dominance and is competitive for Valence and
Arousal, as well.

This general observation is also backed up by
split-half reliability data (SHR; i.e., when ran-
domly splitting all individual ratings in two groups
and averaging the ratings within each group, how
strong is the correlation between these averaged
ratings?). For the EN+ data set, Warriner et al.
(2013) report an SHR of r = .914, .689 and .770
for VAD, respectively. Again, our MTLNN model
performs very competitive with r = .870, .674
and .758, respectively using the COMMON embed-
dings.

5 Conclusion

In this paper, we propose multi-task learning
(MTL) as a simple, yet surprisingly efficient
method to improve the performance and, at the
same time, to deal with existing data limitations

9We also clearly outperform their results for the NL and
ES+ data sets. For these cases, our embedding models were
similar in training size.

in word emotion induction—the task to predict
a complex emotion score for an individual word.
We validated our claim that MTL is superior to
single-task learning by achieving better results
with our proposed method in performance as well
as training time compared to its single-task coun-
terpart. We performed an extensive evaluation of
our model on 9 typologically diverse languages,
using different kinds of word embedding mod-
els for a total 15 conditions. Comparing our
approach to state-of-the-art methods from word
polarity and word emotion induction, our model
turns out to be superior in each condition, thus set-
ting a novel state-of-the-art performance for both
polarity and emotion induction. Moreover, our re-
sults are even competitive to human annotation re-
liability in terms of inter-study as well as split-half
reliability. Since this contribution was restricted to
the VAD format of emotion representation, in fu-
ture work we will examine whether MTL yields
similar gains for other representational schemes,
as well.

Acknowledgments

We would like to thank the Positive Psychology
Center, University of Pennsylvania for providing
us with the embedding model used in Sedoc et al.
(2017), Johannes Hellrich, JULIE Lab, for insight-
ful discussions, and the reviewers for their valu-
able comments.

References
Muhammad Abdul-Mageed and Lyle H. Ungar. 2017.

EMONET: Fine-grained emotion detection with
gated recurrent neural networks. In ACL 2017 —
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics. Vancouver,
British Columbia, Canada, July 30 - August 4, 2017,
volume 1: Long Papers, pages 718–728.

Silvio Amir, Ramón F. Astudillo, Wang Ling, Bruno
Martins, Mário J. Silva, and Isabel Trancoso. 2015.
INESC-ID: A regression model for large scale Twit-
ter sentiment lexicon induction. In SemEval 2015 —
Proceedings of the 9th International Workshop on
Semantic Evaluation @ NAACL-HLT 2015. Denver,
Colorado, USA, June 4-5, 2015, pages 613–618.

Jonathan Baxter. 1997. A Bayesian/information the-
oretic model of learning to learn via multiple task
sampling. Machine Learning 28(1):7–39.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomáš Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics 5(1):135–146.

1915



Margaret M. Bradley and Peter J. Lang. 1994. Measur-
ing emotion: The Self-Assessment Manikin and the
semantic differential. Journal of Behavior Therapy
and Experimental Psychiatry 25(1):49–59.

Margaret M. Bradley and Peter J. Lang. 1999. Af-
fective Norms for English Words (ANEW): Stimuli,
instruction manual and affective ratings. Technical
Report C-1, The Center for Research in Psychophys-
iology, University of Florida, Gainesville, FL.

Sven Buechel and Udo Hahn. 2016. Emotion analy-
sis as a regression problem: Dimensional models
and their implications on emotion representation and
metrical evaluation. In ECAI 2016 — Proceedings
of the 22nd European Conference on Artificial Intel-
ligence. Including Prestigious Applications of Arti-
ficial Intelligence (PAIS 2016). The Hague, Nether-
lands, August 29 - September 2, 2016, volume 285
of Frontiers in Artificial Intelligence and Applica-
tions, pages 1114–1122.

Sven Buechel and Udo Hahn. 2017. EMOBANK:
Studying the impact of annotation perspective and
representation format on dimensional emotion anal-
ysis. In EACL 2017 — Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics. Valencia, Spain,
April 3–7, 2017, volume 2: Short Papers, pages
578–585.

Sven Buechel and Udo Hahn. 2018. Representation
mapping: A novel approach to generate high-quality
multi-lingual emotion lexicons. In LREC 2018 —
Proceedings of the 11th International Conference
on Language Resources and Evaluation. Miyazaki,
Japan, May 7–12, 2018.

Rafael A. Calvo and Sunghwan Mac Kim. 2013. Emo-
tions in text: Dimensional and categorical models.
Computational Intelligence 29(3):527–543.

Rich Caruana. 1997. Multitask learning. Machine
Learning 28(1):41–75.

Ronan Cummins, Meng Zhang, and Ted Briscoe. 2016.
Constrained multi-task learning for automated essay
scoring. In ACL 2016 — Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics. Berlin, Germany, August 7-12, 2016,
volume 2: Short Papers, pages 789–799.

Harris Drucker. 1997. Improving regressors using
boosting techniques. In ICML ’97 — Proceedings
of the 14th International Conference on Machine
Learning. Nashville, Tennessee, USA, July 8-12,
1997, pages 107–115.

Steven Du and Xi Zhang. 2016. Aicyber’s system
for IALP 2016 Shared Task: Character-enhanced
word vectors and boosted neural networks. In IALP
2016 — Proceedings of the [20th] 2016 Interna-
tional Conference on Asian Language Processing.
Tainan, Taiwan, November 21-23, 2016, pages 161–
163.

Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion 6(3-4):169–200.

Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad
Rahwan, and Sune Lehmann. 2017. Using millions
of emoji occurrences to learn any-domain represen-
tations for detecting sentiment, emotion and sar-
casm. In EMNLP 2017 — Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing. Copenhagen, Denmark, Septem-
ber 9-11, 2017, pages 1615–1625.

Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomáš Mikolov. 2018. Learning
word vectors for 157 languages. In LREC 2018
— Proceedings of the 11th International Conference
on Language Resources and Evaluation. Miyazaki,
Japan, May 7–12, 2018.

William L. Hamilton, Kevin Clark, Jure Leskovec, and
Daniel Jurafsky. 2016. Inducing domain-specific
sentiment lexicons from unlabeled corpora. In
EMNLP 2016 — Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing. Austin, Texas, USA, November 1-5,
2016, pages 595–605.

Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In ACL-EACL 1997 — Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics & 8th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics. Madrid, Spain, July 7-12, 1997, pages
174–181.

Kamil K. Imbir. 2016. Affective Norms for 4900
Polish Words Reload (ANPW R): Assessments for
valence, arousal, dominance, origin, significance,
concreteness, imageability and, age of acquisition.
Frontiers in Psychology 7:#1081.

Diederik Kingma and Jimmy Ba. 2015. ADAM: A
method for stochastic optimization. In ICLR 2015
— Proceedings of the 3rd International Conference
on Learning Representations. San Diego, California,
USA, May 7-9, 2015.

Maximilian Köper and Sabine Schulte im Walde. 2016.
Automatically generated affective norms of abstract-
ness, arousal, imageability and valence for 350,000
German lemmas. In LREC 2016 — Proceedings of
the 10th International Conference on Language Re-
sources and Evaluation. Portorož, Slovenia, 23-28
May 2016, pages 2595–2598.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics 3:211–225.

Minglei Li, Qin Lu, Yunfei Long, and Lin Gui. 2017.
Inferring affective meanings of words from word
embedding. IEEE Transactions on Affective Com-
puting 8(4):443–456.

1916



PengFei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In ACL 2017 — Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics. Vancouver, British Columbia, Canada,
July 30 - August 4, 2017, volume 1: Long Papers,
pages 1–10.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015. Representation
learning using multi-task deep neural networks for
semantic classification and information retrieval. In
NAACL-HLT 2015 — Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Denver, Colorado, USA, May
31 - June 5, 2015, pages 912–921.

Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In Proceedings of the Work-
shop on Deep Learning for Audio, Speech and Lan-
guage Processing @ ICML 2013. Atlanta, Georgia,
USA, 16 June 2013.

Tomáš Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Advances in Neural Information Pro-
cessing Systems 26 — NIPS 2013. Proceedings of
the 27th Annual Conference on Neural Information
Processing Systems. Lake Tahoe, Nevada, USA, De-
cember 5-10, 2013, pages 3111–3119.

Saif M. Mohammad and Felipe Bravo-Marquez. 2017.
WASSA-2017 Shared Task on Emotion Intensity. In
WASSA 2017 — Proceedings of the 8th Workshop
on Computational Approaches to Subjectivity, Sen-
timent and Social Media Analysis @ EMNLP 2017.
Copenhagen, Denmark, September 8, 2017, pages
34–49.

Maria Montefinese, Ettore Ambrosini, Beth Fairfield,
and Nicola Mammarella. 2014. The adaptation of
the Affective Norms for English Words (ANEW) for
Italian. Behavior Research Methods 46(3):887–903.

Agnes Moors, Jan De Houwer, Dirk Hermans,
Sabine Wanmaker, Kevin Van Schie, Anne-Laura
Van Harmelen, Maarten De Schryver, Jeffrey
De Winne, and Marc Brysbært. 2013. Norms of
valence, arousal, dominance, and age of acquisition
for 4,300 Dutch words. Behavior Research Methods
45(1):169–177.

Robert Parker, David Graff, Junbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English Gigaword Fifth
Edition. Technical Report LDC2011T07, Linguis-
tic Data Consortium, Philadelphia, Pennsylvania,
USA. https://catalog.ldc.upenn.edu/
LDC2011T07.

Hao Peng, Sam Thomson, and Noah A. Smith. 2017.
Deep multitask learning for semantic dependency
parsing. In ACL 2017 — Proceedings of the 55th An-
nual Meeting of the Association for Computational

Linguistics. Vancouver, British Columbia, Canada,
July 30 - August 4, 2017, volume 1: Long Papers,
pages 2037–2048.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GLOVE: Global vectors for
word representation. In EMNLP 2014 — Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing. Doha, Qatar, Oc-
tober 25-29, 2014, pages 1532–1543.

Jaime Redondo, Isabel Fraga, Isabel Padrón, and
Montserrat Comesaña. 2007. The Spanish adapta-
tion of ANEW (Affective Norms for English Words).
Behavior Research Methods 39(3):600–605.

Sara Rosenthal, Preslav I. Nakov, Svetlana Kir-
itchenko, Saif M. Mohammad, Alan Ritter, and
Veselin Stoyanov. 2015. SemEval 2015 Task 10:
Sentiment analysis in Twitter. In SemEval 2015 —
Proceedings of the 9th International Workshop on
Semantic Evaluation @ NAACL-HLT 2015. Denver,
Colorado, USA, June 4-5, 2015, pages 451–463.

Sascha Rothe, Sebastian Ebert, and Hinrich Schütze.
2016. Ultradense word embeddings by orthogonal
transformation. In NAACL-HLT 2016 — Proceed-
ings of the 2016 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies. San
Diego, California, USA, June 12-17, 2016, pages
767–777.

James A. Russell. 1980. A circumplex model of af-
fect. Journal of Personality and Social Psychology
39(6):1161–1178.

James A. Russell and Albert Mehrabian. 1977. Evi-
dence for a three-factor theory of emotions. Journal
of Research in Personality 11(3):273–294.

David S. Schmidtke, Tobias Schröder, Arthur M. Ja-
cobs, and Markus Conrad. 2014. ANGST: Affective
Norms for German Sentiment Terms, derived from
the Affective Norms for English Words. Behavior
Research Methods 46(4):1108–1118.

João Sedoc, Daniel Preoţiuc-Pietro, and Lyle H. Un-
gar. 2017. Predicting emotional word ratings using
distributional representations and signed clustering.
In EACL 2017 — Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics. Valencia, Spain, April 3-
7, 2017, volume 2: Short Papers, pages 564–571.

Hendra Setiawan, Zhongqiang Huang, Jacob Devlin,
Thomas Lamar, Rabih Zbib, Richard M. Schwartz,
and John Makhoul. 2015. Statistical machine trans-
lation features with multitask tensor networks. In
ACL-IJCNLP 2015 — Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics & 7th International Joint Conference
on Natural Language Processing of the Asian Fed-
eration of Natural Language Processing. Beijing,
China, July 26-31, 2015, volume 1: Long Papers,
pages 31–41.

1917



Agnes Sianipar, Pieter van Groenestijn, and Ton Di-
jkstra. 2016. Affective meaning, concreteness, and
subjective frequency norms for Indonesian words.
Frontiers in Psychology 7:#1907.

Ana Paula Soares, Montserrat Comesaña, Ana P Pin-
heiro, Alberto Simões, and Carla Sofia Frade. 2012.
The adaptation of the Affective Norms for English
Words (ANEW) for European Portuguese. Behavior
Research Methods 44(1):256–269.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In ACL 2016 — Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Berlin, Germany, August 7–12, 2016,
volume 2: Short Papers, pages 231–235.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2014. Dropout: A simple way to prevent
neural networks from overfitting. Journal of
Machine Learning Research 15:1929–1958.

Hans Stadthagen-Gonzalez, Constance Imbault,
Miguel A. Pérez-Sánchez, and Marc Brysbært.
2017. Norms of valence and arousal for 14,031
Spanish words. Behavior Research Methods
49(1):111–123.

Ryan A. Stevenson, Joseph A. Mikels, and Thomas W.
James. 2007. Characterization of the affective
norms for English words by discrete emotional cat-
egories. Behavior Research Methods 39(4):1020–
1024.

Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems 21(4):315–346.

Jin Wang, Liang-Chih Yu, K. Robert Lai, and Xuejie
Zhang. 2016a. Community-based weighted graph
model for valence-arousal prediction of affective
words. IEEE/ACM Transactions on Audio, Speech,
and Language Processing 24(11):1957–1968.

Jin Wang, Liang-Chih Yu, K. Robert Lai, and Xuejie
Zhang. 2016b. Dimensional sentiment analysis us-
ing a regional CNN-LSTM model. In ACL 2016 —
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics. Berlin, Ger-
many, August 7-12, 2016, volume 2: Short Papers,
pages 225–230.

Amy Beth Warriner, Victor Kuperman, and Marc Brys-
bært. 2013. Norms of valence, arousal, and dom-
inance for 13,915 English lemmas. Behavior Re-
search Methods 45(4):1191–1207.

Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang,
Yunchao He, Jun Hu, K. Robert Lai, and Xue-
jie Zhang. 2016a. Building Chinese affective re-
sources in valence-arousal dimensions. In NAACL-
HLT 2016 — Proceedings of the 2016 Conference of

the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. San Diego, California, USA, June 12-17,
2016, pages 540–545.

Liang-Chih Yu, Lung-Hao Lee, and Kam-Fai Wong.
2016b. Overview of the IALP 2016 Shared Task on
dimensional sentiment analysis for Chinese words.
In IALP 2016 — Proceedings of the [20th] 2016 In-
ternational Conference on Asian Language Process-
ing. Tainan, Taiwan, November 21-23, 2016, pages
156–160.

1918


