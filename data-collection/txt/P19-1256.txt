



















































A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2673–2679
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2673

A Simple Recipe towards Reducing Hallucination in
Neural Surface Realisation

Feng Nie1∗ Jin-Ge Yao2 Jinpeng Wang2 Rong Pan1 Chin-Yew Lin2
1Sun Yat-Sen University 2Microsoft Research Asia

1fengniesysu@gmail.com, 1panr@sysu.edu.cn
2{jinge.yao, jinpwa, cyl}@microsoft.com

Abstract

Recent neural language generation systems of-
ten hallucinate contents (i.e., producing irrel-
evant or contradicted facts), especially when
trained on loosely corresponding pairs of the
input structure and text. To mitigate this is-
sue, we propose to integrate a language under-
standing module for data refinement with self-
training iterations to effectively induce strong
equivalence between the input data and the
paired text. Experiments on the E2E chal-
lenge dataset show that our proposed frame-
work can reduce more than 50% relative un-
aligned noise from the original data-text pairs.
A vanilla sequence-to-sequence neural NLG
model trained on the refined data has improved
on content correctness compared with the cur-
rent state-of-the-art ensemble generator.

1 Introduction

Neural models for natural language generation
(NLG) based on the encoder-decoder framework
have become quite popular recently (Wen et al.,
2015; Mei et al., 2016; Wiseman et al., 2017; Wen
et al., 2017; Chisholm et al., 2017; Nie et al., 2018,
inter alia). Albeit being appealing for producing
fluent and diverse sentences, neural NLG models
often suffer from a severe issue of content halluci-
nation (Reiter, 2018a), which refers to the problem
that the generated texts often contain information
that is irrelevant to or contradicted with the input.

Given that similar issues have been less reported
or noticed in the latest neural machine translation
systems, we believe that the origin of the issue for
neural NLG comes from the data side. Current
datasets used for training neural NLG systems of-
ten include instances that do not contain the same
amount of information from the input structure
and the output text (Perez-Beltrachini and Gar-
dent, 2017). There is no exception for datasets

∗Contribution during internship at Microsoft.

MR Name Rating Price
Golden Palace 5 out of 5 Cheap

Reference: Golden Palace is a restaurant specializing
in breakfast in the low price range.

Table 1: A loosely corresponded MR-text pair. Bolded
phrases conforms to the MR, underlined words are
domain-specific additional information, and italic val-
ues in the MR are not realised in the reference.

originally intended for surface realisation (“how to
say”) without focusing on content selection (“what
to say”). Table 1 depicts an example, where
the attribute Rating=5 out of 5 in the in-
put meaning representation (MR) is not verbalised
in a reference text written by human, while the
word restaurant in the reference should refer to
an attribute value EatType=Restaurant not
contained in the MR. Without explicit alignments
in between MRs and the corresponding utterances
for guidance, neural systems trained on such data
often produce unexpected errors.

Previous work attempted at injecting indirect
semantic control over the encoder-decoder archi-
tecture (Wen et al., 2015; Dušek and Jurcicek,
2016; Agarwal et al., 2018) or encouraging con-
sistency during training (Chisholm et al., 2017),
without essentially changing to the noisy train-
ing data. One exception is the Slug2Slug system
(Juraska et al., 2018), where the authors use an
aligner with manually written heuristic rules to fil-
ter out unrealized attributes from data.

In this paper, we propose a simple, automatic
recipe towards reducing hallucination for neural
surface realisers by enhancing the semantic equiv-
alence between pairs of MRs and utterances. The
steps include: (1) Build a language understanding
module (ideally well-calibrated) that tries to parse
the MR from an utterance; (2) Use it to recon-
struct the correct attribute values revealed in the
reference texts; (3) With proper confidence thresh-



2674

olding, conduct self-training to iteratively recover
data pairs with identical or equivalent semantics.

Experiments on the E2E challenge benchmark
(Novikova et al., 2017b) show that our frame-
work can reduce more than 50% relative unaligned
noise from original MR-text pairs, and a vanilla
sequence-to-sequence model trained on the refined
data can improve content correctness in both hu-
man and automatic evaluations, when compared
with the current state-of-the-art neural ensemble
system (Juraska et al., 2018).

2 Approach

Our proposed framework consists of a neural nat-
ural language understanding (NLU) module with
iterative data refinement to induce semantically
equivalent MR-text pairs from a dataset contain-
ing a moderate level of noise.

2.1 Notation

Formally, given a corpus with paired meaning rep-
resentations and text descriptions {(R,X)}Ni=1,
the input MR R = (r1, . . . , rM ) is a set of slot-
value pairs rj = (sj , vj), where each rj contains
a slot sj (e.g., rating) and a value vj (e.g., 5
out of 5). The corpus has M pre-defined slots
, and each slot sj has Kj unique categorical val-
ues vj ∈ (cj,1, . . . , cj,Kj ). The corresponding ut-
terance X = (x1, . . . , xT ) is a sequence of words
describing the MR.

2.2 Neural NLU Model

As shown in Figure 1, the NLU model consists of
a self-attentive encoder and an attentive scorer.

Self-Attentive Encoder. The encoder produces
the vector representations of slot-value pairs in
MR and its paired utterance. A slot-value pair
r can be treated as a short sequence W =
(w1, . . . , wn) by concatenating words in its slot
and value. The word sequence W is first repre-
sented as a sequence of word embedding vectors
(v1, . . . , vn) from a pre-trained embedding matrix
E, and then passed through a bidirectional LSTM
layer to yield the contextualized representations
U sv = (usv1 , . . . ,usvn ). To produce a summary
context vector for U sv, we adopt the same self-
attention structure in Zhong et al. (2018) to ob-
tain the sentence vector cs, due to the effectiveness
of self-attention modules over variable-length se-
quences. Similarly, we obtain the contextualized

Name The golden palace

Food English

EatType None

Price Cheap

Rating High

Name The golden palace, …

Food English, French,  …

Type Pub, restaurant, …

Price Cheap, high, …

Rating High, low, …

𝑒1
1

Name

Food

Type

Price

Rating

Discrete latent variable z

Name =         The   Golden   Palace

Self-attention

Attention

Scoring

𝑃(𝑟|𝑋)

Slot-value pair 𝑟

Output text

𝒖1
sv 𝒖2

𝑠𝑣 𝒖3
𝑠𝑣 𝒖4

𝑠𝑣

The    Golden   Palace     is      …

𝒖1
𝑜 𝒖2

𝑜 𝒖3
𝑜 𝒖4

𝑜

𝒄𝑠

𝒅

𝑤1 𝑤2 𝑤3 𝑤4 𝑥1 𝑥2 𝑥3 𝑥4
Utterance 𝑋

Figure 1: The structure of the neural NLU model.

representations Uo = (uo1, . . . ,uoT ) for the utter-
ance X .

Attentive Scorer. The scorer calculates the se-
mantic similarity between a slot-value pair r (e.g.,
Price=Cheap) and the utterance X (e.g., refer-
ence in Table 1). Firstly, an attention layer is ap-
plied to select the most salient words in X related
to r, which yields the attentive representation d of
utterance X . Given the sentence vector cs of the
slot-value pair r and the attentive vector d of the
utterance X , the normalized semantic similarity is
defined as:

p(r|X) = softmax(−||d− cs||2), where

d =
T∑
t=1

btuot , with bt = softmax((u
o
t )

T cs).
(1)

Model Inference. Each utterance X will be
parsed to an MR Re = (re1, . . . , r

e
M ), with each

slot-value pair rej = (sj , vj) determined by select-
ing the candidate value vj with the maximum se-
mantic similarity for each slot sj :

vj = cj,k, k = argmax
k

p(rej = (sj , cj,k)|X),

(2)
where cj,k denotes the kth categorical value for jth
slot. Since an utterance may not describe any in-
formation about a specific slot s, we add a NONE
value as a candidate value of each slot.

Model Training. The NLU model is optimized
by minimizing the cross-entropy loss:

L(θ) = −
N∑
i

M∑
j

log p(ri,j |Xi; θ) (3)

where θ denotes model parameters, and ri,j de-
notes the jth slot-value pair in the ith training MR.



2675

2.3 Iterative Data Refinement

The performance of NLU can be inaccurate when
trained on noisy data-text pairs. However, mod-
els trained on data with a moderate level of noise
could still be well-calibrated. This could enable
an iterative relabeling procedure, where we only
take MRs produced by NLU with high confidence
together with their utterances as new training MR-
text pairs to bootstrap the NLU training.

Algorithm 1 describes the training procedure.
We first pre-train the NLU model using the orig-
inal data-text pairs for Npre iterations. Then the
NLU model parses relevant MR for every utter-
ance in training data, which can be used as new
training examples (Line 4). However, due to the
inaccuracy of the NLU results, we only use a small
portion (φ is set to 40% on validation) with high
confidence. Moreover, as each MR consists of up
toM slots with some of them being unreliable, we
filter the slot-value pairs with slot probability be-
low average according to slot confidence (Line 8 -
14). Finally, the NLU model is fine-tuned with the
new training corpus De. This process is repeated
for Ntune epochs. The final NLU model is lever-
aged to parse all utterances in the training corpus.
The resulting MRs paired with original utterances
form the refined training corpus for NLG.

3 Experiments

3.1 Setup

Dataset. Our experiments are conducted on E2E
challenge (Novikova et al., 2017b) dataset, which
aims at verbalizing all information from the MR.
It has 42,061, 4,672 and 4,693 MR-text pairs for
training, validation and testing, respectively. Note
that every input MR in this dataset has 8.65 dif-
ferent references on average. The test set has 630
unique input MRs. We examine the effectiveness
of our proposed method in two aspects: 1) reduc-
ing the noise in data-text pairs (NLU), 2) reducing
hallucinated contents in surface realisation (NLG).

Automatic metrics. The well-crafted rule-based
aligner built by Juraska et al. (2018)1 is adopted
to approximately reflect the semantic correctness
of NLU and NLG models. The error rate is cal-
culated by matching the slot values in output ut-
terance: Err = MN , where N is the total number

1 We use the public available evaluation script in
https://github.com/jjuraska/slug2slug/blob/master/slot aligner
/data analysis.py

Algorithm 1 Iterative Data Refinement
Require MR-text pairs D = {(R,X)}N1 , confi-
dence threshold φ, pre-training epochs Npre, tun-
ing epochs Ntune,

1: Train θ with Eq. 3 on D for Npre iterations
2: for iter = 1 to Ntune do
3: Reset self-training corpus De = {}
4: Parse the MR Rei = (r

e
i,1, . . . , r

e
i,M ) for ev-

ery Xi using Eq. 2
5: Slot confid. pj =

∑N
i=1 p(r

e
i,j |Xi) for sj

6: MR confid. fi =
∑M

j=1 p(r
e
i,j |Xi) for Rei

7: Sort {(Re, X)}N1 by MR confidence in re-
verse order

8: for i = 1 to bφ ·Nc do
9: for j = 1 to M do

10: if p(rei,j |Xi) < pj/N then
11: Remove rei,j from R

e
i

12: end if
13: end for
14: De ← De ∪ (Rei , Xi)
15: end for
16: Update θ with Eq. 3 on De

17: end for

of MR-text pairs, and M is the number of wrong
MR-text pairs which contain missing or conflict
slots in the realization given its input MR. BLEU-
4 (Papineni et al., 2002) is also reported, although
currently neither BLEU nor any other automatic
metrics could be convincingly used for evaluating
language generation (Novikova et al., 2017a; Cha-
ganty et al., 2018; Reiter, 2018b, inter alia).

Human Evaluation. We randomly sample 100
data-text pairs from test set and ask three crowd
workers to manually annotate missed (M), added
(A), and contradicted (C) slot values in NLG out-
puts with respect to the input MR, or exact match
(E) if all slot values have been realized in the
given utterance which contains no additional hal-
lucinated information. When evaluating the NLU
systems, missed and added slots refer to the oppo-
site directions, respectively.

Compared Systems. Systems in comparison:

• TGen (Dušek et al., 2018): a sequence-to-
sequence (Seq2Seq) model with reranking.
• Slug2Slug (Juraska et al., 2018): cur-

rent state-of-the-art method on E2E challenge
dataset. It is an ensemble model and uses a rule
based aligner for data cleaning and reranking.



2676

• Seq2Seq: a basic Seq2Seq model trained on
original MR-text pairs with the copy mecha-
nism (Gu et al., 2016; See et al., 2017).
• Seq2Seq+aug: Seq2Seq trained on the MR-

text pairs reconstructed by pre-trained NLU.
• Seq2Seq+aug+iter: Seq2Seq trained on

the MR-text pairs reconstructed by NLU model
with iterative data refinement algorithm.
• Seq2Seq+aligner: Seq2Seq trained on

the MR-text pairs produced by the rule based
aligner (Juraska et al., 2018).

Implementation Details. For all models, we use
fixed pre-trained GloVe vectors (Pennington et al.,
2014) and character embeddings (Hashimoto
et al., 2017). The dimensions of trainable hid-
den units in LSTMs are all set to 400. The epochs
for pre-training Npre and bootstrapping Ntune are
all set to 5 on validation. During training, we
regularize all layers with a dropout rate of 0.1.
We use stochastic gradient descent (SGD) for op-
timisation with learning rate 0.1. The gradient
is truncated by 5. For hyper-parameter φ, we
conduct experiments with different values (φ =
0.2, 0.4, 0.6, 0.8, 1.0), details in Appendix A.

3.2 Main Results
NLU Results. One challenge in E2E dataset is
the need to account for the noise in the corpus
as some of the MR-text pairs are not semanti-
cally equivalent due to the data collection pro-
cess (Dušek et al., 2018). We examine the per-
formance of the NLU module by comparing noise
reduction of the reconstructed MR-text pairs with
the original ones in both training and test sets.
Table 2 shows the automatic results. Applying
our NLU model with iterative data refinement,
the error rates of refined MR-text pairs yields
23.33% absolute error reduction on test set. Hu-
man evaluation in Table 3 shows that our proposed
method achieves 16.69% improvement on infor-
mation equivalence between MR-text pairs. These
results confirm the effectiveness of our method in
reducing the unaligned data noise, and the large
improvement (i.e, 15.09%) on exact match when
applying self-training algorithm suggests the im-
portance of iterative data refinement.

NLG Results. Table 4 presents the automatic re-
sults of different neural NLG systems. We can
see that Seq2Seq+aug+iter achieves com-
parable BLEU score as Slug2Slug but with
4.44% error reduction on content correctness over

Train Err(%) Test Err(%)
Original data 35.50 37.59
NLU refined data 16.31 14.26
w/o self-training 25.14 22.69

Table 2: Automatic evaluation results of different NLU
models on both training and test sets

E(%) M(%) A(%) C(%)
Original data 71.93 0 24.13 3.95
NLU refined data 88.62 5.45 2.48 3.47
w/o self-training 73.53 13.23 8.33 4.91

Table 3: Human evaluation results for NLU on test set
(inter-annotator agreement: Fleiss’ kappa = 0.855)

BLEU(%) Err(%)
TGen 65.90 18.09 (114/630)
Slug2Slug 66.19 6.51 (41/630)
Seq2Seq 66.15 69.37 (374/630)
Seq2Seq+aug 66.49 28.89 (182/630)
Seq2Seq+aug+iter 65.63 2.07 (13/630)
Seq2Seq+aligner 63.81 1.75 (11/630)

Table 4: Automatic metrics for NLG

E(%) M(%) A(%) C(%)
TGen 78.49 15.12 2.69 3.3
Slug2Slug 91.36 2.98 0 5.66
Seq2Seq 44.07 50.65 4.03 0.65
Seq2Seq+aug+iter 93.93 3.36 2.69 0

Table 5: Human evaluation results for NLG (inter-
annotator agreement: Fleiss’ kappa = 0.832)

Slug2Slug. Seq2Seq+aug+iter largely
improves the content correctness over the base-
line Seq2Seq with 67.3% error reduction. Be-
sides, we also replace our NLU module with
the rule based aligner crafted by Juraska et al.
(2018) for data refinement to inspect the differ-
ence between our proposed method and manu-
ally designed rich heuristics. We can observe that
these two models (Seq2Seq+aug+iter and
Seq2Seq+aligner) achieve comparable per-
formance, while our approach is fully automatic
and requires no domain knowledge.

The human evaluation results are shown in Ta-
ble 5. We can find that Seq2Seq+aug+iter
improves 2.59% accuracy on exact match over
Slug2Slug. Specifically, Slug2Slug aug-
ments original training data by only deleting ad-
ditional slot values not realized in the utterance
with an aligner, which is not capable of the sit-
uation where the given utterance contains incor-
rect or additional slot values and leads more con-



2677

Utterance: Located in riverside, near Caf Sicilia,
is the Phoenix, a French pub that is family-friendly and
has average prices and an average rating.
Original MR: name[The Phoenix], eatType[pub],
food[French], priceRange[20-25], area[riverside],
customer rating[3 out of 5], familyFriendly[no],
near[Caf Sicilia]
Refined MR: name[The Phoenix], eatType[pub],
food[French], priceRange[moderate], area[riverside],
customer rating[average], familyFriendly[yes],
near[Caf Sicilia]

Table 6: Example for data refinement; The underscored
item is incorrect.

MR

Name:[The Mill]; EatType:[pub];
Food:[Fast Food];PriceRange:[high];
FaimilyFriendly:[yes];Near:[Caf Sicilia];
Area:[riverside]; Rating:[average]

TGen
The Mill is a high priced family friendly
fast food pub located near Caf Sicilia
in the riverside area.

Slug2Slug
children friendly pub in the riverside
area near Caf Sicilia. It has a high
price range and a high customer rating

Seq2Seq The Mill is a family friendly pub locatednear Caf Sicilia.

Seq2Seq+
aug+iter

The Mill is a children friendly fast food
pub near Caf Sicilia in the riverside area.
It has a high price range and an average
customer rating.

Table 7: Examples of different system outputs.

tradicted errors. Our method can complement and
correct original MR with additional slot values de-
scribed in the paired texts to effectively alleviate
generating contradicted facts. However, due to the
imperfection of NLU model, our method may ig-
nore part of slot values realized in utterances and
produce some additional errors.

3.3 Case Study

Example for refined data. Table 6 depicts a
case for one pair with originally inaccurate MR
while being corrected by NLU module and itera-
tive refinement. Our proposed method is capable
of reducing the unaligned noise for original data.

Example for NLG. Table 7 shows the sentences
generated by different NLG systems. Seq2Seq
without any semantic control tends to generate
shorter descriptions. Slug2Slug and TGenwith
reranker to control the content coverage can gen-
erate more input information, but still misses one
input information and Slug2Slug produces a
contradicted fact (i.e., customer rating). Our pro-
posed method Seq2Seq+aug+iter trained on

refined MR-text pairs, verbalises all the input in-
formation correctly, which shows the importance
of data quality in terms of strong equivalence be-
tween MR and utterance.

4 Discussion

In this paper, we present a simple recipe to re-
duce the hallucination problem in neural language
generation: introducing a language understanding
module to implement confidence-based iterative
data refinement. We find that our proposed method
can effectively reduce the noise in the original
MR-text pairs from the E2E dataset and improve
the content coverage for standard neural surface
realisation (no focus on content selection).

However, the currently presented approach still
has two clear limitations. One is that this simple
approach is implicitly built on an assumption of a
moderate level of noise in the original data, which
makes it possible to bootstrap a well-calibrated
NLU module. We are still on the way to find
out solutions for cases with huge noise (Perez-
Beltrachini and Lapata, 2018; Wiseman et al.,
2017), where heavy manual intervention or exter-
nal knowledge should be desperately needed.

The other limitation of this preliminary work is
that it currently overlooks the challenges of lexical
choices for quantities, degrees, temporal expres-
sions, etc, which are rather difficult to learn merely
from data and should require additional common-
sense knowledge. An example case is in Table 6,
where the original priceRange=20-25 is re-
fined to be priceRange=moderate, which
enhances the correspondence between the MR and
the text but sidesteps the lexical choice for num-
bers which requires localised numerical common-
sense. Additional modules for lexical choices
should be expected for a refined system.

5 Acknowledgement

We thank Zhirui Zhang, Shuangzhi Wu, and
the anonymous reviewers for helpful comments.
Feng Nie is partially supported by National
Key R&D Program of China (2018YFB1004404)
and Key R&D Program of Guangdong Province
(2018B010107005). The contact author of this pa-
per, according to the meaning given to this role by
Sun Yat-Sen University, is Rong Pan.



2678

References
Shubham Agarwal, Marc Dymetman, and Eric

Gaussier. 2018. Char2char generation with rerank-
ing for the E2E NLG challenge. In Proceedings
of the 11th International Conference on Natural
Language Generation, pages 451–456, Tilburg Uni-
versity, The Netherlands. Association for Computa-
tional Linguistics.

Arun Chaganty, Stephen Mussmann, and Percy Liang.
2018. The price of debiasing automatic metrics in
natural language evalaution. In Proceedings of the
56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 643–653, Melbourne, Australia. Association
for Computational Linguistics.

Andrew Chisholm, Will Radford, and Ben Hachey.
2017. Learning to generate one-sentence biogra-
phies from Wikidata. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 1, Long
Papers, pages 633–642, Valencia, Spain. Associa-
tion for Computational Linguistics.

Ondřej Dušek and Filip Jurcicek. 2016. Sequence-to-
sequence generation for spoken dialogue via deep
syntax trees and strings. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics.

Ondřej Dušek, Jekaterina Novikova, and Verena Rieser.
2018. Findings of the E2E NLG challenge. In
Proceedings of the 11th International Conference
on Natural Language Generation, pages 322–328,
Tilburg University, The Netherlands. Association for
Computational Linguistics.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1631–1640, Berlin, Germany. Association for
Computational Linguistics.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple NLP
tasks. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1923–1933, Copenhagen, Denmark. As-
sociation for Computational Linguistics.

Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden,
and Marilyn Walker. 2018. A deep ensemble model
with slot alignment for sequence-to-sequence natu-
ral language generation. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers),
pages 152–162, New Orleans, Louisiana. Associa-
tion for Computational Linguistics.

Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.
2016. What to talk about and how? selective gen-
eration using LSTMs with coarse-to-fine alignment.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 720–730, San Diego, California. Association
for Computational Linguistics.

Feng Nie, Jinpeng Wang, Jin-Ge Yao, Rong Pan,
and Chin-Yew Lin. 2018. Operation-guided neu-
ral networks for high fidelity data-to-text genera-
tion. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 3879–3889, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Jekaterina Novikova, Ondřej Dušek, Amanda Cer-
cas Curry, and Verena Rieser. 2017a. Why we
need new evaluation metrics for NLG. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2241–2252,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Jekaterina Novikova, Ondřej Dušek, and Verena Rieser.
2017b. The E2E dataset: New challenges for end-
to-end generation. In Proceedings of the 18th An-
nual SIGdial Meeting on Discourse and Dialogue,
pages 201–206, Saarbrücken, Germany. Association
for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Laura Perez-Beltrachini and Claire Gardent. 2017.
Analysing data-to-text generation benchmarks. In
Proceedings of the 10th International Conference on
Natural Language Generation, pages 238–242, San-
tiago de Compostela, Spain. Association for Com-
putational Linguistics.

Laura Perez-Beltrachini and Mirella Lapata. 2018.
Bootstrapping generators from noisy data. In Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 1 (Long Papers), pages 1516–1527, New Or-
leans, Louisiana. Association for Computational
Linguistics.

Ehud Reiter. 2018a. Hallucination in neural NLG.

https://www.aclweb.org/anthology/W18-6555
https://www.aclweb.org/anthology/W18-6555
https://www.aclweb.org/anthology/P18-1060
https://www.aclweb.org/anthology/P18-1060
https://www.aclweb.org/anthology/E17-1060
https://www.aclweb.org/anthology/E17-1060
https://www.aclweb.org/anthology/P16-2008
https://www.aclweb.org/anthology/P16-2008
https://www.aclweb.org/anthology/P16-2008
https://www.aclweb.org/anthology/W18-6539
https://doi.org/10.18653/v1/D17-1206
https://doi.org/10.18653/v1/D17-1206
https://doi.org/10.18653/v1/D17-1206
https://www.aclweb.org/anthology/N18-1014
https://www.aclweb.org/anthology/N18-1014
https://www.aclweb.org/anthology/N18-1014
https://www.aclweb.org/anthology/N16-1086
https://www.aclweb.org/anthology/N16-1086
https://www.aclweb.org/anthology/D18-1422
https://www.aclweb.org/anthology/D18-1422
https://www.aclweb.org/anthology/D18-1422
https://doi.org/10.18653/v1/D17-1238
https://doi.org/10.18653/v1/D17-1238
https://www.aclweb.org/anthology/P02-1040
https://www.aclweb.org/anthology/P02-1040
https://www.aclweb.org/anthology/D14-1162
https://www.aclweb.org/anthology/D14-1162
https://www.aclweb.org/anthology/W17-3537
https://www.aclweb.org/anthology/N18-1137
https://ehudreiter.com/2018/11/12/hallucination-in-neural-nlg/


2679

Ehud Reiter. 2018b. A structured review of the validity
of BLEU. Computational Linguistics, 44(3):393–
401.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073–
1083, Vancouver, Canada. Association for Compu-
tational Linguistics.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
1711–1721, Lisbon, Portugal. Association for Com-
putational Linguistics.

Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić,
Milica Gasic, Lina M. Rojas Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017. A network-
based end-to-end trainable task-oriented dialogue
system. In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, pages
438–449, Valencia, Spain. Association for Compu-
tational Linguistics.

Sam Wiseman, Stuart Shieber, and Alexander Rush.
2017. Challenges in data-to-document generation.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2253–2263, Copenhagen, Denmark. Association for
Computational Linguistics.

Victor Zhong, Caiming Xiong, and Richard Socher.
2018. Global-locally self-attentive encoder for dia-
logue state tracking. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1458–
1467, Melbourne, Australia. Association for Com-
putational Linguistics.

0 20 40 60 80 100
 (%)

5

10

15

20

25

30

C
on

te
nt

 C
ov

er
ag

e 
E

rr
or

 R
at

e 
(%

)

Figure 2: The effect of hyperparameter φ for NLG con-
tent coverage performance.

A Effect of φ on NLG model

The parameter φ controls the proportion of rele-
vant MRs produced by NLU model for iterative
training. Figure 2 shows its influence for NLG
on the content coverage measurement. The ex-
perimental result shows NLG models trained on
data produced by self-training achieve error reduc-
tion in content coverage. As the NLU model can
bring inaccurate instances when performing iter-
ative data augmentation, controlling the propor-
tion φ from 20% to 40% can yield better results
compared to introducing all the MRs produced by
NLU for self-training.

https://doi.org/10.1162/coli_a_00322
https://doi.org/10.1162/coli_a_00322
https://www.aclweb.org/anthology/P17-1099
https://www.aclweb.org/anthology/P17-1099
https://www.aclweb.org/anthology/D15-1199
https://www.aclweb.org/anthology/D15-1199
https://www.aclweb.org/anthology/E17-1042
https://www.aclweb.org/anthology/E17-1042
https://www.aclweb.org/anthology/E17-1042
https://www.aclweb.org/anthology/D17-1239
https://www.aclweb.org/anthology/P18-1135
https://www.aclweb.org/anthology/P18-1135

